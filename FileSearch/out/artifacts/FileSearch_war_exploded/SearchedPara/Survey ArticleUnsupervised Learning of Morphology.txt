 Radboud Universiteit and Max Planck Institute for Evolutionary Anthropology University of Gothenburg This article surveys work on Unsupervised Learning of Morphology. We define Unsupervised
Learning of Morphology as the problem of inducing a description (of some kind, even if only morpheme segmentation) of how orthographic words are built up given only raw text data of a language. We briefly go through the history and motivation of this problem. Next, over 200 developments. 1. Introduction
Morphology is understood here in its usual sense in linguistics, namely, as referring to (the linguistic study and description of) the internal structure of words. More specifi-cally, we understand morphology following Haspelmath (2002, page 2) as  X  X he study of systematic covariation in the form and meaning of words. X  For our purposes, we assume that we have a way of identifying the text words of a language, ignoring the fact that the term word has eluded exhaustive cross-linguistic definition. Similarly, we assume a number of commonly made distinctions in linguistic morphology, whose basic import is indisputable, but where there is an ongoing discussion on exactly where to draw the boundaries with respect to particular phenomena in individual languages. tion . Inflectional morphology deals with the various realizations of the  X  X ame X  lexical word, depending on the particular syntactic context in which the word appears. Typical examples of inflection are verbs agreeing with one or more of their arguments in the clause, or nouns inflected in particular case forms in order to show their syntactic rela-tion to other words in the phrase or clause, for example, showing which verb argument they express. Word formation deals with the creation of new lexical words from existing ones, for example, agent nouns from verbs. If the same kinds of mechanisms are used as in inflectional morphology (i.e., the resulting word is derived out of only one existing word), linguists talk about derivational morphology . If two or more existing lexical words are combined in order to make up a new word, the terms compounding or incorporation are used, depending on the categories of the words involved.
 pressing inflectional and derivational categories in languages. Most commonly, how-ever, some form of affixation is involved X  X hat is, some phonological material is added (much more rarely) inside the stem of the word (infixation). Suffixes and prefixes (but rarely infixes) can form long chains, where the different positions, or  X  X lots, X  express and/or prefixing X  X ometimes called concatenative morphology X  X t obviously follows that text words in that language can be segmented into a sequence of morphological stem. 1 tion by necessity omits many intricacies and greatly simplifies a vast scholarship. (For standard, in-depth, introductions to this fascinating field, see, e.g., Nida [1949], Jensen [1990], Spencer and Zwicky [1998], or Haspelmath [2002].) between texts and structured information about the vocabulary of a language. Some kind of morphological analysis and/or generation thus forms a basic component in many natural language processing applications. Many languages have quite complex morphological systems, with the number of potential inflected forms of a single lex-ical word running into the thousands, requiring a substantial amount of work if the linguistic knowledge of the morphological component is to be defined manually. For this reason, researchers often turn to machine learning approaches. This survey article is concerned with unsupervised approaches to morphology learning.
 supervised Learning of Morphology (ULM).
 Input: Raw (unannotated, non-selective 2 ) natural language text data
Output: A description of the morphological structure (there are various levels to be
With: As little supervision (parameters, thresholds, human intervention, model guages; they are nevertheless considered to be ULM for this survey. Morphology may be narrowly taken as to include only derivational and inflectional affixation, where the 310 permuted. 4 This survey also subsumes attempts that take a broader view including clitics 5 and compounding (and there seems to be no reason in principle to exclude incorporation and lexical affixes: see Mithun [1999], pages 37 X 67, for some examples). Many, but not all, approaches focus on concatenative morphology/compounding only. words, that is, raw text data in an orthography that provides a ready-made segmen-only targets word segmentation, that is, segmenting a sentence or a full utterance into words (cf. Goldsmith [2010] who also overviews word segmentation). However, works that explicitly aim to treat both word segmentation and morpheme segmentation in one algorithm are included. Hence, subsequent uses of the term segmentation in the present survey are to be understood as morpheme segmentation rather than word segmentation. We prefer the term segmentation to analysis because, in general in ULM, the algorithm does not attempt to label the segments.

ULM as defined here, the most popular ones being:
Such approaches are excluded from the present survey, unless the required data (e.g., paradigm members) are extracted from raw text in an unsupervised manner as well.
We also exclude the special case of the second approach where morphology learning means not  X  X earning the morphological system of a language, X  but rather  X  X earning the inflectional classes of out-of-vocabulary words, X  namely, approaches where an existing morphological analysis component is used as the basis for guessing in which existing paradigm an unknown text word should belong (e.g., Antworth 1990; Mikheev 1997; Bharati et al. 2001; Forsberg, Hammarstr X m, and Ranta 2006; Lind X n 2008; Lind X n 2009). need of course not correspond to steps taken in an actual algorithm). The division is implicational in the sense that if one can do the morphological analysis of a lower level in the table, one can also easily produce the analysis of any of the levels above it.
Reflecting a fundamental assumption underlying most ULM work, form and meaning (semantics) are kept separate in the table (see Section 2). For example, if one can perform segmentation into stem and affixes, one can decide if two words are of the same stem (if meaning is disregarded) or the same lexeme (if meaning is taken into account). The converse need not hold; it is perfectly possible to answer the question of whether two words are of the same stem with high accuracy, without having to commit to what the actual stem should be.
 reinvent heuristics that have been proposed earlier, and there is little modularization taking place. Previous surveys and overviews for a general audience are Borin (1991),
Batchelder (1997, pages 66 X 68), Powers (1998), Clark (2001, pages 80 X 82), Xanthos (2007, pages 95 X 107), Goldsmith (2001), Daelemans (2004, page 1898), Roark and Sproat (2007, pages 116 X 136), Hammarstr X m (2007b, pages 10 X 15), Chan (2008, pages 48 X 60),
Hammarstr X m (2009b, pages 14 X 21), Borin (2009), Goldsmith (2010), and, to a more limited extent, the related-work sections of individual research papers. Kurimo, Creutz, and Turunen (2007), Kurimo, Creutz, and Varjokallio (2008a, 2008b), Kurimo and
Turunen (2008), Kurimo and Varjokallio (2008), and McNamee (2008) are overviews of systems in the MorphoChallenge of the respective year. However, we will try to be more comprehensive than previous surveys and discuss the ideas in the field critically. 312 impossible, not only because of the great variation in goals but also because most descriptions do not specify their algorithm(s) in enough detail. Fortunately, this aspect is better handled in controlled competitions, such as the Unsupervised Morpheme Analysis X  MorphoChallenge 6 which offers tasks of segmentation of Finnish, English, German,
Arabic, and Turkish. 2. History and Motivation of ULM
Usually and justifiedly, the work of Harris (1955, 1967) is given as the starting point of ULM. From another perspective, however, the same work by Harris can be said to known as American structuralism , to formalize the process of linguistic description into so-called linguistic discovery procedures .
 ization of linguistic discovery procedures is often connected with the name of Leonard
Bloomfield, and its core tenet may be succinctly summed up in Bloomfield X  X  oft-quoted dictum:  X  X he only useful generalizations about language are inductive generalizations X  (Bloomfield 1933, page 20). The so-called  X  X xtremist Post-Bloomfieldians X  took this program a step further:  X  X rom Bloomfield X  X  justified insistence on formal, rather than
Harris) set up as a theoretical aim the description of linguistic structure exclusively in terms of distribution X  (Hall 1987, page 156).
 motivated by, for example, a desire to simulate language acquisition in humans.
Andreev launched a program much like that of Harris. 7 Andreev X  X  work is much less known than that of Harris X  X , and for this reason we will describe it in some detail here.
In a series of publications (Andreev 1959, 1963, 1965b, 1967), he develops an  X  X lgorithm for statistical-combinatory modeling of languages. X  This is part of a research program meaning completely from the process of  X  X iscovery X  of language structure.
 way up to syntax, using basically one and the same approach grounded in text (corpus) statistics. Given our focus on ULM, here we will be concerned only with his approach as applied to morphological segmentation.
 extent in language typology X  X han Harris X  X  work. The algorithm for morphological segmentation is described in some detail in the works of Andreev and his colleagues.
It relies on statistics of letter frequencies in a text corpus, and of average word length in characters and average sentence length in words. From these statistics he calculates a number of heuristic thresholds which are used to iteratively grow affix candidates from characters at given positions in text words, and paradigm candidates from the resulting segmentations. Instead of looking at successor/predecessor counts or transition proba-bilities, Andreev looks at character positions in relation to word edges, from the first and the last character inwards no further than the average word length. At each position, the amount of overrepresentation is calculated for each character found in this position in some word. The overrepresentation ( X  X orrelative function X  in Andreev X  X  terminology) is defined as the relative frequency of the character in this word position divided by its relative frequency in the corpus. The character X  X osition combinations are used in order of decreasing overrepresentation in an iterative see-saw procedure, where affix and stem candidates are collected in alternating iterations of the algorithm. Andreev X  X  approach reflects the same intuition as that of Harris; we would expect word-edge sequences of highly overrepresented characters to be flanked by marked differences in predecessor or successor counts calculated according to Harris X  X  method. ted) is the following, originally presented by Andreeva (1963), but the presentation here is partly based on that in Andreev (1967).
 frequent as in the corpus as a whole (Andreeva 1963, page 49). For the words ending in from corpus statistics, the first affix candidate found was -oj (Russian -&lt; this ending from all words in which it appears and matching the remainders of the words (i.e., putative stems) against the other words of the corpus, yields a set of words from which additional suffix candidates emerge (including the null suffix). This set of words is then iteratively reduced, using the admissible suffix candidates (those below a certain length exceeding a heuristic threshold of overrepresentation) in each step, as long as at least two stem candidates remain. In other words: There must be at least two stems in the corpus appearing with all the suffix candidates. In the Russian experiment reported by Andreeva (1963), a complete adjective paradigm was induced, with 12 different suffixes. The initial suffix candidate, -oj , has a high functional load and conse-quently a high text frequency: It is the most ambiguous of the Russian adjective suffixes, appearing in four different slots in the adjective paradigm, and is also homonymous with a noun suffix.
 of several papers in the volume, and a number of other languages: Albanian (Per X ikov 1965), Armenian (Melkumjan 1965), Bulgarian (Fedulova 1965), Czech (O X igova 1965), English (Malahovskij 1965), Estonian (Hol X  X  1965), French (Kordi 1965), German (Fitialova 1965), Hausa (Fihman 1965a), Hungarian (Andreev 1965a), Latvian (Jakubajtis 1965), Serbo-Croatian (Panina 1965), Swahili (Fihman 1965b), Ukrainian (Eliseeva 1965), and Vietnamese (Jaku X eva 1965). As an aside, we may note that only after the turn of the millennium are we again seeing this variety of languages in ULM work. Most of these studies are small-scale proof-of-concept experiments on corpora of varying sizes (from a few thousand words in many of the studies up to close to one million words for
Russian). The outcomes are more often than not quite small  X  X aradigmoid fragments, X  that is, incomplete and not always in correspondence with traditional segmentations.
It is noteworthy, however, that the method could not produce a single instance of morphological segmentation for Vietnamese (Jaku X eva 1965, page 228), which is as it should be, because Vietnamese is often held forth as a language without morphology. what has been done. In fact, computers are not mentioned at all in most of the papers; on the contrary, it is quite clear that at least some of the experiments have been carried out manually. In principle, because Andreev and the other authors in Andreev (1965b) 314 findings (cf. Altmann and Lehfeldt 1980, pages 195 X 198). To our knowledge, there has been one attempt to do this, by Cromm (1997), who reimplemented the method and tested it on the German Bible, experimenting with various parameter settings and also making some changes to the method itself. He notes that several parameters that Andreev provides mostly without motivation or comment in fact can be changed in a more accepting direction, leading to much increased recall without much loss in precision. Unfortunately, however, in his short paper, Cromm does not provide enough information about the algorithm or the changes that he made to it, so that the
Russian original is still the only publicly available source for the details of Andreev X  X  approach.
 beginning with the supervised morphology learning ideas by Wothke (1985, 1986) and Klenk (1985a, 1985b) which later led to partly unsupervised methods (see the following).
Because full natural language lexica, at the time, were too big to fit in working memory, these authors were looking for a way to analyze or stem running words in a  X  X icht-motivation is now obsolete.
 a few works appearing between the mid 1960s and 1990. Especially in the 1980s, the focus in computational morphology was on the development of finite-state approaches rose greatly, in the wake of a general increased attention during the 1990s to statistical and information-theoretically informed approaches in natural language processing.
In speech processing, the problem of word segmentation is ever-present, and as the computational tools for taking on this problem became increasingly sophisticated and hardware and software, researchers in linguistics and computational linguistics started taking a fresh look at the problems of word segmentation and ULM.
 pulls together a number of strands from earlier work, sets them against a theoretical background informed both by information theory (MDL) and linguistics, and uses them specifically to address the problem of ULM X  X n particular, unsupervised learning of inflectional morphology X  X nd not, for instance, that of word segmentation or of stemming for information retrieval, and so forth.
 tions in the field of first-language acquisition (see, e.g., Brent, Murthy, and Lundberg 1995; Batchelder 1997; Brent 1999; Clark 2001; Goldwater 2007). However, the connec-tion is still rather vague and even if ULM has matured, it is not clear what implications, if any, this has for child language acquisition. Children have access to semantics and pragmatics, not just text strings, and it would be very surprising if such cues were not used at all in first language acquisition. Further, if some ULM technique was shown to be successful on some reasonably sized corpora, it does not automatically follow that children can (and do, if they can) use the same technique. Most current ULM techniques crucially involve long series of number crunching that seem implausible for the child-learning setting.
 generally increased interest in machine learning, both theoretically (as a research area and language learning) and for pragmatic reasons, as a way to reduce the manual work involved in the construction of the lexical and grammatical knowledge bases needed for the realization of sophisticated language technology applications. understanding that language communities are very unequally endowed with language technology resources. There are on the order of 7,000 languages spoken in the world today (Lewis 2009). Their size in number of first-language speakers is very unevenly distributed. The top 30 languages in the world account for more than 60% of its popu-lation. At the other end of the scale, we find that most languages are spoken by quite small communities: financial and other resources that could be spent on the development of language technology, but the cost of, for example, constructing a lexicon or a parser for a lan-guage is more or less constant, and not proportional to the number of speakers of the language.
 of a language in a particular situation X  X here the language could in principle be used, but where there is a choice available between two or more languages X  X s intimately connected with the attitudes towards the language among the participants. This is perhaps the most reliable determiner of language use, and not factors such as effort, lack of vocabulary, and so on, which in many cases seem to be post hoc rationalizations motivating a choice made on attitudinal grounds. Another way of expressing this is that languages are more or less prestigious in the eyes of their speakers, and that linguistic inferiority complexes seem to be common in the world.
 language, we should see it for what it is, namely, a perceived characteristic, something modern information and communication technologies for a language, including the creation of linguistic resources and language technology for it, may serve to raise its status (see, e.g., the papers in Saxena and Borin 2006).
 technology to language communities lacking the requisite resources. However, ULM, which would still exclude a substantial majority of the world X  X  languages (Borin 2009).
Note that the remainder X  X anguages with a tradition of writing X  X re not on the whole small language communities; in the first instance, we are talking about the few hundred most spoken languages in the world, for example, the 313 languages with at least one million native speakers (accounting for about 80% of the world X  X  population) surveyed by the Linguistic Data Consortium some years back in their Low-density language survey (Strassel, Maxwell, and Cieri 2003; Borin 2009). 316 methods could be employed in order to rapidly and cheaply (in terms of human effort) bootstrap basic language technology resources for new languages.
 needed to build computational morphological resources, many such implementations are not released to the public domain. Also, open domain texts will always contain a fair share of (inflected) previously unknown words that are not in the lexicon. There has to be strategy for such out-of-dictionary words X  X  ULM-solving algorithm is one possibility. The ULM problem as specified, therefore, still has a role to play for larger languages.
 chine learning of linguistic information are increasingly seen as providing potential tools in language documentation . 9 diversity as a disaster in the cultural and intellectual sphere on a par with the loss of the world X  X  biodiversity in the ecological sphere, only on a grander scale; languages are going extinct more rapidly than species. Enter language documentation (Gippert,
Himmelmann, and Mosel 2006), which is construed as going well beyond traditional cultural, and social X  X f a language community X  X  day-to-day life, in video and audio recordings of a wide range of sociocultural activities, in still images, and in representa-tive artifacts. Basic linguistic descriptions of lexicon and grammar made on the basis of transcribed recordings still form an important component of language documentation, however, and with the realization that languages are disappearing at a far faster rate than linguists can document them, it is natural to look for ways of making this process less labor-intensive. 10 order):
As noted, the motivation of eliminating the lexicon is now obsolete, whereas the others are active to various degrees. By far the most popular motivation has been, and still is, that of inducing a morphological analyzer/segmentation from raw text data (with little human intervention) in a well-described language. However, as we have argued herein, the timing is right for the momentum to carry over also to under-described languages. 3. Trends and Techniques in ULM 3.1 Roadmap and Synopsis of Earlier Studies A chronological listing of earlier work (with very short characterizations) is given in
Table 2. Several papers are co-indexed if they represent essentially the same line of work by essentially the same author(s).
 niques and ideas individually. However, we will attempt to cover the main trends and look at some key questions in more detail.
 may summarize in the following way. (a) Border and Frequency: In this family of methods, if a substring occurs (b) Group and Abstract: In this family of methods, morphologically related (c) Features and Classes: In this family of methods, a word is seen as made (d) Phonological Categories and Separation: In this family of methods, the 318 320
The first two, (a) and (b), enjoy a fair amount of popularity in the reviewed collection of work, though (a) is much more common and was the only kind used up to about 1997. The last two, (c) and (d), have been utilized only by the sets of authors cited therein. will be used in formal statements:
Subscript letters are dropped when understood from the context. 3.2 Border and Frequency Methods all based on letter successor/predecessor varieties. They were originally presented as applying to utterances made up of phoneme sequences (Harris 1955), but they apply just the same to words, namely, grapheme sequences (Harris 1970). The basic counting strategy, labelled letter successor varieties (LSV) by Hafer and Weiss (1974), is as follows. Table 3 shows an example of a letter successor count on a tiny contrived wordlist. preceding x in the words of W that end in x . LSV/LPV counts for an example word are shown in Table 4.
 variety in letter successors types (i.e., is only interested in which letters ever occur in the successor position, as opposed to being interested in their frequencies). For example, if there are two different letters occurring in successor position, one occurring a thousand times and the other once, Harris X  X  letter successor variety is still two X  X he same as if the two letters occurred once each. Subsequent authors have suggested that the full frequency distribution of the token letter successors carries a better signal of morpheme that we are in the middle of coherent morpheme. Moreover, mere type counts may be influenced by phonotactic constraints (consonant after vowel, etc.), which come out less significant in token frequency counts (Goldsmith 2006, page 6). Already the earliest { | 322 follow-ups to Harris (Gammon 1969; Hafer and Weiss 1974; Juola, Hall, and Boggs 1994) experiment with replacing the raw LSV/LPV counts with the entropy of the character token distribution. The character token distribution after a given segment can be seen as a probability distribution whose events are the characters of the alphabet. The entropy of this probability distribution then measures how unpredictable the next character is after a given segment. In general, for a discrete random variable X with possible values x , ... , x n , the expression for entropy takes the following form: Thus, with alphabet  X  , the letter successor entropy (LSE) for a prefix x is defined as
At least two authors (Golcher 2006; Hammarstr X m 2009b) have questioned entropy as the appropriate measure for highlighting a morpheme boundary. Entropy measures how skewed the distribution is as a whole, that is, how deviant the most deviant member is, in addition to the second member, the third, and so on. If there is no mor-pheme boundary, the morpheme continues with (at least) one character. So one deviant, highly predictable, character is necessary and sufficient to signal a non-break, and it is arguably irrelevant if there are second-and third-place, and so forth, highly predictable characters that also signal the absence of a morpheme boundary. For example, the character token distribution before -ng is shown in Table 5. Obviously, the fact that of the 3,352 occurrences of -ng , 3,258 of them are preceded by -i-, says that the absence of a morpheme boundary is highly likely. Now, does it matter that also another 35 are -o-versus only 4 for -e-? Entropy would also take into account the skewedness of -o-versus -e-, whereas for Hammarstr X m (2009b) and Golcher (2006) only the skewedness of the most skewed character (i.e., the character that potentially constitutes the morpheme continuation) is interesting, in this example -i-. Therefore, these approaches only use the maximally skewed character to predict the presence/absence of a morpheme boundary.
The letter successor max-drop (LSM) for a prefix x is defined as the fraction not occupied by its maximally skewed one-character continuation: on one X  X  theory of affixation, for which the field has no single answer (see Section 3.6, subsequently).
 three, we normalize them to their maxima in order to get a  X  X order X  score maximum achievable LSV is the alphabet size, so the normalized LSV ( x ) =
The maximum achievable LSE is a uniform distribution across the alphabet, so the distribution across the alphabet, so the normalized LSM ( x ) = analogues LPV , LPE , LPM are obvious. Table 6 shows an example word and its normalized predecessor scores of the three kinds.
 product-moment correlation coefficient between the LPH/LPE/LPM-values of all ter-minal segments, as well as the Pearson product-moment correlation coefficient between the LPH/LPE/LPM-ranks of all terminal segments. Most usages in the literature of the letter successor counts have been relative to other counts on the same language. In such cases, the rank correlations show that all three measures can be expected to have near identical effects.
 324
Harris (1955) and Hafer and Weiss (1974); for instance: (a) Cutoff: By far the easiest way to segment a test word is first to pick some (b) Peak and plateau: In the peak and plateau strategy, a cut in a word w is (c) Complete word: A break is made after a word prefix (or before a word
These and similar strategies have been discussed and evaluated in various settings in the literature, and it is unlikely that any strategy based on LSV/LSE/LSM-counts alone will produce high-precision results. The example in Table 6 showing morpheme border heuristics on a specific word illustrates the matter at heart. Any intuitively plausible theory of affixation should allow abundant combination of morphemes without respect to their phonological form, which predicts that high LSV/LSE/LSM values should emerge at morpheme boundaries. However, there appears to be no reason why the converse should hold X  X igh LSV/LSE/LSM values could emerge in other places of the word as well. Indeed, any frequent character at the end or beginning of a word may also be expected to show high LSV/LSE/LSM around it, such as the -e at the end of disturbance which has higher values than, for example, -ance . Therefore, simply inferring that high LSV/LSE/LSM values indicate a morpheme border is not a sound principle in general.
 quence counts is that associated with Ursula Klenk and various colleagues (Klenk and
Langer 1989; Klenk 1991, 1992; Langer 1991; Flenner 1992, 1994, 1995; Jan X en 1992). For each character bigram c 1 c 2 , they record, with some supervision in the form of manual curation, at what percentage there is a morpheme boundary before after c 1 c 2 | , or none. A new word can then be segmented by sliding a bigram window and taking the split which satisfies the corresponding bigrams the best. For example, the bigram splits ng | , g | i ,and | in are relevant to deciding whether sin g segmentation. Exactly how to do the split by sliding the window and combining such however, that the appropriateness of a bigram split is dependent on, for example, the position in a word X  -ed is likely at the end of a word, but hardly in any other position X  and exception lists and cover-up rules had to be introduced, before the approach was abandoned altogether. in the border-and-frequency tradition have incorporated another measure, comple-mentary to a morpheme border heuristic. This measure is nearly always directly or indirectly related to frequency, that is, frequent segments of some kind are singled out.
Frequency has been used in many different ways. The simplest way is to look at the raw frequency of segments of any length, but, inevitably, this will sweep in any short segment. Indeed, better candidates for morphemic segmentation are segments which are somehow overrepresented, that is, more frequent than random. There are various ways to define this property as well, including the following.

Overrepresentation as more-frequent-than-its-length: For a segment x of
Overrepresentation as more-frequent-than-its-parts: For a segment x = c
Overrepresentation as more-frequent-as-suffix: For a segment x , it is overrepresented
With such measures, many authors have singled out affixes above a certain over-representation-value threshold or overrepresentation-rank threshold.
 to interpret them. Although they may be set ad hoc with some success, such settings do not automatically generalize. Such considerations have led many authors to devise compression-inspired models for exploiting skewed frequencies. In particular, several different sets of authors have invoked Minimum Description Length (MDL) as the motivation for a given formula to compress input data into a morphologically analyzed representation. 12 learning/inference process as data compression: For a given set of hypotheses H and data set D , we should try to find the hypothesis in H that compresses D most (Gr X nwald 2007, pages 3 X 40). Concretely, such a calculation can take the the following form. If L ( H ) is the length, in bits, of the description of the hypothesis; and L ( D bits, of the description of the data when encoded with the help of the hypothesis, then MDL aims to minimize L ( H ) + L ( D | H ).
 follows. A particular way Q of describing morphological regularities is conceived that has two components which we may call patterns P and data D . A coding scheme is devised to describe any P and to describe any collection of actual words with some specific P and D . A greedy search is done for a local minimum of the sum L ( P ) + L ( D to describe the set of words W (in some approaches) or the bag of word tokens 326 other approaches) of the input text data. 13 To take one concrete example, Goldsmith X  X  (2006) particular way Q of describing morphological regularities is to allow for a list of stems, a list of affixes, a list of signatures (structures indicating which stems may appear with which affixes, i.e., a list of pointers to stems, and a list of pointers to suffixes). The search is then among different lists of stems, affixes, and signatures to see which is the shortest to account for the words of the corpus. Further details of such coding schemes need not concern us here, but for a range of options see, for example, Goldsmith (2001, 2006), Xanthos, Hu, and Goldsmith (2006), Creutz and Lagus (2007), Argamon et al. (2004), Arabsorkhi and Shamsfard (2006),  X  Cavar et al. (2004b), Baroni (2003), or Brent, Murthy, and Lundberg (1995).
 (2007, pages 37 X 38), is infelicitous for such cases where the P , D -search is not among different description languages , but among varations within a fixed language Q . For does not include other (possibly more parsimonious?) ways of description that do not use stems, affixes, or signatures at all. For the MDL-label to apply with its full philo-sophical underpinnings, the scope must include any possible compression algorithm, namely, any Turing machine. In this respect it is important to note that, compared to the schemes devised so far, Lempel-Ziv compression, another description language, should yield a superior compression (as, in fact, conceded by Baroni 2000, pages 146 X 147).
MDL-inspired optimization schemes have achieved very competitive results in practice, however, and must be considered the leading paradigm to exploit skewed frequencies for morphological analysis. 3.2.3 Paradigm Induction. The next step after segmentation is to induce systematic alter-nation patterns, or (inflectional) paradigms, 14 and this is usually done as an extension of a border-and-frequency approach. For purposes of ULM, a paradigm is typically defined as a maximally large set of affixes whose members systematically occur on an open class of stems. For a number of reasons, finding paradigms is a major challenge.
The number of theoretically possible paradigms is exponential in the number of affixes (as paradigms are sets of affixes). Paradigms do not need to be disjoint; in real languages they are typically not. Rather, words in the same part of speech tend to share affixes across paradigms (Carstairs 1983). In addition, without any language-specific knowl-stem and suffix making up that word). Paradigm induction would be an easy problem if all affixes that could legally appear on a word did appear on each such word in a raw text corpus. This is, as is well known, far from the case. A typical corpus distribution is that a few lexemes appear very frequently but by far most lexemes appear once or only a few times (Baayen 2001). What this means for morphology is that most lexemes will appear with only one or a minority of their possible affixes, even in languages with example, the Classical Greek alternative medial 3p. pl. aorist imperative ending - X  X  X  X  (Blomqvist and Jastrup 1998), may not appear at all even in a very large Classical Greek corpus.
 paradigm according to linguistic analysis. If k lexemes that are inflected according to P occur in a corpus, each of the k lexemes will occur in 1  X  forms i that a lexeme occurs in is likely not to be normally distributed. Most lexemes forms. It appears that for most languages and most paradigms, the number of lexemes that occur in i forms tends to decrease logarithmically in i (Chan 2008, pages 75 X 84). As an example, consider the three most common paradigms in Swedish and the frequency of forms in Table 8.
 least for languages with one-slot morphology, include Zeman 2008, 2009, Hammarstr X m (2009b), and Monson (2009). They explicitly or implicitly make use of the following two heuristics to narrow down the search space:
Although we know of no empirical evulation of them, in the impression of the present authors, the two heuristics appear to be cross-linguistically valid.
 the segmentation is already given. The problem then takes the form of a matrix with 328 stems on one axis and suffixes on the other axis. Chan then makes use of known techniques from linear algebra, in particular Latent Dirichlet Allocation, to break the full matrix into smaller dense submatrices, which, when multiplied together, resemble the full matrix. There is only one humanly tuned threshold, namely, when to stop breaking into smaller parts. 3.3 Group and Abstract
In contrast to the methods that use a heuristic for finding morpheme boundaries, the grouping methods are much less sensitive to continuous segments. String edit distance is the most straightforward metric for which to find pairs or sets of morphologically related words (see, e.g., Gaussier 1999; Yarowsky and Wicentowski 2000; Schone and
Jurafsky 2001a; Baroni, Matiasek, and Trost 2002; Hu et al. 2005a; Bernhard 2006, pages 101 X 117; Bernhard 2007; Majumder et al. 2007; Majumder, Mitra, and Pal 2008). In addition, as unsupervised methods for semantic clustering (e.g., Latent Semantic
Analysis) and distributional clustering became more mature, these could be included as well (Schone and Jurafsky 2000, 2001a; Schone 2001; Baroni, Matiasek, and Trost 2002;
Freitag 2005). More remarkable, however, is that Yarowsky and Wicentowski (2000) and Wicentowski (2002, 2004) have shown that frequency signatures can also be used to (heuristically) find morphologically related words. The example they use is sang versus sing , whose relative frequency distribution in a corpus is 1,427/1,204 (or 1.19/1), whereas singed 15 versus sing is 9/1,204 (Yarowsky and Wicentowski 2000, pages 209 X  210). This way, sing can be heuristically said to be parallel to sang rather than singed , and indeed the distribution for singed versus singe (its true relative) is 9/2, that is, much closer to 1.
 tically extracted. For example, one group might be another might be { bark , barks , barked , barking } . The next step would be to find what is common among several groups, not just one. Abstracting morphological alternations given a family of groups is a thorny issue. For instance, Baroni, Matiasek, and Trost (2002) leave the matter largely in the exploration phase. Wicentowski (2004) presents a finished theory based on constraining the abstraction to find patterns in terms of prefix, suffix, and stem alternations.
 grouping but also to abstracting, is how to find one and the same morphological process (umlauting, adding a suffix, etc.) that operates over a maximal number of groups. The search space is huge, considering not only the group space but also the large number of potential morphological processes itself.
 capable of handling non-concatenative morphology and in that issues of semantics (of stems) are addressed from the beginning.
 (2003) and partly Moon, Erk, and Baldridge (2009) can favorably be seen as a mid-way between the border-and-frequency and group-and-abstract approaches as they rely on sets of four members with a particular affixation arrangement ( X  X quares X ), existence is governed much by the frequency of the affixes in question. 3.4 Features and Classes
The features-and-classes methods share with the group-and-abstract methods the virtue of not being tied to segmental morpheme choices. As mentioned earlier, in this family of methods a word is seen as made up of a set of features which have no internal order X  n -grams in Mayfield and McNamee (2003) and McNamee and Mayfield (2007), and beginning/terminal/internal segments in De Pauw and Wagacha (2007).
 language of Kenya. As designed by De Pauw and Wagacha (2007), initial ( B= ), middle enumerated this way will not be morphologically relevant, whereas a minority is. For example, in this case, I= h is just an arbitrary character without morpheme status, whereas I= ng  X   X thi happens to be equal to a stem. The idea is that arbitrary features such as I= h will be too common in the training data to provide a useful constraint, whereas generalization properties.
 which can be fed into a standard maximum entropy classifier. The next step is, for each word, to ask the classifier for the k closest classes, namely, words (which will include the word itself and k  X  1 others with significant feature overlap). Clearly, such clusters may capture relations that string-edit-distance clustering does not. De Pauw and Wagacha 330 (2007, pages 1517 X 1518) further suggest how specific morphological information, such as prefixes, tonal changes, etc., may be abstracted from such clusters.
 segmental and long-distance phenomena, but are so far largely unexplored and not free from thresholds and parameters. 3.5 Phonological Categories and Separation
These approaches specifically target the special kind of non-concatenative morphology called intercalated morphology (or templatic morphology or root-and-pattern mor-phology) famous mainly from Semitic languages, such as Arabic. They start out by assuming that graphemes can be subdivided into those that take part in the root, and those that take part in the pattern. For the languages so far targeted, Arabic (Rodrigues and  X  Cavar 2005, 2007; Xanthos 2007) and Amharic (Bati 2002), this is largely true, or a transcription is used where it is largely true. Rodrigues and
Bati (2002) hard-code the transition from the graphemic representation of a word to its (potential) root and pattern parts. This can be said to constitute a strong language specific bias, tantamount to supervision. Xanthos (2007), on the other hand, starts out only by assuming that there exists a distinction between root and pattern graphemes and subsequently learns which graphemes are which. See Goldsmith and Xanthos (2009) for an excellent survey on how to do this (something which falls under learn-ing phonological categories rather than morphology learning). Basically, it is possible only because there are systematic combination constraints between different phonemes (approximated by graphemes); for example, vowels and consonants alternate in a very non-random manner.
 learning problem is similar to morphology learning given roots and suffixes, that is, the typical model for learning concatenative morphology, where the task is to weed out noise, to decide where patterns ( X  X uffixes X ) start and end, which patterns are spurious, and so on. All these authors who have addressed intercalated morphology use a variant of MDL (see the border-and-frequency techniques in Section 3.2). The accuracy of ULM on languages with intercalated morphology appears to be similar to the accuracy on other languages (cf. Section 4.3). 3.6 General Strengths and Weaknesses transfer between different groups of authors and there is a fair amount of duplication of work. The lack of a broadly accepted theoretical understanding is possibly related to this fact. Few approaches have an abstract model of how words are formed, and thus cannot explain why (or why not) the heuristics employed fail, what kind of errors are to be expected, and how the heuristics can be improved. Nevertheless, a model for the simplest kind of concatenative morphology is emerging, namely, that two sets of random strings, B and S , combine in some way to form a set of words W . For Gelbukh,
Alexandrov, and Han (2004), the segmentation task is to find minimal size that W  X  X  xy | x  X  X , y  X  Y } . For example, if W = { ad , ae , bd , be , cd , ce size | X | + | Y | = 5with X = { a , b , c } and Y = { d , e (2005) as well as in the word-segmentation version of Deligne (1996), the segmentation task is to find a configuration of splits s i for each w i y occur in as many splits as possible. More precisely, the product, over all words, of the number of splits for the parts x and y should be maximized. Formally, let x be the parts of w i induced by splits s i and let p ( x ) = part of the split. Then the task is to find splits that maximize the following expression:
For example, if W = { ad, ae, bd, be, cd, ce, ggg }, then the configuration of splits a b | e, c | d, c | e, g | gg yields the product (2  X  3) 6  X  (1 and S , but at the cost of a large search space, and whose global maximum is hard to characterize intuitively. The same holds for the extension by Snover (2002). Kontorovich,
Don, and Singer (2003), Snyder and Barzilay (2008), Goldwater (2007), Johnson (2008), and Poon, Cherry, and Toutanova (2009) should also be noted for containing generative models.
 implicitly target languages which have (close to) one-slot morphology, that is, a word (or stem) typically takes not more than one prefix and not more than one suffix. Many it may seem that multi-slot morphology can be handled by the same algorithms as one-slot morphology, by iterating the process used for one-slot morphology. A decade languages do not necessarily generalize to the outermost slot of a multi-slot language. ceivable that the (a) and (b) type of approaches may be mutually enhancing. Results from the (a) methods may serve to cut down the search space for the (b) methods, and the (b) methods may provide a way to circumvent thresholds for the (a) methods.
There is also the possibility of serial combination where, for example, the (a) methods target concatenative morphology and the (b) X  X r (c) X  X ethods attempt the remaining cases. Presumably because most methods so far do not produce a clean, well-defined result, various forms of hybridization of techniques by different authors have yet to be systematically explored.
 principled way, though so far these have been developed in close connection with a particular segmentation method and target language (Schone 2001; Schone and Jurafsky 2001a; Wicentowski 2002, 2004; Tepper 2007; Kohonen, Virpioja, and Klami 2008; Tepper and Xia 2008). 4. Discussion 4.1 Language Dependence of ULM
As we mentioned in Section 3.6, most approaches have an explicit bias towards certain kinds of morphological systems, those for which we introduced the label  X  X ne-slot mor-phology. X  This is of course not a problem, if the purpose is to bootstrap a morphology for some languages which happen to belong to the right type. If the purpose is to say something about human language acquisition or language learning, or if the aim is to 332 devise a method that should work with any language, such a bias naturally becomes problematic.
 in the literature on ULM and other machine learning of morphology are those of language acquisition and of linguistic analysis (e.g., as carried out as part of linguistic fieldwork). Depending on which of the two we choose, the kinds of biases that we may or may not allow become different. Language acquisition in humans is oral (or sign, but for practical reasons, we are leaving sign languages completely out of the discussion here), so expecting written input with word delimiters would then be an inadmissible language acquisition.
 be required in order to carry out the discovery procedures mentioned in Section 2:
Mutatis mutandis , the procedure described in this quote, contains most elements of ULM and related methods proposed in the literature. Note that the quote just given stresses the importance of the knowledge that the linguist brings to the analysis and which in-forms the whole analytical process. This suggests that there may be a level of general knowledge about language (in general or a useful subset of languages), or about lin-guistic analysis, or both, which would be useful to ULM in general, something like the  X  X nowledge X  that white space is a word delimiter in written text, but on a higher level.
One component of a research program on ULM would then be to formulate this kind of general knowledge in a way which makes sense given that the object of study is lan-guage, to test it, and to share it with the community of linguistic scholars. A concrete illustration could be the way that the old notion of proportional analogy (Anttila 1977) is refined and formalized in various ways and used to test segmentation hypotheses in works on ULM from the earliest times onwards (e.g., the  X  X quares X  mentioned in
Section 3.3). 4.2 ULM and Semantics
As traditionally conceived, an inflectional paradigm links a set of word forms to structural descriptions expressed in terms of a stem carrying a lexical meaning and some formal expression of one or more morphosyntactic categories (or grammatical meanings) taken from a closed, small set of such categories. This bears emphasizing, because ULM work generally has been concerned only with the formal expression side of morphology; that is, instead of the traditional it will give us simply although it may tell us that the stem table appears in two paradigms.

ULM, although it is conceivable that the same kind of techniques used, for example, in order to cluster words semantically (e.g., Latent Semantic Indexing/Analysis or
Random Indexing), could be used also to classify the resulting morphs from a ULM segmentation (cf. Schone and Jurafsky [2001b] for a study of inducing part-of-speech class labels in a setting similar to that of ULM). The labelling problem can easily be considered independent of ULM by using a hand-segmented (or segmented by a hand-built morphological parser) input corpus. 4.3 Is ULM of Any Use?
As we said in Section 2, there is an explicit expectation frequently encountered in the more recent literature that ULM and other unsupervised methods could be employed technology resources for new languages. However, looking at the literature, it seems that X  X t least in the area of inflectional morphology X  X he only approaches that have so far produced substantial results are the old-fashioned, hand-coded grammar-based ones, such as the work described by Trosterud (2004), where finite-state morphological processors and constraint grammar-based disambiguation components are developed for a number of related languages. The fact that the languages are related is of great help when dealing with successive languages after the first one. The morphological component for the first language, North S X mi, required approximately 2.5 person-years analogous module for the closely related Lule S X mi was completed in an additional six months (Trosterud 2006). 17 This and other work in the same vein reported in the literature (e.g., by Artola-Zubillaga 2004 and Maxwell and David 2008) is characterized by deep and long-lasting involvement by linguistic expertise and further often by the creative use of digitized versions of conventional printed linguistic resources, especially dictionaries. The following observation is perhaps trivial, but bears stressing, because necessary that tools for providing systems with linguistic knowledge use a conceptual apparatus and notation familiar to the linguists who are supposed to be working with them. Relevant to our purposes here, the same holds for any attempt to kickstart the development of a morphological analyzer by using ULM: If the expectation is that the output of ULM should be manually  X  X ost-edited, X  this output must of course be intelligible to the linguist doing the post-editing. 334 ments, which generally founder on the lack of evaluation data. The MorphoChallenge series does provide adequate gold-standard evaluation data for Finnish, English, Ger-man, Arabic, and Turkish as well as task-based Information Retrieval (IR) evaluation data for English, German, and Finnish. It can be seen that ULM systems are mature enough to enhance IR, but so far, ULM systems are not close to full accuracy on the gold standard and outside commentators have generally been unimpressed with these results (e.g., Mahlow and Piotrowski 2009, page vi). However, many (most?) of the strong-looking systems reported in the literature have not, for one reason or another, taken part in the MorphoChallenge. Taking MorphoChallenge results and proof-of-concept reports together, it seems that high accuracy by ULM systems is presently only achievable if the language has small amounts of one-slot concatenative morphology, whereas for morphologically more complex languages, parameter tuning and/or lower accuracy is to be expected.
 general, benefit significantly from (noisy) ULM, such as Speech Recognition (Hirsim X ki et al. 2003, 2005, 2006; Kurimo et al. 2006) or Machine Translation (Sereewattana 2003;
Virpioja et al. 2007; Bojar, Stra  X  n X k, and Zeman 2008; Kirik and Fishel 2008; De Gispert et al. 2009; Fishel and Kirik 2010) because almost only the Morfessor system has been tested, and results are, if positive, not completely unambiguous. One usage of noisy ULM, at least, is for smoothing language identification models (Hammarstr X m 2007a; Ceylan and Kim 2009).
 low-density languages. There is much ongoing work addressing these issues, however, so we can probably expect some progress in this area (Bird 2009). 4.4 Future Directions
In practice, the near future should define a high-accuracy threshold-minimal system for one-slot morphology languages, using refinements of ideas already extant.
 theory that explains why (or why not) a given algorithm works. Further study of theo-retical properties of (stochastic) combining of string sets/bags are likely to hold the key to the culmination of the border-and-frequency methods X  X ot further experimentation with ad hoc heuristics. The recent increased interest in Bayesian generative models in general in NLP may possibly serve as a catalyst.

De Pauw and Wagacha (2007), is an ingenious generalization that holds numerous advantages over string edit distances. Feature set comparisons are naturally defined over arbitrary collections, whereas string edit distances work on pairs of strings. Many morphologically related words differ in several characters and are therefore not particu-larly close in edit distance. Features instead of edit distances provide a neat framework, based on global properties of the feature distribution, of capturing the fact that some character mismatches do not really matter, whereas some character matches (although not necessarily long) are very significant.
 literature on clustering in other fields. Chan (2006) is a step in this direction, but further steps are lacking; in particular, spectral clustering (of some kind) has not been explored for paradigm induction in ULM. Also here, given the typical skewed stem distributions and skewed suffix distributions (exemplified in Section 3.2), some theoretical work is needed to determine its implications for clustering.
 languages for which data has become available only recently (Abney and Bird 2010).
This would clarify the potential of ULM usefulness for underdescribed and under-resourced languages. 5. Conclusion
After more than half a century of research, the field of ULM has made good progress (as have many other areas of computational linguistics), but there is still a long way to go before it will become practically useful or even theoretically interesting to linguists. In the terms of Table 1 in Section 1, the state of the art of ULM is somewhere in the region of  X  X egmentation X  and  X  X nflection tables, X  if we are talking about linguistic form, but there has been next to no progress at all when it comes to linguistic meaning (e.g., functional labeling of affixes).
 tually achieved sometime in the future X  X  formalized version of a linguistic discovery procedure, that is, a knowledge-heavy enterprise. Instead, recent successes in the area have been largely contingent upon the rapid development in computational linguistics of statistical and information-theoretic knowledge-light (but robust) methodologies. that if ULM is to become a serious alternative to X  X r, equally likely, a natural component of X  X anually built computational morphology systems for a wide and diverse range of languages, and especially if we are to make headway in the area of semantics, we need to see more interaction between the present approaches to ULM with the computational techniques and mathematical modeling tools they can bring to bear on the problem on the one hand, and typologically informed linguistic research on morphology founded on a vast store of knowledge and methodology refined over two millennia on the other. Acknowledgments References 336 338 340 342 344 346 348
