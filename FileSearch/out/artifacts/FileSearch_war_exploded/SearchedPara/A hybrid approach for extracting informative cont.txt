 1. Introduction
The Internet has become a major information source distributed over web pages. Conversely, web pages contain noisy content, including advertisements, banners, menus, and unnecessary links, which can adversely affect performance of text-based processing systems such as search engine, web summarization, question answering and text understanding. In this instance, informative content (i.e. text content, headline, date or author name) can be used to enhance the results of these techniques. However, automatically extracting informative content is difficult, as a web page contains both noisy and informative content in a same file. This file consists of Hypertext Markup Language (HTML) tags and content between these tags that allow us to display pages in web browsers. In this study, we introduce a hybrid approach for obtaining infor-mative content from different web pages.

The Web is an invaluable source of data for studies, especially those that do not have enough natural language texts. In particular, researchers choose online newspapers as an alternative test collection ( Can et al., 2008; Carlberger, Dalianis,
Hassel, &amp; Knutsson, 2001; Savoy, 2007, 2008; Uzun, 2011 ) to improve the ranking of search engine results of their natural language searches. However, as mentioned, this test collection contains noisy texts, compared to the relevance of the con-tent. For example, Uzun et al. (2011a) developed a crawler to obtain news between 1998 and 2008 from the Turkish news-paper Milliyet ( http://www.milliyet.com.tr ). They searched a regular expression pattern that can be used for string manipulation to eliminate noise in web pages. They found the following pattern for web pages between 2003 and 2007.  X 
This is a comment tag that begins with tag . The browser does not display this tag but only gives information about a web page. This pattern matches the opening and closing pair of comment tags. However, using this pattern is not a reliable extraction method for different web domains. Moreover, web designers may be changed their HTML tag naming and hier-archy over time. For example, the designers of Milliyet have used different HTML structure after 2007. Due to varying HTML tag naming and hierarchy, preparing regular expression patterns for extracting the informative content becomes a challenge as described in Section 4.1 . Besides, preparing these patterns is cumbersome. To automate pattern extraction and maintain efficiency, we present a hybrid approach. 1 In this approach, patterns as rules are first obtained by using our learning model which utilizes an appropriate machine learning (ML) technique. Secondly, these rules are used to extract informative content from web pages without using ML inference.

The key issue in content extraction based on the ML techniques is to represent tag information as features obtained from web data. In this study, features such as tag name, word frequency and link frequency are prepared from the Document Ob-ject Model (DOM), which is a language-independent convention and tree-based hierarchy for representing a web page. DOM extracting informative content use DOM-based features related by the attributes and the content of an HTML tag. However, the complexity of the DOM structure is not suitable for effectively extracting informative content in a direct manner when compared with using an appropriate regular expression pattern. The main approach of our study is to learn this pattern from DOM-based features. And then, this pattern is used for effectively extracting informative content.

Many studies focus on extracting only informative content of a web page. In this study, we divide informative content into three blocks including main, headline and article information as shown in Figs. 1 and 4 . These blocks can be used to maximize storage efficiency of text-based processing systems. For extracting these blocks for a web page, appropriate pat-terns can be produced. In this study, we automatically produce these patterns by using a suitable ML method. Firstly, we analyze our datasets based on the number of informative blocks/tag ratio to select most of the important tags. Two layout
HTML tags, TD and DIV, are determined to be important tags for accessing these blocks again. Therefore, our pattern model contains at least one of these tags and the other HTML tags (i.e., H1 X  X 6, FONT, SPAN, EN, UL, and LI) in the block extraction.
HTML tags are also widely used to style content and for visualization. For this reason, HTML-based information on Web is semi-structured, which does not conform to the formal structure. The World Wide Web Consortium (W3C  X  http:// www.w3.org/ ) has presented a fully structured tag system based on XML (Extensible Markup Language), which is a well-formed structure that simply marks up pages with descriptive tags. Fig. 1 gives information about these structures for blocks in HTML and our rule model in XML format.

In Fig. 1 , the H1 and H3 tags do not give clear information for determining patterns because H1 and H3 in the tag of &lt;DIV unnecessary texts. We automatically determine these blocks as shown Fig. 1 b. However, it is difficult process because of the heterogeneous and semi-structured nature of web pages as shown detailed examples in Section 4.1 . The first step of our approach prepares rules in XML format from an HTML document using ML methods. This step (creating DOM, preparing features and applying an ML method) increases complexity. Aware of the time complexity, Uzun et al. (2011b) show that simple string manipulation techniques can efficiently extract informative content without using DOM. Therefore, the second step utilizes a simple string manipulation technique using rules obtained from step one of this task. The experimental meth-odology of the first step is the same with other approaches in algorithmic view. However, the main focus in this paper is not only building a more accurate model with additional features but also producing effective rules for each domain.
The next section presents related works and our own focus. The third section introduces the similarities and differences of the hybrid approach as well as the workflow of the extraction process. The fourth section is dedicated to rule induction and
ML methods and introduces the details of the problem, feature selection, dataset, learning algorithms, metrics and experi-ments. The fifth section covers the efficiency of the extraction algorithm and makes a comparison. The last section provides our conclusions. 2. Related work
Related works can be grouped into two categories. These are automatic extraction techniques and hand-crafted rules. The main focus of automatic extraction techniques is inference through features extracted from HTML. Hand-crafted rules are mostly used to extract information from HTML through string manipulation functions. Though preparing the hand-crafted rules is difficult and cumbersome, their efficiency is high with proper adjustments. The advantage of automatic extraction techniques over hand-crafted rules is their automaticity and easy applicability. This study inherits the qualifications from two different extraction methods by combining the efficiency and automaticity.
 Several researchers have investigated automatic extraction techniques on web pages through web page segmentation. These studies have mostly focused on DOM-based segmentation ( Baluja, 2006 ), location-based segmentation ( Kovacevic,
Cai, Wen, &amp; Ma, 2003 ). DOM-based studies use DOM-level features with trained classifiers to extract useful content from web document templates ( Bar-Yossef &amp; Rajagopalan, 2002; Chakrabarti, Kumar, &amp; Punera, 2008; Chen, Zhou, Shi, Zhang, information ( Chakrabarti, Kumar, &amp; Punera, 2007 ), distribution of segment level text density ratios ( Kohlschutter, 2009;
Fankhauser, &amp; Nejdl, 2010 ). Approaches of these studies are not suitable for generating rules that can be used for string manipulation. Our approach provides a method to generate rules that can be utilized for efficient extraction. In DOM-level approaches ( Gibson, Wellner, &amp; Lubar, 2007; Hofmann &amp; Weerkamp, 2007; Spousta, Marek, &amp; Pecina, 2008; templates and styles across several pages contain noisy blocks. Based on that idea, Bar-Yossef and Rajagopalan (2002) report that eliminating templates increases the precision of a search engine called Clever at all recall levels. Similarly, Lin and Ho (2002) designed InfoDiscover, a tool that extracts informative content from web pages. In their study, they use the TABLE tag to partition the web page into blocks. Several other tools use sequential approaches, including n-grams and conditional ran-dom fields, to clean noisy text from web pages ( Evert, 2008; Spousta et al., 2008 ). However, none of these studies lists the importance of blocks, except that conducted by Song, Liu, Wen, and Ma (2004) . This study emphasizes that using a scheme that weights blocks by their importance is useful for both search engines and data mining applications. Our approach detects unnecessary and three informative blocks (main, headline and article information). Other studies focus on layout HTML tags (DIV and TD) or all HTML tags. However, our study takes into account the appropriate HTML tags for each block. In the main block detection, layout HTML tags can be used to determine the most comprehensively informative texts of a web page. The detection of headline and article information blocks contains all HTML tags.

Location-based segmentation relies on the position features of the areas of interest. These areas are determined by their location and are mostly labeled as left menu, right menu, footer content, etc. This approach depends on the assumption that the location, width and area of certain tags are valuable information for extracting useful content, and they should indeed be combined with label features of these tags. Conversely, in vision-based segmentation, the features used for segmentation are based segmentation approaches rely too much on the DOM structure, which indeed diminishes segmentation efficiency, while some approaches use both visual cues and DOM structure. Other approaches, similar to visual-based segmentation, attempt to identify the most interesting and informative portions of web content ( Baluja, 2006; Chen, Ma, &amp; Zhang, 2003; and distinguish these clusters as template regions that are uninformative. Yi and Liu (2003a, 2003b) utilize a compressed tree structure and site style tree, respectively, to identify uninformative DOM nodes across pages. Two studies focus on TA-
BLE tags. Ma, Goharian, Chowdhury, and Chung (2003) look for repeated blocks to mark as uninformative TD sections, whereas Lin (2002) use entropy over a set of word-features to remove redundant blocks from web pages. All of these studies focus on TD tags. However, nowadays web designers prefer to use DIV tags instead of TD tags. Therefore, this tag is added into our model.

Additionally, there are web scrappers that skip DOM structure creation and use rules instead, including regular expres-sions written in languages such as Java and Perl. These tools consider the judging criteria of efficiency and accuracy in their bor-intensive for extracting information from web templates that change over time. Hand-crafted rules also tend to be impractical for more than a couple of sources. The approach presented in this study is built on the appropriate combination of hand-crafted rules and automatic extraction techniques. 3. The hybrid approach The approach developed in this study involves automatic rule creation instead of manual hand-crafted rule insertion.
These rules are used to infer informative content from simple HTML pages. Similar to other studies, our approach first ex-tracts DOM-based features and utilizes these features to extract informative contents. The difference of this study from ear-is based on two block tags: DIV and TD , selected as the most suitable markers for determining the boundaries of informative content. Because the system is constructed on DIV and TD tags, we can automatically determine the most comprehensive rule sets and maintain efficiency in the informative content extraction. Fig. 2 shows the workflow of this approach.
The workflow given in Fig. 2 shows the learning process, extraction process, rule selection and creation of a well-formed document based on the appropriateness criterion of the rule for the web pages. This workflow consists of two main steps. 1. Rule induction from a ML method 2. Efficient informative content extraction from rules
In the first step, rule induction is performed via ML methods; in the second, the extracted rules are used to determine informative content in web pages. The rules inside a well-formed document that contain simple informative content are con-wise, an ML method occurs to induce the rules and create the well-formed document. This decision is made when the rule produces a single result. In the rule induction phase, marked as step one, DOM is created and features are extracted from this
DOM tree. An ML method then applied. For this step, we compare several different machine learning methods in the follow-ing sections and choose Decision Tree Learning with the Sub-tree Raising method as the most effective and accurate method for the dataset. 4. Rule induction with ML
In the rule induction step, the learning stage produces rules for efficient web content extraction. Preparing the learning stage requires a dataset and the appropriate features derived from this dataset. The dataset should be well-defined, and it should not contain too much noise but should include a low number of samples. In this section, we describe the HTML and DOM structures prior to the dataset and feature selection. We then introduce the feature selection part in the creation of the dataset and the dataset itself. We next give the tested ML methods and metrics. We then conclude the section with a description of the results of ML methods. Moreover, we introduce how rules are obtained from ML results. 4.1. HTML and DOM
HTML is a simple and effective markup language used to develop web sites. HTML contains several tag sets for visualizing content. A web browser simply interprets these tags and creates a web page that a human can easily understand. Developers who want to demonstrate their visual content with richer features, including Javascript, use a hierarchy called DOM. Fig. 3 shows the content of a simple web page in three different views.

Fig. 3 shows that information content is written between HTML tags. An HTML tag is generally formed from the HEAD , which contains necessary information about the web site, and the BODY , which is used to visualize the content. DIV and to as block markers. An A HREF tag is used to give links to different web pages. H1 and P tags are used to format the text. For
LI . Using these tags to extract informative content is not suitable for rule induction. The block tags can thus be used in the only informative content via pre-manual DIV / TD selection and proper adjustments in the DOM hierarchy. Block tags have descriptive parts (including ID and CLASS ) that can be used for producing rules. Fig. 4 gives a simple example of using block tags (i.e., DIV tags) in web pages.

Fig. 4 contains only DIV tags as block tags. TD tags can also be used in this visualization. DIV and TD tags are generally used in web design to separate several different blocks. The TD tag is the most specific tag for TABLE formatting in HTML. Con-versely, the DIV tag became one of the most frequently used tags in HTML, along with the Cascading Style Sheets (CSS) styl-ing, which is used as a reference by HTML. CSS also introduced flexibility and ease of design to HTML. Both TD and DIV web page without involving any closing tag ambiguity.

Block detection directly depends on using proper features in extraction. For instance, when Fig. 4 is explained using the link distribution as a selection criterion, we can determine the informative blocks as those with the least numbers of links, and the uninformative blocks as those that have high numbers of links. Certainly, the link and word distributions for each block are two important selection criteria, but some blocks may contain both informative and uninformative blocks. When such blocks are encountered, the most specific block under the parent block should be tested whether it is informative. This test also changes the word and link distributions of informative and uninformative blocks. In Fig. 4 , the block &lt;DIV id = content&gt; contains both informative and uninformative blocks. To simplify the block extraction problem, we categorize blocks into four different cases. The corresponding blocks given in Fig. 4 are listed below.

Extracting uninformative blocks &lt;DIV id=dynamic_box_left&gt; &lt;DIV class=article_tools&gt; &lt;DIV id=fontsize&gt;
Extracting the main (most informative) block &lt;DIV id=article_body&gt; Extracting the title The H1 tag inside the &lt;DIV id = content-holder class = content_holder&gt; block.

Extracting the text summary, author name, date, image titles and user comments &lt;DIV id = article_date&gt;
The block rules may vary among web sites. However, when rules are extracted for a single web page, they can be used for structurally similar web pages of the same web domain. However, some web pages may contain errors, as in Fig. 4 , where the &lt;DIV id = article_date&gt; block marker contains unrelated information in the tag. These errors might also happen for other blocks, including the main content block. In this study, we try to minimize user-oriented noise on block tags by including web pages from various web domains in our datasets.

In Fig. 4 , a more complex DIV structure is demonstrated in an HTML page. This figure is a good example of how blocks can be nested to form a hierarchy. This creates a simple problem when extracting the main DIV block with a regular expression. cannot find the proper closing of the main block. There are two types of matching mechanisms in regular expressions for this guity, a DOM structure can be used to extract the target match. However, creating a DOM structure increases the complexity of basic string matching problems. In this study, we train a learning model with the necessary features derived from block tags to directly extract informative content blocks from HTML without using DOM. We use DOM-based features in the rule induction phase, but the model does not re-create the DOM in the efficient extraction. 4.2. Datasets used in this study
Almost all ML methods need a specific dataset for the problem to adjust the weights of their algorithms. Similarly, studies involving ML methods about informative content extraction from web pages use hand-annotated samples from various web domains. An old text collection, annotated by Kohlschutter et al. (2010) , contains 621 web pages from 408 different domains selected from Google News until 2008. Moreover, a new text collection, containing 550 web pages from 275 different do-mains selected from Goggle News until 2011, is generated. Finally, we use a double-blind annotation technique to re-anno-tate the samples from these text collections. Using this annotation, we consider the following four classes: Main block : The largest informative block, which contains a DIV and TD tag with a high number of terms.
 Uninformative blocks : The blocks mostly used for Advertisement, Link, and Menu separated by DIV, TD and UL tags. Headline : The tags that contain the web page title.
 Article information : The tags contain the text summary, author name, date information, image titles, and user comments.
To annotate the classes given above, a double-blind annotation technique is used among eight annotators (academic staff, graduate students, and undergraduate students taking the Information Retrieval course). In this technique, two annotators independently mark the samples. A third annotator determines the differences between these two to define a Gold Standard. After this annotation, the statistical results ( Table 1 ) of the text collections are obtained.
 the usage of DIV tag has increased, the usage of TD has decreased between 2008 and 2011. This finding supports Uzun et al. (2011a) that indicate the TD tag has become an old informative block marker, while the DIV tag is a current trend due to evolution of the web. Datasets used in ML should contain enough samples for the learning process. However, there are ex-treme differences between the numbers of some tags in text collections. Therefore, two datasets as training and test dataset are created randomly from these text collections for balancing ML data ( Table 2 ).
 Training Dataset (Dataset-1) : 736 web pages from 573 different domains.
 Test Dataset (Dataset-2) : 434 web pages from 324 different domains.

The statistical results of the datasets in Table 2 are suitable for using these datasets in ML methods. The DIV block marker is the most common tag used to separate the main and irrelevant blocks. On the other hand, article information content highly shares most of the tags. We expect that this block has a lower prediction accuracy due to its uniformly distributed nature. 4.3. Feature selection
As in many other ML approaches, the methods applied in our experiments require that the data be represented as vectors of feature-value pairs. In our experiments, we not only adopt general shallow text features as feature vectors, but we also expand the common approach with combinations of classical features and after extraction features. These features are mostly referred to as shallow text features and are used to find the informative content of the tags from which they are ex-tions of classical features.
 Common approaches that use shallow text features also consider non-nested tags, including H1  X  H6 , P , FONT , A HREF ,
SPAN , EM , UL , and LI . However, our learning model construction also is based on the nested tags (i.e., DIV and TD ). Common approaches thus create a problem in our learning model. For example, a parent DIV tag may contain several uninformative and informative DIV children. This creates noise in the statistics derived from the parent DIV tag, so the features extracted may not contain appropriate link and word frequency ratios. To handle this problem, we extract most child DIV / TD tags to form a flat (non-nested) structure from the parent DIV / TD tags. After extraction (AE) of these child DIV / TD tags, we re-evaluate the values of the same feature sets. This extraction process provides new feature sets to the ML step. Table 4 gives these new features.

Differing from traditional approaches, our study proposes AE features to the learning problem because of the nested nat-ure of the used tags ( DIV and TD ). The effect of these new features for the learning task is investigated in the experiment given in Tables 3 X 5 introduces two tag labels and CSS styling features.

Tag name as a feature is useful for distinguishing whether a tag may belong to one of the four different classes introduced above. According to CSS styling, which is used for visualization of elements in a web page, some tags may contain ID and/or
CLASS attributes. We believe that whether a DIV has an ID or CLASS attribute may have a positive effect on the efficiency of the content extraction step. Fig. 4 supports this idea, but this feature is also investigated in detail below (Section 5 ). 4.4. Machine learning methods applied in this study
Four different common ML methods were applied to our dataset: a na X ve Bayes algorithm, a Bayesian Network Algorithm, an instance-based clustering algorithm ( k -Nearest Neighbor), and a Decision Tree Algorithm to discover an appropriate learning method. Descriptions of each learning algorithm are given below. The experiments are conducted using the Weka library with a tenfold cross-validation test method ( Witten &amp; Frank, 2005 ). 4.4.1. Na X ve Bayes classification
Na X ve Bayesian classification ( Rish, 2001 ) relies on the assumption that attributes are conditionally independent of each other given the class of examples. Though this hypothesis is often inappropriate for real-world problems where attributes strongly depend on each other, this classification approach helps reduce the dimensionality effect by simplifying the prob-lem. Given example X with a feature vector  X  x 1 ; ... ; x following likelihood:
Below are short descriptions of specific settings employed in our Na X ve Bayes classification experiments ( Witten &amp; Frank, 2005 ): Normal Distribution : Standard Na X ve Bayes classifier algorithm for numeric attributes.
 Kernel Estimator : Kernel estimation for numeric attributes rather than Normal Distribution.

Supervised Discretization : Used to convert numeric attributes to nominal ones. 4.4.2. Bayesian Network classification
Bayesian Networks ( Friedman, Geiger, &amp; Goldszmidt, 1997 ), more commonly called belief networks or probabilistic net-works, are directed a-cycling graphs (DAGs) that contain no cycles. In a Bayesian Network, each node corresponds to a random be conducted through inference. Network structure can be given manually instead of learning it from features. Our Weka experiments use settings of two structure-learning search algorithms, namely K2 ( Cooper &amp; Herskovits, 1992 ) and TAN ( Cheng &amp; Greiner, 1999; Rish, 2001 ). Both algorithms are used in local searches for the appropriate structure in Bayesian
Networks . 4.4.3. k-Nearest Neighbor classification k -Nearest Neighbor classification ( Bremner et al., 2005 ) is a nonparametric approach used to estimate the class-condi-tional densities, namely P ( X | C i ). Given the discriminant function as below: we have P ( x | C i )= k i /( N i V k ( x )), where k i is the number of neighbors of the k -nearest that belong to C the n -dimensional hypersphere centered at x , with radius r =|| x x thus be selected appropriately. 4.4.4. Decision Tree Classification
In Decision Tree Learning ( Breiman, Friedman, Olshen, &amp; Stone, 1984 ), trees are composed of decision nodes and terminal gain of selecting an attribute to form a tree must be calculated, and a predefined number of the most informative attributes must be selected to minimize the depth of the tree. In cases where more than one hypothesis is extracted from the training set, the ensemble learning methods are used to increase classifier efficiency by selecting and combining a set of hypotheses from the hypotheses X  space. These hypotheses are combined into a single classifier that makes predictions by taking a vote of its constituents. One common method in ensemble learning is boosting. The boosting model is sequentially induced from the training examples where the example weights are adjusted at each iteration.

The Weka library provides an implementation of the C4.5 Decision Tree Algorithm ( Quinlan, 1993 ) in the J48 class. Some settings employed for Decision Tree Classification in our experiments are briefly explained below: Default Setting : Standard Decision Tree classifier algorithm.
 Reduced Error Pruning : An independent test set to estimate the error at each node.
 No Sub-tree Raising : Used to disable Sub-Tree Raising of the most popular branch.

Unpruned : Used to disable prepruning strategies. 4.5. Classification metrics There are several metrics to assess the performance of the ML Methods. One of them is N -Fold Cross Validation. In N -Fold
Cross Validation, N tests are made on the dataset, and the observed metrics, including accuracy, precision, recall and f -Mea-sure. This definition table is called a Confusion Matrix: the actual values are in rows, and the predicted ones are in columns.
The diagonal cells show the number of correct predictions for both positive and negative cases. Other cells show both mis-classifications and actual class classifications.

The accuracy metric allows for measuring the percentage of correct predictions for the overall data. This metric accounts for both positive and negative instances. According to the definitions given in Table 6 , the following equations define the accuracy, precision and recall, respectively.
In a special case where beta ( b ) equals 1, the f -Measure combines precision and recall by calculating their harmonic mean and can be called f 1 -measure .
 Moreover, the last metric is kappa statistics, which measures the degree to which two different ML methods perform.
Kappa statistics is an alternative to the accuracy measure for evaluating methods. It was first introduced as a metric used to measure the degree of agreement between two observers ( Cohen, 1960 ) and has been used in several disciplines. In
ML, it is a measure to assess the improvement of a method X  X  accuracy over a predictor employing chance as its guide. This measure is defined as: where P o is the accuracy of the method, and P c is the expected accuracy that can be achieved by a randomly guessing method on the same dataset. Kappa statistics has a range between 1 and 1, where 1 is total disagreement (i.e., total misclassifi-cation), and 1 is perfect agreement (i.e., 100% accurate classification). Kappa fundamentally assesses how much better a learning method is compared to the majority, and class distribution-based random classifiers score zero kappa. Landis and Koch (1977) suggest that a kappa score over 0.4 indicates a reasonable agreement beyond chance. 4.6. ML results and error analysis
ML methods can classify the informative and uninformative content within an error margin. Each learning method may have a different error rate for the same dataset. To find the most accurate learning method, we used two datasets for training uate and compare different ML methods with several configurations. Additionally, the test dataset is used to understand the efficiency of obtained results from the training process. In the performance evaluation, we measured the accuracy, precision, recall, f -Measure and kappa statistics. Table 7 gives the training (cross-validated) and testing results.
The Naive Bayes algorithm is one of the simplest algorithms used in the learning task. Though it gives good results with the Supervised Discretization method, it does not perform better than other algorithms due to low kappa results. In Bayesian
Networks, TAN and K2 search methods are tested. These search methods form a proper Bayesian network structure and boost the accuracy. The results of other two methods, k -Nearest Neighbors and Decision Tree, are very close. In k -Nearest mance is obtained in the Decision Tree Learning and its Sub-tree Raising method with 95.76% accuracy in training dataset and 94.88% accuracy in testing dataset. To expand the details of the Decision Tree Learning, Table 8 gives the confusion matrix.

The confusion matrix compares the actual results with the predicted ones in terms of classes. The number of uninforma-tive classes is higher than other classes, which is why they are more likely to be confused in the inference. In Table 8 , the uninformative classes are mis-classified and are confused with the article information class in 272 instances of training data-set and 201 instances of testing dataset. We believe that this is expected, as both classes contain similar link structures and word counts. Manual observations generally show that article information contains shorter links. To overcome this problem, we can set a threshold value for the length of the links. The system may, however, accept the links of  X  X  X end, print and Face-book X  X  this time. The most common approach for short and uninformative links is to group the common words for the same web domain and remove them directly. This makes the approach language dependent, so we did not manage this adjustment to conserve language independency. The result of the Decision Tree Learning algorithm is a binary tree that provides a better understanding of features and their relation. The decision tree in our model consists of 269 decision nodes and 153 leaves. Fig. 5 shows the portion of the actual tree used to predict the blocks of main and article information.

When we analyze Fig. 5 , we see that 614 of 675 Main Blocks are classified correctly with only using D-HTML-AE (Density in HTML  X  After Extraction), R-WF-L-AW-AE (Ration of Word Frequency in Links to all words  X  After Extraction) and WF-L-AE decision tree. As a result, after extraction features and new features like R-WF-L-AW-AE and WF-L-AE derived in our ap-proach have positive effects in Main prediction. On the other hand, other features that do not have AE features are also effec-tive in article information prediction. 449 of 1352 article information blocks are classified correctly by using six features. for the prediction of Main Block.

The information gain, a statistical property, can be used to examine the effects of all features in prediction. Information gain measures how features are effective in different combinations. Fig. 6 shows different feature sets and their information gains for the whole learning process.

In Fig. 6 , each node value corresponds to the information gain of that feature. The last node represents the gain of class distribution that does not correspond to a feature at all. The highest gain is utilized by the R-WF-L-AW (Ratio of Word Fre-quency in Links to All Words) feature that is derived in our approach. However, other features, including R-WF-L-AW-AE (Ratio of Word Frequency in Links to All Words  X  After Extraction), TN (Tag Name), WF-L (The count of terms inside A HREF links placed tags), A-WF-L (The ratio of the number of terms inside A HREF links placed inside tags to the number of links), LF (The count of A HREF links inside tags), WF-AE (The number of terms inside tags  X  After Extraction), D-HTML-AE (Density in
HTML  X  After Extraction), D-HTML (Density in HTML) and WF (The number of terms inside tags), have a significant gain to-gether. AE features have significant effects on learning Main Blocks, and their contribution to overall learning is thus low.
Most other classes (i.e. article information and headline) are non-nested tags, which is why AE features do not change much tures, suggesting that their contribution to the learning is higher.

Kohlschutter et al. (2010) achieved a 0.92 f -Measure score by performing four class (i.e., boilerplate, full-text/comments, headline and supplemental) classifications with Decision Tree Learning. Our results indicate a higher performance, approx-imately 0.96 f -Measure, for four different classes. A one-to-one comparison of these two studies is not possible, since our learning model works on different HTML tags which uses different annotation sets. The main difference of our study is that our model provides a way to generate rules that can be used in efficient extraction by using only string manipulation. Con-sequently, there are learning based approaches but none of them produce rules and there are also hand-crafted rule based approaches that never use a learning model for content extraction. 4.7. Obtaining rules from ML results
After accomplishing the learning process, a well-formed document in XML format is created. This document contains four different classes with their tag information, attributes and content. Fig. 7 gives an example of a document created after learning.

The attribute representations (tag, parenttag, class and id) of each tag are important because they help distinguish and reach content by string search functions. In Fig. 7 , a web page has a Main block within DIV tags, which has an ID attribute with a value of  X  X  X econdLevelColOne X  X  and a parenttag with a null value. This inference can be used to reach the content from other web pages. Every web domain has different sets of tag names and attributes, so similar web pages can utilize this infer-ence. For this situation, if our approach cannot reach the content, the learning phase infers web content. Using this well-formed document as rules, the web site information can be extracted easily. We will now investigate the efficient extraction of web content via the rules represented in this document.
 5. Efficient web content extraction using rules
Web designers generally change web page content after forming the structure of the web page. They often do not touch the structure of the web page for a long time. If they need a different view, a single style change may change the view of all web documents. To maintain such changes, professional designers carefully select names for the HTML tags and attributes.
They especially use CSS on their web pages for efficient updates. Moreover, there are alternative approaches for naming. For instance, content management software users, including Joomla, WordPress, and Drupal, aware of tag naming. Naming is an important issue for the efficient content extraction phase of our approach. If the rules ob-tained from learning are inadequate for extraction, the learning phase is invoked. Naming thus decreases the noise and errors caused by the designers. Proper naming in web pages makes it easy to find content. Fig. 8 gives the rules extracted from the well-formed document.

Investigating the rules in Fig. 8 helps develop a template to create the original web page. For the rules given in Fig. 7 , the web designer intended to hold the Main content inside the DIV tag, which has an ID value of  X  X  X econdLevelColOne X  X . Similarly, the designer used the H2 tag, which has a CLASS value of  X  X  X lackHead X  X  in the web page title, and the parenttag shows that this title is in Main content. The &lt;SPAN class = date&gt; rule gives date information for the article.

A simple web content extractor algorithm can interpret the rules. Though simple regular expressions may be used to ex-tract non-nested blocks, including H2 and SPAN , they are not productive for the nested structures. In this study, Algorithm 1 is coded to handle nested structures.

This algorithm is based on simple string manipulation functions, including search and substring. The number of start and end tags are calculated to check whether the current rule is proper for the content. If the number of start and end tags are equal, a substring function can easily extract the content. 5.1. Rule induction from ML vs. efficient extraction with rules
The timing performance 3 of the content extraction from a web page is analyzed for the two steps of our approach: rule induction and efficient extraction. Table 9 shows the average extraction results of all web pages used in datasets. The timing performance of the rule induction step involves two processes: DOM tree creation and feature extraction from
DOM that increase time complexity in content extraction. In previous approaches that focused on content extraction, timing performance is not considered important; our approach does consider timing performance to be important. In this step, our approach differs from other approaches by creating a well-formed document that contains rules. These rules are used in the efficient web content extraction step. Table 9 shows that the efficient web content extraction step is much faster (with 240 times) than the rule induction step because the efficient web content extraction searches only for the informative tags on a web page. Conversely, DOM considers all tags, so it is a complex process. 5.2. Error analysis of efficient web content extraction step
The automatically obtained rules from the ML step may not always generate the correct results. When a failure occurs, our approach returns to the starting point and generates the DOM tree to accomplish the rule induction via ML methods. There are two common errors caused by web designer mistakes and rules falling behind in this extraction step (see Fig. 9 ).
Designer-oriented mistakes create problems that are similar to those in Fig. 9 (web page 1). In this example, the &lt;DIV id = article&gt; block contains the main content in the first use and the menu in the second use. Extraction through rules ob-on top of DOM and creates the final result without using the rules. CSS styling might prevent designer mistakes. A web de-signer can change the view of the web page with simple CSS changes. While traditional styling in DIV tags affects only single web pages, CSS styling affects all domains. An example of how styling is used inside a DIV tag is given below. &lt;DIV style = PADDING-BOTTOM: 2px; PADDING-LEFT: 2px; PADDING-RIGHT: 2px; COLOR: #035088; FONT-SIZE: 8pt; PAD-
DING-TOP: 2px X  X  align = left&gt;
This example position information shows that font color, font type, and alignment information are placed in DIV tags. The style attributes are mostly important in vision-based web page segmentation.

Rules may sometimes not work at all. This happens when they do not have unique values. For example in the Fig. 9 (web is in effect, two results can be obtained where the second result has a mistake. This means an error in rules. Some web designers usually use attributes such as ID or CLASS to differentiate blocks from one another. But, some web designers do not use any attributes such as ID or CLASS like in this example. When the tag information is not unique, an error occurs in extraction step and then the first step starts again for discovering informative content. Table 10 gives information about the accuracy/error rates of web content extraction rules and percentage of tags which contain ID or CLASS . Error rate is the tion to all rules. Moreover, we investigate the percentage of attributes ( ID or Class ) in proper rules.

In Table 10 , rule accuracy is given for three classes (main, headline, and article information). Main and headline content extraction achieve 83.74% and 75.54% accuracy, respectively. This rate decreases in article information extraction because designers do not care about tag naming in this class. It can be inferred that designers mostly consider this section unimpor-tant, and it contains most designer errors. The accuracy of the Main content extraction reaches 93.69% if the tag contains CLASS and ID attributes. This means that designers are careful to prepare the Main tag naming.

The overall performance concluded that 71.92% of web pages can be extracted directly by rules obtained from a well-formed document. The success rates should be expanded to understand the classes for each tag. Table 11 shows these rates.
Table 11 shows that there is a noticeable drop in the accurate extraction of TD tags. While the main content of the DIV tag is successfully extracted (88.45%), the accuracy of the extraction of the TD tag is low. Traditional successful approaches can racy rate of DIV tag extraction is high. The efficient extraction step of our approach is thus an appropriate solution for web pages that use DIV tags in web design. Moreover, the ML step supports other web pages.

For the cases caused by user mistakes and non-unique tag information, different solutions can be implemented. To extract article information, including the date and author name, several keyword-based methods can be developed. Keyword-based methods may mark content for the specific words without looking to the tag information. However, keyword-based methods are not error-tolerant for different languages, and preparing selective keywords is difficult. 6. Conclusion
This study presents a two-step content extraction approach. The first step involves feature selection, traditional DOM cre-ation, and the ML method. This step generates a well-formed document that contains rules used in the second step. The sec-ond step extracts informative content efficiently using string manipulation functions. In the first stage, the Decision Tree
Learning with Sub-tree Raising method as a suitable method achieved 95.76% accuracy. In the second step, the rules inferred from the first stage are used to efficiently extract three different informative content blocks with a 71.92% of rules that are stage is invoked. This approach is a hybrid solution that uses two different techniques that support each other.
The ML stage of this approach considers after extraction features to overcome the nested ambiguity of block tags and to find the largest informative block, which is mostly the Main content and the informative sub-blocks. Moreover, the R-WF-L-AW (Ratio of Word Frequency in Links to All Words), is derived in this study, is an important feature in ML step.
These features play important roles in web content extraction. This approach can enhance search engine results, web content summarizations and web mining applications. The first stage of this approach is appropriate for platforms in which time per-formance is not important. Conversely, the second stage is an especially useful solution for efficient web content extraction.
Adapting this approach for a crawler to efficiently extract informative content from the web is a future research focus. We also plan to enhance this approach to automatically infer different data, including name, price, comments, and technical product details, from shopping web domains.
 Acknowledgements We thank Salih Kagan Agun and Edip Serdar G X ner for comments, support and encouragement. We thank Onur Erdog  X  an, Ali R X za Alt X nparmak, Cihat Erdog  X  an, Yasin Akman and Emir  X zt X rk for their annotations.
 References
