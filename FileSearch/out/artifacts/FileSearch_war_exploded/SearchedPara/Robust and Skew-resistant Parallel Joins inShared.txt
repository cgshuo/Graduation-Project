 The performance of joins in parallel database management systems is critical for data intensive operations such as query-ing. Since data skew is common in many applications, poorly engineered join operations result in load imbalance and per-formance bottlenecks. State-of-the-art methods designed to handle this problem offer significant improvements over naive implementations. However, performance could be fur-ther improved by removing the dependency on global skew knowledge and broadcasting. In this paper, we propose PRPQ (partial redistribution &amp; partial query), an efficient and robust join algorithm for processing large-scale joins over distributed systems. We present the detailed imple-mentation and a quantitative evaluation of our method. The experimental results demonstrate that the proposed PRPQ algorithm is indeed robust and scalable under a wide range of skew conditions. Specifically, compared to the state-of-art PRPD method, we achieve 16%  X  167% performance improvement and 24%  X  54% less network communication under different join workloads.
 H.2.4 [ Information Systems ]: Database Management X  Systems Algorithm, Performance
In data-intensive environments such as data warehouses and the Web, efficient execution of query operations is cru-cial for the overall performance of the system. An essential component of query operations is the join , which is widely used in various database management systems (DBMSs). A join facilitates the combination of two relations based on a common key. For example, the join between a relation R ever there are significant performance issues with both ap-proaches. For the hash-based scheme, while a near linear speedup has been demonstrated under ideal balancing condi-tions [10] the presence of significant data skew dramatically impacts performance [11] due to node hot spots. Although duplication-based methods can handle skew, the broadcasts of each R i to all the nodes incurs a heavy time-cost and building a large hash table based on n i =1 R i at each node has detrimental impact on performance due to the associ-ated memory and lookup cost [12].

As data skew occurs naturally in various applications [20], it is important for practical data processing systems to per-form efficiently in such contexts. In this paper, we intro-duce the semijoin-alike joins , a novel parallel join approach for handling data skew in distributed architectures. Using this design as a basis, we propose an efficient and robust join algorithm referred to as PRPQ ( partial redistribution &amp; partial query ) which is capable of higher performance than current techniques. We implement our algorithm using the parallel programming language X10 [6] and evaluate its per-formance on an experimental configuration consisting of 192 cores (16 nodes) and datasets of 1 billion tuples. Moreover, we present a quantitative performance comparison with the standard hash-based algorithm as well as the state-of-the-art technique, PRPD, presented in [24].

Our results demonstrate that the proposed PRPQ algo-rithm is: (a) robust against data skew , exhibiting excellent load balancing in the presence of different skew conditions, (b) scalable , with speedup increasing with the number of nodes (threads), (c) highly efficient , since we can process the join 64 M 1 B with high skew in only 10.8 seconds, faster than all methods in the literature, (d) simple ,sincewedo not need global operations such as dataset-wide statistical measures to quantify skew and, in fact, algorithm progres-sion on each node is full determined by its local skew, (e) robust against parameter tuning , since, unlike the state of the art, it is not overly sensitive to thresholds for skew de-tection and (f) novel , since the algorithm does away with the need for a duplication strategy, an approach which has underpinned nearly all other skew-aware technique to date. Finally, our method is shown to consistently outperform the PRPD algorithm with 16%  X  167% runtime improvement and 24%  X  54% less network communication, depending on the parameters used.

The rest of this paper is organized as follows: In Section 2, we report on related work and details on the state-of-art method. In Section 3, we introduce our PRPQ approach and its differences with existing approaches. The detailed implementation of our algorithm is presented in Section 4. Section 5 provides a quantitative evaluation of our algorithm while Section 6 concludes the paper outlining plans for fu-ture work. Related work on joins. Data skew is a significant prob-lem for multiple communities, such as databases [17], data management [3], data engineering [5] and Web data pro-cessing [20]. Joins with extreme skew can be found in the Semantic Web field. For example, in [20], the most frequent item in a real-world dataset appeared in 55% of the entries.
Research in parallel joins on shared memory systems [17, 2, 5] and GPUs [14, 16] has already achieved significant per-lelism as a new distributed geography (namely not just a simple join operation) for handling data skew and apply it for parallel inner joins and outer joins directly .Inthis work, we focus on the inner joins (namely joins ). We intro-duce the semijoin-based joins and the query-based joins [7] and analyse their differences. Moreover, we have refined the methods to achieve better robustness and performance on parallel joins.
 State-of-the-art PRPD. Xu et al. [24] proposed an al-gorithm named partial redistribution &amp; partial duplication (PRPD), which can be considered as a hybrid method com-bining both the hash-based and duplication-based join scheme.
For the two input relations 1 , they partition S into two parts: (1) locally kept part S loc , the high skew part are kept locally and do not join the redistribution phase, and (2) the redistributed part S redis , the tuples with low frequency key are redistributed as in a common hash-based implementa-tion. The relation R is divided into two parts as well: (1) the duplicated part R dup , the tuples in which contain the keys in S loc , which will be broadcast to all other nodes, and (2) the redistributed part R redis , the remaining part of R that is to be redistributed to a single node as normal. After the duplication and the redistribution operations, the final each node. This method efficiently processes the high skew tuples (keys are highly repetitive). All these tuples of S are not redistributed at all; instead, they just broadcast a small number of tuples contains the same keys from R .

Their experimental results show that PRPD can achieve significant performance when compared with the basic hash approach, in the presence of data skew. Even so, PRPD may still suffer from two major problems. (1) Global skew , global operations like statistical calculations or broadcasts for the skew keys at each node are required a priori. As the split of R and S fully relies on the skew keys in S , the final join will fail if any node does not have global knowledge of such keys. (2) Broadcasting , the duplicated part from R will lead to significant network communication as the number of such tuples as well as the number of nodes increases, especially if the system is not well tuned.

In comparison, our proposed PRPQ approach only needs to quantify the local skew and does not duplicate data. As a result it is faster and more robust than PRPD in our eval-uation presented in Section 5.
In this section, we first introduce two efficient skew-resistant join methods: the semijoin-based joins and its variant query-based joins [7]. From that basis, we propose a refined method so as to further improve the join performance and robustness in the presence of data skew. Semijoin-based joins. The approach of semijoin-based distributed joins is shown in Figure 3, where the two com-munication patterns ( redistribution and retrieval )withfull parallelism makes it different from the conventional join ap-proaches and the commonly-used semijoins.

For the join between R and S on their join attributes a and b , the detailed processing can be divided into four steps:
For simplicity, we assume R is uniformly distributed and S is skewed for all of our examples, unless otherwise specified. In this process, we only retrieve values (instead of tuples). The reason is that we can always keep the transferred keys and retrieved values in the same sequence (e.g. by array indexes) so that the &lt; key, value &gt; pair can be easily iden-tified to compute the final join as described in the fourth step. We call this variant as query-based joins because the process of transferring keys to remote nodes and retrieving the corresponding values looks like a query. We also name the distributed pattern used by the two basic algorithms as semijoin-alike approach, as it is derived from the conven-tional semijoin method.

Obviously, the query-based joins can outperform the semijoin-based method when processing joins with high selectivity factors as it removes part of the redundant key transferring. In contrast, it will be slower when the selectivity factor is small, because it needs to fill the unmatched values as null and move such useless values to the requester(s) to keep the sequence of the &lt; key, value &gt; pairs for the final joins. In fact, using a simple counter to record the ratio of the nulls appearing in step 3 of the query-based method can easily guide us to choose a suitable method dynamically during join processing. Namely, when the ratio is high, we can use the query method. Otherwise, we use the basic semijoin ap-proach. In such scenarios, the semijoin-based approach is a sub-instance of the query-based implementation, but sac-rificing part of the available system memory (as the trans-ferred keys in the second step need to be kept in memory during join processing when using the query method). Performance issue. We are more interested in computing joins directly in distributed memory rather than frameworks such as MapReduce [4], which is optimized for on-disk pro-cessing. Therefore, the network communication will be crit-ical for performance. In such a case, the above two methods will meet performance issues in the face of data with little skew: Since the number of transferred keys and retrieved tuples (or values) will be huge when processing large joins, and the communication overhead will become unacceptable consequently. To address this issue and achieve robustness and higher performance in the presence of skew, we propose our optimized method in the following.
As we focus on join performance over different distributed join patterns , and both the two basic methods above use the same semijoin-alike geography, for simplification, we onlychoosethemorecapable query-based joins as a study case in the following 3 . More specially, we only choose the basic query method without any counters to record the ratio of nulls , since the time difference between the semijoin-based and query-based method will be very small using our new method, regardless of different selectivity factors (which we explain later).
Similar to the PRPD algorithm, we divide the skewed in-put relation into two parts: (1) the low skew part, which is processed by the conventional hash-based method, and (2) the high skew part, using the basic query algorithm as described. In this context, the implementation can be considered as a hybrid approach based on both the hash-based and query-based implementation and thus we call it As we do not consider memory consumption in this work. that none of the highly skewed tuples are distributed, but only their unique keys as well as the corresponding returned values, which are always small. Furthermore, as PRPQ adopts the complementary advantages of both hash-based and query-based implementations, the method should, for any kind of inputs, outperform both algorithms for a suitable threshold t . We will exam this conjecture through our eval-uation. In addition, PRPQ has an extra operation, namely quantifying the skew so as to partition the tuples. However, we only need to quantify the local skew (namely for each S ) at each node, which has low computation cost.

Looking back to the semijoin-based joins, if we apply the hybrid idea to that scheme to organize the algorithm PRPS ( partial redistribution &amp; partial semijoin ), the same as PRPQ, its robustness will be highly improved compared to the basic semijoin-based method. Meanwhile, its join per-formance will be nearly the same as PRPQ. The reason is that the number of transferred keys is relatively small, which brings little difference to the number of retrieved tuples or values in the case of different selectivity factors. Conse-quently, this will bring little different in terms of time cost on a common cluster with Gigabit Ethernet. That is also why we focus on the performance of PRPQ, when we com-pared with the state-of-the-art PRPD algorithm [24], in our later evaluation.
Taking a higher level comparison with the PRPD [24] method, there are two main advantages to our approach: (1) we do not need any global knowledge of the relations in the presence of skew, because the skew quantification at each node is totally independent from each other, while [24] requires a global operation to quantify or exchange skew information; and (2) our approach does not involve any re-dundant join (or lookup) operations because each node in our method is either distributing or query what it needs , while [24] is broadcasting , so that some nodes may receive tuples that they do not need.

In fact, the first advantage leads to our method being more flexibleormoreefficientinthefaceofdifferentjoinwork-loads, especially for the unevenly distributed ones. Taking an extreme condition for example, for a 10 6 -node system, if a key in S follows the linear distribution over the com-putation nodes (e.g. appearing 10 6 times on the first node, 10 6  X  1 on the second node etc. and only 1 time on the final node), then how can we define the global skew using PRPD? [24] proposes a solution that redistributes the skew tuples evenly to all the nodes before the join. However, this pre-redistribution will generate extra communication costs, while more complex and careful global statistical operations for all tuples of S are required. The authors in [24] do not provide any detailed implementation or experimental details regarding this pre-procesing. Therefore, for PRPD in the following, we do not consider any rebalancing operations for the uneven skew of S but just adopt a general method, namely each node just broadcasts its local skew keys so as to organize the global skew. In contrast to these, simply using a threshold such as 10 6 / 2, PRPQ will know that the key is skewed in the first half million nodes and not-skewed for the rest of nodes.

Moreover, in the condition with many mid-skewed tuples, for instance, the relation S i at some node i contains 1 mil-lion unique keys (assuming uniformly distributed) with each Algorithm 1 R Distribution 1: Initialize R c :array[array[tuple]]( n ) 2: for tuple  X  list of R do 3: des  X  hash( tuple.key ) 4: R c ( des ) .add ( tuple ) 5: end for 6: for i  X  0 .. ( n  X  1) do 7: Push R c ( i )to r R c ( i )( here )atnode i 8: end for Algorithm 2 Push Query Keys 1: Initialize T :array[hashmap[key,ArrayList(value)]]( n ), 2: Read the skew keys in skew based on t 3: for tuple  X  list of S do 4: des  X  hash( tuple.key ) 5: if tuple.key  X  skew then 6: Add tuple in T ( des ) 7: else 8: Add tuple in S X  c ( des ) 9: end if 10: end for 11: for i  X  0 .. ( n  X  1) do 12: Extract keys in T ( i )to key c ( here )( i ) 13: Push key c ( here )( i )to remote key ( i )( here ), 14: end for implementation on memory following the four phases as de-scribed previously. p1: We first read all the tuples in an ArrayList at each node, and then commence distribution of the relation R . The detailed process is given in Algorithm 1. The array R c is used to collect the grouped tuples, and its size is initialized to the number of computing nodes n . Then, each thread reads the ArrayList of R and groups the tuples according to the hash values of their keys. Next, the grouped items are sent to the corresponding remote nodes. Note that the term here means the id of current computing node (core). p2: The implementation of the second step is given in Algorithm 2. The skew keys are first read into a hashset based on the parameter t . Next all the tuples in S will be checked for skew such that hashmap collects the skew tuples while the arrays S c collects the non-skew tuples. After processing all the tuples, the keys in each hash table will be extracted by an iteration on its keyset . These keys will be kept in key c ,thesameas S c , both are pushed to the assigned remote nodes for further processing.

Both the T and key c are kept in memory for the sub-sequent lookup results, as mentioned in Section 3.2. We synchronize the operation here to guarantee the completion of the data transfer at each node before the next phase com-mences. p3: The implementation of this phase at each comput-ing node is similar to a sequential hash join. The received tuples and key arrays, representing the distributed R , S and grouped query keys respectively. For the tuples, all the &lt; key,value &gt; pairs of R are placed in the local hash table T ,and S looks up the match in T to output the join re-sults for the non-skew tuples. Meanwhile, the query keys access T sequentially to get their values. In this process, if the mapping of a key already exists, its value is retrieved, otherwise, the value is considered as null .Inbothcases,the value of the query key is added into a temporary array so tuples 5 and S to 1B tuples. Because data in warehouses is commonly stored following a column-oriented model, we set the data format to &lt; key, value &gt; pairs, where both the key and value are8-byteintegers.

We use similar workloads as used in recently studies on parallel joins [17, 3, 5]. Keys of two input relations R and S follow the foreign key relationship, and we keep the primary keys in R as unique while adding skew to the correspond-ing foreign keys in S . Meanwhile, when S is uniform, the tuples are created in such a way that each of them matches the tuples in the relation R with the same probability. For the skewed ones, the unique keys of tuples are uniformly distributed and each of them has a match in R 6 . We list the input of S in the Table 1 in bold font indicating default values.

For the Zipf distribution, the skew factor is set to 0 for uniform, 1 for low skew (top ten popular keys appear 14% ofthetime)and1.4forhighskew(toptenpopularkeys appear 68% of the time). For the linear distribution case, we use the function f ( r ) to describe the key distribution, where r is the rank of a key, according to its popularity. For example, f ( r ) = 46341  X  r means that the most popular key appears 46341 times, the second one appears 46340 times etc. This data set can be considered as low-skewed. Mean-while, f ( r ) = 23170 is a dataset, in which keys are uniformly distributed but highly repetitive. Both these two datasets contain 1B tuples with 46341 unique keys.

Moreover, to conduct more complete performance com-parison in the presence of different workloads, we distribute all the tuples in R evenly to all computing nodes while we use both evenly and sort-range methods for S . The former method guarantees that the number of skewed tuples will be the same on each computation node. In the latter method, all tuples are first sorted according to key popularity, and then partitioned in equal-sized chunks and assigned to each node sequentially. This means that the number of skewed tuples can have great variation between computing nodes.
In all experiments, we only count the number of matches, but do not actually output join results. Moreover, for PRPD and PRPQ, we implemented a test series with different t for each data set, as shown in Figure 6. When we present the results in other figures or tables, we always choose the point t with the best achieved run time from Figure 6.
We consider the runtime of the four algorithms 7 the hash-based algorithm (referred to as Hash ), PRPD [24], PRPQ
Throughout the paper, when referring to tuples, M=2 20 and B=2 30 .
The join selectivity factor would be relatively high follow-ing the literature, and the query-based part of our PRPQ could get very small profits from this kind of setting (we do not consider output materialization). Regardless, for low selectivity, as mentioned, our PRPQ can be transferred to PRPS if needed. Thus, we do not conduct additional test results for datasets with different selectivity factors in the following.
Recall again that we focus on performance issues over dif-ferent join patterns. The semijoin-based method will have the same characterization as the query-based method, and the detailed comparisons of such method will be beyond the scope of this paper.
 Table 2: Speedup achived by PRPQ over PRPD with varying the size of inputs (using 192 cores) Skew 1 1.4 Scale 0.5 1 2 0.5 1 2 Speedup 1.42 1.16 1.20 1.44 1.22 1.48 Table 3: Detailed number of received tuples at each core Skew/ 0 1 1.4 Algo. Max. Avg. Max. Avg. Max. Avg.
 Hash 5.94 5.94 62.40 5.93 347.76 5.94 PRPD 5.94 5.94 3.53 3.51 1.16 1.13 PRPQ 5.94 5.94 2.65 2.64 0.53 0.52 Query 5.94 5.94 2.12 2.12 0.43 0.43 the linear distribution f ( r ) = 23170 while Linear 1 refers to f ( r ) = 46341  X  r ; (4) the first two numbers in the paren-thesis indicate the value of t for which the best performance achieved by PRPD and PRPQ respectively while the third one demonstrates the relative speedups of PRPQ over PRPD based on their best runtime.

We can see that, for any given t , PRPQ always performs better than PRPD. Looking at the detailed figures, PRPQ can achieve 16% -176% performance improvement over PRPD. The maximum achieved speedup of 2 . 67  X  happens in the case of Linear 0 evenly dataset. This is due to the fact that the number of picked skew keys is always large and this case, the key distribution at each node follows f ( r )= 23170 / 192 = 121, namely each key appears 121 times. Thus, when t&lt; 121, all the 46341 keys at each node will always be processed as skew keys, which makes the time difference between PRPQ and PRPD large. This also appears in the cases (a),(b) and (e): with a small t at the beginning, a large number of skewed keys leads to a large difference. With in-creasing t , the difference decreases to almost 0, as the num-ber of picked skew keys becomes smaller and smaller.
Finally, the variations of the results achieved for differ-ent t values are only minor for the PRPQ algorithm while those in PRPD change more sharply, demonstrating that our algorithm is less affected by the input parameters (i.e. tun-ing). Defining the t in a range that achieves better perfor-mance would require additional, more complex or costly op-erations, therefore, we can expect that our algorithm could profit more on performance than PRPD in real applications.
We also examine the speedup by varying the cardinalities of the two input relations. For the Zipf distribution, we create data sets in which both relations are half the default size (scale 0.5, namely 32 M 512 M ) and double the size (scale 2, namely 128 M 2 B ). We vary the threshold and record the best achieved runtime. Table 2 shows the results, which demonstrate that our algorithm can achieve higher performance irrespective of the input size.
Communication costs are evaluated through measuring the number of received tuples at each core. The average Figure 7: Average number of received tuples at each core by varying the threshold ( 64 M 1 B with 192 cores). ing. However, is is less than PRPD for each given t , showing the advantage of Query in such a aspect. Combining this with the value where best performance is achieved, t is set to 2 7 and 2 14 for PRPD, values that are greater than the values of 2 4 and 2 5 for PRPQ respectively. This is the rea-son why PRPQ clearly transfers less data than PRPD in Table 3, notably 24%  X  54% less under the skews.
We also analyze the load balancing properties of each al-gorithm based on the number of received tuples. The values for the maximum and average number of received tuples at each core are shown in Table 3 as well. We can see that all four algorithms achieve perfect load balancing when the data set is uniform. With increasing skew, the difference be-tween the value of the maximum and the average for Hash increases, indicating poor load balancing in the presence of skew. In comparison, PRPD and PRPQ have more toler-ance, showing their ability for handling the skew. In the meantime, the basic Query algorithm is always balanced, showing its special characteristic on this metric.
We evaluate the scalability of our PRPQ implementation by varying the number of processing cores on the three de-fault datasets, from 24 cores (2 nodes) up to 192. Results are presented in Figure 8, and each phase there is consistent with the implementation explained in Section 4.2.
It can be seen that PRPQ generally scales well under dif-ferent skews. Notably, the relative speedup achieved be-tween 48 and 96 cores is close to the ideal 2x, which is obvi-ously greater than that between other settings. This could be attributed to the network overhead, in that inter-machine communication is more quickly extended at the beginning. For 192 cores, the data set becomes comparably small for the underlying system and coordination overhead becomes more significant.

Examining the details for each phase, under low skew, phase 2 and 3 scale well and are the dominating factor for the runtime. In the case of high skew, the third phase be-comes comparably much smaller and the second phase start to dominate the performance, which decreases with increas-ing the number of cores. As the second phase mainly focuses on data transfer and the third, on join operations, the net-work load has a higher impact on the join performance than as with increasing the degree of skew for a fixed number of nodes. Furthermore, for high skewed data, PRPQ achieves nearly linear speedup while PRPD and Query do not. This can be attributed to the following reasons: (1) For Query , the frequency of each element at each core decreases with increasing cores, namely the ratio of low frequency elements increases. This in turn has a negative effect on speedup, as Query is not good at processing such low frequent data. (2) In comparison, via the variable t , PRPQ always processes high-frequency elements using Query and low-frequency ele-ments using Hash . This presents an optimal way to process the data and achieves better speedups. (3) The broadcast cost increases with increasing the number of nodes, which results in scalability loss in PRPD.
In this paper, we have introduced a new approach for parallel joins, called PRPQ ( partial redistribution &amp; par-tial query ). The approach has been devised specifically to target joins with skew in shared-nothing architectures. Our experimental results demonstrate the scalability and robust-ness of PRPQ joins against skew. PRPQ achieves significant speedups over the conventional hash approach in the pres-ence of skew and outperforms the state-of-art PRPD algo-rithm [24].

Data duplication is widely used in data engineering to re-duce data movement and load imbalance. As our algorithm is duplication-free, we anticipate that our proposed method will not only be a supplement to existing schemes on parallel joins to minimize runtime but also for other domains. We intend to apply our approach in the semantic web domain, where workloads present very high skew [20].
 Acknowledgments. This work is supported by the Irish Research Council and IBM Research Ireland. [1] M. Al Hajj Hassan and M. Bamha,  X  X n efficient [2] M.-C. Albutiu, A. Kemper, and T. Neumann, [3] S. Blanas, Y. Li, and J. M. Patel,  X  X esign and [4] S. Blanas, J. M. Patel, V. Ercegovac, J. Rao, E. J. [5] G. A. Cagri Balkesen, Jens Teubner and M. T.  X  Oszu, [6] P. Charles, C. Grothoff, V. Saraswat, C. Donawa, [7] L. Cheng, S. Kotoulas, T. E. Ward, and
