 Online searches generally fall into three cate gories, namely, informational, navigational and transactional [5]. A recent study has f ound that the majority of online queries are informational [14], which intend to locate information pertaining to a certain topic. A typical type of informational queries is to simply find out more about a topic such as an entity or an event. For this type of informational queries, instead of showing a ranked list of URLs in the traditional way, a short summary article for the query topic might be a better form to present the search results, from which users can easily di-gest and further explore. Indeed, recently G oogle X  X  search results started to include a summary page on the right hand side backed by Google X  X  Knowledge Graph , demon-strating the need to automatically summarize information related to a query. But it still remains a challenging task to automatica lly generate open-domain summaries without supervision.

In this paper, we take a less ambitious step and propose to study the task of finding related keyphrases given a query. This kind of query-oriented keyphrases can be useful for search in a number of ways. For example, t o generate an extractive summary of the search results, one may select sentences th at maximize the coverage of these related keyphrases. Related keyphrases can also serve as anchor points for further navigation from the original search results in exploratory search.

In Table 1 we show a sample output of our proposed task for the query Pixar ,where the top-10 keyphrases returned by the best configuration of our proposed method are listed. We can see that these top keyphrases are highly relevant to the query.
To find related keyphrases, we transform the task into a query-oriented keyphrase extraction problem where the goal is to extract keyphrases from a set of documents relevant to the given query. Keyphrase extraction has been extensively studied be-fore [12,20,19,22,21,13]. Existing work includes both supervised and unsupervised approaches. Because of the nature of our task, an unsupervised keyphrase extraction method is needed.

Following a framework proposed by Tomokiyo and Hurst [19], we propose a general method for our task which considers both phraseness and informativeness of a candi-date keyphrase. We consider three phraseness criteria based on language models, noun phrase chunking and named entity recognition, re spectively. We also consider four in-formativeness scores using phrase-level Tf-Idf, sum of word-level Tf-Idf, average of word-level Tf-Idf and language models, respectively.

We evaluate the various combinations of the criteria and compare our method with state-of-the-art baselines using 40 queries and a large Wikipedia corpus. We use ground truth both annotated by humans and automatically obtained from Wikipedia articles for evaluation. Experimental results show that the named entity-based phraseness criterion is the best for our task, and the language model-based informativeness score gives the best keyphrase ranking. This configuration o f our method outperforms the two state-of-the-art baseline methods we consider.

Our main contributions are twofold. First, we propose to study a new task of query-oriented keyphrase extraction and provide a general solution based on phraseness and informativeness. Second, we empirically compare different phrasesness and informa-tiveness criteria for this task and find a solution better than the baselines that represent the state of the art of keyphrase extraction. To the best of our knowledge the task of query-oriented keyphrase extraction has not been well studied. A related line of work is s earch results clustering, where oftentimes labels for clusters of documents are automatically generated [15,23]. These labels can be seen as phrases related to the query. A major difference between this line of work and our task is that our related keyphrases are not meant as topical labels for a cluster of documents but rather important concepts related to the query. For example, given the query  X  X ixar, X  related keyphrases may include key people such as  X  X ohn Lasseter X  (CCO) and  X  X ob Iger X  (CEO of the Walt Disney Company), but these names are not likely to represent different topics for documents relevant to Pixar.

In the information retrieval community, people have studied the task of ranking re-lated entities. Examples include the expert finding problem [6,18,1] and the related entity finding task [2,3], both studied in TREC. The INEX workshop also had an entity ranking track in the past few years [10,7]. Retrieving entities instead of just documents has become an important task for search engines and have been studied in [8,9]. What these tasks share in common is that entities of a specific type (e.g. person) or satisfying a specific description (e.g. airlines using Boeing 747 planes) are being sought after. In contrast, we do not impose such restrictions when extracting keyphrases.

There are generally two approaches to keyphrase extraction. Supervised keyphrase extraction relies on a set of training documents whose keyphrases have already been manually extracted to learn a keyphrase extraction model [12,20]. A major limita-tion of such methods is clearly the need for a training corpus. A number of unsu-pervised keyphrase extraction methods have been proposed. In particular, a number of PageRank-based methods such as TextRank [16], SingleRank [22] and ExpandRank [21] have gained much attention. However, in a recent comparative study by Hasan and Ng [13], it shows that a simple Tf-Idf method is more robust than the PageRank-based methods across four different data sets. In our paper we therefore use only the Tf-Idf method as one of our baselines. Another unsupervised keyphrase extraction method which was not compared in [13] was a language model-based method proposed by Tomokiyo and Hurst [19]. The authors stressed the importance of considering both phraseness and informativeness when extracting keyphrases, and proposed to use the KL-divergence between different language models to measure phraseness and infor-mativeness. We find that for our task indeed we need to consider both phraseness and informativeness, but they can be m easured in other alternative ways. Given a query q and a large document collection D , we define our task as to return a ranked list of keyphrases that appear in D and are highly related to q .

Our general approach is to first construct a document set D q that is highly relevant to q and then to apply a keyphrase extraction method to identify and rank related phrases occurring in D q . To construct D q , a straightforward solution is to retrieve a subset of documents from D relevant to q using a standard document retrieval method. Our pre-liminary experiments suggest that using the entire relevant documents for keyphrase extraction may result in many irrelevant phrases which do not co-occur with the query within close proximity and are hence not semantically closely related to the query. We therefore use only paragraphs containing the query from the relevant documents to construct our D q .

The next step is to extract keyphrases from D q . There have been many studies on keyphrase extraction, but existing approaches were not designed or evaluated for query-oriented keyphrase extraction, so it is not clear which existing method would work the best for our problem, nor is it clear whether there might be any better method. To an-swer these questions, we follow a general framework for keyphrase extraction proposed in [19]. In the paper, two criteria are consid ered, namely, phraseness and informative-ness. We study different ways of measuring phraseness and informativeness for our task. In the rest of this section, we first briefly review the notion of phraseness and in-formativeness, and then present the differe nt phraseness and informativeness measures we consider. We end the section by presenting two baseline methods that represent the state of the art. 3.1 Framework Tomokiyo and Hurst [19] proposed to use two criteria to judge whether a sequence of words forms a good keyphrase. (1) Phraseness measures the degree to which a se-quence of words is considered a phrase. For example,  X  X oogle earth X  should have a higher phraseness score than  X  X oogle owns. X  (2) Informativeness measures how well a phrase illustrates the key ideas in a set of documents. For example,  X  X oogle earch X  is more representative of a collection about Google than  X  X nited states. X  Tomokiyo and Hurst [19] measured phraseness and informativeness by using language models. In this paper, we propose to use different ways to measure phraseness and informativeness. The detailed phrasesness and informativeness measures we used are discussed as follows. 3.2 Phraseness Measures Language Model-Based: This measure was first proposed in [19]. To measure phrase-ness, first, a foreground corpus D F and a background corpus D B are identified. The extracted keyphrases are supposed to represent D F . For our task, D q is the foreground corpus and the whole document collection D is the background corpus. Next, a uni-gram language model and an n -gram language model can be learned from each corpus. the phraseness of a phrase w is defined as  X  w (  X  n F  X  1 F ) ,where  X  w ( p q ) is the point-wise KL-divergence between two language models p and q with respect to a sequence of words w , which is defined as follows: The general idea here is that if a word sequence is better modeled by an n -gram lan-guage model than by a unigram language model, then it is more likely to be a phrase.
We set a threshold  X  to filter out n -grams with low phraseness scores. In our experi-ments,  X  is set to 0. This same threshold has been used by Qazvinian et al. [17]. Noun Phrase-Based: The language model-based approach is not the only way to define phraseness. For example, depending on the application, we may be only interested in us-ing noun phrases as keyphrases. In this case, we use noun phrase boundaries as detected by a shallow parser to define candidate keyphrases. In this work, we use a noun phrase chunker 1 to identify noun phrases and filter out those n -grams that are not noun phrases. Named Entity-Based: For our task of extracting related keyphrases, we hypothesize that named entities may also be interesting to users. We therefore can define a more keyphrases. In this paper, we use a named entity tagger [11] to identify named enti-ties and filter out those n -grams that are not named entities. 3.3 Informativeness Measures Language Model-Based: Similar to phraseness, the informativeness of w can also be better modeled by a foreground language m odel than by a background language model, then it is more representative of the foreground corpus.
 Phrase-Level tf-idf-Based: The Tf-Idf scores can also be regarded as an informative-ness measure. This is because if a phrase c ontains words that are frequent in the fore-ground corpus (i.e. with a high Tf score) but not very frequent in general (i.e. with a high Idf score), then the phrase is more likely to be representative of the foreground corpus. We then define a phrase-level Tf-Idf score as follows to measure the informa-tiveness of a phrase. First, the Tf score of a candidate keyphrase w is its frequency in
D number of documents in D that contain the phrase w . Note that these definitions are the same as the Tf and Idf definitions for a single word except that here we consider a sequence of words.
 Sum of tf-idf-Based: Let us use ( w 1 ,w 2 ,...,w L ) to denote the sequence of words in phrase w . Using the word-level Tf-Idf scores, we define the informativeness score of w as L i =1 s ( w i ) ,where s ( w i ) is the Tf-Idf score of w i .
 Average of tf-idf-Based: In the measure above, a phrase w  X  X  score will be dominated by the most informative words within it. And the criterion will prefer a phrase with more words. To leverage the effect of dominant words and allow phrases with fewer informative words to be ranked high, we define a new informativeness measure by using the average word-level Tf-Idf scores, i.e. 1 L L i =1 s ( w i ) . 3.4 Algorithm Outline With the phraseness and informativenes s measures defined above, we now present the outline of our algorithm. 1. Candidate Phrase Generation: For a given query q and a document set D ,we first extract a relevant document set D q by extracting the set of relevant paragraphs containing the query q retrieved from D .From D q , we then generate all possible n -grams where n  X  X  1 , 2 , 3 , 4 } . 2. Keyphrase Filtering: We filter the n -grams according to one of the following three phraseness measures: language model, noun phrase and named entity based measure. We refer to the resulting candidate keyphrase set as P .
 3. Keyphrase Generation: Each phrase in candidate keyphrase set P will be scored by using one of the following four informativeness measures: language model-based infor-mativeness, phrase-level Tf-Idf score, sum of word-level Tf-Idf score, and average of word-level tf-idf score. Then phrases are ranked by their corresponding informativeness scores, resulting in our final keyphrase list.

In summary, in our proposed general method, we consider both phraseness and infor-mativeness, and we allow different ways to define phraseness and informativeness. Note that the language model-based phraseness measure has a phraseness score which can be combined with informativen ess scores to rank phrases. But the other two phrassness measures, noun phrase-based and named entity-based, do not have phraseness scores. To make a fair comparison, we use a threshold for language model-based phraseness measure to filter candidate phrases. 3.5 Baselines We consider the following two baselines for comparison in our experiments. A Tf-Idf Method: As we have pointed out, Hasan and Ng [13] found that a Tf-Idf-based method is a robust keyphrase extraction method when evaluated on four benchmark datasets. We therefore use their Tf-Idf method as our first baseline.
 A Language Model Based Method: Tomokiyo and Hurst [19] proposed to use lan-gauge models to measure both phraseness and informativeness. The details can be found in Section 3.1. In their method, each phrase is scored by summing up its phraseness and informativeness scores, and then phrases are ranked by their corresponding scores. We use this as our second baseline. 4.1 Data Set To evaluate different keyphrase extraction methods for our newly defined task, we need to select a document collection D and a set of queries Q . We decide to use Wikipedia articles for D because of the wide coverage of topics in Wikipedia and its rich textual data. We use a version of Wikipedia collection from the ClueWeb09 data set 2 . The data contains 5,945,485 documents, 8,881,880 uni que terms and 7,700,294,918 total number of terms. It is indexed by Lemur/Indri, an open-source information retrieval toolkit. We perform Porter stemming and stop word removal during indexing.

We u s e 40 queries for our evaluation. These queries are the top-ranked pages from two Wikipedia categories based on the numbers of views. The two categories are  X  X ompanies X  and  X  X ctors and film maker s. X  For each query, we retrieve the top-100 documents using KL-divergence retrieval model and then extract all the paragraphs containing the query to form D q . Note that for each query we specifically remove its main Wikipedia article from the set of relevant documents when constructing D q .This step is important as later we will use the query X  X  corresponding Wikipedia article for evaluation purpose. 4.2 Ground Truth For each query, a method will return a ranked list of keyphrases. To measure its per-formance we need to know which phrases are i ndeed closely related to the query. We consider two ways to obtain the ground truth, one through human annotation and the other through Wikipedia.

First, we created our own manually annot ated ground truth. For each query, we got the top-20 keyphrases from each method. We then randomly mixed these keyphrases and asked two judges to perform annotation. Specifically, the judges were asked to score each keyphrase with 0, 1 or 2, where 0 means the keyphrase is not a meaningful phrase or irrelevant to the query, 1 indicates a meaningful but partially relevant phrase, and 2 indicates a meaningful and relevant phrase. We use the Cohen X  X  kappa coefficient  X  to measure the agreement between judges. We found that  X  ranged from 0.2 to 0.7, which shows fair to good agreement. We used the average scores between the two human judges to form the final ground truth.
 We also consider another kind of ground truth automatically obtained from Wikipedia. Specifically, given a query, we obtain its ma in Wikipedia page. We extract those phrases on this page that are hyperlinks. Because Wiki pedia articles are collectively edited by many online users, they represent the general public consensus, and therefore the hy-perlinked phrases are presumably highly related to the query. Note that not all these hyperlinked phrases are named entities. 4.3 Evaluation Metrics With the human annotated ground truth, we adopt the normalized keyphrase quality measure ( n KQM)usedbyZhaoetal.[24]. n KQM is defined in a way similar to the commonly used n DCG measure as follows: where Q is the set of queries, M q,j is the j -th keyphrase generated by method M for ranking score of the top K keyphrases of query q .

For the noisy ground truth from Wikipedia , because the relevance score is either 0 or 1, we just compute precision at K for each ranked list of keyphrases for the query. We report P @5 , P @10 and P @20 in this paper. Although we could also compute recall values, we do not report them here because we assume that similar to Web search, in our task a user is also usually more interested in the correctness of the top-ranked items rather than the completeness of t he retrieved relevant items. 4.4 Methods for Comparison We have mentioned that we consider two baseline methods. We refer to the Tf-Idf base-line as BL-TI and the language model-based baseline as BL-LM.

As for our own method, we can choose one of the three phraseness criteria to se-lect candidate keyphrases. We refer to the language model-based phraseness criterion as LM, the noun phrase-based criterion as NP and the named entity-based criterion as NE. Each of these phraseness criteria can be coupled with an informativeness score to generate the final ranked keyphrases. We refer to the phrase-level Tf-Idf score for in-formativeness as P, the sum of the word-level Tf-Idf score as W-S, the average of the word-level Tf-Idf score as W-A, and the language model-based informativeness score as LM. The combinations of these shortha nds refer to different configurations of our method. For example, NE-W-A refers to the configuration where we consider candi-date keyphrases that are named entities and we rank them using the average word-level Tf-Idf scores. 4.5 Experiment Results The Effect of Filtering with Phraseness We first examine whether bringing in a phraseness-based filtering stage in our method can improve the performance. BL-TI is the only method that does not have a phraseness component, and it measures informativeness by the sum of the word-level Tf-Idf scores of a candidate keyphrase. The LM-W-S, NP-W-S and NE-W-S configurations of our method can therefore be regarded as augmenting BL-TI with phraseness-based filtering. We therefore first compare these methods. We report the results based on two ways of evaluation, where automatic evaluation uses Wikipedia hyperlinks as ground truth and manual evaluation is based on human judgment.

Table 2 shows the performance measures of these different methods. In automatic evaluation, both NP-W-S and NE-W-S improve the baseline BL-TI, while LM-W-S does not outperform the baseline BL-TI. The manual evaluation results in Table 2 are similar to automatic evaluation. Both NE-W-S and NP-W-S outperform the baseline BL-TI, while LM-W-S still has lower performance compared to the baseline. NE-W-S significantly outperforms other competing algorithms for all the metrics except P @5 . The results show that both noun phrase and named entity-based phraseness measures are helpful for our task.
 Comparison of Informativeness-Based Ranking Next, we would like to compare the effect of different informativeness scores in rank-ing the candidate keyphrases. Because we ha ve already found the named entity-based phraseness criterion is the best among the three we consider for our task, here we only compare configurations of our method that use named entities as candidate keyphrases. Table 3 shows the results of these methods. In both automatic evaluation and manual evaluation, NE-LM achieves the best perfo rmance, and NE-P X  X  pe rformance is better than NE-W-S and NE-W-A. This shows that for informativeness the language model-based method and the phrase-level Tf-Idf scores outperform other methods. Overall, NE-LM significantly outperforms other competing algorithms on all the metrics except P @5 .
 Comparison with Baselines Finally, we compare the best configuration of our method with the two baselines. We also show two additional variations of the BL-LM baseline: BL-LM-P uses only the language model-based phraseness scores to rank keyphrases, while BL-LM-I uses only the language model-based informativeness scores to rank keyphrases.

Table 4 shows the comparison, where our method significantly outperforms the two baseline methods, which represent the state of the art. It shows that for our task, using named entities to extract candidate keyphras es and using language model-based infor-mativeness to rank keyphrases achieves the best results. The fact that BL-LM is better than BL-LM-P and BL-LM-I also shows that both phraseness and informativeness are important.
 Comparison on Queries with Different Agreement Scores Recall that for manual evaluation, we have two judges to evaluate the resulting keyphrases. We show the histogram of the 40 queries X  agreement scores based on Cohen X  X  Kappa coefficient in Figure 1(a). The figure shows these queries range from fair to good agreement. Figures 1(b) and 1(c) show the results of Precision@10 and nkQM@10 on queries with different agreemen t scores. The results show that the NE-LM configuration of our method consisten tly outperforms the baselines on queries with different agreement scores. 4.6 Sample Output To qualitatively compare the results of our method and the two baseline methods, we show the top-10 keyphrases discovered by NE-LM and the two baseline methods for 3 queries in Table 5. Overall speaking, NE-LM could find more meaningful and relevant keyphrases than the baseline methods. For example, for the query  X  X oogle, X  BL-LM would find irrelevant phrases like  X  X eep web X  and  X  X dvanced search web form, X  and BL-TI would find meaningless phrases like  X  X oogle traffic. X  In this paper we studied how to extract a list of keyphrases given a query. Our task was motivated by the observation that for many informational queries in Web search a sum-mary article such as a Wikipedia entry is preferred by online users, and these summary articles usually contain a set of phrases highl y related to the query. To address this task of finding related keyphrases, we used unsupervised keyphrase extraction. Inspired by an existing keyphrase extraction method, we proposed a general method that first uses phraseness to select meaningful candidate keyphrases and then ranks them by infor-mativeness. We proposed to measure phraseness using language models, noun phrase boundaries or named entity boundaries, and we proposed to measure informativeness using Tf-Idf scores or language models. We evaluated these different methods on a Wikipedia corpus from ClueWeb09 using 40 queries that represent popular searches on Wikipedia. We found that for our task it is the best to use named entities as can-didate keyphrases and the language model-based informativeness scores give the best keyphrase ranking. This method also clearly outperforms two baseline methods that represent the state-of-the-art keyphrase extraction techniques.

Our findings are interesting as they suggest that although keyphrase extraction tech-niques have been evaluated on a number of benchmark data sets such as scientific liter-ature, when they are applied to novel tasks, n ew comparison and evaluation needs to be done. Our experiments confirm that both phraseness and informativeness are important criteria to consider.

In the future we plan to look into the problem of characterizing the relations between the related keyphrases and the query. One option is to find support sentences to explain their relations, similar to the work by Blanco and Zaragoza [4]. With such support sentences we can construct more user-friendly summaries for search results.
