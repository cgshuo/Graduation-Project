 For real-life text classification applications, it is usually very expensive to label text documents manually. In order to reduce the amount of labeled documents for training, semi-supervised learning algorithms, which utilize both labeled and unlabeled data, have been used for text classification [1,2,3]. Another approach is to develop classification algorithm based on a set of labeled positive exam-ples and a large amount of unlabeled docu ments [4,5,6,7,8,9,10,11]. Recently, some researchers have proposed labelin g keywords to build a classifier instead of labeling documents manually [12,13].

However, these approaches require either a set of labeled documents or a set of labeled keywords. In this paper, we consider the problem of training a text classifier from a keyword and unlabeled documents. The proposed approach consists of 4 steps. Firstly, we expand the query from a single keyword to a set of query terms, and retrieve an initial s et of documents from the set of unlabeled documents, U . Generally speaking, the retrieved documents could include both relevant and irrelevant documents. Then, an association rule based algorithm is used to discriminate the relevant documents from the irrelevant ones. We consider these relevant documents as positive documents P .Thirdly,weenlarge the positive documents P from U  X  P . And finally, an state-of-art PU [11] learning technique is employed to learn from P and U  X  P .

In order to measure the performance of the proposed approach, we made experiments on the 20Newsgroup dataset, with promising experiment result. The experiment result shows that our proposed approach could help to build good classifiers, and be more applicable to real-life text classification applications.
The rest of the paper is organized as fo llows. Section 2 reviews the related work. Section 3 presents our proposed a pproach. Section 4 gives our experiment results; and finally, section 5 concludes this paper and gives our further work. Due to the problem of manual labeling, with a small set of labeled documents, co-training method is proposed to build text classifiers in [1]. Nigam et al .used an algorithm for learning from labeled and unlabeled documents based on the combination of Expectation Maximization (EM) and Naive Bayes algorithm [3].
For learning from large set of positiv e and unlabeled examples, a number of practical PU learning algorithms were proposed, e.g. S-EM [4], ROC-SVM [5], PEBL [6]. Furthermore, there is a few successful algorithms proposed by the research community to cope with the situation when there is only a small set of positive documents [10,11]. The PNLH-SVM [10] and LPLP [11] methods focus on enlarging positive examples.

The work in [12,13,14] are more related to our research. With unlabeled train-ing data, the approaches have been proposed to build a classifier by labeling keywords instead of labeling training examples [12,13]. The approach in [12] use many keywords to label raw training examples. However, it is too difficult for a user to provide sufficient keywords for accurate learning. So, the approach in [13] utilizes clustering and feature sel ection technique to label some words for each class. And then, the t raining examples are extracted based on the labeled words and NB-EM algorithm is used to build classifiers. In [14], an initial set of documents are retrieved from information resource, from which, DocMine [15] algorithm is used to mine a set of positive documents. And then, the reliable negative examples N are mined to build a SVM classifier.

However, the approaches in [12,13] require labeling keywords manually, which is difficult to provide sufficient keywords. The method in [14] needs more than one search engine. Only a small part of positive documents could be extracted by the search engines. Hence, the posit ive examples mined by the method in [14] would be incompetent for recognizing all the hidden positives in unlabeled documents U as well as those in the test sets.

The approach proposed in this paper builds a classifier based on only a topic keyword w and a large of unlabeled documents U . The proposed approach re-leases users from heavy burden of labelin g documents or labeling keywords man-ually, which makes it more applicable to real-life applications. The approach proposed in this paper consists of four steps.

Step 1: retrieving documents D with respect to query w from the unlabeled documents U ;
Step 2: applying association rule based algorithm to mine a set of positive documents P 1 ,from D ; Step 3: enlarging the positive examples P 1 from U  X  P 1 with the help of P 1 ;
Step 4: employing the state-of-art PU learning techniques to build a text classifier. 3.1 Retrieving Documents D by a Keyword w we use the method proposed in [16] to retrieve documents. The method consists of two steps: (1) term expansion; (2) document retrieval.
 Term Expansion [16]: Firstly, the query w is augmented by synonym terms (the most common sense is taken). Then, the query is augmented by its nearest hypernyms which are semantically simila r to terms already in the query. The set of expanded query term Q could be formulated as: Document Retrieval: According to the default Lucene 1 similarity, the similar-ity between the expanded query Q and a document d couldbeformulatedas: frequency : the number of observation of term t in document d numDocs : total number of documents docF req : the total number of documents in which term t could be observed The set of retrieved documents D could be defined as: 3.2 Mining Positive Documents P 1 from D In last sub-section, a set of documents D could be retrieved from U . Usually, more documents could be retrieved by the expanded query Q than by the key-word w . However many documents in D might be irrelevant to our query. In this paper, it was assumed that the relevant ones are more than irrelevant ones. We employ the DocMine [15] algorithm to label positive examples P 1 from D .
The retrieved documents D are sorted into a descending sequence D seq ac-cording to the similarity between the expanded query Q and the document d . And then, D seq is divided into two equal parts, D 1 and D 2 . D 1 is made up of the top 50% documents in D seq and D 2 is the rest of documents in D seq . We select h % documents without replacement from D 2 randomly, and use these selected documents together with all the documents in D 1 to form one bucket of docu-ments. In this way, we create l buckets of documents. Then, DocMine algorithm is employed to mine the set of positive documents, P 1 . 3.3 Enlarging the Set of Positive Examples P 1 However, the set P 1 mined by DocMine is only a small part of U with a low recal l index. The small positive set P 1 could not adequately represent the whole positive category.

In order to improve the performance of th e text classifier, we enlarge the set of positive examples based on the method proposed in [10]. This method includes two steps: (1) extracting reliable negative examples; (2) enlarging the sets of positive examples.
 Extracting reliable negative examples: In this paper, we employ the algo-rithm ExtractReliableNegative( P 1 , U  X  P 1 ) [10] to extract the negative examples. And then, the negative examples are sorted descendingly according to their sim-ilarity. The set of reliable negative examples, N , is made up of the last 40% of the negative examples in the sequence.
 Enlarging the positive examples [10]: Firstly, we partition N , the set of reli-able negative examples, into k clusters, ( N 1 ,N 2 ,...,N k ), k = N/P 1 .Bydoing so, each partition focuses on a smaller set of more related features. We measure the similarity of the document d  X  U  X  P 1  X  N with the centroid of P 1 , cid P 1 is used to get the centroid of the clusters . Here, the cosine coefficient is used for measuring the similarity between a document d and a centroid cid : According to this similarity, negative examples are extracted from U  X  P 1  X  N repeatedly until no more documents in U  X  P 1  X  N could be extracted. Then, we start to extract positive documents from U  X  P 1  X  N . 3.4 Building Text Classifiers In the previous section, we have mined the positive examples P from the unla-beled documents U by the keyword w .Next, PU learning algorithms are em-ployed to classify the documents in te sting data set. In this paper, ROC-SVM [17], SPY-SVM [17] and NB-SVM [17] are used to build a text classifier. In these three algorithms, ROC, SPY and NB algorithms are used respectively to identify a set of reliable negative documents from the unlabeled documents. And then, SVM learning algorithm is used iteratively to build the final classifier. In this section, we evaluate our proposed approach and compare it with the method based on the positive and unlabeled documents. The LPU 2 system is publically available online and contains ROC-SVM [17], SPY-SVM [17] and NB-SVM [17]. 4.1 DataSet and Experiment Settings We made our experiments on 20Newsgroup 3 dataset. There are 20 categories in the 20Newsgroups dataset, with l0 of them being selected randomly in our experiment. Each category has approximately 1000 articles. For each category, 70% of the documents are used as unlabeled data for training, and the remaining 30% of the documents as the testing documents.

The preprocessing includes stop word removing and stemming. For the doc-ument retrieval, the JWNL 4 is used to expand the query keyword w and the Lucene is used to retrieve the documents. For Mining positive documents from retrieved documents, according to [15], we set l = 5. For enlarging positive doc-uments, according to [10], for feature selection, we select 3000 features. The classical k-means clustering algorithm[18] is used to cluster the reliable neg-ative examples N . For document classification, the three learning algorithms (ROC-SVM, SPY-SVM, NB-SVM) of the LPU system are employed to build text classifiers.

We randomly select 300 positive examples from the training set for each cat-egory and compare our approach with PU classifier, which is trained from 300 positive documents and unlabeled documents. 4.2 Experimental Results In this paper, Recall , P recision and F 1 are used to measure the performance of our text classifier, as they are widely used by the research community of text classification. Table 1, table 2 and table 3 give the classification performance of ROC-SVM, SPY-SVM and NB-SVM, respect ively. In each table, column 1 lists the category name, column 2 lists the corresponding query keyword, column 3, 4 show the performance of our approach based on keyword and PU classifier based on positive examples.

It is obviously from table 1, table 2, and table 3 that the approach proposed in this paper outperforms the PU classifiers, which are trained on 300 positive examples and unlabled examples. Based on a keyword and unlabeled documents, our approach could help to build a good text classifier. We believe that our proposed approach requires less user effort to label a sufficient set of documents, even in case of a small positive examples.
Here, we analyze the number of true positive examples in each step of our pro-posed approach, which is shown in Table 4. In table 4, column 1 lists the category name, column 2,3 and 4 gives the information of true positives in step 1, 2 and 3, respectively. Here | p | shows the number of true positive documents; P recision shows the precision index in the corresponding step positive documents; | D | shows the number of documents retrieved in Step 1; | P 1 | shows the number of documents minedinStep2; | P | shows the number of documents mined after Step 3.
From table 4, it is obviously that DocMine could improve the relevance of the retrieved documents, and the method of enlarging the positive examples could mine more relevant documents from the unlabeled documents. From the category sci.electronics , it could be concluded the performance of a classifier is relevant to the number of the positive examples mined, as a small set of positive examples could not represent all the hidden positive examples in unlabeled documents. Meanwhile, the more irrelevant documents mined in step 3, the worse a classifier performs, eg, sci.crypt , comp.graphics . The more relevant documents and the less irrelevant documents mined in these three steps, the better a classifier performs, eg, soc.religon.christian . In many real-world text classification applications, it is often expensive to obtain enough training examples for building good text classifiers. In this paper, a new approach has been proposed to learn from a keyword w and the unlabeled data U . To cope with lacking of training examples, firstly, term expansion and association rule based algorithm are used to label some positive examples P 1 from U . Then, with the help of P 1 , more positive examples P 2 are extracted from U  X  P 1 . Finally, the state-of-art PU learning algorithm is applied to build a classifier based on P good classifier from a keyword and unlabeled documents. The proposed approach could help to reduce the process of labeling documents manually, and could be more applicable to real-life text classification applications.

In our future work, we plan to study how to identify more positive examples from unlabeled documents U with the help of only a small positive examples P , so as to further improve the performance of our classifier.

