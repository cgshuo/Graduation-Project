 been extensively studied in many literature [11, 12, 13]. In classification problems, it is the process of selecting a subset of the terms occurring in the training set and using selection can alleviate the effect of the curse of dimensionality, improve the generali-zation performance, accelerate the learning process, and enhance the interpretive per-formance of the model, finally improve the performance of prediction models. 
Most existing studies of feature selection are restricted to batch learning, which as-sumes the feature selection task is conducted in an off-line learning fashion and all the features of training instances are given a pr iori. But in real-world applications, train-ing examples often arrive in a sequential manner, and the full information of training data is sometimes expensive to collect. 
An online classifiers should involve only a small and fixed number of features for classification. It is particularly important and necessary when a real-world application has to deal with sequential training data of high dimensionality. There are two differ-learner can access all the features of training instances, and OFS when the learner is only allowed to access a fixed small number of features for each training instance to the second task, it is a more challenging scenario than the first task. 
In OFS process, when a training instance arrive, a fixed small number of features should be recovered rather than ignored. 
In this paper, we utilize Passive-Aggressive Algorithm to make feature selection inputs. We retain the features those have been ignored to improve accuracy and re-duce the standard deviation. Finally we evaluate the performance of the proposed algorithms. 
The main contribution of this paper is summarized as follows: (1) we propose an algorithm named Online Feature Selection based on Passive-Aggressive Algorithm with Retaining Features (RFOFS) to achieve effective online feature selection with full/partial inputs; (2) we validate the effectiveness of RFOFS by conducting an ex-tensive set of experiments. 
The rest of this paper is organized as follows. Section 2 reviews related work. Sec-tion 3 presents the problem and the proposed algorithms. Section 4 discusses our em-pirical studies and Section 5 concludes this work. Recently, online machine learning has become a hot research topic due to the effi-ciency of the algorithms and effectiveness in real-world applications. A classical online learning method is the well-known Perceptron algorithm [3, 4], which updates the model by adding a new example with some constant weight into the current set of support vectors when the incoming example is misclassified. Recently, a lot of new online learning algorithms have been proposed, in which many of them usually follow the criterion of maximum margin learning principle [5, 1, 7], for example, the Pas-sive-Aggressive algorithm [1]. It updates a classifier that is near to the previous func-cently proposed confidence weighted online learning algorithms that exploit the se-cond order information [6, 9, 8]. Most studies of online learning require the access to all the features of training instances. When the number of active features is small and fixed, the online learning problem becomes more challenging [10]. Online learning algorithms are very promising in real-world applications, especially for training large-scale datasets and data being incrementally added. 11]. The existing FS algorithms generally can be grouped into three categories: super-vised, unsupervised, and semi-supervised FS. Supervised FS [15~17] selects features according to labeled training data. And when there is no label information available, unsupervised feature selection [18~20] attempts to select the important features which preserve the original data similarity or manifold structures. Semi-supervised feature selection methods [21~23], as its name says, exploit both labeled and unlabeled data information. The existing supervised FS methods can be further divided into three groups, depending on how they combine the feature selection search with the con-struction of the classification model: Filter methods, Wrapper methods, and Embed-ded methods approaches. Filter methods [10, 15, 13] choose important features by measuring the correlation between individual features and output class labels, without involving any learning algorithm; wrapper methods [16] rely on a predetermined learning algorithm to decide a subset of important features. Although wrapper meth-ods generally tend to outperform filter methods, they are usually more computational-ly expensive than the filter methods. Embedded methods [17] aim to integrate the feature selection process into the model traini ng process. They are usually faster than the wrapper methods and able to provide suitable feature subset for the learning algo-rithm. For unsupervised feature selection, some representative works include Norm Regularized Discriminative Feature Se lection [20]. Feature selection has found many applications [14], including bioinformatics, text analysis and image annotation. Our OFS technique generally belongs to supervised FS. 
We note that it is important to distinguish Online Feature Selection addressed in this work from the previous studies of online streaming feature selection [24]. In significantly from our online learning setting where training instances arrive sequen-tially, a more natural scenario in real-world applications. 3.1 Problem Setting As defined in paper [2], we also consider the problem of online feature selection for binary classification in this paper. We denote the instance presented to the algorithm on round t by x  X  , and each x  X   X  R  X  is a vector of d dimension. We assume that x  X  features from the d features (we assume d is a large number) for efficient linear classi-fication. In each round t, the learner presents a classifier w  X   X  X   X  which will be cal-the classifier w  X  to have at most B non-zero elements, i.e. where B&gt;0 is a predefined constant, and consequently at most B features of x  X  will be used for classification. We refer to this problem as online feature selection. Our goal is to design an effective and stable strategy for online feature selection that is able to make a small number of mistakes and a small standard deviation. 3.2 Baseline Methods A straightforward approach to online feature selection is to modify the Perceptron algorithm by applying truncation. Specifically, in the t-th trial, truncating the classifi-zero. This truncated classifier is then used to classify the received instance. When the (  X  the B largest elements for prediction, it does not guarantee that the numerical values many classification mistakes. 
PA Algorithm avoid this problem by updating the classifier not only when the in-where algorithm attains a margin less than 1, the classifier will be updated and truncated. 
Jialei Wang et.al proposed another efficient algorithm [2]. It also avoid this prob-lem by exploring the sparsity property of L1 norm. And they gives the mistake bound of the Algorithm and proof. Unfortunately, several problems exist in these approaches. After the online feature selection process, some features will be ignored. However, when the new training instances arrive, the features which have been ignored may be selected, then only the new instance was considered for these features . This bias can lead to extreme errors. To see this problem, consider the case where the w  X  =(1,0,0,0,1), and the input pattern  X  =(0,1,1,0,0). It causes a classification mistake, and then updates the w by x. But we can see the 2 to 4 elements in w  X  X  X  are only depending on  X   X  . This is irrational, be-cause online learning classifier w  X  should associate with all the input patterns before . 3.3 RFOFS with Full Inputs stance (i.e. x  X  ,...,x  X  ). We first present a state-of-art OFS algorithm and OFS with PA Algorithm as our baseline. 
To solve the problem, we hold two classifiers. The one truncated is used for feature selection and making prediction. The other one named as  X  X  X _ X   X  is used for updating when misclassified, and it saved the features that have been ignored. 
Based on these ideas, we present a new approach for OFS (Online Feature Selec-tion based on Passive-Aggressive Algorithm with Retaining Features, RFOFS) as shown in Algorithm 4. The online learner maintains a linear classifier  X   X  that has at classifiers and preserving ignored features. 
When a training instance  X  X   X   X ,  X   X  reached, we make prediction by a linear by online gradient descent. Then the classifier is projected to a L2 ball to ensure that the norm of the classifier is bounded. At last the classifier is truncated to generate the classifier which is used for feature selection. 3.4 RFOFS with Partial Inputs In the above discussion, although the classifier w only consists of B non-zero ele-problem of online feature selection by requiring no more than B attributes of x  X  when soliciting input patterns. We note that this may be important for a number of applica-tions when the attributes of objects are expensive to acquire. mation by employing a classical technique for making tradeoff between exploration by online gradient descent. Then the classifier is projected to a L2 ball to ensure that the norm of the classifier is bounded. At last the classifier is truncated to generate the classifier which is used for feature selection. In this section, we conduct an extensive set of experiments to evaluate the perfor-mance of the proposed online feature selection algorithms. We will evaluate the online predictive performance of the two OFS tasks on several benchmark datasets from UCI machine learning repository. 4.1 Datasets We test the proposed algorithms on a number of publicly available benchmarking datasets. All of the datasets can be downloaded either from LIBSVM website 1 or UCI machine learning repository 2 . Besides the UCI data sets, we also adopt two high-dimensional real text classi fi cation datasets based on the bag-of-words representation: the  X  X omp X  versus  X  X ci X  and  X  X ec X  versus  X  X ci X  to form two binary classi fi cation tasks. 
Table 1 shows the statistics of the datasets used in our following experiments. Each sample is a d+1 dimension vector (  X   X   X ,  X   X ,]1[  X   X ,...,]2[  X   X  X  X [ . 4.2 Experimental Setup and Baseline Algorithms We compare our proposed OFS algorithm against the following three baselines:  X  The modified perceptron by the simple truncation step, denoted as  X  X E X  for short;[4]  X  A randomized feature selection algorithm, which randomly selects a fixed number of active features in an online learning task, denoted as  X  X AND X  for short;  X  OFS via PA Algorithm shown in Algorithm 1, denoted as  X  X PA X  for short; [1]  X  OFS via Sparse Projection proposed in paper [2] shown in Algorithm 3, denoted as  X  X FS X  for short.[2] To make a fair comparison, all algorithms adopt the same experimental settings. All the experiments were run over 20 times, each time with a random permutation of a dataset. OFS with full inputs in experiment 1 was evaluated against the RAND, OPA and OFS. OFS with partial inputs in experiment 2 was evaluated against the RAND, PE and OFS. Our experiment was run in matlab. 4.3 Experiment 1: RFOFS with Full Inputs Table 2 summarizes the online predictive performance of the compared algorithms with a fixed fraction of selected features on the datasets. Algorithm magic04 german splice RAND OPA OFS 
RFOFS Algorithm spambase a8a svmguide RAND OPA OFS 
RFOFS Algorithm rcv1  X  X ec X  X s X  X ci X   X  X omp X  X s X  X ci X  RAND OPA OFS 
RFOFS 
Several observations can be drawn from the results. First of all, we found that among all the compared algorithms, the RAND algorithm has the highest mistake rate for all the cases, and the simple OPA algorithm can outperform the RAND algorithm considerably, which shows that it is important to learn the active features in an OFS the simple OPA algorithm, they both achieved a small mistake rate, but their standard deviation is far more than the RAND algorithms. This indicates the two algorithm is algorithm X  X  standard deviation is also the smallest in these four algorithms. This shows that the proposed algorithm is able to considerably improve the performance of the feature selection approach. 
To further examine the online predictive performance, Figure 1(a~i) shows how the mistake rates is varied over iterations accord the entire OFS process on the chosen datasets. Similar to the previous observations, we can see that our OFS algorithm also found that the more the training instances received, the more significant the gain achieved by our OFS algorithm over the other baselines. This is because that the more the training instances received, the more features were ignored in the OPA and OFS algorithm, then the more mistakes occurred in those algorithms. This again verifies the efficacy of our OFS algorithm. 
The bar graph in Figure 2 shows the accuracy of online feature selection algo-rithms with varied fractions of selected features (take  X  X erman X  and  X  X pambase X  da-taset for example). We can see that our OFS algorithm outperform the other three baselines. When the fraction is close to 1, the OPA and Jialei Wang X  X  algorithm grad-time, and all these algorithms use the same linear classifiers for classification. 4.4 Experiment 2: RFOFS with Partial Inputs Table 3 summarizes the online prediction performance of the compared algorithms on four datasets. Algorithm magic04 german splice RAND PE OFS 
RFOFS Algorithm spambase a8a svmguide RAND PE OFS 
RFOFS Algorithm rcv1  X  X ec X  X s X  X ci X   X  X omp X  X s X  X ci X  RAND PE OFS 
RFOFS 
Several observations can be drawn from the results. First, we found that the RAND important to learn the active features for the inputs and the weight vector. Second, we found that the proposed RFOFS algorithms achieved the smallest mistake rates. This shows that the proposed RFOFS technique is effective for learning the most informa-tive features under the partial input situation. 
To further examine the online predictive performance, Figure 4(a~f) shows how the mistake rates is varied over iterations accord the entire OFS process on the chosen datasets. Similar to the previous observations, we can see that the proposed RFOFS algorithm consistently surpassed the other three algorithms for all the situations. Be-sides, we also found that the more the training instances received, the more significant gains achieved by the proposed RFOFS algorithm over the other baselines. This again verifies the efficacy of the proposed RFOFS algorithm. In this paper, we utilized Passive-Aggressive Algorithm and OFS Algorithm to make ignored to improve accuracy and reduce the standard deviation. Then we evaluate the performance of the proposed algorithms for online feature selection on several public datasets. From the experiment results, we can see our algorithm is significantly more efficient and stable than the other algorithms. retaining features more intelligent, and apply our algorithm to some real-world appli-classification. Acknowledgement. This research is supported by the 863 project of China (2013AA013300), National Natural Science Foundation of China (Grant No. 61375054 and 61402045), Tsinghua University Initiative Scientific Research Pro-gram  X  Grant No.20131089256  X  , and Cross fund of Graduate School at Shenzhen, Tsinghua University (Grant No. JC20140001). 
