 H.2.8 [ Database Application ]: Data Mining; I.5.4 [ Pattern Recognition ]: Text Processing Algorithms, Performance, Experimentation
Information extraction from large document collections has re-ceived significant amount of attention recently. A variety of extrac-tion tasks have been considered: identifying and extracting named entities from documents [4], detection of topics/themes [20], ex-traction of customer preferences [5], etc. The extracted information is used in a variety of ways, e.g., to provide business analytics or to answer more sophisticated queri es [8]. Entity extraction technol-ogy has matured and commercia l technology (from Verity, Inxight, etc.) is available for identifying various types of entities such as people, organizations, locations. In addition, a large number of so-lutions to this task have been proposed in research [3, 11].
One particular area of recent interest has been the automatic ex-traction of unary relations (such as is-a-painter , is-a-researcher , or is-a-camera ) and binary relations (such as is-a-painter-of , is-author-of ) between named entities (e.g., [1, 6, 15, 23]). Here, we differentiate between two approaches:  X  X pen X  relation extrac-tion [6] where arbitrary relations are extracted and targeted relation extraction where only a small number of known target relations (e.g., actors, painters, electronic products) are extracted. In this paper, we focus on the extraction of targeted relations. We view the targeted relation extraction as that of categorizing named entities, into a set of target classes such as painters , re-searchers , etc. Henceforth, we use the terms unary relation ex-traction and entity categorization interchangeably.
Most approaches for unary relation extraction from large docu-ment collections base their extraction decisions on the  X  X ontext X  of an entity  X  a window of words around the entity occu rrence  X  within a single document, using rules or machine-learning models to map a context to an extracted target relation. Once an extraction deci-sion has been made, the context information is discarded [3]. While this approach is fast, as only a single traversal through the docu-ment corpus is required, accuracy of the extraction is limited by the fact that only very limited information is taken into account before extraction. To illustrate why this is the case, consider the example scenario where we simply want to classify person names into two classes: is-a-researcher and is-non-researcher . Approaches that consider one context at a time are limited to triggering extraction rules on contexts that allow us to identify a person as a researcher with high confidence from a single context, e.g., Because the context is lost once an extraction decision has been made, any further decision regarding the reliability of a given ex-traction then has to be based on the number of times a particular extraction has been made , and cannot take into account which con-textswereusedtogiverisetoanextraction.

In contrast, first aggregating all contexts for a specific entity al-lows us to think of each individual context as generating one or more features of this entity, allowing us to subsequently combine several less predictive features to arrive at a high-confidence extrac-tion. For example, we can use the combination of features such as  X  X Entity] presents results X  and  X  X Entity] publishes X  , each of which is not sufficiently predictive by itself to allow extraction of the tu-ple ( Entity , is-a-researcher ) (after all, companies present results and newspapers publish), but which  X  when combined  X  make it very likely that the entity in question is a researcher.

In this paper, we build upon the above insight. We propose that the contexts within which an entity occurs across multiple docu-ments be  X  X ggregated X  and then used to categorize an entity. In-formally, the aggregated context is the union of all contexts within which an entity occurs. We identify features from this aggregated context, which an underlying machine-learning model then uses to categorize the entity. As we illustrated above, aggregation across multiple documents allows us to also leverage several low confi-dence features instead of using only high confidence features.
In many scenarios lists of entities related to the target relation are readily available. We can exploit these lists to significantly improve the accuracy of relation extraction by using features that reflect the existence of these related entities in the context of an entity we seek to classify. For example, consider the task of categorizing people as athletes and an example document  X  Yao Ming is drafted by the National Basketball Association.  X  The input entity recognizer may recognize  X  Ya o M i n g  X  as a person name. If we have the list of sports organizations then recognizing the occurrence of one of its mem-bers  X  National Basketball Association  X  is a very useful feature for classifying Yao Ming as an athlete. We refer to such features that reflect the co-occurrence between entites we want to classify and lists of  X  X nown X  entities as list-membership features. The required lists can often be obtained from structured data sources (such as Wikipedia or IMDB) or can be derived from the training data itself.
The main reason why these features make an impact is that the lists allow us to aggregate many instances of a category list into a single feature; while many individual entries in these known lists occur too rarely in documents to become important features in a classifier individually, the resulting list-membership features are sufficiently common to do so. For example, a feature which says that authors are likely to be mentioned along with a book is more likely to be applicable than several different features corresponding to instances of specific books.
The use of multi-document contexts and list-membership fea-tures can benefit extraction accuracy significantly, but it also intro-duces new challenges. Generally, the document corpus we extract from is significantly larger than the available main memory. This means in turn that during extraction we most likely have to con-sider very large numbers of combinations of entities and their con-texts, all of which we cannot store in main memory either. Further, the relevant lists of  X  X nown X  entities themselves, which we need to identify list-membership features, may also require more space than is available in main memory. This issue is acerbated when extracting many relationships in parallel.

Straight-forward implementations of unary relation extraction thus require either (i) multiple passes over the document corpus, invoking the potentially expensive entity extraction multiple times for each document, or (ii) need to materialize significant amounts of entity-context on disk. Both of these processing strategies may be very expensive computationally. Our approach avoids these costs by leveraging knowledge of the underlying feature semantics and statistical properties of large document corpora.

The remainder of the paper is organized as follows: In Section 2, we review related work. In Section 3, we describe the problem of unary relation extraction in detail. We then describe techniques for efficiently extracting features over the aggregated context (Sec-tions 4 and 5). Extensions are discussed in Section 6. We present an experimental evaluation in Section 7, and conclude in Section 8.
A large amount of recent work has fo cused on extracting entities and relations from large document collections (e.g., [1, 6, 15, 23]). While these techniques vary in the amount of processing they per-form for a given occurrence of an entity (e.g., they determine parts of speech of the words around the e ntity, label h ead nouns, and of-ten look up dictionaries), most of these techniques only analyze the context of an entity within an individual document when extracting a relation. Our approach is novel in that it exploits the contexts of entities that span multiple documents.

Recently, a large number of techniques have been proposed for speeding up the processing of large document collections in rela-tion extraction (see [1] for an overview), including (heuristic) fil-tering of documents [2], the use of very simple extraction patterns and specialized index structures [9]. These approaches are orthog-onal to ours and may be combined with it.

The problem of computing the list-membership and cross-document features also has strong similarity to the processing of join and aggregation queries in database systems, where such queries have been studied extensively (see [16] for an overview). Our problem scenario differs in that the extraction of features from documents itself is a central part of the problem statement, something that is currently not supported by relational DBMS.
Our techniques to scale the feature extraction leverage statisti-cal properties found in large corpora of natural language text. As shown recently in [17] for different but related tasks, using the un-derlying statistical properties of skewed or even heavy-tailed dis-tributions can result in much improved processing strategies when processing large document data sets.
In this section, we will study the problem of scalable extraction of unary relations from very large text corpora, when using both aggregated contexts as well as list-membership features for classi-fication. For this purpose, we will first define the necessary notation and subsequently set up the problem of unary relation extraction. Throughout the paper, we will use the following notation. Let D = { d 1 ,...,d m } be the set of input documents. Within these documents, we rec ognize entitie s using an entity recognizer g i.e., g E takes a document d  X  X  and returns all named entities of the desired type (say, persons, organizations, or locations) oc-curring in d .Let E = d  X  X  g E ( d ) denote the set of all entities, which is unknown when we begin processing the documents. Our approach is not tied to any specific entity recognition technique  X  we can use any approach capable of recognizing occurrences of named entities in a document based on the document and other do-main information (e.g., [19, 25]).
 Surface forms vs. canonical forms of entities: When referring to entities we need to differentiate between the entity itself and its various surface forms (i.e., the different ways to refer to this partic-ular entity in a document). For example,  X  Bill_Gates  X  X  William H. Gates  X  X nd X  William Gates III.  X  all refer to the same person. The problem of mapping entity surface forms to entities has been stud-ied in various domains (e.g., [24]); a detailed discussion of such techniques is beyond the scope of the paper. Henceforth, we as-sume that each entity identified is also converted to its canonical representation. In the above example, the representations  X  William H. Gates  X  X nd X  William Gates  X  would be converted to their nor-malized canonical form, say, Bill_Gates .
 Document context and aggregated context: In the context of en-tity categorization, we identify the features occurring in the  X  doc-ument context  X  of an entity. In general, the document context of an entity may involve many aspects of the document such as title, paragraph headers, and the word s around the entity occurrence. In this paper, we only consider a window of words around an entity X  X  occurrence. Suppose an entity e (with a surface form s e of length | s | words) occurs in a document d at position p .The size-K con-text of an entity e with respect to d and position p is the window w entity occurrence. The union of all size-K contexts of an entity e within a document d form the document context for e with respect to d . Suppose the entity Picasso occurs in a document containing one sentence:  X  X he painting by Picasso adorns the main hall of the Museum of Art in New Delhi. X  Then, the size-3 context of Picasso is:  X  X he painting by [Entity: Picasso] adorns the main. X  The size-K aggregated context for an entity e with respect to D is the union of all size-K document contexts of e in all d  X  X  in which e occurs. Classifiers and features: We denote the set of all classifiers we use for unary relation extraction as C = { c 1 ,...,c l } sifier c i is trained to extract one specific relation from the aggre-gate context of an entity. Specifically, each classifier c input a set of features Features ( c i ) , which are extracted from the appropriate aggregated context. We denote the set of all features F = c  X  X  Features ( c ) . We distinguish 2 types of features. Text n -gram features: One set of features we consider are occur-rences of word n -grams, which are identified during the training phase of the classifier to be highly correlated with the target cate-gory. For example, the phrase  X  painting by  X  may be very correlated with the painters category. All n -gram features we consider in this paper are binary, i.e., only the absence/presence of an n -gram in an aggregate context is indicated, but not its frequency; however, our techniques can be extended to non-binary features as well. List-Membership features: The second set of features we con-sider are the list-membership features discussed earlier. To iden-tify these, we use a set of lists L = { L 1 ,...,L f } , where each L is a list of known entities. Regarding list-membership features, we also primarily consider only binary features in this paper; how-ever, it is possible to extend our techniques to list-membership fea-tures that are sensitive to the number of occurrences. Note that the width ( K 2 ) of the context size around each entity for generat-ing list-membership features may be different than that ( K n -gram features. Since it is usually clear which aggregate context we refer to we usually drop the prefixes ( K 1 or K 2 ) while referring to aggregated contexts.
 We can now define the problem of unary relation extraction. Relation Extraction Problem Given a set D of documents, an en-tity recognizer g E ,aset C of classifiers trained on a set tures, a set of existing lists L , the task of unary relation extraction is to classify all e  X  X  using the classifiers contained in
We decompose the relation extraction problem into two compo-nents as shown in Figure 1. The first component, Entity-Feature materialization , generates the set of features, both n-gram and list membership features, grouped b y the corresponding entities across all documents. The second component, Classification , takes this set of features as an input and then applies the classifiers in large number of different classifiers has been proposed for catego-rization in research  X  in this paper, we do not take a position on the particular classification models used. Instead, we will focus on the extraction of features and their materialization, which is challeng-ing due to the sheer volume of data involved.

Conceptually, the task of list-membership feature computation is to compute the bipartite graph G E,L between entities and the lists L . G E,L contains a node for each entity e  X  X  and a node for each list L i  X  X  . G E,L contains an edge ( e, i ) whenever there exists a list-member l  X  L i that occurs in the K 2 -context of e in a document d  X  X  containing e . Figure 2 shows an example bipartite graph. We refer to edges in G E,L as entity-list edges.

Similarly, the task of n -gram feature materialization is to com-pute a bipartite graph G E,F between entities and n-gram text fea-tures (Figure 3). G E,F contains a node for each entity e node for each n-gram feature f  X  X  . G E,F contains an edge ( e, f ) whenever the feature f occurs in the aggregated context of e across all documents in D . We refer to these edges as entity-feature edges.
Figure 4 gives an overview of our architecture. It has four com-ponents: list-membership extraction , n-gram feature extraction , ag-gregation ,and classification . The list-membership extraction com-ponent materializes G E,L ; the n-gram feature extraction compo-nent materializes G E,F . The aggregation component groups all features by each entity. The classi fication component processes each group per entity applying each classifier in C over the set of features in the group. The output of this component consists of entities and their rec ognized categories.
 The main challenge for the materialization of G E,L and G is scalability: both D and L are very likely to exceed in size the main memory available during unary relation extraction, meaning we cannot retain one of them in memory while scanning the other. We will formalize the resulting challenges and describe how to ad-dress them in the following two sections.
In this section, we describe techniques for the computation of list-membership features, i.e., materialization of G E,L .Forthe purpose of list-membership processing, we model each document d  X  X  as a set of entity, list-member pairs: d  X  X  X L , with d containing a pair ( e, l ) if a list-member l is contained in the K context of e in document d . A straight-forward approach to im-plement this functionality would be to scan the documents while keeping L in main memory, extracting entities and list-membership features from each document. The combinations of entities and list-membership features would then be written to disk; after pro-cessing all of D , all such features would be grouped together for each entity. However, this approach has two main issues.
First, the set of lists L may be too large to fit into memory during the document processing. For example, lists of actors (useful for identifying categories such as directors , producers ,oreven movies ) or paper-titles (which  X  when grouped by area or conference  X  can be very useful for categorizing r esearchers into areas) are readily available from web sources and contain more than a million dis-tinct entries. Also, we want to apply multiple classifiers from parallel, each of which may require different lists of known entities, all of which compete for the limited main memory.

Second, this approach for materializing will produce many iden-tical (and thus redundant) edges in the graph G E,L . For example, a popular athlete such as  X  X ichael Jordan X  may be mentioned a large number of times along with many members (such as NBA) in a list of sports organizations.
While it is easy to establish that L may indeed contain a large number of related list members, we also need to ensure that the large number of list members actually makes a difference in classi-fication; otherwise, a simple strategy such as reducing |L| retaining the most important list members may suffice. To test if this is the case, we set up the following experiment. Using a known set of directors (as E ) and a list of actors (as L ) and 3.2 million documents from Wikipedia, we measured the number of entities e  X  X  that co-occur with at least one member of L . Note that all such entity/member pairs would result in an edge in G E,L compare this to the number of entities that co-occur when subsets of the most frequent members (in the corpus) of L are used. We consider subsets of sizes 1%, 2%, 5% and 10% of |L| here.
We observed that using a subset of L sharply reduces the number of entities for which we see (at least one) co-occurrence: in case of a list containing the 1% most frequent members, we  X  X iss X  about 44% of such entities, i.e., the missed entities do not have a list-membership feature indicating that their context in some document contains a known actor. The number of missed entities becomes 38% when we consider the 2% most frequent members, 30% when we consider 5% most frequent members, and 25% when we con-sider 10% most frequent members. Since  X  as we will show in Sec-tion 7.2.1  X  list-membership features have a significant impact on accuracy, this experiment illustrates the requirement for all mem-bers in L to be considered.
There are a number of execution strategies available to us when computing G E,L for lists L larger than the main memory available: Multi-Scan Approach: A straight-forward approach to address the issue of space when computing G E,L for large L wouldbetodivide L into k sublists (e.g., via hashing), each of which is sufficiently small to fit in the available memory and then to iterate k times over D , each time keeping a different sublist of L in memory. This strat-egy has two problems. (1) The document collection D itself is very large, meaning that we have to read in a significant amount of text multiple times. (2) This strategy also requires multiple invocations of the potentially expensive entity-extractor g E 1 ,aswellasallex-ecuting all the required preprocessing (such as tokenizing, parsing, etc.).

As a consequence, this multi-scan approach is typically too ex-pensive in practice and we will not consider this execution strategy. Scanning D once: A simple strategy that avoids multiple scans of D would be to scan D and write all out pairs of entities and their contexts to disk for further processing. However, this strategy is not efficient, due to (a) the large amount of data written and (b) the fact that the vast majority of contexts are not expected to contain an entity that is a member of a list in L , meaning that most of the written contexts are irrelevant.
 Our Approach: Our solution instead writes out a significantly smaller subset of contexts by constructing smaller, approximate set-representations  X  which we call filters  X  of each L i a Bloom Filter ) which together fit in main memory. These approx-imate set representations allow us to test substrings in entity con-texts for being members of L i with no false negatives (but some false positives), allowing us to prune most substrings and contexts right away. We refer to the substrings that are accepted by at least one of the filters and subsequently written to disk as candidate con-texts . However, the existence of false positives means that we have to  X  X erify X  if such a candidate context does contain a list member.
This leads to the processing strategy described in Figure 4: we process each document d  X  X  using the filters to identify all entities with candidate contexts which may contain a list mem-ber. Because of false positives, the candidate contexts then un-dergo a subsequent verification-step that identifies all contexts con-taining members of L and writes out all resulting entity and list-membership feature pairs. As part of the document processing, we also extract n -gram text features from each document (which we X  X l describe in detail in Section 5). Finally, all features found (i.e., both list-membership features and n -gram features) are grouped per en-tity (we refer to this as the aggregation -step) and then submitted to the classifiers in C .
 Challenges: To make the above processing strategy efficient, we have address two challenges: (a) efficiently identifying all candi-date contexts in each document and (b) minimizing the number of  X  X edundant X  candidate contexts and features written out. We will describe these challenges and how we address them in detail in the following two sections. We first describe the processing of individ-ual documents using filters (Section 4.3) and then the pruning of redundant contexts and features (Section 4.4); finally, we describe the implementation of the verification step itself (Section 4.5).
Identifying list members within an entity X  X  context corresponds to identifying whether sub-strings in the context match with any member of a list in L . 2 Therefore, recognizing list-membership features within an entity X  X  context is similar to the multi-pattern matching problem in the string matching literature [21]. Most prior techniques for solving the multi-pattern matching problem (e.g., the Aho-Corasick algorithm) build a trie over all list members. The trie is used to significantly reduce the number of comparisons be-tween subsequences of words in an input document and patterns.
Recall that we use filter -structures, based on bloom filters [7], which are compact probabilistic structures for representing set S of elements. Given an element e , bloom filters allow us to prob-abilistically check whether or not e belongs to S .If e  X  the bloom filter returns true. However, if e/  X  S it may still re-turn true with a low probability. Let Filter L i denote bloom-filter representation of the list L i .

For a substring c in a document we write c  X  Filter L i if c is accepted by the filter. We can trade off the required memory-footprint for the expected false positive rate of the filter, thereby ensuring that the Filter L i structures can all fit in main memory. Document Processing using Filters: Straight-forward usage would be to check all substrings in entity contexts against each filter. In order to reduce the number of these membership checks against each filter, we also maintain a token table TT L consists of all tokens (e.g., single words) occurring in any member of the list L i . We then further sub-divide a given context c into hit sequences , which are sequences of tokens present in TT L now need to evaluate membership of token sub-sequences against the filters for these hit sequences only, as opposed to all substrings of the context. Similar to Bloom Filters, the token table can be compressed by hashing, allowing us to trade off memory against additional membership checks.
We now introduce some notation. The output of the document processing described in the previous section is a list of pairs of en-tities and candidate contexts, which are accepted by one or more of the filters. We use G E,C to denote the bipartite graph between en-tities and candidate contexts. Conceptually, G E,C contains a node for each entity e  X  X  and a node for each candidate string in an entity context which is accepted by a filter. G E,C contains an edge ( e, c ) whenever the candidate context c is accepted by some filter Filter L i and occurs in the K 2 -context of e in a document d We refer to these ( e, c ) edges as entity-candidate edges and denote the list of all such edges that we materialize to disk during the pro-cessing as Edges ( G E,C ) . After a single pass over D , all members of Edges ( G E,C ) undergo a verification-step to identify all candi-date contexts that actually contain a member of L (see Section 4.5). All entity-list combinations ( e, i )  X  G E,L identified during the verification are written to disk; we use Edges ( G E,L ) to denote the list of these combinations.
 Problem Statement: Given a document collection D and a set of lists L = { L 1 ,...,L f } , compute the set of all co-occurrences of entities and (members of) lists in L : G E,L = { E X { 1 ,..., |L|}| X  l  X  L i :( e, l )  X  D } using the minimal over-head under the constraints that (a) each document in D can only be processed once, (b) during the processing only a limited amount of state (bounded by a threshold M ) may be retained in memory. Figure 5: The Frequency-Distribution of Entities follows a power-law
In our processing framework, the overhead is closely tied to the amount of disk I/O required. Hence, in order to minimize the overhead of the processing, we need to minimize (a) the num-ber of redundant ( e, c ) entity-candidate pairs (i.e., the pairs that do not result in a new ( e, i ) -edge in Edges ( G E,L ) ) written to Edges ( G E,C ) and (b) the number of re dundant edges w ritten to Edges ( G E,L ) .

In the following, we will describe two different pruning strate-gies, based on the observed skew in the number of occurrences in list members and entities themselves, that allow us to avoid writing a significant number of edges to Edges ( G E,C ) and to Edges ( G E,L ) , thereby reducing the processing overhead signifi-cantly.
Looking at list-member distributions within Wikipedia for a number of entity categories, we observed that the distribution of entities in large corpora is similar to that of the word distribution found in natural-language text corpora: both (approximately) ex-hibit a power-law. Figure 5 illustrates the distribution of named entities found in a l arge corpus of Wikipe dia documents. Only a small fraction of list members occur very frequently in docu-ments, whereas most occur rarely. Furthermore, documents con-taining members of a list L i often contain more than one member. Many list-members frequently co-occur with one of the few fre-quent members in their list.
 We can leverage these properties as follows: during the scan of D , we keep small sets Prune L i  X  L i , each containing a subset of members from L i that frequently occur in D . To determine the most frequent list members, we use a sample of D to estimate their relative frequencies. Because of the significant skew of the under-lying distribution, a small sample of D is likely to be sufficient for a high-confidence estimate of the most frequent entities. Now, when processing a document d containing an element l  X  Prune L the document context of an entity e , we can write out the resulting ( e, i ) tuple directly to Edges ( G E,L ) , while avoiding writing any ( e, c ) tuples for any c  X  Filter L i and the document context.
The key question for this optimization becomes if a small num-ber of frequent entities can make a big difference with regards to pruning. To understand this, we set up the following experiment: Using a set of 1.7 million known actors , and a corpus of docu-ments D from Wikipedia, we first compute the set of all members of
L contained in D . Then we selected subsets of the 1%, 2%, 5% and 10% most frequent members of L and measured for each the fraction of candidates (using filters without false positives for this experiment) that are pruned by having the corresponding Prune list available during processing; we use the entire size of each doc-ument as the size ( K 2 ) of the entity-context for list-membership extraction. The results are plotted in Figure 6; note that we vary both the size |D| of the document corpus used as well as the size |
Prune L | of the pruning set. Even small sizes of Prune L Figure 6: Small sets of frequent entities result in significant pruning. significant pruning. For example, selecting the 10% most frequent list-members means that more than half of the candidate edges are pruned. Note that the relative pruni ng effectiveness increases with the size of the document corpus.
A second observation we leverage uses the skew of the entity-distribution. If we know that ( e, i ) is in Edges ( G E,L didate context c  X  Filter L i (that passes none of the other filters) occurring in the context of e can only result in a redundant edge ( e, i ) . Therefore, we can ignore such candidates, saving the cor-responding entity-candidate edges. For this reason we maintain in memory the most frequently occurring entities identified by g For each of these entities we maintain all list-ids for which we have observed a list-member (using the Prune L i structures) in the con-text of e . We refer to this set of entities as Prune E ;now,when we encounter a string c  X  Filter L i in the context of an entity e for which ( e, i )  X  Prune E , we can ignore this observation. How-ever, one challenge is that we cannot compute (an estimation) of the entities X  frequencies a priori, as E is not known. Recall that we only have an entity extractor g E as input which when applied to documents extracts entities.
 Estimation of entity frequency: Keeping exact frequency-information for all entities identified in documents requires a significant amount of main memory. Therefore, we resort to a very space-efficient hash-based approximation scheme to track entity frequencies. We employ a sketching technique called Count-Min Sketch (CM-Sketch) [12]. A CM-Sketch is a 2-dimensional array of counters with width w and depth d : f _ count [1 , 1] ...f _ count [ d ,w ] and d hash functions h 1 ,...,h d : E  X  X  1 ,...,w } . All counters are set to 0 ini-tially. Whenever g E extracts an entity e within a document, we iterate over all hash functions h i ( e ) i =1 ...d and increase f _ count [ i, h i ( e )] by 1. We disregard multiple occurrences of an entity within a single document.

Using these structures, we estimate the frequency of an entity e as  X  frq( e ):=min j  X  X  1 ,...,d } f _ count [ j, h j ( e )] . This estimate will not be exact because multiple entities may map to the same bucket due to hash-collisions. Further, the entity frequency estimates are only based on the subset of documents we observed at any point during processing. As is standard in many online estimation sce-narios, we assume that  X  X urrent X  frequency is indicative of the fre-quency over the entire input collection. We are primarily interested in identifying highly frequent entities and CM-sketches have been shown to perform well for the task of identifying such outliers [13]. Allocating memory between structures: The fact that the filter structures as well as Prune L i and Prune E compete for the same limited main memory available during the document processing makes the tuning of their sizes an interesting research challenge. We leave this as a challenge for future work.
We eliminate all false positives edges in G E,C as follows. Note that each edge consists of the entity and the candidate string which potentially equals a list member. Therefore, we join ( equi-join in SQL) L with Edges ( G E,C ) in order to obtain all list member val-ues which equal a candidate string in Edges ( G E,C ) . In fact, if the lists L and Edges ( G E,C ) are both stored in a database, it is straightforward to remove the false positives through a SQL query, which can be executed efficiently by the database system. For every member of a list L i found, we write the corresponding ( e, i ) -tuple to the materialized list Edges ( G E,L ) .
Unlike the lists L , we expect the number of n -gram features used in the classifiers to be small enough to retain a trie of the n -grams used to identify feature occurrences in main memory during the document processing. While processing a document d we match any n -gram features occurring in the document context of an entity e and write the corresponding ( e, f ) pairs to Edges ( G
This means that processing of n -gram features only requires a single pass over D . However, many entity-feature edges may be written to disk multiple times fro m different documents. We found the distribution of the occurrences of the n -grams associated with the n -gram features over large corpora of documents to resemble a power-law as well. To illustrate this, we have plotted the total occurrence-frequencies (for the entire corpus of Wikipedia) of the 32K text n -grams which were most common adjacent to entities (in training data) in Figure 7. This distribution implies that a small subset of entity-feature combinations is likely to be extremely fre-quent. By processing each of these combinations in isolation, we would create many copies of identical (and thus redundant) edges in Edges ( G E,F ) . If instead we we were to store the frequent entity-feature combinations in memory during the document processing and only write them out at the end of processing once, we would be able to significantly reduce the number of redundant copies of edges are written to Edges ( G E,F ) .
 Problem Statement: n -gram Feature Extraction Given a document-corpus D and a set of n-gram features F pute the respective bipartite graph G E,F containing each edge ( e, f )  X  X  X F for each entity-feature pair occurring within (and those edges only), while minimizing the number of redundant entity-feature edges generated.

The main issue is that there is a constraint M agg on the space we can allocate to entity-feature pairs in memory. In the following, we denote the space required to store an entity e by size ( e ) , whereas we assume unit size to store a feature ID. Now, in a static formula-tion where we know all frequencies frq( e, f ) a priori, the problem of minimizing the number of redundant edges would be to find a set R of entity-feature pairs that satisfy the memory-constraint M and maximizes the number of times these pairs occur, since every occurrence beyond the first one corresponds to a redundant edge. However, we do not know which ( e, f ) pairs will be observed as we process documents in the input collection. Hence, we cannot solve the static problem formulation, but need to optimize the ex-pected benefit of the entity-feature pairs we keep in memory dy-namically. Techniques of this kind have been studied extensively in the context of main memory caching algorithms. However, our problem scenario is somewhat different because the objects that are  X  X ached X  (i.e., the entity-feature p airs) are so small that the space-requirement of retaining statistics on each them is difficult to jus-tify. To maximize the utility of the available memory, we evaluated two different classes of approaches.
 Write-on-Full: At any point in time, we add each entity-feature pair seen to the available main memory (without storing any dupli-cate features). Once the main memory store fills up, we write all ( e, f ) pairs contained in it to Edges ( G E,F ) and start over. The advantages of this algorithm are its simplicity and robustness. Leveraging frequency estimation: In addition, we evaluated a number of techniques that estimate frequencies of entities (using a CM-sketch similar to the one used in list-membership process-ing) and those of features (using a small sample of D ). For this purpose, we evaluated a number of different heuristics to decide which pairs to retain, including (i) keeping the features of the most frequent entities, (ii) retaining the most frequent entity-feature pairs themselves and (iii) maximizing the expected benefit per entity rel-ative to the required storage space (which favors retaining frequent entities with many features). Due to space constraints, we omit the details of the algorithms we evaluated. Combined n -gram and list-membership features: One interest-ing extension we are considering for future work is to combine the two types of features considered in this paper to form features such as born_in_[European_City] or works_for_[Tech_Company] where the text in brackets denotes a list whose members we match to obtain the features (e.g., the first feature would match text strings such as  X  born in Paris  X ). We can extend our technology easily to process these additional features as well. n -ary relation extraction: Due to space constraints we did not de-scribe or evaluate the extraction of n -ary relationships (for n&gt; 1 ) in detail. However, it is possible to extend our techniques to this scenario. We need to extend the notion of an entity X  X  context to contexts of a set of entities (i.e., a set of entities occurring within a limited window of words within a document d ); the corresponding notions of document-and aggregate context follow directly. Re-garding the processing strategies, we need to use the corresponding set of entities as the ke y for grouping f eatures (inst ead of a single entity) and for purposes of list-member co-occurrence.
For our experiments, we use linear support vector machine (SVM) models as the underlying classifier. The training algo-rithm we use for our experiments is Sequential Minimal Optimiza-tion [22]. All experiments were executed on an 2.4Ghz Intel dual-core 660 Processor with 4GB of main memory. We used a SQL Server 2005 database as the underlying storage engine. We use 3.2 million page Wikipedia dataset as the document corpus D .
To study the impact of aggregate context and list-membership features in entity-categorization, we compare our approach to single-context rule-based extract ors similar to several state-of-the-art techniques used for fast relation extraction from docu-ments. The classification task is to correctly categorize differ-ent classes of people ( writers, actors, painters, etc. ) based on Wikipedia documents. To generate training and test data, we used Wikipedia documents that contain lists of instances of the respec-tive category (e.g. http://en.wikipedia.org/wiki/-List_of_painters_by_name ), from which instances of the category were extracted using specialized extraction-script. For purposes of repeatability, we  X  instead of using custom entity ex-traction code g E  X  matched entities in D by simply finding these names within each document. To compute the n -gram features, we extracted all n -grams in the size-4 document context for each oc-currence of an entity, using the presence/absence of the 10K most frequent (i.e., occurring in the largest number of contexts within the training data) n -grams among them as features. For the list-membership features, we used the entire document an entity e oc-curs in as e  X  X  aggregate context, using a 10% sample of the entities in the training data for each category as the corpus L .
To instantiate the single-context rule-based classifier with extrac-tion patterns that are highly indicative of the different categories, we used the same document contexts as above as training data and used the algorithm described in [18] (Section 3.2) to determine the 100 K most frequent text patterns (which may include gaps) that are highly correlated to the entity type. An example of such a pat-tern containing a gap would be  X   X  X  ...novel  X , which would match strings such as  X   X  X  latest novel  X  X r X   X  X  acclaimed novel  X  X hichmay be highly correlated to instances of the class writer . Note that the algorithm is exhaustive, in the sense that it outputs the most fre-quent patterns that have high correlation to a class over the space of all possible patterns. When we have two patterns in our result set where one pattern is a substring of the other, we prune the more complex pattern, avoiding large numbers of similar extraction rules.
Now, we compare the precision/recall tradeoff for the resulting classifiers. To measure precision/recall for our approach, we ap-plied the classifier to held out test data and varied the distance from the decision surface we required to assign a category label. Entities that were  X  X oo close X  to the decision surface were not labeled.
In unsupervised or weakly supervised rule-based extraction, the frequency (also referred to as redundancy) of extraction has com-monly been used to assess the confidence assigned to an extracted relation (e.g., [6, 14]). Hence, we use the following approach to obtain precision/recall data for the single-context classifier: we set a threshold on the number of extractions we have to have seen for a specific (entity, category) pair, before we accept it. By sliding this threshold across the domain of extraction frequencies, we can gen-erate a similar precision/recall graph for the rule-based extraction.
Figure 8 shows the results for two example target classes  X  painters and presidents . We can see that the rule-based classifier generates rules with high accuracy, but cannot guarantee high re-call: many of the entities in our training data do not exhibit a single instance of a frequent high-confidence pattern. Overall, the classi-fiers based on aggregate contexts perform better, in particular the ones that leverage list-membership features. We consistently saw these gains across all target classes. For the rule-based classifiers, we also varied the minimum correlation to the target label that was required for a rule to be output  X  our graphs show the results for two settings (60% and 80%). Requiring lower correlation reduces ac-curacy, but results slightly higher recall; even so, there is no cross-over point with the classifiers using aggregate context features.
To study the impact of list-membership features in scenarios where there is no aggregation across many documents, we ran the following experiment: using a set of 21K products from a exist-ing shopping web site, we studied the task of classifying these into the 7 categories , one of which each of them belonged to: CD &amp; DVD Drives , T-shirts , Laptop Computers , Books (Drama) , Books (Thriller) , Books (History) , Books (Romance) . The motivation be-hind this particular choice of categories was that some of these are very  X  X asy X  to distinguish (e.g. t-shirts and laptops) whereas the various subcategories of books are very difficult to classify accu-rately. For classification, we used SVM-classifiers trained on: (a) word n -grams extracted from the product names and descriptions and (b) word n -grams plus seven list-membership features  X  one for each category in the product description (as L , we used a sam-ple of 10% of the entities in the training data), using Wikipedia as the corpus from which we derived co-occurrences between entities and list-members. Here, the second classifier increased the accu-racy over all entities and categories from 93 . 8% to 95 . 5% . Interestingly, while the precision-recall curves for the categories CD &amp; DVD Drives , T-shirts , Laptop Computers were nearly un-changed for the two classifiers, we saw a significant increase in ac-curacy for the book-categories, all of which were more difficult to classify accurately. We plotted two of the resulting precision/recall graphs in Figure 9. As we can see, the improvement in accuracy for the Books -Drama category is very significant (whereas the ones for t-shirts are slight), resulting in the overall gains.
To evaluate the effect of our techniques for extraction of list-membership features for large L , we use the similar setup as previ-
Figure 9: List-Membership impacts  X  X ard X  classes significantly ously, and use a corpus of 1.7 million known actors as L .Fortrans-parency of the results, we  X  X imulate X  the accuracy of the Filter structure, using a rate of false positives to true matches of entities in L of 4:1. First, we measured the number of edges in Edges ( G written to disk for different sizes of Prune E and Prune L the amount of space given to Prune E / Prune L between 1% and 10% of the space required to store all entities/list-members occur-ring at least once in D . As the baseline, we use an approach that writes all edges in Edges ( G E,C ) without any pruning (we do not introduce any false positives for this baseline). Even retaining only a small subset of frequent entities from E and L results in signifi-cant reduction in the number of redundant edges (Figure 10).
To quantify the impact this has on execution times in practice we measured the overhead of (a) the final aggregation and sort step for all entity-feature edges in G E,F and G E,L , (b) the verification step that matches all edges in G E,C with L and (c) the overhead im-posed during the the iteration over D by writing out the edge-sets to disk (measured by computing the difference in time to the process-ing of D without persisting any edges), using a relational DBMS for the aggregation and verification steps. The results are shown in Figure 11. We can see that our techniques reduced the over-head of verification by more than an order of magnitude and halved the overhead of disk writes, reducing the overall processing cost by over 80% compared to the baseline. The remaining overhead is dominated by the cost of writing out the edges in Edges ( G and aggregating them, the reduction of which we will discuss next.
Figure 12: Write-on-Full is competitive with complex techniques
To assess the effectiveness of the techniques discussed in Sec-tion 5, we used a single classifier using 10K n -gram features. To choose the appropriate memory-size to use in these experiments, we pre-computed the size required to store G E,F in its entirety (without any duplicate edges) and set the memory size to a frac-tion of this value (we used 1% and 2% in our experiments). Fig-ure 12 shows the number of entity-feature pairs written for three different strategies: Write-All , which is the baseline that uses no additional memory, Write-on-Full which we described in Section 5 and the best of the more complex strategies utilizing statistical in-formation on entity-and feature-frequency, which seeks to mini-mize redundant edges by r etaining the entities with the largest (es-timated) benefit divided by the size required to store the entity and all its features. We set the amount of storage given to the under-lying CM-Sketch to 5% of the available memory. We can see that both Write-on-Full as well as the more complex strategy reduce the number of tuples written to disk by a factor of about 1/3. The main reason for this result is the heavy skew of both the entity/feature distribution: the most frequent pairs are so frequent that they will re-occur (often many times) before Write-on-Full has to to write the buffer to disk, meaning that we can realize most of the benefit of the  X  X aching X  without frequency-statistics. One of the complex strategies outperforms Write-on-Full slightly, but given its simplic-ity and robustness Write-on-Full is still likely the method of choice in practice. Moreover, Write-on-Full actually outperformed other statistics-based approaches, such as retaining the entities that were (estimated to be) most frequent.
Entity categorization within large text corpora is of importance to a number of emerging applications such as structured querying over unstructured data. For this task, we have shown that the use of aggregate entity contexts and list-membership features in clas-sification can improve accuracy significantly, when compared to techniques using single contexts only. We have shown how to scale up the processing of these aggregate features significantly by lever-aging properties of the entity/feature distribution as well as entity co-occurrence, identifying a number of effective techniques to min-imize the amount of redundant data produced.
