 In this paper, we present, to our knowledge, the first known I/O efficient solutions for computing the k -bisimulation partition of a massive directed graph, and performing maintenance of such a partition upon updates to the underlying graph. Ubiquitous in the theory and application of graph data, bisimulation is a robust notion of node equivalence which intuitively groups together nodes in a graph which share fundamental structural features. k -bisimulation is the standard variant of bisimulation where the topological features of nodes are only considered within a local neighborhood of radius k &gt; 0.

The I/O cost of our partition construction algorithm is bounded by O ( k  X  sort ( | E t | ) + k  X  scan ( | N t | ) + sort ( | N while our maintenance algorithms are bounded by O ( k  X  sort ( | E t | ) + k  X  sort ( | N t | )). The space complexity bounds are O ( | N t | + | E t | ) and O ( k  X | N t | + k  X | E t and | N t | are the number of disk pages occupied by the input graph X  X  edge set and node set, resp., and sort ( n ) and scan ( n ) are the cost of sorting and scanning, resp., a file occupying n pages in external memory. Empirical analysis on a variety of massive real-world and synthetic graph datasets shows that our algorithms perform efficiently in practice, scaling gracefully as graphs grow in size.
 G.2.2 [ Graph Theory ]: Graph algorithms; E.1 [ Data Structures ]: Graphs and networks graph bisimulation; structural index; external memory algorithm
Massive graph-structured datasets are becoming increas-ingly common in a wide range of applications. Examples such as social networks, linked open data, and biological networks have drawn much attention in both industry and academic research. In reasoning over graphs, a fundamental and ubiquitous notion is that of bisimulation, which is a characterization of when two nodes in a graph share basic structural properties such as neighborhood connectivity. Bisimulation arises and is widely adopted in a surprisingly large range of research fields [28]. In data management, bisimulation partitioning (i.e., grouping together bisimilar nodes in order to reduce graph size) is often a basic step in indexing semi-structured datasets [23], and also finds fundamental applications in RDF [25] and general graph data (e.g., compression [5, 9], query processing [17], data analytics [8, 31]).

It is often the case that bisimulation reductions of real graphs result in partitions which are too refined for effective use. Hence, a notion of localized k -bisimulation has proven to be quite successful in data management applications (e.g., [11, 17, 26, 32]). k -bisimulation is the variant of bisimulation where topological features of nodes are only considered within a local neighborhood of radius k &gt; 0. With a pay-as-you-go nature, k -bisimulation is cheaper to compute and maintain, cost adjustable, and faithfully representative of the bisimulation partition within the local neighborhood. Algorithms for bisimulation partitioning have been studied for decades, with well-known algorithms such as those of Paige and Tarjan [24] and more recent work (e.g., [7]), having effective theoretical behavior.

In practice, however, state-of-the-art solutions face a critical challenge: all known approaches for computing bisimulation are internal-memory based solutions 1 . As such, their inherently random memory access patterns do not translate to efficient I/O-bound solutions, where it is crucial to avoid such access patterns. Consequently, when processing graphs which do not fit entirely in main memory the performance of these algorithms decreases drastically.
The reality is that, in practice, many graphs of interest are too large to be processed in main memory. Indeed, massive graphs are now ubiquitous [8, 15]. Furthermore, the size of graphs will only continue to grow as technologies for generating and capturing data continue to improve and proliferate. We can safely conclude that it will become increasingly infeasible to apply existing internal-memory bisimulation partition algorithms in practice.

To process real graphs, therefore, we must necessarily turn to either external memory, distributed, or parallel solutions. There has been some work on parallel (e.g., [27, 30]) and distributed (e.g., [4]) approaches to bisimulation computation, and, recently, external memory solutions on restricted acyclic and tree-structured graphs [16]. However, to our knowledge there is no known effective solution for
With the single exception of Hellings et al. [16] which we discuss below in Section 3.2. computing bisimulation and k -bisimulation partitions on arbitrary graph structures in external memory. Such an algorithm would not only enable us to process big graphs on single machines, but also provide an essential step for parallel and distributed solutions (e.g., MapReduce [20]) to further scale their performance on real graphs. As noted in paper [20] and many other researches (e.g., [19]), in many cases, single machine external memory algorithms are more competitive than distributed algorithms due to their lack of communication overhead and their effective use of available infrastructure. Therefore, the study of external memory solutions is clearly warranted.
 Given these motivations, we have studied external memory solutions for reasoning about k -bisimulation on arbitrary graphs. In this paper, we present the results of our study, which makes the following high-level contributions. The rest of the paper is organized as follows. In the next section we give our basic definitions and data structures used. We then describe in Section 3 our solution for constructing k -bisimulation partition. Next, Section 4 presents algorithms for keeping an existing partition up to date, in the face of updates to the underlying graph. Section 5 presents the results of our empirical study of all algorithms. We then conclude in Section 6 with a discussion of future directions for research.
Our data model is that of finite directed node-and edge-labeled graphs  X  N,E, X  N , X  E  X  , where N is a finite set of nodes, E  X  N  X  N is a set of edges,  X  N is a function from N to a set of node labels L N , and  X  E is a function from E to a set of edge labels L E .

Definition 1. Let k be a non-negative integer and G =  X  N,E, X  N , X  E  X  be a graph. Nodes u,v  X  N are called k -bisimilar (denoted as u  X  k v ), iff the following holds: 1.  X  N ( u ) =  X  N ( v ) , 2. if k &gt; 0 , then  X  u 0  X  N [( u,u 0 )  X  E  X  X  X  v 0  X  N [( v,v 3. if k &gt; 0 , then  X  v 0  X  N [( v,v 0 )  X  E  X  X  X  u 0  X  N [( u,u
It can be easily shown that the k -bisimilar relation is an equivalence relation.

We illustrate Definition 1 with an example. Consider the graph given in Figure 1. It is a small social network graph, in which nodes 1 and 2 are 0-and 1-bisimilar but not 2-bisimilar. 3 each node u of the graph to a partition block , which is the unique subset of nodes in the graph of which the members are k -bisimilar to u .

In particular, we are interested in constructing partition  X  X dentifiers. X 
Definition 2. A k -partition identifier for graph G =  X  N,E, X  N , X  E  X  and k  X  0 is a set of k + 1 functions P = { pId 0 ,..., pId k } such that, for each 0  X  i  X  k , pId function from N to the integers, and, for all nodes u,v  X  N , it holds that pId i ( u ) = pId i ( v ) iff u  X  i v .
A fundamental tool in our reasoning about k -bisimulation is the notion of node signatures.

Definition 3. Let G =  X  N,E, X  N , X  E  X  be a graph, k  X  0 , and P = { pId 0 ,..., pId k } be a k -partition identifier for G . The k -bisimulation signature of node u  X  N is the pair sig k ( u ) = ( pId 0 ( u ) ,L ) where:
L = We then have the following fact.

Proposition 1. pId k ( u ) = pId k ( v ) iff sig k ( u ) = sig ( k  X  0) .
 Proof omitted.
 Proposition 1 is the basis of all algorithms in this paper. The basic idea is that a node X  X  k -bisimulation partition block can be determined by its k -bisimulation signature, which in turn is determined by the ( k  X  1)-bisimulation partition of the graph. Intuitively, in order to compute the k -bisimulation partition, we compute the graph X  X  j -bisimulation (0  X  j  X  k ) partitions bottom-up, starting from j = 0. We call each such intermediate computation the iteration j computation .

It is straightforward to show that the k -bisimulation partition of a graph is unique. Hence, in the sequel, we can
Note that we use  X  E ( u,u 0 ), instead of  X  E (( u,u 0 )), for ease of readability. safely talk about k -partition identifiers as unique objects. Also, note that we will use integer node identifier values to designate nodes in N . Therefore, in the following discussions the functions sig k and pId k both could take node identifiers (i.e., integers) as input.

Table 1 shows one way of assigning k -bisimulation ( k = 0 , 1 , 2) partition identifiers and signatures for the example graph in Figure 1, where the nId denotes the unique identifier for each node, and pId i ( nId ) and sig (0  X  i  X  2 and 0 &lt; j  X  2) are presented accordingly. For k = 0, nodes are grouped into two partitions by node labels (given identifiers A and B ). Then for k = 1 , 2, signatures are constructed according to Definition 3, and then distinct partition identifiers are assigned to distinct signatures, following Proposition 1.
We assume that graphs are saved on disk in the form of fixed column tables (node set as table N t and edge set as table E t ). We also assume that these tables can have several copies sorted on different columns. In later discussions, we will use the notation X.y to refer to column y of table X .
We have the following possible attributes for N t : nId node identifier (note that this is the same nLabel node label pId old nId bisimulation partition identifier for the given pId new nId bisimulation partition identifier for the given pId j nId j bisimulation partition identifier for the and for E t : sId source node identifier tId target node identifier eLabel edge label pId old tId bisimulation partition identifier for the given
We further assume that we have a signature storage facility S , which stores the mapping between signatures and their corresponding partition identifiers. S is a data structure having only one idempotent function called S.insert() . For node u  X  N , S.insert() takes sig j ( u ) (0  X  j  X  k ) as input, and provides pId j ( u ) as output. Essentially S.insert() implements the one to one mapping function from sig j to pId j . The implementation details of S will be discussed in Section 3.2.

For ease of discussion and investigation, we assume in what follows that the N t and E t are each just one file sequentially filled with fixed length records. Moreover, in this paper we make use of sort merge join to the extent possible, since it is a very basic way to achieve I/O efficient results. However, many possibilities could be explored for implementing these data structures (e.g., indexing techniques) and join algorithms to further optimize our presented results. We leave such investigations open for future research.

Finally, we also assume that we have a (possibly external memory based) priority queue available. In our empirical study below, we use the off-the-shelf I/O efficient priority queue implementation provided by the open source STXXL library [6].
Since our focus is on disk-resident datasets, we use stan-dard I/O complexity notions to analyze our algorithms [1]. The primary concern here is to minimize the number of I/Os needed to complete the task at hand.

Suppose we have table X , space to hold B disk pages in internal memory, and X occupies | X | pages on disk. In what follows, we will use the following notation:
We present our algorithm for k -bisimulation partition computation in Algorithm 1. The algorithm is inspired by Proposition 1, meaning for each node in the input graph, to construct its signature and find a one-to-one mapping number (partition identifier) for that signature.

In iteration j = 0, we assign distinct partition identifiers to nodes based on their nLabel s. For other iterations j &gt; 0, our algorithm mainly performs two things for each node ID uId  X   X  nId ( N t ) (line 14 to 17): (1) construct sig and (2) insert sig j ( uId ) to S , record the returning pId in the corresponding row in N t . To prepare the necessary information for constructing sig j ( uId ), we need to fill in the missing columns of E t (line 5 to 10). Several scans and sorts on tables are involved for each iteration. Note that some operations in the algorithm can be merged as one in practice. We present them separately just to make the presentation clearer. A detailed description is given in Section 3.1. node table N t , edge table E t and k , which is the degree of local bisimilarity from Definition 1. The output variables are N t and E t . The schema of N t is ( nId , nLabel , pId pId old nId , pId new nId ); the schema of E t is ( sId , eLabel , tId , k = 0 , line 2 to 4. According to Definition 1, k = 0 means nodes having the same labels should be assigned the same partition identifier. We achieve this by sorting N t nLabel column. When scanning N t , for each new nLabel we encounter, we assign a new integer (e.g., a predefined counter) to the corresponding nId , filling it in the pId and pId new nId columns. This will take O ( sort ( | N t O ( scan ( | N t | )) I/Os. Using a hash map could achieve the same goal as well, with the same I/O upper bound. k &gt; 0 , line 5 to 18. For k &gt; 0, we first perform a recursive call to the algorithm, ensuring we work in a bottom-up manner. For iteration 1 ( k = 1), we sort N Algorithm 1 Compute the k -bisimulation equivalence classes of a graph 1: procedure Build Bisim ( N t , E t , k ) 2: if k = 0 then 3: fill in the pId 0 nId and pId new nId columns of N t . O ( sort ( | N 4: return ( N t , E t ) 6: if k = 1 then 7: N t  X  sort( N t ) by nId . O ( sort ( | N t | )) 8: E t  X  sort( E t ) by tId . O ( sort ( | E t | )) 9: scan N t , move content of column pId new nId to pId old nId 10: fill in the pId old tId column of E t . O ( scan ( | E 11: initialize S 12: F  X   X   X  ( E t ) , where  X  = ( sId,eLabel, pId old tId 14: for each uId  X   X  nId ( N t ) do . overall O ( scan ( | E 15: construct sig k ( uId ) from F . merge join with F 16: pId k ( uId )  X  S.insert ( sig k ( uId )) 17: record pId k ( uId ) in N t . pId new nId where nId = uId 18: return ( N t , E t ) and E t on nId and tId , preparing them for later merge join operations. The algorithm X  X  idea is to construct the signature of each node in order to distinguish it from other nodes according to the k -bisimilar relation. If we can properly fill in the pId old tId column of E t , and join it with N t on nId=sId , the information combined from columns { pId 0 nId ,eLabel, pId old tId } is enough for constructing the signature. The column eLabel is already filled in before algorithm starts. The column pId 0 nId is filled in during iteration 0 (line 2 to 4). The column pId old tId during each iteration j &gt; 0 (line 10). Then for each node ID uId  X  N t , we get its sig k ( uId ), insert it to S in an I/O efficient way, getting pId k ( uId ) in return, and then placing this value in the pId new nId column of N t .

At line 10 of Algorithm 1, to fill in the pId old tId column of E t , we conduct a sort merge join of E t and N both tables are sorted properly in iteration 1), replacing the
At line 15 of Algorithm 1, we sequentially construct the signature sig k ( uId ) for each uId  X   X  nId ( N t ) according to Definition 3, and get the corresponding pId k ( uId ) (using S.insert() ). All pId k ( uId ) will be written back to the pId new nId column of N t (where nId = uId ) right after, so that there is no random access to N t . Note that although by definition sig k is a set, we construct sig k ( uId ) as a string, maintaining elements of the set in sorted order. It is both an easy way for storing a set and handy for implementing S later on (e.g., using a trie). Example run. If we assume the numbering scheme for S is a self-increased counter across iterations, Table 1 would be the intermediate results for running Algorithm 1 on the example graph in Figure 1 ( k = 2), and Table 2 gives the final output of the algorithm.
 let the algorithm run k iterations. Indeed, it can be shown (proof omitted) that after a bounded number of computation iterations, Algorithm 1 would achieve the full (i.e., classical non-localized) bisimulation partition. We could detect this by simply checking the partition size each iteration produces. If two consecutive iterations produce the same number of partition blocks, this means that the algorithm already achieves the full bisimulation partition, and therefore it is safe to terminate the algorithm.
 Data structures for S. The signature storage facility S clearly plays an important role in Algorithm 1. In principle, any data structure that permits an efficient set-equality check will be sufficient. Trie and dictionary are such data structures, for instance. During our experiments, we see that in many of the cases, partition sizes are small and the signatures are short, for which a main memory based data structure is enough. In other cases, signature length could reach several million and partition size into tens of millions, then we need some external memory based solution for S . We could, for example, sort all signatures from F in an I/O efficient way [2], then when scanning these signatures, partition identifiers are assigned. In this case, the overall cost of the S.insert() operation could still be bounded by O ( sort ( | E t | )). Other disk based solutions, such as disk-based tries (e.g., String B-Tree [10] or [13]) or inverted files (e.g., [22]) could also be considered.

In our experiments we use BerkeleyDB (B-Tree or Hash index) to mimic a trie, which, as we show in the experimental results, has acceptable empirical behavior.
 acterization of Algorithm 1.

Theorem 1. Let k  X  0 and G =  X  N,E, X  N , X  E  X  be a graph. Algorithm 1 computes the k-bisimulation partition of G with I/O complexity of O ( k  X  sort ( | E t | ) + k  X  scan ( | N sort ( | N t | )) , and space complexity of O ( | N t | + | E Proof omitted.
 1, the only known solutions for computing bisimulation on graphs in external memory are those of Hellings et al. [16], with I/O complexity of O ( sort ( | N t | + | E t | )). There are two critical differences between their work and ours. (1) Targeting different problems. The solutions of Hellings et al. are designed specifically for the special case of acyclic graphs. Our approach does not rely on such structure, computing bisimulation regardless of the presence or absence of cycles in the graph. (2) Using different techniques. Hellings et al. compute partition blocks level by level, starting from the leaf nodes of the graph. Our approach constructs all partition blocks at each iteration, using data structures and processing strategies which are not tied to any (a)cyclic structure in the graph. In particular, the techniques of Hellings et al. do not generalize to graphs having cyclic structure.
It is easy to show that any edge and node updates on a graph can potentially change the complete k -bisimulation partition of the graph. Therefore, in the worst case, the lower bound of such maintenance cost is the cost of recomputing the k -bisimulation partition from scratch. However, when dealing with real graphs, as we shall see in Section 5, in many cases there is still hope to use data structures such as S and priority queue to maintain the correct partition result instead of recomputing everything. In this section we propose several algorithms for this purpose.

For maintenance algorithms we assume that we have constructed the k -bisimulation partition of graph G =  X  N,E, X  N , X  E  X  , where, as before, G  X  X  N t and E t are stored on disk, containing the historical information kept in N (Table 3); E t is the same as in Algorithm 1, but has two copies with sort orders ( sId,tId ) and ( tId,sId ) to boost performance. We use E tst and E tts to refer to each of these copies.

We further assume that we save the signature storage facility S on disk, which we use and update throughout the maintenance process.

The maintenance problem includes the following subprob-lems.
 Change k . If k increases, we carry out another iteration of computation. If k decreases, the result can be returned directly since we keep the history information in N t . set of new nodes, we assume the new nodes are isolated, stored in the newNodes table, which has the same schema as N t , and that | newNodes | = O ( | N t | ). We first sort N and newNodes by nLabel , then perform a merge join on the nLabel column to fill in the pId 0 nId column of newNodes for all the existing nLabel . For the missing ones, we request a new pId for each of the new nLabel . Then we get the pId 1 ,...,pId k of the newNodes by inserting its pId 0 to S . At the end we append the whole newNodes to N t . The I/O complexity of Add Nodes() is bounded by O ( sort ( | N t | )). set of edges, we assume that the edges are added between existing nodes. If this is not the case, we first call procedure Add Nodes() . The new edges are stored in the newEdges table, having the same schema as E t . For inserting one edge ( s,l,t ) to G , the potential changes are to sig j ( s ) (1  X  j  X  k ), as well as those signatures of all ancestors of s within k steps. So the main work is to detect whether there is some change in sig j ( s ) and propagate those change(s) to its parent nodes X  signatures in later iterations. We use a priority queue pQueue to record and process such changes in a systematic, level-wise manner. For some node ID uId and iteration j , pQueue stores the pair (j,uId) as priority reference. Then whenever we dequeue one element from pQueue , we get the smallest node ID from the lowest iteration (lowest priority reference). Therefore pQueue indicates those nodes whose signatures could change in each iteration level (from 1 up to k ).

At the beginning of the algorithm, we enqueue ( j,s ) to pQueue (  X  ( s,l,t )  X  E t , 0 &lt; j  X  k ). Then, while pQueue is not empty, we dequeue the list of ( j,uId ) pairs with the same j out of the queue, construct the new signature of each such uId , insert it to S , and compare the returning pId j ( uId ) with the old pId j nId value of uId . If the pId remains the same as the old one, we continue; if it changes, we record pId j ( uId ) in N t , and enqueue all ( j + 1 , vId ) pairs to pQueue where vId  X   X  sId (  X  tId = uId ( E t )). Pseudo code is given in Algorithm 2, and a detailed discussion is in Section 4.1.
 example, when removing an edge ( s,l,t ), it is the same idea as adding one. We also (potentially) modify the signature of s , propagating changes to its ancestors via pQueue , then the reasoning is the same. When removing a node, we first remove each incoming edge and each outgoing edge for that node. Then we remove the node from N t . node table N t , edge tables E tst and E tts , the signature storage facility S , the new edge set newEdges and k . The output variables of Algorithm 2 are N t , E tst , E tts S . N t  X  X  schema is given in Table 3, while E tst newEdges  X  X  schema is the same as E t in Algorithm 1. k = 0 , line 2 to 3 of Algorithm 2. For k = 0, since all nodes X  information is properly filled (including the pId 0 nId column in N t ), we only need to add new rows to E tst and E tts according to newEdges . k &gt; 0 , line 4 to 20 of Algorithm 2. For k &gt; 0, for each iteration, which is indicated by j in the algorithm, we need to (1) find out the potential nodes whose signatures could have changed; (2) check whether these signatures have been changed or not; and, (3) propagate any such changes to the parents of these nodes. To record the potential nodes and to perform the propagation, we use a priority queue pQueue . To check signature changes, we reuse the signature storage facility S .

When adding a new edge ( s,l,t )  X  newEdges to the graph, all sig j ( s ) ( j &gt; 0) have the potential to change, and hence we add all pairs ( j,s ), for j  X  { 1 ,...,k } , to pQueue , indicating that we need to check the signature of s in every iteration (line 7 to 8). For each iteration j &gt; 0, we dequeue from pQueue all node IDs in the smallest iteration j , remove duplicates, and save them to a temporary table M , so that M contains in sorted order all node IDs whose signatures would change in iteration j . Then we create an extra table F , preparing for signature constructions. This is achieved by performing a merge join of E tst and M (where E tst .sId  X  M ). Then we fill in F . pId old tId column, as in Algorithm 1. Algorithm 2 Add a set of new edges to existing k -bisimulation partition 2: if k = 0 then 3: merge newEdges into E tst and E tts . O ( sort ( | E t 4: else . k &gt; 0 5: N t  X  sort( N t ) by nId . O ( sort ( | N t | )) 6: create empty priority queue pQueue . overall O ( sort ( | N 7: for j  X  X  1 ,...,k } and ( s,l,t )  X  newEdges do 8: enqueue ( j,s ) to pQueue 9: merge newEdges into E tst and E tts , fill in the pId old tId column . O ( sort ( | E t | )) 10: while pQueue is not empty do 12: F  X   X  sId  X  M ( E tst ) . merge join, O ( scan ( | N t 13: fill in the pId old tId column of F . O ( scan ( | N t 14: H  X   X   X  ( F ), where  X  =( sId , eLabel , pId old tId ) 16: for all uId  X  M do . scan M , N and H , overall O ( scan ( | N t | )) + O ( scan ( | E t | )) + cost of S 17: construct sig j ( uId ) from H 18: pId j ( uId )  X  S.insert ( sig j ( uId )) 19: if pId j ( uId ) is not the same as the corresponding value in N 20: propagate changes to N t and pQueue . O ( scan ( | N t 21: return ( N t , E tst , E tts , S )
After projection on the ( sId , eLabel , pId old tId ) of F and removing duplicates, we get H (line 15), and are ready to construct the signatures. For each uId  X  M , we construct sig j ( uId ) according to the signature definition. The idea of constructing the nodes X  signatures is the same as line 15 of Algorithm 1, only in this case we are not considering every node but only those appearing in pQueue (and later in M ). We then call S . insert(sig j ( uId ) ) for all such uId . If S returns the same pId j ( uId ) as recorded in N t . pId nothing will happen; otherwise we change the N t . pId entry of uId accordingly, and propagate the changes to pQueue . If j &lt; k , we add all parents of uId to pQueue to indicate that we will check these nodes X  signatures in the j + 1 iteration. 2 using two examples. Here we will extend the graph from Figure 1 as in Figure 2. The dashed lines in this figure indicate the two edges which we will add in our examples. 3 to pQueue . Then after checking each of these, the algorithm finds no change in node 2 X  X  signature, therefore no change propagates, and the algorithm stops. We see that comparing with Table 1, the only thing that changes is to add one more row (node 7) to the table. Since node 7 does not have outgoing edges, adding one edge that points into node 7 will not change any existing nodes X  X  signature. Node 7 belongs to the group of node 6, and no other node changes group membership.

In the second case, suppose we add edge (6 ,l, 5) to the original graph of Figure 1. The algorithm first add (1, 6) and (2, 6) to pQueue . Then in iteration 1, the algorithm detects that the signature of node 6 does change, and therefore adds one new pair (2, 2) to pQueue . In iteration 2, both node 2 and node 6 X  X  signatures are checked, and they are both changed. We see that in Table 5 pId 2 ( 1 ) and pId become the same, while pId 2 ( 6 ) changes from K to I . acterization of Algorithm 2.

Theorem 2. Let G =  X  N,E, X  N , X  E  X  be a graph and k  X  0 . After adding a set of new edges to G , Algorithm 2 correctly updates the k -bisimulation partition of G with I/O complexity of O ( k  X  sort ( | E t | ) + k  X  sort ( | N complexity of O ( k  X | N t | + k  X | E t | ) .
 Proof omitted.
 our empirical study (Section 5.3.4), it is not always beneficial to use Algorithm 2, since it performs extra work in each iteration. Heuristics could be adopted to decide when to switch back to Algorithm 1. For example, if at a certain iteration, most of the nodes are placed into pQueue , it is more beneficial to switch back to Algorithm 1. This could be done by simply checking the size of pQueue at the beginning of each iteration.
In this section we present the results of an in-depth experimental study of our algorithms. After introducing our set-up, we show the performance of the algorithms on both synthetic and real datasets. In these experiments, various aspects of the algorithms are investigated while other settings are fixed. A thorough analysis of the k -bisimulation result itself can be found in paper [21]. machine with 2.27 GHz Intel Xeon (L5520, 8192KB cache) processor, 12GB main memory, running Fedora 14 (64-bit) Linux. We use C++ to implement all the algorithms, using GCC 4.4.4 as the compiler. We use the open-source STXXL library [6] to construct the tables and perform the external memory sorting, and use Berkeley DB to implement S . One S is used for all computation iterations (as discussed in Section 3.2). In the experiments we do not exploit any parallelism and restrain ourselves with predefined buffer sizes. We record the running time as well as the I/O volume between the buffer and the disk system. Therefore, the performance (time) of the experiments are comparable to a commodity PC, and the I/O volume can be repeated on other systems. In the following experiments, we set both the STXXL buffer and Berkeley DB buffer to be 128MB, if not otherwise indicated. Please note that we run experiments for the Twitter dataset on a different machine (Intel Xeon E5520, 2.27 GHz, 8192KB cache, 70G main memory, same OS) for limited disk space reason, using a 512MB/512MB buffer setting.
 experiment with various graph datasets. The datasets are collected from public repositories, ranging from synthetic data to real-world data, from several million of edges to more than 1.4 billion edges. In Table 6 we give a description of the datasets, as well as some simple statistics of them. All datasets are accessed on 15 May 2012. Note that due to space limitation, in the following we show the experiment results on a subset of the datasets when the result is representative enough.
 http://dbtune.org/jamendo/ http://thedatahub.org/dataset/l3s-dblp http://haselgrove.id.au/wikipedia.htm http://www.cs.vu.nl/~pmika/swc/btc.html
In Figure 3 we show the experiment results for Algorithm 1 on all datasets. We compute the 10-bisimulation (i.e., k = 10) of these datasets, and measure many aspects of the running behavior for each iteration. Concerning time measurement, we run every experiment 5 times and take the average number. S uses BerkleyDB X  X  B-Tree index in this experiment.

In Figure 3a, we show the number of partition blocks every iteration produces for all datasets. We see that the numbers vary from one dataset to another, where the difference is sometimes more than an order of magnitude, and interestingly, does not directly relate to the size of the dataset. In certain cases (e.g., Twitter) partition size is quite large. Moreover, many of the datasets (e.g., Jamendo, LinkedMDB, DBLP, etc.) reach full bisimulation after 5 iterations. In fact, all datasets (including Twitter) get sufficient partition result after 5 iterations of computation. Here we can reasonably argue that even for Twitter dataset, the partition results after 5 iterations are too refined (e.g., (partition count)/(node count) &gt; 0 . 8).

Figure 3b shows the maximum length of signatures for each iteration. We observe that the signature length is usually quite short, especially comparing with the size of the graph. But there are still cases (e.g., Twitter) that the signature becomes very long (more than 1 million integers), which stresses the need for an I/O efficient solution for S . Note that the synthetic datasets, such as BSBM and SP2B, reach their full bisimulation partition after 3 iterations of computations, and have rather short signatures, indicating that they are highly structured.

Figures 3c and 3d show the I/O volume spent on sort-ing/scanning (STXXL) and on interacting with S (Berkeley DB). We see for most of the datasets, there is no dramatic change cross different iterations. But for Wikilinks and Twitter, the two datasets which have very few partition blocks at the beginning and many at the end, there is a noticeable difference on S for different iterations. In this case I/O on S becomes a comparable factor with sort and scan (I/O on STXXL).

Figure 3e shows the time spent on preparing the signature (line 5 to 13 in Algorithm 1) for each iteration, which is quite stable for all datasets. Figure 3f shows the time on constructing the signature and insert into S (line 14 to 17 in Algorithm 1). In this case datasets with higher degrees tend to cost more time in later iterations, which correlate with their longer signatures and larger number of partition blocks. For all datasets, however, the operations on constructing and looking for signature are the dominant factor for each iteration. This brings us to think about further optimization tasks on construction of signature and implementation of S .
We can conclude that the algorithm is practical to use. It can process a graph with 100 million edges (e.g., WikiLinks and DBPedia) in under 700 seconds for one iteration, and performance scales (almost) linearly with the number of nodes and edges.
As we mentioned in Section 3.2, S could be implemented in several ways. we compare the overall I/O performance of
Build Bisim() using B-Tree and Hash indexes for S on several datasets. We notice that the B-Tree implementation slightly outperforms Hash Index for all datasets. This is most likely due to small caching effects and locality of references during construction of the signatures.
We allocate two buffers, one for scan and sort (STXXL buffer in our case), one for S (BerkeleyDB buffer in our case), in order to analyze the impact of buffer size on our algorithms. To illustrate, we take the DBPedia dataset since it is large enough to show buffer effects. For the sort/scan setting, we set the buffer size ranging from 16MB to 512MB, while keeping the S buffer to 128MB, recording the I/O between the buffer and the disk system. From Figure 4a we see that bigger buffer does improve the performance. But since we only gain in the external memory sorting part, a certain amount of I/Os is inevitable for each iteration. Note that the reason why iteration 1 has higher I/O cost is that in iteration 1 extra sorts on N t and E t are performed.
For the setting on S , we set the buffer size ranging from 16MB to 512MB, while keeping the sort/scan buffer to be 128MB, recording the I/O of the buffer to the disk system. From Figure 4b we also see that more buffer brings less I/O, as expected. However, in this case the buffer size change has a bigger impact on the I/O performance. This indicates that if we have a certain amount of memory space, it is more beneficial to allocate more memory to the S buffer than to the sort/scan buffer. Note that S buffer also shows quite high hit ratio during execution (more than 0.98 for DBPedia in all settings).
In order to measure how well the algorithm scales, we generate different size of SP2B datasets (edge count 1M, 5M, 10M, 50M, 100M, 500M), and measure the I/O and elapsed time for each dataset. In Figure 5 we see that the time spent on each edge is on the order of 10  X  5 and the I/O spent on each edge is under 4000 bytes (which is one typical disk page size). The algorithm X  X  performance scales (almost) linearly with the data size.
Edge updates are common operations for graph data. For our datasets, adding one edge means to add a link between two wiki pages (WikiLinks), to add more information to one publication or author (DBLP), to follow one more person (Twitter) and so on. Sometimes we would like to also add several edges together at once. So in this subsection we test the performance of Algorithm 2 ( Add Edges() ), first adding a single edge and then adding a set of edges.
To create the dataset for testing, we randomly take one edge from the edge set, perform Build Bisim() on the rest of the dataset, and apply Add Edges() on this edge. We believe the edge selection is more natural this way, since it take into account the distribution of edges among nodes. We repeat the experiment 10 times and take the average of the measured numbers. In Figure 6a we show how many nodes are checked for adding one edge to the graph in each iteration. In Figure 6b we show how many nodes actually change their partition IDs in each iteration. From the figures we see that the behavior varies for different datasets; graphs that have larger degrees tend to propagate more changes to later iterations, which complies with our intuition.
Since there is a chance that many nodes are changed but they may all belong to a certain set of partitions, we also examine how many partitions change their members in each iteration. We see that the behavior is closely related to that of Figure 6b.
After edge insertion, if there is no update algorithm available, the only choice to get the k -bisimulation partition is to execute the Build Bisim() from scratch on the new dataset. So this would be the baseline for the Add Edges() algorithm to compare. In the following we compare the overall I/O and time (Figure 7) of the two algorithms. We see that indeed the Add Edges() algorithm always achieves a better performance than using Build Bisim() to recompute the k -bisimulation partition result from scratch, with up to an order of magnitude improvement.
From the above experiments, we see that the performance of the algorithms are highly related to the datasets they process. For some datasets, the update algorithm is very much favorable while in other cases not so much. In the following, we would like to gain a better understanding of this phenomena.

We achieve this with two synthetic datasets, triggering both the extreme cases where the construction algorithm benefits the most and the update algorithm benefits the most. The first dataset, Dbest, shows a best-case scenario that the update algorithm can achieve relative to the construction algorithm. In this case we create a full k-ary tree, with edges pointing from parents to their children. When adding one edge to the tree, we add one edge to the leaf node, so that no node X  X  signature would change after the insertion. In this case the update algorithm does the least amount of work, without propagating any change to further iterations during execution. Figure 8a shows an example of Dbest, which is a binary tree with height 3. The dashed edge is the newly added edge.
The second dataset, Dworst, exhibits a worst-case scenario for the update algorithm, relative to construction. In this case we create a complete graph, with edges all labeled with x . Then when adding one more edge (labeled y ) to one of the nodes, every other node in each iteration is affected and therefore all the nodes X  signatures are changed. The update algorithm has to check all nodes in every iteration. Figure 8b shows an example of Dworst, a complete graph with 5 nodes. The dashed edge is the newly added edge.
We generate Dbest and Dworst on the scale of 100 million edges, and measure the elapsed time and I/O costs (Figure 9) for both the construction ( Build B isim() ) and edge update ( Add E dges() ) algorithms in each iteration. We see that indeed for Dbest, the update algorithm shows a 4 times speed-up in time compared with the construction algorithm. For Dworst, the update algorithm is 2 times slower in time than the construction algorithm.
To test the performance of multiple edges update, we randomly select a set of edges from the dataset (edge count 1, 10, 100, . . . , 1M), and apply the algorithm Add E dges() upon them, recording the I/O and elapsed time perfor-mances. In Figure 10, we show the I/O improvement ratio and time speed up ratio (both construct/update) for all cases (taking the average). A gray line is drawn at y = 1 for both figures to split the space to indicate whether Add E dges() performs better than Build B isim() or not. From the figure we see that for many of the datasets, it is beneficial to do batch update ( Add E dges() up until 10 4 edges. An order of magnitude time speed up is observed for Jamendo, LinkedMDB and DBLP. In fact, if we consider the time cost for Jamendo and DBLP, it is always favorable to use Add E dges() in all cases. For dataset DBPedia, however, changes propagate rapidly in the first few iterations, therefore the construction algorithm (
Build B isim() ) becomes a better choice when there are more than ten edges to be updated.
In this paper we have presented, to our knowledge, the first I/O efficient general-purpose algorithms for construct-ing and maintaining k -bisimulation partitions on massive disk-resident graphs. A theoretical analysis showed, and an extensive empirical study confirmed, that our algorithms are not only efficient and practical to use, but also scale well with the size of the data.

We close by listing a few promising research directions for further study. First, it would be interesting to explore adaptations and extensions of our algorithms for alternative hardware platforms (e.g., multicore, SSD). Second, as we indicated at various points, many alternative data structures and join algorithms can be investigated for optimizing various aspects of the proposed algorithms. Third, because of their bulk streaming-based nature, many aspects of our algorithms naturally lend themselves to state-of-the-art parallel and distributed computing frameworks such as MapReduce. Studying the possibilities for leveraging our solutions to further scale the performance of these frameworks on real world graphs is certainly an interesting research direction. Last but not least, the ideas developed in this paper provide a basis for investigating related problems such as computing and maintaining simulation partitions in external memory (e.g., [12]).
 Research Foundation Flanders (FWO) during her sabbatical visit to Hasselt University, Belgium. The research of YL, GF, JH and PD is supported by the Netherlands Organisation for Scientific Research (NWO).
