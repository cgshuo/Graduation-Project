 Stanford University, Stanford, CA 94305, USA Stanford University, Stanford, CA 94305, USA Network data, such as social networks of friends, cita-tion networks of documents, and hyper-linked networks of webpages, play an increasingly important role in modern machine learning applications. Analyzing network data provides useful predictive models for recommending new friends in social networks ( Backstrom &amp; Leskovec , 2011 ) or scientific papers in document networks ( Nallapati et al. , 2008 ; Chang &amp; Blei , 2009 ).
 Research on networks has focused on various mod-els of network link structure. Latent variable mod-els ( Airoldi et al. , 2007 ; Hoff et al. , 2002 ; Kemp et al. , 2006 ) decompose a network according to hidden pat-terns of connections between the nodes, while mod-els based on Kronecker products ( Leskovec et al. , 2010 ; Kim &amp; Leskovec , 2012a ; 2011a ) accurately model the global network structure. Though powerful, these models account only for the structure of the network, while ignor-ing observed features of the nodes. For example, in social networks users have profile information, and in document networks each node also contains the text of the document that it represents. Above models can find patterns which account for the connections between nodes, but they can-not account for the node features.
 Node features along with the links between them provide rich and complementary sources of information and should be used simultaneously for uncovering, understanding and exploiting the latent structure in the data. In this respect , we develop a network model that considers both the emergence of links of the network and the structure of node features such as user profile information or text of a document. Considering both sources of data, links and node features, leads to more powerful models than those that only con-sider links. For example, given a new node with a few of its links, traditional network models provide a predic-tive distribution of nodes to which it might be connected. However, to predict links of a node, our model does not need to see any of its links. It can predict links using only the node X  X  features. For example, we can suggest a user X  X  friendships based only on the profile information, and rec-ommend hyperlinks of a webpage based only on its tex-tual information. Moreover, given a new node and its links, our model also provides a predictive distribution of node features. This can be used to predict features of a node given its links or even predict missing or hidden features of a node given its links. For example, in our model interests of a user or keywords of a webpage can be predicted using only the connections of the network. Such predictions are out of reach for traditional models of networks.
 We develop a Latent Multi-group Membership Graph ( LMMG ) model of networks that explicitly ties nodes into groups of shared features and linking structure (Figure 1 ). Nodes belong to multiple latent groups and the occurrence of each node feature is determined by a logistic model based on the group memberships of the given node. Links of the network are then generated via link-affinity matri-ces. Each link-affinity matrix  X  tendencies, and an appropriate entry of  X  on whether or not a pair of nodes share the membership in group i . We derive efficient algorithms for model pa-rameter estimation and prediction. We study the perfor-mance of LMMG on real-world social and document net-works. We investigate the predictive performance on three different tasks: node feature prediction, link prediction , and supervised node classification. The LMMG provides sig-nificantly better performance on all three tasks than natura l alternatives and the current state of the art. The Latent Multi-group Membership Graph ( LMMG ) model is a model of a (directed or undirected) network and nodes which have categorical features. Our model contains two important ingredients for innovation (See Figure 1 ). First, the model assigns nodes to latent groups and allows nodes to belong to multiple groups at once. In contrast to multinomial models of group membership ( Airoldi et al. , 2007 ; Chang &amp; Blei , 2009 ) where the membership of a node is shared among the groups (the probability over group memberships of a node sums to 1), we model group memberships as a series of Bernoulli random variables (  X  in Figure 1 ), which indicates that nodes in our model can truly belong to multiple groups. Thus, in contrast to multi-nomial topic models, a higher probability of node member-ship to one group does not necessarily lower the probability of membership to some other group in the LMMG .
 Second, for modeling the links of the network, each group k has associated a link-affinity matrix (  X  in Figure 1 ). Each link-affinity matrix represents a table of link affinities gi ven that a pair of nodes belongs or does not belong to group k Thus, depending on the combination of the memberships of nodes to group k , an appropriate element of  X  sen. For example, the entry (0 , 0) of  X  affinity when none of the nodes belongs to group k , while (1 , 0) stores the link-affinity when first node belongs to the group (1) but the second does not (0). As we will later show that this allows for rich flexibility in modeling the links of the network as well as for uncovering and understanding the latent structure of the network.
 Next we formalize the LMMG model illustrated in Fig-ure 2 and describe it in a generative way. Formally, each node i = 1 , 2 ,  X   X   X  , N has a real-valued group membership  X  ik  X  [0 , 1] the probability that node i belongs to group k . Assuming the Beta distribution parameterized by  X  distribution of group membership  X  group assignment z Since each group membership z a node can belong to multiple groups simultaneously. The group memberships of a node affect both node features and its links. With respect to node features, we limit our focus to binary-valued features and use a logistic function to model the occurrence of node X  X  features based on the groups it belongs to. For each feature F 1 ,  X   X   X  , L ), we consider a separate logistic model where we regard group memberships  X  of the model. In this way, the logistic model represents the relevance of each group membership to the presence of a node feature. For convenience, we refer to the input vector of node i for the logistic model as  X  where  X  where w l -th node feature. The value of each w tribution of group k to the presence of node feature l . In order to model the links of the network, we build on the idea of the Multiplicative Attribute Graph (MAG) (a) Homophily (b) Heterophily (c) Core-periphery model ( Kim &amp; Leskovec , 2012a ). Here each latent group k has associated a link-affinity matrix  X  Each entry of the link-affinity matrix indicates a tendency of linking between a pair of nodes depending on whether they belong to the group k or not. In other words, given the group assignments z  X  X elects X  a row and z linking tendency from node i to node j is captured by  X  the groups, we define the link probability p uct of the link-affinities. Therefore, based on latent group assignments and link-affinity matrices, we determine each entry of the adjacency matrix A  X  { 0 , 1 } N  X  N of the net-work as follows: The parameter matrix  X  respect to the particular group k . The model offers flexibil-ity in a sense that we can represent many types of linking structures. In Figure 3 , by varying the link-affinity matrix, the model can capture heterophily (love of the different), homophily (love of the same), or core-periphery structure. This way the affinity matrix allows us to discover the ef-fects of node features on links of the network.
 The node features and the links of the network are con-nected via group memberships  X  that w the node i belongs to group k with high probability (  X  is close to 1 ), the feature l of node i , F to be 1 . By modeling group memberships using multiple Bernoulli random variables (instead of using multinomial achieve greater modeling flexibility which allows for mak-ing predictions about links given features and features given links. In Section 4 , we empirically demonstrate that the LMMG outperforms traditional models on these tasks. Moreover, if we divide the nodes of the network into two sets depending on the membership to group k , then we can discover how members of group k link to other members as well as non-members of group k , based on the structure of
 X  k . For example, when  X  k has large values on diagonal entries like in Figure 3 (a), members or non-members are likely to link among themselves, while there is low affinity for links between members and non-members. Figure 3 (b) captures exactly the opposite behavior where links are most likely between members and non-members. While the core-periphery structure is captured by link-affinity matr ix in Figure 3 (c) where nodes that share group memberships (the  X  X ore X ) are most likely to link, while nodes in the pe-riphery are least likely to link among themselves. We now turn our attention to LMMG model estimation. Given a set of binary node features F and the network A , we aim to find node group memberships  X  , parameters W of node feature model, and link-affinity matrices  X  . 3.1. Problem formulation When the node features F = { F 1 ,  X   X   X  , L } and the adjacency matrix A  X  { 0 , 1 } N  X  N given, we aim to find the group memberships  X  = {  X  i = 1 ,  X   X   X  , N, k = 1 ,  X   X   X  , K } , the logistic model param-eters W = { w the link-affinity matrices  X  = {  X  apply the maximum likelihood estimation, which finds the optimal values of  X  , W , and  X  so that they maximize the likelihood P ( F, A,  X  | W,  X  ,  X  ) , where  X  = { (  X  k = 1 ,  X   X   X  , K } represents hyper parameters for the Beta prior distributions. In the end, we aim to solve Now we compute the objective function in the above opti-mization problem. Since the LMMG independently gener-ates F and A given group memberships  X  , we decompose the log-likelihood log P ( F, A,  X  | W,  X  ,  X  ) as follows: log P ( F, A,  X  | W,  X  ,  X  ) = log P ( F |  X , W ) + log P ( A |  X ,  X ) + log P (  X  |  X  ) . Hence, to compute log P ( F, A,  X  | W,  X  ,  X  ) , we separately calculate each term of Equation ( 5 ). We obtain log P (  X  |  X  ) and log P ( F |  X , W ) from Equations ( 1 ) and ( 2 ): log P (  X  |  X  ) = X log P ( F |  X , W ) = X where y With regard to the second term in Equation ( 5 ), log P ( A |  X ,  X ) = log X for Z = { z that A is independent of  X  given Z . To exactly calculate log P ( A |  X ,  X ) , we sum P ( A | Z,  X ) P ( Z |  X  ) over every in-stance of Z given  X  and  X  . However, this requires the sum over 2 NK instances. As this exact computation is infeasi-ble, we approximate log P ( A |  X ,  X ) using its lower bound obtained by applying Jensen X  X  Inequality to Equation ( 6 ): Now that we are summing up over N 2 terms, the computation of the lower bound is feasible. We thus maximize the lower bound L of the log-likelihood log P ( A, F,  X  | W,  X  ,  X  ) . To sum up, we aim to maximize where L L
A = E Z  X   X  [log P ( A | Z, W )] regularize the objective function by the L1-norm of W . 3.2. Parameter estimation To solve the problem in Equation ( 8 ), we alternately up-date the group memberships  X  , the model parameters W , and  X  . Once  X  , W , and  X  are initialized, we first update the group memberships  X  to maximize L with fixing W and  X  . We then update the model parameters W and  X  to minimize the function (  X  X  +  X  | W | fixing  X  . Note that L is decomposed into L Therefore, when updating W and  X  given  X  , we separately maximize the corresponding log-likelihoods L We repeat this alternate updating procedure until the solu-tion converges. In the following we describe the details. Update of group memberships  X  . Now we focus on up-dating group memberships  X  given the model parameters W and  X  . We use the coordinate ascent algorithm which updates each membership  X  maximize the lower bound L . By computing the deriva-tives of L update each  X   X  L  X   X  L F  X  L A where F describe the details of Equation ( 9 ) in the full version of this paper ( Kim &amp; Leskovec , 2012b ). Hence, by adding up the group membership  X  for a given learning rate  X  by fixing the others, we can find the optimal group mem-berships  X  given the model parameters W and  X  .
 Update of node feature model parameters W . Now we update the parameters W of node feature model while keeping group memberships  X  fixed. Note that given the group memberships  X  the node feature model and the net-work model are independent of each other. Therefore, find-ing parameters W is identical to running the L1-regularized logistic regression given input  X  and output F data as we penalize the objective function in Equation ( 8 ) on the L1 value of model parameters W . We basically use the gradi-ent method to update W but make it sparse by applying the technique similar to LASSO : if 1 ,  X   X   X  , K and  X  ( K + 1) = 0 ( i.e. , we do not regularize on the intercepts).  X  if w lk crosses 0 while being updated, we assign 0 to w lk , same as in LASSO . By this procedure, we can update the node feature model parameters W to maximize the lower bound of log-likelihood L as well as to maintain the small number of relevant groups for each node feature.
 Update of network model parameters  X  . Next we fo-cus on updating network model parameters,  X  , when the group memberships  X  are fixed. Again, note that the net-work model is independent of the node feature model given the group memberships  X  , so we do not need to consider L or
L F . We thus update  X  to maximize L A given  X  using the gradient method.  X  for a constant learning rate  X  of in the full version of this paper ( Kim &amp; Leskovec , 2012b ). 3.3. Prediction With a fitted model, our ultimate goal is to make predic-tions about new data. In real-world application, the node features are often missing. Our approach is able to nicely handle such missing node features by fitting LMMG only to the observed features. In other words, when we update the group memberships  X  or the feature model parameters W by the gradient method from Equation ( 9 ) and ( 11 ), we only average the terms corresponding to the observed data. For example, when there is missing feature data, Equation ( 9 ) can be converted into as: for the observed data O .
 Similarly, for link prediction we modify the model estima-tion method as follows. While updating the node feature model parameters W based on the features of all the nodes including a new node, we estimate the network model pa-rameters  X  only on the observed network by holding out the new node. This way, the observed features naturally update the group memberships of a new node. We then predict the missing node features or network links by using the estimated group memberships and model parameters. Source code of our algorithms is available at http://snap.stanford.edu . First, we run various prediction tasks: missing node featur e prediction, missing link prediction, and supervised node classification. In all tasks our model outperforms natural baselines. Second, we qualitatively analyze the relation-ships between the node features and the network structure by a case study of a Facebook ego-network and show how the LMMG identifies useful and interpretable latent struc-tures.
 Datasets. For our experiments, we used the following datasets containing networks and node features.  X  AddHealth (AH): School friendship network (458  X  Egonet (EGO): Facebook ego-network of a particular  X  Facebook100 (FB): Facebook network of Cal- X  WebKB (WKB): Hyperlinks between computer sci-We binarized discrete valued features ( e.g. school year) based on whether the feature value is greater than the median value. For the non-binary categorical features ( e.g. major), we used an indicator variable for each pos-sible feature value. Some of these datasets are available at http://snap.stanford.edu .
 Predictive tasks. We investigate the predictive perfor-mance of the LMMG based on three different tasks. We visualize them in Figure 4 . Note that the column represents either features or nodes depending on the type of the task. For each matrix, given 0/1 values in the white area, we pre-dict the values of the entries with question marks. First, assuming that all node features of a given node are com-pletely missing, we predict all the features based on the links of the node (Figure 4 (a)). Second, when all the links of a given node are missing, we predict the missing links by using the node feature information (Figure 4 (b)). Last, we assume only few features of a node are missing and perform supervised classification of a specific node feature given al l the other node features and the network (Figure 4 (c)). Baseline models. Next we introduce natural baseline as well as the state-of-the-art methods. First, for the most basic baseline, when predicting some missing value (node feature or link) of a given node, we average the correspond-ing values of all the other nodes and regard it as the prob-ability of value 1 . We refer to this algorithm as AVG. Sec-ond, as we can view each of the three prediction tasks as the classification task, we use Collective Classification (CC) algorithms that exploit both node features and network de-pendencies ( Sen et al. , 2008 ). For the local classifier of CC algorithms, we use Naive-Bayes (CC-N) as well as logistic regression (CC-L). We also compare the LMMG to the state of the art, Relational Topic Model (RTM) ( Chang &amp; Blei , 2009 ). We give further details about these models in the full version of this paper ( Kim &amp; Leskovec , 2012b ). Task 1: Predicting missing node features. First, we ex-amine the task of predicting missing features of a node where features of other nodes and all the links are observed. We randomly select a node and remove all the feature val-ues of that node and then try to recover them. We quantify the performance by using the log-likelihood of the true fea-ture values over the estimated distributions as well as the predictive accuracy (the probability of correctly predict ing the missing features) of each method.
 Table 1 shows the results of the experiments by measur-ing the average of log-likelihood (LL) and prediction accu-racy (ACC) for each algorithm and each dataset. We notice that LMMG model exhibits the best performance in the log-likelihood for all datasets. While CC-L in general performs the second best, our model outperforms it by up to 23%. The performance gain over the other models in terms of ac-curacy seems smaller when compared to the log-likelihood. However, LMMG model still predicts the missing node fea-tures with the highest accuracy on all the datasets. In particular, the LMMG exhibits the most improvement in node feature prediction on the ego-network dataset (30% in LL and 7% in ACC) over the next best method. Here node features are derived by manually labeling community memberships of each person in the ego-network. Thus, a group of people in the network intrinsically share some node feature value (community membership). In this sense, the node features and the links in the ego-network are di-rectly related to each other and our model successfully ex-ploits this relationship to predict missing node features. Task 2: Predicting missing links. Second, we also con-sider the task of predicting missing links of a specific node while the features of the node are given. Similarly to the previous task, we select a node at random, but here we re-move all its links while observing its features. We then aim to recover the missing links. For evaluation, we use the log-likelihood (LL) of missing links as well as the area under the ROC curve (AUC) of missing link prediction.
 We give the experimental results for each dataset in Ta-ble 2 . Again, the LMMG outperforms the baseline mod-els in the log-likelihood except for the Facebook100 data. Interestingly, while RTM was relatively competitive when predicting missing features, it tends to fail predicting mi ss-ing links, which implies that the flexibility of link-affinit y matrices is needed for accurate modeling of the links. We observe that Collective Classification methods look competetive in some performance metrics and datasets. For example, CC-N gives good results in terms of classifica-tion accuracy, and CC-L performs well in terms of the log-likelihood. As CC-N is a discriminative model, it does not perform well in missing link probability estimation. How-ever, the LMMG is a generative model that produces a joint probability of node features and network links, so it is also very good at estimating missing links. Hence, in overall, the LMMG nicely exploits the relationship between the net-work structure and node features to predict missing links. Task 3: Supervised node classification. Finally, we examine the performance on the supervised classification task. In many cases, we aim to classify entities (nodes) based on their feature values under the supervised setting. Here the relationships (links) between the entities are als o provided. For this experiment, we hold out one feature of nodes as the output class, regarding all other features of nodes and the network as input data. We divide the nodes into a 70% training and 30% test set. Similarly, we mea-sure the average of the log-likelihood (LL) as well as the average classification accuracy (ACC) on the test set. We illustrate the performance of various models in Table 3 . The LMMG model performs better than the other mod-els in both the log-likelihood and the classification accu-racy. It improves the performance by up to 20% in the log-likelihood and 5% in the classification accuracy. We also notice that exploiting the relationship between node features and the global network structure improves the per-formance of supervised node classification compared to the models focusing on local network dependencies ( e.g. , Col-lective Classification methods).
 Case study: Analysis of a Facebook ego-network. Last we qualitatively analyze the Facebook ego-network exam-ple to provide insights into the relationship between node features and network structure. By investigating model pa-rameters ( W and  X  ), we can find not only what features are important for each group but also how each group affects the link structure.
 We begin by introducing the ego-user which we used to create a network between his Facebook friends. We asked our user to label each of his friends with a number of la-bels. He chose to use 14 different labels. They corre-spond to his high school (HS), undergraduate university (U
NIVERSITY ), math olympiad camp (C AMP ), computer programming club (KP ROG ) and work place (KC OMP ) friends. The user also assigned labels to identify friends from his graduate program (CS) and university (ST), bas-ketball (B ASKETBALL ) and squash (S QUASH ) clubs, as well as travel mates (T RAVEL ), summer internship buddies (I NTERN ), family (F AMILY ) and age group (A GE ). We fit the LMMG to the ego-network and each friend X  X  la-bels, and obtained the model parameters W and  X  . We set the number of latent groups to 5 since the previous predic-tion tasks worked well when K = 5 . In Table 4 , for each of 5 latent groups, we represent the top 3 features with the largest absolute value of model parameter | w responding link-affinity matrices  X  We begin by investigating the first group. The top three la-bels the most correlated to the first group are ST, A GE , and I
NTERN . However, notice that I NTERN is negatively corre-lated. This means that group 1 contains students from the same graduate school and age, but not people with whom our user worked together at the summer internship (even though they may be of the same school/age). We also note that  X  that summer interns, who met our Facebook user neither because of shared graduate school nor because of age, form a group within which people are densely connected. On the other hand, people of the same age at the same university also exhibit homophily, but are less densely connected with each other. Such variation in link density that depends on the group memberships agrees with our intuition. Those who worked at the same company actively interact with each other so almost everyone is linked in Facebook. How-ever, as the group of people of the same university or age group is large and each pair of people in that group does not necessarily know each other, the link affinity in this group is naturally smaller than in the intern X  X  group.
 Similarly, groups 2 and 3 form the two sports groups (B
ASKETBALL , S QUASH ). People are connected densely within each of the groups, but less connected to the out-side of the groups. Furthermore, we notice that those who graduated from the same high school (HS) as well as the same undergraduate school (U NIVERSITY ) form another community but the membership to high school is more im-portant than to the undergraduate university (8.7 vs. 2.3). Last, for groups 4 and 5, we note that the corresponding link-affinity matrices are nearly flat ( i.e. values are nearly uniform). This implies that groups 4 and 5 are related to general node features. In this sense, we hypothesize that features like CS, family, math camp, and the company, have relatively little effect on the network structure. The LMMG builds on previous research in machine learn-ing and network analysis. Many models have been de-veloped to explain network link structure ( Airoldi et al. , 2007 ; Hoff et al. , 2002 ; Kemp et al. , 2006 ; Leskovec et al. , 2010 ) and extensions that incorporate node features have also been proposed ( Getoor et al. , 2001 ; Kim &amp; Leskovec , 2011b ; Taskar et al. , 2003 ). However, these models do not consider latent groups and thus cannot provide the benefits of dimensionality reduction or produce interpretable clus -ters useful for understanding network community structure . The LMMG provides meaningful clustering of nodes and their features in the network. Network models of similar flavor have been proposed in the past ( Airoldi et al. , 2007 ; Hoff et al. , 2002 ; Kemp et al. , 2006 ), and some even incor-porate node features ( Chang &amp; Blei , 2009 ; Nallapati et al. , 2008 ; Miller et al. , 2009 ). However, such models have been mainly developed for document networks where they assume multinomial topic distributions for each word in the document. We extend this by learning a logistic model of occurrence of each feature based on node group member-ships. We highlight this difference to previous models. For topic memberships. previous models use multinomial dis-tributions, where a node has a mass of 1 to split among var-ious topics. In contrast, in the LMMG , a node can belong to multiple topics at once without any constraint. While previous work tends to explore only the network or only the features, the LMMG jointly models both so that it can make predictions of one given the other. The LMMG models the interaction between links and group member-ships via link-affinity matrices which provide great flexibi l-ity and interpretability of obtained groups and interactio ns. The LMMG is a new probabilistic model of links and nodes in networks. It can be used for link prediction, node fea-ture prediction and supervised node classification. We have demonstrated qualitatively and quantitatively that t he LMMG proves useful for analyzing network data. The LMMG significantly improves on previous models, inte-grating both node-specific information and link structure to give better predictions.
 Myunghwan Kim was supported by the Kwanjeong Ed-ucational Foundation fellowship. This research has been supported in part by NSF CNS-1010921, IIS-1016909, IIS-1149837, IIS-1159679, Albert Yu &amp; Mary Bechmann Foundation, Boeing, Allyes, Samsung, Yahoo, Alfred P. Sloan Fellowship and the Microsoft Faculty Fellowship. Airoldi, E. M., Blei, D. M., Fienberg, S. E., and Xing, E. P.
Mixed membership stochastic blockmodels. JMLR , 9: 1981 X 2014, 2007.
 Backstrom, L. and Leskovec, J. Supervised random walks: Predicting and recommending links in social networks. In WSDM , 2011.
 Bearman, P. S., Jones, J., and Udry, J. R. The national lon-gitudinal study of adolescent health: Research design. http://www.cpc.unc.edu/addhealth , 1997.
 Chang, J. and Blei, D. M. Relational topic models for doc-ument networks. In AISTATS , 2009.
 Craven, M., DiPasquo, D., Freitag, D., McCallum, A.,
Mitchell, T., Nigam, K., and Slattery, S. Learning to extract symbolic knowledge from the world wide web. In AAAI  X 98 , 1998.
 Getoor, L., Segal, E., Taskar, B., and Koller, D. Probabilis -tic models of text and link structure for hypertext classi-fication. In IJCAI Workshop on Text Learning: Beyond Supervision , 2001.
 Hoff, P., Raftery, A., and Handcock, M. Latent space approaches to social network analysis. Journal of the American Statistical Association , 97:1090 X 1098, 2002. Kemp, C., Tenebaum, J. B., and Griffiths, T. L. Learning systems of concepts with an infinite relational model. In AAAI  X 06 , 2006.
 Kim, M. and Leskovec, J. Network completion problem:
Inferring missing nodes and edges in networks. In SDM , 2011a.
 Kim, M. and Leskovec, J. Modeling social networks with node attributes using the multiplicative attribute graph model. In UAI , 2011b.
 Kim, M. and Leskovec, J. Multiplicative attribute graph model of real-world networks. Internet Mathematics , 8 (1-2):113 X 160, 2012a.
 Kim, M. and Leskovec, J. Latent multi-group membership graph model. arXiv:1205.4546v1 , 2012b.
 Leskovec, J., Chakrabarti, D., Kleinberg, J., Faloutsos, C ., and Ghahramani, Z. Kronecker Graphs: An Approach to Modeling Networks. JMLR , 11:985 X 1042, 2010.
 Miller, K. T., Griffiths, T. L., and Jordan, M. I. Nonpara-metric latent feature models for link prediction. In NIPS  X 09 , 2009.
 Nallapati, R., Ahmed, A., Xing, E., and Cohen, W. W. Joint latent topic models for text and citations. In KDD , 2008. Sen, P., Namata, G., Bilgic, M., Getoor, L., Gallagher, B., and Eliassi-rad, T. Collective classification in network data. AI Magazine , 29(3), 2008.
 Taskar, B., Wong, M. F., Abbeel, P., and Koller, D. Link prediction in relational data. In NIPS , 2003.
 Traud, Amanda L., Mucha, Peter J., and Porter, Ma-son A. Social structure of facebook networks.
