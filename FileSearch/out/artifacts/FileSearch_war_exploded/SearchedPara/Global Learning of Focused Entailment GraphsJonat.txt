 The Textual Entailment (TE) paradigm (Dagan et al., 2009) is a generic framework for applied se-mantic inference. The objective of TE is to recog-nize whether a target meaning can be inferred from a given text. For example, a Question Answer-ing system has to recognize that  X  X lcohol affects blood pressure X  is inferred from  X  X lcohol reduces blood pressure X  to answer the question  X  X hat af-fects blood pressure? X 
TE systems require extensive knowledge of en-tailment patterns, often captured as entailment rules: rules that specify a directional inference re-lation between two text fragments (when the rule is bidirectional this is known as paraphrasing). An important type of entailment rule refers to propo-sitional templates , i.e., propositions comprising a predicate and arguments, possibly replaced by variables. The rule required for the previous ex-ample would be  X  X  reduce Y  X  X affect Y X  . Be-cause facts and knowledge are mostly expressed by propositions, such entailment rules are central to the TE task. This has led to active research on broad-scale acquisition of entailment rules for predicates, e.g. (Lin and Pantel, 2001; Sekine, 2005; Szpektor and Dagan, 2008).

Previous work has focused on learning each en-tailment rule in isolation. However, it is clear that there are interactions between rules. A prominent example is that entailment is a transitive relation, and thus the rules  X  X  X  Y  X  and  X  Y  X  Z  X  imply the rule  X  X  X  Z  X . In this paper we take advantage of these global interactions to improve entailment rule learning.

First, we describe a structure termed an entail-ment graph that models entailment relations be-tween propositional templates (Section 3). Next, we show that we can present propositions accord-ing to an entailment hierarchy derived from the graph, and suggest a novel hierarchical presenta-tion scheme for corpus propositions referring to a target concept. As in this application each graph focuses on a single concept, we term those focused entailment graphs (Section 4).

In the core section of the paper, we present an algorithm that uses a global approach to learn the entailment relations of focused entailment graphs (Section 5). We define a global function and look for the graph that maximizes that function under a transitivity constraint. The optimization prob-lem is formulated as an Integer Linear Program (ILP) and solved with an ILP solver. We show that this leads to an optimal solution with respect to the global function, and demonstrate that the algo-rithm outperforms methods that utilize only local information by more than 10%, as well as meth-ods that employ a greedy optimization algorithm rather than an ILP solver (Section 6). Entailment learning Two information types have primarily been utilized to learn entailment rules between predicates: lexicographic resources and distributional similarity resources. Lexicographic resources are manually-prepared knowledge bases containing information about semantic relations between lexical items. WordNet (Fellbaum, 1998), by far the most widely used resource, spec-ifies relations such as hyponymy , derivation , and entailment that can be used for semantic inference (Budanitsky and Hirst, 2006). WordNet has also been exploited to automatically generate a training set for a hyponym classifier (Snow et al., 2005), and we make a similar use of WordNet in Section 5.1.

Lexicographic resources are accurate but tend to have low coverage. Therefore, distributional similarity is used to learn broad-scale resources. Distributional similarity algorithms predict a se-mantic relation between two predicates by com-paring the arguments with which they occur. Quite a few methods have been suggested (Lin and Pan-tel, 2001; Bhagat et al., 2007; Yates and Etzioni, 2009), which differ in terms of the specifics of the ways in which predicates are represented, the fea-tures that are extracted, and the function used to compute feature vector similarity. Details on such methods are given in Section 5.1.

Global learning It is natural to describe en-tailment relations between predicates by a graph. Nodes represent predicates, and edges represent entailment between nodes. Nevertheless, using a graph for global learning of entailment between predicates has attracted little attention. Recently, Szpektor and Dagan (2009) presented the resource Argument-mapped WordNet , providing entailment relations for predicates in WordNet. Their re-source was built on top of WordNet, and makes simple use of WordNet X  X  global graph structure: new rules are suggested by transitively chaining graph edges, and verified against corpus statistics.
The most similar work to ours is Snow et al. X  X  al-gorithm for taxonomy induction (2006). Snow et al. X  X  algorithm learns the hyponymy relation, un-der the constraint that it is a transitive relation . Their algorithm incrementally adds hyponyms to an existing taxonomy (WordNet), using a greedy search algorithm that adds at each step the set of hyponyms that maximize the probability of the ev-idence while respecting the transitivity constraint.
In this paper we tackle a similar problem of learning a transitive relation, but we use linear pro-gramming. A Linear Program (LP) is an optimiza-tion problem, where a linear function is minimized (or maximized) under linear constraints. If the variables are integers, the problem is termed an In-teger Linear Program (ILP) . Linear programming has attracted attention recently in several fields of NLP, such as semantic role labeling, summariza-tion and parsing (Roth and tau Yih, 2005; Clarke and Lapata, 2008; Martins et al., 2009). In this paper we formulate the entailment graph learning problem as an Integer Linear Program, and find that this leads to an optimal solution with respect to the target function in our experiment. This section presents an entailment graph struc-ture, which resembles the graph in (Szpektor and Dagan, 2009).

The nodes of an entailment graph are propo-sitional templates . A propositional template is a path in a dependency tree between two arguments of a common predicate 1 (Lin and Pantel, 2001; Szpektor and Dagan, 2008). Note that in a de-pendency parse, such a path passes through the predicate. We require that a variable appears in at least one of the argument positions, and that each sense of a polysemous predicate corresponds to a separate template (and a separate graph node): X  X  X  X  X  treat#1 obj  X  X  X  X  Y and X subj  X  X  X  X  treat#1 obj  X  X  X  X  nau-sea are propositional templates for the first sense of the predicate treat . An edge ( u,v ) represents the fact that template u entails template v . Note that the entailment relation transcends beyond hy-ponymy. For example, the template X is diagnosed with asthma entails the template X suffers from asthma , although one is not a hyponoym of the other. An example of an entailment graph is given in Figure 1, left.

Since entailment is a transitive relation, an en-tailment graph is transitive , i.e., if the edges ( u,v ) and ( v,w ) are in the graph, so is the edge ( u,w ) . This is why we require that nodes be sense-specified, as otherwise transitivity does not hold: Possibly a  X  b for one sense of b , b  X  c for an-other sense of b , but a 9 c .

Because graph nodes represent propositions, which generally have a clear truth value, we can assume that transitivity is indeed maintained along paths of any length in an entailment graph, as en-tailment between each pair of nodes either occurs or doesn X  X  occur with very high probability. We support this further in section 4.1, where we show that in our experimental setting the length of paths in the entailment graph is relatively small.
Transitivity implies that in each strong connec-tivity component 2 of the graph, all nodes are syn-onymous. Moreover, if we merge every strong connectivity component to a single node, the graph becomes a Directed Acyclic Graph (DAG), and the graph nodes can be sorted and presented hierarchically. Next, we show an application that leverages this property. In this section we propose an application that pro-vides a hierarchical view of propositions extracted from a corpus, based on an entailment graph.
Organizing information in large collections has been found to be useful for effective information access (Kaki, 2005; Stoica et al., 2007). It allows for easier data exploration, and provides a compact view of the underlying content. A simple form of structural presentation is by a single hierarchy, e.g. (Hofmann, 1999). A more complex approach is hierarchical faceted metadata, where a number of concept hierarchies are created, corresponding to different facets or dimensions (Stoica et al., 2007).
Hierarchical faceted metadata categorizes con-cepts of a domain in several dimensions, but does not specify the relations between them. For ex-ample, in the health-care domain we might have facets for categories such as diseases and symp-toms . Thus, when querying about nausea , one might find it is related to vomitting and chicken pox , but not that chicken pox is a cause of nausea, while nausea is often accompanied by vomitting.
We suggest that the prominent information in a text lies in the propositions it contains, which specify particular relations between the concepts. Propositions have been mostly pre-sented through unstructured textual summaries or manually-constructed ontologies, which are ex-pensive to build. We propose using the entail-ment graph structure, which describes entailment relations between predicates, to naturally present propositions hierarchically. That is, the entailment hierarchy can be used as an additional facet, which can improve navigation and provide a compact hi-erarchical summary of the propositions.

Figure 1 illustrates a scenario, on which we evaluate later our learning algorithm. Assume a user would like to retrieve information about a tar-get concept such as nausea . We can extract the set of propositions where nausea is an argument auto-matically from a corprus, and learn an entailment graph over propositional templates derived from the extracted propositions, as illustrated in Figure 1, left. Then, we follow the steps in the process described in Section 3: merge synonymous nodes that are in the same strong connectivity compo-nent, and turn the resulting DAG into a predicate hierarchy, which we can then use to present the propositions (Figure 1, right). Note that in all propositional templates one argument is the tar-get concept (nausea), and the other is a variable whose corpus instantiations can be presented ac-cording to another hierarchy (e.g. Nabilone and Lorazepam are types of drugs ).

Moreover, new propositions are inferred from the graph by transitivity. For example, from the proposition  X  X elaxation reduces nausea X  we can in-fer the proposition  X  X elaxation helps with nausea X  . 4.1 Focused entailment graphs The application presented above generates entail-ment graphs of a specific form: (1) Propositional templates have exactly one argument instantiated by the same entity (e.g. nausea). (2) The predicate sense is unspecified, but due to the rather small number of nodes and the instantiating argument, each predicate corresponds to a unique sense.
Generalizing this notion, we define a focused entailment graph to be an entailment graph where the number of nodes is relatively small (and con-sequently paths in the graph are short), and predi-cates have a single sense (so transitivity is main-tained without sense specification). Section 5 presents an algorithm that given the set of nodes of a focused entailment graph learns its edges, i.e., the entailment relations between all pairs of nodes. The algorithm is evaluated in Section 6 using our proposed application. For brevity, from now on the term entailment graph will stand for focused entailment graph . In this section we present an algorithm for learn-ing the edges of an entailment graph given its set of nodes. The first step is preprocessing: We use a large corpus and WordNet to train an entail-ment classifier that estimates the likelihood that one propositional template entails another. Next, we can learn on the fly for any input graph: given the graph nodes, we employ a global optimiza-tion approach that determines the set of edges that maximizes the probability (or score) of the entire graph, given the edge probabilities (or scores) sup-plied by the entailment classifier and the graph constraints (transitivity and others). 5.1 Training an entailment classifier We describe a procedure for learning an entail-ment classifier, given a corpus and a lexicographic resource (WordNet). First, we extract a large set of propositional templates from the corpus. Next, we represent each pair of propositional templates with a feature vector of various distributional similar-ity scores. Last, we use WordNet to automatically generate a training set and train a classifier.
Template extraction We parse the corpus with a dependency parser and extract all propositional templates from every parse tree, employing the procedure used by Lin and Pantel (2001). How-ever, we only consider templates containing a predicate term and arguments 3 . The arguments are replaced with variables, resulting in propositional templates such as X subj  X  X  X  X  affect obj  X  X  X  X  Y .
Distributional similarity representation We aim to train a classifier that for an input template pair ( t 1 ,t 2 ) determines whether t 1 entails t 2 . A template pair is represented by a feature vector where each coordinate is a different distributional similarity score. There are a myriad of distribu-tional similarity algorithms. We briefly describe those used in this paper, obtained through varia-tions along the following dimensions:
Predicate representation Most algorithms mea-sure the similarity between templates with two variables ( binary templates ) such as X subj  X  X  X  X  af-fect obj  X  X  X  X  Y (Lin and Pantel, 2001; Bhagat et al., 2007; Yates and Etzioni, 2009). Szpketor and Da-gan (2008) suggested learning over templates with one variable ( unary templates ) such as X subj  X  X  X  X  af-fect , and using them to estimate a score for binary templates.

Feature representation The features of a tem-plate are some representation of the terms that in-stantiated the argument variables in a corpus. Two representations are used in our experiment (see Section 6). Another variant occurs when using bi-nary templates: a template may be represented by a pair of feature vectors, one for each variable (Lin and Pantel, 2001), or by a single vector, where fea-tures represent pairs of instantiations (Szpektor et al., 2004; Yates and Etzioni, 2009). The former variant reduces sparsity problems, while Yates and Etzioni showed the latter is more informative and performs favorably on their data.

Similarity function We consider two similarity functions: The Lin (2001) similarity measure, and the Balanced Inclusion (BInc) similarity measure (Szpektor and Dagan, 2008). The former is a symmetric measure and the latter is asymmetric. Therefore, information about the direction of en-tailment is provided by the BInc measure.

We then generate for any ( t 1 ,t 2 ) features that are the 12 distributional similarity scores using all combinations of the dimensions. This is reminis-cent of Connor and Roth (2007), who used the out-put of unsupervised classifiers as features for a su-pervised classifier in a verb disambiguation task. Training set generation Following the spirit of Snow et al. (2005), WordNet is used to automati-cally generate a training set of positive (entailing) and negative (non-entailing) template pairs. Let T be the set of propositional templates extracted from the corpus. For each t i  X  T with two vari-ables and a single predicate word w , we extract from WordNet the set H of direct hypernyms and synonyms of w . For every h  X  H , we generate a new template t j from t i by replacing w with h . If t  X  T , we consider ( t i ,t j ) to be a positive exam-ple. Negative examples are generated analogously, by looking at direct co-hyponyms of w instead of hypernyms and synonyms. This follows the no-tion of  X  X ontrastive estimation X  (Smith and Eisner, 2005), since we generate negative examples that are semantically similar to positive examples and thus focus the classifier X  X  attention on identifying the boundary between the classes. Last, we filter training examples for which all features are zero, and sample an equal number of positive and neg-ative examples (for which we compute similarity features), since classifiers tend to perform poorly on the minority class when trained on imbalanced data (Van Hulse et al., 2007; Nikulin, 2008). 5.2 Global learning of edges Once the entailment classifier is trained we learn the graph edges given its nodes. This is equiv-alent to learning all entailment relations between all propositional template pairs for that graph.
To learn edges we consider global constraints, which allow only certain graph topologies. Since we seek a global solution under transitivity and other constraints, linear programming is a natural choice, enabling the use of state of the art opti-mization packages. We describe two formulations of integer linear programs that learn the edges: one maximizing a global score function, and another maximizing a global probability function.

Let I uv be an indicator denoting the event that node u entails node v . Our goal is to learn the edges E over a set of nodes V . We start by formu-lating the constraints and then the target functions.
The first constraint is that the graph must re-spect transitivity. Our formulation is equivalent to the one suggested by Finkel and Manning (2008) in a coreference resolution task:
In addition, for a few pairs of nodes we have strong evidence that one does not entail the other and so we add the constraint I uv = 0 . Combined with the constraint of transitivity this implies that there must be no path from u to v . This is done in the following two scenarios: (1) When two nodes u and v are identical except for a pair of words w u and w v , and w u is an antonym of w v , or a hyper-nym of w v at distance  X  2 . (2) When two nodes u and v are transitive opposites, that is, if u = for any word w 4 .

Score-based target function We assume an en-tailment classifier estimating a positive score S uv if it believes I uv = 1 and a negative score other-wise (for example, an SVM classifier). We look for a graph G that maximizes the sum of scores over the edges: where  X  | E | is a regularization term reflecting the fact that edges are sparse. Note that this con-stant needs to be optimized on a development set.
Probabilistic target function Let F uv be the features for the pair of nodes ( u,v ) and F =  X  u 6 = v F uv . We assume an entailment classifier es-timating the probability of an edge given its fea-tures: P uv = P ( I uv = 1 | F uv ) . We look for the graph G that maximizes the posterior probability P ( G | F ) :
Following Snow et al., we make two inde-pendence assumptions: First, we assume each set of features F uv is independent of other sets of features given the graph G , i.e., P ( F | G ) = Q u 6 = v P ( F uv | G ) . Second, we assume the features for the pair ( u,v ) are generated by a distribution depending only on whether entailment holds for ( u,v ) . Thus, P ( F uv | G ) = P ( F uv | I uv ) . Last, for simplicity we assume edges are independent and the prior probability of a graph is a product of the prior probabilities of the edge indicators: P ( G ) = Q u 6 = v P ( I uv ) . Note that although we assume edges are independent, dependency is still expressed using the transitivity constraint. We ex-press P ( G | F ) using the assumptions above and Bayes rule:
Note that the prior P ( F uv ) is constant with re-spect to the graph. Now we look for the graph that maximizes log P ( G | F ) :  X  G = argmax ) = argmax = argmax (in the last transition we omit the constant P u 6 = v log(1  X  P uv ) ). Importantly, while the score-based formulation contains a parameter  X  that re-quires optimization, this probabilistic formulation is parameter free and does not utilize a develop-ment set at all.

Since the variables are binary, both formula-tions are integer linear programs with O ( | V | 2 ) variables and O ( | V | 3 ) transitivity constraints that can be solved using standard ILP packages.

Our work resembles Snow et al. X  X  in that both try to learn graph edges given a transitivity con-straint. However, there are two key differences in the model and in the optimization algorithm. First, Snow et al. X  X  model attempts to determine the graph that maximizes the likelihood P ( F | G ) and not the posterior P ( G | F ) . Therefore, their model contains an edge prior P ( I uv ) that has to be estimated, whereas in our model it cancels out. Second, they incrementally add hyponyms to a large taxonomy (WordNet) and therefore utilize a greedy algorithm, while we simultaneously learn all edges of a rather small graph and employ in-teger linear programming, which is more sound theoretically, and as shown in Section 6, leads to an optimal solution. Nevertheless, Snow et al. X  X  model can also be formulated as a linear program with the following target function: Note that if the prior inverse odds k = this is equivalent to our probabilistic formulation. We implemented Snow et al X  X  model and optimiza-tion algorithm and in Section 6.3 we compare our model and optimization algorithm to theirs. This section presents our evaluation, which is geared for the application proposed in Section 4. 6.1 Experimental setting A health-care corpus of 632MB was harvested from the web and parsed with the Minipar parser (Lin, 1998). The corpus contains 2,307,585 sentences and almost 50 million word tokens. We used the Unified Medical Language System (UMLS) 5 to annotate medical concepts in the cor-pus. The UMLS is a database that maps nat-ural language phrases to over one million con-cept identifiers in the health-care domain (termed CUIs). We annotated all nouns and noun phrases that are in the UMLS with their possibly multi-ple CUIs. We extracted all propositional templates from the corpus, where both argument instantia-tions are medical concepts, i.e., annotated with a CUI (  X  50,000 templates). When computing dis-tributional similarity scores, a template is repre-sented as a feature vector of the CUIs that instan-tiate its arguments.

To evaluate the performance of our algo-rithm, we constructed 23 gold standard entailment graphs. First, 23 medical concepts, representing typical topics of interest in the medical domain, were manually selected from a list of the most fre-quent concepts in the corpus. For each concept, nodes were defined by extracting all propositional templates for which the target concept instanti-ated an argument at least K (= 3) times (average number of graph nodes=22.04, std=3.66, max=26, min=13).

Ten medical students constructed the gold stan-dard of graph edges. Each concept graph was annotated by two students. Following RTE-5 practice (Bentivogli et al., 2009), after initial an-notation the two students met for a reconcili-ation phase. They worked to reach an agree-ment on differences and corrected their graphs. Inter-annotator agreement was calculated using the Kappa statistic (Siegel and Castellan, 1988) both before (  X  = 0 . 59 ) and after (  X  = 0 . 9 ) rec-onciliation. 882 edges were included in the 23 graphs out of a possible 10,364, providing a suf-ficiently large data set. The graphs were randomly split into a development set (11 graphs) and a test set (12 graphs) 6 . The entailment graph fragment in Figure 1 is from the gold standard.

The graphs learned by our algorithm were eval-uated by two measures, one evaluating the graph directly, and the other motivated by our applica-tion: (1) F 1 of the learned edges compared to the gold standard edges (2) Our application provides a summary of propositions extracted from the cor-pus. Note that we infer new propositions by prop-agating inference transitively through the graph. Thus, we compute F 1 for the set of propositions inferred from the learned graph, compared to the set inferred based on the gold standard graph. For example, given the proposition from the corpus  X  X elaxation reduces nausea X  and the edge  X  X  re-duce nausea  X  X help with nausea X  , we evaluate the set {  X  X elaxation reduces nausea X ,  X  X elaxation helps with nausea X  } . The final score for an algo-rithm is a macro-average over the 12 graphs of the test set. 6.2 Evaluated algorithms Local algorithms We described 12 distributional similarity measures computed over our corpus (Section 5.1). For each measure we computed for each template t a list of templates most similar to t (or entailing t for directional measures). In ad-dition, we obtained similarity lists learned by Lin and Pantel (2001), and replicated 3 similarity mea-sures learned by Szpektor and Dagan (2008), over the RCV1 corpus 7 . For each distributional similar-ity measure (altogether 16 measures), we learned a graph by inserting any edge ( u,v ) , when u is in the top K templates most similar to v . We also omit-ted edges for which there was strong evidence that they do not exist, as specified by the constraints in Section 5.2. Another local resource was Word-Net where we inserted an edge ( u,v ) when v was a direct hypernym or synonym of u . For all algo-rithms, we added all edges inferred by transitivity.
Global algorithms We experimented with all 6 combinations of the following two dimensions: (1) Target functions: score-based, probabilistic and Snow et al. X  X  (2) Optimization algorithms: Snow et al. X  X  greedy algorithm and a standard ILP solver. A training set of 20,144 examples was au-tomatically generated, each example represented by 16 features using the distributional similarity measures mentioned above. SVMperf (Joachims, 2005) was used to train an SVM classifier yield-ing S uv , and the SMO classifier from WEKA (Hall et al., 2009) estimated P uv . We used the lpsolve 8 package to solve the linear programs. In all re-sults, the relaxation  X  u,v 0  X  I uv  X  1 was used, which guarantees an optimal output solution. In all experiments the output solution was integer, and therefore it is optimal. Constructing graph nodes and learning its edges given an input con-cept took 2-3 seconds on a standard desktop. 6.3 Results and analysis Table 1 summarizes the results of the algorithms. The left half depicts methods where the develop-ment set was needed to tune parameters, and the right half depicts methods that do not require a (manually created) development set at all. Hence, our score-based LP ( tuned-LP) , where the param-eter  X  is tuned, is on the left, and the probabilis-tic LP ( untuned-LP ) is on the right. The row Greedy is achieved by using the greedy algorithm instead of lpsolve . The row Local-LP is achieved by omitting global transitivity constraints, making the algorithm completely local. We omit Snow et al. X  X  formulation, since the optimal prior inverse odds k was almost exactly 1, which conflates with untuned-LP.

The rows Local 1 and Local 2 present the best distributional similarity resources. Local 1 is achieved using binary templates, the Lin function, and a single vector with feature pairs. Local 2 is identical but employs the BInc function. Local  X  1 and Local  X  2 also exploit the local constraints men-tioned above. Results on the left were achieved by optimizing the top-K parameter on the devel-opment set, and on the right by optimizing on the training set automatically generated from Word-Net.

The global methods clearly outperform local methods: Tuned-LP outperforms significantly all local methods that require a development set both on the edges F 1 measure (p &lt; .05) and on the propositions F 1 measure (p &lt; .01) 9 . The untuned-LP algorithm also significantly outperforms all lo-cal methods that do not require a development set on the edges F 1 measure (p &lt; .05) and on the propositions F 1 measure (p &lt; .01). Omitting the global transitivity constraints decreases perfor-mance, as shown by Local-LP . Last, local meth-ods are sensitive to parameter tuning and in the absence of a development set their performance dramatically deteriorates.

To further establish the merits of global algo-rithms, we compare (Table 2) tuned-LP, the best global algorithm, with Local  X  1 , the best local al-gorithm. The table considers all edges where the two algorithms disagree, and counts how many are in the gold standard and how many are not. Clearly, tuned-LP is superior at avoiding wrong edges (false positives). This is because tuned-LP refrains from adding edges that subsequently induce many undesirable edges through transitiv-ity. Figures 2 and 3 illustrate this by compar-ing tuned-LP and Local  X  1 on a subgraph of the Headache concept, before adding missing edges to satisfy transitivity to Local  X  1 . Note that Local  X  inserts a single wrong edge X-report-headache  X  X-prevent-headache , which leads to adding 8 more wrong edges. This is the type of global considera-tion that is addressed in an ILP formulation, but is ignored in a local approach and often overlooked when employing a greedy algorithm. Figure 2 also illustrates the utility of a local entailment graph for information presentation. Presenting information according to this subgraph distinguishes between propositions dealing with headache treatments and propositions dealing with headache risk groups.
Comparing our use of an ILP algorithm to the greedy one reveals that tuned-LP significantly outperforms its greedy counterpart on both mea-sures (p &lt; .01). However, untuned-LP is practically equivalent to its greedy counterpart. This indicates that in this experiment the greedy algorithm pro-vides a good approximation for the optimal solu-tion achieved by our LP formulation.

Last, when comparing WordNet to local distri-butional similarity methods, we observe low recall and high precision, as expected. However, global methods achieve much higher recall than WordNet while maintaining comparable precision.

The results clearly demonstrate that a global ap-proach improves performance on the entailment graph learning task, and the overall advantage of employing an ILP solver rather than a greedy al-gorithm. This paper presented a global optimization algo-rithm for learning entailment relations between predicates represented as propositional templates. We modeled the problem as a graph learning prob-lem, and searched for the best graph under a global transitivity constraint. We used Integer Linear Programming to solve the optimization problem, which is theoretically sound, and demonstrated empirically that this method outperforms local al-gorithms as well as a greedy optimization algo-rithm on the graph learning task.

Currently, we are investigating a generalization of our probabilistic formulation that includes a prior on the edges, and the relation of this prior to the regularization term introduced in our score-based formulation. In future work, we would like to learn general entailment graphs over a large number of nodes. This will introduce a challenge to our current optimization algorithm due to com-plexity issues, and will require careful handling of predicate ambiguity. Additionally, we will inves-tigate novel features for the entailment classifier. This paper used distributional similarity, but other sources of information are likely to improve per-formance further.
 We would like to thank Roy Bar-Haim, David Carmel and the anonymous reviewers for their useful comments. We also thank Dafna Berant and the nine students who prepared the gold stan-dard data set. This work was developed under the collaboration of FBK-irst/University of Haifa and was partially supported by the Israel Science Foundation grant 1112/08. The first author is grateful to the Azrieli Foundation for the award of an Azrieli Fellowship, and has carried out this re-search in partial fulllment of the requirements for the Ph.D. degree.

