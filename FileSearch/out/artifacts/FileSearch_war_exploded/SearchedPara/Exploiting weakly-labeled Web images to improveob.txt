 The last few years have seen a proliferation of human efforts to collect labeled image data sets for the purpose of training and evaluating visual recogniti on systems. Label information in these collections comes in different forms, ranging from simple o bject category labels to detailed semantic pixel-level segmentations. Examples include Caltech256 [ 14], and the Pascal VOC2010 data set [7]. In order to increase the variety and the number of labeled obj ect classes, a few authors have designed online games and appealing software tools encouraging comm on users to participate in these image annotation efforts [23, 30]. Despite the tremendous resear ch contribution brought by such attempts, one order of magnitude smaller than the number of object cate gories that humans can recognize [3]. several authors have proposed systems that learn from weakl y-labeled Internet photos [10, 9, 29, 20]. Most of these approaches rely on keyword-based image search engines to retrieve image examples of specified object classes. Unfortunately, while image sea rch engines provide training examples loosely related with the query concept. Most prior work has a ttempted to address this problem by means of outlier rejection mechanisms discarding irrele vant images from the retrieved results. However, despite the dynamic research activity in this area , weakly-supervised approaches today clean data (see, e.g., results reported in [9, 29]).
 In this paper we argue that the poor performance of models lea rned from weakly-labeled Internet data is not only due to undetected outliers contaminating th e training data, but it is also a conse-quence of the statistical differences often present betwee n Web images and the test data. Figure 1 shows sample images for some of the Caltech256 object catego ries versus the top six images re-present in the Bing sets, the striking difference between th e two collections is that even the relevant results in the Bing groups appear to be visually less homogen eous. For example, in the case of the classes shown in figure 1(a,b), while the Caltech256 groups c ontain only real photographs, the Bing counterparts include several cartoon drawings. In figure 1( c,d), each Caltech256 image contains only the object of interest while the pictures retrieved by B ing include extraneous items, such as classifiers on Caltech256, given that  X  X aces X  and  X  X eople X  a re separate categories in the data set). Furthermore, even when  X  X rrelevant X  results do occur in the retrieved images, they are rarely outliers detectable via simple coherence tests as there is often some consistency even among such photos. For example, polysemy  X  the capacity of one word to have multi ple meanings  X  causes multiple a type of sea turtle, while in the case of (f) the keyword  X  X ric ycle X  retrieves images of both bicycles as well as motorcycles with three wheels; note, again, that C altech256 contains for both classes only images corresponding to one of the words meanings and that  X  X  otorcycle X  appears as a separate additional category). Finally, in some situations, differ ent shooting distances or angles may produce completely unrelated views of the same object or scene: for e xample, the Bing set in 1(g) includes both aerial and ground views of Mars, which have very little i n common visually.
 images to be used for training until we compare them to the pho tos in the corresponding Caltech256 categories. In this paper we show that a few strongly-labele d examples from the test domain (e.g. a few Caltech256 images for the class of interest) are indeed s ufficient to disambiguate this relevancy problem and to model the distribution differences between t he weakly-labeled Internet data and the test application data, so as to significantly improve recogn ition performance on the test set. The situation where the test data is drawn from a distributio n that is related, but not identical, to the distribution of the training data has been widely studie d in the field of machine learning and it is traditionally addressed using so-called  X  X omain adapta tion X  methods. These techniques exploit ample availability of training data from a source domain to learn a model that works effectively in a related target domain for which only few training examples are available. More for mally, let Here, X denotes the input (a random feature vector) and Y the class (a discrete random variable). The domain adaptation problem arises whenever p t ( X, Y ) differs from p s ( X, Y ) . In covariance shift, it is assumed that only the distributions of the input features differ in the two domain, i.e., sification in the target domain since a model learned from a la rge source training set will be trained to perform well in the dense source regions of X which, under the covariance shift assumption, will generally be different from the dense regions of the tar get domain. Typically, covariance shift much more common and challenging case is when the conditiona l distributions are different, i.e., analyzing data in the source domain may still yield valuable information to perform prediction for test target data. This is precisely the scenario considered in this paper. (a) (b) (c) (d) (e) (f) (g) Figure 1: Images in Caltech256 for several categories and to p results retrieved by Bing image search for the corresponding keywords. The Bing sets are both seman tically and visually less coherent: presence of multiple objects in the same image, polysemy, ca ricaturization, as well as variations in viewpoints are some of the visual effects present in Interne t images which cause significant data distribution differences between the Bing sets and the corr esponding Caltech256 groups. Most of the prior work on learning visual models from image se arch has focused on the task of learned from image search were used to rerank photos on the ba sis of visual consistency. Subsequent approaches [2, 25, 20] have employed similar outlier reject ion schemes to automatically construct clean(er) data sets of images for training and testing objec t classifiers. Even techniques aimed at learning explicit object classifiers from image search [9, 2 9] have identified outlier removal as the key-ingredient to improve recognition. In our paper we focu s on another fundamental, yet largely ignored, aspect of the problem: we argue that the current poo r performance of classification models learned from the Web is due to the distribution differences b etween Internet photos and image test examples. To the best of our knowledge we propose the first sys tematic empirical analysis of domain adaptation methods to address sample distribution differe nces in object categorization due to the use of weakly-labeled Web images as training data. We note that i n work concurrent to our own, Saenko et al. [24] have also analyzed cross-domain adaptation of ob ject classifiers. However, their work studio setups) and by images taken with different camera typ es (a digital SLR versus a webcam). Transfer learning, also known as multi-task learning, is re lated to domain adaptation. In computer vision, transfer learning has been applied to a wide range of problems including object categorization there is a single distribution of the inputs p ( X ) but there are multiple output variables Y is assumed that some relations exist among the tasks; for exa mple, some common structure when learning classifiers p ( Y  X  , . . . ,  X  T are generated from a shared prior p (  X  ) . The fundamental difference is that in domain adaptation we have a single task but different domains, i.e. , different sources of data. As our approach relies on a mix of labeled and weakly-labeled images, it is loosely related to semi-supervised methods for object classification [15, 19]. With in this genre, the algorithm described in [11] is perhaps the closest to our work as it also relies on w eakly-labeled Internet images. How-ever, unlike our approach, these semi-supervised methods a re designed to work in cases where the test examples and the training data are generated from the sa me distribution. 3.1 Experimental setup Our objective is to evaluate domain adaptations methods on t he task of object classification, using photos from a human-labeled data set as target domain exampl es and images retrieved by a keyword-based image search engine as examples of the source domain.
 We used Caltech256 as the data set for the target domain since it is an established benchmark for object categorization and it contains a large number of clas ses (256) thus allowing us to average out performance variations due to especially easy or difficult c ategories. From each class, we randomly sampled n We formed the weakly-labeled source data by collecting the t op n age search for each of the Caltech256 category text labels. A lthough it may have been possible to improve the relevancy of the image results for some of the cla sses by manually selecting less am-biguous search keywords, we chose to issue queries on the unc hanged Caltech256 text class labels to avoid subjective alteration of the results. However, in o rder to ensure valid testing, we removed near duplicates of Caltech256 images from the source traini ng set by a human-supervised process. 3.2 Feature representation and classification model In order to study the effect of large weakly-labeled trainin g sets on object recognition performance, we need a baseline system that achieves good performance on o bject categorization and that supports efficient learning and test evaluation. The current best pub lished results on Caltech256 were obtained by a kernel combination classifier using 39 different featur e kernels, one for each feature type [13]. However, since both training as well testing are computatio nally very expensive with this classifier, this model is unsuitable for our needs. Instead, in this work we use as image representation the clas seme features recently proposed by Torresani et al. [28]. This descriptor is particularly suit able for our task as it has been shown to yield near state-of-the-art results with simple linear sup port vector machines, which can be learned very efficiently even for large training sets. The descripto r measures the closeness of an image to a basis set of classes and can be used as an intermediate rep resentation to learn classifiers for new classes. The basis classifiers of the classeme descripto r are learned from weakly-labeled data vector, in this work we removed from the descriptor 34 attrib utes, corresponding to categories related to Caltech256 classes. We use a binarized version of this des criptor obtained by thresholding to 0 the output of the attribute classifiers: this yields for each image a 2625-dimensional binary vector describing the predicted presence/absence of visual attri butes in the photo. This binarization has been shown to yield very little degradation in recognition p erformance (see [28] for further details). We denote with f ( x )  X  X  0 , 1 } F the binary attribute vector extracted from image x with F = 2625 . Object class recognition is traditionally formulated as a m ulticlass classification problem: given a number of possible classes (in the case of Caltech256, K = 256 ). In this paper we implement multi-class classification using K binary classifiers trained using the one-versus-the-rest scheme and perform prediction according to the winner-take-all strategy. The k -th binary classifier (distin-guishing between class k and the other classes) is trained on a target training set D t ing images of all classes, using the data from the k -th class as positive examples and the data from the remaining classes as negative examples, i.e. D t the feature vector of the i -th image, N labeled data set, and y t set D s the k -th class as keyword. As discussed in the next section, diffe rent methods will make different assumptions on the labels of the source examples.
 We adopt a linear SVM as the model for the binary one-vs-the-r est classifiers. This choice is pri-marily motivated by the availability of several simple yet e ffective domain adaptation variants of SVM [5, 26], in addition to the aforementioned reasons of goo d performance and efficiency. We now present the specific domain adaptation SVM algorithms . For brevity, we drop the subscript k indicating dependence on the specific class. The hyperparam eters C of all classifiers are selected we cope with the largely unequal number of positive and negat ive examples by normalizing the cost entries in the loss function by the respective class sizes. 4.1 Baselines: SVM s , SVM t , SVM s  X  t We include in our evaluation three algorithms not based on domain adaptation and use them as comparative baselines. We indicate with SVM t a linear SVM learned exclusively from the target examples. SVM s denotes an SVM learned from the source examples using the one -versus-the-rest scheme and assuming no outliers are present in the image sear ch results. SVM s  X  t is a linear SVM trained on the union of the target and source examples. Speci fically, for each class k , we train a binary SVM on the data obtained by merging D t to contain only positive examples, i.e., no outliers. The hy perparameter C is kept the same for all K binary classifiers but tuned distinctly for each of the three methods by selecting the hyperparameter value yielding the best multiclass performance on the targe t training set (we used hold out validation on D t 4.2 Mixture of source and target hypotheses: MIXSVM One of the simplest possible strategies for domain adaptati on consists of using as final classifier a convex combination of the two SVM hypotheses learned indepe ndently from the source and target data. Despite its simplicity, this classifier has been shown to yield good empirical results [26]. Let us represent the source and target multiclass hypothese s as vector-valued functions h s ( f )  X  R
K , h t ( f )  X  R K , where the k -th outputs are the respective SVM scores for class k . MIXSVM to the largest output, i.e. k  X  = arg max via grid search by optimizing multiclass error on the target training set. We avoid biased estimates resulting from learning the hypothesis h t and  X  on the same training set by applying a two-stage procedure: we learn 5 distinct hypotheses h t using 5-fold cross validation (with the hyperpameter value found for SVM t ) and compute prediction h t ( f t validation hypothesis that was not trained on that example; we then use these predicted outputs to determine the optimal  X  . Last, we learn the final hypothesis h t using the entire target training set. 4.3 Domain weighting: DWSVM Another straightforward yet popular domain adaptation app roach is to train a classifier using both the source and the target examples by weighting differently the two domains in the learning objec-tive [5, 12, 4]. We follow the implementation proposed in [26 ] and weight the loss function values differently for the source and target examples by using two d istinct SVM hyperparameters, C C , encoding the relative importance of the two domains. The va lues of these hyperparameters are selected by minimizing the multiclass 5-fold cross validat ion error on the target training set. 4.4 Feature augmentation: AUGSVM We denote with AUGSVM the domain adaptation method described in [5]. The key-idea of this approach is to create a feature-augmented version of each in dividual example f , where distinct where 0 indicates a F -dimensional vector of zeros. A linear SVM is then trained on the union of the feature-augmented source and target examples (using a s ingle hyperparameter). The principle behind this mapping is that the SVM trained in the feature-au gmented space has the ability to distin-guish features having common behavior in the two domains (as sociated to the first F SVM weights) from features having different properties in the two domain s. 4.5 Transductive learning: TSVM The previous methods implement different strategies to adj ust the relative importance of the source and the training examples in the learning process. However, all these techniques assume that the source data is fully and correctly labeled. Unfortunately, in our practical problem this assumption is violated due to outliers and irrelevant results being pre sent in the images retrieved by keyword search. To tackle this problem we propose to perform transdu ctive inference on the label of the training data to simultaneously determine the correct labe ls of the source training examples and incorporate this labeling information to improve the class ifier. To address this task we employ the transductive SVM model introduced in [17]. Although this me thod is traditionally used to infer the labels of unlabeled data available at learning time, it o utputs a proper inductive hypothesis and therefore can be used also to predict labels of unseen test ex amples. The problem of learning a transductive SVM in our context can be formulated as follows : where l () denotes the loss function, w is the vector of SVM weights, y s contains the labels of the source examples, and the c t number of positive and negative examples: we set c t wise. The scalar parameter  X  defines the fraction of source examples that we expect to be po sitive and is tuned via cross validation. Note that TSVM solves jointly for the separating hyperplane and the labels of the source examples by trading off maximizatio n of the margin and minimization of the Figure 2: Recognition accuracy obtained with n s = 300 Web photos and a varying number of Caltech256 target training examples. prediction errors on both source and target data. This optim ization can be interpreted as implement-ing the cluster assumption, i.e., the expectation that poin ts in a data cluster have the same label. We be the square of the hinge loss) using the minimization algor ithm proposed in [27], which computes an efficient primal solution using the modified finite Newton m ethod of [18]. This minimization approach is ideally suited to large-scale sparse data sets s uch as ours (about 70% of our features are and selected them by minimizing the multiclass cross valida tion error. We also tried letting  X  vary for each individual class but that led to slightly inferior r esults, possibly due to overfitting. We now present the experimental results. Figure 2 shows the a ccuracy achieved by the different algorithms when using n s = 300 and a varying number of training target examples ( n t ). The accuracy is measured as the average of the mean recognition r ate per class, using m t = 25 test examples for each class. The best accuracy is achieved by the domain adaptation methods TSVM and DWSVM , which produce significant improvements over the SVM traine d using only target examples ( SVM t ), particularly for small values of n t . For n t = 5 , TSVM yields a 65% improvement over the best published results on this benchmark (for the same numbe r of examples, an accuracy of 16 . 7% is reported in [13]). Our method achieves this performance by a nalyzing additional images, the Internet photos, but since these are collected automatically and do n ot require any human supervision, the gain we achieve is effectively  X  X uman-cost free X . It is inte resting to note that while using solely source training images yields very low accuracy ( 14 . 5% for SVM s ), adding even just a single labeled target image produces a significant improvement ( TSVM achieves 18 . 5% accuracy with n t = 1 , and 27 . 1% with n t = 5 ): this indicates that the method can indeed adapt the classi fier to work effectively on the target domain given a small amount of stro ngly-labeled data. It is interesting to note that while TSVM implements a form of outlier rejection as it solves for the la bels of the source examples, DWSVM assumes that all source images in D s DWSVM achieves results similar to those of TSVM : this suggests that domain adaptation rather than outlier rejection is the key-factor contributing to the imp rovement with respect to the baselines. By analyzing the performance of the baselines in figure 2 we ob serve that training exclusively with Web images ( SVM s ) yields much lower accuracy than using strongly-labeled da ta ( SVM t ): this is consistent with prior work [9, 29]. Furthermore, the poor ac curacy of SVM s  X  t compared to SVM t suggests that na  X  X vely adding a large number of source examp les to the target training set without consideration of the domain differences not only does not he lp but actually worsens the recognition. Figure 3 illustrates the significant manual annotation savi ng produced by our approach: the x -axis is the number of target labeled images provided to TSVM while the y -axis shows the number of additional labeled examples that would be needed by SVM t to achieve the same accuracy. Figure 4: Classification accuracy of the differ-ent methods using n t = 10 target training im-ages and a varying number of source examples. The setting n s = 300 in the results above was chosen by studying the recognition a ccuracy as a function of the number of source examples: we carried out an experiment where we fixed the number n t of target training example for each category to an intermedi ate value ( n t = 10 ), and varied the number n s of top image results used as source training examples for eac h class. Figure 4 summarizes the results. We notice that the performance of th e SVM trained only on source images ( SVM s ) peaks at n s = 100 and decreases monotonically after this value. This result c an be explained by observing that image search engines provide images sorte d according to estimated relevancy with respect to the keyword. It is conceivable to assume that imag es far down in the ranking list will often tend to be outliers, which may lead to degradation of recogni tion particularly for non-robust models. Despite this, we see that the domain adaptation methods TSVM and DWSVM exhibit a monotonically non-decreasing accuracy as n s grows: this indicates that these methods are highly robust t o outliers and can make effective use of source data even when increasin g n s causes a likely decrease of the fraction of inliers and relevant results. Contrast these ro bust performances with the accuracy of SVM s  X  t , which grows as we begin adding source examples but then deca ys rapidly after n s = 10 and approaches the poor recognition of SVM s for large values of n s .
 Our approach compares very favorably with competing algori thms also in terms of computational complexity: training TSVM (without cross validation) on Caltech256 with n t = 5 and n s = 300 takes 84 minutes on a AMD Opteron Processor 280 2.4GHz; train ing the multiclass method of [13] using 5 labeled examples per class takes about 23 hours on the same machine (for fairness of com-parison, we excluded cross validation even for this method) . A detailed analysis of training time as a function of the number of labeled training examples is repor ted in figure 5. Evaluation of our model on a test example takes 0.18ms, while the method of [13] requi res 37ms. In this work we have investigated the application of domain a daptation methods to object categoriza-tion using Web photos as source data. Our analysis indicates that, while object classifiers learned exclusively from Web data are inferior to fully-supervised models, the use of domain adaptation methods to combine Web photos with small amounts of strongly labeled data leads to state-of-the-art results. The proposed strategy should be particularly u seful in scenarios where labeled data is scarce or expensive to acquire. Future work will include app lication of our approach to combine data from multiple source domains (e.g., images obtained fr om different search engines or photo sharing sites) and different media (e.g., text and video). A dditional material including software and our source training data may be obtained from [1].
 Acknowledgments We are grateful to Andrew Fitzgibbon and Martin Szummer for d iscussion. We thank Vikas Sind-hwani for providing code. This research was funded in part by NSF CAREER award IIS-0952943.
