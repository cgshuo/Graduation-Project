 We present novel, efficient, model based kernels for time series data rooted in the reservoir computation framework. The kernels are implemented by fitting reservoir models shar-ing the same fixed deterministically constructed state tran-sition part to individual time series. The proposed kernels can naturally handle time series of different length without the need to specify a parametric model class for the time series. Compared with most time series kernels, our kernels are computationally efficient. We show how the model dis-tances used in the kernel can be calculated analytically or efficiently estimated. The experimental results on synthetic and benchmark time series classification tasks confirm the efficiency of the proposed kernel in terms of both generaliza-tion accuracy and computational speed. This paper also in-vestigates on-line reservoir kernel construction for extremely long time series.
 G.3 [ Probability And Statistics ]: Time series analysis; H.2.8 [ Database Applications ]: Data mining Time Series, Reservoir Computing, Kernel Methods
Kernel methods have received considerable attention in the machine learning community dealing with structured data, such as image, graphs, texts or voice signals. How-ever, as an important ubiquitous data type in science and engineering, time series have received relatively less research in the kernel literature [5].

There has been active research on quantification of the  X  X imilarity X  or the  X  X istance X  between time series. However, these measures are not always applicable for kernel approaches as many of such similarity measures are not positive definite, which is a necessary basis for the reproducing kernel Hilbert space to be valid.

A simple way to distinguish between two time series of the same length is to treat the time series as vectors and simply employ a linear kernel or Radial basis kernel. This method can be simple and efficient provided the time series are short and of equal length. However, in many real-world applications, the time series of interest are of variable-length and can be quite long. It is therefore desirable to construct kernels capable of handling possibly long time series of vari-able length. For example, in dynamic time warping [1] time series similarity is quantified through finding an alignment between variable-length multivariate time series.
Another possibility is to use a generative probabilistic model of the time series data and then define the time series kernel through model parameters corresponding to different sequences, e.g. probability product kernel [13], Kullback-Leibler (KL) divergence based kernels [19, 2] and Autore-gressive kernel [5]. These approaches depend on the partic-ular parametric model class. For example, Fisher kernel [11] maps individual time series into score functions of the single generative model that is assumed to be able to  X  X xplain X  most of the data. Often a Hidden Markov Model (HMM) with a fixed number of states is employed. In some situations the assumption of the particular generative model underlying the data can be too strong. In addition, Fisher kernels [11] are computationally expensive because of the calculation of metric tensor (inverse of Fisher information matrix) in the tangent space of the generative model manifold. The  X  X rac-tical X  Fisher kernel used in most of the time replaces the metric tensor with an identity matrix. This can result in a loss of valuable information in the data [25].

The requirement of using a single generative model in ker-nel calculations is relaxed e.g. in the Autoregressive kernel [5]. Sequences are judged to be similar/dissimilar according to the corresponding likelihood profile of a Vector Autore-gressive Model (VAR) under a variety of parameter settings (controlled by the prior). In this case it is less crucial that the VAR model is a faithful model of the data since the base VAR model class is used as a  X  X eature extractor X .
Due to the requirements of many time series applications, the kernel evaluation should happen in real-time. Therefore, computational complexity of kernel construction and eval-uation can play a critical role in applying kernel methods to time series data. However, many of the existing time se-ries kernels are computationally demanding. For example Auto-correlation Operators (DACO) kernel [9] proposed re-cently by Gaidon et al. for action recognition, compares the d ynamic aspects of two time series by using the difference between their auto-correlations. The kernelized DACO in-evitably needs to invert a matrix of size related to the time series length. Thus the kernel can be used for relatively short time series only.

To address the problems mentioned above, we propose novel general time series kernels that can naturally and effi-ciently handle long time series data of variable length. The core idea is to transform the time series into a higher dimen-sional  X  X ynamical feature space X  via reservoir computation models [18] and then represent varying aspects of the signal through variation in the linear readout models trained in such dynamical feature spaces. In this way each time series will be represented by the corresponding readout model of the same xed reservoir. Hence, unlike in the Fisher ker-nel, there will be a different dynamic model for each time series, but all such models will share the same dynamical reservoir. The sequence-specific dynamic models will differ only in the corresponding linear readout models from the reservoir. The intuition is that while the general fixed dy-namic reservoir provides a unique and rich pool of dynamic features for the whole data set, the individual readout mod-els bring enough flexibility to represent specifics of different time series, thus providing a platform for wide applicability across time series of different characteristics and origins.
One can, of course, argue that our approach is yet another variation on model-based kernel construction for time series based on a particular class of dynamic (reservoir) models. However, unlike parametric time series models of a partic-ular from, reservoir models have been extensively shown to be  X  X eneric X  in the sense that they are able to represent a wide variety of dynamical features of the input signals, so that given a task at hand only the linear readout on top of the reservoir needs to be retrained [18]. As stated above, in our formulation, the underlying dynamic reservoir will be the same for all time series -the differences in the signal characteristics in different time series will be captured solely by the linear readout models and will be quantified in the function space of such models.

There are several advantages of such reservoir based time series kernels: 1. The proposed kernels can naturally handle time series 2. General reservoir model is flexible enough so that it 3. Since only the linear readout on top of the reservoir 4. With recursive least squares algorithm to train read-5. Under some assumptions, the model distances between
The rest of this paper is organized as follows. Section 2 reviews the related work on kernels for time series. Section 3 introduces deterministic reservoir computing and proposes time series kernels based on reservoir models. The experi-mental results and analysis are reported in Section 4. Sec-tion 5 studies on-line reservoir kernel construction for ex-tremely long time series. Finally, Section 6 discusses and concludes the paper.
In this section, we will review some of the related work on time series kernels.

Dynamic time warping (DTW) tries to  X  X arp X  the time axis of one (or both) sequences to achieve a better align-ment [1]. DTW has been successfully used in many applica-tions. However, DTW can generate un-intuitive alignments by mapping a single point on one time series onto a large subsection of another time series, leading to inferior results [15]. A time series kernel based on global alignment, moti-vated by DTW, has been proposed in [7], with an efficient version presented in [6].

Autoregressive kernel [5] is another probabilistic kernel for time series. In autoregressive kernel, VAR model class of a given order is used to generate an infinite family of features from the time series. For a given time series s , the likelihood profile p  X  ( s ) across all possible parameter setting (under a matrix normal-inverse Wishart prior  X  ( )) forms a representation of s . Given two time series s i and s j kernel is defined as the dot product of the corresponding sequence representations:
Fisher kernel [11] was proposed to combine the power of generative modelling with discriminant classifiers such as Support Vector Machines. It has been successfully used in numerous applications. Fisher kernel assumes that the gen-erative model p ( s | ) can explain all the data. The Fisher kernel maps each individual data into a vector in the gradi-ent log-likelihood space specified by this generative model. The feature vector ( Fisher score) U s is the gradient of the log-likelihood of the generative model (fit on the data set) for the time series s : The Fisher kernel is then defined as follows: where I is the Fisher information matrix. As mentioned above, calculation of I 1 can be computationally expensive. A routinely used practical  X  X rick X  is to use the identity matrix in place of I [23], which speeds up the computation at the cost of losing some important information [23].
In this paper we will introduce new time series kernels based on a general  X  X emporal filter X  implemented by Echo State Network (ESN) with a simple deterministically con-structed reservoir architecture. Reservoir models [18] have been extensively shown to be able to successfully process and model time series of a surprisingly wide variety of types (from deeper memory deterministic chaotic systems, to shorter memory stochastic sequences) [22, 24].

The ESN reservoir model with N reservoir (state) units represents a parameterized input driven state space model Fi gure 1: Illustration of the time series kernel in the deterministic reservoir model space. The first stage is to train the readout mapping of reservoir models using time series, i.e. generate individual points in the model space to represent time series. The sec-ond stage is to construct the kernel by investigating the model distance. formulated as: of reservoir activations, s ( t ) is the input time series element at time t , R is the reservoir weight matrix ( N  X  N ), V is the input weight matrix ( N  X  O ) to the reservoir, O is the dimensionality of the time series, g (  X  ) is element-wise appli-cation of the tanh transfer function and y ( t ) is the output of the linear readout from the reservoir. The state transition and output parts of the state space model are described by eqs. (1) and (2), respectively.

The main idea is that, provided the reservoir is able to represent a rich set of features of the input time series, the model-based representation of a particular time series s will be given by the linear readout mapping f ( x ) (eq. (2)) op-erating on reservoir activations x , specifically fitted to s on the next-item prediction task y ( t )  X  s ( t + 1) by minimiz-ing the normalized mean square error (NMSE) between the model predictions y ( t ) and targets s ( t + 1). We will denote the readout f ( x ) fitted to sequence s by h ( x ; s ).
The kernel between a pair of time series s i and s j will then be calculated using a model distance d ( h ( x ; s i ) , h ( x ; s tween the corresponding readouts h ( x ; s i ) and h ( x ; s the same  X  X ixed X  general reservoir (eq. (1)).
This paper will focus on specific forms of ESN since they constitute one of the simplest, yet effective forms of RC. ESN has a  X  X on-trainable X  recurrent part ( X  X he reservoir X ) (eq (1)) and a simple linear readout (eq (2)). Typically, the reservoir weights R and the input weights V to the reservoir are randomly generated so that the  X  X cho State Property X  is satisfied. Loosely speaking, this means that the reservoir output would be independent of the initial conditions [12]. Training of ESN can be efficiently performed through linear regression. For more details we refer the interested reader to e.g. [18].

The downside of reservoir models is that their construc-tion is largely driven by a series of randomized model build-ing stages. Recently, Rodan et al. [22] proposed to use a simple deterministic constructed Cycle Reservoirs with regu-lar Jumps (CRJ). This reservoir architecture has been shown to be comparable (or better) than the traditional ESN on a wide variety of time series modeling and prediction tasks [22]. In CRJ the reservoir nodes are connected in a uni-directional cycle with bi-directional shortcuts (jumps) (Fig-ure 1). All cyclic reservoir weights r c have the same value; all jumps r j share the same weight and the input connec-tions r i have the same absolute value with an aperiodic sign pattern. This results in a sparse and deterministically con-structed and simple coupling reservoir weight matrix R .
The reservoir forms a xed non-linear high-dimensional non-autonomous dynamical system with fading memory that acts as a general temporal filter on top of which it is usually sufficient to train a linear readout mapping. As mentioned above, it is natural to represent individual time series by the linear readouts from the fixed dynamic filter that fits the series well.
Using Euclidean metric on the readout parameters to cal-culate the distance between two readout mappings is not satisfying since one should be interested in the model dis-tance in the function space of the readout models, rather than the distance between the model parameterizations.
We will use the L 2 distance in the model space [4], al-though our framework is general and can be applied to any appropriate function distance between the readouts. To sim-plify the notation, we will denote the readout h ( x ; s i to sequence s i by f i ( x ). Consider two mappings f 1 ( x ) and f ( x ), f 1 , f 2 :  X  N  X   X  O , where N is the number of reservoir units, O is the output dimensionality. Their L 2 distance is defined as: where  X  ( x ) is the probability density function on the input (reservoir) domain C . Recall that in (1) we use tanh transfer function and so C = [  X  1 , +1] N .

We will first assume that x is uniformly distributed. Later we will relax this assumption by considering non-uniform  X  ( x ) to reflect the fact that state space activations x in the reservoir can follow a more complex distribution.
The readout model takes the form of an affine mapping: where x = [ x 1 ,  X   X   X  , x N ] T is the state vector, W is the pa-rameter matrix ( O  X  N ) and a = [ a 1 ,  X   X   X  , a o ] T is the bias vector.
Con sider two readouts from the same reservoir Then, where W = W 1  X  W 2 , and a = a 1  X  a 2 .

Note that since C = [  X  1 , 1] N , for any fixed a and W Therefore, it can be shown that where w T i is the i -th row of W , w i,j is the ( i, j )-th element of W .

Scaling of the squared model distance ( L 2 2 ( f 1 , f 2 we obtain wh ich differs from the squared Euclidean distance on the readout parameters by the factor 1 / 3 applied to the differences in the linear part W of the affine readouts. Hence, more importance is given to the  X  X ffset X  than  X  X rientation X  of the readout mapping.
In the above, we assumed that the distribution of reser-voir states x is uniform in C . As mentioned before, it is likely that the state distribution  X  ( x ) will be non-uniform. We will introduce two approaches for allowing general  X  ( x ) -modelling of  X  by a mixture of Gaussians and numerical approximation of the integral (3) by sampling using boot-strapped input series.
 For non-uniform state distribution  X  ( x ), a K -component Gaussian mixture model can be employed to approximate the distribution: where  X  k are mixture coefficients with Then, the distance L 2 ( f 1 , f 2 ) can be obtained as follows:
L 2 ( f 1 , f 2 ) = According to [20] (page 42), for a Gaussian variable x s N ( ,  X ), Therefore, the distance can be obtained as follows: We employed the mixture model construction proposed by Figueiredo et. al [8] that automatically selects the appropri-ate number of mixture components in a top-down manner.
Alternatively, the integral can be numerically approxi-mated by using reservoir activations collected while process-ing the input time series. Assume that for a given time series s , after the initial wash-out [12], m state activations are col-lected x (1) ,  X   X   X  , x ( m ). Then,
However, in some applications the length of the time series is not sufficient to yield a good approximation. We therefore adopted the circular block bootstrap for time series [17] to construct sufficiently long input series. The block length in bootstrapping was automatically determined following [21].
In the above, the function distance between readout map-pings of reservoir models is formulated. Therefore, the three kernels can be defined as follows: where L 2 2 ( f i , f j ) can be Equations (5), (6) and (7) as reser-voir kernel ( RV ), Gaussian mixture model based reservoir kernel ( GMMRV ), and sampling based reservoir kernel ( Sam-plingRV ). The main algorithm is summarized below: Al gorithm 1 Model based Kernel Algorithm ( RV, GMMRV, SamplingRV ) 1 : Input: Set of sequences s 1 ,  X   X   X  , s M ; parameters (num-2: Output: Kernel (Gram) matrix K . 3: for each time series s i , i = 1 ,  X   X   X  , M do 4: Drive the reservoir state evolution with the input se-5: Fit the linear readout f i using ridge regression for the 6: end for 7: Calculate the pairwise model distance matrix L 2 ( f i 8: Calculate the kernel matrix as K ( s i , s j ) = exp { X 
Besides the model distance based kernels introduced above, we also considered the Fisher kernel obtained with the reser-voir model ( FisherRV ).

Endowing the readout with a noise model yields a gener-ative time series model of the form: Assume the i.i.d. noise model " ( t ) follows a Gaussian distri-bution, HMM in Fisher kernel,  X  is the ridge regression parameter.  X  { 10 6 , 10 5 ,  X   X   X  , 10 1 } ,  X   X  { 0 . 1 , 0 . 2 ,  X   X   X   X  { 10 6 , 10 5 ,  X   X   X  , 10 1 } ,  X   X  { 10 5 , 10 4 ,  X   X   X  Th en, wh ere s (1 ..t ) denotes the time series s (1) , s (2) ,
Slightly abusing mathematical notation, the model like-lihood p ( s (1 .. X  )) given the time series s of length  X  can be written as follows: can be obtained as
U =  X  log p ( s (1 .. X  )) No te that the partial derivative U is an ( O  X  N ) matrix. The  X  X ractical X  Fisher kernel for two time series s i and s with scores U i and U j , respectively, can be formulated as where  X  is Hadamard (element-wise) product. In practice, the noise variance  X  2 can be estimated from the original time series and the output of the fitted readout model.
This section presents experimental results of the proposed kernels, RV , GMMRV , SamplingRV , FisherRV , and other existing time series kernels, including autoregressive ( AR ) kernel, Fisher kernel with hidden Markov models ( Fisher ), and dynamic time warping based kernel ( DTW ).

All hyperparameters, such as the kernel width  X  and or-der p in the AR kernel, number of hidden states in the HMM based Fisher kernel etc. have been set by 5-fold cross-validation on the training set. The search ranges for parameters of each algorithm are detailed in Table 1.
In the reservoir based kernels, we used a fixed topology reservoir (cycle with jumps) [22] for all data sets: N = 100, 15 jumps. The cycle weight r c , jump weight r j , input weight r and readout were obtained on the training set. The read-out mapping was trained via ridge regression (hyperparam-eter  X  tuned via cross-validation). To evaluate the readout Fi gure 2: Illustration of three NARMA sequences with different orders (10, 20 and 30). model distance in the SamplingRV kernel, except for long time series in the PEMS data set (Section 5), the boot-strapped time series were 5 times longer than the original ones 1 .
 The implementation of AR kernel was obtained from Marco Cuturi X  X  website 2 . Fisher kernel was obtained Maaten X  X  website 3 .

We employ a well-known, widely accepted and used imple-mentation of SVM  X  LIBSVM [3]. In LIBSVM, we use cross validation to tune the regularization parameter C . After model selection using cross-validation on the training set, the selected model class representatives were retrained on the whole training set and were evaluated on the test set. Multi class classification is performed via the one-against-one strategy (default in LIBSVM).
We employed three NARMA time series models of orders 10, 20 and 30, given by: s ( t +1) = 0 . 3 s ( t )+0 . 05 s ( t ) m = 5 | s | , where | s | indicates the length of the time series. http://www.iip.ist.i.kyoto-u.ac.jp/member/cuturi/ AR.html http://homepage.tudelft.nl/19j49/Software.html Fi gure 3: Illustration of MDS on the model distance among reservoir weights Fi gure 4: Illustration of the performance of com-pared kernels with different noise levels. s ( t +1) = 0 . 2 s ( t )+0 . 004 s ( t ) The inputs u ( t ) form an i.i.d stream generated uniformly in the interval [0 , 0 . 5). We use the same input stream for gener-ating the three long NARMA time series (60,000 items), one for each order. The three sequences are illustrated in Figure 2. The time series are challenging due to non-linearity and long memory.

For each order, the series of 60,000 numbers is partitioned into 200 non-overlapping time series of length 300. The first 100 time series for each order are used as training set, and the other 100 time series form the test set.

As apparent from Figure 2, distinguishing the three NARMA models using the original time series may be challenging. However, when viewing the time series through the model space of fitted reservoir models, the three time series classes become separated, as illustrated in Figure 3 showing 2-dimensional multi-dimensional scaling 4 representation of the pair-wise readout model distances.
Mu ltidimensional scaling (MDS) aims to preserve the pair-wise distance between points, which is suitable to preserve the model distance for visualization.

In order to study robustness of the kernels we corrupt the time series with additive Gaussian noise (zero mean, stan-dard derivation varies in [0.1,0.5]). Figure 4 shows the test set classification accuracy against the noise level. As a base-line we also include results by SVM operating on the time series directly (300-dimensional inputs) -NoKernel . The RV reservoir based kernel outperforms the baseline and the other time series kernels. Fi gure 5: Comparison of generalization accuracy (top) and CPU time (bottom) in seconds of RV , AR , DTW and Fisher kernels on InlineSkate data set.

We used 9 data sets from UCR Time Series Repository [14]. Each data set has already been split into training and test sets (see Table 2). been boldfaced.
 and SamplingRV kernels on nine benchmark data sets.
T able 3 reports performance of the time series kernels on the benchmark data in terms of test set classification accu-racy. SamplingRV kernel outperforms the other kernels on 7 data sets; RV is superior on 2 data sets and DTW outper-forms the other kernels on 1 data set. In terms of compu-tation time 5 , the reservoir kernels are clearly the most effi-cient. Table 4 shows the average CPU time taken to evaluate the kernels in seconds 6 . SamplingRV kernel is obviously the most expensive among the reservoir kernels. Still, it is faster than its state-of-art competitors.

To further compare the computational effectiveness of the kernels, a relatively large data set, InlineSkate from UCR time series repository, has been employed. The data set con-tains 650 time series (100 training, 550 test) of length 1882, belonging to 7 classes. The influence of time series length on the classification performance and computational com-plexity was studied by considering from each training time series only the first  X  elements, with  X  growing from 300 to 1800 in increments of 300. The resulting accuracy and CPU times are shown in Figure 5. Relatively to the other kernels, the reservoir RV kernel has the lowest computational cost, while achieving competitive performance.
Data sets used so far involved univariate time series. In this section, we perform classification on three multivari-ate time series -Brazilian sign language ( Libras ), handwrit-ten characters and Australian language of signs ( AUSLAN ). Unlike the other data sets, the handwritten characters and Th e computational environment is Windows XP with Intel Core 2 Duo 1.66G CPU and 4G RAM. We do not record the cross validation time for SVM. Table 5: Summary of multivariate (variable length) time series classification problems.
 h andwritten 3 6 0-182 20 60 0 22 58 A USLAN data sets contain time series of variable length. Following [5] (previous AR kernel study) we split the data sets into training and test sets as detailed in Table 5.
The results are shown in Figure 6. SamplingRV is supe-rior on all three data sets. RV kernel is outperformed by DTW and AR kernels on Libras and AUSLAN data sets, respectively. In terms of CPU time, RV kernel usually uses the least and AR consumes the most computation time.
Reservoir readouts can be trained in an on-line fashion using Recursive Least Squares (RLS). In RLS, the readout weights W are recursively updated at every time step t : where z ( t ) stands for the innovation vector; s ( t +1) and y ( t ) correspond to the desired (next-item prediction) and calcu-lated (readout) output;  X ( t ) is the error covariance matrix. Fi gure 6: Comparison of generalization accuracy (top) and CPU time (bottom) in seconds of AR , Fisher , DTW , RV and SamplingRV kernels on 3 multivariate time series.  X  X orgetting parameter X  0 &lt;  X  &lt; 1 is usually set to a value close to 1 . 0. In this work  X  is set by cross validation.
This enables us to construct and refine reservoir kernels on-line, as more and more data become available. This can be particularly convenient in situations where individual items to be classified (time series) are not fixed, but appear in an on-line manner.
 We illustrate this approach on a set of long series PEMS-SF (UCI machine learning repository) with 440 time series of length 138,672. The data reports the occupancy rate of different car lanes of San Francisco freeways within 15 months. The generalization performance of on-line RV ker-nel is reported in Figure 7. As expected, the generalization improves monotonically with increasing amount of data. On full data RV kernel achieves 86.13% accuracy. This com-pares favorably with the best reported performance levels (82% s 83%) [5] among a variety of time series kernels, such as AR , global alignment kernel [7], splines smoothing kernel [16] and Bag of vectors kernel [10].
In this paper efficient kernels have been proposed to tackle the challenges in time series classification through kernel ma-chines. Instead of constructing the kernel directly in the original data space, this paper introduces a X  X ernel in the de-terministically constructed reservoir model space X  that rep-Fi gure 7: Generalization accuracy of on-line RV ker-nel on PEMS time series. resents each time series as a reservoir model with the com-mon dynamic part.

We demonstrated the application of the distance defini-tion in the (function) model space of linear readout models. The model distance is different from the Euclidean distance of the readout parameters, indicating that more importance is given to the  X  X ffset X  than  X  X rientation X  of the readout map-ping. We also estimated the model distance by using either sampling methods or a Gaussian mixture model when the reservoir state distribution is non-uniform.

The proposed kernels were compared with other competi-tors on synthetic and benchmark data sets. The results con-firm the effectiveness of reservoir based kernels. The on-line reservoir kernels proposed in Section 5 can process extremely long time series efficiently.

In general, the closed form simple reservoir ( RV ) kernel is the most efficient 7 . However, it is obtained under the (rather unrealistic) assumption of uniform state distribution and the tolerable increase in computational cost by the SamplingRV kernel is well offset by the increase in the classification accu-racy. The GMMRV kernel can also be analytically obtained via approximating the state distribution by a Gaussian mix-ture. Of course, the quality of this kernel depends on how well the state distribution is captured by the Gaussian mix-ture model used.

It is interesting that the Fisher kernel based on the reser-voir model achieves better performance than the Fisher ker-nel based on the HMM model with continuous (Gaussian distributed) emissions. The principal difference between the reservoir model and HMM is that in the reservoir model the state space is infinite (uncountable) with deterministic input-driven dynamics. In HMM the state space is finite and latent, with probabilistic state transitions.
In conclusion, reservoir based time series kernels can achieve superior performance in terms of both generalization accu-racy and computational time, without the need for explicit specification of the parameterized model class for the time series data. This is potentially of great benefit in cases of very large data sets of long time series where the underly-ing parametric model is unknown. Reservoir kernels stand and fall on the ability of the particular dynamic reservoir to
I t is worth noting that there also exist fast implementations of non-kernelized variations of DACO and global alignment kernels. g enerate a rich pool of dynamical features sufficiently repre-senting the variety of time series occurring in a given task. If the echo state property -a cornerstone of reservoir modelling -is not an appropriate modelling assumption, the reservoir kernels cannot be expected to perform well. However, as has been demonstrated numerous times, for most real-world data the fading memory assumption (encapsulated in the echo state property) is appropriate. This work was supported by the European Union Seventh Framework Programme under Grant Agreement INSFOICT-270428 on  X  X aking Sense of Nonsense (iSense) X . The work of H. Chen was supported in part by the National Natural Science Foundation of China under Grants 61203292 and 61311130140, and the One Thousand Young Talents Pro-gram. The work of P. Tino was supported by the Biotechnol-ogy and Biological Sciences Research Council under Grant H012508/1. The work of X. Yao was supported by a Royal Society Wolfson Research Merit Award. [1] D. Berndt and J. Clifford. Using dynamic time [2] A. B. Chan and N. Vasconcelos. Probabilistic kernels [3] C.-C. Chang and C.-J. Lin. LIBSVM: A library for [4] H. Chen, P. Tino, A. Rodan, and X. Yao. Learning in [5] M. Curturi and A. Doucet. Autoregressive kernels for [6] M. Cuturi. Fast global alignment kernels. In [7] M. Cuturi, J.-P. Vert, O. Birkenes, and T. Matsui. A [8] M. A. T. Figueiredo and A. K. Jain. Unsupervised [9] A. Gaidon, Z. Harchaoui, C. Schmid, et al. A time [10] M. M. Hein, M. Hein, and O. Bousquet. Hilbertian [11] T. Jaakkola, M. Diekhans, and D. Haussler. Using the [12] H. Jaeger. The echo state approach to analysing and [13] T. Jebara, R. Kondor, and A. Howard. Probability [14] E. Keogh, Q. Zhu, B. Hu, Y. Hao, X. Xi, L. Wei, and [15] E. J. Keogh and M. J. Pazzani. Derivative dynamic [16] K. Kumara, R. Agrawal, and C. Bhattacharyya. A [17] S. N. Lahiri. Theoretical comparisons of block [18] M. Luko X sevi X cius and H. Jaeger. Reservoir computing [19] P. J. Moreno, P. P.Ho, and N. Vasconcelos. A [20] K. Petersen and M. Pedersen. The matrix cookbook. [21] D. N. Politis and H. White. Automatic block-length [22] A. Rodan and P. Ti X no. Simple deterministically [23] J. Shawe-Taylor and N. Cristinanini. Kernel methods [24] P. Tino, I. Farka X s, and J. van Mourik. Dynamics and [25] L. Van der Maaten. Learning discriminative fisher
