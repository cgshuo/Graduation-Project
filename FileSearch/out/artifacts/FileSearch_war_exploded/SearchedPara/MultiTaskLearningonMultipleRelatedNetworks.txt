 With the rapid proliferation of online social networks, the need for newer class of learning algorithm to simultaneously deal with multiple related networks has become increasingly important. This paper proposes an approach for multi-task learning in multiple related networks, where in we perform different tasks such as classification on one network and clus-tering on the other. We show that the framework can be extended to incorporate prior information about the cor-respondences between the clusters and classes in different networks. We have performed experiments on real-world data sets to demonstrate the effectiveness of the proposed framework.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms Clustering, Classification, Multi task learning, Link Mining
With the rapid proliferation of diverse online social net-works, the need to integrate and analyze relational data from multiple networks has become increasingly important. Typ-ical learning problems for relational data include commu-nity detection (or clustering), collective classification, link prediction, influence maximization, and anomaly detection. However, the focus of previous research has been on learn-ing a single task on a single network or on a k-partite graph constructed from a multi-network with heterogeneous nodes.
This paper departs from previous research by focusing on learning multiple tasks (clustering and classification) in dif-ferent networks. For example, one might be interested in classifying the postings made to an online message forum while simultaneously clustering the network of users who posted the messages or comments. Multi-task learning can also be performed on networks from different domains. For example, suppose we have partially classified the users on Facebook. Can we use this information to help cluster the users on MySpace or other social networking Web sites?
One approach to solve this problem would be to combine all the graphs into one giant network and apply existing state of the art clustering or classification algorithms to the whole network. However such an approach has many limitations. First, the number of classes in one network may be different than the number of communities (clusters) in another net-work. By applying a single algorithm to the entire network, we have no control over the number of clusters or classes that will be found in each induced subgraph of the network. Furthermore, aggregating the graphs into one large network will lose information about the inherent variabilities between different graphs.

Another approach would be to perform the learning task independently on each network. An obvious limitation of this approach is that it does not fully utilize the link infor-mation from other related networks. More importantly, if the learning tasks are somewhat related and there is prior knowledge about the relationships between the clusters and classes in different networks, this approach will not be able to share and utilize this information. This motivates the need to develop a joint algorithm for multi-task learning in multiple related networks.
 There have been recent efforts for clustering multiple graphs. Zhou et al. [6] proposed a new method to combine multi-ple graphs to measure document similarities, where different factorization strategies are used based on nature of different graphs. Tang et al. [4] proposed a linked matrix factor-ization approach for fusing information from multiple graph sources. Chen et al. [1] presented a co-classification frame-work for Web spam and spammer detection in social me-dia based on the maximum margin principle. They demon-strated that joint classification strategy is more effective than independent detection strategy. However, to best of our knowledge none of the previous studies has worked on learning multiple tasks such as classification and clustering simultaneously on multiple related networks.
Let G = ( of a multi-network, where each V i corresponds to a set of nodes of a specific type and M is the number of node types. Furthermore, E s is the set of links connecting the same type of nodes whereas E d is the set of links connecting nodes of different types. The multi-network forms a k-partite graph if
E s =  X  . Each set of nodes of the same type also induces a subgraph G i = ( V i ; E i ), where E i  X  V i  X  V i and  X  i =1 E i . We may consider the induced subgraph network of homogeneous nodes within the multi-network. Definition 1 (Multi-task Multi-network Learning). Given G = ( learning problem is to solve M learning tasks, where each task is associated with an induced subgraph G i = ( V i ; E G .

For brevity, the framework presented in this paper focuses on a multi-network with M = 2, though it can be generalized to graphs containing more than two types of nodes. To simplify the notation, let G 1 = ( V 1 ; E 1 ) and G 2 = ( V be the induced subgraphs of G containing nodes from V 1 and V 2 , respectively. Also, let G 12 = ( V 1  X  V 2 ; E d bipartite graph with links connecting between nodes in V 1 to those in V 2 . The number of nodes in each network are denoted as | V 1 | = n and | V 2 | = m , respectively.
This paper focuses on a multi-task learning problem in which the tasks involve classification and clustering of nodes in two related networks. Without loss of generality, we as-sume that the clustering task is performed on network G 1 and the classification task on network G 2 . Let k 1 be the number of communities (clusters) in G 1 and k 2 be the num-ber of classes in G 2 . Let A , B , and C be the adjacency matrices associated with the graphs G 1 , G 2 and G 12 , respec-tively. We assume that the first l nodes in G 2 are labeled while the remaining m  X  l nodes are unlabeled. The true class information is encoded in an l  X  k 2 binary matrix L , such that L ij = 1 if the node v 2 i  X  V 2 belongs to class j and zero otherwise.
 Our approach is to generate a pseudo label matrix X  X  the cluster membership of node v 1 i in each of the k 1 clusters. Similarly, a pseudo label matrix Y  X  X  m k 2 is generated for the nodes in G 2 such that the i th row of Y gives its membership in each of the k 2 labels.
This section outlines our proposed framework for simul-taneous clustering and classification of multiple related net-works. Specifically, we employ a matrix factorization ap-proach [2] to solve both tasks by optimizing the following joint objective function: where the superscript T denote the matrix transpose oper-ation and D ( P  X  Q ) = Kullback-Leibler distance between P and Q . The first term in the objective function deals with the clustering of nodes in
G 1 by factorizing the adjacency matrix A into a product involving the pseudo label matrix X . The last two terms deal with the classification of nodes in G 2 by estimating the pseudo label matrix Y , taking into account both the link structure ( B ) and class information ( L ). It should be noted that the last term in the objective function does not apply to unlabeled nodes in network G 2 . Thus, Y l is an l  X  k sub-matrix of Y . Finally, the second term in the objec-tive function is used to learn the relationship between the clusters found in network G 1 and the classes obtained for network G 2 .

Clearly, if the pseudo label matrix Y is known, we can augment this information to estimate the pseudo label ma-trix X for network G 1 by minimizing the following objective function Similarly, if the community membership information in net-work G 1 is available, then it can be used as additional infor-mation for classifying the nodes in G 2 . This is accomplished by minimizing the following objective function
L 2 : min Y;W;V D ( B  X  Y W Y T ) + D ( C  X  XV Y T ) + D ( L Thus, we may use an alternating minimization scheme to solve the optimization problem. Since we have five param-eters to estimate ( X , Y , U , W , and V ), we iteratively up-date the value of each parameter by fixing the values of the remaining four parameters. The update formula for each parameter is computed using a gradient descent approach. We omit the details due to space limitations and present the multiplicative update formula below: X The update formula for Y ij depends on whether the node is labeled or not. For i &gt; l (unlabeled nodes) Y whereas for labeled nodes ( i  X  l ), we need to add and nator of update formula (5), where I k is identity matrix Similarly, the update formula for matrices U , W , and V are computed as follows: We first randomly initialize tthe matrices X , Y , U , W , and V to some non-negative entries and then iteratively update the matrices according to the update formula given above. The process is repeated until a specified maximum number of iterations is reached.
Often times, we may have additional information about the relationship between the classes and clusters in the dif-ferent networks. In what follows we give a motivation to incorporate this prior information into the objective func-tion.

Example 1. Consider a multi-network comprising of a citation network between research articles and a co-authorship network between researchers. Suppose we would like to clas-sify the articles according to the following topics: Computer Vision, Pattern Recognition, Arti cial Intelligence, Cell Biology , and Genetics . Similarly, we are interested in classifying the authors according to their research disci-plines. Since an author may work on multiple related topics (Computer Vision and AI or Cell Biology and Genetics), therefore the article classes are not re ected as it is in the author network, rather they are further grouped into coarser clusters, namely, Computer Science and Biological Sci-ence . The author cluster ( Computer Science ) is related to the rst three article clusters, while Biological Science is related to Cell Biology and Genetics . We expect such prior information will enhance the joint clustering results. This information can be encoded in a 5  X  2 prior matrix: where the rows are the article categories and the columns are the author clusters.
 To incorporate prior, we first need to interpret the role of the V matrix in the objective function. This matrix is esti-mated from the KL divergence term, D ( C  X  XV Y T ), where C is the adjacency matrix representation of the links in the bipartite graph G 12 . Since X encodes the cluster member-ship of the nodes in G 1 and Y encodes the class membership of the nodes in G 2 , we could interpret the role of matrix V as capturing the relationship between the clusters in G 1 the classes in G 2 .

Let P be the prior information about the relationship be-tween the clusters and classes. To incorporate this informa-tion, we modify the objective function (1) as follows: where is a user-specified parameter that controls the trade-off between fitting V directly to the data and fitting V to the prior matrix P . If = 0, then the correspondence be-tween clusters is given by the prior matrix. If = 1, then the formulation reduces to the original framework given in (1). In situations where the actual proportion of links be-tween nodes from each cluster in G 1 and each class in G 2 unknown, we may use a non-informative prior with P ij is 0 or 1 indicating whether the i th cluster is expected to be related to the j th class (see Example 1).
This section presents the results of applying the proposed framework to the multi-task learning problem on real-world network data. As a baseline, we use the normalized cut ( Ncut ) algorithm by Shi and Malik [3] for clustering and the label propagation algorithm with local and global con-sistency ( LGC ) by Zhou et al. [5] for classification. For a fair comparison, we applied each baseline algorithm on the entire multi-network G (instead of G 1 and G 2 separately). In each experiment, we set the proportion of labeled nodes in one of the two network to 20%. We use the normalized mutual in-formation (NMI) measure to evaluate clustering results and accuracy to evaluate classification results. We denote our proposed multi-task, multi-network learning framework as Joint or Joint + Prior in the remainder of this section.
Two networks, namely the article networks and editor net-works are available on Wikipedia. The Wikipedia articles are chosen from four broad topics X  X iology, Natural Science, Computer Science and Social Science. Each of the topics are further subdivided into subtopics, as shown in Table 1.
We randomly sampled 6403 articles and 5361 of their cor-responding editors to form the Wikipedia multi-network. Our task is to classify each article into one of the 12 possible sub-categories and to partition the editors into 4 clusters. Table 2: Clustering results of Wikipedia editors. Table 3: Classi cation results of Wikipedia articles. LGC on article network only 0.87 LGC on entire multi-network 0.85
As shown in Table 2, the independent clustering of user network gives very bad results compared to the Joint ap-proach. The cluster NMI increases from 0.08 to 0.37. How-ever, this additional gain comes at the expense of reduced classification accuracy on the article network. Applying LGC on article network alone gives an accuracy of 0.87 which reduces to 0.85, when applied to the entire multi-network. This is because the class information provided by the ar-ticle network is more useful than the  X  X oarse-level X  cluster information provided by the editor network. Furthermore, a user typically contributes to articles across different cate-gories which makes it difficult to decide his/her actual class label. We currently assigned the user to the category to which he/she has made the most contributions. In fact, it is because of this problem, it is difficult to acquire the label information in user network, and thus, clustering becomes a necessary task.
Here we present the results of applying the proposed frame-work to multi-network constructed from different domains. We cluster the users of Digg.com 1 and classify the editors of Wikipedia.org First, articles from Wikipedia are chosen from only three broad topics  X  Natural Science, Computer Science and Social Science and a sample of 4320 editors are chosen. We then sampled 5670 Digg users who have book-marked URLs on the following three topics: Politics, Com-puter Science , and Natural Science . We formed a Digg user-user link from the user-URL matrix. Two Digg users are linked if they have at least URLs in common. Finally, links between Wikipedia editors and Digg users are estab-lished using the contents of Wikipedia articles and Digg X  X  URL description. Specifically, the weight of the link cor-responds to the cosine similarity between the words in the title and description of a URL and the words that appear in the content of a Wikipedia article. Figure 1 shows the spy plot for the Digg user and Wikipedia editor networks. Careful observation of the spy plots reveals three distinct clusters/classes among both networks. However the Digg user network is noisier then Wikipedia editor network. Figure 1: Adjacency matrix plot for Digg user (Left) and Wikipedia editor (Right) networks (best viewed in color).

We first performed Ncut on the Digg data alone and Ncut on the overall network (Digg + Wikipedia + links between them). The confusion matrix is given in Table 4. As can be seen in the adjacency matrix plot, the first two clusters are noisy and heavily interlinked. So we obtain only two predominant clusters.
 Table 4: Confusion matrix for Digg clustering using Ncut We apply the LGC algorithm to propagate labels in the Wikipedia data set. The results are summarized in Table 5. The noisy Digg data has degraded the performance of LGC on the Wikipedia network. Propagating labels only on Wikipedia data set gives an accuracy of 0.71, which reduces to 0.66 when applied to the multi-network. www.digg.com is a popular social news Web site Table 5: Confusion matrix for Wikipedia classi ca-tion using LGC
The presence of noise on the Digg user network combined with noisy links between the Wikipedia and Digg networks resulted in poor performance of the joint learning algorithm. The number of clusters obtained is less than the number we expect. However, by incorporating the prior matrix P = I , ensures that we obtain three clusters on each network. The results are shown in table below. Clearly, the Joint + Prior results are significantly better than both Ncut and LGC.
 Table 6: Confusion matrix for clustering Digg users and classifying Wikipedia editors using Joint with prior
In this paper we have given a framework to perform multi task learning on multiple related networks. We have also introduced the idea of using a prior to guide the cluster-ing process. We have demonstrated a practical use of our algorithm by identifying similar communities on different network domains namely Digg and Wikipedia. This material is based upon work supported in part by ONR grant number N00014-09-1-0663 and ARO grant num-ber W911NF-09-I-0566. [1] F. Chen, P. N. Tan, and A. K. Jain. A co-classification [2] D. D. Lee and H. S. Seung. Algorithms for non-negative [3] J. Shi and J. Malik. Normalized cuts and image [4] W. Tang, Z. Lu, and I. Dhillon. Clustering with [5] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and [6] D. Zhou, S. Zhu, K. Yu, X. Song, B. L. Tseng, H. Zha,
