 Microblogging service has emerged to be a dominant web medium for billions of individuals sharing and spreading in-stant news and information, therefore monitoring the event evolution on microblog sphere is crucial for providing both better user experience and deeper understanding on real-time events. In this paper we explore the problem of gen-erating storylines from microblogs for user input queries. This problem is challenging due to the sparse, dynamic and social nature of microblogs. Given a query of an ongoing event, we propose to sketch the real-time storyline of the event by a two-level solution. We first propose a language model with dynamic pseudo relevance feedback to obtain relevant tweets, and then generate storylines via graph opti-mization. Comprehensive experiments on Twitter data sets demonstrate the effectiveness of the proposed methods in each level and the overall framework.
 H.2.8 [ Information Systems ]: Database applications X  Data mining ; H.3.3 [ Information Search and Retrieval ]: Information filtering Algorithms, Design, Experimentation Social media, microblog, language model, dynamic pseudo relevance feedback, storyline
Microblogging service has rapidly increased its popular-ity in recent years. People are attracted to microblogging sites, such as Twitter, for instant first-hand reports on real-life events. In the meantime, instead of using web search engines, users are more willing to propose event queries on Twitter to obtain information about an ongoing event [33]. Systems that deliver realtime event notification on Twitter are also available [28].

It would be helpful for industry, academia, and end-users, if a skeleton of an event by request is automatically gener-ated from the huge volume of tweets. We refer this problem as Generating Event Storyline from Microblogs (GESM) .For example, Figure 1 presents the storyline based on an event query of  X  X gypt Revolution X . The vertical location of each frame indicates the time-stamp of the corresponding phase. The hierarchical structure depicts how major progress hap-pens in adjacent phases. The branches partition simultane-ously happened tweets into different semantic groups. Auto-generated storylines facilitates easy navigation in microblo-goshpere and also supports a wide range of mining systems on collective intelligence. fire spreading human shields protect Museum Looters destroy Figure 1: A sample storyline for event query  X  X gypt Revolution X 
GESM is a challenging problem. There exist studies in generating storylines from news articles [21, 36, 13]. Also, there are fruitful research efforts on the area of topic detec-tion and tracking (TDT) [1]. However, differences between GESM and prior studies are remarkable. 1) All of previ-ous studies are designed for a collection of well edited facts, e.g., news articles and high quality web pages, which lack ef-fective mechanisms for handling extremely short noisy text streams such as microblogs. 2) To facilitate users informa-tion preferences, GESM requires user identified queries as inputs. Compared with TDT, GESM provides personalized service in massive microblog data. 3) Since GESM is tai-lored for user input queries, a two-level framework is neces-sary: at the low level, finding all relevant tweets through the time-line of the event by a retrieve model; and at the high level, summarizing relevant tweets and the latent structure to produce a storyline.

Challenges of microblog storyline generation arise from the following aspects. First of all, the dynamic and sparse nature of microblogs remains a large obstacle to improve performance of microblog retrieval system for event queries. Event queries usually only contain basic descriptive terms, e.g., the location of the event, the person involved, and the main theme, etc. Microblogs are streams of very short texts reporting recent brief updates. How to match the underly-ing event expressed by the vague event query to potential relevant tweets which possibly not contain any query terms is a severe problem. State-of-art IR models including a num-ber of variants of pseudo relevance feedback have been pro-posed as attempts at this problem. However, they expand the original query based on frequencies, therefore temporal relevant keywords during certain periods may be misjudged if they are not significant in frequency throughout the time-line of event. As we shall see in later sections, this frame-work should be modified to better capture the dynamic of microblogs. Secondly, the social nature of tweets increases the difficulty of integrating semantic similarity with chrono-logical order in generating a storyline. Information sharing in microblog sphere yields numerous duplicate tweets and di-rect and undirect re-tweets. Duplicate tweets and re-tweets are created after the right time point, and they will trig-ger confusion in partitioning the event time-line. Thus a naive method, which employs traditional text summariza-tion strategy in each time segment, is not applicable. In this work, we focus on resolving the above challenges. Major contributions of this work are: (1) A novel prob-lem of generating event storylines from microblogs is pro-posed and a two-level solution to this problem is provided. (2) A dynamic pseudo relevance feedback (DPRF) language model is presented to retrieve relevant tweets given an event query. By making an assumption that the prior probability of pseudo relevance feedback is dependent on the burst pe-riods of given event, DPRF expands the original query with representative keywords in active phases of the event. Thus the accuracy of retrieval is enhanced. (3) The problem of storyline generation on the retrieved microblogs is formu-lated as a graph-based optimization problem and is solved by approximation algorithms of minimum-weight dominat-ing set and directed Steiner tree. The generated storylines ensure both temporal continuity and content coherence.
The rest of the paper is organized as follows. Section 2 in-troduces related work on information retrieval, multi-document summarization and microblog mining. Overview of the frame-work is presented in section 3. Language model based query expansion via dynamic pseudo relevance feedback is described in Section 4. Section 5 presents the framework for storyline generation. In Section 6, experimental results are analyzed and a user study is conducted. We conclude our work in Section 7.
Several research directions are related to our work, in-cluding microblog mining, information retrieval, and multi-document summarization.
The emergence of Twitter motivates recent research works on mining microblogs, including microblog search [8], iden-tifying emerging topics on Twitter [23], and summarizing tweets in a certain period [32]. A few research works have been devoted to event detection [28, 29], but they focus on the detection of novel events without a global view.
To achieve a better performance, several research methods have been proposed to deal with the unique characteristics of microblogs, e.g. expanding tweets by hashtags [7], uti-lizing social relations for identifying influential tweets [10], incorporating sentiment categorization [3], promoting most recent tweet [9], employing transfer latent topic models for overcoming abbreviated texts [42], and expanding queries by recently frequently co-occurred terms [22].

The dynamic and social nature of microblogs is not fully explored by previous research efforts. By adding a tempo-ral dimension in the event storyline generation system, our work sheds light on the understanding and mining of mi-croblogosphere.
Pseudo relevance feedback (PRF) has been proved to be helpful in the IR community. The state-of-art PRF by lan-guage model approaches are surveyed in [41]. In addition, to reinforce model-based feedback, a classifier is adopted before query expansion to determine the  X  X oodness X  of expansion candidates in [4]. Adaptive query expansion is implemented in [38] to select query expansion candidates from different sources.

A few works study text stream retrieval. For example, time-based model in [19] assigns documents with prior as-sociated with the  X  X reshness X  of documents. The temporal factor can be introduced into query expansion [9, 22]. But all of the above mentioned approaches favor only recent doc-uments, therefore are not applicable to cover the lifespan of the whole event. In [11], a temporal profile is constructed for each query throughout its lifespan to categorize the query and predict query performance, however there is no mecha-nism to incorporate the temporal profile into query expan-sion.

Most PRF methods expand the original query in a static manner. On the contrary our method selects event spe-cific expansion terms, which are temporally correlated with query terms in the pseudo relevant documents. Our empir-ical study shows that our DPRF method is superior than previous researches in the scenario of event query retrieval.
Multi-document summarizati on conveys the main and most important meaning of several documents. One type of sum-marization systems select representative sentences, e.g. with significant frequency [40], or structural centroid in sentence graph [18, 13]. Another type is based on matrix decomposi-tion [17, 35, 31]. Some prior researches focus on clustering query-induced results [37].

Unlike multi-document summarization, research in the area of topic detection and tracking (TDT) [1] aims to thread streams of texts. TDT includes five main tasks: story seg-mentation, topic tracking, topic detection, first story detec-tion and link detection. Among the five tasks, topic track-ing and link detection are similar to components of the GES problem addressed in this paper. Most researches in story segmentation and link detection are devoted to clustering and classifying similar texts, without considering the times-tamps of articles, e.g. relevance model in [16] adopts sym-metric similarity comparison. Others consider the influence among articles to be unidirectional and directly dependent, e.g. topic structure is identified in [25] by forgetting out-of-date statistics, the bursty structure is recognized in [12] by estimating state transitio n probability in an infinite state automation.

To conclude, although these methods have been success-fully applied in several domains, they are not applicable to the event storyline generation problem. The quality of gen-erated storyline is determined by the quality of summary in each phase, and the quality of phase segmentation, and pre-vious summarization and TDT methods lack the ability to generate a complete and coherent storyline. On the contrary, our graph optimization based method has a built-in mecha-nism to simultaneously generate the summary for each vir-tual phase and naturally integrate the generated summaries to form the storyline.
Not until recently, a limited number of studies devote to summarizing documents with t ime stamps, mostly news ar-ticles. For example, in [24], an HMM style model is pre-sented to discover evolutionary theme patterns (term distri-butions). BlogScope [2] discovers hot trend and temporal keyword correlations. Similarly, a burstness-aware search framework is presented in [15]. A finite mixture model is presented in [25] for tracking dynamics of topic trends. ETS [36] returns the evolution skeleton along the timeline by extracting representative and discriminative sentences at each phase. In [39] representative sentences are chosen based on relevance, coverage, coherence and cross-date di-versity. In [32] summarization consists of median tweets in each time segment. These timeline generation methods can hardly be applied to our storyline generation problem be-cause the asynchronism of information propagation in the microblogosphere makes it difficult to partition the timeline of an event into different phases.

There also exists few studies on storyline generation. [21] proposes a storyline generation framework to identify events using self-organizing maps and to extract the main storyline by assigning weights to different events based on the similar-ity between the events and given topics. The system then generates storyline-based summaries using term weighting schemes, but does not take the temporal information into consideration. Very recently a pictorial storyline generation method has been proposed [34] that combines image and text analysis to obtain a storyline containing textual, picto-rial, and structural information to provide a sketch of the topic evolution. This method first constructs a multi-view graph, in which each node is a picture and its text descrip-Figure 2: The System Architecture for Event Sto-ryline Generation tion. Then representative nodes are selected by finding a minimum dominant set on the graph. Finally, a Steiner tree approximation is employed to connect the dominating nodes to form a pictorial storyline. In this paper, we take the storyline generation procedures in [34]. However, our work differs from [34] as follows. Unlike the traditional text and image analysis, our work focuses on microblogs which have their unique characteristics. For example, the length of a microblog entry is limited, the content may be noisy, and tags and links appear frequently. These characteristics make storyline generation from microblogs very challenging. Dealing with the dynamic and sparse nature of microblogs is one of the most important contributions in this work.
Given an event query Q , which is a set of user defined key-words or phrases describing an ongoing event in real life, our goal is to mine the storyline from the relevant tweets. The generated storyline should be a graph structure, where each node is labeled by a summary of an individual phase of the event, and each edge represents causal relationship between two phases. Consequently, the proposed framework consists of two models for retrieving relevant tweets and generating storyline respectively. Figure 2 shows the framework of our proposed storyline generation system. In the off-line layer, each tweet is indexed, and the temporal information of each term is stored and pre-processed in the burst detection mod-ule. The retrieving module and storyline generation module are implemented in cascade online layers, details of which will be introduced in section 4 and 5.
In modern information retrieval, language model approaches estimate probability distribution  X  d over the vocabulary W for each document d in the corpus C . By modeling each query Q as  X  Q , relevant documents can be ranked according to query likelihood. However, the original query is usually short and vague, and can not fully cover the underlying in-formation need. To enhance the query expressibility, query expansion is adopted to replace the original query Q by a new high quality query Q . In a pseudo relevance manner, suppose the few top ranked documents d + by the initial query Q builds a relevant model  X  F , we can set the new query to be a linear combination of original query Q and relevant model  X  F [41]: where  X  controls the degree of coherence of the new query to pseudo relevance.

In this paper, we follow relevance model method to infer  X  . The relevance model method approximates  X  F as the query model, and each pseudo relevant document is a sample from the query model. Therefore relevance model method defines term distribution in  X  F as the likelihood of generating terms from pseudo relevance: where p ( Q | d +) = q  X  Q p ( q | d +).
In traditional pseudo relevance feedback (PRF), the prior p ( d +) is usually set to be uniform. However, this assumption doesn X  X  hold in an instant broadcast medium like Twitter. Consider the event  X  X gypt Revolution X  in Figure 1, there are several distinct phases (e.g. 2011-01-24 to 2011-01-26, 2011-02-01 to 2011-02-03) during which the event encounters major progress and discussion bursts out.

Intuitively, in the initial search results of an event query of  X  X gypt Revolution X , a top tweet published on 2011-01-25 is more likely to be a truly relevant tweet than a tweet pub-lished on 2011-01-01 on a near position in the ranking list. Suppose that the event is detected to have K burst peri-ods (detection detail is introduced in the next subsection), the prior distribution of relevant tweets should be centered around each burst period.

We first assume that the prior probability of relevant doc-ument d + is dependent on the distance of t d + to the centroid of burst periods, denoted as  X  = {  X  1  X  X  X   X  K } . We define the following three probability functions, each of which is con-trolled by scale parameter  X  . As shown in Figure 3, these probability functions have various mechanisms to model the effective range of burst period, decay coefficient and skew-ness. 1. Mixture Gaussian Distribution assumes that the 2. Local Power Distribution assumes that the prior 3. Skewed Linear Distribution assumes that the prior Figure 3: An illustration of prior probabilities of pseudo relevant tweets, with two burst periods cen-tered at time points 3 and 14 .
Intuitively, at time point  X  k , each query term should 1) appear more frequently than usual 2) be continuously fre-quent around the time point. Following these intuitions, we propose to detect burst periods of the event by 1) for each query term, finding the time intervals with arbitrary length in which the query term appears constantly frequent; 2) picking the time points within these intervals with the largest sum of frequencies over all query terms.
We first borrow the definition of a term X  X   X  X ursty score X  from [15]. Suppose that w is a term, which occurs for tf ( w,T ) times in any time interval T , then the bursty score of w is defined in Eq.(7): where | T | is the length of the time interval, and | T | the length of observation time, which is experimentally set to be time span of the whole corpus. The bursty score of a term is positive if it has above average observation frequency. Figure 4: An illustration of the storyline generation
A linear time algorithm in [27] is proposed to find time interval T w,j = &lt;st,et,LS,RS&gt; with the maximal cumu-lative burst score B ( w,T w,j ), where st is the starting time of T w,j , et is the ending time of T w,j , LS is the cumulative bursty score before T w,j , RS is the cumulative bursty score after T w,j .

Next, we present Eq.(8) to compute the score of any query term q at each time point: where B ( q,T j ) is the bursty score of q in maximal interval T which is super segment of t . Then we rank each time point by q  X  Q H ( q,t ) and choose the largest K time point  X  k
As discussed in Section 1, to generate the storylines from relevant tweets, obstacles are duplicated tweets and indirect retweets. Intuitively, we can pick up a good tweet to repre-sent similar or duplicated tweets. The representative tweets provide the basic outline for each phase. Then the repre-sentative tweets are connected appropriately to depict the evolving structure of the event. In order to eliminate noisy retweets, only texts published after a certain time can be considered as subsequent phases. Finally, there may be dif-ferent ways of connecting these representative tweets, and an optimistic connection should be the one that connects them most smoothly.

The storyline generation follows the processes described in [34]. Thus, the storyline generation procedure consists of three parts. In the first part, a multi-view tweet graph is constructed, in which the semantic and temporal informa-tion among relevant tweets is stored. Next, representative tweets are extracted by finding a minimum dominant set on the tweet graph. Finally, a minimum steiner tree algo-rithm is employed to connect the representative tweets in each phase [30].

Givenaneventquery Q and a collection of relevant tweets by the method described in Section 4, we can construct a multi-view tweet graph.

Definition 1 (multi-view tweet graph). Amulti-view graph G =( V,W,E,A ) ,where V is a set of vertices (nodes), W is the weights of V , E is a set of undirected edges, which represents the similarities between tweets, and A is a set of directed edges (arcs), which represents the time continuity of the tweets.

Construction of such a graph is controlled by three non-negative real parameters  X ,  X  1 , X  2 ,  X  1 &lt; X  2 .Eachnodein G represents a tweet. We use the cosine measure to calculate similarity between two tweets. To define E ,wejointhetwo nodes by an edge if and only if the text similarity between the two responding tweets is greater than  X  . To define A , we draw an arc from v i to v j if and only if  X  1  X  t j  X  where t i and t j are their respective time stamps. We call [  X  , X  2 ] the temporal window. Also, for each node v i ,its vertex weight, w ( v i ), is 1  X  score ( Q, v i ). In our method, we first find the dominating set on the undirected graph G =( V,W,E ) (i.e., without considering A in the multi-view graph), and then perform the steiner tree algorithm to con-nect the dominating set on the directed graph G =( V,W,A ) (i.e., without considering E in the multi-view graph) which takes the time continuity into consideration and leads to a coherent storyline.

A subset S of the vertex set of an undirected graph is a dominating set if for each vertex u ,either u is in S or is adja-centtoavertexin S . The problem of finding a set of repre-sentative summaries can be viewed as the minimum-weight dominating set problem on the undirected graph ( V,W,E ).
Definition 2 (MWDS). The Minimum-Weight Dom-inating Set Problem (MWDS) is the problem of finding, given a vertex-weighted undirected graph G , from all dominating sets of G =( V,W,E ) , the one whose total vertex weight is the smallest.
 We use the following straightforward greedy algorithm for obtaining an approximate solution (Algorithm 1). This al-gorithm views that the weight of a newly added vertex is evenly shared among its newly covered neighbors and se-lects the node that minimizes this share at each round of iteration. The approximation rate of this algorithm is 1 + log( X  OPT ), where  X  is the maximal degree of G and OPT is the optimal dominating set.
 Algorithm 1: Greedy MWDS Approximation Input : G =( V,W,E ) Output : dominant set S
S  X  X  X  ,T  X  X  X  ; while | S | &lt;W &amp;&amp; S = V do end
Once we select the most representative summary in each phase using the dominating set approximation, we need to generate a natural storyline capturing the temporal and structural information of the event-relevant tweets. To study this problem we use the concept of Steiner trees. Here a Steiner tree of a graph G with respect to a vertex subset S is the edge-induced sub-tree of G that contains all the ver-tices of S having the minimum total cost, where the cost is the total weight of the vertices.
 Definition 3 (Steiner Tree). Given a directed graph G =( V,W,A ) ,aset S of vertices (terminals), and a root q  X  S from which every vertex of S is reachable in G , find the subtree G rooted at q containing S with the smallest total vertex weight. Algorithm 2: Steiner Tree Algorithm Input : G =( V,W,A ) ,S,q,k  X  1
Output : Steiner tree T rooted at q covering at least k
T  X  X  X  ; while k&gt; 0 do end
We apply the Steiner tree approximation in [34] to gen-erate the storyline. In this algorithm, the initial call of A ( k,q, S )with S set to the dominating set calculated by algorithm 2, q set to be event vertex assigned with the earli-est time stamp, and k set to be the size of S . The algorithm takes a level parameter i  X  1. i = 1 is the default case where the straightforward algorithm selects l vertices clos-est to root and returns the union of the shortest paths. The length of an arc ( u, v )  X  A is the vertex weight of u .We will interpret the output tree as the storyline transition from the root to all the other dominating objects as illustrated in Figure 4. For a constant i , the algorithm is known to run in polynomial time and produce an O ( k 1 /i ) approximation.
In the experiments, we evaluate the performance of the proposed framework. In particular, Section 6.2 presents the experiments on tweet retrieval, and Section 6.3 conducts the comparisons on storyline generation. We also conduct a user study to compare our system with different document un-derstanding systems in Section 6.4.
The data set is Tweets2011 corpus for TREC 2011 mi-croblog track. The corpus is comprised of 2 weeks (23th January 2011 until 8th February) of sampled tweets from Twitter. Different types of tweets are presented, including replies and retweets. The corpus is multilingual, including English, Japanese and so on. More details of the collection are illustrated in Table 1.

In pre-processing, we do not remove stop-words. Instead, mentions (@somebody) are removed from the vocabulary. Non-English tweets containing less than one English word with more than 2 characters are filtered. Explicit re-tweets with HTTP code 302 are filtered. Empty tweets and forbid-den tweets with HTTP code 403 and 404 are also filtered. Porter stemmer is adopted in indexing.
TREC 2011 microblog track provides 49 queries and rele-vance judgements for these queries. Each query is associated with an time stamp. Only tweets published before the time stamp are under consideration. After examination, we be-lieve that each query is describing an ongoing real event. Therefore we use the TREC queries in this subsection. We use both highly relevant and relevant tweets annotated by TREC as the ground truth.

The dominant evaluation metric is the precision at top 30 tweets (P@30). However we also provide the mean average precision(MAP), the precision at top 100 tweets (P@100), and the R-precision (R-PREC) as supplementary measures.
We conduct extensive comparative study to verify the en-hancement of the proposed query expansion technology. The retrieve models used in the comparative study include: (1) Lucene Baseline: It searches the original query in Lucene (2) PL2: It is a language modeling baseline by Terrier [26], which scores each document as the divergence from random-ness, with Poisson estimation for randomness, Laplace suc-cession for first normalization, and Normalization 2 for term frequency normalization. The default parameter is set to 10.99; (3) KLJM: The document-query score is computed as the KL-divergence between the document language model to the query model. We adopt JM smoothing for unobserved The recency language modeling baseline in [9], which pro-motes most recent tweets by adding a document prior; (5) Rocchio: It is the pseudo relevance feedback (PRF) query expansion according to the Rocchio formula. The initial results are obtained by RLM. We test the performance of all PRF methods with different numbers of pseudo-relevant tweets (from 20 to 50) and different numbers of expansion terms (from 5 to 20). Due to the limit of space, we only re-port the best performance generated by 10 expansion terms from 30 pseudo-relevant tweets. (6) BO1: Pseudo rele-vance feedback by Terrier, using the scoring model PL2, query expansion model BO1; (7) KL: Pseudo relevance feed-back by Terrier, using the scoring model JMKL, query ex-pansion by maximizing the KL-divergence of pseudo rele-vance to the collection; (8) MG: Dynamic pseudo relevance feedback (DPRF) with mixture Gaussian distribution, with  X  =0 . 5, n = 4, most 4 burst periods, scale parameter  X  =5. Top 30 documents retrieved by RLM on the original query are selected as pseudo relevance feedback; (9) LP: Dynamic pseudo relevance feedback (DPRF) with local power distri-bution, with n = 5 burst periods, scale parameter  X  =1. We do not limit the effect range R . Other settings are the same as MG; (10) SL: Dynamic pseudo relevance feedback (DPRF) with skewed linear distribution, with n = 2 burst periods, scale parameter  X  =7. Weset R =2,bymak-ing the assumption that each burst period lasts for 4 days. Other settings are the same as MG and LP. http://lucene.apache.org/ From Table 2, we have the following observations. 1) Among the four baselines, recency language model performs best, which validates the importance of introducing a non-uniform document prior in microblog retrieval. Language modeling approaches outperform naive Lucene baseline. PL2 outperforms KLJM, because PL2 favors longer tweets, while longer tweets are usually of higher quality. 2) PRF can in-crease the system accuracy, compared with their original scoring functions. However, the significance of the incre-ment depends on how well the baseline performs. 3) Dy-namic pseudo relevance feedback method performs best in terms of P@30, MAP, and R-PREC, no matter which distri-bution is used in quantifying the pseudo relevance prior. It nearly boosts the P@30 performance of Lucene baseline for two times. It also outperforms the recency based method by nearly 20%, and the traditional PRF by nearly 40%. Signif-icance tests show that all the DPRF methods outperforms the best comparative method RLM with a confidence level larger than 99% (denoted as ++ in Table 2), compared with the null hypothesis that DPRF is equivalent with RLM.
In this subsection, we compare the P@30 performance of various parameters for the three strategies used in dynamic pseudo relevance feedback. The effects of the number of de-tected burst periods n and the scale parameter  X  are shown in Figure 5. We have the following observations. 1) Gen-erally, dynamic pseudo relevance feedback is not extremely sensitive to parameters. The worst P@30 performance is higher than 0 . 405, which is better than all the baselines and PRF models. 2) An event is unlikely to have very few burst periods. Therefore n =1and n = 2 usually perform worst. However, since the corpus only consists of tweets published within 17 days, large n does not perform good. Appropriate value is n =4or n =5. 3)Forafixed n , the best scaling parameter is between  X  =3and  X  = 5 for mixture Gaussian distribution. The performance decreases when  X  is too small or too large. Note that a burst period usually lasts no more than 3 days, therefore the scaling parameter is likely to fit in a burst period. 4) For local power distribution, the big-ger  X  is, the more accurate the expansion terms are. Note that big  X  indicates smooth decay in local power distribu-tion. Similarly, for skewed linear distribution, the smaller  X  is, the higher performance can be achieved. Therefore it is safe to claim that the information propagation process in microblogosphere is a long process.
Note that after retrieving the relevant tweets, various doc-ument summarization methods can be adapted to form the storyline by extracting the most relevant tweets. In this sec-tion, we conduct experiments to compare the summarization performance of different approaches including our proposed one, aiming to show the advantages of using the Dominant Set and the Steiner Tree to generate the storyline from the summarization aspect.

The measurement used in this subsection is mainly based on Recall-Oriented Understudy for Gisting Evaluation (ROUGE)  X  an evaluation toolkit for document summarization [20] which automatically determines the quality of a summary by comparing it with the human generated summaries through counting the number of their overlapping textual units (e.g., n-gram, word sequences, and etc.). In particular, F-measure scores of ROUGE-2 and ROUGE-SU4 are presented for our experiments. 49 queries provided by TREC 2011 microblog track are used in the experiments. For each query, first, DPRF is utilized to retrieve the top 1,000 tweets, then 8 students are invited to manually generate the X  X toryline X  (50 tweets are selected) from these 1,000 tweets as the ground truth.
We compare our method with several well-known and re-cent summarization approaches including: 1. Random: randomly selects the sentence as the sum-2. MostRelevant: picks up the sentences which are most 3. Latent Semantic Analysis (LSA): identifies semanti-4. K-means: performs K-means over the sentences, then 5. Non-negative Matrix Factorization (NMF) [17]: per-6. Symmetric Non-negative Matrix Factorization (SNMF) [35]: 7. Spectral Clustering with Normalized Cuts (NCut) [31]: 8. Query-sensitive Mutual Reinforcement Chain (Qs-MRC) [37]: 9. Multi-Document Summarization using Submodularity 10. Dominant Set (DS only): Document summarization
The comparison of our proposed method (DS+ST) with other summarization method sispresentedinTable3. It can be seen from the results that our proposed DS+ST out-performs all the other summarization methods. In addi-tion to the comparison of DS+ST against the other sum-marization methods, we employ the standard t -test to de-termine whether the performance improvement of DS+ST over the others is statistically significant. The results show that the improvements of our DS+ST on both ROUGE2 and ROUGE-SU are significant. with a confidence level greater than 99% Distribution; (b) Skewed Linear Distribution Table 3: The comparison among different summa-rization methods. Notice that  X  X S X  denotes  X  X omi-nant Set X , and X  X T X  X epresents X  X teiner Tree X .  X ++ X  and  X + X  indicate that DS+ST significantly outper-forms the best comparative methods with a confi-dence level greater than 99% and 95%, respectively.
The good results of our method benefit from the follow-ing two aspects. (1) The Dominant Set algorithm (i.e., Algo-rithm1)usedinourmethodcanselecttweetswhicharesim-ilar to both the given query and all the other tweets. Thus it is not only good at extracting the representative informa-tion from the given sentences to form a reasonable summary, but also providing an appropriate mechanism to select the  X  X ominant X  nodes to generate storylines. (2) The Steiner Tree algorithm (i.e., Algorithm 2) is capable of detecting the  X  X utline X  of all the given sentences from the dominant nodes. Thus comparing with the other traditional summa-rization methods, it is able to generate more natural and logical storylines/summaries.

As a result, by combining the Dominant Set algorithm and the Steiner Tree algorithm, our proposed method is suitable for generating the  X  X toryline X  from the messages delivered by microblog services.
In addition to the above comparison with different meth-ods, we further study the summarization results by tun-ing the parameters of the Dominant Set algorithm and the Steiner Tree algorithm.

First of all, we study the Dominant Set algorithm by vary-ing the  X  X imilarity threshold X . We vary the threshold for the similarity between each tweet and the given query from 0.5 to 0.9 with a step size of 0.1 (totally 5 steps).
Secondly, one may notice that a key step before perform-ing the Steiner Tree algorithm is to pick up a root node. A good root node could start a good story from tweets. In gen-eral, a good root should satisfy two conditions: 1) it should start as early as possible in terms of the post date of the tweet; 2) it should be similar to the given query. Usually, we choose the earliest node within the Dominant set as the root. However, we also study how the X  X imilarity to the given query X  influences the final summarization results. In other words, the earliest node may not necessarily be the root, but a later node from which every node of the Dominant set is reachable in graph G can be the root as long as it is more similar to the given query. To choose the root, we vary the similarity to the given query from 0.5 to 0.9 by a step size of 0.1. The comparison results by tuning the parameters are shown in in Figure 6(a) and 6(b). Figure 6: (a) Similarity (between a node and the given query) threshold; (b) Similarity between Root and Query
We have two observations from Figure 6(a). (1) The se-lection of the similarity threshold does influence the sum-marization performance of the Dominant Set algorithm. An inappropriate similarity threshold may weaken the Domi-nant Set greatly. (2) It is hard to claim that a larger sim-ilarity threshold would result in a better performance. In fact, when the similarity threshold is greater than 0.6, the summarization performance decreases as the threshold in-creases. The intuitive explanation is that a large similarity threshold may induce the algorithm to omit some important tweets which are not similar enough to the given query.
The observation from Figure 6(b) is that as the similarity to the given query increases, the summarization performance on both ROUGE2 and ROUGE-SU keeps going down. By analyzing it, we find that a large similarity threshold could lead to a  X  X ate X  root. For example, a  X  X ate X  root may exactly match the query, however, it would start the story from the middle of the whole storyline. In such a case, the tweets before the storyline X  X  middle point are omitted, thus the evolving structure of the storyline is not well maintained.
Since storyline generation is a subjective process, to better evaluate the retrieved tweets and the generated storylines, we conduct a user survey. The subjects of the survey are 18 students at different levels and from various majors of a research university. In this survey, we randomly sample 10 queries and 500 English tweets. Each participant is asked to read these tweets and 3 queries, and compare the results of different systems in a random order from the following point of views: relevance, coverage, coherence, and overall satisfaction. A score of 1 to 5 needs to be assigned to each system according to the user X  X  satisfaction of the results. A rank of 5 (or 1) indicates that the result of the system is the most (or least) satisfactory. We implement the following sys-tems for comparison. (1) Top10-Recency: presents the top 10 retrieved tweets by the recency language model RLM on the original queries. (2) Top10-DPRF: presents the top 10 retrieved tweets using the DPRF query expansion. (3) Re-cencySum: performs document summarization based on the retrieved tweets using the recency language model. MSSF is used as the document summarization method since it ob-tains the best results in Section 6.3.1. (4) DPRFSum: per-forms MSSF based on the retrieved tweets using the DPRF query expansion. (5) RecencyTimeline: generates times-lines [39] based on the retrieved tweets using the recency language model. (6) DPRFTimeline: generates timelines based on the retrieved tweets using DPRF query expansion. (7) RecencyStoryline: generates storylines using the meth-ods proposed in Section 5 based on the tweets retrieved by the recency language model. (8 ) DPRFStoryline: generates storylines based on the tweets retrieved by DPRF query ex-pansion.
 Table 4: Survey Results: User ratings on different systems based on their satisfaction
Table 4 shows the user rated scores for each system. From theresults,wehaveobservationsasfollows. (1)Theper-formance of tweet retrieval is critical. The proposed DPRF query expansion approach outperforms the recency language method. (2) Although the listed top 10 query results are highly relevant to the query, there also exists high redun-dancy among the top-ranking query results, thus the cover-age and coherence of the results are poor. (3) Summariza-tion based results achieve higher overall satisfaction than the methods of listing top query results because it can help users better understand the tweets. (4) Users prefer struc-tured results such as timelines and storylines than pure text summaries. (5) The proposed storyline generation meth-ods outperform the timeline generation method because the structures contained in the storylines can assist users quickly grasp the event evolution.
Generating storylines from microblogs can shed insight into several fields, including event detection, short text min-ing, and text stream mining, etc. The proposed dynamic pseudo relevance feedback model is embedded in a sophis-ticated theoretical framework. Experiments show that it is robust to parameters. The heuristic strategy for finding minimum weighted Steiner tree on a dominant set of rele-vant tweets is efficient to produce summary and evolvement structure.

This is a pioneer work on generating storylines from social media. In the future, we will further improve the approaches in the following aspects. First, the density function in Sec-tion 4 attempts to model the event prior. However since the relevance set is usually small, the assumption may not be satisfied by the observation. Second, in our work, the number of burst periods is pre-fixed in the detection. Ad-vanced burst period detection methods can be investigated and incorporated into our current framework. Last but not least, our current storyline generation is based on multi-view tweet graph and we plan to investigate new frameworks for generating concise and coherent storylines.
 Chen Lin is partially supported by Shanghai Key Laboratory of Intelligent Information Pr ocessing under grant No. IIPL-2011-004, China Natural Scie nce Foundati on under grant No.61102136,61001013, the Natural Science Foundati on of Fujian Province of China under grant No .2011J05158), the Fundamental Research Funds for the Central Universities under Grant No.2011121049. Jingxuan Li and Tao Li are partially supported by US NSF grants IIS-0546280, HRD-0833093, and CNS-1126619 and DHS grants 2009-ST-062-000016 and 2010-ST-062-000039.
