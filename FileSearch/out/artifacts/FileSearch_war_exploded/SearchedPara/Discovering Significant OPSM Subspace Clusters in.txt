 Order-preserving submatrixes (OPSMs) have been accepted as a biologically meaningful subspace cluster model, captur-ing the general tendency of gene expressions across a subset of conditions. In an OPSM, the expression levels of all genes induce the same linear ordering of the conditions. OPSM mining is reducible to a special case of the sequential pat-tern mining problem, in which a pattern and its supporting sequences uniquely specify an OPSM cluster. Those small twig clusters , specified by long patterns with naturally low support, incur explosive computational costs and would be completely pruned off by most existing methods for massive datasets containing thousands of conditions and hundreds of thousands of genes, which are common in today X  X  gene ex-pression analysis. However, it is in particular interest of bi-ologists to reveal such small groups of genes that are tightly coregulated under many conditions, and some pathways or processes might require only two genes to act in concert. In this paper, we introduce the KiWi mining framework for massive datasets, that exploits two parameters k and w to provide a biased testing on a bounded number of candidates, substantially reducing the search space and problem scale, targeting on highly promising seeds that lead to significant clusters and twig clusters. Extensive biological and compu-tational evaluations on real datasets demonstrate that KiWi can effectively mine biologically meaningful OPSM subspace clusters with good efficiency and scalability.
 Categories and Subject Descriptors: H.2.8 [Database Applications]: Data Mining; J.3 [Life and Medical Sciences]: Biology and Genetics General Terms: algorithms, performance Keywords: twig cluster, order-preserving submatrix, sub-space clustering, gene expression data, scalability
As an incomparable breakthrough in experimental mole-cular biology, DNA microarrays, serial analysis of gene ex-Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. pression (SAGE) and similar technologies have enabled a wave of extraordinary genomewide investigations of gene ex-pression, allowing the monitoring of activities of many genes over many different conditions. The resulting expression datacanbeviewedasan n  X  m matrix with n genes (rows) and m conditions (columns), in which each entry gives the expression level of a given gene under a given condition. Clustering is a major tool for gene expression analysis. Coexpression of genes in a cluster can be used to infer func-tional associations between genes and identify coregulation. Since most genes are expected to be tightly coregulated only under certain conditions, subspace clustering has gained pop-ularity in recent years. Pattern-based subspace clustering, where clustering is performed by pattern similarity rather than distance, is particularly meaningful due to the fact that coregulated genes are not necessarily expressed at the same (or even similar) absolute expression levels. For example, a transcription factor may be able to perform its function at a very different concentration fro m its target genes. Recently, order-preserving submatrixes (OPSMs) [6] have been intro-duced and accepted as a biologically meaningful pattern-based subspace cluster model. An OPSM, essentially a sub-space cluster, is a subset of rows and columns in a data matrix where all the rows induce the same linear ordering of the columns, as shown in Figure 1. An OPSM cluster may arise when the expression levels of the coregulated genes rise and fall synchronously in response to a sequence of environ-ment stimuli. Discovery of significant OPSMs can play an essential role in inferring gene regulatory networks.
The OPSM cluster model focuses on the relative order of columns rather than the uniformity of actual values in data matrixes. By sorting the row vectors and replacing the entries with their corresponding column labels, the data matrix can be transformed into a sequence database, and OPSM mining is reduced to a special case of the sequential pattern mining problem with some unique properties. In particular, the sequence database is extremely dense since each column label appears exactly once (assuming no miss-ing values) in each sequence. A sequential pattern uniquely specifies an OPSM cluster, with all the supporting sequences as the cluster contents. The number of supporting sequences is the support for the pattern.

As costs of gene expression analysis continue to decrease, the numbers and sizes of expression datasets have been grow-ing at an ever-increasing rate. To give just two examples, the Gene Expression Omnibus ( GEO) currently records over 70,000 conditions for over 100 different organisms [5] and the Stanford Microarray Database (SMD) contains over 10,000 public experiments for 21 organisms [15]. With expression datasets of potentially tens or hundreds of thousands of both rows and columns, there is a need for algorithms that can handle not only  X  X arge datasets X  but  X  X assive datasets X .
Protein-protein interactions, biological pathway member-ship and coregulation demonstrate  X  X ower law X  relation-ships[3]. Thatistosay,wetendtoobserveasmallnumber of very large gene groups and a large number of very small groups. For gene coregulation, the smaller groups, roughly under 50 genes, are of particular interest since in many cases biologists are looking for tissue or condition specific gene reg-ulation related to some highly specialized process or disease. Some pathways or processes may require only two genes to act in concert. In the context of OPSM discovery, we use the term twig clusters to denote those small clusters specified by long patterns with naturally low support, corresponding to the  X  X wigs X  in a pattern search tree. Discovery of twig clus-ters is of essential biological importance.

Most existing sequential pattern mining methods, breadth-first or depth-first, aim at finding a complete set of patterns and rely on some minimum support threshold, min sup ,to prune the search space. However, the discovery of twig clus-ters requires the smallest possible min sup of 2, making the pruning futile. The challenge becomes even more onerous in OPSM mining due to the fact that the transformed sequence database is extremely dense. Therefore, existing methods do not scale to massive gene expression datasets for small min sup values, failing to produce any twig clusters.
To address these challenges, we propose the mining frame-work KiWi that exploits two parameters k and w to perform a biased testing on a bounded number of candidate patterns, targeting significant OPSMs with a focus on twig clusters. KiWi performs a beam search, at each level maintaining only the k mostpromisingpatternsasseedsforextension in further levels. When testing candidate patterns, KiWi considers only a vertical slice of width w of the supporting sequences, instead of the entire suffixes, ensuring the k spots are reserved for seeds that are likely to be extended into longer patterns. For ranking statistic, KiWi uses weighted support, taking the distribution of data into consideration.
Our main contributions are as follows: 1. We introduce the KiWi framework for mining signif-icant OPSM subspace clusters in massive gene expression datasets. The framework is also applicable to mining se-quential patterns from dense, massive datasets in general. 2. We present a corresponding algorithm that imple-ments the KiWi framework, tailored to the OPSM problem, providing candidate testing optimizations, memory manage-ment and seed management. 3. As an add-on, KiWi can also discover generalized OPSMs where the expression levels of genes induce either the same or opposite linear ordering of the conditions, cap-turing anti-correlations among genes as well. 4. Our comprehensive experimental study on real datasets demonstrates that KiWi can find numerous biologically mean-ingful twig clusters that existing methods fail to; in addition, KiWi scales nicely to massive datasets.

The rest of the paper is organized as follows. Section 2 introduces the objective of the paper and reviews related work. Section 3 proposes the KiWi mining framework and presents the main algorithm. Section 4 reports the experi-mental results, and Section 5 concludes the paper.
Gene expression data can be represented as an n  X  m ma-trix D containing expression levels for n genes (rows) under m conditions (columns). A submatrix S =( R  X  C ), where R is a subset of rows and C a subset of columns in D ,is called an order-preserving submatrix (OPSM) [6] if there is a permutation of the columns in C under which the sequence of expression values in each row of R is strictly ascending. If the constraint is relaxed such that the sequence can be either strictly ascending or descending, S becomes a gener-alized order-preserving submatrix (GOPSM). GOPSMs are able to capture anti-correlations among genes, which can im-ply common process/pathway membership or negative reg-ulation. One gene may repress the expression of other genes (negative regulators) and anti-correlated genes might repre-sent members of opposing pathways. Figure 1 illustrates the concepts of OPSM and GOPSM.

As motivated in the introduction, we want to find signifi-cant OPSMs in massive gene expression data. While various significance measures can be defined, it is consensus that for fixed # columns (# rows), larger # rows (# columns) leads to more significance. Rather than combining them into a single measure, we consider the competing goals of maxi-mizing # columns and # rows in a bicriteria optimization problem. The feasible region of such a bicriteria problem, as shown in Figure 2, contains all feasible solutions, each being a (# rows, # columns) pair such that there exists such an OPSM in the given data matrix. A feasible pair can map to multiple OPSMs. The significant region contains par-tially ordered feasible solutions that are close to the Pareto optimums. While the closeness can be defined in various ways, one straightforward approach is to threshold # rows for each # columns, such that there are no more than t number of OPSMs with their # rows  X  the threshold. The twig region is part of the significant region that is deepest and hardest to reach, containing the longest terminals of the search tree (space). Note that the search space for pat-tern mining can generally be organized in a tree structure, e.g., Rymon X  X  generic set enumeration tree [14] for frequent pattern mining. In this paper, we use significant clusters and twig clusters to denote OPSMs having their (# rows, # columns) pairs falling in the corresponding regions.
In Figure 2,  X  is an imaginary lower bound of min sup , reflecting the minimum min sup for which the complete set of frequent patterns can be efficiently mined using existing algorithms. It may move down with the growth of com-putational power, and move up due to the ever-increasing complexity of data. For dense and massive datasets, a large part of the significant region is normally pruned off, and the complete twig region is always in the prunings. Note that the Pareto front in Figure 2 serves only for the purpose of ex-planation. The dashed curve is a more realistic illustration of the shape of the Pareto front, showing more clearly how severe the pruning damage can be. In our experiment, Pre-fixSpan [13], one of the fastest sequential mining algorithms, did not return after 48 hours on real dataset human-SMD-cDNA (12671 genes and 2852 conditions) [15] for min sup = 2535. It returned after 36 hours on Affymetrix (12332 genes and 1640 conditions) [5] for min sup = 1234 with the maximum pattern length of 7. If we consider  X  = 1234, then it would cross the Pareto front at # columns = 7, while the longest pattern with support  X  2 has length of 161.
The objective of the paper, is to discover significant OPSM subspace clusters from massive gene expression data, with a focus on twig clusters with size as small as 2.
Conventional sequential pattern mining was motivated and introduced [2] in the context of transaction databases, where a sequence is an ordered list of itemsets. A common subse-quence with support beyond the minimum support thresh-old, min sup , is called a sequential pattern. The sequential pattern mining problem is to find the complete set of se-quential patterns with respect to min sup .

OPSM mining can be reduced to a special case of the se-quential pattern mining problem. If we sort each row in the data matrix D in ascending order, and replace the entries with their corresponding column labels, then D is trans-formed into a sequence database, as shown in Figure 3 (b). Each sequential pattern uniquely specifies an OPSM cluster, with all the supporting sequences (sequences containing the pattern) as the cluster contents. The number of supporting sequences is the support for the pattern, i.e., the cluster size. Comparing to conventional sequential pattern mining, the OPSM problem bears some spec ial properties. First, each sequence in the transformed sequence database is an ordered list of 1-item itemsets. Second, each column label (item) in the alphabet appears at most once in the sequences; or, ex-actly once in the absence of missing values. Third, even if with a small alphabet, sequences can be long, easily contain-ing thousands of items (conditions). These properties imply that the transformed sequence database in the OPSM prob-lem is extremely dense and high-dimensional.

In principle, with an additional scan to obtain the sup-porting sequences for each pattern, sequential pattern min-ing methods are applicable to OPSM discovery. However, the dense and high dimensional nature of the transformed sequence database renders min sup -based methods infeasi-ble for massive datasets with low threshold settings.
Subspace clustering (e.g., [1]) aims at discovering clusters embedded in subspaces. Measuring similarity by pattern rather than distance, pattern-based subspace clustering has proved to be particularly meaningful in the application of gene expression analysis. Biclustering [7] discovers local co-herence of genes and conditions in a submatrix of a DNA array. In the biological sense, genes in such clusters have the same amount of response to the conditions.  X  -pCluster [18] models clusters that exhibit shifting or scaling patterns. Scaling pattern can be transformed into shifting pattern by applying a logarithmic function on the raw data. Under shifting pattern, the expression levels of all genes in a clus-ter rise and fall coherently under a subset of conditions.
The  X  -pCluster model can be too restrictive in many ap-plications. As a relaxation, order-preserving submatrixes (OPSMs) [6] are essentially subspace clusters in which the expression levels of all genes induce the same linear or-dering of the conditions. As the OPSM problem is NP-hard, a model-based algorithm is given in [6] to find the  X  X est X  OPSM according to a hard-coded statistical mea-sure. Scalability issues are not considered. The proposed algorithm requires excessive computational resources if ap-plied to large gene expression matrixes. OP-clustering [12] generalizes the OPSM model by grouping attributes into equivalent classes. Their proposed method, OPC-tree, per-forms exhaustive enumeration as in conventional sequential pattern mining, and fails for the same reason to obtain most significant subspace clusters from massive datasets.
Sequential pattern mining was introduced in [2]. Existing methods are essentially either breath-first such as GSP [16] or depth-first such as PrefixSpan [13]. PrefixSpan is known as one of the most efficient algorithms so far.
With the objective to discover significant OPSM subspace clusters including twig clusters in massive datasets, KiWi utilizes two parameters k and w to provide a biased testing on a bounded number of candidates, substantially reduc-ing the search space and problem scale, targeting on highly promising seeds that are likely to lead to long patterns.
We use pattern to denote an ordered subset of column la-bels (pattern elements), e.g., p i =[ p 1 ,p 2 , ..., p i of length i and p i +1 = p i + p i +1 . p i can be used to denote either a candidate or a seed (qualified candidate) at level (iteration) i as they are generated in a level-wise manner, starting with level 1. Conceptually, all seeds from level i are extended by each possible single element, creating candi-dates for level i . Note that this candidate generation step is avoided in the actual implementation. These candidates are assessed using a certain statistic, and the k candidates with the highest values of this statistic are retained as the seeds of level i . The number of candidates is thus upper-bounded by k  X  m and the search space is selectively restricted. Based on the observation that a long pattern segments its supporting sequences into small sections, the elements of a long pattern can be expected to appear early, say, in the next w posi-tions of the supporting sequences. Thus to test candidates, KiWi considers a vertical slice of width w of the supporting sequences, instead of the entire suffixes.

For each seed p i ,the projected database for p i , PD ( p is maintained containing the sequences supporting p i under the w -constraint , that is, a sequence in PD ( p i ) is segmented by the elements of p i into sections of length no more than w except for the last section. Rapidly shrinking, the projected databases provide a horizontal slicing of the sequence data-base and reduce the problem scale effectively from iteration to iteration. The usage of w further performs a vertical slic-ing on the projected databases, dramatically reducing the problem scale since w is usually very small comparing to m .The truncated projected database for p i with respect to w , TPD ( p i ,w ), contains some truncated regions of width at most w , each for a sequence in PD ( p i ) called window ( p that immediately follows the appearance of p i .

In testing candidate p i +1 ,only TPD ( p i ,w ) is considered, which does not only substantially improve efficiency, but is also necessary for long pattern discovery. Should we con-sider the entire suffixes of the sequences, some unpromising candidates that appear very late in the sequences would gain high ranks to take many of the k spots that are reserved for promising seeds, and the iteration would soon terminate and fail to grow long patterns.

To rank candidates, support seems to be an apparent and natural choice as the statistic. Experiments also show this choice can produce reasonably good results. Nevertheless, support does not consider the distribution of column labels in the sequence database. Moreover, many candidates may have tied support values thus cannot be well distinguished. We introduce the novel concept of weighted support . While each supporting sequence contributes 1 to support, its con-tribution to weighted support depends on  X  X ow it supports X , i.e., on the position of the supported pattern in the sequence. Specifically, let r be a sequence supporting a candidate p suffix ( p i ,r )isthesuffixof r starting at element p i ,whose length, | suffix ( p i ,r ) | , signals the potential of p to grow in r . The weighted support for p i is defined as the summation of all such suffix lengths over the supporting se-quences of p i , as exemplified in Figure 3 (b). The usage of weighted support significantly improves the quality of seeds, leading to the discovery of longer patterns with even small k values. Also, it makes w an insensitive parameter.
All the seeds from different levels constitute the output pattern set SEEDS ( k, w )  X  Complete (2), where Complete (2) is the complete set of patterns with support of at least 2. Figure 3 (e) shows Complete (2) in a tree structure. Once k and w are determined, SEEDS ( k, w ) is determined. In KiWi , p l  X  SEEDS ( k, w )  X  p i  X  SEEDS ( k, w )for1  X  i that is, for p l to be discovered, each of its prefixes p appear among the top k seeds in the i th iteration.
Running example. In Figure 3, the raw data matrix with 4 conditions and 6 genes in (a) is transformed into a 4  X  6 sequence database as shown in (b). (b)  X  (d) ex-emplify the PDs and TPDs (the shaded areas) along the growing process of pattern [0, 1] with respect to w =2, from which we can see how the problem scale is greatly re-duced level by level. (e) shows Complete (2) and the actual support for each pattern. (e) also shows the pattern discov-ery process for k =6and w = 4, in which support is used as the statistic for simplicity of illustration. For each level of 0 to 4, the non-gray-colored patterns are candidates and the dark-colored ones are eventually chosen as seeds. The results in (e) are reasonably good, but one longest pattern, [2 , 3 , 1 , 0] is missing. Level 2 of (e) also shows the ranking problem that, some candidates with support 3 are chosen as seeds, some not. (f) shows the mining process when w is adjusted to 2. The results in (f) are very good with both the longest patterns discovered, together with most of the more frequent patterns at each level. Comparing to (e), the full length case, the support values are suppressed but in a biased fashion. Some candidates appearing early have their support values less suppressed, thus they are favored and have increased probabilities to emerge as seeds.

When weighted support is used, w is insensitive. For w =4,3,or2,exactlythesameresultsarereturnedasin(f). The underlined numbers in (f) for level 1 show the weighted support values for the candidates, e.g., the weighted support for [0] is 10, which can be verified in (b). In (b), the three 0-suffixes for sequences supporting [0] are underlined. Note that only TPD ([ ] , 2) is considered in testing candidate [0]. From (f) we can also see that, if weighted support is used, even for k = 2, the two longest patterns will still be returned.
More discussions and insights on the ranking statistics and choices of k and w are detailed in [8]. Existing depth-first methods such as PrefixSpan [13] cannot perform within-level comparisons to keep the most promising patterns to work on; thus most computations are wasted on recursively mining patterns that are eventually shown to be insignificant, de-spite their advantage of parsimonious memory usage. Most existing breath-first methods such as GSP [16] are inferior to PrefixSpan due to various reasons [13], the large memory consumption is a serious inherent problem. KiWi , however, can well-bound the memory usage and significantly reduce the search space and problem scale due to the introductions of k and w .Notethatif k is big enough and w is set to m , KiWi returns the complete set of patterns.

Algorithm 1 gives the pseudocode for the KiWi pattern discovery algorithm with certain low level details.
Overview. Column labels are renamed with integers 0 to m  X  1 before the raw data matrix is transformed into the sequence database SB ; thus each sequence in SB is a per-mutation of a subset of { 0 , 1 , ..., m  X  1 } . The output pattern set SEEDS contains seeds from different levels, at most k for each level and organized as linked lists in order to save space. seeds is a list of seeds, each being a pattern and a list of elements conceptually (physically, an element in some linked list). [ ] denotes the empty list. projections is a list of projected databases, each for a seed in seeds with the same index, represented as a list of supporting sequence indexes. mSeeds is a list of integers used for statistic counting, which always has size of m  X | seeds | since we preserve m positions for each seed. The data structures seeds , mSeeds ,and pro-jections are reused from iteration to iteration.
 Initially, seeds contains one empty seed [ ] (line 1), and PD ([ ]) contains all the n sequence indexes (line 2). As long as seeds is not empty (line 3), we continue to grow the seeds. In each iteration, we first perform candidate testing (line 4, 5, 6, 7, 8), then, seeds is updated with the (at most) k can-didates having top values in mSeeds (line9). Theprojected databases are updated as well for the seeds in seeds (line 10). Finally, seeds gets rid of the seeds with support less than 2 (line 11) and is written to disk (line 12). For effi-ciency, instead of recording the supporting sequence indexes for each candidate during testing, we update projections af-ter the update of seeds , then only at most k seeds need to be processed and the update takes negligible time.
Candidate testing. Generally, support counting is the major time consumer in frequent (sequential) pattern min-ing. KiWi achieves great efficiency in statistic counting due to several reasons. First, since only the top k seeds are al-lowed to grow, the number of candidates is well bounded and the search space is selectively restricted. Second, the hor-izontal (line 6) and vertical (line 7) slicing of the sequence database dramatically reduce the problem scale from the en-tire database to a particular truncated projected database.
While these principles guarantee an efficient runtime com-plexity of candidate testing, technically, KiWi is also care-fully designed to achieve the best efficiency within the com-plexity. m is not big comparing to conventional sequential pattern mining; thus, we preserve m positions in mSeeds for each seed since it has at most m ways to grow one more el-ement. This fixed 1-to-m correspondence allows us to avoid explicit candidate generation. Note that the candidate gen-eration and pruning time is not trivial in GSP like methods when the database is dense. In more details, let seeds [ s ]+ e be a candidate with e as the newly appended element, then, s = m  X  s + e , meaning that the statistic incrementing for the candidate will be done in mSeeds [ s ] (line 8). Conversely, knowing s , an index in mSeeds ,wehave e = s mod m and s = s div m , meaning that the statistic stored in mSeeds [ s ] is for candidate seeds [ s ]+ e . This converse derivation is used in line 9 to update seeds , growing the linked lists properly. In Algorithm 1, lines 4  X  8 perform statistic counting. First, mSeeds is initialized with m  X | seeds | number of 0 X  X  (line 4). Then, for each seed in seeds (line 5), for each ele-ment in the truncated projected database for the seed, (lines 6, 7), weighted support is incremented for the corresponding candidate (line 8), precisely, seeds [ s ]+ e .
 Algorithm 1 KiWi pattern discovery Input: n  X  m sequence database SB , k , w Output: SEEDS ( k, w ) 1: seeds = [[ ]]; 2: projections = [[0 , 1 , ... n  X  1]]; 3: while ( | seeds | != 0) { 4: mSeeds .initialize( m  X | seeds | , 0); 5: for each seed in seeds with index s 6: for each sequence r in projections [ s ] 7: for each element e in window ( seeds [ s ] ,w )of r 8: mSeeds [ m  X  s + e ]+= | suffix ( e, r ) | ; 9: seeds .update( k, mSeeds ); 10: projections .update( seeds ); 11: seeds .cleanup(); 12: seeds .writeup(); }
Other techniques used by KiWi in pattern discovery, such as the use of inverse SB ( ISB ), memory management and seeds management are detailed in [8].

Complexities. The runtime of KiWi is dominated by candidate testing in lines 4  X  8 of Algorithm 1. By upper-bounding the number of visits of the TPDs , we can obtain the worst case complexity as O ( lkwn ), where l is the length of the longest pattern. Note that for each pattern, there are at most n supporting sequences. For the average case analysis we assume a random data matrix. In such a matrix, each pattern of length i is on average supported by n  X  1 number of sequences. Knowing that i !  X  2 i for i  X  1, the average case runtime is O ( kwn  X  ( 1 1! + 1 2! + ... + 1 ( average case runtime is linear in n for fixed min sup of 2. Some other algorithms also show similar empirical results, but their support is defined as a fraction of n , i.e., min sup increases proportionally with the increase of n .
The worst case space complexity is O ( mn + km + kn ), where mn is for SB and ISB , km for mSeeds ,and kn for projections since each pattern can be supported by at most n sequences. The memory taken by seeds is O ( k ) and neg-ligible. It is reasonable to assume SB can be well accom-modated in memory, and normally n m ,thus projections would incur the largest memory consumption. In practice, the support for each seed reduces dramatically along the growing of the seed. It can be very big for the first iteration, but in that case the number of patterns is upper bounded by min( k, m ). Some memory management techniques are also employed to avoid memory spill caused by projections . Overall, the memory consumption in KiWi is well bounded.
OPSM formation and GOPSM mining. To actually form OPSM clusters, we need to retrieve the supporting sequences for the output patterns, which were not stored for space efficiency. We provide a pattern querying phase following pattern discovery, allowing users to specify inter-esting patterns, and only for which SB is scanned to form the corresponding OPSMs. A pattern extension heuristic is also applied for the patterns after obtaining their support-ing sequences. Surprisingly, only some simple adjustment is sufficient to allow KiWi to mine GOPSMs, i.e., doubling SB by appending a list of reversely ordered sequences, or more efficiently, mining SB forward and backward simulta-neously. [8] has more details regarding OPSM formation, pattern extension, and GOPSM mining.

Comprehensive experiments were performed on massive real and synthetic datasets for biological and computational evaluations of KiWi . The real datasets included Affymetrix (HG-U133A) experiments from GEO (GPL96) [5], SAGE libraries also from GEO (GPL4), and cDNA experiments from SMD [15]. Affymetrix probes, cDNA clones and SAGE tags were normalized and mapped to gene/protein identifiers as previously reported [9]. For comparison with existing algorithms, two small datasets, SU [17], a smaller dataset of Affymatrix (HG-U95A) experiments, and breast cancer [10], were also used. Table 1 lists the sizes of these datasets. The experiments were run on a 2.8GHz/2GB Windows machine.
Biologically meaningful groups of coregulated genes should tend to have common (statistically over-represented) biolog-ical processes and transcription factor binding sites (TFBS). In this series of experiments, we use two methods to demon-strate that KiWi is able to identify clusters from gene ex-pression datasets of very large size, that are consistent with biological expectation for coregulated genes.

Methods. The first method uses the Gene Ontology (GO), a set of structured, controlled vocabularies to iden-tify functional associations between gene products [4]. Cur-rent GO annotation and external reference files were down-loaded from the Gene Ontology Annotation resource at EBI (www.ebi.ac.uk/GOA). Using these files, and the Ensembl Perl API (www.ensembl.org) the gene identifiers in the clus-ter data were mapped to protein identifiers compatible with GO. Each cluster of protein IDs was then submitted to the High-Throughput GoMiner command-line interface [19]. Statistically over-represented GO terms were defined using a Fisher X  X  exact test and corrected for multiple testing by false discovery rate detection (100 permutations). The sec-ond method uses the oPOSSUM tool to identify statistically over-represented transcription factor binding sites (TFBS) [11]. The oPOSSUM API was downloaded and installed (www.cisreg.ca/cgi-bin/oPOSSUM/opossum). Each cluster of genes was submitted to the software and statistically over-represented TFBSs were defined using the Z -score option.
Results. Biological validation was performed on the affy-metrix dataset with 12332 genes and 1640 conditions. For the validation, a set of representative clusters were chosen with size from 5 to 10 and minimum dimensionality of 15. A total of 634 clusters met these criteria, with 22 clusters containing anti-correlated genes. The GO analysis shows that clusters identified by KiWi are significantly more likely to share a common biological process than random expecta-tion (Figure 4 (a)). For example, if we consider a P -value threshold of 0.01, more than 10% of clusters have at least one significant GO term compared to the random expecta-tion of close to zero. Similarly, the TFBS analysis shows that clusters identified by KiWi are significantly more likely than random expectation to share sequences bound by the same transcription factor (Figure 4 (b)). For example, if we consider a Z -score of 30, more than 10% of clusters have at least one TFBS over-represented in the regulatory regions for these genes. Random expectation for this same Z -score threshold is close to zero. To summarize, KiWi success-fully identifies small groups of correlated and anti-correlated genes that belong to a common function or process and/or share common transcription factor binding sites.
In this series of experiments, we show that KiWi is able to effectively discover significant OPSM clusters including twig clusters using real datasets; we also demonstrate the efficiency and scalability of KiWi using synthetic data.
Discovery of twig clusters. To demonstrate KiWi can well reach the twig region, we compared KiWi with LoPad ,a dynamic programming based algorithm that finds the (real) longest pattern with respect to min sup of 2. Table 1 reports the longest pattern length and running time (in seconds) for LoPad and KiWi respectively. We observe that in all cases KiWi was able to find the optimal results or very close, in significantly shorter time. Note that LoPad ,withruntime complexity of O ( m 2 n 2 ), did not return after one week for the cDNA dataset and the program was terminated.
 Table 1: KiWi vs. LoPad : longest pattern discovery.
Discovery of significant clusters. To assess the statis-tical significance of an OPSM, [6] computes an upper bound on the probability that the corresponding model would ap-pear in a random matrix, and their algorithm (referred to as Alg ) tries to find the most significant OPSM one at a time. They reported several significant OPSMs with differ-ent pattern lengths on the breast cancer dataset. While they did not give the runtime, KiWi , in just 1.47 seconds, found much better results for every case they reported as shown in Table 2. In addition, since this is a very small dataset, we were able to set min sup = 2 and obtain the results from PrefixSpan in 2085 seconds (with output file size of 12 GB), from which we can see that our results are actually optimal.
We have defined significant clusters as OPSMs having their (# rows, # columns) pairs falling in the significant region. We also gave a quantitative definition of the signif-icant region, i.e., to threshold # rows (support) for each # columns (pattern length), such that there are no more than t number of OPSMs with their # rows  X  the threshold. To evaluate the coverage of significant clusters by KiWi ,we used the small breast cancer dataset since it was the only one we obtained output from PrefixSpan for min sup =2. For t = 100 and 1000 (values inside the brackets) respectively, Table 3 shows the support range above the threshold, the number of significant clusters returned by PrefixSpan and KiWi , together with the coverage for each pattern length of 1  X  16. The KiWi results were obtained in 24 seconds, about 1.2% of PrefixSpan , but they cover on average 92.6% ( t = 100) and 80.7% ( t = 1000) of the significant clusters. Table 2: KiWi vs. Alg : significant OPSM discovery.
Efficiency and scalability. We used synthetic data (random matrixes of different sizes) for the runtime analysis of KiWi . The results shown in Figure 5 (a)  X  (c) confirm the average case complexity of O ( kwn ). In (d), the runtime does not show a clear trend when varying m , which is con-sistent with our complexity analysis. (c) and (d) together show that KiWi scales nicely with respect to n and m .
Accepted as a biologically meaningful subspace cluster model, order-preserving submatrixes (OPSMs) capture the general tendency of gene expressions across a subset of con-ditions. Biologists are particularly interested in OPSMs consisting of a small number of genes sharing expression patterns over many conditions, which we call twig clusters. Most existing methods fail to discover those twig clusters in massive datasets due to the explosive computational costs. In this paper, we introduced the KiWi framework for mining significant OPSM subspace clusters in massive gene expres-sion data, focusing on twig clusters. KiWi exploits two pa-rameters k and w to perform a biased testing on a bounded number of candidates, keeping only highly promising seeds that will likely lead to significant clusters and twig clus-ters. Our extensive experiments on real and synthetic data demonstrated that KiWi scales well to massive datasets and can discover numerous biologically meaningful OPSM sub-space clusters that cannot be mined using existing methods.
The concept of twig clusters and the KiWi framework are not limited to OPSM mining, but applicable to sequential pattern mining in general for dense and massive datasets.
