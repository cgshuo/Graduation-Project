
The Live Nugget Extractor system provides users with a method of efficiently and accurately collecting relevant in-formation for any web query rather than providing a sim-ple ranked lists of documents. The system utilizes an online learning procedure to infer relevance of unjudged documents while extracting and ranking information from judged doc-uments. This creates a set of judged and inferred relevance scores for both documents and text fragments, which can be used for test collections, summarization, and other tasks where high accuracy and large collections with minimal hu-man effort are needed.
 Categories and Subject Descriptors: H.3.3 [Information Search and Retrieval] Keywords: information retrieval; relevance assessment; text extraction; test collection; nuggets
With Information Retrieval research increasingly moving from document retrieval to retrieval of text fragments, the needs is growing for systems which can create new test col-lections based on smaller text units and give users a way to guide their search for text answers, rather than ranked lists of documents. To this end, we have created an interface that allows a user to input a query and iteratively judge both documents and text fragments, or nuggets, for rele-vance 12 . These judgments are actively incorporated into a relevance model and new documents and nuggets are pre-sented to maximize relevant information returned.

We have previously shown that the nuggets system allows for test collections to be created with high information re-call and accuracy of inferred document relevance using a relatively small number of judgments [3]. Further, it creates
A live demonstration of the interface can be accessed at http://fiji.ccs.neu.edu/nuggets/demo/ .
We gratefully acknowledge support provided by NSF IIS-1256172.
 collections which can be used to evaluate both document-centric and text-centric systems by extracting text fragments during the judging process. The system has been used for the English track of NTCIR-10 1CLICK-2 and adapted to Japanese to compare to a manual methodology.

The demo system presented here builds upon our previ-ous work [3] for test collection creation by creating a live system which accepts a query from the user, presents doc-uments and nuggets for judgment feedback, then reweights documents and nuggets according to the feedback. The mo-tivation for this is to create a collection of documents to pass into summarizer systems and simultaneously find the text against which to evaluate the output of such systems.
To the best of our knowledge, there exists no similar tool for collecting relevant information without fully manual fact-base extraction. Pooling methods using ranker systems fol-lowed by exhaustive judging or statistical sampling are com-mon, but neither can scale with modern collections. TREC, a U.S. government effort, coordinates professional assessors to judge up to 3,000 documents per query, but even this effort has fatigue-driven errors [1] and many relevant docu-ments not even considered for assessment [2].

Although direct relevance feedback at the document level is too expensive in web search, our active framework for judging and refining limits the manual work required and provides inferred relevance of text fragments. This is a highly expensive operation if performed manually, more com-parable to TREC X  X  track on document assessment, which re-quired considerably more manual work than our system. In previous work, we showed that our method also outperforms both Relevance Feedback and Learning to Rank schemes.
Basic query refinement and expansion exist in popular search engines, providing a query X  X  common similar searches and related entities. This, however, is not based on the information relevant to the query, and does not allow the user to see ranked text fragments based on this refinement. As we will show, our system allows the user to guide the search both for types of relevant documents and information, while easily viewing the progress of the inference and the refined set of relevant information.
The interface for the Nugget Extractor system presents the user with the ability to enter a query and then perform panel by inferred relevance, with judged nuggets visible in lower panels. iterative judgments on documents and nuggets to grow the space of relevant information.

The left side of the interface allows the user to read a document or an expandable ranked list of documents, and judge them for relevance. The right side of the interface allows the user to view the information that has been ex-tracted so far, sorted by inferred relevance, and optionally to judge individual nuggets.

Additionally, a user may manually create a nugget. This can be useful to expand the scope of the search, in a similar fashion to iterative query reformulation. The nugget can be in any category, to allow for both positive and negative examples to aid refinement of the relevance space.

Once a user feels they have completed their search, they may export lists of relevance for nuggets and documents. They may also see lists of unjudged items sorted by their inferred relevance scores.
The User Interface provides the interaction for the man-ual portions of the extraction system, but most of the work is done behind the scenes. We use an iterative procedure to incorporate judgments on documents and nuggets into a belief graph of relevance, incorporating the observations and modifying the weights based on matches between documents and judgments. This procedure can be seen in full in our previous work [3].
 Figure 2 illustrates the backend system, where our TEXT MATCHING system creates the links between documents and nuggets, allowing assessments on either side to influence the inferred relevance scores for all other objects.
When a user enters a query, a set of candidate documents are retrieved and cleaned for the interface and then the first set of documents is displayed to the user. When the user judges a document, it is placed in the QREL and the backend procedure extracts nuggets from the document if it is marked relevant. Then all nuggets which matched the document are reweighted according to its judgment. Finally, the new Figure 2: The overall design of assessment process: Iteratively, documents are selected and assessed, and nuggets are extracted and [re]weighted. nuggets are also matched against all documents, and new document scores are computed from the new weights on all nuggets. Judging a nugget reweights the documents which match it, as does creating a new nugget.
Our Nugget Extractor system allows users to cover the relevant information space of a large number of documents without having to fully read and extract information from each one. This is useful in creating lists of facts for a query, creating document or nugget test collections with both ex-plicit and implicit relevance judgements, and in guided search. [1] B. Carterette and I. Soboroff. The effect of assessor [2] V. Pavlu, S. Rajput, P. B. Golbus, and J. A. Aslam. IR [3] S. Rajput, M. Ekstrand-Abueg, V. Pavlu, and J. A.
