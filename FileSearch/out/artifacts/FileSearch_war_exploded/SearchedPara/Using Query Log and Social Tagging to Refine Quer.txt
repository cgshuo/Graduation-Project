 An important way to improve users X  satisfaction in Web search is to assist them to issue more effective queries. One such approach is query refinement (reformulation), which generates new queries according to the current query issued by users. A common procedure for conducting refinement is to generate some candidate queries first, and then a scoring method is designed to assess the quality of these candidates. Currently, most of the existing methods are context based. They rely heavily on the context relation of terms in the his-torical queries, and cannot detect and maintain the semantic consistency of queries. In this paper, we propose a graphical model to score queries. The proposed model exploits a latent topic space, which is automatically derived from the query log, to assess the semantic dependency of terms in a query. In the graphical model, both term context dependency and topic context dependency are considered. This also makes it feasible to score some queries which do not have much available historical term context information. We also uti-lize social tagging data in the candidate query generation process. Based on the observation that different users may tag the same resource with different tags of similar mean-ing, we propose a method to mine these term pairs for new candidate query construction.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Query Formulation, Search Process  X  Algorithms, Experimentation Query Refinement, Social Tagging, Query Log Mining
Extensive research works have been conducted to improve the performance of search engines by exploiting search logs [1, 11, 21, 29]. One important area is to help users issue more effective queries. There are two major directions in this research, namely, query suggestion (recommendation) and query refinement (reformulation). In query suggestion, his-torical queries are recommended to users according to a cer-tain similarity measure with the current query issued by users. This suggestion can be performed based on the click graph [13, 26] or query flow graph [5, 6]. However, it cannot provide new queries which are not contained in the query log. On the other hand, in query refinement, the query log is used in a more delicate manner that the current query is refined by exploiting the term dependency information in the historical queries. The generated query may be an exist-ing query in the query log, or a totally new query. According to the operations performed, there are three major types of refinement, namely, substitution [22, 32], expansion [3, 11, 18], and deletion [24, 36]. A common procedure of carrying out refinement is to generate some candidate queries first, and then a scoring method is used to assess the quality of these candidates.

A contextual model was proposed by investigating the context similarity of terms in historical queries [32]. Two terms in a pair with similar contexts are used to substitute each other in new query generation. Then, a context based translation model is employed to score the generated queries. Jones et al. employed hypothesis likelihood ratio to identify those highly related query phrase or term pairs in all user sessions [22]. Then, these phrase pairs are utilized to gener-ate new queries for the input query. All the above methods make use of query log, and exploit the context information to generate term pairs and score new queries. For example, Table 1 shows top 15 output queries of an existing context based method, which is used as the comparison baseline in our experiments, for the original query  X  X restling ring in-structions X . We can observe that the context based method has two shortcomings. First, the context based candidate term generation process is easily affected by noise when deal-Table 1: Top 15 results given by a context based method for original query  X  X restling ring instruc-tions X .  X   X  represents the unchanged original terms. ing with ambiguous terms. For example, considering the term  X  X nstructions X , due to the fact that  X  X nstructions X  has very diverse contexts in historical queries, a lot of noise is in-volved in its candidate list such as  X  X fghans X ,  X  X oncho X , etc. Furthermore, typos such as  X  X nstrutions X  and  X  X nstuctions X  also degrade the performance when dealing with the typo-prone terms. Second, context based method cannot detect and maintain consistency of semantic meaning when scor-ing a new query. For example, as shown in Table 1, a good output query is  X  X restling ring manual X , but it is ranked as the 13th. The reason is that the scoring function highly de-pends on the co-occurrence of the terms in the contexts of historical queries.  X  X hampionship ring X  is quite popular in query log owing to NBA. Consequently,  X  X hampionship ring instructions X  is assigned a high score although the semantic meaning of the terms in this refined query is not consistent.
Furthermore, when an unfamiliar or rare query is issued by a user, existing context based methods cannot work well too. The definition of unfamiliar query is given as follows: definition 1 (Unfamiliar query). Let ( t 1 ,  X  X  X  ,t n ) de-note a particular query q with length greater than 1, i.e. n&gt; 1 .Let ( t i ,  X  X  X  ,t j ) denote a sub-sequence of q ,where 1  X  i&lt;j  X  n .Ifnoneof ( t i ,  X  X  X  ,t j ) s has ever been ob-served before as a whole query or a sub-sequence of other queries, q is regarded as an unfamiliar query.
 The main reason for ineffective handling of unfamiliar queries by existing context based methods is that these methods mine the historical queries to obtain hints for refining the issued query. When there is no or very little related informa-tion, the performance will inevitably be affected. First, it is not easy to figure out the candidate terms to refine the query. Second, the statistical information may not be reliable to differentiate the quality of different candidate queries. One may expect that as the amount of available query log in-creases, the number of unfamiliar queries will decrease. We conducted an investi gation on the percentage of unfamiliar queries in an existing query log used in our experiments and found out that the situation is not as optimistic as what we expect. This query log consists of queries issued to a search engine for a period of 3 months. We started with the queries of the first 2 months as historical queries on hand. At the be-ginning, the queries in the first day of the third month were Table 2: Top 30 tags of  X  X ttp://www.dvguru.com/ X . The tag frequency is given on the right of each tag. used to simulate the newly issued queries, and we calculated the percentage of unfamiliar queries in that day. Then, these queries were added into historical query set so that they were treated as historical queries when we proceeded to the sec-ond day of the third month. The same statistical procedure was repeated for all the subsequent days. The statistical information is shown in Figure 1. We can see that the per-centage of unfamiliar queries does not change significantly when the amount of historical queries accumulate. Thus, it shows that a certain percentage of unfamiliar queries will always be issued by users. Consequently, it poses another challenge in generating candidate terms for those unfamiliar queries due to the lack of context information. Moreover, it also causes difficulty in scoring of new queries.
In summary, the context based methods have limitations in both major tasks, namely, candidate generation and query scoring. In candidate generation, noise will be involved as exemplified in Table 1 for the terms with diverse meaning. The candidates cannot be generated properly when lacking context information for some terms. In query scoring, the context based methods cannot detect and maintain the se-mantic consistency of the terms in the query. Furthermore, if the query is not familiar, the scoring task becomes even harder. To tackle the problems in the first task, we propose a method to generate better candidate term pairs by con-sidering useful information from social tagging data. Social tagging is an important application and a valuable semantic repository, and its potential in enhancing Web search perfor-mance has been studied in personalized search [7] and query suggestion [16]. Currently, millions of users are involved in some famous social tagging systems such as Delicious 1 and Flickr 2 , and they tag and share their favorite online re-sources. For example, in Delicious data, the tag set of URL  X  X ttp://www.dvguru.com/ X , which is a Web site for users to share their experience on digital videos, is given in Table 2. From this tag set, term pairs with similar meaning such as  X  X ilm X  and  X  X ovie X ,  X  X ips X  and  X  X utorial X  can be easily found. This is due to the fact that different users may choose their own preferred tags to express the same concept. Therefore, if two tags co-occur frequently in different resources X  tag sets, they are very likely to have closely related meaning. More-over, users regularly maintain their own tagging data and correct some typos and inaccurate tags, so social tagging data contains less noise than query log.

To tackle the problems in the second task, we propose a graphical model which can score a query taking into account the semantic consistency of the terms in the query as well as the term context. First we discover the latent topics in the http://www.delicious.com/ http://www.flickr.com query log, and these topics are utilized to construct a latent topic sequences of the terms in a query. When we score a query with the proposed graphical model, the latent topic sequence of the query is used as hidden evidence to guide the semantic dependency assessment.
As mentioned above, there are two major tasks to refine a query. The first task is to generate some candidate queries for the original query via operations such as substitution. The second task is to score the generated candidate queries, and rank them according to the predicted quality. In the first task, we focus on query term substitution. We first determine a set of possible substitution terms for each term in the original query. These terms are utilized to substitute one or more original terms in the original query. As discussed in Section 1, the noise and the lack of suf-ficient context information are the major problems when using query log. To tackle these problems, we develop a method to generate better candidate term pairs by consid-ering useful information from social tagging data. Based on the observation that terms with similar meaning are often used to tag the same resource by different users, we can extract some candidate substitution pairs. Then a filter-ing method is proposed with a delicately designed similarity function. This method is capable of filtering out different types of noisy pairs such as  X  X orth X  and  X  X arolina X ,  X  X ree X  and  X  X usic X .

In the second step, we propose a graphical model to score the quality of the candidate queries generated in the first step. In the proposed graphical model, the semantic consis-tency of the queries is considered. Two groups of variables are designed in the graphical model. One group corresponds to the terms, and the other group corresponds to the con-cepts of the terms. Unlike existing context based methods, we examine the query quality taking into account the se-mantic consistency of the consecutive terms in a latent con-cept space. We first derive a set of pseudo-documents from the query log data. Then these pseudo-documents are used to generate a latent concept space. The graphical model can capture the concept dependency between consecutive terms. Therefore, even when the term context information is not sufficient in the query log, it can still give reliable score by considering the semantic dependency of the consecutive terms in the latent concept space.
To generate candidate terms from social tagging data for query refinement, we make use of a bookmark data, namely Delicious, to illustrate our method and test the performance in the experiment. Note that the proposed method is appli-cable to other types of social tagging data. In this compo-nent, we first develop a method for mining term pairs from the bookmark data. After that, we consider the pseudo-context of the candidates to filter out noise.
A piece of bookmark, denoted by B lu = { t 1 ,t 2 ,  X  X  X } ,is a set of tags which are given by a user u to summarize the content of a URL, l such as { film, tutorial, blog } and { technology, tips } . Our candidate term extraction method is based on the observation that different users may use tags with similar meaning to describe the same resource. To capture this kind of co-occurrence, we aggregate all tags from different users of the same URL together, and compose a set of distinct tags for each URL, which is denoted by T
The mutual information of two tags, t 1 and t 2 ,canbe calculated as in Equation 1: where X t i is a binary random variable indicating whether term t i appears in a particular tag set T l . For example, P ( X t 1 =1 ,X t 2 = 1) is the proportion of the tag sets which contain t 1 and t 2 simultaneously. Because commonly used terms may be used to tag a lot of URLs such as  X  X ree X  and  X  X ood X . To cope with this problem, we adopt the normalized form of mutual information: where H ( t i )istheentropyof t i  X  X  distribution among all the tag sets, and calculated as in Equation 3: If NMI ( t 1 ,t 2 ) is greater than a threshold, the term pair t and t 2 will be stored for the next stage of term filtering.
The term pairs generated in the previous step may in-volve noise. The first kind of noise is mainly caused by noun phrases. For example,  X  X orth X  is found as the top 1 candidate for  X  X arolina X , and  X  X nited X  is top 1 for  X  X tates X . These are obviously not good candidates for term refine-ment. The main reason is that the phrases, i.e.  X  X orth carolina X  and  X  X nited states X , are used by many users to tag URLs. Thus, the NMI values of two terms in the same phrase are large. Another kind of noise is caused by the frequently used terms. For example,  X  X ree X  is found as one of the candidates for many terms such as  X  X usic X ,  X  X ovie X ,  X  X ilm X , etc. By manually examining the data, we find that users maintain a large amount of bookmarks for these free resources on the Web. We propose a method to filter out this undesirable candidate terms.

For each tag t , we first construct its pseudo-context in the bookmark environment, denoted by C ( t ), which is com-posed of all tags that co-occur with t in at least one B lu cnt t i | C ( t ) denotes the frequency of t i in the pseudo-context of t . It is equivalent to the co-occurrence frequency of t and t in all B lu s. Then the pseudo-context document of tag t , D , is defined as: where w t i is t i  X  X  weight in D t , which can be computed as: where D T denotes the set of all pseudo-context documents, and T is the vocabulary of the bookmark data. Then, we calculate a refined cosine similarity between a pair of pseudo-context documents: sim ( D t i , D t j )= where  X  ( t i ,t j | t k ) is the relation factor between term t t given t k , which is calculated as: where cnt t i ,t j | C ( t k ) is the number of B lu sthatcontain t , t j and t k simultaneously. Therefore, for the terms which co-occur with  X  X orth carolina X  frequently will contribute less when calculating the similarity of D carolina and D north design of  X  ( t i ,t j | t k ) is based on the following observation: one URL is seldom tagged by the same user with two terms that have similar meaning such as  X  X ilm X  and  X  X ovie X ,  X  X ar X  and  X  X utomobile X . According to Equation 7, the value of  X  ( X  X ar X  ,  X  X utomobile X  |  X  X epair X ) tends to be small, while, the value of  X  ( X  X arolina X  ,  X  X orth X  |  X  X ottery X ) tends to be large. Consequently, the contribution of the term  X  X epair X  to the similarity between D car and D automobile is increased, and the contribution of the term  X  X ottery X  to the similarity be-tween D carolina and D north is decreased. The refined cosine similarity can also filter out the terms which are frequently used such as  X  X ree X  and  X  X owto X , because their contexts are quite diverse, and cannot have a high similarity with other specific terms X  contexts.
In this section, we first present the graphical model and the scoring method. Then we describe the details of the model, namely, latent topic discovery from the query log, and model parameter training.
Aquery q is composed of a sequence of terms, which can be denoted by q : t 1  X  X  X  t n , where the superscript indi-cates the position of the term, and t r (1  X  r  X  n )maytake any term in the vocabulary. We propose a graphical model which considers the dependency among latent semantic top-ics. Figure 4 depicts the design of the graphical model. The query terms, i.e. t r , are observable and represented by filled nodes, and the latent topics, denoted by z r , of the corre-sponding terms are unobservable and represented by empty nodes. The discovery of latent topics will be discussed later.
The transition probability, denoted by P ( z r +1 | z r ), be-tweentopicsisconsidered,where z r and z r +1 are the se-mantic topics of t r and t r +1 respectively. We also make use of the probability of obtaining the term t r +1 given t term depends on its latent topic and the preceding term, and each topic depends on its preceding topic. When we score a query with this graphical model, the latent topic se-quence of the query is used as hidden evidence to guide the semantic consistency assessment. For queries with good se-mantic consistency, their corresponding topic sequences will
Figure 2: The graphical model for scoring a query. increase the score of their quality. For these queries, the transition probability between the consecutive topics in the topic sequence is higher.

When we employ this graphical model to score a candidate query, the score of a query q can be denoted as P ( t 1  X  X  X  which is given in Equation 8: We marginalize over all possible latent semantic topic se-quences to calculate the joint probability of generating the term in query q . Based on the structure of the designed graphical model, P ( t 1: n ,z 1: n ) can be calculated as in Equa-tion 9 by the chain rule: When r =1, P ( z 1 | z 0 )= P ( z 1 )and P ( t 1 | z 1 ,t
This graphical model shares some resemblance to Hid-den Markov Models (HMM). However, one major difference is that there are dependencies between neighboring terms, which capture their co-occurrence.
As mentioned above, the scoring function needs to con-cretize the latent topic sequence in the graphical model de-picted in Figure 4. The possible latent topic paths compose a trellis structure, which is shown in Figure 4.1. To marginal-ize the joint probability P ( t 1: n ,z 1: n )(refertoEquation9) over all possible latent topic paths, we can employ a dynamic programming method, similar to the forward algorithm. The forward variable  X  r ( i ) is defined as: which is the score of the partial query t 1  X  X  X  t r given the topic z at the position r .When r =1,weset  X  1 ( i )= P ( z 1 = z ) P ( t 1 | z 1 = z i ,t 0 ). The recursive calculation of  X  is:  X  r ( i )= where P ( z r = z i | z r  X  1 = z j ) is independent of the position r , and equal to P ( z i | z j ). Finally the score of a query can be calculated by summing over all possible z i for  X  n ( i ): Figure 3: Trellis structure of latent topic sequences. Therefore, to conduct the scoring, we need three kinds of model parameters, namely, P ( z i ), P ( z j | z i ), and P ( t which can be learned in the model training phase.
The latent topics of the terms in queries are discovered from the query log. These latent sematic topics discov-ered are transferred to initialize the model parameters of the graphical model. Then, the historical queries are treated as training sequences to learn the model parameters.
A typical format of the records in query log can be rep-resented as a triple ( query,clicked url, time ). One direct way of using query log data for latent topic analysis is to treat each clicked url as a single document unit. This ap-proach will suffer from the data sparseness problem since most of URLs only involve very small number of queries. Instead of adopting such a simple strategy, we aggregate all the queries related to the same host together, and con-struct one pseudo-document for each host, denoted by H . For example, the pseudo-document of  X  X ww.mapquest.com X  consists of the queries such as  X  X apquest X ,  X  X ravel map X ,  X  X riving direction X , etc. Some general Web sites such as  X  X n.wikipedia.org X  and  X  X ww.youtube.com X  are not suitable for latent topic analysis, because they involve large amount of queries which cover very diverse aspects. To tackle this problem, we first sort the host pseudo-documents in de-scending order according to the number of distinct query terms they have. Then, the top ranked pseudo-documents are eliminated in our latent topic discovery process.
We employ the standard Latent Dirichlet Allocation (LDA) algorithm [4] to conduct the latent semantic topic analysis on the collection of pseudo-documents. In particular, Gibb-sLDA 3 package is used to generate a set of latent topics Z = { z 1 ,z 2 ,  X  X  X  ,z N } . Each topic z i is associated with a multinomial distribution of terms, denoted by P ( t k | where t k is a term. Each t k in a certain pseudo-document H is assigned a topic, denoted by z ( t m |H )inthispaper.
We transfer the latent topics discovered above to initialize the model parameters in the beginning of training phase: Leibler divergence between two distributions P (  X | z i )and P ( When z i and z j are highly related topics, the value of  X  is large.  X  P ( t k ,t l | z j ) is the probability that t in the topic z j ,and  X  P ( t k | z j ) is the probability of t http://gibbslda.sourceforge.net/  X  P ( t k ,t l | z j )and  X  P ( t k | z j ) are calculated as: where cnt ( t m ,t n | z j )denotestheco-occurrencetimeof t and t n in the topic z j , and calculated as: cnt ( t m ,t n | z j )= where 1 {} is an indicator function, and z ( t m |H ) is the topic assigned to t m in the pseudo-document H ,whichcanbe obtained from the result of LDA in the previous step. Then we utilize a global language model to smooth  X  P ( t l | z in the query log data, cnt ( t ) denotes the frequency of t .
Note that we do not directly apply the probability P ( t k generated by LDA as  X  P ( t k | z j )inEquation15. Thereasonis that  X  P ( t l | z j ,t k ) should satisfy the constraint of 1, and using P ( t k | z j ) may violate this constraint. To conduct the training process, we refine the standard Baum-Welch algorithm [2]. Given a set of historical queries, we preprocess them so that the f requency of distinct queries is aggregated and collected. Since we assume that each query is independent of the others, the intermediate calcula-tion can be illustrated with a single query. Then, the inter-mediate results for each single query are aggregated to ob-tain the final updating formulas with the similar way in [25].
For a query q : t 1  X  X  X  t n , we define backward variable  X  which is the score of partial query t r +1  X  X  X  t n given z t . We define  X  n ( i ) = 1. The recursive calculation of  X  is: Then the joint probability of a query q and z r = z i can be derived as:
Therefore, P ( q ) can also be computed by i  X  r ( i )  X  r
Now we introduce two conditional probabilities. P ( z r = z | q ) is the probability that z r takes the value z i in q ,which canbecalculatedas: The joint probability of z r = z i and z r +1 = z j in a given q is P ( z r = z i ,z r +1 = z j | q ): P ( z r = z i ,z r +1 = z j | q )=  X  r ( i ) P ( z j
The next step is to aggregate the intermediate results and obtain the final updating formulas of P ( t l | z j ,t k ), P ( z and P ( z i ). P ( t l | z j ,t k ) is designed as follows: where Q is the set of training queries, and F ( q )isthe frequency of q ,  X  2 is used to control the weights of two parts, namely the initial value  X  P ( t l | z j ,t k ), and the newly estimated part. In this way, the value of P ( t l | z j ,t not solely depend on the queries which have sub-sequence ( t ,t l ), a proportion of the initial  X  P ( t l | z j ,t k in P ( t l | z j ,t k ). Therefore, when a new query contains term sequence ( t k ,t l ), and no previous queries contain ( t the value of P ( t l | z j ,t k ) can still be calculated relying on  X  The updating formulas of P ( z j | z i )and P ( z i ) are straight-forward and they are given as follows:
After one iteration, new parameters, namely, P ( t l | z j iterative procedure will stop when a pre-defined condition is satisfied.
The query log data used in our experiments is the AOL data [27] spanning three months from 1 March, 2006 to 31 May, 2006. The raw data is composed of queries and clicks recorded by a search engine. Since it contains a lot of noise, we first clean the raw data as done by most of the existing works. If a query contains non-alphabetical char-acter, or is a host navigation query such as  X  X oogle.com X  and  X  X ww.yahoo.com X , it is removed. Then, the stop words are removed from the remaining queries. We adopt a hy-brid method to detect user session boundary [19]. First, consecutive queries in the same session have to share at least one term. Second, the interval between two consec-utive queries should be less than 10 minutes. After sessions are detected, we remove those sessions without any clicked URL. In each session, we remove the queries which are at the end of the session and have no clicked URL. Finally we ob-tain 7 , 041 , 319 sessions, in which 1 , 276 , 498 are multi-query sessions. There are 4 , 315 , 124 unique queries and 673 , 073 distinct terms.

The data set is split into two subsets by the time stamp: the historical set and the test set. The historical set con-tains the first two months X  log, and the test set contains the third month X  X  log. The pseudo-documents for latent topic analysis are constructed with the historical set as mentioned in Section 4.3. If a host involves less than 5 queries, it will be eliminated. Then all pseudo-documents are ranked in descending order according to the number of distinct terms they have. In order to filter out those Web sites that are too general, we remove URLs from the top 0.1% of the pseudo-documents. Finally, 189,670 pseudo-documents are retained and fed into the latent topic discovery algorithm, where the number of topics is set to 30. In the historical set, 1,882,638 queries with at least one clicked URL are used to build the training set, which is used in the parameter estimation.
Currently, the largest existing bookmark data collection is provided by Wetzker et al. [34]. In our experiment, we use a subset of the data spanning two years, namely, 2006 and 2007. In this period, 101,485,670 bookmarks were as-signed to 39,232,530 distinct URLs by 891,479 users, includ-ing 2,523,190 distinct tags. To tackle the vocabulary gap between the AOL log and bookmark data, we filter out the tags that are not contained by the query log such as  X  X o-cialsoftware X ,  X  X isualstudio X , etc. To tackle the problem of data sparseness, we only consider the URLs which are tagged by at least five users. After this filtering, 1,910,547 URLs are collected and used in candidate term generation. And the similarity threshold value used is 0.19 when filtering the noise. The smoothing parameter value of  X  1 is 3,000 in Equation 19.
For conducting the comparison, we implement a context based term association (CTA) method proposed by Wang and Zhai [32], and follow the advice of the authors on some implementation details. This method is based on an obser-vation that terms with similar meaning tend to co-occur with the same or similar terms in the queries. For example, both  X  X uto X  and  X  X ar X  often occur with  X  X ental X ,  X  X ricing X , etc. Therefore, both candidate term generation and query scor-ing in CTA are conducted by exploiting the association of term contexts in the historica l queries. A contextual model is defined to capture context similarity, and a translation model is defined to score the quality of a candidate query. We denote the two steps of CTA, namely, candidate term generation, and query scoring, as  X  X TA-CAND X  and  X  X TA-SCR X  respectively. We generate the contextual and transla-tion models for the top 100,000 terms in the historical set. Then the threshold given in [32] is applied to filter out the noise. We mainly compare the performance of our frame-work and CTA for query term substitution. A conservative strategy is adopted, specifically, only one term in the origi-nal query will be replaced. This is also the strategy adopted by some previous works such as [32].

Because manual evaluation of the output of query refine-ment methods is time-consuming and very labor intensive, we conduct an automatic evalua tion by utilizing the session information of query log. In a search session, when users feel unsatisfied with the results of current query, they may refine current query and search again. After obtaining satis-factory results, they may stop searching. The importance of theterminalURLwaswelldiscussedinpreviousworks[14, 35]. Therefore, an automatic and reliable evaluation can be conducted based on this observation. We introduce the definitions of two kinds of query as follows: definition 2 (Satisfied query). In a user session, the query which causes at least one URL clicked and is located in the end of the session is called a satisfied query. definition 3 (Unsatisfied query). The query which is located just ahead of the satisfied query in the same user session.
 We collect a set of unsatisfied queries used as the input, and collect their corresponding satisfied queries treated as the benchmark query set of the refinement task. Note that this requirement is only for conducting automatic evalua-tion, and our framework can take any query as input.
The candidate terms for 12 selected terms are given in Ta-ble 3. For each term, the top 5 candidates are given along with their NMI scores. In general, there are four kinds of relations between the terms in a substitution pair. The first kind refers to the fact that the candidates X  meaning is iden-tical with input terms X  meaning such as  X  X utomotive X  and  X  X uto X  for  X  X ar X ,  X  X ids X  for  X  X hildren X ,  X  X ottery X  for  X  X otto X , etc. The second kind is that the candidate terms X  meaning is related to the input term X  X  meaning, such as  X  X vd X  for  X  X d X ,  X  X itness X  for  X  X ealth X ,  X  X state X  for  X  X ouse X ,  X  X essenger X  for  X  X sn X . etc. The third kind refers to the fact that the candidate terms have a similar property of the input terms such as  X  X cq X  and  X  X im X  for  X  X sn X . The last kind refers to other types of strongly associated relation. In the last case, although the candidate terms do not have similar meaning or property of the inputs, the relationship between them is quite obvious. For example,  X  X owerball X  is a kind of lottery ticket,  X  X ock X  is a genre of popular music. Interestingly, two non-English terms are suggested for  X  X usic X , namely,  X  X u-sica X  from Spanish and  X  X usik X  from Swedish, and both are correct. We can see that social tagging data can provide us some useful term pairs with similar or related meaning. We find that when dealing with high frequency terms, CTA suggests a lot from typos. The candidates of the top 12 most frequent terms in the AOL log are given in Ta-ble 4. It can been seen that typos are recommended by CTA for 8 input terms, except for  X  X yspace X ,  X  X bay X ,  X  X ank X  and  X  X apquest X . After the filtering is performed with the threshold given in [32], the top 5 remaining candidates for  X  X ounty X  are  X  X ount X ,  X  X outy X ,  X  X ouny X ,  X  X onty X  and  X  X ade X , and for  X  X yrics X  are  X  X isten X ,  X  X ong X ,  X  X ryics X ,  X  X yrcis X  and  X  X y-ics X . The main reason of this observation is that their can-didate generation method is context based, and the filtering method relies on the co-occurrence of two terms among dif-ferent sessions. After users issue a query containing typo terms and cannot get satisfied results, they may correct it manually. But this typo is already recorded in the query log. Worse still, most of search engines can return some results even for a query with typo. So after users click any one of Figure 4: Comparison of scoring performance be-tween CTA-SCR and Our-SCR. the results for the query with typo, we cannot easily filter out this query. Therefore, the typo and its corresponding correct term have very similar context and often co-occur in the sessions. In contrast, our method utilizes the book-mark data, which has a better quality compared with the query log. When a user provides a typo tag, it may be corrected when the user comes back to maintain his or her bookmarks. On the other hand, the bookmark data also has its own shortcoming. A user may connect several terms as a single tag, such as  X  X ooglemaps X  and  X  X earchengine X . Fortu-nately, it is not difficult to split this kind of tags into single terms.
Recall that our framework contains two major compo-nents, namely, the graphical model for scoring and the can-didate term generation from social tagging data, and they aim at solving different steps or tasks in query refinement. To compare the performance of query scoring between our graphical model and CTA, we randomly select 1,000 multi-query sessions from the test set of the query log, and the unsatisfied queries in these sessions are used as input of the query refinement task. This query set is named as Query Set 1 (QS1). As mentioned before, the corresponding satisfied queries in these sessions are used as the benchmark results.
The scoring component of CTA, i.e. CTA-SCR, is a term context based method, and it is designed to score the candi-date query generated from the context based candidate gen-eration method, i.e. CTA-CAND. To compare the perfor-mance of two scoring methods, namely, CTA-SCR, and our graphical model (called Our-SCR), we apply CTA-CAND to generate candidate terms from the historical query set for them. For a query q in QS1, CTA-CAND outputs a list of candidate terms for each term of q . Then, each candidate term is used to generate a candidate query. All candidate queries of q are fed into one scoring method to assess the quality. Then, according to their score, the top candidates are determined as the final output results for q .
We adopt the metrics P@K (precision at K) to evaluate the result, and K is the number of top result queries given by the model. For each method, at most 25 result queries are considered here. The performance of both models is given in Figure 4, where CCOS denotes the scoring performance of our graphical model. It can be observed that our graphical model outperforms CTA (denoted by CCCS) significantly Figure 5: Refinement performance of different com-binations on QS2. when K is small. The P@5 value of our graphical model is 0.085, while the P@5 value of CTA is 0.062. The differences between the performances of our method and CTA are sig-nificant with significance level 0.05. The major reason for the different performance is that our graphical model scores a query taking into account the semantic consistency of the terms, which cannot be captured by the comparison base-line. Because most of the input queries have less than 25 candidate queries, both methods achieve similar P@25.
The effect of the parameter  X  2 is depicted in Figure 6. In general, a large value of  X  2 can achieve better result than a small value. Recall that we randomly generate QS1 from the test set. Commonly used queries have more chance to be selected, because they are issued many times by users. Therefore, there is more informa tion from historical queries that can be used in scoring their candidate queries. Refer to Equation 25, a large value of  X  2 reserves small amount P ( t l | z j ,t k ) depends more on the re-estimation part from the historical queries. This weight distribution is preferred when dealing with queries in QS1. If the value of P ( t l | z j depends on the re-estimation part, i.e.,  X  2 =1 . 0, the result is worse than that when we reserve some initial estimation of  X  estimation from the latent topic analysis can capture some intrinsic feature of the queries in Web search.
To investigate the efficacy of our candidate term genera-tion method based on social tagging data, we select another query set from the test set in a similar way of selecting QS1, named as Query Set 2 (QS2). The only difference is that the queries in QS2 are unfamiliar, so that we can investi-gate whether social tagging data can help solve the problem of lacking information.

The queries in QS2 are unfamiliar, therefore, this poses some challenges in both tasks of query refinement as we discussed in Section 1. Figure 5 depicts the results of differ-ent combinations. Our-CAND denotes the candidate term generation component of our framework. It can be ob-served that the combination OCOS performs the best, and followed by combination OCCS. Combinations CCOS and CCCS achieve similar performance. In summary, the com-binations with Our-CAND as the candidate term genera-Figure 6: The effect of  X  2 under the CCOS method in the comparison of the scoring performance. Figure 7: The effect of  X  2 under the OCOS method in unfamiliar query refinement. tion component can perform better. This is mainly due to the lack of information for these unfamiliar query, and good candidate terms cannot be generated by CTA-CAND. Fur-thermore, Our-SCR performs better than CTA-SCR. Our graphical model scores the query in a latent semantic space. Therefore, even when the term context information is very sparse, Our-SCR can still achieve a reliable score.
The effect of parameter  X  2 in the combination OCOS is given in Figure 7. It can be observed that when refining the unfamiliar queries, a small value of  X  2 , say 0.3, can achieve a better performance in query scoring, because a small value of  X  2 reserves larger amount of initial estimation value, i.e.  X  P ( t l | z j ,t k ). When the historical query information is sparse,
We conduct some case studies to analyze the query refine-ment result qualitatively. Table 5 depicts the top 5 output queries from our model for 4 input queries, namely  X  X utch folklore X  X nd X  X acing logistics X  X rom QS2, and X  X ids anger dis-orders X  and  X  X iscount gucci eyeglasses X  from QS1. To refine  X  X utch folklore X  and  X  X acing logistics X , the main difficulty is the generation of candidate terms. For example,  X  X olklore X  is not a frequently used term in the historical query set, but in the bookmark data it is employed to tag many URLs. Our candidate generation method can figure out some very good candidate terms such as  X  X yths X ,  X  X ythology X , and  X  X egend X . On the other hand, to refine X  X ids anger disorders X , the main difficulty is to detect and maintain the semantic meaning consistency of the candidate queries. Because each term in X  X ids anger disorders X  X as tens of candidate substitu-tion terms, the semantic consistency is not easy to maintain when generating a new query.
Huang et al. analyzed and evaluated different types of query refinement in the query log [20]. Except the three mentioned types, namely, substitutions, expansion, and dele-tions, they also investigated some other fine-grained types such as stemming [28], spelling correction [10], abbreviation expansion [33], etc. In [17], a Conditional Random Field model is proposed to conduct these fine-grained refinement operations. Chirita et al. exploited user X  X  personal informa-tion repository such as text documents, emails and cached Web pages to implicitly personalize the query expansion for the user [9]. Sadikov et al. conducted an investigation on clustering the refinements of the same original query [30]. The clusters provide a summary of the possible diverse in-terests of the users who issue the same original query. Their method can also be used to achieve a better personalized query refinement. Some works investigated the effectiveness of using anchor texts in query refinement [12, 23]. They find that anchor text is also an effective resource.

In query suggestion, a commonly used method is mining the click graph [13, 26]. In [26], a sub-bipartite graph of queries and URLs is generated for a particular input query. Then the hitting time of each node is computed by perform-ing a random walk on the subgraph. After that, the query nodes which have the top n earliest hitting time are used as the suggestions. Deng et al. constructed a query-to-query graph instead of the bipartite graph, and the random walk is conducted on the entire graph [13]. They also proposed an entropy based method to calculate the weight of the edges. Cao et al. investigated context aware query suggestion by mining query log [8]. They used a clustering method on the bipartite graph to summarize queries into concepts. Then user sessions are employed to mine the concept patterns, which can help to provide more precise suggestions.
In this paper, we present a framework for performing query refinement, and this framework mainly has two contri-butions. First, a graphical model is proposed to score can-didate queries, and this model can detect and maintain the semantic consistency of terms in queries. Second, a method is presented to mine term pairs with similar meaning from social tagging data. From the experimental results, we ob-serve that taking the semantic consistency into account can help to give a more reliable prediction of query quality. Fur-thermore, social tagging data can provide useful information in generating similar term pairs, which is especially impor-tant for some unfamiliar terms in previous queries.
Currently, phases are separated into single words in our framework in both latent topic analysis and term depen-dency estimation. One future direction is to promote our model from single word level to phrase level. Some models such as topical N-grams model [31] and phrase translation model [15] have attested the efficacy of the phrase level so-lution. In our current model, the score of a candidate query is independent of the current session in which the original query locates and the user who issues the original query. Therefore, another future direction is to consider more per-sonalized or other available information in scoring queries such as the users X  Web search history, existing queries in the current user session, etc. [1] E. Agichtein, E. Brill, and S. Dumais. Improving web [2] L.E.Baum,T.Petrie,G.Soules,andN.Weiss.A [3] B. Billerbeck, F. Scholer, H. E. Williams, and [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] P. Boldi, F. Bonchi, C. Castillo, D. Donato, A. Gionis, [6] P. Boldi, F. Bonchi, C. Castillo, D. Donato, and [7] Y. Cai and Q. Li. Personalized search by tag-based [8] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen, and [9] P. A. Chirita, C. S. Firan, and W. Nejdl. Personalized [10] S. Cucerzan and E. Brill. Spelling correction as an [11] H. Cui, J.-R. Wen, J.-Y. Nie, and W.-Y. Ma.
 [12] V. Dang and B. W. Croft. Query reformulation using [13] H. Deng, I. King, and M. R. Lyu. Entropy-biased [14] D. Downey, S. Dumais, D. Liebling, and E. Horvitz. [15] J. Gao, X. He, and J.-Y. Nie. Clickthrough-based [16] J. Guo, X. Cheng, G. Xu, and H. Shen. A structured [17] J. Guo, G. Xu, H. Li, and X. Cheng. A unified and [18] B. He and I. Ounis. Combining fields for query [19] D. He, A. G  X  oker, and D. J. Harper. Combining [20] J. Huang and E. N. Efthimiadis. Analyzing and [21] T. Joachims. Optimizing search engines using [22] R. Jones, B. Rey, O. Madani, and W. Greiner. [23] R. Kraft and J. Zien. Mining anchor text for query [24] G. Kumaran and J. Allan. Effective and efficient user [25] S. E. Levinson, L. R. Rabiner, and M. M. Sondhi. An [26] Q. Mei, D. Zhou, and K. Church. Query suggestion [27] G. Pass, A. Chowdhury, and C. Torgeson. A picture of [28] F.Peng,N.Ahmed,X.Li,andY.Lu.Context [29] F. Radlinski and T. Joachims. Query chains: learning [30] E. Sadikov, J. Madhavan, L. Wang, and A. Halevy. [31] X. Wang, A. McCallum, and X. Wei. Topical n-grams: [32] X. Wang and C. Zhai. Mining term association [33] X. Wei, F. Peng, and B. Dumoulin. Analyzing web [34] R. Wetzker, C. Zimmermann, and C. Bauckhage. [35] R. W. White, M. Bilenko, and S. Cucerzan. Studying [36] X. Xue, S. Huston, and W. B. Croft. Improving
