 Zheng Chen, Suzanne Tamang, Adam Lee, Xiang Li, Marissa Passantino, and Heng Ji The increasing number of open evaluations and shared resources has made it possible for many natural language processing tasks to benefit from system combination. For-system combination for information extraction. 
The KBP Slot Filling task involves learning a pre-defined set of attributes for per-son and organization entities based on a source collection of documents. A query con-list of slots to ignore. For example, [Andy Warhol, ABC-20080611-9372, PER, SF7, should return all slot types except per:date_of_birth . KBP 2010 defined 26 slot types pants from Information Extraction (IE) and Question Answering (QA) communities. munity effort. 
In this paper we present a state-of-the-art Slot Filling system that includes two bot-including statistical answer re-ranking and Markov Logic Networks (MLN) based cross-slot reasoning. We evaluate performance across our pipelines, with the systems from KBP2009 and human annotators. Figure 1 depicts the general procedure of our approach. All three pipelines begin with an initial query processing stage where query expansion techniques are used to im-alternative approaches to the KBP task: IE, pattern matching and QA. After genera-tion of the best answer candidate sets form the individual systems, they are combined to re-rank confidence on the system-wide an swer set and for cross-slot reasoning. These pipelines are organized in two forms: bottom-up IE based approaches that ex-tract all possible attributes for a given query and then fill in the slots by mapping and inference (Section 3.1 and 3.2); and top-down QA based approach that search for an-swers constructed from target entities and slot types (Section 3.3). 3.1 Pattern Matching learning from query-answer (q-a) pairs, and then apply these patterns to find answers to unseen queries. For example, given the pair (Michael Jackson, 50) for slot per:age , we can extract sentences in which Michael Jackson and 50 co-occur: 
Pattern can be constructed as This approach consists of the following steps: (1) Selection of query-answer pairs from infobox fields to KBP slots. q-a pairs are split into two sets: half for pattern ex-traction, and the other half for pattern assessment. (2) Pattern extraction and answer co-occur. In addition to populating static patterns for different q-a pairs, general as possible. (3) Pattern assessment For each q-a pair from a stand-alone development set, we search the top 1000 docu-each sentence and if it can be matched, extract the entity at the exact place as the an-filter those with precision below a threshold. (4) Pattern matching To obtain candidate answers for slot filling, we locate the sentences where q occurs, apply the patterns generated by step (3) and extract the answer when there is a pattern that produce the answer. (5) Filtering answers with inappropriate entity types, erroneous answers that are not in dictionary resources conjunction relation). 3.2 Supervised IE and events defined in NIST Automatic Content Extraction Program (ACE 2005) 4 . Relation extraction and event extraction are based on maximum entropy models, in-corporating diverse lexical, syntactic, semantic and ontological knowledge. ACE2005 apply the following mapping between ACE relation/event and KBP. tuple &lt; q, a, r X  &gt; for slot filling is em j . indicates that the trigger word t indicates an event type e and the involving arguments role constrain t of PER SON, then we return em j as the answer. 3.3 Question Answering We also apply the web module of an open domain QA system, OpenEphyra (Schlae-fer et al., 2007) to retrieve candidate answers for the KBP slot filling task. Since can-didate answers must be entailed in the KB and a corresponding document id identified, additional answer processing is necessary to determine the candidate an-swer X  X  relevance and retrieve the corresponding docid(s) for the document collection. 
To estimate the relevance, R , of a q-a pair, we use the joint probability of observ-ing both the query and answer by means of the answer pattern probability: where NEAR is defined as within the same sentence boundary. At the sentence query pattern q and answer pattern a : After the relevance scores are calculated, the values for each KBP slot are rescaled from 0-1 in order to facilitate the comparison of relevance values among different slot. We enhance the above three pipelines based on the following extensions. We hypothesize candidate answers to the query. By obtaining more potential correct answers in the ranked list, we can exploit effective learning-to-rank technique to select the best answer (Section We design a novel cross-slot reasoning a pproach based on Markov Logic Networks (MLN) to further refine the quality of answers and predict new answers (Section 4.3). 4.1 Query Expansion In order to generate informative natural language questions from each pair of &lt;query name, slot type&gt;, we develop the following expansion methods. (1) Template expansion We generated 59 question templates for the 16 organization slots and 62 question templates for the 26 person slots. For example, the following semantically equivalent questions are generated for the  X  X erson: age X  slot type: During candidate answer generation, the tag &lt;p er&gt; is replaced by the target. On aver-age, each target value produced an initial set of 112 candidates per slot. After filter-ing by stop words, and sufficient co-occurrence with the query and answer pattern in the reference corpus, queries averages 4.9 answers each in the baseline results, which recall leads to approximately a four-fold improvement. (2) Name expansion The query name may be mentioned in its alternative names in the corpus, thus, name expansion can help improve the recall of slot filling. Wikipedia uses redirect links to indicate navigations among pages that mention the same entity. For example, the en-tity name "Seyed Ali Khamenei" is redirected to "Ali Khamenei". We mine redirect resources to form extra query names. 4.2 Statistical Re-ranking We develop a Maximum Entropy (MaxEnt) based supervised re-ranking model to re-pairs to predict the confidence of each candi date answer X  X  correctness. We incorporate the following semantics and global statistics based features into the re-ranker: 4.3 MLN Based Cross-Slot Reasoning In the slot filling task, each slot is often dependent on other slots, but systems built for the slot filling task often ignore these dependencies and process each slot individually. be per:siblings of X ). Therefore we develop a reasoning component to approach a real hand, we can design propagation rules to enhance recall, for example, X was born on date Y  X  X  X  X  age is approximately (the current year  X  Y ). 
We noticed that heuristic inferences are highly dependent on the order of applying rules, and the performance may have been limited by the thresholds which may over-fit a small development corpus. We use Markov Logic Networks (Richardson and weight to each first order logic formula, allowing for violation of those formulas with some penalty. We use the Alchemy toolkit (Kok et al., 2007) to encode inference rules such as: cluding per:title , per:country_of_death and org:number_of_em ployees/members , as well as soft rules such as per:birth_of_date to per:age propagation. In this section we present the overall performance of our three pipelines by comparing to the best system at KBP2009 evaluation and human annotation, and break down the performance to demonstrate the impact of key techniques. 5.1 Data and Scoring Metric We randomly select 21 queries (11 persons and 10 organizations) and the entire source collection (1,289,649 documents in total) from KBP 2009 evaluation corpora o evaluate our methods 5 . For each query, we combine the human annotation from LDC based on exhaustive search (110 instances) and the correct answers from KBP 2009 unique instances. human annotator can only achieve lower than 50% recall, we follow the human as-sessment procedure as in KBP 2009 evaluation. We ask another human annotator to manually check all those instances generated by the system but not in human annota-set ( K  X  ). Since this human assessment procedure is time consuming, we only apply it break down analysis experiments. form scoring metric based on standard Precision, Recall and F-measure. Since only non-NIL answers are informative in appli cations, we focus on scoring non-NIL an-it matches any instance in the answer key. We added additional answer normaliza-tions to the scorer in order to get more reliable scores and speed up human assessment 362 country name variants (e.g.  X  X he United States = USA X ) and a list of 423 person fullname-nickname pairs. If a system returns an answer set S , we compute the follow-ing scores: 5.2 System/Human Comparison were evaluated against an incomplete answer key set, it indicates that pattern match-ing can achieve the best result. Supervised IE method performs the worst because not all of the slot types have corresponding relation and event types. QA method obtained comparable precision but much lower reca ll because the candidate answers are re-answer validation. Supervised IE 13.09 12.82 12.95 
One advantage of QA is that answer candidates can still receive a high confidence Archbishop Peter Akinola consecrated Martyn Minns of Virginia as Bishop of the Church of Nigeria for an outreach programme called Convocation of Anglicans in North America.  X  
The most common mistakes of the QA pipeline are directly related to the use of co-occurrence statistics. Although semantic evidence indicates an answer is invalid, it is fighters parade beneath yellow flags beside the faces of Nasrallah and Abbas Mous-sawi , Nasrallah's predecessor who was assassinated, along with his wife and son, in an attack by an Israeli helicopter pilot  X , but since only co-occurrence were used, the sawi, assassinated&gt; and should have been returned for per:cause_of_death . 
In addition, we compared our absolute scores with the LDC human annotator and non-NIL slots. The results are shown in Table 2. 
From Table 2 we can see that our pattern matching approach achieved significantly We can also conclude that Slot Filling is a very challenging task even for human be-cause the human annotator can only find 41.83% answers. 5.3 Impact of Statistical Re-ranking We then evaluate the impact of combination methods and will demonstrate they are also essential steps to achieve good slot filling results. We will describe the results for statistical re-ranker in this subsection and cross-slot reasoning in 5.4 respectively. (about 5%) both precision and recall of the QA pipeline. 
Supervised Re-ranking helps to mitigate the impact of errors produced by scoring based on co-occurrence. For example, when applied to our answer set, the answer  X  Clin-per:children due to frequent co-occurrence that was reduced and consequently removed  X  1976  X  X id not have a high co-occurrence in the text collection, but was bumped up by the re-ranker based on the slot type feature org:founded . 5.4 Impact of Cross-Slot Reasoning Experimental results demonstrate the cross-slot reasoning approach described in Sec-tion 4.3 can enhance the quality of slot filling in two aspects: (1) It can generate new results for the slots which the pipelines failed altogether; (2) It can filter out or correct precision of slot filling without any loss in recall. Table 4 presents the number of spu-rious errors removed by this approach when it is applied to various pipelines. based approaches (Bikel et al., 2009) and QA based methods (Li et al., 2009). Some previous work (e.g. Schiffman et al., 2007) demonstrated that IE results can be used to significantly enhance QA performance. 
Answer validation and re-ranking has b een crucial to enhance QA performance 2003) has showed that high performance for QA systems can be achieved using as few as four features in re-ranking. Our results on the QA pipeline support this finding. The same related work (Huang et al., 2009) reports that systems viewed as a re-ranker better implementation choice. (Bikel et al., 2009) designed inference rules to improve the performance of slot fill-works (MLN). state-of-the-art performance using several novel approaches including statistical answer re-ranking and cross-slot reasoning based on Markov Logic Networks (MLN). Experi-mental results demonstrated that the pattern matching pipeline outperforms the top sys-tem in KBP2009 evaluation. This work was supported by the U.S. Army Research Laboratory under Cooperative Agreement Number W911NF-09-2-0053, the U.S. NSF CAREER Award under Grant IIS-0953149, Google, Inc., DARPA GALE Program, CUNY Research Enhancement Program, PSC-CUNY Research Program, Faculty Publication Program and GRTI Program. The views and conclusions contained in this document are those of the au-pressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on. 
