 1. Introduction
In the general area of sentiment analysis, sarcasm is a disruptive factor that causes the polarity of a message to flip. Unlike a simple negation, a sarcastic message often conveys a negative opinion using only positive words  X  or even intensified, hyperbolic positive words. Likewise, but less frequently, sarcasm can flip the polarity of an opinion with negative words to the intended positive meaning. The detection of sarcasm is therefore important, if not crucial, for the development and refinement of sentiment analysis systems, but is at the same time a serious conceptual and technical challenge. In this article we introduce a sarcasm detection system for tweets, messages on the microblogging service offered by
Twitter. 1 In doing this we are helped by the fact that sarcasm appears to be a commonly recognized concept by many Twitter users, who explicitly mark their sarcastic messages by using hashtags such as  X #sarcasm X  or  X #not X . Hashtags in tweets are explicitly marked keywords, and often act as categorical labels or metadata in addition to the body text of the tweet ( Chang, 2010 ). By using the explicit hashtag any remaining doubt a reader may have is taken away: the message is not to be taken literally; it is sarcastic.  X 
While such hashtags primarily function as conversational markers of sarcasm, they can be leveraged as annotation labels in order to generate a model of sarcastic tweets from the text co-occurring with these hashtags. A clear advantage of this approach is the easy acquisition of a vast amount of training data. On the other hand, its performance is dependent on the correctness of two assumptions: first that users who include one of the selected hashtags in their tweet actually intended to convey sarcasm and indeed intended to flip the polarity of the message, and second that the pattern of sarcasm in a tweet still holds when the hashtag is excluded from it as a training label. We set out to test these assumptions along with the qual-ity of the resulting sarcasm detection system by applying it on a realistically large and unbiased sample of tweets (of which the vast majority is non-sarcastic) posted on the same day.
 The hashtag as a marker of sarcasm has been leveraged in previous research to detect sarcasm in tweets ( Gonz X lez-
Ib X nez, Muresan, &amp; Wacholder, 2011; Reyes, Rosso, &amp; Buscaldi, 2012 ). One contribution of this paper to the existing body of work is that a sarcasm classifier is trained on several markers of sarcasm in tandem, the most frequent being  X #not X , and performance is assessed on a realistically large and unbiased sample of tweets. Furthermore, we provide insight into the role of hyperbole in sarcastic tweets, and we perform a cross-lingual comparison of the use of sarcasm in Twitter by annotating French tweets ending with  X #sarcasme X .

This paper is structured as follows. In Section 1.1 we discuss the concepts of sarcasm, the broader category of verbal irony, and their communicative function according to the literature. In Section 2 we offer a brief survey of related work on the development of automatic detectors of polarity in social media. Our experimental setup is described in Section 3 . We report on the results of our experiments in Section 4 and analyse our results in view of the theoretical work discussed earlier in
Section 5 . We summarize our results, draw conclusions, and identify points for future research in Section 6 . 1.1. Definitions
Twitter members mark their sarcastic messages with different hashtags. As described in more detail in Section 3.1 ,we find that four words tend to be used as hashmarks in sarcastic posts:  X #sarcasm X ,  X #irony X ,  X #cynicism X  and  X #not X . Although sarcasm, irony and cynicism are not synonymous, they have much in common. This is especially true for sarcasm and irony; many researchers treat those phenomena as strongly related ( Attardo, 2007; Brown, 1980; Gibbs &amp; O X  X rien, 1991; Kreuz &amp;
Roberts, 1993; Mizzau, 1984; Muecke, 1969 ), and sometimes even equate the terms in their studies in order to work with a usable definition ( Grice, 1978; Tsur, Davidov, &amp; Rappoport, 2010 ). Cynicism is more mocking and tells us more about human beliefs than irony and sarcasm ( Eisinger, 2000 ), but there is a close correlation between these concepts ( Yoos, 1985 ). The hashtag  X #not X  is not the name of a rhetorical device or trope such as sarcasm, irony and cynicism, but it is a conventionally used meta-communication marker to indicate the message contains a shift in evaluative valence.

In psycholinguistics and cognitive linguistics sarcasm has been widely studied, often in relation with concepts such as cynicism, and with verbal irony as a broader category term. A brief overview of definitions, hypotheses and findings from communication studies regarding sarcasm and related concepts may help clarify what the hashtags convey.

In this study, we are interested in sarcasm as a linguistic phenomenon, and how we can detect it in social media mes-sages. Yet, Brown (1980) warns that sarcasm  X  X s not a discrete logical or linguistic phenomenon X  (p. 111), while verbal irony is. Indeed, Reyes and Rosso (2012) see sarcasm  X  X s specific extension[s] of a general concept of irony X  (p. 755). In line with the extensive use of #sarcasm in tweets to mark verbal irony, we take the liberty of using the term sarcasm while verbal irony would be the more appropriate term. Even then, according to Gibbs and Colston (2007) the definition of verbal irony is still a  X  X roblem that surfaces in the irony literature X  (p. 584).

There are many different theoretical approaches to verbal irony. It should (a) be evaluative, (b) be based on incongruence of the ironic utterance with the co-text or context, (c) be based on a reversal of valence between the literal and intended meaning, (d) be aimed at some target, and (e) be relevant to the communicative situation in some way ( Burgers, Van
Mulken, &amp; Schellens, 2011 ). Although it is known that irony is always directed at someone or something (the sender himself, the addressee, a third party, or a combination of the three, see Burgers et al. (2011) and Livnat (2004) ) and irony is used rel-atively often in dialogic interaction ( Gibbs, 2007 ), these two elements of irony are hardly examinable in the case of Twitter: the context of the Twitter messages is missing and it is inconvenient to investigate interaction. Therefore, it is hard to inter-pret the communicative situation and the target of the message. However, it is possible to analyse texts, such as tweets, on their evaluative meaning and a potential valence shift in the same way as Burgers et al. (2011) did. Burgers et al. X  X  own def-
Thus, a sarcastic utterance involves a shift in evaluative valence, which can go two ways: it could be a shift from a literally positive to an intended negative meaning, or a shift from a literally negative to an intended positive evaluation. Since Reyes,
Rosso, and Veale (2013) also argue that users of social media often use irony in utterances that involve a shift in evaluative valence, we use the definition of verbal irony of Burgers et al. (2011) in this study on sarcasm, and we use both terms syn-onymously. The definition of irony as saying the opposite of what is meant is commonly used in previous corpus-analytic studies, and is reported to be reliable ( Kreuz, Roberts, Johnson, &amp; Bertus, 1996; Leigh, 1994; Srinarawat, 2005 ). In order to ensure that the addressees detect the sarcasm in the utterance, senders use markers in their utterances.
Attardo (2000) states that those markers are clues a writer can give that  X  X lert a reader to the fact that a sentence is ironical X  (p. 7). The use of markers in written and spoken interaction may be different ( Jahandarie, 1999 ). In spoken interaction, sar-casm is often marked with a special intonation ( Attardo, Eisterhold, Hay, &amp; Poggi, 2003; Bryant &amp; Tree, 2005; Rockwell, 2007 ), air quotes ( Attardo, 2000 ) or an incongruent facial expression ( Attardo et al., 2003; Muecke, 1978; Rockwell, 2003 ). In written communication, authors do not have such clues at their disposal. Since sarcasm is more difficult to com-ees do not pick up on the sarcasm and interpret the utterances literally. To avoid misunderstandings, writers use linguistic markers for irony ( Burgers, Van Mulken, &amp; Schellens, 2012b ): tropes (a metaphor, hyperbole, understatement or rhetorical question), schematic irony markers (repetition, echo, or change of register), morpho-syntactic irony markers (exclamations, interjections, or diminutives), or typographic irony markers (such as capitalization, quotation marks and emoticons). Thus, besides hashtags to mark the opposite valence of a tweet, Twitter members may also use linguistic markers. A machine-learning classifier that learns to detect sarcasm should in theory be able to discover at least some of the features that
Burgers et al. (2012b) list, if given sufficient examples of all of them in a training phase. While metaphor and understatement may be too complex to discover, exclamations and typographical markers should be easy to learn. Hyperbole, or the  X  X peaker overstating the magnitude of something X  ( Colston, 2007, p. 194 ), may be discovered by the classifier by the fact that it is often linked to words that signal intensity, as we now analyse in more detail. 1.2. Linguistic markers of hyperbole
Especially in the absence of visual markers, sarcastic utterances need strong linguistic markers to be perceived as sarcas-that a sarcastic utterance with a hyperbole ( X  X antastic weather X ) is identified as sarcastic with more ease than a sarcastic utterance without a hyperbole ( X  X he weather is good X ). While both utterances convey a literally positive attitude towards the weather, the utterance with the hyperbolic  X  X antastic X  may be easier to interpret as sarcastic than the utterance with the non-hyperbolic  X  X ood X . Hyperbolic words carry intensity. Bowers (1964) defines language intensity as  X  X he quality of lan-guage which indicates the degree to which the speaker X  X  attitude toward a concept deviates from neutrality X  (p. 416). Accord-ing to Van Mulken and Schellens (2012) , an intensifier is a linguistic element that can be removed or replaced while respecting the linguistic correctness of the sentence and context, but resulting in a weaker evaluation. Intensifiers, thus, strengthen an evaluative utterance and could make an utterance hyperbolic. Typical word classes of intensifiers used for hyperbolic expressions, inter alia, are adverbs ( X  X ery X ,  X  X bsolutely X ) and adjectives ( X  X antastic X  instead of  X  X ood X ), in contrast to words which leave an evaluation unintensified, like  X  X retty X ,  X  X ood X  and  X  X ice X . According to Liebrecht (in preparation) , typographical elements such as capitals and exclamation marks are also intensifying elements which can create hyperbolic utterances. So, there is an overlap between linguistic elements to intensify and linguistic elements to overstate utterances. It may be that senders use such elements in their tweets to make the utterance hyperbolic, in order to signal sarcasm. 2. Related research
The automatic classification of communicative constructs in short texts has become a widely researched subject in recent years. Large amounts of opinions, status updates, and personal expressions are posted on social media platforms such as
Twitter. The automatic labeling of their polarity (to what extent a text is positive or negative) can reveal, when aggregated or tracked over time, how the public in general thinks about certain things. See Montoyo, Mart X nez-Barco, and Balahur (2012) for an overview of recent research in sentiment analysis and opinion mining.

A major obstacle for automatically determining the polarity of a (short) text are constructs in which the literal meaning of the text is not the intended meaning of the sender, as many systems for the detection of polarity primarily lean on positive and negative words as markers. The task to identify such constructs can improve polarity classification, and provide new insights into the relatively new genre of short messages and microtexts on social media. Previous works describe the clas-sification of emotions ( Davidov, Tsur, &amp; Rappoport, 2010a; Mohammad, 2012 ), irony ( Reyes et al., 2013 ), sarcasm ( Davidov, et al., 2012 ).
 Most common to our research are the works by Reyes et al. (2013), Tsur et al. (2010), Davidov et al. (2010b) and Gonz X lez-
Ib X nez et al. (2011) . Reyes et al. (2013) collect a training corpus of ironic tweets labeled with the hashtag  X #irony X , and train classifiers on different feature sets representing higher-level concepts such as unexpectedness, style, and emotions. The clas-sifiers are trained to distinguish  X #irony X -tweets from tweets containing the hashtags  X #education X ,  X #humour X , or  X #politics X , achieving F1-scores of around 70. Tsur et al. (2010) focus on product reviews on the World Wide Web, and try to identify sarcastic sentences from these in a semi-supervised fashion. Training data is collected by manually annotating sarcastic sen-tences, and retrieving additional training data based on the annotated sentences as queries. Sarcasm is annotated on a scale from 1 to 5. As features, Tsur et al. infer patterns from these sentences consisting of high-frequency words and content words. Their system achieves an F1-score of 79. Davidov et al. (2010b) apply a comparable system on a small set of tweets manually annotated on sarcasm, and achieve an F-score of 83. When testing the system on tweets marked with  X #sarcasm X , the F-score drops to 55. They state that apart from indicating the tone of a tweet,  X #sarcasm X  might be used as a search anchor and as a reference to a former sarcastic tweet, adding a fair amount of noise to the data. Gonz X lez-Ib X nez et al. (2011) aim to distinguish sarcasm from literally positive and negative sentiments in tweets. Tweets belonging to all three categories were collected based on hashtags describing them ( X #sarcasm X  and  X #sarcastic X  for sarcastic tweets) and tested through 5-fold cross-validation on a set comprising 900 tweets for each of the three categories. As features they make use of word unigrams and higher-level word categories. While the classifier achieves an accuracy of only 57%, it outperforms human judgement.
Gonz X lez-Ib X nez et al. (2011) conclude that the lack of context makes the detection of sarcasm in tweets difficult, both for humans and for machines.

In the works described above, a system is tested in a controlled setting: Reyes et al. (2013) compare irony to a restricted set of other topics, Tsur et al. (2010) take from the unlabeled test set a sample of product reviews with 50% of the sentences classified as sarcastic, and Gonz X lez-Ib X nez et al. (2011) train and test in the context of a small set of positive, negative and sarcastic tweets. In contrast, we apply a trained sarcasm detector to a real-world test set representing a realistically large sample of tweets posted on a random day, the vast majority of which is not sarcastic. Detecting sarcasm in social media is, arguably, a needle-in-a-haystack problem: of the 2.25 million tweets we gathered on a single day, 353 are explicitly marked with the #sarcasm or its pseudo-synonyms. It is therefore only reasonable to test a system in the context of a typical distribution of sarcasm in tweets. 3. Experimental setup 3.1. Data 3.1.1. Hashtag selection
As argued in Section 1.1 , while  X #sarcasm X  ( X #sarcasme X  in Dutch  X  we use the English translations of our hashtags throughout this article) is the most obvious hashtag for sarcastic tweets, we base our training set on an expanded set of pseudo-synonymous hashtags. We also found empirical evidence that we need to expand the set of hashtags. Liebrecht,
Kunneman, and Van den Bosch (2013) reported that training a classifier solely on  X #sarcasm X  as a training label resulted in high weights for hashtags that have the same function as  X #sarcasm X : to switch the evaluative valence or give a description of the type of tweet. While Qadir and Riloff (2013) expand sets of hashtags denoting the emotion of a tweet by bootstrapped learning, this approach does not seem appropriate for the more subtle rhetorical instrument of sarcasm. We decided to extract all hashtags from the ranked list of features from the ( Liebrecht et al., 2013 ) study and manually examine the tweets accompanying them by means of twitter.com . From this examination, we selected the hashtags that almost unambiguously denoted sarcasm in a tweet in addition to  X #sarcasm X :  X #irony X ,  X #cynicism X , and  X #not X . The former two denote tropes com-parable to sarcasm, while the latter is also typically used to switch the evaluative valence of a message. Hashtags that only partly overlap in function such as  X #joke X  or  X #haha X  were not included due to their ambiguous usage (either shifting the eval-uative valence of a message or simply denoting a funny tweet). 3.1.2. Data collection
For the collection of tweets we made use of a database provided by the Netherlands e-Science Centre consisting of IDs of a substantial portion of all Dutch tweets posted from December 2010 onwards ( Tjong Kim Sang &amp; Van den Bosch, 2013 ). this database, we collected all tweets that contained the Dutch versions of the selected hashtags  X #sarcasm X ,  X #irony X ,  X #cyni-cism X , and  X #not X  until January 31st 2013. This resulted in a set of 644,057 tweets in total. Following Mohammad (2012) and
Gonz X lez-Ib X nez et al. (2011) , we cleaned up the dataset by only including tweets in which the given hashtag was placed at the end or exclusively followed by other hashtags or a url. Hashtags placed somewhere in the middle of a tweet are more likely to be a grammatical part of the sentence than a label ( Davidov et al., 2010b ), and may refer to only a part of the tweet. Addi-tionally, we discarded re-tweets (repostings of an earlier tweet by someone else). Applying these filtering steps resulted in 406,439 tweets in total as training data. Table 1 offers more details on the individual hashtags;  X #not X  occurs a factor more fre-quently than  X #sarcasm X , which in turn occurs a factor more frequently than  X #irony X ; X #cynicism X  again occurs a factor less frequently.

We trained a classifier on sarcastic tweets by contrasting them against a background corpus. For this, we took a sample of tweets in the period from October 2011 until September 2012 (not containing tweets with any of the sarcastic hashtags). To provide the classifier with an equal number of cases for the sarcasm and background categories and thus produce a training set without class skew, 406,439 tweets were selected randomly, equal to the amount of sarcastic tweets. Again, we did not include re-tweets in the sample.

To test our classifier in a realistic setting, we collected a large sample of tweets posted on a single day outside the time frame from which the training set is collected, namely February 1, 2013. After removal of re-tweets, this set of tweets con-tains approximately 2.25 million tweets, of which 353 carry one of the sarcasm hashtags at the end.

While the distribution of sarcastic versus other tweets on the test day is highly imbalanced, we chose not to copy this distribution to the training stage. As pointed out by Chawla, Japkowicz, and Kotcz (2004) , in an imbalanced learning context classifiers tend to be overwhelmed by the large classes and ignore the small ones. To avoid the influence of class size, we decided to completely balance the tweets with and without  X #sarcasm X  in the training set. This is likely to drive the classifier to overshoot its classification of tweets as sarcastic in the testset, but we consider only the top of its confidence-based rank-ing; in our evaluation of the ability of the classifier to detect sarcasm in tweets that lack an explicit hashtag, we evaluate the ranking of the classifier with precision at n &lt; 250. 3.2. Winnow classification
All collected tweets were tokenized. 3 Punctuation, emoticons, and capitalization information were kept, as these may be used to signal sarcasm ( Burgers et al., 2012b ). We made use of word uni-, bi-and trigrams as features (including punctuation and emoticons as separate words). User names and URLs were normalized to  X  X SER X  and  X  X RL X  respectively. We removed fea-tures containing one of the hashtags from the training set. Finally, we removed terms that occurred three times or less or in two tweets or less.

As classification algorithm we made use of Balanced Winnow ( Littlestone, 1988 ) as implemented in the Linguistic Clas-sification System. 4 This algorithm is known to offer state-of-the-art results in text classification, and produces interpretable per-class weights that can be used to, for example, inspect the highest-ranking features for one class label. The a and b param-number of iterations was bounded to a maximum of three. 3.3. Evaluation
To evaluate the outcome of our machine learning experiment we ran two evaluations. The first evaluation focuses on the 353 tweets in the test set ending with one of the selected sarcasm-hashtags, among 2.25 million other non-sarcastic tweets.
We measured how well these tweets were identified using the true positive rate (TPR, also known as recall), false positive rate (FPR) and their joint score, the area under the curve (AUC). AUC is a common evaluation metric that is argued to be more resistant to skew than F-score, due to relying on FPR rather than precision ( Fawcett, 2004 ).

For the second evaluation we manually inspect the test tweets identified by the classifier as sarcastic, but that do not carry any of the sarcastic hashtags. While they would be labeled as false positives in the first evaluation, the absence of one of these hashtags does not necessarily imply the tweet is non-sarcastic. In fact, the proper detection of sarcastic tweets not explicitly marked as such with a hashtag would be the ideal functionality of our classifier. For this evaluation we make use of the classifier X  X  characteristic to assign per-instance scores to each label, which can be seen as its confidence in that label. We rank its predictions by the classifier X  X  confidence on the  X  X arcasm X  label and inspect manually which of the top-ranking tweets is indeed sarcastic. Based on this manual annotation we can compute the precision at different rank numbers, which may reveal whether the top-ranked false positives are in fact sarcastic tweets. 4. Results
Results for the first evaluation are displayed in Table 2 . Of the 353 tweets explicitly marked with the  X #sarcasm X  or its pseudo-synonyms on the test day, 307 (87%) are identified as sarcastic, in addition to 376,144 tweets not containing such a hashtag. Because this latter amount is not a big part of the 2.25 million tweets in the test set, the FPR is fairly low and a good AUC of 0.85 is achieved.

Besides generating an absolute winner-take-all classification, our Balanced Winnow classifier assigns scores to each label that can be seen as its confidence in that label. We can rank its predictions by the classifier X  X  confidence on the  X  X arcasm X  label and inspect manually which of the top-ranking tweets that do not contain any of the four target hashtags is indeed sarcastic. We generated a list of the 250 most confident  X  X arcasm X -labeled tweets. Three annotators (three of the authors of this paper) judged these tweets as being either sarcastic or not. The instructions beforehand were to positively annotate tweets that were clearly expressing a positive or negative valence that is shifted by the language use. In case of doubt, for example due to the lack of context, a tweet should be annotated as non-sarcastic. The sarcasm should be clear from the text in a tweet; the annotator was not allowed to enquire into the conversational context when a tweet was addressed to one or more twitter users.

When taking the majority vote of the three annotators as the golden label, a curve of the precision at all points in the ranking can be plotted. This curve is displayed in Fig. 1 . The overall performance at the end of the plotted curve is about 0.35. After peaking at a precision of 0.6 after 10 tweets, precision decreases rapidly before stabilizing after rank 50. Precision scores are lower if sarcasm is only labeled with unanimous agreement between the annotators, ending below 0.3.
In order to test for intercoder reliability, Cohen X  X  Kappa was used. In line with Siegel and Castellan (1988) , we calculated a mean Kappa based on pair-wise comparisons of all possible coder pairs. The mean intercoder reliability between the three possible coder pairs is moderate at j  X  : 53. The average mutual F-score over all annotator pairs is 0.72, indicating that anno-tators disagree in about a quarter of all cases. 5. Analysis 5.1. Reliability of the training set
An important additional check on our results concerns the reliability of the user-generated sarcastic hashtags as golden labels, as Twitter users cannot all be assumed to understand what sarcasm is, or be versed in using tropes. The three anno-tators who annotated the ranked classifier output also coded a random sample of 250 tweets with sarcastic hashtags from the training set. The tweets were sampled proportional to the percentage of the four hashtags in the training set (e.g.: 162  X #not X -tweets, 86  X #sarcasm X -tweets and 2  X #irony X -tweets). The instructions beforehand were to decide whether a tweet contains a positive or negative valence, which is shifted by means of the hashtag at the end.

The average score of agreement between the three possible coder pairs turned out to be moderate ( j  X  : 44), but due to the majority of the tweets being genuinely sarcastic, the mutual F-score between the annotators is 0.94, indicating a dis-agreement on a fairly random 6% of cases. Taking the majority vote over the three annotations as the reference labeling, 212 of the 250 annotated sarcastic tweets, about 90%, were found to be sarcastic. Using hashtags as golden labels thus intro-duces about 10% noise into the labeled training data. The outcome of the annotated tweets for  X #not X  and  X #sarcasm X  sepa-rately is in balance, with respective scores of 90% and 91%. These outcomes are in line with a similar annotation that was conducted in Liebrecht et al. (2013) , sampling 250 tweets that were all labeled with the hashtag  X #sarcasm X . Of these tweets, 85% were judged as being actually sarcastic. 5.2. Predictors of a sarcastic tweet
While the classifier performance gives an impression of its ability to detect sarcastic tweets, the strong indicators of sar-casm as discovered by the classifier may provide additional insights into the usage of sarcasm by Twitter users. Including only word unigrams, bigrams, and trigrams as features brings about an unbiased classifier model to be analysed. We set out to analyse the feature weights assigned by the Balanced Winnow classifier ranked by the strength of their connection to the sarcasm label, taking into account the 500 tokens and n -grams with the highest positive weight towards the sarcasm class. These words and n -grams provide insight into the topics Twitter users are talking about. Even though Liebrecht et al. (2013) reported on topical words appearing as strong predictors, relating to school, the weather, holidays, public transport, soccer, and television programs, in our current study, which is based on a significantly larger training set, such topics are hardly present in the top-500. The 500 words and n -grams are mostly adverbs and adjectives that realize a positive evalu-ation (including intensifiers), exclamations, and non-sarcastic hashtags for meta-communication.

We suspected that the sarcastic utterances contained many intensifiers to make the tweets hyperbolic. The list of stron-gest predictors shows that some intensifiers are indeed strong predictors of sarcasm, such as (with and without capitals) geweldig (awesome), heerlijk (lovely), prachtig (wonderful), boeiend (fascinating), allerleukste (most fun), perfect , and super .
However, besides these intensifiers many unintensified positive adverbs and adjectives occur in the list of strongest predic-tors as well, such as interessant (interesting), gezellig (cozy), leuk (fun), handig (handy), slim (smart), charmant (charming) and nuttig (useful). Considerably less negative words occur as strong predictors. This supports our hypothesis that the utterances are mostly positive, while the opposite meaning is meant. This finding corresponds with the results of Burgers et al. (2012b) , who show that 77% of the ironic utterances in Dutch communication are literally positive. It also concurs with the observa-tion that a sarcastic utterance always implies an evaluation: these (positive) adverbs and adjectives explicitly indicate (and thus mark) that the sender intentionally conveys an attitude towards his or her message.

A substantial set of positive exclamations are found by the classifier as strong predictors. Exclamations are another means to make an utterance hyperbolic and thereby sarcastic. Examples of Dutch exclamations within the top-500 of most predic-tive features are (with and without # or capitals): jippie, yes, goh, joepie, jeej, jeuj, yay, woehoe , and wow .
The fourth group of features in the top-500 are non-sarcastic hashtags that signal meta-communication, such as  X #humor X ,  X #lml X  (love my life),  X #wehebbenerzinin X  (we are looking forward to it),  X #gaatgoed X  (all is well),  X #bedankt X  (thanks), and  X #grapje X  (joke).

To inspect in more detail the actual occurrence of the four types of words that constitute the top-500 of most predictive features, we further analyse the sarcastic tweets without sarcastic hashtags that our classifier correctly identifies in the top-250 ranked tweets of our test day, and contrast this with the tweets in our training set that do have a sarcastic hashtag. Fig. 2 displays two proportional Venn diagrams of occurrences in these two sets of tweet of the four aforementioned categories of markers: intensified and unintensified adverbs and adjectives, exclamations, and non-sarcastic hashtags. The Venn diagram on the right in Fig. 2 visualizes the proportions of tweets without a hashtag correctly identified as being sarcastic that have one or more of these four categories, or none of them (the circle labeled  X  X one X ). The overlap between circles in the Venn diagram visualize which proportion of tweets have a combination of two or more of the four marker categories. The left dia-gram represents all tweets in the training set with a sarcastic hashtag, while the right diagram represents sarcastic tweets without a sarcastic hashtag. As can be seen in the figure, most sarcastic tweets without a hashtag have unintensified eval-uative words. The other three categories occur less frequently, and some of these occur in combination, the most frequent combination being between unintensified evaluative words and exclamations.

The left diagram differs in three aspects from the right diagram: first, the number of tweets containing none of the major linguistic sarcasm markers is the largest category; second, the four categories almost never co-occur. The overall third obser-vation is that the presence of a sarcastic tag in the training tags appears to mute the occurrence of non-sarcastic hashtags.
These differences suggests that the presence of an explicit sarcasm hashtag requires fewer other clues to signify sarcasm. 5.3. #sarcasme in French tweets
We have shown that a polarity shift between the actual and intended valence of a message can to a certain extent be recognized automatically in the case of Dutch tweets, by means of hashtag labels. This complements previous findings for
English tweets. Thus, in both languages such hashtags are predominantly applied in the same way. Future research would be needed to chart the prediction of sarcasm in languages that are more distant to Dutch. As the findings in this analysis sug-gests, sarcasm may be signaled rather differently in other cultures ( Goddard, 2006 ). Languages may use the same type of marker in different ways, like a different intonation in spoken sarcasm by English and Cantonese speakers ( Cheang &amp; Pell, 2009 ). Such a difference between languages in the use of the same marker may also apply to written sarcastic utterances, such as tweets.

To investigate the potential success of leveraging hashtag marked tweets in other language regions, we set out to annotate a sample of 500 French tweets ending with #sarcasme (French for  X  X arcasm X ). The tweets were harvested from topsy.com , by means of the otter API. 5 We queried tweets containing #sarcasme, setting the language to French and including all days in 2012 and 2013. This resulted in 8301 tweets. From this sample, we removed retweets and tweets that did not end in #sarcasme, and took a random sample of 500 tweets. The tweets were annotated by one of the authors and a second person.
Both annotators were L1 speakers of Dutch with a French L2 near-native proficiency. The instructions beforehand were the same as for the annotation of the Dutch #sarcasm tweets sampled from the training data.

The annotators marked 63% of the tweets both as sarcastic, with a moderate j of .43 and a mutual F-score of .85. The percentage of sarcastically marked tweets by both annotators is smaller than the 90% attained with the Dutch tweets. When we split the three annotators of the Dutch tweets in pairs of two, allowing a better comparison with the two annotators of the French tweets, the percentages are also higher (.85, .82, and .84).

Speakers of French seem to be more lenient with the hasthag #sarcasme than speakers of Dutch (or English for that mat-ter), because they also use it to signal other rhetorical figures, such as paradoxes, rhetorical questions and other types of humor. Since the instruction explicitly asked annotators to look for a shift in evaluative valence for a tweet being labeled as sarcastic, the percentage of polarity shifting tweets is accordingly lower. Moreover, the number of tweets without an explicit evaluation was also considerable. Apparently, users of French in tweets more heavily rely on context (and on the receiver being able to interpret the tweet correctly) than Dutch or English users do. The difference between Dutch and French sarcastic tweets suggests that culture also influences the use and reception of sarcasm (and especially the use of  X #sarcasme X ). This is in line with Holtgraves (2005) who also argues that the use and interpretation of non-literal meanings can be culture-specific. 6. Conclusion
In this study we developed and tested a system that detects sarcastic tweets in a realistic sample of 2.25 million Dutch tweets posted on a single day, trained on a set of 406 thousand tweets, harvested over time, marked by the hashtags  X #sarcasm X ,  X #irony X ,  X #cynicism X , or  X #not X  by the senders, plus 406 thousand tweets without these tags. The classifier attains an AUC score of .84 and is able to correctly spot 309 of the 353 tweets among the 2.25 million that were explicitly marked with the hashtag, with the hashtag removed. Testing the classifier on the top 250 of the tweets it ranked as most likely to be sarcastic, but that did not have a sarcastic hashtag, it attains only a 35% average precision. We can conclude that it is fairly hard to distinguish sarcastic tweets from literally intended tweets in an open setting, though the top of the classifier X  X  rank-ing does identify many sarcastic tweets not explicitly marked with a hashtag.

An additional linguistic analysis provides some insights into the characteristics of sarcasm on Twitter. We found that most tweets contain a literally positive message, and contain four types of markers for sarcasm: intensified as well as unin-tensified evaluative words, exclamations, and non-sarcastic hashtags. Intensified evaluative words and exclamations induce hyperbole, but they occur less frequently in sarcastic tweets than unintensified evaluative words. Note that we based our selection of marker categories on the top-500 of most predictive features; other linguistic markers from Burgers et al. (2012b) did not occur in this set and were not included in this study. The differences between the occurrence or absence of markers displayed in the two Venn diagrams of Fig. 2 indicate that the inclusion of a sarcastic hashtag reduces the use of linguistic markers that otherwise would be needed to mark sarcasm. Arguably, extralinguistic elements such as hashtags can be seen as the social media equivalent of non-verbal expressions that people employ in live interaction when conveying sarcasm. As Burgers et al. (2012a) show, the more explicit markers an ironic utterance contains, the better the utterance is understood, the less its perceived complexity is, and the better it is rated. Many Twitter users already seem to apply this knowledge.

To investigate the usefulness of a sarcastic hashtag to train sarcasm detection in other language regions, we annotated 500 French tweets containing #sarcasme, finding that the majority of French tweets could indeed be labeled as sarcastic, but to a lesser extent than Dutch tweets. In other words: also in French, the hashtag signals a polarity switch in most cases.
Apart from hashtag usage, markers of sarcasm can be language-specific. In Dutch, for example, diminutives can mark irony ( Burgers et al., 2012a ), while the neighbor language English does not have this device. Dedaic  X  (2005) shows other language-specific markers that have been associated with irony in the Croatian language; as did Bennett-Kastor (1992) for the Ghanese language Sissala. Knowledge of specific sarcasm markers in a language could be used as explicitly added features to our system.
Although this study provides insights into the use of sarcasm in Twitter messages and training a machine learning classifier to detect this rhetoric device automatically, we must emphasize that we focused in our analysis on the hyperbole, one of the linguistic markers people use for sarcasm ( Burgers et al., 2012b ), and we did find evidence for the fact that typical hyperbole inducers, i.e. exclamations and intensifiers, appear among the most predictive features of our classifier. In future work we need to explore means to allow our classifier to detect other linguistic markers, such as rhetorical questions, rep-etition, echo, change of register, interjections, or diminutives, many of which cannot simply be inferred from the presence of words or n -grams of words. Since gender, education and profession can be predictors of the use of irony, just like previous use of irony, we also need to further explore the possibilities of including speaker and context characteristics in the model ( Wallace, 2013 ).

Another strand of future research would be to expand our scope from sarcasm to other more subtle variants of irony, such as understatements, euphemisms, and litotes ( Burgers et al., 2012b ). Following Giora, Fein, Ganzi, Levi, and Sabah (2005) , there seems to be a spectrum of degrees of irony from the sarcastic  X  X ax is exceptionally bright X  via the ironic  X  X ax is not exceptionally bright X , the understatement  X  X ax is not bright X  to the literal  X  X ax is stupid X . In the first three utterances a gap exists between what is literally said and the intended meaning of the sender. The greater the gap or contrast, the easier et al., 2005 ). We may need to combine the sarcasm detection task with the problem of the detection of negation and hedging markers and their scope ( Morante &amp; Daelemans, 2009; Morante, Liekens, &amp; Daelemans, 2008 ) in order to arrive at a compre-hensive account of polarity-reversing mechanisms, which in sentiment analysis is still highly desirable.
 References
