 University of Leeds
We compare two ways of obtaining lexical knowledge for antecedent selection in other -anaphora and definite noun phrase coreference. Specifically, we compare an algorithm that relies on links encoded in the manually created lexical hierarchy WordNet and an algorithm that mines corpora by means of shallow lexico-semantic patterns. As corpora we use the British National Corpus show that (a) the knowledge encoded in WordNet is often insufficient, especially for anaphor X  anaphora, the Web-based method outperforms the WordNet-based method; (c) for definite NP coreference, the Web-based method yields results comparable to those obtained using WordNet set; (d) in both case studies, the BNC-based method is worse than the other methods because of data sparseness. Thus, in our studies, the Web-based method alleviated the lexical knowledge gap often encountered in anaphora resolution and handled examples with context-dependent relations between anaphor and antecedent. Because it is inexpensive and needs no hand-modeling of lexical knowledge, it is a promising knowledge source to integrate into anaphora resolution systems. 1. Introduction
Most work on anaphora resolution has focused on pronominal anaphora, often achieving good accuracy. Kennedy and Boguraev (1996), Mitkov (1998), and Strube, Rapp, and Mueller (2002), for example, report accuracies of 75.0%, 89.7%, and an
F -measure of 82.8% for personal pronouns, respectively. Less attention has been paid to nominal anaphors with full lexical heads, which cover a variety of phenomena, such as coreference (Example (1)), bridging (Clark 1975; Example (2)), and comparative anaphora (Examples (3 X 4)). 1
In Example (1), the definite noun phrase (NP) the periodical corefers with the magazine . 2 In Example (2), the definite NP the hood can be felicitously used because a related entity has already been introduced by the NP the jacket , and a part-of relation between the two entities can be established. Examples (3) X (4) are instances of other -anaphora. Other -anaphora are a subclass of comparative anaphora (Halliday and Hasan 1976; Webber et al. 2003) in which the anaphoric NP is introduced by a lexical modifier (such as other, such, and comparative adjectives) that specifies the relationship (such as set-complement, similarity and comparison) between the entities invoked by anaphor and antecedent. For other -anaphora, the modifiers other or another provide a set-complement to an entity already evoked in the discourse model. In Example (3), the NP other, far-reaching repercussions refers to a set of repercussions excluding increasing costs and can be paraphrased as other (far-reaching) repercussions than (increasing) costs . Similarly, in Example (4), the NP another such facility refers to a group home which is not identical to the specific (planned) group home mentioned before.
 to understand anaphors with full lexical heads. For the examples above, we need that costs can be or can be viewed as repercussions of an event, and that institutional homes are facilities. Therefore, many resolution systems that handle these phenomena (Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b; Modjeska 2002; Gardent, Manuelian, and Kow 2003, among others) rely on hand-crafted resources of lexico-semantic knowledge, such as the WordNet lexical hierarchy (Fellbaum 1998). 3 In Section 2, we summarize previous work that has given strong indications that such resources are insufficient for the entire range of full NP anaphora. Additionally, we discuss some serious methodological problems that arise when fixed ontologies are used that have been encountered by previous researchers and/or us: the costs of building, maintaining and mining ontolo-gies; domain-specific and context-dependent knowledge; different ways of encoding information; and sense ambiguity. 368 bases, which we call the corpus-based approach. A number of researchers (Hearst 1992; Berland and Charniak 1999, among others) have suggested that knowledge bases be enhanced via (semi)automatic knowledge extraction from corpora, and such enhanced knowledge bases have also been used for anaphora resolution, specifically for bridging (Poesio et al. 2002; Meyer and Dale 2002). Building on our previous work (Markert, Nissim, and Modjeska 2003), we extend this corpus-based approach in two ways. First, we suggest using the Web for anaphora resolution instead of the smaller-size, but less noisy and more balanced, corpora used previously, making available a huge additional source of knowledge. 4 Second, we do not induce a fixed lexical knowledge base from the Web but use shallow lexicosyntactic patterns and their Web frequencies for anaphora resolution on the fly. This allows us to circumvent some of the above-mentioned methodological problems that occur with any fixed ontology, whether constructed manually or automatically.
 sources of lexical knowledge for the task of antecedent selection or antecedent ranking in anaphora resolution. We focus on two types of full NP anaphora: other-anaphora (Section 4) and definite NP coreference (Section 5). 5 In both case studies, we compare an algorithm that relies mainly on the frequencies of lexico-syntactic patterns in corpora (both the Web and the BNC) with an algorithm that relies mainly on a fixed ontology (WordNet 1.7.1). We specifically address the following questions: 1. Can the shortcomings of using a fixed ontology that have been stipulated 2. How does corpus-based knowledge acquisition compare to using 3. To what extent is the answer to the previous question dependent on the
In Section 6 we discuss several aspects of our findings that still need elaboration in future work. Specifically, our work is purely comparative and regards the different lexical knowledge sources in isolation . It remains to be seen how the results carry forward when the knowledge sources interact with other features (for example, grammatical preferences). A similar issue concerns the integration of the methods into anaphoricity determination in addition to antecedent selection. Additionally, future work should explore the contribution of different knowledge sources for yet other anaphora types. 2. The Knowledge Gap and Other Problems for Lexico-semantic Resources
A number of previous studies (Harabagiu 1997; Kameyama 1997; Vieira and Poesio 2000; Harabagiu, Bunescu, and Maiorano 2001; Strube, Rapp, and Mueller 2002;
Modjeska 2002; Gardent, Manuelian, and Kow 2003) point to the importance of lexical and world knowledge for the resolution of full NP anaphora and the lack of such knowledge in existing ontologies (Section 2.1). In addition to this knowledge gap, we summarize other, methodological problems with the use of ontologies in anaphora resolution (Section 2.2). 2.1 The Knowledge Gap for Nominal Anaphora with Full Lexical Heads
In the following, we discuss previous studies on the automatic resolution of coreference, bridging and comparative anaphora, concentrating on work that yields insights into the use of lexical and semantic knowledge. 2.1.1 Coreference. The prevailing current approaches to coreference resolution are evaluated on MUC-style (Hirschman and Chinchor 1997) annotated text and treat pronominal and full NP anaphora, named-entity coreference, and non-anaphoric coreferential links that can be stipulated by appositions and copula. The performance of these approaches on definite NPs is often substantially worse than on pronouns and/or named entities (Connolly, Burger, and Day 1997; Strube, Rapp, and Mueller 2002; Ng and Cardie 2002b; Yang et al. 2003). For example, for a coreference resolution algorithm on German texts, Strube, Rapp, and Mueller (2002) report an F -measure of 33.9% for definite NPs that contrasts with 82.8% for personal pronouns.
 whereas pronouns are mostly anaphoric in written text, definite NPs do not have to be so, inducing the problem of whether a definite NP is anaphoric in addition to determining an antecedent from among a set of potential antecedents (Fraurud 1990;
Vieira and Poesio 2000). 6 Second, the antecedents of definite NP anaphora can occur at considerable distance from the anaphor, whereas antecedents to pronominal anaphora tend to be relatively close (Preiss, Gasperin, and Briscoe 2004; McCoy and Strube 1999).
An automatic system can therefore more easily restrict its antecedent set for pronominal anaphora.
 whereas entities that are not in focus are referred to by definite descriptions (Hawkins 1978; Ariel 1990; Gundel, Hedberg, and Zacharski 1993), because the head nouns of anaphoric definite NPs provide the reader with lexico-semantic knowledge. Antecedent accessibility is therefore additionally restricted via semantic compatibility and does not need to rely on notions of focus or salience to the same extent as for pronouns. Given this lexical richness of common noun anaphors, many resolution algorithms for coreference have incorporated manually controlled lexical hierarchies, such as WordNet. They use, for example, a relatively coarse-grained notion of semantic compatibility between a few high-level concepts in WordNet (Soon, Ng, and Lim 2001), or more detailed hyponymy and synonymy links between anaphor and antecedent head nouns (Vieira and Poesio 370 2000; Harabagiu, Bunescu, and Maiorano 2001; Ng and Cardie 2002b, among others).
However, several researchers have pointed out that the incorporated information is still insufficient. Harabagiu, Bunescu, and Maiorano (2001) (see also Kameyama 1997) report that evaluation of previous systems has shown that  X  X ore than 30% of the missed coreference links are due to the lack of semantic consistency information between the anaphoric noun and its antecedent noun X  (page 59). Vieira and Poesio (2000) report results on anaphoric definite NPs in the WSJ that stand in a synonymy or hyponymy relation to their antecedents (as in Example (1)). Using WordNet links to retrieve the appropriate knowledge proved insufficient, as only 35.0% of synonymy relations and 56.0% of hyponymy relations needed were encoded in WordNet as direct or inherited links. 7 The semantic knowledge used might also not necessarily improve on string matching: Soon, Ng, and Lim (2001) final, automatically derived decision tree does not incorporate their semantic-compatibility feature and instead relies heavily on string matching and aliasing, thereby leaving open how much information in a lexical hierarchy can improve over string matching.
 knowledge). We investigate whether the knowledge gap for definite NP coreference can be overcome by using corpora as knowledge sources as well as whether the incorporation of lexical knowledge sources improves over simple head noun matching. 2.1.2 Comparative Anaphora. Modjeska (2002) X  X ne of the few computational studies on comparative anaphora X  X hows that lexico-semantic knowledge plays a larger role than grammatical salience for other -anaphora. In this article, we show that the semantic knowledge provided via synonymy and hyponymy links in WordNet is insufficient for the resolution of other -anaphora, although the head of the antecedent is normally a synonym or hyponym of the head of the anaphor in other -anaphora (Section 4.4). 2.1.3 Bridging. Vieira and Poesio (2000) report that 62.0% of meronymy relations (see Example (2)) needed for bridging resolution in their corpus were not encoded in WordNet. Gardent, Manuelian, and Kow (2003) identified bridging descriptions in a
French corpus, of which 187 (52%) exploited meronymic relations. Almost 80% of these were not found in WordNet. Hahn, Strube, and Markert (1996) report experiments on 109 bridging cases from German information technology reports, using a hand-crafted, domain-specific knowledge base of 449 concepts and 334 relations. They state that 42 (38.5%) links between anaphor and antecedents were missing in their knowledge base, a high proportion given the domain-specific task. In this article, we will not address bridging, although we will discuss the extension of our work to bridging in Section 6. 2.2 Methodological Problems for the Use of Ontologies in Anaphora Resolution
Over the years, several major problems have been identified with the use of ontologies for anaphora resolution. In the following we provide a summary of the different issues raised, using the examples in the Introduction. 2.2.1 Problem 1: Knowledge Gap. As discussed above, even in large ontologies the lack of knowledge can be severe, and this problem increases for non-hyponymy rela-tions. None of the examples in Section 1 are covered by synonymy, hyponymy, or meronymy links in WordNet; for example, hoods are not encoded as parts of jackets, and homes are not encoded as a hyponym of facilities. In addition, building , extending , and maintaining ontologies by hand is expensive. 2.2.2 Problem 2: Context-Dependent Relations. Whereas the knowledge gap might be reduced as (semi)automatic efforts to enrich ontologies become available (Hearst 1992; Berland and Charniak 1999; Poesio et al. 2002), the second problem is intrinsic to fixed context-independent ontologies: How much and which knowledge should they include? Thus, Hearst (1992) raises the issue of whether underspecified, context-or point-of-view-dependent hyponymy relations (like the context-dependent link between costs and repercussions in Example (3)) should be included in a fixed ontology, in addition to universally true hyponymy relations. Some other hyponymy relations that we encountered in our studies whose inclusion in ontologies is debatable are age:(risk) factor , coffee:export , pilots:union , country:member . 2.2.3 Problem 3: Information Encoding. Knowledge might be encoded in many different ways in a lexical hierarchy, and this can pose a problem for anaphora resolution (Humphreys et al. 1997; Poesio, Vieira, and Teufel 1997). For example, although magazine and periodical are not linked in WordNet via synonymy/hyponymy, the gloss records magazine as a periodic publication . Thus, the desired link might be derived through the analysis of the gloss together with derivation of periodical from periodic . However, such extensive mining of the ontology (as performed, e.g., by Harabagiu,
Bunescu, and Maiorano [2001]) can be costly. In addition, different information sources must be weighed (e.g., is a hyponymy link preferred over a gloss inclusion?) and combined (should hyponyms/hyperonyms/sisters of gloss expressions be considered recursively?). Extensive combinations also increase the risk of false positives. 2.2.4 Problem 4: Sense Proliferation. Using all senses of anaphor and potential an-tecedents in the search for relations might yield a link between an incorrect antecedent candidate and the anaphor due to an inappropriate sense selection. On the other hand, considering only the most frequent sense for anaphor and antecedent (as is done in
Soon, Ng, and Lim [2001]) might lead to wrong antecedent assignment if a minority sense is intended in the text. So, for example, the most frequent sense of hood in
Wo rd N e t i s criminal , whereas the sense used in Example (2) is headdress . The alterna-tives are either weighing senses according to different domains or a more costly sense disambiguation procedure before anaphora resolution (Preiss 2002). 3. The Alternative: Corpus-Based Knowledge Extraction
There have been a considerable number of efforts to extract lexical relations from corpora in order to build new knowledge sources and enrich existing ones without time-372 consuming hand-modeling. This includes the extraction of hyponymy and synonymy relations (Hearst 1992; Caraballo 1999, among others) as well as meronymy (Berland and Charniak 1999; Meyer 2001). 10 One approach to the extraction of instances of a particular lexical relation is the use of patterns that express lexical relations structurally explicitly in a corpus (Hearst 1992; Berland and Charniak 1999; Caraballo 1999; Meyer 2001), and this is the approach we focus on here. As an example, the pattern NP otherNP 2 usually expresses a hyponymy/similarity relation between the hyponym
NP 1 and its hypernym NP 2 (Hearst 1992), and it can therefore be postulated that two noun phrases that occur in such a pattern in a corpus should be linked in an ontology via a hyponymy link. Applications of the extracted relations to anaphora resolution are less frequent. However, Poesio et al. (2002) and Meyer and Dale (2002) have used patterns for the corpus-based acquisition of meronymy relations: these patterns are subsequently exploited for bridging resolution.
 lem 1 in Section 2.2.1), the incorporation of the acquired knowledge into a fixed ontology yields other problems. Most notably, it has to be decided which knowl-edge should be included in ontologies, because pattern-based acquisition will also find spurious, subjective and context-dependent knowledge (see Problem 2 in
Section 2.2.2). There is also the problem of pattern ambiguity, since patterns do not necessarily have a one-to-one correspondence to lexical relations (Meyer 2001).
Following our work in Markert, Nissim, and Modjeska (2003), we argue that for the task of antecedent ranking, these problems can be circumvented by not constructing a fixed ontology at all. Instead, we use the pattern-based approach to find lexical relationships holding between anaphor and antecedent in corpora on the fly . For instance, in Example (3), we do not need to know whether costs are always repercus-sions (and should therefore be linked via hyponymy in an ontology) but only that they are more likely to be viewed as repercussions than the other antecedent candidates.
We therefore adapt the pattern-based approach in the following way for antecedent selection.

As the patterns can be elaborate, most manually controlled and linguistically processed corpora are too small to determine the pattern frequencies reliably. Therefore, the size of the corpora used in some previous approaches leads to data sparseness (Berland and Charniak 1999), and the extraction procedure can therefore require extensive smoothing.
Thus as a further extension, we suggest using the largest corpus available, the Web, in the above procedure. The instantiation for the correct antecedent facilities in Example (4), for instance, does not occur at all in the BNC but yields over 1,500 hits on the Web. 12 The competing instantiations (listed in Step 3) yield 0 hits in the BNC and fewer than 20 hits on the Web.
 coreference and other-anaphora that evaluate the ontology-and corpus-based ap-proaches in general and our extensions in particular. 4. Case Study I: Other -Anaphora
We now describe our first case study for antecedent selection in other -anaphora. 4.1 Corpus Description and Annotation
We use Modjeska X  X  (2003) annotated corpus of other -anaphors from the WSJ. All anaphora to anaphoric NPs with full lexical heads modified by other or another (Examples (3) X (4)), thereby excluding idiomatic non-referential uses (e.g., on the other hand ), reciprocals such as each other , ellipsis, and one -anaphora. The excluded cases either are non-anaphoric or do not have a full lexical head and would therefore re-quire a mostly non-lexical approach to resolution. Modjeska X  X  corpus also excludes 374 other -anaphors with structurally available antecedents: In list contexts such as
Example (5), the antecedent is normally given as the left conjunct of the list:
A similar case is the construction Xs other than Ys . For a computational treatment of other -NPs with structural antecedents, see Bierner (2001).
 instances of other -anaphors with NP antecedents in a five-sentence window. In this study we use the 408 (81.6%) other -anaphors in the corpus that have NP antecedents within a two -sentence window (the current or previous sentence). which the anaphor provides the set complement. The tag lenient was used to annotate previous mentions of the same entity. In Example (6), all other bidders refers to all bidders antecedents are underlined. All other potential antecedents (e.g., offer in Example (6)), are called distractors. The antecedent can be a set of separately mentioned entities, like May and July in
Example (7). For such split antecedents (Modjeska 2003), the latest mention of each set member is annotated as correct , so that there can be more than one correct antecedent to an anaphor. 14 4.2 Antecedent Extraction and Preprocessing
For each anaphor, all previously occurring NPs in the two-sentence window were automatically extracted exploiting the WSJ parse trees. NPs containing a possessive NP modifier (e.g., Spain X  X  economy ) were split into a possessor phrase ( Spain ) and a possessed entity ( Spain X  X  economy ). 15 Modjeska (2003) identifies several syntactic positions that cannot serve as antecedents of other -anaphors. We automatically exclude only NPs preceding an appositive other -anaphor from the candidate antecedent set. In  X  X ary
Elizabeth Ariail, another social-studies teacher,  X  X heNP Mary Elizabeth Ariail cannot be the antecedent of another social-studies teacher as the two phrases are coreferential and cannot provide a set complement to each other.
 whole data set is 4,272, with an average of 10.5 antecedent candidates per anaphor. compounds was retained, as modification results in data sparseness for the corpus-based methods, and compounds are often not recorded in WordNet.
 were classified into the ENAMEX MUC-7 categories (Chinchor 1997)
ORGANIZATION and LOCATION , using the software ANNIE (GATE2; http://gate.ac. uk). We then automatically obtained more-fine-grained distinctions for the NE cate-gories LOCATION and ORGANIZATION , whenever possible. We classified small gazetteers for these subcategories were extracted from the Web. Second, if an entity marked as LOCATION by ANNIE occurred in exactly one of these gazetteers both the state and the river gazetteer), then the label was left at the We further classified an ORGANIZATION entity by using its internal makeup as follows.
We extracted all single-word hyponyms of the noun organization from WordNet and used the members of this set, OrgSet , as the target categories for the fine-grained distinctions. If an entity was classified by ANNIE as an element &lt; ORG &gt; of OrgSet as its final lemmatized word (e.g., Deutsche Bank )or as &lt; ORG &gt; (here, BANK ). In cases of ambiguity, again, no subclassification was carried out. No further distinctions were developed for the category expression matching to classify numeric and time entities into as well as DOLLAR or simply NUMBER . This subclassification of the standard cate-gories provides us with additional lexical information for antecedent selection. Thus, in Example (8), for instance, a finer-grained classification of South Carolina into provides more useful information than resolving both South Carolina and Greenville
County as LOCATION only:
Finally, all antecedent candidates and anaphors were lemmatized. The procedure of extraction and preprocessing results in the following antecedent sets and anaphors for Examples (3) and (4): A 3 = { ..., addition, cost, result, exposure, member, measure ana = repercussion and A 4 = { ..., ordinance, Moon Township [= location], home, handicapped, mile } and ana = facility .
 set. 17 NE resolution is clearly important as 205 of 468 (43.8%) of correct antecedents are NEs. 376 4.3 Evaluation Measures and Baselines
For each anaphor, each algorithm selects at most one antecedent as the correct one. If this antecedent provides the appropriate set complement to the anaphor (i.e., is marked in the gold standard as correct or lenient ), the assignment is evaluated as correct.
Otherwise, it is evaluated as wrong. We use the following evaluation measures: Precision is the number of correct assignments divided by the number of assignments, recall is the number of correct assignments divided by the number of anaphors, and F-measure is based on equal weighting of precision and recall. In addition, we also give the coverage of each algorithm as the number of assignments divided by the number of anaphors.
This last measure is included to indicate how often the algorithm has any knowledge to go on, whether correct or false. For algorithms in which the coverage is 100%, precision, recall, and F -measure all coincide.
 baseline ( baselineREC ), always selects the antecedent candidate closest to the anaphor.
The second ( baselineSTR ) takes into account that the lemmatized head of an other -anaphor is sometimes the same as that of its antecedent, as in the pilot X  X  claim . . . other bankruptcy claims . For each anaphor, baselineSTR string-compares its last (lemmatized) word with the last (lemmatized) word of each of its potential antecedents. If the strings match, the corresponding antecedent is chosen as the correct one. If several antecedents produce a match, the baseline chooses the most recent one among them. If no antecedent produces a match, no antecedent is assigned.
 only the original antecedents for string matching, disregarding named-entity res-olution. If string-comparison returns no match, a back-off version ( baselineSTR chooses the antecedent closest to the anaphor among all antecedent candidates, thereby yielding a 100% coverage. The second variation ( baselineSTR for named entities for string matching; again a back-off version ( baselineSTR a recency back-off. This baseline performs slightly better, as now cases such as that in Example (8) ( South Carolina . . . another state , in which South Carolina is resolved to can also be resolved. The results of all baselines are summarized in Table 2. Results of the 100% coverage backoff algorithms are indicated by Precision of anaphors covered by the string-matching baselines baselineSTR assigned by the recency back-off in baselineSTR  X  v 1 and baselineSTR from the antecedent sets, since they are lexically not very informative and are also not encoded in WordNet. This removes 49 (10.5%) of the 468 correct antecedents (see Table 1); however, we can still resolve some of the anaphors with pronoun antecedents if they also have a lenient non-pronominal antecedent, as in Example (6).
After pronoun deletion, the total number of antecedents in our data set is 3,875 for 408 anaphors, of which 419 are correct antecedents, 160 are lenient, and 3,296 are distractors. 4.4 Wordnet as a Knowledge Source for Other -Anaphora Resolution 4.4.1 Descriptive Statistics. As most antecedents are hyponyms or synonyms of their anaphors in other -anaphora, for each anaphor ana , we look up which elements of its antecedent set A anaid are hyponyms/synonyms of ana in WordNet, considering all senses of anaphor and candidate antecedent. In Example (4), for example, we look up whether ordinance , Moon Township , home , handicapped ,and mile are hyponyms or synonyms of facility in WordNet. Similarly, in Example (9), we look up whether Will
Quinlan [= PERSON ], gene ,and risk are hyponyms/synonyms of child .
As proper nouns (e.g., Will Quinlan ) are often not included in WordNet, we also look up whether the NE category of an NE antecedent is a hyponym/synonym of the anaphor (e.g., whether person is a synonym/hyponym of child) and vice versa (e.g., whether child is a synonym/hyponym of person). This last inverted look-up is necessary, as the NE category of the antecedent is often too general to preserve the normal hyponymy relationship to the anaphor. Indeed, in Example (9), it is the inverted look-up that captures the correct hyponymy relation between person and child. If the single look-up for common nouns or any of the three look-ups for proper nouns is successful, we say that a hyp/syn relation between candidate antecedent and anaphor holds in WordNet. Note that each noun in WordNet stands in a hyp/syn relation to itself. Table 3 summarizes how many correct/ lenient antecedents and distractors stand in a hyp/syn relation to their anaphor in WordNet.
 nificantly more often than distractors do ( p &lt; 0 . 001, t -test). The use of WordNet hyponymy/synonymy relations to distinguish between correct/lenient antecedents and distractors is therefore plausible. However, Table 3 also shows two limitations 378 of relying on WordNet in resolution algorithms. First, 57% of correct and lenient antecedents are not linked via a hyp/syn relation to their anaphor in WordNet.
This will affect coverage and recall (see also Section 2.2.1). Examples from our data coffee:export ,and pilot(s):union , including both missing universal hyponymy links and context-stipulated ones. Second, the raw frequency (296) of distractors that stand in a hyp/syn relation to their anaphor is higher than the combined raw frequency for correct/lenient antecedents (248) that do so, which can affect precision. This is due to both sense proliferation (Section 2.2.4) and anaphors that require more than just lexical knowledge about antecedent and anaphor heads to select a correct antecedent over a distractor. In Example (10), the distractor product stands in a hyp/syn relation-ship to the anaphor commodity and X  X isregarding other factors X  X s a good antecedent candidate. 20 4.4.2 The WordNet-Based Algorithm. The WordNet-based algorithm resolves each anaphor ana to a hyponym or synonym in A anaid , if possible. If several antecedent candidates are hyponyms or synonyms of ana , it uses a tiebreaker based on string match and recency. When no candidate antecedent is a hyponym or synonym of ana , string match and recency can be used as a possible back-off. son for tiebreaker and back-off can again use the original or the replaced anteced-ents, yielding two versions, algoWN v 1 (original antecedents) and algoWN antecedents).

The back-off algorithm algoWN  X  v 1 uses baselineSTR  X  v 1 can be assigned:
Both algoWN v 1 and algoWN v 2 achieved the same results, namely, a coverage of 65.2%, precision of 56.8%, and recall of 37.0%, yielding an F -measure of 44.8%.
The low coverage and recall confirm our predictions in Section 4.4.1. Using backoff algoWN  X  v 1 / algoWN  X  v 2 achieves a coverage of 100% and a precision/recall/ F -measure of 44.4%. 4.5 Corpora as Knowledge Sources for Other -Anaphora Resolution
In Section 3 we suggested the use of shallow lexico-semantic patterns for obtaining anaphor X  X ntecedent relations from corpora. In our first experiment we use the Web, which with its approximately 8,058M pages 23 is the largest corpus available to the
NLP community. In our second experiment we use the same technique on the BNC, a smaller (100 million words) but virtually noise-free and balanced corpus of contem-porary English. 4.5.1 Pattern Selection and Instantiation. The list-context XsandotherYs explicitly expresses a hyponymy/synonymy relationship with X being hyponyms/synonyms of Y (see also Example (5) and [Hearst 1992]). This is only one of the possible structures that express hyponymy/synonymy. Others involve such , including ,and especially (Hearst 1992) or appositions and coordination. We derive our patterns from the list-context because it corresponds relatively unambigously to hyponymy/synonymy relations (in contrast to coordination, which often links sister concepts instead of a hyponym and its hyperonym, as in tigers and lions , or even completely unrelated concepts). In addition, it is quite frequent (for example, and other occurs more frequently on the Web than such as and other than ). Future work has to explore which patterns have the highest precision and/or recall and how different patterns can be combined effectively without increasing the risk of false positives (see also
Section 2.2.3). 380
Web. For the Web algorithm ( algoWeb ), we use the following pattern: Given an anaphor ana and a common-noun antecedent candidate a in instantiate (W1) by substituting a for N 1 and ana for N
Example (4) is (homeORhomes)andotherfacilities ( WI c 1 instantiation is parallel to the WordNet hyp/syn relation look-up for common nouns. antecedent for N 1 ,and ana for N 2 . An instantiated pattern for Example (9) is
ORpersons)andotherchildren ( WI p 1 in Table 4). In this instantiation, N a hyponym of N 2 ( child ); instead N 2 is a hyponym of N queries in Section 4.4.1). Therefore, we also instantiate (W1) by substituting ana for N and the NE type of the antecedent for N 2 ( WI p 2 in Table 4). Finally, for NE antecedents, we use an additional pattern: which we instantiate by substituting the original NE antecedent for N ana for N 2 ( WI p 3 in Table 4). The three instantiations for NEs are parallel to the three hyp/syn relation look-ups in the WordNet experiment in Section 4.4.1. We submit these instantiations as queries to the Google search engine, making use of the Google API technology.
 BNC. For BNC patterns and instantiations, we exploit the BNC X  X  part-of-speech tagging.
On the one hand, we restrict the instantiation of N 1 and N and on the other hand, we allow occurrence of modification to improve coverage. We therefore extend (W1) and (W2) to the patterns (B1) and (B2). (B1), for example, also matches  X  X omes and the other four facilities. X  Otherwise the instantiations are produced parallel to the Web (see Table 4). We search the instantiations in the BNC using the IMS Corpus Query Workbench (Christ 1995). For both algoWeb and algoBNC , each antecedent candidate a in score. The procedure, using the notation for the Web, is as follows. We obtain the raw frequencies of all instantiations in which a occurs ( WI c and WI p 3 for proper names) from the Web, yielding freq ( WI and freq ( WI p 3 ). The maximum WM a over these frequencies is the score associated with each antecedent (given an anaphor ana ), which we will also simply refer to as the antecedent X  X  Web score. For the BNC, we call the corresponding maximum score BM and refer to it as the antecedent X  X  BNC score. This simple maximum score is biased toward antecedent candidates whose head nouns occur more frequently overall. In a previous experiment we used mutual information to normalize Web scores (Markert,
Nissim, and Modjeska 2003). However, the results achieved with normalized and non-normalized scores showed no significant difference. Other normalization methods might yield significant improvements over simple maximum scoring and can be explored in future work. 4.5.2 Descriptive Statistics. Table 5 gives descriptive statistics for the Web and BNC score distributions for correct/lenient antecedents and distractors, including the mini-mum and maximum score, mean score and standard deviation, median, and number of zero scores, scores of one, and scores greater than one.
 icantly higher scores for correct/lenient antecedents (mean: 2,416.68/807.63; median: 68/68.5) than for distractors (mean: 290.97; median: 1). Moreover, the method produces significantly fewer zero scores for correct/lenient antecedents (19.6%/22.5%) than for distractors (42.3%). 27 Therefore the pattern-based Web method is a good candidate for distinguishing correct/lenient antecedents and distractors in anaphora resolution. In addition, the median for correct/lenient antecedents is relatively high (68/68.5), which ensures a relatively large amount of data upon which to base decisions. Only 19.6% of correct antecedents have scores of zero, which indicates that the method might have high coverage (compared to the missing 57% of hyp/syn relations for correct antecedents in WordNet; Section 4.4).
 are significantly higher than that of the distractors, this is due to a few outliers; more interestingly, the median for the BNC score distributions is zero for all antecedent groups. This will affect precision for a BNC-based algorithm because of the small amount of data decisions are based on. In addition, although the number of zero scores 382 for correct/lenient antecedents (85.9%/86.9%) is significantly lower than for distractors (97.5%), the number of zero scores is well above 80% for all antecedent groups. Thus, the coverage and recall of a BNC-based algorithm will be very low. Although the
BNC scores are in general much lower than Web scores and although the Web scores distinguish better between correct/lenient antecedents and distractors, we observe that
Web and BNC scores still correlate significantly, with correlation coefficients between 0.20 and 0.35, depending on antecedent group. 28 corpora, but it is expected to depend on large corpora to be really successful. 4.5.3 The Corpus-Based Algorithms. The prototype Web-based algorithm resolves each anaphor ana to the antecedent candidate in A anaid with the highest Web score above zero.
If several potential antecedents achieve the same Web score, it uses a tiebreaker based on string match and recency. If no antecedent candidate achieves a Web score above zero, string match and recency can be used as a back-off. String comparison for tiebreaker and back-off can again use the original or the replaced antecedents, yielding two versions, algoWeb v 1 (original antecedents) and algoWeb v 2 (replaced antecedents).

The back-off algorithm algoWeb  X  v 1 uses baselineSTR  X  v 1 can be assigned (parallel to the back-off in algoWN  X  v 1 algoWeb v 1 and algoWeb v 2 can overrule string matching for anaphors in StrSet
This happens when the Web score of an antecedent candidate that does not match the anaphor is higher than the Web scores of matching antecedent candidates. In particular, there is no guarantee that matching antecedent candidates are included in A WM anaid . In that respect, algoWeb v 1 and algoWeb
WordNet algorithms: Matching antecedent candidates are always synonyms of the anaphor (as each noun is a synonym of itself) and therefore always included in
Therefore the WordNet algorithms can be seen as a direct extension of baselineSTR ; that is, they achieve the same results as the string-matching baseline on the sets that the Web algorithms overrule string matching. Instead we can use string matching prior to Web scoring, use the Web scores only when there are no matching antecedent candidates, and use recency as the final back-off. This variation then achieves the same results on the sets StrSet v 1 / StrSet v 2 as the WordNet algorithms and the string-matching baselines. In combination with the possibility of using original or replaced antecedents for string matching this yields four algorithm variations overall (see Table 6). The results (see Table 7) do not show any significant differences according to the variation explored.
 rithms, using the BNC scores instead of Web scores. The results (see Table 8) are disappointing because of data sparseness (see above). No variation yields considerable improvement over baselineSTR v 2 in the final precision  X  384 tions just apply a string-matching baseline, either as a back-off or prior to checking BNC scores, depending on the variation used. 4.6 Discussion and Error Analysis
The performances of the best versions of all algorithms for other -anaphora are summa-rized in Table 9. 4.6.1 Algorithm Comparison. Algorithms are compared on their final precision two tests throughout this article. We used a t -test to measure the difference between two algorithms in the proportion of correctly resolved anaphors. However, there are many examples which are easy (for example, string-matching examples) and that therefore most or all algorithms will resolve correctly, as well as many that are too hard for all algorithms. Therefore, we also compare two algorithms using McNemar X  X  test, which only relies on the part of the data set in which the algorithms do not give the same answer. 30 If not otherwise stated, all significance claims hold at the 5% level for both the t -test and McNemar X  X  test.
 showing that the  X  X ame predicate match X  is quite accurate even though not very frequent (coverage is only 30.9%). The WordNet-based and Web-based algorithms achieve a final precision that is significantly better than the baselines X  as well as algoBNC  X  X . Most interestingly, the Web-based algorithms significantly outperform the
WordNet-based algorithms, confirming our predictions based on the descriptive statis-tics. The Web approach, for example, resolves Examples (3), (4), (6), and (11) (which
WordNet could not resolve) in addition to Examples (8) and (9), which both the Web and WordNet algorithms could resolve. in Section 2.2. In particular, Problem 1 proved to be quite severe, as algoWN achieved a coverage of only 65.2%. Missing links in WordNet also affect precision if a good distractor has a link to the anaphor in WordNet, whereas the correct antecedent does not (Example (10)). Missing links are both universal relations that should be included in an ontology (such as home:facility ) and context-dependent links (e.g., age:(risk) factor , costs:repercussions ; see Problem 2 in Section 2.2.2). Further mining of WordNet beyond following hyponymy/synonymy links might alleviate Problem 1 but is more costly and might lead to false positives (Problem 3). To a lesser degree, the WordNet algo-rithms also suffer from sense proliferation (Problem 4), as all senses of both anaphor and antecedent candidates were considered. Therefore, some hyp/syn relations based on a sense not intended in the text were found, leading to wrong-antecedent selection and lowering precision. In Example (11), for instance, there is no hyponymy link be-tween the head noun of the correct antecedent ( question ) and the head noun of the anaphor ( issue ), whereas there is a hyponymy link between issue and person = [Mr.
Dallara] (using the sense of issue as offspring ) as well as a synonymy link between number and issue . While in this case considering the most frequent sense of the anaphor issue as indicated in WordNet would help, this would backfire in other cases in our data set in which issue is mostly used in the minority sense of stock, share . Obviously, prior word sense disambiguation would be the most principled but also a more costly solution. (11) While Mr. Dallara and Japanese officials say the question of investors
The Web-based method does not suffer as much from these problems. The linguistically motivated patterns we use reduce long-distance dependencies between anaphor and antecedent to local dependencies. By looking up these patterns on the Web we make use of a large amount of data that is very likely to encode strong semantic links via these local dependencies and to do so frequently. This holds both for universal hyponymy relations (addressing Problem 1) and relations that are not necessarily to be included in an ontology (addressing Problem 2). The problem of whether to include subjective and context-dependent relations in an ontology (Problem 2) is circumvented by using Web scores only in comparison to Web scores of other antecedent candidates.
In addition, the Web-based algorithm needs no hand-processing or hand-modeling whatsoever, thereby avoiding the manual effort of building ontologies. Moreover, the local dependencies we use reduce the need for prior word sense disambiguation (Prob-lem 4), as the anaphor and the antecedent constrain each other X  X  sense within the 386 context of the pattern. Furthermore, the Web scores are based on frequency, which biases the Web-based algorithms toward frequent senses as well as sense pairs that occur together frequently. Thus, the Web algorithm has no problem resolving issue to question in Example (11) because of the high frequency of the query question OR questions and other issues . Problem 3 is still not addressed, however, as any corpus can encode the same semantic relations via different patterns. Combining patterns might therefore yield problems similar to those presented by combining information sources in an ontology.

Unlike the Web-based algorithms, the BNC-based ones make use of POS tagging and observe sentence boundaries, thus reducing the noise intrinsic to an unprocessed corpus like the Web. Moreover, the instantiations used in algoBNC allow for modifi-cation to occur (see Table 4), thus increasing chances of a match. Nevertheless, the
BNC-based algorithms performed much worse than the Web-based ones: Only 4.2% of all pattern instantiations were found in the BNC, yielding very low coverage and recall (see Table 5). 4.6.2 Error Analysis. Although the Web algorithms perform best, algoWEB 194 errors (47.6% of 408). Because in several cases there is more than one reason for a wrong assignment, we use the decision tree in Figure 1 for error classification. By using this decision tree, we can, for example, exclude from further analysis those cases that none of the algorithms could resolve because of their intrinsic design.
 pronouns as well as not dealing with split antecedents (44 cases, or 22.7% of all mis-of the several correct antecedents has indeed been chosen by our algorithm, but all the correct antecedents need to be found to allow for the resolution to be counted as correct.
 algorithm selects a distractor instead of the correct antecedent because the NER module either leaves the correct antecedent unresolved (which could then lead to very few or zero hits in Google) or resolves the named entity to the wrong NE category. String matching is a minor cause of errors (under 10%). This is because, apart from its being generally reliable, there is also a possible string match only in just about 30% of the cases (see Table 2).
 dependent and very unconventional relations, such as the description of dolls as winners in Example (12). (12) Coleco bounced back with the introduction of the Cabbage Patch
In such cases, the relation between the anaphor and antecedent head nouns is not frequent enough to be found in a corpus even as large as the Web. rored in the high percentage of zero-score errors (24.7% of all mistakes). Although the
Web algorithm suffers from a knowledge gap to a smaller degree than WordNet, relation.
 scores than the correct antecedent. A common reason is that the wished-for relation is attested but rare and therefore other candidates yield higher scores. This is simi-reduce data sparseness, can sometimes lead to the elimination of information that could help disambiguate among several candidate antecedents. Lastly, lexical informa-tion, albeit crucial and probably more important than syntactic information (Modjeska features, such as grammatical function, NP form, and discourse structure, could prob-ably help when very good distractors cannot be ruled out by purely lexical methods (Example (10)). The integration of the Web feature in a machine-learning algorithm using several other features has yielded good results (Modjeska, Markert, and Nissim 2003). 388 5. Case Study II: Definite NP Coreference
The Web-based method we have described outperforms WordNet as a knowledge source for antecedent selection in other -anaphora resolution. However, it is not clear how far the method and the achieved comparative results generalize to other kinds of full NP anaphora. In particular, we are interested in the following questions:
As a contribution, we investigate the performance of the knowledge sources discussed for other -anaphora in the resolution of coreferential NPs with full lexical heads, concentrating on definite NPs (see Example (1)). The automatic resolution of such anaphors has been the subject of quite significant interest in the past years, but results are much less satisfactory than those obtained for the resolution of pronouns (see Section 2).
 antecedents is again, in general, one of hyponymy or synonymy, making an extension of our approach feasible. However, other-anaphors are especially apt at conveying context-specific or subjective information by forcing the reader via the other-expression to accommodate specific viewpoints. This might not hold for definite NPs. 5.1 Corpus Collection
We extracted definite NP anaphors and their candidate antecedents from the MUC-6 of 60 documents. The documents were automatically preprocessed in the following way: All meta-information about each document indicated in XML (such as WSJ cat-egory and date) was discarded, and the headline was included and counted as one sentence. Whenever headlines contained three dashes, everything after the dashes was discarded.
 tation concentrating on anaphoric definite NPs. All definite NPs which are in, but not at the beginning of, a coreference chain are potential anaphors. We excluded definite
NPs with proper noun heads (such as the United States ) from this set, since these do not depend on an antecedent for interpretation and are therefore not truly anaphoric.
We also excluded appositives, which provide coreference structurally and are therefore not anaphoric. Otherwise, we strictly followed the MUC annotation for coreference in our extraction, although it is not entirely consistent and not necessarily comprehensive (van Deemter and Kibble 2000). This extraction method yielded a set of 565 anaphoric definite NPs.
 closest to the anaphor as the correct antecedent, whereas all other previous mentions in C are regarded as lenient . NPs that occur before the anaphor but are not marked as being in the same coreference chain are distractors . Since anaphors with split antecedents are not annotated in MUC, anaphors cannot have more than one correct antecedent. In
Example (13), the NPs with the head nouns Pact , contract ,and settlement are marked as coreferent in MUC: In our annotation, the settlement is an anaphor with a correct antecedent headed by contract and a lenient antecedent Pact . Other NPs prior to the anaphor (e.g., Canada or the IWA-Canada union ) are distractors. (13) Forest Products Firms Tentatively Agree On Pact in Canada. A group of
With respect to other -anaphora, we expanded our window size from two to five sen-tences (the current and the four previous sentences) and excluded all anaphors with no correct or lenient antecedent within this window size, thus yielding a final set of 477 anaphors (84.4% of 565). This larger window size is motivated by the fact that a window size of two would cover only 62.3% of all anaphors (352 out 565). 5.2 Antecedent Extraction, Preprocessing, and Baselines
All NPs prior to the anaphor within the five-sentence window were extracted as antecedent candidates. 36 We further processed anaphors and antecedents as in Case Study I (see Section 4.2): Modification was stripped and all NPs were lemmatized.
In this experiment, named entities were resolved using Curran and Clark X  X  (2003) NE tagger rather than GATE. 37 The identified named entities were further subclassified into finer-grained entities, as described for Case Study I.
 14,233, with an average of 29.84 antecedent candidates per anaphor. This figure is much higher than the average number of antecedent candidates for other -anaphors (10.5) because of the larger window size used. The data set includes 473 correct antecedents, 803 lenient antecedents, and 12,957 distractors. Table 11 shows the distribution of NP types for correct and lenient antecedents and for distractors.
 annotation also includes anaphors whose antecedent is not an NP but, for exam-ple, a nominal modifier in a compound. Thus, in Example (14), the bankruptcy code is annotated in MUC as coreferential to bankruptcy-law , a modifier in bankruptcy-law protection . 390
In our scheme we extract the bankruptcy code as anaphoric but our method of extract-ing candidate antecedents does not include bankruptcy-law . Therefore, there are four anaphors in our data set with no correct/lenient antecedent extracted. These cannot be resolved by any of the suggested approaches.
 significance tests for precision  X  . We also use the same baseline variations baselineREC , baselineSTR v 1 ,and baselineSTR v 2 (see Table 12 and cf. Table 2). The recency baseline per-forms worse than for other -anaphora. String matching improves dramatically on simple recency. It also seems to be more relevant than for our other -anaphora data set, achieving higher coverage, precision, and recall. This confirms the high value of string matching that has been assigned to coreference resolution by previous researchers (Soon, Ng, and Lim 2001; Strube, Rapp, and Mueller 2002, among others).
 agrees in number with its antecedent. Therefore, we also explored variations of all algorithms that as a first step delete from A anaid all candidate antecedents that do not agree in number with ana . 38 The algorithms then proceed as usual. Algorithms that use number checking are marked with an additional n in the subscript. Using number checking leads to small but consistent gains for all baselines.
 ods, thereby removing 70 of 473 (14.8%) of correct antecedents (see Table 11). After pronoun deletion, the total number of antecedents in our data set is 12,940 for 477 anaphors, of which 403 are correct antecedents, 658 are lenient antecedents, and 11,879 are distractors. 5.3 WordNet for Antecedent Selection in Definite NP Coreference
We hypothesize that again most antecedents are hyponyms or synonyms of their anaphors in definite NP coreference (see Examples (1) and (13)). Therefore we use the same look-up for hyp/syn relations that was used for other-anaphora (see Section 4.4), including the specifications for common noun and proper name look-ups. Parallel to
Table 3, Table 13 summarizes how many correct and lenient antecedents and distractors stand in a hyp/syn relation to their anaphor in WordNet.
 hyp/syn relation to their anaphor significantly more often than distractors do ( t -test, p &lt; 0 . 001). Hyp/syn relations in WordNet might be better at capturing the relation between antecedent and anaphors for definite NP coreference than for other -anaphora:
A higher percentage of correct and lenient antecedents of definite NP coreference (71.96%/67.78%) stand in a hyp/syn relation to their anaphors than is the case for other -anaphora (43.0%/42.5%). At the same time, though, there is no difference in the percentage of distractors that stand in a hyp/syn relation to their anaphors (9% for other -anaphora, 8.80% for definite NP coreference). For our WordNet algorithms, this is likely to translate directly into higher coverage and recall and potentially into higher precision than in Case Study I. Still, about 30% of correct antecedents are not in a hyp/syn relation to their anaphor in the current case study, confirming results by Harabagiu,
Bunescu, and Maiorano (2001), who also look at MUC-style corpora. is alleviated by a quite high number of lenient antecedents, whose resolution can make up for a missing link between anaphor and correct antecedent. additional two algorithms that include number checking. Results are summarized in Table 14.
 corresponding versions of the string-matching baseline (i.e., algoWN 392 additional lexical knowledge to string matching. As expected from the descriptive statistics discussed above, the results are better than those obtained by the WordNet algorithms for other -anaphora, even if we disregard the additional morphosyntactic number constraint. 5.4 The Corpus-Based Approach for Definite NP Coreference
Following the assumption that most antecedents are hyponyms or synonyms of their anaphors in definite NP coreference, we use the same list-context pattern and instantiations that were used for other-anaphora, allowing us to evaluate whether they are transferrable. The corpora we use are again the Web and the BNC.
 rect/lenient antecedents and distractors, with significantly higher means/medians for correct/lenient antecedents (median 472/617 vs. 2 for distractors), as well as signifi-cantly fewer zero scores (8% for correct/lenient vs. 41% for distractors). This indicates transferability of the Web-based approach to coreference. Compared to other -anaphora, the number of zero-scores is lower for correct/lenient antecedent types, so that we expect better overall results, similar to our expectations for the WordNet algorithm. distractors, since the number of zero scores for correct/lenient antecedents (68.98%/ 58.05%) is significantly lower than for distractors (96.97%). Although more than 50% of correct/lenient antecedents receive a zero score, there are fewer zero scores than for other -anaphora (for which more than 80% of correct/lenient antecedents re-ceived zero scores). However, BNC scores are again in general much lower than Web scores, as measured by means, medians, and zero scores. Nevertheless, Web scores and BNC scores correlate significantly, with the correlations reaching higher coeffi-cients (0.53 to 0.65, depending on antecedent group) than they did in the case study for other -anaphora.
 described for other -anaphora and are marked by the same subscripts. The variations that include number checking are again marked by a subscript n . Tables 15 and 16 report the results for all the Web and BNC algorithms, respectively. 5.5 Discussion and Error Analysis 5.5.1 Algorithm Comparison. Using the original or the replaced antecedent for string in interesting differences in any of the approaches discussed. Also, number matching provides consistent improvements. Therefore, from this point on, our discussion will disregard those variations, that use original antecedents only ( v 1, v 1 n , v 3, and v 3 n )as well as algorithms that do not use number matching ( v 2, v 4). We will also concentrate on the final precision  X  of the full-coverage algorithms. The set of anaphors that are cov-ered by the best string-matching baseline, prior to recency back-off , will again be denoted by StrSet v 2 n . Again, both a t -test and McNemar X  X  test will be used, when statements about significance are made.
 higher number of string-matching antecedent/anaphor pairs in coreference, the higher precision of string matching, and to a lesser degree, the lower number of unusual redescriptions.
 corresponding baselines. The first striking result is that the Web algorithm variation algoWeb v 2 n , which relies only on the highest Web scores and is therefore allowed to overrule string matching, does not outperform the corresponding string-matching baseline baselineSTR v 2 n and performs significantly worse than the corresponding
WordNet algorithm algoWN v 2 n . This contrasts with the results for other-anaphora. When the results were examined in detail, it emerged that for a considerable number of anaphors in StrSet v 2 n , the highest Web score was indeed achieved by a distractor with a high-frequency head noun when the correct or lenient antecedent could be instead found by a simple string match to the anaphor. This problem is much more severe than 394 for other -anaphora because of (1) the larger window size that includes more distractors and (2) the higher a priori precision of the string-matching baseline, which means that overruling string matching leads to wrong results more frequently. Typical examples involve named-entity recognition and inverted queries. Thus, in Example (15), the anaphor the union is coreferent with the first occurrence of the union , a case easily resolved by string matching. However, the distractor organization [= Chrysler Canada] achieves a higher Web score, because of the score of the inverted query union OR unions and other organizations . 42
Several potential solutions exist to this problem, such as normalization of Web scores or penalizing of inverted queries. The solution we have adopted in algoWeb use Web scores only after string matching, thereby making the Web-based approach more comparable to the WordNet approach. Therefore, baselineSTR algoWN v 2 n (as well as algoBNC v 4 n ) all coincide in their decisions for anaphors in StrSet and only differ in the decisions made for anaphors that do not have a matching antecedent candidate. Indeed, algoWeb v 4 n performs significantly better than the base-lines at the 1% level, and results rise from a precision  X  for algoWeb v 4 n . It also significantly outperforms the best BNC results, thus showing that overcoming data sparseness is more important than working with a controlled, tagged, and representative corpus. Furthermore, shows better performance than WordNet in the final algorithm variation (71.3% vs. 66.2%). 43 According to results of a t -test, however, this last difference is not significant. McNemar X  X  test, concentrating on the part of the data in which the methods differ, shows instead significance at the 1% level. such a large number of anaphors are covered by simple string matching, leaving only a small data set on which the lexical methods can differ. Thus, StrSet 477 cases (268 of which are assigned correctly by baselineStr the other methods are confined to the set of the remaining 146 anaphors. Of these 146, baselineStr  X  v 2 n assigns the correct antecedent to 13 (8.9%) anaphors by using a recency back-off, the best WordNet method to 55 anaphors (37.67%), and the best Web method to 72 anaphors (49.31%). Therefore the Web-based method is a better complement to string matching than WordNet, which is reflected in the results of McNemar X  X  test.
Anaphor X  X ntecedent relations that were not covered in WordNet but that did not prove a problem for the Web algorithm were again both general hyponymy relations, such as cuts:concessions and legislation:attack . 5.5.2 Error Analysis. The best-performing Web-based algorithm, algoWeb the wrong antecedent for a given anaphor in 137 of 477 cases (28.7%). Again, we use the decision tree in Figure 1 to classify errors. Design errors now do not include split antecedents but do include errors that occur because the condition of number agreement was violated, pronoun deletion errors, and the four cases in which the antecedent is a non-NP antecedent and therefore not extracted in the first place (see Section 5.1 and Example (14)). Table 17 reports the frequency of each error type.
 under 15% of the mistakes. Also rare are zero-score errors (only 8%). When compared to the number of zero-score errors in other anaphora (24.7%), this low figure suggests that other -anaphora is more prone to exploit rare, unusual, and context-dependent redescriptions than full NP coreference. Nevertheless, it is yet possible to find non-standard redescriptions in coreference as well which yield zero scores, such as the use of transaction to refer to move in Example (16). (16) Conseco Inc., in a move to generate about $200 million in tax deductions,
Much more substantial is the weight of errors due to string matching, tiebreaker deci-sions, and the presence of good distractors (the main reason for errors of type other ), which together account for over three-quarters of all mistakes.
 70% of the cases with a precision of 80.9%). However, because algoWeb 396 rules string matching, the errors of baselineSTR v 2 n are preserved here and account for 24.1% of all mistakes. 44 Tiebreaker errors are quite frequent too (24.8%), as our far-from-sophisticated tiebreaker was needed in nearly half of the cases (224 times; 47.0%). higher than the correct/lenient antecedent. In Example (17), for instance, a distractor with a higher Web score ( comment ) prevents the algorithm from selecting the correct antecedent ( investigation ) for the anaphor the inquiry .
Example (18) shows how stripping modification might have eliminated information crucial to identifying the correct antecedent: Only the head process was retained of the anaphor arbitration process , so that the surface link between anaphor and antecedent ( arbitration ) was lost and the distractor securities industry , reduced to industry ,was instead selected. 6. Open Issues 6.1 Preprocessing and Prior Assumptions
Our algorithms build on two main preprocessing assumptions. First, we assume perfect base-NP chunking and expect results to be lower with automatic chunking. Neverthe-less, since automatic chunking will affect all algorithms in the same way, we do expect comparative results to stand. We are not, however, dependent on full parsing, as no parsing-dependent grammatical features are used by the algorithms.
 manually determined, as we restrict our study to antecedent selection for the NPs that are marked in the MUC corpus as coreferent. One of the reasons why pronoun res-olution has been more successful than definite NP resolution is that whereas pronouns are mostly anaphoric, definite NPs do not have to be so (see Section 2). In fact, it has been argued by several researchers that an anaphora resolution algorithm should proceed to antecedent selection only if a given definite NP is anaphoric (Ng and Cardie 2002a; Ng 2004; Uryupina 2003; Vieira and Poesio 2000, among others), therefore advocating a two-stage process which we also follow in this article. Although recent work on automatic anaphoricity determination has shown promising results (Ng 2004; Uryupina 2003), our algorithms will perform worse when building on non-manually determined anaphors.
Future work will explore the extent of such a decrease in performance. 6.2 Directions for Improvement
All algorithms we have described can be considered blueprints for more complex versions. Specifically, the WordNet-based algorithms could be improved by exploiting information encoded in WordNet beyond explicitly encoded links (glosses could be mined, too, for example; see also Harabagiu, Bunescu, and Maiorano [2001]). The Web-based algorithms could similarly benefit from the exploration of different patterns and their combination, as well as from using non-pattern-based approaches for hyponymy detection (Shinzato and Torisawa 2004). In addition, we have evaluated the contribution of lexical resources in isolation rather than within a more sophisticated system that integrates additional non-lexical features. It is unclear whether integrating such knowl-edge sources in a full-resolution system might even out the differences between the
Web-based and the WordNet-based algorithms or exacerbate them. Modjeska, Markert, and Nissim (2003) included a feature based on Web scores in a naive Bayes model for other -anaphora resolution that also used grammatical features and showed that the addition of the Web feature yielded an 11.4-percentage-point improvement over using a WordNet-based feature. This gives some indication that additional grammatical features might not be able to compensate fully for the knowledge gap encountered in
WordNet. 6.3 Extension to Yet Other Anaphora Types
Using the Web for antecedent selection in anaphora resolution is novel and needs further study for other types of full NP anaphora than the ones studied in this article.
If an anaphora type exploits hyponymy/synonymy relationships between anaphor and antecedent head nouns, it can in principle be treated with the exact same pattern we used in this article. This holds, for example, for demonstratives and such -anaphors. The latter, in particular, are similar to other -anaphora in that they establish a comparison between the entity they invoke and that invoked by the antecedent and are also easily used to accommodate subjective viewpoints. They should therefore benefit especially from not relying wholly on standard taxonomic links.
 hyponymy relations. For example, bridging exploits meronymy and/or causal rela-tions (among others). Therefore, patterns that express  X  X art-of X  links, for example, such as XofY and genitives, would be appropriate. Indeed, these patterns have been recently used in Web search for antecedent selection for bridging anaphora by Poesio et al. (2004). They compare accuracy in antecedent selection for a method that inte-grates Web hits and focusing techniques with a method that uses WordNet and fo-cusing, achieving comparable results for both methods. This strenghtens our hypothesis that antecedent selection for full NP anaphora without hand-modeled lexical knowl-edge has become feasible. 7. Conclusions
We have explored two different ways of exploiting lexical knowledge for antecedent selection in other -anaphora and definite NP coreference. Specifically, we have compared a hand-crafted and -structured source of information such as WordNet and a simple and inexpensive pattern-based method operating on corpora. As corpora we have used the BNC and also suggested the Web as the biggest corpus available. 398 lexical links often exploited in coreference are not included in WordNet. We have also shown the presence of an even more severe knowledge gap for other -anaphora (see also
Question 1 in Section 1). Largely because of this knowledge gap, the novel Web-based method that we proposed proved better than WordNet at resolving other -anaphora.
Although the gains for coreference are not as high, the Web-based method improves more substantially on string-matching techniques for coreference than WordNet does (see the success rate beyond StrSet v 2 n for coreference, Section 5.5). In both studies, the
Web-based method clearly outperformed the BNC-based one. This shows that, for our tasks, overcoming data sparseness was more important than working with a manually controlled, virtually noise-free, but relatively small corpus, which addresses Question 2 in Section 1: Corpus-induced knowledge can indeed rival and even outperform the knowledge obtained via lexical hierarchies, as long as the corpus is large enough.
Corpus-based methods can therefore be a very useful complement to resolution al-gorithms for languages for which hand-crafted taxonomies have not yet been created but for which large corpora do exist. In answer to Question 3 in Section 1, our results suggest that different anaphoric phenomena suffer in varying degrees from missing knowledge and that the Web-based method performs best when used to deal with phenomena that standard taxonomy links do not capture that easily or that frequently exploit subjective and context-dependent knowledge.
 of the intrinsic limitations of ontologies, specifically, the problem of what knowledge should be included (see Section 2.2). It is also inexpensive and does not need any postprocessing of the Web pages returned or any hand-modeling of lexical knowledge. without hand-crafted lexical knowledge is feasible. This might also be the case for yet other full NP anaphora types with similar properties X  X n issue that we will explore in future work.
 Acknowledgments References 400
