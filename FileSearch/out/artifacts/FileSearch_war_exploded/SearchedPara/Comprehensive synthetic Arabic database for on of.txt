 ORIGINAL PAPER Raid M. Saabni  X  Jihad A. El-Sana Abstract Developing and maintaining large comprehen-sive databases for script recognition that include different shapes for each word in the lexicon is expensive and diffi-cult. In this paper, we present an efficient system that auto-matically generates prototypes for each word in a lexicon using multiple appearances of each letter. Large sets of dif-ferent shapes are created for each letter in each position. These sets are then used to generate valid shapes for each word-part. The number of valid permutations for each word is large and prohibits practical training and searching for var-ious tasks, such as script recognition and word spotting. We apply dimensionality reduction and clustering techniques to maintain compact representation of these databases, without affecting their ability to represent the wide variety of hand-writingstyles.Inaddition,adatabaseforoff-linescriptrecog-nition is generated from the on-line strokes using a standard dilation technique, while making special efforts to resemble pen X  X  path. We also examined and used several layout tech-niques for producing words from the generated word-parts. Our experimental results show that the proposed system can automatically generate large databases, whose quality is at least as good as the manually generated ones.
 Keywords Arabic  X  Script  X  Recognition  X  Database  X  Synthetic  X  PCA  X  Kmeans 1 Introduction The recognition of cursive handwriting is a challenging task because of the huge variations and individuality of personal handwriting.Inscriptswherecursivewritingisoptional,such as Latin scripts, it is possible to restrict the recognition to the non-cursive handwriting and provide a partial solution. How-ever, such an option is not valid for inherently cursive scripts, such as the Arabic script.

The research in script recognition has distinguished between two main approaches X  X egmentation-based and segmentation-free. The segmentation-based approaches seg-ment an input word into individual characters, which are then recognized and combined to identify the input word. The segmentation-free approaches recognize the whole word at once,withoutsegmentingintocharacters.Recently,theholis-tic approach has been attracting more interest and become widely accepted in handwriting recognition research [ 5 , 9  X  11 , 18 , 30 , 31 , 42 ]. However, the holistic approach compares continuous words or sub-words and is required to maintain large databases X  X  recognition model for each word in the lexicon. In addition, the training and recognition demand the existence of the handwritten shapes for the entire lexicon and thegenerationof theseshapes manuallyis expensiveandtime consuming.

Many Latin databases for handwritten script recognition have been developed, especially for English scripts [ 20 , 36 , 17 , 40 ]. In contrast, very few databases have been devel-oped for the Arabic script and fewer have become publicly available [ 1 , 2 , 21 , 28 , 29 ]. Research groups have developed private databases [ 2 , 21 , 28 ], which rarely become publicly available. To the best of our knowledge, no database has been developed to include the entire handwritten words or word-parts in the Arabic language because of the tremendous effort needed for such a task. That explains the unavailability of a standard comprehensive database (on-line or off-line) for Arabic handwriting script recognition.

In this paper, we present a new approach for efficient gen-eration of an on demand synthetic comprehensive database for on-line and off-line Arabic script recognition research. The generated database includes multiple shapes for each word that represent multiple personal handwriting styles. We have developed a novel approach to generate synthetic shapes of any Arabic word using manually generated handwriting sets of prototypes representing the various ways of writing each letter. To keep the database compact, we reduce redun-dancy by applying clustering and dimensionality reduction techniques. The compact set still covers the huge variety of writing styles while keeping the database compact for practi-cal applications. The shape of a word-part, which is generated from basic elements of characters of one-pixel width, does not reflect realistic scanned off-line writing. To overcome this limitation, we extend the stroke width based on the properties of each word-part.

The rest of this paper is organized as follows: in Sect. 2 ,we present a brief introduction to Arabic script characteristics; in Sect. 3 , we explore the closely related work of Arabic script recognition and databases; the proposed system is discussed in detail in Sect. 4 . Sections 5 and 6 present experimental results and discuss directions for future work. 2 Characteristics of Arabic script The Arabic script is used as the alphabet for several languages, such as Farsi, Urdu, Malay, Swahili, Hausa, and Ottoman Turkish. It is written from right to left in a semi-cursive manner in handwriting as well as machine printing. The Arabic script is similar to western scripts in that it has a strict alphabet consisting of letters, numerals, and punctua-tion marks but is different in the way it combines letters into words and the way it treats vowels.

The Arabic script consists of 28 basic letters, 12 additional special letters such as ( ), and 8 diacritics. A letter in Arabic usually has several (2 X 6) different shapes X  X nitial, medial, final, and isolated X  X ccording to its adjacent letters and its position within the word. As a result, the 28 basic letters in Arabic script have close to 120 different shapes when additional strokes (Dots, Hamza, Madda, and others) are included. Some letters interrupt the cursiveness of a word by prohibiting a connection to the following letters and split-ting words into connected groups of letters called compo-nents. Each component includes one or more letters and, with its additional strokes, forms a part of word, which we call word-part .

An Arabic word-part,  X  , has a main part , which is totally cursive, and a complementary part , which includes all the additional strokes of the letters within  X  (the complementary part could be empty). Several letters share the same body part and differ by the complementary parts. For example, the word-parts bayt ( ) and tabeth ( ) share the same main body and differ in the complementary part.

In order to construct a comprehensive synthetic database that includes the entire Arabic lexicon, we have explored large collections of Arabic texts and extracted 300,000 dif-ferent words assembled from 48,000 different word-parts. After ignoring the additional strokes, the number of different word-parts went down to 28,500, organized in a word-part lexicon . Since the text collections were very large (over 5 million words) and came from different domains and peri-ods, we believe these results closely represent a large fraction of the Arabic lexicon (see [ 32 ] for more details). We have used our novel technique to synthesize large sets of shapes for each word-part in the word-part lexicon. 3 Related work In the research in Arabic script, compared to Latin and Chinese, recognition has recently attracted the interest of researchers. This delay exposed researchers to results and techniques used in other scripts, which were adapted and improved to meet the needs and challenges of Arabic scripts recognition. Segmenting cursive Arabic words into charac-ters is a hard task due to the large variety of different writ-ing styles and the absence of constraints and consistency. Attempts have been made to recognize isolated forms of Arabic letters and avoid word segmentation by forcing a non-cursive style of writing [ 6 , 16 , 24 , 27 ]. In parallel, seg-mentation-based methods were developed or adapted and improved [ 3 , 8 , 14 , 19 , 35 ].

Incorrect segmentation of a given word or word-part into letters results in poor recognition rates. The complexity and difficulties of segmenting Arabic handwritten words into letters shifted the research focus to the segmentation-free (holistic) approach. In the holistic approach [ 4 , 12 , 19 , 23 ], complete words are processed to be recognized without seg-menting words into characters. Character-based recognition approaches are required to store the various forms of each character, while holistic approaches are required to maintain the large database that stores multiple shapes for each word in the lexicon, which are used for training and recognition. The holistic approach was initially used for recognition tasks that require a small vocabulary, such as check verification, mail sorting, and keyword searching. Recently, the develop-ment of efficient holistic-based recognition methods to han-dle large vocabularies has attracted more interest [ 22 , 32 , 33 ]. Such development requires large databases for training and evaluation as well as efficient processing in terms of time and space.
Wang et al. [ 41 ] presented a method to synthesize cur-sive handwriting words guided by a deformable model. The process concatenates ligatures, strokes, and isolated letters generated from learned models to determine word trajectory. Varga and Bunke [ 38 ] presented a method for generating synthetic handwritten text lines using images of text lines of cursive handwriting. Thinning/thickening and other geomet-rical transformations were used as perturbation models to generate the synthetic lines. They used the synthetic data to improve the learning process of HMM-based off-line cursive handwriting recognition. Varga et al. [ 39 ] present a method for synthesizing English cursive handwriting from text lines using templates of characters and the delta log normal model. To generate a text line, they concatenate a perturbed version of the characters in the text line based on the given tem-plates. Overlapping strokes and delta log normal velocity profiles were used to draw the text line. Cheng and Lopres-ti [ 13 ] conducted experiments using a mixture of real English handwritten text lines and text lines altered from existing handwriting with various distortion degrees, based on a per-turbation method from [ 37 ]. They aim to calibrate distortion parameter settings for Varga and Bunke X  X  [ 37 ] perturbation model and compare the effects of parameter settings on dif-ferent writing styles. Preliminary experimental results show that appropriate parameter settings for different handwriting styles make it possible to generate a large quantity of train-ing and testing data for building better off-line handwriting recognition systems.

Many databases for Latin handwritten script recognition, such as UNIPEN [ 36 ], IAM [ 26 ] CEDAR [ 20 ], NIST [ 17 ], and IRONOFF [ 40 ], were developed. In contrast, very few databases weredevelopedfor Arabicscript andfewer became publicly available. The IFN/ENIT off-line database for Ara-bic words is one of the first publicly available databases and became the first standard database for Arabic. It includes 946 Tunisian town/villages names and postal codes writ-ten by 411 people. A Persian version of the IFN/ENIT was recently released, including city names handwritten in Farsi and consists of 7,271 binary images of 1,080 Iranian prov-ince/city names, collected from 600 writers. Another known database for Arabic handwriting recognition is the CENP-ARMI Arabic checks, which was released in 2003 [ 2 ] and consisted of legal and courtesy amounts on bank checks and isolated handwritten digits. Several standard databases have been developed recently for research of Farsi/Arabic off-line handwritten recognition [ 15 , 28 , 34 ]. Mozaffari et al. [ 28 ] presented a new comprehensive database for isolated off-line handwritten Farsi/Arabic numbers and characters for use in optical character recognition research. It includes the gray scale images of 52,380 characters and 17,740 numer-als, which were scanned from Iranian school entrance exam forms during the years 2004 X 2006 at 300dpi. Soliman-pour et al. [ 34 ] described an approach toward a standard handwritten Farsi database including isolated digits, letters, numerical strings, legal amounts used for checks, and dates. The ADAB database for on-line Arabic script recognition had been published by Haikal et al. [ 15 ] as part of a compe-tition in on-line Arabic handwriting script recognition at the ICDAR2009. The database consists of 15,158 Arabic words from 937 Tunisian town/village names written by more than 130 different writers.

In conclusion, researchers have mostly developed their own small datasets or large databases that are not available to the public [ 2 , 7 , 12 , 21 ]. None of these databases were developed to include the entire Arabic lexicon. 4 Our approach In this paper, we present a novel approach for generating syn-theticcomprehensivedatabasesfortraining,testing,andeval-uatingArabicscriptrecognitionsystems.Thesystemconsists of three main procedures: handwriting prototypes extraction, word-part generation, and shape clustering . The handwrit-ing prototypes extraction procedure is responsible for gener-ating sets of different handwriting prototypes for each letter in each position by enabling users to manually enter complete words that cover the various shapes of the letters at differ-ent positions. The word-part generation procedure generates multiple shapes for each word-part  X  in a given lexicon by concatenating the shapes of its constituting letters in the right position and order. The existence of multiple shapes for each letter and the need to consider many permutations gener-ate a huge number of shapes for each word-part, which are too large for practical use. To overcome this difficulty, the third procedure clusters the shapes of each word-part into groups. We have found that selecting small representative shapes from each cluster dramatically reduces the size of the database without affecting its representativeness. Figure 1 shows the flow diagram and the different stages of our sys-tem. It starts with accepting the ASCII code of a given Arabic word-part (right end) and uses a predefined set of handwrit-ing prototypes to synthesize word-part images and cluster them to generate a compact set of representatives.
In our current implementation, we ignored additional strokes (dots) and concentrated on synthesizing the main component of a word-part. Nevertheless, additional strokes could be easily added in a post-processing phase.

Our system provides the ability to easily generate compre-hensive databases for on-line and off-line script recognition. The generated database includes additional properties, such as local and global features at the character and word-part levels, which simplifies the study of various script recogni-tion and word-part segmentation algorithms.

In the rest of this section, we discuss in detail the three procedures of the proposed system. (a) (b) 4.1 Handwriting prototypes extraction This procedure enables a human operator to enter multiple shapes for each letter of the Arabic script, while taking into account the variance in the shapes of Arabic letters accord-ing to their position in the word. For that purpose, it includes a graphic user interface (GUI) that simplifies entering var-ious letter and word shapes by different means. Currently, our system supports three schemes to capture the different shapes of letters: 1. The operator enters the different shapes of a letter for 2. The system posts a sequence of word-parts, and the oper-3. Off-line word-parts (word-parts written on paper) are
The set of word-parts feed into the system aims to cover various shapes of all the letters and ligatures in the Arabic script.

Our system is able to extract handwriting prototypes from existing databases and extend them to cover to larger lexi-cons. It scans word-parts in the database, extracts the vari-ous occurrences of each letter in the different positions, and generates sets of handwriting prototypes. Databases that do not include segmentation of the handwritten word-parts to individual characters are required to go through a manual segmentation phase for each word and word-part. To capture unusual ways of writing pairs of special ligatures such as ( ) and ( ) we have considered them as additional single letters to the alphabet. In a post-processing step, human operators eliminate unreal shapes of word-parts mostly generated using special cases of writing letters that are part of these special ligatures. For example, the shape of the letter ( ) in the lig-ature ( ) is a special way of writing this letter in it X  X  medial form.

Typically, the shapes of each letter in a generated font are similar, and some are almost identical. To eliminate redun-dant samples and retain compact set of shapes, we apply hierarchical clustering, which produces sets that faithfully represent the variety of handwriting shapes of each letter.
The human operator is advised to use their personal hand-writing styles and imitate different common writing styles for ligatures or letters in different positions. For example, the two letters ( ) have different writing shapes and users are guided to write them in vertical order to cover such a style (see Fig. 5 ). The system also guides the attention of operators to special ligatures, such as ( ) and other com-mon pairs of consequent letters, which may not concatenate naively (Fig. 2 ).

The system computes and sets global features, such as loops, for the generated prototypes per word-part in the data-base. The deterministic loop properties are extracted directly from the text of word-part. To consider these properties in a non-deterministic manner, we store the probability of com-putationally detecting a loop within a word-part shape. This probability is determined based on the size and the ratio between the short and the long diameters of the loop (see Figs. 3 , 4 ). Nevertheless, researchers and developers can access the strokes representing each character and word-part and extract additional properties. These properties are used to generate the features for the generated words and integrate them into the database. Stroke-based features, such as end point, split point, or high curvature points, are extracted and added to the database (Fig. 5 ).

The generation of an off-line representation uses these fea-tures to imitate off-line writing by integrating off-line prop-erties, such as end-point smoothing and thickening of split points. These feature points, the original stroke, and the con-catenation points between letters within the word-part are available in the database. They can be utilized by researchers as a ground truth to test and evaluate thinning and character segmentation algorithms.

In our approach, characters and ligatures have different writing styles and thus encounter different numbers of loops, which call for careful treatment to avoid eliminating legiti-matecandidates inaloop-basedcandidatefiltering; for exam-ple, the existence of loop in the letter ( ) is not consistent even in different printed fonts X  X he medial form may include zero, one, two, or three loops (see Fig. 2 for details) X  X nd letters ( ), ( ) and ( ) can be written with or without a loop. Such inconsistency in the number of loops complicates the preprocessing and post-processing candidate pruning phase. In our approach, loops are extracted and counted separately for each different letter shape, and filtering is applied across the different word shapes (see Fig. 4 ). We have adopted this policy to avoid deterministic pruning, which can eliminate words with degenerated loops or different writing styles of the same letters, as mentioned previously. Different shapes of a letter may contribute different local and global features to a word-part. These data are highly important, as it is used to filter out candidates in a non-deterministic manner when considering global features such as loops. 4.2 Synthesizing word-parts and word shapes The synthesizing process uses the designed handwriting pro-totypes to generate a polyline representation of each word-part that forms the on-line database. These polylines are used to generate off-line databases by increasing the width of the lines and adding noise to simulate scanned off-line word-parts.

The generated fonts are used to construct a writer-inde-pendent open vocabulary database of word-part shapes. For each word-part  X  in the lexicon  X  , the system determines the shape of the letters in  X  , while taking into consideration the position of each letter and ignoring the additional strokes. The shapes representing  X  are generated by concatenating the various shapes for each letter in the right order X  X enerat-ing the valid permutations of  X  . This simple concatenation is performed by stitching the endpoint of each letter shape to the start point of the following one and smoothing the stitching region. Our current system uses an Arabic language word-parts lexicon that includes a large fraction of all word-parts in the Arabic language X  X round 48,000 word-parts. It is also capable of generating a database for any given lexicon, such as Farsi or Urdo, which uses the Arabic alphabet. For some languages, it may require adding shapes of letters that do not exist in the Arabic language.

Let l p i be a vector (v 0 ,...,v n i ) with length n i , represent-ing the i th shape of the letter l in the position p . To generate one shape for the word-part  X  = ( l 1 , l 2 ,..., l m ) with length m , we concatenate the vectors l ini i sents one appearance of a letter in a specific form, where ini, med, and fin stand for the initial, middle, and final positions of a letter within a word-part, respectively. The concatena-tion is performed by joining the endpoint of the vector l the start point of l i + 1 , while taking into account the appro-priate positions of the two letters. Points in the vector l are adjusted to be aligned to the end point of the previous let-ter X  X  vector. To achieve seamless stitching, we apply a simple smoothing to the stitching region of each two adjacent shapes of letters.

Obviously, no concatenation is applied for one-letter word-parts. Two-letter word-parts are constructed by con-catenating the initial and final vectors of the corresponding letters. As expected, such a scheme for word-part shape gen-eration produces huge sets of shapes for each word-part in the lexicon. For example, a word-part that contains five letters with eight different shapes for each letter have 32,768 = different shapes. Many of these shapes are similar as they display only minor differences, which calls for techniques to reduce redundancy.

During the generation process, global and local features are extracted from the font classes, for example, the num-ber of loops in a word-part,  X  , is determined based on the assigned loop count for each letter in  X  . Note that the same word-part may have various shapes that have dif-ferent properties depending on the complexity of its con-stituting letter shapes, for example, different number of loops. Such diversity in word-part representation enables delicate treatment of various features in a non-determinis-tic manner, which is essential for holistic-based recognition approaches.

We create the off-line images of the word-parts from the generated on-line word-part shapes X  X he ordered strokes X  by applying a standard dilation process. In general handwrit-ing, the curves near end points are usually smooth and thin due to a pen lift, and areas around a split point and curved strokes are often thicker than the average width of the stroke. Our off-line handwriting generation algorithm determines these properties based on a small number of user-determined parameters. In the final step, we use two methods to simu-late the process of printing and scanning. In the first method, we replace the expected process of printing and scanning by the methods presented in [ 25 ], and to simulate the scan-ning process, we use different degradation factors. The sec-ond method uses a convolution with a Gaussian kernel (see Fig. 6 ) to add noise to the generated images.

The generation of words from word-parts is performed based on a predefined layout scheme , which determines the position of the shapes of word-parts with respect to each other. To represent the different writing styles, these schemes apply different layout methods, such as tilting word-parts horizontally, and semi-vertically with homogeneous, heter-(a) (b) (d) (c) ogeneous, or zero distances. In the Arabic script, there are only six disconnective letters ( ) which means that any non-final Arabic word-part has to end with one of these six letters. Observing the different writing styles and their different behaviors, we have noticed that the three letters ( ) at the end of a word-part may allow or encourage the consequent word-part to overlap or touch the current word-part within a word. The other three of the six letters prohibit overlapping or touching, unless the consequent word-part starts with the letters ( and ). We have utilized this observation to generate various shapes of a given word by tilt-ing the different word-parts within a word using the different layout schemes (see Fig. 6 ).

We generate words from the given lexicon using the gen-erated word-part X  X  shapes and based on the following three different layout schemes that determine different handwrit-ing styles.  X  A reasonable gap to concatenate the word-parts within  X  Enabling selected word-parts, based on their constituting  X  Based on the first and last letters of the different word-
Even though these methods do not represent all the dif-ferent styles, still they include most of them. It is important to note that these styles are easily extended to cover more layout schemes. 4.3 Dimensionality reduction and clustering The generated representation for each word-part in the lexi-con is too large for practical use. Fortunately, we have real-ized that a large fraction of these representations have very few or no differences. Such a high percentage of redundancy of the generated shapes for each word-part, which often include tens of thousand of items, could be reduced dra-matically by clustering and dimensionality reduction tech-niques. This step aims to generate compact sets, defined as the smallest sets that represent the wide variety of shapes for each word-part. We have adopted three techniques to build compact sets: Hierarchical clustering, principal component analysis (PCA), and K-means clustering.

Let S ( X ) ={ s i ( X ) } n i = 1 be a set of n vectors s i = (v  X  . To enable efficient and accurate processing, we simplify the stroke s i in a semi-uniform manner. Let us denote the simplified vector s i by s  X  i , where  X  is the error tolerance used to control the simplification process. We define the feature  X  j at the point p j , on a given vector (point sequence), as the angle between the segment p j , p j + 1 and the following segment. For each point vector s  X  i , we generate a feature vec-tor f  X  i using the features  X  j for 0  X  j &lt; n i .Wealsousea parameter k to determine the desired cardinality.

We first apply PCA on the covariance matrix of the n vec-tors f i and use the m eigenvectors derived from the largest m eigenvalues for dimensionality reduction. The original sam-ples transformed by the m eigenvectors are clustered using the K-Means clustering technique. The results are then trans-formedbacktotheoriginalvectorsandusedasthe k centroids to extract the representative vectors within each cluster. The result of the third step is a set of k vectors representing the k shapes in our desired compact set. The constants k and m are fixed for each word-part as a percentage of the differ-ent shapes for each letter and the length of the word-part. In these clustering methods, we adopted the Euclidean dis-tance to measure differences between shapes, which requires applying length normalization on the feature vectors.
To synthesize an off-line word, we applied the same tech-nique of clustering using the contour of the shape. The result-ing compact sets were very similar to those we obtained using the one-pixel width stroke. Therefore, we decided to apply the dilation on the clustering results, yield from stroke, for efficient processing. Holistic approaches, which rely on con-tour or sliding windows techniques can use the original X  non-compact X  X nd apply their own techniques for cluster-ing. Holistic approaches using words as one component can adapt their own technique to reduce the size of the lexicon for each word, if needed. We believe that our layout meth-ods represent various writing styles; nevertheless, additional reduction techniques can be used to obtain different compact sets. 5 Experimental results To test and evaluate our approach, we performed sev-eral experiments on two Arabic databases: Adab [ 1 ] and IFN/ENIT [ 29 ], which are often used to train and evalu-ate Arabic handwriting recognition systems. We adapt the Adab [ 1 ] and the IFN/ENIT [ 29 ] databases to be used with our on-line and off-line Arabic handwriting recognition sys-tems, which expect individual word-parts as the basic units for recognition. The database contents were slightly modi-fied to include each word-part as one connected component, that is, incorrectly split components for single word-parts rejoined to form a single one and touching components split manually to individual word-parts. The resulting databases include word-parts, most of them with multiple appearance and demarcations that indicate the split of word-parts to let-ters. We also extended the properties of the IFN/ENIT word-parts to include the beginning and end drawing points.
We have evaluated the synthetically generated on-line and off-line databases against the manually generated Adab and IFN/ENIT databases, respectively. To compare the perfor-mance of two different recognition systems, we train and compare their recognition performance using the same data-base. To compare two databases, we use them to train two instances of the same recognition system and compare their recognition performance.

We used our on-line Arabic Script recognition system [ 33 ] for on-line handwriting recognition and adapted it to off-line handwriting recognition by using the bounding contour of a word-part as the input stroke. We compared the rec-ognition precision and recall rates of each system using a synthetically generated database versus the manually gener-ated benchmarks (IFN/ENIT and Adab). Next, we overview the handwriting prototypes and datasets used to evaluate the quality of a synthesized database.

The following three sets were used to evaluate on-line handwriting recognition: 1. Manually modified Adab database (MM-Adab) : A mod-2. Synthetic generation from Adab (SG-Adab) : The letters 3. Synthetic generation from user (SG-ON-User) : 48 writ-
The following three sets were used to evaluate off-line handwriting recognition: 1. Manually modified IFN/ENIT database (MM-IFN) :A 2. Synthetic generation from IFN/ENIT (SG-IFN) :This 3. Synthetic generation from user (SG-OFF-User) : 48 writ-
We split each set to two sub-sets, one for training and the other for testing. The training set includes 60 % of the sam-ples, and the testing includes the rest (40 %) of the samples. Our experimental study aims to evaluate the quality of the synthesizeddatabases,whichisestimatedbasedontheability of the generated samples to cover the variety of handwriting as well as is done by manually generated datasets.
Table 1 presents recognition performances of the manual set compared to the synthetic sets. The three rows for the on-line recognition system represent the Manually Modified (MM-Adab), Synthetic Generation based on the Adab data-base (MM-Adab), and Synthetic Generation based on users on-line handwriting (SG-On-User). The same rows represent the (MM-IFN,SG-IFN and SG-Off-User) for the off-line sys-tem, using the IFN/ENIT database and images of the users handwriting. The similarity in the recognition results shows the ability of the synthesized samples to capture the vari-ous writing styles of different users. The synthesized sam-ples can also provide better performance (as can be seen in the third column), which results from extending the variety of handwriting by combining various handwritings into one shape (word-part). The richness of the synthesized samples depends on the number of writers and the variety of their handwriting.

In order to evaluate the efficiency of deterministic and non-deterministic utilization of the loop as a global features, we experimented with the following three options:  X  The recognition system ignores the loop feature.  X  The loop feature was used as a filtering step directly on  X  We use the loop feature across the shapes of the word-
To evaluate the contribution of the integrated properties to the accuracy and time response, we selected a set of 400 word-parts from the datasets based on the Adab database. These word-parts were selected to include at least one loop. Table 2 reports the recognition rates and reduction of time obtained using each of the three options. For a query shape s to be recognized, the process starts by determining the num-ber of loops in s . The first option did not use number of loops in the filtering process. The second option filtered out the shapes (word-parts) that have different numbers of loops in their textual representation. The third option filtered out the shapes by comparing the number of loops integrated into the database, which were determined directly from the shapes without considering their textual representation.

Table 2 shows that using the loop feature to filter candidate words from the lexicon reduces the average recognition time by 80 %. The recognition rates are less encouraging when the loop feature is used in a deterministic manner. The sec-ond and third rows in Table 2 show that using global features for pruning candidates improves response time. The recog-nition rates are improved when the loop feature is used in a non-deterministic manner across different shapes as seen in the third line.

Research in segmentation into characters can use the recorded split points and the global features for training, test-ing, and evaluation. Thinning algorithms can use the synthet-ically generated off-line words with the original strokes as skeletons and feature points, such as end, split, and curvature points for testing and evaluations. 6 Conclusion and future work We have presented an efficient approach for generating large datasets of synthetic shapes of a given lexicon of words in order to generate a comprehensive database that includes dif-ferentshapesforeachwordintheArabicscript.Ourapproach requires a lexicon that determines the set of words and word-parts, and a set of handwriting prototypes . These fonts could be generated manually by human writers or extracted auto-matically from a given small dataset of word shapes. The results we have presented show the credibility of the pro-cedure and report a small improvement on the recognition rates that result from the inherited ability of this approach to generate many shapes representing the wide variety of writing styles. Our approach aims to produce a comprehen-sive handwriting database for the Arabic language and can be used to construct different databases for additional languages that use the Arabic alphabet. We also present a simple and efficient technique for extending a given small database to a comprehensive one. Extending this approach for western scripts such as handwriting Latin and English can be done following different rules for connecting letters to each other.
The databases we generated are compact and comprehen-sive X  X hey include all the words and word-parts in a given Arabic lexicon. For each character, word-part, and word shape, the database includes integrated data for local and global features that could be used by researchers for devel-oping, training, and evaluating new techniques. We believe this database can contribute to various research approaches in Arabic script recognition, word spotting, and character/word skeletonization.
 References
