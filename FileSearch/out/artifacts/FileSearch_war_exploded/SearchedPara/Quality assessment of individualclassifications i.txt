 REGULAR PAPER Matja  X  z Kukar Abstract Although in the past machine learning algorithms have been suc-fact that often they cannot produce reliable and unbiased assessments of their predictions X  quality. In last few years, several approaches for estimating reliability or confidence of individual classifiers have emerged, many of them building upon the algorithmic theory of randomness, such as (historically ordered) transduction-based confidence estimation, typicalness-based confidence estimation, and transductive reliability estimation. Unfortunately, they all have weaknesses: either they are tightly bound with particular learning algorithms, or the interpretation of reliability estimations is not always consistent with statistical confidence levels. In the paper we describe typicalness and transductive reliability estimation frame-works and propose a joint approach that compensates the above-mentioned weak-nesses by integrating typicalness-based confidence estimation and transductive reliability estimation into a joint confidence machine. The resulting confidence machine produces confidence values in the statistical sense. We perform series of tests with several different machine learning algorithms in several problem domains. We compare our results with that of a proprietary method as well as with kernel density estimation. We show that the proposed method performs as well as proprietary methods and significantly outperforms density estimation methods.
 Keywords Data mining  X  Machine learning  X  Typicalness  X  Transduction  X  Quality assessment 1 Introduction Usually machine learning algorithms output only bare predictions (classifications) for the new unclassified examples. While there are ways for almost all machine particular classification, so far there is no general method to assess the quality (confidence, reliability) of a single classification. We are interested in the assess-ment of classifier X  X  performance on a single example and not in average perfor-mance on an independent dataset. Such assessments are very useful, especially in risk-sensitive applications (medical diagnosis, financial and critical control ap-plications) because there it often matters, how much one can rely upon a given prediction. In such cases an overall quality measure of a classifier (e.g. classifica-tion accuracy, mean squared error, ...)withrespecttothewholei nput distribution would not provide the desired value. Another possible use of quality assessment of single classifications is in ensembles of machine learning algorithms for selecting or combining answers from different classifiers [ 13 ].
 classifiers, (decision trees and rules, Bayesian classifiers, neural networks, nearest neighbour classifiers, ...) in order to interpret their decision as a probab ility dis-tribution over all possible classes. In fact, we can trivially convert every machine learning classifier X  X  output to a probability distribution by assigning the predicted class the probability 1, and 0 to all other possible classes. The posterior probability of the predicted class can be viewed as a classifier X  X  confidence (reliability) of its prediction. However, such estimations may in general not be good due to inherent biases of the applied algorithms. 1 Reliability estimation of a classification ( y )of a single example ( x ), given its true class ( y ) should have the following property: If Eq. ( 1 ) holds, or even better, if it approaches equality, a reliability measure can be treated as a confidence value [ 15 ]. 1.1 Related work Several methods for inducing probabilistic descriptions from training data, fig-uring the use of density estimation algorithms, are emerging as an alternative to more established approaches for machine learning. Frequently kernel density es-timation [ 30 ] is used for density estimation of input data using diverse machine learning paradigms such as probabilistic neural networks [ 25 ], Bayesian networks and classifiers [ 9 ], and decision trees [ 24 ]. By this approach a chosen paradigm, coupled with kernel density estimation, is used for modelling the probability dis-tribution of input data. Alternatively, stochastically changing class labels in the training dataset is proposed [ 6 ] in order to estimate conditionally class probabil-ity.
 space into reliable and unreliable regions [ 1 ]. Such meta-learning approaches have ensemble of classifiers [ 23 ].
 chine learning algorithm for a particular problem [ 18 ] based on performance and characteristics of other, simpler learning algorithms. In our problem of confidence estimation such an approach would result in learning to predict confidence value based on characteristics of single examples.
 [ 22 ], in connection with algorithmic theory of randomness. Here, approximations of randomness deficiency for different methods (SVMs, ridge regression) have been constructed in order to estimate confidence of single predictions. The draw-back of this approach is that confidence estimations need to be specifically de-signed for each particular method and cannot be applied to other methods. tion principle, has been proposed in [ 13 ]. While it is general and independent of the underlying classifier, interpretation of its results isn X  X  always possible in the statistical sense of confidence levels.
 transduction [ 8 , 15 , 19 ]. By this approach, a  X  X trangeness X  measure of a single ex-ample is used to calculate its typicalness, and consequently a confidence in clas-sifier X  X  prediction. The main drawback of this approach is that for each machine learning algorithm an appropriately constructed strangeness measure is needed. where transductive reliability estimation serves as a generic strangeness measure in the typicalness framework. We compare the experimental results to those of kernel density estimation and show that the proposed method significantly out-performs it. We also suggest how the basic transduction principle can be used to significantly improve the results of kernel density estimation so it almost achieves the results of transductive typicalness.
 typicalness and transduction, outline the process of their integration, and review kernel density estimation methods used for comparison. In Sect. 3 we evaluate how our methodology compares to other approaches in 15 domains with 6 ma-chine learning algorithms. In Sect. 4 we present some conclusions and directions for future work. 2 Methods and materials The produced confidence values should be valid in the following sense. Given some possible label space Y , if an algorithm predicts some set of labels Y  X  Y with confidence t for a new example which is truly labelled by y  X  Y ,thenwe would expect the following to hold over randomization of the training set and the new example: Note that Eq. ( 2 ) is very general and valid for both classification ( Y is predicted set of classes) and regression problems ( Y is a predicted interval). As we deal only with single predictions in this paper, Eq. ( 2 ) can be simplified to a single predicted class value ( Y ={ y } ): 2.1 Typicalness In the typicalness framework [ 15 , 16 , 22 ] we consider a sequence of examples ( z unknown label y n + 1 , all drawn independently from the same distribution over Z = X  X  Y where X is an attribute space and Y is a label space. Our only assumption is therefore that the training as well as new (unlabelled) examples are independently and identically distributed ( iid assumption).
 possible labelling for a new example x n + 1 . We postulate some labels y n + 1 and for each one we examine how likely (typical) it is that all elements of the extended the same distribution or how typically iid the sequence is. The more typical the sequence, the more confident we are in y n + 1 . To measure the typicalness of se-quences, we define, for every n  X  N , a typicalness function t : Z n  X  X  0 , 1 ] which, for any r  X  X  0 , 1 ] has the property sequence is unusual because it will be produced at most 5% of the time by any iid process. It has been shown [ 15 ] that we can construct such functions by consider-ing the  X  X trangeness X  of individual examples. If we have some family of functions then we can associate a strangeness value with each example and define the following typicalness function We group individual strangeness functions  X  i into a family of functions A n : n  X  N ,where A n : Z n  X  R n for all n . This is called an individual strangeness mea-sure if, for any n , any permutation  X  :{ 1 ,..., n } X  X  1 ,..., n } , any sequence ( z criterion [ 15 ]: ( X  The meaning of this criterion is that the same value should be produced for each individual element in sequence, regardless of the order in which their individual strangeness values are calculated. This is a very important criterion, because it can be proven [ 15 ] that the constructed typicalness function Eq. ( 7 ) satisfies the condition from Eq. ( 4 ), provided that the individual strangeness measure satisfies the Eq. ( 8 ).
 measures, ranging between 0 for most typical examples, and some positive upper bound, (up to + X  ), for most atypical examples. 2.1.1 Typicalness in machine learning In the machine learning setup, for calculating the typicalness of a new example z the training set ( z 1 ,..., z n ) ,Eq.( 7 ) changes to we need to construct an appropriate strangeness measure and modify the algo-rithm accordingly. 2 Then, for each new unlabelled example x , all possible la-bels y  X  Y are considered. For each label y a typicalness of labelled example t (( with  X  X ost typical X  class, that is the one that maximizes { t (( x , y )) } .ByEq.( 7 ) the second largest typicalness is an upper bound on the probability that the ex-cluded classifications are correct [ 19 ]. Consequently, the confidence is calculated as follows: 2.2 Transductive reliability estimation Transduction is an inference principle that takes a training sample and aims at es-timating the values of a discrete or continuous function only at given unlabelled points of interest from input space, as opposed to the whole input space for in-duction. In the learning process the unlabelled points are suitably labelled and included into the training sample. The usefulness of unlabelled data has also been advocated in the context of co-training. It has been shown [ 2 ] that for every better-than-random classifier its performance can be significantly boosted by utilizing only additional unlabelled data.
 solving a more general problem as an intermediate step. The reasoning behind this principle is that, in order to solve a more general task, resources may be wasted or compromises made which would not have been necessary for solving only the problem at hand (i.e. function estimation only on given points). This common-sense principle reduces a more general problem of inferring a functional depen-dency on the whole input space (inductive inference) to the problem of estimating the values of a function only at given points (transductive inference). 2.2.1 A formal background Let X be a space of attribute descriptions of points (examples) in a training sample (dataset), and Y a space of labels (continuous or discrete) assigned to each point. Given a probability distribution P , defined on the input space X  X  Y , a training sample consisting of l points, is drawn iid (identically independently distributed) accord-ing to P . Additional m data points (working sample) with unknown labels are drawn in the same manner. The goal of transductive in-ference is to label all the points from the sample W using a fixed set H of functions f : X  X  Y in order to minimize an error functional both in the training sample S and in the working sample W (effectively, in S  X  W ). In contrast, inductive infer-ence aims at choosing a single function f  X  H that is best suited to the unknown probability distribution P .
 a working sample. This can be done by labelling every point from a working sample with every possible label value; however given m working points this leads to a combinatorial explosion yielding n m possible labellings. For each possible labelling, an induction process on S  X  W is run, and an error functional (error rate) is calculated. In case of m = 1 we can significantly reduce the computational complexity by labelling a point with a label predicted from S only [ 12 ]. probability that it is correct). If the iid assumption holds, the training sample S as well as the joint correctly labelled sample S  X  W should both reflect the same underlying probability distribution P .
 labelling. Unfortunately, this problem in general belongs to the non-computable class [ 14 ], so approximation methods have to be used [ 12 , 29 ].
 many uses. In risk-sensitive applications (medical diagnosis, financial and critical control applications) it often matters, how much one can rely upon a given predic-tion. In such a case a general reliability measure of a classifier (e.g. classification accuracy, mean, squared error, ...) with respect to the whole i nput distribution would not provide the desired warranty. Another use of reliability estimations is in combining answers from different predictors, weighed according to their relia-bility. 2.2.2 Why does transduction work? There is a strong connection between the transduction principle and the algorith-mic (Kolmogorov) complexity. Let the sets S and S  X  W be represented as binary strings u and v , respectively. Let l (v) be the length of the string v and C (v) its Kolmogorov complexity, both measured in bits. We define the randomness defi-ciency of the string v as following [ 14 , 29 ]: Randomness deficiency measures how random is the respective binary string and therefore the set it represents. The larger it is, more regular is the string (and the set). If we could calculate the randomness deficiency (but we cannot, since it is not computable), we could do it for all possible labellings of the set S  X  W and select the labelling of W with largest randomness deficiency as the most probable one [ 29 ]. That is, we would select the most regular one. We could also construct a universal Martin-L  X  of X  X  test for randomness [ 14 ]: That is, for all binary strings of fixed length n , the probability of their randomness deficiency  X  being greater than m is less than 2  X  m .Thevalue2  X   X ( x ) is therefore a p -value function for our randomness test [ 29 ].
 mogorov complexity, it is not computable. Therefore we need feasible approxi-mations to use this principle in practice. Extensive work has been done by using Support Vector Machines [ 4 , 22 , 29 ], however no general approach exists so far. 2.2.3 A machine learning interpretation In machine learning terms, the sets S and S  X  W are represented by the in-duced models M S and M S  X  W . The randomness of the sets is reflected in the (Kolmogorov) complexity of the respective models. If for the set S  X  W the la-belling of W with the largest randomness deficiency is selected, it follows from our definition of randomness deficiency (Eq. ( 13 )) that since the length l (v) is constant, the Kolmogorov complexity C ( M S  X  W ) is minimal. Therefore the model M S  X  W is most similar to the M S .
 the (finite) models M S and M S  X  W . Greater difference between them means that the set S  X  W is more random than the set S and (under the assumption that S is sufficient for learning effective model) that W consist of (at least some) improp-erly labelled the atypical examples.
 calculate changes between model descriptions (assuming that they can be effi-ciently coded; black-box methods are thus out of question). However, there exists another way.
 of a function only at given points of interest from input space (the set W ), we are interested only in model change considering this example. Therefore we can compare the classifications (or even better, probability distributions) of models M S and models M S  X  W . Obviously, the labelling of W that would minimally change the model M S is as given by M S . We will examine this approach in more detail in the next section.
 originating from Kolmogorov complexity are described in more detail in [ 13 ]. Basically, we have a two-step process, featuring an inductive step followed by a transductive step .  X  X n inductive step is just like an ordinary inductive learning process in machine  X  X  transductive step is almost a repetition of an inductive step. A machine  X  After the reliability is calculated, the example in question is removed from the training set. New examples are not permanently included in the training set; this would be improper since the correct class is at this point still unknown. Although retraining for each new example seems to be highly time consuming, it is not such a problem in practice, especially if incremental learners (such as naive Bayesian classifier) are used.
 ductive reliability estimation is that we disturb a classifier by inserting a new ex-ample in a training set. A magnitude of this disturbance is an estimation of the classifier X  X  instability (unreliability) in a given region of its problem space. fications as a probability distribution over all possible classes, we need a method to measure the difference between two probability distributions. The difference mea-sure D should ideally satisfy all requirements for a distance (i.e. nonnegativity, tri-angle nonequality and symmetry), however in practice nonnegativity suffices. For calculating the difference between probability distributions, a Kullback-Leibler divergence is frequently used [ 5 , 26 ]. A Kullback-Leibler divergence, sometimes referred to as a relative entropy or I -divergence, is defined between probability distributions P and Q : In our experiments we use a symmetric Kullback-Leibler divergence, or J -divergence, which is defined as follows: J (
P , Q ) is limited to the interval [ 0 ,  X  X  ,where J ( P , P ) = 0. Since in this con-text we require the values to be from the [ 0 , 1 ] interval we normalize the diver-gence in the spirit of Martin-L  X  of X  X  test for randomness.
 However, measuring the difference between probability distributions does not al-ways perform well. There are at least a few exceptional classifiers (albeit trivial ones) where the original approach utterly fails. 2.2.4 Assessing the classifier X  X  quality: the curse of trivial models So far we have implicitly assumed that the model produced by the classifier is good (at the very least better than random). Unsurprisingly, our approach works very well with random classifiers (probability distributions are randomly calcu-lated) by effectively labelling their classifications as unreliable [ 11 ]. constant classifier is such that it classifies all examples into the same class C k with probability 1. In such cases our approach always yields reliability 1 since there is no change in probability distribution. A majority classifier is such that it classifies all examples into the same class C k that is the majority class in the training set. Probability distribution is always the same and corresponds to the distribution of classes in the training set. In such cases our approach yields reliability very the example in question), that is at most for 1 / N ,where N is number of training examples. In large datasets this change is negligible.
 example, a physician that always diagnoses an incoming patient as ill is a constant classifier. On the other hand, a degenerated  X  overpruned  X  decision tree (one leaf only) is a typical majority classifier.
 we also need to take in account the quality of classifier X  X  underlying model and appropriately change our definition of reliability.
 ability estimations actually estimate the conditional reliability with respect to the model M ability theorem for the whole model
Rel ( y i ) = P ( model M is good )  X  P ( y i is true class of x i | model M is good ) or even better for the partial models for each class y i Now we only need to estimate the unconditional probabilities In machine learning we have many methods to estimate the quality of the induced model, e.g. a cross-validation computation of classification accuracy is suitable for estimation of Eq. ( 21 ). However it may be better to calculate it in a less coarse way, since at this point we already know the predicted class value ( y i ). certain class is correct. Our approach is closely related to the calculation of post-test probabilities in medical diagnostics [ 3 , 17 ]. Required factors can be easily estimated from the confusion matrix (Definition 1 ) with internal testing. Definition 1 A confusion matrix (CM) is a matrix of classification errors obtained with an internal cross validation or leave-one-out testing on the training dataset. The ij -th element c ij stands for the number of classifications to the class i that should belong to the class j .
 (true positives ratio) and specificity (true negatives ratio) values for multi-class problems. Basically, for N classes we have N two-class problems. Let C p be a correct class in certain case, and C a class, predicted by the classifier in the same case. For each of possible classes C i , i  X  X  1 .. N } , we define its class sensitivity Se ( C C Class conditional probability is calculated for each class C i , given its prior proba-bility P ( C i ) , approximated with the prevalence of C i in the training set, its class specificity ( Sp ) and sensitivity ( Se ): butions P and Q , and index i = argmax P that determines the class with max. probability ( C i ). According to the Eq. ( 20 ) we calculate the reliability estimations by Multiplication by class conditional probabilities accounts for basic domain char-acteristics (prevalence of classes) as well as classifier X  X  performance. This includes class sensitivity and specificity, and it is especially useful in an automatic setting for detecting possible anomalies such as default (either majority or constant) clas-sifiers that  X  of course  X  cannot be trusted. It is easy to see that in this case we have one class with sensitivity 1 and specificity 0, whereas for all other classes we have sensitivity 0 and nonzero specificity. In the first case, the class post-test probability is equal to its prior probability, whereas in the second case it is 0. 2.3 Merging the typicalness and transduction frameworks There is a very good reason for merging typicalness and transductive reliability estimation frameworks together. While transduction gives good reliability estima-tions, they are often hard to interpret in the statistical sense. On the other hand, the typicalness framework gives clear confidence values, however in order to achieve this a good strangeness measure  X ( z i ) needs to be constructed.
 sure  X  i = C ,where C is some constant value. Unfortunately, this does us no good, since it treats all examples as equally strange and can be considered as most conservative strangeness measure. It is therefore necessary to construct a sensi-ble strangeness measure. In [ 15 , 19 ] some ideas on how to construct strangeness measures for different machine learning algorithms are presented.
 always use transductive reliability estimation. We may speculate that most reliable examples are also least strange. Therefore we define the strangeness measure for anewexample z n + 1 = ( x n + 1 , y n + 1 ) , described with attribute values x n + 1 and labelled y n + 1 , given the training set ( z 1 ,..., z n ) as follows: It can be shown that such a strangeness function satisfies the criterion from Eq. ( 8 ) and therefore has the property required by Eq. ( 7 ).
 Theorem 1 The strangeness measure  X ( z i ) = 1  X  Rel (( x i , y i )) is independent of the order in which the examples X  strangeness values are calculated.
 Proof The training set is only temporarily changed by including a suitably la-belled new example in a transductive step (Fig. 1 . It is restored back to the initial training set as soon as the reliability estimation is calculated. Therefore the train-ing set remains invariant for all new examples for which the reliability estimation needs to be calculated. It follows that it is irrelevant in which order the examples are presented and the criterion for Eq. ( 8 ) is therefore satisfied. Note that Eq. ( 8 ) does not require that examples are ordered in any particular way, but only that any permutation of the order of their evaluations produces the same result for each example.
 the typicalness setting this expression can be even more simplified). It is positive, and the  X  X ore strange X  examples have higher strangeness values, as suggested in [ 15 ]. 2.3.1 Simplification of transductive reliability estimation for application within the typicalness framework Alternatively, the calculation of the strangeness measure can, in the context of typicalness and reliability estimation, be much simplified. Simplifications are twofold. 1. Since the only requirement for strangeness measure is that is is positive, no 2. The typicalness framework efficiently deals with extremely deviant classi-(asymmetric Kullback-Leibler divergence). is straightforward. For all training examples, reliability estimation is calculated by leave-one-out testing, and they are labelled as correctly or incorrectly classified. For each new example x with classification y its confidence conf (( x , y )) is cal-culated as in Sect. 2.1 ,Eq.( 10 ). Regardless of the number of classes in original problem, there are only two possibilities (meta-classes) for each classification. It is either correct or incorrect. Therefore we always deal with exactly two meta-classes that represent correct classifications and incorrect classifications. As we want the confidence to reflect the probability of a correct classification, we need to invert the confidence values for incorrect meta-class: 2.4 Meta-learning and kernel density estimation The problem of estimating a confidence value can also be viewed as a meta-learning problem where the original class value is replaced by the correctness of its prediction. Let y be a meta-class for training examples obtained with internal leave-one-out testing (i.e. y = 1 for correct and y = 0 for incorrect classifica-tions). We can calculate the confidence in a given prediction of a new, previously unseen example x by estimating the function y ( x ) with a nearest neighbour clas-sifier: Here N K ( x ) is the set of K points nearest to x according to some distance mea-sure. However, such simple estimations may be problematic when the attribute space is large (lots of multi-valued, possibly correlated, attributes), and sparsely populated (relatively small number of training examples). Our experimental re-sults (Table 2 ) also shows this problem, as using a nearest neighbour meta-learner results in lowest performance of all methods. 3 Therefore, a transformation of input space is necessary to reduce the dimensionality of input space. We have chosen the principal component analysis (PCA) methodology on the training data, and two components with largest variances were selected as data descriptors. On aver-age, the sum of the two components X  relative variances is about 0.7. This means, that the two principal components describe about 70% of data variability. weights that decrease smoothly with distance from the target point. This leads us to kernel density estimation [ 28 ] in reduced and uncorrelated data space. It can be estimated by using the Nadaraya-Watson kernel weighted average: where  X  =[  X  1 , X  2 ] is a vector of kernel parameters (bandwidths), and K  X  ( x , x i ) is a simplified (uncorrelated) bivariate gaussian kernel: As the PCA involves a numerical procedure that transforms a number of possi-bly correlated input variables (attributes) into a (smaller) number of uncorrelated variables (principal components), it is therefore perfectly justified to use a sim-plified bivariate Gaussian kernel for density estimation on uncorrelated variables. Our experiments have shown, that indeed in all cases the correlation between the largest two principal components is less than 10  X  14 , also negligible. For the bi-variate Gaussian kernels, appropriate bandwidths were calculated from training data according to the rule of thumb as described by Wand [ 30 ,p.98].
 each training example, a correctness of its classification was determined by the leave-one-out testing methodology. Training examples were partitioned in sets of correctly and incorrectly classified examples, and used for kernel density estima-tions of correct and incorrect classifications. For each new examples, principal components were calculated and used to calculate the density of correct classifi-ordinates. The confidence value of a new example was calculated as cd /( cd + id ) [ 7 ]. 2.5 Improving kernel density estimation by transduction principle The procedure described in Sect. 2.4 is computationally fast when applied to new examples as it involves only calculating the principal components (scaling and one matrix multiplication), and two fast uncorrelated density estimations. Unfor-tunately, its performance (Table 2 ) compared to transductive confidence estimation is rather uninspiring. The performance, however, can be easily improved by using some ideas from meta learning and transduction frameworks. Namely, we can eas-ily extend the original data description by including the predicted class as well as class probability distributions. They may be obtained with internal leave-one-out testing on the training set.
 class and class distribution is predicted by the original classifier, and the exam-ple X  X  description is enhanced by the classifier X  X  prediction. An enhanced example description is then used in the density estimation procedure as described in Sect. 2.4 . 2.6 Testing methodology To validate the proposed methodology we performed extensive experiments with 6 different machine learning algorithms  X  naive and semi naive Bayesian clas-naive Bayesian classifier (a combination of KNN and naive Bayesian classifier) [ 13 ], two kinds of Assistant (ID3-like decision trees) with both information gain and ReliefF [ 20 ] impurity measures. Experiments were performed with 14 well-known benchmark datasets from the UCI repository (Mesh, Breast cancer, Di-abetes, Heart, Hepatitis, Iris, Chess endgame (king-rook vs. king), LED, Lym-phography, Primary tumor, Rheumatology, Soybean, Voting), and on a real-life problem of nuclear cardiology diagnostics (Nuclear).
 internal leave-one-out testing its correctness  X  whether it was correctly (1) or in-correctly (0) classified. For reliability estimations, confidence values and density estimations, we calculated their correlation with correctness. In an ideal case (each correct example has value 1, each incorrect 0), the result would be 1.
 incorrectly classified examples. For each method (reliability estimations, confi-dence values, and density estimations) we calculated the boundary b that maxi-mizes purity (information gain) of the discriminated examples. The boundary b is calculated by maximizing Eq. ( 33 ).
 Here, S is the set consisting of all examples, in the set S 1 there are unreliable examples { z i : Rel ( z i )&lt; b } whereas in the set S 2 there are reliable examples { z equal to the entropy of classifications H ( S ) .
 ample was reserved, while learning and preparatory calculations were performed on the rest, in many cases two nested leave-one-out testings were carried out. Fi-nal results are averages of leave-one-out experiments on all examples from the dataset.
 database of 600.000 customers of a large local corporation. Here, due to large quantities of data, the testing methodology was slightly different. While leave-one out testing was still used for obtaining strangeness values for the training (50%) dataset, the remaining data was used as an independet testing set. 3Results Experimental results were obtained with two different setups. The first one con-sists of series of experiments on well-known (UCI) problem domains. These re-sults were used to validate our approach and compare it with existing ones. The second experimental setup consists of applications in a real-life commercial data mining system. It also presents some valuable practical considerations. 3.1 Experiments on benchmark problems Results of confidence estimation on the KNN (nearest neighbour) algorithm are compared with the TCM-NN nearest neighbour confidence machine [ 19 ], where a tailor-made strangeness measure for confidence estimation in a typicalness frame-work was constructed. In Table 1 experimental results in 15 domains are shown. Results of TCM-NN are slightly better, as could be expected from the tailor-made method, though the differences are not significant with two-tailed, paired t -test). 3.1.1 Reliability, confidence and density estimation The obtained confidence values are compared with transductive reliability estima-tions and density estimations. Our first goal was to evaluate the performance of confidence values in terms of correlation with correctness, and its ability to sepa-rate correct and incorrect classifications in terms of information gain. Our second goal was to see whether confidence values are more easily interpretable than trans-ductive reliability estimations.
 dence levels. This is a typical example and probably the most important result of our work, as it makes them easily statistically interpretable. On average, the best decision boundary for reliability estimations is 0 . 74, on the other hand, for con-fidence it is about 0 . 45. Also, the mass of correct and incorrect classification has shifted towards 1 and 0, respectively.
 ity estimations in terms of correlation with correctness. From Fig. 3 it is clear that this is because of the shift towards 1 and 0. Information gains do not differ significantly. ferent picture. Here, in terms of correlation with correctness as well as for for information gain criterion, the differences are significant ( p &lt; 0 . 01 with two-tailed, paired t -test). Figure 2 depicts typical density estimations for both correct and incorrect classifications. On average, the best decision boundary for density estimations is 0 . 52.
 (10-NN) performed worst (although 10 was a tuned parameter). This was ex-Density estimations (Den.) performed significantly better on a reduced attribute attributes improve the performance of density estimation (Tr. den.) quite signifi-cantly ( p &lt; 0 . 05 with two-tailed, paired t -test). While it does not reach perfor-mance of transductive reliability or confidence estimations, it is much easier to compute as it does not require re-learning of a classifier. 3.2 Real-life application and practical considerations We also did a practical application of integration of decision support system with data mining methods working with data from extensive customer relationship management (CRM) survey for a large local corporation. It turned out that im-mense quantities of raw data had been collected and needed to be assessed. Thus the use of data mining methods was called for. The system was implemented in Oracle 9i application framework using Oracle X  X  Data Mining (ODM) database ex-tension. An Adaptive Bayesian Network classifier was used. The database con-sisted of about 600,000 customers X  records consisting of up to 100 attributes. The preparatory calculations (leave-one-out testing on training dataset) were quite lengthy as they took more than a week. However, producing a confidence estima-tion for a single customer was much more acceptable; depending on system use it took about a minute.
 information) than the probability estimations of the applied Adaptive Bayesian Network classifier. There was also improvement of more than 10% of the confi-dent classification (confidence  X  95%). In practice this could (and in near future probably will) save significant amounts of CRM campaign money.
 slowness. It needs more than a week to perform preparatory calculations and it took again more than a week to calculate confidence values for all testing exam-ples (customer records from independent set). It may therefore not be suitable for quick on-line analysis of an overall situation, but is perfectly suited for assessment of individual customers. A great advantage of typicalness/transduction approach over other approaches (such as kernel density estimation) is that it can be easily implemented even with relatively closed (no source code available for modifica-tions) commercial data mining systems. 4 Discussion We propose an approach that compensates the weaknesses of typicalness-based confidence estimation and transductive reliability estimation by integrating them into a joint confidence machine.
 ier to interpret. Contrary to the basic typicalness and transductive confidence esti-mation, the described approach is not bound to the particular underlying classifier. This is an important improvement since this makes possible to calculate confi-dence values for almost any classifier, no matter how complex it is.
 (confidence estimation on a KNN algorithm and TCM-NN nearest neighbour con-fidence machine) show that the proposed approach performs similarly to the spe-cially modified algorithm. There is no significant reduction in performance while there is a huge gain in generality.
 dence values significantly outperform density estimations. However, this does not mean that density estimations should not be used as they are much easier to com-pute and do not require re-learning of a classifier. Their performance can also be significantly improved by using additional transductive attributes.
 several problem domains show that there is no reduction of discrimination per-formance with respect to transductive reliability estimation. More important than this, statistical interpretability of confidence values makes it possible to use appli-cations in risk-sensitive problems with strict confidence limits.
 perform the leave-one-out testing in advance, and requires temporary re-learning of a classifier for each new example. However, this may not be a problem if incre-mental learners (such as naive Bayesian classifier) are used. In other cases, density estimation with included transductive attributes may also be used.
 ness problems as well as in medical diagnostics and prognostics.
 References
