 Sequential data is prevalent in many scientific and commer-cial applications such as bioinformatics, system security and networking. Similarity search has been widely studied for symbolic and time series data in which each data object is a symbol or numeric value. Textual event sequences are se-quences of events, where each object is a message describing an event. For example, system logs are typical textual even-t sequences and each event is a textual message recording internal system operations, statuses, configuration modifica-tions or execution errors. Similar segments of an event se-quence reveals similar system behaviors in the past which are helpful for system administrators to diagnose system prob-lems. Existing search indexing for textual data only focus on unordered data. Substring matching methods are able to efficiently find matched segments over a sequence, however, their sequences are single values rather than texts. In this paper, we propose a method, suffix matrix , for efficiently searching similar segments over textual event sequences. It provides an integration of two disparate techniques: locality-sensitive hashing and suffix arrays. This method also sup-ports the k -dissimilar segment search. A k -dissimilar seg-ment is a segment that has at most k dissimilar events to the query sequence. By using random sequence mask pro-posed in this paper, this method can have a high probability to reach all k -dissimilar segments without increasing much search cost. We conduct experiments on real system log data and the experimental results show that our proposed method outperforms alternative methods using existing techniques. H.3.1 [ Content Analysis and Indexing ]: Indexing meth-ods Algorithms, Experimentation Textual Sequence; Similarity Search; Log Event
Sequential data is prevalent in many real-world applica-tions such as bioinformatics, system security and network-ing. Similarity search is one of the most fundamental tech-niques in sequential data management. A lot of efficien-t approaches are designed for searching over symbolic se-quences or time series data, such as DNA sequences, stock prices, network packets and video streams. A textual event sequence is a sequence of events, where each event is a plain text or message. For example, in system management, most system logs are textual event sequences which describe the corresponding system behaviors, such as the starting and stopping of services, detection of network connections, soft-ware configuration modifications, and execution errors [24, 22, 29, 30, 36, 37]. System administrators utilize the event logs to understand system behaviors. Similar system events reveal potential similar system behaviors in history which help administrators to diagnose system problems. For ex-ample, four log messages collected from a supercomputer [4] in Sandia National Laboratories are listed below: The four log messages describe a failure in restarting of the ntpd (Network Time Protocol daemon). The system admin-istrators need to first know the reason why the ntpd could not restart and then come up with a solution to resolve this problem. A typical approach is to compare the current four log messages with the historical ntpd restarting logs and see what is the difference with them. Then the administrators can find out which steps or parameters might cause this fail-ure. To retrieve the relevant historical log messages, the four log messages can be used as a query to search over the his-torical event logs. However, the size of the entire historical logs is usually very large, so it is not efficient to go through all event messages. For example, IBM Tivoli Monitoring 6.x [1] usually generates over 100G bytes system events for just one month from 600 windows servers. Searching over such a large scale event sequence is challenging and the search-ing index is necessary for speeding up this process. Current system management tools and software can only search a s-ingle event by keywords or relational query conditions [1, 3, 2]. However, a system behavior is usually described by sev-eral continuous event messages not just one single event, as shown in the above ntpd example. In addition, the number of event messages for a system behavior is not a fixed num-ber, so it is hard to decide what is the appropriate segment length for building the index.

Existing search indexing methods for textual data and sequential data can be summarized into two different cate-gories. In our problem, however, each of them has its own limitation. For the textual data, the locality-sensitive hash-ing (LSH) [14] with the Min-Hash [10] function is a com-mon scheme. But these LSH based methods only focus on unordered data [14, 7, 27]. In a textual event sequence, the order information cannot be ignored since different or-ders indicate different execution flows of the system. For sequential data, the segment search problem is a substring matching problem. Most existing methods are hash index based, suffix tree based, suffix arrays based or BOWTIE based [15, 23, 18, 5, 19, 9]. These methods can keep the or-der information of elements, but their sequence elements are single values rather than texts. Their search targets are the matched substrings. In our problem, the similar segments are not necessary to be matched substrings.
To combine both advantages of the locality-sensitive hash-ing (LSH) and suffix arrays, this paper proposes a method, suffix matrix , to search similar segments over textual se-quences. This method first creates a set of independent hash functions and maps the textual sequence into a set of hash-value sequences. Then, it constructs a suffix matrix where each row is a suffix array generated from a hash-value sequence. By using binary search over this suffix matrix, this method is able to find out similar segments with a high probability. Meanwhile, it can reduce the collision probabil-ity with dissimilar segments. This method can also search k -dissimilar segments. A k -dissimilar segment is a segmen-t which has at most k dissimilar events to the query se-quence. By using the random sequence mask proposed in this paper, suffix matrix is able to maintain a high proba-bility to reach all k -dissimilar segments without increasing much search costs. suffix matrix is a systematic integration of LSH and suffix arrays. We conduct experiments on real system log data. The experimental results show that suf-fix matrix outperforms the straightforward combinations of existing techniques.

The rest of the paper is organized as follows: In Section 2, we formulate the problem of this paper. Then, we discuss the straightforward potential solutions using existing meth-ods. Section 3 presents our suffix matrix method and theo-retically analyzes its performance. In Section 4, we conduct experiments on real system log data and present the empir-ical results. Section 5 describes the related work. Finally, Section 6 concludes this paper.
Let S = e 1 e 2 :::e n be a sequence of n event messages, where e denotes the i -th event, i = 1 ; 2 ; :::; n . | S | denotes the length of sequence S , which is the number of events in S . E denotes the universe of events. sim ( e i ; e j ) is a similarity function which measures the similarity between event e i and event e j , where e i  X  X  , e j  X  X  . In this paper, Jaccard coef-ficient [28] with 2-shingling [11] is utilized as the similarity function sim (  X  ;  X  ) because each event is a textual message. Definition 1. (Segment) Given a sequence of events S = e 1 :::e n , a segment of S is a sequence L = e m +1 e where l is the length of L , l  X  n , and 0  X  m  X  n  X  l . The problem of this paper is formally stated as follows.
Problem 1. (Problem Statement) Given an event se-quence S and a query event sequence Q , find all segments with length | Q | in S which are similar to Q .
 Similar segments are defined based on the event similarity. Given two segments L 1 = e 11 e 12 :::e 1 l , L 2 = e 21 e consider the number of dissimilar events in L 1 and L 2 . If the number of dissimilar event pairs is at most k , then L 1 and L 2 are similar. This definition is also called k -dissimilar: where and is a user-defined threshold for the event similarity. The k -dissimilar corresponds to the well-known k -mismatch or k -error in the subsequence matching problem [20]. The locality-sensitive hashing (LSH) [14] with the Min-Hash [10] function is a common scheme for the similarity search over texts and documents. LSH is a straightforward solution for our problem. We can consider each segment as a small  X  X ocument X  by concatenating its event messages. Fig-ure 1 shows a textual event sequence S = e 1 e 2 :::e i +1 where e i is a textual event. In this sequence, every 4 adjacent event messages are seen as a X  X ocument X , such as L i +1 , L and so on. The traditional LSH with the Min-Hash function can be utilized on these small  X  X ocuments X  to speed up the similar search. This solution is called LSH-DOC as a base-line method in this paper. However, this solution ignores the order information of events, because the similarity score obtained by the Min-Hash does not consider the order of elements in each  X  X ocument X .

To preserve the order information, we can distribute the hash functions to individual regions of segments. For exam-ple, the length of the indexed segment is 4, and we have 40 hash functions. We assign every 10 hash functions to every event in the segment. Then, each hash function can only b e used to index the events from one region of the segment. Figure 2 shows a sequence S with several segments L i +1 ,..., L i +4 , where p 1 ,..., p 4 are 4 regions of each segment and each region contains one event. Every p j has 10 hash functions to compute the hash values of the contained event, j = 1 ; :::; 4. If the hash signatures of two segments are identical, it is probably that every region X  X  events are similar. Thus, the order information is preserved. This solution is called LSH-SEP as another baseline method in this paper.
 k -dissimilar segments are two segments which contain at most k dissimilar events inside. To search the k -dissimilar segments, a common approach is to split the query sequence Q into k + 1 non-overlapping segments. If a segment L has at most k dissimilar events to Q , then there must be one seg-ment of Q which has no dissimilar event with its correspond-ing region of L . Then, we can use any search method for exact similar segments to search the k -dissimilar segments. This idea is applied in many biological sequence matching algorithms [5]. But there is a drawback for the two previ-ous potential solutions: they all assume that the length of indexed segments l is equal to the length of query sequence |
Q | . The query sequence Q is given by the user at runtime, so | Q | is not fixed. However, if we do not know the length of the query sequence Q in advance, we cannot determine the appropriate segment length l for building the index. If l &gt; | Q | , none of the similar segments could be retrieved correctly. If l &lt; | Q | , we have to split Q into shorter subseg-ments of length l , and then query those shorter subsegments instead of Q . Although all correct similar segments can be retrieved, the search cost would be large, because the sub-segments of Q are shorter than Q and the number of retrieve candidates is thus larger [20]. Figure 3 shows an example for the case l &lt; | Q | . Since the length of indexed segments is l and less than | Q | , LSH-DOC and LSH-SEP have to split Q into subsegments L 1 , L 2 and L 3 , | L i | = l , i = 1 ; ::; 3. Then, LSH-DOC and LSH-SEP use the three subsegments to query the segment candidates. If a segment candidate is similar to Q , its corresponding region must be similar to a subsegment L i , but not vice versa. Therefore, the acquired candidates for L i must be more than those for Q . Scanning a large number of candidates is time-consuming. Therefore, the optimal case is l = | Q | . But | Q | is not fixed at runtime.
In this section, we present the suffix matrix method and discuss its theoretical performance.
Let h be a hash function from LSH family. h maps an event to an integer, h : E X  X  h , where E is the universe of textual events, and Z h is the universe of hash values. In suf-fix matrix, Min-Hash [10] is the hash function. By taking a Min-Hash function h , a textual event sequence S = e 1 :::e mapped into a sequence of hash values h ( S ) = h ( e 1 ) :::h ( e Suppose we have m independent hash functions, we can have m distinct hash value sequences. Then, we create m suffix arrays from the m hash value sequences respectively. The suffix matrix of S is constructed by the m suffix arrays, where each row is a suffix array.
 Definition 2. (Suffix Matrix) Given a sequence of events S = e 1 :::e n and a set of independent hash functions H = { h 1 ; :::; h m } , let h i ( S ) be the sequence of hash values, i.e., h ( S ) = h i ( e 1 ) :::h i ( e n ) . The suffix matrix of S is M [ A 1 ; :::; A T m ] T , where A T i is the suffix array of h i = 1 ; :::; m .
 We illustrate the suffix matrix by an example as follows:
Example 1. Let S be a sequence of events, S = e 1 e 2 e 3 H is a set of independent hash functions for events, H = { h 1 ; h 2 ; h 3 } . For each event and hash function, the computed the hash value is shown in Table 1.

L et h i ( S ) denote the i -th row of Table 1. By sorting the suffixes in each row of Table 1, we could get the suffix matrix M For instance, the first row of M S;m : 3021 , is the suffix array of h 1 ( S ) = 0210 .
 There are a lot of efficient algorithms for constructing the suffix arrays [15, 23, 18]. The simplest algorithm is sorting all suffixes of the sequence with a time complexity O ( n log n ). Thus, the time complexity of constructing the suffix matrix M
S;m is O ( mn log n ), where n is the length of the historical sequence and m is the number of hash functions.
Similar to the traditional LSH, the search algorithm based on a suffix matrix consists of two steps. The first step is to acquire the candidate segments. Those candidates are po-tential similar segments to the query sequence. The second step is to filter the candidates by computing their exact sim-ilarity scores. Since the second step is straightforward and is the same as the traditional LSH, we only present the first step of the search algorithm.

Given a set of independent hash functions H = { h 1 ; :::; h and a query sequence Q = e q 1 e q 2 :::e qn , let Q H = [ h M S;m ( i ) and Q H ( i ) denote the i -th rows of M S;m and Q resp ectively, i = 1 ; :::; m , j = 1 ; :::; n . Since M S;m fix array, we obtain these entries that matched with Q H ( i ) by a binary search. M S;m has m rows, we apply m binary searches to retrieve m entry sets. If one segment appears at least r times in the m sets, then this segment is considered to be a candidate. Parameters r and m will be discussed at a later stage of this section.

Algorithm 1 states the candidates search algorithm. h ( i ) is the i -th hash function in H . Q h i is the hash-value se-quence of Q mapped by h i . SA i is the i -th row of the suffix matrix M S;m , and SA i [ l ] is the suffix at position l in SA CompareAt ( Q h i , SA i [ l ]) is a subroutine to compare the or-der of two suffixes Q h i and SA i [ l ] for the binary search. If Q i is greater than SA i [ l ], it returns 1; if Q h i is smaller than SA i [ l ], it returns  X  1; otherwise, it returns 0. Extract ( Q SA i ; pos ) is a subroutine to extract the segments candidates from the position pos . Since H has m hash functions, C [ L ] records the number of times that the segment L is extract-ed in the m iterations. The final candidates are only those segments which are extracted for at least r times. The time cost of this algorithm will be discussed in Section 3.3. A lgorithm 1 SearchCandidates ( Q; ) 1: Create a counting map C 2: for i = 1 to | H | do 5: lef t  X  0, right  X  X  SA i | X  1 7: continue 8: end if 10: continue 11: end if 12: pos  X  X  X  1 13: // Binary search 14: while right  X  lef t &gt; 1 do 15: mid  X  X  X  ( lef t + right ) = 2  X  17: if ret &lt; 0 then 18: right  X  mid 19: else if ret &gt; 0 then 20: lef t  X  mid 21: else 22: pos  X  mid 23: break 24: end if 25: end while 26: if pos =  X  1 then 27: pos  X  right 28: end if 29: // Extract segment candidates 31: C [ L ]  X  X  [ L ] + 1 32: end for 33: end for 34: for L  X  X  do 35: if C [ L ] &lt; r then 36: del C [ L ] 37: end if 38: end for
If a segment L of S is returned by the Algorithm 1, we call L is reached by this algorithm. We illustrate how the binary search works for one hash function h i  X  H by the following example.

Example 2. Given an event sequence S with a hash func-tion h i  X  H , we compute the hash value sequence h i ( S ) shown in Table 2. Let the query sequence be Q , and h i ( Q ) = 31 , where each digit represents a hash value. The sorted su ffixes of h i ( S ) are shown in Table 3. We use h i ( Q ) = 31 to search all matched suffixes in Table 3. In Algorithm 1, by using the binary search, we could find the matched suffix : 310 . Then, the Extract subroutine probes the neighborhood of suffix 310 , to find all matched suffixes with h i ( Q ) . Final-ly, the two segments at position 4 and 1 are extracted. If the two segments are extracted for at least r independent hash functions, then the two segments are the final candidates re-turned by the Algorithm 1.

Lemma 1. Given an event sequence S and a query event sequence Q , L is a segment of S , | L | = | Q | , 1 and 2 two thresholds for similar events, 0  X  2 &lt; 1  X  1 , then: where F (  X  ; n; p ) is the cumulative distribution function of Bi-nomial distribution B ( n; p ) , and r is a parameter for Algo-rithm 1.

Proof. Let X  X  first consider the case N dissim ( L; Q; 1 ) = 0, which indicates every corresponding events in L and Q are similar and the similarity is at least 1 . The hash func-tion h i belongs to the LSH family, so we have P r ( h i ( e h ( e 2 )) = sim ( e 1 ; e 2 )  X  1 . L and Q have | Q | events, so for one hash function, the probability that hash values of all those events are identical is at least | Q | 1 . Once those hash values are identical, L must be found by a binary search over one suffix array in M S;m . Hence, for one suffix ar-ray, the probability of L being found is | Q | 1 . M S;m has m suffix arrays. The number of those suffix arrays that L is found follows the Binomial distribution B ( m; | Q | 1 ). Then, the probability that there are at least r suffix arrays that L is reached is 1  X  F ( r ; m; | Q | 1 ) = F ( m  X  r ; m; 1 second case that N dissim ( L; Q; 2 )  X  k indicates there are at least dissimilar k events and their similarities are less than . The probability that hash values of all those events in L and Q are identical is at most k 2 . The proof for this case is analogous to that of the first case.
Lemma 1 is to ensure that if a segment L is similar to the query sequence Q , then it is very likely to be reached by our algorithm; if L is dissimilar to the query sequence Q , then it is very unlikely to be reached. The probabilities shown in this lemma are the false negative probability and the false positive probability. The choice of r controls the tradeoff between the probabilities. F-measure is a combined measurement for the two factors [26]. The optimal r is the one that maximizes the F-measure score. Since r can only be an integer, we can enumerate all possible values of r from 1 to m to find the optimal r .

However, this algorithm cannot handle the case that if there are two dissimilar events inside L and Q . The algo-rithm narrows down the search space step by step according to each element of Q . A dissimilar event in Q would lead the algorithm to incorrect following steps.
Figure 4 shows an example of a query sequence Q and a segment L . There is only one dissimilar event pair between Q  X 1133 X  and L  X 1933 X , which is the second one,  X 9 X  in L with  X 1 X  in Q . Clearly, the traditional binary search can-not find  X 1933 X  by using  X 1133 X  as the query. To overcome this problem, a straightforward idea is to skip the dissimilar event between Q and L . However, the dissimilar event can be any event inside L . We do not know which event is the dissimilar event to skip before knowing Q . If two similar segments are allowed to have at most k dissimilar events, the search problem is called the k -dissimilar search. Our proposed method is summarized as follows: Offline Step: Online Step:
A sequence mask is a sequence of bits. If these bits are randomly and independently generated, this sequence mask is a random sequence mask.

Definition 3. A random sequence mask is a sequence of random bits in which each bit follows Bernoulli distribution with parameter : P ( bit = 1) = , P ( bit = 0) = 1  X  , where 0 : 5  X  &lt; 1 .

Figure 5 shows a hash-value sequence h ( S ) and two ran-dom sequence masks: M 1 and M 2 . M i ( h ( S )) is the masked sequence by AND operator: h ( S ) AND M i , where i = 1 ; 2. White cells indicate the events that are kept in M i ( h ( S )), and dark cells indicate those events to skip. The optimal mask is the one such that all dissimilar events are located in the dark cells. In other words, the optimal mask is able to skip all dissimilar events. We call this kind of random sequence masks as the perfect sequence masks. In Figure 5, there are 2 dissimilar events in S : the 4th event and the 8th event. M 1 skips the 4th event and the 8th event in their masked sequences, so M 1 is a perfect sequence mask. Once we have a perfect sequence mask, previous search algorithms can be applied on those masked hash value sequences with-out considering dissimilar events.
 Lemma 2. Given an event sequence S , a query sequence Q , and f independent random sequence masks with param-eter , let L be a segment of S , | Q | = | L | . If the number of dissimilar event pairs of L and Q is k , then the proba-bility that there are at least m perfect sequence masks is at least F ( f  X  m ; f; 1  X  (1  X  ) k ) , where F is the cumulative probability function of Binomial distribution.

Proof. Since each bit in each mask follows the Bernoulli distribution with parameter , the probability that the cor-responding bit of one dissimilar even is 0 is 1  X  in one mask. Then, the probability that all corresponding bits of k dissimilar events are 0 is (1  X  ) k in one mask. Hence, the probability that one random sequence mask is a perfect se-quence mask is (1  X  ) k . Then, F ( f  X  m ; f; 1  X  (1  X  ) probability for this case happens m times in f independent random sequence masks.
A randomly masked suffix matrix is a suffix matrix, where each suffix array is masked by a random sequence mask. We use M S;f; to denote a randomly masked suffix matrix, where S is the event sequence to index, f is the number of independent LSH hash functions, and is the parameter for each random sequence mask. Note that, M S;f; still consists of f rows by n = | S | columns.

Lemma 3. Given an event sequence S , a randomly masked suffix matrix M S;f; of S and a query sequence Q , L is a segment of S , | L | = | Q | . If the number of dissimilar events between L and Q is at most k , then the probability that L is r eached by Algorithm 1 is at least P r ) ; where and r are parameters of Algorithm 1.

This probability combines the two previous probabilities in Lemma 1 and Lemma 2. m becomes a hidden variable, which is the number of perfect sequence masks. By consider-ing all possible m , this lemma is proved. Here the expected number of kept events in every | Q | events by one random sequence mask is | Q | X  .
Given an event sequence S and its randomly masked suffix matrix M S;f; , n = | S | , the cost of acquiring candidates mainly depends on the number of binary search on suffixes. Recall that M S;f; is f by n . Each row of it is a suffix array. f binary searches must be executed. Each binary search cost is log n . The total cost of acquiring candidates is f log n .
The cost of filtering candidates mainly depends on the number of candidates acquired. Let Z h denote the universe of hash values. Given an event sequence S and a set of hash functions H , Z H;S denotes the set of hash values output by each hash function in H with each event in S . Z H;S  X  X  h because some hash value may not appear in the sequence S . In average, each event in S has Z = |Z H;S | distinct hash values. Let Q be the query sequence. For each suffix array in M S;f; , the average number of acquired candidates is: Th e total number of acquired candidates is at most f  X  N
Candidate . A hash table is used to merge the f sets of candidates into one set. Its cost is f  X  N Candidate . To sum up the two parts, given an interleaved suffix matrix M S;f; and a query sequence Q , the total search cost is Wh y the potential solutions are not efficient? For potential solutions (i.e., LSH-DOC and LSH-SEP) and suffix matrix, the second part of cost is the major cost of the search. Here we only consider the number of acquired can-didates to compare the analytical search cost. The average number of acquired candidates by LSH-DOC and LSH-SEP is at least: Wh en | Q | X   X  log Z f + | Q | = ( k +1), f  X  N Candidate Z depends on the number of 2-shinglings, which is approxi-mated to the square of the vocabulary size of log messages. Hence, Z is a huge number, log Z f can be ignored. Since  X  0 : 5, k  X  1, we always have | Q | X   X | Q | = ( k + 1). There-fore, the acquired candidates of suffix matrix are less than or equal to those of LSH-DOC and LSH-SEP.
The parameters f and balances the search costs and search result accuracy. These two parameters are decided in the offline step before building the suffix matrix. Let Cost max be the search cost budget, the parameter choos-ing problem is to maximize P r reach subject to Cost search Cost max . A practical issue is that the suffix matrix is con-structed in the offline phase, but | Q | and can only be known in the online phase. A simple approach to find out the opti-mal f and is looking at the historical queries to estimate |
Q | and . This procedure can be seen as a training pro-cedure. Once the two offline parameters are obtain, other parameters are found by solving the maximization problem. The objective function P r reach is not convex, but it can be solved by the enumeration method since all tuning parame-ters are small integers.

The next question is how to determine Cost max . We can choose Cost max according to the average search cost curve. Figure 6 shows a curve about the analytical search cost and the probability P r reach , where m =  X  Cost search = (log n + choose Cost max between 100 and 200, because larger search costs would not significantly improve the accuracy any more. Fi gure 6: Average Search Cost Curve ( n = 100 K; |Z H;S | = 16 ; = 0 : 5 ; | Q | = 10 ; = 0 : 8 ; k = 2 )
The time complexity of the offline suffix matrix construc-tion is O ( n log n ). The online search is O (log n ). The only problem for scaling suffix matrix when the memory cost ex-ceeds the limitation. In this case, the suffix matrix can be stored in the external memory or a distributed system.
In this section, we conduct experiments on real system event logs to evaluate our proposed method. We implement LSH-DOC, LSH-SEP and our method in Java 1.6. Table 4 summarizes our experimental machine.
Our experimental system logs are collected from two dif-ferent real systems. Apache HTTP error logs are collected from the server machines in the computer lab of a research center and have about 236,055 log messages. Logs of Thun-derBird [4] are collected from a supercomputer in Sandi-a National Lab. The first 350,000 log messages from the ThunderBird system logs are used for this evaluation. Testing Queries Each query sequence is a segment randomly picked from th e event sequence. Table 5 lists detailed information about the 6 groups, where | Q | indicates the length of the query sequences. The true results for each query are obtained by the brute-force method, which scans through every segment of the sequence one by one to find all true results. We compare our method with baseline methods LSH-DOC, LSH-SEP stated in Section 2.1. The two methods are both LSH based methods applying to the sequential data. In or-der to handle the k -dissimilar approximation queries, the indexed segment length l for LSH-DOC and LSH-SEP can be at most | Q | = ( k + 1) = 3, so we set l = 3.
Suffix matrix and LSH based methods all consist of two steps. The first step is to search segment candidates from its index. The second step is filtering acquired candidates by computing their exact similarities. Because of the second step, the precision of the search results is always 1.0. Thus, the quality of results only depends on the recall. By appro-priate parameter settings, all the methods can achieve high recalls, but we also consider the associated time cost. For a certain recall, if the search time is smaller, the performance is better. An extreme case is the brute-force method that always has the 1.0 recall, but it has to visit all segments of the sequence, so the time cost is huge. We define the recall ratio as a normalized metric for evaluating the goodness of the search results:
RecallRatio = where recall min is a user-specified threshold for the mini-mum acceptable recall. If the recall is less than recall min the search result is then not acceptable by the user. In our evaluation, recall min = 0 : 5, which means any method should capture at least half of the true results. The unit of the search time is millisecond. RecallRatio is expressed as the portion of true results obtained per millisecond. Clearly, RecallRatio is higher, the performance is better.
LSH-DOC, LSH-SEP and suffix matrix have different pa-rameters. We vary the value of each parameter in each method, and then select the best performance of each method to compare. LSH-DOC and LSH-SEP have two parameters to set, which are the length of hash vectors b and the num-ber of hash tables t . b varies from 5 to 35. t varies from 2 to 25. We also consider the different number of buckets for LSH-DOC and LSH-SEP. Due to the Java heap size limita-tion, the number of hash buckets is fixed to be 8000. For suffix matrix, r is chosen according to Section 3.1.2. f and m vary from 2 to 30. varies from 0.5 to 1.
 Figure 7 shows the RecallRatios for each testing group. Overall, suffix matrix achieves the best performance on the two data sets. However, LSH based methods outperform suffix matrix on short queries (TG1). Moreover, in Apache Logs with TG4, LSH-SEP is als better than suffix matrix. To find out the reason why in TG1 suffix matrix performs worse than LSH-DOC or LSH-SEP, we record the number of acquired candidates for each method and the number of true results. Figure 8 shows the actual acquired candidates for each testing group with each method. Table 6 shows the numbers of true results for each testing group. From Figure 8, we can see that suffix matrix acquired much more candi-dates than other methods in TG1. In other words, suffix ma-trix has a higher collision probability of dissimilar segments in its hashing scheme. To overcome this problem, a com-mo n trick in LSH is to make the hash functions be X  X tricter X . For example, there are d + 1 independent hash functions in LSH family, h 1 ,..., h d and h . We can construct a  X  X tricter X  e probability of h i is P r [ h i ( e 1 ) = h i ( e 2 )] = sim ( e which can be large if is large, i = 1 ; :::; d . But the collision probability of h  X  is
Figure 9 shows the performance of the suffix matrix by using  X  X tricter X  hash functions (denoted as  X  X uffixMatrix(Strict) X ) in TG1. Each  X  X tricter X  hash function is constructed by 20 independent Min-Hash functions. The testing result shows,  X  X uffixMatrix(Strict) X  X utperforms all other methods for both Thunderbird logs and Apache logs in TG1. Table 7 are the parameters and other performance measures of  X  X uffixMa-trix(Strict) X . By using  X  X tricter X  hash functions, the suffix matrix reduces 90% to 95% of previous candidates. As a re-sult, the search time becomes much smaller than before. The choice of the number of hash functions for a  X  X tricter X  hash function, d , is a tuning parameter and determined by the data distribution. Note that the parameters of LSH-DOC and LSH-SEP in this test are already tuned by varying the values of b and t .

To verify Lemma 3, we vary each parameter of suffix ma-trix and test the recall of search results. We randomly sam-ple 100,000 log messages from the ThunderBird logs and randomly pick 100 event segments as the query sequences. The length of each query sequence is 16. Other querying criteria are k = 5 and = 0 : 5. Figure 10 shows the recalls by varying each parameter. Figure 10(a) shows that the in-crease of m will improve the recall. Figure 10(c) verifies that if r becomes larger, the recall will decrease. As for Figure 10(b), since the random sequence masks are randomly gener-ated, the trends of the recall are not stable and a few jumps are in the curves. But generally, the recall curves drop down when we enlarge the for the random sequence mask. To sum up, the results shown by Figure 10 can partially verify Lemma 3 proposed in Section 3.
Space cost is an important factor for evaluating these methods [9] [15] [13] [20]. If the space cost is too large, the index cannot be loaded into the main memory. To ex-clude the disk I/O cost for the online searching, we load all event messages and index data into the main memory. The total space cost can be directly measured by the allocated heap memory size in JVM. Note that the allocated memory does not only contain the index, it also includes the original log event messages, 2-shinglings of each event message and the corresponding Java objects information maintained by JVM. We use Java object serialization to compute the exact size of the allocated memory. Figure 11 shows the total used memory size for each testing group. The parameters of each method are the same as in Figure 7. The total space costs for LSH-SEP and suffix matrix are almost the same because they both build the hash index for each event message only once. But LSH-DOC builds the hash indices for each event l times since each event is contained by l continuous segments, where l is the length of the indexed segment and l = 3.
Indexing time is the time cost for building the index. Fig-ure 12 shows the indexing time for each method that has the same parameters for Figure 7. The time complexities of LSH-DOC and LSH-SEP are O ( nlbt  X  c h ) and O ( nbt  X  where n is the number of event messages, l is the indexed segment length, b is the length of the hash vector, t is the number of hash tables, and c h is the cost of Min-Hash func-tion for one event message. Although for each testing group, the selected LSH-DOC and LSH-SEP may have different b and t , in general LSH-SEP is more efficient than LSH-DOC. The time complexity of suffix matrix for building the index is O ( mn log n + mn  X  c h ), where m is the number of rows of the suffix matrix. It seems that the time complexity of suffix matrix is bigger than LSH based methods if we only consider n as a variable. However, as shown in Figure 12, suffix matrix is actually the most efficient method in build-ing index. The main reason is m  X  b  X  t . In addition, the time cost of Min-Hash function, c h , is not small since it has to randomly permute the 2-shinglings of an event message.
The similarity search problem in low-dimensional data s-paces has been studied extensively. A number of tree struc-ture based algorithms are devised to support the similarity queries and nearest neighbors queries, such as R-Tree [16], KD-Tree [8] and SR-Tree [17]. These previous algorithm-s are known to work well in low-dimensional data spaces. But for high-dimensional data spaces, their search time cost or indexing space cost grows to an exponential number of the dimensionality. In textual information retrieval and im-age processing domains, the descriptor of a data object is usually a high-dimensional vector. Hence, those tree struc-tured based algorithms are not appropriate in these domains. Locality-Sensitive Hashing (LSH) is a randomized approx-imate algorithm for the similar search in high-dimensional data space [14, 6]. It is applicable for high dimensional da-ta and has been successfully used in image data or textual data. Min-Hash is a widely used hash function for textual data [10], which can quickly estimate the sim ( x; y ) of x and y . In natural language processing, a w-shingling is a set of unique contiguous subsequences of words/terms in a docu-ment. The similarity function sim ( x; y ) is usually chosen as the Jaccard similarity over the w-shinglings of x and y .
Substring search in sequential data has been studied for years. Suffix tree and suffix array are two typical method-s for on-line searching matched substrings over a sequence [35, 23]. By using a binary search over the suffix array, the method can find matched substring in O (log n ), where n is the length of the string. Compressed suffix arrays and BWT-based compressed full-text indices make further effort-s to reduce the search time and space cost based on suffix arrays [15, 12]. Time series data is real-valued sequence da-ta. A lot of efficient similarity search methods are proposed and studied for time series data [25, 21]. But their target is a set of data points, rather than a set of segments of the sequence. Moreover, each data point in time series is a real-valued vector, not a textual message or document.
In system management, log and system event analysis is a fundamental method to maintain, diagnose and optimize large production systems [36, 37, 33, 31, 34, 32]. Log event search as a basic functionality is embedded in many system management, log analysis and system monitoring products [1, 3, 2]. Users can input relational query conditions or a set of keywords to query related system event logs in history. This kind of log search has no difference with a traditional database query or a keywords search. Their search target is single events, not continuous subsequence of events.
This paper proposes suffix matrix to search similar seg-ments over large textual sequences. It combines ideas of lo-cality sensitive hashing and suffix arrays to reduce the search space. By using random sequence mask proposed in this pa-per, this method can have a high probability to reach all k -dissimilar segments without increasing much search costs. We conduct experiments on real system log data and exper-imental results show that our proposed method outperforms alternative methods using existing techniques.
 The work is supported in part by US National Science Foun-dation under grants HRD-0833093, CNS-1126619, and IIS-1213026, by Army Research Office under grant number W911NF-12-1-0431, and by the National Natural Science Foundation of China under Grant No. 61070151.
