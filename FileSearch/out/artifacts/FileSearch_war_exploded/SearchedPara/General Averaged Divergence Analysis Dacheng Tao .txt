 An important subspace method is the Fisher X  X ao linear discriminant analysis (LDA) , which has been successfully applied in many fields such as biometrics, bioinformatics, and multimedia retr ieval. However, LDA has a critical drawback: the projection to a subspace tends to merge those classes that are close together in the original feature space. If the separated classes are sampled from Gaussian distributions, all with identical covariance matrices, then LDA maximizes the mean value of the Kullback X  X eibler (KL) divergences between the different classes. We generalize this point of view to obtain a framework for choosing a subspace by 1) generalizing the KL divergence to the Bregman divergence and 2) generalizing the arithmetic mean to a general mean. The framework is named the general averaged divergence analysis (GADA) . Under this GADA framework, a geometric mean divergence analysis (GMDA) method based on the geometric mean is studied. A large number of experiments based on synthetic data show that our method significantly outperforms LDA and several representative LDA extensions. has a problem in merging classes that are close together in the original feature space, as shown in Fig. 1. This is referred to as the class separation problem in this paper. As pointed out by McLachlan [14], Loog et al. [11], and reduces the recognition rate. Fig.1 shows an example in which LDA does not select the optimal subspace for pattern classification. To improve its performance, a weighted LDA (WLDA) [8] is introduced. However, the recognition rate of WLDA is sensitive to the selection of the weighting function. Loog et al. [11] developed another weighting method for LDA, namely the approximate pairwise accuracy criterion (aPAC). The advantage of aPAC is that the projection matrix can be obtained by the eigenvalue decomposition. problem, we first generalize LDA to obtain a general averaged divergence analysis (GADA). If different classes are assumed to be sampled from Gaussian densities with different expected values but identical covariances, then LDA maximizes the mean value of the Kullback X  X eibler (KL) divergences [3] between the different pairs of densities. Our generalization of LDA has two aspects: 1) the KL divergence is replaced by the Bregman divergence [2]; and 2) the arithmetic mean is replaced by a general mean function. By choosing selection algorithms is obtained, with LDA included as a special case. investigate the effectiveness of the geometric mean and KL divergence based subspace selection for solving the class separation problem. The geometric mean amplifies the effects of the small divergences and at the same time reduces the effects of the large divergences. The method is named the geometric mean divergence analysis (GMDA). low dimensional subspace in which the different classes of measurements are well separated. The subspace is spanned by a set of vectors, the columns of a matrix [ ] that a training set of measurements is available. The n measurements S and the within X  X lass scatter matrix where  X x is the mean vector of the total training set. The projection matrix * W of LDA is defined by discriminant analysis [9]; otherwise LDA is known as Rao discriminant analysis [16]. from Gaussian densities with different expected values but identical covariances, then LDA maximizes the mean value of the KL divergences between the different pairs of densities. We propose a framework, the General Averaged Divergence Analysis , for choosing a discriminative subspace by: 1) generalizing the distortion measure from the KL divergence to the Bregman divergence, and 2) generalizing the arithmetic mean to a general mean function. Definition 1 (Bregman Divergence): Let : US R  X  be 
SR +  X  . The first derivative of U is U  X  , which is a monotone function. The inverse function of U  X   X   X  = . The sample probability of the i th class is p pyi == x . The difference at () function U and the tangent line to U at  X  X  is given by: is where d  X  is the Lebesgue measure. The right-hand side of (4) is also called the U X  X ivergence [15]. Because U is a convex function, () () () , dp q  X  X  is non X  X egative. Consequently, the Bregman divergence is non X  X egative. Because () () () , dp q  X  X  is in general not symmetric, the Bregman divergence is also not symmetric. Detailed information about the Bregman divergence can be found in [15]. reduces to the usual KL-divergence, pN x X   X   X  , where class samples and matrix of the i th class, the KL divergence [3] is KL p p d N between the projected densities () | T p yi = Wx and general mean, where ()  X   X  is a strict monotonic real-valued increasing function of ()  X   X  ; class (usually, we can set The general averaged divergence function measures the average of all divergences between pairs of classes in the subspace. We obtain the projection matrix * W by maximizing the general averaged divergence function V  X  W over W for a fixed optimization algorithm for subspace selection based on (8) is given in Table 1. Note that, usually, the concavity of V  X  W cannot be guaranteed. To reduce the effects of local maxima [1], we choose a number of different initial projection matrices, perform optimizations on them, and then select the best one. spanned by the columns of W , then W can be replaced by WM where M is an mm  X  matrix in which the columns of WM are orthogonal. mean based method for choosing a subspace, Observation 1: LDA maximizes the arithmetic mean of the KL divergences between all pairs of classes, under the assumption that the Gaussian distributions for the different classes all have the same covariance matrix. The projection matrix * W in LDA can be obtained by maximizing a particular () V  X  W . Proof. the i th class and the j th class in the projected subspace with the assumption of equal covariance matrices ( =  X  ) is as follows: Dpp  X  =+ Then, we have Because 1 [10], and obtained by the generalized eigenvalue decomposition. Example: Decell and Mayekar [5] maximized the arithmetic mean of all symmetric KL divergences between all pairs of classes in the projected subspace. The symmetric KL divergences are given by 
Input: Training samples ( 1 ic  X  X  X  ) and j is the j th sample in the i (1 is the dimension of of different initial values for the projection matrix. 
Output: Optimal linear projection matrix * W . 1. for 1: mM = { 2. 3. while 4. 5. 1 tt  X + 6. }// while on line 3 7. }// for on line 1 8. tr tr .
SKLp p KLp p KLp p =+++ X  X  X   X  X   X  X   X   X   X  X   X  X  maximizing the arithmetic mean of all KL divergences. discriminant analysis (ODA) based on the same objective function used in [5], but used iterative majorization to obtain a solution. Iterative majorization speeds up the training stage. Furthermore, they generalized ODA for a multimodal case as the multimodal ODA (MODA) by combining it with Gaussian Mixture Models (GMM) learnt by a normalized cut for the multimodal case. Each class is modelled by a GMM. measurement vectors in a given class are sampled from a single Gaussian distribution. This assumption often fails in large real X  X orld data sets, such as those used for multi X  X iew face/gait recognition, natural image classification or texture classification. modeled by a GMM. Many methods for obtaining GMMs are described in the literature. Examples include KMeans [6], GMM with expectation X  X aximization (EM) [6], graph X  X ut [17], and spectrum clustering. Unfortunately, these methods are not adaptive, in that the number of subclusters must be specified, and some of them (e.g., EM and KMeans) are sensitive to the initial values. In our algorithm we use a recently introduced GMM X  X M like algorithm proposed by Figueiredo and Jain [7], which was named the GMM X  X J method. The reasons for choosing GMM-FJ are as follows: it finds the number of values of the parameters than EM; and it can avoid the boundary of the parameter space. We assume that the measurements in each class are sampled from a GMM and the projection matrix W can be obtained by maximizing the general averaged divergences, which measure the averaged distortion between any pair of subclusters in different classes, i.e., subcluster in the i th class; () || kl between the k th subcluster in the i th class and the l subcluster in the j th class. divergences is used to find a suitable subspace to project the feature vectors. The main benefit of using the arithmetic mean is that the projection matrix can be obtained by the generalized eigenvalue decomposition. However, LDA is not optimal for multiclass classification [14] because of the class separation problem mentioned in Section I. Therefore, it is useful to investigate other choices of  X  in (8). increases the effects of the small divergences and at the same time reduces the effects of the large divergences. On setting () () log x x  X  = in (9) the generalized geometric mean of the divergences is obtained. The required subspace * W is given by generalized geometric mean is upper bounded by the arithmetic mean of the divergences, i.e., divergences. For example, in the special case of for all , ij , geometric mean as an example for practical applications. optimizing the logarithm of (14), we have class and the j th class in the projected subspace, mean and the KL divergence based subspace selection algorithm based on Table 1, we need the first order derivative of () L W , and  X =  X  obtained from (13). geometric mean divergence analysis (GMDA), and compare GMDA with LDA [8], aPAC [11], WLDA (similar to aPAC, but with a different weighting function), HLDA [12], ODA [4], and MODA [4]. We use the selection methods for the heteroscedastic problem [12], we generate two classes such that each class has 500 samples, drawn from a Gaussian distribution. The two classes have identical mean values but different covariances. As shown in Fig. 2, LDA, aPAC, and WLDA separate class means without taking the differences between covariances into account. In contrast, HLDA, ODA, and GMDA consider both the differences between class means and the differences between class covariances, so they have less training errors, as shown in Fig. 2. distribution of a class using a GMM, because samples in the class may be drawn from a multimodal distribution. To demonstrate the classification ability of the multimodal extension of GMDA, we generate two classes; each class has two subclusters, and samples in each subcluster are drawn from a Gaussian. Fig. 3 shows the selected subspaces of different methods. In this case LDA, WLDA, and aPAC do not select the suitable subspace for classification. However, the multimodal extensions of ODA and GMDA can find the suitable subspace. Furthermore, although HLDA does not take account of multimodal classes, it can select the suitable subspace. This is because in this case the two classes have similar class means but significantly different class covariance matrices when each class is modeled by a single Gaussian. For complex cases, e.g., when each class consists of more than 3 subclusters, HLDA will fail to find the optimal subspace for classification. significantly reduce the classification errors caused by the too strong effects of the large divergences between certain classes. To demonstrate this point, we generate three classes and the samples in each class are drawn from a Gaussian distribution. Two classes are close together and the third is far away. However, LDA, HLDA, and ODA do not give good results. The aPCA and WLDA algorithms are better than LDA but neither of them gives the suitable projection direction. The results obtained from aPCA are better than those obtained from WLDA, because aPAC uses a better weighting strategy than WLDA. which is a generalization of the data generation model used by Torre and Kanade [4], to evaluate MGMKLD in terms of accuracy and robustness. The accuracy is measured by the average error rate and the robustness is measured by the standard deviation of the classification error rates. In this data generation model, there are five training/testing sets, the data generator gives 200 samples for each of the five classes (therefore, 1,000 samples in total). Moreover, the samples in each class are obtained from a Gaussian. Each Gaussian density is a linear transformation of a  X  X tandard normal distribution X . The linear transformations are defined by sample in this class, and corresponding normal distribution. The as follows: () () matrix sampled from () 0, 5 N . Based on this data generation model, 800 groups (each group with the training and testing samples) of synthetic data are generated from the model. first utilized to select a given number of features. Then the Mahalanobis distance [6] and the nearest neighbour rule are used to examine the accuracy and robustness of GMDA in comparison with LDA and its extensions. The baseline algorithms are LDA, aPAC, WLDA, HLDA, and ODA. times based on randomly generated data sets. The experimental results are reported in Tables 2-5. Tables 2 and 4 show the average error rates of LDA, aPAC, WLDA, HLDA, ODA, and GMDA based on the Mahalanobis distance and the nearest neighbour rule, respectively. Herein arithmetic mean values are computed on different feature dimensions from 1 to 6 (by column). Correspondingly, the standard deviations under each condition, which measure the robustness of the classifiers, are given in Tables 3 and 5. We have twenty feature dimensions for each sample and all the samples are divided into five classes. Therefore, the maximal feature number for LDA, aPAC, and WLDA is 5 X 1=4; in contrast, HLDA, ODA, and GMDA can extract more features than LDA, aPAC, and WLDA. Based on Tables 2-5, it can be concluded that GMDA outperforms LDA, aPAC, WLDA, HLDA, and ODA, consistently. subspace selection method for classification. Let us first study the relationship between () L W , which is defined in (15), and the training error rate. Experiments are done on a randomly selected data set from 800 data sets generated at the beginning of this section. We set training iterations as 200. In Fig. 5, the left shows that the classification error rates decrease with the increase of the training iterations and the right shows that the objective function values () L W increase with the increase of the training iterations monotonically. Therefore, the classification error rates decrease with the increase of 
L W . This means that maximizing () L W will be useful to achieve a low classification error rate. between different classes change with the increasing number of training iterations, because it is helpful to deeply understand how and why GMDA reduces the class separation problem. In Fig. 6, we show how KL divergences change in GMDA over the 1 st , 2 nd , 5 20 th , and 200 th training iterations, respectively. The small KL divergences, which are less than 2, are marked with rectangles. There are 5 classes, so we can map KL divergences to a 55  X  matrix with zero diagonal values. denote it as () iteration. Because the KL divergence is not symmetric, the matrix is not symmetric, i.e., () () left 55  X  matrix), there are 8 values less than 2. In the 2 iteration (the top right 55  X  matrix), there are only 4 However, these two divergences have decreased to 1.0439 and 1.0366 in the 200 th iteration (the bottom right 55  X  matrix) in comparing with the 20 th iteration (the bottom left 55  X  matrix) to guarantee the increase of () divergences between them are very small. distributions, all with identical covariance matrices, then the Fisher X  X ao linear discriminant analysis (LDA) maximizes the mean value of the Kullback X  X eibler (KL) divergences between the different classes. We have generalized this point of view to obtain a framework for choosing a subspace by 1) generalizing the KL divergence to the Bregman divergence and 2) generalizing the arithmetic mean to a general mean. The framework is named the general averaged divergence analysis (GADA). divergence based subspace selection is then studied. LDA has a critical drawback in that the projection to a subspace tends to merge those classes that are close together in the original feature space. A large number of experiments based on synthetic data have shown that our method significantly outperforms LDA and several representative LDA extensions in overcoming this drawback. [1] S. Boyd and L. Vandenberghe, Convex Optimization , Cambridge University Press, 2004. [2] L.M. Bregman,  X  X he Relaxation Method to Find the Common Points of Convex Sets and Its Application to the Solution of Problems in Convex Programming, X  USSR Compt. Math. and Math. Phys. , no. 7, pp. 200 X 217, 1967. [3] T. M. Cover and J. A. Thomas, Elements of Information Theory . New York: Wiley, 1991. [4] F. De la Torre and T. Kanade  X  X ultimodal Oriented Discriminant Analysis, X  Int X  X  Conf. Machine Learning , 2005. [5] H. P. Decell and S. M. Mayekar,  X  X eature Combinations and the Divergence Criterion, X  Computers and Math. With Applications , vol. 3, pp. 71 X 76, 1977. Classification. John Wiley and Sons Inc. 2001. [7] M. Figueiredo and A.K. Jain,  X  X nsupervised learning of finite mixture models, X  IEEE Trans. Pattern Analysis and Machine Intelligence , vol. 24, no. 3, pp. 381 X 396, 2002. recognition (Second Edition). Academic Press. 1990. [9] R. A. Fisher,  X  X he Statistical Utilization of Multiple Measurements, X  Ann. Eugenics , vol. 8 pp. 376-386, 1938. [10] M. Loog,  X  X pproximate Pairwise Accuracy Criteria for Multiclass Linear Dimension Reduction: Generalizations of the Fisher Criterion, X  Delft Univ. Press, 1999. [11] M. Loog, R. P.W. Duin, and R. Haeb X  X mbach,  X  X ulticlass Linear Dimension Reduction by Weighted Machine Intelligence , vol. 23, no. 7, pp. 762 X 766, July 2001. [12] M. Loog and R. P.W. Duin,  X  X inear Dimensionality Reduction via a Heteroscedastic Extension of LDA: The Chernoff Criterion, X  IEEE Trans. Pattern Analysis Machine Intelligence , vol. 26, no. 6, pp. 732 X 739, June 2004. [13] J. Lu, K.N. Plataniotis, and A.N. Venetsanopoulos,  X  X ace Recognition Using LDA Based Algorithms, X  IEEE Trans. Neural Networks , vol. 14, no. 1, pp. 195 X 200, 2003. [14] G.J. McLachlan, Discriminant Analysis and Statistical Pattern Recognition , Wiley, New York, 1992. [15] N. Murata, T. Takenouchi, T. Kanamori, and S. Eguchi,  X  X nformation Geometry of U X  X oost and Bregman Divergence, X  Neural Computation , vol. 16, no. 7, pp. 1,437 X  1,481, 2004. [16] C. R. Rao,  X  X he Utilization of Multiple Measurements in Problems of Biological Classification, X  J. Royal Statistical Soc. , B, vol. 10, pp. 159-203, 1948. [17] J. Shi and J. Malik,  X  X ormalized Cuts and Image Segmentation, X  IEEE Trans. Pattern Analysis and Machine Intelligence , vol. 22, no. 8, pp. 888 X 905, Aug. 2000. classification. projection direction for classification. direction for classification. , 5 th , 10 th , 20 th , and 200 th training iterations. (Mahalanobis distance). and GMDA (Mahalanobis distance). (Nearest neighbor rule). and GMDA (Nearest neighbor rule). 
