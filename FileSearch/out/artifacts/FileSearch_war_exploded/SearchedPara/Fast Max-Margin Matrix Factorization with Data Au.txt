 For detailed derivation and analysis of Gibbs sam-pling in IBP models with a linear Gaussian likelihood, we recommend the extended version 6 of (Doshi-Velez et al., 2009). Below we concentrate on our discovery of a more computationally efficient way to sample new latent features, which achieves linear complexity with respect to the Poisson truncation level  X  .
 As indicated in Eq. (26), when sampling Z  X  i = 1 &gt; for each row of Z , we need to calculate |  X  ijk  X  The calculation of determinant for a k -by-k matrix is of complexity O ( k 3 ) with LU decomposition, and this would bring about an overall complexity of O (  X  4 ) and hence restrict our choice of the truncation level. How-ever, we may take advantage of the special form of  X  ijk i to reduce the computational cost to O (  X  ). Specifically, we find that for matrices X k of the follow-ing form: we have and by solving these recursions we obtain the closed form solution Furthermore, according to Cramer X  X  rule, we have and accordingly, Then by taking b ij = may apply Eq. (31) to calculate |  X   X  1 ijk  X  and thus may apply Eq. (33) for its calculation. With b ij and  X  ij already at hand and updated from previous steps, the incremental cost of calculating Eq. (31) and (33), and hence Eq. (26), for k i = 0 , 1 ,..., X  is thus reduced to O (  X  ).
 Below we discuss the asymptotic computational com-plexity of each iteration in our Gibbs sampling meth-ods. Specifically, to draw samples from each condi-tional distribution, there are typically two individual costs, one for the calculation of the sufficient statistics, e.g.,  X  r ij in Eq. (22), B j and b j in Eq. (23), etc., and the other for the actual drawing of the samples from the corresponding distribution. Normally, the first one is linear w.r.t. the number of the observed entries |I| , while the second one is independent of |I| but linear to the number of samples to be drawn, i.e. the num-ber of parameters in the model. We list the results in Table 3 and 4.
 Note that we use the Cholesky decomposition B  X  1 j = R j R j both to calculate b j (23) where B j v is calculated as R j \ ( R &gt; j \ v ) 7 and to draw samples from N ( b as b j + R j \ x where x  X  X  ( 0 ,I ).
 Minjie Xu xuj-10@mails.tsinghua.edu.cn Jun Zhu dcszj@mail.tsinghua.edu.cn Bo Zhang dcszb@mail.tsinghua.edu.cn Matrix factorization has been a key technique in learn-ing latent factor models for many applications such as collaborative prediction (Srebro et al., 2005; Salakhut-dinov &amp; Mnih, 2008; Zhou et al., 2010). Given a user-item preference matrix Y  X  R N  X  M , which is partially observed and usually sparse, matrix factor-ization aims to find a low-rank matrix X  X  R N  X  M that simultaneously approximates the observed entries of Y under some loss measure (e.g., the commonly used squared error) and reconstructs the missing en-tries. Max-margin matrix factorization (M 3 F) (Sre-bro et al., 2005) extends the model by adopting hinge loss, which is applicable to binary, discrete ordinal, or categorical data that are typical for a preference sys-tem, and a sparsity-inducing norm regularizer. For the binary case where Y ij  X  X  X  1 } and one predicts by b Y ij = sign( X ij ), the optimization problem of M 3 F is defined as where k X k  X  is the nuclear norm of X , I is the indices of the observed entries and h ( x ) , max(0 , 1  X  x ) is the hinge loss. Problem (1) can be equivalently formulat-ed as a semi-definite program (SDP) and learned by standard SDP solvers, but it is unfortunately very slow and scales to only thousands of users and items. An alternative M 3 F model based on a variational formulation of the nuclear norm is then proposed in (Rennie &amp; Srebro, 2005) and it solves an equiva-lent problem on the factorized form X = UV &gt; instead: where U  X  R N  X  K and V  X  R M  X  K are interpreted as the user coefficient matrix and the item factor ma-trix respectively, and K is the number of latent fac-tors. We use U i to denote the i th row of U , and V j likewise. By replacing hinge loss with a smooth sur-rogate, a gradient descent solver has been developed and it scales to millions of users and items. However the solver works with a truncated problem where K is pre-specified. Alternatively, (Xu et al., 2012) sug-gests a block-wise coordinate descent algorithm that directly works with hinge loss by use of SVMs for each row of U and V . This method achieves comparable ac-curacy with the gradient descent solver yet is almost as time-consuming. (Xu et al., 2012) also introduces a nonparametric model for M 3 F which automatically re-solves the unknown number of latent factors. However both its dependency on a truncation level for practical inference and the time-consuming SVM steps therein call for further improvement.
 In this paper, we present a novel interpretation of the M 3 F problem, that is to formalize the deterministic regularized risk minimization (RRM) problem (2) as an equivalent maximum a posteriori (MAP) estima-tion problem, and by use of the data augmentation techniques recently developed for SVMs (Polson &amp; S-cott, 2011), we are able to perform simple yet highly efficient MCMC sampling and thus drastically increase the efficiency of solving M 3 F problems. Furthermore, to bypass the model selection issue of the factorized M 3 F models (i.e. selecting the number of latent factors K ), we extend our probabilistic formulation to incor-porate Bayesian nonparametrics and build thereupon a nonparametric M 3 F model, which again enjoys a sim-ple and efficient MCMC sampling algorithm. Com-pared with the previous nonparametric M 3 F (Xu et al., 2012), which resorts to variational approximation with truncated mean-field assumptions, our sampling algo-rithm is both assumption-free and truncation-free. The paper is structured as follows. Section 2 formu-lates M 3 F as a MAP estimation problem and presents the MCMC sampling algorithm via data augmenta-tion; Section 3 presents the nonparametric M 3 F exten-sion; Section 4 presents empirical results on two preva-lent collaborative filtering data sets and demonstrate efficiency improvement. Finally, Section 5 concludes. We start with a discussion on the generic formulation of RRM as MAP estimation. 2.1. RRM as MAP, A New Look Given a set of training data X = {X n } N n =1 , many machine learning problems, including M 3 F, can be cast as solving a RRM problem generally written as where we denote the model (parameters) by M ;  X ( M ) is the regularizer which is critical to save the mod-el from over-fitting; loss; and C is the balancing factor, or regularization constant. Normally for supervised tasks where train-ing labels are available, X n = ( x n ,y n ) and where f ( M ; x ) is termed the discriminant function , which gives a prediction score s , and L ( y,s ) the loss function .
 Generally RRM is a deterministic optimization prob-lem without any resort to a probabilistic background. For example, in the case of M 3 F for binary data, we have M = ( U,V ), X = { (( i,j ) ,Y ij ) | ij  X  X } , and s = f ( U,V ; ( i,j )) = U i V &gt; j , L ( Y ij ,s ) = h ( Y In contrast, MAP estimation is backed up by Bayesian inference methodology and, given a prior distribution p ( M ) and a likelihood term L ( M|X ) , p ( X|M ) 1 , solves for the optimal model by maximizing the posterior distribution p ( M|X )  X  p 0 ( M ) L ( M|X ). Quite often we adopt the i.i.d. assumption on the data generation process so that the likelihood factorizes as p ( X|M ) = L ( M|X n ) , p ( X n |M ), the problem reads To better disclose the correspondence between R-RM (3) and MAP (6), we introduce two new concepts, namely the delegate prior and the delegate likelihood (abbreviated as dele-prior and dele-likelihood in the sequel). Given a prior-likelihood pair ( p 0 , L ), a dele-prior  X  p 0 ( M ) can be any non-negative function defined solely on M while a dele-likelihood  X  L ( M|X n ) can be any non-negative function defined jointly on X n and M (although viewed as a function merely of M just as is L ), as long as the original  X  X enuine X  prior-likelihood pair can be uniquely recovered as where the normalizing factors are  X  n ( M ) , R  X  L ( M|X n ) d X n and  X  0 , Note that although  X  L and L appear only different by a normalizing factor when viewed as a probability of X n , they can be completely different when viewed as a function of M . As for  X  p 0 and p 0 , since scaling  X  p any positive constant carries no effect on the resulting genuine prior p 0 , we can normalize  X  p 0 if necessary. For any qualified delegate prior-likelihood pair (  X  p 0 and its uniquely induced genuine pair ( p 0 , L ), or the other way round, for any genuine pair and all of its compatible delegate pairs, the MAP problem (6) remains intact since The delegate prior-likelihood pair provides an alterna-tive, presumably easier way to factorize the posterior since the dele-likelihood does not necessarily comply with the normalization constraint for X n . Now we may easily convert the RRM problem (3) into MAP estimation by setting the delegate pair as and solving the following delegate form of MAP Directly solving problem (8) does not save us any ef-fort. Yet as we shall demonstrate below, given a prop-erly augmented representation, we may resort to ex-isting probabilistic techniques, e.g., MCMC sampling methods, to perform highly efficient approximate in-ference that achieves comparable or even better per-formances. Moreover, this probabilistic interpretation naturally leads to a nonparametric Bayesian extension of the model in consideration.
 Before a full exposition of the new algorithms for M 3 F and its nonparametric extension, we provide more insights on this probabilistic interpretation. Specifically, given a prior-likelihood pair ( p 0 , L ), we can calculate the posterior distribution according to Bayes X  theorem, or alternatively by solving a functional optimization problem (Zhu et al., 2013b) where P is a space of valid probability distribu-tions; KL( q ( M ) k p 0 ( M )) is the KL-divergence; and E [log L ( M|X n )] = that to distinguish from the posterior p ( M|X ) by Bayes X  rule, we use q ( M ) to represent a posterior dis-tribution derived from our generic inference procedure. By use of any compatible delegate pair, we rewrite problem (9) in an equivalent yet more general way as 2 by ignoring a constant term log  X  0 . Now if we sub-stitute the RRM-induced delegate pair (7) into (10) and denote b  X ( q ( M )) , KL( q ( M ) k  X  p 0 ( M )) where  X  p ( M ) , e  X   X ( M ) /Z is the normalized dele-prior and b R ( q ( M ); X n ) , E q [ R ( M ; X n )], we obtain the following equivalence The significance of this alternative representation is that it inspires an interesting observation. That is, given a properly defined deterministic RRM problem on ( M , X ), we can actually solve it in two successive phases, the first one seeking an optimal distribution  X  q ( M ) by solving an induced functional minimization problem defined on ( q ( M ) , X ), and the second finding thereupon an optimal point estimate by reading out the most probable model according to  X  q ( M ). When viewed individually, the first phase itself natu-rally suggests a probabilistic extension to the original deterministic risk minimization problem ( X  , R ): where we can set  X  ( M ) =  X  p 0 ( M ) to retain the equiv-alence (11), or specify  X  ( M ) to be any other proper distribution that we believe serves a good regularizer. 2.2. A Probabilistic Formulation of M 3 F We now apply the above generic discussions to the specific case of M 3 F. For M 3 F with binary prefer-ence scores, it suffices to substitute the definition of ( X  , R ) (5) into the RRM-induced delegate pair (7). Below we concentrate on the more common case of M 3 F with ordinal ratings, where Y ij  X  X  1 , 2 ,...,L } . As in (Srebro et al., 2005), we introduce thresh-olds  X  0  X   X  1  X   X  X  X   X   X  L  X  1 , where  X  0 =  X  X  X  , to discretize R into L intervals. Hence the model is updated as M = ( U,V,  X  ) where  X  = (  X  1 ,..., X  L  X  1 ) &gt; and the prediction rule is changed accordingly to b Y ij = max r | U i V &gt; j  X   X  r  X  1 , r = 1 ,...,L . For hard-margin, we would require  X  Y while in a soft-margin setting, we define the discrimi-nant function and the loss function to be 3 is the generalized hinge loss with margin parameter ` . When `  X  1, the loss thus defined is an upper bound to the sum of absolute differences between the predicted ratings and the true ratings, a loss measure closely related to Normalized Mean Absolute Error (N-MAE) (Marlin &amp; Zemel, 2004; Srebro et al., 2005). Furthermore, we can learn a more flexible model to capture users X  diverse rating criteria by replacing user-common thresholds  X  with user-specific ones  X  = (  X  i 1 ,..., X  i ( L  X  1) ) &gt; . And we may as well regular-ize these thresholds  X  =  X  1: N with where  X  = (  X  1 ,..., X  L  X  1 ) &gt; and  X  1 &lt;  X  X  X  &lt;  X  specified as a prior guidance towards an ascending se-quence of large-margin thresholds. Then the overall regularizer becomes  X ( M ) =  X ( U,V ) +  X (  X  ). Note that when  X   X  X  X  , the regularizer on  X  plays no part. Regularizer and loss fully specified, we may again follow the generic discussions above and obtain the following probabilistic formulation 4 : 2.3. Connections with Previous Methods In the literature of PAC-Bayes learning theory, the loss (or risk) term in problem (12) corresponds to that of a Gibbs classifier (McAllester, 2003; Germain et al., 2009), which is a stochastic classifier that randomly chooses a classifier M according to q ( M ) to classify a data sample x . An alternative formulation of inferring a posterior distribution of classifiers that has received much attention is the one induced from an expected classifier, which can be generally written as Maximum entropy discrimination (MED) (Jaakkola et al., 1999) represents one such example where the loss function L is hinge loss. MED has been adopted in various max-margin models, including max-margin supervised topic models (Zhu et al., 2009) and the probabilistic formulation of max-margin matrix fac-torization (Xu et al., 2012). It X  X  obvious that the two problems (12) and (18) only differ in their choice of the loss term, with (18) choosing loss of expectation while (12) expectation of loss . Actually we have given that L (  X  ,s ) is a convex function, e.g., hinge loss, squared loss, the loss function of M 3 F for ordinal rat-ings (14), etc. Therefore our new formulation gives a more relaxed model while at the same time is much easier to solve (say, through Bayes X  theorem) compared with problem (18), for which approximate variational methods are very often required, along with additional assumptions on the posterior distribution (Zhu et al., 2009; Xu et al., 2012). Note that a Gibbs max-margin topic model has been presented in (Zhu et al., 2013a) with data augmentation; And our work differs by p-resenting a different viewpoint as detailed above and dealing with the challenging problem of matrix factor-ization. 2.4. Data Augmentation for M 3 F We now present a simple and efficient algorithm for learning M 3 F within its probabilistic formulation. Our algorithm builds on the statistical idea of data aug-mentation (Tanner &amp; Wong, 1987; van Dyk &amp; Meng, 2001), whose general principle is to introduce auxiliary variables so as to facilitate Bayesian inference on the original variables of interest.
 Specifically in our case, the form of the dele-likelihood  X  L (17) is very hard to manipulate due to the  X  X ax X  operator inherited from the hinge loss thereof. Fortu-nately, it is discovered in (Polson &amp; Scott, 2011) that e  X  2 max( u, 0) enjoys the representation as a location-scale mixture of Gaussians, namely e where  X  (  X  | X  ,  X  ) is the normal density function. This en-ables us to augment the original model M = ( U,V,  X  ) by introducing auxiliary variables  X  likewise. Then for each delegate likelihood (17), we have Eq. (20) suggests an augmented model M 0 = ( M ,  X  ) with posterior q ( M 0 )  X   X  p 0 ( M ) where Note that  X  L ( M|X ij ,  X  ij ) =  X  ij ( M ) p ( X ij ,  X  thus is not , by definition, a valid  X  X ele-likelihood X   X  L ( M 0 |X ij ) for the augmented model M 0 .
 This augmented representation favors Gibbs sampling in that the Gaussian form of  X  L ( M|X ij ,  X  ij ) appears  X  X onjugate X  to the Gaussian delegate prior  X  p 0 ( M ) (16) with respect to U i , V j and  X  i individually, and further-more, each auxiliary variable  X  ijr in its inverse can be shown to follow an inverse Gaussian (Polson &amp; Scott, 2011) for its conditional distribution, hence implying simple conditional distributions. Below, we summa-rize the conditional distributions and the derivation is similar to that for SVMs (Polson &amp; Scott, 2011). For auxiliary variables,  X  ijr are conditionally inde-pendent of each other given ( M , X ) and we have an inverse Gaussian distribution from which samples can be efficiently drawn.
 For item factors, V j are also conditionally independent and q ( V j |M 0 \ V ) = N ( b j ,B j ) where Similar results apply to user factors U i and are omitted to save space.
 Finally, thresholds  X  ir are again conditionally inde-pendent and q (  X  ir |M 0 \  X  ) = N ( a ir ,A ir ) where With the above conditional distributions, we can de-velop a Gibbs sampling algorithm for the augmented model q ( M ,  X  ) by alternately drawing samples from each of the conditional distributions with random ini-tialization. By ignoring  X  we implicitly obtain the target posterior q ( M ). Solving M 3 F for U and V (2) instead of directly for X (1) has resulted in much more scalable method-s (Rennie &amp; Srebro, 2005; Xu et al., 2012). One re-sulting problem nevertheless, is to explicitly handle the latent factor dimension, i.e. the number of columns, K , of the two matrices. A typical solution relies on some general model selection procedure, e.g., cross-validation, which enumerates and compares many can-didate models with different values of K and thus can be computationally expensive.
 To solve this problem, (Xu et al., 2012) introduces a probabilistic model for M 3 F that is induced from ex-pected classifiers (18) and built accordingly a nonpara-metric M 3 F model termed infinite probabilistic M 3 (iPM 3 F) which automatically resolves the unknown number of latent factors. However their formulation was rooted in the MED framework and consequent-ly resorted to a complicated approximate variational learning algorithm with mean-field assumptions. In order for a practical solution, (Xu et al., 2012) fur-ther set an upper bound, namely the truncation level, to the number of latent factors. Both the mean-field assumptions and the truncation level introduce extra bias into the posterior inference. And what X  X  more, it requires some domain knowledge to properly set the truncation level: a higher level indicates more param-eters and thus more time for solution while a lower level puts model-complexity sufficiency at risk and is prone to hamper the model X  X   X  X nfinite X  flexibility. Below, we propose an alternative nonparametric Bayesian M 3 F model (termed Gibbs iPM 3 F) by adopt-ing the probabilistic formulation induced from Gibbs classifiers (12) instead. Again, by use of data augmen-tation, we design efficient Gibbs sampling algorithms which is both assumption-free and truncation-free. 3.1. Gibbs iPM 3 F Unlike the parametric Gibbs M 3 F which is induced from a deterministic RRM problem, we directly build our nonparametric model from a probabilistic set-ting (12). Specifically, we reuse the empirical loss as defined by Eq. (13) and (14) since they naturally fit here. While for the dele-prior  X  ( M ), it should not on-ly be flexible enough to allow Bayesian inference on factor matrices with an unbounded number of column-s, but, what X  X  even more important, be favorable to sparse matrices as well so that only a finite number of features would be  X  X ctive X  for any finite data set. The Indian buffet process (IBP) (Griffiths &amp; Ghahra-mani, 2005) appears to feed our need for this case. Think of a binary matrix Z as recoding customers X  behavior of sampling dishes from an infinite long buffet. Then IBP specifies a stochastic process that generates binary matrices Z as follows: 1. The first customer samples the first Poisson(  X  ) 2. The i th customer first samples dishes that The process above induces a distribution for the lof -equivalent class of binary matrices. We de-note this distribution by IBP(  X  ) and define Gibbs iPM 3 F to be solving problem (12) where we replace U by Z and specify the normalized dele-prior  X  ( M ) as 3.2. A Gibbs Sampling Algorithm Since the dele-likelihood in Gibbs iPM 3 F remains the same as Gibbs M 3 F, it is expected that exactly the same data augmentation technique (20) can be applied here. Moreover, since the dele-prior of V and  X  are also simply reused from Gibbs M 3 F, their conditional dis-tributions would remain the same as in Eq. (23)&amp;(24) given that we replace U i with Z i .
 For the binary latent feature matrix Z , we follow the uncollapsed Gibbs sampling (Doshi-Velez et al., 2009) where V is not marginalized over but kept in the conditions. Specifically, for existing features, we have the conditional distribution where  X  ( Z ik | Z  X  ( ik ) ) = Bernoulli( cording to the exchangeable IBP and  X  L ( M|X ij ,  X  ij is just as defined in Eq. (21) with U replaced by Z . While for new features Z  X  i = 1 &gt; k sample k i  X  Z  X  0 and adopt the partially collapsed sampler where the new latent features V i X   X  R M  X  k i are integrated out and thus obtain  X  Then conditioned on the newly sampled Z  X  i (or k i ), we draw the corresponding new features V i X  We conduct experiments on the MovieLens 1M and the EachMovie data sets, and compare our results with M 3 F ( Smooth Hinge , truncated) (Rennie &amp; S-rebro, 2005), bcd M 3 F( X  X cd X  for  X  X lock-wise coordi-nate descent X , truncated) (Xu et al., 2012) and iPM 3 F (truncated-mean-field, infinite) (Xu et al., 2012). Data sets: The MovieLens data set contains 1,000,209 anonymous ratings (ranging from 1 to 5) of 3,952 movies made by 6,040 users, among which 3,706 movies are actually rated and every user has at least 20 ratings. The EachMovie data set contains 2,811,983 ratings of 1,628 movies made by 72,916 users, among which 1,623 movies are actually rated and 36,656 users has at least 20 ratings. As in (Marlin &amp; Zemel, 2004; Rennie &amp; Srebro, 2005), we discarded users with few-er than 20 ratings, leaving us with 2,579,985 ratings. There are 6 possible rating values, { 0 , 0 . 2 ,..., 1 } and we mapped them to { 1 , 2 ,..., 6 } .
 Protocol: As in (Marlin &amp; Zemel, 2004; Rennie &amp; Srebro, 2005; Xu et al., 2012), we adopt the all-but-one protocol to construct training sets and test set-s. And we consider both weak and strong generaliza-tion, where weak indicates all users contribute to the learning of the latent factors while strong transfers the learned movie latent factors from one group of user-s to another. As in previous methods, we randomly partition the users into 5,000 and 1,040 for weak and strong in MovieLens, and 30,000 and 6,565 in Each-Movie. We repeat the random partition thrice, test our model against each of them and report the aver-aged Normalized Mean Absolute Error (NMAE).
 Implementation details 5 : We perform cross-validation to choose the best regularization constant C from the same 11 candidate values that are log-evenly distributed between 0 . 1 3 / 4 and 0 . 1 2 as in (Xu et al., 2012). According to (Rennie &amp; Srebro, 2005), factor numbers higher than 50 yield similar performances and hence they choose K = 100 as a compromise between model capacity and computational complexity. There-fore we also set the truncation level K to be 100 for iPM 3 F and all the parametric M 3 F methods. Other hyper-parameters are set as follows:  X  = 3,  X  = 1, ` = 9,  X  = 1 . 5 ` ;  X  1 ,..., X  L  X  1 are set to be symmetric with respect to 0, with a step-size of 3 ` .
 Point estimate : We sought point estimate because our model formulation adopts a risk term that is in-duced from stochastic Gibbs classifiers. More specif-ically, we compared both the single samples M ( m ) drawn from each Gibbs sampling iteration and the Rao-Blackwellizedly averaged samples Fig. 1 illustrates the difference between these two es-timates, where dashed curves represent single samples while solid curves represent averaged samples. It seem-s taking average of the samples not only stabilizes the results but also continuously reduces test error as well as the original objective value. Test error : We report NMAE error of the averaged samples for Gibbs M 3 F and Gibbs iPM 3 F. As shown in Table 1, Gibbs M 3 F significantly outperforms previ-ous parametric M 3 F models, for both weak and strong generalization tasks. We believe this largely attributes to our additionally introduced regularizer for  X  (15); For the nonparametric models, although Gibbs iPM 3 F only obtains comparable, or even marginally worse test performance compared with iPM 3 F, we consider it to be the cost of exchanging accuracy for efficiency since our alternative relaxed loss term (19) favors the devel-opment of much more efficient learning algorithms. Training time : In Table 2, the training time of M 3 F is directly cited from (Rennie &amp; Srebro, 2005) and it was measured on a  X  X ingle 3.06GHz Pentium 4 CPU X  while all other 4 methods were measured by MATLAB with single computational thread on a 4-core 3.00GHz Intel i5 CPU. In both cases, our proposed methods achieved drastic efficiency gain. Note that M 3 F works with a derivable Smooth Hinge while our methods di-rectly work with the hinge loss without solving time-consuming SVMs. Also note that for the nonparamet-ric Gibbs iPM 3 F, its number of active factors K is constantly changing during the sampling process, and so is the running time for each iteration, as shown in Fig. 2(b). We discuss the asymptotic computational complexity of our methods in the appendix.
 RRM-MAP duality : Although Gibbs iPM 3 F is directly defined from problem (12) without explicit reference to any underlying RRM as Gibbs M 3 F, we may still find the corresponding RRM by choosing the regularizer as  X  0 ( M ) =  X  log  X  ( M ) and thus obtain We can see more clearly in this form that the variance parameters  X  and  X  each has their own right in weigh-ing the regularizer and cannot be offset by the regu-larization constant C . The benefit of acquiring this induced RRM is that it allows us to calculate the ob-jective value and use it as a criterion of convergence. We found that for Gibbs iPM 3 F, the induced regu-larized risk (calculated from single samples) does not necessarily decrease during the sampling process yet we still observed a stable trend of NMAE going down. Convergence : It might be true that the Markov chain itself has not properly converged when we stop the iteration based on objective values and validation error, especially for the IBP-involved nonparametric model. Yet from our experience and experiments, the Gibbs sampler works quite well and the curves do dis-play a clear trend of convergence in Fig. 1. We hope to come up with further detailed analysis in future work. Latent dimension : Fig. 2(a) shows the running number of latent factors inferred from data by Gibbs iPM 3 F with the three randomly constructed training sets as indicated by different colors and this clearly il-lustrates the flexibility of nonparametric models. The  X  X ptimal X  latent dimension appears to be around 450 for MovieLens and 200 for EachMovie. We empha-size however, given any specific hyper-parameters, the corresponding Gibbs M 3 F model always has its own optimal solution that comes with a latent dimension and a test error; And by  X  X ptimal X  latent dimension we actually mean the one with the best test error. Poisson truncation level : When sampling new la-tent factors k i in Gibbs iPM 3 F, we specify a Poisson truncation level  X  as did (Doshi-Velez et al., 2009) so that k i greater than  X  get directly rejected. This would not be a problem since we find that the cost of sam-pling new latent factors Z i X  can be reduced to linear to  X  and thus we may set  X  to be sufficiently large without worrying about its impact on efficiency. We defer details into the appendix. In our current imple-mentation, we choose  X  = 10.
 Factor alignment : Averaging samples from Gibbs iPM 3 F is a little bit tricker than from Gibbs M 3 F, s-ince we are constantly facing newly generated factors as well as nullified factors that get crossed out and missing from subsequent samples. We compared two different methods for this. The first one ignores such factor alignment and sum two samples directly as is, padding zero wherever necessary. While the second one respects such correspondence between factors and makes sure they are always properly aligned before av-eraging. Our experiments indicate no telling difference between these two methods. We have presented a novel probabilistic interpretation of max-margin matrix factorization, which naturally leads to a simple and fast algorithm by exploring the ideas of data augmentation. Moreover, we generalized the ideas to present a new nonparametric Bayesian max-margin matrix factorization model, which again has a simple and efficient sampling algorithm without making any restricting assumptions on the posterior distributions or setting a truncation level to the num-ber of latent factors as in existing variational methods. This work is supported by the National Basic Research Program (973 Program) of China (Nos. 2013CB329403, 2012CB316301), National Natural Sci-ence Foundation of China (Nos. 91120011, 61273023), and Tsinghua University Initiative Scientific Research Program (No. 20121088071).
 Doshi-Velez, F., Miller, K. T., van Gael, J., and Teh,
Y. W. Variational inference for the Indian buffet process. In Proceedings of the International Con-ference on Artificial Intelligence and Statistics , vol-ume 12, 2009.
 Germain, P., Lacasse, A., Laviolette, F., and Marc-hand, M. PAC-Bayesian learning of linear classifier-s. In International Conference on Machine Learning (ICML) , pp. 353 X 360, 2009.
 Griffiths, T. and Ghahramani, Z. Infinite latent fea-ture models and the Indian buffet process. Technical report, Gatsby Computational Neuroscience Unit, 2005.
 Jaakkola, T., Meila, M., and Jebara, T. Maximum entropy discrimination. In Advances in Neural In-formation Processing Systems (NIPS) , 1999.
 Marlin, B. and Zemel, R. S. The multiple multiplica-tive factor model for collaborative filtering. In Inter-national Conference on Machine Learning (ICML) , 2004.
 McAllester, D. PAC-Bayesian stochastic model selec-tion. Machine Learning , 51:5 X 21, 2003.
 Polson, N. G. and Scott, S. L. Data augmentation for support vector machines. Bayesian Analysis , 6(1): 1 X 24, 2011.
 Rennie, J. D. M. and Srebro, N. Fast maximum mar-gin matrix factorization for collaborative prediction.
In International Conference on Machine Learning (ICML) , 2005.
 Salakhutdinov, R. and Mnih, A. Bayesian probabilistic matrix factorization using markov chain monte car-lo. In International Conference on Machine Learn-ing (ICML) , 2008.
 Srebro, N., Rennie, J. D. M., and Jaakkola, T.
Maximum-margin matrix factorization. In Advances in Neural Information Processing Systems (NIPS) , 2005.
 Tanner, M. A. and Wong, W. H. The calculation of posterior distributions by data augmentation. Jour-nal of the Americal Statistical Association (JASA) , 82(398):528 X 540, 1987. van Dyk, D. and Meng, X. The art of data augmen-tation. Journal of Computational and Graphical S-tatistics (JCGS) , 10(1):1 X 50, 2001.
 Xu, M., Zhu, J., and Zhang, B. Nonparametric max-imum margin matrix factorization for collaborative prediction. In Advances in Neural Information Pro-cessing Systems (NIPS) , 2012.
 Zhou, M., Wang, C., Chen, M., Paisley, J., Dunson,
D., and Carin, L. Nonparametric Bayesian matrix completion. In Sensor Array and Multichannel Sig-nal Processing Workshop (SAM) , pp. 213 X 216, 2010. Zhu, J., Ahmed, A., and Xing, E. P. MedLDA: Maxi-mum margin supervised topic models for regression and classification. In International Conference on Machine Learning (ICML) , 2009.
 Zhu, J., Chen, N., Perkins, H., and Zhang, B. Gibb-s max-margin topic models with fast sampling al-gorithms. In International Conference on Machine Learning (ICML) , 2013a.
 Zhu, J., Chen, N., and Xing, E. P. Bayesian inference with posterior regularization and appli-cations to infinite latent svms. arXiv Report ,
