 Bruno Scherrer Bruno.Scherrer@inria.fr INRIA Nancy -Grand Est, Team Maia, FRANCE Mohammad Ghavamzadeh Mohammad.Ghavamzadeh@inria.fr Victor Gabillon Victor.Gabillon@inria.fr INRIA Lille -Nord Europe, Team SequeL, FRANCE Matthieu Geist Matthieu.Geist@supelec.fr Suplec, IMS Research Group, Metz, FRANCE Modified Policy Iteration (MPI) ( Puterman &amp; Shin , 1978 ) is an iterative algorithm to compute the optimal policy and value function of a Markov Decision Process (MDP). Starting from an arbitrary value function v 0 , it generates a sequence of value-policy pairs where G v k is a greedy policy w.r.t. v k , T  X  man operator associated to the policy  X  k , and m  X  1 is a parameter. MPI generalizes the well-known dynamic programming algorithms Value Iteration (VI) and Pol-icy Iteration (PI) for values m = 1 and m =  X  , respec-tively. MPI has less computation per iteration than PI (in a way similar to VI), while enjoys the faster conver-gence of the PI algorithm ( Puterman &amp; Shin , 1978 ). In problems with large state and/or action spaces, ap-proximate versions of VI (AVI) and PI (API) have been the focus of a rich literature (see e.g., Bertsekas &amp; Tsitsiklis 1996 ; Szepesv  X ari 2010 ). The aim of this paper is to show that, similarly to its exact form, ap-proximate MPI (AMPI) may represent an interesting alternative to AVI and API algorithms.
 In this paper, we propose three implementations of AMPI (Sec. 3 ) that generalize the AVI implementa-tions of Ernst et al. ( 2005 ); Antos et al. ( 2007 ); Munos &amp; Szepesv  X ari ( 2008 ) and the classification-based API algorithm of Lagoudakis &amp; Parr ( 2003 ); Fern et al. then provide an error propagation analysis of AMPI (Sec. 4 ), which shows how the L p -norm of its perfor-mance loss can be controlled by the error at each iter-ation of the algorithm. We show that the error prop-agation analysis of AMPI is more involved than that of AVI and API. This is due to the fact that neither the contraction nor monotonicity arguments, that the error propagation analysis of these two algorithms rely on, hold for AMPI. The analysis of this section unifies those for AVI and API and is applied to the AMPI im-plementations presented in Sec. 3 . We detail the anal-ysis of the classification-based implementation of MPI (CBMPI) of Sec. 3 by providing its finite sample analy-sis in Sec. 5 . Our analysis indicates that the parameter m allows us to balance the estimation error of the clas-sifier with the overall quality of the value approxima-tion. We report some preliminary results of applying CBMPI to standard benchmark problems and compar-ing it with some existing algorithms in ( Scherrer et al. , 2012 , Appendix G ). We consider a discounted MDP  X  X  , A , P, r,  X   X  , where S is a state space, A is a finite action space, P ( ds  X  for all ( s, a ), is a probability kernel on S , the re-ward function r : S X A X  R is bounded by R max , and  X   X  (0 , 1) is a discount factor. A determinis-tic policy is defined as a mapping  X  : S X  X  . For a policy  X  , we may write r  X  ( s ) = r P ( ds  X  | s ) = P a state s is defined as the expected discounted sum of rewards received starting from state s and follow-ing the policy  X  , i.e., v  X  ( s ) = E the expected discounted sum of rewards received start-ing from state s , taking action a , and then following the policy. Since the rewards are bounded by R max , the values and action-values should be bounded by V max = Q max = R max / (1  X   X  ). The Bellman oper-ator T  X  of policy  X  takes a function f on S as input E  X  r  X  ( s ) +  X f ( s  X  ) | s  X   X  P  X  ( . | s ) T f = r  X  +  X P  X  f . It is known that v  X  is the unique fixed-point of T  X  . Given a function f on S , we say that a policy  X  is greedy w.r.t. f , and write it as timal value function. It is also known that v  X  is the unique fixed-point of the Bellman optimality operator T : v  X  max  X  T  X  v = T G ( v ) v , and that a policy  X  is greedy w.r.t. v  X  is optimal and its value satisfies v In this section, we describe three approximate MPI (AMPI) algorithms. These algorithms rely on a func-tion space F to approximate value functions, and in the third algorithm, also on a policy space  X  to repre-sent greedy policies. In what follows, we describe the iteration k of these iterative algorithms. 3.1. AMPI-V For the first and simplest AMPI algorithm presented in the paper, we assume that the values v k are rep-resented in a function space F X  R |S| . In any state estimated as follows: where  X  a  X  X  and 1  X  j  X  M , r ( j ) a and s ( j ) a samples of rewards and next states when action a is taken in state s . Thus, approximating the greedy action in a state s requires M |A| samples. The al-gorithm works as follows. It first samples N states each sampled state s ( i ) , it generates a rollout of size m , i.e., a t is the action suggested by  X  k +1 in state s ward and next state induced by this choice of ac-tion. For each s ( i ) , we then compute a rollout estimate  X  v biased estimate of is computed as the best fit in F to these estimates, i.e., Each iteration of AMPI-V requires N rollouts of size m , and in each rollout any of the |A| actions needs M samples to compute Eq. 3 . This gives a total of Nm ( M |A| +1) transition samples. Note that the fitted value iteration algorithm ( Munos &amp; Szepesv  X ari , 2008 ) is a special case of AMPI-V when m = 1. 3.2. AMPI-Q In AMPI-Q, we replace the value function v : S X  R with an action-value function Q : S X A X  R . The Bellman operator for a policy  X  at a state-action pair ( s, a ) can then be written as [ T  X  Q ]( s, a ) = E and the greedy operator is defined as In AMPI-Q, action-value functions Q k are represented in a function space F X  R |S X A| , and the greedy action The evaluation step is similar to that of AMPI-V, with the difference that now we work with state-action pairs. We sample N state-action pairs from a distribution  X  on S X A and build a rollout set D ( s ( i ) , a ( i ) )  X  X  k , we generate a rollout of size m , i.e., r t and s by this choice of action. For each ( s ( i ) , a ( i ) )  X  X  then compute the rollout estimate which is an unbiased estimate of  X  ( T  X  to these estimates in F , i.e., Each iteration of AMPI-Q requires Nm samples, which is less than that for AMPI-V. However, it uses a hypothesis space on state-action pairs instead of states. Note that the fitted-Q iteration algo-case of AMPI-Q when m = 1. 3.3. Classification-Based MPI The third AMPI algorithm presented in this paper, called classification-based MPI (CBMPI), uses an ex-plicit representation for the policies  X  k , in addition to the one used for value functions v k . The idea is similar to the classification-based PI algorithms ( Lagoudakis Gabillon et al. , 2011 ) in which we search for the greedy policy in a policy space  X  (defined by a classifier) instead of computing it from the estimated value or action-value function (like in AMPI-V and AMPI-Q). In order to describe CBMPI, we first rewrite the MPI formulation (Eqs. 1 and 2 ) as Note that in the new formulation both v k and  X  k +1 are functions of ( T  X  mate version of this new formulation. As described in Fig. 1 , CBMPI begins with arbitrary initial policy  X  1  X   X  and value function v 0  X  X  . 1 At each iteration k , a new value function v k is built as the best approx-imation of the m -step Bellman operator ( T  X  in F ( evaluation step ). This is done by solving a re-gression problem whose target function is ( T  X  To set up the regression problem, we build a rollout set D k by sampling n states i.i.d. from a distribution  X  . 2 For each state s ( i )  X  X  k , we generate a roll-out and next state induced by this choice of action. From this rollout, we compute an unbiased estimate  X  v k ( s ( i ) of  X  and use it to build a training set This training set is then used by the regressor to com-pute v k as an estimate of ( T  X  The greedy step at iteration k computes the policy  X  as the best approximation of G ing a cost-sensitive classification problem. From the definition of a greedy policy, if  X  = G for each s  X  X  , we have  X  By defining Q k ( s, a ) = rewrite Eq. 8 as The cost-sensitive error function used by CBMPI is of the form To simplify the notation we use L  X  k instead of L  X   X  To set up this cost-sensitive classification problem, we build a rollout set D  X  k by sampling N states i.i.d. from a distribution  X  . For each state s ( i )  X  X   X  k and each action a  X  X  , we build M independent rollouts of size m + 1, i.e., 3  X  s t +1 are the reward and next state induced by this choice of action. From these rollouts, we compute an unbiased estimate of Q k ( s ( i ) , a ) as  X  Q k ( s Given the outcome of the rollouts, CBMPI uses a cost-sensitive classifier to return a policy  X  k +1 that mini-mizes the following empirical error with the goal of minimizing the true error L  X  k (  X  ;  X  ). Each iteration of CBMPI requires nm + M |A| N ( m +1) (or M |A| N ( m + 1) in case we reuse the rollouts, see Footnote 3) transition samples. Note that when m tends to  X  , we recover the DPI algorithm proposed and analyzed by Lazaric et al. ( 2010 ). In this section, we derive a general formulation for propagation of error through the iterations of an AMPI algorithm. The line of analysis for error propagation is different in VI and PI algorithms. VI analysis is based on the fact that this algorithm computes the fixed point of the Bellman optimality operator, and this operator is a  X  -contraction in max-norm ( Bert-sekas &amp; Tsitsiklis , 1996 ; Munos , 2007 ). On the other hand, it can be shown that the operator by which PI updates the value from one iteration to the next is not a contraction in max-norm in general. Unfortunately, we can show that the same property holds for MPI when it does not reduce to VI (i.e., m &gt; 1). Proposition 1. If m &gt; 1 , there exists no norm for which the operator that MPI uses to update the values from one iteration to the next is a contraction. Proof. Consider a deterministic MDP with two states { s 1 , s 2 } , two actions { change, stay } , rewards r ( s 0 , r ( s 2 ) = 1, and transitions P ch ( s 2 | s 1 ) = P P 0. Their corresponding greedy policies are  X  = ( st, ch ) and  X   X  = ( ch, st ), and the next iterates of v and v be computed as ( T  X  ) m v =  X  while v  X   X  v = than the norm of v  X  v  X  as long as m &gt; 1. We also know that the analysis of PI usually relies on the fact that the sequence of the generated values is non-decreasing ( Bertsekas &amp; Tsitsiklis , 1996 ; Munos , 2003 ). Unfortunately, it can be easily shown that for m finite, the value functions generated by MPI may decrease (it suffices to take a very high initial value). It can be seen from what we just described and Propo-sition 1 that for m  X  = 1 and  X  , MPI is neither contract-ing nor non-decreasing, and thus, a new line of proof is needed for the propagation of error in this algorithm. To study error propagation in AMPI, we introduce an abstract algorithmic model that accounts for potential errors. AMPI starts with an arbitrary value v 0 and at each iteration k  X  1 computes the greedy policy error . Thus, we write the new policy  X  k as Eq. 10 means that for any policy  X   X  , AMPI then generates the new value function v k with some error  X  k , called the evaluation step error Before showing how these two errors are propagated through the iterations of AMPI, let us first define them in the context of each of the algorithms presented in Section 3 separately.
 AMPI-V:  X  k is the error in fitting the value function v . This error can be further decomposed into two parts: the one related to the approximation power of F and the one due to the finite number of sam-ples/rollouts.  X   X  k is the error due to using a finite num-ber of samples M for estimating the greedy actions. AMPI-Q:  X   X  k = 0 and  X  k is the error in fitting the state-action value function Q k .
 CBMPI: This algorithm iterates as follows: Unfortunately, this does not exactly match with the model described in Eqs. 10 and 11 . By introducing the auxiliary variable w k  X  = ( T  X  w k +  X  k , and thus, we may write Now, Eqs. 12 and 13 exactly match Eqs. 10 and 11 by replacing v k with w k and  X  k with (  X P  X  The rest of this section is devoted to show how the errors  X  k and  X   X  k propagate through the iterations of an AMPI algorithm. We only outline the main arguments that will lead to the performance bound of Thm. 1 and report most proofs in ( Scherrer et al. , 2012 ). We follow the line of analysis developped by Thiery &amp; Scherrer ( 2010 ). The results are obtained using the following three quantities: 1) The distance between the optimal value function and the value before approximation at the k th itera-tion: d k  X  = v  X   X  ( T  X  2) The shift between the value before approximation and the value of the policy at the k th iteration: s k  X  ( T 3) The Bellman residual at the k th iteration: b k  X  = v  X  T  X  We are interested in finding an upper bound on the loss l k  X  = v  X   X  v  X  per bound d k and s k , which requires a bound on the Bellman residual b k . More precisely, the core of our analysis is to prove the following point-wise inequali-ties for our three quantities of interest.
 Lemma 1 (Proof in ( Scherrer et al. , 2012 , Ap-pendix A )) . Let k  X  1 , x k  X  = ( I  X   X P  X  and y k  X  =  X   X P  X  Since the stochastic kernels are non-negative, the bounds in Lemma 1 indicate that the loss l k will be if we define  X  as a uniform upper-bound on the errors |  X  | and |  X   X  k | , the first inequality in Lemma 1 implies that b k  X  O (  X  ), and as a result, the second and third means that the loss will also satisfy l k  X  O (  X  ). Our bound for the loss l k is the result of careful ex-pansion and combination of the three inequalities in Lemma 1 . Before we state this result, we introduce some notations that will ease our formulation. Definition 1. For a positive integer n , we define P n as the set of transition kernels that are defined as follows: 1) for any set of n policies {  X  1 , . . . ,  X  (  X P  X  2) for any  X   X  (0 , 1) and ( P 1 , P 2 )  X  P n  X  P n (1  X   X  ) P 2  X  P n .
 Furthermore, we use the somewhat abusive notation  X  n for denoting any element of P n . For example, if we write a transition kernel P as P =  X  1  X  i +  X  2  X  j  X 
 X  i +  X  2  X  j + k , it should be read as there exist P 1 P 2  X  P j , P 3  X  P k , and P 4  X  P k + j such that P =  X  Using the notation introduced in Definition 1 , we now derive a point-wise bound on the loss.
 Lemma 2 (Proof in ( Scherrer et al. , 2012 , Ap-pendix B )) . After k iterations, the losses of AMPI-V and AMPI-Q satisfy while the loss of CBMPI satisfies l k  X  2 where h ( k )  X  = 2 Remark 1. A close look at the existing point-wise error bounds for AVI ( Munos , 2007 , Lemma 4.1) and API ( Munos , 2003 , Corollary 10) shows that they do not consider error in the greedy step (i.e.,  X   X  k = 0) and that they have the following form: This indicates that the bound in Lemma 2 not only unifies the analysis of AVI and API, but it generalizes them to the case of error in the greedy step and to a finite horizon k . Moreover, our bound suggests that the way the errors are propagated in the whole family of algorithms VI/PI/MPI does not depend on m at the level of the abstraction suggested by Definition 1 . 4 The next step is to show how the point-wise bound of Lemma 2 can turn to a bound in weighted L p -norm, which for any function f : S X  R and any distribu-tion  X  on S is defined as  X  f  X  p, X   X  = Munos ( 2003 ; 2007 ); Munos &amp; Szepesv  X ari ( 2008 ), and the recent work of Farahmand et al. ( 2010 ), which pro-vides the most refined bounds for API and AVI, show how to do this process through quantities, called con-centrability coefficients , that measure how a distribu-tion over states may concentrate through the dynamics of the MDP. We now state a lemma that generalizes the analysis of Farahmand et al. ( 2010 ) to a larger class of concentrability coefficients. We will discuss the po-tential advantage of this new class in Remark 4 . We will also show through the proofs of Thms. 1 and 3 , how the result of Lemma 3 provides us with a flex-ible tool for turning point-wise bounds into L p -norm bounds. Thm. 3 in ( Scherrer et al. , 2012 , Appendix D ) provides an alternative bound for the loss of AMPI, which in analogy with the results of Farahmand et al. ( 2010 ) shows that the last iterations have the high-est impact on the loss (the influence exponentially de-creases towards the initial iterations).
 Lemma 3 (Proof in ( Scherrer et al. , 2012 , Ap-( g ) i  X  X  be functions satisfying all distributions  X  and  X  , we have with the following concentrability coefficients with the Radon-Nikodym derivative based quantity We now derive a L p -norm bound for the loss of the AMPI algorithm by applying Lemma 3 to the point-wise bound of Lemma 2 .
 Theorem 1 (Proof in ( Scherrer et al. , 2012 , Ap-pendix D )) . Let  X  and  X  be distributions over states. iterations, the loss of AMPI satisfies while the loss of CBMPI satisfies cients C l,k,d q are defined as Remark 2. When p tends to infinity, the first bound of Thm. 1 reduces to When k goes to infinity, Eq. 17 gives us a general-ization of the API ( m =  X  ) bound of Bertsekas &amp; Tsitsiklis ( 1996 , Prop. 6.2), i.e., Moreover, since our point-wise analysis generalizes those of API and AVI (as noted in Remark 1 ), the L -bound of Eq. 15 unifies and generalizes those for API ( Munos , 2003 ) and AVI ( Munos , 2007 ). Remark 3. Canbolat &amp; Rothblum ( 2012 ) recently (and independently) developped an analysis of an approximate form of MPI. Also, as mentioned, the proof technique that we used is mainly based on that in Thiery &amp; Scherrer ( 2010 ). While Canbolat &amp; Roth-blum ( 2012 ) only consider the error in the greedy step and Thiery &amp; Scherrer ( 2010 ) that in the value up-date, our work is more general in that we consider both sources of error  X  this is required for the analysis of CBMPI. Thiery &amp; Scherrer ( 2010 ) and Canbolat &amp; Rothblum ( 2012 ) provide bounds when the errors are controlled in max-norm, while we consider the more general L p -norm. At a more technical level, Thm. 2 in Canbolat &amp; Rothblum ( 2012 ) bounds the norm of the distance v  X   X  v k , while we bound the loss v  X   X  v If we derive a bound on the loss (using e.g., Thm. 1 in Canbolat &amp; Rothblum 2012 ), this leads to a bound on the loss that is looser than ours. In particular, this does not allow to recover the standard bounds for AVI/API, as we managed to (c.f. Remark 2 ).
 Remark 4. We can balance the influence of the con-centrability coefficients (the bigger the q , the higher the influence) and the difficulty of controlling the er-rors (the bigger the q  X  , the greater the difficulty in controlling the L pq  X  -norms) by tuning the parameters q and q  X  , given the condition that 1 q + 1 q  X  = 1. This potential leverage is an improvement over the existing bounds and concentrability results that only consider specific values of these two parameters: q =  X  and q = 1 in Munos ( 2007 ); Munos &amp; Szepesv  X ari ( 2008 ), and q = q  X  = 2 in Farahmand et al. ( 2010 ). Remark 5. For CBMPI, the parameter m controls the influence of the value function approximator, can-celling it out in the limit when m tends to infinity (see Eq. 16 ). Assuming a fixed budget of sample tran-sitions, increasing m reduces the number of rollouts used by the classifier, and thus, worsens its quality; in such a situation, m allows to make a trade-off between the estimation error of the classifier and the overall value function approximation. In this section, we focus on CBMPI and detail the pos-sible form of the error terms that appear in the bound of Thm. 1 . We select CBMPI among the proposed al-gorithms because its analysis is more general than the others as we need to bound both greedy and evaluation step errors (in some norm), and also because it displays an interesting influence of the parameter m (see Re-mark 5 ). We first provide a bound on the greedy step error . From the definition of  X   X  k for CBMPI (Eq. 12 ) and the description of the greedy step in CBMPI, we can easily observe that  X   X   X  k  X  1 , X  = L  X  k  X  1 (  X  ;  X  Lemma 4 (Proof in ( Scherrer et al. , 2012 , Ap-pendix E )) . Let  X  be a policy space with finite VC-dimension h = V C ( X ) and  X  be a distribution over the state space S . Let N be the number of states in D  X  k  X  1 drawn i.i.d. from  X  , M be the number of rollouts per state-action pair used in the estimation of  X  Q k  X  1 , and  X  iteration k  X  1 of CBMPI. Then, for any  X  &gt; 0 ,  X   X  with probability at least 1  X   X  , where We now consider the evaluation step error . The eval-uation step at iteration k of CBMPI is a regression problem with the target ( T  X  set mates of the target computed according to Eq. 7 . Dif-ferent function spaces F (linear or non-linear) may be used to approximate ( T  X  sider a linear architecture with parameters  X   X  R d and bounded (by L ) basis functions {  X  j } d j =1 ,  X   X  j  X  We denote by  X  : X X  R d ,  X  (  X  ) = the feature vector, and by F the linear function space  X   X  R d } . Now if we define v k as the truncation (by V max ) of the solution of the above linear regression problem, we may bound the evaluation step error us-ing the following lemma.
 Lemma 5 (Proof in ( Scherrer et al. , 2012 , Ap-pendix F )) . Consider the linear regression setting de-scribed above, then we have with probability at least 1  X   X  , where and  X   X  is such that f  X  (w.r.t.  X  ) of the target function ( T  X  From Lemmas 4 and 5 , we have bounds on  X   X   X  k  X  1 , X  and  X   X  k  X  1 , X   X  X  X   X  k  X  2 , X  . By a union bound argument, we thus control the r.h.s. of Eq. 16 in L 1 -norm. In the context of Thm. 1 , this means p = 1, q  X  = 1 and q =  X  , and we have the following bound for CBMPI: notations of Thm. 1 and Lemmas 4 and 5 , after k it-erations, and with probability 1  X   X  , the expected loss E [ l k ] =  X  l k  X  1 , X  of CBMPI is bounded by Remark 6. This result leads to a quantitative ver-sion of Remark 5 . Assume that we have a fixed budget for the actor and the critic B = nm = NM | A | m . Then, up to constants and logarith-mic factors, the bound has the form  X  l k  X  1 , X   X  O  X  trade-off in the tuning of m : a big m can make the in-fluence of the overall (approximation and estimation) value error small, but that of the estimation error of the classifier bigger. In this paper, we studied a DP algorithm, called mod-ified policy iteration (MPI), that despite its generality that contains the celebrated policy and value itera-tion methods, has not been thoroughly investigated in the literature. We proposed three approximate MPI (AMPI) algorithms that are extensions of the well-known ADP algorithms: fitted-value iteration, fitted-Q iteration, and classification-based policy iteration. We reported an error propagation analysis for AMPI that unifies those for approximate policy and value iteration. We also provided a finite-sample analysis for the classification-based implementation of AMPI (CBMPI), whose analysis is more general than the other presented AMPI methods. Our results indi-cate that the parameter of MPI allows us to control the balance of errors (in value function approximation and estimation of the greedy policy) in the final per-formance of CBMPI. Although AMPI generalizes the existing AVI and classification-based API algorithms, additional experimental work and careful theoretical analysis are required to obtain a better understanding of the behaviour of its different implementations and their relation to the competitive methods. Extension of CBMPI to problems with continuous action space is another interesting direction to pursue.
 Acknowledgments The second and third authors would like to thank French National Research Agency (ANR) under project LAMPADA n  X  ANR-09-EMER-007, European Community X  X  Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreement n  X  231495, and PASCAL2 European Network of Excel-lence for supporting their research.

