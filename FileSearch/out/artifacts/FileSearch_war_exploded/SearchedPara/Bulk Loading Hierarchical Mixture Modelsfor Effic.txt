 With an abundance of streaming data due to widely deployed sensors or other data gathering devices, analysis of streaming data such as stream classification has recently received much attention in dat a mining research. Algorithms that work on data streams have to cope with the limited and often also varying amount of computation time. Traditionally they got for a certain task a fixed time budget which was known in advance, i.e. they were tailored to the specific application. These budget algorithms can neither provide a results in less time nor exploit additional time to improve their result. In contrast, so called anytime algorithms can provide a result after a very short initialization, improve their result incrementally when more time is av ailable and hold the most recent result ready at any time. In data mining anytime solutions have been proposed for many tasks such as clustering [10], top-k processing [2] and classification [5,16].
Anytime algorithms are the natural choice for varying data streams since they flexibly exploit all available time to improve the quality of their result. Recently it has been shown in [12] that also on constant data streams anytime classifiers can improve the classification accuracy over that of traditional budget approaches. With their superiority on varying and constant data streams, applications for anytime classifiers are numerous and range from industrial applications, such as machine monitoring, over sorting tasks to robotics and health applications. [11]. Our focus is on improving the performance of anytime Bayesian classification. Improving the anytime accuracy togeth er with the results from [12] leads to better classification results on both constant and varying data streams. Classification aims at determining the class label of unknown objects based on training data. Different classification approaches are discussed in the literature including nearest neighbor classifiers, d ecision trees or support vector machines. Bayes classifiers constitute a statistical approach that has been successfully used in numerous application domains. Another classification approach is represented by Bayesian classification using kernel de nsity estimation [9]. Especially for huge data sets the estimation error using kernel densities is known to be very low and even asymptotically optimal [3].
 Anytime classification is real time classification up to a point of interruption. In addition to high classification accuracy as in traditional classifiers, anytime classifiers have to make best use of the limited time available, and, most notably, they have to be interruptible at any given point in time. This point in time is usually not known in advance and may vary greatly. Anytime classification has for example been discussed for support v ector machines [5] or nearest neighbor classification [16].

For Bayesian classification based on kernel densities an anytime algorithm called Bayes tree has been proposed in [1 5]. The Bayes tree is a balanced tree structure and is basically an extension o ftheR-tree[8].Itst ores in each entry a pointer and a minimum bounding rectangle (MBR) and additionally a cluster feature representing the corresponding subtree. In [15] the tree is constructed trough iterative insertion, i.e. no optimization is performed with respect to over-lapping or quality of the resulting mixture densities. In this paper we propose several methods for bulk loading mixture densities in the Bayes tree and show in experimental evaluation that they outperform the results from [15]. Before going into detail on the different bulk loading approaches in Sections 3.2 and 3.3 we briefly review Bayesian classification and describe the structure and working of the Bayes tree proposed in [15]. 3.1 Bayesian Classification and the Bayes Tree Given a set of classes C and an object space X a classifier is a function G that assigns the class label G ( x ) to an object x  X  X . Based on a statistical model of the distribution of class labels the Bayes classifier assigns to an object x the class c i with the highest posterior probability P ( c i | x ). With Bayes rule it holds: In the Bayes tree the class-conditional density p ( x | c i ) is estimated using Gaussian mixture models in the inner nodes and Gaussian kernel estimators at the leaf level. Iterative refinement of mixture components enables anytime kernel density estimation for efficient and interruptible classification. The general idea of the Bayes tree is a hierarchy of mixture d ensities stored in a multidimensional index. Each level of the tree stores a complete model of the entire data at a dif-ferent granularity. The node entries consist of pointers to a subtree and a single Gaussian representing the objects in the subtree. All objects stored in the leaves of the Bayes tree are d -dimensional ke rnels. The mean  X  s and the variance vector  X  s can be computed from th e cluster features.

Answering a probability density query uses a complete model which is avail-able at each level of the tree. Besides these full models, the Bayes tree allows for local refinement of the model (to adapt flexibly to the query) and thus pro-vides models composed of coarser and finer representations. The current mixture model components, i.e. their corresponding entries, are stored in a frontier .The current entries in the frontier have to represent each stored object exactly once. This is made sure by removing the entry e s that is refined from the frontier and adding its child entries e s  X  j ,j =1 ... X  s instead. The probability density for a query object is then calculated with respect to the current frontier.
For tree traversal best first descent using a probabilistic priority measure has proven to yield the best results in [15]. On e Bayes tree is built per class, therefore several improvement strategies have b een proposed to decide which tree has the right to refine its model in the next time step. Extensive experiments showed that refining the k most probable classes ( qbk ) in turns yielded the best results throughout. k =min { 2 , log( m ) } ,where m is the number of classes, showed the best performance on all tested data sets. For more details please refer to [15]. 3.2 Machine Learning and Statistical Approaches Our goal in this work is to improve the performance of the Bayes tree. The ac-curacy of the Bayes tree results is based on the quality of the mixture densities stored in its entries. The iterative insertion performed in [15] does not consider the quality of the resulting Gaussian components. We develop and evaluate sev-eral bulk loading approaches that try to overcome this shortcoming and improve the quality of the mixture densities.
 Goldberger. Since the Bayes tree is a statistical approach to classification we looked for statistical methods to create a smaller mixture model from a given mixture model. Starting bottom up with a mixture model that contains a kernel estimator for each training set item we c reate successively coarser models that represent good approximations.

Our first statistical approach is based on [7] and is called Goldberger in the following. The Goldberger approach assumes two initial mixture models f and g to be given, where f is the finer model with r components and g an approxi-mation with s components, hence r&gt;s . Each component is assigned a weight and is specified by its mean and covariance matrix. To measure the quality of the approximation [7] defines the distance between two mixture densities as: Definition 1. Let f = r i =1  X  i f i and g = s j =1  X  j g j be two mixture densities containing r and s Gaussian components f i and g j with their respective weights  X  i and  X  j . The distance between f and g is then defined using the Kullback-Leibler divergence KL [4] as follows The optimal model  X  g reducing f to s components is  X  g =argmin g ( d ( f, g )). Since thereisnoclosedformtocompute X  g , a local optimum is computed iterating the following two steps until the distance d ( f, g ) does no longer decrease. Therein  X  ( i ): { 1 ...r } X  X  1 ...s } is a mapping function that assigns each component in f to a component in g .  X  Regroup: update  X  :  X  ( i )=argmin s j =1 { KL ( f i ,g j ) }  X  Refit: for each component g j recompute weight  X  j ,mean  X  j and covariance We devise a bulk loading technique based on [7] as follows. To initialize the mixture g we compute a first mapping  X  0 by assigning 0 . 75  X  M components from f to one component in g according to the z-curve o rder of their mean values. M is given through the fanout, which in turn is dictated by the page size. When no more changes occur in step 2, the resulting components g j are converted to Bayes tree nodes containing the entries f i with  X  ( i )= j . Since the final  X  might map more than M components from f to a single component in g ,we investigated several strategies to restrict the fanout to the given boundaries. First we reformulated the regroup step into an integer linear program with constraints regarding the resulting fanout. However, for realistic problem sizes, this approach took way too long to compute a complete bulk loading. Hence, we decided for a post processing after the mapping  X  was computed, which splits the nodes that contain too many entries. Therefore two representatives are computed by moving the mean along the dimension a with the highest variance  X  a by an =  X  a / 2 in both direction. A Gaussian is placed over the two representatives and the mapping of the entries to the representat ives is computed as in the regroup step. If a node contains too few entries it is merged with the node closest to it in terms of the Kullback-Leibler divergence.
 Virtual sampling. The second approach, called virtual sampling, uses the work presented in [17] and does not rely on t he KL divergence. The virtual sampling approach assumes a given mixture model f = i =1 ..r  X  i  X  f i containing r com-ponents and computes a coarser mixture model g = j =1 ..s  X  j  X  g j with s&lt;r components. The components f i = G ( x,  X  i , X  i ) constitute multivariate Gaus-sian normal distributions with their respective weight  X  i (analogue for g j ). To derive an algorithm the following model is utilized: the mixture g can be com-puted using samples R 1 ...R r from each component in f with R =  X  i =1 ..r R i and | R i | =  X  i  X | R | . Assuming independence of the sample points from different components in f yields the assumption that they can be assigned to different components in g while samples from the same f i are likely to be assigned to the same g j . Based on this assumption hidden variables z ij are introduced that indicate for each component f i its assignment to the corresponding g j . While the z ij are binary during initialization, they can take values between 0 and 1 during theiterations.ThehiddenvariablesareusedinamodifiedExpectationMaxi-mization algorithm to compute the coarser mixture g as follows (superscripts f and g are added for readability to indicate the origin of the components):  X  Expectation: z ij =  X  Maximization: The above equations are independent of the actual samples R i and can be com-puted directly from the mixture components in f , hence virtual sampling .To use the described bottom up method for bulk loading we have to provide an initialization for the hidden variables z ij . The initialization of the mixture g is done as in the goldberger approach described above. After getting the final val-ues for z ij from the virtual sampling algorithm, we assign each f i to that g j with the maximum z ij for all j . Moreover, the result has to comply with the fanout parameters m and M of the Bayes tree. This is achieved through merging and splitting of the resulting components g j . If a component g j is assigned less than m components f i , these components from f are assigned to the g j with the second highest z ij .Ifmorethan M components f i are assigned to one g j , g j is duplicated while moving the resulting two means in opposite direction along the dimension with the highe st variance. The respective f i are reassigned to the more probable candidate according to the density of their mean  X  i .Aftermerg-ing and splitting the corresponding mixture parameters are adapted following the above equations.
 EMTopDown. Besides the above mentioned bottom up approaches we imple-mented a top down approach that recursively splits the training set into several clusters. In contrast to the previous approach, where Gaussian components were merged and mapped, we now operate solely on the data objects. More precisely, we start by applying the EM [6] algorithm to the complete training set. The desired number M of resulting clusters is always set to the fanout which is again given through the page size. If the EM returns less than m clusters, the biggest resulting cluster is split again such that the total number of resulting clusters is at most M . In the rare case that the EM returns a single cluster, this cluster is split by picking the two farthest elements and assigning the remaining elements to the closest of the two. Finally, if a resulting cluster contains more than L objects (the capacity of a leaf node), the cluster is recursively split using the procedure described above. Otherwise t he items contained in that cluster are stored in a leaf node, its corresponding entry is calculated and returned to build the Bayes tree. The EM approach may result in an unbalanced tree, which differs from the primary Bayes tree idea. However, as we will see in the experimental section, the results show that this is not a drawback but even leads to better anytime classification performance. 3.3 Data Base Driven Approaches Since the Bayes tree extends the R-tree, we employ traditional R-tree bulk load-ing algorithms for comparison. We implemented two types of space filling curves, namely Hilbert curve and z-curve. We briefly describe the Hilbert curve ap-proach, the z-curve bulk loading works analogously. The bulk loading according to the Hilbert curve is a bottom up approach where in the first step the Hilbert value for each training set item is calculated. Next the items are ordered ac-cording to their Hilbert value and put into leaf nodes w.r.t. the page size. After that the corresponding entry for each resulting node is created, i.e. MBR, cluster features (CF) and the pointer. These ste ps are repeated using the mean vectors as representatives until all entries fit into one node, the root node. Theory on creating multidimensional Hilbert curves can be found in [1], for implementa-tion guide lines see [13]. Additionally we implemented the partitioning approach presented in [14] that is called sort-tile-recursive. The basic idea is to build a hier-archy of rectangles which have, at the same level of the hierarchy, approximately the same expansion in each dimension. For details please refer to [14]. The three proposed bulk loading techniques Goldberger , virtual sampling and EMTopDown are compared to the existing R-tree bulk loading approaches Hilbert , z-curve and STR and the previous results from [15] (called Iterative in the graphs since it performs iterative insertion of objects). We also used the same settings as in [15], i.e. we use the same data sets, performed 4-fold cross validation and show the classification accuracy after each node averaged over the four folds. We used global best descent and the qbk improvement strategy as they showed the best results in [15]. Please note that the bulk loading is done per fold once and offline and the resulting classifier is then used on the data stream. Since our focus is on anytime classification, we do not study the time performance of the bulk loading algorithm but the performance of the resulting classifier, i.e. its anytime classification accuracy.
 The top left part of figure 1 shows the results for the pendigits data set. The Goldberger approach fails to improve the accuracy over the iterative insertion for the first 50 nodes. After that it performs slightly better, but cannot increase the accuracy by more than 1%. Virtual sampling performs worst on this data set. The Hilbert and z-curve bulkload yield comparable results, their corresponding curves show a steep increase similar to t he iterative insertion and show bet-ter performance in most cases. After falling behind during the first nodes, STR performs equaly well compared to Iterative. The EMTopDown bulkload outper-forms all other approaches and improves the accuracy over the iterative insertion constantly by 3% or more on this data set.

The performance of the Goldberger bulkload stayed below the iterative inser-tion in the majority of our experiments. Just on the Letter data set it improved the accuracy for larger time allowances (cf. Figure 1, right). For the first 40 nodes Goldberger and Iterative perform equally well, after that the accuracy of Iterative stays behind that of Goldberger. While the virtual sampling and STR bulkload shows similar performance to Iter ative, Hilbert and z-curve (which are again in close proximity to each other) show constantly better accuracy than It-erative. The EMTopDown again constantly yields the best accuracy up to 13% better than the iterative insertion.

To facilitate an easier comparison between the different approaches we report the values for the normalized area under the anytime curves for Pendigits, Letter, Vowel, USPS and Verbmobil in Figures 1 (bottom) and 2 (top) respectively. Throughout the data sets Hilbert and z-curve show nearly the same performance, while z-curve is usually slightly behind Hilbert except for the USPS data set. STR ranges between these two and the iterative insertion; it is never better than the former and never beaten by the latter. Surprisingly both statistical approaches exhibit the same weakness a s STR, i.e. they never outperform the z-curve bulk load (except Goldberger on Letter) and several times show even worse performance than iterative insertion. This is especially interesting since both approaches are initialized using the z-curve, however, only with 0 . 75  X  M entries per node (cf. Section 3.2). We discuss the reasons for this shortcoming of the statistical approaches at the end of the section where we analyze the structure of the resulting trees. Finally, the EMTopDown bulk load shows constantly the best performance on all data sets despite the unbalanced resulting trees. Again we defer the analysis to the end of the section and first discuss a different issue. Figure 2 (bottom) shows the results for the gender and covertype data sets. For readability only the results for Hilbert and EMTopDown are shown. For both data sets k =2forthe qbk improvement strategy (cf. S ection 3.1). The graphs for EMTopDown and Hilbert usi ng the global best descent ( glo ) show an oscillating behavior on both data sets. For comparison we recapitulated the breadth first traversal ( bft ), the results are plotted as well. As was found in [15], the global best descent performs better than breadth first traversal. However, the graphs for bft do not show the oscillating behavior mentioned above. Since k =2,there is obviously a certain percentage of object whose class decision changes in favor of (or against) the tree which is currently refined. More precisely, these objects are likely positioned on the decision boundary between the two most probable classes. In global best descend refining mixture components close to the objects, and hence close to the decision boundary, affects the corresponding posterior probabilities more heavily than refinement of a farther component as in breadth first traversal. If we assume the oscillation to be a higher frequency added to a smooth underlying anytime curve, the percentage of these borderline objects corresponds to the amplitude. However, on balance, the oscillating behavior does not affect the superiority of the bulk loading over the iterative insertion.
To find reasons for the surprising ranking of the individual algorithms, we analyzed the structure of the resulting trees. Since more entries in a node cor-respond to more detailed information compared to less entries, we looked at the degree to which the nodes were filled in the different approaches. To this end we computed the average fanout per level of the trees from root to leaf. However, the resulting figures did not reveal any correlation to the found ranking of the ap-proaches. For example, both space filling curve approaches always fill the nodes to nearly 100% (except for the last one per level and the root), but the EMTop-Down sometimes produces less than M entries for a given node.

We found a correlation between the performance of the algorithm and the variance of the mixture components in th e resulting trees. More precisely, we calculated the average variance of all ent ries per level, Figure 3 shows the result-ing numbers for Hilbert, Goldberger, EMTopDown and Iterative on the Letter and Gender data sets. The variances are normalized by the variance of the entire data set per class. Level 0 corresponds to the leaves, Level 1 is above the leaves etc. The trees resulting from different a pproaches can have different heights as can be seen in the graphs. Hilbert bulk load fills each node to 100% and con-sequently yields the smallest trees, while the unbalanced trees resulting from EMTopDown are up to twice as high on the Gender data set.

EMTopDown and Hilbert show significantly smaller average variances com-pared to the iterative insertion. While the corresponding variances for Gold-berger are smaller than those of Iterative for the Letter data set they are larger on the Gender data set. This is in line with the observed anytime classification performances. We found comparable si milarities between the two measures on the other data sets. The average variances per level achieved by the EMTopDown bulk load were constantly amongst the lowest compared to all other approaches. This explains and underlines the superior performance of EMTopDown.

In general the EMTopDown shows the best results in terms of anytime classi-fication accuracy on all tes ted data sets and continuo usly improves the accuracy over that of the previous results in [15] up to 13%. This proves the effectiveness of our bulk loading approach for hierarchical anytime classifiers. We proposed three bulk loading approaches for hierarchical mixture models to improve Bayesian classification on data streams using the Bayes tree. We com-pared our approaches to the previously proposed iterative insertion [15] and three known R-tree bulk loading algorithms on a range of real world data sets. Experimental results showed that our novel EMTopDown bulk load constantly outperformed all other approaches a nd improved the accuracy by up to 13%. Surprisingly our two statistical approaches were outperformed by existing R-tree bulk loadings based on space filling curves. Further analysis attributed this shortcoming to a structural property of the resulting Bayes trees. The results of the analysis were in line with the classification results found in the experiments confirming the superior performance of our new EMTopDown bulk loading in terms of anytime classification accuracy.
 Acknowledgments. This work has been supported by the UMIC Research Centre, RWTH Aachen University.

