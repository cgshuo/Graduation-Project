 Consumer review sites and recommender systems typically rely on a large volume of user-contributed ratings, which makes rating ac-quisition an essential component in the design of such systems. User ratings are then summarized to provide an aggregate score representing a popular evaluation of an item. An inherent problem in such summarization is potential bias due to raters X  self-selection and heterogeneity in terms of experiences, tastes and rating scale interpretations. There are two major approaches to collecting rat-ings, which have different advantages and disadvantages. One is to allow a large number of volunteers to choose and rate items di-rectly (a method employed by e.g. Yelp and Google Places). Al-ternatively, a panel of raters may be maintained and invited to rate a predefined set of items at regular intervals (such as in Zagat Sur-vey). The latter approach arguably results in more consistent re-views and reduced selection bias, however, at the expense of much smaller coverage (fewer rated items).

In this paper, we examine the two different approaches to collect-ing user ratings of restaurants and explore the question of whether it is possible to reconcile them. Specifically, we study the prob-lem of inferring the more calibrated Zagat Survey ratings (which we dub  X  X xpert ratings X ) from the user-contributed ratings ( X  X rass-roots X ) in Google Places. To achieve this, we employ latent factor models and provide a probabilistic treatment of the ordinal ratings. We can predict Zagat Survey ratings accurately from ad hoc user-generated ratings by employing joint optimization. Furthermore, the resulting model show that users become more discerning as they submit more ratings. We also describe an approach towards cross-city recommendations, answering questions such as  X  X hat is the equivalent of the Per Se 1 restaurant in Chicago? X 
This work was done while CT was an intern at Google. Per Se is a famous high-end restaurant in New York City.
 H.2.8 [ Database Management ]: Data Mining; H.3.m [ Information Storage and Retrieval ]: Miscellaneous; J.4 [ Social and Behavioral Sciences ]: Miscellaneous Restaurant ratings; Google Places; Zagat Survey; Latent variables
The aggregate or average scores of user ratings carry signifi-cant weight with consumers, which is confirmed by a considerable body of work investigating the impact of online reviews on product sales [5, 7, 6, 11, 23, 41, 24]. For instance, Luca [24] finds that a one-star increase in Yelp rating leads to 5-9% increase in revenue; Ye et al. [41] show positive correlation between average rating and hotel online bookings. In a similar vein, Chevalier and Mayzlin [7] demonstrate that the relative market share of a given book across amazon.com and bn.com is related to the differences across the two sites in the average star rating of the reviews. 2
Due to the high visibility and salience of aggregate scores, ac-quisition of a large number of reliable user ratings becomes a crit-ical task in building a recommender service. An inherent problem in user-contributed ratings (and the derived item-level aggregate scores) is potential bias due to two sources. First, raters choose items according to their personal interests (e.g. which restaurants they visit) and they also decide which items they rate. As a re-sult, many raters only have experience with a limited set of items, and a particular item may be rated preferentially by a certain, non-independent group of users. Second, users have different tastes and different notions of what constitutes a good item or experi-ence. They also differ in their understanding and familiarity with the evaluation process (e.g. the meaning of  X  X our stars X  on the com-monly used five-star rating scale can be different to different raters). In this paper, we focus on restaurant ratings, examining two differ-ent approaches to acquiring user ratings, and attempt to address the problem of rater bias that is inherent to user-contributed data.
The first popular strategy we consider here is ad hoc data collec-tion, where users voluntarily submit reviews of places they know, which is employed by Yelp 3 and Google Places 4 . An advantage of this approach is that it allows a system to gather a large number of
We use the terms reviews and ratings interchangeably in this pa-per. www.yelp.com www.google.com/places ratings for many businesses relatively quickly. The problem with this approach is twofold. First, such voluntarily contributed ratings tend to exhibit a higher self-selection bias . Users are more likely to rate places where they have had either a great or a terrible expe-rience, which is known as the brag-and-moan phenomenon [16]. Ratings are typically submitted ad hoc , soon after users visit a restaurant, which may amplify rating bias. Second, users have dif-ferent motivations to submit ratings. Some users see reviews as part of their online persona, which affects which places they rate pub-licly [13]. In other words, some might choose to only rate high-end restaurants, while others, for example, may prefer and review only burger joints or bars. Compounding these problems is that some businesses have been known to hire people to write fake positive reviews which may further increase rating bias [27].

An alternative approach is to maintain a panel of raters that are invited to review a predetermined set of restaurants at regular in-tervals. Among review providers that employ this approach are the Michelin Guide 5 and Zagat Survey 6 . Focusing on a predeter-mined set of restaurants mitigates the effects of user self-selection bias; and maintaining a panel of raters makes the motivations to rate more consistent and objective. Repeated surveys at regular intervals decrease the variance in users X  evaluations. However, the panel approach achieves the higher quality of ratings at the expense of much smaller coverage. Usually, an inclusion of a restaurant in Zagat Survey is already a sign of popularity or distinction. In this paper, we examine two datasets representing two different approaches to collecting user ratings of restaurants. We obtained access to the user-contributed ( X  X rassroots X ) ratings from Google Places (GP) 7 and the aggregate  X  X xpert X  scores from Zagat Survey, which are published in the Zagat guides and on the web. We set out to understand and reconcile these different approaches. An inter-esting research question is whether the aggregate scores from the two approaches correlate with each other and whether it is possible to infer expert scores using  X  X rassroots X  data.

It somewhat complicates our task that GP ratings are one-dimensional (reflecting overall quality) whereas Zagat ratings for restaurants distinguish between three different aspects: food, decor, and service. Therefore, a naive approach, consisting of rescal-ing the average ratings in GP, is clearly insufficient. We need a method for inferring different Zagat quality dimensions from over-all GP ratings.

To solve this problem, we employ latent factor models from col-laborative filtering. We are inspired by the matrix factorization ap-proach, which is commonly used in state-of-art collaborative fil-tering. Our strategy is to use the latent features for restaurants, obtained from GP ratings, as feature vectors to regress on Zagat scores. More specifically, we add a virtual Zagat food reviewer, a decor reviewer and a service reviewer to the modeling process. To improve accuracy we perform joint optimization over GP ratings and Zagat ratings. www.michelintravel.com/michelin-guides/ www.zagat.com . Zagat restaurant guides were named as  X  X  necessity second only to a valid credit card X  by the New York Times. ( http://www.nytimes.com/1997/11/12/ dining/zagat-s-new-york-survey-entries-up-contributors-off.html )
In May 2012 Google Places was succeeded by a new prod-uct called Google+ Local, see http://googleblog. blogspot.com/2012/05/localnow-with-dash-of-zagat-and.html . We performed our analysis using data acquired prior to the launch of Google+ Local and hence we use the former product name i.e. Google Places.

Since GP ratings are discrete , we use an exponential family model to handle the discrete ordinal data. Furthermore, we explicitly al-low for different users and different places to have varying degrees of rating uncertainty (e.g. some restaurants might be mediocre but consistently so). Finally, we control for the effects of location, cui-sine, and price to infer Zagat ratings based on reviews posted on GP.

To summarize, we investigate the problem of predicting three-dimensional Zagat scores from one-dimensional user-contributed ratings in Google Places. We conduct an extensive comparison of different models and validate that joint optimization on expert Za-gat ratings and  X  X rassroots X  GP ratings improves the performance both in terms of RMSE (root-mean-square error) and correlation measures. We show that the model is able to reconcile the two different approaches reasonably well.

There are a number of other interesting findings from our re-sults: (1) It turns out to be considerably easier to infer food and service scores than decor scores from GP ratings. That suggests that Google Places users tend to care more about food and service than decor when they rate. (2) The results on user bias suggest that more experienced users tend to be more discerning. (3) We also find that decor and service scores consistently im-prove with price level, while the same only holds for food scores in the more expensive price categories, thus pointing to decor and ser-vice as the key differentiators between moderate and inexpensive restaurants.
 Section 2 summarizes related work. Section 3 provides a descrip-tion of the data used in the experiments and formulates the main problem. Section 4 introduces statistical models explored in this paper and describes the associated inference algorithms. Section 5 describes experiment setup and evaluation measures. Finally, Sec-tion 6 presents some empirical results; and we conclude with Sec-tion 7.
In this paper, we investigate how to aggregate noisy user-contributed GP ratings to predict Zagat expert ratings. In this context, the two most relevant fields of research are crowdsourced labeling and collaborative filtering.
Recently, massively distributed data collection techniques have become quite popular thanks to the advances in online commerce and growth of the Internet audience. Crowdsourcing services such as Amazon X  X  Mechanical Turk allow researchers to perform various labeling tasks. Researchers have studied the challenging problem of how to obtain accurate labels from groups of volunteer or paid workers [31, 10, 40]. A popular approach is to model the accu-racy of individual labelers, their aptitude for a specific task, and the difficulty of the task.

The problem is inherently related to the literature on standard-ized tests, particularly Item Response Theory [30]. Dawid and Skene [9] develop a method to handle polychtomous latent class variables. Whitehill et al. [40] simultaneously estimate the true label, item difficulty, and coder expertise. In our problem, the dif-ficulty of rating a place can be interpreted as item difficulty in Item Response Theory, even though the sources of variability might be quite different, i.e. a restaurant might simply serve food of varying quality as opposed to food that is difficult to assess.
Active learning is also employed to study how to better make use of labelers [45, 35]. Sheng et al. [35] use relabeling to obviate the effects of noise and Vijayanarasimhan et al. [38] identify promising crowdsourcing annotations given a limited budget. However, our setting is somewhat different, since we have a large number of user-contributed ratings from GP. Our main goal is to make better use of the datasets we already have.

Sample selection bias is an important problem in different dis-ciplines, such as economics, sociology and machine learning. The seminal paper by Heckman [14] analyzes a two stage approach. In the first stage, the probability of selection is estimated and then the probability is used as an explanatory variable in second stage. However, it is difficult to estimate the probability of selection with-out additional features and labels in our setting. We try to consider closeby places in a variation of our model to mitigate the effects of selection bias.
Collaborative filtering (CF) is a leading approach to building rec-ommender systems [33]. Koren and Bell [20] give a comprehensive overview on the state-of-art techniques. The data provided by GP falls squarely into the category of recommendation problems.
One of the most successful techniques is latent factor modeling, partially due to strong theoretical justifications for distributions on strongly exchangeable random variables: The Aldous-Hoover the-orem [3, 15] states that matrix-valued random variables that are invariant under row and column permutations must follow a latent variable model.

A popular strategy, which we adopt in this paper, is to character-ize both items and users as vectors in a space automatically inferred from observed ratings. In our problem, restaurants correspond to  X  X tems X  in collaborative filtering. Since Zagat ratings are in three dimensions, different parts of the latent space for restaurants could be related to food, decor and service respectively.

One of the most successful realizations of latent factor models is based on matrix factorization [39, 19, 42, 32, 17, 2, 29, 34]. These methods have become popular in recent years because of good scalability with predictive accuracy. It is worth noting that SVD++ in [20] adds smoothing vectors to user vectors to mitigate selection bias. Studies such as [21] explore the ordinal property of user-contributed ratings. Koren [18] also considers collaborative filtering with temporal dynamics.

A few recent studies [22, 28, 44] are concerned with transferring information between several domains of observation. This is re-lated to our goal of inferring Zagat ratings from the user-contributed GP ratings. However, in our problem, the two datasets are col-lected by different processes (ad hoc rating collection vs. planned surveys) and employ different scales. Moreover, the Zagat rating data effectively amounts to three highly different dimensions, with ratings available as one set of summary scores per place, i.e. as a triple like (food, decor, service). Hence the approach of Aizenberg et al. [2] does not directly apply. In their music recommendation system, they assume access to a very large number of playlists, considerably in excess of the latent representation dimensionality.
Notably, the work of Umyarov and Tuzhilin [36] present a frame-work for incorporating aggregate rating information for improving individual recommendation, which addresses the opposite direction of our problem. [2] also presents similar thoughts.
After the above informal discussion of previous research and a general introduction to integrating two disparate data streams, we now provide a formal description and discuss properties of the data in greater detail.
We focus our study on two datasets of ratings of US restaurants obtained respectively from Google Places and Zagat. The GP data is a random sample of 2 million user-contributed reviews collected between 2009 and 2011 in the range of { 1 , 2 , 3 , 4 , 5 } . Users vol-untarily submit ratings for places that they visited. This is common practice in online recommendation sites such as Yelp and IMDb. In this approach the aggregate scores tend to have a larger bias, for two reasons. First, raters self-select into providing the ratings, and thus are more likely to "brag and moan," which leads to bimodal or J-shaped rating distributions [16]. Submitting ratings shortly after the dining experience amplifies the "brag-and-moan" effect and in-creases variance. Second, there is considerable rater heterogeneity with respect to their motivations to contribute ratings.

As mentioned in the introduction, Zagat takes a different ap-proach by effectively employing a panel of raters and inviting them to rate a predetermined set of restaurants at regular intervals there is a pre-selected set of restaurants 9 evaluated post factum (i.e. not immediately after the dining experience), the bias of self-selection and  X  X rag-and-moan X  is somewhat mitigated in the pro-cess. Repeated surveys also decrease the variance in users X  expe-rience. This allows for more calibrated and less bimodal scores than user-contributed ratings. Finally, Zagat ratings are collected separately for food, decor and service on a 4-point Likert scale { 0 , 1 , 2 , 3 } . For each restaurant, the average of all the ratings for each dimension is computed after filtering out untrustworthy users. This yields scalars in the interval [0 , 3] , which we try to predict in our experiments. Zagat presents these dimensional scores by multiplying them by 10, i.e. on the characteristic 30-point scale in Zagat guides. Zagat data also contains a prevalence of relatively high food ratings. This is a direct consequence of the fact that an inclusion of a restaurant in Zagat survey is a sign of distinction or popularity. We use the ratings of all restaurants in Zagat survey.
An important aspect of the review data is that the noise is highly heteroscedastic [43]. That is, the variance of ratings is dependent on the actual score and additional covariates. As we will show in the experiments, it is helpful to model the variance of each restau-rant explicitly. The variance vanishes for restaurants with low and high aggregate ratings. This is not too surprising given that there are upper and lower limits to the scores that can be assigned. For instance, an excellent restaurant will have all reviewers agreeing on the excellence: Since there is nothing better than 3 or 5 points respectively in Zagat or GP that can be assigned, the variance will vanish. Interestingly, after controlling for scale differences, Zagat user ratings have a smaller standard deviation than the GP user rat-ings, suggesting that Zagat contributors are indeed more consistent.
Our goal is to predict Zagat expert ratings of restaurants based on user-contributed ratings in GP. In the rest of this paper, we use the formal variable definitions in Table 1.
 For a given place p , our goal is to estimate the three aspects of a Zagat rating s pz based on the Zagat ratings of other places and the available GP ratings s pr for place p . Clearly this task would be impossible if we had no access to Zagat ratings, since the latter are
We had no control over how the Zagat scores were collected.
In this paper, we refer to this set of restaurants as Zagat places, and restaurants not in this set as non-Zagat places. needed for calibration purposes. Formally, we aim to estimate: s pz |{ s pr } X  X  s p 0 r } X  X  s p 0 z } where p 6 X  X  z and p
Our method of choice is a latent factor model with bias. In a nut-shell, such models use inner products and bias  X  u,v  X  + b to infer the rating of a place. We assess the fidelity of the estimates both by reporting the root-mean-square error (RMSE), i.e. the square root of the average squared deviation between estimates and true rat-ings. In the experiment on GP ratings only, we also compute the data log-likelihood to see the effects of different variations of mod-els. While the former amounts to a quantifiable prediction error, the latter specifies how good our model is in representing the dis-tribution of the scores, which can lead to a better representation of place vectors.
The statistical model for inferring ratings for both GP and Zagat consists of three key components: These components are combined to obtain an estimate of the like-lihood of the data. We use a maximum-a-posteriori estimate for inference. To address concerns of computational efficiency we use joint stochastic gradient descent, as is common in collaborative fac-torization methods [20]. In the following section, we give a descrip-tion of the statistical components and how the model is applied to GP and Zagat ratings. Inner Product. We assume that for each rating s pz or s pr exists a latent score y pz or y pr respectively that can be obtained by means of an inner product model between attributes specific to a rater and attributes specific to a place. That is, we assume Here Eq. (2a) is a standard factorization model with bias. To model the bias , Eq. (2b) follows from the assumption that the biases for a given rating are additive between raters and places, and further-more, we want to take a common rating bias into account. In other words, we effectively perform row-wise and column-wise mean re-moval on the rating matrix.

The place factor  X  u p decomposes hierarchically Eq. (2c) based on the side information available at prediction time. That is, provided that we know the location, category, and price level of a place, it is reasonable to assume that these terms should affect the latent attribute space of a place.

Finally, for the rater factor , Eq. (2d) takes selection bias between raters into account by assuming that raters visiting similar places should share common preferences. This is a direct application of the model described by Bell and Koren [20], with an application of hierarchical models from [1].
 Emissions model. We now need to connect the latent score y the observed scores s pr . If we ignore the fact that s pr discrete (in the case of Zagat it is the average of a finite number of curated ratings and thus continuous, while for GP it can only take 5 distinct values), we obtain s pr  X  X  ( y pr , X   X  1 ) , i.e.  X  log p ( s pr | y pr ) =  X  Note that for notational convenience we parametrize the Gaussian model in terms of its precision  X  rather than variance  X  2 The larger  X  , the lower the noise that we assume in the estimation process.

A more principled approach to modeling discrete data is to em-ploy a properly normalized exponential family model. Denote by Y = { 1 ,..., 5 } the ordinal range set of ratings. In this case we may replace the normalization in (3) by:  X  log p ( s pr | y pr ) =  X  The advantage of the above model is that it is capable of modeling rating distributions for both excellent and poor places, simply by picking y pr &gt; 5 and y pr &lt; 1 respectively. These choices concen-trate the distribution more towards the extreme ranges of ratings. In the experiment we show that this choice does lead to a better likelihood estimate.

Note that, unlike in (3), the expectation of the renormalized Gaus-sian distribution does not satisfy E [ s pr | y pr ] = y compute the expectation via: This is also what we use to verify the accuracy of the estimates in an RMSE sense. Note that (4) has a unique minimum and can be reparameterized convexly.
 Priors. To complete the model specification, the final piece re-quired is to discuss the priors on the latent variables u,v , the biases b , and the precisions  X  . We impose a Gaussian prior on the first two latent variables, b r and b p , a flat (improper) prior on the bias b , and a Gamma prior on the precisions. These choices are in the spirit of SVD++ [20].

More specifically, we assume that: In other words, we assume that all the parameters are drawn from a Normal distribution with precision  X  . For simplicity, we use the same precision  X  to avoid a large number of parameters.

Furthermore, we model the common bias as a scalar, normally distributed according to b  X  X  (0 , X  0  X  1 ) . Here  X  0 denotes the pre-cision of the biases. For an improper prior on the latter set  X  the same as in [20]. With this improper prior, b can be computed by averaging across all the ratings.
 Finally, the precisions  X  are drawn from a Gamma distribution. Note that one design choice is to model the precision of each place individually. We have:  X  log p (  X  p |  X , X  ) =  X  X  p  X  (  X   X  1) log  X  p  X   X  log  X  + log  X (  X  ) . Note that the hyperparameters  X  and  X  are fixed. Thus, for in-ference purposes we are only concerned with the contribution of  X  X  p  X  (  X   X  1) log  X  p to the log-likelihood.
To predict the rating for a user, we can treat the problem simi-lar to a classical collaborative filtering problem. We address it by combining the components regarding s pr | y pr (Gaussian or renor-malized Gaussian), the priors on u,v,b , the priors on the precisions  X  , and the choice of whether we model rating uncertainty as being specific to a user or to a place. Such a multitude of choices leads to six different models that we can explore experimentally: SVD++ or SVD . We assume that s pr | y pr is Gaussian and that all SVD++ Rater or SVDRat . The change is that we now assume that SVD++ Place or SVDPla . Same as above, only now we use  X  SVD++ Renormalized or SVDRen . This is the same as SVD++, SVD++ Renormalized Rater or SVDRenRat . As with SVD++ Renormalized Place or SVDRenPla . As above, but with To simplify representation, we refer to these six models as SVD , SVDRat , SVDPla , SVDRen , SVDRenRat , SVDRenPla respec-tively in the experiments, as denoted above.

For illustration purposes we summarize the objective function for the last model. We use R to denote the set of reviews. Up to additive constants and perusing the inner product model of ( 2) the negative log-likelihood L is given by As a review of our different components,  X  p in (7) corresponds to place precision. Alternatively, we can use  X  r to consider user preci-sion. The first part in (7a) corresponds to the simplified assumption in Eq. (3) (without additive constants), while the second part in (7a) is for the renormalization. (7b), (7c), (7d) correspond to the priors for different variables in the model.

Inference is performed by joint stochastic gradient descent in all parameters of L as we traverse the set of reviews and ratings R . This is known to yield accurate results [17].
We now proceed to integrating Zagat ratings with those obtained from GP. In this way, the latent attributes for places can be used to model not only GP but also aspects that are important for Zagat, such as food, decor and service. We achieve this goal by adding three virtual raters with associated factors v zf ,v zd and v instance, the equation amounts to an estimate of the Zagat food rating.

In learning how these attributes correlate with the observed Zagat ratings we are able to obtain an  X  X nstant foodie X : A mechanism for translating the idiosyncratic, crowdsourced ratings from GP into the more consistent, better-calibrated ratings 10 of Zagat.
That said, before embarking on a full model of Zagat ratings we need to address the fact that Zagat ratings are not awarded at ran-dom. The mere fact of a restaurant being  X  X agat rated X  promises a modicum of quality. In other words, there is an inherent selec-tion bias, skewing toward good restaurants. Hence we may decom-pose the distribution into first modeling whether a score is observed (observed p ,p  X  X  z ) and only then capture the actual value of the observation. It is similar to Heckman X  X  correction [14]. We add a softmax term to our objective function. Note that, we only perform this correction for Zagat ratings. 11
Using an exponential family model yields: Here C p denotes a set of closeby places in the local neighborhood of p (we choose 5 places within a 2 mile radius). This is to ensure
Recall that the Zagat publishes ratings on a [0 , 30] scale and we rescale the loss correspondingly.
We could add this to the GP ratings as well. But adding this to all the GP places can induce more noise. Compared to the differ-ences between non-Zagat places and Zagat places, randomly se-lected closeby places for GP ratings ( p,r ) is not clearly distin-guished from places that users chose to visit. that we only compare locally-equivalent places regarding their in-clusion in Zagat. v rated is an additional vector to evaluate whether a place is rated in Zagat ratings.

This yields two options when including Zagat ratings: a plain version that regresses on the ratings as if they were additional users and a location calibrated version that uses Eq. (8) for debiasing.
For conciseness we use a factorial notation to describe the exper-iments. In (factorial) combination with the models of the previous section this yields the following grammar: { X  X ,Clo}  X  SVD  X  { X  X ,Ren}  X  { X  X ,Rat,Pla}.

For instance CloSVDRenPla amounts to a model considering (1) closeby places to Zagat places, (2) rating debiasing using SVD approaches, (3) the renormalized Gaussian model for GP ratings, and (4) a place-dependent precision latent variable.
Maximizing the log-posterior is relatively straightforward by em-ploying a stochastic gradient descent procedure. Due to the non-convexity of the objective (it is convex in factors but not jointly so) we use a rather conservative learning rate adjustment [26] via Here t is a counter of how many observations have already been seen. We investigate different values for a in the experiment. More-over, for the Gamma prior we set  X  =  X  = 2 to adjust shape and rate respectively. All precision variables (  X  ) are initialized to 1, all latent factors ( u,v ) are initialized to random values, and all the bi-ases ( b p ,b r ) are initialized to 0. We traverse all ratings ( p,r ) and a set of Zagat ratings (if applicable) to perform stochastic gradi-ent descent updates. To ensure good convergence we traverse the space of observations 20 times (fewer would suffice but this is to ensure that we have accurate results for all settings). This leads to algorithm 1 below.
 Algorithm 1 Stochastic gradient descent Input All the Google ratings and some Zagat ratings.
 Output Latent factors for places.

Initialize counter t  X  0 for d in 1:N do end for
Note that the slices L t in the algorithm are essentially just parts of the negative log-likelihood L that are specific to a particular rat-ing instance ( p,r,s pr ) on GP or a corresponding triplet of Zagat rating.
We give a description of the experimental setup and evaluation measures. Then we show that our models outperform the baseline. Furthermore, we show that a joint model of Zagat ratings and GP ratings improves overall performance. Moreover, we find that place precision and exponential renormalization via a proper generative model helps in the context of predicting Zagat expert ratings. We conclude with a number of analyses and findings from the resulting model.
In the experiments below, we use 100-dimensional latent vectors throughout the experiments.
First, we want to conduct experiments on GP ratings to see how different versions of collaborative filtering algorithms perform and whether adding different components improves the performance. Also, to assess recommendation performance we need to address the fact that in the absence of additional features it is impossible to learn meaningful place vectors and predict accurately for places with very few ratings. Therefore, we test only on the latest ratings for restaurants with at least 3 reviews.
 Baseline. For experiments on GP ratings, our main objective is to see how different models compare with each other. Thus SVD effectively our baseline.
 Cross-validation. For experiments on GP ratings, in order to avoid bias by systematically removing a large number of recent ratings for validation purposes, we perform 5 -fold cross-validation in the following way. We randomly partition the latest ratings for restau-rants with at least 3 reviews into five partitions, one of which is used for testing, one for validation and three for training. Results are then averaged over the partitions. While there are considerably more advanced model selection tools from statistical learning the-ory available [37], the above is consistent and considerably easier to implement.
 Parameters. The parameter range investigated for the GP Gaus-sian Priors (  X  in Eq. (6)) was { 0 . 001 , 0 . 005 , 0 . 01 , 0 . 05 , was { 5000 , 10000 , 50000 , 100000 } . Baseline. For the Zagat ratings, we use two baselines: Average transformation. This algorithm simply takes the aver-Linear regression transformation. This is a strong baseline that Cross-validation. For Zagat estimation, we use a classical 5 -fold cross-validation since no validation set is needed for parameter se-lection. We directly use the best parameter in the corresponding GP experiments. The best parameter is mostly consistent between different folds in our experiments on GP. Whenever the best param-eters are different for different folds, we simply take the majority choice, i.e., the parameter setting that provides the most best per-formances in 5 folds.
We consider three evaluation measures in the experiment. For experiments on GP ratings, we try to predict user ratings for places. We use RMSE as a straightforward metric to measure the loss be-tween predicted scores and actual scores. We also include the log-
SVD refers to SVD++ in our paper. likelihood as our evaluation metric to see how good the model rep-resents the distribution of user ratings. For experiments on Zagat ratings, we try to predict ratings for each place aggregately. We use RMSE in this case, too. Since we have two lists of place scores, it also makes sense to see how good they correlate with each other, thus we also include Pearson Correlation as a second metric.
In the descriptions below, we use s to denote the true rating and, with some slight abuse of notation (in the context of renormalized Gaussians) we use y for the predicted rating.
 RMSE. The Root Mean Square Error describes the average differ-Log-likelihood. This measures more directly the statistical fidelity Pearson Correlation. This measures the degree of linear depen-
We first assess the performance of the estimates on GP ratings, to see whether modeling user precision or place precision improves on SVD (SVD++) and whether discretizing the ratings improves performance.

As we can see from Table 2, SVD provides the best performance in terms of RMSE, while SVDRenRat , i.e. SVD++ with expo-nential renormalization and user precision provides the best log-likelihood estimates. Employing exponential renormalization im-proves the log-likelihood but leads to worse performance in RMSE since what we are optimizing in this context is not the total loss but a renormalized likelihood. As expected, the choice of models depends on the choice of objective function.

It seems that considering user precision always works better than place precision. This suggests that variability for a given user is more helpful than variability for a given place in predicting user ratings for places.
We now describe estimation of Zagat ratings which constitutes the main task of this paper. Table 4 shows the performance of dif-ferent models and the two baseline approaches. Average baseline . As expected, the average score translation from GP ratings provides the worst performance (with RMSE being high at 0 . 539 ). The correlation between average score and Zagat decor score is quite low (correlation= 0 . 075 ). This suggests that when submitting the overall, single-dimensional rating, users tend to care more about food (correlation= 0 . 375 ) and service (correlation= 0 . 258 ) than restaurant decoration. And even for the food dimension, which has the highest correlation with the overall GP ratings, the value is low. This indicates that there are indeed substantive differences between the two sets of users and the two methodologies for col-lecting user evaluations.
 Linear regression baseline . The linear regression baseline per-forms quite well in predicting food and service scores. Note that an RMSE loss of 0 . 259 in the 3.0-scale is just 2 . 59 in the usual Zagat 30-point rating. We see a clear drop in accuracy on decor compared with food and service. This confirms that we can mainly learn about food and service related features from GP ratings alone. And the similar performance on food and service actually shows that normal users have the ability to distinguish food quality as well as service quality.
Jointly modeling Zagat ratings and GP ratings can further im-prove the performance compared to the linear regression baseline. All of the models that jointly optimize the loss on Zagat ratings and GP ratings improve both RMSE and correlation measures com-pared to the linear baseline. 13 This is a very encouraging result, suggesting our general approach improves Zagat rating prediction and it is possible to reconcile the two approaches to collecting user ratings.

However, the effects of considering user precision, place pre-cision, exponential renormalization and closeby places are quite complicated:
There is one exception on Food RMSE in CloSVDRat , i.e. in the model considering closeby places to Zagat places and rater preci-sion. Table 2: Google Places accuracy. Algo-rithm labels as described in Section 4.2.
 Note that SVD refers to SVD++ in our pa-per.
 Table 3: Zagat RMSE loss, stratified based on price levels.
As a result of the above observations, the model that provides the best average RMSE is SVDRenPla , i.e. factorization using a renormalized distribution and using place-specific precision. It im-proves by 0 . 024 in terms of average RMSE compared to the linear regression baseline, and by 0 . 257 against the naive average scores from GP ratings. As a reference of scale, in [17], SVD++ improves RMSE by 0 . 01 compared to SVD in 5 star ratings. 14
In terms of correlation, the improvements are also significant, especially in decor. But the best correlation in different dimensions is approached by different models. This suggests that we cannot make any conclusive arguments about the power of different mod-els. Note that the improvements relative to the baseline occur in all three aspects , which suggests that it is important to do joint opti-mization on two datasets.

When jointly modeling Zagat and GP ratings we see a larger improvement in decor compared to food and service. This suggests that GP ratings alone convey more information regarding food and service rather than decor, which is consistent with the observation in the simple average baseline.
 RMSE By Price Levels. We further check the RMSE performance of estimating the Zagat ratings stratified on price levels for the model SVDRenPla , since it provides the best performance. As we can see from Table 3, there is a clear reduction in error for $$$ and $$$$ places compared to $ and $$ places: we see a drop in RMSE in all the three dimensions and the drop in decor is more significant. For example, comparing the RMSE in $$$ places to $$ places, the RMSE in decor is decreased by 0 . 049 , while the RMSE in food is decreased by 0 . 018 . This indicates that our model does learn that price level is a better signal for decor than for food. On the other hand, since the scores are restricted from above, the es-timation problem actually becomes easier (at least on average) for pricier restaurants  X  they are likely to provide better food, service and decor due to efficiency of the markets.
To investigate implications, we shall focus on the best perform-ing statistical model, SVDRenPla . We investigate place effects [1, 5] opposed to [0, 3] in our case. such as which cuisines may be, a priori, highly and poorly rated. We also investigate issues such as whether rater bias is a function of rater X  X  experience; and whether the latent space representation is inherently meaningful by an application of cross-city search.
We now study how place effects such as price levels and cuisine types affect the final scores in Zagat ratings. Recall that our results are based on ratings submitted by users who have chosen to patron-ize specific places and then submit their ratings. In other words, the findings reported herein reflect dining experience through the eyes of the users who chose to report it.
 Price levels. To investigate the effects of pricing we compute the inner products between the latent price attributes u $ and the associ-ated rating vectors v zf ,v zd ,v zs . This effectively amounts to price specific offsets. Figure 1 shows how scores in food, decor and ser-vice change with price. With the exception of food for the $ or $$ price levels we see a monotone increase in each of the three dimen-sions. The fact that estimated food score for the $$ places is slightly lower than that in the cheaper restaurants suggests that at the low-est price level, service and decor may be the key differentiators. Alternatively, this may be due to other reporting bias that we did not account for, i.e. customers having relatively lower expectations when rating inexpensive restaurants. We find that the estimated decor score exhibits the largest increase as price increases. The difference in the decor score between the $$$$ and $ places is as high as 1 . 0 , which corresponds to 10 . 0 on the usual 30-point Zagat scale. It appears that more expensive places are differentiated more by design and service rather than food (again as estimated from the ratings reported by GP users).
 Cuisine types. We now check how the estimated scores vary by cuisine type. For that we first compute the inner products between u cuisine and v zf ,v zd , and v zs , then sort within each dimension. Ta-ble 5 lists the top 5 cuisines types and the bottom 5 cuisine types. We observe Latin American and Korean restaurants among the top rated based on the food score, while hamburger and buffet-style restaurants appear at the bottom. Again, note that this is based on data from users who choose to patronize and rate respective restau-
Some of the findings may be also observed by simply averaging, which can be seen as sanity-check. Here we are more interested in exploring the results of our model. Figure 1: Price Level Effects. The numbers are all relative to the $ category. Note that the food score remains essentially un-changed for the $ vs. the $$ category.
 Table 5: Best rated and lowest rated cuisine types based on latent variables. rant types, so the results reflect the perception of those users rather than an objective consensus. For example, it is plausible that vege-tarian restaurants would be rated mainly by vegetarians, who might give them higher scores than non-vegetarians. However, the fact that fast food restaurants occupy the bottom tier on decor and ser-vice but not on food provides some external validity for our analy-sis.
Another question on user behavior is whether rater bias is related to rater X  X  experience. Figure 2 shows estimated user bias vs. log of the number of ratings per user. 16 As can be seen, the results are noisy but there is a clear decreasing trend, suggesting that users who give more ratings have a more negative bias. This indicates that as customers rate more places they may become less impres-sionable. Interestingly, Byers et al. [4] and Godes and Silva [12] respectively observe that average ratings for restaurants and books decrease over time. Our observation is consistent with this trend but from the perspective of raters. Similarly, McAuley and Leskovec [25] find that experts give more  X  X xtreme X  ratings: they rate the top products more highly, and the bottom products more harshly.
Our use of latent variable models also makes possible what we shall call  X  X earch by example X . This can come in handy when mak-ing recommendations to people who are traveling or moving to a
We use logarithmic binning because the number of ratings per user is heavy-tailed. We remove data points with more than 256 ratings, because the error bars become too large to be meaningful. Figure 2: Rater bias as a function of the number of reviews. The error bar corresponds to 95% confidence intervals. different city. Suppose that the user is quite familiar with his or her home city and would like to find an equivalent of a particular place in the destination city. For instance, we can use the latent features to find restaurants most similar to  X  X ary Danko X  (a well known fine dining restaurant in San Francisco) in New York City. To do so, we remove city effects from the model and fix restaurant category. After that, we use the Euclidean distance between latent vectors u p + u $ to find the closest restaurants in the destination city to the selected restaurant in the source city.

We show a few such examples in Table 6. We choose a couple of characteristic places from San Francisco and New York, and for each of them find the best match in a different city. We note that the matches intuitively make sense, e.g. with Per Se corresponding to Alinea. Some notable exceptions are Denny X  X  as a purported San Francisco equivalent of Shake Shack (where one should probably expect In-N-Out Burger) and Starbucks suggested by our model as one of Chicago X  X  answers to Tartine (a popular and well-regarded San Francisco bakery).
We have discussed the problem of inferring expert Zagat-style three-dimensional restaurant ratings based on noisy user-contributed one-dimensional ratings from Google Places. Inspired by research in collaborative filtering, we employ a latent factor model to link Zagat ratings with GP ratings.

Joint optimization over the two datasets can indeed improve the performance in terms of both RMSE and Pearson correlation com-pared to the baseline. Curiously, we find that the improvement is more prominent in estimating decor than in food and service scores. This indicates that user-contributed ratings in GP are more likely to reflect the quality of food and service. Without the joint optimiza-tion, most latent features do not provide information about decor.
We have explored a number of variations on our model. Ex-ponential renormalization leads to better performance in terms of log-likelihood in GP ratings and also the best performance in terms of average RMSE for Zagat ratings. It validates the effectiveness of considering the ordinal rankings using exponential family models. Based on different evaluation measures, the best performance was generally achieved by incorporating either user precision or place precision. This suggests that in similar applications it may be use-ful to explicitly model rating variance within users and places.
In general, we have shown that it is possible to reconcile the two quite different approaches to collecting user ratings. Using the Table 6: Similar places in different cities based on latent vari-able data. SRC represents source, and DST means destination. noisier user-contributed ratings from Google Places, we are able to infer Zagat-style expert ratings reasonably well. This suggests that it may be a good idea to combine these two approaches (ad hoc rat-ing collection and planned surveys) in the design of recommender systems so that a large number of ratings can be collected and then transformed to aggregate scores of better quality.

Posterior analysis also suggests some interesting research prob-lems in user behavior analysis. For instance, we observe that as users submit more ratings, they tend to become more discerning overall. It is not clear whether this phenomenon holds in differ-ent application domains; there may be multiple behavioral expla-nations. Therefore, a better understanding of the cognitive, so-cial, and technological processes that drive the production of user-contributed ratings is necessary for designing better recommenda-tion platforms.
 Acknowledgements. We thank Lillian Lee, Steven Scott, and An-drew Tomkins for helpful comments and discussions. We thank Jessa Mittleman for suggesting the idea of  X  X earch by example X . Chenhao Tan is partially supported by NSF grant IIS-0910664 out-side of the internship.
