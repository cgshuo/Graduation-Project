
MARCELLO CIRILLO, LARS KARLSSON, and ALESSANDRO SAFFIOTTI  X  Orebro University, Sweden 1. INTRODUCTION
Public interest in home robots is steadily increasing and people are looking at robots as new means to improve the quality of their everyday life. The aging of the population in many countries, for instance, could open a wide space for new applications [Tapus et al. 2007]. Robots could then become effective work-ers, precious butlers and, eventually, friendly helpers in our houses. Similar opportunities could also arise in industrial environments.

The identification of suitable interfaces for the interaction between humans and robots is only one of the many challenges that the cohabitation introduces.
The presence of humans in the space where robots operate also has a profound influence on how the embodied agents should perform high-level reasoning and plan their actions.

As many researchers have already pointed out, a classical AI planning sys-tem in which the robot is in control and the state of the world is only affected by the actions of the robot [Nau et al. 2004] is not applicable anymore. In order to be effective collaborators in household environments, the robots should include the state of the humans in their control loop [Jenkins et al. 2007]. We believe that, as remarked by Hoffman and Breazeal [2007b, 2007a], the interaction and teamwork of robots and humans can be greatly improved if the former can anticipate the forthcoming actions of the latter.

In this article, we propose human-aware planning as a way to improve the ability of robots to coexist with humans. Human-aware planning adheres to the aforesaid suggestion by Hoffman and Breazeal and goes beyond it, in that the robot takes human actions into account not only when acting, but also when planning its future tasks. In human-aware planning the robot uses a forecast of the entire plan, or set of possible plans, that the human is expected to perform, and generates plans that achieve the given goals while respecting a given set of interaction constraints with the human. Interaction constraints can be used to enable a closer cooperation between the robot and the human without endangering the safety and comfort of the latter.

In general terms, human-aware planning can be applied to situations in which there is a controllable agent (the robot) whose actions we can plan, and an uncontrollable agent (the human) whose action we can only try to predict, sharing the same environment. The main contribution of this article is to propose a technique for human-aware planning.

A simple example summarizing our approach to human-aware planning can be seen in Figure 1. A robotic vacuum cleaner has the goal to clean the apartment in which a human lives. The robot is provided with a prediction of what the person is going to do in the morning hours (at the bottom of the figure) and it plans accordingly its cleaning operations (the policy at the top of the figure) avoiding to operate where the user is predicted to be. This is indeed a simple example (we have only one prediction for the human and the robot policy generated by the planner is simply a sequence of actions), but it clarifies a core point of our approach: human actions are predicted, not planned, and they are used as input for the task planner of the robot.

For simplicity, in this article we assume that there is only one robot and one human. However, we do consider multiple alternative plans of the human, and we allow the domain to be partially observable. Accordingly, our planner generates policies conditional on what the human is observed doing. Actions have duration, although for the purpose of this work we assume that durations are deterministic.

The main focus of this article is on the planning algorithm. In order to provide the complete picture, however, we also describe the inclusion of our planner in a full framework for human-aware task planning. This framework includes a plan recognition system for estimating the plans carried out by a person, the human-aware planner itself, and an execution and monitoring system through which the robot plans are executed. The framework has been implemented and tested in a smart home, thus showing that our human-aware planner can be incorporated in a closed loop from real sensor data to real actuation. 2. RELATED WORK
In recent years human-robot interaction and human-robot cohabitation have become very active research topics in different disciplines. Autonomous and semi-autonomous robots are more and more integrated in everyday environ-ments, such as in museums [Shiomi et al. 2006], train stations [Shiomi et al. 2008], and shopping malls [Kanda et al. 2009]. Researchers in the robotics field are trying to overcome technical problems and finally bring autonomous robots into everyday environments. Examples include the DustBot project [Mazzolai et al. 2008], where a group of cooperating robots is used to improve the management of urban hygiene; the URUS project [Sanfeliu and Andrade-
Cetto 2006], that aims at the development of new ways of cooperation between network robots and human beings in urban areas; and the PEIS Ecology project [Saffiotti et al. 2008], in which advanced domestic tasks are performed through the cooperation of many simple robot devices pervasively embedded in the environment.

At the same time, psychologists and designers are at work to create more socially acceptable robots and to understand the reactions generated by robots on human subjects in different environments and situations. Examples of such interdisciplinary studies can be found in the work of Hiolle et al. [2009], that tries to assess how humans would react to the interaction with robots showing different attachment profiles, and in the study of Lee et al. [2009], in which de-signers, computer scientists, behavioral scientists, and robotics experts worked together to design a socially acceptable robot intended to provide snacks in office buildings. Similar studies have been performed that specifically address the needs of the elderly population [Cortellessa et al. 2008].

In the field of robotics, works that consider human-robot cohabitation of-ten take a viewpoint which is different from the one that we adopt here, fo-cusing on aspects such as safety (e.g., in the MORPHA project [Graf et al. 2004]), acceptable motion (e.g., within COGNIRON [Akin Sisbot et al. 2006] and in the frame of the URUS project [Pandey and Alami 2009]) or human-aware manipulation [Sisbot et al. 2007]. The problem of task planning in the presence of humans is currently still open, although some researchers have started exploring the issue [Alami et al. 2006; Broz et al. 2008]. Montreuil et al. [2007], Clodic et al. [2008], and Galindo et al. [2008] have addressed the complementary problem of how a robot could generate collaborative plans involving a human. In our work, by contrast, the robot does not plan actions for the human, but instead tries to forecast actions and plans of the human from previous actions. Our approach also diverges from the techniques devel-oped for plan merging [Gravot and Alami 2001], because in our case the hu-man is not controllable, and because some of the human X  X  actions can prompt new goals for the robot. As a consequence, the actions of the human cannot be rescheduled, and the plan of the robot cannot be created a priori and then rearranged. There has been extensive work on planning with external events (as human actions can be considered). An early example is the work of
Blythe [1994], which used Bayesian nets to compute the probability of success in the presence of probabilistic external events. However, the events considered in Blythe [1994] can occur in any state where their preconditions are satisfied and they do not have a concurrent nature, while in our case the planner can-not avoid the occurrence of a human action by negating a set of preconditions and temporal reasoning is used to synchronize human and robot actions. Our approach is also somehow reminiscent of early work on collaborative planning [Grosz and Kraus 1996].

An important aspect of our planning problem is that actions can be executed simultaneously and have durations, and that aspect has been addressed before in the literature. For instance, Mausam and Weld [2008] present an MDP model (fully observable) based on the concept of interwoven epoch search space, adapted from Haslum and Geffner [2001]. 3. THE PLANNER: HA-PTLPLAN
To support human-aware planning, we identified a set of important function-alities that need to be incorporated into a planner: (1) support for alternative hypotheses of the human plans, where a human (2) temporal duration of actions, both from the robot and from the human side; (3) possibility to specify Interaction Constraints ( IC ), that is, formulas that (4) support for partial goal achievement; and (5) support for observations on effects of human actions.

In this section, we present HA-PTLplan, a human-aware planner that in-corporates the preceding functionalities. Later in the article, we show how this planner has been incorporated into a full experimental system, including a simple plan recognition module and an execution module.

Our planner takes a set of possible human agendas, the interaction con-straints, and a set of goals as input and generates a policy for the robot that is compliant with the inputs. The possible agendas, together with an associated probability distribution, are assumed estimated by a separate plan recogni-tion module. As one doesn X  X  know in advance which of these agendas is the actual one, the planning problem has the property of partial observability. HA-
PTLplan, which is an extension of PTLplan [Karlsson 2001], generates policies which are conditional on observations relating to the human X  X  actions. 3.1 States, Situations and Actions A state s is represented in terms of a set of state variables and their values. The set of all states is denoted S .

An action a has preconditions Pr a : S  X  X  T , F } , time duration t cost function Cost a : S  X  R + , and a transition function Res such that Res a ( s , s ) is the conditional probability of going to state s when action a is performed in state s ( P ( s | s , a )). We assume in this work that an action always takes the same amount of time to perform, and cannot be inter-rupted once started. The end state, however, can both depend on the starting state and be stochastic. Note that actions include both those performed by the human and by the robot, with the notable difference that human actions do not have preconditions. The reason for this is that we do not want to predi-cate on what the human can do at planning time, and that the actions per-formed by the human can introduce new goals for the robot. For instance, in the cleaning robot example, a human action that entails a massive use of the kitchen would introduce the new goal for the robot to clean that room after-ward. We use HA and RA to denote the sets of human actions and robot actions, respectively.

An agenda is a finite list of consecutive human actions: ( a a  X  HA for all i .

A situation is a tuple s , rt , ht , ha where s is a state, rt when the robot latest action ended, ht  X  R + is the time when the human latest action ended, and ha is the remaining agenda of the human. The set of situations is denoted . Note that the concept of a situation can be generalized in a straightforward manner to have more robots and more humans, but for the sake of simplicity we only consider one of each here.

We can now define what happens when the robot performs an action a in a situation s , rt , ht , ha by extending the function Res situations. We assume that the outcome of an action applied in a state s is independent from the outcome of previous or future actions. The extended function, formalized in Eq. (1) that follows, is composed by two parts. The first part is the recursive case when the next human action a finishes before the robot current action a . In such case, the algorithm generates a new situation s , rt , ht , ha , where s represents the state of the world after the application of the human action a , rt is unchanged as the robot action has not been applied yet, ht is equal to the sum of the previous human time and the time duration of the human action applied ( ht + t a ), and ha is the human agenda without the first element, that is, a .
 If human actions were deterministic, there would be one state s , where
Res a ( s , s ) = 1, and a single new situation s , rt , ht outcomes, human action a could generate multiple intermediate situations and more than one branch could lead from situation s , rt , ht
All intermediate situations would have the same ht and ha , as human actions have deterministic duration, but possibly different states s . To calculate Res we need to sum the probabilities of all the possible outcomes of action a . The second part of function Res a applies when the robot current action a finishes before the next human action a , provided there is one.

Res a ( s , rt , ht , ha , s , rt , ht , ha ) =  X   X   X   X   X   X   X   X   X   X   X  when a = first ( ha )  X  ha = rest ( ha )  X  ht = ht + t a  X 
Res a ( s , s )
As an example, illustrated in Figure 2, consider three states s , s robot action clean such that t clean = 5and Res clean ( s actions watchTV and eatDinner such that t w atchTV = 4, Res t
Res clean ( s , 5 , 3 , ( w atchTV , eatDinner ) , s 2 , 10
Figure 3(a) shows an example of robot and human action transition where the human action has probabilistic effects. In this case, the human agenda in the initial situation is ( cook , eatDinner ), where t cook outcome, such that after its execution the kitchen can be dirty ( s with different states, s 3 and s 4 . In the former the kitchen is marked as dirty, whileinthelatteritisnot.

Finally, there are cases in which the same situation can be reached from different branches. Let X  X  consider the case represented in Figure 3(b). Here, human action grill ( t grill = 4, Res grill ( s , s 1 ) = lead to two different situations, one with state s 1 , where there is smoke in the kitchen, and another situation with state s 2 , where there is not. The results of the robot action v entilateKitchen , though, will remove the smoke if present.
Therefore, when the v entilateKitchen action is applied, the resulting situa-tion is the same independently from the intermediate belief situation. More formally
Res v entK ( s , 5 , 3 , ( grill , eatDinner ) , s 3 , 10
Res grill ( s , s 1 )  X  Res ( s 1 , 5 , 7 , ( eatDinner ) 0 . 5  X  1 + 0 . 5  X  1 = 1 .

P ROPOSITION 1. The result of the function Res a is always a probability dis-tribution.

P ROOF . As action durations are deterministic, both on human and robot side, and rt and ht are strictly monotone, the generation of situations when a robot action is applied is a tree structured in levels, where situation s , rt , ht , ha is at level 0 with an associated probability of 1. We now prove by induction that each subsequent level k is a probability distribution over n situations.

In the base case (level 0), we have only one situation  X  with probability p probability distribution p k , 1 , p k , 2 ,..., p k , n such that p k , 1 + p k , 2 + X  X  X + p k , n will still have a probability distribution over n k + 1 situations.
If we apply action a k + 1 to  X  k , i (0  X  i  X  n k ), we get a number of new sit-q , q probabilities at level k + 1 will be
If the final situation  X  = s , rt , ht , ha is at level k and can be reached through different branches, then there will be multiple identical situations  X  associated with each one of them. 3.2 Interaction Constraints
An Interaction Constraint ( IC ) is a formula that specifies how the robot should or should not interact with the user. In the vacuum cleaner scenario, for instance, we could specify that the robot should never clean or station in a room where, according to the agenda, the user is performing some actions. The main difference between interaction constraints and goals is that the latter are goals of achievement , that is, they must be true only in final states (or situa-tions, in our case) and can be violated in some cases leading to a less efficient, but still acceptable policy. Interaction constraints, on the other hand, are goals of maintenance : they must be true in all situations and a violation would not lead to a valid policy.

Our planner allows the specification of IC as temporal first-order logic for-mulas in the form al w ays (  X  ), where  X  is a first-order logic formula. Such for-mulas can use as arguments ht , rt and every predicate describing the state of the world. The constraints are checked in every situation and, in case of violation, the situation is discarded and the corresponding branch dismissed. An example of how interaction constraints are applied is shown in Figure 4.
Interaction rules can also include temporal constraints for the robot, for exam-ple, a service should be provided within 10 minutes after the human has gone to bed.

Let X  X  assume that a robotic walking aid is deployed in an apartment with an elderly woman. As interaction constraint, we want to invalidate every policy containing a situation in which the woman is in the bedroom and the walking aid is not in the same room, to be of support if she needs to stand up from the bed. This constraint is captured by the formula 3.3 Partial Observability
The framework presented before is sufficient as long as we have full observ-ability, that is, we always know the current situation, including the agenda of the user. Obviously, this is seldom the case in any real application. Therefore, we introduce partial observability [Kaebling et al. 1998] by defining a belief situation to be a probability distribution over situations. This implies that the robot can have alternative hypotheses about the current state of the world, the human X  X  agenda, and so on. Partial observability involves observations as results of actions. Those are introduced in the standard way by defining a function Obs a : S  X  O  X  [0 , 1] which specifies the probability of having an ob-servation o  X  O when action a is performed resulting in state s . An observation is a set of literals, that can be empty. If a is an action by the robot, then a is assumed to be a sensing action (like testing whether a door is open or closed).
If a is an action by the human, then o is still assumed to be an observation by the robot, but an indirect observation of a or some effect of a (if the TV is switched off, the system can observe that the watchTV action is over). Notice that also in the second case, the observation is dependent on the state, so we can specify that a human action only results in an observation under certain conditions.

The observation function Obs a only concerns states, not situations. For the latter, it becomes a function Obs a :  X   X   X  [0 , 1], such that Obs specifies the probability of a transition to a situation  X   X  sequence  X   X  (where is the set of all observation sequences) when action a is executed in situation  X  . This observation sequence consists of the obser-vations resulting from all human actions, that can provide observations, being completed between the situation in which a robot action starts and the one in which the same action ends. The last element of the observation sequence would be the observation resulting from the robot action. Note that this can be an empty observation, if a is not a sensing action. It must also be noticed that some human actions are not observed by the system and give the empty observation.

Eq. (2) formally defines the function Obs a .Asitwaswith Res is composed by two parts. In the base case, where the action a of the robot ends before the first human one, it returns the probability of getting observation o as first observation in sequence  X  when action a is performed resulting in state s , times the probability of getting from state s to s .Notethat s is the state of the resulting situation s , rt , ht , ha . The recursive case is defined as the probability of getting observation o  X  O : o = first (  X  ) when human action a is applied resulting in state s , times the probability of getting from state s to s , times the probability of observing the rest of the observation sequence  X  ( rest (  X  )) in the transition from intermediate situation s uation s , rt , ht , ha . In the recursive case, since we consider probabilistic outcomes of human action a , we could have multiple intermediate situations and more than one branch could lead to situation s , rt , observation sequence  X  . We therefore need to sum the probabilities of all the possible outcomes s of action a which will lead to the final situation. As each observation is independent from the previous and successive ones, it is easy to see that, as it was for Res a , the result of each sum and the final result of Obs indeed a probability distribution. Also note that calculation of Obs the terms from the calculation of Res a . Thus, Res a is built into Obs the case when there is only the robot actions, observations cannot be computed separately from transitions.

Obs a ( s , rt , ht , ha , s , rt , ht , ha , X  ) =  X   X   X   X   X   X   X   X   X   X   X   X   X   X   X 
As an example, let us consider the transition described in Figure 3(a). If we as-sume that the dirt in the kitchen would be detected, then we calculate Obs from situation s , 5 , 3 , ( cook,eatDinner ) to situation s [ dirt ( k ) = 1] (Eq. (3)). In the same way, it would then be straightfor-s , 10 , 7 , ( EatDinner ) .

In the standard POMDP model [Kaebling et al. 1998], different observations state b . It is possible to calculate the current belief state as the conditional probability distribution over the actual states, given the sequence of observa-tions and actions so far. Given the current belief state b , the action performed a , and the observation perceived o , it is possible to calculate the next belief state b as [Russell and Norvig 2003],
We can easily adapt the preceding formula to calculate the new belief situation b (  X  ), that is, the probability of a situation  X  in belief situation b , resulting from an action a performed in a belief situation b with observations in this case the factor Res a ( s , s ) is part of the definition of function Obs (Eq. (2)).
The denominator  X  in the previous equation is a normalizing factor, and is sequence  X  when action a is taken in b is
As there is a one-to-one correspondence between belief situations and observa-tion sequences, we also have that P ( b | b , a ) = P (  X  | 3.4 Planning Problem and Solution
A human-aware planning problem consists of: (2) a set of goals G to achieve in terms of constraints on states (logical for-(3) a set of interaction constraints IC ; (4) a set of robot actions RA ;and (5) a set of human actions HA .

To show a simple yet concrete example of a planning problem we can go back once again to the robotic vacuum cleaner scenario. The initial belief situation b (4) includes two different situations, each with a state representing the initial state of the world, and a different human agenda. Agendas, in this example, are composed by two human actions each.
The set of goals (5) includes two distinct objectives: to clean the kitchen and to clean the bedroom. In our setup, we attribute a greater value V to the former of these goals (5).

The IC (6) of this problem is simple: the robot should never perform an action or part of an action in the same room where the human is supposed to be. always (forall r: (not((robot-in=r) and (human-in=r)))) (6)
Finally, we complete the description of this human-aware planning problem providing the library of human actions the system can predict ( HA ) and a list of actions the robot can perform ( RA ). Robot actions are specified in terms of name, preconditions, time, and effects; the latter may be context dependent and/or stochastic, and may involve sensing (for the robot). In other words, the actions are of the type commonly found in POMDPs. Human actions are formally identical to the ones of the robot, but do not have preconditions. Our representation of action schemata is very close to the one defined in the
Probabilistic Planning Domain Definition Language (PPDDL1.0) as described can lead to observation, as our domains are partially observable. Robot actions are also parametric, as it can be seen in the following example, where the room to be cleaned is specified at planning time when the action is instantiated.
As an example of probabilistic and context-related outcomes, we can modify the previous action changing its results .
In this case, if the action is applied in a context in which the room is not clean, then the effect will be a cleaner room with probability 1. Otherwise, if the room doesn X  X  need cleaning, it will remain unaffected. In both cases the robot will be able to observe the effect of the cleaning ( obs(dirt(r) ).

The solution to a human-aware planning problem is a robot policy .A policy is a graph n 0 , N , E where the nodes N are marked with the robot actions to per-form there ( n 0 is the initial node), and the edges E are marked with observation sequences (possibly empty). Some nodes are marked with the special actions success and failure , and these nodes are terminal. The policy is executed by performing the action of the current node, and then selecting the edge matched with the observations that occur, following it to the next node, and repeating the process until a terminal node ( success , failure ) is reached.
A policy n 0 , N , E is admissible in an initial belief situation b the following conditions hold. Each node n i  X  N can be assigned a belief situa-from the action reaching to some other belief situation b observation sequence  X  , there is an edge marked with  X  from n node n j that has been assigned b j ; and there are no other edges. In addition, there should be no edges from terminal nodes.

A policy violates an interaction constraint ic  X  IC if and only if there is a node in it with an assigned belief situation in which ic is false.
The value (success degree) of a terminal node n j in a policy with assigned are computed (maximized) according to the Bellman equations, as is done for
Markov decision processes [Puterman 1994]. The cost is computed analogously but has lower priority: if there are several policies with maximal value, the one with lowest cost will be selected. The value and cost of the entire policy are the value and cost of the initial node n 0 .

A policy solves a planning problem to a degree p if: it is admissible, it only contains actions from RA , it has a value of p (or more) for the goals in G , and no interaction constraints in IC are violated. In addition, one can add requirements for the human agendas to be completed, alternatively not completed, in the success nodes.

Figure 5 shows an example of the first nodes of a robot policy that could be generated in the scenario described earlier. The initial node n the initial belief situation b 0 , that contains two possible situations, with the same initial state ( s ) and two different human agendas. When the policy is generated, the planner predicts which observations will become available at execution time and plans accordingly. For example, after the first waiting action ( stay-in-docking ), the human will be observed preparing a meal ( prepareMeal ) or watching television ( watchTV ); the system considers this information at planning time to branch in two different nodes ( n 1 and n time, will choose the appropriate robot action, continuing the execution on the right branch. If the human has been observed watching TV, for instance, then it will be safe for the robot to move to the kitchen and to clean it, because it knows that the human is not preparing or eating a meal. 3.5 The Algorithm We have extended the planner PTLplan [Karlsson 2001; Bouguerra and
Karlsson 2005] for probabilistic and partially observable domains to work on our representation for human-aware planning. PTLplan is a progressive plan-ation, exploring belief states/situations reachable from there until a policy with sufficient value has been found. PTLplan, which itself is an extension of TLplan [Bacchus and Kabanza 2000], uses temporal logic formulas to eliminate belief states/situations from the search space. The algorithm of the human-aware planner is detailed in pseudocode in Figure 6. As mentioned in the previous sections, our main extensions to the original planner are:  X  X he use of HA to include human actions, the addition to the belief situations of agendas consisting of such actions, and the fact that the effects of the human actions are taken into consideration when the new belief situations are computed in steps 5 and 7;  X  X he possibility to generate policies with partial goal achievement and the use of weights V ( g ) (step 10) to prioritize the accomplishment of a subset of the goals g  X  G ;  X  X he introduction of IC , that are checked in every state encountered while the new belief situation is computed (step 6), to avoid undesirable situations from the human side.
 We refer to Karlsson [2001] for further technical details about the original
PTLplan. We expect that other planners could be used as the basis for building a human-aware planner, by operating extensions similar to the ones given earlier. 3.6 Termination Conditions
In our planning problem, human actions can affect the state of the world, sometimes activating robot goals or invalidating previously achieved ones. For instance, in the vacuum cleaner scenario, the robot has the goal to clean all the rooms. The fact that they might be clean in the initial belief situation does not prevent the human from using them later and therefore invalidating some of the goals. A policy that greedily tries to achieve all the goals could therefore be suboptimal. On the other hand, we want also to avoid to plan robot actions when we have a lack of information on what the human is doing. We therefore adopt a search strategy in which the search is halted in every belief situation that either: (1) contains at least a situation in which the human agenda is empty ( rest ( ha ) = X  ); or (2) contains only human situations in which all human agendas have at most one action left ( rest ( rest ( ha )) search strategy is the one used in all the examples and experiments detailed in the article. 3.7 Complexity
As described in the previous sections, our algorithm is based on a breadth-first search strategy. The complexity of breadth-first search, both in memory and search time, is O ( b d ), where b is the number of children for each node and d is the depth of the goal. In our case, b is the number of robot action instances ( ai ) applicable at each planning step, times the number of possible outcomes of each action.

Our algorithm stops the search when it reaches a belief situation containing situations with zero or one human action left. Therefore the size of the search space (and the time required to explore it) is influenced by the length in time of the human agendas that the planner receives as input. The use of observations also influences the search space, because it allows the branching of the search tree in multiple belief situations.

To calculate the order of complexity of our human-aware planning problem in the worst-case scenario, when no heuristics are employed, we make some assumptions: (1) all the human agendas have the same length (in time) t every human action in the n ha human agendas is observable; (3) interaction constraints are never violated (hence, there will be no pruning of the search space); (4) all robot actions have a fixed duration t ra .

Under such conditions and with only one human agenda, the complexity of the search, both in memory and search time, would be in the order of O ( b
This is because we would start with one belief situation b situation and the observations we could get from the human actions would not lead to branching. Therefore, since we would continue the search until we apply all the human actions in the agenda, we would need to reach a depth of t
When n ha human agendas are used, we would have an equal number of are observable, this would lead to a branching of the search tree from b level 0 to n ha belief situations at level 1, each containing a single situation with one human agenda. The algorithm would then apply every robot action instance to every new belief situation to obtain level 2. Therefore, the algorithm should explore n ha subtrees of the same size of the one in which only one human agenda was provided. Under such conditions, the complexity of the search is in 4. PERFORMANCE EVALUATION
To evaluate the performance of HA-PTLplan as a stand-alone component, we designed two scenarios. The first one, the vacuum cleaner scenario, is an extension of the real test that we performed with the full framework. The second one is a purely simulated scenario, in which an autonomous table is re-quired to serve the human when needed. Our goal is to test the performance of the planner with respect to the number of applicable actions, the number of hu-man agendas provided, and the number of events that compose each agenda.
We tested each scenario first with a full exploration of the search space, to see the total difficulty of each problem, and then using heuristics, to verify what can be achieved in terms of planning time, success rate, and cost. The heuristic we used is based on domain knowledge, that is, we specified search rules based on our knowledge of the scenario that would prune less promising branches. For instance, for our vacuum cleaner scenario, we could specify a rule that invalidates the branches in which the robot actions are a sequence of movements from room to room without performing any cleaning action, as we know that such behavior is not likely to lead to an efficient final policy. All the agendas provided to the planner as an input are assumed to be equally probable. 4.1 The Vacuum Cleaner Scenario
A robotic vacuum cleaner is deployed in an apartment. The set of goals G given to the agent specifies that each room of the apartment must be clean and that the agent must be back at its charging station at the end of the plan. The only interaction constraint IC provided is the one already described in the previous sections, that is, the robot should never perform an action in a room where the human is supposed to be.

The robotic agent is provided with a set RA of four parametric actions: sleep(r) . All actions are specified in terms of name, preconditions, effects, and time. The effects include a measure of cost, which in this case represents battery consumption. It should be noted that the number of action instances applicable at each step is dependent on the number of rooms of which the apartment is composed.

The initial belief situation for the planner is automatically generated accord-ing to the number of rooms in the apartment and to the human agendas that are provided at each run. The elements common to all initial belief situations can be formalized as where n is the number of rooms generated and each i j is set to 1 with probability 0.3, 0 otherwise.

As said before, domain knowledge can be injected into the planner by means of formulas expressed in temporal logic. An example of the formulas used follows.
The operator next specifies the check to be performed in the next belief situation relatively to the one in which the formula is first evaluated. This formula rules out any transition from a belief situation b to a belief situation b such that: in b the robot is in a room with a dirt level d is still not in the room and the level of dirt is unchanged. Therefore, the robot is forced to clean whenever it is possible.

We analyzed the performance of our planner in three different test setups. 4.1.1 First Setup. In the first setup, we fixed the number of rooms in the apartment to 3, plus a special location, the robot-docking, where the robot can recharge its batteries and is virtually unreachable by the human. For this setup, we automatically generated 81 problems, grouped in 9 combinations, varying the number of human agendas (1, 3, 5) and the number of human actions that compose each of the agendas (1, 3, 5). The problems generated for this experimental setup are fully solvable, that is, a complete search would lead to a robot policy where all the goals have been achieved.

The human actions, also generated automatically, can be of two types: the user can move from one room to another (in such case the duration of the event set to a random time value, spanning from 10 to 120 minutes, when the problem is generated). Moreover, each event can be marked at problem generation time as observable by the system with probability 0.3 and can produce effects in the environment (that is, it can add new dirt on the floor and therefore affecting the goals of the robot) with probability 0.2. The actions of the human are specified like those of the robot but they do not have preconditions, as we do not need to test their applicability in the planning process. An example of a human action follows.
Each problem is solved twice: first exploring the full search space, and then injecting domain knowledge into the planner and perform another search.
Figure 7(a) shows the results in terms of execution time for this setup. As expected, the runtime increases with both the number of human agendas em-ployed and the number of events that compose each agenda. The use of domain knowledge proved to cut by almost 10 times the execution time and the number of nodes explored in the search. The generated policies solved the correspond-ing problems with the same success degree and the same costs as the ones generated without using domain knowledge.

It is worth mentioning that our actual scenarios, in which we use the planner in a real environment as the core of our human-aware framework, usually have the same complexity of the test runs of this setup. Therefore, our real problems are typically solved in a matter of a few seconds, which is more than acceptable considering that the time granularity we employ in this domain is one minute. 4.1.2 Second Setup. The problems analyzed in this second setup are gen-erated in the same way detailed earlier. The difference here is the number of rooms (5 instead of 3) and therefore the number of applicable actions at each step. Also in this case, we tested the planner on 81 automatically generated, fully solvable problems, grouped in 9 combinations.

Figure 7(b) shows the outcome of this setup in terms of CPU time. As in the previous setup, domain knowledge speeds up the computation, reducing the required time to less than 20 seconds even for the most difficult problems. The use of domain knowledge did not affect the success rate degree. However, in 5 out of 81 problems a slight increase in the cost has been observed. In the worst case, one of the problems with 5 human agendas each composed by 3 human events, the cost of the plan increased by 4.6% compared to the one generated by brute force.
 4.1.3 Third Setup. The third and final setup is meant to extensively test the performance of the planner also under circumstances that would not nor-mally arise in a real environment. In this setup, we generated 450 problems, each with a potential success degree of at least 0.8 (that is,
Here, the number of agendas used is increased to (5, 7, 9) and the number of events per agenda to (3, 5, 7). Fifty problems have been generated for each of the 9 combinations.

As we can see from Figure 7(c), also in this case the progression formulas significantly prune the search space (bottom). However, little loss in terms of success degree can be observed: the solutions of 5.1 % of the problems us-ing domain knowledge reported a loss in success degree of at most 6 %. The highlighted areas in Figure 7(c) show the problems whose complexity are com-parable to the ones we could face in our real settings when we would like to consider a high number of alternative human agendas: as can be noted, the planning time when search control formulas are used is hardly ever above one minute (dashed line). 4.2 The Moving Table Scenario
In the second scenario, the interaction constraints IC of the planner are de-signed to provide a service to the user, instead of avoiding interference with him. In this case, we used the planner to generate policies for a moving table that can autonomously bring drinks to the user [Di Lello and Saffiotti 2009].
The IC of the table is to be ready with a drink within ten minutes from the raise of a request by specific actions of the user.
The goal G of the table is to be back at its docking station when the execution of the policy is over. In this case, the set RA is composed by 5 parametric actions, that are specified as in the previous example. The agent can move from room to room, stay or sleep in a room (the sleep action lasts longer and has a lower cost), get a drink from the fridge, and deliver it in the room where the human is located.

We tested this scenario in a single setup, in which the robot must satisfy the user X  X  requests in an apartment composed by 3 rooms plus the special robot-docking location. As in the previous scenario, we automatically generated the initial belief situations, while keeping the initial positions of both robot and human fixed. We generated 300 problems, each with a potential success degree of at least 0.8, grouped into 6 combinations according to the number of human agendas (5,7) and the number of human events per agenda (3,5,7).

The human events that compose the agendas have the same structure of the ones detailed in the vacuum cleaner scenario. Each of them, at problem generation time, has a 0.3 probability of being observable by the system, and a 0.5 probability of arising the request for a drink. The duration of each event is also generated as in the previous scenario.

The 300 problems were first solved by brute force, and then by providing domain knowledge to the planner (one of the formulas employed, for instance, specifies that the robot should move only to the robot-docking or in the room where the human is located, in case he requires a drink).

As can be observed from Figure 8, also in this case the use of domain knowl-edge sped up the computation. The increased performance came at the price of a drop in the success degree in the solution of 7.3 % of the problems of at most 20%. 5. A FULL SYSTEM TEST CASE
We now describe how our planner can be incorporated into a full system for human-aware task planning. The system we describe has been imple-mented and deployed in a real robotic home environment [Saffiotti et al. 2008], including a variety of sensors and a mobile robot, where a single person is act-ing (see Figure 9(b)). We present here a single illustrative run to demonstrate the possibility to integrate our human-aware planner in a wider, real frame-work, the capability of the planner to work online and, in more general terms, the feasibility of our approach to human-robot interaction.

The system (Figure 9(a)) is composed of three main parts: a human plan recognition module, the planner, and an execution and monitoring module. 5.1 Plan Recognition Module
The task of the plan recognition module is to produce an estimate of the possible agendas being executed by the human, by taking as input sensor readings and a predefined set of potential agendas. From the perspective of human-aware planning, the most important aspects of this estimate are the forecast of the future actions that the human will perform and the detection of completion of recognized activities.

The used plan recognition module has a multilayer structure, and it refines and abstracts the data received as input from the sensors step after step. At the base layer, sensor data are collected and coupled. Using a rule-based system, simple instantaneous actions are detected (e.g., the presence of the user in the kitchen and the fact that the fridge door is open let us infer that the human is using the fridge). In the next layers, we use hidden Markov models to identify more complex activities and finally to recognize human plans among a set of predefined ones. Similar approaches have already been successfully used [Aoki et al. 2005], although in our case the HMMs are defined beforehand and not learned from training data. It should be emphasized that this component is not intended to advance the state-of-the-art, since the research issues related to plan recognition are not the focus of our work. However, it proved to be sufficiently efficient and robust to noise in the sensor data for the purposes of our experiments. 5.2 Execution and Monitoring Module
The executor receives from the planner the sequence of actions that the robot should perform and it sends them, one by one and with the appropriate timing, to the robot itself.

The monitoring module provides continuous support to the execution of the plan. If the plan recognition module updates the human plans identified, then a replanning signal is raised: the execution is suspended, the status of the robot and the effects on the environment of the actions of the robot so far executed are passed to the planner. The planner can thus calculate a new plan that will be consistent with both the environment status and the new human plans. 5.3 A Test Run
We have tested the system in our home environment where one user is execut-ing daily activities in the morning, from 8 am to 1 pm, following one of eight predefined agendas, as discussed in the previous section. During the same time span, a robotic vacuum cleaner has the task to clean the floor in all the rooms that need it, avoiding interference with the user. This means that the robotic vacuum cleaner must operate and wait only in rooms which the system has predicted as not occupied by the human at that time. In our test run, all rooms were marked as dirty from the beginning, so the robot must clean all of them.
The test run was performed in real time, that means that the experiment took five hours. The data processed by the plan recognition module was collected by real sensors in real time and plan recognition, planning, and plan execution were performed online.

The agenda executed by the user in our test run was the one marked A1 in Figure 10. Another agenda ( A2 ) among the predefined ones is very similar to this one. After a delay, the plan recognition module identified these two agendas as possible given the received observations, and passed both of them to the planner as suitable candidates for the subsequent actions of the human.
In general, when more than one agenda has been identified as possible, the policy to be executed by the robot will contain branches and the observation of the last completed human activity should be used to discriminate which action to perform next. In our case, the output policy contained one branch: the robot had to follow the policy marked b in case the system would observe that the human has completed the RELAX activity at the time of the branching.
Otherwise, in case such observation would not arise, the robot would follow the a branch. It is worth noting that in both branches the robot does not go back to the kitchen after the user has had lunch. This is a design choice: the planner does not generate policies longer in time than the human agendas, since we do not want to plan actions when there is no information about the state and the activity of the human.

In the actual run, the robot executed successfully every action marked in a darker shade, and it used the observation variable RELAX to discern the correct course of action. 6. CONCLUSIONS AND FUTURE WORK
We have presented HA-PTLplan, a human-aware planner designed to be part of a larger framework deployed in a real environment. To the best of our knowl-edge, this is the first planner able to take into account forecasted human actions at planning time. Such actions do not only impose constraints on the robot, for example, never schedule the cleaning of a room while the human is there. They can also be the source of new goals, for instance, the kitchen must be cleaned after the human has used it for cooking. The possibility of observing the results of some human actions can then be employed by the planner to identify the best policy to achieve the identified goals.

A limitation of our current approach is the use of actions with fixed du-ration. This is mostly a problem for the actions of the human, as they work as constraints in the planning process. As for the robot actions, this is not a major limitation because, as noted by Mausam and Weld [2008], one can first plan with only the expected duration of the actions and then improve or replan the policy with other possible/actual durations, still obtaining policies that are quite close to the optimum. Another limitation of our framework is the fact that we consider only one human and one robot in the environment.

In our future work we intend to overcome these limitations, expanding the planning algorithm to cope with multiple humans and introducing human ac-tivities with uncertain temporal duration. We will also test our system in more structured environments than household settings, like a factory production line, where the human agendas are less flexible because they are defined in strict protocols. Finally, we would like to expand our system to deal with mul-tiple robots with different tasks.

We are grateful to F. Pecora and R. Alami for many useful discussions on human-aware planning.

