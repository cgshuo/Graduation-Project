 For ambiguous queries, conventional retrieval systems are bound by two conflicting goals. On the one hand, they should diversify and strive to present results for as many query intents as possible. On the other hand, they should provide depth for each intent by displaying more than a sin-gle result. Since both diversity and depth cannot be achieved simultaneously in the conventional static retrieval model, we propose a new dynamic ranking approach. In particular, our proposed two-level dynamic ranking model allows users to adapt the ranking through interaction, thus overcoming the constraints of presenting a one-size-fits-all static ranking. In this model, a user X  X  interactions with the first-level ranking are used to infer this user X  X  intent, so that second-level rank-ings can be inserted to provide more results relevant to this intent. Unlike previous dynamic ranking models, we pro-vide an algorithm to efficiently compute dynamic rankings with provable approximation guarantees. We also propose the first principled algorithm for learning dynamic ranking functions from training data. In addition to the theoreti-cal results, we provide empirical evidence demonstrating the gains in retrieval quality over conventional approaches. H.3.3 [ Information Search and Retrieval ]: Retrieval Models Algorithms, Experimentation, Theory Diversified Retrieval, Structured Learning, Submodular Op-timization, Web Search, Ranked Retrieval
Search engine users often express different information needs using the same query. For such ambiguous queries , a single query can represent multiple intents  X  ranging from coarse ( e.g. , queries such as apple , jaguar and SVM ) to fine-grained ambiguity ( e.g., the query apple ipod with the intent of either buying the device or reading reviews).

Conventional retrieval methods do not explicitly model query ambiguity, but simply rank documents by their prob-ability of relevance independently for each result [8]. A ma-jor limitation of this approach is that it favors results for the most prevalent intent. In the extreme, the retrieval sys-tem focuses entirely on the prevalent intent, but produces no relevant results for the less popular intents. Diversification-based methods (e.g. [3, 12, 4, 11, 9]) try to alleviate this problem by including at least one relevant result for as many intents as possible. However, this necessarily leads to fewer relevant results for each intent. Clearly, there is an inherent trade-off between depth (number of results provided for an intent) and diversity (number of intents served) in the con-ventional ranked-retrieval setting, since increasing one in-variably leads to a decrease of the other. How can we avoid this trade-off and obtain diversity while not compromising on depth?
We argue that a key to solving the conflict between depth and diversity lies in the move to dynamic retrieval models [2] that can take advantage of user interactions. Instead of presenting a single one-size-fits-all ranking, dynamic re-trieval models allow users to adapt the ranking dynamically through interaction. Brandt et al. [2] have already given theoretical and empirical evidence that even limited interac-tivity can greatly improve retrieval effectiveness. However, Brandt et al. [2] did not provide an efficient algorithm for computing dynamic rankings with provable approximation guarantees, nor did they provide a principled algorithm for learning dynamic ranking functions. In this paper, we re-solve these two open questions.

In particular, we propose a new two-level dynamic rank-ing model . Intuitively, the first level provides a diversified ranking of results on which the system can sense the user X  X  interactions. Conditioned on this feedback, the system then interactively provides a second-level rankings. A possible layout is given in Figure 1. The left-hand panel shows the first-level ranking initially presented to the user. The user then chooses to expand the second document ( e.g. by click-ing) and a second-level ranking is inserted as shown in the right panel. Conceptually, the retrieval system maintains two levels of rankings, where each second-level ranking is conditioned on the head document in the first-level ranking.
To operationalize the construction and learning of two-level rankings in a rigorous way, we define a new family of submodular performance measure for diversified retrieval. Many existing retrieval measures (e.g., Precision@k, DCG, Intent Coverage) are special cases of this family. We then F igure 1: A user interested in the animal  X  X aguar X  interacts with the first-level ranking (left) and ob-tains second-level results (right). operationalize the problem of computing an optimal two-level ranking as maximizing the given performance measure. While this optimization problem is NP-hard, we provide an algorithm that has an 1  X  e  X  (1  X  1 e ) a pproximation guarantee.
Finally, we also propose a new method for learning the (mutually dependent) relevance scores needed for two-level rankings. Following a structural SVM approach, we learn a discriminant model that resembles the desired performance measure in structure, but learns to approximate unknown in-tents based on query and document features. This method generalizes the learning method from [11] to two-level rank-ings and a large class of loss functions.
Current methods for diversified retrieval are static in na-ture, i.e., they stay unchanged through a user session. On the other hand, a dynamic model can adapt the ranking based on interactions with the user. The primary motiva-tion for using a dynamic model is addressing the inherent trade-off between depth and diversity in static models.
Consider the example with four (equally likely) user in-tents { t 1 ,...,t 4 } and documents { d 1 ,...,d 9 } with relevance judgments U ( d j | t i ) as given in Table 1. On the one hand, a non-diversified static ranking method could present d 7 d 8  X  d 9 as its top three documents, providing two rele-vant documents for intents t 3 and t 4 but none for intents t and t 2 . On the other hand, a diversified static ranking d  X  d 1  X  d 4 covers all intents, but this ranking lacks depth since no user gets more than one relevant document.
As an alternative, consider the following two-level dy-namic ranking. The user is presented with d 7  X  d 1  X  d 4 as the first-level ranking. Users can now expand any of the first-level results to view a second-level ranking. Users in-terested in d 7 (and thus having intent t 3 or t 4 ) can expand that result and receive a second-level ranking consisting of d 8 and d 9 . Similarly, users interested in d 1 will get d d ; and users interested in d 4 will get d 5 and d 6 .
For this dynamic ranking, every user gets at least one relevant result after scanning at most three documents (i.e. the first-level ranking). Furthermore, users with intents t and t 4 receive two relevant results in the top three positions of their dynamically constructed rankin d 7  X  d 8  X  d 9  X  d 1  X  d 4 . Similarly, users with intent t 1 also receive two relevant results in the top three positions while those with intent t 2 still receive one relevant result. This illustrates how a dynamic two-level ranking can simultaneously provide diversity and increased depth.
 T able 1: Utility U ( d j | t i ) of document d j for intent t
In the above example, interactive feedback from the user was the key to achieving both depth and diversity. More generally, we assume the following model of user behav-ior , which we denote as policy  X  d . Users expand a first-level document if and only if that document is relevant to their intent. When users skip a document, they continue with the next first-level result. When users expand a first-level result, they go through the second-level rankings before con-tinuing from where they left off in the first-level ranking. It is thus possible for a user to see multiple second-level rank-ings. Hence we do not allow documents to appear more than once across all two-level rankings.

Note that this user model differs from the one proposed in [2] in several ways. First, it assumes feedback only for the first-level ranking, while the model in [2] requires that users give feedback many levels deep. Second, unlike in [2], we model that users return to the top-level ranking. We conjecture that these differences make the two-level model more natural and appropriate for practical use.

We now define some notation used in this paper. The documents shown in a first-level ranking of length L are called the head documents. The documents shown in a second-level ranking are called the tail documents. The number of tail documents is referred to as the width W . A row denotes a head document and all its tail docu-ments. Static rankings are denoted as  X  while two-level rankings are denoted as  X  = ( X  1 ,  X  2 ,...  X  i ,.. ). Here  X  ( d i 0 ,d i 1 ,....,d ij ,... ) refers to the i th row of a two-level rank-ing, with d i 0 representing the head document of the row and d ij denoting the j th tail document of the second-level rank-ing. We denote the candidate set of documents to rank for a query q by D ( q ), the set of possible intents by T ( q ) and the distribution over an intent t  X  X  ( q ), given a query q , by P [ t | q ]. Unless otherwise mentioned, dynamic ranking refers to a two-level dynamic ranking in the following.
To define what constitutes a good two-level dynamic rank-ing, we first define the measure of retrieval performance we would like to optimize. We then design our retrieval algo-rithms and learning methods to maximize this measure. In the following, we start with evaluation measures for one-level rankings, and then generalize them to the two-level case.
Existing performance measures range from those that do not explicitly consider multiple intents (e.g. NDCG, Avg Prec), to measures that reward diversity. Measures that reward diversity give lower marginal utility to a document, if the intents the document is relevant to are already well represented in the ranking. We call this the diminishing returns property. The extreme case is the  X  X ntent coverage X  measure (e.g. [9, 11]), which attributes utility only to the first document relevant for an intent.
We now define a family of measures that includes a whole range of diminishing returns models, and that includes most existing retrieval measures. Let g : R  X  R with g (0) = 0 be a concave, non-negative, and non-decreasing function that models the diminishing returns, then we define the utility of the ranking  X  = ( d 1 ,d 2 ,...,d k ) for intent t as
The  X  1  X  ...  X   X  k  X  0 are discount factors and U ( d | t ) is the relevance rating of document d for intent t . For a distribution of user intents P [ t | q ] for query q , the overall utility of a static ranking  X  is the expectation
Note that many existing retrieval measures are special cases of this definition. For example, if one chose g to be the identity function, one recovers the intent-aware mea-sures proposed in [1] and the modular measures defined in [2]. Further restricting P [ t | q ] to put all probability mass on a single intent leads to conventional measures like DCG for appropriately chosen  X  i . At the other extreme, chosing g ( x ) = min( x, 1) leads to the intent coverage measure [9, 11]. Since g can be chosen from a large class of functions, this family of performance measures covers a wide range of diminishing returns models.
We now extend our family of performance measures to dynamic rankings. The key change for dynamic rankings is that users interactively adapt which results they view.
How users expand first-level results was defined in Sec-tion 2 as  X  d . Under  X  d , it is natural to define the utility of a dynamic ranking  X  analogous to Equation (1).

U g ( X  | t ) = g Like for static rankings,  X  1  X   X  2  X  ... and  X  i 1  X   X  i 2 are position-dependent discount factors. Furthermore, we again take the expectation over multiple user intents as in Equation (2) to obtain U g ( X  | q ).

Note that the utility of a second-level ranking for a given intent is zero unless the head document in the first-level ranking has non-zero relevance for that intent. This encour-ages second-level rankings to only contain documents rele-vant to the same intents as the head document, thus provid-ing depth. The first-level ranking, on the other hand, pro-vides diversity as controlled through the choice of function g . The  X  X teeper X  g diminishes returns of additional relevant documents, the more diverse the first-level ranking gets.
In this section, we provide an efficient algorithm for com-puting dynamic rankings that maximize the performance measures defined in the previous section. In the proposed greedy Algorithm 1, the operator  X  denotes either adding a document to a row, or adding a row to an existing ranking. In each iteration, Algorithm 1 considers every document in the remaining collection as the head document of a candi-date row. For each candidate row, W documents are greed-ily added to maximize the utility U g ( X  | q ) of the resulting partial dynamic ranking  X . Once rows of length W are con-structed, the row which maximizes the utility is added to the ranking. The above steps are repeated until the ranking has L rows. Algorithm 1 is efficient, requiring O ( |T| ) space and O ( |T||D| 2 ) time.
 Algorithm 1 for computing a two-level dynamic ranking. Output: A dynamic ranking  X .

 X   X  new two level() while |  X  | X  L do
Our greedy algorithm is closely related to submodular function maximization. Maximizing monotonic submodu-lar functions is a hard problem, but a greedily constructed set gives an (1  X  1 /e ) approximation [6] to the optimal. Since the definition of our utility in (2) involves a concave func-tion, it is not hard to show that selecting a ranking of rows is a submodular maximization problem. Moreover, given the head document, finding the best row is also a submodular maximization problem. Thus, finding a dynamic ranking to maximize our utility is a nested submodular maximiza-tion problem, and we can show the following approximation guarantee for Algorithm 1.
 Lemma 1. Algorithm 1 is (1  X  e  X  (1  X  1 e ) ) approximate. The proof can be found in [7]. It follows [5], but generalizes the result from max coverage to our more general setting.
In the previous section, we showed that a dynamic rank-ing can be efficiently computed when all the intents and relevance judgments for a given query are known. In this section, we propose a supervised learning algorithm that can predict dynamic rankings on previously unseen queries.
Our goal here is to learn a mapping from a query q to a dynamic ranking  X . We pose this as the problem of learn-ing a weight vector w  X  R N from which we can make a prediction as follows:
As further explained below,  X ( q,  X )  X  R N is a joint feature-map between query q and dynamic ranking  X .
 Given a set of training examples ( q i ,  X  i ) n i =1 , the structural SVM framework [10] can be use to learn a discriminant func-tion by minimizing the empirical risk 1 n P n i =1  X ( X  i ,h where  X  is a loss function. Unfortunately, however, the  X  are typically not given directly as part of the training data. Instead, we assuming that we are given training data of the form ( q i , D ( q i ) , T ( q i ) , P [ t | q ] : t  X  T ( q compute the dynamic rankings  X  i by maximizing the util-ity U g (approximately) using Algorithm 1. These  X  i  X  X  will be used as the training labels henceforth. Figure 2: Average number of intents covered (left) and average number of documents for prevalent in-tent (right) in the first-level ranking.

A key aspect of structural SVMs is to appropriately define the joint-feature map  X ( q,  X ). For our problem, we propose w &gt;  X ( q,  X ) := X where V D ( q ) denotes an index set over the words in the can-didate set D ( q ). The vector  X  v denotes word-level features (for example, how often a word occurs in a document) for the word corresponding to index v . The utility U g ( X  | v ) is analogous to (3) but is now over the words in the vocabulary (rather than over intents). The word-level features are remi-niscent of the features used in diverse subset prediction [11]. The key assumption is that the words in a document are correlated with the intent since documents relevant to the same intent are likely to share more words than documents that are relevant to different intents.

The second term in Eq. 5 captures the similarity between head and tail documents. In this case, V D ( q )  X D ( q ) an index set over all document pairs in D ( q ). Consider an index s that corresponds to documents d 1 and d 2 in D ( q ).  X  ( X ) is a feature vector describing the similarity between d and d 2 in  X  when d 1 is a head document in  X  and d 2 occurs in the same row as d 1 (  X  s ( X ) is simply a vector of zeros otherwise). An example of a feature in  X  s ( X ) that captures the similarity between two documents is their TFIDF cosine.
Using these features, w &gt;  X ( q,  X ) models the utility of a given dynamic ranking  X . During learning, w should be selected so that better rankings receive higher utility than worse rankings. This is achieved by solving the following structural SVM optimization problem for w : s.t.  X  i,  X   X  : w &gt;  X ( q i ,  X  i )  X  w &gt;  X ( q i ,  X )  X   X ( X  The constraints in the above formulation ensure that the predicted utility for the target ranking  X  i is higher than the predicted utility for any other  X . The objective function in (6) minimizes the empirical risk while trading it off (via the parameter C &gt; 0) with the margin. The loss between  X  and  X  is given by  X ( X  i ,  X  | q i ) := 1  X  U g ( X  | q i ) that the loss is zero when  X  =  X  i . It is easy to see that a dynamic ranking  X  has a large loss when its utility is low compared to the utility of  X  i .

Even though the Eq. (6) has an exponential number of constraints, the quadratic program in Eq. 6 can be solved in polynomial time using a cutting-plane algorithm [10]. In each iteration of the cutting-plane algorithm, the most vio-lated constraints in (6) are added to a working set and the resulting quadratic program is solved. Given a current w , the most violated constraints are obtained by solving:
Algorithm 1 can be used to solve the above problem, even though the formal approximation guarantee does not hold in this case. Once a weight vector w is obtained, the dynamic ranking for a test query can be obtained from Eq. (4). Experiments were conducted on the TREC 6-8 Interactive Track (TREC) and the Diversity Track of TREC 18 using the ClueWeb collection (WEB). The 17 queries in TREC contain between 7 to 56 different manually judged intents. In the case of WEB, we used 28 queries with 4 or more in-tents. We consider the probability P [ t ] of an intent propor-tional to the number of documents relevant to that intent. A key difference between the two datasets is that the most prevalent intent covers 73 . 4% of all relevant documents for the WEB dataset, but only 37 . 6% for TREC.

The number of documents in the first-level ranking was set to 5. The width of the second-level rankings was set to 2. For simplicity, we chose all factors  X  i and  X  ij in Equations (1) and (3) to be 1. Further, we chose U ( d | t ) = 1 if document d was relevant to intent t and set U ( d | t ) = 0 otherwise.
More details about the experiments and additional results can also be found in [7].
The key design choice of our family of utility measures is the concave function g . As Algorithm 1 directly optimizes utility, we explore how the choice of g affects various prop-erties of the two-level rankings produced by our method.
We experiment with four different concave functions g , each providing a different diminishing-returns model. At one extreme, we have the identity function g ( x ) = x which cor-responds to modular returns. Using this function in Eq. (1) leads to the intent-aware Precision measure proposed in [1], and it is the only function considered in [2]. We therefore refer to this function as PREC. It is not hard to show that Algorithm 1 actually computes the optimal two-level rank-ing for this choice of g . On the other end of the spectrum, we study g ( x ) = min( x, 2). By remaining constant after two, this function discourages presenting more than two relevant documents for any intent. This measure will be referred to as SAT2 (short for  X  X atisfied after two X ). In between these two extremes, we study the square root function (SQRT) g ( x ) =
To explore how dynamic rankings can differ, we used Algo-rithm 1 to compute the two-level rankings (approximately) maximizing the respective measure. Figure 2 shows how g influences diversity. The left-hand plot shows how many different intents are represented in the top 5 results of the first-level ranking on average. The graph shows that the stronger the diminishing-returns model, the more different intents are covered in the first-level ranking. In particu-lar, the number of intents almost doubles on both datasets when moving from PREC to SAT2. In contrast, the number of documents on the most prevalent intent in the first-level ranking decreases, as shown in the right-hand plot. This illustrates how the choice of g can be used to control the desired amount of diversity in the first-level ranking.
Table 2 provides further insight into the impact of g , now also including the contributions of the second-level rankings. Table 2: Performance when optimizing and evaluat-ing using different performance measures for TREC.
 The rows correspond to different choices for g when evaluat-ing expected utility according to Eq. (3), while the columns show which g the two-level ranking was optimized for. Not surprisingly, the diagonal entries of Tables 2 show that the best performance for each measure is obtained when opti-mizing for it. The off-diagonal entries show that different g used during optimization lead to substantially different rankings. This is particularly apparent when optimizing the two extreme performance measures PREC and SAT2; opti-mizing one invariably leads to rankings that have a low value of the other. In contrast, optimizing LOG or SQRT results in much smoother behavior across all measures, and both seem to provide a good compromise between depths (for the prevalent intent) and diversity. The results for WEB are qualitatively similar and are omitted for space reasons.
The ability to simultaneously provide depth and diversity was a key motivation for our dynamic ranking approach. We now evaluate whether this goal is indeed achieved. We compare the two-level rankings produced by Algorithm 1 (denoted Dyn ) with several static baselines. These static baselines are also computed by Algorithm 1, but with the width of the second-level rankings to 0.

First, we compare against a diversity-only static ranking that maximizes intent coverage as proposed in [11] (denoted Stat-Div ). Second, we compare against a depth-only static ranking by chosing g to be the identity function (denoted Stat-Depth ). And, third, we produce static rankings that optimize SQRT, LOG, and SAT2 (denoted Stat-Util ). Note that both Dyn and Stat-Util optimize the same measure that is used for evaluation.

To make a fair comparison between static and dynamic rankings, we measure performance in the following way. For static rankings, we compute performance using the expec-tation of Eq. (1) at a depth cutoff of 5. In particular, we measure PREC@5, SQRT@5, LOG@5 and SAT2@5. For two-level rankings, the number of results viewed by a user depends on how many results he or she expands. So, we truncate any user X  X  path through the two-level ranking after visiting 5 results and compute PREC@5, SQRT@5, LOG@5 and SAT2@5 for the truncated path.

Results of these comparisons are shown in Figure 3. First, we see that both Dyn and Stat-Util outperform Stat-Div, illustrating that optimizing rankings for the desired evalua-tion measure leads to much better performance than using a proxy measure as in Stat-Div. Note that Stat-Div never tries to present more than one result for each intent, which explains the extremely low  X  X epth X  performance in terms of PREC@5. But Stat-Div is not competitive even for SAT2, since it never tries to provide a second result.

Second, at first glance it may be surprising that Dyn out-performs Stat-Depth even on PREC@5, despite the fact that Stat-Depth explicitly (and globally optimally) optimizes depth. Figure 3: Comparing the retrieval quality of Static vs. Dynamic Rankings for TREC and WEB.
 To understand consider the following situation where A is the prevalent intent, and there are three documents relevant to A and B and three relevant to A and C. Putting those sets of three documents into the first two rows of the dynamic ranking provides better PREC@5 than sequentially listing them in the optimal static ranking.

Overall we find the dynamic ranking method outperform-ing all static ranking schemes on all the metrics  X  in many cases with a substantial margin. This gain is more pro-nounced for TREC than for WEB. This can be explained by the fact that WEB queries are less ambiguous, since the single most prevalent intent accounts for more than 70% of all queries on average.
So far we have evaluated how far Algorithm 1 can con-struct effective two-level rankings if the relevance ratings are known. We now explore how far our learning algorithm can predict two-level rankings for previously unseen queries. For all experiments in this section, we learn and predict using SQRT as the choice for g , since it provides a good trade-off between diversity and depth as shown above.

We performed standard preprocessing such as tokeniza-tion, stopword removal and Porter stemming. Since the fo-cus of our work is on diversity and not on relevance, we rank only those documents that are relevant to at least one intent of a query. This simulates a candidate set that may have been provided by a conventional retrieval method. This setup is similar to that used by Yue and Joachims [11].
Many of our features in  X  v follow those used in [11]. These features provide information about the importance of a word in terms of two different aspects. A first type of feature de-scribes the overall importance of a word. A second type of feature captures the importance of a word in a document. An example of this type of feature is whether a word ap-pears with frequency at least y % in the document? Finally, we also use features  X  s that model the relationship between the documents in the second-level ranking and the corre-sponding head document of that row. Examples of this type of feature are binned features representing TFIDF similarity of document pairs and the number of common words that appear in both documents with a frequency at least x %.
Dynamic vs. Static: In the first set of experiments, we compare our learning method ( Dyn-SVM ) for two-level rankings with two static baselines. The first static baseline is the learning method from [11] which optimizes diversity (referred to as Stat-Div ). It is one of the very few learn-ing methods for learning diversified retrieval functions, and was shown to outperform non-learning methods like Essen-Figure 4: Performance of learned functions, compar-ing static &amp; dynamic rankings for TREC and WEB. tial Pages [9]. We also consider a random static baseline (referred to as Stat-Rand ), which randomly orders the can-didate documents. This is a competent baseline, since all our candidate documents are relevant to at least one intent.
Figure 4 shows the comparison between static and dy-namic rankings. For TREC, Dyn-SVM substantially out-performs both static baselines across all performance met-rics, mirroring the results we obtained in Section 6.2 where the relevance judgments were known. This shows that our learning method can effectively generalize the multi-intent relevance judgments to new queries. On the less ambiguous WEB dataset the differences between static and dynamic rankings are smaller. While Dyn-SVM substantially out-performs Stat-Rand , Stat-Div is quite competitive on WEB.
Learning vs. Heuristic Baselines: We also compare against alternative methods for constructing two-level rank-ings. We extend the static baselines Stat-Rand and Stat-Div using the following heuristic. For each result in the static ranking, we add a second-level ranking using the documents with the highest TFIDF similarity from the candidate set. This results in two dynamic baselines, which we call Dyn-Rand and Dyn-Div . The results are shown in Figure 5.
Since we compare two-level rankings of equal size, we mea-sure performance in terms of expected utility. On both datasets Dyn-SVM performs substantially better than Dyn-Rand . This implies that our method can effectively learn which documents to place at the top of the first-level rank-ing. Surprisingly, simply extending the diversified ranking of Dyn-Div using the TFIDF heuristic produces dynamic rank-ings are are competitive with Dyn-SVM . In retrospect, this is not too surprising for two reasons. First, our experiments with Dyn-SVM use rather simple features to describe the re-lationship between the head document and documents in the second-level ranking  X  most of which are derived from their TFIDF similarity. Stronger features exploiting document ontologies or browsing patterns could easily be incorporated into the feature vector. Second, the learning method of Dyn-Div is actually a special case of Dyn-SVM when using the SAT1 loss (i.e. satisfied after a single relevant document) and second-level rankings of width 0. However, we argue that it is still highly preferable to directly optimize the de-sired loss function and two-level ranking using Dyn-SVM , since the reliance on heuristics may fail on other datasets.
We proposed a two-level dynamic ranking approach that provides both diversity and depth for ambiguous queries by exploiting user interactivity. In particular, we showed that Figure 5: Comparing learned dynamic rankings with heuristic baselines for TREC and WEB. the approach has the following desirable properties. First, it covers a large family of performance measures, making it easy to select a diminishing returns model for the application setting at hand. Second, we presented an efficient algorithm for constructing two-level rankings that maximizes the given performance measure with provable approximation guaran-tees. Finally, we provided a structural SVM algorithm for learning two-level ranking functions, showing that it can ef-fectively generalize to new queries.

This work was funded in part under NSF Awards IIS-0905467, IIS-0713483, and IIS-0812091. [1] R. Agrawal, S. Gollapudi, A. Halverson, and S. Ieong. [2] C. Brandt, T. Joachims, Y. Yue, and J. Bank.
 [3] J. Carbonell and J. Goldstein. The use of MMR, [4] H. Chen and D. R. Karger. Less is more: probabilistic [5] D. S. Hochbaum and A. Pathria. Analysis of the [6] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [7] K. Raman, P. Shivaswamy, and T. Joachims.
 [8] S. Robertson. The probability ranking principle in ir. [9] A. Swamintahan, C. Metthew, and D. Kirovski.
 [10] I. Tsochantaridis, T. Joachims, T. Hofmann, and [11] Y. Yue and T. Joachims. Predicting diverse subsets [12] C. X. Zhai, W. W. Cohen, and J. Lafferty. Beyond
