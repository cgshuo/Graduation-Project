 1. Introduction
Research paper search engines, such as CiteSeer ( Lawrence, Giles, &amp; Bollacker, 1999 ) and Cora ( McCal-
They are also becoming increasingly used for recruiting and hiring decisions. Thus the information quality of such systems is of significant importance. This quality critically depends on an information extraction compo-these meta-data are further used in many component applications such as field-based search, author analysis, and citation analysis.

Previous work in information extraction from research papers has been based on two major machine learn-ing techniques. The first is hidden Markov models (HMM) ( Seymore et al., 1999; Takasu, 2003 ). An HMM learns a generative model over input sequence and labeled sequence pairs. While enjoying wide historical suc-cess, standard HMM models have difficulty modeling multiple non-independent features of the observation
SVM classifiers can handle many non-independent features. However, for this sequence labeling problem, Han windows of labels. Solving the information extraction problem in two steps loses the tight interaction between state transitions and observations.

In this article, we present results on the research paper meta-data extraction task using a Conditional Ran-information extraction in general. The CRF approach draws together the advantages of both finite state
HMM and discriminative SVM techniques by allowing use of arbitrary, dependent features and joint inference over entire sequences. CRFs have been previously applied to other tasks such as name entity extraction icons, and layout.

One problem in citation domain is that a publication can be cited in various ways. For example, some citations use complete information including author, title, book title, conference venue, date, location, and publisher, 2003; Lawrence et al., 1999 ). However, such co-referential information could strongly constrain each other in making consistent segmentation decisions. We refer to such a task as constraint information extraction.
Our second contribution in this article is to address the constraint information extraction issue. We propose a novel approach for creating canonical citations for a publication and improving citation segmentation.
Given a number of co-referential citations referring to the same publication, our model creates a canonical citation for this publication based on segmentation uncertainty, and then uses the created canonical citation to re-rank the segmentations for each individual citation.

In standard segmentation experiments, we describe a large collection of experimental results on a number of real world datasets. Dramatic improvements are obtained in comparison with previous SVM and HMM based results, reducing average F1 error by 36%, and word error rate by 78% in comparison with the previous best
SVM results. In co-referent constraint information extraction, we present experimental results on the four sections of CiteSeer citation-matching data ( Lawrence et al., 1999 ), with a significant error rate reduction of 6 X 14% on extraction performance.

We organize the rest of the paper as follows. We first describe conditional random fields in Section 2 . Then a novel approach for co-reference constraint information extraction. We systematically study the these prob-lems in Section 6 and conclude the paper in Section 7 . 2. Conditional random fields
Conditional random fields (CRFs) are undirected graphical models trained to maximize a conditional prob-which corresponds to a finite state machine, and is suitable for sequence labeling. A linear-chain CRF with parameters K ={ k , ... } defines a conditional probability for a state (or label an input sequence x = x 1 x T to be where Z x is the normalization constant that makes the probability of all state sequences sum to one, f ( y t 1 , y t , x , t ) is a feature function which is often binary-valued, but can be real-valued, and k weight associated with feature f k . The feature functions can measure any aspect of a state transition, y have value 1 when y t 1 is the state T ITLE , y t is the state A values make the event unlikely.

Given such a model as defined in Eq. (1) , the most probable labeling sequence for an input x , can be efficiently calculated by dynamic programming using the Viterbi algorithm. Calculating the marginal probability of states or transitions at each position in the sequence by a dynamic-programming-based infer-ence procedure is very similar to forward X  X ackward for hidden Markov models.

The parameters may be estimated by maximum likelihood X  X aximizing the conditional probability of a set of label sequences, each given their corresponding input sequences. The log-likelihood of training set {( x i =1, ... , M } is written
Following the standard exponential family model theory, we maximize Eq. (2) , which corresponds to sat-isfying the following equality, wherein the empirical count of each feature matches its expected count accord-ing to the model P K ( y j x ).

CRFs share all of the advantageous properties of standard maximum entropy models, including their con-vex likelihood function, which guarantees that the learning procedure converges to the global maximum. Tra-used to train CRFs, however, it has been found that a quasi-Newton gradient-climber, BFGS, converges much instead on two other aspects of CRF deployment, namely regularization and selection of different model struc-tures and feature types. 3. Regularization in CRFs
To avoid over-fitting, log-likelihood is often penalized by some prior distribution over the parameters. To get an idea how the parameters are distributed, we trained a CRF with Gaussian prior on one of our datasets and graphed the distribution of these parameters. Fig. 2 shows an empirical distribution of parameters, K . distribution are the Gaussian prior, exponential prior, and hyperbolic-L paper we provide an empirical study of these three priors. 3.1. Gaussian prior With a Gaussian prior, log-likelihood (2) is penalized as follows: where r 2 k is a variance. Maximizing (3) corresponds to satisfying able: rather than matching exact empirical feature frequencies, the model is tuned to match discounted feature frequencies. Chen and Rosenfeld (2000) discuss this in the context of other discounting procedures common in language modeling. We call the term subtracted from the empirical counts (in this case k value.

The variance can be feature dependent. However for simplicity, constant variance is often used for all fea-tures. In this paper, however, we experiment with several alternate versions of Gaussian prior in which the variance is feature dependent.

Although Gaussian (and other) priors are gradually overcome by increasing amounts of training data, per-of the Gaussian prior dependent on feature counts. 1. Threshold cut: In language modeling, e.g., Good-Turing smoothing, only low frequency words are smoothed. Here we apply the same idea and only smooth those features whose frequencies are lower than a threshold (seven in our experiments, following standard practice in language modeling). 2. Divide count: Here we let the discounted value for a feature depend on its frequency in the training set,
In this way, we increase the smoothing on the low frequency features more so than the high frequency features. 3. Bin-based: We divide features into classes based on frequency. We bin features by frequency in the training set, and let the features in the same bin share the same variance. The discounted value is set to be in each bin may be set independently by cross-validation. 3.2. Exponential prior
Whereas the Gaussian prior penalizes according to the square of the weights (an L here is to create a smoothly differentiable analogue to penalizing the absolute-value of the weights (an L at zero, and thus provide a kind of soft feature selection that improves generalization.
Goodman (2003) proposes an exponential prior, specifically a Laplacian prior, as an alternative to Gauss-ian prior. Under this prior, where a k is a parameter in exponential distribution.
 Maximizing (4) would satisfy where sign( k k ) is the sign of k k .

This corresponds to the absolute smoothing method in language modeling. We set the a share the same constant whose value can be determined using absolute discounting a  X  are the number of features occurring once and twice ( Ney, Essen, &amp; Kneser, 1995 ). 3.3. Hyperbolic-L 1 prior of normal distributions. While less frequently used in natural language processing, it has been commonly used for analyzing data from various scientific areas such as finance.
 Under a hyperbolic prior, which corresponds to satisfying
The hyperbolic prior was also tested with CRFs in McCallum and Li (2003) . 4. Exploration of feature space
Wise choice of features is always vital to the performance of any machine learning solution. Feature induc-tion ( McCallum, 2003 ) has been shown to provide significant improvements in CRFs performance. The focus in this section is on three other aspects of the feature space. 4.1. State transition features
In CRFs, state transitions are also represented as features. The feature function f ferent Markov-order structures. We define four different state transitions features corresponding to different
Markov order for different classes of features. Higher-order features model dependencies better, but also cre-ate more data sparse problem and require more memory in training. represented as f ( y t , x ). There are no separate parameters or preferences for state transitions at all. 2. First-order + transitions: Here we add parameters corresponding to state transitions. The feature functions used are f ( y t , x ), f ( y t 1 , y t ). 3. Second-order: Here inputs are examined in the context of the current and previous states. Feature function are represented as f ( y t 1 , y t , x ). 4. Third-order: Here inputs are examined in the context of the current, two previous states. Feature function are represented as f ( y t 2 , y t 1 , y t , x ). 4.2. Local features, layout features and lexicon features
One of the advantages of CRFs (and also standard maximum entropy models in general) is that they easily all these features in our research paper extraction problem, evaluate their individual contributions, and give some guidelines for selecting good features.
 features, and external lexicon resources. The features are summarized in Table 2 . 5. Co-reference constraint information extraction In the citation domain, a publication can be cited in different ways. These variant citations are co-referent.
Traditionally, when segmenting these citations, they are considered independent of each other. However, constraints and imposes the consistency of segmentation decisions. Further, it is desirable to create a canon-ical citation for these variants in order to eliminate duplication and produce uniform citation. To address these problems, we propose a constraint information extraction algorithm which takes advantages of segmentation uncertainty to create canonical citations, which in turn are used to re-rank citation segmenta-tions.

Our solution to co-referent information extraction is based on segmentation uncertainty. In finite state state sequence, is normally used for segmenting sequences. However, in practice, the most likely sequence may not be the best segmentation. A better segmentation could have lower likelihood, as we will show in experi-ments. N-best decoding is a technique commonly used in speech recognition ( Schwartz &amp; Chow, 1990 ). It returns the top N-best segmentations ranked by likelihood. One can then use additional information to re-rank the segmentations. The additional information we have here is the knowledge that these citations are referring to the same publication. The re-ranking approach has been successful in other natural language processing tasks such as parsing ( Collins, 2000 ) and named entity extraction ( Collins, 2002 ).

Table 3 is an example of using a canonical citation to re-rank the segmentations. The fields of the canonical son Wesley X  X  was tagged as a location (bolded in the table). The second likely segmentation (shown at the bot-tom), however is correct. Given the canonical citation, this segmentation would be preferred since it has smaller distance.

Our constraint information extraction model consists of two components. The first component creates a canonical citation by selecting field attributes from the N-best segmentations. The second component uses the created canonical citation to re-rank the segmentations, based on the criteria that a better segmentation gives higher co-referential score. 5.1. Creating a canonical citation a number canonical database fields for this publication.

Canonical database fields creation (also called data integration) is a challenging problem that has attracted Cohen, McAllester, &amp; Kautz, 2000 ). We create canonical database fields based on segmentation uncertainty. tions has N most likely segmentations. For each of the K fields, we assign its value by selecting an attribute from the corresponding field of the M  X  N segmentations. Thus, to create a canonical citation, we have to enu-combinations that exact inference can be performed here. In our experiments, there are a maximum of 13 cita-was 5 or less. However, when M becomes large, one must resort to approximation algorithms to compute the prototype.

Given the attributes of a publication, we compute scores for all ( publication, segmentation ) pairs. The seg-with the highest score are chosen as the canonical citation for this publication.
 5.2. Improving segmentation based on canonical citation
After a canonical citation is created, we can use it to re-rank the N-best segmentations for each individual citation. The rational for re-ranking is that a better segmentation gives better co-referential score with the canonical citation. The co-referential score between the canonical citation and a segmentation is measured by a field string distance, which is the sum of the edited distance of all fields. distance to the prototype is considered as a better segmentation. 6. Empirical study 6.1. Hidden Markov models
Here we also briefly describe a HMM model we used in our experiments as baselines. We relax the inde-pendence assumption made in standard HMM and allow Markov dependencies among observations as depicted in Fig. 4 . As in CRFs, we can vary Markov orders in state transition and observation transitions.
In our experiments, a model with second-order state transitions and first-order observation transitions per-lihood estimation with absolute smoothing. 6.2. Datasets
In standard information extraction experiments, we experiment with three datasets of research paper con-tent. Two consist of the headers of research papers. The other consists of pre-segmented citations from the reference sections of research papers. The citation dataset and one of the header datasets are part of a stan-dard benchmark on which previous studies have evaluated performance ( Han et al., 2003; McCallum et al., 2000; Seymore et al., 1999 ). The other header dataset was created by the authors due to inadequacies with the standard benchmark (no font or formatting information).

However, these datasets do not have co-referent citations and we have to use other datasets for our co-ref-erence information extraction experiments. We use CiteSeer data ( Lawrence et al., 1999 ) as our co-reference information extraction dataset. 6.2.1. Paper header dataset
A header of a research paper is defined to be all of the words from the beginning of the paper up to either phone number, keywords, web address, degree, publication number, and page ( Seymore et al., 1999 ).
The first header dataset contains 935 headers. Following previous research ( Han et al., 2003; McCallum et al., 2000; Seymore et al., 1999 ), we use a randomly selected 500 samples for training and the remaining 435 for testing. We refer this dataset as H1 .
We have also created a second header dataset, comprising 450 headers. This dataset contains font informa-tion which we expected to be an good indicator of field boundaries. The papers are randomly selected among 8000 papers we found by crawling from Internet across many university and research institution websites. We use 300 samples for training and 150 for testing. We refer this dataset as H2 . 6.2.2. Paper reference dataset 6.2.3. Co-reference IE dataset
The co-reference dataset contains approximately 1500 citations to 900 papers. The citations have been man-ually labeled for co-reference and manually segmented into fields, such as author, title, etc. The dataset has four subsets of citations, each one centered around a topic (e.g., reinforcement learning). The statistics of the four sections is shown in Table 4 . 6.3. Performance measures
To give a comprehensive evaluation, we measure extraction performance using several different metrics. In addition to the previously-used word accuracy measure (which overemphasises accuracy of the abstract field), in the information retrieval literature), and whole instance accuracy for measuring overall performance in a way that is sensitive to an error in any part of header or citation. 6.3.1. Measuring field-specific performance 1. Word Accuracy: We define A as the number of true positive words, B as the number of false negative words, C as the number of false positive words, D as the number of true negative words, and
A + B + C + D is the total number of words. Word accuracy is calculated to be A 2. F1-measure: Precision, recall and F1 measure are defined as follows: 6.3.2. Measuring overall performance 1. Overall word accuracy: Overall word accuracy is the percentage of words whose predicted labels equal their true labels. Word accuracy favors fields with large number of words, such as the abstract . 2. Averaged F-measure: Averaged F-measure is computed by averaging the F1-measures over all fields. Aver-age F-measure favors labels with small number of words, which complements word accuracy. We should look at both word accuracy and average F-measure in evaluation.
 3. Whole instance accuracy: An instance here is defined to be a single header or reference. Whole instance accuracy is the percentage of instances whose words are all correctly labeled. 6.4. Standard information extraction results
We first report the overall results by comparing CRFs with HMMs, and with the previously best bench-mark results obtained by SVMs ( Han et al., 2003 ). We then break down the results to analyze various factors now shown, following past practice ( Seymore et al., 1999; Han et al., 2003 )]. The results we obtained with
CRFs use second-order state transition features and line-break layout features. The results we obtained with the HMM model use a second-order model for transitions, and a first-order for observations. The results on
SVM were obtained from Han et al. (2003) by computing F1 measures from the precision and recall numbers they report.

Table 6 shows the results on dataset H2 . Table 7 shows the results on dataset R1 . SVM results are not avail-able for these datasets. 6.5. Constraint information extraction results
One hypothesis behind the constraint information extraction model is that the best segmentation (with the segmentation could be another one from the N-best list. To verify this hypothesis, we conduct an experiment that always selects the segmentation that has the highest word accuracy from the N-best list. This experiment serves as a upper bound performance that our model could achieve.

The upper bound results (F-measure) on each of the four topics in our co-reference dataset are summarized in Table 8 . The big gap between top N and top 1 performance indicates great potential to improve segmen-tation based on optimally selecting segmentations.

The second hypothesis behind our approach is that co-reference information provides extraction with strong constraints and should be beneficial to extraction. To verify this, we conduct co-referent information extraction using true co-reference information contained in our data. Table 9 shows the improved segmenta-tion performance using true co-reference information. We achieve 3.8 X 17.9% error rate reduction on the four might hope to improve by using co-reference.

However, in practice, true co-reference information is not available. We have to resort to clustering algorithms to automatically generate co-reference information. We use a graph partition algorithm for co-reference resolution ( McCallum &amp; Wellner, 2003 ) which gives very good results on our datasets (above 94% F1-measure) ( Wellner, McCallum, Peng, &amp; Hay, 2004 ). We apply our co-reference IE system based on this co-reference results.

Table 10 shows the improved segmentation performance using machine generated co-reference. We obtain 6.2 X 14.2% improvement on the four datasets To test the significance of the improvements, we use McNemar  X  s the improvements on the four datasets are statistically significant.

Interestingly, machine generated co-reference achieves comparable improvements to true co-reference, which is supposed to be an upper bound. This can be attributed to two reasons. First, since we only improve mation is available), the numbers reported in Tables 9 and 10 are not on exactly the same data. The true upper bound for machine generated co-reference could be higher. Second, this also indicates that our co-reference performance is good enough for the purpose of co-reference information extraction and is useful in real situations. 6.6. Analysis 6.6.1. Overall performance comparison
From Tables 5 X 7 , one can see that CRF performs significantly better than HMM, which again supports the previously reported SVM best results, yielding new state of the art performance on this task. the performance on nearly all the fields. The overall word accuracy is improved from 92.9% to 98.3%, which corresponds to a 78 % error rate reduction. However, as we can see word accuracy can be misleading since the
HMM model even has a higher word accuracy than the SVM, although it performs much worse than SVM in versus 93.8% F-measure) which pushes the overall accuracy up. A better comparison can be made by compar-ing the field-based F-measures. Here, in comparison to the SVM, CRFs improve the F1 measure from 89.7% to 93.9%, an error reduction of 36 %.
 6.6.2. Effects of regularization
The results of different regularization methods are summarized in Table 11 . Setting Gaussian variance of features depending on feature count performs better (from 93.3% to 93.9%, an error reduction of 9%). Results are averaged over five random runs. In our experiments we found the Gaussian prior to consistently perform better than the others. An exponential prior performs comparatively to the Gaussian prior. The performance of the exponential prior critically depends on the choice of a (0.1 in our case). Though the exponential prior does not give better accuracy in our case, it induces sparse solutions which require less main memory to store as other alternatives. 6.6.3. Effects of exploring feature space first column is the four different state transition models, the second column is the overall word accuracy of these models. Comparing the rows, one can see that the second-order model performs the best, but not sig-nificantly better than the first-order + transitions and the third-order model. However, the first-order model performs significantly worse. This is because the first-order model ignores f ( y sition feature. The third order should perform better if enough training data was available. dramatically increases the performance, raising the F1 measure from 88.8% to 93.4%, whole sentence accu-racy from 40.1% to 72.4%. Adding lexicon features alone improves the performance. However, when com-bining lexicon features and layout features, the performance is worse than that with layout features alone.
The reason is that a line break is a good sign of field change. When using lexicon features, some words that current author lexicon contains 280,351 authors which might be too general. A more representative author lexicon might avoid hurting the performance. The improvement in performance due to layout features motivates us to design a new dataset H2 which contains font information. We expect font change to be a good indicator of field change. The results of font features are shown in Table 14 , which confirms the effect of layout features in this meta-data extraction task.
 6.6.4. Error analysis Table 15 is the classification confusion matrix of header extraction (field page is not shown to save space).
Most errors happen to  X  X  X ote X  X  and  X  X  X eyword X  X  fields. Some errors such as the confusion between email and keyword can be easily fixed by post-processing, email can be recognized using regular expressions. The start of key words can also be mostly recognized by matching  X  X  X eywords X  X  or  X  X  X ey words X  X . Errors of note are more difficult to fix. Increasing the amount of training data may improve the performance of distinguishing between these confusing fields. 7. Conclusions and future work
We have applied conditional random fields to information extraction from research papers, and investi-gated the issues of regularization and feature spaces in CRFs. We have provided an empirical exploration of a few previously-published priors for conditionally-trained log-linear models. We find that the Gaussian erature. Feature engineering is a key component of any machine learning solution X  X specially in condition-ally-trained models with such freedom to choose arbitrary features X  X nd plays an even more important role than regularization. We obtain new state-of-the-art performance in extracting standard fields from to facilitate future research in this task X  X specially field-F1, rather than word accuracy.
We have presented an algorithm for co-reference constraint information extraction based on segmentation uncertainty. It creates canonical citations and re-ranks citation segmentations at the same time. Experiments on four datasets show significant improvements on extraction performance.

Several issues remain to be investigated. The critical component of co-reference constraint information extraction is canonical citation creation, which is an open data integration problem. The performance of co-reference IE largely depends on the quality of canonical citations. Given the large potential room for tion. Our work is incorporated into a new paper search engine REXA, available at http://rexo.info . Acknowledgements
We sincerely thank Marjorie Freedman and Linnea Micciulla for proof reading. This work was supported in part by the Center for Intelligent Information Retrieval, in part by SPAWARSYSCEN-SD grant number
N66001-02-1-8903, in part by the National Science Foundation Cooperative Agreement number ATM-9732665 through a subcontract from the University Corporation for Atmospheric Research (UCAR) and in part by The Central Intelligence Agency, the National Security Agency and National Science Foundation under NSF grant # IIS-0326249. Any opinions, findings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.
 References
