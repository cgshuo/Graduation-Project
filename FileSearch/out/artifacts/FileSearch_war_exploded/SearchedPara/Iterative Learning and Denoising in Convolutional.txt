 To prove the theorem, we use the convergence results in (Bottou, 1998) and show that the required assump-tions to ensure convergence holds for the proposed al-gorithm. For simplicity, these assumptions are listed here: 1. The cost function E ( w ( ` ) )isthree-timesdi  X  er-2. The usual conditions on the learning rates are ful-3. The second moment of the update term should not 4. When the norm of the weight vector w ( ` ) is larger 5. When the norm of the weight vector is Also recall that A x = x ( ` ) ( x ( ` ) ) &gt; , and A = X} represent the correlation among patterns in the training set X ,so E ( w ( ` ) )= ( w To start, assumption 1 holds trivially as the cost function is three-times di  X  erentiable, with continuous derivatives. Furthermore, E ( w ( ` ) ) 0. Assumption 2 holds because of our choice of the step size  X  t , as mentioned in the lemma description.
 Assumption 3 ensures that the vector w ( ` ) could not escape by becoming larger and larger. Due to the con-straint k w ( ` ) k 2 = 1, this assumption holds as well. Assumption 4 holds as well because: E Finally, assumption 5 holds because: sup Since all necessary assumptions hold for the learning algorithm 1, it converges to a local minimum where Next, we prove the desired result, i.e. the fact that in the local minimum, the resulting weight vector is orthogonal to the patterns, i.e. Aw ( ` ) = 0. Since  X  w The first term is always greater than or equal to zero. Now as for the second term, we have that | ( w ( ` ) i ) |  X  | w i | and sign( w the i th entry of w ( ` ) . Therefore, 0  X   X  w ( ` )  X  (  X  w of ( ?? ) are greater than or equal to zero. And since the left hand side is known to be equal to zero, we Therefore, we must have  X  w ( ` )  X  x = 0, for all x 2 X . Which simply means the vector w  X  is orthogonal to all the patterns in the training set.
 Although in problem (2) we have the constraint k w ( ` ) k 2 = 1 to make sure that the algorithm does not converge to the trivial solution w ( ` ) = 0, due to ap-proximations we made when developing the optimiza-tion algorithm, we should make sure to choose the pa-rameters such that the all-zero solution is still avoided. To this end, denote w 0 ( ` ) ( t )= w ( ` ) ( t )  X  following inequalities: k w it is therefore su cient to have 2  X  t k ( w ( ` ) ( t )) k k w As a result, in order to have k w ( ` ) ( t + 1) k 2 2 &gt; 0, it is su cient to have 2  X  t k ( w ( ` ) ( t )) k k w ( ` ) ( t ) k 2 . Finally, since we have | ( w ( ` ) k w ( ` ) ( t ) k 2 . Therefore, having 2  X  t &lt; 1  X  In the case of a single error, we are sure that the cor-rupted node will always be updated towards the cor-rect direction. For simplicity, let X  X  assume the first pat-tern neuron of cluster ` is the noisy one. Furthermore, let z = { 1 ,..., 0 } be the noise vector. Denoting the i th column of the weight matrix by W ( ` ) y This means that the noisy node gets updated towards the correct direction.
 Therefore, the only source of error would be a cor-rect node gets updated mistakenly. Let P x i denote the probability that a correct pattern neuron x i gets updated. This happens if | g x i | &gt; ' . For ' = 1, this neighborhood of x i is di  X  erent from the neighborhood of x 1 among the constraint nodes. More specifically, versa. Therefore, letting P 0 x N ( x i ) \ N ( x 1 ) 6 = N ( x i ), we note that Therefore, to get an upper bound on P x i , we bound P i in cluster l , d ( l ) avg = pattern neurons and finally d ( l ) min be the minimum de-gree of pattern neurons in cluster l .Then,weknow that a noisy pattern neuron is connected to d ( l ) avg con-straint neurons on average. Therefore, the probabil-ity of x i and x 1 share exactly the same neighborhood would be: Taking the average over the pattern neurons, we have where C t is the set of correct nodes at iteration t and  X  Therefore, the probability of correcting one noisy in-put, P c =1 P e 1 P 0 e would be The proof is similar to Theorem 3.50 in ( ? ). Each cluster node receives an error message from its neigh-boring pattern nodes with probability z . Now consider a given noisy pattern neuron which is connected to a the cluster node v ( ` ) with degree e d ` sends an error message during iteration t of Algorithm 3. This event happens if the cluster node v ( ` ) receives at least one error message from its other neighbors among pattern neurons along its input edges, i.e. if it is connected to more than one noisy pattern neuron. Therefore, As a result, if  X  ( t ) shows the average probability that a cluster node sends a message declaring the violation of at least one of its constraint neurons, we will have,  X  ( t )= E e d Now consider a given pattern neuron x i with degree d i . This node will remain noisy in iteration t + 1 of Algo-rithm 3 if it was noisy in the first place and in iteration t +1 all of its neighbors among constraint neurons send a violation message. Therefore, the probability of this that z (0) = p e , the average probability that a pattern neurons remains noisy will be z ( t +1) = p e  X  Therefore, the decoding operation will be successful if z for z 2 [0 ,p e ].
 The proof is based on construction: we construct a data set X with the required properties such that it can be memorized by the proposed neural network. To start, consider a matrix G 2 R k  X  n with rank k and k = rn ,with0 &lt;r&lt; 1. Let the entries of G be non-negative integers, between 0 and 1, with We start constructing the patterns in the data set as follows: consider a random vector u 2 R k with integer-valued-entries between 0 and 1, where 2. We set the pattern x 2 X to be x = u  X  G , if all the entries of x are between 0 and S 1. Obviously, since both u and G have only non-negative entries, all entries in x are non-negative. Therefore, it is the S 1upper bound that we have to worry about.
 The j th entry in x is equal to x j = u  X  g j ,where g j is the j th column of G . Suppose g j has d j non-zero elements. Then, we have: Therefore, denoting d  X  = max j d j , we could choose , and d  X  such that to ensure all entries of x are less than S .
 As a result, since there are k vectors u with integer entries between 0 and 1, we will have k = rn patterns forming X . Which means C = rn ,which would be an exponential number in n if 2.
 As an example, if G is selected to be a sparse 200  X  400 matrix with 0 / 1entries(i.e. = 2) and d  X  = 10, and u is also chosen to be a vector with 0 / 1elements(i.e. = 2), then it is su cient to have S 11, i.e. the maximum firing rate of neurons should be 11 to have a pattern retrieval capacity of C =2 rn .
 Remark 1 Note that the inequality ( ?? ) was obtained for the worst-case scenario and in fact is very loose. Therefore, even if it does not hold, we will still be able to memorize a very large number of patterns since a big portion of the generated vectors x will have entries less than S . These vectors correspond to the message vectors u that are  X  X parse X  as well, i.e. do not have all entries greater than zero. The number of such vectors is a polynomial in n , the degree of which depends on
