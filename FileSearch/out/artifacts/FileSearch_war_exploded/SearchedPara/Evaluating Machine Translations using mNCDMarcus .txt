 Automatic evaluation of machine translation (MT) systems requires automated procedures to en-sure consistency and efficient handling of large amounts of data. In statistical MT systems, au-tomatic evaluation of translations is essential for parameter optimization and system development. Human evaluation is too labor intensive, time con-suming and expensive for daily evaluations. How-ever, manual evaluation is important in the com-parison of different MT systems and for the valida-tion and development of automatic MT evaluation measures, which try to model human assessments of translations as closely as possible. Furthermore, the ideal evaluation method would be language in-dependent, fast to compute and simple.

Recently, normalized compression distance (NCD) has been applied to the evaluation of machine translations. NCD is a general in-formation theoretic measure of string similar-ity, whereas most MT evaluation measures, e.g., BLEU and METEOR, are specifically constructed for the task. Parker (2008) introduced BAD-GER, an MT evaluation measure that uses NCD and a language independent word normalization method. BADGER scores were directly compared against the scores of METEOR and word error rate (WER). The correlation between BADGER and METEOR were low and correlations between BADGER and WER high. Kettunen (2009) uses the NCD directly as an MT evaluation measure. He showed with a small corpus of three language pairs that NCD and METEOR 0.6 correlated for translations of 10 X 12 MT systems. NCD was not compared to human assessments of translations, but correlations of NCD and METEOR scores were very high for all the three language pairs.
V  X  ayrynen et al. (2010) have extended the work by including NCD in the ACL WMT08 evaluation framework and showing that NCD is correlated to human judgments. The NCD measure did not match the performance of the state-of-the-art MT evaluation measures in English, but it presented a viable alternative to de facto standard BLEU (Pa-pineni et al., 2001), which is simple and effective but has been shown to have a number of drawbacks (Callison-Burch et al., 2006).

Some recent advances in automatic MT evalu-ation have included non-binary matching between compared items (Banerjee and Lavie, 2005; Agar-wal and Lavie, 2008; Chan and Ng, 2009), which is implicitly present in the string-based NCD mea-sure. Our motivation is to investigate whether in-cluding additional language dependent resources would improve the NCD measure. We experiment with relaxed word matching using stemming and a lexical database to allow lexical changes. These additional modules attempt to make the reference sentences more similar to the evaluated transla-tions on the string level. We report an experiment showing that document-level NCD and aggregated NCD scores for individual sentences produce very similar correlations to human judgments. Figure 1: An example showing the compressed sizes of two strings separately and concatenated. Normalized compression distance (NCD) is a sim-ilarity measure based on the idea that a string x is similar to another string y when both share sub-strings. The description of y can reference shared substrings in the known x without repetition, in-dicating shared information. Figure 1 shows an example in which the compression of the concate-nation of x and y results in a shorter output than individual compressions of x and y .

The normalized compression distance, as de-fined by Cilibrasi and Vitanyi (2005), is given in Equation 1, with C ( x ) as length of the compres-sion of x and C ( x,y ) as the length of the com-pression of the concatenation of x and y .
 NCD ( x,y ) = NCD computes the distance as a score closer to one for very different strings and closer to zero for more similar strings.

NCD is an approximation of the uncomputable normalized information distance (NID), a general measure for the similarity of two objects. NID is based on the notion of Kolmogorov complex-ity K ( x ) , a theoretical measure for the informa-tion content of a string x , defined as the shortest universal Turing machine that prints x and stops (Solomonoff, 1964). NCD approximates NID by the use of a compressor C ( x ) that is an upper bound of the Kolmogorov complexity K ( x ) . Normalized compression distance was not con-ceived with MT evaluation in mind, but rather it is a general measure of string similarity. Implicit non-binary matching with NCD is indicated by preliminary experiments which show that NCD is less sensitive to random changes on the character level than, for instance, BLEU, which only counts the exact matches between word n-grams. Thus comparison of sentences at the character level could account better for morphological changes.
Variation in language leads to several accept-able translations for each source sentence, which is why multiple reference translations are pre-ferred in evaluation. Unfortunately, it is typical to have only one reference translation. Paraphras-ing techniques can produce additional translation variants (Russo-Lassner et al., 2005; Kauchak and Barzilay, 2006). These can be seen as new refer-ence translations, similar to pseudo references (Ma et al., 2007).

The proposed method, mNCD, works analo-gously to M-BLEU and M-TER, which use the flexible word matching modules from METEOR to find relaxed word-to-word alignments (Agar-wal and Lavie, 2008). The modules are able to align words even if they do not share the same surface form, but instead have a common stem or are synonyms of each other. A similarized transla-tion reference is generated by replacing words in the reference with their aligned counterparts from the translation hypothesis. The NCD score is com-puted between the translations and the similarized references to get the mNCD score.
 Table 1 shows some hand-picked German X  English candidate translations along with a) the reference translations including the 1-NCD score to easily compare with METEOR and b) the simi-larized references including the mNCD score. For comparison, the corresponding METEOR scores without implicit relaxed matching are shown. The proposed mNCD and the basic NCD measure were evaluated by computing correlation to hu-man judgments of translations. A high correlation value between an MT evaluation measure and hu-man judgments indicates that the measure is able to evaluate translations in a more similar way to humans.

Relaxed alignments with the METEOR mod-ules exact , stem and synonym were created for English for the computation of the mNCD score. The synonym module was not available with other target languages. 4.1 Evaluation Data The 2008 ACL Workshop on Statistical Machine Translation (Callison-Burch et al., 2008) shared task data includes translations from a total of 30 MT systems between English and five European languages, as well as automatic and human trans-lation evaluations for the translations. There are several tasks, defined by the language pair and the domain of translated text.

The human judgments include three different categories. The R ANK category has human quality rankings of five translations for one sentence from different MT systems. The C ONST category con-tains rankings for short phrases (constituents), and the Y ES /N O category contains binary answers if a short phrase is an acceptable translation or not.
For the translation tasks into English, the re-laxed alignment using a stem module and the synonym module affected 7 . 5 % of all words, whereas only 5 . 1 % of the words were changed in the tasks from English into the other languages.
The data was preprocessed in two different ways. For NCD we kept the data as is, which we called real casing (rc). Since the used METEOR align module lowercases all text, we restored the case information in mNCD by copying the correct case from the reference translation to the similar-ized reference, based on METEOR X  X  alignment. The other way was to lowercase all data (lc). 4.2 System-level correlation We follow the same evaluation methodology as in Callison-Burch et al. (2008), which allows us to measure how well MT evaluation measures corre-late with human judgments on the system level.
Spearman X  X  rank correlation coefficient  X  was calculated between each MT evaluation measure and human judgment category using the simplified equation where for each system i , d i is the difference be-tween the rank derived from annotators X  input and the rank obtained from the measure. From the an-notators X  input, the n systems were ranked based on the number of times each system X  X  output was selected as the best translation divided by the num-ber of times each system was part of a judgment.
We computed system-level correlations for tasks with English, French, Spanish and German as the target language 1 . We compare mNCD against NCD and relate their performance to other MT evaluation measures. 5.1 Block size effect on NCD scores V  X  ayrynen et al. (2010) computed NCD between a set of candidate translations and references at the same time regardless of the sentence alignments, analogously to document comparison. We experi-mented with segmentation of the candidate trans-lations into smaller blocks, which were individ-ually evaluated with NCD and aggregated into a single value with arithmetic mean. The resulting system-level correlations between NCD and hu-man judgments are shown in Figure 2 as a function of the block size. The correlations are very simi-lar with all block sizes, except for Spanish, where smaller block size produces higher correlation. An experiment with geometric mean produced similar results. The reported results with mNCD use max-imum block size, similar to V  X  ayrynen et al. (2010). Figure 2: The block size has very little effect on the correlation between NCD and human judg-ments. The right side corresponds to document comparison and the left side to aggregated NCD scores for sentences. 5.2 mNCD against NCD Table 2 shows the average system level correlation of different NCD and mNCD variants for trans-lations into English. The two compressors that worked best in our experiments were PPMZ and bz2. PPMZ is slower to compute but performs slightly better compared to bz2, except for the Method Parameters mNCD PPMZ rc .69 .74 .80 .74 mNCD bz2 rc .64 .73 .73 .70 mNCD PPMZ lc .66 .80 .79 .75 mNCD bz2 lc .59 .85 .74 .73 Table 2: Mean system level correlations over all translation tasks into English for variants of mNCD and NCD. Higher values are emphasized. Parameters are the compressor PPMZ or bz2 and the preprocessing choice lowercasing (lc) or real casing (rc).

Method Parameters E N D E F R E S Table 3: mNCD versus NCD system correlation R
ANK results with different parameters (the same as in Table 2) for each target language. Higher values are emphasized. Target languages D E , F R and E S use only the stem module. lowercased C ONST category.

Table 2 shows that real casing improves R ANK correlation slightly throughout NCD and mNCD variants, whereas it reduces correlation in the cat-egories C ONST , Y ES /N O as well as the mean. The best mNCD (PPMZ rc) improves the best NCD (PPMZ rc) method by 15% in the R ANK category. In the C ONST category the best mNCD (bz2 lc) improves the best NCD (bz2 lc) by 3.7%. For the total average, the best mNCD (PPMZ rc) improves the the best NCD (bz2 lc) by 7.2%. Table 3 shows the correlation results for the R
ANK category by target language. As shown al-ready in Table 2, mNCD clearly outperforms NCD for English. Correlations for other languages show mixed results and on average, mNCD gives lower correlations than NCD. 5.3 mNCD versus other methods Table 4 presents the results for the selected mNCD (PPMZ rc) and NCD (bz2 rc) variants along with the correlations for other MT evaluation methods from the WMT X 08 data, based on the results in Callison-Burch et al. (2008). The results are av-erages over language pairs into English, sorted by R ANK , which we consider the most signifi-cant category. Although mNCD correlation with human evaluations improved over NCD, the rank-ing among other measures was not affected. Lan-guage and task specific results not shown here, re-veal very low mNCD and NCD correlations in the Spanish-English news task, which significantly Table 4: Average system-level correlations over translation tasks into English for NCD, mNCD and other MT evaluations measures degrades the averages. Considering the mean of the categories instead, mNCD X  X  correlation of .74 is third best together with  X  X osbleu X .

Table 5 shows the results from English. The ta-ble is shorter since many of the better MT mea-sures use language specific linguistic resources that are not easily available for languages other than English. mNCD performs competitively only for French, otherwise it falls behind NCD and other methods as already shown earlier. We have introduced a new MT evaluation mea-sure, mNCD, which is based on normalized com-pression distance and METEOR X  X  relaxed align-ment modules. The mNCD measure outperforms NCD in English with all tested parameter com-binations, whereas results with other target lan-guages are unclear. The improved correlations with mNCD did not change the position in the R
ANK category of the MT evaluation measures in the 2008 ACL WMT shared task.

The improvement in English was expected on the grounds of the synonym module, and indicated also by the larger number of affected words in the mNCD (PPMZ rc) .37 .82 .38 .63 Table 5: Average system-level correlations for the R
ANK category from English for NCD, mNCD and other MT evaluation measures. similarized references. We believe there is poten-tial for improvement in other languages as well if synonym lexicons are available.

We have also extended the basic NCD measure to scale between a document comparison mea-sure and aggregated sentence-level measure. The rather surprising result is that NCD produces quite similar scores with all block sizes. The different result with Spanish may be caused by differences in the data or problems in the calculations.
After using the same evaluation methodology as in Callison-Burch et al. (2008), we have doubts whether it presents the most effective method ex-ploiting all the given human evaluations in the best way. The system-level correlation measure only awards the winner of the ranking of five differ-ent systems. If a system always scored second, it would never be awarded and therefore be overly penalized. In addition, the human knowledge that gave the lower rankings is not exploited.

In future work with mNCD as an MT evalu-ation measure, we are planning to evaluate syn-onym dictionaries for other languages than En-glish. The synonym module for English does not distinguish between different senses of words. Therefore, synonym lexicons found with statis-tical methods might provide a viable alternative for manually constructed lexicons (Kauchak and Barzilay, 2006).
