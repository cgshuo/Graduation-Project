 1. Introduction factoid questions involving a narrow answer type (e.g., country name for the above example question). Thus, called information nuggets. Each answer nugget is naturally represented by a short noun phrase or a verb phrase. Good definitional QA systems should find out more answer nuggets with shorter length.
The general architecture of definitional QA systems consists of following three components: question analysis, passage retrieval, and answer selection. The question target extracted from the question sentence in the question analysis phase is used for composing a passage retrieval query. The query can be expanded auskas, 2004 ), or extracted from the WordNet synset ( Wu et al., 2004 ). Usually all sentences in the retrieved passages by the query are used as answer candidates. The top ranked candidates based on several and answer selection components.

After documents relevant to the question target are retrieved, the sentences containing the target are in documents, and an anaphora is usually used to indicate it. Therefore, it is necessary to expand retrieved resolution could not improve the performance. The full coreference resolution is computationally expensive, essary to limit the resolution scope to the anaphora referring to the question target which can be correctly resolved. Thus, this paper suggests a passage expansion method using simple pronoun resolution rules.
By using shorter text segments instead of the sentence itself, we can reduce answer granularity, and include more information per unit length. Harabagiu et al. (2003), Blair-Goldensohn, McKeown, and Schlaikjer (2003), Xu, Weischedel, and Licuanan (2004) extracted shorter text segments from retrieved sentences as gories such as genus, species, cause, and effect. Contrary to the previous works, we extract noun and verb phrases.

External definitions, definitions from external resources such as online dictionaries and encyclopedias, are ranking answer candidates, ours is different in that we identify the target type and build the terminology according to the type. We think that term importance in definition varies according to the target type. For (i.e. terms used for defining something), and find out definition-like candidates using it.
The remainder of this paper is organized as follows. Our contributions for candidate extraction and ranking
Finally, we conclude our work in Section 5 . 2. Answer candidate extraction
The question target is assumed to be expressed explicitly in the question sentence. Questions that need more this paper.
The target is extracted from the question sentence, and the type of the target is identified using the named ing answer candidates in later stages. 2.1. Passage retrieval and expansion 2.1.1. Two-phase retrieval
As the target tends to be expressed differently between documents and the question, a lot of relevant infor-using a more relaxed one. The query for the document retrieval consists of words and phrases of the target is generated with the two words. The remaining words are also used as single query words. For example, for a target Berkman Center for Internet and Society , the query would include a phrase berkman _ center and two words internet and society .

Once the documents are retrieved, the passages consisting of sentences that contain the head word of the target can be generated. We check whether the passage can be expanded to the multiple-sentence passage using a simple anaphora resolution technique described in the next section. 2.1.2. Passage expansion using target-focused anaphora resolution
We also carry out a passage expansion method to retrieve sentences where the question target is used as a personal pronoun. We try to resolve only the pronoun which can be correctly resolved. When the question tences according to the target type: Person : If the starting word of the next sentence is he or she , it is replaced with the question target. target.
 in (b) is replaced with Bill Clinton in (a). (a) Former president Bill Clinton was born in Hope, Arkansas. (b) He was named William Jefferson Blythe IV after his father, William Jefferson Blythe III.
Using this simple method, we can extract the informative sentences related to the question target without using a full anaphora resolution method. 2.2. Candidates extraction using syntactic patterns
All sentences in the retrieved passages are usually used as answer candidate. However, a sentence may be they are used as answer candidates. Otherwise, the sentences are used as the candidates. We extract noun phrases and verb phrases from the sentences using the syntactic patterns shown in Table 1 . In this study, we use the syntactic information generated by the Conexor FDG parser ( Tapanainen &amp; Jarvinen, 1997 ).
The syntactic information analyzed by the syntactic parser is prone to errors. Thus, we complement the error-prone syntactic information with POS information as follows: inserted between the two words. For the RelVP example in Table 1 , if the phrase  X  X  X orn Brooklyn X  X  is extracted, the phrase is changed into  X  X  X orn in Brooklyn X  X .
 miner or preposition, the immediate noun phrase is put together into the extracted phrase. For the above example, if the phrase  X  X  X orn in X  X  is extracted, the phrase is altered into  X  X  X orn in Brooklyn X  X .
If the extracted phrase is an incomplete one, that is, ended with one of the POSs such as conjunction or relative pronoun, the last word is removed from the extracted phrase. For an example sentence  X  X  X opland was born in Brooklyn and won Oscar. X  X , if the phrase  X  X  X orn in Brooklyn and X  X  is extracted, the phrase is changed into  X  X  X orn in Brooklyn X  X .

The phrases containing more than two content words and a noun or a number are considered to be valid candidates.

We eliminate redundant candidates using a word overlap measure and semantic class matching of the head word. A pair of candidates are considered to be redundant when the candidates highly overlap (above 80%) with the modest word overlap (50%). Once the redundancy is detected, the more highly overlapped candidate is eliminated. Although the redundancy is a trouble to make a short novel definition, the redundant informa-candidates in the candidate ranking phase. 3. Answer candidate ranking and 1 and combine them into the final score.
 3.1. Redundancy
Important facts or events are usually mentioned repeatedly. As the redundancy is checked in the previous candidate extraction phase, the redundancy score of answer candidate C could be expressed by the following redundancy ratio: where r represents the redundant count of answer candidate C in the candidate set, and n is the total number of answer candidates. The redundancy score is calculated by the following scaled version: where max j RddRatio ( C j ) is the maximum redundancy ratio among all candidates. 3.2. Local term statistics
As important facts or events are mentioned repeatedly, the target-related terms occur frequently. Thus the resents a score based on the term statistics in the retrieved passages and is calculated as follows: where sf i is the number of sentences in which the term t all terms, and j C j is the number of all content words in the answer candidate C . 3.3. External definitions
Definitions of a question target extracted from external resources such as online dictionary or encyclopedia equation: bolic tangent sigmoid function: Since the probability ratio ExtRatio ( C ) is above 0, the score Ext ( C ) is between 0 and 1. Each probability is estimated by MLE (maximum likelihood estimation): where freq i , E is the number of occurrences of the term t normalizing the probabilities. 3.4. Definition terminology
Although the external definitions are useful for ranking candidates, it is obvious that they cannot cover all the possible targets. In order to alleviate the problem, we device a definition terminology score which reflects how the candidate phrase is definition-like. For the definition terminology, we collected external follows: where DefTerm ( t i ) is the definition terminology score for a term t maximum value of the score.

In order to measure the score DefTerm ( t i ), we tried several measures including ones which have been used lowing two-way contingency table of a term t and a definition class D ; a is the number of times t and D co-number of times neither t nor D occurs, and N is the total number of documents.
 Mutual information : It is a criterion commonly used in statistical language modeling of word associations. Rare terms have a higher score than common terms by the mutual information:
Information gain : It measures the number of bits of information obtained for category prediction by know-ing the presence or absence of a term in a document: v 2 statistic : It measures the lack of independence between t and D : text is used:
The probabilities are estimated by MLE as follows: 3.5. Score combination The criteria mentioned so far are linearly combined into a score: where k s are tuning parameters satisfying selected for the final answer. 4. Experimental results 4.1. Experiments setup
We have experimented with 50 TREC 2003 topics and 64 TREC 2004 topics, and found the answer from the AQUAINT corpus used for TREC Question Answering Track evaluation. The TREC answer set for the in TREC 2004, the answers for definitional questions exclude the answers for factoid and list questions questions of a target can be considered to be the answer for definitional questions of the target. Thus we expanded the TREC 2004 answers by adding the answers for factoid questions of each topic, and used them to evaluate for TREC 2004 topics. 2
The evaluation of systems involves matching up the answer nuggets and the system output. Because the manual evaluation such as TREC evaluation requires a lot of cost, we evaluated our system using the auto-matic measure POURPRE ( Lin &amp; Demner-Fushman, 2005 ). In order to automatically match the nuggets, a match score for each nugget n i in TREC answer A is calculated based on term co-occurrences as follows: where j n i j is the number of terms in the answer nugget, and j n system output nugget s j . The POURPRE estimates the TREC metric, recall R , allowance a , precision P , and F -measure using the match score as follows: tem output, and r 0 and a 0 are the number of vital and okay nuggets, respectively, that have non-zero match heavily.
 We used external definitions from various online sites: Biography.com, Columbia Encyclopedia, Wikipedia,
FOLDOC, The American Heritage Dictionary of the English Language, and Online Medical Dictionary. The external definitions are collected in the query time by throwing a query consisting of the head words of the engine based on BM25 of OKAPI ( Sparck Jones, Walker, &amp; Robertson, 1998 ), and processed top 200 docu-ments retrieved in all experiments. 4.2. Passage expansion
Table 2 shows the effect of passage expansion using target-focused anaphora resolution. The upper two candidate extraction and ranking. Without PE means that the passage expansion is not applied, and With PE mance for all sentences, the performance can be a little bit improved for top ranked candidates. The median number of added sentences is three and two for TREC 2003 and 2004, respectively. As only a few sentences are added sentences improve the ranking by promoting the candidates eligible for the answer. 4.3. Candidate extraction
Table 3 shows the performance according to the candidate units. Sent only and Phrase only mean that all sentences and phrases are used as answer candidates, respectively. Phrase + sent is the system that uses phrases if any syntactic pattern is matched but uses raw sentences otherwise. The passage expansion is con-ducted to all the three systems, and the performance is measured for all candidates which are not ranked. of shorter text, the phrase system covers 81.3% (0.3993/0.4909) and 78.7% (0.4233/0.5380) of answer nuggets in the all sentences for TREC 2003 and 2004 data, respectively. The Phrase + sent balances recall and precision.

Figs. 1 and 2 show the changes in performance of each system according to the answer length measured by the number of non-white-space characters. As shown in the figures, the phrase-based system Phrase only is better than the sentence-based one Sent only for short answers until about 900 bytes for TREC 2003 and 600 bytes for TREC 2004, but we had better use the phrases and sentences together for longer answers.
For TREC 2004 data, the performance of Phrase + sent is slightly worse than Sent only in long answers. It is probably because of the insufficient phrases. The median number of extracted phrases is 81.5 for TREC 2004 together. 4.4. Candidate ranking
Table 4 shows the ranking performance of each definition terminology measure. In order to compare the tion is the worst.

Table 5 shows the result of ranking score combinations where rdd , loc , ext , and tmn indicate the redun-terminology score is very good measure. The definition terminology score is expected to make the performance robust, even if there is no definition about the question target in the external resources. 4.5. Comparison with TREC participant systems
We compared our proposed system with the previous TREC participant systems. For this experiments, the raw run files of each system are offered by NIST. 3 For TREC 2004 evaluation, run files for only the defini-tional questions are used. Table 6 shows the POURPRE evaluation result of TREC top five systems and our proposed system Proposed of which answer length is set to 1500 bytes and 2000 bytes, respectively.
Because the responses of the TREC 2004 participant systems are generated on the assumption that the answer with the original TREC 2004 answer. Our systems may be slightly underestimated for TREC 2004 questions of-the-art definitional QA systems. 5. Conclusions
This paper proposed a definitional question answering system that extracts answer candidates based on lin-ogy. Our interesting findings can be summarized as follows:
The passage expansion technique using a simple target-focused anaphora resolution technique can add ing performance.

The phrase extraction method based on syntactic patterns is useful for a short definition. However, for a long definition, it is better to use phrases and sentences together.

The external definitions and the definition terminology turn out to be efficient and harmonic measures for ranking candidates.

In this study, we designed our definitional QA system which can be compromised with error-prone linguis-we will try to gradually relax the constraint without degrading the performance.
 References
