 In traditional bandit models, the learner is presented with a set of K actions. On each of T rounds, an adversary (or the world) first chooses rewards for each action, and afterwards the learner decides which action it wants to take. The learner then receives the reward of its chosen action, but does compete with the best fixed arm in hindsight. In the more general  X  X xperts setting, X  each of N experts recommends an arm on each round, and the goal of the learner is to perform as well as the best expert in hindsight.
 it already knows to be good and exploring actions for which its estimates are less certain. One such real-world problem appears in computational advertising, where publishers try to present their customers with relevant advertisements. In this setting, the actions correspond to advertisements, and choosing an action means displaying the corresponding ad. The rewards correspond to the payments from the advertiser to the publisher, and these rewards depend on the probability of users clicking on the ads.
 Unfortunately, many real-world problems, including the computational advertising problem, do not fit so nicely into the traditional bandit framework. Most of the time, advertisers have the ability to display more than one ad to users, and users can click on more than one of the ads displayed to them. To capture this reality, in this paper we define the slate problem . This setting is similar to the traditional bandit setting, except that here the advertiser selects a slate, or subset, of S actions. In this paper we first consider the unordered slate problem, where the reward to the learning algo-rithm is the sum of the rewards of the chosen actions in the slate. This setting is applicable when all actions in a slate are treated equally. While this is a realistic assumption in certain settings, we also deal with the case when different positions in a slate have different importance. Going back to our computational advertising example, we can see not all ads are given the same treatment (i.e. an ad displayed higher in a list is more likely to be clicked on). One may plausibly assume that for every ad and every position that it can be shown in, there is a click-through-rate associated with the (ad, position. This is a very general user model used widely in practice in web search engines. To ab-stract this, we turn to the ordered slate problem, where for each action and position in the ordering, the adversary specifies a reward for using the action in that position. The reward to the learner then is the sum of the rewards of the (actions, position) pairs in the chosen ordered slate. 1 This setting chosen slate are revealed, rather than just the total cost of the slate.
 Finally, we show how to tackle these problems in the experts setting, where instead of competing with the best slate in hindsight, the algorithm competes with the best expert, recommending different slates on different rounds.
 One key idea appearing in our algorithms is to use a variant of the multiplicative weights expert algorithm for a restricted convex set of distributions. In our case, the restricted set of distributions over actions corresponds to the one defined by the stipulation that the learner choose a slate instead of individual actions. Our variant first finds the distribution generated by multiplicative weights and then chooses the closest distribution in the restricted subset using relative entropy as the distance metric  X  this is a type of Bregman projection, which has certain nice properties for our analysis. Previous Work. The multi-armed bandit problem, first studied by Lai and Robbins [15], is a classic problem which has had wide application. In the stochastic setting, where the rewards of the arms are i.i.d., Lai and Robbins [15] and Auer, Cesa-Bianchi and Fischer [2] gave regret bounds of This non-stochastic setting of the multi-armed bandit problem is exactly the specific case of our problem when the slate size is 1 , and hence our results generalize those of Auer et al. which can be recovered by setting s = 1 .
 Our problem is a special case of the more general online linear optimization with bandit feedback problem [1, 4, 5, 11]. Specializing the best result in this series to our setting, we get worse regret bounds of O ( p T log( T )) . The constant in the O (  X  ) notation is also worse than our bounds. For a more specific comparison of regret bounds, see Section 2. Our algorithms, being specialized for the slates problem, are simpler to implement as well, avoiding the sophisticated self-concordant barrier techniques of [1].
 This work also builds upon the algorithm in [18] to learn subsets of experts and the algorithm in [12] for learning permutations, both in the full information setting. Our work is also a special case of the Combinatorial Bandits setting of Cesa-Bianchi and Lugosi [9]; however, our algorithms obtain better regret bounds and are computationally more efficient.
 Our multiplicative weights algorithm also appears under the name Component Hedge in the inde-pendent work of Koolen, Warmuth and Kivinen [14]. Furthermore, the expertless, unordered slate problem is studied by Uchiya, Nakamura and Kudo [17] who obtain the same asymptotic bounds as appear in this paper, though using different techniques. Notation. For vectors x , y  X  R K , x  X  y denotes their inner product, viz. P i x i y i . For ma-trices X , Y  X  R s  X  K , X  X  Y denotes their inner product considering them vectors in R sK , viz. P ij X ij Y ij . For a set S of actions, let 1 S be the indicator vector for that set. For two distributions p and q , let RE ( p k q ) denote their relative entropy, i.e. RE ( p k q ) = P i p i ln( p i q Problem Statement. In a sequence of rounds, for t = 1 , 2 ,...,T , we are required to choose a slate from a base set A of K actions. An unordered slate is a subset S  X  X  of s out of the K actions. An ordered slate is a slate together with an ordering over its s actions; thus, it is a one-to-one mapping  X  : { 1 , 2 ,...,s }  X  A . Prior to the selection of the slate, the adversary chooses losses 3 for the actions in the slates. Once the slate is chosen, the cost of only the actions in the chosen slate is revealed. This cost is defined in the following manner: of the algorithm is defined to be Here, the subscript S is used as a shorthand for ranging over all slates S . The regret for the ordered slate problem is defined analogously.
 where the expectation is taken over the internal randomization of the algorithm.
 Competing with policies. Frequently in applications we have access to N policies which are algo-rithms that recommend slates to use in every round. These policies might leverage extra information that we have about the losses in the next round. It is therefore beneficial to devise algorithms that have low regret with respect to the best policy in the pool in hindsight, where regret is defined as: algorithm X  X  chosen slate. The regret is defined analogously for ordered slates. More generally, we may allow policies to recommend distributions over slates, and our goal is to minimize the expected regret with respect to the best policy in hindsight, where the expectation is taken over the distribution recommended by the policy as well as the internal randomization of the algorithm.
 Our results. We are now able to formally state our main results: Theorem 2.1. There are efficient (running in poly ( s,K ) time in the no-policies case, and in poly ( s,K,N ) time with N policies) randomized algorithms achieving the following regret bounds: To compare, the best bounds obtained for the no-policies case using the more general algorithms [1] and [9] are O ( p s 3 K ln( K/s ) T ) in the unordered slates problem, and O ( s 2 p K ln( K ) T ) in the ordered slates problem. It is also possible, in the no-policies setting, to devise algorithms that have regret bounded by O ( We omit these algorithms in this paper for the sake of brevity. Algorithm MW ( P ) Initialization: An arbitrary probability distribution p (1)  X  X  on the experts, and some  X  &gt; 0 .
For t = 1 , 2 ,...,T : 3.1 Main algorithmic ideas Our starting point is the Hedge algorithm for learning online with expert advice. In this setting, on each round t , the learner chooses a probability distribution p ( t ) over experts, each of which then The main idea of our approach is to apply Hedge (and ideas from bandit variants of it, especially Exp3 [3]) by associating the probability distributions that it selects with mixtures of (ordered or unordered) slates, and thus with the randomized choice of a slate. However, this requires that the selected probability distributions have a particular form, which we describe shortly. We therefore need a special variant of Hedge which uses only distributions p ( t ) from some fixed convex subset P of the simplex of all distributions. The goal then is to minimize regret relative to an arbitrary distribution p  X  X  . Such a version of Hedge is given in Figure 1, and a statement of its performance below. This algorithm is implicit in the work of [13, 18].
 Theorem 3.1. Assume that  X  &gt; 0 is chosen so that for all t and i ,  X ` i ( t )  X   X  1 . Then algorithm MW ( P ) generates distributions p (1) ,..., p ( T )  X  X  , such that for any p  X  X  , Here, ( ` ( t )) 2 is the vector that is the coordinate-wise square of ` ( t ) . 3.2 Unordered slates with no policies To apply the approach described above, we need a way to compactly represent the set of distributions over slates. We do this by embedding slates as points in some high-dimensional Euclidean space, and then giving a compact representation of the convex hull of the embedded points. Specifically, we represent an unordered slate S by its indicator vector 1 S  X  R K , which is 1 for all coordinates j  X  S , and 0 for all others. The convex hull X of all such 1 S vectors can be succinctly described [18] as the convex polytope defined by the linear constraints P K j =1 x j = s and x j  X  0 for j = 1 ,...,K . An algorithm is given in [18] (Algorithm 2) to decompose any vector x  X  X  into a convex combination of at most K indicator vectors 1 S . We embed the convex hull X of all the 1 S vectors in the simplex of distributions over the K actions simply by scaling down all coordinates by s so that they sum to 1 . Let P be this scaled down version of X . Our algorithm is given in Figure 2.
 Step 3 of MW ( P ) requires us to compute the arg min p  X  X  RE ( p k  X  p ( t + 1)) , which can be solved by convex programming. A linear time algorithm is given in [13], and a simple algorithm (from [18]) is the following: find the least index k such that clipping the largest k coordinates of p to 1 s and rescaling the rest of the coordinates to sum up to 1  X  k s ensures that all coordinates are at most 1 s , and output the probability vector thus obtained. This can be implemented by sorting the coordinates, and so it takes O ( K log( K )) time. Bandit Algorithm for Unordered Slates
Initialization: Start an instance of MW ( P ) with the uniform initial distribution p (1) = 1 K 1 . Set
For t = 1 , 2 ,...,T : We now prove the regret bound of Theorem 2.1. We use the notation E t [ X ] to denote the expectation of a random variable X conditioned on all the randomness chosen by the algorithm up to round t , assuming that X is measurable with respect to this randomness. We note the following facts: Note that if we decompose a distribution p  X  P as a convex combination of 1 s 1 S vectors and ran-domly choose a slate S according to its weight in the combination, then the expected loss, averaged over the s actions chosen, is ` ( t )  X  p . We can bound the difference between the expected loss (aver-Using this bound and Theorem 3.1, if S ? = arg min S P t ` ( t )  X  1 s 1 S , we have E [ Regret T ] We now bound the terms on the RHS. First, we have E [(  X  ` ( t )) 2  X  p ( t )] = X so all we need to check is that q (1  X   X  ) s ln( K/s ) KT  X  s X  K , which is true for our choice of  X  . Bandit Algorithm for Ordered Slates
Initialization: Start an instance of MW ( P ) with the uniform initial distribution p (1) = 1 sK 1 . Set 3.3 Ordered slates with no policies A similar approach can be used for ordered slates. Here, we represent an ordered slate  X  by the subpermutation matrix M  X   X  R s  X  K which is defined as follows: for i = 1 , 2 ,...,s , we have M i, X  ( i ) = 1 , and all other entries are 0 . In [7, 16], it is shown that the convex hull M of all the M matrices is the convex polytope defined by the linear constraints: P K j =1 M ij = 1 for i = 1 ,...,s ; P i =1 M ij  X  1 for j = 1 ,...,K ; and M ij  X  0 for i = 1 ,...,s and j = 1 ,...,K . Clearly, all subpermutation matrices M  X   X  M . To complete the characterization of the convex hull, we can show (details omitted) that given any matrix M  X  X  , we can efficiently decompose it into a convex combination of at most K 2 subpermutation matrices.
 We identify matrices in R s  X  K with vectors in R sK in the obvious way. We embed M in the simplex of distributions in R sK simply by scaling all the entries down by s so that their sum equals one. Let P be this scaled down version of M . Our algorithm is given in Figure 3.
 The projection in step 3 of MW ( P ) can be computed simply by solving the convex program. In practice, however, noticing that the relative entropy projection is a Bregman projection, the cyclic projections method of Bregman [6, 8] is likely to work faster. Adapted to the specific problem at hand, this method works as follows (see [8] for details): first, for every column j , initialize a dual variable  X  j = 1 . Then, alternate between row phases and column phases. In a row phase, iterate over all rows, and rescale them to make them sum to 1 s . The column phase is a little more complicated: and scale the column by  X  0 , and update  X  j  X   X  j / X  0 . Repeat these alternating row and column phases until convergence to within the desired tolerance.
 also that L ( t )  X  p 0 ( t )  X  L ( t )  X  p ( t )  X   X  .
 Using this bound and Theorem 3.1, if  X  ? = arg min  X  P t L ( t )  X  1 s M  X  , we have E [ Regret T ] We now bound the terms on the RHS. First, we have E [(  X  L ( t )) 2  X  p ( t )] = X Bandit Algorithm for Unordered Slates With Policies
Initialization: Start an instance of MW with no restrictions over the set of distributions over the N
For t = 1 , 2 ,...,T : Finally, we have RE ( 1 s M  X  ? k p (1)) = ln( K ) . Plugging these bounds into the bound of Theo-rem 3.1, we get the stated regret bound from Theorem 2.1: 4.1 Unordered Slates with N Policies In each round, every policy  X  recommends a distribution over slates  X   X  ( t )  X  P , where P is the X scaled down by s as in Section 3.2. Our algorithm is given in Figure 4.
 P  X  Using this bound and Theorem 3.1, if  X  ? = arg min  X  P t ` ( t )  X   X   X  ( t ) , we have
E [ Regret T ] where e  X  ? is the distribution (vector) that is concentrated entirely on policy  X  ? . We now bound the terms on the RHS. First, we have Bandit Algorithm for Ordered Slates with Policies
Initialization: Start an instance of MW with no restrictions, over the set of distributions over the N
For t = 1 , 2 ,...,T : The first inequality above follows from Jensen X  X  inequality, and the second one is proved exactly as in Section 3.2. Finally, we have RE ( e  X  ? k p (1)) = ln( N ) . Plugging these bounds into the bound above, we get the stated regret bound from Theorem 2.1: tions. 4.2 Ordered Slates with N Policies In each round, every policy  X  recommends a distribution over ordered slates  X   X  ( t )  X  X  , where P is M scaled down by s as in Section 3.3. Our algorithm is given in Figure 5.
 for brevity. We get the stated regret bound from Theorem 2.1: In this paper, we presented efficient algorithms for the unordered and ordered slate problems with regret bounds of O ( Bregman projections on a convex set representing the convex hull of slate vectors.
 Possible future work on this problem is in two directions. The first direction is to handle other user models for the loss matrices, such as models incorporating the following sort of interaction between the chosen actions: if two very similar ads are shown, and the user clicks on one, then the user is less likely to click on the other. Our current model essentially assumes no interaction. The second direction is to derive high probability O ( the presence of policies. The techniques of [3] only give such algorithms in the no-policies setting. [1] A BERNETHY , J., H AZAN , E., AND R AKHLIN , A. Competing in the dark: An efficient algo-[2] A UER , P., C ESA -B IANCHI , N., AND F ISCHER , P. Finite-time analysis of the multiarmed [3] A UER , P., C ESA -B IANCHI , N., F REUND , Y., AND S CHAPIRE , R. E. The nonstochastic [4] A WERBUCH , B., AND K LEINBERG , R. Online linear optimization and adaptive routing. J. [5] B ARTLETT , P. L., D ANI , V., H AYES , T. P., K AKADE , S., R AKHLIN , A., AND T EWARI , [6] B REGMAN , L. The relaxation method of finding the common point of convex sets and its [7] B RUALDI , R. A., AND L EE , G. M. On the truncated assignment polytope. Linear Algebra [8] C ENSOR , Y., AND Z ENIOS , S. Parallel optimization . Oxford University Press, 1997. [9] C ESA -B IANCHI , N., AND L UGOSI , G. Combinatorial bandits. In COLT (2009). [11] H AZAN , E., AND K ALE , S. Better algorithms for benign bandits. In SODA (2009), pp. 38 X 47. [12] H ELMBOLD , D. P., AND W ARMUTH , M. K. Learning permutations with exponential weights. [13] H ERBSTER , M., AND W ARMUTH , M. K. Tracking the best linear predictor. Journal of [14] K OOLEN , W. M., W ARMUTH , M. K., AND K IVINEN , J. Hedging structured concepts. In [15] L AI , T., AND R OBBINS , H. Asymptotically efficient adaptive allocation rules. Advances in [16] M ENDELSOHN , N. S., AND D ULMAGE , A. L. The convex hull of sub-permutation matrices. [17] U CHIYA , T., N AKAMURA , A., AND K UDO , M. Algorithms for adversarial bandit problems [18] W ARMUTH , M. K., AND K UZMIN , D. Randomized PCA algorithms with regret bounds that
