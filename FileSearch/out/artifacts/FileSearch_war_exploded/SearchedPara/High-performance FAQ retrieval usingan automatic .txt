 1. Introduction
Many B2C (business-to-customer) companies run call centers to help customers access their products. A recent study reports that the companies spend $20 to $25 per call ( GARTNER, 1998 ). With the emergence of e-commerce systems ( Muller &amp; Pischel, 1999 ), successful information access on e-commerce websites accommodating both customer needs and business requirements becomes essential. As a useful tool for information access, most commercial sites provide customers with keyword search. However, sometimes the keyword search does not perform well in specific domains like FAQ (frequently asked question) retrie-ders, 1999 ):
Information suppliers do not know users actual questions. Instead, the information suppliers construct question candidates in advance by using their own knowledge. Then, they answer the question candi-dates. However, the question candidates do not always satisfy users needs.

Each FAQ consists of a small number of words, unlike ordinary documents. Information suppliers choose words in each FAQ according only to their knowledge. However, the words may not be properly used in the FAQs.

These deficiencies may raise lexical disagreement problems in keyword search, as shown in Fig. 1 (a real sample that is collected from http://www.lgeshop.com ). In Fig. 1 , the query  X  X  X  d like to send back my goods. X  X  and the FAQ  X  X  X ow can I return merchandise? X  X  have a very similar meaning, but there is no over-lap between the words in the two sentences. This fact often makes keyword search systems mislead users into false FAQs. To resolve the lexical disagreement problems, we propose a cluster-based FAQ retrieval system called FRACT (Faq Retrieval And Clustering Technique). FRACT periodically collects and refines additional information like date and time. Then, FRACT clusters the query logs into predefined FAQ categories. When a user inputs his/her query, FRACT calculates the similarities between the query and each FAQ cluster. According to the similarities, FRACT ranks and returns a list of relevant FAQs.
This paper is organized as follows. In Section 2, we review the previous works on clustering and FAQ retrieval. In Section 3, we propose a method of clustering query logs and a cluster-based FAQ retrieval method. In Section 4, we analyze the results of our experiments. Finally, we draw some conclusions in Sec-tion 5. 2. Previous works
Unlike AI (artificial intelligence) question answering systems that focus on generation of new answers resentative FAQ retrieval systems are FAQ Finder ( Hammond, Burke, Martin, &amp; Lytinen, 1995; Burke et al., 1997 ), Auto-FAQ ( Whitehead, 1995 ), and Sneiders system ( Sneiders, 1999 ). FAQ Finder was de-signed to improve navigation through already existing external FAQ collections. To match users queries to the FAQ collections, FAQ Finder identifies verb and noun phrases using a syntactic parser. Then, FAQ Finder performs concept matching using semantic knowledge like WordNet ( Miller, 1990 ).
Auto-FAQ matches users queries to predefined FAQs using a keyword comparison method based on shallow NLP (natural language processing) techniques. Sneiders system classifies keywords into three types: required keywords, optional keywords and irrelevant keywords. Sneiders system retrieves and ranks relevant FAQs according to the three types. Although the representative systems perform well, we think that it is not easy to expand them into other domains because they require high-level knowledge bases or handicraft rules. To resolve the problem, FRACT uses a lower knowledge resource like a MRD (machine readable dictionary) and shallow NLP techniques like morphological analysis. The
MRD is a machine readable version of an ordinary dictionary that consists of headwords and their definitions.

To cluster documents, we need to select a similarity measure and a clustering algorithm. Popular simi-ficients (van Rijsbergen, 1797). However, the similarity measures may not be appropriate to cluster short documents like query logs (i.e. sentences) as there is often very little overlap between the words in query logs. Therefore, we propose a new similarity measure using a MRD. To bridge the lexical chasms between query logs, FRACT expands words in query logs into their definitions by using the MRD. Using the sim-ilarities between the definitions, FRACT computes the conceptional similarities between the query logs.
Both partitioning and hierarchical agglomerative clustering algorithm have been studied in the context mental results.

There have been numerous studies on how clustering can be employed to improve retrieval results ( Liu roids match the user s query. The query specific clustering methods group the set of documents retrieved by an IR system on a query. The main goal of the query specific clustering methods is to improve the rankings of relevant documents on searching time. Some studies have shown that cluster-based retrieval did not out-expect that FRACT will outperform traditional document-based retrieval systems because the size of FAQ collections is probably much smaller than the size of ordinary document collections, as mentioned in ( Wil-let, 1988; Voorhees, 1985 ). 3. High-performance FAQ retrieva lof FRACT 3.1. System overview
FRACT consists of two sub-systems; FRACT/CL and FRACT/IR, as shown in Fig. 2 . FRACT/CL periodically collects query logs and filters out uninformative ones among the collected query logs. Then,
FRACT/CL groups the refined query logs by using original FAQs as seed data. Based on the clustering results, when users input their queries, FRACT/IR calculates the similarities between the queries and each cluster. According to the similarities, FRACT/IR ranks and returns relevant FAQs. 3.2. FRACT/CL: Query log clustering system
To cluster query logs, we must establish a similarity measure, and then choose a clustering algorithm to group query logs based on the similarity measure. To solve the lexical disagreement problems between query logs, we propose a similarity measure using a MRD. We call the similarity measure DicSim since we use the definitions of words for measuring similarity.

DicSim is separated into two steps. The first step is the word-by-word comparison of query logs. Let ql l th word of the k th query log is given by where n  X  d ql j mon content words between the definition of ql j i and the definition of ql verbs, adjectives and adverbs, while functional words involve prepositions, conjunctions and interjections.
The definitions of each content word are obtained from the MRD. If ql headword, we consider that ql j i itself is its definition. In Eq. (1) , s  X  ql 1953 ). The significance score is defined as the normalized information of ql
In Eq. (2) , freq ql j content words in the set of query logs.

For example, if the word purchase with significance score of 0.2 is defined as  X  X  X o obtain in exchange for money or its equivalent X  X  and the word buy with significance score of 0.4 is defined as  X  X  X o acquire in ex-change for money or its equivalent X  X , n ( d purchase ) and n ( d 3. Therefore, the similarity of the two words will be 0.6 (i.e. (0.3 Eq. (1) represents the weighted proportion of overlap between two definitions.

A MRD is a closed paraphrasing system of natural language. Each of its headwords is defined by a phrase which is composed of the headwords and their derivations ( Kozima &amp; Furugori, 1993 ). The head-select the correct definition d ql j
To filter out inadequate definitions, we assume that meanings of words can be restricted according to application domains. For example, bank has two meanings; an embankment, and a place where money is kept. However, in FAQs about an ordinary commercial bank, the meaning of bank will be probably the second one. Based on this assumption, we automatically generate a reduced version of the MRD that is adjusted to a specific domain. Heuristic 1 shows how to filter out inadequate definitions.
Heuristic 1. How to select adequate definitions 1. Extract all content words from FAQs, compute significance scores of the content words, and create a word pool that consists of the content words and their significance scores. 2. Fetch a content word from the word pool, and consult a MRD for the definitions of the content word. 3. If the content word is found, fetch all definitions of the content word from the MRD. 4. Compute scores of each definition as follows: where s  X  def j i  X  is the significance score of the j th content word in the i th definition def ber of content words in def i . The significance scores are obtained from the word pool. 5. Select the definition with the highest score. If all scores are the same, select the first definition. 6. Repeat 2 to 5 until the word pool is empty.

The second step of DicSim calculation is the sentence-by-sentence comparison using Eq. (3) . Let ql the i th query log. The DicSim score between the i th query log and the k th query log is given by where m is the number of content words in ql i . In brief, Eq. (3) chooses the maximum match score between content words in two queries. Then, Eq. (3) averages these maxima for all content words.
We use a modified K -means algorithm as an example of partitioning methods in our static clustering experiments, primarily because of its efficiency. The number K is an input to the algorithm that specifies the desired number of clusters. In this paper, we set the number K to the number of predefined FAQs.
The modified K -means algorithm, first, takes the K FAQs each as the centroid of a unique cluster and sets than the threshold value. To prevent different FAQs from being assigned to the same cluster, none of the K
FAQs are compared to each other. Third, the cluster centroids are recomputed based on clusters formed in roids. Fourth, the modified K -means algorithm repeats the second step and the third step using unassigned gradually decreasing the threshold value. We expect that the threshold decreasing method will prevent the cluster centroids from leaning excessively toward query logs with lower DicSim scores because query logs with higher DicSim scores are included into proper clusters in advance. In our experiments, we decreased the threshold value from 0.5 to 0.01. After all clustering steps are completed, the modified K -means algo-rithm discards the query logs that are not assigned to any clusters. 3.3. FRACT/IR: cluster-based FAQ retrieval system
In a vector space model, given an m -word query q ={ q 1 , q we can represent q and f each as a vector of word frequencies ~ q and between two word frequency vectors ~ q and ~ f is the cosine coefficient between them, as shown individual FAQs using the clusters of query logs, we modify Eq. (4) , as shown
In Eq. (5) , c f is the cluster c that includes the FAQ f . freq occurs in c f . We calculate freq c max_cval( w ) is a normalizing factor that represents the maximum value among the values of the word w . According to the normalizing scheme, we expect that cluster centroids lean toward axes with high word values, as shown in Fig. 3 . k is a general symbol for smoothing.
 FRACT/IR calculates similarities between a query and each FAQ using Eq. (5) , and then ranks relevant FAQs according to the similarities. FRACT/IR can alleviate the lexical disagreement problems, as
FRACT/IR calculates the similarities between a query and each cluster consisting of much more words than FAQs themselves. For example, if FRACT/IR has the centroid vector [ goods:0.5, merchandise:0.1, method:0.1, return:0.5, send:0.1 ] associated with the FAQ  X  X  X ow to return goods X  X , FRACT/IR may return the FAQ  X  X  X ow to return goods X  X  with a high rank when users input the query  X  X  X  method to send back merchandise X  X  because there are three common words between the query and the centroid although there is no common word between the query and the FAQ.
 4. Evaluation 4.1. Data sets and experimental settings We collected 405 Korean FAQs from three web sites; LGeShop ( http://www.lgeshop.com , 91 FAQs), Hyundai Securities ( http://www.youfirst.co.kr , 81 FAQs) and KTF ( http://www.ktf.com , 233 FAQs). LGe-
Shop is an internet shopping mall, Hyundai Securities is a security corporation, and KTF is a mobile com-munication company. For two months, we also collected a large amount of query logs that are created by previous search engines. After eliminating additional information except users queries, we automatically selected 4126 unique query logs (1549 query logs from LGeShop, 744 query logs from Hyundai Securities and 1833 query logs from KTF) that consist of two or more content words. Then, we manually classified query logs into the FAQs categories and annotated each query log with the identification numbers of the
FAQ categories. Finally, we constructed a test collection called KFAQTEC (Korean test collection for evaluation of FAQ retrieval systems). KFAQTEC consists of 4531 sentences (405 FAQs and 4126 query logs). The number of content words per query is 5.588, and the number of FAQ categories per content word is 0.016. Table 2 shows the sample of KFAQTEC. The manual annotation was done by graduate students majoring in language analysis, and was post-processed for consistency. The inter-coder agreement score of KFAQTEC was 95%.

To experiment with FRACT from the various viewpoints, we reorganized KFAQTEC into three types of data sets; FOLD, SEED, and TRAIN. Each data set consists of seed data, training data, and test data.
The seed data and training data have two uses, respectively. The seed data is used as both retrieval target sentences for FRACT/IR and clustering seeds for FRACT/CL. The training data is used as both cluster-ing-target sentences for FRACT/CL and parameter-tuning data for FRACT/IR. As the result of param-eter tuning only using the training data, we set k in Eq. (6) to 0.7 since FRACT/IR showed the highest performance in that point. The test data is a set of sentences completely disjoint from both the seed data
FRACT. FOLD is a data set that consists of a seed data (405 FAQs), 10 pairs of training data (3713 sen-tences per training data) and testing data (413 sentences per testing data). Using FOLD, we performed 10-and training data (3713 sentences per training data), and a testing data (413 sentences). Using SEED, we evaluated the performances of FRACT according to various seed data for query log clustering. In other words, we assumed that each seed data was original FAQ lists. Then, we clustered the training data asso-ciated with each seed data. Based on the clustering results, we evaluated the performances of FRACT.
TRAIN is a data set that consists of a seed data (405 FAQs), 10 training data, and a testing data (413 sentences). In TRAIN, each training data contains 371 X 3713 sentences at an increasing rate of 10%. Using
TRAIN, we evaluated the performances of FRACT according to the sizes of training data for query log clustering.

To evaluate the performances of FRACT/CL, we computed the precisions of clustering and the recall rates. To evaluate the performances of FRACT/IR, we computed the MRRs (Mean Reciprocal Rank) and the miss rates. The MRR represents the average value of the reciprocal ranks of the first correct FAQs given by each query, as shown ( Voorhees &amp; Tice, 1999 )
In Eq. (7) , rank i is the rank of the first correct FAQ given by the i th query, and q ries. The miss rate means the ratio of the cases that FRACT/IR fails to return correct FAQs, as shown 4.2. Experimental results
The first experiment performed was to evaluate FRACT/CL using 10 training data in FOLD. We cal-culated the average precisions and average recall rates of FRACT/CL at different similarity measures, as coefficients. JACCARD is the Jaccard coefficients. As shown in Table 3 , DicSim outperformed all of the other measures (COS, DICE and JACCARD) in both the average precision and the average recall rate.
Specifically, FRACT/CL using DicSim clustered about 129 query logs more than the systems using the other measures. There were not any overlaps between the words in the 129 query logs and the words in the original FAQs. This fact reveals that the lexical disagreement problems often occur in short document clustering (i.e. sentence clustering) and DicSim can partially bridge the lexical chasms.
In the second experiment, we compared the average performances of FRACT/IR with those of tradi-tional IR systems by using FOLD, as shown in Table 4 .In Table 4 , IDEAL-FRACT means FRACT/IR with ideal FRACRT/CL of which both the precision and the recall rate are 1.0. TF  X  IDF is the simple TFIDF retrieval model ( Salton &amp; McGill, 1983 ), OKAPI is the Okapi BM25 retrieval model ( Robertson,
Walker, Jones, Beaulieu, &amp; Gatford, 1994 ), and KL is the KL-divergence language model using JM back experiment, we set the number of feedback documents to 5. We implemented these traditional IR sys-tems using LeMur Toolkit version 3.0 ( Lemur-3.0, 2000 X 2004 ). As shown in Table 4 , FRACT/IR outperforms all comparison systems except IDEAL-FRACT in both the average MRR and the average miss rate. Specifically, FRACT/IR reduced the average miss rate by 5.2 X 9.6%. This fact reveals that
FRACT/IR can resolve some lexical disagreement problems between queries and FAQs. The difference be-tween the performance of FRACT/IR and the performance of IDEAL-FRACT is 0.11834 MRR (0.09309 miss rate). This fact reveals that the more we can increase the performance of FRACT/CL, the more we can increase the performance of FRACT/IR.
 In the third experiment, we compared the performances of FRACT with those of OKAPI by using SEED, as shown in Table 5 . Table 5 shows that FRACT is less sensitive to seed data than the representative
IR system, OKAPI. As shown in Table 5 , the difference between the highest MRR of FRACT/IR and the lowest MRR of FRACT/IR is 0.2331. This fact shows how important it is to construct well-organized FAQs.

In the last experiment, we computed the performances of FRACT by using TRAIN, as shown in Table 6 . Table 6 shows that FRACT needs at least 5.5 times as many query logs as original FAQs in order to accomplish less than 0.15 miss rate. 4.3. Failure analysis
We analyzed the cases that FRACT failed to retrieve correct FAQs with high ranks. We found some reasons why the FAQs were low ranked or missed as follows:
There were many cases that FRACT/CL failed to correctly cluster query logs into relevant FAQs. As shown in Table 4 , we can reduce the gaps between the MRR of FRACT/IR and the MRR of IDEAL-FRACT if we can increase the precision of FRACT/CL.

There were still the lexical disagreement problems between users queries and clusters. FRACT can resolve some of the lexical disagreement problems because FRACT uses clusters to smooth FAQs. How-ever, we found many cases where there was very little overlap between words in queries and words in clusters. To solve the problem at a basic level, we should study new methods that match users queries against FAQs in the semantic levels.

There were some cases where several FAQs were associated with only one query. In these cases, we could not select the FAQs that were entirely relevant to those queries. To solve the problem, information sup-pliers should accurately construct initial FAQs and should constantly update the FAQs. 5. Conclusion
We presented a high-precision FAQ retrieval system using query log clustering. The FAQ retrieval sys-query log clustering system gathers logs of users queries and groups the logs into original FAQ categories.
The cluster-based retrieval system uses the clustering results to bridge the lexical chasms between queries and FAQs. In the experiment, we found that the proposed system outperforms traditional IR systems in
FAQ retrieval and resolves some of the lexical disagreement problems. For further studies, we will collect various types of FAQs and will perform more experiments on the proposed system. We will also continue to study similarity measures for query log clustering.
 Acknowledgment
This work was supported by the Korea Research Foundation Grant funded by Korean Government (MOEHRD) (No. M01-2004-000-20036-0).
 References
