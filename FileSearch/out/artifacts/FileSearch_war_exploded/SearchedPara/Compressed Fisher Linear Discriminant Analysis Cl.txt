 We consider random projections in conjunction with classifi -cation, specifically the analysis of Fisher X  X  Linear Discrim inant (FLD) classifier in randomly projected data spaces.
Unlike previous analyses of other classifiers in this set-ting, we avoid the unnatural effects that arise when one in-sists that all pairwise distances are approximately preserv ed under projection. We impose no sparsity or underlying low-dimensional structure constraints on the data; we instead take advantage of the class structure inherent in the prob-lem. We obtain a reasonably tight upper bound on the esti-mated misclassification error on average over the random choice of the projection, which, in contrast to early dis-tance preserving approaches, tightens in a natural way as the number of training examples increases. It follows that, for good generalisation of FLD, the required projection di-mension grows logarithmically with the number of classes. We also show that the error contribution of a covariance misspecification is always no worse in the low-dimensional space than in the initial high-dimensional space. We con-trast our findings to previous related work, and discuss our insights.
 H.2.8 [ Database Management ]: Database applications X  Data mining ; I.5.1 [ Pattern Recognition ]: Models X  Sta-tistical Theory Dimensionality Reduction, Random Projection, Compressed Learning, Linear Discriminant Analysis, Classification
Dimensionality reduction via random projections has at-tracted considerable interest for its theoretical guarant ees to approximately preserve pairwise distances globally, and fo r the computational advantages that it offers. While this the-ory is quite well developed, much less is known about exact guarantees on the performance and behaviour of subsequent data analysis methods that work with the randomly pro-jected data. Obtaining such results is not straightforward, and is subject to ongoing recent research efforts.
In this paper we consider the supervised learning problem of classifying a query point x q  X  R d as belonging to one of several Gaussian classes using Fisher X  X  Linear Discriminan t (FLD) and the classification error arising if, instead of lea rn-ing the classifier in the data space R d , we instead learn it in some low dimensional random projection of the data space R ( R d ) = R k , where R  X  M k  X  d is a random projection ma-trix with entries drawn i.i.d from the Gaussian N (0 , 1 /d ). FLD is one of the most enduring methods for data classi-fication, yet it is simple enough to be tractable to detailed formal analysis in the projected domain.

The main practical motivations behind this research are the perspective of mitigating the issues associated with th e curse of dimensionality by working in a lower dimensional space, and the possibility of not having to collect or store the data in its original high dimensional form.

A number of recent studies consider efficient learning in low dimensional spaces. For example, in [5] Calderbank et al demonstrate that if the high dimensional data points have a sparse representation in some linear basis, then it is possi ble to train a soft-margin SVM classifier on a low dimensional projection of that data whilst retaining a classification pe r-formance that is comparable to that achieved by working in the original data space. However, the data points must be capable of a sparse representation which for a general class-learning problem may not be the case.

In [9] Davenport et al prove high probability bounds (over the choice of random projection) on a series of signal pro-cessing techniques, among which are bounds on signal clas-sification performance for a single test point using Neyman-Pearson detector and on the estimation of linear functions o f signals from few measurements when the set of potential sig-nals is known but with no sparsity requirement on the signal. Similarly, in [12] Haupt et al demonstrate that ( m + 1)-ary hypothesis testing can be used to specify, from few measure-ments, to which of a known collection of prototypes a signal belongs. More recently bounds on least squares regression in projected space, with no sparsity requirement on the data, have been presented by Maillard and Munos in [14].
We also do not require our data to have a sparse represen-tation. We ask whether we can still learn a classifier using a low-dimensional projection of the data (the answer is  X  X es X  ), and to what dimensionality k can the data be projected so that the classifier performance is still maintained.
We approach these questions by bounding the probability that the classifier assigns the wrong class to a query point if the classifier is learned in the projected space. Such bounds on the classification error for FLD in the data space are al-ready known, for example they are given in [4, 15], but in neither of these papers is classification error in the projec ted domain considered; indeed in [9] it is stated that establish -ing the probability of error for a classifier in the projected domain is, in general, a difficult problem.

As we shall see in the later sections, our bounds are reason-ably tight and as a direct consequence we find that the pro-jection dimension required for good generalisation of FLD in the projection space depends logarithmically on the number of classes. This is, of course, typically very low compared to the number of training examples. Unlike the bounds in [3], where the authors X  use of the Johnson-Lindenstrauss Lemma 1 has the unfortunate side-effect that their bound loosens as the number of training examples increases, be-cause of the increased accuracy of mean estimates our bound will tighten with more training data.
 The structure of the remainder of the paper is as follows: We briefly describe the supervised learning problem and de-scribe the FLD classifier. We then bound the probability that FLD will misclassify an unseen query point x q having learned the classifier in a low dimensional projection of the data space and applied it to the projected query point R x This is equivalent to bounding the expected (0 , 1)-loss of the projected FLD classifier. Finally we discuss our findings and indicate some possible future directions for this work.
In a supervised learning problem we observe N examples of training data T N = { ( x i , y i ) } N i =1 where ( x some (usually unknown) distribution with x i  X  D x  X  R d and y i  X  C , where C is a finite collection of class labels partitioning D .
 For a given class of functions H , our goal is to learn from T the function  X  h  X  H with the lowest possible generalisation error in terms of some loss function L . That is, find  X  h such that L (  X  h ) = arg min point with unknown label y .
 Here we use the (0 , 1)-loss L (0 , 1) as a measure of performance defined by: In the case we consider here, the class of functions H con-sists of instantiations of FLD learned on randomly-projecte d bound the probability that an arbitrarily drawn and previ-ously unseen query point x q  X  D x is misclassified by the learned classifier. Our approach is to bound:
For a proof of the JLL and its application to random pro-jection matrices see e.g. [8, 1] and then to bound the corresponding probability in a ran-dom projection of the data: For concreteness and tractability, we will do this for the Fisher Linear Discriminant (FLD) classifier, which is briefl y reviewed below.
FLD is a generative classifier that seeks to model, given training data T N , the optimal decision boundary between classes. It is a successful and widely used classification method. The classical version is formulated for 2-class pro b-lems, as follows. If  X  0 ,  X  =  X  0 =  X  1 and  X  0 and  X  1 are known then the optimal classifier is given by Bayes X  rule [4]: h ( x q ) = 1 log (1  X   X  0 ) f 1 ( x q ) = 1 log 1  X   X  0 where 1 ( A ) is the indicator function that returns one if A is true and zero otherwise, and f y is the probability den-sity function of the y -th data-class, in its simplest form the multivariate Gaussian N (  X  y ,  X ), namely: In the sequel, we shall assume that the observations x are drawn from one of m +1 multivariate Gaussian classes 2 D x P y =0  X  y N (  X  y ,  X ) with unknown parameters  X  y and  X  that we need to estimate from training data. As usual,  X  y is a vector of means and  X  is a full-rank covariance matrix.
Our main result is theorem 4.8, which bounds the esti-mated misclassification probability of the two-class Fishe r X  X  Linear Discriminant classifier (FLD) when the classifier is learnt from a random projection of the original training dat a and the class label is assigned to a query point under the same random projection of the data. Our bound holds on average over the random projection matrices R , in contrast to other bounds in the literature [8, 3] where the techniques depend upon the fact that under suitable conditions ran-domly projected data satisfies the Johnson Lindenstrauss Lemma with high probability. We commence by an analysis of the error probability of FLD in the data space, which provides us with an upper bound on the error probability in a suitable form to make our subsequent average error analysis tractable in the pro-jection space. Here we also show how we can deal with multiclass cases, and we highlight the contribution of the estimated error and the estimation error terms in the over-all generalisation error. Our data space analysis also has the advantage of being slightly more general than existing ones
The Gaussianity of the true class distributions can be re-laxed to sub-Gaussian class densities without any effort  X  see comment in the Appendix section 7.0.4. in e.g. [4, 15], in that it holds also for non-Gaussian but sub-Gaussian classes.

We then review some tools from matrix analysis in prepa-ration for the main section. The proof of our main result, namely the bound on the estimated error probability of FLD in a random projection of the data space, then follows, along with the required dimensionality of the projected space as its direct consequence.
Our starting point is the following lemma which, for com-pleteness, is proved in the appendix: Lemma 4.1. Let x q  X  D x . Let H be the class of FLD functions and let  X  h be the instance learned from the training data T N . Assume that we have sufficient data so that  X  y ( X   X  and  X   X  y lie on the same side of the estimated hyperplane)  X  y,  X  y  X  X  = { 0 , 1 } , y 6 =  X  y . Then the probability that x misclassified is bounded above by Pr x q [  X  h ( x q ) 6 = y ] 6 with  X  y the mean of the class from which x q was drawn, estimated class means  X   X  0 and  X   X  1 , model covariance  X  class priors  X  0 and 1  X   X  0 , and estimated class priors  X   X  1  X   X   X  0 .
The multi-class version of FLD may be analysed in exten-sion to the above analysis as follows: Lemma 4.2. Let C = { 0 , 1 , . . . , m } be a collection of m + 1 classes partitioning the data.
 Let x q  X  D x . Let H be the class of FLD functions and let  X  h be the instance learned from the training data T N . Then, the probability that an unseen query point x q is misclassified by FLD is given by Pr x q [  X  h ( x q ) 6 = y ] 6 :
Proof. The decision rule for FLD in the multi-class case is given by: Without loss of generality, we again take the correct class to be class 0 and we assume uniform estimated priors, the non-uniform case being a straightforward extension of lemma 4.1. Hence: and so misclassification occurs when: Then since if A  X  X  X  B , Pr( A ) = Pr( B ), we have:
Pr x q [  X  h ( x q ) 6 = 0] = Pr x q where (4.4) follows by the union bound. Writing out (4.5) via Bayes X  rule, we find a sum of 2-class error probabilities of the form that we have dealt with earlier, so (4.5) equals: X applying the bounding technique used for the two-class case m times to each of the m possible incorrect classes. The line of thought is then the same for y = 1 , . . . , y = m in turn.
Owing to the straightforward way in which the multiclass error bound boils down to sums of 2-class errors, as shown in lemma 4.2 above, it is therefore sufficient for the remainder of the analysis to be performed for the 2-class case, and for m + 1 classes the error will always be upper bounded by m times the greatest of the 2-class errors. This will be used later in Section 4.5.

Next, we shall decompose the FLD bound of lemma 4.1 into two terms, one of which will go to zero as the number of training examples increases. This gives us the opportu-nity to assess the contribution of these two sources of error separately. Lemma 4.3. Let x q  X  D x and let H be the class of FLD functions and  X  h be the instance learned from the training data T N . Write for the estimated error  X  B ( X   X  0 ,  X   X  X 4.1. Then Pr x q [  X  h ( x q ) 6 = y ] 6 the class from which x q was drawn, estimated class means  X   X  y with  X   X  yi the i -th component, model covariance  X   X  , and estimated class priors  X   X  y and 1  X   X   X  y .

Proof. We will use the mean value theorem 3 , so we start by differentiating B y  X  X  with respect to  X  0 . Writing  X   X  B 1 for the two exp terms in (4.7), we have: Since the exponential term is bounded between zero and one, the supremum of the i -th component of this gradient exists provided that |  X   X  1 +  X   X  0  X  2  X  0 | &lt;  X  and |  X   X  1 we have that
B 6  X  0  X  B 0 + max Now applying the mean value theorem again w.r.t.  X  1 de-composes the latter term similarly, then taking the maxi-mum over both classes yields the desired result. We call the two terms obtained in (4.8) the  X  X stimated error X  and  X  X stimation error X  respectively. The estimation error can be bounded using Chernoff bounding techniques, and converges to zero with increasing number of training examples.
Mean value theorem in several variables: Let f be differen-tiable on S , an open subset of R d , let x and y be points in S such that the line between x and y also lies in S . Then:
In the remainder of the paper, we will take uniform model priors for convenience. Then the exponential terms of the estimated error (4.7) are all equal and independent of y , so using that P y  X  X   X  y = 1 the expression (4.7) of the estimated error simplifies to:  X  B ( X   X  0 ,  X   X  1 ,  X   X  ,  X ) = We now have the groundwork necessary to prove our main result, namely a bound on this estimated misclassification probability if we choose to work with a k -dimensional ran-dom projection of the original data. From the results of lemma 4.3 and lemma 4.2, in order to study the behaviour of our bound, we may restrict our attention to the two-class case and we focus on bounding the estimated error term  X  which, provided sufficient training data, is the main source of error. Before proceeding, the next section gives some technical tools that will be needed.
First, we note that due to linearity of both the expecta-tion operator E[ ], and the random projection matrix R , the projection of the true mean  X  and sample mean  X   X  from the data space to the projected space coincides with the true mean R X  and the sample mean R  X   X  in the projected space. Furthermore, the projected counterparts of the true covari -ance matrix  X  and the model covariance  X   X  are given by R  X  R T and R  X   X  R T respectively. Hence, we may talk about projected means and covariances unambiguously.

We will, in the proof that follows, make frequent use of several results. Apart from lemma 4.6 these are more or less well-known, but for convenience we state them here for later reference.
 Lemma 4.4 (Rayleigh quotient. ([13], Theorem 4.2.2 Pg 176)) . If Q is a real symmetric matrix then its eigenvalues  X  satisfy: and, in particular: Lemma 4.5 (Poincar  X e Separation Theorem. ([13], Corol-lary 4.3.16 Pg 190)) . Let S be a symmetric matrix S  X  X  d let k be an integer, 1 6 k 6 d , and let r 1 , . . . , r k orthonormal vectors. Let T = r T i S r j  X  M k (that is, in our setting, the r T i are the rows and the r j the columns of the random projection matrix R  X  M k  X  d and so T = RSR T ). Arrange the eigenvalues  X  i of S and T in increasing magni-tude, then: and, in particular: Lemma 4.6 (Corollary to lemmata 4.4 and 4.5) . Let Q be symmetric positive definite, such that  X  min ( Q ) &gt; 0 and so Q is invertible. Let u = R v , v  X  R d , u 6 = 0  X  R k . Then: Proof: We use the eigenvalue identity  X  min ( Q  X  1 ) = 1 / X  Combining this identity with lemma 4.4 and lemma 4.5 we have: Since RQR T is symmetric positive definite. Then by positive definiteness and lemma 4.5 it follows that: And so by lemma 4.4: Lemma 4.7 (Kantorovich Inequality. ([13], Theorem 7.4.41 Pg 444)) . Let Q be a symmetric positive definite matrix Q  X  M d with eigenvalues 0 &lt;  X  min 6 . . . 6  X  max . Then, for all v  X  R d : With equality holding for some unit vector v .
 This can be rewritten:
We now proceed to the promised bound. Theorem 4.8. Let x q  X  X  x = N (  X  y ,  X ) . Let H be the class of FLD functions and let  X  h be the instance learned from the training data T N . Let R  X  M k  X  d be a random projection matrix with entries drawn i.i.d from the univariate Gaus-sian N (0 , 1 /d ) . Then the estimated misclassification error  X  Pr
R, x q [  X  h ( R x q ) 6 = y ] is bounded above by: with  X  y the mean of the class from which x q was drawn, estimated class means  X   X  0 and  X   X  1 , model covariance  X 
Proof. We will start our proof in the dataspace, high-lighting the contribution of covariance misspecification i n the estimated error, and then make a move to the projected space with the use of a result (lemma 4.9) that shows that this component is always non-increasing under the random projection.

Without loss of generality we take x q  X  X  (  X  0 ,  X ), and for convenience take the estimated class priors to be equal i.e. 1  X   X   X  0 =  X   X  0 . By lemma 4.1, the estimated misclassification error in this case is upper bounded by: Now, in the Kantorovich inequality (lemma 4.7) we can take: where we use the fact ([13], Theorem 7.2.6, pg. 406) that since  X   X   X  1 is symmetric positive definite it has a unique sym-metric positive semi-definite square root: and we will take our positive definite Q to be Q =  X   X   X  1 / 2 (ibid. pg. 406). Then, by lemma 4.7 we have the expression (4.21) is less than or equal to: where the change in argument for the eigenvalues comes ([16], pg. 29). After simplification we can write this as: The term g (  X   X   X  1  X ) is a function of the model covariance misspecification, e.g. due to the imposition of diagonal or spherical constraints on  X   X . The following lemma shows that this term of the error can only decrease or stay the same after a random projection.
 Lemma 4.9 (Non-increase of covariance misspecification er-ror in the projected space) . Let Q be a symmetric positive definite matrix. Let K ( Q ) =  X  max ( Q )  X  rocal of the condition number of Q . Let g ( Q ) be as given in the theorem 4.8. Then, for any fixed k  X  d matrix R with orthonormal rows: Proof: We will show that g ( ) is monotonic decreasing with K on [1 ,  X  ) , then show that K (( R  X   X  R T )  X  1 R  X  R and hence g (( R  X   X  R T )  X  1 R  X  R T ) &gt; g (  X   X   X  1 Step 1 We show that g is monotonic decreasing: Step 2 We show that K (( R  X   X  R T )  X  1 R  X  R T ) 6 K (  X  Finally putting the results of steps 1 and 2 together gives the lemma 4.9.

Back to the proof of theorem 4.8, we now move into the low dimensional space defined by any fixed random projec-tion matrix R (i.e. with entries drawn from N (0 , 1 /d ) and orthonormalised rows). By lemma 4.9, we can upper bound the projected space counterpart of (4.23) by the following: exp  X  1 This holds for any fixed orthonormal matrix R , so it also holds for a fixed random projection matrix R .

Note, in the dataspace we bounded Pr x q [  X  h ( x q ) 6 = y ] but in the projected space we want to bound: This is the expectation of (4.35) w.r.t. the random choices of R . So we have: Pr 6 E R exp  X  6 E R exp  X  where the last step is justified by lemma 4.6. Now, since the entries of R where drawn i.i.d from N (0 , 1 /d ), the term ( X   X  1  X   X   X  0 ) T R T R ( X   X  1  X   X   X  0 ) = k R ( X   X  1  X   X   X  0 and (4.39) is therefore the moment generating function of a  X  2 distribution.
 Hence we can rewrite (4.39) as: A similar sequence of steps proves the other side, when x N (  X  1 ,  X ), and gives the same expression. Then putting the two terms together, applying the law of total probability with P y  X  X   X  y = 1 finally gives theorem 4.8. Although we have taken the entries of R be drawn from N (0 , 1 /d ) this was used only in the final step, in the form of the moment generating function of the  X  2 distribution. In consequence, other distributions that produce inequality in the step from equation (4.39) to equation (4.40) suffice. Such distributions include sub-Gaussians and some examples of suitable distributions may be found in [1]. Whether any deterministic projection R can be found that is both non-adaptive (i.e. makes no use of the training labels) and still yields a non-trivial guarantee for FLD in terms of only the data statistics seems a difficult open problem.
For both practical and theoretical reasons, we would like to know to which dimensionality k we can project our orig-inal high dimensional data and still expect to recover good classification performance from FLD. This may be thought of as a measure of the difficulty of the classification task.
By setting our bound to be no more than  X   X  (0 , 1) and solving for k we can obtain such a bound on k for FLD that guarantees that the expected misclassification probability (w.r.t. R ) in the projected space remains below  X  : Corollary 4.10. [to Theorem 4.8] Let k, d, g ( ) ,  X   X  y ,  X  , be as given in theorem 4.8. Then, in order that the probabil-ity of misclassification in the projected space remains below  X  it is sufficient to take: k &gt; 8 d X  max ( X ) Proof: In the 2-class case we have:  X  &gt; 1 + 1 log(1 / X  ) 6 k then using the inequality (1 + x ) 6 e x ,  X  x  X  R we obtain: Using (4.44) and lemma 4.2, it is then easy to see that to expect no more than  X  error from FLD in an m + 1 -class problem, the required dimension of the projected space need only be: k &gt; 8 d X  max ( X ) as required.

We find it interesting to compare our k bound with that given in the seminal paper of Arriaga and Vempala [3]. The analysis in [18] shows that the bound in [3] for randomly pro-jected 2-class perceptron classifiers is equivalent to requir ing that the projected dimensionality where  X  is the user-specified tolerance of misclassification probability, N is the number of training examples, and L/l is the diameter of the data ( L = max n =1 ,...,N k x n k vided by the margin (or  X  X obustness X , as they term it). In our bound, g ( ) is a function that encodes the quality of the model covariance specification,  X  and k are the same as in [3] and the factor d X  max ( X ) k  X   X  1  X   X   X  0 k  X  2  X  which, should be noted, is exactly the reciprocal of the squared class sep-aration as defined by Dasgupta in [6]  X  may be thought of as the  X  X enerative X  analogue of the data diameter divided by the margin in (4.46).

Observe, however, that (4.46) grows with the log of the training set size, whereas ours (4.44) grows with the log of the number of classes. This is not to say, by any means, that FLD is superior to perceptrons in the projected space. Instead, the root and significance of this difference lies in t he assumptions (and hence the methodology) used in obtaining the bounds. The result in (4.46) was derived from the pre-condition that all pairwise distances between the training points must be approximately preserved uniformly cf. the Johnson-Lindenstrauss lemma [8]. It is well understood [2] that examples of data sets exist for which the k = O (log N ) dimensions are indeed required for this. However, we con-jecture that, for learning, this starting point is too stron g a requirement. Learning should not become harder with more training points  X  assuming of course that additional examples add  X  X nformation X  to the training set.

Our derivation is so far specific to FLD, but it is able to take advantage of the class structure inherent in the classi fi-cation setting in that the misclassification error probabil ity is down to very few key distances only  X  the ones between the class centers.

Despite this difference from [3] and approaches based on uniform distance preservation, in fact our conclusion shou ld not be too surprising. Earlier work in theoretical computer science [6] proves performance guarantees with high proba-bility (over the choice of R ) for the unsupervised learning of a mixture of Gaussians which also requires k to grow loga-rithmically with the number of classes only. Moreover, our finding that the error from covariance misspecification is al -ways non-increasing in the projection space is also somewhat expected, in the light of the finding in [6] that projected co-variances tend to become more spherical.

In closing, it is also worth noting that the extensive empir-ical results in e.g. [7] and [11] also suggest that classifica tion (including non-sparse data) requires a much lower projectio n dimension than that which is needed for global preservation of all pairwise distances cf. the JLL. We therefore conjec-ture that, all other things being equal, the difficulty of a classification task should be a function only of selected dis -tances, and preserving those may be easier that preserving every pairwise distance uniformly. Investigating this mor e generally remains for further research.
We present three numerical tests that illustrate and con-firm our main results.

Lemma 4.9 showed that the error contribution of a covari-ance misspecification is always no worse in the low dimen-sional space than in the high dimensional space. Figure 1 shows the quality of fit between a full covariance  X  and its diagonal approximation  X   X  when projected from a d = 100 dimensional data space into successively lower dimensions k . We see the fit is poor in the high dimensional space, and it keeps improving as k gets smaller. The error bars span the minimum and maximum of g ([ R  X   X  R T ]  X  1 R  X  R observed over 40 repeated trials for each k . Figure 1: Experiment confirming Lemma 4.9; the error contribution of a covariance misspecification is always no worse in the projected space than in the data space. The best possible value on the vertical axis is 1 , the worst is 0 . We see the quality of fit is poor in high dimensions and improves dramatically in the projected space, approaching the best value as k decreases.
The second set of experiments demonstrates Corollary 4.10 of our Theorem 4.8, namely that for good generalisa-tion of FLD in the projected space, the required projection dimension k is logarithmic in the number of classes.
We randomly projected m equally distanced spherical unit variance 7-separated Gaussian classes from d = 100 dimen-sions and chose the target dimension of the projected space as k = 12 log( m ). The boxplots in figure 2 show, for each m tested, the distribution of the empirical error rates over 100 random realisations of R , where for each R the empiri-cal error was estimated from 500 independent query points. Other parameters being unchanged, we see the classification performance is indeed maintained with this choice of k . Figure 2: Experiment illustrating Theorem 4.8 &amp; its Corollary 4.10. With the choice k = 12 log( m ) and k  X  i  X   X  j k = 7 ,  X  i 6 = j , the classification performance is kept at similar rates while the number of classes m varies. Figure 3: Experiment illustrating Theorem 4.8. We fix the number of classes, m + 1 = 10 , and the data dimensionality, d = 100 , and vary the projection di-mensionality k . The classification error decreases nearly exponentially as k  X  d .

The third experiment shows the effect of reducing k for a 10-class problem in the same setting as experiment two. As expected, the classification error in figure 3 decreases nearl y exponentially as the projected dimensionality k tends to the data dimensionality d . We note also, from these empirical results, that the variability in the classification perform ance also decreases with increasing k . Finally, we observe that the worst performance in the worst case is still a weak learner that performs better than chance.
We proved our theorem 4.8 for classes with identical co-variance matrices in order to ensure that our exposition was reasonably sequential, as well as to keep our notation as un-cluttered as possible. However, it can be seen from the proof that this is not essential to our argument and, in particular , the following two simple extensions can be easily proved:
By replacing  X  in equation (7.1) with  X  y , the following analogue of theorem 4.8 can be derived for the 2-class case when the true class structure is Gaussian (or sub-Gaussian) but with different class covariance matrices  X  0 and  X  1 :
In a similar way if the classes have a multimodal structure then, by representing each class as a finite mixture of Gaus-provided the conditions of theorem 4.8 hold for the Gaus-sian in the mixture with the greatest contribution to the misclassification error, we can upper bound the correspond-ing form of equation (7.1), for the case y = 0 as follows:
X 6 max
Where M y is the number of Gaussians in the mixture for class y , w yi is the weight of the i -th Gaussian in the mixture, P i w yi = 1, and  X  yi ,  X  yi are the corresponding true mean and true covariance.

The proof then proceeds as before and the resultant bound, which of course is nowhere near tight, still gives k of the same order as the bound in theorem 4.8. In this setting, the condition  X  y &gt; 0 implies that the centres of the Gaussian mixture components are at least nearly linearly separable. In the high-dimensional data space this can still be a reason -able assumption (unless the number of mixture components is large), but it is clear that in practice it is much less like ly to hold in the projected space.
We considered the problem of classification in non-adaptive dimensionality reduction using FLD in randomly projected data spaces.

Previous results considering other classifiers in this setti ng gave guarantees on classification performance only when all pairwise distances were approximately preserved under pro-jection. We conjectured that, if one were only interested in preserving classification performance, that it would be suffi-cient to preserve only certain key distances. We showed that , in the case of FLD, this is sufficient (namely preserving the separation of the class means). We employed a simple gen-erative classifier in our working, but one might imagine that e.g. for projected SVM it would be sufficient to preserve only the separation of the support vectors. Our only assump-tion on the data was that the distribution of data points in each class be dominated by a Gaussian and, importantly, we did not require our data to have a sparse or implicity low-dimensional structure in order to preserve the classifi -cation performance. We also showed that misspecification of the covariance of the Gaussian in the projected space has a relatively benign effect, when compared to a similar mis-specification in the original high dimensional data space, a nd we proved that if k = O log( m ) then it is possible to give guarantees on the expected classification performance (w.r. t R ) of projected FLD.

One practical consequence of our results, and the other similar results in the literature, is to open the door to the possibility of collecting and storing data in low-dimensio nal form whilst still retaining guarantees on classification pe r-formance.

Moreover, answering these questions means that we are able to foresee and predict the behaviour of a randomly pro-jected FLD classifier, and the various factors that govern it , before actually applying it to a particular data set.
Future research includes an analysis of the behaviour of the projected classifier when there is only a small amount of training data, and the extension of our results to general multimodal classes.

We note that the finite sample effects are characterised by (i) the estimation error and (ii) the fact that when the condition  X  y &gt; 0 holds in the data space it is possible that random projection causes it to no longer hold in the pro-jected space.
 The estimation error (i) depends on the quality of the es-timates of the class means and class covariances, and these can be analysed using techniques that are not specific to working in the randomly projected domain, e.g. [14, 17]. Observe, however, that because there are fewer parameters to estimate, the estimation error in the projected space mus t be less than the estimation error in the data space. The second effect (ii) involves the probability that two vec-tors in the data space with angular separation  X   X  (0 ,  X / 2), have angular separation  X  R &gt;  X / 2 in the projected space following random projection. We can show that this proba-bility is typically small, and our results regarding this effe ct are currently in preparation for publication [10]. [1] D. Achlioptas. Database-friendly random projections: [2] N. Alon. Problems and results in extremal [3] R. Arriaga and S. Vempala. An algorithmic theory of [4] P. Bickel and E. Levina. Some theory for Fisher X  X  [5] R. Calderbank, S. Jafarpour, and R. Schapire. [6] S. Dasgupta. Learning Mixtures of Gaussians. In [7] S. Dasgupta. Experiments with random projection. In [8] S. Dasgupta and A. Gupta. An Elementary Proof of [9] M.A. Davenport, M.B. Wakin and R.G. Baraniuk. [10] R.J. Durrant and A. Kab  X an. Finite Sample Effects in [11] D. Fradkin and D. Madigan. Experiments with [12] J. Haupt, R. Castro, R. Nowak, G. Fudge and A. Yeh. [13] R.A. Horn and C.R. Johnson. Matrix Analysis . CUP, [14] O.-A. Maillard and R. Munos. Compressed [15] T. Pattison and D. Gossink. Misclassification [16] K.B. Petersen and M.S. Pedersen. The Matrix [17] J. Shawe-Taylor, C. Williams, N. Cristianini, and [18] T. Watanabe, E. Takimoto, K. Amano and
Proof. (of lemma 4.1) We prove one term of the bound using standard techniques, the other term being proved sim-ilarly.
 Without loss of generality let x q have label y = 0. Then the probability that x q is misclassified is given by Pr x q [ y | y = 0] = Pr x q [  X  h ( x q ) 6 = 0]: =Pr x q log 1  X   X   X  0 for all  X  0 &gt; 0. Exponentiating both sides gives: =Pr x q exp ( X   X  1  X   X   X  0 ) 6 E x q exp ( X   X  1  X   X   X  0 ) by Markov inequality. Then, isolating terms in x q we have Pr = exp  X  1 This expectation is of the form of the moment generating function of a multivariate Gaussian and so: exp  X  T 0  X  0  X   X   X  1 ( X   X  1  X   X   X  0 )+ 1 where  X  0 is the true mean, and  X  is the true covariance matrix, of D x q .

Thus, we have the probability of misclassification is bounde d above by the following: exp  X  1 . . . +  X  T 0  X  0  X   X   X  1 ( X   X  1  X   X   X  0 ) + 1 Now, since this holds for every  X  0 &gt; 0 we may optimise the bound by choosing the best one. Since exponentiation is a monotonic increasing function, in order to minimise the bound it is sufficient to minimise its argument. Differenti-ating the argument w.r.t  X  0 and setting the derivative equal to zero then yields: This is strictly positive as required, since the denominator is always positive ( X  is positive definite, then so is  X   X   X  1 and the numerator is assumed to be positive as a precondi-tion in the theorem.

Substituting  X  0 back into the bound then yields, after some algebra, the following Pr x q [  X  h ( x q ) 6 = 0] 6 exp The second term, for when x q  X  N (  X  1 ,  X ), can be derived similarly and gives Pr x q [  X  h ( x q ) 6 = 1] 6 exp Finally, putting these two terms together and applying the law of total probability (since the classes in C partition the data): Pr x q [  X  h ( x q ) 6 = y ] = P y  X  X  Pr[ x q  X  N (  X  that Pr x q [  X  h ( x q ) 6 = y ] 6  X  0 exp (1  X   X  0 ) exp
We should confirm, of course, that the requirement that  X  y &gt; 0 is a reasonable one. Because the denominator in (7.2) is always positive the condition  X  y &gt; 0 holds when: ( X   X   X  y +  X   X  y  X  2  X  y ) T  X   X   X  1 ( X   X   X  y  X   X   X  y )  X  2 log It can be seen that  X  y &gt; 0 holds provided that for each class the true and estimated means are both on the same side of the decision hyperplane. Furthermore, provided that  X  y &gt; 0 holds in the data space we can show that w.h.p (and independently of the original data dimensionality d ) it also holds in the projected space [10], and so the requirement  X  y &gt; 0 does not seem particularly restrictive in this setting.
We note that, in (7.1) it is in fact sufficient to have in-equality. Therefore our bound also holds when the true dis-tributions D x of the data classes are such that they have a moment generating function no greater than that of the Gaussian. This includes sub-Gaussian distributions, i.e. di s-tributions whose tail decays faster than that of the Gaussian.
