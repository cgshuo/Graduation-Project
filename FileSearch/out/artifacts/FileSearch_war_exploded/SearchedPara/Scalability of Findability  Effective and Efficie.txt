 It is crucial to study basic principles that support adaptive and scalable retrieval functions in large networked environ-ments such as the Web, where information is distributed among dynamic systems. We conducted experiments on de-centralized IR operations on various scales of information networks and analyzed effectiveness, efficiency, and scala-bility of various search methods. Results showed network structure, i.e., how distributed systems connect to one an-other, is crucial for retrieval performance. Relying on partial indexes of distributed systems, some level of network cluster-ing enabled very efficient and effective discovery of relevant information in large scale networks. For a given network clustering level, search time was well explained by a poly-logarithmic relation to network size (i.e., the number of dis-tributed systems), indicating a high scalability potential for searching in a growing information space. In addition, net-work clustering only involved local self-organization and re-quired no global control  X  clustering time remained roughly constant across the various scales of networks.
 H.3.4 [ Information storage and retrieval ]: Systems and Software X  Distributed systems, Information networks Algorithms, Performance, Experimentation distributed IR, scalability, network clustering, decentralized search, weak tie, strong tie, clustering paradox, connectivity
In today X  X  digital environments, there exist a variety of in-formation networks where information is distributed among dynamic systems. On the Web, for example, individual web sites host diverse information topics and form a network by means of hyperlinks. Likewise, digital libraries interoperate with one another and serve information distributed across collections in a network. For reasons such as copyright and privacy, lots of information cannot be fully collected and in-dexed in advance for retrieval purposes. In addition to this is the dynamics of many environments such as the deep web and peer-to-peer networks, in which it is not only difficult to gather information but also challenging to keep an index up to date.

Centralized IR solutions can hardly survive the continued growth of today X  X  information spaces  X  they are vulnerable to scalability demands [3]. A distributed architecture is de-sirable and, due to many constraints, is often the only choice. Distributed (federated) IR research is a response to the chal-lenge of retrieving information from distributed sources. Re-cent distributed IR research has focused on intra-system re-trieval fusion/federation, cross-system communication, and distributed information storage and retrieval algorithms [9, 23].

Classic distributed information retrieval has shown some potential of efficiently and effectively bringing distributed in-formation together. However, the reliance on centralization of a metasearch server will continue to suffer from critical problems such as scalability, single point failure, and fault tolerance. Further decentralization of meta search models will involve issues beyond the main focus of federated IR research.

Research has been done under the theme of peer-to-peer information retrieval (P2P-IR) and, more recently, large scale distributed systems for IR (LSDS-IR) [5, 10, 16, 11]. While classic distributed IR often focuses on tens, if not hundreds, of distributed collections, P2P-or LSDS-IR usually envisions an IR problem situated in thousands and even millions of distributed, dynamic systems. The magnitude, distribution, and dynamics of information in such an environment remain a great challenge in IR. Applications of this research include not only search in peer-to-peer environments but also infor-mation retrieval in digital libraries, intelligent information discovery on the deep web, distributed desktop search, and agent-assisted web surfing etc.

Finding relevant information in distributed networked en-vironments transforms into a problem concerning informa-tion retrieval and complex networks. In this study, we focus on how relevant information can be effectively and efficiently found in large scale information networks, where no central-ized index can possibly be built. We investigate the impact of network structure/topology on the effectiveness and effi-ciency of decentralized IR operations relying on distributed indexes. We test the proposed retrieval methods in a grow-ing information space and examine the scalability potential.
While traditional IR and distributed IR research provides basic tools for attacking decentralized search problems, the evolving dynamics and heterogeneity of today X  X  networked environments challenge the sufficiency of classic methods and call for new innovations [3]. Whereas peer-to-peer of-fers a new type of architecture for application-level questions and techniques to be tested, research on complex networks studies related questions in their basic forms [2, 23].
In an open, dynamic information space such as a peer-to-peer network, people, information, and technologies are all mobile and changing entities. Identifying where relevant collections are for the retrieval of information is essential. Without global information, decentralized IR methods have to rely on individual indexes in distributed nodes and their limited local intelligence to collectively construct paths to desired information.

Recent years have seen growing popularity of peer-to-peer (P2P) networks for large scale information sharing and re-trieval [17]. There have been ongoing discussions on the applicability of existing P2P search models for IR, the ef-ficiency and scalability challenges, and the effectiveness of traditional IR models in such environments [23]. Some re-searchers applied Distributed Hashing Tables (DHTs) tech-niques to structured P2P environments for distributed re-trieval and focused on building an efficient indexing struc-ture over peers [7, 18, 21].

Others, however, questioned the sufficiency of DHTs for dealing with high dimensionality of IR (e.g., a large number of terms for document representation) in dynamic P2P envi-ronments [5, 17, 16]. For retrieval with a large feature space, which often requires frequent updates to cope with a tran-sient population, it is challenging for distributed hashing to work in a traffic-and space-efficient manner. Unstructured overlay systems work in an nondeterministic manner and have received increased popularity for being fault tolerant and adaptive to evolving system dynamics [17].
Research on complex networks provides valuable princi-ples for searching/navigation in distributed systems. Not only do many information networks such as the Web share the common phenomenon of small world but they also ap-pear to be searchable [2]. Particularly, studies showed that without global information about where targets are, mem-bers of a very large network are able to collectively construct short paths (if not the shortest) to destinations [15, 22, 8].
The implication in IR is that relevant information, in var-ious networked environments, is very likely a small number of connections/links away from the one who needs it and is potentially findable. This indicates potentials for decentral-ized retrieval algorithms to traverse an information network to find relevant information efficiently. However, this is not an easy task because not only relevant information is a few degrees/connects away but so is all information.

To find relevance in a densely-packed  X  X mall world X  net-work remains very challenging. Nonetheless, research has demonstrated how nodes connect to one another and the structure of the network they thus form have critical impacts on how searches function. Network clustering, sometimes by means of semantic overlay, can significantly improve effec-tiveness and efficiency of IR operations in an information network.

Clustering , the process of bringing similar entities together, is useful for information retrieval. Traditional IR research utilized document-level clustering to support exploratory searching and to improve retrieval effectiveness. In large scale distributed IR, topical clustering techniques such as semantic overlay networks ( SONs ) have been widely used, in which systems containing similar information form semantic groups for efficient searches [5, 10, 16].

Research indicated that a proper degree of network clus-tering with some presence of remote connections has to be maintained for efficient searches [15, 20]. Clustering reduces the number of  X  X rrelevant X  links and aids in creating topi-cal segments useful for orienting searches. With very strong clustering, however, a network tends to be fragmented into local communities with abundant strong ties but few weak ties to bridge remote parts [12]. Although searches might be able to move gradually toward targets, necessary  X  X ops X  become unavailable. We refer to this phenomenon as the Clustering Paradox , in which neither strong clustering nor weak clustering is desirable. The Clustering Paradox has re-ceived attention in complex network research and requires further scrutiny in a decentralized IR context [15, 14].
We have developed a multi-agent decentralized search ar-chitecture named TranSeen for finding relevant information distributed in networked environments. We illustrate the conceptual model in Figure 1 (a) and major components in Figure 1 (b). The TranSeen system is an implementation in Java, based on two well-known open-source platforms: 1) JADE, a multi-agent system/middle-ware that complies with the FIPA (the Foundation for Intelligent Physical Agents) specifications [6], and 2) Lucene, a high-performance library for full-text search [13]. Figure 1: Conceptual Framework. (a) Global View of agents working together to route a query in the network space. (b) Agent Internal View of how com-ponents function within an agent.

Assume that agents, representatives of distributed infor-mation systems, reside in an n dimensional (hypersphere) space. An agent X  X  location in the space represents its infor-mation topicality. Therefore, finding relevant sources for an information need is to route the query to agents in the rel-evant topical space. To simplify the discussion, assume all agents can be characterized using a two-dimensional space. Figure 1 (a) visualizes a 2D circle (1-sphere) representation of the information space. Let agent A u be the system that receives a query from the user whereas agent A v has the relevant information. The problem becomes how agents in the connected society, without global information, can col-lectively construct a short path to A v so that relevant in-formation can be retrieved from there. In Figure 1 (a), the query traverses a search path A u  X  A b  X  A c  X  A d  X  A v to reach the target. While agents A b and A d help move the query toward the target gradually (through strong ties), agent A c has a remote connection (weak tie) for the query to  X  X ump. X 
When an agent receives a query, it first conducts local search operations to retrieve relevant information from its individual document collection. If local results are unsatis-factory, e.g., relevance/similarity scores do not reach a pre-defined threshold, the agent will contact its neighbors for help. Therefore, there requires a mechanism for matching query representation with potential good neighbors  X  either the neighboring agent is more likely to have relevant infor-mation to answer the query directly or more likely to con-nected with relevant targets. Agents explore their neighbor-hoods through interactions (e.g., query-based sampling), de-velop some knowledge about neighbors X  topicality and con-nectivity, and serve as local decision makers in the search process. They are essentially metasearch systems for one another.
As discussed earlier, network structure plays an important role in decentralized search. We used a parameter called the clustering exponent  X  to guide network clustering for decen-tralized search: the probability p r oftwonodesbeingcon-nected/linked is proportional to r  X   X  ,where r is the pairwise topical distance and  X  the clustering exponent .

The clustering exponent  X  , as shown in Figure 2, describes a correlation between the network (topological) space and the search (topical) space [15, 8]. When  X  is small, con-nectivity has little dependence on topical closeness  X  local segments become less visible as the network is built on in-creased randomness. As shown in Figure 3 (c), the network is a random graph given a uniform connectivity distribution at  X  =0. When  X  is large, weak ties (long-distance connec-tions) are rare and strong ties dominate [12]. The network becomes highly segmented. As shown in Figure 3 (a), when  X   X  X  X  , the network is very regular (highly clustered) given that it is extremely unlikely for remote pairs to connect. Given a moderate  X  value, as shown in Figure 3 (b), the network becomes a narrowly defined small world ,inwhich both local and remote connections present. Figure 3: Network Clustering: Impact of Clustering Exponent  X  .

The clustering exponent  X  influences the emergence of lo-cal segments and overall network clustering. In complex network research, it has been shown that only with some particular value of  X  , search time (i.e., search path length) is optimal and bounded by a po ly-logarithmic function of network size [15]. One important aspect of this research is to study the impact of network structure on decentralized IR effectiveness and efficiency.
This section elaborates on specific algorithms used in the research. Section 4.1 presents the basic functions for infor-mation representation, neighbor representation, and similar-ity measurement. Section 4.2 describes four search (neighbor selection) algorithms based on neighbor similarity and/or connectivity. Section 4.3 elaborates on the function for agent rewiring (clustering) based on the clustering exponent  X  .
We used the Vector-Space Model (VSM) for information (document and query) representation [4]. Given that infor-mation was highly distributed, a global term space was not assumed. Instead, each agent processed information it in-dividually had and produced a local term space, which was used to represent each information item using the TF*IDF (Term Frequency * Inverse Document Frequency) weight-ing scheme. An information item was then converted to a numerical vector where a item t was computed by: where tf ( t ) is the frequency of the term t of the term space in the information item, N is the total number of informa-tion items (e.g., documents) in an agent X  X  local collection, and df ( t ) is the number of information items in the set con-taining the term t of the term space. We refer to log ( N as IDF. IDF values were computed within the information space of an agent given no global information.
Following a simple federated IR model, we allowed agents to collect document frequency (DF) information from neighors (distributed systems) and to use it to create metadocuments for neighbor representation [19]. Treating each metadocu-ment as a normal document, it was then straightforward to calculate neighbor frequency (NF) values of terms, i.e., the number of metadocuments (neighbors) containing a particu-lar term. A metadocument (neighbor) was then represented asavectorwhereaterm t was computed by: where df ( t ) is the frequency of the term t of the term space in the metadocument, N is the total number of an agent X  X  neighbors (metadocuments), and nf ( t )isthenum-ber of neighbors containing the term t . We refer to this func-tion as DF*INF , or document frequency * inverse neighbor frequency.
Based on the TF*IDF (or DF*INF ) values obtained above, pair-wise similarity values can be computed. Given a query q , the similarity score of a document d matching the query was computed by : where tf ( t ) is term frequency of term t in document d , idf(t) the inverse document frequency of t , coord ( q, d )aco-ordination factor based on the number of terms shared by q and d ,and queryNorm ( q ) a normalization value for query q given the sum of squared weights of query terms. The function is a variation of the well-known cosine similarity measure. Additional details can be found in [13, 4].
When an agent found no sufficiently relevant information from its local collection, it forwarded the query to another agent. We proposed the following four neighbor selection strategies, i.e., search methods, to be tested and compared in experiments.
The Random Walk (RW) strategy ignores knowledge about neighbors and simply forwards a query to a random neigh-bor. Without any learning module, Random Walk is pre-sumably neither efficient nor effective. Hence, the Random Walk served as the search performance lower-bound.
Let k be the number of neighbors an agent has and S = [ s , .., s k ] be the vector about neighbors X  similarity scores to a query. The SIM method sorts the vector and forwards the query to the neighbor with the highest score. We assumed that agents were cooperative  X  that is, they shared with one another document frequency (DF) values of key terms in their collections, based on which a meta document were cre-ated as representative of a neighbor X  X  topical area. A query was then compared with each meta document, represented by DF*INF (see Equation 2), to generate the similarity vec-tor S .
In the degree-based strategy, information about neigh-bors X  degrees, i.e., their numbers of neighbors, was known to the current agent. Let D =[ d 1 , .., d k ] denote degrees of an agent X  X  neighbors. The DEG method sorts the D vector and forwards the query to the neighbor with the highest degree, regardless of what a query is about [1].
The SimDeg method combines information about neigh-bors X  relevance to a query and their degrees. [20] reasoned that a navigation decision relies on the estimate of a neigh-bor X  X  distance from the target, or the probability that the neighbor links to the target directly, and proposed a measure based on the product of a degree term ( d ) and a similarity term ( s ) to approximate the expected distance. Following the same formulation, the SimDeg method used a combined measure SD =[ s 1  X  d 1 , .., s k  X  d k ] to rank neighbors, given neighbor relevance vector S =[ s 1 , .., s k ] and neighbor de-gree vector D =[ d 1 , .., d k ]. A query were forwarded to the neighbor with the highest sd value.
We used the clustering exponent  X  to guide agent self-organization and network clustering. For each agent, the first step was to determine how many neighbors it should have. Given the web collection (Section 5.1) used in this study, we obtained each agent X  X  (i.e., a web domain) inde-gree based on hyperlink analysis and normalized the degree to a value d  X  [30 , 60]. Once agent u determined its degree d , a number of random agents were selected for u such that the total number of random neighbors d T d u ( d T  X  150 in this study). Then, the current agent ( u ) used its metadoc-ument to query each of the d T neighbors ( v ) to determine their topical distance r uv . Finally, the following probability function was used by the agent to decide who should remain as neighbors (overlay): p uv  X  r  X   X  uv ,where  X  is the clustering exponent and r uv pairwise topical distance.
We used the ClueWeb09 Category B collection created by the Language Technologies Institute at CMU for IR exper-iments, which contains a crawl of 50 million English pages during Jan -Feb 2009. Analysis of the hyperlink graph pro-duced Figures 4 (a) in-degree frequency distribution and (b) Site size (#pages per site) distribution based on 50 , 221 , 776 pages extracted from 2 , 777 , 321 unique domains (treated as sites) (on log/log coordinates).
Given the large size of the data collection, it is nearly im-possible to manually judge the relevance of every document and to establish a complete relevance base. While previ-ous research on large scale distributed information retrieval mainly relied on similarity thresholds to do automatic rele-vance judgment, such an approach was rather arbitrary and was biased by the centralized IR system that served as the gold standard [5, 16]. (a) In-degree distribution (b) Site size distribution
Serving diverse users in an open, dynamic environment, implies that some queries are likely to be narrowly defined. We reasoned that relevant information is rare when a query is very specific. In this study, we used documents (web pages with title and content) as queries to simulate decentralized searches. We obtained a set of query documents by sam-pling documents from the 100 most popular web domains. Removing queries that were too broad or vague resulted in 85 queries.
To make searches more realistic/challenging and auto-matic evaluation more objecti ve, we considered extreme rar-ity of relevant documents given very specific information needs. We decided that, given each query, there was only one relevant document among all documents distributed in the network and the task was to find that exact document. When a query document was issued to a random system/site in the network, the task involved finding the system who hosted it. The strength of this task is that relevance judg-ment was more objective provided the relative unambigu-ity of a  X  X osting X  relationship. The extreme rarity, however, posed a great challenge on the proposed decentralized search methods.
This study focused on effectiveness and efficiency of IR op-erations in networks and scalability of decentralized search. We emphasized the finding of exact/rare information in large distributed environments and proposed the use of the follow-ing evaluation metrics.
Of various evaluation metrics used in TREC and IR, pre-cision and recal l are the basic forms. Whereas precision P measures the fraction of retrieved documents being relevant, recall R evaluates the fraction of relevant documents being retrieved. The harmonic mean of precision and recall, known as F 1 , is computed by F 1 = 2  X  P  X  R P + R [4].
In experiments, we measured the search path length L (i.e., the number of agents involved) and actual time  X  taken to find relevant information for each query. The average search length  X  L of all queries was calculated to measure ef-ficiency. When fewer agents are involved, the entire dis-tributed system is considered to be more efficient. Likewise, average search time  X   X  was calculated to evaluate efficiency.
One important objective of this research was to learn how decentralized IR systems can function and scale in very large information network. For scalability, we ran experiments on different network size scales N  X  [10 2 , 10 3 , 10 4 ]. First, we used the 100 most highly linked web domains to form a 100-agent network and conducted experiments on it. Then, we extended the network to 1 , 000 and 10 , 000 systems/sites for additional experiments. Table 1 shows the total number of documents on each network scale. After experiments, we analyzed the functional relationships of effectiveness and efficiency to network size.

Pseudo code in Algorithm 1 illustrates how different ex-perimental parameters were co mbined for the simulations. Experiments were conducted on a Linux cluster of 10 PC nodes, each having Dual Intel Xeon e5405 (2.0 Ghz) Quad Core Processors (8 processors), 8 GB fully buffered system memory, and a Fedora 7 installation. The computer nodes were connected internally through a dedicated 1Gb network switch. Agents were distributed among the 80 processors. The Java Runtime Environment version was 1 . 6 . 0 07. Algorithm 1 Simulation Experiments 1: for each Network Size  X  [10 2 , 10 3 , 10 4 ] do 2: for each  X   X  [0 , .., 15] do 3: rewire network with the  X  value 4: for each Search Method do 5: for each Query do 6: assign query to a random agent 7: repeat 8: forward query from one another 9: until relevant agent found OR search path L  X  10: if sufficient relevant information found then 11: send the results back 12: else 13: send failure message back 14: end if 15: end for 16: measure effectiveness P , R ,and F 1 17: measure efficiency  X   X  and  X  L 18: end for 19: end for 20: end for
We conducted experiments on networks of 10 2 ,10 3 ,and 10 4 systems. We set the max search length length L 20% of network population so that even less effective/efficient methods will be able to persist in searches. Figures 5 and 6 present results on IR effectiveness (recall, precision, and F ) while Figures 7, 8, and 9 report on efficiency (search path length and time) against different network clustering conditions (guided by clustering exponent  X  ).
As shown in Figures 5 and 6, the similarity-based search (SIM) and similarity*degree (SimDeg) method performed very well in terms of effectiveness, showing a very large ad-vantage in recall over the degree-based (DEG) and random-walk (RW) methods. When the network was under some proper clustering conditions (e.g., with  X   X  10 for network 10,000), the SIM and Sim*Deg methods achieved nearly 100% recall. Precision was 1 . 0 for all conditions because a document was retrieved only when it exactly matched the query.
The DEG search method, biased toward highly linked (popular) sites in the searches, achieved moderate perfor-mance between SIM and RW methods and had improved performance in larger networks, e.g., a roughly 0 . 7recall in the 10,000-system network. Random walk (RW) consis-tently performed below a 0 . 4 recall across all network sizes and  X  conditions.
Figures 7, 8, and 9 show very high efficiency of the SIM and SimDeg search methods across the network sizes, espe-cially under stronger clustering conditions. The efficiency gap between the SIM/SimDeg and RW/DEG methods in-creased dramatically as network size increased. For exam-ple, in the 100-node network, while SIM searched roughly 5 hops and 150 milliseconds to find exact match for each query, it took RW more than 15 hops and 400 milliseconds to reach 20% of targets (a 3-time difference in efficiency). When the network size increased to 10 , 000, RW search took 50 seconds and traversed about 1 , 500 nodes on average to reach a &lt; 0 . 4 recall whereas SIM search took less than 4 seconds and roughly 110 nodes to achieve a 1 . 0recall X  X  more than 10-time difference in efficiency.
Figures 7 -9 demonstrate that network structure had a great impact on decentralized IR performance, particularly on efficiency in larger networks. While search efficiency (in terms of search path length and search time) under different clustering conditions only differed slightly in the 100-agent network, the difference was much larger in the 10 , 000-agent network (Figure 9). For example, the average search path length for the SIM method decreased from 6 to 5 (a 20% difference) when the clustering exponent was changed from 0 (random network) to 10 (strong clustering) in the 100 net-work. In the 10 , 000-agent network, however, the same de-gree of change in network clustering led to a roughly 200% difference in search efficiency. Statistical tests indicated that SIM search achieved significantly better results with a bal-anced level of network clustering (i.e., at  X  = 10) than with over-or weak-clustering networks. The significant differ-ences not only appeared in the 10 , 000-system network but also in the 100-and 1000-system networks.
For each network size, we identified network clustering conditions under which superior performance was observed (i.e., at  X  = 10) and plotted average search path length (effi-ciency) against network size in Figure 10. As discussed ear-Figure 9: Efficiency on Network 10 , 000 . Y is log transformed. lier, SIM and SimDeg searches consistently achieved nearly 1 . 0 recall and precision across the various network sizes, much better than DEG and RW methods. DEG search tended to perform slightly better in larger networks than in smaller ones. However, as shown in Figure 10, search path length for RW and DEG dramatically increased in larger networks, while the increases for SIM and SimDeg were rel-atively moderate. Figure 10: Scalability of all search methods with  X  = 10 . X denotes network size and is log transformed.
SIM and SimDeg methods appeared to be much more scal-able than RW and DEG methods. To better understand the scalability of SIM search and to predict how it could per-form in even larger networks (e.g., a network of millions of nodes), we conducted further analysis on the relationship of its efficiency to network size.

Previous research on complex networks suggested that op-timal network clustering supports scalable searches, in which search time is a poly-logarithmic function of network size [15]. We relied on a generalized regression model that mod-eled search path length L (and search time  X  ) against log-transformed network size N . The model was specified to reach the origin (0 , 0) because, when log ( N ) = 0 (i.e., N = 1), there is only one node and no effort is needed to search a network. The best fit for search path length L was produced bythemodelinTable2,inwhich L =0 . 0275  X  log 6 10 ( N )with a nearly perfect R 2 =0 . 997 1 .
  X  0.0275 0 . 0042 65.73 &lt; 2 E  X  16 *** R 2 =0 . 997 (adj. 0 . 9968), F = 4320 on 1 and 13 DF
The same model was also applied to identify a poly-logarithmic function of search time  X  and network size N with a smaller R 2 =0 . 752. Apparently, search time involves other factors such as machine load fluctuation and is less predictable than search path length.

Overall, the scalability analysis supports search time as a poly-logarithmic function of network size  X  so that when an information network continues to grow in magnitude, it is still promising to conduct effective IR and search opera-tions within a manageable time limit. Although we found the order of the poly-logarithmic relationship to be roughly 6 in this study, a smaller exponent can be expected when other factors on network structure and search methods can be optimized.
Our search methods relied on local indexes and a struc-ture self-organized by distributed systems in the network. Without global information and centralized control, net-work clustering was performed locally  X  distributed systems formed the network structure in terms of their limited op-portunities to interact and individual preferences and con-straints on building indexes for others. This local mecha-nism for clustering demonstrated a high level of scalability. As shown in Figure 11, average clustering time  X  c remained relatively constant, &lt; 1 sec, across all network size scales N  X  [10 2 , 10 3 , 10 4 , 10 5 ].
We conducted experiments on decentralized IR operations on various scales of information networks and analyzed effec-tiveness, efficiency, and scalability of proposed search meth-ods. Results showed network structure, i.e., how distributed systems connect to one another, is crucial for retrieval per-formance. With a balanced level of network clustering un-der local topical guidance, similarity-based search functions (i.e., SIM and SimDeg) were found to perform very effi-ciently while maintaining a high level of effectiveness even in very large networks. For example, in searches for sin-gle unique documents among the 4 . 4 million documents dis-tributed among 10 , 000 agents/systems, selectively involving only 110 agents within 4 seconds yielded 100% precision and 100% recall with a guiding clustering exponent  X  = 10. Un-der these conditions, more importantly, search time was well
Each of the three X levels has multiple data points. Future work will integrate whether the relationship can be used to predict search efficiency on larger scales. explained by a poly-logarithmic function of network size, suggesting high scalability of the proposed methods.
In addition, the network clustering function that sup-ported very high effectiveness and efficiency of IR operations in large networks is itself scalable. Clustering only involved local self-organization and required no global control  X  clus-tering time remained roughly constant across the various network sizes N  X  [10 2 , 10 3 , 10 4 , 10 5 ].

This study provides guidance on how IR operations can function and scale when today X  X  information spaces continue to grow in magnitude. Particularly, we have found that con-nectivity among distributed systems, based on local network clustering, is crucial to the scalability of decentralized meth-ods. The clustering paradox on decentralized search perfor-mance appears to have a scaling effect and deserves special attention for IR operations in large scale networks. We appreciate valuable discussions with Gary Marchionini, Munindar P. Singh, Diane Kelly, Jeffrey Pomerantz, Jos  X  eR. P  X erez-Ag  X  uera, and Simon Spero, and constructive comments from SIGIR X 10 reviewers. We thank the NC Translational and Clinical Sciences (TraCS) Institute for support. [1] L. Adamic and E. Adar. How to search a social [2] R. Albert and A.-L. Barab  X  asi. Statistical mechanics of [3] R. Baeza-Yates, C. Castillo, F. Junqueira, [4] R. Baeza-Yates and B. Ribeiro-Neto. Modern [5] M. Bawa, G. S. Manku, and P. Raghavan. Sets: search [6] F. L. Bellifemine, G. Caire, and D. Greenwood. [7] M. Bender, S. Michel, P. Triantafillou, G. Weikum, [8] M. Bogu  X  n  X a, D. Krioukov, and K. C. Claffy. [9] J. Callan, F. Crestani, and M. Sanderson. SIGIR 2003 [10] A. Crespo and H. Garcia-Molina. Semantic overlay [11] C. Doulkeridis, K. Norvag, and M. Vazirgiannis. [12] M. S. Granovetter. The strength of weak ties. [13] E. Hatcher, O. Gospodneti  X  c, , and M. McCandless. [14] W. Ke and J. Mostafa. Strong ties vs. weak ties: [15] J. M. Kleinberg. Navigation in a small world. Nature , [16] J. Lu and J. Callan. User modeling for full-text [17] E. K. Lua, J. Crowcroft, M. Pias, R. Sharma, and [18] T. Luu, F. Klemm, I. Podnar, M. Rajman, and [19] A. L. Powell and J. C. French. Comparing the [20] O. Simsek and D. Jensen. Navigating networks by [21] G. Skobeltsyn, T. Luu, I. P. Zarko, M. Rajman, and [22] D. J. Watts, P. S. Dodds, and M. E. J. Newman. [23] I. P. Zarko and F. Silvestri. The CIKM 2006 workshop
