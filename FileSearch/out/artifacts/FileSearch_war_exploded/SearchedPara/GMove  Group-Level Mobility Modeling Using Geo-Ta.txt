 Understanding human mobility is of great importance to various applications, such as urban planning, traffic scheduling, and loca-tion prediction. While there has been fruitful research on modeling human mobility using tracking data ( e.g. , GPS traces), the recent growth of geo-tagged social media (GeoSM) brings new opportu-nities to this task because of its sheer size and multi-dimensional nature. Nevertheless, how to obtain quality mobility models from the highly sparse and complex GeoSM data remains a challenge that cannot be readily addressed by existing techniques.
We propose GM OVE , a group-level mobility modeling method using GeoSM data. Our insight is that the GeoSM data usually con-tains multiple user groups, where the users within the same group share significant movement regularity. Meanwhile, user grouping and mobility modeling are two intertwined tasks: (1) better user grouping offers better within-group data consistency and thus leads to more reliable mobility models; and (2) better mobility models serve as useful guidance that helps infer the group a user belongs to. GM OVE thus alternates between user grouping and mobility modeling, and generates an ensemble of Hidden Markov Models (HMMs) to characterize group-level movement regularity. Further-more, to reduce text sparsity of GeoSM data, GM OVE also features a text augmenter. The augmenter computes keyword correlations by examining their spatiotemporal distributions. With such correla-tions as auxiliary knowledge, it performs sampling-based augmen-tation to alleviate text sparsity and produce high-quality HMMs.
Our extensive experiments on two real-life data sets demonstrate that GM OVE can effectively generate meaningful group-level mo-bility models. Moreover, with context-aware location prediction as an example application, we find that GM OVE significantly outper-forms baseline mobility models in terms of prediction accuracy.
Understanding human mobility has been widely recognized as a corner-stone task for various applications, ranging from urban plan-ning and traffic scheduling to location prediction and personalized activity recommendation. In the past few decades, the importance of this task has led to fruitful research efforts in the data mining community [7, 27, 13, 4, 26]. While most of the previous works use tracking data ( e.g. , GPS traces) to discover human movement regularity, the recent explosive growth of geo-tagged social me-dia (GeoSM) brings new opportunities to this task. Nowadays, as GPS-enabled mobile devices and location-sharing apps are pene-trating into our daily life, every single day is witnessing millions of geo-tagged tweets, Facebook checkins, etc. Compared with con-ventional tracking data, GeoSM possesses two unique advantages for mobility modeling: (1) First, in addition to spatial and temporal information, each GeoSM record also includes text. Hence, a user X  X  GeoSM history reveals not only how she moves from one location to another, but also what activities she does at different locations. (2) Second, the GeoSM data has a much larger size and coverage. Due to privacy concerns, gathering tracking data typically requires a huge amount of time and money to engage sufficient volunteers. In contrast, the GeoSM data naturally covers hundreds of millions of users and has a much larger volume.

Considering its multi-dimensional nature and sheer size, it is clear that GeoSM serves as an unprecedentedly valuable source for human mobility modeling. Hence, we study the problem of us-ing large-scale GeoSM data to unveil human movement regularity. Specifically, we answer the following two questions:
Unveiling human mobility using GeoSM data can greatly benefit various applications. Consider traffic jam prevention as an exam-ple. While existing techniques [7, 26, 25] can detect sequential patterns to reflect the populace flow between geographical regions, the GeoSM data allows us understand populace flow beyond that. Suppose severe traffic jams occur frequently in a region R . With the mobility model learnt from massive GeoSM data, we can un-derstand what are people X  X  typical activities in R , which regions do people come from, and why do people take those trips. Such understandings can guide the government to better diagnose the root causes of traffic jams and take prevention actions accordingly. Another example application is context-aware location prediction. Previous studies [15, 14, 4] typically use the spatiotemporal regu-larity of human mobility to predict the next location a person may visit. By leveraging GeoSM data, we can incorporate activity-level transitions to infer what activities a user will engage subsequently. As such, the mobility models learnt from GeoSM can potentially improve context-aware location prediction remarkably.

Despite its practical importance, the task of modeling human mobility with GeoSM data is nontrivial due to several challenges: (1) Integrating diverse types of data. The GeoSM data involves three different data types: location, time, and text. Considering the totally different representations of those data types and the compli-cated correlations among them, how to effectively integrate them for mobility modeling is difficult. (2) Constructing reliable mod-els from sparse and complex data. Unlike intentionally collected tracking data, the GeoSM data is low-sampling in nature, because a user is unlikely to report her activity at every visited location. Such data sparsity makes it unrealistic to train a mobility model for each individual. On the other hand, one may propose to aggregate the data of all users to train one single model, but the obtained model could then suffer from severe data inconsistency because different users can have totally different moving behaviors. (3) Extracting interpretable semantics from short GeoSM messages. The GeoSM messages are usually short. For example, any geo-tagged tweets contain no more than 140 characters, and most geo-tagged Insta-gram photos are associated with quite short text messages. It is nontrivial to extract reliable knowledge from short GeoSM mes-sages and build high-quality mobility models.

We propose GM OVE , an effective method that models human mobility from massive GeoSM data. GM OVE relies on the Hidden Markov Model (HMM) to learn the multi-view latent states and people X  X  transitions between them. To obtain quality HMMs from the highly sparse and complex GeoSM data, we propose a novel idea of group-level mobility modeling. The key is to group the users that share similar moving behaviors, e.g. , the students study-ing at the same university. By aggregating the movements of like-behaved users, GM OVE can largely alleviate data sparsity without compromising the within-group data consistency.

To achieve effective user grouping and mobility modeling, we find that these two sub-tasks can mutually enhance each other: (1) better user grouping offers better within-group data consistency, which helps produce more reliable mobility models; and (2) bet-ter mobility models can serve as useful knowledge, which helps infer the group a user belongs to. Based on this observation, we de-velop an iterative framework, in which we alternate between user grouping and group-level mobility modeling. We theoretically and empirically show that, such a process leads to better user grouping and mobility models after each iteration, and finally generates an ensemble of high-quality HMMs.

Another important component of GM OVE is a text augmenter, which leverages keyword spatiotemporal correlations to reduce text sparsity. While each raw GeoSM message is short and noisy, we find that using millions of GeoSM records, we are able to extract the intrinsic correlations between keywords by examining their spa-tiotemporal distributions. Consider two users who are having din-ner at the same Italian restaurant and posting tweets with two differ-ent keywords:  X  X asta X  and  X  X izza X . Although these two keywords do not co-occur in the same tweet, they are spatially and temporally close. We hence quantify the spatiotemporal correlations between keywords, and use such correlations as auxiliary knowledge to aug-ment raw GeoSM messages. We show that such augmentation can largely reduce text sparsity and lead to highly interpretable states.
To summarize, we make the following contributions: 1. We formulate the problem of mobility modeling using mas-2. We propose a novel group-level mobility modeling frame-3. We design a strategy that leverages the spatiotemporal distri-4. We perform extensive experiments using two real data sets
In this section, we formulate the problem of mobility modeling using geo-tagged social media, and explore several characteristics of this problem to motivate the design of GM OVE .
A GeoSM record x is defined as a tuple  X  u x ,t x ,l x ,e (1) u x is the user id; (2) t x is the creating timestamp (in second); (3) l x is a two-dimensional vector representing the user X  X  location when x is created; and (4) e x , which is a bag of keywords from a vocabulary V , denotes the text message of x .

Let us consider a set of users U = { u 1 ,u 2 ,...,u M } . For a user u  X  U , her GeoSM history is a sequence of chronologically ordered GeoSM records S u = x 1 x 2 ...x n . To understand u  X  X  mo-bility, one may propose to use the entire sequence S u . Neverthe-less, the sequence S u is low-sampling in nature, and the temporal gaps between some pairs of consecutive records can be very large, e.g. , several days. To generate reliable mobility models, we only use the dense parts in S u , which we define as trajectories .
D EFINITION 1 (T RAJECTORY ). Given a GeoSM sequence S = x x 2 ...x n and a time gap threshold  X  t &gt; 0 , a subsequence S 0 = x i x i +1 ...x i + k is a length-k trajectory of S if S (1)  X  1 &lt; j  X  k , t x j  X  t x j  X  1  X   X  t ; and (2) there are no longer subsequences in S that contains S 0 and meanwhile also satisfies condition (1).
E XAMPLE 1. In Figure 1, given a user X  X  GeoSM history S = x x 2 x 3 x 4 x 5 and  X  t = 2 hours, there are two trajectories: x and x 4 x 5 . These trajectories correspond to the user X  X  two moving behaviors: (1) having dinner at an Italian restaurant after work; and (2) going to bar after taking exercises.
A trajectory essentially leverages the time constraint to extract reliable moving behaviors in the GeoSM history. With the extracted trajectories, our general goal is to use them to understand the mo-bility of the users in U . Nevertheless, the number of trajectories of one single user is usually too limited to train a mobility model, while aggregating the trajectories of all the users in U could suffer from severe data inconsistency and result in an ambiguous model.
To address the dilemma, we propose to study group-level user mobility . Our key observation is that, aggregating the trajectories of like-behaved users can largely alleviate data sparsity and incon-sistency, and thus unveil the movement regularity for that group. Consider a group of students studying at the same university, they may share similar hotspots like classrooms, gyms, and restaurants. Many students also have similar moving behaviors, e.g. , moving from the same residence hall to classrooms, and having lunch to-gether after classes. As another example, a group of tourists in New York may all visit locations like the JFK Airport, the Metropolitan Museum, the 5th Avenue, etc. . While the data of one individual is limited, the collective movement data from a group of like-behaved people can be informative about their latent hotspots and the move-ment regularity. It is important to note that, one user can belong to several different groups, e.g. , a student may also go sightseeing in the city on weekends. Therefore, the users should be grouped softly instead of rigidly.

Based on the above observation, we formulate the problem of group-level mobility modeling as follows. Given the set U of users and their trajectories extracted from historical GeoSM records, the task of group-level mobility modeling consists of two sub-tasks: (1) user grouping : softly group the users in U such that the members in the same group have similar moving behaviors; and (2) mobility modeling : for each user group, discover the latent states that reflect the users X  activities from a multidimensional (where-what-when) view, and find out how the users move between those latent states.
GM OVE is built upon the Hidden Markov Model (HMM), an effective statistical model for sequential data. Given an observed sequence S = x 1 x 2 ...x n , HMM defines K latent states Z = { 1 , 2 ,...,K } that are not observable. Each record x i n ) corresponds to a latent state z i  X  Z , which has a probabilistic distribution that governs the generation of x i . In addition, the latent state sequence z 1 z 2 ...z n follows the Markov process, namely, the state z i only depends on the previous state z i  X  1 .

As discussed above, there exists a dilemma when applying HMM to understand the mobility of the users in U : training an HMM for each user is unrealistic due to data sparsity, while training one sin-gle HMM for all the users leads to an ambiguous model due to data inconsistency. Therefore, GM OVE performs group-level mobility modeling by coupling the subtasks of user grouping and mobility modeling. It aims at dividing the users in U into like-behaved user groups and training an HMM for each group. By aggregating the movement data within the same group, we can alleviate data spar-sity without compromising data consistency.

Now the question is, how can we group the users such that the users within the same group have similar moving behaviors? Our idea is that the sub-tasks of user grouping and mobility modeling can mutually enhance each other: (1) better user grouping leads to more consistent movement data within each group, which can im-prove the quality of the HMM; and (2) high-quality HMMs more accurately describe the true latent states and the transitions be-tween them, which helps infer which group a user should belong to. Hence, GM OVE employs an iterative framework that alternates between user grouping and HMM training. Later we will see, as the iteration continues, the user grouping gets more and more ac-curate, and the mobility models get more and more reliable, and such a process will finally converge to a stable solution. Another important component of GM OVE is a text augmenter. When training an HMM for each group, we model each latent state to generate multidimensional (spatial, temporal, and textual) ob-servations of the user X  X  activity. While the spatial and temporal ob-servations are clean, the major challenge in obtaining high-quality latent states is that the text messages are mostly short and noisy. To address this problem, we observe that keywords X  spatiotem-poral distributions can reveal their intrinsic correlations. The text augmenter thus quantifies the spatiotemporal correlations between keywords to obtain auxiliray knowledge. With such knowledge, it performs a weighted sampling process to augment raw text mes-sages. Such augmentation can largely overcome text sparsity and suppress noise to help produce high-quality HMMs.

Figure 2 summarizes the framework of GM OVE . Given the in-put trajectories, GM OVE consists of two major modules to pro-duce group-level mobility models. The first module is the text aug-menter, which examines the spatiotemporal correlations between keywords to augment raw messages. With the augmented trajecto-ries, the second module obtains an ensemble of HMMs by alternat-ing between user grouping and HMM training. In the following, we elaborate these two module in Section 3 and 4, respectively.
In this section, we describe GM OVE  X  X  text augmenter, which first computes keyword correlations and then performs sampling-based text augmentation.
 Keyword correlation computation. Our computation of keyword correlation relies on spatiotemporal discretization. Let us consider a spatiotemporal space D that consists of three dimensions: (1) the longitude dimension x ; (2) the latitude dimension y ; and (3) the time dimension t . We partition D into N x  X  N y  X  N t equal-size grids, where N x , N y and N t are pre-specified integers that control discretization granularity. Based on such discretization, we define the concepts of grid density and signature for each keyword.
D EFINITION 2 (G RID D ENSITY ). Given a keyword w , the den-sity of w in grid  X  n x ,n y ,n t  X  (1  X  n x  X  N x , 1  X  n n  X  N t ) is defined as w  X  X  frequency in that grid, namely where c w ( n x ,n y ,n t ) is the number of GeoSM records that contain w and meanwhile fall in grid  X  n x ,n y ,n t  X  .

D EFINITION 3 (S IGNATURE ). Given a keyword w , its signa-ture s w is a N x N y N t -dimensional vector, where d w ( n the value for the (( n t  X  1) N x N y +( n y  X  1) N x + n
The signature of a keyword encodes how frequently that key-word appears in different spatiotemporal grids. Intuitively, if two keywords are semantically correlated ( e.g. ,  X  X asta X  and  X  X izza X ), they are more likely to frequently co-occur in the same grid and thus have similar signatures. Below, we measure the spatiotem-poral correlation between two keywords by simply computing the similarity of their signatures.
 words w i and w j , let s w i and s w j be their signatures. The spa-tiotemporal correlation between w i and w j , denoted as corr ( w is the cosine distance between s w i and s w j .
 Sampling-based augmentation. Definition 4 quantifies keyword correlations based on their spatiotemporal distributions. With Def-inition 4, we further select out a set of vicinity keywords from the vocabulary for each keyword.

D EFINITION 5 (V ICINITY ). For any keyword w  X  V , its vicin-ity N w is N w = { v | v  X  V  X  corr ( w,v )  X   X  } where  X   X  0 is a pre-specified correlation threshold.

The vicinity concept is important as it identifies other keywords that are highly correlated to the target keyword and suppresses noisy keywords. Based on Definition 5, we are now ready to de-scribe the text augmentation process. Given an input GeoSM record x , our goal is to augment the original text message e x by incorpo-rating many other keywords that are semantically relevant to e shown in Algorithm 1, the augmentation simply performs sampling by using keyword correlation as auxiliary knowledge. Specifically, we first sample a keyword w from the original message based on normalized TF-IDF weights, and then sample a relevant keyword v from w  X  X  vicinity. The probability of a keyword v being sampled is proportional to its spatiotemporal correlation with w , namely corr ( w,v ) . We repeat the sampling process until the augmented message reaches a pre-specified length L .
 Algorithm 1: Text augmentation.
 Input : A GeoSM record x , the target length L .

Output : The augmented text message of x .
A x  X  The original text message e x ; while len ( A x ) &lt; L do 4 Sample a word v  X  X  w with probability corr ( w,v ) P 5 Add v into A x ; return A x ;
In this section, we describe GM OVE  X  X  second module that learns an ensemble of HMMs. Below, we first present an iterative refine-ment framework in Section 4.1, Then we introduce the procedures for the HMM training and user grouping in Section 4.2 and 4.3, respectively. Finally, we prove the convergence of GM analyze its time complexity in Section 4.4.
To generate high-quality group-level HMMs, GM OVE employs an iterative refinement framework that performs the following steps: 1. Initialization: Let U = { u 1 ,u 2 ,...,u M } be the user set, 2. HMM Training:  X  g  X  G , use the membership vectors to 3. User Grouping:  X  u  X  U , use H new to update u  X  X  member-4. Iterating: Check for convergence using the log-likelihood
In the HMM training step, the task is to use weighted trajectories to train a new HMM for each group. Below, we first define the HMM for generating the trajectories, and then discuss how to infer model parameters.
Let us consider a trajectory S = x 1 x 2 ...x N . Each observation x (1  X  n  X  N ) is multidimensional: (1) l n is a two-dimensional vector denoting latitude and longitude; (2) t n is a timestamp; and (3) e n is a bag of keywords denoting the augmented message.
To generate the sequence x 1 x 2 ...x N , we assume there are K latent states Z = { 1 , 2 ,...,K } . As shown in Figure 3, each obser-vation x n corresponds to a latent state z n  X  Z , and the sequence of the latent states z 1 z n ...z N follows a Markov process parameter-ized by: (1) a K -dimensional vector  X  that defines the initial distri-bution over the K latent states, i.e. ,  X  k = p ( z 1 = k ) (1  X  k  X  K ) ; and (2) a K  X  K matrix A that defines the transition probabilities among the K latent states. Suppose the ( n  X  1) -th ( n &gt; 1 ) latent sate is z n  X  1 = j , then the n -th state z n depends only on z More formally, if the state z n  X  1 is j , the probability for the state z to be k is given by A jk , i.e. , p ( z n = k | z n  X  1 = j ) = A
Meanwhile, a state z n generates x n by generating the location l , the timestamp t n , and the keywords e n independently, i.e. , p ( x n | z n = k ) = p ( l n | z n = k )  X  p ( t n | z n For each observation x n , we assume the following spatial, tem-poral, and textual distributions: (1) The location l n is generated from a bi-variate Gaussian, i.e. , p ( l n | z n = k ) = N ( l where  X  lk and  X  lk are the mean and variance matrix for state k . (2) The timestamp t n is generated from a uni-variate Gaussian, i.e. , p ( t n | z n = k ) = N ( t n mod 86400 |  X  tk , X  tk ) , where  X  are the mean and variance for state k . Note that we convert the raw absolute timestamp t n (in second) into the relative timestamp in one day. (3) The keywords e n are generated from a multinomial distribution, i.e. , p ( e n | z n = k )  X  ability of choosing word v for state k , and e v n is the number of v  X  X  occurrences in e n .
When training the HMM for group g , let us use { S 1 ,S 2 to denote the input trajectories, and each trajectory S r is associated with a weight w r , denoting the probability that the user of S r belongs to group g . The group-level HMM is parame-terized by  X  = {  X , A , X  l ,  X  l , X  t , X  t , X  } . We use the Expectation-Maximization (EM) algorithm to estimate these parameters. It is important to note that, when applying EM to train the HMM for group g , we use the previous HMM H g to initialize model param-eters. Later we will see, such an initialization ensures the iterative framework of GM OVE obtains better HMMs after each iteration and finally converge.

Starting with the initial parameters, the EM algorithm alternates between the E-step and the M-step round by round 1 to generate a new HMM. In the ( t + 1) -th round E-step, it utilizes the estimated parameters at the t -th round  X  ( t ) , and computes the expectation of the complete likelihood Q ( X ) . In the M-step, it finds a new esti-mation  X  ( t +1) that maximizes the Q-function. Below, we present the details of the E-step and the M-step.
 E-Step. In the E-step, the key is to compute the distribution of the latent states based on the old parameter set  X  ( t ) . Given a trajectory S r = x r, 1 x r, 2 ...x r,N ,  X  1  X  n  X  N , we first use the Baum-Welch algorithm to compute two distributions:
Here,  X  ( z r,n ) can be computed in a forward fashion, and  X  ( z can be computed in a backward fashion:  X  ( z r,n ) = X where the initial distributions  X  ( z r, 1 ) and  X  ( z r,N
Based on  X  ( z r,n ) and  X  ( z r,n ) , we are now able to compute the following distributions for the latent states: (1)  X  ( z tribution of the n -th latent state for the r -th trajectory; and (2) in the r -th trajectory. These two distributions are given by where p ( S r ) = P z M-Step. In the M-step, we derive the best estimation for the pa-rameter set  X  ( t +1) = {  X , A , X  g ,  X  g , X  t , X  t , X  } . Based on  X  ( z and  X  ( z r,n  X  1 ,z r,n ) , let us define
We update the parameters in  X  ( t +1) as follows (please see Ap-pendix for the detailed derivation):
We intentionally use two different terms, namely iteration and round , to distinguish the two different iterative processes: (1) GM OVE  X  X  iterative refinement framework; and (2) the EM algo-rithm for HMM training.  X  A  X   X   X   X   X 
Once the new ensemble of HMMs, H new , are obtained, we use them to softly assign each user into the G groups. For each user u , we update the membership vector M u such that the g -th dimension is the posterior probability that u belongs to group g . Let us denote the set of u  X  X  trajectories as J u , in which the j -th ( 1  X  j  X  | J trajectory is S j u . Based on the new HMM ensemble H new puted using the forward scoring algorithm of HMM. Accordingly, the probability of observing J u is given by:
Using the Bayes X  theorem, we further derive the posterior prob-ability that user u belongs to group g as follows: where p ( u | g ; H new ) is given by Equation 8, and p ( g ) is estimated from the membership vectors: p ( g ) = P u  X  U M u ( g ) / | U | . Fi-nally, we obtain the new membership vector M new u as M new p ( g | u ; H new ) . Now we proceed to prove the convergence of GM OVE .

T HEOREM 1. The iterative refinement framework of GM OVE is guaranteed to converge.

P ROOF . With Jensen X  X  inequality, the log-likelihood of the input trajectory data satisfies:
Recall that, after each iteration, GM OVE sets M u ( g ) to the pos-terior probability p ( g | u ; H ) . The quantity p ( u,g ; H ) /M thus a constant, and hence the equality in Equation 11 is guaran-teed to hold. Such a property allows us to prove the log-likelihood is non-decreasing after each iteration. More formally, we have
In the above, the key step is Equation 13. It holds because in its iterative refinement framework, GM OVE uses the parameters of the previous HMM ensemble to initialize the current HMM ensemble, which guarantees p ( u,g ; H ) to be non-decreasing after the HMM refinement. As the total likelihood is non-decreasing after every iteration, we have proved the convergence of our algorithm.
There is a tight connection between our iterative refinement frame-work and the traditional EM algorithm. The only difference be-tween the two is that, in the EM algorithm, the constructed lower bound is typically convex and thus can be easily optimized to global optimum; while in our case, the constructed lower bound is still non-convex and we need to use yet another EM algorithm (HMM refinement) to optimize this lower bound. As the lower bound is non-convex, we have to make sure the HMM training of every iter-ation initialize with the parameters learnt from the previous itera-tion, otherwise we may risk running into different local optimums and thus break the convergence guarantee of our algorithm. In this section, we evaluate the empirical performance of GM All the algorithms were implemented in JAVA and the experiments were conducted on a computer with Intel Core i7 2.4Ghz CPU and 8GB memory.
Our experiments are based on two real-life data sets, both of which are crawled using Twitter Streaming API during 2014.10.10  X  2014.10.30. The first data set, referred to as LA, consists of 0.6 million geo-tagged tweets in Los Angeles. After grouping the tweets by user id and extracting trajectories with a time constraint of three hours (Definition 1), we obtain around 30 thousand tra-jectories. The second data set, referred to as NY, consists of 0.7 million geo-tagged tweets in New York, and there are 42 thousand trajectories after extracting with the three-hour time constraint. The reason we choose these two cities is because we want to verify the performance of GM OVE on the data sets that have quite different population distributions  X  the populace of Los Angeles is spread out in many different districts, while the populace of New York is relatively concentrated in Manhattan and Brooklyn.
In this subsection, we run GM OVE on LA and NY and use sev-eral illustrative cases to verify its effectiveness.
In the first set of experiments, we demonstrate the effectiveness of the text augmentation module of GM OVE . To this end, we ran-domly choose several tweets in both LA and NY, and use GM to augment the raw tweet messages. Table 1 shows the augmen-tation results for four example tweets (two from LA and two from NY). As a preprocessing step, we performed stemming and stop-words removal for the raw tweets, and also discretize the spatiotem-poral space by partitioning each dimension into ten bins. When augmenting each tweet, the correlation threshold is set to  X  = 0 . 2 and the augmentation size is set to L = 100 .
 Table 1: Text augmentation examples on LA and NY. The number be-
Examining Table 1, we find that GM OVE  X  X  text augmenter is quite effective: given the short raw tweets, GM OVE generates se-mantically comprehensive messages by including relevant keywords that are not mentioned in the raw tweets. Consider the first tweet as an example. A fan of the Lakers (a basketball team) was post-ing to support his team. The original tweet message contains only three meaningful keywords:  X  X obe X ,  X  X akers X , and  X  X an X . By com-puting the spatiotemporal correlations between different keywords, GM OVE successfully identifies several other keywords ( e.g. ,  X  X ame X ,  X  X taples X ,  X  X enter X ) that are relevant to the three seed keywords. The augmented tweet becomes much more semantically compre-hensive, making it possible to connect with other Lakers-related tweets to form high-quality latent states. Similar phenomena are also observed on the other three tweets.
In this set of experiments, we exemplify the group-level mobil-ity models generated by GM OVE on LA and NY. We are interested in the following questions: (1) can GM OVE indeed discover high-quality latent states and meaningful transitions among them? and (2) do different group-level mobility models reflect different mobil-ity patterns? To answer the above questions, we ran GM OVE and NY with the following parameter settings:  X  = 0 . 2 , L = 100 , G = 80 , and K = 10 . While GM OVE discovers 80 group-level mobility models on both LA and NY, we choose two representative mobility models learnt on LA due to space limit. In Figure 4(a) and 4(b), we visualize those two example mobility models.

Based on the keywords and locations of the latent states, we can say the mobility model in Figure 4(a) likely corresponds to the tra-jectories of a group of UCLA students. This is because most la-tent states are centered around the UCLA campus, and meanwhile carry college-life semantics ( e.g. , home, school, gym, friends). We have the following interesting findings from the mobility model: (1) First, the latent states are highly interpretable. For instance, the top keywords of state 1 are  X  X chool X ,  X  X cla X ,  X  X ourse X , etc. . It clearly reflects the students X  on-campus activities, such as attend-ing classes, taking exams, and staying with friends. (2) Second, the transitions between latent states are practically meaningful. Still consider state 1 as an example. As shown by the transition matrix, after taking classes at campus, the top subsequent activities are go-5 9 ing back home (state 0 &amp; 8), having dinner (state 2 &amp; 6), and doing sports (state 7). Those transitions are all highly consistent with a student X  X  lifestyle in real life, and thus reflects the high quality of the underlying group-level HMM.

The mobility model shown in Figure 4(b) probably corresponds states ( e.g. , airport, hotel, Hollywood, beach) are highly interpretable and the transitions ( e.g. , going to hotel from airport, having food after sightseeing) are intuitive. More importantly, when compar-ing the mobility models in Figure 4(a) and 4(b), one can clearly observe that these two models reflect totally different hotspots and moving behaviors. This phenomenon suggests that GM OVE indeed effectively partition the users based on their behaviors, and generate highly descriptive HMMs for different groups.
While GM OVE can be used for various real-life applications, we choose context-aware location prediction as an example to quanti-tatively evaluate the performance of GM OVE . Task description. Context-aware location prediction aims at pre-dicting the next location of a user, given the current observation of the user X  X  movement. Formally, given a ground-truth trajectory x x 2 ...x N , we assume the sequence x 1 x 2 ...x N  X  1 known, and try to recover x N from a pool of candidate locations. The candidate pool, denoted as C x is formed as follows: from all the geo-tagged tweets, we select the ones whose geographical dis-tance to x N is no larger than three kilometers and the temporal dif-ference to x N is no larger than five hours. Once the candidate pool is obtained, we use the given mobility model to rank all the can-didates in the descending order of visiting probability, and check whether the grouth-truth record x N appears in the top-k results ( k is an integer that controls the result list size).

Intuitively, the better a mobility model is, the more likely it can recover the ground-truth record x N ( i.e. , x N should be ranked higher in the result list). Hence, we use the prediction accuracy to mea-sure the performance of the given mobility model. Given T test trajectories, for each test trajectory, we use a mobility model to re-trieve the top-k results and see whether the ground truth record x appears in the result list. Let T 0 denote the number of trajectories for which the ground-truth records are successfully recovered, then the prediction accuracy is Acc @ k = T 0 /T . On both LA and NY, we randomly choose 30% trajectories as test data, and use the rest 70% data for model training.
 Compared Methods. To compare with GM OVE , we implemented the following mobility models for context-aware location predic-tion: (1) L AW [2] is a prevailing mobility model. Based on the dis-placement distribution, it models human mobility as a L X vy flight with long-tailed distributions. (2) G EO is based on existing works [14, 4] that train one HMM from the input trajectories. It uses only spatial information by assuming each latent state generates the location from a bi-variate Gaussian. (3) S INGLE is an adaption of GM OVE , which does not perform user grouping, but trains one HMM using all the trajectories. (4) N O A UG is another adaption of GM OVE , which does not perform text augmentation.

When computing the visiting probability for any candidate lo-cation, both G EO and S INGLE simply use the obtained HMM and the forward scoring algorithm to derive the likelihood of the se-quence. For GM OVE and N O A UG , they both produce an ensem-ble of HMMs, and each HMM provides a visiting probability for the candidate location. Hence, GM OVE and N O A UG need to first compute the membership vector for the target user based on her historical trajectories, and then use membership values as weights to derive a weighted score.

Note that, while there are other methods [15, 16, 6, 21] optimized for location prediction ( e.g. , by using a supervised learning frame-work), we have not included them for comparison. This is because GM OVE is designed for mobility modeling, and we use context-aware location prediction as a quantitatively evaluation for the qual-ity of the obtained mobility models. We do not claim GM OVE outperform state-of-the-art location prediction techniques. Parameter Settings. GM OVE has four major parameters: (1) the correlation threshold  X  ; (2) the augmentation size L ; (3) the number of user groups G ; and (4) the number of latent states K . We study the effects of different parameters as follows: (1)  X  = 0, 0.1, 0.2 , 0.3, 0.4, 0.5; (2) L = 0, 20, 40 , 60, 80, 100; (3) G = 5, 10, 20, 30, 40, 50, 60, 70, 80 , 90, 100; (4) K = 3, 4, 5, 10 , 15, 20, 25, 30. When studying the effect of one parameter, we fix the other parameters to their default values, as denoted by the bold numbers.
Figure 5 reports the accuracies of different mobility models for top-k prediction on LA and NY. The parameters of GM OVE are set to the default values as such a setting offers the highest prediction accuracy. The parameters of all the other compared models are also carefully tuned to achieve the best performance.
As shown in Figure 5(a) and 5(b), GM OVE significantly outper-forms all the baseline methods on both LA and NY for different k values. Comparing the performance of GM OVE and S INGLE we find that the prediction accuracy of GM OVE is about 12.7% better on average. This suggests that there indeed exist multiple groups of people that have different moving behaviors. As S GLE trains one model for all the input trajectories, it suffers from severe data inconsistency and mixes different mobility regulari-ties together. In contrast, GM OVE employs the iterative refine-ment framework to perform group-level mobility modeling. Such a framework effectively distinguishes different mobility regularities and thus achieves much better accuracy. Meanwhile, by examin-ing the performance of GM OVE and N O A UG , we find that the text augmenter of GM OVE is also effective and improves the prediction accuracy by about 3%. As aforementioned, while each raw tweet message is very short, the augmented message becomes much more semantically comprehensive. The augmentation process allows us to connect the tweets that are intrinsically relevant and thus gener-ate higher-quality HMMs. Another interesting finding is that the performance of S INGLE is consistently better than G EO and L Such a phenomenon verifies the fact that integrating multiple (spa-tial, temporal, and textual) signals can better describe the users X  moving behaviors than using only spatial information, and thus achieve better location prediction accuracy. As mentioned earlier, there are four major parameters in GM  X  , L , K , and G . After tuning those parameters on LA and NY, we find that the trends for all the four parameters are very similar on LA and NY. We only report the results on LA to save space.
We first study the effects of  X  and L . Figure 6(a) shows the prediction accuracy of GM OVE when  X  varies from 0 to 0.5. As shown, the performance of GM OVE first increases with  X  and then decreases. This phenomenon is expected: a too small correlation threshold tends to include irrelevant keywords in the augmentation process; while a too large threshold constrains a keyword X  X  vicinity to include almost only itself, making the augmentation ineffective. Figure 6(b) shows the performance of GM OVE when the augmen-tation size L varies. We find that the prediction accuracy first in-creases with L and quickly becomes stable. This suggest that the augmentation size should not be set to a too small value in practice.
We proceed to study the effects of G and K . Figure 6(c) reports results when G increase from 5 to 100. Not hard to observe, the pre-diction accuracy increases significantly with G before it gradually becomes stable. This is expected. In big cities like Los Angeles and New York, it is natural that there are a large number of user groups that have totally different lifestyles. When G increases, the GM OVE model fits better with the intrinsic data complexity, and thus improves the prediction accuracy. Figure 6(d) shows the effect of K on the performance of GM OVE . Interestingly, we observe the performance of GM OVE is not sensitive to the number of latent states as long as K  X  5 . This suggests that when the user groups are fine-grained enough ( e.g. , G = 80 ), the number of  X  X otspots X  for each group is usually limited in practice. In the final set of experiments, we report the training time of GM OVE . When tuning the four parameters of GM OVE , we find that the training time of GM OVE does not change much with  X  and L . Therefore, we only report the effects of G and K . Figure 7(a) and 8(a) show that the time cost of GM OVE generally increases with G , but scales well. This is a good property of GM situations where G needs to be set to a large number. In Figure 7(b) and 8(b), we can see that the time cost increases superlinearly with K . This is mainly because of the intrinsic nature for HMM training, where time complexity of the EM algorithm is quadratic in K .
Generally, existing approaches on mobility modeling can be clas-sified into three classes: pattern-based , law-based , and model-based . Pattern-based approaches mine regular mobility patterns from movement data. Existing studies have designed effective meth-ods for mining different types of movement patterns. Specifically, Giannotti et al. [7] define a T-pattern as Region-of-Interest (RoI) sequence that frequently appears in the input trajectories. By par-titioning the space, they use frequent sequential pattern mining to extract all the T-patterns. Several studies have attempted to find a set of objects that are frequently co-located. Efforts along this line include mining flock [11], swarm [12], and gathering [26] patterns. Meanwhile, there has also been study on mining periodic move-ment patterns. For instance, Li et al. [13] first use density-based clustering to extract reference spots from GPS trajectories, and then detect periodic visiting behaviors at those spots.

While the above studies focus on pure GPS trajectory data, a number of studies mine mobility patterns from semantic trajecto-jectories to semantic places using a background map, and then ex-tract frequent place sequences as sequential patterns. Zhang et al. [25] introduce a top-down approach to mine fine-grained move-ment patterns from semantic trajectories. Our work differs from these studies in two aspects. First, we consider unstructured text instead of structured category information. Second, instead of ex-tracting a large number of patterns, we develop a statistical model that comprehensively and concisely summarizes people X  X  moving behaviors.
 Law-based approaches investigate the physical laws that govern human X  X  moving behaviors. Brockmann et al. [2] find that human mobility can be approximated by a continuous-time random-walk model with long-tail distributions. Gonzalez et al. [8] study human mobility using mobile phone data. They find that people periodi-cally return to a few previously visited locations, and the mobility can be modeled by a stochastic process centered at a fixed point. Song et al. [20] report that around 93% human movements are pre-dictable due to the high regularity of people X  X  moving behaviors. They [19] further propose a self-consistent microscopic model to predict individual mobility.
 Model-based approaches attempt to learn statistical models from movement data. Cho et al. [3] observe that a user usually moves around several centers ( e.g. , home and work) at fixed time slots, and thus propose to model a user X  X  movement as a mixture of Gaus-sians. Their model can further incorporate social influence based on the fact that a user is more likely to visit a location that is close to the locations of her friends. Wang et al. [21] propose a hybrid mobility model that uses heterogeneous mobility data for better lo-cation prediction. There are some studies [18, 23, 9, 10, 24] on geographical topic discovery, aiming to discover people X  X  activities in different geographical regions and/or time periods. While our work also uses statistical models to unveil human mobility, it dif-fers from the above studies in that it focuses on extracting the latent states of human movements as well as the sequential transitions .
The most relevant works to our study are those HMM-based ap-proaches [4, 22]. Mathew et al. [14] discretize the space into equal-size triangles using Hierarchical Triangular Mesh. By assuming each latent state has a multinomial distribution over the triangles, they train an HMM on the input GPS trajectories. Deb et al. [4] propose the probabilistic latent semantic model, which essentially uses HMM to extract latent semantic locations from cell tower and bluetooth data. Ye et al. [22] use HMM to model user check-ins on location-based social networks. By incorporating the category information of places, their obtained HMM is capable of predicting the category of the next location for the user.

Although our proposed GM OVE method also relies on HMM, it differs from the above works in two aspects. First, none of the above studies consider text data for mobility modeling. In contrast, we simultaneously model spatial, temporal, and textual information to obtain interpretable latent states. Second, they perform HMM training with the movement data of either one individual or a given collection of users. GM OVE , however, couples the subtasks of user grouping and mobility modeling, and generates high-quality user groups as well as group-level HMMs.
In this paper, we studied the novel problem of modeling human mobility using geo-tagged social media (GeoSM). To effectively unveil mobility regularities from the sparse and complex GeoSM data, we proposed the GM OVE method. Our method distinguishes itself from existing mobility models in three aspects: (1) it enables the mutual enhancement between the subtasks of user grouping and mobility modeling, so as to generate high-quality user groups and reliable mobility models; (2) it leverages keyword spatiotempo-ral correlations as auxiliary knowledge to perform text augmenta-tion, which effectively reduces text sparsity present in most GeoSM data; and (3) it integrates multiple (spatial, temporal, and textual) signals to generate a comprehensive view of users X  moving behav-iors. Our experiments on two real-life data sets show that the strate-gies employed by GM OVE are highly effective in generating qual-ity mobility models. Meanwhile, using context-aware location pre-diction as an example task, we found GM OVE can achieve much better prediction accuracy than baseline mobility models.
There are several future directions based on this work. First, we currently use a pre-specified time constraint to extract reliable tra-jectories from raw GeoSM history. In the future, it is interesting to adapt GM OVE such that it can automatically infer the reliability of the transitions and extract trajectories from the raw data. Sec-ond, besides context-aware location prediction, we plan to apply GM OVE for other tasks in the urban computing context. By the year 2030, it is estimated over 60% of the world population will be located within a city X  X  boundary. One of the huge challenges facing many government organizations, including the US military, is un-derstanding and preparing for complex urban environments. From disaster relief to anomaly movement detection, the challenges as-sociated with operating within the so-called mega-cities are many. The ability to accurately model group-level human mobility will dramatically improve the understanding of normal urban life pat-terns and help address such challenges.
This work was sponsored in part by the U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), National Science Foundation IIS-1017362, IIS-1320617, and IIS-1354329, HDTRA1-10-1-0120, and Grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov), and MIAS, a DHS-IDS Center for Multimodal Information Access and Synthe-sis at UIUC. Luming Zhang is supported by the National Nature Science Foundation of China (Grant No. 61572169) and the Funda-mental Research Funds for the Central Universities. The views and conclusions contained in this document are those of the author(s) and should not be interpreted as representing the official policies of the U.S. Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation hereon. We derive the updating rules for the HMM parameters in the M-step as follows. Given the weighted trajectories { S 1 ,S and the old paramter set  X  ( t ) , the expectation of the complete like-lihood is
Meanwhile, since z k r,n is a binary variable, we have:
The three terms in Equation 15, 16, and 17 are independent, we can thus optimize the parameters  X  , A , and {  X  l ,  X  l , X  arately. To optimize  X  , we need to maximize Equation 15, namely
Using the Lagrange multiplier, we can easily obtain the updating rule for  X  (Equation 1) that maximizes the above objective function.
Now consider the optimization of A . With Equation 16 and 19, we can write down the following objective function, which can be similarly optimized using use the Lagrange multiplier to produce Equation 2:
Finally, as the spatial, temporal, and textual observations are in-dependent, the parameter sets {  X  kg ,  X  kg } , {  X  kt ,  X  be derived separately based on 17. Recall that the spatial and tem-poral distributions are assumed to be Gaussian, we simply take the derivatives of the objective function and set them to zeros, which produces the updating rules in Equation 3, 4, 5, and 6. To optimize  X  , we again combine Equation 18 with the Lagrange multiplier, and obtain the maximizer shown in Equation 7.
