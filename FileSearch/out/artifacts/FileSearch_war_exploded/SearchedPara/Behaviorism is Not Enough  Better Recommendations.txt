 Behaviorism is the currently -dominant paradigm for building and evaluating recommender systems. Both the operation and the eval-uation of recommender system applications are most often driven by analyzing the behavior of users. In this paper, we argue that lis-tening to what users say  X  about the items and recommendations they like, the control they wish to exert on the output, and the ways in which they perceive the system  X  and not just observing what they do will enable important developments in the future of recom-mender systems. We provide both philosophical and pragmatic motivations for this idea , describe the various points in the recom-mendation and evaluation processes where explicit user input may be considered, and discuss benefits that may result from considered incorporation of user preferences at each of these points . In partic-ular, we envision recommender applications that aim to support users X  better selves : helping them live the life that they desire to lead. For example, recommender -assisted behavior change requires algorithms to predict not what users choose or do now , inferable from behavioral data , but what they should choose or do in the fu-ture to become health ier , fit ter , more sustainable , or culturally aware. We hope that our work will spur useful discussion and many new ideas for recommenders that empower their users.
 Recommender system s; participatory design; user studies; algorith-mic filtering; information filtering Recommender system developers rely on two different types of data in order to power and evaluate recommenders: expressed pref-erences, including data like user ratings, survey responses, and other explicit judgements from the user; and behavioral data or im-plicit feedback, arising from recommendation interactions and other actions the user takes on the system . Recommender systems have long used both types of data : early work typically used ratings for recommendation and a combination of user input and behavior observation to evaluate the system [9] .
 In recent years, behaviorism has ascended to be the dominant par-adigm for recommendation. This empha sis takes many forms, but is particularly visible in the trends of favoring implicit feedback over explicit ratings, behavioral evaluations such as A/B tests measuring user activity instead of surveys or ethnographic analyses that capture users X  experience and perception of the system , and the practice of ignoring preference when it disagrees with behavior. User behavior is a valuable source of input . Implicit feedback data is readily available  X  soliciting ratings or responses from users is a challenging t ask  X  and is very good for predicting future user behavior. Behavior -based evaluations directly measure users X  visi-ble responses, which translate to short -term revenue -generating actions such as purchases .
 We argue that this is not enough. While behavioral data rightly plays a significant role in recommendation, we must not neglect users X  stated preferences and ambitions . Further, we submit that eliciting stated desires from users has great potential to enable uniquely empowering recommendation experiences.
 Over the last half -decade, significant progress has been made on developing evaluation strategies that integrate behavioral and sub-jective data. These methods are effective for taking advantage of the relative strengths of both classes of data and form a starting point for the types of evaluations and systems that we envision. At present, recommender systems research has yielded an extensive knowledge base of how to produce effective recommendations that will prompt users to action. But there are several open questions that we must engage users, explicitly, to address:  X  Are users satisfied with their choices and actions, both in short  X  When a user is dissatisfied with their choices, what keeps them  X  Is the recommender feeding users X  better or lesser selves? Is it In this paper, we first outline some philosophical and pragmatic motivations for increasing user participation in the recommenda-tion process. We will then examine where in the recommendation process explicit user input could be used, and propose certain spe-cific benefits we see for increased use of user input in each. We hope the arguments we advance will spur useful discussion about how to design, build, a nd study recommenders that enable users to find material and courses of action that empower them to live more fulfilled and fulfilling lives.
 Our motivations broadly divide into two categories: a values -based philosophical arg ument for why user participation is an intrinsically good objective and a pragmatic argument centered around the kinds of applications that a new focus on user input may enable. Users and system developers often have di fferent visions . E ven when the designer X  X  vision is supported by empirical data on user behavior , the result can still be poorly received if the users feel like their values are not being respected, or that they have no control over the technology in their lives.
 Franklin advocate s for a recognition of the role of reciprocity in our technological development [7] . Reciprocity  X  the give -and -take present in egalitarian face -to -face human interaction , a  X  X enuine communication among interacting parties X   X  is important for tech-nology to empower , rather than dominate, its users. Without reciprocity, users can only take or leave what is given to them . A major theme of Franklin X  X  work is the need to careful ly consider the values embedded in our technologies. Suchman strikes a similar note wh en laying out the merits of participatory design , stating that it  X  X akes explicit the critical, and inevitable, presence of values in the system design process X  [21] .
 Values are present in all system designs. The question isn X  X  whether a system embeds some set of values ; r ather, whose values (and what values ) does it embed ? A re the se values explicitly articulated and subject to discussion ? Are designers transparent about the reasons for decisions? Giving credit to user perspectives is itself a value that can be included in or rejected by a technological process, and in-cluding user input can enable debate and discussion about the other values the system embodies.
 The values of recommender systems ar e seen most clearly in our optimization criteria and evaluation metrics. Sales, lift, engage-ment, click -through rate: all of these are value judgements about the kinds of activities tha t reflect a good recommendation ( and of whether a recommender should ev en be deployed ) . Does engage-ment, for example, truly reflect what users need from a service [6] ? The recent debate over Twitter  X  X  introduction of algorithmic filter-ing to user timelines highl ights how conflicts over values can arise in recommender systems. Changing from the traditional reverse -chronological timeline to a recommender -filtered one was a signif-icant change in core service functionality, and it displeased a number of vocal users. One of the common complaints was that Twitter did not seem to understand how existing users used its sys-tem and the actual problems they faced. If users felt heard, and like their desires and problems were respected, might Twitter X  X  algo-rithmic filtering h ave been better received? T he issue of filter bubbles , brought to public attention by Pariser [15] , is another matter of concern for some users of modern infor-mation systems : does algorithmic filtering isolate us into pockets of one -sided information, to the detriment of ourselves and society? Suchman observe s that  X  X ntil we become familiar wit h and take se-riously each other X  X  concerns there will be little hope for a mutually satisfactory future in the deve lopment of work and technology X  [21] . Giving credit to user concerns is a key dynamic in building technology that truly satisfies those it i mpacts. Besides our philosophical concerns, there are several practical rea-sons why increasing the role of explicit user input of preferences and goals can enable significant advances in recommender systems. First, behavioral data without proper grounding in theory and in subjective evaluation might just result in local optimization or short term quick wins, rather than long term satisfaction. Further, u ser behavior may be driven by outside factors, such as a lack of mean-ingful alternatives. When can we know from the behavior of a user if the recommendations help to fulfill their needs and goals? Neil Hunt raised a striking example in his keynote at RecSys 2014. NetFlix relies heavily on behavioral data for evaluating recom-menders and other service components, but their key metric  X  users watching movies  X  cannot distinguish between a user deriv-ing value from the service and addiction. The recommender, which helps many users find movies and shows that enrich their lives, may be doing some users a disservice by encouraging them to continue watching to the neglect of other important concerns in their lives. Engaging users about their concerns and goals can be a valuable tool in mitigating this kind of problem in recommender systems. This concern is e specially relevant in the emerging area of lifestyle modifications and behavioral change . R ecommending the kinds of things users like or do now will not help them to achieve the behav-ior they want , but rather will rei nforce their current behavior. These systems require more than implicit feedback , and optimization for user needs beyond immediate satisfaction. For example, the user model in such a system need s to understand the users' goals or de-sires and the algorithm needs to optimize for attaining them . There are certainly many other applications that can be envisioned, but several of these ideas have a common theme: can the recom-mender help users become their better selves? We now survey the dif ferent points in the recommendation process where explicit user input can be integrated, and propose applica-tions that c ould benefit from increasing the role of explicit user input in these areas. Participatory design itself is most relevant in the process of design-ing a recommender application and its algorithm(s). In this phase, users can express what they want the recommender to help the m accomplish. What do they desire from news recommendations? People recommendations? Algorithmic sorting of status updates ? Recipes? Consumer products? User input must be balanced with product vision, particularly in the initial design stage ; we are not arguing against a need for expert judgement in design. Moreover, users might not know what they wa nt ( as they might not be aware of their possibly -latent needs ) , but they can respond to ideas , sharing what they like or dislike , and might be prompted to come up with even better ideas. We also do not oppose the use of behavioral data to influence d esign  X  the rise of A/B testing for design as opposed to relying entirely on poten-tially -unsubstantiated judgement is welcome. We are arguing, rather, for including users X  expressed desires as a third leg in the design process of recommenders and related systems . Recommenders can be designed so users have control over which of several algorithms is providing them with recommendations. This is a direct means of giving users agency in the recommend a-tion process, and with suitable explanations of the algorithms it can allow the users to adapt the system to their goals at any given time. There has been some work on allowing user s to choose their rec-ommenders [5] , but there is much more to be done in understanding how we can provide users with meaningful control over the w ay in which their recommendations are produced. We could envision systems where users are provided with a number of recommenda-tion techniques that they can mix and match in order to achieve a particular type of recommendation list; a more sophisticated ver-sion, perhaps, of the diversity control proposed by Ziegler [22] or the user adjustments provided by MovieL ens [8] .
 D ifferent recommenders have different characteristics, which can make them more or less effective for meeting various user infor-mation ne eds [4, 13, 14] . One approach to this varying effectiveness is to attempt to automatically adapt the recommender system to the user X  X  need [10] . This can be thought of as a recommender of rec-ommenders , w here one algorithm  X  X ecommends X  the best recommendation algorithm to use in a particular context or task ; this is one form of context -aware recommendation [2] . A nother ap-proach is to put the user in control of the process , allowing them to directly or indirectly determine the algorithm used. This has a num-ber of benefits, including increasing user agency an d collecting data that can be used to test and develop future meta -recommenders. Once the recommender system has been designed and its algorithm selected, the ongoing process of recommendation depends on input data from the user. Thi s typically comes in two forms: explicit ra t-ings or preference judgements; and implicit feedback in the form of clicks, purchases, and other user events . While explicit ratings dominate d the early days of recommender systems, much recent at-tention has focu sed on implicit feedback. The reasons for this are fairly clear: implicit data is far more plentiful, as it can be gleaned from users X  ordinary actions instead of requiring them to take time to express preference, and the resulting recommendations produce better user response in many practical applications.
 Drawing from experience at Netflix and elsewhere , Amatriain noted that implicit signals are better than explicit ones  X  X lmost al-ways X  [1] . Indeed, many current applicati ons (especially given their lim ited domains of multimedia or e -commerce) do fare better with implicit data . But we see great potential in exploring the cases out-side of  X  X lmost always X : in what settings does explicit user input result in a better recommender system ? One example where it has been shown to result in better recommen-dations is Buzzr [16] . They studied several models for news item recommendation, and found that taking into account the user X  X  ex-plicit subscriptions produced more useful recommendations than recommending from a broader pool of feeds.
 User input need not be complicated. Adding a  X  X ust read X  flag to Facebook or Twitter, so that users can indicate they want to read everything posted by certain friends, can help them retain control. These examples show that explicit user input could improve some types of recommendations . B ut we believe that there are applica-tions where explicit input beyond behavior is likely necessary , not merely beneficial , and it can be particularly useful to consider the difference between actual behavior and stated preference. When a user says one thing and does another  X  which they often do, as Hunt and others have observed  X  there are two possibilities :  X  T he user does not understand their true desires . I n such case s  X  T he user is not satisfied with their behavior and may wish to A pplications where past behavior seems insufficient include :  X  Finding speakers who bring perspectives that have been un- X  Reducing the stress and angst of a social media feed overpop- X  Selecting high -impact next tasks from a to -do list.  X  Changing musical or cinematic tastes.  X  Eating healthier foods.  X  Changing energy consumption behavior.
 Some of these applications have a specific b ehavior al change com-ponent. To support such use cases, we believe it is useful to draw measures and concepts from social psychology. Behavior al change has been extensively studied by social psychologists, who have de-veloped and tested social -cognitive models to describ e how people can put good intentions into action.
 To illustrate, m any theories in this area assume that users have spe-cific goals, and that implementation plans or actions are needed to help them overcome barriers to achieving these goals . For example, the Transtheoretical Model [17, 18] describes behavior al change as a 5 -stage process : it starts with precontemplation and proceeds through contemplation , preparation , action , and finally to mainte-nance . The earlier stages require awareness, whereas the l ater stages require motivation and commitment. In each stage, a reco m-mender might help with different suggestions: raising awareness may help the user in the precontemplation stage, whereas a person-alized action plan might help support the preparation stag e. Once the change is in maintenance mode, the recommender should avoid suggestions that might prompt the user to regress.
 There are many research questions that arise from the goal of build-ing such a system . H ow can we represent such action plans, stages and goals in a recommender system algorithm? What kinds of in-terfaces do we need to elicit the user X  X  goals and present recommendations that w i ll help them achieve them? Providing adequate recommendations in this area has not yet been exte nsively studied . In the related area of Persuasive Technology a lot of progress has been made u sing the persuasion principles by Cialdini [3] , but these attempts focus mostly on the how to per-suade, not what to persuade with. Personalization is done on the message [11] but not yet on its content. However, effective e -coaching should not only address the how, but should also under-stand the what and where [19] . T he  X  X hat X  (personalized recommendations) and on what moment (context awareness) are clearly challenges that recommender systems can take up.
 For one example of how psychological theory can guide the struc-ture of an algorithm , S tarke et al. [20] designed an ene rgy -savi n g recommender system based on a Rasch scal e, which orders behav-iors according to how costly or difficult they are to implement . For example , turning of f the lights when leaving a room is easier than buying a n energy -efficient washing machine . The Rasch model can also represent level s of ability on the same scale enabling the rec-ommender to suggest energy -saving measures that fit a persons X  ability. Moreover, the ordering of the scale can represent a hierar-chy of goals the user mig ht wish to attain in the future on their quest for their better self . Finally, we argue that explicit user input is useful in evaluating rec-ommenders. A/B tests , the common gold standard for recommender assessment , are efficient and effective for testing the impact of dif-ferent algorithms or designs on user behavior but are often limited in their ability to explain why users acted in a particular way . In addition to grounding experiments in theory, collecting subjective responses from users can help a great deal in explaining behavior. Use r studies [12] are one means of doing this . Soliciting the user X  X  subjective perceptions of the recommendations provides valuable insight into their thought processes, and allows us to better under-stand how algorithmic concepts of constructs such as diversity do or do not map to users X  perceptions of those concerns [4] . User studies are not a panacea for good research. Well -designed A/B tests can test theories and lead to generalizable knowledge , and poorly -conceived or underpowered user studies can produce little of lasting value; further, good studies are in general difficult to de-sign, execute, and analyze, so the tool is not responsible for weak results. That said, it is easier to design s urveys whose results pro-vide theoretical insight into human behavior than it is to design strictly behavioral studies , because the link between psychological processes and obse rvable outcomes is more direct.
 In addition to survey -based user studies, qualitative and ethno-graphic studies can elicit how the users feel about the recommender system, its embedding application, and the way that interacts with the rest of their lives.
 Thus, to advance our knowledge about how recommendations are received by th eir users, and the various psychological factors that affect their suitability, it is necessary to conduct many targeted user studies with explicit, subjective responses , not just behavioral stud-ies , and to integrate these with qualitative analysis of user desires and choices . Further, we expect that deeper understanding of users X  subjective experience of recommendation can improve commercial recommendation applications. In this paper, we have laid out several ways in which explicit user input ca n be harnessed to improve recommender applications. There X  X  a great deal of research needed in order to enable future applications to fully realize this potential. Several important ques-tions that arise from our argument include:  X  How can we scale participa tory design to Internet -scale appli- X  What does participatory design of algorithms look like?  X  How can we harness the discrepancies between what users say  X  How can we provide meaningful control over the recommen-Inv esting significant effort in harnessing the power of user input and explicit preference judgements has the potential to enable sub-stantial new recommender systems that empower and enrich users X  lives. Just as importantly, it can help us to maintain user ag ency in an increasingly algorithm -guided information world.
 We thank Jennifer Ekstrand for her valuable insights and feedback on draft versions of this paper.
 [1] Amatriain, X. 2016. Past, Present, and Future of Recom-[2] Anand, S.S. and Mobasher, B. 2007. Contextual Recom-[3] Cialdini, R.B. 2001. Influence: Science and Practice . Allyn [4] Ekstrand, M.D., Harper, F.M., Willemsen, M.C. and Kon-[5] Ekstrand, M.D., Kluver, D., Harper, F.M. and Konstan, J.A. [6] Evans, J . 2014. Twitter X  X  Huge Mistake. TechCrunch . [7] Franklin, U.M. 2004. The Real World of Technology . House [8] Harper, F.M., Xu, F., Kaur, H., Condiff, K., Chang, S. and [9] Hill, W., Stead, L., Rosenstein, M. and Furnas, G. 1995. [10] Kapoor, K., Kumar, V., Terveen, L., Konstan, J.A. and [11] Kaptein, M., Markopoulos, P., de Ruyter, B. and Aarts, E. [12] Knijnenburg, B., Willemsen, M., Gantner, Z., Soncu, H. and [13] McNee, S., Kapoor, N. and Konstan, J.A. 2006. Don X  X  Look [14] McNee, S., Riedl, J. and Konstan, J.A. 2006. Making re c-[15] Pariser, E. 2011. The Filter Bubble: How the New Person-[16] Phelan, O., McCarthy, K., Bennett, M. and Smyth, B. 2011. [17] Prochaska, J.O. and DiClemente, C.C. 1984. The tr anstheo-[18] Prochaska, J.O. and Norcross, J.C. 2001. Stages of change. [19] Rutjes, H., Willemsen, M. C. and IJsselsteijn, W.A. 2016. [20] Starke, A., Willemsen, M.C. and Snijd ers, C. 2015. Saving [21] Suchman, L. 1993. Foreword. Participatory Design: Princi-[22] Ziegler, C. -N., McNee, S., Konstan, J.A. and Lausen, G. 
