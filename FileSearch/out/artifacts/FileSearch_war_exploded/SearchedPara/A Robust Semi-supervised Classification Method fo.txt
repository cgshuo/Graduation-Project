 The transfer learning problem of designing good classifiers with a high generalization ability by using labeled samples whose distribution is different from that of test samples is an important and challenging research issue in the fields of machine learning and data mining. This paper focuses on designing a semi-supervised classifier trained by using unla-beled samples drawn by the same distribution as test sam-ples, and presents a semi-supervised classification method to deal with the transfer learnin g problem, based on a hybrid discriminative and generative model. Although JESS-CM is one of the most successful semi-supervised classifier design frameworks and has achieved the best published results in NLP tasks, it has an overfitting problem in transfer learning settings that we consider in this paper. We expect the over-fitting problem to be mitigated with the proposed method, which utilizes both labeled and unlabeled samples for the discriminative training of classifiers. We also present a re-fined objective that formalizes the training algorithm and classifier form. Our experimental results for text classifi-cation using three typical benchmark test collections con-firmed that the proposed method outperformed the JESS-CM framework with most transfer learning settings. I.2.6 [ Artificial Intelligence ]: Learning; H.2.8 [ Database Applications ]: Data Mining Algorithms, Experimentation transfer learning, semi-supervised classifier, hybrid genera-tive and discriminative model, text classification
Statistical classifiers based on machine learning methods are fundamental and powerful tools for actual tasks such as automatic categorization, information extraction, and data mining. Such classifiers are trained by using labeled sam-ples, each of which is a data sample x with a class label y . Many machine learning methods work well under a common assumption, namely that labeled samples are governed by the same underlying joint distribution p ( x ,y )astestsam-ples about which the classifiers are expected to make predic-tions. In practice, however, the source distribution, p s governing labeled samples is often different from the target distribution, p t ( x ), governing test samples, and thus it is not easy to satisfy the assumption. For example, a labeled sam-ple set must be continuously updated to satisfy the assump-tion in news article classification tasks, since news articles are written about new topics daily. For spam filtering tasks, we need to collect labeled samples for every user, as users receive different e-mails. The collection of labeled samples is very expensive, because data samples are manually assigned to class labels. Therefore, it is an important and challenging problem to design good classifiers with a high generalization ability by using only labeled and unlabeled samples collected from source and target distributions, p s ( x )and p t ( x spectively, where p s ( x ) = p t ( x ). The problem is transfer learning, and this has been investigated as sample selection bias [33], covariate shift [26], or unsupervised domain adap-tation [16] problems. (See [24] and [16] for a comprehensive survey.)
Existing approaches to the transfer learning problem are categorized into instance weighting and feature representa-tion change . These approaches are often both used with conventional supervised or semi-supervised classifiers [34, 7]. The main instance weighting approaches are based on assigning instance dependent weights to the loss function to minimize the expected loss over a target distribution (e.g. [26, 33, 27, 3]). The weight for each labeled sample x n is estimated by using unlabeled samples, according to the empirical risk minimization framework [30]. Then, su-pervised classifiers are trained by using the weighted labeled samples. Instance weighting approaches to semi-supervised classifiers trained on labeled and unlabeled samples have also been presented, where the unlabeled samples were provided with instance weights [10, 17, 20, 32].

Feature representation change approaches are based on transforming the feature space X of data sample x into an-other feature space Z that reduces the distance between p ( z ,y )and p t ( z ,y ). Unlabeled samples are used for esti-mating the transformation function g : X X  X  .Inthetrans-formed feature space, supervised or semi-supervised classi-fiers can be used on condition that the source distribution is similar to the target distribution. Namely, the problem caused by the difference between the source and target dis-tributions is mitigated by such a transformation. Methods for changing the feature representation have been proposed based on extracting relevant features from the two distribu-tions, such as pivot -feature-based methods [2, 4] and graph-based methods (e.g. [23, 31]).

As described above, typical methods based on instance weighting and feature representation change employ super-vised or semi-supervised classifiers as base classifiers, and provide pre-processing or inter-processing methods for ap-plying the base classifiers to the transfer learning problem. The instance weighting and feature representation change are effective for the transfer l earning problem, and we also expect that designing better base classifiers will be useful for improving classification performance. Therefore, we explore a semi-supervised learning method for designing better base classifiers.

Recently, semi-supervised classifiers have been proposed based on hybrid generative/discriminative models [18, 14, 28, 29, 1]. As reported in [19], generative models unsuit-able for real data samples provide worse classification per-formance than discriminative models. Since it is usually difficult to design true generative models for real data sam-ples, discriminative models often achieve better classification performance than generative models, when both models are trained on labeled samples. In contrast, many successful machine learning algorithms that analyze the distribution of unlabeled samples have utilized generative models [25]. The hybrid semi-supervised classifiers were developed to benefit from the respective advantages of generative and discrimi-native models, and the effect of such hybrids was confirmed empirically. The hybrid classifiers are similar in that they are trained based on maximizing the conditional likelihoods of discriminative models for only labeled samples.
We propose a robust semi-supervised classification method for designing good classifiers with a high generalization abil-ity when the source and target distributions are different. In our formulation, classifiers are constructed by combin-ing discriminative and generative models, which is simi-lar to a previous semi-supervised classification framework, namely the Joint probability model Embedding style Semi-Supervised Conditional Model (JESS-CM) , which substan-tially improved the performance of natural language pro-cessing (NLP) tasks [28, 29]. The proposed method pro-vides a classifier training algorithm improved to deal with the difference between source and target distributions. Only labeled samples are used for the discriminative training of the JESS-CM classifiers, while both labeled and unlabeled samples are utilized for the discriminative training of clas-sifiers designed with the proposed method. We expect our training method to mitigate the overfitting of classifiers into the source distribution and to incorporate the target dis-tribution into the classifiers effectively. We also present a refined objective that formalizes the training algorithm and classifier form.

We apply the proposed method to text classification tasks by employing a naive Bayes (NB) model and a multinomial logistic regression (MLR) model as generative and discrim-inative models, respectively. Using three test collections of text classification, we show experimentally that the pro-posed method gives us better semi-supervised classifiers for the transfer lea rning problem.
In this paper, we focus on multiclass and single-label clas-sification, which is the task of selecting a class label from K pre-defined class labels for each data sample. Let the feature vector of a data sample be denoted by x =( x 1 ,...,x i ,..., x ) T  X  X  , and a class label be represented by y  X  X  = { 1 ,...,k,...,K } . Here, V is the dimension of the feature vector, and X is the feature space of the data samples. a T represents the transposed vector of a .

In our transfer learning setting related to sample selec-tion bias, covariate shift, or unsupervised domain adapta-tion, the source and target tasks are the same, while the source and target domains are different. The feature space and class label set of the source domain are the same as those of the target domain, X s = X t and Y s = Y t , but the data sample distributions of the two domains are very different, p ( x ) = p t ( x ). It is assumed that the real conditional prob-ability distributions of the two domains are the same or sim-ilar, P s ( y | x ) P t ( y | x ). For estimating P t ( y main, whereas no labeled samples are available in the target domain. Only an unlabeled sample set D u = { x m } M m =1 be utilized in the target domain.

In this paper, we call labeled samples from the source do-main labeled source samples , and we call unlabeled samples from the target domain unlabeled target samples .Weexplore a method for designing good semi-supervised classifiers with a high generalization ability in the target domain. We stress that we have only unlabeled samples from the target domain.
This section provides a brief overview of probabilistic gen-erative and discriminative models.
A generative model for classification is generally designed by using a joint probability distribution p ( x ,y ) consisting of data sample x and class label y . The joint probability dis-tribution is modeled according to data type: for example, multinomial distribution and Gaussian distribution models are used for text data and continuous feature vectors, respec-tively. Generative classifiers learn such generative models and estimate the class label of x by using the class posterior probability P ( y | x ) derived according to the Bayes rule.
Let p g ( x ,y ;  X  y ) represent the joint probability distribution of a generative model for class label y ,and X =[  X  1 ,..., ...,  X  K ] be a model parameter set. In supervised learning settings, the  X  value is often obtained by using the MAP estimation method, where  X  is estimated to maximize an objective function, J s g ( X ), based on the log-likelihood of the generative model with respect to labeled samples as where p ( X ) is a prior probability distribution of  X .
Discriminative classifiers learn a discriminant function, f ( x ,y ; W ), so that they estimate the class label of labeled samples as accurately as possible. Namely, discriminative classifiers learn class boundaries for labeled samples directly. f ( x ,y ; W ) is a function that provides the class estimate for data sample x as  X  y =argmax y f ( x ,y ; W ), where W is a model parameter set.

In a multinomial logistic regression (MLR) model, the conditional probability of y given x is modeled as using a linear discriminant function, f ( x ,y ; W )= where W =[ w 1 ,..., w k ,..., w K ] T and w k is a parameter vector for the k th class label. In supervised learning set-tings, the W value can be estimated to maximize an objec-tive function, J s d ( W ), based on the conditional log-likelihood with respect to labeled samples as where p ( W ) is a prior probability distribution of W .
Next, we review a previous semi-supervised classifier de-sign framework, Joint probability model Embedding style Semi-Supervised Conditional Model (JESS-CM) [28, 29], based on a hybrid generative and discriminative model. The JESS-CM classifiers consist of a linear discriminative function, f ( x , y ; W ), and a generative model, p g ( x , y ; X ), where W and  X  are the parameter sets of the discriminative function and the generative model, respectively 1 . The conditional probability distribution, P ( y | x ; W,  X ,  X ), of the JESS-CM classifiers is defined as P ( y | x ; W,  X ,  X ) = exp { f ( x , y ; W ) } p g ( x , y ; X ) where  X  provides the combination weight of the generative model.

The parameters of the JESS-CM classifiers are estimated by using two objective functions. According to the definition provided in [28, 29], the objective functions for parameter estimation of Eq. (4) are written as L ( W,  X  |  X )  X  L ( X  | W,  X  )  X  The W and  X  values are computed to maximize L 1 ( W,  X  |  X ) for a fixed  X  value, and the  X  value is computed to maxi-mize L 2 ( X  | W,  X  )forfixed W and  X  values. Namely, the dis-criminative function and combination weight are trained on labeled samples with a discriminative approach as described
Original JESS-CM classifiers are constructed by using mul-tiple generative models. Since the method for combining and training the discriminative function and generative models does not depend on the number of generative models, J , we show the JESS-CM framework at J = 1 to simplify the discussion. Figure 1: Outline of hybrid generative and discrim-inative models. in Section 2.2, and the generative model is trained on un-labeled samples. Since these parameter values are mutually dependent, they are updated by alternating and iterative maximizations of the two objective functions, L 1 ( W,  X  | and L 2 ( X  | W,  X  ). We outline the classifier form and training method of JESS-CM in Fig. 1 (a).
As mentioned in Sections 1 and 2, we propose a semi-supervised learning method called Maximum Hybrid Log-likelihood Expectation (MHLE) , which is designed to obtain better classifiers from unlabeled target samples and labeled source samples. Figure 1 (b) shows an outline of the pro-posed method. The discriminative and generative models composing the proposed classifier are trained on both la-beled and unlabeled samples, while the discriminative func-tion used for JESS-CM is trained using only labeled samples. In our task setting, the training method for JESS-CM may lead to the overfitting of classifiers into the data samples of a source domain, because the distribution of labeled source samples is different from those of target data samples. We expect the method for training the proposed classifier to mitigate such an overfitting problem and to improve the classification performance in a target domain.

This section presents the classifier form and parameter es-timation algorithm of the proposed method. We also show that they are formalized theoretically as a solution that max-imizes the log-likelihood expectation of the discriminative and generative models.
Our semi-supervised classifier design method uses the gen-erative model, p g ( x ,y ;  X ), and the discriminative model, P ( y | x ; W ), shown in Section 2. In our formulation, we assume that there is a common conditional probability dis-tribution P c ( y | x ) that the data samples obey in both the source and target domains. The true distribution of P c ( y is expected to satisfy for a data sample x n whose correct class label is y n .
We first consider a method for training the generative and discriminative models on a given P c ( y | x ). If the true dis-tribution of P c ( y | x ) is known, the parameter value of the discriminative model can be estimated as the W that max-imizes an objective function, J d ( W | P c ), based on the min-imization of the Kullback-Leibler (KL) divergence between P ( y | x )and P d ( y | x ; W ),
J d ( W | P c )  X  X  X  where D ( P c ( y | x ) || P d ( y | x ; W )) = We can also estimate the parameter value of the gener-ative model as the  X  that maximizes an objective func-tion, J g ( X  | P c ), based on the log-likelihood expectation of p ( x ,y ;  X  y ), where E Employing the labeled sample set, D l , as training data, we can reduce the objective functions shown in Eqs. (8) and (10) to those shown in Eqs. (3) and (1). Namely, Eqs. (8) and (10) are simple extensions of Eqs. (3) and (1), respectively. However, the true distribution of P c ( y | x ) is unknown. Thus, P c ( y | x ) should also be estimated from the given train-ing data, as well as the parameters of the discriminative and generative models, W and  X . In our formulation, we esti-mate P c ( y | x ), W , and  X  to maximize the objective function, J ( W,  X  ,P c |  X  ), defined by a linear combination of J and J g ( X  | P c )as where  X  (  X  0) is the combination weight of J d ( W | P c J ( X  | P c ). By using the Lagrange multiplier method under the constraint that maximizes J ( W,  X  ,P c |  X  ) is obtained as Namely, we provide the estimate of P c ( y | x )byusingthe weighted product of the discriminative and generative mod-els. P c ( y | x ; W,  X  , X  ) is reduced to P d ( y | x ; W )when  X  =0, and assigns P c ( X  y | x )=1and P c ( y = X  y | x )=0to x when  X   X  and  X  y =argmax y p g ( x ,y |  X  y ). We utilize P c ( y | x for estimating the { P c ( k | x m ) } k,m of unlabeled samples, and employ it as the conditional probability distribution of the proposed classifier.

We estimate the values of W and  X  by applying labeled and unlabeled sample sets, D l = { ( x n ,y n ) } N n =1 and D { x m } M m =1 ,to J ( W,  X  ,P c |  X  ) shown in Eq. (12). For the es-timation, we substitute Eq. (7) for the { P c ( k | x n ) labeled samples applied to both J d ( W | P c )and J g ( X  We also substitute Eq. (13) for the { P c ( k | x m ) } k,m unlabeled samples applied to them. Then, we can derive the objective function: J ( W,  X  |  X  )= The estimates of W and  X  can be computed as the values that maximize J ( W,  X  |  X  )forafixed  X  value.
We also present a method for estimating the value of the weight parameter,  X  . As shown in Eq. (13),  X  provides the importance of p g ( x ,y ;  X  y ) in the classifier, P c ( y If the correct class labels of many data samples in the target domain are predicted by computing arg max y p g ( x ,y ;  X  we can obtain a high performance classifier by adopting a large positive  X  value. Otherwise, a small  X  value may give us a good classifier. We would like to estimate a  X  value that is suitable for the test samples from training samples. How-ever, we cannot estimate the  X  value from the maximization of the objective function shown in Eq. (12), because the ob-jective function expresses the likelihoods of W and  X , and does not contain a criterion for evaluating the  X  value. To estimate  X  , W , and  X  simultaneously, we need to extend the objective function.

For the extension, we assume that a generative model, p ( x ,y ;  X  y ), would provide a good classifier and should be given a large positive  X  value when the log-likelihood expec-tation of the generative model, J g ( X  | P c ), is large. Then, we introduce a heuristic criterion for evaluating the  X  value as  X .  X   X  2 / 2 C is a penalty factor of  X  ,and C ( &gt; 0) is a hy-perparameter. Since the  X  value that maximizes G (  X  )is C n mate when we obtain a  X  estimate that provides a large J ( X  | P c ) value. This follows our assumption. Therefore, by merging G (  X  ) into the objective function shown in Eq. (12), we design an extended objective function: Since the third and fourth terms on the right side of Eq. (16) are independent of W and  X , these added terms give us an extension only for  X  estimation.

We set the value of  X  (0) y ,y  X  X  1 ,...,k,...,K } at which is independent of class label y . When the genera-tive model is designed to satisfy p g ( x ,y ;  X  (0) )= f ( where f ( x ;  X  (0) ) is a class independent function, the distri-bution form of P c ( y | x )thatmaximizes J ( W,  X  ,P c , X  )cor-responds to those shown in Eq. (13). Then, by applying D l and D u to J ( W,  X  ,P c , X  ), we can provide the objective function, J ( W,  X  , X  ), for estimating the W , X ,and  X  values from labeled and unlabeled samples as J ( W,  X  , X  )= This equation can be derived by substituting Eqs. (7) and ples, respectively, in the same way as that used for deriving Eq. (14).
We can estimate the parameter values of W , X ,and  X  that provide the local maximum of J ( W,  X  , X  ) around the initial parameter values 2 , W (0) , X  (0) ,and  X  (0) ,byusing an iterative computation such as the EM algorithm [11]. Specifically, let  X  ( t ) = { W ( t ) ,  X  ( t ) , X  ( t ) } value set of  X  = { W,  X  , X  } in the ( t )th step. We define a Q -function as where It follows that J ( X )  X  J ( X  ( t ) )  X  Q ( X  ,  X  ( t ) )  X  Therefore, by setting we can monotonically increase the objective function J ( X ). In the ( t + 1)th step, the solution of W ( t +1) that satisfies Eq. (21) is computed as since q d ( W,  X  ( t ) ) is independent of  X  and  X  .Wealsocom-pute
In our experiments, we employed fixed initial values com-puted by using labeled and unlabeled samples, as described in Section 6.2 1. Input training set, D l = { ( x n ,y n ) } N n =1 and 2. Initialize parameter set,  X  (0) = { W (0) ,  X  (0) , X  (0) } 3. Perform the following until convergence. 4. Output parameter estimates  X   X = {  X  W,  X   X  ,  X   X  } after
Figure 2: Algorithm for parameter estimation. becausewerestrict  X  to  X   X  0, as defined in Section 4.1. After that, by computing we obtain the solution of the parameter values given by Eq. (21). We summarize the algorithm for the parameter estimation in Fig. 2.
For text classification problems, we employed a naive Bayes (NB) model as the generative model p g ( x ,y ;  X  y ), using the independent word-based representation known as the Bag-of-Words (BOW) representation. Let x =( x 1 ,...,x i ,..., x ) T represent the feature vector of a document, where x i denotes the frequency of the i th word in the document and V denotes the number of vocabulary words included in a text data set. In an NB model, the probability distribu-tion of document x in class y is regarded as a multinomial distribution: Here,  X  yi &gt; 0and that the i th word appears in a document belonging to class y ,and  X  y =(  X  y 1 ,..., X  yi ,..., X  yV ) T is a parameter vector of the NB model for class y .  X  y represents the probability of class y . For the proposed classifier, we adopted  X  1 /K,  X  y and used feature vectors normalized with vector size | x | = As p ( X ) in Eq. (20), we used a Dirichlet prior: p ( X )  X  Q rameter. In the setting, the solution of  X  ( t +1) =[  X  ( that satisfies Eq. (23) is computed as where I y n ( y ) is a function indicating that I y n ( y )=1( I 0) when y = y n ( y = y n ).
For the discriminative model P d ( y | x ; W ) of the proposed method, we applied the multinomial logistic regression (MLR) model shown in Section 2.2. As p ( W ) in Eq. (19), we used a Gaussian prior [8]: p ( W )  X  computing the W ( t +1) solution of Eq. (22) in the setting, we used the L-BFGS algorithm [21], which is a quasi-Newton method. In this computation, global convergence is guaran-teed, since q d ( W,  X  ( t ) ) is a concave function of W .There-fore, in each step of our parameter estimation algorithm, we can estimate the global optimal solution of parameters that maximizes the Q -function shown in Eq. (18)
To evaluate our proposed method empirically, we used the three test collections, WebKB 3 , SRAA 4 and 20 newsgroups (20news) 5 , which are often employed as benchmark tests for classifiers in text classification tasks.

The WebKB dataset contains web pages collected from four universities and miscellaneous web pages. This dataset consists of seven categories, and each page belongs to one of them. Following the setup in [22], we used four categories: course , faculty , project ,and student . There were 4199 web pages in these categories. We used the web pages collected from four universities as source domain data samples, and we used miscellaneous web pages as target domain data sam-ples. We removed the tags and links from the web pages, and vocabulary words that were included either in the sto-plist or in only one article. We used 18525 vocabulary words to construct the feature vectors of the web pages.
The SRAA dataset contains 73218 UseNet articles from four discussion groups named sim-auto , sim-aviation , real-auto ,and real-aviation . We used the four categories for multi-class classification. In our task setting, we assume that the classification task and feature space of the source and target domains are the same, but that the data sample distributions of the two domains are different, as described in Section 2. To evaluate the proposed method in the task setting, we divided the articles into two subsets by using a spherical K-means clustering algorithm [13], and then used the two subsets as source and target domains in our exper-iments. We removed all header text other than  X  X ubject X  from the articles, and any vocabulary words included either in the stoplist or in only one article. We used a total of 61526 vocabulary words.

The 20news dataset consists of 20 different UseNet dis-cussion groups. For our experiments, we used five groups: comp.* . There were 4881 articles in the groups. We divided the articles into the source and target domains and selected vocabulary words in the same way as with the SRAA data set. We used a total of 19383 vocabulary words.
We examined the text classification performance of the proposed method, MHLE . We compared the performance of MHLE with that of four conventional semi-supervised classi-http://www.cs.cmu.edu/afs/cs.cmu.edu/project/theo-20/www/data/webkb-data.gtar.gz http://www.cs.umass.edu/ mccallum/data/sraa.tar.gz http://people.csail.mit.edu/jrennie/20Newsgroups/20news-18828.tar.gz fication methods, JESS-CM , NB/EM- X  [22], MLR/MER [15], and TSVM [9]. We also examined the performance of three supervised classifiers, NB , MLR ,and SVM ,whichwere trained only on labeled samples.

The JESS-CM classifier was constructed by using the com-bination of a generative model and a discriminative function, as described in Section 3. To apply JESS-CM to text clas-sification, we employed the NB model and the discriminant function of the MLR model. To estimate the parameters of the MLR and NB models, we employed the same Gaussian and Dirichlet priors as those shown in Section 5. We selected the hyperparameter value of the Gaussian prior from seven candidate values of  X  2  X  X  10 n } 6 n =  X  1 . The hyperparameter value  X  =  X   X  1 of the Dirichlet prior was selected from four candidate values of  X   X  X  10 n }  X  1 n =  X  4 We implemented the NB/EM- X  classifier by applying the EM- X  algorithm [22] to the NB model. The EM- X  algo-rithm trains the NB model to maximize the weighted log-likelihood with respect to labeled and unlabeled samples. We selected the weighting parameter for unlabeled samples from thirteen candidate values of  X   X  X  0 . 01 , 0 . 02 , 0 . 05 , 0 . 1 the NB/EM- X  and NB classifiers as for the NB model in-cluded in the JESS-CM classifier, and set the candidate val-ues  X  =  X   X  1 of the hyperparameter as  X   X  X { 1  X  10 n , 2 10
We also implemented the MLR/MER classifier by apply-ing a minimum entropy regularizer [15] and the MLR model. We selected the weighting parameter for the regularizer from eighteen candidate values of  X   X  X { 1  X  10 n , 2  X  10 n , 5 10 n } 0 n =  X  5 } . We employed the same Gaussian prior and can-didate values of the hyperparameter for the MLR/MER and MLR classifiers as for the MLR model included in the JESS-CM classifier.

As the TSVM and SVM classifiers, we used Universvm 6 , an SVM implementation introduced in [9], which employs the concave-convex procedure (CCCP) for TSVM optimiza-tion. We used a linear kernel and selected the symmetric ramp loss parameter value from four candidate values of s sults in [9]. The cost parameter values for the labeled and unlabeled examples were selected from the candidate values
For the MHLE classifier, we selected the hyperparameter value of the Gaussian and Dirichlet priors from the same candidate values as for the JESS-CM classifier. For the hy-perparameter value of the penalty factor shown in Eq. (16), we set four candidate values of C  X  X  ( N + M )  X  10 n } 3 where N and M are the numbers of labeled and unlabeled samples. We set  X  (0) =0, W (0) as the parameter value of the MLR model trained with labeled samples, and  X  (0) based on the average feature value of all the labeled and unlabeled samples.

For all the classifiers described above, we tuned the hy-perparameter and weighting parameter values with a 5-fold cross-validation of the labeled samples. We used word-frequency features to design the feature vectors of the data samples for the SRAA and 20news datasets based on word frequency, but binary features for the WebKB dataset, be-cause the binary features provided better classification per-formance for WebKB than the word-frequency features. For http://www.kyb.tuebingen.mpg.de/bs/people/fabee/ universvm.html six of the classifiers (excluding NB/EM- X  and NB) we used normalized feature vectors whose L1-norm was a constant value to mitigate the effect of vector size on computation.
We randomly selected labeled source samples and unlabeled target samples from data subsets for the source and target domains, respectively. We selected 2500 unlabeled target samples for each SRAA and 20news dataset. We selected 2000 unlabeled target samples from the WebKB dataset. For each dataset, we selected 200 source test samples and 800 target test samples from the remaining data samples of the source and target domain s, respectively. We made ten different evaluation sets for each dataset by this random selection, and examined the classifier performance using the average classification accuracies of the test samples over the ten evaluation sets.
Table 1 shows the average classification accuracies over ten experiments obtained with eight classifiers. Each number in parenthesis in the table denotes the standard deviation of the ten experiments. Asterisks against each method show that the difference between the average classification accu-racies of the method and MHLE is significant ( p&lt; 0 . 05) in the Wilcoxon test, which has been used for statistical com-parisons of the abilities of classifiers (e.g. [12]). Each num-ber in the final column, r l ( r u ) shows the word-overlap ratio between the labeled and unlabeled sample sets, which was computed as the percentage of the number of vocabulary words included in both the labeled and unlabeled sample sets against the total number of vocabulary words included in the labeled (unlabeled) sample set.

We first compare the MHLE classifier with the pure gen-erative and discriminative classifiers except for JESS-CM. As shown in Table 1 (a), we found that in most cases the MHLE classifier performed better than the others for the target test samples. This result shows that the hybrid gen-erative/discriminative model was useful for obtaining good classifiers with high generalization performance from only la-beled source samples and unlabeled target samples. TSVM outperformed MHLE only when N = 100 for the WebKB dataset. In this case, the SVM classification performance was considerably better than that of MLR. Since the MHLE classifier is constructed by combining MLR and generative models, this result is reasonable.

A comparison of MHLE with JESS-CM showed that MHLE performed better than JESS-CM for the target test samples of the WebKB and 20news datasets. For the source test sam-ples of the WebKB and 20news datasets, the classification performance of MHLE was similar to that of JESS-CM as shown in Table 1 (b). Figure 3 also shows the classification accuracies when training the MHLE and JESS-CM classi-fiers by using 1000 labeled samples and various numbers of unlabeled samples for the 20news dataset. In Fig. 3, we see that using many unlabeled samples improved the clas-sification performance of MHLE for the target domain and maintained that for the source domain. These results for the source and target test samples show that the MHLE clas-sifier was not underfitted to the source domain although it was better fitted to the target domain than the JESS-CM classifier.

As described in Sections 3 and 4.3, the parameters of both the MHLE and JESS-CM classifiers were estimated by us-ing iterative computations. Figure 4 shows examples of the Classification Accuracy (%) Figure 3: Classification accuracies (%) obtained with MHLE and JESS-CM trained by using 1000 labeled samples and various numbers, M , of unla-beled samples for the 20news dataset. classification accuracies obtained with MHLE and JESS-CM classifiers at the t th iteration step in training. CA ( T,C ) and CA ( S, C ) represent the classific ation accuracies of tar-get and source test samples, respectively. To analyze the ef-fect of the method for training the MHLE classifier, we also examined the individual classification performance of the discriminative and generative models composing the MHLE classifier. CA ( T,DM )and CA ( T,GM ) represent the classi-fication accuracies of the target test samples obtained with P ( y | x ; W )and p g ( x ,y ; X ).

As shown in Fig. 4, the CA ( T,DM )valuesofMHLE for 20news and WebKB increased gradually with the iter-ative computation of the parameter estimates, and then the CA ( T,C ) values of MHLE also increased. On the other hand, the CA ( T,C ) values of JESS-CM hardly improved es-pecially for WebKB. These results indicate that the iterative computation of MHLE was more effective than that of JESS-CM. In each iteration step, the discriminative model com-posing the MHLE classifier was trained on unlabeled target samples whose { P c ( y | x ; W,  X  , X  ) } y were computed by using the parameter estimates in the previous step, and then was gradually fitted to the unlabeled target samples. In contrast, the JESS-CM classifier was always trained to maximize the conditional likelihood of only labeled source samples. We assume that using unlabeled target samples for training the discriminative model of MHLE improved its generalization ability in the target domain, and that the improvement was effective in obtaining better classification accuracies.
The classification performance of MHLE was similar to that of JESS-CM for the target test samples of SRAA. Ta-ble 1 shows that the r l and r u values for SRAA were smaller than those of 20news at each N setting. Small r l and r u values indicate that there is a large difference between the distributions of the labeled source and unlabeled target sam-ples. We assume that these results were generated by de-signing the MHLE classifier using a single generative model for labeled source and unlabeled target samples. This mod-eling is appropriate for sample selection bias settings, where it is assumed that all data samples are drawn from a sin-gle distribution, and the labeled and unlabeled samples are selected with different probabilities. If the labeled and unla-beled samples are drawn from different distributions, it may be adequate to use individual generative models for the two types of samples, p g ( x | y ;  X  ( l ) y )and p g ( x | y ; supposition, we designed an MHLE classifier for the tar-get domain, P c ( y | x ;  X  W,  X   X  ( u ) y ,  X   X  ), by employing p trained only on unlabeled samples. Using the classifier, we obtained better classification accuracies of 45.8 ( N = 50), 48.7 ( N = 200), and 49.4 ( N = 1000) percents for the target test samples.
 these classifiers in a standard semi-supervised learning set-ting, where all the data samples were selected from a single domain. For an empirical evaluation in the setting, we se-lected labeled, unlabeled, and test samples from the original datasets without splitting them into two domains. Table 2 shows the classification accuracies obtained with these clas-sifiers in the setting. As shown in Table 2, the classification performance of MHLE was similar to that of JESS-CM, and in most cases the MHLE classifier outperformed the other classifiers. We confirmed experimentally that MHLE was also useful for obtaining good classifiers with a high gener-alization ability when the distributions of labeled and unla-beled samples were similar, and thus that MHLE was robust to the difference between the distributions.
The proposed method provides an extension of the gener-ative/discriminative model combination method employed when designing JESS-CM classifiers [28]. Other combina-tion frameworks of generative and discriminative models for semi-supervised classification have been proposed based on the parameter coupling prior (PCP) [18, 14, 1] or multi-conditional learning (MCL) methods [14]. The basic idea of the PCP method is to employ a discriminative model as a classifier, and to utilize generative models for incorpo-rating unlabeled samples into the classifier. Gaussian [18, 14] or conjugate Beta priors [1] are used to couple the pa-rameters of generative and discriminative models, and these parameters are estimated under the constraint imposed by the priors. The MCL method is based on designing genera-tive and discriminative models with shared parameters, and on estimating the parameter that maximizes the weighted likelihood combination of these models. This combination method is similar to the  X  X onvex-combination X  method [6]. A unified framework for the PCP and convex-combination methods has also been presented [5].

Some work has been undertaken to extend semi-supervised classifiers to transfer learning problems by applying instance weighting. In [17], a training method for a discriminative classifier is presented based on a combination of self-training and instance weighting approaches, which improves the clas-sification performance by weighting the labeled and unla-beled samples used for training. A method for selecting un-labeled samples to improve the self-training based classifica-tion has also been proposed, where unlabeled samples con-taining both domain-independent and domain-specific infor-mation are used for training [32]. In [10], a training method for a generative classifier is presented based on the semi-supervised EM- X  [22] algorithm. The method estimates a trade-off parameter  X  between labeled and unlabeled data using the KL-divergence between source and target domains. method for dealing with transfer learning problems, where the distribution of labeled samples is different from that of the target test samples, and only unlabeled samples are available in the target domain. The main idea of the pro-posed method is to construct a classifier by combining a generative model with a discriminative model trained on both labeled and unlabeled samples. The learning algo-rithm of the proposed method is formalized by defining a conditional probability distribution P c ( y | x ) for the classi-fier as the solution that minimizes the KL-divergence from a probabilistic discriminative model P d ( y | x ) and maximizes the log-likelihood expectation of a generative model p g ( on labeled and unlabeled samples. Experimental text clas-sification results confirmed that the proposed method was effective in mitigating the overfitting of the classifier to the source domain, and then in improving the classification per-formance in the target domain. Future work will involve ex-tending the proposed method by applying instance weight-ing and feature representation change approaches. We will also apply the proposed method to multimodal data clas-sification, sequence labeling, and tree labeling problems in which different discriminative and generative models are em-ployed, to show the usefulness of the method. [1] A. Agarwal and H. Daum  X  e III. Exponential family [2] R. K. Ando and T. Zhang. A framework for learning [3] S. Bickel, M. Bruckner, and T. Scheffer. Discriminative [4] J. Blitzer, R. Mcdonald, and F. Pereira. Domain [5] G. Bouchard. Bias-variance tradeoff in hybrid [6] G. Bouchard and B. Triggs. The tradeoff between [7] O. Chapelle, B. Sch  X  olkopf, and A. Zien, editors. [8] S. F. Chen and R. Rosenfeld. A Gaussian prior for [9] R. Collobert, F. Sinz, J. Weston, and L. Bottou. Large [10] W. Dai, G.-R. Xue, Q. Yang, and Y. Yu. Transferring labeled samples and fixed numbers of unlabeled samples. [11] A. P. Dempster, N. M. Laird, and D. B. Rubin. [12] J. Dem X  sar. Statistical comparisons of classifiers over [13] I. S. Dhillon and D. S. Modha. Concept [14] G. Druck, C. Pal, X. Zhu, and A. McCallum.
 [15] Y. Grandvalet and Y. Bengio. Semi-supervised [16] J. Jiang. A literature survey on domain adaptation of [17] J. Jiang and C. Zhai. Instance weighting for domain [18] J. A. Lasserre, C. M. Bishop, and T. P. Minka. [19] P. Liang and M. I. Jordan. An asymptotic analysis of [20] X.Ling,W.Dai,G.-R.Xue,Q.Yang,andY.Yu.
 [21] D. C. Liu and J. Nocedal. On the limited memory [22] K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. [23] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang. [24] S. J. Pan and Q. Yang. A survey on transfer learning. [25] M. Seeger. Learning with labeled and unlabeled data. [26] H. Shimodaira. Improving predictive inference under [27] S. Sugiyama and K.-R. M  X  uller. Input-dependent [28] J. Suzuki and H. Isozaki. Semi-supervised sequential [29] J. Suzuki, H. Isozaki, X. Carreras, and M. Collins. An [30] V. Vapnik. The Nature of Statistical Learning Theory . [31] Z. Wang, Y. Song, and C. Zhang. Knowledge transfer [32] D.Wu,W.S.Lee,N.Ye,andH.L.Chieu.Domain [33] B. Zadrozny. Learning and evaluating classifiers under [34] X. Zhu. Semi-supervised learning literature survey.
