 Kilho Shin yshin@ai.u-hyogo.ac.jp When applying the kernel method to speci c prob-This condition is mandatory not only for a reproduc-ing kernel Hilbert space to exist but for performing kernel multivariate analysis, such as SVM. Secondly, an efficient algorithm to compute the kernel must ex-ist. Even if these two conditions are met, the choice of the kernel still impact the results.
 Assume that data objects are (row) vectors of real val-ues of the same dimension, for example. They are in a common space, and the kernel K ( x ; y ) = xy T is canonically associated. Although this kernel is posi-tive de nite and efficiently computed, whether we can obtain good results with this kernel is not certain. We need a latitude to tune kernels, and the polynomial and Gaussian kernels provide methods. The polyno-adjustable parameters c and d , and Gaussian kernel as K ( x ; y ) = exp eter . We can optimize these parameters by means of cross validation and grid search, for example. Also, the kernel method is good at dealing with data structured in the form of strings, trees and graphs. In fact, we have an effective framework to incorporate structural information of data objects into the design of kernels. To illustrate, we rst see the R-convolution kernel that Haussler ( 1999 ) introduced as follows: R x 2 to a D -dimensional vector x  X  2  X  and d :  X  d  X  d ! R are kernels. Haussler has proved ample of the R-convolution kernel. Given a pair of character-long substrings of x and y with x  X  = y  X  . As an R-convolution kernel, we de ne it so that Shin &amp; Kuboyama ( 2008 ) have generalized the R-convolution kernel to introduce the mapping kernel: The Cartesian product R 1 ( x ) R 1 ( y ) is replaced with arbitrary M x;y . Also, K is positive de nite, if M x;y is symmetric (( x ( x  X  ; z  X  ) 2 M The all-subsequence kernel is an example of the map-ping kernel. The kernel counts the identical sparse weights of is symmetric and transitive, and is positive de nite. Therefore, this kernel is positive de nite.
 Another interesting example of the mapping kernel is derived from the generic framework of edit distance. Let d ( x; y ) = min : x ! y ( ) be an edit distance be-tween objects x and y , where : x ! y and ( ) denote an edit script from x to y and its cost. As Cortes et al. ( 2004 ) proved for Levenshtein edit dis-tance, e d ( x;y ) is not always positive de nite. By contrast, when we de ne a parameterized family of distances by d ( x; y ) = 1 log of cases, but lim !1 d ( x; y ) = d ( x; y ) also holds. Hence, we de ne mapping kernels K by K ( x; y ) = M x;y is the set of mappings (traces) across all of the edit scripts that transform x into y of Levenshtein distance, M x;y is determined by For-ing y  X  for x  X  , and we assume that the costs of dele-tion and insertion are 1. For Levenshtein distance, is symmetric and transitive, and hence, the derived mapping kernels are positive de nite, while this does ( Kuboyama et al. , 2006 ).
 Many other important kernels in the literature are also shown to be mapping kernels, including the gap-Shin &amp; Kuboyama ( 2010 ) reported that 18 of the 19 tree kernels surveyed were mapping kernels.
 mapping kernel is de ned in the more general form of and the following interesting and useful theorem holds. Theorem 1 ( Shin &amp; Kuboyama ( 2008 )) For Formula ( 2 ), the following are equivalent. 1. f M x;y j x; y 2 g is symmetric and transitive. 2. K is positive de nite for any positive de nite k . Nevertheless, the examples in the literature all em-ploy the restricted setting of k ( x  X  ; y  X  ) = Is this just a coincidence? What if  X  tutes for the initial motivation of our research.
 The answer to the rst question is negative: The set-ting of k ( x  X  ; y  X  ) = putational feasibility of the resulting mapping kernels. For example, for the all-subsequence kernel and the kernel derived from Levenshtein distance, Formula ( 3 ) holds thanks to k ( x  X  ; y  X  ) = mula yields an efficient dynamic-programming-based algorithm to compute the respective mapping kernels. To answer the second question, we need the theory of partitionable kernels that Shin ( 2011 ) has developed. The theory de nes partitionable kernels so that, if a partitionable kernel is speci ed for the inner kernel k of a mapping kernel, and if M x;y meets a certain condi-tion, the mapping kernel has a set of recurrence formu-las to compute itself. All of  X  amples of the partitionable kernel.
 Furthermore, each partitionable kernel is associated with a unique non-negative integer, named hidden de-gree , and a partitionable kernel of hidden degree h is de ned through association with other h 1 par-titionable kernels (De nition 2 ). In other words, if h &gt; 1, whether a given string kernel is partitionable is not always an easy question to answer, since it re-quires to nd the remaining (hidden) associated ker-hidden degree one, if, and only if, it is of the form of k ( x  X  ; y  X  ) = 1 also relates to computational complexity of mapping kernels: Approximately, a mapping kernel de ned with a partitionable kernel of hidden degree h has h times greater complexity than when h = 1.
 We have two ndings from the above. First, all of the examples of mapping kernels in the literature merely take advantage of the easiest portion of the entire par-titionable kernels, that is, the class of partitionable kernels of hidden degree one.
 Secondly, the research of partitionable kernels of hid-den degree two is signi cantly attractive for two rea-sons: It is an exploration to a new frontier of kernel design, and we have good opportunities to discover novel important kernels; Mapping kernels for hidden degree two will be merely twice slower than the map-ping kernels known so far in the literature. Nevertheless, it is also a fact that we have a problem to solve: The space of partitionable kernels of hidden degree two might be too broad to nd good kernels. Since partitionable kernels of hidden degree one are characterized to be k ( x  X  ; y  X  ) = 1 we can perform an efficient linear search on to nd optimal kernels. By contrast, no such a characteriza-tion is known for hidden degree two.
 To solve this problem, we start with giving a math-ematical characterization to the partitionable kernels of hidden degree two. In fact, our main theorem (The-orem 4 ) asserts: Any partitionable kernel of hidden degree two belongs to one of three types; Furthermore, determined according to the type to which belongs, includes at most one adjustable parameter .
 Interestingly, this theorem presents an efficient method to tune partitionable kernels of hidden degree two: We rst divide the entire space for hidden degree two into three subspaces, each of which corresponds to one of the three types, and then evaluate cross validation optimal kernels by means of the two-dimensional grid search on t and , for example. Otherwise, the faster linear search on t can apply. In this section, we outline the theory introduced by Shin ( 2011 ), while Lemma 1 is a new result. Like many other string kernels in the literature, a partitionable kernel compares two strings of the same length. Such a string kernel, called an integral kernel for convenience, is represented as a series of kernels alphabet, and 0 is the singleton set of the null string meets the condition speci ed in De nition 2 , based on the notion of partitions of string pairs (De nition 1 ). De nition 1 Let x = x 1 : : : x k 2 k and y = y : : : y k 2 k . A two-partition of ( x ; y ) is a pair x y j +1 : : : y k for some 0 j k . If j = 0 , x L = y L = If j = k , x R = y R =  X  .
 kernels De nition 2 A family of integral kernels is said to be partitionable , if, and only if, there exist n -dimensional square matrices Q 1 ; : : : ; Q n such that i ( x ; y ) = for 8 i = 1 ; : : : ; n , 8 k 0 , 8 x 2 k , 8 y 2 k and 8 When an integral kernel [ ] is a member of a parti-tionable family of integral kernels, we simply say that [ ] is partitionable. Furthermore, if [ ] is partition-able, we de ne its hidden degree , denoted by hd( [ ] ), as the minimum of n such that [ ] is a member of a constant function [ ] 0, we de ne hd( [ ] ) = 0. The partitionable kernel class is abundant, that is, in-cludes an in nite number of instances.
 tionable, where : ! R is an arbitrary kernel. e d ( x ; y ) = e 1 ( x ; y ) = Evidently, e [ k ] d is the elementary symmetric polyno-e d ( x ; y ) = e have hd( e [ ] d ) = d + 1 and hd( e [ ] 1 ) = 1 . Example 2 illustrates the relation between partition-able kernels and computation of mapping kernels. Example 2 We determine M x;y by Formula ( 1 ), and let K d ( x; y ) =
K d ( x; y ) = holds, for the rst characters x 1 and y 1 of x and y , To obtain recurrence formulas to compute K d , we need to evaluate the rst term. When d = 1 , Thus, we obtain Formula ( 3 ). For d = 0 , apparently holds. For d &gt; 0 , e 1 ( x 1 ; y 1 ) e to compute K 0 ; : : : ; K d 1 together.
 Theorem 2 generalizes the results of Example 2 . Theorem 2 ( Shin ( 2011 , Theorem 3)) Let M x;y tionable kernel family. There exists a set of recurrence formulas that reduces the computation of
K i ( x; y ) = y j is smaller than x or y .
 Intuitively speaking, if a family of M x;y is pretty de-composable , any member M x;y can be decomposed into one or more M x and y j is smaller than x or y , and M x bined with each other by means of the set union [ and the string concatenation  X  : For two string x  X  and x , x  X   X  x  X  X  means the canonical concatenation of x  X  and x  X  X  ; For two sets S  X  and S  X  X  of strings, we let S M x;y by Formula ( 1 ) can be decomposed as M For the formal de nition, we ask the reader to refer to Shin ( 2011 , De nition 6). Furthermore, the follow-ing are important to note: Almost all of the kernels in the literature categorized as mapping kernels have recurrence formulas due to Theorem 2 , and the recur-multaneously, where K i is determined by [ ] i ; Hence, pute the mapping kernel K determined by [ ] . Partitionable string kernels have many other good properties: The space of partitionable kernels is closed under addition, scalar multiplication and multiplica-and permutation 2 S k .
 In addition, Theorem 3 characterizes the partitionable kernels of hidden degree one.
 Theorem 3 (Theorem 2, ( Shin , 2011 )) ( a homogeneous polynomial in [1] j ( x  X  ; y  X  ). element of Q i . i ( x ; y ) =
Q i [ j 1 ;  X  1 ] mathematical induction on k .  X  3.1. The main theorem Although partitionable kernels with hidden degree higher than one will de nitely produce novel mapping kernels, too high hidden degrees will result in compu-tationally unfeasible mapping kernels. This motivates us to study the case of hidden degree two.
 Theorem 4 If [ ] is partitionable with hidden de-gree two, [ ] is a linear combination of a kernel pair (~ 1 ; ~ Table 1 and 2 . We use the following notation: ~ i ( x ; y ) = o 2 ( j 1 ; : : : ; j k ) =
F 2 ( x ) = Example 3 The kernel e [ ] 1 ( x ; y ) = a partitionable kernel of hidden degree two, and belongs to Type II. In fact, if we let = 0 , ~ [1] 1 = 1 and ~ 2 = , we have ~ Example 4 The kernel ( e [ ] 1 e [ ] 1 )( x ; y ) =  X  tionable kernel of hidden degree two, and belongs to Type II. In fact, if we let = 0 , ~ [1] 1 = 1 and ~ 2 = 1 2 , we have ~ 3.2. Preliminaries We show some preliminary results that we will use to prove Theorem 4 .
 When kernels forming a partitionable family are lin-early transformed, the conversion rules for the associ-ated matrices are given as follows.
 Lemma 2 For a partitionable kernel family ( 1 ; : : : ; such that a 1  X  = 0 . Further, we let Q  X  i [1 ; 1] = Q  X  i [1 ; k ] =
Q  X  i [ j; k ] = Q i [ j; k ] mined by ~ Q 1 = 2 ; : : : ; n .
 The following lemmas show important properties of Q 1 and Q 2 assuming n = 2. Their proofs are given in Appendix Appendix A and Appendix B .
 Lemma 3 If hd( [ ] ) = 2 , Q 1 and Q 2 are symmetric. Lemma 4 If hd( [ ] ) = 2 , there exists a partitionable is diagonal. 3.3. A proof to Theorem 4 By Lemma 4 , [ ] is a linear combination of some par-titionable kernel family ( [ ] 1 ; [ ] 2 ) with Q 1 = and Q 2 = ondly, we can assume = 1, since we can modify Q 1 into Q 1 = suming = 1, we evaluate [3] 1 in two different ways as follows. By comparing the coefficients of the terms, we obtain =  X  and = 0. In the same way, we can obtain = 0 and  X  (  X  1) =  X  , by evaluating [3] 2 . Finally, we have two cases to investigate: Case 1. = 0 and  X  (  X  1) =  X  ; Case 2.  X  = 0, = 0 and  X  = 1. We investigate these cases.
 Case 1. If  X   X  = 0, we replace [ ] 2 with  X  [ ] 1 +  X  2 . Due to Lemma 2 , Q 2 is converted into Q 2 = [  X  +  X   X  2 0 If  X  = 0,  X  = 1 follows from  X  (  X  1) =  X  . We convert Q 2 into Q 2 = 2 with Case 2. Since  X  = 0 indicates Type II, we investigate the case of  X   X  = 0. We can convert Q 2 into Q 2 = by replacing [ ] 2 with  X  [ ] 2 . This yields Type III. to Theorem 3 . On the other hand, the assertions for Type II and Type III can be veri ed by mathematical induction on k .
 ~ 1 ( x ~ 1 ( x ~ 2 ( x ~ 1 ( x o c o assertion follows from the hypothesis of induction. Type III. In the same way, we have ~ 1 ( x ~ 1 ( x ~ 2 ( x o c o assertion follows from the hypothesis of induction. In particular, if j k = 2, we have the following by the re-currence formula with respect to F n ( x ). Theorem 4 asserts that a partitionable kernel [ ] with hidden degree two is determined by the follow-ing parameters: Type , a pair of character-wise ker-to search optimal t and . 4.1. Selecting ~ [1] 1 and ~ [1] 2 The character-wise (label) kernels that are used in the literature are limited: Constant functions , Kro-x  X  ;y  X  are the most common, and Gaussian function e common candidates.
 For example, let M x;y be the one de ned for the all-M x;y is a pair of substrings such that  X  ter pairs between x  X  and y  X  . Hence, the resulting mapping kernel counts the pairs of substring pairs of the same length using the number of identical char-acter pairs as weights, whereas the all-subsequence kernel uses k as weights. In the same way, if we ~ 2 ( x ~ 2 ( x 4.2. Searching optimal t and After determining ~ 1 and ~ 2 , the combination of cross 2001 ).
 For this purpose, it is desirable to calculate Gram ma-are polynomials in rather than simple numeric val-ues. This signi cantly improves the efficiency of the search, since we can avoid recalculating kernel val-ues with expensive dynamic-programming-based algo-rithms, whenever the value of is changed.
 To perform cross validation, we assume to use C-SVM in this paper. Thus, the adjustable parameters that we can change include t , and the regularization pa-rameter C of C-SVM. In addition, if we use adjustable sociated parameters can be included.
 When the number of the adjustable parameters to ex-amine is d , we perform the d -dimensional grid search. The most signi cant advantage of using the grid search consists in the fact that we can take advantage of par-allel computation to accelerate the search. It is important to answer when and how mapping kernels with the novel partitionable kernels that we study in this paper are effective. Although large-scale experiments are necessary to answer this ques-tion, we only ran a preliminary experiment, using three datasets retrieved from the KEGG/GLYCAN database ( Hashimoto et al. , 2006 ), which contain gly-can structures, represented as rooted ordered trees and annotated relating to colon cancer, cystic brosis and leukemia cells. Table 3 describes these datasets. The size of a tree is the number of the vertices, the height is the length of the longest downward path, and the degree is the maximum number of child vertices. We use ve kernels for trees, referred to as Count , Weighted , Type I , II and III . Count counts the identical pairs of substructures (forests) counts the same substructure pairs with the de-cay factor 1 2 ( Kuboyama et al. , 2006 ) ( K Wght =  X  isomorphic substructure pairs. On the other hand, are mapping kernels de ned with partitionable kernels of hidden degree two, whose speci cation is given in Table 4 . To make the experiment simple, we assume = 0, instead of searching an optimal value for . The steps of the experiment are as follows: We ran-domly generate ve pairs of training and test data subsets from each dataset, train C-SVM with the training subsets, and then measure AUC (Area Un-der Curve) of ROC (Receiver Operation Characteris-tic) curve with the test subsets. During training C-SVM, we optimize the regularization parameter C for C-SVM, and the parameter t for the Type I , II and III kernels, by means of cross validation and grid search. Table 5 shows the results. We should note that Type II showed good performance for Cystic-Fibrosis, while the other kernels could show only poor performance. Cystic-Fibrosis is characterized by the highest average degree, and hence, contains trees with more compli-cated structures than the other two datasets. The runtime scores with MacBook Air show that Type I ,
II and III were 5 times slower than the others on average: Type I , II and III commonly took around 20, 30 and 600 seconds to generate the Gram matri-ces for Colon-Cancer, Cystic-Fibrosis and Leukemia, respectively. Characterizing the class of partitionable kernels with hidden degree three still interests us. Also, intensive experiments using a variety of datasets will justify the effectiveness of partitionable kernels.
 Since follows from [1] i = ( 2 are linearly dependent, and we can let Q i diagonal. By the hypothesis of induction we have with Q 1 = If  X  = 0, by Lemma 2 , we see that replacing [ ] 2 with Q 1 = Q into Q 1 = Q  X  1 + x Q  X  2 , where case to the case of  X  = 0. Otherwise, by replacing [ ] 1 we obtain
Q
Q Hence, Q 1 is modi ed to Berg, C., Christensen, J. P. R., and Ressel, R. Har-monic Analysis on semigroups. Theory of positive de nite and related functions . Springer, 1984. Chang, C.-C. and Lin, C.-J. Libsvm: a library for support vector machines. http://www.csie.ntu.edu.tw/~cjlin/libsvm/, 2001. Collins, M. and Duffy, N. Convolution kernels for nat-ural language. In Advances in Neural Information
Processing Systems 14 [Neural Information Process-ing Systems: Natural and Synthetic, NIPS 2001] , pp. 625{632. MIT Press, 2001.
 Cortes, C., Haffner, P., and Mohri, M. Rational ker-nels: Theory and algorithm. Journal of Machine Learning Research , 1:1{50, 2004.
 Hashimoto, K., Goto, S., Kawano, S., Aoki-Kinoshita,
K. F., and Ueda, N. Kegg as a glycome informatics resource. Glycobiology , 16:63R { 70R, 2006. Haussler, D. Convolution kernels on discrete struc-tures. UCSC-CRL 99-10, Dept. of Computer Sci-ence, University of California at Santa Cruz, 1999. Kashima, H. and Koyanagi, T. Kernels for semi-structured data. In the 9th International Conference on Machine Learning (ICML 2002) , pp. 291{298, 2002.
 Kuboyama, T., Shin, K., and Kashima, H. Flexi-ble tree kernels based on counting the number of tree mappings. In Proc. of Machine Learning with Graphs , 2006.
 Leslie, C. S., Eskin, E., and Stafford Noble, W. The spectrum kernel: A string kernel for SVM protein classi cation. In Paci c Symposium on Biocomput-ing , pp. 566{575, 2002.
 Lodhi, H., Shawe-Taylor, J, Cristianini, N, and H.,
Watkins C. J. C. Text classi cation using string kernels. Advances in Neural Information Processing Systems (NIPS 2000) , 13, 2001.
 Lu, S. Y. A tree-to-tree distance and its application to cluster analysis. IEEE Transactions on Pattern
Analysis and Machine Intelligence (PAMI) , 1:219  X  224, 1979.
 Shin, K. and Kuboyama, T. A generalization of Haus-sler's convolution kernel { Mapping kernel. In ICML 2008 , 2008.
 Shin, K. and Kuboyama, T. A generalization of Haus-sler's convolution kernel { Mapping kernel and its application to tree kernels. J. Comput. Sci. Tech-nol , 25(5)::1040{1054, 2010.
 Shin, K. Partitionable kernels for mapping kernels. In ICDM 2011 , pp. 645{654, 2011.
 Ta X , K. C. The tree-to-tree correction problem. JACM , 26(3):422{433, July 1979.
 Wagner, R.A. and Fischer, M.J. The string-to-string correction problem. JACM , 21(1):168{173, 1974. Zhang, K. Algorithms for the constrained editing distance between ordered labeled trees and related
