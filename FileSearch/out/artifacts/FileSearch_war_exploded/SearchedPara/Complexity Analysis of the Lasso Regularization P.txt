 Julien Mairal julien@stat.berkeley.edu Bin Yu binyu@stat.berkeley.edu Department of Statistics, University of California, Berke ley. Without a priori knowledge about data, it is often dif-ficult to estimate a model or make predictions, either because the number of observations is too small, or the problem dimension too high. When a problem solu-tion is known to be sparse, sparsity-inducing penalties have proven to be useful to improve both the quality of the prediction and its intepretability. In particu-lar, the  X  1 -norm has been used for that purpose in the Lasso formulation ( Tibshirani , 1996 ).
 Controlling the regularization often requires to tune a parameter. In a few cases, the regularization path X  that is, the set of solutions for all values of the regular-ization parameter, can be shown to be piecewise lin-ear ( Rosset &amp; Zhu , 2007 ). This property is exploited in homotopy methods, which consist of following the piecewise linear path by computing the direction of the current linear segment and the points where the direc-tion changes (also known as kinks). Piecewise linearity of regularization paths was discovered by Markowitz ( 1952 ) for portfolio selection; it was similarly exploited by Osborne et al. ( 2000 ) and Efron et al. ( 2004 ) for the Lasso, and by Hastie et al. ( 2004 ) for the support vector machine (SVM). As observed by G  X artner et al. ( 2010 ), all of these examples are in fact particular in-stances of parametric quadratic programming formula-tions, for which path-following algorithms appear early in the optimization literature ( Ritter , 1962 ). In this paper, we study the number of linear segments of the Lasso regularization path. Even though expe-rience with data suggests that this number is linear in the problem size ( Rosset &amp; Zhu , 2007 ), it is known that discrepancies can be observed between worst-case and empirical complexities. This is notably the case for the simplex algorithm ( Dantzig , 1951 ), which per-forms empirically well for solving linear programs even though it suffers from exponential worst-case complex-ity ( Klee &amp; Minty , 1972 ). Similarly, by using geomet-rical tools originally developed to analyze the simplex algorithm, G  X artner et al. ( 2010 ) have shown that the complexity of the SVM regularization path can be ex-ponential. However, to the best of our knowledge, none of these results do apply to the Lasso regulariza-tion path, whose theoretical complexity remains un-known. The goal of our paper is to fill in this gap. Our first contribution is to show that in the worst-case the number of linear segments of the Lasso regulariza-tion path is exactly (3 p +1) / 2, where p is the number of variables (predictors). We remark that our proof is constructive and significantly different than the ones proposed by Klee &amp; Minty ( 1972 ) for the simplex algo-rithm and by G  X artner et al. ( 2010 ) for SVMs. Our ap-proach does not rely on geometry but on an adversarial scheme. Given a Lasso problem with p variables, we show how to build a new problem with p + 1 variables increasing the complexity of the path by a multiplica-tive factor. It results in explicit pathological examples that are surprisingly simple, unlike pathological exam-ples for the simplex algorithm or SVMs.
 Worst-case complexity analyses are by nature pes-simistic. Our second contribution on approximate reg-ularization paths is more optimistic. In fact, we show that an approximate path for the Lasso with at most O (1 / ery point on the path is guaranteed to be optimal up to a relative  X  -duality gap. We follow in part the method-ology of Giesen et al. ( 2010 ) and Jaggi ( 2011 ), who have presented weaker results but in a more general setting for parameterized convex optimization prob-lems. Our analysis builds upon approximate optimal-ity conditions, which we maintain along the path, lead-ing to a practical approximate homotopy algorithm. The paper is organized as follows: Section 2 presents some brief overview of the Lasso. Section 3 is devoted to our worst-case complexity analysis, and Section 4 to our results on approximate regularization paths. In this section, we present the Lasso formulation of Tibshirani ( 1996 ) and well known facts, which we exploit later in our analysis. For self-containedness and clarity reasons we include simple proofs of these results. Let y be a vector in R n and X = [ x 1 , . . . , x be a matrix in R n  X  p . The Lasso is formulated as: where the  X  1 -norm induces sparsity in the solution w and  X  &gt; 0 controls the amount of regularization. Under a few assumptions, which are detailed in the sequel, the solution of this problem is unique. We denote it by w  X  (  X  ) and define the regularization path P as the set of all solutions for all positive values of  X  : 1 The following lemma presents classical optimality and uniqueness conditions for the Lasso solution (see Fuchs , 2005 ), which are useful to characterize P : Lemma 1 ( Optimality Conditions of the Lasso). A vector w  X  in R p is a solution of Eq. ( 1 ) if and only if for all j in { 1 , . . . , p } , Assuming the matrix X J = [ x j ] j  X  J to be full rank, the solution is unique and we have where  X  , sign( X  X  ( y  X  Xw  X  )) is in { X  1; 0; +1 } p , and the notation u J for a vector u denotes the vector of size | J | recording the entries of u indexed by J . Proof. Eq. ( 2 ) can be obtained by considering sub-gradient optimality conditions. These can be writ-ten as 0  X  { X  X  X  ( y  X  Xw  X  ) +  X  p : p  X   X  k w  X  k 1 } , where  X  k w  X  k 1 denotes the subdifferential of the  X  1 -norm at w  X  . A classical result ( Borwein &amp; Lewis , 2006 ) says that the subgradients p are the vectors in R p such that for all j in { 1 , . . . , p } , p j = sign( w if w  X  j 6 = 0, and | p j | X  1 otherwise. This gives Eq. ( 2 ). The equalities in Eq. ( 2 ) define a linear system that has a unique solution given by ( 3 ) when X J is full rank. Let us now show the uniqueness of the Lasso solution. Consider another solution w  X   X  and choose a scalar  X  in (0 , 1). By convexity, w  X  X  ,  X  w  X  + (1  X   X  ) w  X   X  is also a solution. For all j /  X  J , we have | x j  X  ( y  X  Xw  X  X  bining this inequality with the conditions ( 2 ), we nec-essarily have w  X  X  also a solution of the following reduced problem: When X J is full rank, the Hessian X  X  J X J is positive definite and this reduced problem is strictly convex. Thus, it admits a unique solution w  X  X  J = w  X  J . It is then easy to conclude that w  X  = w  X  X  = w  X   X  . With the assumption that the matrix X J is always full-rank, we can formally recall a well-known property of the Lasso (see Markowitz , 1952 ; Osborne et al. , 2000 ; Efron et al. , 2004 ) in the following lemma: Lemma 2 ( Piecewise Linearity of the Path).
 Assume that for any  X  &gt; 0 and solution of Eq. ( 1 ) the matrix X J defined in Lemma 1 is full-rank. Then, the regularization path { w  X  (  X  ) :  X  &gt; 0 } is well defined, unique and continuous piecewise linear.
 Proof. The existence/uniqueness of the regularization path was shown in Lemma 1 .
 sparsity patterns. Let us now consider  X  1 &lt;  X  2 such see that the solution w  X  X  ,  X  w  X  (  X  1 ) + (1  X   X  ) w  X  satisfies the optimality conditions of Lemma 1 for  X  =  X  X  1 +(1  X   X  )  X  2 , and that w  X  (  X  X  1 + (1  X   X  )  X  2 )= w This shows that whenever two solutions w  X  (  X  1 ) and w  X  (  X  2 ) have the same signs for  X  1 6 =  X  2 , the regu-larization path between  X  1 and  X  2 is a linear segment. As an important consequence, the number of linear segments of the path is smaller than 3 p , the number of possible sparsity patterns in { X  1 , 0 , 1 } p . The path P is therefore piecewise linear with a finite number of kinks. Moreover, since the function  X   X  w  X  (  X  ) is piecewise linear, it is piecewise continuous and has right and left limits for every  X  &gt; 0. It is easy to show that these limits satisfy the optimality conditions of Eq. ( 2 ). By uniqueness of the Lasso solution, they are equal to w  X  (  X  ) and the function is in fact continuous. Assuming again that X J is always full rank, we can now present in Algorithm 1 the homotopy method ( Osborne et al. , 2000 ; Efron et al. , 2004 ). Algorithm 1 Homotopy Algorithm for the Lasso. 1: Inputs: a vector y in R n ; a matrix X in R n  X  p ; 2: initialization: set  X  to k X  X  y k  X  ; we have 3: set J , { j 0 } such that | x j 0  X  y | =  X  ; 4: while  X  &gt; 0 do 5: Set  X  , sign( X  X  ( y  X  Xw  X  (  X  )); 6: compute the direction of the path: 7: Find the smallest step  X  &gt; 0 such that: 8: replace  X  by  X   X   X  ; record the pair (  X , w  X  (  X  )); 9: end while 10: Return: sequence of recorded values (  X , w  X  (  X  )). It can be shown that this algorithm maintains the opti-mality conditions of Lemma 1 when  X  decreases. Two assumptions have nevertheless to be made for the al-gorithm to be correct. First, ( X T J X J ) has to be in-vertible, which is a reasonable assumption commonly made when working with real data and when one is in-terested in sparse solutions. When ( X T J X J ) becomes ill-conditioned, which may typically occur for small values of  X  , the algorithm has to stop and the path is truncated. Second, one assumes in Step 7 of the algo-rithm that the value  X  corresponds to a single event | x j  X  ( y  X  Xw  X  (  X   X   X  )) | =  X   X   X  for j in J  X  or w  X  zero for j in J . In other words, variables enter or exit the path one at a time. Even though this assumption is reasonable most of the time, it can be problematic from a numerical point of view in rare cases. When the length of a linear segment of P is smaller than the numerical precision, the algorithm can fail. In con-trast, our approximate homotopy algorithm presented in Section 4 is robust to this issue. In the next sec-tion, we present our worst-case complexity analysis of the regularization path, showing that Algorithm 1 can have exponential complexity. We denote by {  X   X  (  X  ) , sign( w  X  (  X  )) :  X  &gt; 0 } the set of sparsity patterns in { X  1 , 0 , 1 } p encountered along the path P . We have seen in the proof of Lemma 2  X  (  X  ) =  X   X  (  X  1 ) for all  X   X  [  X  1 ,  X  2 ], and thus the num-ber of linear segments of P is upper-bounded by 3 p . With an additional argument, we can further reduce this number, as stated in the following proposition: Proposition 1 ( Upper-bound Complexity).
 Let assume the same conditions as in Lemma 2 . The number of linear segments in the regularization path of the Lasso is less than (3 p + 1) / 2 .
 Proof. We have already noticed that the number of lin-ear segments of the path is at most 3 p . Let us consider  X  (  X  1 ) 6 =0 for  X  1 &gt; 0. We now show that for all  X  2 we have  X   X  (  X  2 ) 6 =  X   X   X  (  X  1 ), and therefore the number of different sparsity patterns on the path P is in fact less than or equal to (3 p +1) / 2.
 Let us assume that there exists  X  2 &gt; 0 with  X   X  (  X  2  X   X   X  (  X  1 ), and look for a contradiction. We define the the solution of the reduced problem for all  X   X  0: which is well defined since the optimization problem is strictly convex (the conditions of Lemma 2 imply that X J  X  is full rank). We remark that  X w  X  (  X  1 ) = w ity conditions of Lemma 1 , it is then easy to show are opposite to each other and non-zero, we have to show that the function  X   X  k  X w  X  (  X  ) k 1 should be non-increasing, and we obtain a contradiction. In the next proposition, we present our adversarial strategy to build a pathological regularization path. Given a Lasso problem with p variables and a path P , we design an additional variable along with an extra dimension, such that the number of kinks of the new path  X  P increases by a multiplicative factor compared to P . We call our strategy  X  X dversarial X  since it con-sists of iteratively designing  X  X athological X  variables. Proposition 2 ( Adversarial Strategy).
 Let us consider y in R n and X in R n  X  p such that the conditions of Lemma 2 are satisfied and y is in the span of X . We denote by P the regularization path of the Lasso problem corresponding to ( y , X ) , by k the number of linear segments of P , and by  X  1 &gt; 0 the smallest value of the parameter  X  corresponding to a kink of P . We define the vector  X y in R n +1 and the where y n +1 6 = 0 and 0 &lt;  X  &lt;  X  1 / (2 y  X  y + y 2 Then, the regularization path  X  P of the Lasso problem associated to (  X y ,  X  X ) exists and has 3 k  X  1 linear seg-ments. Moreover, let us consider {  X  1 = 0 ,  X  2 , . . . , the sequence of sparsity patterns in { X  1 , 0 , 1 } p of P (the signs of the solutions w  X  (  X  ) ), ordered from large to small values of  X  . The sequence of sparsity patterns in { X  1 , 0 , 1 } p +1 of the new path  X  P is the following: ( z }| { Let us first make some remarks about this proposition: new path  X  P are related to those of P . More precisely, they have either the form [  X  i  X  , 0]  X  or [  X   X  i  X  , 1] where  X  i is a sparsity pattern in { X  1 , 0 , 1 } p of P . controls its norm. With  X  small enough, the ( p +1)-th variable enters late the path  X  P . As shown in Eq. ( 4 ), the first k sparsity patterns of  X  P do not involve this variable and are exactly the same as those of P . behavior of the path  X  P . The first k kinks of  X  P are the same as those of P , and after these first k kinks we have y  X  Xw  X  (  X  ). Then, the ( p +1)-th variable enters the path and we heuristically have The left side of Eq. ( 5 ) tells us that when the ( p +1)-th variable is inactive, the coefficients associated to the first p variables should be close to w  X  (  X  ). At the same time, the right side of Eq. ( 5 ) tells us that when the ( p +1)-th variable is active, these same p coeffi-cients should be instead close to  X  w  X  (  X  ). According to Eq. ( 4 ), the signs of these p coefficients along the path switch from  X  k =sign( w  X  (  X  )) to  X   X  k by following the sequence  X  k ,  X  k  X  1 , . . . , (  X  1 = 0 =  X   X  1 ) ,  X   X  resulting in a path with 3 k  X  1 linear segments. The proof below more rigorously describes this strategy: Proof. Existence of the new regularization path : Let us rewrite the Lasso problem for (  X y ,  X  X ). = min Let (  X w  X  ,  X  w  X  ) be a solution for a given  X  &gt; 0. By fixing  X  w =  X  w  X  in Eq. ( 6 ) and optimizing with respect to  X w , we obtain an equivalent problem to ( 6 ): with the change of variable  X w = (1  X  2  X   X  w  X  )  X w  X  assuming 1  X  2  X   X  w  X  6 = 0. The solution of this problem is unique since it is a point of P and we therefore have  X w  X  = Since the last column of  X  X is not in the span of the first p columns by construction of  X  X , it is then easy to see that the conditions of Lemma 2 are necessarily satisfied and therefore (  X w  X  ,  X  w  X  ) is in fact the unique solution of Eq. ( 6 ). Since this is true for all  X  &gt; 0, the regularization path is well defined, and we denote from now on the above solutions by  X w  X  (  X  ) and  X  w  X  Maximum number of linear segments : We now show that the number of linear segments of the path is upper-bounded by 3 k  X  1. Eq. ( 7 ) shows that is one of the k sparsity patterns from P , whereas we have three possibilities for sign(  X  w  X  (  X  )), namely { X  1 , 0 , +1 } . Since one can not have two non-zero spar-sity patterns that are opposite to each other on the same path, as shown in the proof of Proposition 1 , the number of possible sparsity patterns reduces to 3 k  X  1. Characterization of the first k linear segments : Let us consider  X   X   X  1 and show that  X w  X  (  X  ) = w  X  (  X  ) and  X  w  X  (  X  )=0 by checking the optimality conditions of Lemma 1 . The first p equalities/inequalities in Eq. ( 2 ) are easy to verify, the last one being also satisfied: | 2  X  y  X  ( y  X  Xw  X  (  X  ))+  X y 2 n +1 | X  2  X  k y k 2 2 +  X y where the last inequality is obtained from the defi-nition of  X  . Since this inequality is strict, this also ensures that there exists 0 &lt;  X   X  1 &lt;  X  1 such that  X w  X  (  X  ) = w  X  (  X  ) and  X  w  X  (  X  ) = 0 for all  X   X   X   X  have therefore shown that the first k sparsity patterns of the regularization path are given in Eq. ( 4 ). Characterization of the last 2 k  X  1 segments : We mainly use here the form of Eq. ( 7 ) and a few continuity arguments to characterize the rest of the path. First, we remark that for all  X  in [0 , 1  X  ), there exists a value for  X  &gt; 0 such that  X  w  X  (  X  ) =  X  . This is true because: (i)  X   X   X  w  X  (  X  ) is continuous; (ii)  X  w 0; (iii)  X  w  X  (0 + )= 1  X  . Point (i) was shown in Lemma 2 , point (ii) in the previous paragraph, and point (iii) is go to 0 when  X  goes to 0 + .
 We now consider two values  X   X  1 ,  X   X  2 &gt; 0 such that  X  w (  X   X  1 ) = 0,  X  w  X  (  X   X  2 ) = 1 2  X  and  X  w  X  (  X  )  X  (0 , all  X   X  (  X   X  1 ,  X   X  2 ). On this open interval, we have that (1  X  2  X   X  w  X  (  X  )) &gt; 0, and the continuous function  X   X  this observation with Eq. ( 7 ), we obtain that all spar-appear on the regularization path. With similar con-tinuity arguments, it is easy to show that all sparsity pear on the path as well.
 We had previously identified k of the sparsity patterns, and now have identified 2 k  X  1 different ones. Since we have at most 3 k  X  1 linear segments, the set of sparsity patterns on the path  X  P is entirely characterized. The fact that the sequence of sparsity patterns is the one given in Eq. ( 4 ) can easily be shown by reusing similar continuity arguments.
 With this proposition in hand, we can now state the main result of this section: Theorem 1 ( Worst-case Complexity).
 In the worst case, the regularization path of the Lasso has exactly (3 p + 1) / 2 linear segments.
 Proof. We start with n = p = 1, and define y = [1], and X = [1], leading to a path with k = 2 segments. We then recursively apply Proposition 2 , keeping n = p , choosing at iteration p + 1, y p +1 = 1, and a fac-tor  X  =  X  p +1 satisfying the conditions of Proposition 2 . Denoting by k p the number of linear segments at iter-ation p , we have that k p +1 = 3 k p  X  1, and it is easy to show that k p =(3 p +1) / 2. According to Proposition 1 , this is the longest possible regularization path. Note that this example has a particularly simple shape: y , 3.1. Numerical Simulations We have implemented Algorithm 1 in Matlab, opti-mizing numerical precision regardless of computational efficiency, which has allowed us to check our theoreti-cal results for small values of p . For instance, we ob-tain a path with (3 p + 1) / 2 = 88 574 linear segments for p = 11, and present such a pathological path in Figure 1 . Note that when p gets larger, these exam-ples quickly lead to precision issues where some kinks are very close to each other. Our implementation and our pathological examples will be made publicly avail-able. In the next section, we present more optimistic results on approximate regularization paths. We now present another complexity analysis when ex-act solutions of Eq. ( 1 ) are not required. We follow in part the methodology of Giesen et al. ( 2010 ), later re-fined by Jaggi ( 2011 ), on approximate regularization paths of parameterized convex functions. Their re-sults are quite general but, as we show later, we obtain stronger results with an analysis tailored to the Lasso. A natural tool to guarantee the quality of approximate solutions is the duality gap. Writing the Lagrangian of problem ( 1 ) and minimizing with respect to the primal variable w yields the following dual formulation of ( 1 ): where  X  in R n is a dual variable. Let us denote by f  X  ( w ) the objective function of the primal prob-lem ( 1 ) and by g  X  (  X  ) the objective function of the dual ( 8 ). Given a pair of feasible primal and dual vari-ables ( w ,  X  ), the difference  X   X  ( w ,  X  ) , f  X  ( w )  X  g is called a duality gap and provides an optimality guar-antee (see Borwein &amp; Lewis , 2006 ): In plain words, it upper bounds the difference between the current value of the objective function f  X  ( w ) and the optimal value of the objective function f  X  ( w  X  (  X  )). In this paper, we use a relative duality gap criterion to guarantee the quality of an approximate solution: 3 Definition 1 (  X  -approximate Solution). let  X  be in [0 , 1] . A vector w in R p is said to be an  X  -approximate solution of problem ( 1 ) if there exists  X  in R n such that k X  X   X  k  X   X   X  and  X   X  ( w ,  X  )  X   X f  X  Given a set  X  P , {  X w (  X  )  X  R p :  X  &gt; 0 } , we say that an  X  -approximate regularization path if any point  X w (  X  ) of  X  P is an  X  -approximate solution for problem ( 1 ). Our goal is now to build  X  -approximate regularization paths and study their complexity. To that effect, we introduce approximate optimality conditions based on small perturbations of those given in Lemma 1 : Definition 2 ( OP T  X  (  X  1 ,  X  2 ) Condition). Let  X  1  X  0 and  X  2  X  X  X   X  1 . A vector w in R p satisfies the OP T  X  (  X  1 ,  X  2 ) condition if and only if for all 1  X  j  X  p ,  X  (1  X   X  2 )  X  x j  X  ( y  X  Xw ) sign( w j )  X   X  (1+  X  1 ) if w Note that when  X  1 =  X  2 = 0, this condition reduces to the exact optimality conditions of Lemma 1 . Of inter-est for us is the relation between Definitions 1 and 2 . Let us consider a vector w such that OP T  X  (  X  1 ,  X  2 ) is satisfied. Then, the vector  X  , 1 1+  X  ble for the dual ( 8 ) and we can compute a duality gap:  X  ( w ,  X  ) = f  X  ( w )  X  g  X  (  X  ) From Eq. ( 9 ), it is easy to show that  X  k w k 1 +  X   X  Xw  X  1+  X  1  X  k w k 1 , and we can obtain the following bound: From this upper bound, we derive our first result: Proposition 3 ( Approximate Analysis).
 Let y be in R n and X in R n  X  p such that the condi-tions of Lemma 2 are satisfied. Let  X   X  , k X  X  y k  X  be the value of  X  corresponding to the start of the path, and  X  1 &gt; 0 be the one corresponding to the last kink. For all  X   X  (0 , 1) , there exists an  X  -approximate regular-ization path with at most Proof. From Eq. ( 9 ), one can show by a simple cal-culation that an exact solution w  X  (  X  ) for a given  X  ing to Eq. ( 10 ), there exists a dual variable  X  such in [  X ,  X  (1  X  solution for the parameter  X   X  . Between  X   X  and  X  1 , we can obtain an  X  -approximate piecewise linear (in fact piecewise constant) regularization path by sampling solutions w  X  (  X  ) for  X  in {  X   X  ,  X   X  (1  X   X   X  ) k ,  X  1 } with  X   X  (1  X  segments of the corresponding approximate path is at most Note that the term  X   X  / X  1 is possibly large, but it is controlled by a logarithmic function and can be con-sidered as constant for finite precision machines. In other words, the complexity of the approximate path is upper-bounded by O (1 / ysis of Giesen et al. ( 2010 ) and Jaggi ( 2011 ) give us: can be obtained with a weaker approximation guaran-tee than ours. Namely, a bound  X   X   X  along the path, where  X  is a duality gap, whereas we use relative du-ality gaps of the form  X   X   X f  X  ( w ); 4 Interestingly, this bound is proven to be optimal in the context of param-eterized convex functions on the  X  1 -ball. Our result show that such bound can be improved for the Lasso. along the path, which can easily provide complexity bounds for the full path of different problems, notably support vector machines, but not for the Lasso. Proposition 3 is optimistic, but not practical since it requires sampling exact solutions of the path P . We introduce an approximate homotopy method in Algo-rithm 2 which does not require computing exact solu-tions and still enjoys a similar complexity. It exploits the piecewise linearity of the path, but uses a first-order method ( Beck &amp; Teboulle , 2009 ; Fu , 1998 ) when the linear segments of the path are too short. Algorithm 2 Approximate Homotopy for the Lasso. 1: Inputs: a vector y in R n , a matrix X in R n  X  p , 2: initialization: set  X  to k X  X  y k  X  ; set  X w (  X  ) = 0; 3: set  X  = 1 +  X / 2  X  4: set J , { j 0 } such that | x j 0  X  y | =  X  ; 5: while  X   X   X  1 do 6: if ( X  X  J X J ) is not invertible then go to 12 ; 7: set  X   X  , (1 / X  ) X  X  ( y  X  X  X w (  X  )); 8: compute the approximate direction of the path: 9: if  X   X   X  X  10: replace  X  by  X   X   X  ; 11: else 12: replace  X  by  X  (1  X   X  13: use a first-order optimization method to find 14: set J = { j  X  X  1 , . . . , p } :  X w j (  X  ) 6 = 0 } . 15: end if 16: record the pair (  X ,  X w (  X  )); 17: end while 18: Return: sequence of recorded values (  X ,  X w (  X  )). Note that when  X  = 0, Algorithm 2 reduces to Algo-rithm 1 . Our approach exploits the following ideas, which we formally prove in the sequel. Assume that  X w (  X  ) satisfies OP T  X  (  X / 2 ,  X / 2). Then,  X   X  step sizes for  X  greater than or equal to  X  X  OP T  X  (  X / 2 ,  X / 2), but when two kinks are too close to each other X  X hat is,  X  &lt;  X  X  solution for the parameter  X   X  =  X  (1  X   X  isfies OP T  X   X  (  X / 2 ,  X / 2). Any first-order method can be used for that purpose, e.g., a proximal gradient method ( Beck &amp; Teboulle , 2009 ), using the current value  X w (  X  ) as a warm start.
 Note also that when ( X  X  J X J ) is not invertible, the method uses first-order steps. The next proposition precisely describes the guarantees of our algorithm. Proposition 4 ( Analysis of Algorithm 2 ).
 Let y be in R n and X in R n  X  p . For all  X  1 &gt; 0 and  X   X  (0 , 1) , Algorithm 2 returns an  X  -approximate regular-ization path on [  X   X  ,  X  1 ] . Moreover, it terminates in at most Proof. We first show that any solution on the path is an  X  -approximate solution. First, it is easy to check that OP T  X  (  X / 2 ,  X / 2) is always satisfied at Step 6 . This is either a consequence of Step 13 , or be-cause the direction  X w J (  X   X  ) = ( X  X  J X J )  X  1 ( X maintains OP T  X   X  (  X / 2 ,  X / 2) when  X   X  varies between  X  and  X   X   X  . From Eq. ( 10 ), we obtain that  X w (  X  ) is an  X  -approximate solution whenever OP T  X  (  X / 2 ,  X / 2) is sat-isfied. Thus, we only need to check that  X w (  X  ) is also an  X  -approximate solution for  X   X  in [  X ,  X  (1  X   X  for  X  3  X  0, it is easy to check that OP T  X  (  X / 2 ,  X / 2) im-Setting  X  3 =  X  show that the desired condition is satisfied.
 Since the step size for  X  is always greater than  X  X  the maximum number of iterations is upper-bounded by We remark that the scalar  X  is very close to 1 and therefore the complexity is similar to the one of Propo-sition 3 , with a logarithmic function controlling the possibly large term  X   X  / X  1 . This algorithm is practi-cal in different aspects: (i) it is almost as simple to implement as the homotopy method; (ii) it is robust to cases where two kinks are too close for the classical homotopy method to work; (iii) it provides optimality guarantees along the path; (iv) whenever possible, it explicitly exploits the piecewise linearity of the path. We next present experiments to verify our analysis. 4.1. Numerical Simulations We have implemented Algorithm 2 with a few mod-ifications to the code used in Section 3.1 . The inner solver is a coordinate descent algorithm (see Fu , 1998 ), with a stopping criterion based on Definition 2 . We consider 4 datasets. The first one dubbed SYNTH consists of a pure noise fitting scenario with no statisti-cal meaning. The entries of the corresponding vector y and matrix X are i.i.d. draws from a standard normal distribution. The next dataset is called PATHOL and is a pathological example obtained from the analysis of Section 3 . Finally, we consider two datasets based on real data, respectively dubbed MADELON 5 and PC-MAC 6 . For each dataset, we center and normalize the columns of X and the vector y , and choose the param-eter  X  1 corresponding to the last kink of the true path. For all datasets, we compute the full regularization path using Algorithm 1 and several  X  -approximate regularization paths using Algorithm 2 . Note that the path of PCMAC was stopped around  X   X  10  X  4 where the matrix X  X  J X J became ill-conditioned and the Lasso solution dense. As a simple sanity check, we first experimentally verify the correctness of Proposi-tions 3 and 4 , by sampling solutions on the approx-imate path we obtain, computing duality gaps, and checking that the solutions are indeed  X  -approximate. We conclude that our experimental results match our theoretical analysis. We present the different path complexities in Table 1 .
 Interestingly, the complexity of the pathological ex-ample significantly reduces when one is looking for an approximate solution. For example, for  X  = 10  X  3 , the complexity of the approximate path is less than 0 . 5% the one of the full path. This significantly contrasts with the pessimistic result obtained in Section 3 . As expected, the two examples based on real data exhibit a path complexity of the same order of the problem size, which also significantly reduces when  X  increases. We have presented new results on the regularization path and thus on homotopy methods for the Lasso. First, we have shown that the path has an exponen-tial worst-case complexity, which, as far as we know, had never been formally proved before. Our second re-sult is more optimistic, and shows that when an exact path is not required, only a relatively small number of points on the path need to be computed. Finally, we propose a practical approximate homotopy algorithm, which can provide such approximate paths at a desired precision.
 This paper was supported in part by NSF grants SES-0835531, CCF-0939370, DMS-1107000, DMS-0907632, and by ARO-W911NF-11-1-0114.
 Beck, A. and Teboulle, M. A fast iterative shrinkage-thresholding algorithm for linear inverse problems. SIAM J. Imaging Sci. , 2(1):183 X 202, 2009.
 Borwein, J. M. and Lewis, A. S. Convex analysis and nonlinear optimization: theory and examples . Springer, 2006.
 Dantzig, G. B. Maximization of a linear function of variables subject to linear inequalities. In Koop-mans, T .C. (ed.), Activity Analysis of Production and Allocation , pp. 339 X 347. Wiley, New York, 1951. Efron, B., Hastie, T., Johnstone, I., and Tibshirani,
R. Least angle regression. Ann. Stat. , 32(2):407 X  499, 2004.
 Fu, W. J. Penalized regressions: The bridge versus the Lasso. J. Comput. Graph. Stat. , 7(3):397 X 416, 1998. Fuchs, J. J. Recovery of exact sparse representations in the presence of bounded noise. IEEE T. Inform. Theory. , 51(10):3601 X 3608, 2005.
 G  X artner, B., Jaggi, M., and Maria, C. An exponen-tial lower bound on the complexity of regularization paths. preprint arXiv:0903.4817v2 , 2010.
 Giesen, J., Jaggi, M., and Laue, S. Approximating pa-rameterized convex optimization problems. In Algo-rithms -ESA , Lectures Notes Comp. Sci. 2010. Hastie, T., Rosset, S., Tibshirani, R., and Zhu, J. The entire regularization path for the support vector ma-chine. J. Mach. Learn. Res. , 5:1391 X 1415, 2004. Jaggi, M. Sparse Convex Optimization Methods for Machine Learning . PhD thesis, ETH Z  X urich, 2011. Klee, V. and Minty, G. J. How good is the simplex algorithm? In Shisha, O. (ed.), Inequalities , volume III, pp. 159 X 175. Academic Press, New York, 1972. Markowitz, H. Portfolio selection. J. Financ. , 7(1): 77 X 91, 1952.
 Osborne, M., Presnell, B., and Turlach, B. A new approach to variable selection in least squares prob-lems. IMA J. Numer. Anal. , 20(3):389 X 403, 2000. Ritter, K. Ein verfahren zur l  X osung param-eterabh  X angiger, nichtlinearer maximum-probleme. Math. Method Oper. Res. , 6(4):149 X 166, 1962.
 Rosset, S. and Zhu, J. Piecewise linear regularized solution paths. Ann. Stat. , 35(3):1012 X 1030, 2007. Tibshirani, R. Regression shrinkage and selection via
