 Abstract The Rovereto Emotion and Cooperation Corpus (RECC) is a new resource collected to investigate the relationship between cooperation and emotions in an interactive setting. Previous attempts at collecting corpora to study emotions have shown that this data are often quite difficult to classify and analyse, and coding schemes to analyse emotions are often found not to be reliable. We collected a corpus of task-oriented (MapTask-style) dialogues in Italian, in which the segments of emotional interest are identified using psycho-physiological indexes (Heart Rate and Galvanic Skin Conductance) which are highly reliable. We then annotated these segments in accordance with novel multimodal annotation schemes for cooperation (in terms of effort) and facial expressions (an indicator of emotional state). High agreement was obtained among coders on all the features. The RECC corpus is to our knowledge the first resource with psycho-physiological data aligned with verbal and nonverbal behaviour data.
 Keywords Cooperation Emotions Multimodal Dialogue Annotation 1 Introduction The interpretation of emotional subjective experience has raised a great interest in the last decade. Scientists have come to realize that emotions are important predictors of (non) cooperation. Researchers showed that when respondents were treated unfairly, they felt not just anger, but sadness, irritation, and contempt (Sanfey et al. 2003 ; Xiao and Houser 2005 ). Those emotions are reflected in facial expressions. Many studies claimed that cooperators can be identified by honest and non falsifiable signals, allowing for mutual selection among cooperators (Dawkins 1976 ; Frank 1988 ; Hamilton 1964 ; Trivers 1971 ). It has been proposed that the display of spontaneous positive emotion, also known as  X  X  X uchenne X  X  smiles, 1 can serve as a relatively honest signal of positive subjective experience (Ekman and Friesen 1982 ; Frank and Ekman 1993 ). The findings suggested that cooperative individuals display higher levels of positive emotions than non cooperators.
These findings have had a significant impact on Human X  X omputer Interaction research, as well X  X t is widely expected that next-generation interfaces will need to understand and emulate behavioral cues such as affective and social signals (Bianchi and Lisetti 2002 ). Picard ( 1997 ) suggested several applications where it is beneficial for computers to recognize human affective states. Decoding the users X  emotions, a computer can become a more effective tutor and could learn the user X  X  preferences. A system able to sense the cooperative or non cooperative stance of a human will be capable of adapting and responding to these affective states. As a consequence that system will be perceived as more natural, efficacious, and trustworthy.

As a result, many multimodal corpora have been collected in several languages and with different eliciting methods with the goal of shedding light on key aspects of the role of emotions in verbal and non-verbal interaction. Some of the research questions addressed in this work are how the expression of emotions modifies speech (Magno Caldognetto et al. 2004 ) and gesture (Poggi and Vincze 2008 )or more generally the relationship among dialogue acts, topic segmentation and the so called  X  X  X motional X  X  or  X  X  X ocial X  X  area of communication (Carletta 2007 ; Pianesi et al. 2006 ). This substantial corpus collection effort has, however, raised many questions about the reliability of the coding schemes used in this work. The aim of testing a coding scheme reliability is to assess whether the scheme features are sufficiently shared by a group of annotators; and in many cases, the features chosen to annotate emotions do not seem to be entirely reliable (for a review and discussion of the problem, see Cavicchio and Poesio 2008 ).
 In this paper we present a new multimodal resource, the Rovereto Emotive and Cooperation Corpus (RECC). The aim of RECC is twofold. First of all, RECC is a task-oriented corpus collected to investigate the relationship between (non) cooperation and emotions. Secondly, RECC was collected with the intent of overcoming the limitations of current coding schemes for emotion. In RECC, psycho-physiological data were recorded and aligned with audiovisual data X  X o our knowledge, RECC is the first resource in which audiovisual and psycho-physiological data are recorded and aligned together in this way. This alignment enables the identification of segments of potential emotional interest in a reliable way. Cooperation was then annotated in terms of effort. Instead of annotating emotion directly, emotive facial expressions were annotated as a surface indicator that might provide another cue to the emotional state. Both the coding scheme for cooperation and that for facial expressions were found to be reliable; and both cooperation and facial expressions were found to be predictive of cooperation.
The structure of this article is as follows. In Sect. 2 we provide background on cooperation, emotion, and their annotation. In Sect. 3 we discuss the design of the corpus. In Sect. 4 we discuss corpus transcription and validation. Section 5 is about the distribution of the corpus, and finally Sect. 6 contains some conclusions. 2 Background 2.1 Emotion and cooperation It is one of the key tenets of modern pragmatics that dialogue is governed by the Cooperative Principle 2 proposed by Grice ( 1975 ), further broken down in the four Maxims of Quantity, Quality, Relation and Manner. These maxims were the starting point for Clark X  X  empirical investigations of cooperation in dialogue that led to the extremely influential view of interaction as joint action formulated e.g., in (Clark 1996 ). It is also widely accepted, however, that an agent X  X  degree of cooperation is modulated by a number of factors.

Emotions have been found to be important predictors of cooperation and non-cooperation in game-theoretic studies. For example, Pillutla and Murnighan ( 1996 ) found that the anger induced by unfair offers in a group of respondents was positively correlated with the tendency to not cooperate. Other researchers showed that when respondents were treated unfairly, they felt not just anger, but sadness, irritation, and contempt (Bosnam et al. 2001). Many economic games studies focused on facial expressions of emotion (Brown and Moore 2002 ; Krumhuber et al. 2007 ; Mehu et al. 2007 ; Oda et al. 2009 ; Scharlemann et al. 2001 ; Brown et al. 2003 ; Matsumoto et al. 1986 ). Unlike these studies, the aim of the present research was to study the effect of emotion on cooperation and non cooperation in unscripted, ecological communication. 2.2 Annotating emotions One difficulty in the study of the role of emotion in interaction is that annotating emotion has proven a difficult task. In the literature we can find two types of emotion coding schemes. The first group of proposals derive from Craggs and Woods X  work ( 2004 ). Craggs and Woods X  annotators were asked to label a given emotion with a main emotive term (e. g. anger, sadness, joy etc.) correcting the emotional state with a score ranging from 1 (low) to 5 (very high). A second group of proposals focused instead on the annotation of the valence of the emotions X  positive, neutral and negative (Martin et al. 2006 ; Callejas and Lopez-Cozar 2008 ; Devillers et al. 2005 ). Both these approaches, however, resulted in poor agreement among coders 3 (from 0.46 to 0.68 in Craggs and Wood 2004 ; from 0.36 to 0.56 in Callejas and Lopez-Cozar 2008 ; from 0.46 to 0.63 in Devillers et al. 2005 ). Cowie et al. ( 2000 ) proposed Feeltrace, a labelling tool for annotation of vocal expression and music. This software is based on the coding of two emotive dimensions: activation and evaluation. It has been shown that Feeltrace is fine grained enough to distinguish between neutral state and emotive states, although it is not enough sensitive to distinguish between anger and fear.

Given that annotating emotions directly is problematic, a plausible alternative is to record surface signals that may provide clues to emotional state instead. We recorded two such signals in RECC: psycho-physiological patterns (heart rate and skin conductance recorded trough a surface sensor system) and facial expressions. The psycho-physiological pattern of emotions had been widely investigated (see for example Wagner et al. 2005 ; Villon and Lisetti 2007 ). The available sensors do not provide a direct indication of the emotional state being experienced; however, Villon and Lisetti found out that heart rate could be used as an indicator of valence, whereas skin conductance could be used as an indicator of arousal.

Current corpora and datasets focused on the study of verbal and non verbal aspects of emotions often deal with a limited number of stereotypical emotive facial expressions. The first example of a database focused on facial expression of emotions is the collection of pictures by Ekman and Friesen ( 1975 ), which is based on an early version of the basic emotion theory. Most facial expression recognition research (see Pantic and Rothkrantz 2003 ; Fasel and Luettin 2003 for two comprehensive reviews) has been inspired by the Ekman and Friesen X  X  work on Action Units (AUs), a set of facial features proposed to code facial expressions. Ekman and Friesen X  X  FACS (Facial Action Coding System; Ekman and Friesen 1978 ) was the first coding system for facial expression. This system is extremely fine grained in analyzing every movement of the face. As regards FACS reliability, an inter-annotators X  agreement experiment investigated using pairwise agreement the reliability of FACS. It assured a good inter coder agreement (ranging between 0.822 and 0.863). Nevertheless, pairwise agreement is not a method accounting for chance agreement. Hoque et al. ( 2009 ), did on FACS an inter coder reliability study using kappa. They found low agreement coefficient and suggested to focus on a set of AUs to detect the most discriminative and least discriminative AUs for the relevant affective states. Based on observation and manual coding of the data, AUs related to lip movement and eye/lid/brow movement were found to be more relevant to detect the affective states. 2.3 Davies X  coding scheme for cooperation It has been proven remarkably difficult to find robust and reliable coding schemes for cooperation, as well. The best known proposal in this respect is by Davies ( 1998 ).
The basis for Davies X  proposal, formulated most clearly in a later paper (Davies 2006 ), is the hypothesis that adherence to Grice X  X  Cooperative Principle can be measured in terms of the effort expended by the participants to an interaction. The coding scheme that Davies derives from this hypothesis (Davies 1998 ) aims at measuring the participants X  effort while performing a given dialogue move. Her coding scheme analyzes instructions, response and follow ups to the introduction of a new game landmark. 4 The coding scheme accounts for the way a landmark is introduced (e.g. highlighted or given as known with respect to previous utterances) or whether a move pointed out a mismatch in the knowledge shared by the two speakers. In particular, Davies X  coding schemes tries to distinguish between the levels of effort that participants embark on their utterances. This was reflected in a weighting system that took into account the effort invested by each speaker in each utterance. The use of this system provided a positive or negative score for each dialogue move with respect to the effort invested. For example, when an instance of a particular behavior was found, a positive coding was attributed with respect to the effort level. Instead, a negative coding was attributed when an instance where a particular behavior should have been used is not found. The sum of all the dialogue scores were considered as an account for the total effort invested in the dialogue by each speaker. Reliability tests run on Davies X  coding scheme (Davies 1998 ) had Kappa scores ranging from 0.69 to 1.0. Despite that, Davies remarked that the coder agreement was not significant for some of the features because they were seldom present in the corpus. 3 Corpus design RECC consists of 21 interactions, 14 with a confederate, for a total of 280 min of audiovisual and psycho-physiological recordings (heart rate and the skin conduc-tance). The psycho-physiological response was recorded and synchronized with video and audio recordings. The psycho-physiological data were recorded with a BIOPAC MP150 system. The face to face interactions were recorded with 2 Canon VC-C50i Digital Cameras and 2 free field Sennheiser half-cardioid microphones with permanently polarized condenser placed in front of each speaker.

We elicited unscripted yet controlled conversations using the Map task (Anderson et al. 1991 ). In the Map Task two participants, the Giver and the Follower, have in front of them a map with some features. The maps were not identical. The Giver task was to drive the other participant, the Follower, from a starting point (the bus station) to the finish (the Castle of Rovereto). The features in between the start and the finish were in different positions and had a different name (see Fig. 1 ). We decided not to advice the participants about those differences. Therefore, it was up to them to discover the differences in the two maps. The Giver and the Follower were both native Italian speakers and they did not know each other before the task. As in the HCRC Map Task, our corpus interactions have two conditions: full screen and short screen. In the full screen condition there was a barrier between the two speakers. In the short screen condition a short barrier was placed between the speakers allowing Giver and Follower to see each others X  face. Screen condition was counterbalanced.
 The key feature of RECC is the addition of a new condition, emotion elicitation . The recording procedure of RECC was influenced by the work of Anderson, Linden and Habra ( 2005 ). In this work, the participants performed a numerical task. During the task, their heart rate, skin conductance and temperature were measured. A confederate at given time points interrupted the task and gave negative feedbacks on their task performance. The aim was to elicit negative emotions such as anger and/or frustration.

In emotion elicitation condition the Follower or the Giver can alternatively be a confederate, with the aim of getting the other participant angry. 5 14 Italian native speakers (average age = 28.6, dv = 4.36) matched with a confederate partner were recorded. The recordings were performed as follows. Before the task we recorded the psycho-physiological baseline of each participant for five minutes. Then we recorded the first 3 min of the psycho-physiological outputs from the beginning of the task, which we called task condition . Here the speaker was not challenged by the confederate. After that the confederate (the same person in all the interactions) performed uncooperative utterances in carefully controlled circumstances by acting negative emotion elicitation lines at minutes 4, 9 and 14 of the interaction. The following lines were given by the confederate when acting the Follower role:  X  You are sending me in the wrong direction, try to be more accurate!;  X  It X  X  still wrong, you are not doing your best, try harder! Again, from where you  X  You X  X e obviously not good enough at giving instructions.

A control group of 7 pairs of participants (average age = 32.16, dv = 2.9) were also recorded while playing the Map Task with the same maps but without confederate (control condition). Eye contact, communicative role (Giver and Follower) and gender (male or female) were counterbalanced.

Our hypothesis was that the confederate X  X  uncooperative utterances would lead to a reduced level of cooperation in the other participant. To test it, we first checked if the eliciting protocol adopted caused a change in participants X  heart rate and skin conductance. A one way within subjects ANOVA was performed in confederate condition. The Heart Rate (HR) was confronted over five time points of interest (baseline, task, after minute 4, after minute 9, after minutes 14), that is to say 19.54) = 125, p &lt; .0001) . Between-group post hoc tests revealed that there was a significant effect of the three sentences with respect to the task condition. As regards skin conductance values, there was a linear increase of the number of positive peaks of conductance over time. The 14 participants to this session completed a subjective valence ratings on a 7.5 cm visual analogue emotion rating scale form (adapted from Bradley and Lang 1994 ). 12 of them rated the experience in the negative range. As regards the control group, in addition to a baseline measurement, the HR was measured over 4 times at equal intervals dung the interaction. A one way within subjects ANOVA showed the effect of Time was non-significant ( F (2.1, 27) = 1.9, p &lt; .16) . Considering the skin conductance values, the number of positive peaks over time had a constant rate. 4 Corpus transcription and annotation The givers and the followers X  conversational turns were transcribed and aligned with the annotation of cooperation, facial expression (upper and lower face configuration), turn taking and gaze direction. All the videos were annotated using ANViL (ANnotation of Video and Language; Kipp 2001 ). In Table 1 we reported the total number of conversational turns and words transcribed, together with the cooperation, the facial expression (mouth and eyebrows) and the gaze markables segmented so far. 4.1 Transcription method Every conversation was divided into turns related to the Giver and the Follower. In order to make the subsequent processing easier and the form of the transcribed files more uniform, we adopted the conventions of the LUNA corpus (Rodr X   X  guez et al. 2007 ). Two transcribers fixed the problems in each orthographic transcription and run a validation script to find unrecognized spelling and transcription codes. Movements produced by the upper and lower part of the face and gaze were transcribed as well. In the first step, transcribers marked the beginning and the end of each individual action in each video segment. As a second step, the precision of the boundaries was verified. If the beginning or the end of each event had a very large error (&gt;200 ms), the transcribers modified the appropriate event. If too many events had been coded previously, then the transcribers deleted the unnecessary ones. If there was a missing event, they could set it by adding the duration of the new event. 4.2 Corpus annotation The coding scheme we adopted is mainly focused on the annotation of cooperation and facial expressions. The other two indexes we codify are the dialogue turn management and gaze direction. The latter is another important cue to classify turn segments. To validate the coding scheme, six independent coders, all Italian native speakers, annotated a subset of the corpus consisting in 64 conversational turns taken from the confederate and the control recording sessions. For both sessions, the annotated utterances were aligned with the HR measures. A Kappa statistic (Siegel and Castellan 1988 ) was measured on the data annotated by the six coders to assess agreement. 4.3 Coding scheme for cooperation The cooperation features we adopted are inspired by Davies X  ( 1998 ), though some substantial modifications have been carried out. In Davies X  coding scheme negative codings are scored when a particular dialogue behavior that should have been used is absent. We realized that attributing negative codings to the absence of felicitous dialogue acts was too much challenging for coders without a specific training. Our cooperation typology is similar to the HCRC Map Task coding scheme. In our coding scheme we used check , question answering and giving instruction to measure the knowledge sharing (i.e. the grounding) between the two speakers. In order to calculate the cooperation level each dialogue move had a weight of 0 or 1. Those weightings took into account the level of effort needed in each move. The lowest value (0) was attributed when a behavior requiring an effort (like for example adding information on a new map landmark) did not occur. On the other hand the positive weighting value (1) is attributed when an effort move take place in the dialogue. We also attribute a weight of 0 to giving instruction move which is in the area of  X  X  X inimum needs X  X  of the dialogue task. Effort was calculated on an individual basis and not as a joint activity because, as Davies ( 2006 ) pointed out, in the map task the minimization of the effort is done on the basis of the singular speaker. In Table 2 we report the description of the coding scheme features for cooperation and the inter-annotators X  agreement score (six annotators). 4.4 Coding scheme for facial expression As regards emotions, we already had from HR and Skin conductance the level of arousal and from the self-report measures we had the attribution of the valence. We therefore annotate facial expressions to find out which of them are correlated to (non) cooperation. Facial expressions are  X  X  X econstructed X  X  in eyebrows and mouth shapes. In Table 3 we report the description of the features for the lower and the upper face configurations and the Kappa scores.
 4.5 Coding scheme for turn management As regards turn management this system has the purpose of managing the flow of interaction, minimizing overlapping speech and pauses (Yngve 1970 ; Duncan 1972 ; Sacks et al. 1974 ; Goodwin 1981 ). Turn management is quite systematic in Map Task dialogues, probably because there are only two participants. Turn management is generally coded by three general features: Turn gain, Turn end and Turn hold .An additional dimension entails whether the speakers both agree upon a change in conversation turn. According to Duncan, in conversation back-channel cues are also used. In Duncan X  X  proposal, back-channel cues are considered as an alternative to turn-taking; this is because in Duncan X  X  perspective back-channels are reasonably not viewed as speaker turns (Duncan 1974 ; Duncan and Fiske 1977 ), but as optional utterances that occur during the turn of another speaker. Nevertheless, considering back-channels as optional is quite reductive, given the fact that they are so frequently produced in human communication and that participants in a conver-sation even expect to receive back-channels. Therefore, we included back-channels in our turn management annotation scheme as a separate category. In Table 4 turn management annotation features and validation is reported. 4.6 Coding scheme for gaze direction In the western culture, gaze direction is strictly linked to turn management. When a listener intended to take turn most of the times she/he pulled away her/his gaze, which was typically directed at the speaker X  X  face up until the turn release (Taylor and Cameron 1987 ; Levinson 2006 ). In Table 5 we report the annotation features and the statistics performed on gaze annotation. Gaze annotation features are inspired by the work of Allwood et al. ( 2007 ). 4.7 Validation results All the features of our coding scheme had Kappa scores above 0.75. The coding scheme is therefore highly reliable. Nevertheless, two features of the coding scheme in the annotation categories of Lower Face configuration and Gaze direction had a negative Kappa score and a high p value ( side -turn: K =-0.006; p &lt; 0.906; 1 corner Up :K =-0.005; p &lt; 0.910). We should check in future annotations whether the dataset we annotated was too small to test coder agreement for those specific features or those features were not entirely relevant to describe the phenomena in RECC. 5 Public releases RECC is a publically available resource. At the following web address http://www. clic.cimec.unitn.it/Files/PublicData/recc.html it is possible to find the annotated files together with the XML specification file of the coding scheme, the annotation manual, the reports and the papers on the corpus collection and validation. We reported in each annotated file the corresponding HR value. For privacy and ethical issues, the corpus video and audio recordings are available only on request and only for research purposes. 6 Conclusions RECC is a corpus built with the goal of investigating cooperation and emotions in face to face dialogues. It includes audiovisual recordings aligned with psycho-physiological data. Our expectation is that researchers will acquire from RECC elicitation method and RECC annotation scheme a range of features that are necessary for the progress in the domain of multimodal dialogue studies.

Our coding scheme reliability was very high when compared with other multimodal annotation schemes, in particular for emotion annotation. This is because we analyzed emotions with a coding scheme based on the decomposition of the several factors composing an emotive event. In particular, we did not refer to emotive terms directly.

The RECC coding scheme is an important step towards the creation of annotated multimodal resources which are crucial to investigate multimodal communication. Particularly, the RECC coding scheme can aid exploring how different emotive sets (positive or negative) modify cooperation in different cultural settings; how turn management and sequencing strategies are expressed in different cultural settings; how gaze can enhance or disrupt cooperation; how emotions modifies the multimodal communication.
 Our findings will be hopefully taken into account in the design of Human Computer Interfaces as well.
 References
