 low-dimensional problems.
 CDNs then cease to be exact. .
 for structure specification. a vector of observed values for random variables in the set V and let x , x nodes for function node s . Define the operator  X  variables in set A . For example,  X  S specification of functions and each function variables in the CDN is then given by the product of CDFs each CDF For example, in the CDN of Figure 1(a), each diamond correspo nds to a function a CDF. In the sequel we will assume that both F and CDN functions of parameter vector  X  and so F  X  F ( x )  X  F ( x  X   X  ) and the marginal CDF for any subset A  X  V is obtained simply by taking limits such that F ( x lim 2.1 Inference and learning in CDNs as differentiation conditional CDF F ( x for CDF F ( x  X   X  ) , the PDF is defined as P ( x  X   X  ) =  X  likelihood estimation, we require the gradient vector  X  requires us to compute a vector of single derivatives  X  parameters in the model. 2.2 Message-passing algorithms for differentiation in non -loopy graphs CDNs, computing mixed derivatives of the form  X  tive  X  P ( x  X   X  ) =  X  x [ F ( x  X   X  )] for non-loopy CDNs [6].
 To estimate model parameters  X  for which the likelihood over i.i.d. data samples x optimized, we can further make use of the gradient of the log-likelihood  X  x = [ x 1 ,  X  X  X  X  ,x K ] . The mixed derivative of the product is then given by [5] fashion from the product rule of differentiation: Lemma 3.1. Let G = ( V,S,E ) be a CDN and let F ( x ) = Y in
V . Let M 1 ,M 2 be a partition of the function nodes S and let G 1 ( x C G that are arguments to G Proof. Define L = C  X  the variable intersection set S the problem such that C  X  S gradient computations into a series of simpler computation s so that  X  CDN in which we will pass messages corresponding to local der ivative computations. 3.1 Differentiation in junction trees In a CDN G = ( V,S,E ) , let { C S intersection C For each directed edge ( i,j ) we define the separator set as S and a corresponding junction tree are shown in Figures 1(a), 1(b). Since T is a tree, we can root the tree at some node in C , say r . Given r , denote by j of elements of C that are in the subtree of T rooted at j and containing i . Also, let  X  of neighbors of i in T , such that  X  M 1 ,  X  X  X  X  ,M n neighbors in G are contained in C included in C write the joint CDF as where T r corresponds to computing where we have defined messages m  X  expression. We notice that for any given i  X  C with A  X  C quantity m recursively re-write the above as where in the last step we note that whenever B T S message m m m  X  symbolic terms. 3.2 Maximum-likelihood learning in junction trees While computing P ( x  X   X  ) =  X  function. The likelihood is equal to the message m tion of its gradient  X  the mixed derivative computation. The gradient of each mess age m g gradient vector for the CDF modelled by G . 3.3 Running time analysis in the next Theorem.
 Theorem 3.2. The time and space complexity of the JDiff algorithm is Proof. The complexity of Step 1 in Algorithm 1 is given by P C j derivatives  X  since the cost of computing derivatives for each A  X  C with S ways that we can choose the variables not in the separator S of order given by O max Algorithm 1: JDiff: A junction tree algorithm for computing the likeliho od  X  Data : Observations and parameters ( x ,  X  ) Output : Likelihood and gradient  X  foreach Node j  X  X  do 1 foreach Subset A  X  C j do 2 foreach Neighbor k  X  X  X  j T j 3 foreach Subset A  X  S j,k do end nentially with the tree-width. time makes practical to learn from data. 4.1 Symbolic differentiation tools such as Mathematica [16] and D* [4]. The task here was to symbolically compute  X  GB of RAM. The JDiff algorithm was implemented in MATLAB. A ju nction tree was constructed 580 s. and for D*, 6 . 7 s. to 12 . 7 s. 4.2 Learning models for rainfall and H1N1 data used the bivariate logistic distribution with Gumbel margi ns [2], given by model parameters using settings described in the Supplemen tal Information. approaches for modelling heavy-tailed data, we also tested the following:  X  Gaussian bi-directed (BDG) and Markov (MRF) models with the same topology as the loopy
CDNs for log-transformed data with  X  x = log( x + constraints were (  X  )  X  1  X 
CDF,  X  F is the Winsorized estimator [11] of the CDF for random variab le X and parameters model was set to be same as those of the BDG and MRF models.  X  Here we designed the BDG and MRF models to have the same graphi cal structure as the loopy nonparanormal and log-transformed Gaussian BDGs. loopy CDNs will also be of practical value.
Academy of Sciences USA (PNAS) 103 , 2015-2020. [2] de Haan, L. and Ferreira, A. (2006) Extreme value theory. Springer. 130-137. tions on Graphics 26(3) .
Intelligence (UAI) , 290-297. http://hdl.handle.net/1807/19194 tions on graphs. Journal of Machine Learning Research W&amp;CP Series 9 , 342-349.
Information Systems Processing (NIPS) 20 , 761-768. niques, MIT Press. 2328.
Series B (Methodological) 50(2) , 157224. 1(3) , 588-590. of Statistics 14(1) , 138-150. [16] Wolfram Research, Inc. (2008) Mathematica, Version 7. 0. Champaign, IL.
