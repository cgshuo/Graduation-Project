 Automatic image annotation plays a critical role in keyword-based image retrieval systems. Recently, the nearest-neighbor based scheme has been proposed and achieved good perfor-mance for image annotation. Given a new image, the scheme is to first find its most similar neighbors from labeled images, and then propagate the keywords associated with the neigh-bors to it. Many studies focused on designing a suitable dis-tance metric between images so that all labeled images can be ranked by their distance to the given image. However, higher accuracy in distance prediction does not necessarily lead to better ordering of labeled images. In this paper, we propose a ranking-oriented neighbor search mechanism to rank labeled images directly without going through the in-termediate step of distance prediction. In particular, a new learning to rank algorithm is developed, which exploits the implicit preference information of labeled images and under-lines the accuracy of the top-ranked results. Experiments on two benchmark datasets demonstrate the effectiveness of our approach for image annotation.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing image annotation; nearest-neighbor based scheme; learning to rank
In recent decades, the number of digital images has been growing rapidly and there is an increasingly urgent demand for effective image retrieval techniques. Users often prefer searching images with a textual query, which can be achieved by first annotating images manually, and then searching over the annotations using the query. However, manual image an-notation is a laborious and time-consuming process. There-fore, many efforts have been devoted to the research on au-tomatic image annotation.

The goal of automatic image annotation is to assign a few relevant keywords to an image that can reflect its visual content. Recently, the nearest-neighbor based scheme [4] has become increasingly attractive because of its superior performance and straightforward framework. It is on the assumption that visually similar images are more likely to share common keywords. Given a new image, the nearest-neighbor based scheme first finds a set of its most similar neighbors from labeled images, and then propagates the key-words associated with the neighbors to it.

In spite of the simplicity of the nearest-neighbor based annotation framework, there are some critical issues that remain to be addressed. One important aspect of the frame-work is how to perform the process of the nearest neighbor search effectively. Many studies [4, 5, 6] focused on design-ing a suitable visual distance metric between images, which is then used to rank all labeled images according to their dis-tance to the new image. Typically, the work aimed to weight and combine the distances from different dimensions in vi-sual feature space. Despite the encouraging results achieved, we argue that higher accuracy in distance prediction does not necessarily lead to better ordering of labeled images, which is the ultimate goal of our problem. For example, let u and v denote two labeled images whose true distances to the new image are 4 and 5 respectively. Suppose a method has predicted the distance to be 5 for u and 4 for v . Although it is a desirable result in terms of prediction error, it fails in ensuring the correct ordering of u and v . In light of this, it is necessary to shift our attention from approximating the absolute distance between images to directly predicting the relative ordering of labeled images.

In this paper, we propose a ranking-oriented neighbor search mechanism, which uses the learning to rank [3] tech-niques to directly produce the ordering of all labeled images for a new image, without going through the intermediate step of distance prediction. Unlike regular learning to rank methods, our proposed ranking algorithm exploits the im-plicit preference information hidden in the training images. In addition, since only the k nearest neighbors are generally considered in the nearest-neighbor based scheme, we enforce the ranking model to focus more on the correctness of the re-sults in top-k positions. A boosting algorithm [1] is utilized to solve the resulting optimization problem in our approach.
In this section, we introduce our ranking-oriented neigh-bor search mechanism in details. For the ease of explanation, we first give some notations.

Let X be an image collection, and all keywords appearing in the collection are W = { w 1 ,w 2 ,...,w c } ,where c is the total number of unique keywords. In image annotation task, we are given n labeled training images, T = { x ( i )  X  X  | i 1 ,...n } , each of which is associated with a c -dimensional by the j th keyword w j and y ( i ) j =0otherwise. Givena query image q  X  X  , our goal is to find a ranking function H : X X T  X  R such that H ( q,x ( i ) ) represents the relevance of the labeled image x ( i ) with respect to q ,and x ( i )
To resolve the above challenge, we seek to exploit the learning to rank (hereinafter referred to as LTR for short) techniques to learn the optimal ranking function H from the training data. Although LTR has been extensively studied [3], it is not straightforward to directly apply regular LTR techniques to our problem for the following two reasons. First, unlike standard LTR tasks where some preference in-formation (in the forms of pairwise or listwise constraints) is often explicitly given to supervise the learning process, in our problem, preference information is only implicitly avail-able in the training set. Moreover, generally we only con-sider the k nearest neighbors for a new image to prohibit the potential noisy keywords introduced by those distant neighbors. Therefore, in our ranking problem, the correct ordering of the top-k resultsiscrucial,andthemistakesin low ranks may not deteriorate the final performance. It is necessary to redesign the training procedure to ensure the top-k results are as accurate as possible.

Based on the above analysis, to facilitate our ranking task, in the following, we first generate the implicit preference information hidden in the training data. With the preference information, we further present a new LTR algorithm that underlines the accuracy of the top-k results.
As no explicit preference information is given for our prob-lem, the first step before LTR is to derive some preference information from the training data. Specifically, we sepa-rately submit each labeled image as a query and look for the information that could indicate the relative ordering among the other labeled images with respect to it.

It is notable that the intuition behind the nearest-neighbor based methods is that similar images should share more common keywords. This means that given an image, its close neighbors may have a higher keyword agreement with it compared to those distant neighbors. In accordance with this principle, we consider measuring the relative distance between labeled images by the consistency of their keywords. Given two label vectors y and  X  y , their consistency CON is estimated in a manner similar to F 1 measure: With this definition, for a labeled image x ( i ) , we define R of the other corresponding labeled images, i.e., T\ x ( i ) Then, let  X  i denote the total ranking of T\ x ( i ) with respect to x ( i ) , which can be derived in descending order of R i  X  ( x j ) stands for the position of image x j  X  X \ x ( i ) .
Although  X  i seems a natural way of representing the pref-erence information associated with x ( i ) , the variations in the importance of partial orderings with different ranking positions cannot be easily reflected in such a form of linear ordering. To allow encoding this kind of information, in this paper, we construct the preference information in the form of ordered pairs of images, and also assign each pair a weight to represent the importance of its being satisfied. In particular, asetof p ordered pairs W i = { w ( x j m i x q m ) | m =1 is further randomly picked up from  X  i ,where x j i x q de-notes a ordered pair indicating that the labeled image x j ranked before x q in  X  i ,and w ( x j i x q ) is its corresponding importance weight. To determine the value of w ( x j i x q we first define  X  i asanewrankingof T\ x ( i ) ,whichex-changes the positions of x j and x q in  X  i .Thenwecalculate the NDCG@ k metric of  X  i : where r l denotes the relevance degree of the image with po-sition l in  X  i ,and N k is a normalization factor chosen so that the NDCG@ k of the original ranking  X  i is 1. Furthermore, the exact form of w ( x j i x q )isgivenas In the above, if x j or x q involves the top-k instances in we take the drops of  X  i in terms of NDCG@ k as the value of w ( x j i x q ). Intuitively, as NDCG includes a position discount factor in its definition, incorrectly ordering higher ranks of  X  i canleadtogreaterlossesintermsofNDCG@ k . As a result, a large weight will be assigned to the ordered pair at high positions. On the contrary, if both x j and x pear behind the k th position in  X  i , there is no effect on the value of NDCG@ k when their positions exchange. There-fore, we set w ( x j i x q )tobe  X  , which is a small constant. Finally, we repeat the above process for each labeled im-age and give the ultimate set of preference information as P =  X  n i =1 W i , which will be used as input training data for the following LTR algorithm.
With the derived preference information set P ,wenow present the formulation of the proposed top-k focused LTR algorithm. The basic idea is that the optimal ranking func-tion H should be consistent with the preference information in
P as much as possible. To this end, we define the ranking error of H with respect to P as follows: Here, we introduce W ijq = w ( x j i x q )and H ij = H ( x ( i ) ,x for the simplicity of description. I (  X  ) is an indicator function that outputs 1 if the input boolean variable is true and zero otherwise. In fact, err measures the weighted number of the preference pairs misordered by H . As described in Section 2.1, the preference pairs at high positions have relatively larger weights, thus the incorrect orders of these pairs will result in more severe ranking errors; whereas the pairs only involving the instances behind the k th positions have been assigned small weights, and misordering them may affect little on the error. As a result, through minimizing err ,we can find the optimal ranking function H that gives priority to ensuring the correctness of the top-k results.
However, the ranking error defined in (4) is a non-smooth function as the indicator function I (  X  ) is non-smooth. It is well known that directly optimizing a non-smooth func-tion is computationally infeasible. To address the problem, we follow the idea of AdaBoost algorithm by replacing the indicator function I ( x  X  y ) with an exponential function exp ( x  X  y ). The resulting new ranking error is: Since it always holds that exp ( x  X  y )  X  I ( x  X  y ), by mini-mizing the new error err , we effectively reduce the original ranking error err . Besides, another advantage of using err from the theoretical property of AdaBoost, i.e., minimizing the exponential loss can not only reduce the training errors but also increase the margins of the training samples, and the enlarged margins are the key to ensure a low generaliza-tion error for test instances.

In our study, we utilize the RankBoost [1] algorithm to learn the optimal ranking function H by minimizing err .To guarantee the correct execution of the algorithm, we need to give a group of ranking features F = { f 1 ,...,f g } ,where each ranking feature f i defines a linear ordering of the im-ages to be ranked. To this purpose, we calculate the dis-tance of all ranked images to the query image in the space of a certain visual feature, and a ranking feature is gener-ated in ascending order of the distance. It should be noted that the ranking features are only related to the ordering of the ranked images rather than the actual numerical values of their distance. Algorithm 1 shows the details of the Rank-Boost algorithm. The algorithm operates for T iterations. For each successive iteration t =1 , 2 ,...,T , it maintains a weight distribution D ( t ) over the preference pairs in P weights are set according to the importance of preference pairs (line 1). At iteration t ,aweakranker h t is created from F based on the current weight distribution D ( t ) (line 3). We use the same generation process of weak ranker as described in [1]. Then the algorithm chooses a weight co-efficient  X  t for h t by measuring its ranking accuracy on all preference pairs (line 4). Intuitively, a greater coefficient is given to the more accurate weak ranker. Meanwhile, the weight distribution D ( t ) is updated according to the perfor-mance of h t (line 5). The preference pairs misordered by h have their weights increased, whereas the weights are de-creased for those pairs that are ordered correctly. Therefore, the weak ranker in the next iteration h t +1 will concentrate more on the  X  X ard X  pairs for h t . Oncealltheweakrankers have been created, the algorithm outputs the final ranking Algorithm 1 Rankboost algorithm for minimizing err Input: P , F and T Output: H 1: Initialize a distribution D over all preference pairs in 2: for t =1 ,...,T do 3: Create a weak ranker h t : R r  X  R r  X  R from F 6: end for 7: return H ( q,x )= function H through their weighted combination, and  X  t is the corresponding contribution of h t (line 7).

After finding the optimal ranking function H ,givenanew image q ,weemploy H to produce the total ranking of all labeled images with respect to q , and take the top-k results as its k nearest neighbors, which is denoted by N H ( q )= { NN 1 ,...,NN k } .
With the set of neighbor images N H ( q ), the next step is to evaluate the keyword relevance and propagate a certain number of the most relevant keywords to the new image q .In most previous work, researchers determined the relevance of a keyword by the majority or weighted voting of the nearest neighbors. However, there is no theoretical guarantee that the keywords selected by this manner are always the suitable annotations for q . In [2], Li et al. demonstrated that the difference between the keyword frequency in local neighbor set and that in entire image collection is a good keyword relevance indicator. Therefore, in our study, we adopt the similar method to compute the relevance of the keyword w with respect to q : where kf N H ( q ) ( w ) is the number of labeled images contain-ing w in N H ( q ), and kf prior ( w ) denotes the total frequency of w in the entire training collection.
We conduct experiments on two benchmark datasets: Corel 5K and IAPR TC 12. The two datasets have been widely used in previous studies so we can directly compare the ex-periment results. Each image on both datasets is repre-sented with the same visual features as described in [4].
On Corel 5K and IAPR TC12, all comparative methods are required to annotate each image with 5 most relevant keywords. The quality of predicted annotations is assessed by retrieving test images using the keywords in annotation
Figure 1: Effect of the variation of k on Corel 5K. vocabulary. For a keyword w , its precision and recall is computed as follows: where N w denotes the number of images correctly annotated with w , N p denotes the number of images predicted to have w ,and N r is the number of images annotated with w in ground-truth. The average precision ( P )andrecall( R )are computed over all keywords as two evaluation measures. In addition, we also consider another measure to assess the coverage of correctly annotated keywords, i.e., the number of keywords with non-zero recall ( N +).

In our approach, the number of neighbors considered in the nearest-neighbor based scheme, k ,isaparametertobe determined. In the experiments, the optimal value of k is found via an exhaustive search with a 5-fold cross-validation on training set. Figure 1 presents the performance compar-isons by varying k from 30 to 400 on Corel 5K. We can see that the best results can be achieved when k = 200, and a very small or large value of k degrades the performance. This is reasonable because a small number of neighbors can-not provide sufficient information to reflect the characteris-tics of a new image, while too many neighbors may introduce some information irrelevant to that image. On IAPR TC12, similar variation trend of performance can be observed as changes, and the optimal value of k is around 500. There-fore, we set k = 200 for Corel 5K and k = 500 for IAPR TC12 in our later experiments.
To investigate the efficacy of our ranking-oriented nearest-neighbor based method for image annotation, which is de-noted by RNN, we compare it with some previous distance-oriented methods, i.e., MSC [5], JEC [4], LASSO [4] and GS [6]. Besides, we design a modified version of our origi-nal method, NW-RNN, which adopts a similar ranking al-gorithm to RNN but without considering the procedure of preference pair weighting in equation (3). Instead, NW-RNN assigns equal weight to all preference pairs. As a re-sult, it is difficult for NW-RNN to ensure the correctness of the top-ranked results sufficiently. Table 1 shows the anno-tation results of different approaches.

As clearly observed in the table, on Corel 5K, NW-RNN gains comparable performance with RNN in N +, but loses a lot a in terms of P and R respectively. Such results under-lines the importance of focusing more on the correctness of the top-ranked results for the success of our ranking-oriented Table 1: Performance comparison in terms of P % ,R % and N + between our method and previous published work.
 neighbor search mechanism. In addition, RNN outperforms all the distance-oriented approaches listed in different eval-uation measures. The performance increase over the best distance-oriented method (GS) still achieves 1% , 1% and 3intermsof P , R and N +. On IAPR TC12, RNN is superior to other approaches as well. These improvements suggest that the annotation results provided by our method is preferable.
In this paper, we have introduced a novel image annota-tion method, which adapts the conventional nearest-neighbor based approaches with a ranking-oriented neighbor search mechanism. A new learning to rank algorithm is devel-oped to directly produce the ordering of all labeled im-ages. It leverages the implicit preference information of training data and underlines the accuracy of the top-ranked results. Experiments have demonstrated the effectiveness of our method for image annotation. For future study, we plan to examine the scalability of our method and experiment on large-scale web image datasets.
This work is supported by the Natural Science Founda-tion of China (61272240,60970047,61103151), the Doctoral Fund of Ministry of Education of China (20110131110028) and the Natural Science Foundation of Shandong Province (ZR2012FM037). [1] Y. Freund, R. Iyer, R. Schapire, and Y. Singer. An [2] X. Li, C. Snoek, and M. Worring. Learning social tag [3] T.-Y. Liu. Learning to rank for information retrieval. [4] A. Makadia, V. Pavlovic, and S. Kumar. A new baseline [5] C. Wang, S. Yan, L. Zhang, and H.-J. Zhang.
 [6] S. Zhang, J. Huang, Y. Huang, Y. Yu, H. Li, and
