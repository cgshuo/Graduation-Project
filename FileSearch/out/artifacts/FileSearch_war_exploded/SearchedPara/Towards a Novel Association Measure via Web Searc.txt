 The study of measuring semantic similarity between words or short text segments (e.g. entities) has become very important for many web-related tasks, including word clustering [4], name disambiguation [2] and community mining [1], etc. In recent years, web-based association measures have been well studied to evaluate the seman-tic similarity between two words or entities [1, 2, 9, 11]. In contrast with knowledge-based measures relying on existing knowledge databases or taxonomies (e.g. Word-Net) [5], web-based measures make use of the up-to-date web search results returned by web search engines (e.g. Yahoo Search) and they can reflect the up-to-date seman-tic similarity between two words or entities. Moreover, web-based measures can be successfully applied to compute the semantic similarity between new words or enti-ties, which are usually not defined in any existing knowledge database. typical association measures to leverage web search results for evaluating semantic similarity between two queries (i.e. words or entities), and they take all search results for a query as a coarse-grained single topic and use a single term vector to represent all the search results, and then the similarity value between two queries is obtained by computing the inner-product between the term vectors for the queries. In this study, the web-relevance similarity measure is implemented by first concatenating all the titles and snippets in the search results into a single document D ( q i ) for each query q i and then computing the cosine similarity between documents as the semantic similar-ity between two queries q 1 and q 2 . where ) ( very large and the results usually contain diverse information about the query: a cluster facet of the query. For example, some results for query  X  X pple X  are about the apple fruit, while other results are about the Apple computer. Even for the Apple computer, the results can be also grouped into a few clusters, each cluster referring to a specific aspect of the Apple computer, such as its price, its hardware, its software, and so on. It is not subtopics about the query, each subtopic representing a specific facet of the query. If the each other, the queries are semantically similar from the whole perspective; if only other, the queries are semantically similar on some facets denoted by the common sub-with each other, the queries are not semantically similar at all. Search Results Clustering &amp; Mining) to evaluate semantic similarity between words or entities by first discovering the subtopics in the search results for each query and then measuring the consistency between the subtopic sets, rather than taking all search results for each query as a single topic represented by a single vector. The k-means clustering algorithm is used for grouping the search results and discovering the subtopics. And then the optimal matching method is employed to measure the consistency between the subtopic sets by formalizing the problem as the optimal matching problem in the graph theory. The normalized optimal matching value is used as the semantic similarity value between the queries. Experiments have been performed on the benchmark Miller Charles X  dataset, and the results demonstrate the good effectiveness of the proposed WSRCM measure. Moreover, the perform-ance is improved by using the ensemble techniques to combine a few similarity measures. 806 X. Wan and J. Xiao 2.1 Overview The basic idea of the proposed WSRCM measure is to measure the semantic similarity between two queries by measuring the consistency between the discovered subtopics related to the queries, under the assumption that the corresponding subtopics in the topic representation of the search results for the query in previous work is coarse-grained. three steps: mercial search engine and the search results are retrieved for the query. related to the query. optimal matching method is used for measuring their consistency and the semantic similarity value is returned. 2.2 Search Results Retrieval In this study, we base our experiments on Yahoo Search because Yahoo Search out-performs Google Search and Live Search for most web-based measures in our pilot study. Yahoo Search is one of the most popular search engines used today and it returns the estimated result number and at most 1000 results for each query. We ex-tract each result record consisting of the title, snippet and url from the returned result page. Instead of downloading the full web pages, we use only the titles and snippets in the search results for efficient computation. The title and snippet are concatenated into a single text for each search result. All the 1000 (or fewer) returned search results are study and n is set to 1000 by default. 2.3 Search Results Clustering It has been a challenging task to group search results into meaningful clusters. Various methods have been proposed to address this problem, and they aim to facilitate users to traditional ranked list of search results. Note that it is not the focus of this study to group search results into accurate and meaningful clusters for user browsing, so we simply clusters and we expect the clusters can represent the real subtopics for the query to some extent. We choose the efficient k-means algorithm for search results clustering. 
The k-means algorithm is a partition based clustering algorithm. After the cluster number k is given, the algorithm randomly selects k documents as the initial centroids of the k clusters and then iteratively assigns all documents to the closest cluster, and recomputes the centroid of each cluster, until the centroids do not change. The simi-larity value between a document and a cluster centroid is computed using the standard study we heuristically set k = n /10, which means that the average document number in each cluster is 10. The time complexity of the k-means algorithm is O( nkl ), where l is the number of iterations taken by the algorithm to converge. 
We then filter out the trivial clusters with less than two documents from the k clus-ters. We believe that such clusters are outliers in the clustering results and they cannot represent the real subtopics for the given query. Lastly, only the remaining non-trivial clusters are used for cluster matching. 
To our knowledge, there exist advanced clustering algorithms which can find more accurate and meaningful subtopics from the search results, which will be exploited in our future work. 2.4 Subtopic Cluster Matching measure the consistency between the sets of topic clusters from a global perspective. We formalize this problem as the optimal matching problem and allow only one-to-one matching between the topic clusters. A globally optimal solution can be achieved by solving the optimal matching problem. 808 X. Wan and J. Xiao edge e ij in G . The weight w ij is set to the cosine similarity value between the two clus-ter texts as follows: M share the same node. Given the weighted bipartite graph G , OM is to find the matching M ~ that has the largest total weight. O(( p + q )( m +( p + q )log( p + q )), where m is the number of matching edges. total weight in M ~ as the semantic similarity value between queries q 1 and q 2 : where min{| X |,| Y |} returns the minimum subtopic number in X and Y . We compare various unsupervised web-based association measures based on the popular Miller-Charles dataset [6], which contains 30 word-pairs 1 rated by a group of (perfect synonymy). The dataset is actually a subset of Rubenstein-Goodenough X  X  original dataset [8]. The Miller-Charles dataset has been widely used in previous work for evaluating word similarity measures. tion between automatic computed values and human-labeled values. Given two rat-ings A and B , the Pearson X  X  correlation coefficient is computed as follows: geometric mean ensemble meaningful. 
In this study, we follow most previous research work [1, 2] and use only 28 pairs for evalua-tions, because of the omission of two word pairs in earlier versions of WordNet. measures, including WebJaccard, WebDice, WebOverlap, WebPMI 2 and We-bRelvence. Furthermore, we investigate the following two ensemble measures to fuse the well-performing measures in the experiments: Fusion1 (Average Mean): The similarity value is the average of the similarity values computed by WebPMI, WebRelevance and WSRCM, respectively. Fusion2 (Geometric Mean): The similarity value is the geometric mean of the simi-larity values computed by WebPMI, WebRelevance and WSRCM, respectively. 
Table 1 shows the results and the similarity values for each measure is normalized by dividing by the maximum value. 
Seen from Table 1, the proposed WSRCM measure outperforms all the popular web-based measures. In particular, WSRCM much outperforms the WebRelevance measure, which validates the assumption of WSRCM that it can evaluate word se-mantic similarity at a fine-grained level based on the discovered subtopics. We can also see that the two ensemble measures outperform all individual measures, which demonstrates the good effectiveness of the ensembles. Moreover, we can see that the Fusion2 measure (geometric mean) outperforms the Fusion1 measure (average mean). The WebJaccard, WebDice, WebOverlap, WebPMI measures rely on the number of hits returned by the web search engine, and they are defined the same as in [1]. 810 X. Wan and J. Xiao 
In the above experiments, the expected cluster number k is set to n /10=100, where n =1000 is the number of returned search results for a query. We now vary k from 10 to 1000 when n is fixed to 1000. Figure 2 shows the curve of correlation results. Seen from the figure, WSRCM performs well when k is set in the range of [50, 200], though the performance is affected by the cluster number: very small or very large subtopics in the search results for the query. 
We now compare WSRCM, WebRelevance and the two ensembles with respect to different numbers of top search results. The number n of top results used in the above measures varies from 100 to 1000. For WSRCM, the corresponding cluster number k when few top results ( n  X  300) are used for computation, WSRCM does not perform well, even worse than WebRelevance. While when more top results ( n  X  400) are used, WSRCM can always outperform WebRelevance. Moreover, the best performance for WSRCM (when n =1000) is better than the best performance for WebRelevance (when n =100). We explain the results by that the clustering of few top results might not produce good subtopic clusters because the few top results are usually very coher-ent. We can also see that the two ensemble measures can always outperform the indi-vidual measures and the geometric mean ensemble always outperforms the average mean ensemble.
 Nov. 14, 2007. To our knowledge, the search results for a given query can vary fre-investigate how the change of search engine affects the performance of the association measures, we conducted the experiments once a week from Nov. 14, 2007 and Jan. 2, 2008, and there were totally eight dates when we repeated the experiments. Figure 4 shows the correlation results of the association measures with respect to different dates. 
We can see from Figure 4 that almost all measures are affected by the time dimen-sion, which demonstrates that the search engine does return different results for a given query at different time. Fortunately, we observe that the proposed WSRCM measure is effective for most dates and the two ensemble measures are always per-forming very well. Moreover, the geometric mean ensemble always outperforms the average mean ensemble. 
In future work, we will explore advanced clustering algorithm to better discover meaningful subtopic clusters from search results. We will apply the propose measure to other web-related tasks (e.g. query suggestion) to validate its robustness. Acknowledgments. This work was supported by NSFC (60703064,60873155), RFDP (20070001059), National High-tech R&amp;D Program (2008AA01Z421) and NCET . 1. Bollegala, D., Matsuo, Y., Ishizuka, M.: Measuring semantic similarity between words us-812 X. Wan and J. Xiao 3. Kuhn, H.W.: The Hungarian method for the assignment problem. Naval Res. Logist. 5. Mihalcea, R., Corley, C., Strapparava, C.: Corpus-based and knowledge-based measures of 6. Miller, G., Charles, W.: Contextual correlates of semantic similarity. Language and Cogni-8. Rubenstein, H., Goodenough, J.B.: Contextual Correlates of Synonymy. Communications 12. Zamir, O., Etzioni, O.: Grouper: A dynamic clustering interface to web search results. In: 
