 We present a new method for detecting interpretable subgroups with exceptional transition behavior in sequential data. Identifying such patterns has many potential applications, e.g., for studying human mobility or analyzing the behavior of internet users. To tackle this task, we employ exceptional model mining, which is a general ap-proach for identifying interpretable data subsets that exhibit unusual interactions between a set of target attributes with respect to a cer-tain model class. Although exceptional model mining provides a well-suited framework for our problem, previously investigated model classes cannot capture transition behavior. To that end, we introduce first-order Markov chains as a novel model class for ex-ceptional model mining and present a new interestingness measure that quantifies the exceptionality of transition subgroups. The mea-sure compares the distance between the Markov transition matrix of a subgroup and the respective matrix of the entire data with the distance of random dataset samples. In addition, our method can be adapted to find subgroups that match or contradict given transition hypotheses. We demonstrate that our method is consistently able to recover subgroups with exceptional transition models from syn-thetic data and illustrate its potential in two application examples. Our work is relevant for researchers and practitioners interested in detecting exceptional transition behavior in sequential data. Keywords: Subgroup Discovery; Exceptional Model Mining; Markov chains; Transitions; Sequential data
Exceptional Model Mining [ 13 , 31 ], a generalization of the clas-sic subgroup discovery task [ 3 , 25 ], is a framework that identifies patterns which contain unusual interactions between multiple target attributes. In order to obtain operationalizable insights, it empha-sizes the detection of easy-to-understand subgroups, i.e., it aims to find exceptional subgroups with descriptions that are directly interpretable by domain experts. In general, exceptional model min-ing operates as follows: A target model of a given model class is computed once over the entire dataset, resulting in a set of model parameters. The same parameters are also calculated for each sub-group in a large (often implicitly specified) candidate set, using only the instances covered by the respective subgroup. A subgroup is considered as exceptional or interesting if its parameter values differ significantly from the ones of the overall dataset. While exceptional model mining has been implemented for a variety of model classes including classification [ 31 ], regression [ 12 ], Bayesian network [ 15 ] and rank correlation [ 11 ] models, it has not yet been applied using models for sequential data.

In this paper, we aim to apply exceptional model mining to dis-cover interpretable subgroups with exceptional transition behavior. This enables a new analysis method for a variety of applications. As one example, assume a human mobility dataset featuring user transitions between locations. The overall transition model could for example show that people either move within their direct neigh-borhood or along main roads. Detecting subgroups with exceptional transition behavior goes beyond this simple analysis: It allows to automatically identify subgroups of people (such as  X  X ale tourists from France") or subsegments of time (such as  X 10 to 11 p.m.") that exhibit unusual movement characteristics, e.g., tourists moving between points-of-interest or people walking along well-lit streets at night. Other application examples could include subgroups of web-users with unusual navigation behavior or subgroups of companies with unusual development over time, cf. [24].

The main contribution of this paper is a new method that en-ables mining subgroups with exceptional transition behavior by introducing first-order Markov chains as a novel model class for exceptional model mining . Markov chains have been utilized for studying sequential data about, e.g., human navigation [ 37 , 44 ] and mobility [ 20 ], meteorology [ 19 ], or economics [ 24 ]. To apply excep-tional model mining with this model, we derive an interestingness measure that quantifies the exceptionality of a subgroup X  X  transition model. It measures how much the distance between the Markov transitions matrix of a subgroup and the respective matrix of the entire data deviates from the distance of random dataset samples. This measure can be integrated into any known search algorithm. We also show how an adaptation of our approach allows to find subgroups specifically matching (or contradicting) given hypotheses about transition behavior (cf. [ 8 , 43 , 45 ]). This enables the use of exceptional model mining for a new type of studies, i.e., the detailed analysis of such hypotheses. We demonstrate the potential of the proposed approach with synthetic as well as real-world data.
The remainder of this work is organized as following: We sum-marize our background in Section 2. Then, the main approach for mining subgroups with exceptional transition behavior is introduced in Section 3. Section 4 presents experiments and results. Finally, we discuss related work in Section 5, before we conclude in Section 6. Our solution extends Exceptional Model Mining with first-order Markov Chain Models . In the following, we give a brief overview of both techniques.
We formally define a dataset D as a multiset of data instances i  X  I described by a set of attributes A consisting of describing attributes A
D  X  A and model attributes A M  X  A . A subgroup consists of a sub-group description p : D  X  X  true , f alse } that is given by a Boolean function, and a subgroup cover c ( p ) , i.e., the set of instances de-approach works with any pattern description language to describe subgroups. As a canonical choice, we focus in our experiments on conjunctions of selection conditions over individual describing attributes, i.e., attribute-value pairs in the case of a nominal attribute, or intervals in the case of numeric attributes. Hence, an example de-scription of a subgroup could be: Age &lt; 18  X  Gender = Male . Due to combinatorial explosion, a large number of subgroups can be formed even from comparatively few selection conditions.

From this large set of candidate subgroups, exceptional model mining identifies the ones that are unusual ( X  X nteresting X ) with re-spect to a target model class and the model attributes A M for traditional subgroup discovery the target concept is given by a Boolean expression over a single attribute (e.g.,  X  X lass = Good X  ) and a subgroup is considered as interesting if the expressions holds more (or less) often than expected, exceptional model mining con-siders more complex target concepts. Given a specific model class (such as a correlation or a classification model), the model parame-ters for a subgroup can be computed depending on the instances of the respective subgroup. A subgroup is then considered as interest-ing if its model parameters deviate significantly from the parameters of the model that is derived from all dataset instances. For example, consider a study about the correlation (model class) between the two model attributes exam preparation time of students and the final score they achieve. A finding of exceptional model mining could be:  X  X hile overall there is a positive correlation between the exam prepa-ration time and the score (  X  = 0 . 3 ), the subgroup of males that are younger than 18 years shows a negative correlation (  X  =  X  0 . 1 ) X  .
The goal of finding exceptional subgroups is accomplished by using a quality measure q that maps a subgroup to a real number (a score) based on the supposed interestingness of its model parameters and performing a search for the subgroups with the highest scores.
In this paper, we introduce first-order Markov chains as a target concept for exceptional model mining. Markov chains are stochastic systems modeling transitions between states s 1 ,..., s m served sequence of states corresponds to a sequence of assignments of random variables X 1 ,..., X z , X i  X  X  s 1 ,..., s m } . The commonly employed first-order Markov chain model assumes that this pro-cess is memoryless, i.e., the probabilities of the next state at time  X  + 1 only depend on the current state at time  X  : P ( X  X  + 1 s ,..., X  X  = s i P ( s j | s i ) . First-order Markov chain modeling is an established and robust method that underlies many analyses and algorithms [ 22 , 44 ], one of the most prominent examples being Google X  X  PageRank [ 37 ].
The parameters of a first-order Markov chain model can be speci-fied by a stochastic transition matrix T = ( t i j ) with matrix elements t = P ( s j | s i ) displaying the conditional probability for a transition from state i to state j ; thus, the sum of elements for each matrix row is 1. When working with datasets containing transitions, we can easily derive the stochastic transition matrix (i.e., the model parameters) with the maximum likelihood from a transition matrix containing counts for each transition by normalizing each row. Thus, we use the term transition matrix for both, stochastic matrices and count matrices, if specifics are clear from the context.
Given a set of state sequences and additional information on the sequences or parts of sequences, our main goal is to discover subgroups of transitions that induce exceptional transition models. We formalize this as an exceptional model mining task.

For that purpose, we first prepare the dataset D of transitions with model attributes A M and describing attributes A D (see Section 3.1). These allow to form a large set of candidate subgroup descriptions and to filter the dataset accordingly. For each candidate subgroup g , we then determine the corresponding set of transitions and compute its transition matrix T g . By comparing this matrix to a reference matrix T D derived from the entire data, we can then calculate a score according to an interestingness measure q (see Section 3.2). In order to detect the subgroups with the highest scores, standard exceptional model mining search algorithms are utilized to explore the candidate space (see Section 3.3). The automatically found subgroups then should be assessed by human experts (see Section 3.4). In a variation of our approach, we do not use the transition matrix of the entire data T D for comparison with the subgroup matrices T g , but instead employ a matrix T H that expresses a user-specified hypothesis. This allows for finding subgroups that specifically match or contradict this hypothesis (see Section 3.5).
We consider sequences of states and additional background in-formation about them. Since we will perform exceptional model mining on a transition level , we split the given state sequences in order to construct a tabular dataset, in which each instance corre-sponds to a single transition. For each instance, the source and target state represent the values of the model attributes A M from which the model parameters, i.e., the transition matrix of the Markov chain model, are derived. Each instance is also associated with a set of describing attributes A D based on the given background information.
Figure 1(a-b) illustrates such a preparation process for a simple example. It shows sequences of states (e.g., certain locations) that users have visited and some background knowledge, i.e., some user information and the time of each visit (Figure 1a). This informa-tion is integrated in a single data table (Figure 1b). It contains two columns for the transition model attributes A M , i.e., for the source and the target state of each transition. Additional describing attributes A D capture more information on these transitions. This includes information specific to a single transition such as the de-parture time at the source state but also information on the whole sequence that is projected to all of its transitions, e.g., user data or the sequence length. Example subgroup descriptions that can be expressed based on these attributes are "all transitions by female users" , "all transitions on Saturdays" , or combinations such as "all transitions between 13:00h and 14:00h from users older than 30 years that visited at least three locations" . As different types of information can be considered for the construction of the descriptive attributes, the approach is very flexible. (a) Sequence data with background knowledge transition model attributes A M and descriptive attributes A
We aim to find subgroups that are interesting with regard to their transition models. For quantifying interestingness, we employ an interestingness measure q that assigns a score to each candidate subgroup. The score is based on a comparison between the transi-tion matrix of the subgroup T g and a reference transition matrix T that is derived from the overall dataset. In short, the interestingness measure that we propose expresses how unusual the distance be-tween the transition matrix of a subgroup and the reference matrix is in comparison to transition matrices of random samples from the overall dataset . For that purpose, we first define a distance mea-sure on transition matrices. Then, we show how this distance can be compared against transition matrices built from random dataset samples. We describe those two steps in detail before discussing more specific issues.
 Distance measure and weighting. First, we compute the reference transition (count) matrix T D = ( d i j ) for the overall dataset D as exemplified in Figure 1c. To evaluate a subgroup g , all instances in the tabular dataset that match its subgroup description are identified and a transition matrix T g = ( g i j ) is determined accordingly (see, e.g., Figure 1d and Figure 1e). Then, a distance measure is employed to measure the difference of transition probabilities in these matrices. For both matrices T D and T g , each row i describes a conditional categorical probability distribution for the next state given that state s was observed directly before. In literature, several methods have been proposed to compare such distributions. Here, we focus on the total variation distance  X  tv , also called statistical distance or (excluding the constant factor) Manhattan distance . For one row, this is computed as the sum of absolute differences between the normalized row entries, i.e., between transition probabilities: We then aggregate this value over all states (matrix rows). Since in our setting differences in states with many observations in the sub-group should be more important than those with less observations, we weight the rows with the number of transitions w i =  X  j the corresponding source state s i in the subgroup: The factor 1 2 can be omitted as it is constant across all subgroups. States that do not occur in a subgroup are weighted with 0 and can be ignored in the computation even if transition probabilities are formally not defined in this case.

As an example, consider the transition matrix for the entire ex-ample dataset (Figure 1c) and the one for the subgroup Gender = f (Figure 1d). The weighted total variation for this subgroup is com-puted as follows:  X  tv ( Gender = f , D ) = 2  X  ( | 0 2  X  |  X  2 4 | ) + 0  X  NA + 1  X  ( | 1 1  X  2 4 | + | 0 1  X  2 4 | + |
Of course, there are also alternatives to the total variation dis-tance measure that we can use, e.g., the Kullback-Leibler divergence  X  ( g , D , i ) =  X  j g i j  X  log g i j d weighted total variation as it naturally extends existing approaches for interestingness measures from classical pattern mining: it can be considered as an extension of the multi-class weighted relative accuracy measure for multi-class subgroup discovery [ 1 ]. Addition-ally, it can also be interpreted as a special case of belief update in a Bayesian approach as it has been proposed in [ 42 ] for traditional pattern mining. We provide a proof for this in the Appendix. Despite this focus, we also conducted a large set of experiments with all three distance measures in parallel with overall very similar results. Comparison with random samples. The measure  X  tv describes a weighted distance between transition matrices. Yet, it is heavily influenced by the number of transitions covered by a subgroup. For example, small subgroups might be over-penalized by small weight-ing factors w i , while very large subgroups can be expected to reflect the distribution of the overall dataset more precisely. Thus, using directly as an interestingness measure does not consistently allow for identifying subgroups that actually influence transition behavior in presence of noise attributes, cf. Section 4.2.

To account for these effects, we propose a sampling-based nor-malization procedure. First, we compute the weighted distance  X  ( g , D ) of the subgroup g to the reference matrix as described before. Then, we draw a set of r random sample transition datasets R = { R 1 ,..., R r } , R i  X  D from the overall dataset D , each contain-ing as many transitions as the evaluated subgroup g . Then, we compute the weighted distances  X  tv ( R i ) for each of these sam-ples, and build a distribution of false discoveries (cf. [ 14 ]) from the obtained scores. In particular, we compute the mean value  X  (  X  tv ( R 1 , D ) ,...,  X  tv ( R r , D )) and the sample standard deviation  X  (  X  tv ( R 1 , D ) ,...,  X  tv ( R r , D )) for the distances of the random sam-ples. A subgroup is considered as interesting if the distance of the subgroup strongly deviates from the distances of the random sam-ples. We quantify this by a (marginally adapted) z-score , which we will use as the interestingness measure q in our approach: with  X  being a very small constant to avoid divisions by zero. Thus, q ( g , D ) quantifies how unusual the difference of the transition matrix of the subgroup g and the reference matrix is compared to a random set of transitions drawn from the overall data that contains the same number of transitions .

The rationale for using sampling without replacement is that the subgroup itself also cannot contain multiple instances of the same transition. As a consequence, even subgroups selected by random noise attributes would appear to be exceptional compared to samples with replacement in some settings. Sampling without replacement is also equivalent to randomizing (shuffling) the entries of the column for the target state as it has been suggested in pattern mining for the statistical validation of the resulting patterns [14], see also [21]. Stratification of samples. When drawing random samples equally across all states, high scores q tv can exclusively be caused by a peculiar distribution of source states in a subgroup. However, this is not desirable when studying transition behavior. Consider, e.g., a dataset D , where transitions for all but one source state (matrix rows) are deterministic (the transition probability is 1 for a single target state), and all source states have the same number of observed transitions. Then, random transition samples R i will be drawn mostly from the deterministic states and thus, will consistently have very small weighted distances  X  tv ( R i , D ) . Now, if any subgroup g only contains transitions from the non-deterministic source state, a random deviation from the underlying transition probabilities is likely. Yet, even if this deviation and thus the distance small on an absolute scale, this distance would still be higher than the ones of the random samples. As a consequence, g appears as an exceptional subgroup with respect to its transition probabilities, even if only the distribution of source states differs.

To address this issue, we adapt our sampling procedure: we do not use simple random sampling, but instead apply stratified sampling w.r.t. the source states of the transitions. Thus, we draw the random samples R 1 ,..., R r in such a way that for each source state in the data, each random sample contains exactly as many transitions as the evaluated subgroup. Note, that we do not stratify with respect to the target states since a different distribution of these states signals different transition behavior.
 Significance. To ensure that our findings are not only caused by random fluctuations in the data, the z-score q tv which we employ as our interestingness score can be used as a test statistic for a z-test on statistical significance. Yet, this test requires a normal distribution of the weighted distances  X  tv ( R i , D ) obtained from the samples. Although in many practical situations the distribution of the sampled distances is approximately normally distributed, this does not necessarily hold in all cases. We thus propose a two-step approach to assess statistical significance of the results. First, we use a normality test such as the Shapiro-Wilk-Test [ 41 ] on the set of distance scores obtained for the sample set R . If the test does not reject the assumption of normality, a p-value can be directly computed from the z-score. If normality is rejected, a substantially larger set of random samples can be drawn to compute the empirical p-value of a specific subgroup [ 21 ], i.e., the fraction of samples that show a more extreme distance score than the subgroup. Although this is computationally too expensive to perform for every single candidate subgroup, it can be used for confirming significance for the most interesting subgroups in the result set.

For both methods one must consider the multiple comparison problem [ 23 ]: if many different subgroups are investigated (as it is usually done in pattern mining), then some candidates will pass standard significance tests with unadapted significance values by pure chance. Therefore an appropriate correction such as Bonferroni correction [16] or layered critical values [47] must be applied. Estimate the effect of limited sample numbers. Determining the interestingness score q tv ( g , D ) requires to choose a number of ran-dom samples r . While fewer samples allow faster computation, results might get affected by random outliers in drawn samples. To estimate the potential error in the score computation caused by the limited number of samples, we employ a bootstrapping approach [ 17 ]: we perform additional sampling on the weighted distances of the original samples S = {  X  tv ( R 1 , D ) ,..., From this set, we repeatedly draw (e.g., 10 , 000 times)  X  X ootstrap replications X  , i.e., we draw r distance values by sampling with replacement from S and compute the subgroup score q tv for each replication. The standard deviation of the replication scores provides an approximation of the standard error compared to an infinitely large number of samples, cf. [ 18 ]. In other words, we estimate how precise we compute the interestingness score q tv with the cho-sen value of r compared to an infinite number of samples. If the calculated standard error is high compared to the subgroup score, re-computation with a higher number of samples is recommended.
To detect interesting subgroups, we enumerate all candidate sub-groups in the search space in order to find the ones with the highest scores. For this task, a large variety of mining algorithms has been proposed in the pattern mining literature featuring exhaustive as well as heuristic search strategies, e.g., depth-first search [ 25 ], best-first search [ 46 , 51 ], or beam-search [ 27 , 29 ]. In this paper, we do not focus on efficient algorithms for exceptional model mining, but apply a depth-first mining algorithm as a standard solution.
Candidate evaluation in our approach is computationally slightly more expensive than for traditional subgroup discovery. That is, the runtime complexity for determining the score of a single subgroup in our implementation is O ( r  X  ( N + S 2 )) for a dataset with N transi-tions, S different states, and a user chosen parameter of r samples: selecting the set of instances from a subgroup as well as drawing a stratified sample requires O ( N ) operations per subgroup and sample. The transition matrices for each of these transition sets can also be built in linear time. The weighted distance for each of the r samples and the subgroup can then be determined in O ( S 2 ) as a constant number of operations is required for each of the S 2 matrix cells.
A typical problem in pattern mining is redundancy, i.e., the result set often contains several similar subgroups. For example, if the subgroup male induces an exceptional transition model and thus achieves a high score, then also the subgroup males older than 18 can be expected to feature a similarly unusual model and receive a high score X  X ven if age does not influence transition behavior at all. A simple, but effective approach to reduce redundancy in the result set is to adapt a minimum improvement constraint [ 7 ] as a filter criterion. To that end, we remove a subgroup from the result set if the result also contains a generalization, i.e., a subgroup described by a subset of conditions, with a similar (e.g., less than 10% difference) or a higher score.
Automatic discovery algorithms with the proposed interestingness measure can detect subgroups with exceptional transition models. Yet, to interpret the results, manual inspection and assessment of the top findings is crucial as this allows users to identify in what aspects the found "interesting" subgroups differ from the overall data. For that purpose, a comparison between the subgroup transition matrix and the reference matrix is required. Yet, manual comparison can be difficult for large matrices (state spaces). Therefore, we recommend to assess subgroups with summarizing key statistics , such as the num-ber of transitions in a subgroup, the weighted distance  X  subgroup and reference transition matrices, the unweighted raw dis-tance  X  tv =  X  i  X  tv ( sg , D , i ) , or the distribution of source and target states. Additionally, exemplification , e.g., by displaying represen-tative sequences, and visualizations are helpful tools for subgroup inspection. In that regard, we propose a graph-based visualization to get a quick overview of the differences between subgroup and reference transition matrices, see Figure 3 for an example. Here, each state is represented as a node and directed edges represent the differences of transition probabilities between the states. The width of an edge represents the amount of change in the transition probability, the color indicates if it is a decrease or increase. Edges without significant differences can be removed from the graph to increase visibility. In addition to that, application-specific visual-izations often allow a natural view on the data, see Figure 2 for an example featuring geo-spatial data.
In addition to comparing subgroups to the overall dataset, our approach can also detect subgroups that specifically contradict or match a user-defined hypothesis. Following the concepts of [ 43 ], we can express such a hypothesis as a belief matrix T H = ( h higher values h i j indicate a stronger belief in transitions from state s to state s j . An example of a hypothesis considering the example formalizes a belief that users from state A (first row) will always go to state B , and users from the states B and C will proceed to any of the three states with equal probability.

Given a hypothesis matrix, the interestingness score of a subgroup is computed analogously to the original case, but instead of using the transition matrix derived from the overall dataset T D we use the hypothesis belief matrix T H for the computation of the weighted distance  X  tv . A subgroup g (exceptionally) contradicts a hypothesis H , if its transition matrix T g has a significantly larger dis-tance to the hypothesis matrix T H than the stratified random samples of the dataset. To find subgroups that match a hypothesis specifically well instead of contradicting it, the inverted interestingness measure  X  q tv ( g , D ) can be used instead.
We demonstrate the potential of our approach with synthetic and empirical data. With synthetic data, we show that our approach is able to recover (combinations of) conditions that determine the tran-sition probabilities in presence of noise attributes. With empirical data, we illustrate possible application scenarios and findings.
As selection expressions for subgroup descriptions, we used all attribute-value pairs for nominal attributes, and all intervals ob-tained by equal-frequency discretization into five groups for nu-meric attributes. For computing the interestingness measure, we used r = 1 , 000 random samples. If not stated otherwise, we fo-cused on subgroups with simple descriptions, i.e., no combinations of selection conditions were considered. For the empirical stud-ies, we confirmed our top results to be statistically significant on an  X  = 0 . 01 level using the procedure presented in Section 3.2. Our implementation, an extension of the VIKAMINE data mining environment [4], and the synthetic datasets are publicly available
We start with a synthetic dataset directly generated from two first-order Markov chain transition matrices.
 Experimental setup. We created two 5  X  5 matrices of transition probabilities by inserting uniformly distributed random values in each cell and normalizing the matrices row-wise. Then, for each generated instance, one of the matrices was chosen based on two attributes, a ternary attribute A and a binary attribute B . If both attributes take their first values, i.e., A = A1 and B = B1 , then transi-tions were generated from the first matrix, otherwise from the second matrix. For each combination of values, we generated 10 , 000 transi-tions, resulting in 60,000 transitions overall. For each transition, we additionally generated random values for 20 binary noise attributes, each with an individual random probability for the value true . We employed our approach with a maximum search depth of two selec-tors to find subgroups with different transition models compared to the overall dataset. Our approach should then detect the subgroup A = A1  X  B = B1 as the most relevant one.
 Results. The top-5 result subgroups are displayed in Table 1. It shows the number of covered transitions (instances), the score of the interestingness measure q tv including the standard error of its computation estimated by bootstrapping (  X  ), the weighted total variation  X  tv between the subgroup and the reference transition matrix, and its unweighted counterpart  X  tv . Result tables for the following experiments will be analogously structured.

We observe that our approach successfully recovered the sub-group of transitions that were generated from a different probability matrix, i.e., the subgroup ( A = A1  X  B = B1 ) . This subgroup re-ceives the best score q tv by a wide margin. The subgroup with the next highest score ( A = A1 ) is a generalization of this subgroup. Since it contains transitions generated from both matrices in a dif-ferent mixture, it also features indeed an unusual transition model compared to the entire dataset. In the same way, the next subgroups all feature the attributes A and B that actually influence the transition behavior, and none of the noise attributes. These top subgroups all pass a Bonferroni-adjusted statistical significance test as described in Section 3.2 with an empirical p-value of p 10  X  10 , while all subgroups containing only noise attributes (not among the shown top subgroups) do not pass such a test with a critical value of
Our second demonstration example features a set of transitions generated by a random walker in a network of colored nodes. Experimental setup. First, we generated a scale-free network con-sisting of 1 , 000 nodes (states) with a Barab X si-Albert model [ 6 ]. That is, starting with a small clique of nodes, new nodes with degree 10 were inserted to the graph iteratively using preferential attach-ment. Then, we assigned one of ten colors randomly to each node. On this network, we generated 200 , 000 sequences of random walks with five transitions each, resulting in 1 , 000 , 000 transitions over-all. For each sequence, we randomly assigned a walker type. With Table 1: Top subgroups for random transition matrix data. For each subgroup, we show the number of instances covered by this subgroup, the interestingness score q tv , the weighted total variation  X  tv and the unweighted total variation  X  tv .
 a probability of 0 . 8 , the walk was purely random , i.e., given the current node of the walker, the next node was chosen with uniform probability among the neighbouring nodes. Otherwise, the walk was homophile , i.e., transitions to nodes of the same color were twice as likely. For each transition, the resulting dataset contains the source node, the target node, the type of the respective walker (random or homophile), and additionally the values for 20 binary noise attributes, which were assigned with an individual random probability each.

With this data, we performed three experiments. In the first, we searched for subgroups with different transition models compared to the entire data. In the second and third experiment, we explored the option of finding subgroups that contradict  X  respectively match  X  a hypothesis. For that purpose, we elicited a hypothesis matrix T
H = ( h i j ) that expresses belief in walkers being homophile , i.e., in transitions being more likely between nodes of the same color. Towards that end, we set a matrix value h i j to 1 if i and j belong to the same color and h i j = 0 otherwise. Edges of the underlying network were ignored for the hypothesis generation.
 Results. Table 2 presents the results for the three experiments. As intended, exceptional model mining identified the subgroups that in-fluence the transition behavior as the top subgroups for all three tasks. In the first experiment (see Table 2a) both subgroups described by the Type attribute are top-ranked. For the second experiment (see Ta-ble 2b), the subgroup Type=Random receives the highest score. This subgroup should by construction indeed expose the least homophile behavior since any subgroup described by noise attributes contains transitions from homophile as well as non-homophile walkers. Its complement subgroup Type=Homophile does not contradict our hy-pothesis and thus does not appear in the top subgroups. By contrast and as expected, the subgroup Type=Homophile receives the highest score in the third experiment that searches for subgroups matching the homophile hypothesis, while Type=Random is not returned as a top result, cf. Table 2c. For all three experiments, the statistical significance of the top subgroups described by the Type attribute was decisive ( p 10  X  10 ), while the top findings for the noise attributes were not significant at the Bonferroni-adjusted  X  = 0 . 05 level. Table 2: Top subgroups for the random walker datasets. For each subgroup, we show the number of instances covered by this subgroup, the interestingness score q tv , the weighted total variation  X  tv and the unweighted total variation  X  tv .
In additional experiments (no result tables shown), we employed the weighted distance  X  tv directly as an interestingness measure. By doing so, we were not able to recover the relevant subgroups as they were dominated by several random noise subgroups. This shows the necessity of a comparison with random samples.

We also experimented extensively with different parametrizations (e.g., different walker type probabilities or different numbers of node colors). Consistently, we were able to identify the two subgroups Type=Random and Type=Homophile as the top subgroups.
In addition to the synthetic datasets, we present illustrative exam-ples with empirical data; we start with data from Flickr. Experimental setup. For this dataset, we crawled all photos with geo-spatial information (i.e., latitude and longitude) at street-level accuracy in Manhattan from the years 2010 to 2014 on Flickr. Each photo was mapped according to its geo-location to one of the 288 census tracts (administrative units) that we use as states in our model (see also [ 20 ]). Based on this information, we built sequences of different tracts (i.e., no self-transitions) that users have taken photos at. Additionally, we elicited a wide range of describing attributes for each transition, i.e., the number of photos the respective user has uploaded from Manhattan, the number of views the source photo of the transition received on Flickr, as well as the month, the weekday and the hour this photo was taken. We added two more features based on the user X  X  origin, that is, the tourist status and the nationality (country). We considered a user to be a tourist if the time from her first to her last photo does not exceed 21 days, cf. [ 10 ]. Country information of a user was derived from the location field in her user profile by extracting the country using a combination Table 3: Top subgroups for the Flickr dataset. For each sub-group, we show the number of instances covered by this sub-group, the interestingness score q tv , the weighted total variation  X  tv and the unweighted total variation  X  tv . (b) Comparison to the Proximate-PoI hypothesis, contradicting of querying GeoNames 2 and specialized regular expressions. The country information was only available for about half of the users. Overall, our dataset contained 386,981 transitions and allowed to construct 163 selection conditions.

In a first experiment, we aimed at discovering subgroups with different transition models compared to the entire data. Addition-ally, we investigated an existing hypothesis about the trails derived from Flickr photos, that is, the Proximate-PoI hypothesis. This hypothesis has been shown to be one of the best hypotheses for explaining movements in Flickr photo data [ 8 ]. It expresses that users at a certain location are most likely to go to another location that is (a) nearby and (b) contains important points of interest (PoI), such as tourist attractions or transportation hubs. To construct this hypothesis, the locations of points of interest have been extracted from DBPedia [30], see [8] for further construction details. Results. Table 3 reports our results: the most exceptional subgroups in comparison with the overall data (see Table 3a) describe transi-tions by users that take either very many (more than 714) or very few (less than 25) photos. We explain this by the fact that users with overall fewer photos are more likely to travel a longer distance before taking another picture, resulting in more long distance tran-sitions. The next two subgroups Tourist=True and Tourist=False suggest that tourists continue their trip to different locations than lo-cals, e.g., as they are more interested in touristic attractions. Further top subgroups with deviating transition models involve the number of views pictures receive on Flickr and the country of origin.
Table 3b and Table 3c display the top subgroups that contradict the Proximate-PoI hypothesis, respectively match it. We observe that users with small amounts of pictures and non-tourists do not move as the investigated hypothesis suggests. Also, night time mobility (roughly 21h  X  1h, see the result table for exact subgroup ordering) does not match this hypothesis, maybe due to the closing of touristic attractions at night times. By contrast, tourists and users with many pictures as well as transitions at midday are especially consistent with the Proximate-PoI hypothesis.

Although we discover these exceptional subgroups from the large set of candidates automatically, it has to be investigated post-hoc how the transition models deviate. In that direction, we studied the subgroup Tourist=True in detail. For that purpose, we first computed the source state with the most unusual distribution of target states, i.e., the row that contributes the highest value to the weighted total variation q tv . For the tourist subgroup, this state (tract) corresponds to the central park. We then visualized the transition probabilities for this single state for the entire dataset and the subgroup with the VizTrail visualization system [ 9 ], see Figure 2. It can be observed that tourists are less likely to move to the northern parts of Manhattan, but are more likely to take their next picture in the city center or at the islands south of Manhattan. For a second investigated subgroup, i.e., the subgroup of transitions between 22h and 23h, this effect is even more pronounced as almost no transitions from the central park to the northern or north-eastern tracts can be observed. Note, that this visualization only covers the transition probabilities of a single state, not the overall transition matrix used for detecting interesting subgroups. Additionally, we analyzed data from the LastFM music service. Experimental setup. We used the 1K listening data 3 containing the full listening history of 1 , 000 LastFM users featuring more than 19 , 000 , 000 tracks (songs) by more than 170 , 000 artists. With this data, we studied sequences of music genres (such as rock , pop , rap , classical , etc.) of songs that users listened to, focusing on a list of 16 main genres. Since genre information is difficult to obtain on a track-level, we labeled each track with the best fitting genre for the respective artist as obtained by the EchoNest API 4 . In doing so, we could determine genres for more than 95% of the tracks. We then constructed genre transitions for each user based on the sequence of tracks she had listened to. We filtered subsequent tracks of the same artist to remove cases where the user listened to all songs of a single album. Additionally, we removed all transitions with unknown source or target state (genre). Thus, we obtained a dataset of 9 , 020 , 396 transitions between tracks. Background knowledge includes user information about age, gender, origin and the year of signup to LastFM, and the point in time the source song of the transition was played, i.e., the hour of the day, the weekday, the month and the year. This allowed to generate 86 selection conditions. On this data, we applied our approach twice to find subgroups with exceptional transition behavior: once for subgroups described by a single selection condition only, and once including combinations of two selection conditions (search with depth 2).
 Results. Results for single selection conditions are displayed in Ta-ble 4a. We can see that the country of origin of users is an important factor: the majority of top subgroups is described by this attribute. Specifically, users from the United States, from Finland, and from Argentina exhibit transitions between music genres that are unusual compared to the entire data. By contrast, date and time influence the transitions between genres only little and do not describe any of the top subgroups. For subgroups described by combinations of conditions, see Table 4b, we can see that users with a high number of tracks show unusual transition behavior, especially if they signed up to the system early, or if they are in a certain age group.
Figure 3 visualizes differences in transition behavior in compari-son to the overall dataset for two top-subgroups. Here, each node represents a state (genre). The first graph gives an impression on the transition probabilities in the entire dataset. Stronger arrows represent higher probabilities. We omit probabilities below 0 . 1 . The next two graphs show deviations from these probabilities in the sub-Table 4: Top subgroups for the LastFM dataset. For each sub-group, we show the number of instances covered by this sub-group, the interestingness score q tv , the weighted total variation  X  tv and the unweighted total variation  X  tv . groups Country=US and Country=Finland . Green arrows indicate transitions that are more likely in the subgroup than in the over-all data, red arrows imply less likely transitions. Stronger arrows represent higher deviations; small deviations ( &lt; 0 . 05) are omitted.
We observe that users from the US and Finland deviate from the overall behavior in characteristic ways. For example, users from the US tend to skip to Rock more often, while the same is true for users from Finland with regard to Metal . Also, we observe interesting dynamics between genres that go beyond the different target state distributions: for example, users from the US are more likely to listen to Reggae after a song from the World genre, while the preference for Rock decreases in this case. We can also see, that although Rock is overall more popular in the US, it follows a track of Reggae less likely than in the entire data.
Mining patterns in sequential data has a long history in data mining. However, large parts of research have been dedicated to the tasks of finding frequent subsequences efficiently, see for example [ 2 , 36 , 50 ]. Other popular settings are sequence classification [ 33 , 49 ] and sequence labeling [ 26 ]. Unlike this work, these methods do not aim to detect subgroups with unusual transition behavior.
Our solution is based on exceptional model mining, a generaliza-tion of subgroup discovery [ 3 , 25 ]. This classical data mining task aims at finding descriptions of data subsets that show an unusual statistical distribution of a target concept. Traditional subgroup dis-covery focuses on a single attribute as a target concept. Exceptional model mining [ 13 , 31 ] was proposed as a generalized framework that facilitates more complex target concepts over multiple target attributes. For exceptional model mining, different model classes, e.g., classification [ 31 ] and regression models [ 12 ], Bayesian net-works [ 15 ] as well as advanced mining algorithms [ 28 , 29 , 32 ] have been proposed. No models featuring sequential data have been explored for exceptional model mining so far.

We have presented an approach to detect subgroups with ex-ceptional transition models, i.e., subgroups that show unusual dis-tributions of the target states in first order Markov chain models. The results from our approach may correlate with subgroups that could also be obtained by multi-class subgroup discovery [ 1 ] that investigates the distribution of target states. However, such a static analysis aims to achieve a different goal than our analysis of be-havior dynamics and will not capture all subgroups with exceptional transition models. For example, in the random walker synthetic dataset (see Section 4.2) the distribution of target states is approxi-mately uniform for all subgroups by construction, also for the ones that influence the transition behavior. As a consequence and in contrast to our method, a static analysis could not recover the excep-tional subgroups. Furthermore, the task of finding subgroups that match or contradict a hypothesis of dynamic state transitions (e.g., as demonstrated in the Flickr example, see Section 4.3) cannot be formulated as a more traditional subgroup discovery task.
Our interestingness measure is inspired by previous methods. The weighted distance measure can be considered as an adaptation of the multi-class weighted relative accuracy [ 1 ] or as a special case of the Bayesian belief update [ 42 ]. The randomization/sampling processes to capture significant differences of subgroups also builds upon previous approaches. In that direction, Gionis et al. [ 21 ] utilize swap randomization to construct alternative datasets in order to ensure the statistical significance of data mining results. For subgroup discovery, analyzing a distribution of false discoveries obtained by randomization has been proposed to assess subgroups and interestingness measures [ 14 ]. We have extended these methods to exceptional model mining with complex targets and have used it directly in the interestingness measure for the subgroup search.
For modeling sequential processes, Markov chains have been used in a wide variety of applications ranging from user naviga-tion [ 38 , 44 ] to economical settings and meteorological data [ 19 ]. The mixed markov model extension [ 39 ] of classical Markov chains features separate transition matrices for  X  X egments" of users, but these segments are not interpretable, i.e., have no explicit descrip-tions. The work maybe closest to ours is [ 40 ], where the authors detect outliers of user sessions with respect to their probability in a Markov-chain model; outliers are then manually categorized into several interpretable groups. By contrast, our solution allows to iden-tify descriptions of groups that show unusual transition behavior automatically from large sets of candidate subgroups.

Recently, also the comparison of hypotheses about Markov chain models has been popularized [ 8 , 43 , 45 ]. The approach proposed in this paper enables extensions to this line of research with more fine-grained analyses: we cannot only compare hypotheses against each other, but also identify (sets of) conditions under which a given hypothesis is matched or contradicted.
In this paper, we have introduced first-order Markov chains as a novel model class for exceptional model mining in sequence data with background knowledge. This enables a novel kind of analysis: it allows to detect interpretable subgroups that exhibit exceptional transition behavior, i.e., induce different transition models compared to the entire dataset. In addition, we have presented a variation of the standard task that compares subgroups against user-defined hypothe-ses, enabling a detailed analysis of given hypotheses about transition behavior. We have illustrated the potential of our approach by ap-plying it to both synthetic and empirical data. For synthetic data, the proposed method successfully recovered exceptional transitions from artificial noise attributes.

In the future, we aim to improve and extend our approach in several directions. First, the proposed interestingness measure is currently based on individual transitions. As a consequence, few very long sequences (e.g., of very active users) can strongly influence the results. To avoid dominance of such sequences, weighting of the transition instances according to the overall activity could be applied in future extensions, cf. [ 5 ]. In addition, we intend to investigate ways of speeding-up the mining process, e.g., by optimistic estimate pruning [ 48 ] or by using advanced data structures [ 32 ], and more sophisticated options to reduce redundancy, cf. [ 34 , 35 ]. Finally, we would like to generalize the proposed model class to Markov chains of higher order or even more advanced sequential models that potentially also take indirect state transitions into account. Acknowledgements. This work was partially funded by the Ger-man Science Fund project "PoSTs II" and the Austrian Science Fund project "Navigability of Decentralized Information Networks". In Bayesian statistics, one X  X  current beliefs H are expressed by Bayesian probabilities over parameters  X  . Given new informa-tion I , the prior belief P (  X  | H ) is updated to a posterior belief P (  X  | H , I ) . Here, we show that the total variation measure  X  to the amount of Bayesian belief update in the setting of Markov chain models if the reference matrix T D is used as a very strong prior. That means that both measures ultimately imply the same ranking of subgroups. The belief update implied by a subgroup g is defined by the difference between the prior distribution and the posterior distribution after observing the instances covered by g . The amount of belief update was proposed in [ 42 ] as an interestingness measure for pattern mining in traditional settings.
 As [ 43 ] suggests, we can elicit the matrix of a Dirichlet prior T = ( a i j ) through a reference matrix of (pseudo-)observations T
D = ( d i j ) using the formula a i j = ( k  X  d i j ) + 1 . Here, k specifies the strength of the belief expressed by the prior. It is updated to a posterior according to observed transitions in a subgroup g given in a transition matrix T g = ( g i j ) . In this context, according to [ 44 ], the expected probabilities E [ p i j ]( X ) for a state transition from state s to state s j in the prior are a i j the posterior are c i  X  g i j determine the overall belief update BU for all state transitions, we compute the absolute difference between the posterior and the prior for each cell and aggregate over all cells in the matrix: Now, assume that we have a very strong belief in the prior, i.e., k  X   X  and thus a i j g i j . Then, the right hand sum converges to the total variation  X  tv between the observed transition matrix T and the reference matrix T D . The factor  X  j g i j corresponds to the weights w i . The additional factor 1 constant across all subgroups if a i j g i j since a i j is independent from the evaluated subgroup. Overall, the weighted total variation  X  tv describes the amount of belief update a subgroup induces to a prior that reflects a very strong belief in the transition probabilities given by the reference matrix T D .
