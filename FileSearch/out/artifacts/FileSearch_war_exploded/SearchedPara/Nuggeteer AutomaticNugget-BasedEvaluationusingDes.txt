 The TREC Definition and Relationship questions are e v aluated on the basis of information nuggets, abstract pieces of kno wledge that, tak en together , comprise an a nswer . Nuggets are described infor -mally , with abbre viations, misspellings, etc., and each is associated with an importance judgement:  X  X ital X  or  X  X kay X . 1 In some sense, nuggets are lik e W ordNet synsets, and their descriptions are lik e glosses. Responses may contain more than one nugget X  X hen the y contain more than one piece of kno wledge from the answer . The median scores of today X  s systems are frequently zero; most responses contain no nuggets (V oorhees, 2005).

Human assessors decide what nuggets mak e up an answer based on some initial research and on pools of top system responses for each question. Answer k e ys list, for each nugget, its id, importance, and description; tw o e xample answer k e ys ar e sho wn in Figures 1 and 2. Assessors mak e binary deci-sions about each response, whether it contains each nugget. When multiple responses contain a nugget, the assessor gi v es credit only to the (subjecti v ely) best response.

Using the judgements of the assessors, the fi-nal score combines the recall of the a v ailable vi-tal nuggets, and the length (discounting whitespace) of the system response as a proxy for precision. Nuggets v alued  X  X kay X  contrib ute to precision by in-creasing the length allo w ance, b ut do not contrib ute to recall. The scoring formula is sho wn in Figure 3.
Qid 87.8:  X  X ther X  question f or tar get Enrico F ermi 2 vital Called the atomic Bomb an e vil thing 3 okay Achie v ed the first controlled nuclear chain reaction 4 vital Designed and b uilt the first nuclear reactor 6 okay co-de v eloper of the atomic bomb funding the A UC paramilitary or ganization. 3 okay some lando wners support A UC for protections services 5 okay The A UC mak es mone y by taxing Colombia X  s drug trade 7 okay Man y A CU fighters are former go v ernment soldiers
Let r # of vital nuggets returned in a response a # of okay nuggets returned in a response
R # of vital nuggets in the answer k e y l # of non-whitespace characters in the entire
Then
Finally , the F (  X  ) =
Automatic e v aluation of systems is highly desir -able. De v elopers need to kno w whet her one sys-tem performs better or w orse than another . Ideally , the y w ould lik e to kno w which nuggets were lost or g ained. Because there is no e xhausti v e list of snip-pets from the document collection that contain each nugget, a n e xact automatic solution is out of reach. Manual e v aluation of system responses is too time consuming to be ef fecti v e for a de v elopment c ycle.
The Qa viar system first described an approximate automatic e v aluation technique using k e yw ords, and Pourpre w as the first publicly a v ailable implemen-tation for these nugget-based tasks. (Breck et al., 2000; Lin and Demner -Fushman, 2005). Pourpre calculates an id f -or count-based, stemmed, unigram similarity between each nugget description and each candidate system response. If this similarity passes a threshold, then it uses this similarity to assign a par -tial v a lue for recall and a partial length allo w ance, reflecting the uncertainty of the automatic judge-ment. Importantly , it yields a ranking of systems v ery similar to the of ficial ranking (See T able 2).
Nuggeteer of fers three important impro v ements:  X  interpretability of the scores, as compared to  X  use of kno wn judgements for e xact information  X  information about indi vidual nuggets, for de-
Nuggeteer mak es scores interpretable by making binary decisions about each nugget and each system response, j ust as assessors do, and then calculating the final score in the usual w ay . W e will sho w that Nuggeteer X  s absolute error is comparable to human error , and that the 95% confidence interv als Nugge-teer reports are correct around 95% of the time.
Nuggeteer assumes that if a system response w as e v er judged by a human asse ssor to contain a partic-ular nugget, then other identical responses also con-tain that nugget. When this is not true among the hu-man judgements, we claim it is due to annotator er -ror . This assum ption allo ws de v elopers to add their o wn judgements and ha v e the responses the y X  v e ad-judicated scored  X  X  xactly X  by Nuggeteer .

These features empo wer de v elopers to track not only the numeric v alue of a change to their system, b ut also its ef fect on retrie v al of each nugget. Nuggeteer b uilds one binary classifier per nugget for each question, based on n -grams (up to trigrams) in the description and optionally in an y pro vided judgement files. The classifiers use a weight for each n -gram, an informati v eness measure for each n -gram, and a threshold for accepting a r esponse as bearing the nugget. 2.1 N -gram weight The id f -based weight for an n -gram w 1 ... w n is the sum of unigram id f counts from the A Q U AINT corpus of English ne wspaper te xt, the corpus from which responses for the TREC tasks are dra wn. W e did not e xplore using n -gram id f s. A tf component is not meaningful because the data are so sparse. 2.2 Inf ormati v eness Let G be the set of nuggets for some question. Infor -mati v eness of an n -gram for a nugget g is calculated based on ho w man y other nuggets in that question (  X  G ) contain the n -gram. Let i ( g , w where co u nt ( g , w 1 ... w n ) is the number of occur -rences of the n -gram in responses containing the nugget g .
 Then informati v eness is:
This captures the Bayesian intuition that the more outcomes a piece of e vidence is associated with, the less confidence we can ha v e in predicting the out-come based on that e vidence. 2.3 J udgement Nuggeteer does not guess on responses which ha v e been judged by a human to contain a nugget, or those which ha v e unambiguously judged not to, b ut as-signs the kno wn judgement. 2
F or unseen responses, we determine the n -gram recall for each nugget g and candidate response w 1 ... w l by breaking the candi date into n -grams and finding the sum of scores:
R ec al l ( g , w 1 ... w l ) = (3)
The candidate is considered to contain all nuggets whose recall e xceeds some threshold. Put another w ay , we b uild an n -gram language model for each nugget, and assign those nuggets whose predicted lik elihood e xceeds a threshold.

When se v eral responses contain a nugget, Nugge-teer picks the fir st (instead of the best, as assessors can) for purposes of scoring. 2.4 P arameter Estimation W e e xplored a number of parameters in the scor -ing function: stemming, n -gram size, id f weights vs. count weights, and the ef fect of remo ving stop-w ords. W e t ested all 24 combinations, and for each e xperiment, we cross-v alidated by lea ving out one submitted system, or where possible, one submitting institution (to a v oid training and testing on poten-tially v ery similar systems). 3
Each e xperiment w as performe d using a range of thresholds for Equation 3 abo v e, and we se-lected the best performing threshold for each data set. 4 Because the threshold w as selected after cross-v alidation, it is e xposed to o v ertraining. W e used a single global threshold to minimize this risk, b ut we ha v e no reason to think that the thresholds f or dif fer -ent nuggets are related.

Selecting thresholds as part of the t raining process can maximize accurac y while eliminating o v ertrain-ing. W e therefore e xplore d Bayesian models for au-tomatic threshold selection. W e model assignment of nuggets to responses as caused by the scores ac-cording to a noisy threshold funct ion, with separate ied thresholds and error rates by entire dataset, by question, or by indi vidual nugget, e v aluating them using Bayesian model selection. F or our e xperiments, we used the definition ques-tions from TREC2003, the  X  X ther X  questions from TREC2004 and TREC2005, and the relation-ship questions from TREC2005. (V oorhees, 2003; V oorhees, 2004; V oorhees, 2005) The distrib ution of nuggets and questions is sho wn for each data set in T able 1. The number of nuggets by number of Figure 4: Percents of nuggets, binned by the number of systems that found each nugget. system responses assigned that nugget (dif ficulty of nuggets, in a sense) is sho wn in Figure 4. M ore than a quarter of relationship nuggets were not found by an y system. Among all data sets, man y nuggets were found in none or just a fe w responses. W e report correlation ( R 2 ), and K endall X  s  X  b , follo w-ing Lin and Demner -Fushman. Nuggeteer X  s scores are in the same range as real system scores, so we also report a v erage root mean squared error from the of ficial results. W e  X  X orrected X  the of ficial judge-ments by assigning a nugget to a res ponse if that response w as judged to contai n that nugget in an y assessment for an y system. 4.1 Comparison with P our pr e (Lin et al., 2005) report Pourpre and Rouge perfor -mance with Pourpre optimal thresholds for TREC definition ques tions, as reproduced in T able 2. Nuggeteer X  s results are sho wn in the last column. 5 T able 3 sho ws a comparison of Pourpre and Nuggeteer X  s correlations with of ficial scores. As e x-question o v er all systems.

Run micro, cnt macro, cnt micro, idf macro, idf def ault stop D 2003 (  X  = 3 ) 0.846 0.886 0.848 0.876 0.780 0.816 0.879 D 2003 (  X  = 5 ) 0.890 0.878 0.859 0.875 0.807 0.843 0.849 O 2004 (  X  = 3 ) 0.785 0.833 0.806 0.812 0.780 0.786 0.898 O 2005 (  X  = 3 ) 0.598 0.709 0.679 0.698 0.662 0.670 0.858 R 2005 (  X  = 3 ) 0.697 1 Run R 2 R 2  X  mse D 2003 (  X  = 3 ) 0.963 0.966 0.067 D 2003 (  X  = 5 ) 0.965 0.971 0.077 O 2004 (  X  = 3 ) 0.929 0.982 0.026 O 2005 (  X  = 3 ) 0.916 0.952 0.026 R 2005 (  X  = 3 ) 0.764 0.993 0.009 T able 3: Correlation ( R 2 ) and Root Mean Squared Error ( pre/Nuggeteer and of ficial scores, for the same set-tings as the  X  comparison abo v e. pected from the K endall  X  s  X  comparisons, Pourpre X  s correlation is about the same or higher in 2003, b ut f ares progressi v ely w orse in the subsequent tasks.
T o ensure that Pourpre scores correlated suf-ficiently with of ficial scores, Lin and Demner -Fushman used the dif ference in of ficial score be-tween runs whose ranks Pourpre had sw apped, and sho wed that the majority of sw aps were between runs whose of ficial scores we re less than the 0.1 apart, a threshold for assessor agreement reported in (V oorhees, 2003).

Nuggeteer s cores are not only correlated with, b ut actually meant to approximate, the assessment scores; thus we can use a stronger e v aluation: root mean squared error of Nuggeteer scores ag ainst of-ficial scores. This estimates the a v erage dif ference between the Nuggeteer score and the of ficial score, and at 0.077, the estimate is belo w the 0.1 thresh-old. This e v aluation is meant to sho w that the scores are  X  X ood enough X  for e xperimental e v alua-tion, and in Section 4.4 we will substantiate Lin and Demner -Fushman X  s observ ation t hat higher correla-tion scores may reflect o v ertraining rather than ac-tual impro v ement.
 Accordingly , rather than reporting the best Nuggeteer scores (K endall X  s  X  and R 2 ) abo v e, we follo w Pourpre X  s lead in reporting a single v ariant (no stemming, bigrams) that performs well across the data sets. As with Pourpre X  s e v aluation, the par -Figure 5: Scatter graph of of ficial scores plot-ted ag ainst Nuggeteer scores ( id f term weighting, no stemming, bigrams) for each data set (all F-measures ha v e  X  = 3 ), with the Nuggeteer 95% confidence interv als on the score. Across the four datasets, 6 systems (3%) ha v e an of ficial score out-side Nuggeteer X  s 95% confidence interv al. ticular thresholds for each year are e xperimentally optimized. A scatter plot of Nuggeteer performance on the definition tasks is sho wn in Figure 5. 4.2 N -gram size and stemming A h ypothesis adv anced with Pourpre is that bigrams, trigrams, and longer n -grams will primarily account for the fluenc y of an answer , rather than its semantic content, and thus not aid the scoring process. W e included the option to use longer n -grams within Nuggeteer , and ha v e found that using bigram s can yield v ery slightly better results than using uni-grams. From inspection, bigrams sometimes capture named entity and grammatical order features.
Experiments with Pourpre sho wed that stemming hurt slightly at peak performances. Nuggeteer has the same tendenc y at all n -gram sizes.

Figure 6 compares K endall X  s  X  o v er the poss i-ble thresholds, n -gram lengths, and stemming. The choice of threshold matters by f ar the most. 4.3 T erm weighting and stopw ords Remo ving stopw ords or gi ving unit weight to all terms rather than an id f -based weight made no sub-stantial dif ference in Nuggeteer X  s performance. Figure 6: Fix ed thresholds vs. K endall X  s  X  for uni-grams, bigrams, or trigrams a v eraged o v er the three years of definition data using F (  X  = 3) .

Model log optimally biased coin -2780 global threshold -2239 per -question thresholds -1977 per -nugget thresholds -1546 per -nugget errors and thr . -1595 T able 4: The probabi lities of the data gi v en se v eral models: a baseline coin, three models of dif ferent granularity with globally specified f alse positi v e and ne g ati v e error rates, and a model with too man y pa-rameters, where e v en the e rror rates ha v e per -nugget granularity . W e select t he most probable model, the per -nugget threshold model. 4.4 Thr esholds W e e xperimented with Bayesian models for auto-matic threshold selection. In the models, a system response contains or does not contain each nugget as a function of the response X  s Nuggeteer score plus noise. T able 4 sho ws that, as e xpected, the best mod-els do not mak e assumptions about thresholds be-ing equal within a question or dataset. It is interest-ing to note that Bayesian inference catches the o v er -parametrization of the model where error rates v ary per -nugget as well. In essence, we do not need those additional parameters to e xplain the v ariation in the data.

The  X  of the bes t selection of parameters on the 2003 data set using the model with one threshold per nugget and global errors is 0.837 ( W e ha v e indeed o v ertrained the best threshold for this dataset (compare  X  =0.879, bles 2 and 3), suggesting that the num eric dif fer -ences in K endall X  s T au sho wn between the Nugge-teer , Pourpre, and Rouge systems are not indicati v e of true performance. The Bayesian model promises settings free of o v ertr aining, and thus more accurate judgements in t erms of classification accurac y . 4.5 T raining on System Responses Intuiti v ely , if a f act is e xpressed by a system re-sponse, then another response with s imilar n -grams may also contain the same f act. T o test this intuition, we tried e xpanding our judgement method (Equa-tion 3) to select the maximum judgement score from among those of the nugget description and each of the system responses judged to contain that nugget.
Unfortunately , the assessors did not mark which portion of a response e xpresses a nugget, so we also find spurious s imilarity , as sho wn in Figure 7. The fi-nal results are not conclusi v ely better or w orse o v er -all, and the process is f ar more e xpensi v e.
W e are currently e xploring the same e xtension for multiple  X  X ugget descriptions X  generated by manu-ally selecting the appropriate portions of system re-sponses containing each nugget. 4.6 J udgment Pr ecision and Recall Because Nuggeteer mak es a nugget classification for each system response, we can report precision and recall on the nugget assignments. T able 5 sho ws Nuggeteer X  s agreement rate with assessors on whether each response contains a nugget. 6 4.7 No v el J udgements Approximate e v aluation will tend to underv alue ne w results, simply because the y m ay not ha v e k e yw ord o v erlap with e xisting nugget descr iptions. W e are therefore creating tools to help de v elopers manually assess their system outputs.

As a proof of concept, we ran Nuggeteer on the best 2005  X  X ther X  system (not gi ving Nuggeteer
Data set best F(  X  = 1 ) default F(  X  = 1 ) 2003 defn 0.68  X  .01 0.66  X  .02 2004 other 0.73  X  .01 0.70  X  .01 2005 other 0.87  X  .01 0.86  X  .01 2005 reln 0.75  X  .04 0.72  X  .05 T able 5: Nuggeteer agreement with of ficial judge-ments, under best settings for each year , and under the def ault settings. the of ficial judgements), and manualy corrected its guesses. 7 Assessment took about 6 hours, and our judgements had precision of 78% and recall of 90%, for F-measure 0.803  X  0.065 (compare T able 5). The of ficial score of .299 w as still within the confidence interv al, b ut no w on the high side rather than the lo w (.257  X  .07 ), because we found the answers quite good. In f act, we were often tempted to add ne w nuggets! W e later learned that it w as a manual run, produced by a student at the Uni v ersity of Maryland. Pourpre pioneered automatic nugget-based assess-ment for definition questions, and thus enabled a rapid e xperimental c ycle of system de v elopment. Nuggeteer impro v es on that functionality , and crit-ically adds:  X  an interpretable score, comparable to of ficial  X  a reliable confidence interv al on the estimated  X  scoring kno wn responses e xactly ,  X  support for impro ving the accurac y of the score  X  a more rob ust training process
W e ha v e sho wn that Nugge teer e v aluates the def-inition and relationship tasks with comparable rank sw ap rates to Pourpre. W e e xplored the ef fects of stemming, t erm weighting, n -gram size, stopw ord remo v al, and use of system responses for training, all with little ef fect. W e sho wed that pre vious meth-ods of selecting a threshold o v ertrained, a nd ha v e question id 1901, r esponse r ank 2, r esponse scor e 0.14 assigned nug g et description : born brooklyn n y 1900 briefly described a promising w ay to select finer -grained thresholds automatically .

Our e xperiences in using judgements of system responses point to the need for a better annotation of nugget content. It is possible to gi v e Nuggeteer multiple nugget descriptions for each nugget. Man-ually e xtrac ting the rele v ant portions of correctly-judged system responses may not be an o v erly ardu-ous task, and may of fer higher accurac y . It w ould be ideal if the community X  X ncluding the assessors X  were able to create and promulg ate a gold-standard set of nugget descriptions for pre vious years. Nuggeteer currently supports e v aluation for the TREC definition,  X  X ther X , and relationship tasks, for the A Q U AINT opinion pilot 8 , and is under de v el-opment for the D ARP A GALE task 9 . W e w ould lik e to thank Jimmy Lin and Dina Demner -Fushman for v aluable discussions, for Fig-ure 3, and T able 2, and for creating P ourpre. Thanks to Ozlem Uzune r and Sue Felshin for v aluable com-ments on earlier drafts of this paper and to Boris Katz for his inspiration and support.
