 Recently, there has been increased community in-terest in the theoretical and practical analysis of what Morante and Sporleder (2012) call modality and negation , i.e. linguistic expressions that mod-ulate the certainty or factuality of propositions. Automated analysis of such aspects of meaning is important for natural language processing tasks which need to consider the truth value of state-ments, such as for example text mining (Vincze et al., 2008) or sentiment analysis (Lapponi et al., 2012). Owing to its immediate utility in the cura-tion of scholarly results, the analysis of negation and so-called hedges in bio-medical research liter-ature has been the focus of several workshops, as well as the Shared Task at the 2011 Conference on Computational Language Learning (CoNLL).

Task 1 at the First Joint Conference on Lex-ical and Computational Semantics ( * SEM 2012; Morante and Blanco, 2012) provided a fresh, prin-cipled annotation of negation and called for sys-tems to analyze negation X  X etecting cues (affixes, words, or phrases that express negation), resolv-ing their scopes (which parts of a sentence are ac-tually negated), and identifying the negated event or property. The task organizers designed and documented an annotation scheme (Morante and Daelemans, 2012) and applied it to a little more than 100,000 tokens of running text by the nov-elist Sir Arthur Conan Doyle. While the task and annotations were framed from a semantic perspec-tive, only one participating system actually em-ployed explicit compositional semantics (Basile et al., 2012), with results ranking in the middle of the 12 participating systems. Conversely, the best-performing systems approached the task through machine learning or heuristic processing over syn-tactic and linguistically relatively coarse-grained representations; see  X  2 below.

Example (1), where  X  X  marks the cue and {} the in-scope elements, illustrates the annotations, including how negation inside a noun phrase can
In this work, we return to the 2012 * SEM task from a deliberately semantics-centered point of view, focusing on the hardest of the three and Daelemans (2012) characterize negation as an  X  X xtra-propositional aspect of meaning X  (p. 1563), we in fact see it as a core piece of composi-tionally constructed logical-form representations. Though the task-specific concept of scope of negation is not the same as the notion of quan-tifier and operator scope in mainstream under-specified semantics, we nonetheless find that re-viewing the 2012 * SEM Shared Task annotations with reference to an explicit encoding of seman-tic predicate-argument structure suggests a sim-ple and straightforward operationalization of their concept of negation scope. Our system imple-ments these findings through a notion of functor-argument  X  X rawling X , using as our starting point the underspecified logical-form meaning represen-tations provided by a general-purpose deep parser.
Our contributions are three-fold: Theoretically, we correlate the structures at play in the Morante and Daelemans (2012) view on negation with formal semantic analyses; methodologically, we demonstrate how to approach the task in terms of underspecified, logical-form semantics; and prac-tically, our combined system retroactively  X  X ins X  the 2012 * SEM Shared Task. In the following sections, we review related work ( X  2), detail our own setup ( X  3), and present and discuss our ex-perimental results ( X  4 and  X  5, respectively). Read et al. (2012) describe the best-performing submission to Task 1 of the 2012 * SEM Confer-ence. They investigated two approaches for scope resolution, both of which were based on syntac-tic constituents. Firstly, they created a set of 11 heuristics that describe the path from the preter-minal of a cue to the constituent whose projec-tion is predicted to match the scope. Secondly they trained an SVM ranker over candidate con-stituents, generated by following the path from a cue to the root of the tree and describing each candidate in terms of syntactic properties along the path and various surface features. Both ap-proaches attempted to handle discontinuous in-stances by applying two heuristics to the predicted scope: (a) removing preceding conjuncts from the scope when the cue is in a conjoined phrase and (b) removing sentential adverbs from the scope. The ranking approach showed a modest advan-tage over the heuristics (with F 1 equal to 77.9 and 76.7, respectively, when resolving the scope of gold-standard cues in evaluation data). Read et al. (2012) noted however that the annotated scopes did not align with the Shared Task X  X rovided con-stituents for 14% of the instances in the training data, giving an F 1 upper-bound of around 86.0 for systems that depend on those constituents.

Basile et al. (2012) present the only submission to Task 1 of the 2012 * SEM Conference which employed compositional semantics. Their scope resolution pipeline consisted primarily of the C&amp;C parser and Boxer (Curran et al., 2007), which pro-duce Discourse Representation Structures (DRSs). The DRSs represent negation explicitly, including representing other predications as being within the scope of negation. Basile et al. (2012) describe some amount of tailoring of the Boxer lexicon to include more of the Shared Task scope cues among those that produce the negation operator in the DRSs, but otherwise the system appears to directly take the notion of scope of negation from the DRS and project it out to the string, with one caveat: As with the logical-forms representations we use, the DRS logical forms do not include function words as predicates in the semantics. Since the Shared Task gold standard annotations included such ar-guably semantically vacuous (see Bender, 2013, p. 107) words in the scope, further heuristics are needed to repair the string-based annotations com-ing from the DRS-based system. Basile et al. re-sort to counting any words between in-scope to-kens which are not themselves cues as in-scope. This simple heuristic raises their F 1 for full scopes from 20.1 to 53.3 on system-predicted cues. The new system described here is what we call the MRS Crawler. This system operates over the normalized semantic representations provided by the LinGO English Resource Grammar (ERG; to meaning representations in the format of Mini-mal Recursion Semantics (MRS; Copestake et al., 2005). MRS makes explicit predicate-argument relations, as well as partial information about scope (see below). We used the grammar together with one of its pre-packaged conditional Maxi-mum Entropy models for parse ranking, trained on a combination of encyclopedia articles and tourism brochures. Thus, the deep parsing front-end system to our MRS Crawler has not been adapted to the task or its text type; it is applied in an  X  X ff the shelf X  setting. We combine our system with the outputs from the best-performing 2012 submission, the system of Read et al. (2012), firstly by relying on the latter for system negation tem combination as described in  X  3.4 below.
Scopal information in MRS analyses delivered by the ERG fixes the scope of operators X  X uch as negation, modals, scopal adverbs (including sub-ordinating conjunctions like while ), and clause-embedding verbs (e.g. believe ) X  X ased on their position in the constituent structure, while leaving the scope of quantifiers (e.g. a or every , but also other determiners) free. From these underspec-ified representations of possible scopal configu-rations, a scope resolution component can spell out the full range of fully-connected logical forms (Koller and Thater, 2005), but it turns out that such enumeration is not relevant here: the notion of scope encoded in the Shared Task annotations is not concerned with the relative scope of quantifiers and negation, such as the two possible readings of However, as shown below, the information about fixed scopal elements in an underspecified MRS is sufficient to model the Shared Task annotations. 3.1 MRS Crawling Fig. 1 shows the ERG semantic analysis for our running example. The heart of the MRS is a mul-tiset of elementary predications (EPs). Each ele-mentary prediction includes a predicate symbol, a label (or  X  X andle X , prefixed to predicates with a colon in Fig. 1), and one or more argument positions, whose values are semantic variables. Eventualities ( e i ) in MRS denote states or activ-ities, while instance variables ( x j ) typically corre-spond to (referential or abstract) entities. All EPs have the argument position ARG0 , called the dis-tinguished variable (Oepen and L X nning, 2006), and no variable is the ARG0 of more than one non-quantifier EP.

The arguments of one EP are linked to the argu-ments of others either directly (sharing the same variable as their value), or indirectly (through so-called  X  X andle constraints X , where = q in Fig. 1 de-notes equality modulo quantifier insertion). Thus a well-formed MRS forms a connected graph. In addition, the grammar links the EPs to the ele-ments of the surface string that give rise to them, via character offsets recorded in each EP (shown in angle brackets in Fig. 1). For the purposes of the present task, we take a negation cue as our en-try point into the MRS graph (as our initial active EP), and then move through the graph according to the following simple operations to add EPs to the active set: Argument Crawling Add to the scope all EPs whose distinguished variable or label is an argu-ment of the active EP; for arguments of type h k , treat any = q constraints as label equality. Label Crawling Add all EPs whose label is iden-tical to that of the active EP.
 Functor Crawling Add all EPs that take the dis-tinguished variable or label of the active EP as an argument (directly or via = q constraints). Our MRS crawling algorithm is sketched in Fig. 2. To illustrate how the rules work, we will trace their operation in the analysis of example (1), i.e. traverse the EP graph in Fig. 1.

The negation cue is nothing , from character po-sition 46 to 53. This leads us to _no_q as our en-try point into the graph. Our algorithm states that for this type of cue (a quantifier) the first step is functor crawling (see  X  3.3 below), which brings _know_v_1 into the scope. We proceed with ar-gument crawling and label crawling , which pick up _the_q  X  0:3  X  and _german_n_1 as the ARG1 . Further, as the ARG2 of _know_v_1 , we reach thing and through recursive invocation we acti-vate _of_p and, in yet another level of recursion, _the_q  X  57:60  X  and _matter_n_of . At this point, crawling has no more links to follow. Thus, the MRS crawling operations  X  X aint X  a subset of the MRS graph as in-scope for a given negation cue. 3.2 Semantically Empty Word Handling Our crawling rules operate on semantic represen-tations, but the annotations are with reference to the surface string. Accordingly, we need projec-tion rules to map from the  X  X ainted X  MRS to the string. We can use the character offsets recorded in each EP to project the scope to the string. How-ever, the string-based annotations also include words which the ERG treats as semantically vacu-ous. Thus in order to match the gold annotations, we define a set of heuristics for when to count vac-uous words as in scope. In (1), there are no se-mantically empty words in-scope, so we illustrate these heuristics with another example: The MRS crawling operations discussed above paint the EPs corresponding to is , thing , of , conse-quence , I , and overlooked as in-scope (underlined in (3)). Conversely, the ERG treats the words that , there , which , and have as semantically empty. Of these, we need to add all except that to the scope. Our vacuous word handling rules use the syntac-tic structure provided by the ERG as scaffolding to help link the scope information gleaned from con-tentful words to vacuous words. Each node in the syntax tree is initially colored either in-scope or out-of-scope in agreement with the decision made by the crawler about the lexical head of the corre-sponding subtree. A semantically empty word is determined to be in-scope if there is an in-scope syntax tree node in the right position relative to it, as governed by a short list of templates organized by the type of the semantically empty word (par-ticles, complementizers, non-referential pronouns, relative pronouns, and auxiliary verbs).

As an example, the rule for auxiliary verbs like have in our example (3) is that they are in scope when their verb phrase complement is in scope. Since overlooked is marked as in-scope by the crawler, the semantically empty have becomes in-scope as well. Sometimes the rules need to be iterated. For example, the main rule for relative pronouns is that they are in-scope when they fill a gap in an in-scope constituent; which fills a gap in the constituent have overlooked , but since have is the (syntactic) lexical head of that constituent, the verb phrase is not considered in-scope the first time the rules are tried.

Similar rules deal with that (complementizers are in-scope when the complement phrase is an ar-gument of an in-scope verb, which is not the case here) and there (non-referential pronouns are in-scope when they are the subject of an in-scope VP, which is true here). 3.3 Re-Reading the Annotation Guidelines Our MRS crawling algorithm was defined by look-ing at the annotated data rather than the annota-tion guidelines for the Shared Task (Morante et al., 2011). Nonetheless, our algorithm can be seen as a first pass formalization of the guidelines. In this section, we briefly sketch how our algorithm cor-responds to different aspects of the guidelines.
For negated verbs, the guidelines state that  X  X f the negated verb is the main verb in the sen-tence, the entire sentence is in scope. X  (Morante et al., 2011, 17). In terms of our operations de-fined over semantic representations, this is ren-dered as follows: all arguments of the negated verb are selected by argument crawling , all in-tersective modifiers by label crawling , and func-tor crawling (Fig. 2, line 8) captures modal auxil-iaries and non-intersective modifiers. The guide-lines treat predicative adjectives under a separate heading from verbs, but describe the same desired annotations (scope over the whole clause; ibid., p. 20). Since these structures are analogous in the semantic representations, the same operations that handle negated verbs also handle negated predica-tive adjectives correctly.

For negated subjects and objects, the guidelines state that the negation scopes over  X  X ll the clause X  and  X  X he clause headed by the verb X  (Morante et al., 2011, 19), respectively. The examples given in the annotation guidelines suggest that these are in fact meant to refer to the same thing. The negation cue for a negated nominal argument will appear as a quantifier EP in the MRS, triggering line 3 of our algorithm. This functor crawling step will get to the verb X  X  EP, and from there, the process is the same as the last two cases.

In contrast to subjects and objects, negation of a clausal argument is not treated as negation of the verb (ibid., p. 18). Since in this case, the negation cue will not be a quantifier in the MRS, there will be no functor crawling to the verb X  X  EP.

For negated modifiers, the situation is somewhat more complex, and this is a case where our crawl-ing algorithm, developed on the basis of the anno-tated data, does not align directly with the guide-lines as given. The guidelines state that negated at-tributive adjectives have scope over the entire NP (including the determiner) (ibid., p. 20) and anal-ogously negated adverbs have scope over the en-tire clause (ibid., p. 21). However, the annotations are not consistent, especially with respect to the treatment of negated adjectives: while the head noun and determiner (if present) are typically an-notated as in scope, other co-modifiers, especially long, post-nominal modifiers (including relative clauses) are not necessarily included: Furthermore, the guidelines treat relative clauses as subordinate clauses and thus negation inside a relative clause is treated as bound to that clause only, and includes neither the head noun of the relative clause nor any of its other dependents in its scope. However, from the perspective of MRS, a negated relative clause is indistinguishable from any other negated modifier of a noun. This treat-ment of relative clauses (as well as the inconsis-tencies in other forms of co-modification) is the reason for the exception noted at line 7 of Fig. 2. By disallowing the addition of EPs to the scope if they share the label of the negation cue but are not one of its arguments, we block the head noun X  X  EP (and any EPs only reachable from it) in cases of relative clauses where the head verb inside the rel-ative clause is negated. It also blocks co-modifiers like great , own , and the phrases headed by ready and about in (4) X (7). As illustrated in these exam-ples, this is correct some but not all of the time. Having been unable to find a generalization cap-turing when comodifiers are annotated as in scope, we stuck with this approximation.

For negation within clausal modifiers of verbs, the annotation guidelines have further informa-tion, but again, our existing algorithm has the cor-rect behavior: The guidelines state that a negation cue inside of the complement of a subordinating conjunction (e.g. if ) has scope only over the sub-ordinate clause (ibid., p. 18 and p. 26). The ERG treats all subordinating conjunctions as two-place predicates taking two scopal arguments. Thus, as with clausal complements of clause-embedding verbs, the embedding subordinating conjunction and any other arguments it might have are inac-cessible, since functor crawling is restricted to a handful of specific configurations.
As is usually the case with exercises in for-malization, our crawling algorithm generalizes be-yond what is given explicitly in the annotation guidelines. For example, all arguments that are treated as semantically nominal (including PP ar-guments where the preposition is semantically null) are treated in the same way as subjects and objects; similarly, all arguments which are seman-tically clausal (including certain PP arguments) are handled the same way as clausal complements. This is possible because we take advantage of the high degree of normalization that the ERG accom-plishes in mapping to the MRS representation.
There are also cases where we are more spe-cific. The guidelines do not handle coordination in detail, except to state that in coordinated clauses negation is restricted to the clause it appears in (ibid., p. 17 X 18) and to include a few examples of coordination under the heading  X  X llipsis X . In the case of VP coordination, our existing algorithm does not need any further elaboration to pick up the subject of the coordinated VP but not the non-negated conjunct, as shown in discussion of (1) in  X  3.1 above. In the case of coordination of negated NPs, recall that to reach the main portion of the negated scope we must first apply functor crawl-ing. The functor crawling procedure has a general mechanism to transparently continue crawling up through coordinated structures while blocking fu-
On the other hand, there are some cases in the annotation guidelines which our algorithm does not yet handle. We have not yet provided any anal-ysis of the special cases for save and expect dis-cussed in Morante et al., 2011, pp. 22 X 23, and also do not have a means of picking out the overt verb in gapping constructions (p. 24).

Finally, we note that even carefully worked out annotation guidelines such as these are never fol-lowed perfectly consistently by the human annota-tors who apply them. Because our crawling algo-rithm so closely models the guidelines, this puts our system in an interesting position to provide feedback to the Shared Task organizers. 3.4 Fall-Back Configurations The close match between our crawling algorithm and the annotation guidelines supported by the mapping to MRS provides for very high precision and recall when the analysis engine produces the not always provide the desired analysis, largely because of idiosyncrasies of the genre (e.g. voca-tives appearing mid-sentence) that are either not handled by the grammar or not well modeled in the parse selection component. In addition, as noted above, there are a handful of negation cues we do not yet handle. Thus, we also tested fall-back con-figurations which use scope predictions based on MRS in some cases, and scope predictions from the system of Read et al. (2012) in others. Our first fall-back configuration (Crawler N in Table 1) uses MRS-based predictions whenever there is a parse available and the cue is one that our system handles. Sometimes, the analysis picked by the ERG X  X  statistical model is not the correct analysis for the given context. To com-bat such suboptimal parse selection performance, we investigated using the probability of the top ranked analysis (as determined by the parse selec-tion model and conditioned on the sentence) as a confidence metric. Our second fall-back configu-ration (Crawler P in Table 1) uses MRS-based pre-dictions when there is a parse available whose con-ditional probability is at least 0 . 5 . 8 We evaluated the performance of our system using the Shared Task development and evaluation data (respectively CDD and CDE in Table 1). Since we do not attempt to perform cue detection, we report performance using gold cues and also using the system cues predicted by Read et al. (2012). We used the official Shared Task evaluation script to compute all scores. 4.1 Data Sets The Shared Task data consists of chapters from the Adventures of Sherlock Holmes mystery nov-els and short stories. As such, the text is carefully annotated with token-level information about the cues and scopes in every negated sentence. The training set contains 848 negated sentences, the development set 144, and the evaluation set 235. As there can be multiple usages of negation in one sentence, this corresponds to 984, 173, and 264 instances, respectively.

Being rule-based, our system does not require any training data per se. However, the majority of our rule development and error analysis were per-formed against the designated training data. We used the designated development data for a single final round of error analysis and corrections. The system was declared frozen before running with the formal evaluation data. All numbers reported 4.2 Results Table 1 presents the results of our various config-urations in terms of both (a) whole scopes (i.e. a true positive is only generated when the predicted scope matches the gold scope exactly) and (b) in-scope tokens (i.e. a true positive for every token the system correctly predicts to be in scope). The table also details the performance upper-bound for system combination, in which an oracle selects the system prediction which scores the greater token-wise F 1 for each gold cue.

The low recall levels for Crawler can be mostly attributed to imperfect parser coverage. Crawler N , which falls back just for parse failure brings the recall back up, and results in F 1 levels closer to the system of Read et al. (2012), albeit still not quite advancing the state of the art (except over the development set). Our best results are from Crawler P , which outperforms all other configura-tions on the development and evaluation sets.
The Oracle results are interesting because they show that there is much more to be gained in com-bining our semantics-based system with the Read et al. (2012) syntactically-focused system. Further analysis of these results to draw out the patterns of complementary errors and strengths is a promising avenue for future work. 4.3 Error Analysis To shed more light on specific strengths and weak-nesses of our approach, we performed a manual er-ror analysis of scope predictions by Crawler, start-ing from gold cues so as to focus in-depth analy-sis on properties specific to scope resolution over MRSs. This analysis was performed on CDD, in order to not bar future work on this task. Of the 173 negation cue instances in CDD, Crawler by it-self makes 94 scope predictions that exactly match the gold standard. In comparison, the system of Read et al. (2012) accomplishes 119 exact scope matches, of which 80 are shared with Crawler; in other words, there are 14 cue instances (or 8% of all cues) in which our approach can improve over the best-performing syntax-based submission to the original Shared Task. We reviewed the 79 negation instances where Crawler made a wrong prediction in terms of ex-act scope match, categorizing the source of failure into five broad error types: (1) Annotation Error In 11% of all instances, we consider the annotations erroneous or inconsistent. These judgments were made by two of the authors, who both were familiar with the annotation guide-lines and conventions observable in the data. For example, Morante et al. (2011) unambiguously state that subordinating conjunctions shall not be in-scope (8), whereas relative pronouns should be (9), and a negated predicative argument to the cop-ula must scope over the full clause (10): (2) Parser Failure Close to 30% of Crawler fail-ures reflect lacking coverage in the ERG parser, i.e. inputs for which the parser does not make available an analysis (within certain bounds on treated the ERG as an off-the-shelf system, but coverage could certainly be straightforwardly im-proved by adding analyses for phenomena partic-ular to turn-of-the-20th-century British English. (3) MRS Inadequacy Another 33% of our false scope predictions are Crawler-external, viz. owing to erroneous input MRSs due to imperfect disam-biguation by the parser or other inadequacies in the parser output. Again, these judgments (assign-ing blame outside our own work) were double-checked by two authors, and we only counted MRS imperfections that actually involve the cue or in-scope elements. Here, we could anticipate improvements by training the parse ranker on in-domain data or otherwise adapting it to this task. (4) Cue Selection In close to 9% of all cases, there is a valid MRS, but Crawler fails to pick out an initial EP that corresponds to the negation cue. This first type of genuine crawling failure often re-lates to cues expressed as affixation (11), as well as to rare usages of cue expressions that predomi-nantly occur with different categories, e.g. neither as a generalized quantifier (12): (5) Crawler Deficiency Finally, a little more than 16% of incorrect predictions we attribute to our crawling rules proper, where we see many instances of under-coverage of MRS elements (13, 14) and a few cases of extending the scope too wide (15). In the examples below, erroneous scope predictions by Crawler are indicated through un-derlining. Hardly any of the errors in this category, however, involve semantically vacuous tokens. The example in (1) nicely illustrates the strengths of the MRS Crawler and of the abstraction pro-vided by the deep linguistic analysis made pos-sible by the ERG. The negated verb in that sen-tence is know , and its first semantic argument is The German . This semantic dependency is di-rectly and explicitly represented in the MRS, but the phrase expressing the dependent is not adja-cent to the head in the string. Furthermore, even a system using syntactic structure to model scope would be faced with a more complicated task than our crawling rules: At the level of syntax the de-pendency is mediated by both verb phrase coordi-nation and the control verb profess , as well as by the semantically empty infinitival marker to .
The system we propose is very similar in spirit to that of Basile et al. (2012). Both systems map from logical forms with explicit representations of scope of negation out to string-based annotations in the format provided by the Shared Task gold standard. The main points of difference are in the robustness of the system and in the degree of tai-loring of both the rules for determining scope on the logical form level and the rules for handling se-mantically vacuous elements. The system descrip-tion in Basile et al. (2012) suggests relatively little tailoring at either level: aside from adjustments to the Boxer lexicon to make more negation cues take the form of the negation operator in the DRS, the notion of scope is directly that given in the DRS. Similarly, their heuristic for picking up semanti-cally vacuous words is string-based and straight-forward. Our system, on the other hand, models the annotation guidelines more closely in the def-inition of the MRS crawling rules, and has more elaborated rules for handling semantically empty words. The Crawler alone is less robust than the Boxer-based system, returning no output for 29% of the cues in CDE. These factors all point to higher precision and lower recall for the Crawler compared to the Boxer-based system. At the to-ken level, that is what we see. Since full-scope re-call depends on token-level precision, the Crawler does better across the board at the full-scope level. A comparison of the results is shown in Table 2.
A final key difference between our results and those of Basile et al. (2012) is the cascading with a fall-back system. Presumably a similar system combination strategy could be pursued with the Boxer-based system in place of the Crawler. Our motivation in this work was to take the design of the 2012 * SEM Shared Task on negation analy-sis at face value X  X s an overtly semantic problem that takes a central role in our long-term pursuit of language understanding . Through both theoreti-cal and practical reflection on the nature of repre-sentations at play in this task, we believe we have demonstrated that explicit semantic structure will be a key driver of further progress in the analy-sis of negation. We were able to closely align two independently developed semantic analyses X  the negation-specific annotations of Morante et al. (2011), on the one hand, and the broad-coverage, MRS meaning representations of the ERG, on the other hand. In our view, the conceptual correla-tion between these two semantic views on nega-tion analysis reinforces their credibility.
Unlike the rather complex top-performing sys-tems from the original 2012 competition, our MRS Crawler is defined by a small set of general rules that operate over general-purpose, explicit mean-ing representations. Thus, our approach scores high on transparency, adaptability, and replicabil-ity. In isolation, the Crawler provides premium precision but comparatively low recall. Its limi-tations, we conjecture, reflect primarily on ERG parsing challenges and inconsistencies in the tar-get data. In a sense, our approach pushes a larger proportion of the task into the parser, mean-ing (a) there should be good opportunities for parser adaptation to this somewhat idiosyncratic text type; (b) our results can serve to offer feed-back on ERG semantic analyses and parse rank-ing; and (c) there is a much smaller proportion of very task-specific engineering. When embed-ded in a confidence-thresholded cascading archi-tecture, our system advances the state of the art on this task, and oracle combination scores sug-gest there is much remaining room to better ex-ploit the complementarity of approaches in our study. In future work, we will seek to better un-derstand the division of labor between the systems involved through contrastive error analysis and possibly another oracle experiment, constructing gold-standard MRSs for part of the data. It would also be interesting to try a task-specific adaptation of the ERG parse ranking model, for example re-training on the pre-existing treebanks but giving preference to analyses that lead to correct Crawler results downstream.
 We are grateful to Dan Flickinger, the main devel-oper of the ERG, for many enlightening discus-sions and continuous assistance in working with the analyses available from the grammar. This work grew out of a discussion with colleagues of the Language Technology Group at the University of Oslo, notably Elisabeth Lien and Jan Tore L X n-ning, to whom we are indebted for stimulating co-operation. Furthermore, we have benefited from comments by participants of the 2013 DELPH-IN Summit, in particular Joshua Crowgey, Guy Emerson, Glenn Slayden, Sanghoun Song, and Rui Wang.
