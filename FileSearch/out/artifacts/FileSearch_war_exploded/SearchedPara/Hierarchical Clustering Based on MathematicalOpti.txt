 Multilevel hierarchical clustering consists of grouping data objects into a hi-erarchy of clusters. It has a long history (see e.g. [2], [5], [15]) and has many important applications in various domains, since many kinds of data, includ-ing observational data collected in th e human and biological sciences, have a hierarchical, nested, or clustered structure. Hierarchical clustering algorithms are useful to determine hierarchical mu lticast trees in the network topology identification, Grid computing using in e-Science, e-Medicine or e-Commerce, Multimedia conferencing, Large-scale dissemination of timely information, ...
A hierarchical clustering of a set of obj ects can be described as a tree, in which the leaves are precisely the objects to be c lustered. A hierarchical clustering scheme produces a sequence of clusterings in which each clustering is nested into the next clustering in the sequence. Sta ndard existing methods for Multilevel hierarchical clustering are often based upon nonhierarchical clustering algorithms coupled with several iterative control str ategies to repeatedly modify an initial clustering (reordering, and reclus tering) in search of a better one.
To our knowledge, while mathematical programming is widely used for non-hierarchical clustering problems there exist a few optimization models and tech-niques for multilevel hierarchical clustering ones. Except the work in [14] we have not found other approaches using mathematical programming model for multilevel hierarchical clustering.

In this paper we investigate an efficient optimization approach for a model of this class, that is bilevel hierarchical clustering. The problem can be stated as follows. Given a set A of p objects A := { a j  X  IR n : j =1 , ..., p } ,ameasured distance, and an integer k .Wearetochoose k +1 members in A ,oneasthe assign other members of A to their closest centre. The total centre is defined as the closest object to all centres (in the sense that the sum of distances between it and all centres is the smallest).
 Our approach is based on mathematical optimization via DC (Difference of Convex functions) programming -which deals with DC programs, i.e., the min-imization of a DC function over a convex set -and DC optimization Algorithm called DCA. They were introduced by Pham Dinh Tao in their preliminary form in 1986 and have been extensively developed since 1994 by Le Thi Hoai An and Pham Dinh Tao to become now classic and more and more popular (see e.g. [7], [8] -[12], [16], [17] and references ther ein ). DCA has been successfully applied to many large-scale (smooth or nonsmooth) nonconvex programs in various do-mains of applied sciences, in particular in data analysis and data mining ([1], to be more robust and efficient than standard methods.

We propose in this work a new optimization formulation that seems to be appropriate for hierarchical clustering. This is a nonsmooth, nonconvex prob-lem and can be reformulated as a DC program which we then suggested using DC programming approach and DCA to solve. Preliminary numerical results on some artificial and real-world databases demonstrate that the proposed al-gorithm is very promising and more efficient than some existing optimization based clustering algorithms.

The paper is organized as follows. Sectio n 2 introduces a novel optimization model for the bilevel hierarchical clustering problem. Section 3 deals with DC programming and DCA for solving the underlying bilevel hierarchical clustering problem. For the reader X  X  convenience, at the beginning of this section we provide a brief introduction to DC programming and DCA. Computational results are reported in the last section. In [14] the authors have proposed two nonsmooth, nonconvex optimization mod-els for the bilevel hierarchical clustering problem in the context of determining a multicast group. Th ey considered the set A as the set of p nodes in the plane, and the measured distance is the Euclidean distance. The disadvantages of their models are the following:  X  first, the total centre is d etermined according to other centres -this is not  X  second, in their approach using the artifi cial centres the constraints do not  X  third, these problems can be formulated as DC programs, but it is not suit-
In this work we introduce a novel model that seems to be more appropri-ate: we search simultaneously the total centre and other centres. Moreover, by considering the squared Euclidean distance as the measured distance we get a DC program for which DCA is explicitly determined and very inexpensive.
Denoting by x i , i =1 ,...,k the centre of clusters in the second level and x k +1 the total centre we can formulate the problem in the form min The objective function containing the two terms is nonsmooth and nonconvex. The first term is a cluster function while the second term presents the distance between the total centre and the other cen tres. The constrai nt ensures that all centres are in the set A . The advantage of this formulation is that all centres are found in the same time.

This is a hard constrained global optimization problem. Using penalty tech-nique in DC programming ([8], [12]) leads us to the more tractable unconstrained nonsmooth nonconvex optimization problem (  X &gt; 0 is the penalty parameter):
We will prove in Section 3 that this problem can be reformulated as a DC program and show how to use DCA for solving it. 3.1 A Brief Presentation of DC Programming and DCA To give the reader an easy understanding of the theory of DC programming &amp; DCA and our motivation to use them for solving Problem (2) , we briefly outline these tools in this section. Let  X  0 (IR n ) denote the convex cone of all lower semicontinuous proper convex functions on IR n . The vector space of DC life objective functions and is closed under all the operations usually considered in optimization.

Consider the general DC program with g, h  X   X  0 (IR n ) . Such a function f is called DC function, and g  X  h ,DC decomposition of f while the convex functions g and h are DC components of f.
If g or h are polyhedral convex functions then ( P dc ) is called a polyhedral DC program.

It should be noted that a constrained DC program whose feasible set C is convex can always be transformed into an unconstrained DC program by adding the indicator function  X  C of C (  X  C ( x )=0 if x  X  C, +  X  otherwise) to the first DC component g .

Let is characterized as a pointwise supremum of a collection of affine functions, say we have with  X  ( y ):=inf x  X  IR n { g ( x )  X  [ x, y  X  h  X  ( y )] } ( P y ) .
It is clear that ( P y ) is a convex program and Finally we state the dual program of ( P dc ) that is written, in virtue of the natural convention in DC programming, say +  X  =+  X  X  X  (+  X  ) : We observe the perfect symmetry between primal and dual DC programs: the dual to ( D dc ) is exactly ( P dc ) .
 DC programming investigates the structure of the vector space DC (IR n ) , DC duality and optimality conditions for DC programs. The complexity of DC programs resides, of course, in the lack of practical optimal globality conditions. We developed instead the following necessary local optimality conditions for DC programs in their primal part, by symmetry their dual part is trivial (see [8] -[12], [16], [17] and references therein): (such a point x  X  is called critical point of g  X  h or for ( P dc ) ), and The condition (4) is also sufficient for many classes of DC programs. In particular it is sufficient for the next cases quite often encountered in practice:  X  In polyhedral DC programs with h being a polyhedral convex function (see  X  In case the function f is locally convex at x  X  ([10], [12]).
 Based on local optimality conditions and duality in DC programming, the DCA consists in the construction of two sequences { x k } and { y k } , candidates to be optimal solutions of primal and dual programs respectively, such that the { verifying local optimality conditions and These two sequences { x k } and { y k } are determined in the way that x k +1 (resp. y k ) is a solution to the convex program ( P k ) (resp. ( D k )) defined by The interpretation of DCA is simple: at each iteration one replaces in the primal DC program ( P dc ) the second component h by its affine minorization h ( x ):= h ( x k )+ x  X  x k ,y k at a neighbourhood of x k to give birth to the the second DC component g  X  of the dual DC program ( D dc ) is replaced by its performs so a double linearization with the help of the subgradients of h and g  X  and the DCA then yields the next scheme: First of all, it is worth noting that our works involve the convex DC components g and h but not the DC function f itself. Moreover, a DC function f has infinitely many DC decompositions which have crucial impacts on the qualities (speed of convergence, robustness, efficiency, globality of computed solutions,...) of DCA. For a given DC program, the choice of optimal DC decompositions is still open. Of course, this depends strongly on the very specific structure of the problem being considered. In order to tackle the large scale setting, one tries in practice to choose g and h such that sequences { x k } and { y k } can be easily calculated, i.e. either they are in explicit form or their computations are inexpensive.
It is proved in [8] -[12], [16], [17]) that DCA is a descent method without linesearch which enjoys th e following properties: ii) If the optimal value  X  of problem ( P dc ) is finite and the infinite sequences iii) DCA has a linear convergence for general DC programs. iv) DCA has a finite convergence for polyhedral DC programs.

For a complete study of DC programmi ng and DCA the redear is referred to [7], [8] -[12], [16], [17] and referen ces therein. The solution of a nonconvex program by DCA must be composed of two stages: the search of an appropriate DC decomposition and that of a good initial point. We shall apply all these DC enhancement features to solve problem (2) in its equivalent DC program given in the next. 3.2 Solving Problem (2) by DCA To simplify related computations in DCA for solving problem (2) we will work then X  X  IR ( k +1)  X  n whose i th row X i is equal to x i for i =1 , ..., k +1 : product and its Euclidean norm: ( Tr denotes the trace of a square matrix). We will reformulate problem (2) as a DC program in the matrix space IR ( k +1)  X  n and then describe DCA for solving it. DC Formulation of (2). According to the property we can write the objective function of (2), denoted F ,as F ( X )= where It is easy to see that G and H are convex functions and then (2) is DC program in the form
According to Section 3.1, d etermining the DCA scheme applied to (8) amounts We shall present below the computation of  X  X  ( X ) and  X  X   X  ( Y ) . Computing of  X  X  ( X ) . We have where The functions h 1 j,i are differentiable and Hence the subdifferential of H 1 can be explicitly determined as follows: ( co de-notes the convex hull) Likewise we have and the functions h 2 i,j are differentiable of which the derivative is computed as The subdifferential of H 2 ( X ) is therefore also explicitly determined.Finally for H 3 we get Computing of  X  X   X  ( X ) . Let G 1 and G 2 be the functions defined by Then, according to (7): G 1 in the form On the other hand we can express G 2 as where W =( w ij )  X  IR ( k +1)  X  ( k +1) is the matrix defined by The convex function G is then a positive definite quadratic form on IR ( k +1)  X  n and its gradient is given by  X  G ( X )=(  X  +1) iff Y =  X  G ( X ) ,weget Y =[(  X  +1) pI + W T W ] X  X  (  X  +1) A or [(  X  +1) pI + W T W ] X = Y +(  X  +1) A. This permits us to compute explicitly X as follows: with B = Y +(  X  +1) A and c =(  X  +1) p .

In the matrix space IR ( k +1)  X  n , according to (7), (18), (19) and (20) the DC program (8) then is minimizing the difference of the simple convex quadratic function and the nonsmooth convex function. This nice feature is very convenient for applying DCA, which consists in solving a sequence of approximate convex quadratic programs whose solutions are explicit.
 We can now describe our DCA scheme for solving (2).
 Algorithm DCA Initialization.
 Let X (0)  X  IR ( k +1)  X  n and &gt; 0 be small enough. Set l =0 .
 Repeat  X  Compute Y ( l )  X   X  X  ( X ( ( l ) ) with the help of the formulations ((9) -(16));  X  Compute X ( l +1)  X   X  X   X  ( Y ( l ) ) via (23) ;  X  Set l = l +1 +1 | ) .
 Find again the real centres. Let X  X  be the solution obtained by DCA and (corresponding to a solution of problem (1) are determined by How to find a good initial point for DCA. Finding a good starting point is important for DCA to reach global solutions. For this, we combine alternatively the two procedures by exploiting simultaneously the efficiency of DCA and the K-means algorithm. More precisely, starting with a point X (0) with X (0) i randomly chosen among the points in A we perform one iteration of DCA, namely set Y of K-means to obtain X (1) . We note that at each iteration DCA returns k +1  X  X entres X  while K-means return k  X  X entres X  of clusters f rom which the  X  X otal centre X  is determined via the formula (25) below. This procedure can then be repeated some times to provide a good initial point for the main DCA as will be shown in numerical simulations.

The combined DCA -K-means procedure, denoted IP , to find a good initial point for the main DCA is described as follows: Procedure IP: let q be a positive integer.
 Let X (0)  X  IR k  X  n such that X (0) i is randomly chosen among the points of A . For t = 0 , 1 , ..., q do t1. Compute Y ( t ) by the formulations ((9) -(16)) and X ( t +1) by (23); t2. Assign each point a j  X  X  into the cluster that has the closest centre X 1 ,...,X t3. For each i  X  X  1 ,...,k } recompute Z i as the centres of the cluster  X  i :
Update X ( t +1) i := Z i for i =1 , ...k +1 . enddo Ouput: set X (0) := X ( q ) .

We note (from several numerical tests ) that the alternative DCA -K-means procedure is better than the combination of the complete K-means (until the convergence) and DCA. Our experiments are composed of two sets of data. The first data set is the geographical locations of 51 North American cities studied in [3], [4], [14] with k =6 . Those works consist in investigating the hierarchical clustering algorithms for multicast group hierarchies. We got this data from the picture included in [3].
In the first numerical experiment we compare our algorithm DCAIP (DCA with the procedure IP for finding the initial point) with an optimization method based on K-means algorithm denoted OKM .Wetake q =5 in the procedure IP, =10  X  6 ,and  X  =2 (the penalty parameter in (2)).

In OKM we used the code of K-means algorithm which is available on the web site:  X  X ttp://www.fas.umonteral.ca/biol/legendre/ X  for finding the centres of clusters at the second level. The neare st city to this  X  X entre X  is then taken as total centre  X  x k +1 is determined by the next way: Since the K-means clustering algorithm is a heuristic technique and is influenced by the choice of initial centres, we have run DCAIP and OKM ten times from the same initial centres that are randomly chosen from the set . The total costs given by the algorithms are reported in Table 1 (left). The total cost of the tree is computed as where A i is the cluster with the centre x i for i =1 ,...,k .

In Table 1 (right) we present the best results given by the algorithms pro-posed in [3] ( KMC )andin[14]( 1-km )and DCAIP for this dataset. In [3] the algorithm KMC has been proposed for multilevel hierarchical clustering where
Initial point DCAIP OKM the hierarchical trees are formed by rep eated application of K-means algorithm at each hierarchical level. The procedure is beginning at the top layer where all members are partitioned into k clusters. From each of the k clusters found, a representative member is chosen to act a s a server. The top-level servers become children of the source (the root of the tree). Each cluster is then again decom-posed using clustering algorithms to form a new layer of sub-clusters, whose servers become children of the server in the cluster just partitioned. And so on, until a suitable terminating condition is reached. Some variants of KMC have been proposed in [4]. We note that KMC is a variant of OKM in which the K-means algorithm returns the  X  X uclidean centre X  and the root of the tree is the Euclidean centre of the six servers.

In [14] the authors have proposed four variants of their optimization algo-rithms, based on the derivative-free discrete gradient method, for two nonsmooth nonconvex problems. They have compared their algorithms and the optimization algorithm based on K-means with the same initial points. The 1-km algorithm (a version of their optimization algorithm with the initial point given by K-means) provides the best results among their four variants algorithms.

The best total cost given by DCAIP , OKM and 1-km among ten tests with different initial points and the one of KMC among two tests ( 6 distributed throughout the data set, and 6 in South West) is reported in Table 1 (right).
In the second numerical experiment w e use a randomly generated database with up to 50000 objects in higher dimension al spaces. We first generate k centres of clusters. The points of each cluster a re randomly generat ed in a circle whose centre is the centre of this cluster. The nu mbersofpointsinclustersarerandomly chosen. In Table 2 (left) we present the total cost given by DCAIP and OKM with the same initial point.

For testing the efficiency of procedure IP we perform two versions of DCA with and without procedure IP .Werun DCAIP and DCA on ten test prob-lems. The results are reported in Table 2 (right). Here  X  X ter X  denotes the number of iterations of the algorithm and all CPU are computed in seconds.
From numerical experiments we see that DCA is always the best for both dataset, and it is very inexpensive: it solves problems with large dimension in a short time. On the other hand, Procedure IP is efficient for finding a good starting point for DCA. We have proposed, for solving a bilevel clustering problem with the squared Eu-clidean distance, a new and efficient approach based on DC programming and DCA. The considered hierarchical clu stering problem has been formulated as a DC program in the suitable matrix space and with a natural choice of DC decomposition in order to make simpler and so much less expensive the compu-tations in the resulting DCA. It fortunately turns out that our algorithm DCA is explicit, and very inexpensive. An interesting procedure that combines DCA and K-means is introduced for initializing DCA. Preliminary numerical simu-lations show the robustness, the efficiency and the superiority of our algorithm with respect to other optimization based clustering algorithms. The efficiency of our approach comes from two facts:  X  The optimization model is appropriate for multilevel clustering : it requires  X  The optimization algorithm DCA is very suitable to this model.
 The efficiency of DCA suggests to us investigating it in the solution of other models of bilevel clustering problems as well as the higher level for hierarchical clustering. Works in these d irections are in progress.

