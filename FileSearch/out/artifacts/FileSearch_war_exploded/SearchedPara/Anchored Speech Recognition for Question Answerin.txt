 In this paper, we focus on finding answers to user questions from spoken documents, such as broad-cast news stories and conversations. In a typical question answering system, the user query is first processed by an information retrieval (IR) system, which finds out the most relevant documents among massive document collections. Each sentence in these relevant documents is processed to determine whether or not it answers user questions. Once a candidate sentence is determined, it is further pro-cessed to extract the exact answer.

Answering factoid questions (i.e., questions like  X  X hat is the capital of France?  X ) using web makes use of the redundancy of information (Whittaker et al., 2006). However, when the document collection is not large and when the queries are complex, as in the task we focus on in this paper, more sophis-ticated syntactic, semantic, and contextual process-ing of documents and queries is performed to ex-tract or construct the answer. Although much of the work on question answering has been focused on written texts, many emerging systems also enable either spoken queries or spoken document collec-tions (Lamel et al., 2008). The work we describe in this paper also uses spoken data collections to answer user questions but our focus is on improv-ing speech recognition quality of the documents by making use of the wording in the queries. Consider the following example:
Answering such questions requires as good ASR transcriptions as possible. In many cases, though, there is one generic ASR system and a generic lan-guage model to use. The approach proposed in this paper attempts to improve the ASR performance by re-recognizing the candidate sentence using lexical information from the given question. The motiva-tion is that the question and the candidate sentence should share some common words, and therefore the words of the answer sentence can be estimated from the given question. For example, given a fac-toid question such as:  X  What is the tallest build-ing in the world?  X , the sentence containing its an-swer is highly likely to include word sequences such as:  X  The tallest building in the world is NAME  X  or  X  NAME, the highest building in the world, ...  X , where NAME is the exact answer.

Once the sentence supporting the answer is lo-cated, it is re-recognized such that the candidate an-swer is constrained to include parts of the question word sequence. To achieve this, a word network is formed to match the answer sentence to the given question. Since the question words are taken as a ba-sis to re-recognize the best-candidate sentence, the question acts as an anchor , and therefore, we call this approach anchored recognition .

In this work, we restrict out attention to questions about the subject, the object and the locative, tempo-ral, and causative arguments. For instance, the fol-lowings are the questions of interest for the sentence Obama invited Clinton to the White House to discuss the recent developments : Who invited Clinton to the White House? Who did Obama invite to the White House? Why did Obama invite Clinton to the White House? The goal in sentence extraction is determining the sentence that is most likely to contain the answer to the given question. Our sentence extractor relies on non-stop word n -gram match between the ques-tion and the candidate sentence, and returns the sen-tence with the largest weighted average. Since not all word n -grams have the same importance (e.g. function vs. content words), we perform a weighted sum as typically done in the IR literature, i.e., the matching n -grams are weighted with respect to their inverse document frequency (IDF) and length.
A major concern for accurate sentence extraction is the robustness to speech recognition errors. An-other concern is dealing with alternative word se-quences for expressing the same meaning. To tackle the second challenge, one can also include syn-onyms, and compare paraphrases of the question and the candidate answer. Since our main focus is on ro-bustness to speech recognition errors, our data set is limited to those questions that are worded very similarly to the candidate answers. However, the approach is more general, and can be extended to tackle both challenges. When the answer is to be extracted from ASR out-put, the exact answers can be erroneous because (1) the exact answer phrase might be misrecognized, (2) other parts of the sentence might be misrecognized, so the exact answer cannot be extracted either be-cause parser fails or because the sentence cannot match the query.

The question in the example in the Introduction section is concerned with the object of the predicate  X  is  X  rather than of the other predicates  X  understand  X  or  X  was  X . Therefore, a pre-processing step is needed to correctly identify the object (in this example) that is being asked, which is described next.

Once the best candidate sentence is estimated, a syntactic parser (Harper and Huang, ) that also out-puts function tags is used to parse both the ques-tion and candidate answering sentence. The parser is trained on Fisher, Switchboard, and speechified Broadcast News, Brown, and Wall Street Journal treebanks without punctuation and case to match in-put the evaluation conditions.
 An example of such a syntactic parse is given in Figure 2. As shown there, the  X  X BJ X  marks the sur-face subject of a given predicate, and the  X  X MP X  tag marks the temporal argument. There are also the  X  X IR X  and  X  X OC X  tags indicating the locative ar-gument and the  X  X RP X  tag indicating the causal ar-gument. Such parses not only provide a mechanism to extract information relating to the subject of the predicate of interest, but also to extract the part of the sentence that the question is about, in this ex-ample  X  a Russian-made rocket [which] is certainly not a weapon used by the Greek military  X . The ex-traction of the relevant part is achieved by matching the predicate of the question to the predicates of the subsentences in the best candidate sentence. Once such syntactic parses are obtained for the part of the best-candidate sentence that matches the question, a set of rules are used to extract the argument that can answer the question. In this study we employed a state-of-the-art broad-cast news and conversations speech recognition system (Stolcke et al., 2006). The recognizer performs a total of seven decoding passes with alternating acoustic front-ends: one based on Mel frequency cepstral coefficients (MFCCs) aug-mented with discriminatively estimated multilayer-perceptron (MLP) features, and one based on per-ceptual linear prediction (PLP) features. Acoustic models are cross-adapted during recognition to out-put from previous recognition stages, and the output of the three final decoding steps are combined via confusion networks.

Given a question whose answer we expect to find in a given sentence, we construct a re-decoding net-work to match that question. We call this process an-chored speech recognition , where the anchor is the question text. Note that this is different than forced alignment, which enforces the recognition of an au-dio stream to align with some given sentence. It is used for detecting the start times of individual words or for language learning applications to exploit the acoustic model scores, since there is no need for a language model.

Our approach is also different than the so-called flexible alignment (Finke and Waibel, 1997), which is basically forced alignment that allows skipping any part of the given sentence, replacing it with a re-ject token, or inserting hesitations in between words. In our task, we require all the words in the ques-tion to be in the best-candidate sentence without any skips or insertions. If we allow flexible alignment, then any part of the question could be deleted. In the proposed anchored speech recognition scheme, we allow only pauses and rejects between words, but do not allow any deletions or skips.

The algorithm for extracting anchored recognition hypotheses is as follows: (i) Construct new recogni-tion and rescoring language models (LMs) by inter-polating the baseline LMs with those trained from only the question sentences and use the new LM to generate lattices -this aims to bias the recogni-tion towards word phrases that are included in the questions. (ii) Construct for each question an  X  X n-chored X  word network that matches the word se-quence of the question, allowing any other word se-quence around it. For example if the question is WHAT did Bruce Gordon say? , we construct a word network to match Bruce Gordon said ANYTHING where  X  X NYTHING X  is a filler that allows any word (a word loop). (iii) Intersect the recognition lat-tices from step (i) with the anchored network for each question in (ii), thus extracting from the lattice only the paths that match as answers to the ques-tion. Then rescore that new lattice with higher order LM and cross-word adapted acoustic models to get the best path. (iv) If the intersection part in (iii) fails then we use a more constrained recognition network: Starting with the anchored network in (ii) we first limit the vocabulary in the ANYTHING word-loop sub-network to only the words that were included in the recognition lattice from step (i). Then we com-pose this network with the bigram LM (from step (i)) to add bigram probabilities to the network. Vocab-ulary limitation is done for efficiency reasons. We also allow optional filler words and pauses to this network to allow for hesitations, non-speech events and pauses within the utterance we are trying to match. This may limit somewhat the potential im-provement from this approach and we are working Question Type ASR Output Manual Trans.
 Temporal Arg. 94% 98% Temporal Arg. 55% 73% 63% towards enhancing the vocabulary with more candi-date words that could contain the spoken words in the region. (v) Then we perform recognition with the new anchored network and extract the best path through it. Thus we enforce partial alignment of the audio with the question given, while the regu-lar recognition LM is still used for the parts outside the question. We performed experiments using a set of questions and broadcast audio documents released by LDC for the DARPA-funded GALE project Phase 3. In this dataset we have 482 questions (177 subject, 160 ob-ject, 73 temporal argument, 49 locative argument, and 23 reason) from 90 documents. The ASR word error rate (WER) for the sentences from which the questions are constructed is 37% with respect to noisy closed captions. To factor out IR noise we as-sumed that the target document is given.

Table 1 presents the performance of the sentence extraction system using manual and automatic tran-scriptions. As seen, the system is almost perfect when there is no noise, however performance de-grades about 12% with the ASR output.

The next set of experiments demonstrate the per-formance of the answer extraction system when the correct sentence is given using both automatic and manual transcriptions. As seen from Table 2, the answer extraction performance degrades by about 35% relative using the ASR output. However, using the anchored recognition approach, this improves to 23%, reducing the effect of the ASR noise signifi-cantly 1 by more than 30% relative. This is shown in the last column of this table, demonstrating the use of the proposed approach. We observe that the WER of the sentences for which we now get cor-rected answers is reduced from 45% to 28% with this approach, a reduction of 37% relative. We have presented a question answering system for querying spoken documents with a novel an-chored speech recognition approach, which aims to re-decode an utterance given the question. The pro-posed approach significantly lowers the error rate for answer extraction. Our future work involves han-dling audio in foreign languages, that is robust to both ASR and machine translation noise.

