 Chinese word segmentation is the first step of many NLP and IR tasks. Over the past years, word segmentation system X  X  performance has been improved. How-ever there are still some challenging problems. One of these problems is how to unearth helpful information from large scale unlabeled data and use this infor-mation to improve word segmentation system X  X  performance. Former researchers have tried to use auto-segmented result of large scale unlabeled data[1] and s-tatistical magnitudes like mutual information, accessory variety[2] to help the semi-supervised learning system. Performance improvement is achieved in their works.
 derive information from unlabeled data and improve the semi-supervised learning systems performance[3][4]. However, there hasn X  X  been any work that applies clustering to word segmentation. The main reason is that there is no natural word boundary in Chinese. Traditional routine of clustering words cannot be applied to segmentation task directly. But, as character is the minimum unit of Chinese language, it X  X  promising that we build clusters from character and use this character clustering information to assist word segmentation. In this paper, we try to employ Brown clustering algorithm to build character-based clusters and embed contextual information into the character cluster. Finally we compile the clustering result into features and use this features to improve the word segmentation task. Experiments shows our character clustering results can help improving word segmentation performance.
 intuitive motivation and theoretical analysis of our character-based clustering method. Section 3 introduces the semi-supervised model we use to incorporate clustering results. Section 4 presents experimental results and empirical analysis. Section 5 gives some conclusion and future work. 2.1 Brown clustering Data sparsity is always an issue in many NLP tasks. Capturing generality from unlabeled data is a promising way to address this issue.[4] Intuitively, charac-ter under the similar context environment tends to have similar function when compositing words. Supposing there is some criterion reflecting this similarity, we can use this criterion to help our word segmentation system. For example, in the following sentence  X ...  X  X  X  X  X  X  X  X  X  X  X  ... X (... the shorting in futures ...),  X   X  X  X   X (the shorting) is a financial term which barely occurs in newswire. While, similar context may occurs in  X ...  X  X  X  X  X  X  X  X  X  X  X  ... 1  X  (... the disciplinary of-fence...) and this context is a typical newswire. This kind of similarity provides a clue for inferring the segmentation of  X   X  X  X   X .
 The class-based bigram language model considers the sentence as a sequence of characters and there is a hidden class behind each character in the sentence. Figure 1 illustrate the class-based bigram language model where c i represent i th character of the sentence and C i is its cluster. modeled as follow: derive a hierarchical clustering of words with a bottom-up clustering algorithm, which is known as Brown clustering algorithm[5][4]. The input of Brown cluster-ing is a sequence of item. The output is a binary tree, which can be represented by a string of 01. 2.2 Unigram Character Clustering In our unigram character clustering model, we follow our model definition as mentioned in section 2.1 and the cluster of a character only depends on the character itself. As result of the unigram character clustering model, each char-acter is allocated with a single cluster. In our experiment, sentence is split into sequence of characters and brown clustering algorithm is employed on the se-quence. Table 1 illustrates some result of the unigram character clustering model.
For the clustering result shown in table 1, it seems our clustering model works well by clustering the Chinese digit into one cluster and Chinese metal name into another cluster. However, farther analytics will cast doubt on these result-s X  effect. Syntactical and semantical function of same Chinese character varies under different circumstance. Simply dropping the contextual environment and clustering the character into mono-clustering will introduce a lot of ambiguity. Clustering result in Table 1 also shows this problem. The Chinese character  X   X   X  can be a family name (translated as  X  X e X ), while it can also indicating part of the plant(translated as  X  X eaf X ). When used as a family name,  X   X   X  is usually the leading character of a word. But when used as leaf,  X   X   X  can composite word like  X   X  X  X   X (leaf),  X   X  X  X  X  X   X (a Chinese idiom which means having ones view of the important overshadowed by the trivial) and used as middle or end of a word. In following section, our experimental result also prove unigram character clustering doesn X  X  work well. 2.3 Bigram and Trigram Character Clustering To settle the problems mentioned above, same character under different context circumstance should be categorized into different clusters. We incorporate con-textual information by considering character X  X  bigram and trigram. The model of P ( c 1 :::n ) changes into in bigram case and in trigram case.
 result shows that our model cluster  X   X   X  under the environment where it X  X  used as leading character and means leaf into same cluster. The second column capture the sentence segmentation like  X ...  X  X  X  X  X  X  X  X  X  X  ... X ,  X ...  X  X  X  X  X  X  X  X  ... X ,  X ...  X  X  X  X  X  X  X  ... X  2 . In this situation, the bigram  X   X  X  X   X  provide a clue for segmentation. The third column cluster the rare word  X   X  X  X   X  into a cluster of common words. Analogously, trigram model gives similar results. Previous study[1][2][6] has presented a simple yet effective semi-supervised method of incorporating information derived from large scale unlabeled data. Their method introduces new semi-supervised feature into robust machine learning model. In this paper, we follow their work and employ a conditional random fields (CRFs) model to incorporate character clustering results. This model is a character-based sequence labeling model, in which a character is labeled a tag representing the position of its position in word. We follow the work in [1] and select tagset of 6-tag style (B, B2, B3, I, E, S). 3.1 Baseline Features We employ a set of simple but widely used feature as baseline feature. The features we use are listed below. { character unigram: c s ( i 2 s i + 2) { character bigram: c s c s +1 ( i 2 s i + 1), c s c s +2 ( i 2 s i ) { character trigram: c s 1 c s c s +1 ( s = i ) { repetition of characters: is c s equals c s +1 ( i 1 s i ), is c s equals c s +2 { character type: is c i an alphabet , digit , punctuation or others 3.2 Mutual Information Features In order to compare character clustering with traditional semi-supervised fea-ture, we follow previous work[2] and feed mutual information to our semi-supervised model. Mutual information of two character is define as, down to integer. These integer value are integrated into CRF model as a type of features. 3.3 Clustering Features We compile character clusters result into a kind of feature. When clustering al-gorithm is performed over large scale unlabeled data, a lexicon indicating ngram is cluster is maintained. For each character c i in sentence, we extract the clusters of ngram and integrate them into our CRF model as a type of features. features are listed below, { For our unigram character clustering model, brown clustering results of char-{ For our bigram character clustering model, we extract brown ( c i 1 c i ) and Here, brown ( x ) represents the clusters of ngram x . 4.1 Settings To test the character clustering X  X  effect on Chinese word segmentation, we select CTB5.0 and CTB6.0 as our labeled data. For these two data set, we split the data according to the recommandation in the document. Some statistic of the data is listed in Table 3.
 because of its huge quantity and broad coverage. Xinhua news(from 2000 to 2010) is chosen from Chinese Gigawords as unlabeled data in our experiment, which has about 500 million characters.
 centage of words that are correctly segmented in model output, and recall r as percentage of words that are correctly segmented in gold standard output. training pharse, stochastic gradient descent is set as training algorithm. Two parameters f eature:possible trainstions = 1 and f eature:possible states = 1 is configed to enable negative features.
 the character clusters. Algorithm X  X  running time on an Xeon(R) 2.67GHz server is list in Table 4. We didn X  X  maintain clustering results of 500 and 1000 clusters in trigram case, because it would consume too much time.
 4.2 Results According to previous study, number of clusters controls the representation capicity of clustering result[8]. We conduct experiment on various settings of cluster number. Figure 2 shows our experimental result on CTB5.0 and CT-B6.0. baseline means the CRFs model trained with baseline features. Symbol  X + X  means model trained both with baseline features and the new features. Ex-periment results shows that in bigram case, model of c = 500 and c = 1000 respectively achieve best accuracy on CTB5.0 and CTB6.0 X  X  development data. In trigram case, model of c = 200 perform better than that of c = 100 in both data set. analysis. Unigram character clustering(+unigram) have almost no effect on Chi-nese word segmentation. However when increase the order of clustering mod-el(+bigram,+trigram), increasement in F1-score is achieved. This result proves character clustering model considering contextual information is effective on Chi-nese word segmentation. Theoretically, trigram clustering eploits more contex-tual information and is expected to have better performance than bigram clus-tering. However, in both the CTB5.0 and CTB6.0, performance of model with trigram(+trigram) clusters is slightly lower than bigram model(+bigram). One reason maybe that trigram clustering introduce much noise due to exponential increased vocabulary size. Another reason for this may result from the limited number of clusters in trigram model. It takes more than 5 days to compute tri-gram brown clustering result of 200-clusters and it takes more of the 500-clusters cases. provment when we increase the number of clusters. But in trigram experiment, performance improvement is achieved when we transfer from 100-clusters to 200-clusters. Generally, we can conclude that fine-grained clusters help the word segmentation more.
 test data with best c configuration. Experiment result is show in Tabel 5. From this table, we can see our method outperform the baseline model. Significance tests between our method and baseline model also demostrate that the improve-ments of our cluster features(+bigram,+trigram) is significant with p value &lt; 10 4 .
 ters with stronger association is more likely to combine and composite a word. We compare our character clustering model that is configed with best c with model incorporated with MI. Table 5 shows the comparison results. In our experiment, we have seen that system incorporate with mutual information outperforms the character clustering model.
 local information between two characters. The character clustering which con-sidering global information of a sentence makes a good complement of this lim-itation. In our experiments, we have seen further improvement on performance when two methods are combined. In this paper, we propose a method of building clusters from Chinese character. Contextual information is considered when we perform character clustering algo-rithm to address character ambiguity. Experimental result shows our character clustering result can help improve word segmentation performance.
 Also, we will try to use the character clusters to help other character-based NLP task like character-based Chinese parsing model.
 This work was supported by National Natural Science Foundation of China (NS-FC) via grant 61133012, the National  X 863 X  Major Projects via grant 2011AA01A207, and the National  X 863 X  Leading Technology Research Project via grant 2012AA011102.
