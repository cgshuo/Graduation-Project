 of models at the same time and then perform model comparison s hould provide additional insight represent them using a common/comparable hierarchy, and fin ally use a marginal likelihood or a modeling sparsity, stochastic search over the order of the v ariables and model comparison. data assumes linearity and Gaussian variables hence they ca n only recover the DAG structure up perform model comparison between the factor and DAG models i nferred with fixed orderings taken from the candidate set.
 future work. combination of parent nodes plus a driving signal z corresponds to a link in the DAG. Solving for x we can rewrite the problem as ment alone is not enough to ensure identifiability (up to scal ing and permutation of columns P pruning B and the chi-square test is used for model selection.
 Bayesian approach to learning a sparse factor models and DAG s, and the stochastic search for P only input that comes from the first part is a set of candidate o rders. specify the joint distribution or probability of everything , e.g. for the factor model, as where X = [ x use the predictive densities on a test set as a yardstick. noise x = P  X  1 matrix  X  , i.e. uncorrelated noise, to account for independent measu rement noise, P permutation matrix for the rows of A and P P A to be triangular. Instead we infer P Bayesian model as follows where  X  placing a gamma prior on the precision of  X  with shape s standardized, the selection of hyperparameters for  X  noise X  are supported. The prior should favor small values of  X   X  i = 1 the experiments).
 for convenience as a scale mixture of Gaussians [ 14 ] where z has an exponential distribution acting as mixing den-sity. Furthermore, we place a gamma distribution on  X  jugate families. We let the components of Z have on average unit variance. This is achieved by setting  X  / X  r = 2 (we set  X  s = 4 and  X  r = 2 ). Alternatively one may use a t distribution X  X gain as scale mixture of Gaussians X  X hich can to interpolate between very heavy-tailed (power law) and very light tails, i.e. be-coming Gaussian when degrees of freedom approaches infinity. However such flexibility comes at the price of being more difficult to select its hyperparameters, be-cause the model could become unidentified for some settings. the elements a each element a elements within the same factor to share information. The hi erarchy can be written as where  X  ( ) is a Dirac  X  -function. The prior above specify a point mass mixture over a r . The expected probability of a with mean  X  elements  X  precision  X  zero elements in A is specified through the slab distribution depending on  X   X  for instance t model easier to specify and tend to have better mixing proper ties [ 15 ]. The masking matrix r parameters  X  e.g.  X  this is for example  X  omitting parameters is shown in Figure 1 .
 DAG We make the following Bayesian specification of linear DAG mo del of equation ( 1 ) as needs only shared variance parameter  X  for the Laplace distributed z A is equivalent to change of variance in z  X  p = 100 For the linear DAG model we are not performing joint inferenc e of P and the model parameters. Rather we use a set of P s found for the factor model to be good candidates for the DAG. The stochastic search for P = P model, propose P  X  above its diagonal (through masking matrix M ) The procedure can be seen as a simple approach for generating hypotheses about good, close to size m when a test set is available or just the likelihood if not. Given that our model produces both DAG and a factor model esti mates at the same time, it could to compute by sampling, requiring for example thermodynami c integration. With Gibbs sampling, we draw samples from the posterior distributions p ( A ,  X  , X  | X , ) and p ( B , X   X  matrices for clarity.
 distribution from the scale mixture representation as where U can average over p ( A ,  X  , X  | X , ) to obtain p ( X  X  | X , M ( 4 ) is just a Laplace distribution with mean BX where [ BX ] the hyperprior on  X  of equation ( 5 ) instead of just fixing its value. Among the existing approaches to DAG learning, our work is mo st closely related to LiNGAM obtaining an ordering for the variables. The order search in LiNGAM assumes that there is not roughly O ( N in our case is O ( N binary data or time series by selecting some suitable prior d istributions. Much work on Bayesian models for DAG learning already exist. For example, the approach pre-exhaustive order enumeration is not an option. and 2000 as burn-in for the DAG 2 . Hyperparameter settings are discussed in Section 3 . LiNGAM suite We evaluate the performance of our model against LiNGAM 3 using the artificial a matrix with elements r the AUC can be still computed. rate. (c) Frequency of AUC being greater than 0.9. (d) Number of estimated correct orderings. LiNGAM. The true positive rate results in Figure 2(a) show that LiNGAM outperform our approach link in the DAG to be zero, p ( b two-core 2.5GHz machine, for d = 10 and N = 500 : LiNGAM took in average 10 seconds and our method 170 seconds. However, when doubling the number of variables the times were 730 and 550 seconds for LiNGAM and our method respectively, which is in agreement with our complexity estimates. Bayesian networks repository Next we want to compare some of the state of the art (Gaussian) sparse candidate pruning then DAG-search (DSC) [ 6 ], L1MB then DAG-search (DSL) [ 8 ], sparse-of reversed links found due to ordering errors. scale problems regardless of their identifiability issues.
 Model comparison For this experiment we have generated 1000 different datase ts/models with the true model to be a factor model or a DAG uniformly. In order to generate a factor model we between the two proposed hypotheses.
 Protein-signaling network The dataset introduced in [ 17 ] consists on flow cytometry measure-ments of 11 phosphorylated proteins and phospholipids (Raf , Erk, p38, Jnk, Akt, Mek, PKA, PKC, PIP PIP ( links that corresponds to approximately two-thirds of tota l true positives. We have proposed a novel approach to perform inference and mo del comparison of sparse factor models and DAGs within the same framework. The key ingredien ts for both Bayesian models are (non-parametric Dirichlet process, temporal Gaussian pro cesses and discrete).
