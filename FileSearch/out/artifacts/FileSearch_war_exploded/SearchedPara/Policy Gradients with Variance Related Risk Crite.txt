 Aviv Tamar avivt@tx.technion.ac.il Dotan Di Castro dot@tx.technion.ac.il Shie Mannor shie@ee.technion.ac.il In both Reinforcement Learning (RL; Bertsekas &amp; Tsitsiklis, 1996) and planning in Markov Decision Pro-cesses (MDPs; Puterman, 1994), the typical objective is to maximize the cumulative (possibly discounted) expected reward, denoted by J . When the model X  X  parameters are known, several well-established and ef-ficient optimization algorithms are known. When the model parameters are not known, learning is needed and there are several algorithmic frameworks that solve the learning problem efficiently, at least when the model is finite. In many applications, however, the decision maker is also interested in minimizing some form of risk of the policy. By risk, we mean reward criteria that take into account not only the expected reward, but also some additional statistics of the to-tal reward such as its variance, its Value at Risk, etc. (Luenberger, 1998). Risk can be measured with re-spect to two types of uncertainties. The first type, termed parametric uncertainty is related to the imper-fect knowledge of the problem parameters. The sec-ond type, termed inherent uncertainty is related to the stochastic nature of the system (random reward and transition function). Both types of uncertainties can be important, depending on the application at hand. Risk aware decision making is important both in plan-ning and in learning. Two prominent examples are in finance and process control. In financial decision making, a popular performance criterion is the Sharpe Ratio (SR; Sharpe, 1966)  X  the ratio between the ex-pected profit and its standard deviation. This measure is so popular that it is one of the reported metrics each mutual fund reports annually. When deciding how to allocate a portfolio both types of uncertainties are important: the decision maker does not know the model parameters or the actual realization of the mar-ket behavior. In process control as well, both uncer-tainties are essential and a robust optimization frame-work (Nilim &amp; El Ghaoui, 2005) is often adopted to overcome imperfect knowledge of the parameters and uncertainty in the transitions. In this paper we focus on inherent uncertainty and use learning to mitigate parametric uncertainty.
 The topic of risk-aware decision making has been of in-terest for quite a long time, and several frameworks for incorporating risk into decision making have been sug-gested. In the context of MDPs and addressing inher-ent uncertainty, Howard &amp; Matheson (1972) proposed to use an exponential utility function, where the factor of the exponent controls the risk sensitivity. Another approach considers the percentile performance crite-rion (Filar et al., 1995), in which the average reward has to exceed some value with a given probability. Ad-dressing parameter uncertainty has been done within the Bayesian framework (where a prior is assumed on the unknown parameters, see Poupart et al., 2006) or within the robust MDP framework (where a worst-case approach is taken over the parameters inside an uncertainty set). Much less work has been done on risk sensitive criteria within the RL framework, with a notable exception of Borkar &amp; Meyn (2002) who con-sidered exponential utility functions and of Geibel &amp; Wysotzki (2005) who considered models where some states are  X  X rror states, X  representing a bad or even catastrophic outcome.
 In this work we consider an RL setup and focus on risk measures that involve the variance of the cumulative reward , denoted by V . Typical performance criteria that fall under this definition include (a) Maximize J s.t. V  X  c (b) Minimize V s.t. J  X  c (c) Maximize the Sharpe Ratio: J/ (d) Maximize J  X  c The rationale behind our choice of risk measure is that these performance criteria, such as the SR mentioned above, are being used in practice. Moreover, it seems that human decision makers understand how to use variance well, and that exponential utility functions require determining the exponent coefficient which is non-intuitive.
 Variance-based risk criteria, however, are computa-tionally demanding. It has long been recognized (So-bel, 1982) that optimization problems such as (a) are not amenable to standard dynamic programming techniques. Furthermore, Mannor &amp; Tsitsiklis have shown that even when the MDP X  X  parameters are known, many of these problems are computationally intractable, and some are not even approximable. This is not surprising given that other risk related criteria such as percentile optimization are also known to be hard except in special cases.
 Despite these somewhat discouraging results, in this work we show that this important problem may be tackled successfully, by considering policy gradient type algorithms that optimize the problem locally . We present a framework for dealing with performance cri-teria that include the variance of the cumulative re-ward. Our approach is based on a new fundamen-tal result for the variance of episodic tasks. Previous work by Sobel, 1982 presented similar equations for the infinite horizon discounted case, however, the im-portance of our result is that the episodic setup al-lows us to derive policy gradient type algorithms . We present both model-based and model-free algorithms for solving problems (a) and (c), and prove that they converge. Extension of our algorithms to other perfor-mance criteria such as (b) and (d) listed above is im-mediate. The effectiveness of our approach is further demonstrated numerically in a risk sensitive portfolio management problem. In this section we present the framework considered in this work and explain the difficulty in mean-variance optimization. 2.1. Definitions and Framework We consider an agent interacting with an unknown environment that is modeled by an MDP in discrete time with a finite state set X , { 1 ,...,n } and finite action set U , { 1 ,...,m } . Each selected action u  X  U at a state x  X  X determines a stochastic transition to the next state y  X  X with a probability P u ( y | x ). For each state x the agent receives a corresponding re-ward r ( x ) that is bounded and depends only on the current state 1 . The agent maintains a parameterized policy function that is in general a probabilistic func-tion, denoted by  X   X  ( u | x ), mapping a state x  X  X into a probability distribution over the controls U . The sume that  X   X  ( u | x ) is a differentiable function w.r.t.  X  . Note that for different values of  X  , different probabil-ity distributions over U are associated for each x  X  X . We denote by x 0 ,u 0 ,r 0 ,x 1 ,u 1 ,r 1 ,... a state-action-reward trajectory where the subindex specifies time. For notational easiness, we define x k i , u k i , and r k x ...,x k , u i ...,u k , and r i ...,r k , respectively, and R i to be the cumulative reward along the trajectory R i = P Under each policy induced by  X   X  ( u | x ), the environ-ment and the agent induce together a Markovian transition function, denoted by P  X  ( y | x ), satisfying P tion will be valid throughout the rest of the paper. Assumption 2.1. Under all policies, the induced Markov chain P  X  is ergodic, i.e., aperiodic, recurrent, and irreducible.
 Under assumption 2.1 the Markovian transition func-tion P  X  ( y | x ) induces a stationary distribution over the state space X , denoted by  X   X  . We denote by E  X  [  X  ] and Var  X  [  X  ] to be the expectation and variance operators w.r.t. the measure P  X  ( y | x ).
 There are several performance criteria investigated in the RL literature that differ mainly on their time hori-zon and the treatment of future rewards (Bertsekas &amp; Tsitsiklis, 1996). One popular criterion is the av-erage reward defined by  X   X  = P x  X   X  ( x ) r ( x ). Under this criterion, the agent X  X  goal is to find the parame-ter  X  that maximizes  X   X  . One appealing property of this criterion is the possibility of obtaining estimates of  X   X   X  from simulated trajectories efficiently, which leads to a class of stochastic gradient type algorithms known as policy gradient algorithms. In this work, we also follow the policy gradient approach, but focus on the mean-variance tradeoff. While one can consider the tradeoff between  X   X  and Var  X  [ r ( x )], defined as the variance w.r.t the measure  X   X  , these expressions are not sensitive to the trajectory but only to the induced stationary distribution, and represent the per-round variability.
 Consequently, we focus on the finite horizon case, also known as the episodic case, that is important in many applications. Assume (without lost of generality) that x  X  is some recurrent state for all policies and let  X  , min { k &gt; 0 | x k = x  X  } denote the first passage time to x .
 Let the random variable B denote the accumulated re-ward along the trajectory terminating at the recurrent state x  X  Clearly, it is desirable to choose a policy for which B is large in some sense 2 . In this work, we are interested in the mean-variance tradeoff in B .
 We define the value function as and the trajectory variance function as Note that the dependence of J ( x ) and V ( x ) on  X  is suppressed in notation.
 The questions explored in this work are the following stochastic optimization problems: (a) The constrained trajectory-variance problem: (b) The maximal SR problem: In order for these problems to be well defined, we make the following assumption: Assumption 2.2. Under all policies J ( x  X  ) and V ( x  X  ) are bounded.
 For the SR problem we also require the following: Assumption 2.3. We have V ( x  X  ) &gt; for some &gt; 0. In the next subsection we discuss the challenges in-volved in solving problems (3) and (4), which motivate our gradient based approach. 2.2. The Challenges of Trajectory-Variance As was already recognized by Sobel (1982), optimizing the mean-variance tradeoff in MDPs cannot be solved using traditional dynamic programming methods such as policy iteration. Mannor &amp; Tsitsiklis showed that for the case of a finite horizon T , in general, solving problem (3) is hard and is equivalent to solving the subset-sum problem. Since our case can be seen as a generalization of a finite horizon problem, (3) is a hard problem as well. One reason for the hardness of the problem is that, as suggested by Mannor &amp; Tsitsiklis, the underlying optimization problem is not necessarily convex. In the following, we give an example where the set of all ( J ( x  X  ) ,V ( x  X  )) pairs spanned by all possible policies is not convex.
 Consider the following symmetric deterministic MDP with 8 states X = { x  X  ,x 1 a ,x 1 b ,x 2 a ,x 2 b ,x 2 c and two actions U = { u 1 ,u 2 } . The reward is equal to 1 or  X  1 when action u 1 or u 2 are chosen, respec-tively. The MDP is sketched in Figure 1, left pane. We consider a set of random policies parameterized by  X   X  [0 , 1] and  X  2  X  [0 , 1], such that  X  ( u 1 | x  X  ) =  X   X  ( u 1 | x 1 a ) =  X  ( u 1 | x 1 b ) =  X  2 .
 Now, we can achieve J ( x  X  )  X  X  X  2 , 0 , 2 } with zero vari-i.e., only with the deterministic policies. Any  X  2 &lt; J ( x  X  ) &lt; 2, J ( x  X  ) 6 = 0, can be achieved but only with a random policy, i.e., with some variance. Thus, the region is not convex. The achievable ( J ( x  X  ) ,V ( x pairs are depicted in the right pane of Figure 1. In this section we present formulae for the mean and variance of the cumulated reward between visits to the recurrent state. The key point in our approach is the following observation. By definition (1), a transition to x  X  always terminates the accumulation in B and does not change its value. Therefore, the following Bellman like equation can be written for the value function
J ( x ) = r ( x ) + X Similar equations can be written for the trajectory variance, and in the following lemma we show that these equations are solvable, yielding expressions for J and V .
 Proposition 3.1. Let P be a stochastic matrix corre-sponding to a policy satisfying Assumption 2.1, where its ( i,j ) -th entry is the transition from state i to state j . Define P 0 to be a matrix equal to P except that the column corresponding to state x  X  is zeroed (i.e., P ( i,x  X  ) = 0 for i = 1 ,...,n ). Then, (a) the matrix I  X  P 0 is invertible; (b) J = ( I  X  P 0 )  X  1 r ; (c) V = ( I  X  P 0 )  X  1  X  , where  X   X  R n and Proof. (a) Consider an equivalent Stochastic Shortest Path (SSP) problem where x  X  is the termination state. The corresponding transition matrix P ssp is defined j 6 = x  X  , and P ssp ( x  X  ,x  X  ) = 1. Furthermore, let P R n  X  1  X  n  X  1 denote the matrix P with the x  X   X  X h row and column removed, which is also the transition matrix of the SSP problem without the terminal state. By the irreducibility of P in Assumption 2.1 P ssp is proper , and by proposition 2.2.1 in (Bertsekas, 2006) we have that I  X  P  X  is invertible.
 Finally, observe that by the definition of P 0 we have thus, det( I  X  P 0 ) 6 = 0. (b) Choose x  X  X  1 ,...,n } . Then, where we excluded the recurrent state from the sum since after reaching the recurrent state there is no further rewards by definition (2). In vectorial form, J = r + P 0 J where using (a) we conclude that J = ( I  X  P 0 )  X  1 r . (c) Choose x  X  X  1 ,...,n } . Then, V ( x ) = E where in the second equality we took the first term out of the summation, and in the third equality we used the definition of J and V . Next, we show that r ( x ) 2 is equal to  X  ( x ): r ( x ) 2 + 2 r ( x ) X = ( r ( x )  X  J ( x )) X = X Proposition 3.1 can be used to derive expressions for the gradients w.r.t.  X  of J and V . Let A  X  B denote the element-wise product between vectors A and B . The gradient expressions are presented in the following lemma.
 Lemma 3.2. We have and where  X   X  =  X  P 0 J 2 +2 P 0 ( J  X  X  X  J )  X  2 P 0 J  X  (  X  P 0 J + P The proof is a straightforward differentiation of the expressions in Lemma 3.1, and is described in Section A of the supplementary material 3 .
 We remark that similar equations for the infinite hori-zon discounted return case were presented by Sobel (1982), in which I  X  P 0 is replaced with I  X   X P , where  X  &lt; 1 is the discount factor. The analysis in (Sobel, 1982) makes use of the fact that I  X   X P is invertible, therefore an extension of their results to the undis-counted case is not immediate. In this section we derive gradient based algorithms for solving problems (3) and (4). We present both exact algorithms, which may be practical for small problems, and simulation based algorithms for larger problems. Our algorithms deal with the constraint based on the penalty method, which is described in the following subsection. 4.1. Penalty methods One approach for solving constrained optimization problems (COPs) such as (3) is to transform the COP to an equivalent unconstrained problem, which can be solved using standard unconstrained optimiza-tion techniques. These methods, generally known as penalty methods , add to the objective a penalty term for infeasibility, thereby making infeasible solutions suboptimal. Formally, given a COP we define an unconstrained problem where g ( x ) is the penalty function , typically taken as g ( x ) = (max (0 ,x )) 2 , and  X  &gt; 0 is the penalty coeffi-cient. As  X  increases, the solution of (10) converges to the solution of (9), suggesting an iterative procedure for solving (9): solve (10) for some  X  , then increase  X  and solve (10) using the previous solution as an initial starting point.
 In this work we use the penalty method to solve the COP in (3). An alternative approach, which is de-ferred to future work, is to use barrier methods , in which a different penalty term is added to the objective that forces the iterates to remain within the feasible set (Boyd &amp; Vandenberghe, 2004). 4.2. Exact Gradient Algorithm When the MDP transitions are known, the expressions for the gradients in Lemma 3.2 can be immediately plugged into a gradient ascent algorithm for the fol-lowing penalized objective function of problem (3) Let  X  k denote a sequence of positive step sizes. Then, a gradient ascent algorithm for maximizing f  X  is  X  k +1 =  X  k +  X  k (  X  J ( x  X  )  X   X g 0 ( V ( x  X  )  X  b )  X  V ( x Let us make the following assumption on the smooth-ness of the objective function and on the set of its local optima. 4 Assumption 4.1. For all  X   X  R K  X  and  X  &gt; 0, the objective function f  X  has bounded second derivatives. Furthermore, the set of local optima of f  X  is countable. Then, under Assumption 4.1, and suitable conditions on the step sizes, the gradient ascent algorithm (11) can be shown to converge to a locally optimal point of f  X  .
 For the SR optimization problem (4), using the quo-tient derivative rule for calculating the gradient of S , we obtain the following algorithm  X  which can be shown to converge under similar condi-tions to a locally optimal point of (4).
 When the state space is large, or when the model is not known, computation of the gradients using equations (6) and (7) is not feasible. In these cases, we can use simulation to obtain unbiased estimates of the gradi-ents, as we describe in the next section, and perform a stochastic gradient ascent. 4.3. Simulation based optimization When a simulator of the MDP dynamics is available, it is possible to obtain unbiased estimates of the gra-dients  X  J and  X  V from a sample trajectory between visits to the recurrent state. The technique is called the likelihood ratio method, and it underlies all policy gradient algorithms (Baxter &amp; Bartlett, 2001; Mar-bach &amp; Tsitsiklis, 1998). The following lemma gives the necessary gradient estimates for our case.
 Lemma 4.2. We have and where the expectation is over trajectories.
 The proof is given in Section B of the supplementary material.
 ing Lemma 4.2 we devise the estimator  X   X  J ( x  X  ) , R 0  X  log P x  X  J ( x  X  ). Furthermore, using the Markov property of the state transition and the fact that the only depen-dance on  X  is in the policy  X   X  , the term  X  log P x  X   X  1 can be reduced to making the computation of  X   X  J ( x  X  ) from an observed trajectory straightforward. Assume for the moment that we know J ( x  X  ) and V ( x  X  ). Then  X   X  V ( x ( R estimate of  X  V ( x  X  ), and plugging  X   X  V and  X   X  J in (11) gives a proper stochastic gradient ascent algorithm. Unfortunately, we cannot calculate J ( x  X  ) exactly with-out knowing the model, and obtaining an unbiased es-timate of J ( x )  X  J ( x ) from a single trajectory is impos-sible (for a similar reason that the variance of a random variable cannot be estimated from a single sample of it). We overcome this difficulty by using a two time-scale algorithm, where estimates of J and V are cal-culated on the fast time scale, and  X  is updated on a slower time scale.
 The algorithm updates the parameters every episode, upon visits to the recurrent state x  X  . Let  X  k where k = 0 , 1 , 2 ,... denote the times of these visits. To ease notation, we also define x k = ( x  X  mulated rewards observed between visits, and denote z k ,  X  log P ( x k ) to be the likelihood ratio derivative. The simulation based algorithm for the constrained optimization problem (3) is  X  J  X  V  X  where  X  k and  X  k are positive step sizes. Similarly, for optimizing the SR (4), we change the update rule for  X  to In the next theorem we prove that algorithm (13) con-verges almost surely to a locally optimal point of the corresponding objective function. The proof for Al-gorithm (14) is essentially the same and thus omit-ted. For notational clarity, throughout the remainder of this section, the dependence of J ( x  X  ) and V ( x  X  ) on  X  is made explicit using a subscript.
 Theorem 4.3. Consider algorithm (13) , and let Assumptions 2.1, 2.2, and 4.1 hold. If the step size sequences satisfy P k  X  k = P k  X  k =  X  , P most surely Proof. (sketch) The proof relies on representing Equa-tion (13) as a stochastic approximation with two time-scales (Borkar, 1997), where  X  J k and  X  V k are updated on a fast schedule while  X  k is updated on a slow schedule. Thus,  X  k may be seen as quasi-static w.r.t.  X  J k and  X  suggesting that  X  J k and  X  V k may be associated with the following ordinary differential equations (ODE) For each  X  , the ODE (16) can be solved analytically to yield J ( t ) = J  X  + c 1 e  X  t and V ( t ) = V  X   X  2 J  X  c e  X  2 t + c 2 e  X  t , where c 1 and c 2 are constants, and { J  X  ,V  X  } is a globally asymptotically stable fixed point which satisfies In turn, due to the timescale difference,  X  J k and  X  V the iteration for  X  k may be replaced with their station-ary limit points J  X  and V  X  , suggesting the following ODE for  X  Under Assumption 4.1, the set of stable fixed point of (18) is just the set of locally optimal points of the objective function f  X  . Let Z denote this set, which by Assumption 4.1 is countable. Then, by Theorem 5 in Leslie &amp; Collins, 2002 (which is extension of Theo-rem 1.1 in Borkar, 1997),  X  k converges to a point in Z almost surely. In this section we apply the simulation based algo-rithms of Section 4 to a portfolio management prob-lem, where the available investment options include both liquid and non-liquid assets. In the interest of understanding the performance of the different algo-rithms, we consider a rather simplistic model of the corresponding financial problem. We emphasize that dealing with richer models requires no change in the algorithms.
 We consider a portfolio that is composed of two types of assets. A liquid asset (e.g., short term T-bills), which has a fixed interest rate r l but may be sold at every time step t = 1 ,...,T , and a non-liquid asset (e.g., low liquidity bonds or options) that has a time dependent interest rate r nl ( t ), yet may be sold only after a maturity period of N steps. In addition, the non-liquid asset has some risk of not being paid (i.e., a default) with a probability p risk . A common invest-ment strategy in this setup is laddering  X  X plitting the investment in the non-liquid assets to chunks that are reinvested in regular intervals, such that a regular cash flow is maintained. In our model, at each time step the investor may change his portfolio by investing a fixed fraction  X  of his total available cash in a non-liquid asset. Of course, he can only do that when he has at least  X  invested in liquid assets, otherwise he has to wait until enough non-liquid assets mature. In addi-tion, we assume that at each t the interest rate r nl ( t ) takes one of two values -r high nl or r low nl , and the tran-sitions between these values occur stochastically with switching probability p switch . The state of the model at each time step is represented by a vector x ( t )  X  R N +2 where x 1  X  [0 , 1] is the fraction of the investment in liq-uid assets, x 2 ,...,x N +1  X  [0 , 1] is the fraction in non-liquid assets with time to maturity of 1 ,...,N time steps, respectively, and x N +2 ( t ) = r nl ( t )  X  E At time t = 0 we assume that all investments are in liquid assets, and we denote x  X  = x ( t = 0). The binary action at each step is determined by a stochastic pol-icy, with probability  X   X  ( x ) = + (1  X  2 ) / 1 + e  X   X x of investing in a non-liquid asset. Note that this  X  -constrained X  softmax policy comes to satisfy Assump-tion 2.3. Our reward is just the logarithm of the return from the investment (which is additive at each step). The dynamics of the investment chunks are illustrated in Figure 2.
 We optimized the policy parameters using the sim-ulation based algorithms of Section 4 with three different performance criteria: (a) Average re-ward: max J ( x  X  ), (b) Variance constrained re-ward max J ( x  X  ) s.t. V ( x  X  )  X  b , and (c) the SR max J ( x  X  ) p V ( x  X  ). Figure 3 shows the distribution of the accumulated reward. As anticipated, the pol-icy for criterion (a) was risky, and yielded higher gain than the policy for the variance constrained criterion (b). Interestingly, maximizing the SR resulted in a very conservative policy, that almost never invested in the non-liquid asset. The parameters for the ex-periments are detailed in the supplementary material, Section C. This work presented a novel algorithmic approach for RL with variance related risk criteria, a subject that while being important for many applications, has been notoriously known to pose significant algorithmic chal-lenges. Since getting to an optimal solution seems hard even when the model is known, we adopted a gradient based approach that achieves local optimality.
 A few issues are in need of further investigation. First, we note a possible extension to other risk measures such as the percentile criterion (Delage &amp; Mannor, 2010). This will require a result reminiscent to Propo-sition 3.1 that would allow us to drive the optimiza-tion. Second, we could consider variance in the opti-mization process to improve convergence time in the style of control variates . Policy gradient algorithms are known to suffer from high variance when the re-current state in not visited frequently. One technique for dealing with this difficulty is by using control vari-ates (Greensmith et al., 2004). Imposing a variance constraint as described in this work also acts along this direction, and may in fact improve performance of such algorithms even if variance is not part of the cri-terion we are optimizing. Third, policy gradients are just one family of algorithms we can consider. It would be interesting to see if a temporal-difference style algo-rithm can be developed for the risk measures consid-ered here. Lastly, we note that experimentally, maxi-mizing the SR resulted in a very risk averse behavior. This interesting phenomenon deserves more research. It suggests that it might be more prudent to consider other risk measures instead of the SR.

