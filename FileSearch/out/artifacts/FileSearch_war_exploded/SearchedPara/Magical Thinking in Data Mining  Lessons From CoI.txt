 are mobile homes that are normally towed by cars. They are called trailers in North America.) The training set contained information on 5822 customers, Of which 348, about 6%, had caravan policies. 
The test set contained information on 4000 customers randomly drawn from the same population. For each customer, the values of 85 features were given. Contest participants were asked to identify which 800 of the test customers were most likely to have caravan policies. The training and test sets used in the contest are now available in the UCI repository. 
Like other data mining contests, the CoIL contest was a valuable testbed for measuring the "end to end" successfulness of data min-ing methods, The real-world usefulness of an algorithm depends not only on its theoretical properties, but also on the ease with which practitioners can apply it without falling victim to overfit-ting and other traps. In principle overfitting can be avoided with almost all methods, but in practice some methods are much more likely to lead their users astray. Contests such as this one provide guidance about which methods are de facto more robust, and about where data mining practitioners need guidance most. 
Figure 1, adapted from [8], shows a histogram of the scores achieved by the 43 individuals or teams that submitted entries to the contest. 
The winning entry, which was mine, identified 121 caravan policy holders among its 800 top predictions. The next best methods iden-tified 115 and 112 policy holders. The mean score was 95.4, with a standard deviation of 19.4. The distribution of scores is clearly not normal, with a long tail to the left. The most common score was 109, and the median score was 103. 
The data mining algorithm used in the winning entry was standard naive Bayesian learning. The second best entry, due to Petri Kon-tkanen, also used a naive Bayesian classifier. Many of the meth-ods described in the 29 reports by participants are much more so-phisticated. They include combinations of backpropagation neural 426 authors of reports applied a well-defined algorithm for feature se-lection that did not require human guidance. Moreover, none of the authors, including myself, used a systematic method to detect important feature interactions. 
Some participants used methods designed to evaluate subsets of features, as opposed to individual features, but these methods failed to detect the interaction between the number of car policies and the total premium paid for car policies, because these two features are highly correlated. For example, one author incorrectly wrote "if 'Contribution car policies' is chosen, the information contained in 'Number of car policies' is already included in the model?' Simi-larly, despite using a "multidimensional visualisation tool," another team also excluded the "number of car policies" feature after in-eluding the "contribution car policies" feature. While the number of car policies and the total premium paid for car policies are corre-lated, the features together do provide more information than either by itself. A different team wrote that in the output of their commercial tool, "some contribution[s] may be related to 2 or more variables. In this last case, the contribution expresses the fact that the important in-formation is brought by the 'additional knowledge' brought by the second variable when the first one is already known." Their tool correctly identified the "number of car policies" feature as highly predictive, but it did not detect the additional predictiveness of the "contribution car policies" feature or the interaction between these two features. Identifying this interaction is important for two rea-sons. First, exploiting it gives better predictive accuracy. Second, as discussed in Section 2, it is the only pattern in the training data that has been shown to be statistically reliable and surprising and actionable. A simple calculation shows that the test dataset provided in the CoIL contest is too small to reveal reliably which learning meth-ods are more accurate. Consider the null hypothesis that a learning method achieves p = 12% accuracy in the top 20% of its predic-tions. On a randomly chosen test set of 4000 examples, the ex-pected number of correct predictions is ,u = pn = 96 where n = 0.2. 4000 = 800, which is similar to the average number of cor-rect predictions achieved by CoIL contest participants. The antic-ipated standard deviation of the number of correct predictions is = ~ -p) = 9.2. In order for us to reject the null hypothe-sis with a confidence of over 95%, a learning method would have to score more than two standard deviations above or below the ex-pected number, that is less than 78 correct or more than 114 correct. Any method that scores between 78 and 114 correct is not statis-tically distinguishable from the null hypothesis method. Only the winning entry and possibly the second best entry are significantly better than the average CoIL contest entry. It is possible to compare two different learning methods with the same training and test datasets in a way that is more sensitive than the simple binomial calculation above, using McNemar's hypoth-esis test [2]. Let A and B designate two learning algorithms and let nl0 be the number of test examples classified correctly by A but incorrectly by B. Similarly, let n01 be the number classified incorrectly by A but correctly by B. McNemar's test is based on calculating the statistic ten by participants reveal an implicit belief that somehow one par-ticular learning method is ideal, and with a little more intelligence, or luck, or data, a person could have discovered this method. For example, one author wrote "But after the announcement of the true policy owners it has become clear that the application of neural net-works isn't necessary, [my] scoring system itself could be used in-stead and, even more, would have performed better." Later he wrote "Unfortunately I have discovered this fact after the announcement of the true policy owners." This type of thought process is called "magical thinking" by an-thropologists. In the words of a famous passage by Gregory Bate-son, "The practitioner of magic does not unlearn his magical view of events when the magic does not work. In fact, the proposi-tions which govern punctuation have the general characteristic of being self-validating" [1]. In any culture, humans have a certain set of expectations that they use to explain the results of their ac-tions. When something surprising happens, rather than question the expectations, people typically believe that they should have done something slightly different. Unless people understand the issue of randomness and statistical significance in an intuitive way, they are liable to believe always that if only they had done something differently in the model building process, their model would have performed better. In the quotation above from Bateson, the word "punctuation" means the way in which we find patterns in our perceptions of our expe-riences. The "propositions" that govern punctuation are the princi-ples or expectations that we have learned to use to organize our per-ceptions. Whatever these principles are, if we have learned them, it is because they appeared to be useful in the past. As pointed out in a different context by Thomas Kuhn [6], practitioners of sci-ence do not unlearn their scientific worldview when science cannot explain a certain phenomenon. Instead, they either ignore the phe-nomenon, or they redouble their efforts to understand it scientifi-cally. Whether scientific or magical, it is difficult, and indeed occa-sionally dangerous, to set aside an entire worldview. Nevertheless, sometimes progress comes from doing so, and magical thinking in data mining is often counter-productive. There are several related manifestations of magical thinking in data mining. One manifestation is non-probabilistic thinking. For ex-ample, a participant wrote "On analyzing the data I found many in-consistent instances (i.e. same values for all attributes but differing [class] values) and removed the minority class of them for training since they might confuse [sic] the algorithm" Similarly, other par-ticipants wrote "The original dataset was cleaned in the way that all contradictory records were reclassified as positive." Another manifestation of magical thinking is the belief that an ideal data mining method does exist that yields classifiers with ideal ac-curacy. Perhaps the clearest example of this belief is provided by the author who wrote "Two important points [sic] about the CART software from Salford systems is its ability to identify an optimal tree, one providing the least amount of error between the learning and population of examples, different methods may be best given different training or test datasets, because of randomness in how these datasets are drawn from the population. A third manifestation of magical thinking in data mining is the be-lief that if a particular learning method or classifier has not per-formed well, this is always due to some specific cause that can be and should be fixed. This belief is best illustrated by comments from a team that submitted a classifier that would have scored 115, but then withdrew this entry and submitted a classifier that scored than maximizing accuracy. Any formal contest must have a definite structure and cannot mimic a real world scenario with complete fidelity. Nevertheless, mea-suring accuracy on an arbitrary percentage of the test set is too far from reality for a data mining contest. In contests that are better de-signed, the performance of participants is evaluated using a matrix of real-world costs and benefits. In this respect the contest orga-nized in conjunction with the 1998 KDD conference, for example, was more realistic than the CoIL contest. An open issue for future data mining contests is how they can be used to compare and evaluate not just data mining algorithms, but also methodologies for data mining. It is noteworthy that none of the reports written by CoIL contest participants mention using any part of the CRISP-DM European standard methodology for data mining [9]. Although it is difficult to say with certainty that one learning method gives more accurate classifiers than another, it is possible to say with certainty that for practical reasons, some learning methods are less suitable than others for the CoIL contest task and all similar tasks. Some data mining software is too slow for real applications. For example, concerning a commercial tool one participant wrote "an analysis of the whole training data set took about 2 hours on a Pen-tium Ill with 500 MHz. Changing that [search depth] value to 3 made the analysis last for about 28 hours." Interestingly, a web page for this tool said after the contest that its "high performance enables you to quickly and reliably analyze small to extremely large quantities of data." Many data mining methods are not flexible enough to cope with the variety of data types found in real commercial data. Most methods can handle numeric features and discrete-valued features. How-ever, commercial data often contains features that are mixed: some training examples have numeric values for a feature, and other ex-amples have symbolic values for the same feature. The symbolic values may have common meanings, for example "missing" or "not applicable" or they may be domain-specific, for example "account frozen ." Even if all the values of a feature appear to be numeric, it is often the case that some specific values are really discrete special cases, Occasionally, some values such as 999 are error codes. For almost all account balance or payment attributes, zero is a value that is common and that has a meaning that is quite different from that of any non-zero amount. Data mining methods that discretize numer-ical values can handle mixed attributes and zero or other specific codes as special cases easily. Other data mining methods often lose a lot of useful information by treating zero and other special codes as ordinary numeric values. Some participants appeared to be unaware that software was avail-able that is much more capable than what they used. For example, one team wrote "the number of features in target selection is typi-cally large (50 to 250 features are common), and hence clustering in such a high dimensional space is computationally prohibitive. Moreover the data then tends to be sparse (there is always a feature where two records differ), and clustering algorithms fail in deal-ing with such data." These statements about clustering methods are false. The k-means algorithm, for example, can handle datasets with millions of records and hundreds of dimensions, where no two records are identical, in effectively linear time [4]. Of course the cations. If necessary, new interaction features and boosting can improve accuracy without impairing comprehensibility. The nega-tive technical lesson is that reliable algorithms are still not available for doing feature subset selection and detecting feature interactions at the same time. The third and most important lesson is more sociological than tech-nical. There is a clear lack of awareness and understanding among some researchers and many practitioners in data mining of the issue of statistical significance. In many applications, the categories that we particularly want to model are rare. Given a training set with a small number of members of rare categories, it is pointless to apply excessively complicated learning methods, or to use an excessively time-consuming model search method, in either case, one is likely to overfit the data and to fool oneself. Fooling oneself happens both in estimating predictive accuracy and also in interpreting models to discover actionable insights. Given a small dataset such as the one for the CoIL challenge, only a few predictive relationships are statistically reliable. Other apparent relationships are likely to be spurious. Acknowledgments: The author is grateful to Gregory Piatetsky-Shapiro, Steve Gallant, Paul Kube, and Peter van der Putten for careful and useful comments on drafts of this paper. [1] G. Bateson. The logical categories of learning and [2] T. G. Dietterich. Approximate statistical tests for comparing [3] C. Elkan. Boosting and naive Bayesian leaming. Technical [4] F. Farnstrom, J. Lewis, and C. Elkan. Scalability for [5] G. Gigerenzer. Adaptive Thinking: Rationality in the Real [6] T. S. Kuhn. The Structure of Scientific Revolutions. [7] S. D. Silvey. Statistical Inference. John Wiley &amp; Sons, Inc., [8] CoIL challenge 2000: The insurance company case, [9] R. Wirth and J. Hipp. CRISP-DM: Towards a standard [ I 0] B. Zadrozny and C. Elkan. Learning and making decisions [l I] B. Zadrozny and C. EIkan. Obtaining calibrated probability 
