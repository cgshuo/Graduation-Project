 Query expansion is an effective method to improve the usability of multimedia search. Most existing multimedia search engines are able to automatically expand a list of textual query terms based on text search techniques, which can be called textual query expansion (TQE). However, the annotations (title and tag) around web videos are generally noisier for text-only query expansion and search matching. In this paper, we propose a novel multi-modal query expansion (MMQE) fra mework for web video search to solve the issue. Compared with traditional methods, MMQE provides a more intuitive query suggestion by transforming tex-tual query to visual presentation based on visual clustering. Paral-lel to this, MMQE can enhance the process of search matching with strong pertinence of intent-specific query by joining textual, visual and social cues from both metadata and content of videos. Experimental results on real web videos from YouTube demon-strate the effectiveness of the proposed method. H.3.3 [ Information Search and Retrieval ]: Search process General Terms : Design, Performance, Experimentation Most of existing popular multimedia search engines (such as Google, Yahoo! and Bing) allow users to represent their search intents by issuing the query as a list of keywords, and provide textual query expansion (TQE) st rategy for disambiguation. Al-ternatively, the authors in [5] formulate a visual query suggestion (VQS) framework by providing image presentations to help users express their search intent for im age search; nevertheless it is still mainly depending on textual clus tering to overcome query ambi-guity. As we all know, compared with images and audios, the annotations around web videos are usually much noisier, which could result in the unsatisfactor y performance in video search application when query expansion is only based on textual cues. Motivated by these analyses, actually, the noisiness of annotations around web videos is the main ch allenge which limits the effec-tiveness of web video search. To address this challenge, we pro-pose a novel query expansion framework (see Figure 1), named multi-modal query expansion (MMQE) for web video search application. The main contributions of this paper are twofold: words clustering. Different from image annotations, video annota-tions are much noisier. Therefore, we depend on the more reliable and objective visual cues of videos to suggest search intent for users. We first use annotations around videos to transform textual query to visual query set by word co-occurrence statistics. This process may bring lots of noisy videos due to the ambiguous textual query and the noisy annotations. For example, as illustrated in Figure 1, some noisy videos about somebody talk ing about  X  X ush attacked by shoe X  are expanded into the visual query set. After that, we resort to affinity propagation [1] to automatically cluster the videos in the visual query set into different sear ch intent categories based on SIFT feature. Then users can choose th eir intent-specific category intui-tively to perform the following search process. With the selected intent-specific category, three expansion strategies are designed to boost the web search quality. positive samples, and negative samples are randomly extracted from testing dataset. Actually, any classi fier model is extensible in this framework. Considering the fact that the positive samples are of high quality and limited quantity, in this work, we choose SVM as the visual classifier model, and leverage the time efficiency and the performance effects acceptably. Secondly, a textual ranker is designe d to expand the original textual query with more relevant keywor ds. We calculate the tag frequen-cies in the intent-specific category and empirically extract the tags whose frequencies are greater than half of the maximum frequency as the expansion keywords. Both expansion strategies motioned above result in the improvement of search recall. Thirdly, a social re-ranker is utilized to refine the average fusion list of visual classifier and textual ranker according to the social rela-tionships among videos. Considering the fact that relevant videos are generally with similar social relationships, such as communities and categories, we employ a manifold ranking algorithm [2] to util-ize these social relationships for search improvement. On the basic assumption that users only care about the top order relation of search results, we firstly constr uct a correlation graph on the top n fusion list. Each vertex denotes a video entity, and each edge de-notes the social correlation between two video entities. Recommen-dation and category information are utilized to assign the weight of edges. If the corresponding two videos both have the same two, one or none of above cues, the weight of edges will be given high, mid-list are utilized as pseudo query poi nts and ranking cues are reserved to trigger the propagation of social relationships on the graph. After convergence, the resultant ranking score of each video is in propor-tion to the probability that it is rele vant to the intent-specific query. The process of social re-ranker me ntioned above can adjust a num-ber of relevant videos to a closer top position, and therefore contrib-ute to the improvement of search precision. For example, as illus-trated in Figure 1, the visual effect of the final search list is to some extent better than the fusion list after social re-ranking. We conduct the MMQE framework on MCG-WEBV [4], a web video dataset containing 80,031 re presentative YouTube videos. The parameters n and k are empirically set to be 1000 and 10 to meet the requirement of near real-time search. We select ten topics with the most popular view from the existing studies [4] and extract no more than two keywords as the corresponding initial queries 
