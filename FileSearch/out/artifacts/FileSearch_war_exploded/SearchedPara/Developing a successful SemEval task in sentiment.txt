 Preslav Nakov 1  X  Sara Rosenthal 2  X  Svetlana Kiritchenko 3  X  Saif M. Mohammad 3  X  Zornitsa Kozareva 4  X  Alan Ritter 5  X  Veselin Stoyanov 6  X  Xiaodan Zhu 3 Abstract We present the development and evaluation of a semantic analysis task that lies at the intersection of two very trendy lines of research in contemporary computational linguistics: (1) sentiment analysis, and (2) natural language processing of social media text. The task was part of SemEval, the International Workshop on Semantic Evaluation, a semantic evaluation forum previously known as SensEval. The task ran in 2013 and 2014, attracting the highest number of participating teams at SemEval in both years, and there is an ongoing edition in 2015. The task included the creation of a large contextual and message-level polarity corpus consisting of tweets, SMS messages, LiveJournal messages, and a special test set of sarcastic tweets. The evaluation attracted 44 teams in 2013 and 46 in 2014, who used a variety of approa-ches. The best teams were able to outperform several baselines by sizable margins with improvement across the 2 years the task has been run. We hope that the long-lasting role of this task and the accompanying datasets will be to serve as a test bed for comparing different approaches, thus facilitating research.
 Keywords Sentiment analysis Twitter SemEval The Internet has democratized content creation enabling a number of new technologies, media and tools of communication, and ultimately leading to the rise of social media and an explosion in the availability of short informal text messages that are publicly available. Microblogs such as Twitter, weblogs such as LiveJournal, social networks such as Facebook, and instant messengers such as Skype and Whatsapp are now commonly used to share thoughts and opinions about anything in the surroundingworld, along with the old-fashioned cell phone messagessuch as SMS. This proliferation of social media content has created new opportunities for studying public opinion, with Twitter being especially popular for research purposes due to its scale, representativeness, variety of topics discussed, as well as easy public access to its messages (Huberman et al. 2008 ; Java et al. 2007 ; Kwak et al. 2010 ).
Despite all these opportunities, the rise of social media has also presented new challenges for natural language processing (NLP) applications, which had largely relied on NLP tools tuned for formal text genres such as newswire, and thus were not readily applicable to the informal language and style of social media. That language proved to be quite challenging with its use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, e.g., RT for re-tweet and #hashtags. 1 In addition to the genre difference, there is also a difference in length: social media messages are generally short, often length-limited by design as in Twitter, i.e., a sentence or a headline rather than a full document. How to handle such challenges has only recently been the subject of thorough research (Barbosa and Feng 2010 ; Bifet et al. 2011 ; Davidov et al. 2010 ; Jansen et al. 2009 ; Kouloumpis et al. 2011 ; O X  X onnor et al. 2010 ; Pak and Paroubek 2010 ; Tumasjan et al. 2010 ).

The advance in NLP tools for processing social media text has enabled researchers to analyze people X  X  opinions and sentiments on a variety of topics, especially in Twitter. Unfortunately, research in that direction was further hindered by the unavailability of suitable datasets and lexicons for system training, development and testing.
Until the rise of social media, research on opinion mining and sentiment analysis had focused primarily on learning about the language of sentiment in general, meaning that it was either genre-agnostic (Baccianella et al. 2010 ) or focused on newswire texts (Wiebe et al. 2005 ) and customer reviews (e.g., from web forums), most notably about movies (Pang et al. 2002 ) and restaurants, but also about hotels, digital cameras, cell phones, MP3 and DVD players (Hu and Liu 2004 ), laptops, etc. This has given rise to several resources, mostly word and phrase polarity lexicons, which have proved to be very valuable for their respective domains and types of texts, but less useful for short social media messages such as tweets.
Over time, some Twitter-specific resources were developed, but initially they were either small and proprietary, such as the i-sieve corpus (Kouloumpis et al. 2011 ), were created only for Spanish like the TASS corpus (Villena-Roma  X  n et al. 2013 ), or relied on noisy labels obtained automatically based on emoticons and hashtags (Go et al. 2009 ; Mohammad 2012 ; Mohammad et al. 2013 ; Panget al. 2002 ). Moreover, they all focused on message-level sentiment only, instead of expression-level sentiment in the context of a tweet. In fact, the first large-scale freely available resource for sentiment analysis on Twitter were the datasets that we developed for SemEval-2013 task 2 (Nakov et al. 2013 ), which we further extended for SemEval-2014 Task 9 (Rosenthal et al. 2014 ) as well as for the upcoming SemEval-2015 Task 10 (Rosenthal et al. 2015 ). They offered both message-level and expression-level annotations.
The primary goal of these SemEval tasks was to serve as a test bed for comparing different approaches, thus facilitating research that will lead to a better understanding of how sentiment is conveyed in social media. These tasks have been highly successful, attracting wide interest at SemEval and beyond: they were the most popular SemEval tasks in both 2013 and 2014, attracting 44 and 46 participating teams, respectively, and they have further fostered the creation of additional freely available resources such as NRC X  X  Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al. 2013 ), which the NRC team developed for their participation in SemEval-2013 task 2, and which were key for their winning the competition. Last but not least, even though named Sentiment Analysis in Twitter , the tasks also included evaluation on SMS and LiveJournal messages, as well as a special test set of sarcastic tweets.
In the remainder of this article, we first introduce the problem of contextual and message-level polarity classification (Sect. 2 ). We then describe the process of creating the training and the testing datasets (Sect. 3 ) and the evaluation setup (Sect. 4 ). Afterwards, we list and briefly discuss the participating systems, the results, and the lessons learned (Sects. 5 and 6 ). Finally, we compare the task to other related efforts (Sect. 7 ), and we point to possible directions for future research (Sect. 9 ). SemEval-2013 task 2 (Nakov et al. 2013 ) and SemEval-2014 Task 9 (Rosenthal et al. 2014 ) had two subtasks: an expression-level subtask and a message-level subtask. Participants could choose to participate in either or both. Below we provide short descriptions of the objectives of these two subtasks.

Subtask A: contextual polarity disambiguation Given a message containing a marked instance of a word or a phrase, determine whether that instance is positive, negative or neutral in that context. The instance boundaries were provided: this was a classification task, not an entity recognition task.
Subtask B: message polarity classification Given a message, decide if it is of positive, negative, or neutral sentiment. For messages conveying both positive and negative sentiment, the stronger one is to be chosen.

Each participating team was allowed to submit results for two different systems per subtask: one constrained, and one unconstrained. A constrained system could only use the provided data for training, but it could also use other resources such as lexicons obtained elsewhere. An unconstrained system could use any additional data as part of the training process; this could be done in a supervised, semi-supervised, or unsupervised fashion.
 Note that constrained/unconstrained refers to the data used to train a classifier. For example, if other data (excluding the test data) was used to develop a sentiment lexicon, and the lexicon was used to generate features, the system would still be constrained. However, if other, manually or automatically labeled data (excluding the test data) was used with the original data to train the classifier, then such a system would be considered unconstrained. 2 In this section, we describe the process of collecting and annotating our datasets of short social media text messages. We will focus our discussion on general tweets as collected for SemEval-2013 Task 2, but our testing datasets also include sarcastic tweets, SMS messages and sentences from LiveJournal, which we will also describe. 3.1 Data collection First, we gathered tweets that express sentiment about popular topics. For this purpose, we extracted named entities using a Twitter-tuned NER system (Ritter et al. 2011 ) from millions of tweets, which we collected over a 1-year period spanning from January 2012 to January 2013; for downloading, we used the public streaming Twitter API.

We then identified popular topics as those named entities that are frequently mentioned in association with a specific date (Ritter et al. 2012 ). Given this set of automatically identified topics, we gathered tweets from the same time period which mentioned the named entities. The testing messages had different topics from training and spanned later periods; this is true for both Twitter2013-test, which used tweets from later in 2013, and Twitter2014-test, which included tweets from 2014.
The collected tweet data were greatly skewed towards the neutral class. In order to reduce the class imbalance, we removed messages that contained no sentiment-bearing words using SentiWordNet as a repository of sentiment words. Any word listed in SentiWordNet 3.0 with at least one sense having a positive or a negative sentiment score greater than 0.3 was considered a sentiment-bearing word. 3
We annotated the same Twitter messages for subtask A and subtask B. However, the final training and testing datasets overlap only partially between the two subtasks since we had to discard messages with low inter-annotator agreement, and this differed between the subtasks.

After the annotation process, we split the annotated tweets into training, development and testing datasets; for testing, we further annotated three additional out-of-domain datasets: 4  X  SMS messages from the NUS SMS corpus 5 (Chen and Kan 2013 );  X  LiveJournal sentences from LiveJournal (Rosenthal and McKeown 2012 );  X  Sarcastic tweets a small set of tweets containing the #sarcasm hashtag. 3.2 Annotation process Our datasets were annotated for sentiment on Mechanical Turk. 6 Each sentence was annotated by five Mechanical Turk workers, also known as Turkers. The annotations for subtask A and subtask B were done concurrently. Each Turker had to mark all the subjective words/phrases in the tweet message by indicating their start and end positions and to say whether each subjective word/phrase was positive, negative, or neutral (subtask A). Turkers also had to indicate the overall polarity of the message (subtask B). The instructions we gave to the Turkers, along with an example, are shown in Fig. 1 . Several additional examples (Table 1 ) were also available to the annotators.

Providing all the required annotations for a given message (a tweet, an SMS, or a sentence from LiveJournal) constituted a Human Intelligence Task, or a HIT. In order to qualify for the task, a Turker had to have an approval rate greater than 95 %, and should have completed 50 approved HITs. We further discarded the following types of annotations: 7  X  messages containing overlapping subjective phrases;  X  messages marked as subjective but having no annotated subjective phrases;  X  messages with every single word marked as subjective;  X  messages with no overall sentiment marked.

For each message, the annotations provided by several Turkers were combined as follows. For subtask A, we combined the annotations using intersection as shown in the last row of Table 2 . A word had to appear in 2/3 of the annotations in order to be considered subjective. Similarly, a word had to be labeled with a particular polarity (positive, negative, or neutral) 2/3 of the time in order to receive that label. We also experimented with other methods of combining annotations: (1) by computing the union of the annotations for the sentence, and (2) by taking the annotations provided by the worker who annotated the most HITs. However, we found that these methods were not as accurate. We plan to explore further alternatives in future work, e.g., using the MACE adjudication method (Hovy et al. 2013 ).

For subtask B, the polarity of the entire sentence was determined based on the majority of the labels. If there was a tie, the sentence was discarded (these are likely to be controversial cases). In order to reduce the number of rejected sentences, we combined the objective and the neutral labels, which Turkers tended to mix up.
For the sarcastic tweets, we slightly altered the annotation task. The tweets were shown to the Turkers without the #sarcasm hashtag, and the Turkers were asked to determine whether the tweet was sarcastic on its own. Furthermore, the Turkers had to indicate the degree of sarcasm as (a) definitely sarcastic, (b) probably sarcastic, and (c) not sarcastic. Although we do not use the degree of sarcasm at this time, it could be useful for analysis as well as possibly excluding tweets that do not appear to be sarcastic. For the SMS and the LiveJournal messages, the annotation task was the same as for tweets, but without the annotations for sarcasm.

The obtained annotations were used as gold labels for the corresponding subtasks. Consecutive tokens marked as subjective serve as target terms in subtask A. The statistics for all datasets are shown in Tables 3 and 4 for subtask A and B, respectively. Each dataset is marked with the year of the SemEval edition it was produced for. An annotated example from each source is shown in Table 5 .
When building a system to solve a task, it is good to know how well we should expect it to perform. One good reference point is human performance and agreement between annotators. Unfortunately, as we derive annotations by agreement, we cannot calculate standard statistics such as Kappa directly. Instead, we decided to measure the agreement between our gold standard annotations (derived by agreement) and the annotations proposed by the best Turker, the worst Turker, and the average Turker (with respect to the gold/consensus annotation for a particular message). Given a HIT, we just calculate the overlaps as shown in the last column in Table 2 , and then we calculate the best, the worst, and the average, which are respectively 13/13, 9/13 and 11/13, in the example. Finally, we average these statistics over all HITs that contributed to a given dataset, to produce lower, average, and upper averages for that dataset. The accuracy (with respect to the gold/consensus annotation) for different averages is shown in Table 6 . Since the overall polarity of a message is chosen based on majority, the upper bound for subtask B is 100 %. These averages give a good indication about how well we can expect the systems to perform. For example, we can see that even if we used the best annotator for each HIT, it would still not be possible to get perfect accuracy, and thus we should also not expect perfect accuracy for an automatic system. 3.3 Tweet delivery Due to Twitter X  X  terms of service, we could not deliver the annotated tweets to the participants directly. Instead, we released annotation indexes and labels, a list of corresponding Twitter IDs, and a download script 8 that extracts the corresponding tweets via the Twitter API.

As a result, the task participants had access to different number of training tweets depending on when they did the downloading, 9 as over time some tweets were deleted. Another major reason for tweet unavailability was Twitter users changing the status of their accounts from public to private. Note that this account status change goes in both directions and changes can be made frequently; thus, some task participants could actually download more tweets by trying several times on different dates. The participating systems were required to perform a three-way classification for both subtasks. A particular marked phrase (for subtask A) or an entire message (for subtask B) was to be classified as positive , negative or objective/neutral .We evaluated the systems by computing a score for predicting positive/negative phrases/messages. For instance, to compute positive precision, P pos , we find the number of phrases/messages that a system correctly predicted to be positive, and we divide that number by the total number it predicted to be positive. To compute positive recall, R pos , we find the number of phrases/messages correctly predicted to be positive and we divide that number by the total number of positives in the gold We carry out similar computations for the negative phrases/messages, F neg . The overall score is then the average of the F 1 -scores for the positive and negative classes: F  X  X  F pos  X  F neg  X  = 2.

We provided the participants with a scorer that outputs the overall score F ,as well as P , R , and F 1 scores for each class (positive, negative, neutral) and for each test set. In the first edition of the task (SemEval-2013), there were 28 submissions by 23 teams for subtask A, and 51 submissions by 38 teams for subtask B; a total of 44 teams took part in the task overall. In the second year (SemEval-2014), the task again attracted a high number of participants: there were 27 submissions by 21 teams for subtask A, and 50 submissions by 44 teams for subtask B, a total of 46 different teams. 10 Eighteen teams participated in both years.

Most of the submissions were constrained, with just a few unconstrained. In any case, the best systems were constrained both years. Some teams participated with both a constrained and an unconstrained system, but the unconstrained system was not always better than the constrained one. There was a single ranking, which included both constrained and unconstrained systems, where the latter were marked accordingly. 5.1 Systems Algorithms In both years, most systems were supervised and used a variety of handcrafted features derived from n -grams, stems, punctuation, part-of-speech (POS) tags, and Twitter-specific encodings such as emoticons, hashtags, and abbreviations. The most popular classifiers included support vector machines (SVM), Maximum Entropy (MaxEnt), and Na X   X  ve Bayes.

Notably, only one of the top-performing systems in 2013, teragram (Reckman et al. 2013 ) (SAS Institute, USA), was entirely rule-based, and fully relied on hand-written rules. We should also mention the emerging but quite promising approach of applying deep learning, as exemplified by the top-performing SemEval-2014 teams of coooolll (Tang et al. 2014 ) (Harbin Institute of Technology and Microsoft Research China) and ThinkPositive (dos Santos 2014 ) (IBM Research Brazil). 11
Preprocessing In addition to standard NLP steps such as tokenization, stemming, lemmatization, stop-word removal and POS tagging, most teams applied some kind of Twitter-specific processing such as substitution/removal of URLs, substitution of emoticons, spelling correction, word normalization, abbreviation lookup, and punctuation removal. Several teams reported using Twitter-tuned NLP tools such as POS and named entity taggers (Gimpel et al. 2011 ; Ritter et al. 2011 ).
External lexical resources Many systems relied heavily on existing sentiment lexicons. Sentiment lexicons are lists of words (and sometimes phrases) with prior associations to positive, negative, and sometimes neutral sentiment. Some lexicons provide a real-valued or a discrete sentiment score for a term to indicate its intensity. Most of the lexicons that were created by manual annotation tend to be domain-independent and include a few thousand terms, but larger lexicons can be built automatically or semi-automatically. The most popular lexicons used by participants in both years included the manually created MPQA Subjectivity Lexicon (Wilson et al. 2005 ), Bing Liu X  X  Lexicon (Hu and Liu 2004 ), as well as the automatically created SentiWordNet (Baccianella et al. 2010 ). The winning team at SemEval-2013, NRC-Canada (Mohammad et al. 2013 ), reported huge gains from their automatically created high-coverage tweet-specific sentiment lexicons (Hash-tag Sentiment Lexicon and Sentiment140 lexicon). 12 They also used the NRC Emotion Lexicon (Mohammad and Turney 2010 , 2013 ) and the Bing Liu Lexicon (Hu and Liu 2004 ). The NRC lexicons were released to the community, and were used by many teams in the subsequent editions of the SemEval Twitter sentiment task.

In addition to using sentiment lexicons, many top-performing systems used word representations built from large external collections of tweets or other corpora. Such representations serve to reduce the sparseness of the word space. Two general approaches for building word representations are word clustering and word embeddings. The Brown clustering algorithm (Brown et al. 1992 ) groups syntac-tically or semantically close words in a hierarchy of clusters. The CMU Twitter NLP tool provides word clusters produced with the Brown clustering algorithm on 56 million English-language tweets. Recently, several deep learning algorithms have been proposed to build continuous dense word representations, called word embeddings (Collobert et al. 2011 ; Mikolov et al. 2013 ). Similar to word clusters, syntactically or semantically close words should have similar embedding vectors. The pre-trained word embeddings are publicly available, 13 but they were generated from news articles. Therefore, some teams chose to train their own word embeddings on tweets using the available software word2vec (Mikolov et al. 2013 ). The coooolll team (Tang et al. 2014 ) (Harbin Institute of Technology and Microsoft Research China) went one step further and produced sentiment-specific word embeddings. They extended the neural network C&amp;W model (Collobert et al. 2011 ) to incorporate the sentiment information on sentences and modified the loss function to be a linear combination of syntactic loss and sentiment loss. Similarly, at SemEval-2015, the UNITN team (Severyn and Moschitti 2015 ) used an unsupervised neural language model to initialize word embeddings that they further tuned by a deep learning model using a separate corpus and distant supervision; they then continued training in a supervised way on the SemEval data.
 Further details on individual systems can be found in the proceedings of SemEval-2013 (Manandhar and Yuret 2013 ), SemEval-2014 (Nakov and Zesch 2014 ), and SemEval-2015 (Nakov et al. 2015 ). 5.2 Baselines There are several baselines that one might consider for this task, and below we will explore some of the most interesting ones.

Majority class This baseline always predicts the most frequent class as observed on the training dataset. As our official evaluation metric is an average of the F-score for the positive and for the negative classes, it makes sense to consider these two classes only. For our training dataset, this baseline predicts the positive class for both subtasks A and B as it is more frequent for both subtasks.

Target X  X  majority class This baseline is only applicable to subtask A. For that subtask, we can calculate the majority class for individual target terms. If a target term (a word or a phrase) from the test set occurs as target in the training dataset, this baseline predicts the most frequent class for that term. If the frequencies tie between two classes, the priority in the order of positive, negative, and neutral is used to break the tie. For example, if a term appears the same number of times as positive and as negative in the training dataset, we predict positive class for the term. If a target term does not occur in the training data, we predict the most frequent class from the entire training dataset, i.e., the positive class.
Lexicon-based We add up the scores for lexicon words or phrases matched in the target term (for subtask A) or in the entire message (for subtask B), and we predict a positive class if the cumulative sum is greater than zero, a neutral class if it is zero, and a negative class if it is \ 0. If no matches are found, we predict neutral. We calculate this baseline using three different sentiment lexicons: MPQA Subjectivity Lexicon, Bing Liu X  X  Lexicon, and SentiWordNet 3.0. We use a score of 1 for a positive entry and a score of 1 for a negative entry in the MPQA and Bing Liu X  X  lexicons. As SentiWordNet has a real-valued positive score and a real-valued negative score assigned to a word sense, for it we average positive and negative scores over all senses of a word and we subtract the average negative score from the average positive score to get the final sentiment score for the target word or phrase. SVM unigrams This is a more sophisticated baseline, which trains a Support Vector Machine classifier on the training dataset, using unigrams as features. In the experiments, we used the LibSVM package (Chang and Lin 2011 ) with linear kernel and a value of the C parameter that we optimized on the development dataset.
SVM unigrams ? bigrams This baseline is similar to the previous one with the exception that the feature set now includes unigrams and bigrams.

Table 7 shows the macro-averaged F-scores for different baselines. First, note that for almost all baselines the scores for subtask A are substantially higher than the corresponding scores for subtask B. Second, we can see that for subtask A the Target X  X  Majority Class baseline and the SVM unigrams baseline achieve remarkable results: by simply predicting a target X  X  majority class one can obtain F-scores in the low seventies, and by training an SVM model with only unigram features one can get F-scores in the low eighties. For comparison, for subtask B the SVM unigrams baseline only achieves F-scores in the fifties. We explore the differences between the subtasks and the corresponding datasets in more detail in Sect. 6.7 .

Overall, for both subtasks, statistical machine learning yields stronger baseline results than simple lexicon-based methods. Therefore, it is not surprising that most participants relied on statistical learning from the training dataset and used sentiment lexicons to obtain additional features.

Finally, note that most baselines perform badly on sarcastic tweets, even though the Majority Class baseline score on this dataset does not significantly differ from the corresponding scores on the other test datasets. 5.3 Results The results for the 2014 edition of the task are shown in Tables 8 and 9 , and the corresponding team affiliations are shown in Table 11 . The tables show results on the two progress test datasets (tweets and SMS messages), which are the official test datasets from the 2013 edition of the task, and on the three official 2014 test sets (tweets, tweets with sarcasm, and LiveJournal). There is an index for each result showing its relative rank within the respective column. The systems are ranked by their score on the Twitter-2014 test set, which is the official ranking for the task; all remaining rankings are secondary. 5.3.1 Subtask A Table 8 shows the results for subtask A, which attracted 27 submissions from 21 teams at SemEval-2014. There were seven unconstrained submissions: five teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only. The best systems were constrained.

Comparing Table 8 to Table 7 , we can see that all participating systems outperformed the Majority Class baseline by a sizable margin. However, some systems could not beat the Target X  X  Majority Class baseline, and most systems could not compete against the SVM-based baselines. 5.3.2 Subtask B The results for subtask B are shown in Table 9 . The subtask attracted 50 submissions from 44 teams at SemEval-2014. There were eight unconstrained submissions: six teams submitted both a constrained and an unconstrained run, and two teams submitted an unconstrained run only. As for subtask A, the best systems were constrained.

Comparing Table 9 to Table 7 , we see that almost all participating systems outperformed the Majority Class baseline, but some ended up performing slightly lower on some of the datasets. Moreover, several systems could not beat the remaining stronger baselines; in particular, about a third of the systems could not compete against the SVM-based baselines. In this section, we analyze the results from several perspectives. In particular, we discuss the progress over the first 2 years of the SemEval task, the system independence of the training domain, the need for external lexical resources, the impact of different techniques for handling negation and context, and the differences between the two subtasks. 6.1 Progress over the first 2 years As Table 11 shows, 18 out of the 46 teams in 2014 also participated in the 2013 edition of the task. Comparing the results on the progress Twitter test dataset Nakov et al. ( 2013 ) and Rosenthal et al. ( 2014 ), we can see that NRC-Canada , the 2013 winner for subtask A, has now improved their F-score from 88.93 (Mohammad et al. 2013 ) to 90.14 (Zhu et al. 2014b ), which is the 2014 best. The best score on the progress SMS test dataset in 2014 of 89.31 belongs to ECNU (Zhao et al. 2014 ); this is a big jump compared to their 2013 score of 76.69 (Tiantian et al. 2013 ), but it is lower compared to the 2013 best of 88.37 achieved by GU-MLT-LT (Gu  X  nther and Furrer 2013 ).
 For subtask B, on the Twitter progress test dataset, the 2013 winner, NRC-Canada , improves their 2013 result from 69.02 (Mohammad et al. 2013 ) to 70.75 (Zhu et al. 2014b ), which is the second best in 2014; the winner in 2014, TeamX , achieves 72.12 (Miura et al. 2014 ). On the SMS progress test set, the 2013 winner, NRC-Canada , improves its F-score from 68.46 to 70.28. Overall, we see consistent improvements on the progress test datasets for both subtasks: 0 X 1 and 2 X 3 points absolute for subtasks A and B, respectively.

For both subtasks, the best systems on the Twitter2014-test dataset are those that performed best on the progress Twitter2013-test dataset: NRC-Canada for subtask A, and TeamX (Fuji Xerox Co., Ltd.) for subtask B. However, the best results on Twitter2014-test are substantially lower than those for the Twitter2013-test for both subtask A (86.63 vs. 90.14) and subtask B (70.96 vs. 72.12). This is so despite the Majority Class baselines for Twitter2014-test being higher than those for Twitter2013-test: 42.2 versus 38.1 for subtask A, and 34.6 versus 29.2 for subtask B. Most likely, having access to the Twitter2013-test at development time, teams have overfitted on it. It could be also the case that some of the sentiment lexicons that were built in 2013 have become somewhat outdated by 2014. 6.2 Performance on out-of-domain data All participating systems were trained on tweets only. No training data were provided for the other test domains, SMS and blogs, nor was there training data for sarcastic tweets. Some teams, such as NRC-Canada , performed well across all test datasets. Surprisingly, on the out-of-domain test datasets they were able to achieve results comparable to those they obtained on tweets, or even better. Other teams, such as TeamX , chose to tune a weighting scheme specifically for class imbalances in tweets and, as a result, were only strong on Twitter datasets.

The Twitter2014-sarcasm dataset turned out to be the most challenging test dataset for most of the participants in both subtasks. The differences in performance on general and sarcastic tweets was 5 X 10 points for subtask A and 10 X 20 points for subtask B for most of the systems.
 6.3 Impact of training data size As we mentioned above, due to Twitter X  X  terms of service, we could not deliver the annotated tweets to the participants directly, and they had to download them on their own, which caused problems as at different times different subsets of the tweets could be downloaded. Thus, task participants had access to different number of training tweets depending on when they did the downloading.

To give some statistics, in the 2014 edition of the task, the number of tweets that participants could download and use for subtask B varied between 5215 tweets and 10,882. On average, the teams were able to collect close to 9000 tweets; teams that did not participate in 2013, and thus had to download the data later, could download about 8500 tweets.

The difference in training data size did not seem to have had a major impact. In fact, the top two teams in subtask B in 2014 [ coooolll (Tang et al. 2014 ) and TeamX (Miura et al. 2014 )] used \ 8500 tweets for training. 6.4 Use of external resources The participating systems were allowed to make use of external resources. As described in Sect. 2 , a submission that directly used additional labeled data as part of the training dataset was considered unconstrained . In both 2013 and 2014, there were cases of a team submitting a constrained and an unconstrained run and the constrained run performing better. It is unclear why unconstrained systems did not always outperform the corresponding constrained ones. It could be because participants did not use enough external data or because the data they used was too different from Twitter or from our annotation method.

Several teams chose to use external (weakly) labeled tweet data indirectly, by creating sentiment lexicons or sentiment word representations, e.g., sentiment word embeddings. This approach allowed the systems to qualify as constrained , but it also offered some further benefits. First, it allowed to incorporate large amounts of noisily labeled data quickly and efficiently. Second, the classification systems were robust to the introduced noise because the noisy data were incorporated not directly as training instances but indirectly as features. Third, the generated sentiment resources could be easily distributed to the research community and used in other applications and domains (Kiritchenko et al. 2014a ).

These newly built sentiment resources, which leveraged on large collections of tweets, yielded large performance gains and assured top ranks for the teams that made use of them. For example, NRC-Canada reported 2 and 6.5 points of absolute improvement for subtasks A and B, respectively, by using their tweet-specific sentiment lexicons. On top of that, the coooolll team achieved another 3 X 4 points absolute improvement on the tweet test datasets for Subtask B thanks to sentiment-specific word embeddings.

Most participants greatly benefited from the use of existing general-domain sentiment lexicons. Even though the contribution of these lexicons on top of the Twitter-specific resources was usually modest, namely 1 X 2 points absolute, on the Twitter test datasets, the general-domain lexicons were particularly useful on out-of-domain data, such as the SMS test dataset, where their use resulted in gains of up to 3.5 points absolute for some participants.

Similarly, general-domain word representations, such as word clusters and word embeddings, showed larger gains on the out-of-domain SMS test dataset (1 X 2 points absolute) than on Twitter test datasets (0.5 X 1 points absolute). 6.5 Negation handling Many teams incorporated special handling of negation into their systems. The most popular approach transformed any word that appeared in a negated context by adding a suffix _NEG to it, e.g., good would become good_NEG (Das and Chen 2007 ; Pang et al. 2002 ). A negated context was defined as a text span between a negation word, e.g., no, not, shouldn X  X  , and a punctuation mark or the end of the message.

Alternatively, some systems flipped the polarity of sentiment words when they occurred in a negated context, e.g., the positive word good would become negative when negated. The RTRGO team (Gu  X  nther et al. 2014 ) reported an improvement of 1.5 points absolute for Subtask B on Twitter data when using both approaches together.

In Zhu et al. ( 2014a ), the authors argued that negation affects different words differently, and that a simple reversing strategy cannot adequately capture this complex behavior. Therefore, they proposed an empirical method to determine the sentiment of words in the presence of negation by creating a separate sentiment lexicon for negated words (Kiritchenko et al. 2014b ). Their system, NRC-Canada , achieved 1.5 points of absolute improvement for Subtask A and 2.5 points for Subtask B by using sentiment lexicons generated for affirmative and negated contexts separately. 6.6 Use of context in subtask A As suggested by the name of subtask A, Contextual Polarity Disambiguation ,a model built for this subtask is expected to explore the context around a target term. For example, the top-performing NRC-Canada system used unigrams and bigrams extracted within four words on either side of the target term. The system also extracted additional features from the entire message in the same way as it extracted features from the target terms themselves. The inclusion of these context features resulted in F-score improvements of 4.08 points absolute on Twitter2013-test and 2.41 points on SMS2013-test. The second-best system in 2013, AVAYA (Becker et al. 2013 ), used dependency parse features such as the paths between the head of the target term and the root of the entire message. Similarly, the third-best BOUNCE system (Ko  X  kciyan et al. 2013 ) used features and words extracted from neighboring target phrases, achieving 6.4 points of absolute improvement on Twitter2013-dev. The fourth-best LVIC-LIMSI system (Marchand et al. 2013 ) also used the words surrounding the target terms during development, but their effect on the overall performance was not reported. The SentiKLUE system (Evert et al. 2014 ), second-best in 2014, used context in the form of automatically predicted message-level polarity. 6.7 Why subtask A seems easier than subtask B The performance of the sentiment analysis systems is significantly higher for subtask A than for subtask B. A similar difference can also be observed for many baselines, including the SVM-unigrams baseline. Furthermore, a simple Target X  X  Majority Class baseline showed surprisingly strong results on subtask A. Thus, we analyzed the data in order to determine why these baselines performed so well for subtask A. We found that 85.1 % of the target words in Twitter2013-test and 88.8 % of those in Twitter2014-test occurred as target tokens in the training data. Moreover, the distribution of occurrences of a target word that has been observed with different polarities is skewed towards one polarity.

Finally, the average ratio of instances pertaining to the dominant polarity of a target term to the total number of instances of that target term is 0.80. (Note that this ratio is calculated for all target words that occurred more than once in the training and in the test datasets.) These observations explain, at least in part, the high overall result and the dominant role of unigrams for subtask A.

We have conducted an experiment to examine the impact of sentiment resources in subtask A in the situation where the test targets would not appear in the training set. For this, we split the Twitter2013-test set into three subsets. In the first subset,  X  X  X argets fully seen in training X  X , each instance has a target with the following property: there exist instances in the training data with exactly the same target; this subset comprises 55 % of the test set. In the second subset,  X  X  X argets partially seen in training X  X , each instance has a target X with the following property: there exist instances in the training data whose target expression includes one or more, but not all, tokens in X ; this subset comprises 31 % of the test set. In the third subset,  X  X  X argets unseen in training X  X , each instance has a target X with the following property: there are no instances in the training data whose target includes any of the tokens in X ; this subset comprises 14 % of the test set. We then ran the top-performing NRC-Canada system on each of the three subsets (a) using all features, (b) using all but the lexicon features, and (c) using all but the n -gram features. The results are shown in Table 10 . We can see that on instances with unseen targets the sentiment lexicons play the most prominent role, yielding a gain of 14.54 points absolute. Sentiment analysis has enjoyed a lot of research attention over the last 15 years, especially in sub-areas such as detecting subjective and objective sentences; classifying sentences as positive, negative, or neutral; and more recently, detecting the target of the sentiment. Much of this work focused on customer reviews of products and services, but tweets, Facebook posts, and other social media data are now increasingly being explored. Recent surveys by Pang and Lee ( 2008 ) and Liu and Zhang ( 2012 ) give detailed summaries of research on sentiment analysis.
Initially, the problem was regarded as standard document classification into topics, e.g., (Pang et al. 2002 ) experimented with various classifiers such as maximum entropy, Na X   X  ve Bayes and SVM, using standard features such as unigram/ bigrams, word counts/present, word position and POS tagging. Around the same time, other researchers realized the importance of external sentiment lexicons, e.g., (Turney 2002 ) proposed an unsupervised approach to learn the sentiment orientation of words/phrases: positive versus negative. Later work studied the linguistic aspects of expressing opinions, evaluations, and speculations (Wiebe et al. 2004 ), the role of context in determining the sentiment orientation (Wilson et al. 2005 ), of deeper linguistic processing such as negation handling (Pang and Lee 2008 ), of finer-grained sentiment distinctions (Pang and Lee 2005 ), of positional information (Raychev and Nakov 2009 ), etc. Moreover, it was recognized that in many cases, it is crucial to know not just the polarity of the sentiment, but also the topic towards which this sentiment is expressed (Stoyanov and Cardie 2008 ).

Naturally, most research in sentiment analysis was done for English, and much less efforts were devoted to other languages (Abdul-Mageed et al. 2011 ; Chetv-iorkin and Loukachevitch 2013 ; Jovanoski et al. 2015 ; Kapukaranov and Nakov 2015 ; Perez-Rosas et al. 2012 ; Tan and Zhang 2008 ).

Early sentiment analysis research focused on customer reviews of movies, and later of hotels, phones, laptops, etc. Later, with the emergence of social media, sentiment analysis in Twitter became a hot research topic. Yet, there was a lack of suitable datasets for training, evaluating, and comparing different systems. This situation changed with the emergence of the SemEval task on Sentiment Analysis in Twitter, which ran in 2013 X 2015 (Nakov et al. 2013 ; Rosenthal et al. 2014 , 2015 ). The task created standard datasets of several thousand tweets annotated for sentiment polarity.
 In fact, there was an even earlier shared task on sentiment analysis of text: the SemEval-2007 Affective Text Task (Strapparava and Mihalcea 2007 ). However, it was framed as an unsupervised task where newspaper headlines were to be labeled with eight affect categories X  X ositive and negative sentiment, as well as six emotions (joy, sadness, fear, anger, surprise, and disgust). For each headline X  X ffect category pair, human annotators assigned scores from 0 to 100 indicating how strongly the headline expressed the affect category. In contrast, in our task, we focus on tweets, SMS messages, and blog posts. Moreover, apart from our main subtask on message-level sentiment, we also include a subtask on determining phrase-level sentiment.

Since our 2013 shared task, several other shared tasks have been proposed that further explored various sub-problems in sentiment analysis. We describe them briefly below. 7.1 Aspect-based sentiment analysis The goal of the SemEval-2014 Task 4 on Aspect-Based Sentiment Analysis (ABSA) was to identify aspect terms and the sentiment towards those aspect terms from customer reviews, where the focus was on two domains: laptops and restaurants (Pontiki et al. 2014 ). 14
For example, a review may gush positively about the lasagna at a restaurant, but negatively about the long wait before the food has arrived. In the restaurant domain, the aspect terms were further aggregated into coarse categories such as food , service , ambiance , price , and miscellaneous . The goal was to identify these aspect categories and the sentiment expressed towards them.

The ABSA task attracted 32 teams, who contributed 165 submissions. There is substantial overlap in the approaches and resources used by the participants in our task and in the ABSA task. Moreover, one of the top performing systems in our competition, NRC-Canada , also participated in the ABSA task and achieved the best scores in three out of the six subtask-domain combinations, including two out of the three sentiment subtasks (Zhu et al. 2014c ). The use of automatically created in-domain sentiment resources proved to be valuable for this task as well. Other useful features were derived from dependency parse trees in order to establish the relationship between aspect terms and sentiment expressions.

There is an ongoing follow-up task, SemEval-2015 Task 12 (Pontiki et al. 2015 ), which consolidates the subtasks from 2014 into a principled unified framework, where opinion target expressions, aspects and sentiment polarities are linked to each other in tuples. This is arguably useful when generating structured aspect-based opinion summaries from user reviews in real-world applications (e.g., customer review sites). The task is further extended to multiple sentences, and a new domain is added: reviews of hotels. Overall, this follow-up task has attracted 93 submissions by 16 teams. 7.2 Sentiment analysis of figurative language Social media posts are often teeming with creative and figurative language, rich in irony, sarcasm, and metaphors. The SemEval-2015 Task 11 (Ghosh et al. 2015 )on Sentiment Analysis of Figurative Language 15 is interested in understanding how this creativity impacts perceived affect. For this purpose, tweets rich in irony, sarcasm, and metaphor were annotated on an 11-point discrete scale from 5 (most negative) to  X  5 (most positive). The participating systems were asked to predict this human-annotated fine-grained sentiment score, and were evaluated not only on the full dataset, but also separately on irony, sarcasm, and metaphor. One of the goals of the task was to explore how conventional sentiment analysis techniques can be altered to deal with non-literal content.

While our task also had evaluation on sarcastic tweets, for us this was just a separate (arguably harder) test set: we did not focus specifically on sarcasm and we did not provide specific training data for it. In contrast, SemEval-2015 Task 11 was fully dedicated to analyzing figurative language on Twitter (which includes not only sarcasm, but also irony and metaphor); moreover, they used an 11-point scale, while we were interested in predicting three classes. The task has attracted 15 teams, who submitted 29 runs. 7.3 Detecting events and polarity towards events SemEval-2015 Task 9 CLIPEval Implicit Polarity of Events (Russo et al. 2015 ) focuses on the implicit sentiment polarity towards events. 16 There are two subtasks. The first one asks to determine the sentiment (positive, negative, or neutral) towards an event instance, while the second one requires to identify both event instantiations and their associated polarity values. The task is based on a dataset of events annotated as instantiations of pleasant and unpleasant events previously collected in psychological research (Lewinsohn and Amenson 1978 ; MacPhillamy and Lewin-sohn 1982 ). It has attracted two teams who submitted three runs. 7.4 Sentiment analysis of movie reviews A popular test bed for sentiment analysis systems has been the movie reviews dataset from rottentomatoes.com collected initially by Pang and Lee ( 2005 ). State-of-the-art results were obtained on this test set using a recursive deep neural network (Socher et al. 2013 ): an F-score of 85.4 on detecting review-level polarity (positive or negative). Even though this method does not require any handcrafted features or external semantic knowledge, it relies on extensive phrase-level sentiment annotations during training, which are expensive to acquire for most real-world applications.

Very comparable results (an F-score of 85.5) were reported using more conventional machine learning techniques, and crucially, large-scale sentiment lexicons generated automatically from tweets (Kiritchenko et al. 2014b ).
Finally, there is an ongoing 2015 Kaggle competition Classify the sentiment of sentences from the Rotten Tomatoes dataset , which aims to bring together sentiment analysis systems for fine-grained sentiment analysis of movie reviews. 17 8.1 The SemEval-2015 edition of the task In addition to the two subtasks described above (contextual and message-level polarity), we have added three new subtasks 18 in 2015. The first two focus on the sentiment towards a given topic in a single tweet or in a set of tweets, respectively. The third new subtask asks to determine the strength of prior association of Twitter terms with positive sentiment; this acts as an intrinsic evaluation of automatic methods that build Twitter-specific sentiment lexicons with real-valued sentiment association scores.  X  Topic-based message polarity classification Given a message and a topic,  X  Detecting trends towards a topic Given a set of messages on a given topic from  X  Determining the strength of twitter sentiment terms Given a word or a phrase, 8.2 Outlook on SemEval-2016 There is a new edition of the task which will run as part of SemEval-2016. In this new edition, 19 we will focus on sentiment with respect to a topic, but on a five-point scale, which is used for human review ratings on popular websites such as Amazon, TripAdvisor, Yelp, etc. From a research perspective, moving to an ordered five-point scale means moving from binary classification to ordinal regression .
We further plan to continue the trend detection subtask, which represents a move from classification to quantification , 20 and is on par with what applications need. In real-world applications, the focus often is not on the sentiment of a particular tweet, but rather on the percentage of tweets that are positive/negative.

Finally, we plan a new subtask on trend detection, but using a five-point scale, which would get us even closer to what business (e.g., marketing studies), and researchers, (e.g., in political science or public policy), want nowadays. From a research perspective, this is a problem of ordinal quantification (Esuli and Sebastiani 2010 ). We have presented the development and evaluation of a SemEval task on Sentiment Analysis in Twitter. The task included the creation of a large contextual and message-level polarity corpus consisting of tweets, SMS messages, LiveJournal messages, and a special test set of sarcastic tweets. It ran in 2013, 2014, and 2015, attracting the highest number of participating teams in all 3 years, with new challenging subtasks added in 2015, and some coming in 2016.
 The task has fostered the creation of some freely-available resources such as NRC X  X  Hashtag Sentiment lexicon and the Sentiment140 lexicon (Mohammad et al. 2013 ), which the NRC-Canada team initially developed for their participation in SemEval-2013 task 2, and which were key for their winning the competition. Further specialized resources were developed for 2014 and for 2015 as well.
We hope that the long-lasting role of this task and the accompanying datasets, which we release freely 21 under a Creative Commons Attribution 3.0 Unported License 22 , will be to serve as a test bed for comparing different approaches and for fostering the creation of new relevant resources. This would facilitate research, would lead to better understanding of how sentiment is conveyed in social media, and ultimately to the creation of better sentiment analysis systems.

In future work, we plan to extend the task with new data from additional domains. We further plan to work on getting the setup as close as possible to what real-world applications need; this could mean altering the task/subtask definition, the data filtering process, the data annotation procedure, and/or the evaluation setup. Last but not least, we are interested in comparing annotations obtained from crowdsourcing with annotations from experts (Borgholt et al. 2015 ).
 References
