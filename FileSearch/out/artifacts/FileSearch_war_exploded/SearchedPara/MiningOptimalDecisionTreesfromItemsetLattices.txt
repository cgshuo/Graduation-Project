 We presen t DL8 , an exact algorithm for nding a decision tree that optimizes a ranking function under size, depth, accuracy and leaf constrain ts. Because the disco very of op-timal trees has high theoretical complexit y, until now few e orts have been made to compute suc h trees for real-w orld datasets. An exact algorithm is of both scien ti c and prac-tical interest. From a scien ti c point of view, it can be used as a gold standard to evaluate the performance of heuris-tic constrain t-based decision tree learners and to gain new insigh t in traditional decision tree learners. From the appli-cation point of view, it can be used to disco ver trees that cannot be found by heuristic decision tree learners. The key idea behind our algorithm is that there is a relation between constrain ts on decision trees and constrain ts on itemsets. We sho w that optimal decision trees can be extracted from lattices of itemsets in linear time. We give sev eral strate-gies to ecien tly build these lattices. Exp erimen ts sho w that under the same constrain ts, DL8 obtains better results than C4.5 , whic h con rms that exhaustiv e searc h does not alw ays imply over tting. The results also sho w that DL8 is a useful and interesting tool to learn decision trees under constrain ts.
 H.2.8 [ Database Managemen t ]: Database applications| Data Mining ; I.2.8 [ Arti cial Intelligence ]: Problem Solv-ing, Con trol Metho ds, and Searc h| Dynamic programming Algorithms, Exp erimen tation, Performance Decision trees, Frequen t itemsets, Formal concepts, Constrain t-based mining Cop yright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
Decision trees are among the most popular prediction mo dels in mac hine learning and data mining, because there are ecien t, relativ ely easily understandable algorithms and the mo dels are easy to interpret. From this persp ectiv e, it is surprising that mining decision trees under constrain ts has not been given much atten tion. For the problems listed be-low, curren tly no broadly applicable algorithm exists even though steps in this direction were made in [8] for the last problem: In the interactiv e pro cess that kno wledge disco very in data-bases is, the abilit y to pose queries that answ er these ques-tions can be very valuable.

Most well-kno wn algorithms for building decision trees, for instance C4.5 , use a top-do wn induction paradigm, in whic h a good split is chosen heuristically . If suc h algorithms do not nd a tree that satis es the speci ed constrain ts, this does not mean that suc h a tree does not exist|it only means that the chosen heuristic is not good enough to nd it. An exact algorithm could be desirable to answ er queries without uncertain ty. Furthermore, to assess the qualit y of heuristic learners, it is of interest to kno w, for a sucien tly large num ber of datasets, what their true optim um under given constrain ts is. This would allo w us to gain deep er insigh t in the predictiv e beha vior of decision trees. For instance, [18] rep orted that for small, mostly arti cial datasets, small decision trees are not alw ays preferable in terms of general-ization abilit y, while [27] sho wed that when learning rules, exhaustiv e searc hing and over tting are orthogonal. An e-cien t algorithm for learning decision trees under constrain ts allo ws us to investigate these observ ations for larger datasets and more complex mo dels.

To the best of our kno wledge, few attempts have been made to implemen t a general and exact algorithm for learn-ing decision trees under constrain ts; most people have not seriously considered the problem as it is kno wn to be NP-complete [12], and therefore, an ecien t algorithm most likely does not exist. This theoretical result however does not imply that the problem is unsolv able in all cases. In the data mining literature, sev eral exp onen tial problems have still been sho wn to be solv able in practice. In particular, the problem of frequen t itemset mining has attracted a lot of researc h [1, 32, 11], and man y frequen t itemset mining algorithms have been applied successfully despite the exp o-nen tial nature of this problem.

In this pap er, we prop ose DL8, an exact algorithm for building decision trees that does not rely on the traditional approac h of heuristic top-do wn induction, and addresses the problem of nding exact optimal decision trees under con-strain ts. Its key feature is that it exploits a relation between constrain ts on itemsets and decision trees. Even though our algorithm is not exp ected to work on all possible datasets, we will pro vide evidence that for a reasonable num ber of datasets, our approac h is feasible and therefore a useful ad-dition to the data mining toolb ox.

This pap er is organized as follo ws. In Section 2, we in-troduce the concepts of decision trees and itemsets. In Sec-tion 3, we describ e precisely whic h optimal trees we are look-ing for. In Section 4, we motiv ate the use of suc h optimal trees. In section 5, we presen t our algorithm and its connec-tion to frequen t itemset mining. In Section 6, we evaluate the eciency of our algorithm; we compare the accuracy and size of the trees computed by our system with the trees learned by C4.5 . Section 7 gives related work. We conclude in Section 8.
Let us rst introduce some terminology regarding frequent itemsets and decision trees .

Let I = f i 1 ; i 2 ; : : : ; i m g be a set of items and let D = f T 1 ; T 2 ; : : : ; T n g be a bag of transactions, where eac h trans-action T k is an itemset suc h that T k I . A transaction T k con tains a set of items I I i I T k . The transac-I I is the set of iden ti ers of all transactions that con tain itemset I .

The frequency of an itemset I I is de ned to be the num ber of transactions that con tain the itemset, i.e. freq ( I ) = An itemset I is said to be frequen t if its supp ort is higher than a given threshold minsup ; this is written as supp ort ( I ) minsup (or, equiv alen tly, freq ( I ) minfr eq ).
In this work, we are interested in nding frequen t itemsets for databases that con tain examples lab eled with classes c 2 C . If we compute the frequency freq c ( I ) of an itemset I for eac h class c separately , we can asso ciate to eac h itemset the class lab el for whic h its frequency is highest. The resulting rule I ! c ( I ), where c ( I ) = argmax c 0 2 C freq c 0 class asso ciation rule .

A decision tree aims at classifying examples by sorting them down a tree. The leaves of a tree pro vide the classi -cations of examples [17]. Eac h node of a tree speci es a test on one attribute of an example, and eac h branc h of a node corresp onds to one of the possible values of the attribute. We assume that all tests are boolean; nominal attributes are transformed into boolean attributes by mapping eac h pos-sible value to a separate attribute. The input of a decision tree learner is then a binary matrix B , where B ij con tains the value of attribute i of example j .
 Our results are based on the follo wing observ ation.
Obser vation 1. Let us transform a binary table B into transactional form D such that T j = f i j B ij = 1 g[f: i j B 0 g . Then the examples that are sorte d down every node of a decision tree for B are char acterize d by an itemset of items occurring in D .
 For example, consider the decision tree in Figure 1. We can determine the leaf to whic h an example belongs by chec king whic h of the itemsets f B g , f: B; C g and f: B; : C g it con-tains. We denote the set of these itemsets with leaves ( T ). Similarly , the itemsets that corresp ond to paths in the tree are denoted with paths ( T ). In this case, paths ( T ) = f; ; f B g ; f: B g ; f: B; C g ; f: B; : C gg .

Con trary to what is common in the data mining literature, in this case it is essen tial that we include negative items , suc h as : B , in the itemsets.

The leaves of a decision tree corresp ond to class asso cia-tion rules, as leaves have asso ciated classes. In decision tree learning, it is common to specify a minim um num ber of ex-amples that should be covered by eac h leaf. For asso ciation rules, this would corresp ond to giving a supp ort threshold.
The accuracy of a decision tree is deriv ed from the num-ber of misclassi ed examples in the leaves: accuracy ( T ) = e ( T ) = X
A further illustration of the relation between itemsets and decision trees is given in Figure 2. In this gure, every node represen ts an itemset; an edge denotes a subset relation. Highligh ted is one possible decision tree, whic h is nothing else than a set of itemsets. The branc hes of the decision tree corresp ond to subset relations.

From the theory of frequen t itemset mining, it is kno wn that itemsets form a lattic e (these are typically depicted as in Figure 2). In this pap er we presen t DL8, an algorithm for mining D ecision trees from L attices.
The problems that we address in this pap er, can be seen as queries to a database. These queries consist of three parts. in this lattice The rst part speci es the constrain ts on the nodes of the decision trees. The set T 1 is called the set of locally constr aine d decision trees and DecisionT rees is the set of all possible decision trees. The predicate p ( I ) expresses a constrain t on paths. In our simplest setting, p ( I ) := ( freq ( I ) m inf req ). The predicate p ( I ) must ful ll these prop erties:
We can distinguish two types of local constrain ts: coverage-based constrain ts, suc h as frequency , of whic h the ful llmen t is entirely dep enden t on t ( I ), and pattern-based constrain ts, suc h as the size of an itemset, of whic h the ful llmen t de-pends on the prop erties of the (items in the) itemset it-self. In the follo wing, we consider only coverage-based con-strain ts; extensions to pattern-based constrain ts are possi-ble, but beyond the scop e of this pap er.

The second (optional) part expresses constrain ts that refer to the tree as a whole. Set T 2 is called the set of glob ally constr aine d decision trees . Form ula q ( T ) is a conjunction of constrain ts of the form f ( T ) , where f ( T ) can be
In the mandatory third step, we express a preference for a tree in the set T 2 . The tuples r ( T ) = [ r 1 ( T ) ; r 2 ( T ) ; : : : ; r n lexicographically and de ne a ranke d set of glob ally con-straine d decision trees ; r i 2f e; ex; size ; depth g . Our curren t algorithm requires that at least e and siz e or ex and siz e be used in the ranking; If depth (resp ectiv ely siz e ) is used in the ranking before e or ex , then q must con tain an atom depth ( T ) maxdepth (resp ectiv ely size ( T ) maxsize ).
We do not constrain the order of size ( T ), e ( T ) and depth ( T ) in r . We are minimizing the ranking function r ( T ), thus, our algorithm is an optimization algorithm. The trees that we searc h for are optimal in terms of the problem setting that is de ned in the query .

To illustrate our querying mec hanism we will now give sev eral examples.
 Quer y 1. Smal l Accurate Trees with Frequent leaves. In other words, we have p ( T ) := ( freq ( I ) minfr eq ), q ( T ) := true and r ( T ) := [ e ( T ) ; size ( T )]. This query investigates all decision trees in whic h eac h leaf covers at least minfr eq examples of the training data. Among these trees, we nd the smallest most accurate one. To retriev e accurate trees of bounde d size , Query 1 can be extended suc h that q ( T ) := size ( T ) maxsize .

One possible scenario in whic h DL8 can be used, is the follo wing. Assume that we have already applied a heuris-tic decision tree learner, suc h as C4.5 , and we have some idea about decision tree error ( maxerr or ) and size ( maxsize ). Then we can run the follo wing query: Quer y 2. Accurate Trees of Bounde d Size and Accuracy. This query nds the smallest tree that achiev es at least the same accuracy as the tree learned by C4.5 .

The previous queries aim at nding compact mo dels that maximize training set accuracy . Suc h trees migh t however over t training data. Another application of DL8 is to ob-tain trees with high expected accuracy . Sev eral algorithms for estimating test set accuracy have been presen ted in the literature. One suc h estimate is at the basis of the reduc ed error pruning algorithm of C4.5 . Essen tially , C4.5 com-putes an additional penalt y term x ( freq 1 ( I ) ; : : : freq eac h leaf I of the decision tree, from whic h we can deriv e a new estimated num ber of errors We can now also be interested in answ ering the follo wing query .
 Quer y 3. Smal l Accurate Prune d Trees.
 This query would nd the most accurate tree after pruning suc h as done by C4.5 . E ectiv ely, the penalt y terms ensure that trees with less leaves are sometimes preferable even if they are sligh tly less accurate.
To motiv ate our work, it is useful to brie y consider two examples that illustrate what kind of trees cannot be found if the well-kno wn information gain (ratio) heuristic of C4.5 is used to answ er Query 1 of Section 3.

As a rst example, consider the database in Figure 3, in whic h we have 2 target classes. The last column indicates how man y times an example is rep eated. Assume that we are interested in answ ering Query 1 with minfr eq = 10. An optimal tree exists (see Figure 1), but a heuristic learner will not nd it, as it prefers attribute A in the root: A has information gain 0 : 33 (resp. ratio 0 : 54), while B only has information gain 0 : 26 (resp. ratio 0 : 37).

As a second example, consider the database in Figure 4, whic h is a variation of the XOR problem. Then the correct answ er to Query 1 with minfr eq = 1 is given in Figure 5(a), but the use of information gain (ratio) would yield the tree in Figure 5(b), as the information gain (resp. ratio) of A is 0 : 098 (resp. 0 : 098), while the information gain of C is 0 : 029 (resp. 0 : 030).

We learn from these examples that the prop ortions of ex-amples can `fool' heuristic decision trees into an sub optimal shap e, as also observ ed in [22]. Optimal learners are less sensitiv e to suc h beha vior.

We will now presen t the DL8 algorithm for answ ering decision tree queries. Pseudo-co de of the algorithm is given in Algorithm 1.

The main idea behind DL8 is that the lattice of item-sets, as depicted in Figure 2, can be traversed bottom-up, and that we can determine the best decision tree(s) for the transactions t ( I ) covered by an itemset I by com bining for all i 2I , the optimal trees of its children I [f i g and I [f: i g in the lattice. The main prop erty that we exploit is that if a tree is optimal, then also the left-hand and righ t-hand branc h of its root must be optimal; this applies to every subtree of the decision tree.

More formally , the parameters of DL8 are the local con-strain t p , the ranking function r , and the global constrain ts; eac h global constrain t is passed in a separate parameter; global constrain ts that are not speci ed, are assumed to be set to 1 . The most imp ortan t part of DL8 is its re-cursiv e searc h pro cedure. Giv en an input itemset I , DL8-Recursive computes one or more decision trees for the transactions t ( I ) that con tain the itemset I . More than one decision tree is returned only if a depth or size constrain t is tion, and let k be the index of the obligatory error function allo wed value of depth d and size s , DL8-Recursive outputs the best tree T that can be constructed for the transactions t ( I ) according to the ranking [ r k ( T ) ; : : : ; r n size ( T ) s and depth ( T ) d .

In DL8-Recursive , we use sev eral functions: l ( c ), whic h returns a tree consisting of a single leaf with class lab el c ; n ( i; T 1 ; T 2 ), whic h returns a tree that con tains test i in the root, and has T 1 and T 2 as left-hand and righ t-hand branc hes; e t ( T ), whic h computes the error of tree T when only the transactions in TID-set t are considered; and -nally , we use a predicate pure ( I ) whic h blo cks the recursion if all examples t ( I ) belong to the same class.

The algorithm is most easily understo od if maxdepth = 1 , maxsize = 1 , maxerr or = 1 and r ( T ) = [ e ( T )]; in this case, DL8-Recursive com bines only two trees for eac h i 2I , and returns the single most accurate tree in line 34.

The correctness of the algorithm follo ws from the follo wing observ ations. (line 1-8) the valid ranges of sizes and depths are computed (line 11) for eac h depth and size satisfying the constrain ts (line 19) a candidate decision tree for classifying the exam-(line 20) if all examples in a set of transactions belong to the (line 23) in this line the anti-monotonic prop erty of the pred-(line 22{33) these lines mak e sure that eac h tree that should
A key feature of DL8-Recursive is that in line 34 it stores every results that it computes. Consequen tly, DL8 avoids that optimal decision trees for any itemset are computed more than once; furthermore, we do not need to store the entire decision trees with every itemset; it is sucien t to store the root and statistics (error, possibly size and depth); left-hand and righ t-hand subtrees can be reco vered from the stored results for the left-hand and righ t-hand itemsets if necessary .

Note that in our algorithm, we output the best tree ac-cording to the ranking. The k best trees can also straigh t-forw ardly be output.

To ecien tly index the itemsets I , a trie data structure can be used [7].

As with most data mining algorithms, the most time con-suming operations are those that access the data. DL8 re-quires frequency coun ts for itemsets in line 20, 23 and 32. In the follo wing, we will pro vide four related strategies to obtain the frequency coun ts that are necessary to chec k the constrain ts and compute accuracies: the simple single-step approac h, the frequen t itemset mining (FIM) approac h, the constrained FIM approac h, and the closure based single-step approac h.
 Algorithm 1 DL8( p , maxsize , maxdepth , maxerr or ; r ) 1: if maxsize 6 = 1 then 2: S f 1 ; 2 ; : : : ; maxsize g 3: else 4: S f1g 5: if maxdepth 6 = 1 then 6: D f 1 ; 2 ; : : : ; maxdepth g 7: else 8: D f1g 9: T DL8-Recursive ( ; ) 10: if maxerr or 6 = 1 then 11: T f T j T 2T ; e ( T ) maxerr or g 12: if T = ; then 13: return unde ned 14: return argmin T 2T r ( T ) 15: 16: pro cedure DL8-Recursive( I ) 17: if DL8-Recursive ( I ) was computed before then 18: return stored result 19: C f l ( c ( I )) g 20: if pur e ( I ) then 21: store C as the result for I and return C 22: for all i 2I do 23: if p ( I [f i g ) = true and p ( I [f: i g ) = true then 24: T 1 DL8-Recursive ( I [f i g ) 25: T 2 DL8-Recursive ( I [f: i g ) 26: for all T 1 2T 1 ; T 2 2T 2 do 27: C C[f n ( i; T 1 ; T 2 ) g 28: end if 29: T ; 30: for all d 2 D; s 2 S do 31: L f T 2Cj depth ( T ) d ^ size ( T ) s g 32: T T [f argmin T 2L [ r k = e t ( I ) ( T ) ; : : : ; r 33: end for 34: store T as the result for I and return T 35: end pro cedure The most straigh tforw ard approac h, referred to as DL8-Simple , computes the itemset frequencies while DL8 is ex-ecuting. In this case, once DL8-Recursive is called for an itemset I , we obtain the frequencies of I in a scan over the data, and store the result to avoid later recomputations. An alternativ e approac h is based on the observ ation that every itemset that occurs in a tree, must satisfy the local constrain t p . If p is a minim um frequency constrain t, we can use a frequen t itemset miner to obtain the frequencies of itemsets in a prepro cessing step. DL8 then operates on the resulting set of itemsets, annotating every itemset with optimal decision trees.

Man y frequen t itemset miners have been studied in the literature; all of these can be used with small mo di cations to output the frequen t itemsets in a con venien t form and determine frequencies in multiple classes [1, 32, 11, 29].
We implemen ted an extension of Apriori that rst com-putes and stores in a trie all frequen t itemsets, and then runs DL8 on the trie. This approac h is referred to as Apriori-Freq+DL8 . Compared to other itemset miners, we exp ect that the additional run time to store all itemsets in Apriori is the lowest, as Apriori already builds a trie of candidate itemsets itself.

If we assume that the output of the frequen t itemset miner consists of a graph structure suc h as Figure 2, then DL8 operates in time linear in the num ber of edges of this graph. Unfortunately , the frequen t itemset mining approac h may compute frequencies of itemsets that can nev er be part of a decision tree. For instance, assume that f A g is a frequen t itemset, but f: A g is not; then no tree answ ering example Query 1 will con tain a test for attribute A ; itemset f A g is redundan t. In this section, we sho w that an additional local, anti-monotonic constrain t can be used in the frequen t itemset mining pro cess to mak e sure that no suc h redundan t itemsets are enumerated. Pro ofs of the theorems given in this section can be found in [21].

If we consider the DL8-Simple algorithm, an itemset I = f i ; : : : ; i n g is stored only if there is an order [ i k of the items in I (whic h corresp onds to an order of recursiv e calls to DL8-Recursive ) suc h that for none of the prop er pre xes I 0 = [ i k 1 ; i k 2 ; : : : ; i k It is helpful to negate the pure predicate, as one can easily see that : pure is an anti-monotonic predicate (ev ery sup er-set of a pure itemset, must also be pure). From now on, we will refer to : pure as a leaf constr aint , as it de nes a prop erty that is only allo wed to hold in the leaves of a tree. We can now formalize the principle of itemset relevancy .
Definition 1. Let p 1 be a local anti-monotonic tree con-straint and p 2 be an anti-monotonic leaf constr aint. Then the relev ancy of I , denote d by rel ( I ) , is de ne d by rel ( I ) = Theorem 1. Let L 1 be the set of itemsets stor ed by DL8-Simple , and let L 2 be the set of itemsets f I Ij rel ( I ) = true g . Then L 1 = L 2 .
 Relev ancy is a prop erty that can be pushed in a frequen t itemset mining pro cess.

Theorem 2. Itemset relevancy is an anti-monotonic prop-erty.

It is relativ ely easy to integrate the computation of rele-vancy in frequen t itemset mining algorithms, as long as the order of itemset generation is suc h that all subsets of an itemset I are enumerated before I is enumerated itself. As-sume that we have already computed all relev ant itemsets that are a subset of an itemset I . Then we can determine for eac h i 2 I if the itemset I i is part of this set, and if so, we can deriv e the class frequencies of I i [: i using the i either I i is not relev ant, or the predicate p ( I i [: i ) fails, we can prune I .

Pruning of this kind can be integrated in both depth-rst and breadth-rst frequen t itemset miners. In case depth is the rst ranking function, level-wise algorithms suc h as Apriori have an imp ortan t bene t: after eac h level of item-sets is generated, we could run DL8 to obtain the most ac-curate tree up to that depth. Apriori can stop at the lowest level at whic h a tree is found that ful ls the constrain ts. We implemen ted two versions of DL8 in whic h the relev ancy constrain ts are pushed in the frequen t itemset mining pro-cess: DL8 -Apriori , whic h is based on Apriori [1], and DL8 -Ecla t , whic h is based on Ecla t [32].
 In the simple single-step approac h, we stored the optimal decision trees for every itemset separately . However, if the local constrain t is only coverage based, it is easy to see that for two itemsets I 1 and I 2 , if t ( I 1 ) = t ( I 2 ), the result of DL8-Recursive ( I 1 ) and DL8-Recursive ( I 2 ) must be the same. To reduce the num ber of results that we have to store, we should avoid storing suc h duplicate sets of results.
The solution that we prop ose is to compute for every item-set its closur e . Let i ( t ) be the function whic h computes for a TID-set t , then the closur e of itemset I is the itemset i ( t ( I )). An itemset I is c losed i I = i ( t ( I )). If t ( I t ( I 2 ) it is easy to see that also i ( t ( I 1 )) = i ( t ( I the trie data structure that is used in the simple single-step approac h, we could index the results on i ( t ( I )) instead of I itself.
 We incorp orate this observ ation as follo ws in Algorithm 1. Before executing line 17, we mak e a pass over the data to determine the closure I 0 of the itemset I , and to collect the frequencies of all itemsets f I [f i g ; I [f: i gj i 2Ig . In line 17 we chec k if DL8-Recursive ( I ) was already computed ear-lier by searc hing for I 0 in a trie data structure. In line 34, we asso ciate the result to I 0 instead of I itself.
Our single-step approac h whic h relies on closed itemset indexing is called DL8-Closed . Obviously , DL8-Closed will nev er consider more itemsets than DL8-Simple , DL8-Apriori or DL8-Ecla t ; itemsets stored by DL8-Closed may however be longer as they con tain all items in their closure.

Our implemen tation of DL8-Closed is based on opti-mization strategies that are common in depth-rst frequen t itemset miners, suc h as the use of pro jected databases, with mo di cations that mak e sure that the space complexit y of our algorithm is ( n + m ), where n is the size of the trie that stores all closed itemsets, and m is the size of the bi-nary matrix that con tains the data. For more details see [21].
In this section we compare the di eren t versions of DL8 in terms of eciency; furthermore, we compare the qualit y of the constructed trees with those found by J48 , the Java implemen tation of C4.5 [26] in Weka [30]. All exp erimen ts were performed on Intel Pentium 4 mac hines with in be-tween 1GB and 2GB of main memory , running Lin ux. DL8 and the frequen t itemset miners were implemen ted in C++. The exp erimen ts were performed on UCI datasets [20]. Numerical data were discretized before applying the learn-ing algorithms using Weka 's unsup ervised discretization
Datasets #Ex #T est Datasets #Ex #T est a-credit 653 56 segmen t 2310 55 balance 625 13 soybean 630 45 diab etes 768 25 vehicle 846 55 g-credit 1000 77 vote 435 49 ionosphere 351 99 yeast 1484 23 mushro om 8124 116 zoo 101 15 pendigits 7494 49 Figure 8: Prop erties of the algorithms used in the exp erimen ts metho d with a num ber of bins equal to 4. We limited the num ber of bins in order to limit the num ber of created at-tributes. Figure 6 gives a brief description of the datasets that we used in terms of the num ber of examples and the num ber of attributes after binarization.
The applicabilit y of DL8 is limited by two factors: the amoun t of itemsets that need to be stored, and the time that it tak es to compute these itemsets. We rst evalu-ate exp erimen tally how these factors are in uenced by con-strain ts and prop erties of the data. Furthermore, we deter-mine how the di eren t approac hes for computing the item-set lattices compare. A summary of the algorithms can be found in Figure 8. Besides DL8-Apriori , DL8-Ecla t and DL8-Closed , we also include unmo di ed implemen ta-tions of the frequen t itemset miners Apriori [1], Ecla t [32] and LCM [29] in the comparison. These implemen tations were obtained from the FIMI website [3]. The inclusion of unmo di ed algorithms allo ws us to determine how well relev ancy pruning works, and allo ws us to determine the trade-o between relev ancy pruning and trie construction. Furthermore, we also perform frequen t and closed itemset mining exp erimen ts on data with only the positiv e items, to estimate the added complexit y of using negativ e items.
Results for four datasets are listed in Figure 7. We aborted runs of algorithms that lasted for longer than 1500s. More results can be found in [21]. We only sho w datasets here in whic h frequen t itemset miners manage to run within 1500s.
The results clearly sho w that in all cases the num ber of closed relev ant itemsets is the smallest. The di erence be-tween the num ber of relev ant itemsets and the num ber of frequen t itemsets becomes smaller for lower minim um fre-quency values. The num ber of frequen t itemsets is so large in most cases, that it is imp ossible to compute or store them within a reasonable amoun t of time or space. In those datasets where we can use low minim um frequencies (15 or smaller), the closed itemset miner LCM is usually the fastest; for low frequency values the num ber of closed item-sets is almost the same as the num ber of relev ant closed itemsets. Bear in mind, however, that LCM does not out-put itemsets in a form that can be used ecien tly by DL8 . In all cases, DL8-Closed is faster than DL8-Apriori or DL8-Ecla t . In those cases where Apriori-Freq+DL8 can store the entire output of Apriori in memory , we see that the additional run time for storing these results is signi can t. On the other hand, if we perform relev ancy pruning, the resulting algorithm is usually faster than the original miner.
In the datasets sho wn here, the num ber of attributes is relativ ely small. For the datasets with larger num ber of attributes, suc h as ionosphere and splice, we found that only DL8-Closed managed to run for supp ort thresholds lower than 25%, but still was unable to run for supp ort thresholds lower than 10%.
Figure 9 pro vides the results of exp erimen ts in whic h we used strati ed 10-fold cross-v alidation to compute the train-ing and test accuracies of DL8-Closed and J48 . For eac h dataset, we lowered this frequency to the lowest value that still allo wed the computation to be performed within the memory of our computers. For J48 , results are pro vided for pruned trees and unpruned trees; for DL8 results are pro vided in whic h the e (unpruned) and ex (pruned) er-ror functions are optimized (cf. Queries 1 and 3 of Section 3). We used a corrected two-tailed t-test [19] with a sig-ni cance threshold of 5% to compare the test accuracies of both systems. A test set accuracy result is in bold when it is signi can tly better than its coun terpart result on the other system.

First, both algorithms were applied with the same mini-mum frequency constrain t. The exp erimen ts sho w that both with and without pruning the optimal trees computed by DL8 have a better training accuracy than the trees com-puted by J48 with the same frequency values. Furthermore, on the test data, in both cases DL8 is signi can tly better than J48 on 9 of the 20 datasets and only signi can tly worse on one dataset. When pruned trees are compared to un-pruned ones, the sizes of the trees are on average 1.75 times smaller for J48 and 1.5 time smaller for DL8 . After prun-ing, DL8 's trees are still 1.5 times larger than J48 's ones. A closer insp ection of these trees rev eals a similar phenomenon as we discussed for the data in Figure 3: C4.5 's trees are smaller as it creates trees with small num bers of incorrectly classi ed examples in the leaves, whic h cannot be split o without violating the constrain ts. In cases where DL8 's ac-curacy is signi can tly better, the pruned trees of DL8 are 3 to 9 nodes larger than those of J48 . These results con rm earlier ndings whic h sho w that smaller trees are not alw ays desirable. If we compare the multiple minim um frequency constrain ts, it turns out that the best test accuracy results are not alw ays obtained for the lowest minim um frequencies.
Second, in the last six columns of Figure 9, we give re-sults for J48 with its default minfr eq = 2 setting, both when using the discretized data, and when using the original, non-discretized data. The test accuracies of J48 with minfr eq = 2 are compared with the test accuracies of DL8 for the various minfr eq values, when using pruning. The results of the sig-ni cance test are given in the \S" column: \+" means that J48 is signi can tly better, \-" that it is signi can tly worse and \0" that there is no signi can t di erence. This compar-ison sho ws that the constrain ts have a negativ e impact on the accuracy for 7 of the 20 datasets. Furthermore, J48 is better on 3 additional datasets if discretization is not per-formed beforehand, rev ealing that a good discretization is sometimes bene cial. The results of DL8 under constrain ts indicate however that the impacts of these constrain ts on ac-curacies are not as large as one would susp ect if one would only consider the results of J48 using constrain ts.
One of the strengths of DL8 is that it allo ws us to explic-itly restrict the size or accuracy . We therefore studied the relation between decision tree accuracies and sizes in more detail. In Figure 10, we sho w results in whic h the average size of trees constructed by J48 , is tak en as a constrain t on the size of trees mined by DL8. None of the results given by DL8 are signi can tly better nor signi can tly worse than those given by J48 .

DL8 can also compute, for every possible size of a decision tree, the smallest error on training data that can possibly be achiev ed. For two datasets, the results of suc h a query are given in Figure 11. In general, if we increase the size of Figure 10: In uence of the size constrain t on the test accuracy of DL8 (unpruned) a decision tree, its accuracy impro ves quic kly at rst. Only small impro vemen ts can be obtained by further increasing the size of the tree. Figures suc h as Figure 11 are of practical interest, as they allo w a user to trade-o the interpretabilit y and the accuracy of a mo del.

The most surprising conclusion that may however be dra wn from all our exp erimen ts, is that optimal trees perform re-mark ably well on most datasets. Our algorithm investigates a vast searc h space, and could consequen tly be very sensitiv e to over tting. Still, its results are comp etitiv e in all cases. The exp erimen ts indicate that the constrain ts that we em-ployed, either on size, or on minim um supp ort, are sucien t to reduce mo del complexities and achiev e good predictiv e accuracies.
The searc h for optimal decision trees dates bac k to the 70s, when sev eral dynamic programming algorithms for building suc h trees were prop osed [9, 16, 24, 28, 15]. This early work concen trated on nding small summarizations of input data, and did not study the prediction of unseen examples. Optimization criteria were based on the cost of attributes, and the size or the depth of a tree. Afterw ards, atten tion mostly shifted to heuristic decision tree learners, whic h were found to obtain satisfactory results for man y datasets in a fraction of the run time; theoretical results were obtained that sho w to what exten t heuristic decision trees can be considered optimal [14, 6]. Still, the idea of exhaustiv ely nding optimal decision trees under certain constrain ts was also studied [2, 18], but only for much smaller datasets and smaller types of trees than studied in this pap er. Recen tly [4] presen ted a dynamic programming algorithm that is quite similar to DL8 and its early ancestors. A new optimization criterion was introduced for nding optimal dyadic decision trees, whic h use a xed mec hanism for discretization of data. This algorithm was only applied on small datasets, and did not investigate the link with data mining algorithms.
More recen tly, pruning strategies of decision trees have been studied [10]. DL8 can be conceiv ed as the generaliza-tion of these pruning strategies to another data structure.
Algorithmically , the relevancy constrain t presen ted in Sec-tion 5 with p 2 = true is closely related to the condensed rep-resen tation of -free itemsets [5]. For = minsup j D j and p ( I ) := ( freq ( I ) minfr eq ), it can be sho wn that if an item-set is -free, it is also relevant . DL8-Closed emplo ys ideas that have also been exploited in the formal concept analysis (FCA) comm unit y and in closed itemset miners [23].
A popular topic in data mining is curren tly the selection of itemsets from a large set of itemsets found by a frequen t itemset mining algorithm (see for instance, [31]). DL8 can be seen as one suc h algorithm for selecting itemsets. It is however the rst algorithm that outputs a well-kno wn type of mo del, and pro vides accuracy guaran tees for this mo del.
We presen ted DL8 , an algorithm for nding decision trees that maximize an optimization criterion under constrain ts, and successfully applied this algorithm on a large num ber of datasets.

We sho wed that there is a clear link between DL8 and frequen t itemset miners, whic h means that it is possible to apply man y of the optimizations that have been prop osed for itemset miners also when mining decision trees under constrain ts. The investigation that we presen ted here is only a starting point in this direction; it is an open ques-tion how fast decision tree miners could become if they were thoroughly integrated with algorithms suc h as LCM or FP-Gro wth. Our investigations sho wed that high run times are however not as much a problem as the amoun t of memory required for storing huge amoun ts of itemsets. A challenging question for future researc h is what kind of condensed repre-sen tations could be dev elop ed to represen t the information that is used by DL8 more compactly .

In exp erimen ts we compared the test set accuracies of trees mined by DL8 and C4.5 . Under the same frequency thresholds, we found that the trees learned by DL8 are of-ten signi can tly more accurate than trees learned by C4.5 . When we compare the best settings of both algorithms, J48 performs signi can tly better for 45% of the datasets. E-ciency considerations prev ented us from applying DL8 on the thresholds where C4.5 performs best, but preliminary results indicate that the best accuracies are not alw ays ob-tained for the lowest possible frequency thresholds.
Still, our conclusion that trees mined under declarativ e constrain ts perform well both on training and test data, means that constrain t-based tree miners deserv e further study . Man y open questions regarding the instabilit y of decision trees, the in uence of size constrain ts, heuristics, pruning strategies, and so on, may be answ ered by further studies of the results of DL8 . Future challenges include extensions of DL8 to other types of data, constrain ts and optimization criteria. DL8 's results could be compared to man y other types of decision tree learners [22, 25].

Giv en that DL8 can be seen as a relativ ely cheap type of post-pro cessing on a set of itemsets, DL8 suits itself per-fectly for interactiv e data mining on stored sets of patterns. This means that DL8 migh t be a key comp onen t of induc-tive databases [13] that con tain both patterns and data. Siegfried Nijssen was supp orted by the EU FET IST pro ject \Inductiv e Querying", con tract num ber FP6-516169. Elisa Fromon t was supp orted through the GO A pro ject 2003/8, \Inductiv e Kno wledge bases", and the FW O pro ject \Foun-dations for inductiv e databases". The authors thank Luc De Raedt and Hendrik Blo ckeel for man y interesting discus-sions; Ferenc Bodon and Bart Goethals for putting online their implemen tations of resp ectiv ely Apriori and Ecla t , whic h we used to implemen t DL8, and Takeaki Uno for pro-viding LCM. We also wish to thank Daan Fierens for pre-pro cessing the data that we used in our exp erimen ts. [1] R. Agra wal, H. Mannila, R. Srik ant, H. Toivonen, and [2] P. Auer, R. C. Holte, and W. Maass. Theory and [3] R. J. Bayardo, B. Goethals, and M. J. Zaki, editors. [4] G. Blanc hard, C. Sch X afer, Y. Rozenholc, and K. R. [5] J.-F. Boulicaut, A. Byk owski, and C. Rigotti. [6] T. G. Dietteric h, M. J. Kearns, and Y. Mansour. [7] E. Fredkin. Trie memory . Communic ations of the [8] E. Fromon t, H. Blo ckeel, and J. Struyf. Integrating [9] M. R. Garey . Optimal binary iden ti cation [10] M. N. Garofalakis, D. Hyun, R. Rastogi, and K. Shim. [11] J. Han, J. Pei, and Y. Yin. Mining frequen t patterns [12] L. Hya l and R. L. Riv est. Constructing optimal [13] T. Imielinski and H. Mannila. A database persp ectiv e [14] M. J. Kearns and Y. Mansour. On the boosting abilit y [15] A. Lew. Optimal con version of extended-en try [16] W. S. Meisel and D. Mic halop oulos. A partitioning [17] T. Mitc hell. Machine Learning . McGra w-Hill, New [18] P. M. Murph y and M. J. Pazzani. Exploring the [19] C. Nadeau and Y. Bengio. Inference for the [20] D. Newman, S. Hettic h, C. Blak e, and C. Merz. UCI [21] S. Nijssen and E. Fromon t. Mining optimal decision [22] D. Page and S. Ray. Skewing: An ecien t alternativ e [23] N. Pasquier, Y. Bastide, R. Taouil, and L. Lakhal. [24] H. J. Payne and W. S. Meisel. An algorithm for [25] F. Pro vost and P. Domingos. Tree induction for [26] J. R. Quinlan. C4.5: Programs for Machine Learning . [27] J. R. Quinlan and R. M. Cameron-Jones.
 [28] H. Schumac her and K. C. Sev cik. The syn thetic [29] T. Uno, M. Kiy omi, and H. Arim ura. LCM ver. 2: [30] I. H. Witten and E. Frank. Data Mining: Practic al [31] X. Yan, H. Cheng, J. Han, and D. Xin. Summarizing [32] M. J. Zaki, S. Parthasarath y, M. Ogihara, and W. Li.
