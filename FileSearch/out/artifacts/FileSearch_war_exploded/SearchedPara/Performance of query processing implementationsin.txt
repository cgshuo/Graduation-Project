 1. Introduction
In the last decade, a shift has been observed from the Boolean model of query processing to the more effective ranking-based model. In text retrieval systems employing the ranking-based model, similarity cal-culations are performed between a user query and the documents in a collection. As a result of these cal-culations, the user is presented a set of relevant documents, ranked in decreasing order of relevance to the query. The similarity calculations and document ranking, which form the major source of overhead in query processing, can be implemented in many ways, using different data structures and algorithms. The main focus of this work is on advantages and disadvantages of these data structures and algorithms. list. Each inverted list I i keeps entries, called postings, about the documents in which term t posting p 2I i includes a document id field p . d = j and a weight field p . w = w ( t and d j .

In construction of the inverted index, usually, the tf-idf (term frequency-inverse document frequency) weighting scheme ( Salton &amp; McGill, 1983 ) is used to compute w ( t tf-idf variant where f ( t i , d j ) is the number of times term t i appears in document d f ( t i ) is the number of documents containing t i , and D is the number of documents.

In processing a query, only the inverted lists associated with the query terms are used. Specifically, if we have a query Q X f t q of Q inverted lists, in which each list I q
Since, in Eq. (1) , we already approximated cosine normalization by the mons, 1997 ), the cosine similarity metric can be simplified as assuming that all query terms have equal importance. That is, to calculate the similarity between query Q and document d j , we need to accumulate the weights w  X  t tion dedicated to document d j . These memory locations are called accumulators. An accumulator a typi-cally keeps an integer document id field a . d and a floating point score field a . s , which contains the accumulated similarity value for document a . d . After all accumulator updates are completed, sorting them in decreasing order of finalized a . s values gives a ranking of documents.

Both time and space are critical in ranking-based text retrieval. Especially, in cases where the inverted index is completely stored in volatile memory (a common practice for Web search engines) and disk acces-ses are avoided, similarity calculations and document ranking directly determine the query processing times. Considering the existence of search engines which indexed more than four billion pages, it is easily under four different categories for query processing in ranking-based text retrieval, taking time and space needs into consideration. To our knowledge, six of these implementations are not discussed in any publi-cation before.

The rest of the paper is organized as follows. In Section 2 , we give pointers to the related work on effi-cient query processing. In Section 3 , we describe the implementation techniques and present an analysis of their asymptotic time and space complexities. In Section 4 , we evaluate the practical performance of each technique on a large (30 GB) document collection. In Section 5 , we present a discussion on advantages and disadvantages of the techniques and conclude. 2. Related work 1995; Wong &amp; Lee, 1993 ). These optimizations are based on limiting the number of processed query terms and postings (short-circuit evaluation) or limiting the memory allocated to accumulators. They mainly dif-fer in their choice for the processing order of postings and when to stop processing them.
Buckley and Lewit (1985) proposed an algorithm which traverses query terms in decreasing order of fre-quencies and limits the number of processed query terms by not evaluating the inverted lists for high-fre-quency terms whose postings cannot affect the final ranking. Harman and Candela (1990) used an insertion threshold on query terms, and the terms whose score contribution are below this threshold are not allowed to allocate new accumulators. Moffat et al. (1994) proposed two heuristics which place a hard limit on the memory allocated to accumulators. Turtle and Flood (1995) presented simulation results for the perfor-mance analysis of two optimization techniques, which employ term-ordered and document-ordered in-verted list traversal. Wong and Lee (1993) proposed two optimization heuristics which traverse postings in decreasing magnitude of weights. For a similar strategy, Persin (1994) used thresholds for allocation and update of accumulators.
 guarantee that best-matching documents are ranked correctly. Approximate optimizations may trade effec-tiveness for efficiency producing a partial ranking, which does not necessarily contain the best-matching documents, or may present them in an incorrect order. Our focus in this work is not on partial query eval-uation or approximate optimizations. We investigate the complexities of implementations and data struc-tures in total document ranking as well as their performance in practice.

Throughout the paper, we take an information retrieval point of view in analyzing various implementa-tion techniques. However, there exists a significant amount of related work in the database literature. The interested reader may refer to prior works by Lehman and Carey (1986), Goldman, Shivakumar, Venkat-asubramanian, and Garcia-Molina (1998), Bohannon, Mcllroy, and Rastogi (2001), Hristidis, Gravano, and Papakonstantinou (2003), Elmasri and Navathe (2003) and Ilyas et al. (2004) . 3. Query processing implementations
The analyses presented in this work are based on processing of a single query Q X f t
Q distinct terms over a document collection with D documents. u denotes the total number of postings in the processed Q inverted lists I q tinct document ids in these postings is denoted by e . The text retrieval system returns the most relevant (highly ranked) s documents to the user as the result of the query. Table 1 displays the notation used in the paper.

Although other orderings are possible, the postings in our inverted lists are ordered by increasing doc-ument id since this ordering is strictly required by some of the algorithms we implemented. Moreover, this 1993; Zobel &amp; Moffat, 1995 ). In postings, we store normalized tf scores  X  f  X  t is not pre-computed in postings but computed during query processing, allowing easy updates over the in-verted index.

In a query processing implementation, depending on the operations on accumulators, we distinguish five Descriptions of these phases are given below.

Creation : Each document d i is associated with an accumulator a
Depending on the implementation, either previously allocated locations are used as accumulators or space is dynamically allocated for accumulators as needed. In this phase, some auxiliary data structures may also be allocated and initialized.

Update : Once an accumulator a i is created for a document d p . d = i is simply added to the score of accumulator a i to perform u updates since each posting incurs a single update.

Extraction : The accumulators with nonzero scores (i.e., a extracted. Such accumulators are located and passed to the selection phase as input. Since an accumu-lator is extracted exactly once, there are always e extraction operations.

Selection : This phase compares each extracted accumulator score with the previously extracted ones and selects the accumulators having the top s scores. This way, the set S constructed.

Sorting : The accumulators in S top are sorted in decreasing order of their scores, and their document ids are returned to the user in this sorted order.

The asymptotic run-time costs for the creation, update, extraction, selection and sorting phases are rep-resented by Time C , Time U , Time E , Time S , and Time R and u 6 QD . Moreover, we assume s D , Q T , and u = O ( D ).

Depending on the processing order of postings, we make a broad classification of query processing implementations as term-ordered (TO) and document-ordered (DO).We further classify TO processing as static (TO-s) and dynamic (TO-d), according to the strategy used in allocation of accumulators. Simi-larly, we classify DO processing as multiple (DO-m) and single (DO-s), according to the number of accu-mulators allocated. For TO-s, TO-d, DO-m, and DO-s approaches, we present 4, 3, 2, and 2 implementations, respectively ( Fig. 1 ). To the best of our knowledge, the implementations TO-s4, TO-d1, TO-d2, TO-d3, DO-m1, and DO-m2 are not discussed in any other publication. 3.1. Implementations for term-ordered (TO) processing
In TO processing, inverted lists are sequentially processed. The postings of a term are completely ex-hausted before the postings of the next term are processed. Extraction and selection phases are performed in an interleaved manner. In TO-s, D accumulators are allocated statically. In TO-d, at most e accumula-tors are allocated on demand, thus saving space if D is very high. 3.1.1. Implementations with static accumulator allocation (TO-s)
In TO-s implementations, an array A of D accumulators is statically allocated. Each array element a  X  A  X  i is used as an accumulator. Before processing a query, accumulator fields are initialized as a and a i . s = 0. Similarity updates for document d i are performed over a same for all TO-s implementations. These implementations mainly differ in extraction, selection, and sort-ferent TO-s implementations. 3.1.1.1. TO-s1: accumulator array, accumulators with nonzero scores sorted. The most naive implementation accumulators. If e D , most accumulators are never updated and their score fields remain zero. In this case, it is better to first pick the nonzero accumulators and then sort those ( Witten et al., 1999 ). Costs for this approach are as follows:
Creation : Array A of D accumulators is allocated, and its accumulators are initialized. This type of allo-cation is a one-time O ( D )-cost operation independent of the number of incoming queries. However, reinitialization of the accumulators between consecutive queries require O ( e ) operations. Hence, Time C = O ( e ).

Update : Each term q j is considered in turn, and for each posting p 2I formed over the corresponding accumulator field a i . s , i.e., a and writing a total of u values between two locations. Hence, Time
Extraction : Since it is not known which accumulators have nonzero score fields, the whole A array must be traversed to locate them. During this traversal, nonzero accumulators are picked and stored at the first e elements of array A . Traversing the whole array and checking the score fields require O ( D ) com-parisons. Hence, Time E = O ( D ).
 array elements. Time S = O (1).

Sorting : Sorting the first e array elements in decreasing order of the scores gives a ranking. The document ids in the first s array elements are returned as the set S of Time R = O ( e lg e ).

The running time of this implementation is Time T = O ( e + u + D +1+ e lg e )= O ( D + e lg e ). The stor-age overhead is S = O ( D ). 3.1.1.2. TO-s2: accumulator array, max-priority queue for nonzero accumulators. An improvement over
TO-s1 is to use a max-priority queue implemented as a binary heap H scores ( Moffat et al., 1994 ). The max-heap H max contains e accumulators, keyed by their scores. This approach avoids the cost of sorting the whole set of nonzero accumulators if s &lt; e . Creation , Update : Similar to TO-s1. Time C = O ( e ), Time order to store the accumulators in H max . Hence, no extra storage is necessary for implementing the max-priority queue.
 Extraction : Similar to TO-s1. Time E = O ( D ).
 Selection : Extracted accumulators in the first e elements of array A are treated as elements of heap
H max , using their score fields as the key and document id fields as the data. Since there are e extracted accumulators, the heap can be built with O ( e ) operations. After building, the root of H accumulator with the highest score. The top s accumulators are obtained by repeatedly performing s extract-max operation on H max . Time S = O ( e + s lg e ).

Sorting : This phase involves no work since accumulators are extracted from H the selection phase. Time R = O (1).
 3.1.1.3. TO-s3: accumulator array, min-priority queue for top s accumulators. A variation over TO-s2 is to employ, instead of a max-priority queue, a min-priority queue implemented as a min-heap H et al., 1999 ). At any time, the min-heap H min contains at most s accumulators, keyed by their scores. Creation , Update : Similar to TO-s1. Time C = O ( e ), Time Extraction : The A array is traversed, and nonzero accumulators are passed to the selection phase. Time E = O ( D ).

Selection : As long as the number of accumulators in H min ply added to H min . Once it contains s accumulators, H min keeps a smin , the accumulator with the minimum score observed so far. The score a . s of each extracted accumulator a is compared with a smin . s . If the incoming score a . s is less than the current minimum a smin . s , the accumulator a is simply ignored. Otherwise, accumulator a the extracted accumulator a is inserted into H min . Building the min-heap from the first s extracted accu-mulators has a cost of O ( s ). In the worst case, all remaining accumulators must be inserted into H This has a cost of O (( e s )lg s ). Hence, Time S = O ( s +( e s )lg s ).
 Sorting : Accumulators in H min are sorted in decreasing order of scores. Time accumulator with the smallest score to be entered into the set S linear time.
 Creation , Update : Similar to TO-s1. Time C = O ( e ), Time Extraction : This phase involves no work. Time E = O (1).

Selection : The accumulator with the s th largest score can be selected in worst-case linear time by the median-of-medians selection algorithm ( Cormen, Leiserson, Rivest, &amp; Stein, 2001 ) over the accumula-tors in A . Instead of this algorithm, the randomized selection algorithm ( Cormen et al., 2001 ), which has expected linear-time complexity, could be used for run-time efficiency in practice. This algorithm returns a s th , the accumulator having the s th largest score and places the remaining s 1 accumulators that should appear in S top in the array elements following a tions. Time S = O ( D ).
 Sorting : Accumulators in S top are sorted in decreasing order of scores. Time 3.1.2. Implementations with dynamic accumulator allocation (TO-d)
If e D , array A contains too many unused accumulators and hence wastes lots of space. In such a case or the case where array A is too large to fit into the volatile memory, it may be a good idea to use a dy-namic data structure D and allow on-demand space allocation for accumulators. In this approach, accumu-lators are stored in nodes of D and are located using their document ids as keys. In this section, AVL tree for this purpose. In what follows, we discuss these three alternatives, starting with the AVL tree. Our time analyses for the hashing and skip list alternatives are expected-time analyses. The algorithm for TO-d implementations is given in Fig. 3 . 3.1.2.1. TO-d1: AVL tree of accumulators, min-priority queue for top s accumulators. In this implementation, an AVL tree T containing at most e nodes is used to store the accumulators. Each node of T keeps an accu-of document ids. In the case of a binary search tree implementation, with such a posting storage scheme, new ture, which dynamically balances the height of the tree, making accumulator search less costly.
Creation : If an accumulator needs to be updated in T and it is not already there, a tree node is dynam-ically allocated to store the accumulator. The cost of node allocation is constant, i.e., O (1). Hence, Time C = O ( e ).

Update : For each posting p , nodes of T are searched to locate the accumulator to be updated, where update cost for an accumulator is proportional with the height of the AVL tree. Hence, Time U = O ( u lg e ).

Extraction : When all updates are completed, accumulators can be extracted from nodes of T in any order. Each extracted accumulator is passed to the selection phase. Traversing the AVL tree has a cost of Time E = O ( e ).
 Selection : The min-priority queue mechanism of TO-s3 is used. Time Sorting : Similar to TO-s3. Time R = O ( s lg s ).

AVL tree and O ( s ) for the min-priority queue. S = O ( e ). 3.1.2.2. TO-d2: hashing of accumulators, min-priority queue for top s accumulators. Another implementation processed, hashing techniques that require static allocation (such as open addressing) cannot be used. Here, we use hashing with chaining ( Horowitz &amp; Sahni, 1978 ). In this implementation, accumulators are placed into B buckets, where each bucket keeps a linked list of accumulators. The bucket b for an accumulator a is determined by applying a hash function on the document id field (e.g., b = a . d mod B ).
Creation : Selecting the appropriate number B of buckets is the most important step in this implementa-tion. Allocating too many buckets may increase space consumption. On the contrary, if too few buckets are allocated, the number of accumulators per bucket increases. Since accumulators are sequentially searched in each bucket, this increases the query processing time. In this implementation, B pointers are needed to keep the list heads. Each list node stores an accumulator and has a pointer to the next node in the linked list. It is necessary to dynamically allocate a total of e list nodes. Hence, Time C = O ( B + e ).

Update : For a posting p , the bucket to be searched is determined by hashing p . d to a bucket. The accu-mulators in a bucket are searched by following the links between list nodes. If an accumulator with document id is found, the search ends. In this case, a new node which contains an accumulator is allo-document ids. Each bucket stores e / B list nodes on the average. Hence, these many comparisons are nec-essary to locate an accumulator. Time U = O ( ue / B ).

Extraction : Accumulators are extracted from the buckets and passed to the selection phase. Since exactly e nodes must be extracted, Time E = O ( e ).
 Selection , Sorting : Similar to TO-s3. Time S = O ( s +( e s )lg s ), Time
O ( B + e ) for the hash table and O ( s ) for the min-priority queue. S = O ( B + e ). to use a skip list S to store and search the accumulators. Skip lists balance themselves probabilisti-cally rather than explicitly (e.g., rotations in AVL trees). Although they have bad worst-case time complexities, they have good expected-time complexities for insert and find operations and perform well in practice.

Creation : A list node is dynamically allocated in S to store an accumulator and a set of forward pointers to the following list nodes. The number of forward pointers in each node is determined randomly, but it is limited from above. Since e list nodes must be allocated, Time
Update : For each posting p , the nodes in S are searched to locate the accumulator to be updated, where binary search. If the accumulator is located in S , its score field is updated as a . s = a . s + p . w .
Otherwise, a new node is allocated and inserted into S after initializing its accumulator as a . d = p . d and a . s = p . w . The expected update cost for an accumulator is O (lg e ). Hence, Time Extraction : Nodes of S are visited sequentially, and accumulators are passed to the selection phase. Time E = O ( e ).
 Selection , Sorting : Similar to TO-s3. Time S = O ( s +( e s )lg s ), Time skip list and O ( s ) for the min-priority queue. S = O ( e ). 3.2. Implementations for document-ordered (DO) processing Two important features in the inverted index structure let us devise another query processing strategy.
First, the postings of a term are stored in increasing order of document ids. That is, while traversing an inverted list, once a document id is seen in a posting, there cannot be a smaller document id in one of the succeeding postings in that list. Second, the number of query terms is limited. We have Q terms to be processed. These observations allow us to process the inverted lists in parallel instead of processing them consecutively. This way, it is possible to compute a complete score for a document before all postings in the lists are completely processed. In DO processing, update, extraction, and selection phases are performed in an interleaved manner. The implementations differ in their choice for the number of accumulators allocated, the data structures employed to store the accumulators, and the processing order of the list heads. 3.2.1. Implementations with multiple accumulator allocation (DO-m)
Implementations in the DO-m category use a structure M , which contains at most Q accumulators at i.e., each element h [ i ] points at the posting I h  X  i q a .  X  = i if accumulator a is associated with inverted list I a . d from any inverted list may update the score field a . s , only the postings from list I a . d . The document id a . d of each accumulator a is equal to a document id in one of the postings in
I mented by a sorted array or a dynamic data structure. These alternatives are described below. The algo-rithm for DO-m implementations is given in Fig. 4 .
 3.2.1.1. DO-m1: sorted array of accumulators, array of posting pointers, min-priority queue for top s accumulators. In this approach, Q accumulators are kept in an array sorted in decreasing order of document ids.

Creation : An accumulator array A and an array h for marking current list heads, each of size Q , are at the first posting I 1 q mulators in A . Hence, Time C = O ( e + Q ).
 than Q occupied accumulators in A , updates are performed over the accumulators using the postings at the current list heads (pointed by h ) which are not currently associated with an accumulator in A .In processing of a posting p  X I h  X  i q a is updated using p . Otherwise, a new accumulator is created in A and is initialized as a . d = p . d , lator admin with the minimum document id is located, extracted, and passed to the selection phase.
Then, h [ a dmin .  X  ] is incremented by 1, and hence it points to the posting p  X I Since the A array is maintained in decreasing order of document ids, an accumulator can be located in
O (lg Q ) time using binary search. Although update of an accumulator is an O (1)-time operation once it is located, insertion of a new accumulator after a failed search requires shifting O ( Q ) accumulators in the array. Considering the fact that there are u e accumulator updates and e insertions, Time
Q + eQ ). Extraction is simple since the accumulator with the smallest document id is always the last ele-ment of the array. Time E = O ( e ).
 Selection , Sorting : Similar to TO-s3. Time S = O ( s +( e s )lg s ), Time ority queue. S = O ( Q + s ). 3.2.1.2. DO-m2: AVL tree of accumulators, array of posting pointers, min-priority queue for top s accumulators. Instead of a sorted array, an AVL tree T can be used as a dynamic structure to store the accumulators.

Creation : Array h is allocated and initialized similar to DO-m1. Nodes of AVL tree T are dynamically allocated. For each accumulator with a distinct document id, a tree node must be allocated although T contains no more than Q nodes at any time. Hence, Time C = O ( e + Q ).

Update , Extraction : Update and extraction phases are similar to DO-m1. However, in processing a post-ing, both update of an existing accumulator and insertion of a new one require O (lg Q ) operations in the worst case. Hence, Time U = O ( u lg Q ). The accumulator with the smallest document id is contained within the left-most leaf node in T . This leaf node can be reached by following the left links iteratively starting from the root of T until a node with no children is reached. With this approach, extraction is an
O (lg Q )-time operation. However, it is possible to improve this by an implementation trick. If each node keeps a link to its parent node, and the node with the smallest document id in T is remembered by a pointer, it turns out that extraction is an O (1)-time operation. Hence, Time Selection , Sorting : Similar to TO-s3. Time S = O ( s +( e s )lg s ), Time O ( Q ) for the AVL tree, O ( Q ) for the array of posting pointers, and O ( s ) for the min-priority queue. S = O ( Q + s ).
 3.2.2. Implementations with single accumulator allocation (DO-s)
Implementations in the DO-s category require the use of only a single accumulator a updates are performed on this single accumulator. Here, we describe two different implementations that belong to this category. The algorithm for DO-s implementations is given in Fig. 5 . 3.2.2.1. DO-s1: single accumulator, array of posting pointers, min-priority queue for top s accumulators. In this very simple approach, two passes are made over the list heads. In the first pass, the smallest document id among the currently unprocessed postings is determined. In the second pass, the postings with this small-est document id are picked and used to update a dmin .

Creation : The single accumulator a dmin , which stores the information about the currently minimum doc-ument id, is allocated. The h array is allocated and initialized as in DO-m1. The cost of reinitializing a dmin is O ( e ). Hence, Time C = O ( e + Q ).

Update , Extraction : A pass is made over the postings pointed by the h array. Within these postings, a posting p dmin with the minimum document id p dmin . d is found. Accumulator a minimum document id are found. The score field a dmin . s of accumulator a weights in each such posting. h [ i ] for each inverted list I to point at the next posting in the list. Once all updates over a the selection phase. This procedure is repeated until all postings are consumed. Since two passes are made over h for each distinct document id, Time U = O ( eQ ). Extracting a Hence, Time E = O ( e ).
 Selection , Sorting : Similar to TO-s3. Time S = O ( s +( e s )lg s ), Time 3.2.2.2. DO-s2: single accumulator, min-priority queue for posting pointers, min-priority queue for top s accumulators. In this implementation, instead of the h array in the DO-s1 implementation, a min-priority queue is used so that there is no need for the first pass, which searches for the minimum document id. Here, we describe an improved version of the implementation described by Kaszkiel, Zobel, and Sacks-Davis (1999) .

Creation : Similar to DO-s1. However, h is a min-priority queue implemented as a min-heap of postings pointers, keyed by the document ids in the postings they point at. Time
Update, Extraction : The min-priority queue h is built using the postings at the list heads. The following procedure is repeated until all postings are processed. The root of h stores posting p with the minimum document id among the current list heads. a a dmin . s =0. h is traversed in reverse order (starting from the Q th element down to the first element), and the postings with p . d = p dmin . d are located. Each such posting p is used to update a belongs to, and h is heapified at the node containing p . This approach avoids building the heap ( Kaszkiel et al., 1999 ) at each pass. After the posting p dmin at the root performs its update, a passed to the selection phase. In this approach, the heap is heapified exactly once for each posting, and hence Time U = O ( u lg Q ). Extraction has a cost of Time Selection , Sorting : Similar to TO-s3. Time S = O ( s +( e s )lg s ), Time queue of top s accumulators. S = O ( Q + s ). 4. Experimental results 4.1. Experimental platform
In the experiments, a Pentium IV 2.54 GHz PC, which has 2 GB of main memory, 512 KB of L2 cache, and 8 KB of L1 cache, is used. As the operating system, Mandrake Linux, version 13 is installed. All algo-rithms are implemented in C and are compiled in gcc with O2 optimization option. Due to the randomized nature of some of the implementations, experiments are repeated 10 times, and the average values are re-ported. All experiments are conducted after booting the system into the single user mode.
As the document collection, results of a large crawl performed over the  X  .edu  X  domain, i.e., the educa-tional US Web sites, is used. The entire collection is around 30 GB and contains 1,883,037 Web pages (doc-uments). After cleansing and stop-word elimination, there remains 3,325,075 distinct index terms. The size of the inverted index constructed using this collection is around 2.7 GB.

In query processing, four different query sets  X Q short ; Q sentences within the documents of the collection. Queries in Q up of between 1 and 5 query terms. Queries in Q medium contain between 6 and 25 query terms. This type of queries is observed in relevance feedback. Queries in Q large table also presents the minimum, maximum, and average e and u values observed during the experiments.
For each query set, three answer sets  X S small ; S large ; and S documents, respectively. S full expects all documents with a nonzero score to be returned to the user. Prop-erties of these answer sets, and the minimum, maximum, and average number of top documents actually returned as answer to queries in Q short are displayed in Table 3 . 4.2. Experiments on execution time
Fig. 6 presents the running times of implementations for different types of query and answer sets. Among the static-accumulator implementations in the TO-s category, for S implementation TO-s3 performs the best if queries contain a few terms, i.e., when Q same answer sets, the linear-time selection scheme TO-s4 performs slightly better than TO-s3 if
Q implementation TO-s2. The TO-s1 implementation, which requires sorting the nonzero accumulators, is outperformed in all experiments, but the gap between TO-s1 and the others closes as the queries get longer. For Q huge and S full combination, TO-s1 is almost as good as TO-s2 and TO-s3.

Among the dynamic-accumulator implementations in the TO-d category, for Q hashing implementation TO-d2 performs the best. For this implementation, we used an adaptive bucket size B = u / Q due to the time-space trade-off mentioned in Section 3.1.2.2 . For query sets Q sults are achieved by TO-d2 and the AVL tree implementation TO-d1, which perform almost equally well. Increasing the number of terms in queries seems to favor TO-d1, which is the fastest implementation for
Q
In the DO-m category, although the run-time complexity for the AVL tree implementation DO-m2 is better than that of the sorted array implementation DO-m1, in practice, DO-m1 is faster than DO-m2 the cost of accumulator shifts in the sorted array implementation. However, if queries get longer, DO-m2 starts to perform better than DO-m1. Interestingly, for Q the average.
 In the DO-s category, for short queries, the two-pass DO-s1 implementation is faster than the one-pass
DO-s2 implementation. As the number of query terms increase, DO-s2 starts to perform better. This can be explained by the fact that visiting the list heads in the first pass of DO-s1 brings an additional overhead, which dominates when queries are long. It is observed that, for Q DO-s1.

Among all implementations, if all documents with a nonzero score are returned, TO-s2 performs the best with TO-s3 displaying close performance. Otherwise, if answers are partially returned, performance de-pends on the number of query terms. For example, if queries are short DO-s1 is the best choice, whereas TO-s4 is the fastest implementation for medium and long query sizes.

It should also be noted that, for aggregate querying scenarios, the winners may change. For example, in the case the user is interested in the top 10 documents and 40% or more of the queries come from Q while the remaining 60% or less are of type Q medium requiring all top documents, then TO-s3 is preferable to both DO-s1 and TO-s2 in that it provides the best average query processing time. Taking this fact into cution times are first normalized with the smallest execution time. Then, the normalized time values are averaged and displayed across each query and answer set category.

According to Fig. 7 , DO-s1 and DO-m1 perform better than the rest for query set Q
Q long , TO-s3 is better than the others. For S small and S slightly outperforms TO-s3. On the overall, the local winners of the four categories are TO-s3, TO-d2, DO-m1, and DO-s2, where TO-s3 is also the global winner.

Fig. 8 displays the percent dissection of execution times for different query processing phases, i.e., cre-sorting phase. However, for most implementations, the sorting overhead is relatively less important, except s2, TO-s3, TO-s4, DO-s1, and DO-s2 implementations is occupied by the overhead of this phase. The extraction phase seems to be relatively important for DO-m1 and DO-s1 implementations. The respective reasons of this high overhead for DO-m1 and DO-s1 are the high amount of accumulator shift operations the update phase incurs the highest overhead. This overhead is especially high for TO-d implementations.
The creation overhead is usually negligible. 4.3. Experiments on scalability
In this section, we provide some experimental results that evaluate scalability of the implementations with increasing number of query terms, increasing number of extracted postings, increasing answer set sizes, and increasing number of documents. In the plots, instead of displaying the actual data curves which con-to simplify drawings and ease understanding. For the same purpose, we provide a single representative curve in cases where more than one curves have a very similar behavior and hence overlap. 4.3.1. Effect of number of query terms (Q)
Fig. 9 shows the query processing performance for varying number of query terms. This plot is obtained by submitting 100 queries, where i th query contains i query terms, and retrieving highly ranked 10 docu-ments at each query. As expected, DO-s1 is the implementation most affected from increasing query sizes.
Other DO implementations as well as TO-d implementations are also affected since increasing number of query terms results in more posting updates, i.e., increases the overhead of the update phase. The impact on
TO-s implementations is relatively limited since update operations are not costly and extraction and selec-tion operations have a considerable overhead for this type of implementations. 4.3.2. Effect of number of extracted accumulators (e)
In order to investigate the effect of the number of extracted postings on the query processing perfor-mance, we used a query set consisting of 100 queries, where each query has a single term. The queries trieved. Fig. 10 shows the performance variation for increasing number of extracted accumulators. Except for TO-s1, the TO-s implementations are not affected much by the increasing number of extractions since they anyway traverse the whole accumulator array and check every score field. The different behavior of
TO-s1 is basically due to the overhead of sorting. Among the TO-d implementations, TO-d2 seems to scale best with increasing e . DO implementations perform quite well since there is only a single term in the queries.
 4.3.3. Effect of number of retrieved documents (s)
Fig. 11 shows how the performance is affected by increasing size of answer sets. To obtain this plot, we experiment, the size of the answer set equals i % of the documents with a nonzero score, i.e., s
According to Fig. 11 , as expected, the number of returned documents has no effect on TO-s1 since all non-zero documents are anyway sorted. For TO-s2, the curve is almost linear since the complexity of the selec-tion phase is s lg e and e is fixed. The linear behavior of TO-s4 is also due to the linear-time selection heuristic employed. All other implementations have a similar behavior which complies with their O ( e lg s ) complexity. The performance gap between the curves is due to the overheads of other phases. An interesting observation obtained from Fig. 11 is that a trade-off can be made between TO-s2, TO-s3, and TO-s4 imple-mentations depending on the percentage of retrieved documents. 4.3.4. Effect of dataset size (D)
In this section, we investigate the scalability of the implementations with respect to the document collec-tion size. In the experiments, we use document collections of three different sizes  X  D
D the rest of the experiments. Table 4 gives the number of documents and number of distinct terms in these collections. In all experiments, we use the medium-length query set Q swer sets.
 the speedups, which is calculated as QPT  X  D  X  = QPT  X  D 0 for two document collections D and D 0 such that j D j &gt; j D combination, there is almost no scalability problem for most of the implementations as we increase the size of the document collection from small to medium, i.e., the query processing times double as the collection size doubles. However, scalability begins to become an issue when we further increase the size of the document collection. The best scalability is observed for DO-s1, whereas the least scalable implementation due to the increasing overhead of the sorting phase, which does not scale well. 4.4. Experiments on space consumption
Fig. 13 displays the peak space consumption of each implementation. This value is equal to the maxi-mum amount of space allocation for inverted lists, accumulators, and some auxiliary data structures, ob-served at any time while running the query processor for a query and answer set pair. It excludes the space immediately de-allocated at the moment it is no longer needed.

In TO implementations, the peak space consumption is reached when space for accumulators plus an inverted list is allocated. In TO-s implementations, the peak consumption is reached when the space for the space for all inverted lists is allocated and the number of accumulators is at the maximum.
According to Fig. 13 , for short queries, DO implementations are the most space-efficient. However, there cally because the storage amount of postings dominates that of accumulators since more inverted lists must be in the memory at the same time. For Q medium ; Q long ; and Q amount of space. Among TO-d implementations, TO-d2 is the most space-efficient implementation. 5. Concluding discussion
Time complexities for different phases of the algorithms are summarized in Table 6 . According to this table, in general, TO-s implementations differ in their selection phase whereas the update phase is discrim-inating for TO-d and DO implementations. Table 7 gives the total time and space complexities. The pro-TO implementations and O ( u ) for the DO implementations.

It should be noted that different variants, which perform well under certain circumstances, can be created by slight modifications over the algorithms presented in this work. For example, TO-s4 can be modified so that in the extraction phase nonzero accumulators are placed in the first e elements, and the median-of-medians selection algorithm can be run only on these accumulators. In our experiments on this variant (although not reported here), we observed that this implementation is the fastest in processing short queries.

Similarly, DO-s2 can be modified using a pruning strategy such that only the postings having the min-imum document id and their left and right children in the heap are checked. This approach performs well on long queries, but the bookkeeping overhead dominates at short queries. Similar optimizations are pos-sible for space consumption. For example, TO-s2 and TO-s3 can be modified such that the accumulator array keeps only the scores. This decreases the space consumption to half of its original as long as s 6 D /2. Although our results indicate that TO-d implementations perform poorly, for querying scenarios where D and Q are high but e is low, implementations in TO-d category can be both time-and space-efficient.

To summarize, the results show that there is no single, superior implementation. Depending on the prop-erties of the computing system, document collection, user queries, and answer sets, each implementation has its own advantages. Currently, we are working on a hybrid system which will, depending on the param-eters, intelligently select and execute the most appropriate implementation taking both time and space effi-ciency into consideration. Clearly, for a better analysis, the experiments need to be repeated on a larger document collection where D and T are much higher. For this purpose, we have started a large crawl of the Web and plan to repeat the experiments on this larger collection.
 References
