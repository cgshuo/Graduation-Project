 This paper revisits the static term-based pruning technique presented in [2] for ad-hoc retrieval, addressing different is-sues concerning its algorithmic design not yet taken into account. Although the original technique is able to retain precision when a considerable part of the inverted file is re-moved, we show that it is possible to improve precision in some scenarios if some key design features are properly se-lected.
 H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Performance, Experimentation Pruning, indexing, efficiency
Static pruning of inverted files is an attractive technique to reduce storage needs and query processing times. Pruning is intended to remove the pointers (term-document pairs) most likely not to affect query performance. The algorithm presented in [2] yields excellent results at keeping the top-k answers for a given query and scoring function (Smart X  X  tf-idf), making this approach suitable for high-demanding environments. It involves two parameters, k and  X  , that set the number of top-documents ( k ) that are scored the same (within an error of  X  ) whether the original or the pruned inverted file is used, for any query less than a certain size ). Hence, the rankings produced are similar. In brief, the pruned inverted file is produced by discarding, for ev-ery term, the documents that score lower than a certain TREC disks 4&amp;5 2G 301-450+601-700 250 threshold. The cut-off value is set to z t  X   X  where z t is the k -th highest score for a given term. Results presented at [1] and [2] prove that the technique conveys excellent results for maintaining precision when a high quantity of the data is removed.

Our belief is that by issuing some key features, the tech-nique can go further than just faithfully preserving the doc-ument ranking, and be able to increase precision values at some pruning levels and under certain conditions. We used a probabilistic-based scoring function (BM25) instead of Smart X  X  tf-idf, addressed some features not considered in the original model, and identified such conditions through trec-style intensive evaluation.
We experimented with three different document/topics sets, described in table 1, and measured P@10 and MAP. Terms were stemmed using Porter X  X  algorithm. As well, we considered three types of queries: short (title only), medium (title + description) and long (title + description + narra-tive).

The k and  X  values in the original description of the al-gorithm must be selected carefully, as they hold important properties regarding to ranking similarity before and after pruning; especially,  X  should be low. In this paper we con-sider k and  X  just as parameters that determine the num-ber of pointers removed (percentage of pruning), without focusing on theoretical guarantees. Choosing different k values yields very correlated results, although higher val-ues ( k = 30) are preferable for obtaining better MAP values whereas lower ( k = 10) values seem more adequate if a good behaviour at P@10 is desired, which goes accordingly with the top-k preserving property construction of the algorithm. Some of the other considerations we took into account are summarised next.

Terms with document frequency df &gt; N/ 2 can be dis-carded from the inverted file, where N is the total number of documents. This value comes naturally from BM25 X  X  idf for-for every document. It turned out that ruling out terms Figure 1: MAP and P@10 for short queries at dif-ferent pruning levels, baseline and different settings (WT2g collection) with df &gt; N/ 2 is always a good option. This holds true es-pecially for long queries, where those terms are more likely to appear.

In the original work ([2]) it is stated that every score must be shifted, otherwise the pruning level obtained is negligi ble. This shifting is done by subtracting the global minimum score. As we considered  X  as just a parameter, we omitted the shifting step.

Following the description of the idealised pruning algo-rithm in [2], document lengths ( dl ) should be the same in both the original and the pruned inverted file. However, it makes sense to update them in order for the inverted file to be coherent . Experiments demonstrated that the best prun-ing and precision tradeoffs are obtained when the document lengths are updated.

A last design consideration was whether to update the av-erage document length avgdl in the pruned inverted file or not. Correcting this value portrays a new term frequency ( tf ) normalisation factor. Updating avgdl performs slightly better than not doing it for long queries, whereas for short queries updating is outperformed by not doing it. There-fore, the new term frequency normalisation introduced by the algorithm seems adequate for a short-queries scenario.
To be more concrete, we present next some of the pre-cision vs. pruning results using standard MAP and P@10 measures. To illustrate the effects of some of these design considerations figure 1 shows three precision curves obtain ed using the WT2g collection and short queries. Our baseline is the result using BM25, not updating the dl s, not updat-ing the avgdl and not removing terms with df &gt; N/ 2 (this setting is the one that retains better the original ranking) . The other two curves are obtained removing terms, updating the dl s (the best setting found empirically), and c 2 updates the avgdl while c 1 does not. With the baseline settings, the method is better for maintaining the top-k results, which is easier for shorter queries, but the precision curves are de-creasing. On the other hand, a different parameter selection improves significantly over the original precision values, up to a 50% pruning level for MAP and 70% for P@10.

Overall, using a suitable combination of parameters prun-ing is able to improve MAP and P@10. Considering the pre-cision values obtained with the original not pruned inverte d file as our baseline, it can be retained up to a 35-50% prun-ing level in most cases, although the method tends to favour short queries and the P@10. In that case, MAP(P@10) is over the baseline up to a 30-50%(60-70%) pruning level. Maximum improvements go up to 12% for MAP and 10% for P@10. These values are collection-dependent and lower with long queries. The behaviour is better in web collec-tions than in disks 4&amp;5, where MAP improvements are very small, even though the original precision value can still be maintained up to a 40-60% pruning level.

Parameter selection is crucial, and different algorithm set -tings lead to totally different performances. P@10 presents a good behaviour with any query size, whereas MAP be-haves better with short queries. Some parameter combi-nations (document length update, selective term removal) are consistently better than the rest, although the improve -ments are more noticeable in the web collections (WT2g especially). Finally, the curves are not always monotoni-cally decreasing, and high pruning values may increase pre-cision, probably due to the score function ranking high bad document-term pointers, which are removed at those prun-ing levels.

We end this section showing that the effect of pruning updating the dl and without updating the avgdl in BM25 X  X  score, is to alter the tf contributions in a particular fashion. The tf normalisation in BM25 is tf n = tf nf where nf = (1  X  b ) + b dl avgdl , and b  X  [0 .. 1] is a constant (typically 0.75). If  X  terms are removed from a document by the pruning algorithm, the new normalisation factor would be nf  X  = nf  X  b  X  avgdl . This has the global effect of softening the dl contribution, and it can be compared with selecting a lower b value. In this case, b  X  = b  X   X  which implies that nf  X  = nf  X   X  ( dl avgdl  X  1), and hence both normalisation factors have the same analytical form for long documents ( dl &gt; avgdl ), and different otherwise.
In this work we outlined and experimented with several new variants of the most well-known inverted file pruning al-gorithm [2], and stated how it is possible to tweak the tech-nique so that the precision is improved when some informa-tion is removed selectively. As a main conclusion, discardi ng pointers from an inverted file off-line and independently of the queries, is able to devise satisfactory results, efficien cy and effective-wise, for ad-hoc retrieval. As well, carefull y addressing the algorithmic issues involved brings some new intuitions regarding to weighting schemes or term frequenc y normalisation.
 Acknowledgements. The work reported here was co-funded by SEUI and FEDER under project MEC TIN2005-08521-C02 and  X  X unta de Galicia X  under project PGIDIT06PXIC10501PN. [1] D. Carmel, E. Amitay, M. Herscovici, Y. S. Maarek, [2] D. Carmel, D. Cohen, R. Fagin, E. Farchi,
