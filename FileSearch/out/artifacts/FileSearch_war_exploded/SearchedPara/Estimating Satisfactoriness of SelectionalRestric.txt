 YOICHI TOMIURA Kyushu University SHOSAKU TANAKA
Ritsumeikan University and TORU HITAKA Kyushu University (retired March 2003) 1. INTRODUCTION tional restriction has been used for syntactic and word sense disambiguation. of syntactic-category-level grammar (not simultaneously, of course). However, the construction that  X  X ith a fork X  modifies  X  X at X  is semantically valid, the construction that  X  X ith a fork X  modifies  X  X ast X  is semantically invalid.
We focus here on a syntactic construction of Japanese in which a postposi-tional phrase modifies a verb. A postpositional phrase is composed of a noun postpositional particle is p is denoted by n , p . When the construction that is called  X  X o-occurrency X  in this paper.

Co-occurrencies expressed with semantic primitives of nouns or semantic categories in a thesaurus have been used widely for syntactic and semantic disambiguation. Semantic primitives or semantic categories of nouns have to categories.

This paper proposes a new method for estimating the co-occurrency of word combinations n , p , v from a tagged corpus. Of course, a set of co-occurrences that are observed in the corpus is just only a partial set of the word combi-nations that can co-occur. Therefore, we need to estimate co-occurrencies of word combinations not observed in the corpus. The proposed method estimates co-occurrencies based on the multiple regression model. The independent vari-ables of this model correspond to the modifier n , p . Unlike a conventional multiple regression analysis, the independent variables are also parameters to be learned. This is the characteristic of our method. 2. METHOD FOR ESTIMATING COOCCURRENCY 2.1 Model particle, V ={ v 1 , v 2 , ... , v N } be a set of verbs, and C
C i , j can be calculated with the following multiple regression formula:
M  X  ( K  X  1) matrix whose ( i , k )-element is x 2.2 Learning Parameters of the Model
We would like to estimate X and A with co-occurrence data S , the sequence of tical combinations. Co-occurrencies of word combinations that are observed in a corpus are expected to be high, whereas co-occurrencies of word combinations that are not are expected to be low. Our method then estimates the parameters
X and A by minimizing the following objective function F ( X , A ), so that the co-occurrencies calculated with Eq. (1) have the above tendency: pp i with i and v j with j . The combination i , j
S ) is a positive sample for estimating X and A . The combination i , j is regarded as a quasi-negative sample with a weight and frequency. How to set  X  i , j is described in Section 2.3.
 The objective function F ( X , A ) is also expressed as follows: where
Let the ( K  X  K ) matrix D j ( X ) and the K -dimensional vector b as:
The simultaneous equation  X  F ( X , A ) / X  a k , j = 0( k pressed using D j ( X ) and b j ( X ) as follows: equation Eq. (2) for each j = 1, 2, ... , and N .

Likewise, the simultaneous equation  X  F ( X , A ) / X  x i , k is expressed as follows: ( K  X  1)  X  ( K  X  1) matrix D i ( A ) and the ( K  X  1)-dimensional vector b defined as: can be found by solving the simultaneous equation Eq. (3) for each i and M .

Now we suppose that X = X m and A = A m . When A m + 1 is the solution of the simultaneous equation Eq. (2) at X = X m for each j =
X m + 1 is the solution of the simultaneous equation Eq. (3) at A i = 1, 2, ... , and M , then the following relation holds true:
Therefore, we can find the value of X and A that locally minimize the objective the simultaneous equations Eq. (2) and (3) as above. 2.3 How to Set  X  i , j depends on  X  i , j and the higher  X  i , j is, the closer to 0 the estimated C and we think there is still room for improvement.

We suppose that pp i , v j is not observed in a corpus. Let f ( i , j f ( i , j ; S
Now we consider a probabilistic phenomenon where there are m kinds of different possible results. If we observe this phenomenon f times and we never that the result e can not be observed. Therefore, thinking as follows would be of pp i . Furthermore, the larger the value of f (  X  , j ; S appearance of v j .

On the other hand, however large the value of f ( i ,  X  ; S we can not affirm that pp i , v j can not co-occur.

We then assume that  X  i , j has the following characteristics: (1) increasing function with respect to f ( i ,  X  ; S  X  ) / N and f ( for any i , j /  X  S  X  is lower than 1.

In the experiment on estimating co-occurrencies described later, we use the following function for  X  i , j : where  X  is a meta-parameter of our model. 3. RELATED WORK
The proposed method for estimating co-occurrencies can be regarded as a kind of smoothing method and is related to the previous studies about smoothing the probability P ( pp , v ) and the conditional probability P ( v 3.1 Linear Interpolation and Back-off Smoothing linear interpolation or the back-off smoothing. The linear interpolation esti-mates the smoothed conditional probability of pp , v given pp as follows: where  X  is an interpolation coefficient and P represents the maximum likeli-hood estimate. The back-off smoothing estimates  X  P ( v | where P d represents the Good-Turing discounted estimate, P represents the maximum likelihood estimate, and  X  ( pp ) is a normalization factor. With both methods, when pp , v 1 and pp , v 2 are not in S ,
That, however, is too simple to agree with our linguistical intuition. 3.2 Smoothing Using Semantic Categories
The probability P ( n , p , v ) and the conditional probability P ( v estimated through smoothing using semantic categories of words. For exam-ple, if we can use semantic categories of nouns in a thesaurus, the smoothed probability P ( n , p , v ) is expressed as follows: where the sum is over all semantic categories of n . P ( n roughly estimated using the maximum likelihood estimate P ( n ) and P ( n , p semantic categories, and (3) the placement of the categories in the thesaurus, most appropriate knowledge for the smoothing. 1 3.3 Similarity-Based Model Grishman and Sterling [1994] computed the smoothed frequency f occurrence w , r , w  X  2 using the  X  X onfusion probability X  P P
C ( w and is defined as: where P represents the maximum likelihood estimate.

Dagan et al. [1999] described several methods for smoothing the conditional probability of a co-occurrence using similarity measures such as the confusion probability, the Kullback-Leibler (KL) divergence, the Jensen-Shannon (JS) di-vergence, and the L 1 norm computed with co-occurrences observed in a corpus.
The smoothed probability of a word pair w 1 , w 2 conditional on the appearance of word w 1 is calculated as follows: where f is the observed frequency in a corpus, P d represents the Good-Turing ilarity between w 1 and w 1 , and norm( w 1 ) = w pseudo-word disambiguation experiment. 3
There have also been many studies about the similarity between words based on co-occurrences, such as Hindle [1990] and Nagamatsu and Tanaka [1996].
These methods characterize words using co-occurrences. Our method esti-mates the parameters X and A ; x ( i ) and a ( j )in X and A characterize the characterizes modifiers and words using co-occurrences like the above meth-ods. We compare our method with some similarity-based methods on a syntactic disambiguation task in Section 4.3. 3.4 Model Using Hidden Semantic Classes
In Pereira et al. [1993] and Lee [1997], the conditional probability of a verb X  noun pair with the verb X  X bject relation given noun n is decomposed as follows using hidden sense classes: following objective function F : cluster membership entropy. They are defined as: where P represents the maximum likelihood estimate. In Pereira et al. [1993] and Lee [1997], a deterministic annealing procedure is used to find the number of clusters through a sequence of phase transitions by continuously increasing the meta-parameter  X  in F .

Hoffmann proposed PLSI, which is a statistical document indexing method and has been used in the field of information retrieval [Hofmann 1999]. In the
PLSI model, the joint probability of a document d (  X  D ) and a word w ( estimated as follows: rule, P ( d , w ) is reparameterized as: following log-likelihood function L : for maximizing L is based on the EM algorithm.

Equations (6) and (7) are the same decomposition: both are based on statis-co-occurrency Eq. (1). The differences between them are the parameters X  space and how to estimate the parameters, especially the objective function. We com-pare the proposed method with the PLSI model on a syntactic disambiguation task in Section 4.3. 3.5 Case Frame Construction
Kawahara and Kurohashi [2002] constructed the case frame automatically from an untagged corpus as follows: 2. Group examples that have a common predicate word (verb and adjective, for parse using the case frame instances disambiguates a syntactic structure based on the example-based method using the case frame instances as examples. A than a co-occurrency. However, their method uses a thesaurus in constructing the case frame instances and in parsing, and would also suffer the problem coming from a thesaurus. 4. EXPERIMENT
Japanese word combinations noun , post positional par ticl e , verb and its evaluation experiments. 4.1 Experiment on Estimating Co-Occurrency and its Conditions
The samples S are constructed as follows: 1. Divide sentences in the EDR Japanese Corpus (JCO-V020E) disambiguation experiment described in Section 4.3). The sequence S combinations noun , post positional -par ticl e , verb from the former are then extracted, where postpositional particles are one of  X  X A, X   X  X O, X   X  X I, X   X  X E, X   X  X ARA, X   X  X ORI, X  and  X  X E. X  2. Repeat the following procedure ( m = 0, 1, 2, ... ) until S 3. S = S m , PP ={ pp | pp , v  X  S m } and V ={ v | pp , v
S includes infrequent nouns and verbs. If pp , v satisfies Eq. (8), we expect will be poor and that would be the case with PLSI, which is compared to our method in Section 4.3. This is why we remove such pairs in Step (2). While we have restricted PP and V , we do not remove pp , v in S 0 methods using the similarity-based model in Section 4.3.

There are 201,206 co-occurrences by token and 155,824 by type in S includes 55,190 pairs of nouns and postpositional particles and 8256 verbs by type. Furthermore, in S there are 161,235 co-occurrences by token and 117,906 by type. M , the size of PP , is 19,442, and N , the size of The co-occurrencies of combinations in PP  X  V have been estimated with
S , based on the proposed method. The method described in Section 2.2 is for function F ( X , A ) and such ( X , A ) is dependent on X we regard ( X , A ) as the optimum ( X , A ) for each dimension K as follows: 1. Prepare ten kinds of X 0 ,( X 1 0 , X 2 0 , ... , X 10 each X t 0 is generated from the uniform distribution on [ 2. For each t = 1, 2, ... , 10, repeat the update of A and X starting from X til the value of the objective function converges sufficiently to get ( X which minimizes F ( X , A ). 3. Find ( X , A ) such that : 4.2 Direct Evaluation of the Estimated Co-Occurrency
Subjectively evaluating the estimated co-occurrencies is very difficult and, of course, it is not objective. Consequently, we have evaluated the estimated co-occurrencies by how clear the following tendencies are: 1. The estimated co-occurrencies of pp , v  X  S are high; 2. the average of the estimated co-occurrencies of pp , v is very low; different from that of S ) are high.

We have used the RWC Text Database Ver.2 as the source of S . the average and the variance of the estimated co-occurrencies of combinations estimated co-occurrencies, when the dimension K = 8 and  X  = in Table II means the following ratio: Clearly, the estimated co-occurrencies have the above tendencies.
Let C be the sample mean of the co-occurrencies of 97,260 combinations can be estimated to be lower than about 10  X  5 by Chebyshev X  X  inequality as follows: where E [ C ] is estimated at 0.138 and Var [ C ] is estimated at 0
Table I. This means that the proposed method is very good at estimating the co-occurrencies of the word combinations that are not observed in one corpus but are in another. 4.3 Indirect Evaluation of the Estimated Co-Occurrency
We have experimented on disambiguating syntactic structures of Japanese sen-tences to evaluate the co-occurrencies estimated by the proposed method. The following is one of the representative forms of (partial) sentences that have a syntactic ambiguity: where n is a noun, ps is a sequence of postpositional particles that includes  X  X E X  (let that postpositional particle be denoted by p ), v ws 1 is the null string or a sequence of words without verbs, ws of words including at least one noun without verbs, v 1 modifies a noun in ws like a relative clause in English, and there is no word in ws modifies v 1 or v 2 : sometimes n , p modifies v 1 and sometimes v sentences in Figure 1 are examples with the form (9). n , p modifies v case of (a) and (d) and modifies v 2 in the case of (b) and (c).
There have been various factors reported that are useful for deciding which word modifies which [for example, Uchimoto et al. 1999]. Here we use the fol-lowing two types of surface information: ps includes the Japanese postpositional particle  X  X A X  the next content word of the noun n is v 1 n , p modifies v 2 mostly when ps includes  X  X A, X  and n , p modifies v tendency can be seen in the data described in Section 4.3.1. However, you will find from the data that it is difficult to decide whether n , p modifies v when a sentence does not have such surface characteristics. Therefore, we have experimented on disambiguating syntactic structures using co-occurrencies for
Japanese sentences with the form (9), where ps does not include  X  X A X  and the of co-occurrencies and compared them: the co-occurrency estimated by the proposed method; the co-occurrency estimated by the PLSI model; divergence as the similarity measure); the co-occurrency estimated by the similarity-based model (using the simi-larity measure calculated with a thesaurus). 4.3.1 Training and Test Data. We have extracted the (partial) sentences with the form (9) from the source sentences for the evaluation described in
Section 4.1 and then built the data for evaluation. The form of all data is as follows: in the sentence, d is  X  X rue X  only when the next content word of the noun n is v in the sentence, and ans is the information about whether n , p modifies v The target for the experiment on the disambiguation is Type 1. The data with
Type 1 are used as both training data for the meta-parameters of each method and test data. 4.3.2 Disambiguation of Syntactic Structures Using Co-Occurrencies.
Table III shows that sentences with Type 1 have the tendency that n , p mod-ifies v 1 , although it is weaker than in the case of Type 2. This tendency is, therefore, reflected in the following methods for the disambiguation.
The method Base is the base line for the others, and decides whether n , p modifies v 1 or v 2 as follows:
The methods for disambiguation based on the estimated co-occurrencies de-cide whether n , p modifies v 1 or v 2 as follows: and  X  is a threshold (0  X   X   X  1).

REG is the method for disambiguation based on the co-occurrencies esti-mated by our method. Because the co-occurrencies calculated with Eq. (1) are as C ( n , p , v )in REG . The meta-parameters of REG are the dimension K , Eq. (4) and the threshold  X  .

In the other methods compared to REG , C ( n , p , v ) is regarded as P ( v
Based on the PLSI model, the conditional probability of pp , v given pp is decomposed as follows:
PLSI is the method for disambiguation based on the co-occurrencies calculated with Eq. (10). Learning parameters of the PLSI model suffer the local optimum problem like the proposed method, and the estimated parameters are depen-dent on the initial values of parameters. Consequently, we have tried ten kinds of initial values of parameters and found the optimum values of parameters for each dimension K like learning parameters of our model (cf. Section 4.1). The meta-parameters of PLSI are the dimension K and the threshold
SB(JS; S ) , SB(JS; S 0 ) , SB-G(JS; S ) , and SB-G(JS; S ambiguation based on the co-occurrencies estimated by the similarity-based model using the JS divergence as the similarity measure. In SB(JS; S ) and follows: where JS ( pp , pp ) is the JS divergence between P ( | pp ) and P ( a meta-parameter. Equation (12) is the same as Eq. (5) on is the same as taht described in Dagan et al. [1999]. The difference between
SB(JS; S ) and SB(JS; S 0 ) is the co-occurrence data used in them. SB(JS; S ) uses S to calculate the frequency f . The calculation of P calculate the frequency f . The sum in Eq. (13) is over all pp , such that some probability is calculated by instead of Eq. (11). The co-occurrence data S and S 0 are so sparse that the effect of the Good-Turing discount might be too strong, thus possibly making  X  ( pp ) too large. That is why we have tried SB-G(JS; S ) and SB-G(JS; S difference between SB-G(JS; S ) and SB-G(JS; S 0 ) is the co-occurrence data used in them. The meta-parameters of SB(JS; S ) , SB(JS; S and SB-G(JS; S 0 ) are  X  in Eq. (14) and the threshold  X  .

SB-G(TH; S ) and SB-G(TH; S 0 ) are also methods for disambiguation based on the co-occurrencies estimated by the similarity-based model. They, however, use the similarity calculated with a thesaurus. They calculate the conditional probability of pp , v given pp as follows: where SIM ( n , n ) is the similarity between noun n and n calculated with a thesaurus and  X  is a meta-parameter. We have calculated this similarity using the EDR Japanese Word Dictionary JWD-V020 and the EDR Concept Dictio-nary CPD-V020.1 as a thesaurus: 6 where CM ( n , n ) denotes the set of concepts that n and n have in common as represents the minimum length of the paths from the root concept to n via c .
The difference between SB-G(TH; S ) and SB-G(TH; S 0 ) is the co-occurrence data used in them. The meta-parameters of SB-G(TH; S ) and SB-G(TH; S are  X  and the threshold  X  .

With those methods (except Base ), we obtain the accuracy of the disam-biguation for sentences of Type 1 by tenfold cross-validation (nine blocks are for the training of meta-parameters and one block is for the test). The space of the threshold  X  is { 0, 0 . 025, 0 . 050, ... ,1 . 00 } parameters for each method are 4.3.3 Results of the Disambiguation Experiment. Table IV shows the accu-racy of the disambiguation by each method. The accuracy of REG is the best.
The accuracy of PLSI is only 0.8% higher than that of Base . SB-G(JS; S the best of the methods based on the conditional probability estimated by the similarity-based model using the JS divergence as the similarity measure, but methods based on the conditional probability estimated by the similarity-based model, and its accuracy is about 2% higher than that of Base . Furthermore, at a p -value of 0 . 01 between REG and SB-G(JS; S 0 ) , and at a p -value of 0 between REG and SB-G(TH; S ) .

The estimated values of models X  meta-parameters by tenfold cross-validation are
K = 8,  X  = 44, 46, 48, 50 (with REG ),
K = 9 (with PLSI ),  X  = 16, 17 (with SB-G(JS; S  X  = 6, 8 (with SB-G(TH; S ) ), for every test data block.

Dagan et al. [1999] evaluated their estimating method by the pseudo-word disambiguation experiment. In that experiment, they selected the noun X  X erb pairs for the 1000 most frequent nouns in the corpus they used, and about 600,000 co-occurrences were used for estimating P ( v | n ). On the other hand,
S consists of about 160,000 of n , p , v co-occurrences, which include about that this sparseness of our co-occurrence data is one of the reasons why the accuracies of PLSI , SB(JS;  X  ) and SB-G(JS;  X  ) are about the same with that of Base . We also think that SB-G(TH;  X  ) is more accurate than SB(JS;
SB-G(JS;  X  ) because the similarity between nouns calculated with a thesaurus is more trustworthy than the JS divergence calculated with such sparse co-occurrence data. On the other hand, we think the proposed method would es-timate the co-occurrencies very well even from such sparse co-occurrence data, using the quasi-negative samples effectively.

The accuracy of the proposed method does not reach the level needed for
Although our method does not need a thesaurus, there are some pairs of a noun and postpositional particle whose noun should be categorized by a thesaurus [ X 10-GATSU NI X  ( X  X n October X  in English),  X 2000-NENDAI NI  X  ( X  X n the 2000s X  in English), and  X  X YUYOUKU DE  X  ( X  X n New York X  in English), for instance].
That is why the accuracy is relatively low. The proposed method will estimate the co-occurrency more accurately from co-occurrence data where nouns in some types of postpositional phrases are categorized by a thesaurus than from raw co-occurrence data. 5. CONCLUSIONS on the multiple regression model. It was evaluated using some exper-dent variables are parameters to be learned, and it effectively uses the quasi-negative samples created from the positive samples.

The proposed method does not need a thesaurus, but we project that a co-occurrency can be estimated more accurately using partially categorized co-occurrence data by a thesaurus.

The co-occurrence data we used this time were extracted from a tagged cor-pus. It is necessary to automatically extract co-occurrences from an untagged in the result when our method estimates the co-occurrency from very large-scale and noisy co-occurrence data.

We expect that the idea that an argument just before a verb tightly restricts the use of the verb in Kawahara and Kurohashi [2002] will be a hint for im-proving our method. Regarding n , p , v as one unit, the co-occurrency of n , p and n , p , v can be estimated based on our method. Moreover, if we use very large-scale samples, the accuracy of syntactic disambiguation based on such co-occurrency will be greatly improved.

