 We focus on the multiclass classification problem. Let X be an Denote by n i the number of observations in the cluster C features have been standardized to have mean 0 and variance 1. metrics such as Euclidean distance.
 described as follows. Let Y  X  R n  X  Q with Y ik =1if x i  X  C find the linear transformation W =[ w 1 , ..., w K ], K  X  Q  X  criterion successively solves the problem tions optimal scoring problem, and/or the Fisher discriminant problem. ( 1 ) leads us to consider the following optimization problem Here  X   X  [0 , 1] and  X   X  0 are tuning parameters, and || w of w , i.e. the number of non-zero components of vector w k . the problem ( 2 ), Clemmensen et al. [ 2 ] replaced the 0 from [ 13 ] showed that the piecewise linear function [ 17 ] (Capped-the problem of feature selection in SVM. The success of Capped-us to apply it for the SOS problem.
 Our method is based on DC programming and DCA (DC Algorithms) intro-the list of references in http://lita.sciences.univ-metz.fr/ [ 11 ],[ 12 ],[ 18 ],[ 19 ] and references therein). in Section 3. Finally, Section 4 concludes the paper. umn vectors unless transposed to a row vector by a superscript tors x, y  X  R n and 1  X  p&lt;  X  , the inner product and p The piecewise linear approximation of the 0 -norm [ 17 ] is given by subject to rithm consists of holding  X  k fixed and optimizing with respect to DCA, and then holding w k fixed and optimizing with respect to formulated as Let s =( I  X  Q k  X  1 Q T k  X  1 D ) D  X  1 Y T Xw k . Then  X  For a fixed  X  k , we have to solve the following problem min programming and DCA. 2.1 DC Programming and DCA A general DC program is that of the form: where G, H are lower semi-continuous proper convex functions on a function F is called a DC function, and G  X  H a DC decomposition of while G and H are the DC components of F . Note that, the closed convex constraint x  X  C can be incorporated in the objective function of ( the indicator function on C denoted by  X  C which is defined by x  X  C , and +  X  otherwise.
 For a convex function  X  , the subdifferential of  X  at x 0  X  ( x ) &lt; +  X  X  , denoted by  X  X  ( x and the conjugate  X   X  of  X  is Then, the following program is called the dual program of ( metry between primal and dual DC programs: the dual to ( D The necessary local optimality condition for the primal DC program ( grams, for example, for DC polyhedral programs, or when function convex at x  X  ([ 11 ]).
 Tucker point (KKT) of (P dc )) if DCA consists in constructing two sequences { x l } and { y equivalent to determining x l +1  X   X  X   X  ( y l )).
 Generic DCA scheme Initialization: Let x 0  X  R n be an initial guess, l  X  0. Repeat -Calculate y l  X   X  X  ( x l ) -Calculate x l +1  X  arg min { G ( x )  X  x, y l : x  X  R n -l  X  l +1 Until convergence of { x l } .
 [ 11 ],[ 18 ]. It is worth mentioning that  X  DCA is a descent method ( without linesearch ): the sequences  X  X f G ( x l +1 )  X  H ( x l +1 )= G ( x l )  X  H ( x l ), then  X  X CAhasa linear convergence for general DC programs, and has a finite algorithmic point of view.
 [ 11 ],[ 18 ],[ 19 ] and the references therein.
 to investigate it for solving the problem ( 6 ). 2.2 DCA for Solving the Problem ( 6 ) The approximation  X   X  can be presented as a DC function [ 9 ]: where g  X  ( x )=  X  | x | and h  X  ( x )=  X  1 + max { 1 , X  | x |} be rewritten as follows where
G ( w k ):= and iteration l , we have to compute a subgradient v l of H at convex program of the form ( P l ), namely H is differentiable and v l =  X  H ( w l k ) is calculated as follows: where sgn( w kj ) is the sign of w kj .
 DCA for solving ( 10 )-( 12 ) can described as follows Algorithm 1.. (DCA applied to ( 10 )-( 12 )) method [ 4 ].
 2.3 Description of the Main Algorithm DCA can be described as follows.
 Algorithm 2.
 Let D = 1 n Y T Y . for k =1to K , compute k -th discriminant vector w k follows: end for predicted class for a test observation x is and  X  k is the mean vector of the k -th class. 3.1 Comparative Algorithms We will compare our proposed Algorithm 2 (AS DCA) with the methods pro-scoring problem, namely problem.
 is available from CRAN ( http://cran.r-project.org/ ). 3.2 Datasets rized in Table 1 and generated as follows: is assumed to have a multivariate normal distribution N ( of  X  1 are 0 and 500 test samples.
 tributions N (  X  1 , X  )and N (  X  2 , X  ), each of dimension of  X  100 100 tuning samples and 10000 test samples are generated. i  X  C k then samples are generated with equal probabilities for each class. (SRBCT 1 , ALL/AML 2 , Leukemia microarray 3 , MLL-Leukemia cer and Nakayama 6 ). All the datasets are preprocessed by normalizing each of these datasets is summarized in Table 2 . 3.3 Experimental Setups on a PC Intel i7 CPU3770, 3.40 GHz of 8GB RAM.
 zero. We select relevant features as follows: feature i is deleted if { 0 . 01 , 0 . 02 , 0 . 04 , 0 . 06 , 0 . 1 , 0 . 4 , 2 } and 10 trials for each experimental setting.
 training set and test set.
 3.4 Numerical Results and SDA approaches. The numbers of discriminant vectors used example, as in Figure 1 .
 We observe from computational results that: We also see that the AS DCA approach is better than the SDA and PLDA approaches on all the real datasets. The AS DCA approach selects from 0 to 3 . 05% of features while the SDA (resp. PLDA) approach selects from 0 to 10 . 66% (resp. 27 . 45% to 68 . 4%) of features.
 AS DCA approach on the LEU (resp. NAK) dataset.
 Training time: The training time given in Table 5 shows that the AS DCA datasets. (SOS) problem. Using an appropriate approximation of the 0 approach and it superiority with respect to the 1 -norm approaches. the Fisher X  X  discriminant problem.

