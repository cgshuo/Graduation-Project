 Matrix rank minimization problem is in general NP-hard. The nuclear norm is used to substitute the rank function in many recent studies. Nevertheless, the nuclear norm ap-proximation adds all singular values together and the ap-proximation error may depend heavily on the magnitudes of singular values. This might restrict its capability in dealing with many practical problems. In this paper, an arctan-gent function is used as a tighter approximation to the rank function. We use it on the challenging subspace clustering problem. For this nonconvex minimization problem, we de-velop an effective optimization procedure based on a type of augmented Lagrange multipliers (ALM) method. Exten-sive experiments on face clustering and motion segmentation show that the proposed method is effective for rank approx-imation.
 I.5 [ Pattern recognition ]: Clustering X  Algorithm ; G.1.6 [ Optimization ]: Constrained optimization Subspace Clustering; Rank Minimization; Nuclear Norm; Nonconvex Optimization
Matrix rank minimization arises in control, machine learn-to solve due to the discontinuity and nonconvexity of the rank function. Existing algorithms are largely based on the nuclear norm heuristic, i.e., to replace the rank by the nuclear norm [11]. The nuclear norm of a matrix X , de-k X k  X  = P i  X  i ( X ). Under some conditions, the solution to the nuclear norm heuristic coincides with the minimum rank solution [31, 32]. However, since the nuclear norm is the con-vex envelop of rank( X ) over the unit ball { X : k X k 2 c  X  may deviate from the rank of X in many circumstances [4, 3]. The rank function counts the number of nonvanishing singu-lar values, while the nuclear norm sums their amplitudes. As a result, the nuclear norm may be dominated by a few very large singular values. Variations of standard nuclear norm are shown to be promising in some recent research [14, 2, 29]. A number of nonconvex surrogate functions have come up to better approximate the rank function, such as Loga-rithm Determinant [11, 16], Schatten-p norm [26], truncated nuclear norm [14] and others [24]. In general, they are to solve the following low-rank minimization problem: where  X  i ( Z ) denotes the i -th singular value of Z  X  R h (  X  ) is a potentially nonconvex, nonsmooth function, and g (  X  ) is a loss function. By choosing h ( z ) = z , the summation of the first term in (1) goes back to the nuclear norm k Z k problem (1) becomes the well known convex relaxation of the rank minimization problem: In this paper, we will propose a new nonconvex rank ap-proximation and consider subspace clustering as a specific application.
In many real-world applications, high-dimensional data reside in a union of multiple low-dimensional subspaces rather than one single low-dimensional subspace [7]. Subspace clus-tering deals with exactly this structure by clustering data points according to their underlying subspaces. It has nu-merous applications in computer vision [30] and image pro-cessing [25]. Therefore subspace clustering has drawn sig-nificant attention in recent years [36]. In practice, the un-derlying subspace structure is often corrupted by noise and outliers, and thus the data may deviate from the original subspaces. It is necessary to develop robust estimation tech-niques.

A number of approaches to subspace clustering have been proposed in the past two decades. According to the sur-vey in [36], they can be roughly divided into four cate-gories: 1) algebraic methods; 2) iterative methods; 3) sta-tistical methods; and 4) spectral clustering-based methods. Among them, spectral clustering-based methods have ob-tained state-of-the-art results, including sparse subspace clus-tering (SSC) [9], and low rank representation (LRR) [21]. It is difficult to solve (6) directly because the objective func-tion is neither convex nor concave. We convert it to the following equivalent problem: min Now we resort to a type of augmented Lagrange multipliers (ALM) [20] method to solve (7). For simplicity of notation, we denote  X  i =  X  i ( J ) and  X  t i =  X  i ( J t ). The corresponding augmented Lagrangian function is: where  X  &gt; 0 is a penalty parameter and Y 1 , Y 2 are La-grangian multipliers. The variables E , J , and Z can be updated alternatively, one at each step, while keeping the other two fixed. For the ( t + 1)th iteration, the iterative scheme is given as follows.
 For Z t +1 , by fixing E t , J t , Y t 1 and Y t 2 , we have: It is evident that the objective function of (9) is a strongly convex quadratic function which can be solved directly. By setting the first derivative of it to zero, we have:
Z where I  X  X  n  X  n is the identity matrix.
 For J t +1 , we have: J Then we can convert it to problem (5). The first term in (5) is concave while the second term convex in  X  , so we can apply difference of convex (DC) [13] (vector) optimization method. A linear approximation is used at each iteration of DC programing. At iteration k + 1, whose closed-form solution is where  X  k =  X  X  (  X  k ) is the gradient of f (  X  ) at  X  to a local optimal point  X   X  . Then J t +1 = Udiag {  X   X  For E t +1 , we have the following subproblem: Algorithm 1 Arctan Rank Minimization Input: data matrix X  X  X  m  X  n , parameters  X  &gt; 0 , X  0 &gt; 0, and  X  &gt; 1.
 Initialize: J = I  X  X  n  X  n , E = 0, Y 1 = Y 2 = 0. REPEAT 1: Update Z by (10). 2: Solve (11). 3: Solve E by either (15), (16) or (17) according to l . 4: Update Y 1 by (18) and Y 2 by (19). 5: Update  X  by  X  t +1 =  X  X  t .
 UNTIL stopping criterion is met.
 Depending on different regularization strategies, we have dif-ferent closed-form solutions. For squared Forbenius norm, it is again a quadratic problem, For l 1 and l 2 , 1 norm, we use the lemmas from Appendix B. Let Q = X  X  XZ t +1 + Y t 1  X  t , we can solve E element-wisely as below: In the case of l 2 , 1 norm, we have The update of Lagrange multipliers is: The procedure is outlined in Algorithm 1.

Figure 2: Sample face images in Extended Yale B.
After obtaining optimal Z  X  , we can build the similarity graph matrix W . As argued in [9], some postprocessing of the coefficient matrix can improve the clustering perfor-mance. Following the angular information based technique of [21], we define e U = U  X  1 / 2 , where U and  X  are from the skinny SV D of Z  X  = U  X  V T . Inspired by [17], we define W as follows: where e u i and e u j denote the i -th and j -th columns of  X   X  N  X  controls the sharpness of the affinity between two Algorithm 2 Subspace Clustering by ARM Input: data matrix X , number of subspaces k .
 Do 1: Obtain optimal Z  X  by solving (7). 2: Compute the skinny SVD Z  X  = U  X  V T . 3: Calculate e U = U ( X ) 1 / 2 . 4: Construct the affinity graph matrix W by (20). 5: Perform NCuts on W . points. Increasing the power  X  enhances the separation abil-ity in the presence of noise. However, an excessively large  X  would break affinities between points of the same group. In order to compare with LRR 1 , we use  X  = 2 in our experi-ments, then we have the same postprocessing procedure as LRR. After obtaining W , we directly utilize a spectral clus-tering algorithm NCuts [33] to cluster the samples. Algo-rithm 2 summarizes the complete subspace clustering steps of the proposed method.
 Figure 3: Affinity graph matrix W with five and ten subjects.
Since the objective function (6) is nonconvex, it would not be easy to prove the convergence in theory. In this paper, we mathematically prove that our optimization algorithm has at least a convergent subsequence which converges to an accumulation point, and moreover, any accumulation point of our algorithm is a stationary point. Although the final so-lution might be a local optimum, our results are superior to the global optimal solution from convex approaches. Some previous work also reports similar observations [39, 12, 42]. reformulate our objective function:
L ( J,Z,E,Y 1 ,Y 2 , X  ) = G ( J,Z,E ) +  X  Y 1 ,X  X  XZ  X  E  X 
Lemma 3.1. The sequences of { Y t 1 } and { Y t 2 } are bounded.
As we confirmed with an author of [21], the power 2 of its equation (12) is a typo, which should be 4.

Proof. J t +1 satisfies the first-order necessary local opti-mality condition, Let X  X  define  X  i = 1 1+(  X  cording to (41) in Appendix B, and 0 &lt; 1 (23), we conclude that Y t +1 2 is bounded.

Similarly, for E t +1 Here  X  E denotes the subgradient operator [6]. Because || E || is nonsmooth only at E ij = 0, we define [  X  E k E k 1 ] ij E ij = 0. Then 0  X  k  X  E k E k 1 k 2 F  X  mn is bounded. There-fore, { Y t +1 1 } is bounded.

Lemma 3.2. { J t } , { E t } and { Z t } are bounded if P  X  , P 1  X  t &lt;  X  and X T X is invertible.
 Proof.
 Iterating the inequality (27) gives that Under the given conditions on {  X  t } , both terms on the right-hand side of the above inequality are bounded, thus The left-hand side in the above equation is bounded and each term on the right-hand side is nonnegative, so each term is bounded. Therefore, E t +1 is bounded. XZ t +1 bounded according to the last term on the right-hand side of (29), and thus after multiplying a constant matrix X T have X T XZ t +1 is bounded. Under the condition that X T is invertible, by multiplying a constant matrix ( X we have that ( X T X )  X  1 X T XZ t +1 = Z t +1 is bounded. Fi-nally, J t +1 is bounded because the second to the last term is bounded. Therefore, { J t } , { E t } and { Z t } are bounded.
Theorem 3.1. The sequence { J t ,E t ,Z t ,Y t 1 ,Y t 2 } gener-ated by Algorithm 1 has at least one accumulation point, un-is invertible. For any accumulation point { J  X  ,E  X  ,Z  X  { J  X  ,E  X  ,Z  X  } is a stationary point of optimization problem (21), under the conditions that  X  t ( J t +1  X  J t )  X  0 , and  X 
Proof. Based on the conditions on the penalty parame-ter sequence {  X  t } and X T X , Algorithm 1 generates a bounded the Bolzano-Weierstrass theorem, at least one accumulation to { J  X  ,E  X  ,Z  X  ,Y  X  1 ,Y  X  2 } . As shown below, { J a stationary point of problem (21), under additional condi-tions that  X  t ( E t +1  X  E t )  X  0 and  X  t ( J t +1  X  J t 0. Therefore, J  X  = Z  X  .
 X  X  XZ  X  .

For Z t +1 , the first-order optimality condition is If  X  t ( J t +1  X  J t )  X  0 and  X  t ( E t +1  X  E t )  X  0, we have X  X 
L ( J,Z,E,Y 1 ,Y 2 , X  ) | E  X  = 0. Therefore, { J  X  ,E  X  ,Z satisfies the KKT conditions of L ( J,E,Z,Y 1 ,Y 2 ) and thus { J  X  ,E  X  ,Z  X  } is a stationary point of (21).
This section presents experiments with the proposed al-gorithm on the Extended Yale B (EYaleB) [18] and Hopkins 155 databases [35]. They are standard tests for robust sub-space clustering algorithms. As shown in [9], the challenge in the Hopkins 155 dataset is due to the small principal an-gles between subspaces. For EYaleB, the challenge lies in the small principal angles and another factor that data points from different subspaces are close. Our results are compared with several state-of-the-art subspace clustering algorithms, including LRR [21], SSC [9], LRSC [10, 37], spectral curva-ture clustering (SCC) [5], and local subspace affinity (LSA) [40], in terms of misclassification rate. For fair comparison, we follow the experimental setup in [9] and obtain the re-sults.

As other methods do, we tune our parameters to obtain the best results. In general, the value of  X  depends on prior knowledge of the noise level of the data. If the noise is heavy, a small  X  should be adopted.  X  0 and  X  affect the convergence speed. The larger their values are, the fewer iterations are required for the algorithm to converge, but meanwhile we may lose some precision of the final objective function value. In the literature, the value of  X  is often chosen between 1 and 1.1. The iteration stops at a relative normed difference of 10  X  5 between two successive iterations, or a maximum of 150 iterations.
Face clustering refers to partitioning a set of face images from multiple individuals to multiple subspaces according to the identity of each individual. The face images are heavily contaminated by sparse gross errors due to varying lighting conditions, as shown in Figure 2. Therefore, k E k 1 is used to model the errors in our experiment. The EYaleB database contains cropped face images of 38 individuals taken under 64 different illumination conditions. The 38 subjects were divided into four groups as follows: subjects 1 to 10, 11 to 20, 21 to 30, and 31 to 38. All choices of n  X  X  2 , 3 , 5 , 8 , 10 } are considered for each of the first three groups, and all choices of n  X  X  2 , 3 , 5 , 8 } are considered for the last group. As a result, there are { 163 , 416 , 812 , 136 , 3 } combinations corresponding to different n . Each image is downsampled to 48  X  42 and is vectorized to a 2016-dimensional vector.  X  = 10  X  5 ,  X  0 and  X  = 1 . 03 are used in this experiment.
 Table 1 provides the best performance of each method. As shown in the table, our proposed method has the lowest mean clustering error rates in all five settings. In particu-lar, in the most challenging case of 10 subjects, the mean clustering error rate is as low as 3.85%. The improvement is significant compared with other low rank representation based subspace clustering, i.e., LRR and LRSC. For exam-ple, 19% and 11% improvement over LRR can be observed in the cases of 10 and 8 subjects, respectively. This demon-strates the importance of accurate rank approximation. In addition, the error of LSA is large maybe because LSA is based on MSE. Since the MSE is quite sensitive to outliers, LSA will fail to deal with large outliers.

Figure 3 shows the obtained affinity graph matrix W for the five and ten subjects scenarios. We can see a distinct block-diagonal structure, which means that each cluster be-Figure 4: Recovery results of two face images. The three columns from left to right are the original im-age ( X ), the error matrix ( E ) and the recovered im-age ( XZ ), respectively.
 Table 1: Clustering error rates (%) on the EYaleB database.
 comes highly compact and different subjects are well sepa-rated.

In Figure 4, we present the recovery results of some sam-ple faces from the 10-subject clustering case. We can see that the proposed algorithm has the benefit of removing the corruptions in data.

Figure 5 plots the progress of objective function values of (6). It is observed that with more iterations, the value of objective function decreases monotonically. This empirically verifies the convergence of our optimization method.
We compare the average computational time of LRR, SSC, and ARM as a function of the number of subjects in Figure 6. All the experiments are conducted and timed on the same machine with an Intel Xeon E3-1240 3.40GHz CPU that has 4 cores and 8GB memory, running Ubuntu and Matlab (R2014a). We can observe that the computational time of SSC is higher than LRR and ARM, while ARM is a little slower than LRR in most cases.
Motion segmentation involves segmenting a video sequence of multiple moving objects into multiple spatiotemporal re-gions corresponding to different motions. These motion se-Figure 5: Convergence curve of the objective func-tion value in (6).
 Figure 6: Average computational time (sec) of the algorithms on the EYaleB database as a function of the number of subjects. quences can be divided into three main categories: checker-board, traffic, and articulated or non-rigid motion sequences. The Hopkins 155 dataset includes 155 video sequences of 2 or 3 motions, corresponding to 2 or 3 low-dimensional sub-spaces of the ambient space. Each sequence represents a data set and so there are 155 motion segmentation prob-lems in total. Several example frames are shown in Figure 7. The trajectories are extracted automatically by a tracker, so they are slightly corrupted by noise. As in [21, 22], k E k is adopted in the model. In this experiment,  X  = 2,  X  0 = 10 and  X  = 1 . 05.

We use the original 2F-dimensional feature trajectories in our experiment. We show the clustering error rates of different algorithms in Table 2. ARM outperforms other al-gorithms in mean error rate. Especially, its all mean error rates are around 1 . 5%. This again demonstrates the effec-tiveness of using arctangent as a rank approximation. Figure 7: Example frames from two video sequences of the Hopkins 155 database with traced feature points.
 Table 2: Segmentation error rates (%) on the Hop-kins 155 Dataset.

Figure 8 shows the culstering error rate of ARM for dif-ferent  X  over all 155 sequences. When  X  is between 1 and 3, the clustering error rate varies between 1 . 48% and 2 . 19%. This demonstrates that ARM performs well under a pretty wide range of values of  X  . This is another advantage of ARM over LRR [21].
In this work, we propose to use arctangent as a concave rank approximation function. It has some nice properties compared with the standard nuclear norm. We apply this function to the low rank representation-based subspace clus-tering problem and develop an iterative algorithm for opti-mizing the associated objective function. Extensive experi-mental results demonstrate that, compared to many state-of-the-art algorithms, the proposed algorithm gives the low-est clustering error rates on many benchmark datasets. This fully demonstrates the significance of accurate rank approx-imation. Interesting future work includes other applications of the arctangent rank approximation; for example, matrix completion. Since LRR can only ensure its validity for in-dependent subspace segmentation, it is worthwhile to inves-tigate somewhat dependent yet possibly disjoint subspace clustering.
Clustering error rate (%) Figure 8: The influence of parameter  X  of ARM on clustering error of Hopkins 155 database.
This work is supported by US National Science Founda-tion Grants IIS 1218712. The corresponding author is Qiang Cheng. [1] A. Beck and M. Teboulle. A fast iterative [2] J.-F. Cai, E. J. Cand`es, and Z. Shen. A singular value [3] E. J. Cand`es and B. Recht. Exact matrix completion [4] E. J. Cand`es and T. Tao. The power of convex [5] G. Chen and G. Lerman. Spectral curvature clustering [6] F. H. Clarke. Optimization and nonsmooth analysis , [7] E. Elhamifar and R. Vidal. Sparse subspace clustering. [8] E. Elhamifar and R. Vidal. Clustering disjoint [9] E. Elhamifar and R. Vidal. Sparse subspace clustering: [10] P. Favaro, R. Vidal, and A. Ravichandran. A closed [11] M. Fazel. Matrix rank minimization with applications . [12] P. Gong, J. Ye, and C.-s. Zhang. Multi-stage [13] R. Horst and N. V. Thoai. Dc programming: [14] Y. Hu, D. Zhang, J. Ye, X. Li, and X. He. Fast and [15] K. Kanatani. Motion segmentation by subspace [16] Z. Kang, C. Peng, J. Cheng, and Q. Cheng. Logdet [17] F. Lauer and C. Schnorr. Spectral clustering of linear [18] K.-C. Lee, J. Ho, and D. Kriegman. Acquiring linear [19] A. S. Lewis and H. S. Sendov. Nonsmooth analysis of [20] Z. Lin, R. Liu, and Z. Su. Linearized alternating [21] G. Liu, Z. Lin, S. Yan, J. Sun, Y. Yu, and Y. Ma. [22] G. Liu, Z. Lin, and Y. Yu. Robust subspace [23] G. Liu, H. Xu, and S. Yan. Exact subspace [24] C. Lu, J. Tang, S. Y. Yan, and Z. Lin. Generalized [25] Y. Ma, H. Derksen, W. Hong, and J. Wright.
 [26] K. Mohan and M. Fazel. Iterative reweighted [27] B. Nasihatkon and R. Hartley. Graph connectivity in [28] A. Y. Ng, M. I. Jordan, Y. Weiss, et al. On spectral [29] F. Nie, H. Huang, and C. H. Ding. Low-rank matrix [30] S. Rao, R. Tron, R. Vidal, and Y. Ma. Motion [31] B. Recht, M. Fazel, and P. A. Parrilo. Guaranteed [32] B. Recht, W. Xu, and B. Hassibi. Null space [33] J. Shi and J. Malik. Normalized cuts and image [34] M. Soltanolkotabi, E. J. Candes, et al. A geometric [35] R. Tron and R. Vidal. A benchmark for the [36] R. Vidal. A tutorial on subspace clustering. IEEE [37] R. Vidal and P. Favaro. Low rank subspace clustering [38] Y.-X. Wang and H. Xu. Noisy sparse subspace [39] S. Xiang, X. Tong, and J. Ye. Efficient sparse group [40] J. Yan and M. Pollefeys. A general framework for [41] J. Yang, W. Yin, Y. Zhang, and Y. Wang. A fast [42] Z. Zhang and B. Tu. Nonconvex penalization using [43] Y.-B. Zhao. An approximation theory of matrix rank
Theorem A.1. For  X  &gt; 0 and A  X  R m  X  n , the following problem is solved by the vector minimization so that Z  X  = Udiag (  X   X  ) V T with the SVD of A being Udiag (  X   X  A ) V T .
 Proof. Let A = U  X  A V T be the skinny SVD of A , then  X 
A = U T AV . Denoting X = U T ZV which has exactly the same singular values as Z , we have F ( Z ) +  X  = F ( X ) +  X  = F ( X  X ) +  X  = F ( X  X ) +  X   X  F ( X  X ) +  X  = F ( X  X ) +  X  = F ( X  Z ) +  X  = f (  X  ) +  X   X  f (  X   X  ) +  X  In the above, (33) holds because the Frobenius norm is uni-tarily invariant; (34) holds because F ( X ) is unitarily invari-ant; (36) is true by von Neumann X  X  trace inequality; and (38) holds because of the definition of X . Therefore, (38) is a lower bound of (32). Note that the equality in (36) is attained if X =  X  X . Because  X  Z =  X  X = X = U T ZV , the SVD of Z is Z = U  X  Z V T . By minimizing (39), we get  X  Therefore, eventually we get Z  X  = Udiag (  X   X  ) V T , which is the minimizer of problem (30).

Theorem B.1. [19] Suppose F : R m  X  n  X  R is repre-sented as F ( X ) = f  X   X  ( X ) , and f : R n  X  R is absolutely symmetric and differentiable, where X  X  R m  X  n with SVD X = Udiag (  X  ) V T , the gradient of F ( X ) at X is
Lemma B.1. [1] For  X  &gt; 0 , and K  X  R s  X  t , the solution of the problem is given by L  X  ( K ) , which is defined component-wisely by
Lemma B.2. [41] Let H be a given matrix. If the optimal solution to is W  X  , then the i -th column of W  X  is
