 The workshop focuses on the three areas of interest to CIKM to discuss how to envisage and design evaluation infrastruc-tures able to store, manage, and make accessible the scien-tific data and knowledge of interest for advancing the eval-uation of information retrieval and access tools.

Main goal is to understand how to make use of the exper-tise of the three scientific areas in a cooperative way to avoid the duplication of efforts which may occur when addressing the problem separately in each specific area and to trigger synergies and joint actions on the issue.

Main purposes of the workshop are the identification of a roadmap and the definition of initial best practices to guide the development of the necessary evaluation infras-tructures.
 H.2.8 [ Database Management ]: Database applications X  Scientific databases ; H.3.3 [ Information Storage and Re-trieval ]: Information Search and Retrieval X  Search process ; H.3.4 [ Information Storage and Retrieval ]: Systems and Software X  Performance evaluation (efficiency and effec-tiveness) ; H.5.2 [ Information Interfaces and Presenta-tion ]: User Interfaces X  Benchmarking, evaluation, method-ology ; D.2.8 [ Software Engineering ]: Metrics X  Complex-ity measures, performance measures Design, Experimentation, Management, Measurement, Per-formance experimental evaluation, scientific data, evaluation infras-tructure, data test collection, best practices
Information Retrieval has a strong and long tradition dat-ing back to the 1960s in producing and processing scientific data resulting from the experimental evaluation of search al-gorithms and search systems. This attitude towards evalua-tion has led to fast and continuous progress in the evolution of information retrieval systems and search engines.
However, in order to make the data test collections, that are used in the context of the evaluation activities, under-standable and usable they must be endowed with some aux-iliary information, i.e., provenance, quality, context. There-fore, there is a need for metadata models able to describe the main characteristics of evaluation data. In addition, in or-der to make distributed data collections accessible, sharable, and interoperable, there is a need for advanced data infras-tructures.

In contrast, the information retrieval area has barely ex-plored and exploited the possibilities for managing, storing, and effectively accessing the scientific data produced dur-ing the evaluation studies by making use of the methods typical of the database and knowledge management areas. Over the years, the information retrieval area has produced a vast set of large test collections which have become the main benchmark tools of the area and contribute to repro-ducible and comparable experiments. However, these same collections have not been organised into coherent and inte-grated infrastructures which make them accessible, search-able, citable, exploitable, and re-usable to all possibly inter-ested researchers, developers, and user communities.
It is thus time for these three communities  X  informa-tion retrieval, databases, and knowledge management  X  to join efforts, meet, and cooperate to envisage and design use-ful infrastructures able to coherently manage pertinent data collections and sources of information, and so take concrete steps towards developing them.

To address the workshop issues, experts have been invited to give two keynote addresses and six relevant papers have been accepted, among the submitted ones, for presentation. The two following sections are reporting the addressed spe-cific topics.
The keynote address by Norbert Fuhr of the University of Duisburg-Essen in Germany 1 , entitled An Infrastructure for Supporting the Evaluation of Interactive Information Re-trieval , addresses the presentation of a testbed for the eval-uation of interactive information access. Starting with the INEX 2 interactive track in 2004, the group lead by professor Fuhr developed the Daffodil (now ezDL) framework, provid-ing an experimental framework for interactive retrieval, that allows for easy exchange or extension of the system com-ponents. Moreover, this framework also contains tools for http://www.is.informatik.uni-duisburg.de/staff/ fuhr.html.en https://inex.mmci.uni-saarland.de/ organizing laboratory experiments. Besides extensive log-ging (including the possibility to exploit eye tracking data), the system allows for presenting questionnaires at all stages of a search session (pre-/post-task/session), as well as the scheduling of search tasks and monitoring task time. The keynote address by Maurizio Lenzerini of the Sapienza University of Rome, Italy 3 , entitled Ontology-based data man-agement , addresses how the ontology-based data manage-ment aims at accessing and using data by means of a con-ceptual representation of the domain of interest in the un-derlying information system. The talk provides an intro-duction to ontology-based data management, by illustrating the main ideas and techniques for using an ontology to ac-cess the data layer of an information system. Then, it de-scribes an architecture for ontology-based data access and discussed the issue of choosing the appropriate language for expressing the various components of the architecture, by illustrating the main advantages one gains in managing the information system through the ontology. Finally, the issue of developing methodologies and tools for the design and us-age of ontology-based data management solutions have been explained.
The paper Principles for Robust Evaluation Infrastruc-ture by Justin Zobel (Department of Computer Science and Software Engineering, The University of Melbourne, Aus-tralia), William Webber (Department of Computer Science and Software Engineering, The University of Melbourne, Australia), Mark Sanderson (School of Computer Science and Information Technology, RMIT University, Australia), and Alistair Moffat (Department of Computer Science and Software Engineering, The University of Melbourne, Aus-tralia ) makes reference to the standard  X  X ranfield X  approach to the evaluation of information retrieval systems that has been used and refined for nearly fifty years. Over the last few years, investigation of the strengths and limitations of this approach have led to identification of serious flaws in some experiments. Since the knowledge of these flaws can prevent their perpetuation into future work and informs the design of new experiments and infrastructures, the authors review relevant aspects of evaluation and, based on their research and observations over the last decade, outline principles on which new infrastructures should rest.

The paper A Lightweight Framework for Reproducible Pa-rameter Sweeping in Information Retrieval by Richard Eckart de Castilho (Ubiquitous Knowledge Processing Lab, Techni-cal University of Darmstadt, Germany) and Iryna Gurevych (Ubiquitous Knowledge Processing Lab, Technical Univer-sity of Darmstadt, Germany) introduces a lightweight frame-work for parameter sweep experiments geared towards evo-lution, efficiency and reproducibility of experiments running on a single machine. To reduce the computational effort of running an experiment with many different parameter settings, the framework uses the tasks and the dataflow de-pendency information to maintain and reuse intermediate results whenever possible.
 The paper Evaluation with the VIRTUOSO platform by G  X erard Dupont (CASSIDIAN, Elancourt, France), Ga  X  el de Chalendar (CEA, Fontenay-aux-Roses, France), Khaled Khe-http://www.dis.uniroma1.it/~lenzerin/index.html/ liff (CASSIDIAN, Val de Reuil, France), Dmitri Voitsekho-vitchy (CEA, Fontenay-aux-Roses, France), G  X eraud Canet (CEA, Fontenay-aux-Roses, France), and St  X ephan Brunes-saux (CASSIDIAN, Val de Reuil, France) describes a soft-ware architecture for providing an open technical framework for the integration of tools for collection, processing, analy-sis and communication of open source information. The in-tegration of heterogeneous components is implemented in a way that also permit the comparison of capabilities of multi-ple tools. The platform that supports the evaluation frame-work has been named VIRTUOSO. It supports an evaluation framework that allows to deploy and run evaluation kits for different use-cases.

The paper Use Cases as a Component of Information Ac-cess Evaluation by Jussi Karlgren, Anni J  X  arvelin, Preben Hansen, and Gunnar Eriksson, all authors at the Swedish In-stitute of Computer Science, Sweden, argues that use cases for information access can be written to give explicit pointers towards benchmarking mechanisms and that if use cases and hypotheses about user preferences, goals, expectation and satisfaction are made explicit in the design of research sys-tems, they can more conveniently be validated or disproved -which in turn makes the results emanating from research ef-forts more relevant for industrial partners, more sustainable for future research and more portable across projects and studies.

The paper PatOlympics -An Infrastructure for Interac-tive Evaluation of Patent Retrieval Tools by Mihai Lupu (Vienna University of Technology, Austria) presents the in-frastructure behind the PatOlympics interactive evaluation campaign. This infrastructure, consisting of a relational database back-end, a Java processing core and a JavaScript interface, makes it possible for real users and researchers to interact in a competitive environment, while maintaining, to the extent possible, the evaluation procedures of standard information retrieval campaigns.
 The paper Infrastructure and Workflow for the Formal Evaluation of Semantic Search Technologies by Stuart N. Wrigley (Department of Computer Science, University of Sheffield, UK), Ra`ul Garc  X  X a-Castro (Facultad de Inform`atica Universidad Polit  X ecnica de Madrid, Spain), and C`assia Tro-jahn (INRIA and LIG, Montbonnot Saint Martin, France) describes an infrastructure for the automated evaluation of semantic technologies and, in particular, semantic search technologies. For this purpose, an evaluation framework is introduced which follows a service-oriented approach for evaluating semantic technologies and uses the Business Pro-cess Execution Language (BPEL) to define evaluation work-flows that can be executed by process engines.
 We would like to thank those institutions and individuals who have made this workshop possible: the ACM CIKM 2011 Conference organisation, the Program Committee mem-bers, and the Department of Information Engineering of the University of Padua, Italy.
 The DESIRE 2011 workshop has been supported by the PROMISE network of excellence 4 (contract n. 258191), as part of the 7th Framework Program of the European Com-mission. http://www.promise-noe.eu/
