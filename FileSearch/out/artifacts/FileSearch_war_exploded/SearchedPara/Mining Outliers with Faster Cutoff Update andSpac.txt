 Hawkins [1] defines an outlier as  X  X n observation which deviates so much from other observations as to arouse suspicions that it was generated by a different mechanism X . Outlier detection has many applications such as detecting malicious activities in computer systems and crim inal activities in commercial organiza-tions.

Knorr and Ng [2] first proposed a distance-based outlier definition that  X  X n object O in a data set T is an outlier if at least fraction p of the objects in T lies greater than distance D from O . X  Distance-based outliers generalize certain statistical outlier tests. As it is not ha rd to define a metric distance function, it readily applies to numerous applications. Knorr and Ng proposed the simple Nested-Loop algorithm (NL) [2] to find outliers by calculating their distances to all other objects or until an object can be confirmed as a non-outlier. Its complexity is O ( kN 2 )where k is the number of dimensions and N is the number of objects in the data set.

Even though efficient methods ([3] and [ 4]) have been developed, in practice, it is non-trivial to define an appropriate value for D . Knorr and Ng commented that it may take numerous iterations of trial and error to find D .In[3],aninteresting range [ D min , D max ]of D was found by an expensive brute-force method.
Later, Ramaswamy et al. [5] proposed a closely related definition without the need to set D :  X  X iven integers k and n , an object O is an outlier if it is one of the n objects with the larges t distances to their k th nearest neighbors. X  Users can simply specify how many outliers they want the algorithm to return without worrying about distance D . Moreover, the outliers returned are ranked and easier for users to examine. In fact, methods to detect such outliers can also be readily applied to the definition of Knorr and Ng by substituting k =(1  X  p ) | T | , letting n be a very large value and initializing a cutoff distance, which is assumed to be unknown, to the value of given D . Therefore, our work focuses on the definition of Ramaswamy et al.

Efficient methods ([6] and [7]) were pro posed to detect such outliers, where parameters are easier to specify without knowing the value of D .In[6],Bayand Schwabacher proposed ORCA, which extends NL by employing a pre-processing step of randomizing the data set. ORCA i s shown to give near linear time per-formance on real, large and high-dimensional data sets. In [7], Ghoting et al. presented RBRP (Recursive Binning an d Re-Projection), which extends NL by employing a pre-processing step of data clustering. In addition to the enhance-ment of [7], we observed the following two other areas of inefficiency of ORCA. We will discuss how to improve them to further increase the time saving. 1. ORCA prunes away an object if the object X  X  distance to its (so far) k th 2. The memory space is under-utilized leading to an increase in disk I/O cost.
Therefore, to solve the above two prob lems, we propose two techniques: (i) faster cutoff update, and (ii) space utilization after pruning. Both techniques complement each other in reducing execution time. The resulting algorithms are called RC and RS respectively. Details will be described in Sect. 3. In Sect. 4, we will discuss the experiments done on RC, RS and RCS (a hybrid approach combining both techniques). We will first describe the working principle of the ORCA algorithm [6] in Sect. 2. The ORCA algorithm calculates for each data object a score which can be defined as either (i) the average distance to the object X  X  k nearest neighbors or (ii) the distance to the object X  X  k th nearest neighbor. A higher score means that the object is further away from the other objects and hence is more dissimilar to the others. The top n outliers are defined as objects with the top n scores.
The algorithm (see Fig. 1) consists of three main components: (1) nested loops, (2) randomization and (3) a pruning rule. The nested loop computes the distances of an object to all oth er objects to find the object X  X  k nearest neighbors. The randomization refers to randomizing the order of the objects in the data set so that an object drawn sequentially from the data set will have the same chance to be a nearby neighbor of any object. The n th top score from the objects processed so far is called the cutoff value . Any object with a score smaller than the cutoff value cannot be the top n outliers and we can stop computing its distance to other objects. This condition acts as a pruning rule to reduce the number of distance computations. 3.1 Randomization with Faster Cutoff Update (RC) Our first technique solves the first problem described in Sect. 1 and is applied in the early processing stage. We define the early processing stage to be from the start of processing to the time when the ratio of new to old cutoff values is smaller than a threshold, that is, when the cutoff value no longer increases quickly. In our experiment, we set the threshold to be 1 . 1 b/ 100 for a start batch size of b .

There are two characteristics in this stage. Firstly, there is more room to reduce the computational cost because the cutoff value is small. Secondly, the computational cost decrea ses quickly in consecutive batch because better top n outliers are frequently found and so a larger cutoff value is found frequently. To save the computational cost, we take advantage of the rapid increase in cutoff value and propose using a smaller batch size in this stage. Using a smaller batch size causes a larger cutoff value to be used earlier and so the computational cost can be lowered. However, using a smaller b atch size will also increase the disk I/O cost because processing each batch r equires a disk scan of the data set.
The smaller batch size used in this stage is referred as the start batch size and is set to be 4 n or 100, whichever is larger. The reason is that after processing the first batch of objects, the temporary top n outliers will be a quarter of objects (or less) with the highest scores. The cutoff value will be at least at the upper quartile of the scores of this first batch, which would be a good initial guess. If the batch size is too small, the initial guess of the cutoff value would be subjected to severe random variations. Hence, the start batch size is set to be at least 100 objects.
 We call the algorithm with this technique RC, which replaces Step 3 of the ORCA algorithm (in Fig. 1) by the following STEP:  X  If it is in the early processing stage, load a batch of unprocessed objects of 3.2 Randomization with Space Utilization After Pruning (RS) Our second technique solves the second problem described in Sect. 1 and is applied in the main processing stage. We define the main processing stage to be from the time that a specified percentage of the data set is processed to the end of processing. In our experiment, we set the percentage to be 10%.

There are two characteristics in this stage. Firstly, there is little room to reduce the computational cost because the cutoff value starts getting close to its final value. Secondly, the computational cost decreases slowly in consecutive batch because better top n outliers are infrequent to find and so a larger cutoff value is found infrequently.

To reduce the total running time, we could increase the batch size so that the number of data scans and the disk I/O time can be reduced at the expense of small increase in computational time. However, this approach increases the memory requirement. Instead, we utilize the space of pruned objects and fill it with unprocessed objects. Then, more objects can be processed during one data scan. Thus, the number of data scans and the disk I/O time decrease. The frequency of filling the space controls the number of objects processed in one data scan. In our experiment, the frequency is set to be every 10% of a complete data scan.

Figure 2 shows the algorithm of RS. In Step 6, objects with scores lower than the cutoff value are pruned. In Step 9, at the end of every fixed percentage of a complete data scan, objects that hav e been compared with all objects in the data set are removed and used to update the top n outliers and the cutoff value. In Step 10, objects with scores lower th an the updated cutoff value are pruned. In Step 11, the space obtained from previous pruning steps is filled with new unprocessed objects. 3.3 Hybrid Approach Randomization with faster Cutoff update and Space utilization after pruning (RCS), combining RC and RS, improves both the performance of the early and the main processing stages. In this section, we evaluate the time performance of our proposed algorithms (RC, RS and RCS) on several data sets (see the first column of Fig. 3) by comparing them with ORCA. We also study how the performance scales with the data size. The effects of p arameters introduced with the proposed algorithms are studied at the end of this section. Our experiments are conducted on a lightly loaded Intel Core 2 Duo computer with 2 GHz processors and 2 GB memory. 4.1 Performance against Different Data Sets Figure 3 compares the running time of ORCA, RC, RS and RCS on the four real data sets. The running time includes both CPU and disk I/O time but not the pre-processing and shuffling time. The continuous variables of the data sets are normalized to the range of [0,1] and the categorical variables are represented by integers. The parameter values used are the same as the previous study [6]: the number of nearest neighbors to consider, k = 5 and the regular batch size = 1000. Two outlier percentages, 0.01% and 0.1% are tested.

There are three points to note. Firs tly, the speed of RCS, combining RC and RS, is as fast as 1.4 to 2.3 times that of ORCA. Secondly, the running time with the outlier percentage set to 0.1% is longer. The reason is that if the outlier percentage is higher, the cutoff v alue is smaller. More objects need to be compared with to find k neighbours within the smaller cutoff value. Hence, the running time is longer. Thirdly, in each of the outlier percentages, the percentages of time saved by RC shown in the fifth column have decreasing trends and the percentages saved by RS shown in the sixth column have an increasing trend. This is strongly related to the size of the data set and is studied in the next subsection. RC does not save any time in rows 5a, 3b and 4b because the start batch size exceeds the regular batch size. In these cases, the start batch size is set as the regular batch size and RC works as ORCA. 4.2 Varying the Data Size In this subsection, we investigate how the performance of RC, RS and RCS is affected by the data set size. The data set s of different sizes were generated by truncating the shuffled data set  X  X DD cup 1999 X . Two outlier percentages, 0.01% and 0.1% are tested. Only the results with the outlier percentage set to 0.01% are included because of the page limit. However, the results of these two outlier percentages are similar.
 Figure 4(a) shows the percentage of running time saved compared with ORCA. As the data size N increases, the time saving of RC decreases but the time saving of RS increases. The improvement of RCS, where RC and RS complement each other, is relatively insensitive to the increase in the data size.

The reason why the time saving of RC decreases with N is that the start batch size of RC equals 4 n =4  X N where n and  X  are the number and the percentage of outliers respectively. As N increases, the start batch size also increases. The difference between using a smaller start batch size and the original regular batch size decreases and so is the effect of RC.

To examine why RS improves more with more data, we decompose the total running time into CPU and disk I/O time. Fig. 4(b) shows that RS gives constant disk I/O time improvement over ORCA except the first two points of small data sizes. The disk I/O cost per object in the last data scan depends on the number of unprocessed objects remaining to process. The total numbers of data scans of the two small data sizes are 2 and 3, so the time improvement is seriously affected by the number of unprocessed objects remaining to process in the last data scan. RS cannot help reduce the disk I/O cost as it can refill no more unprocessed objects. Fig. 4(c) shows that RS consumes insignificantly small and constant extra CPU time over ORCA. Mor eover, we observed that the ratio of the disk I/O cost to the computational cost increases with N . Hence, when N increases, the saving of the disk I/O cost becomes more significant to the overall time saving. As a result, the overall time saving of RS increases with N . 4.3 Effects of the Parameters of RC and RS In this subsection, we study the effects of the start batch sizes and the ending time of RC (see Figs. 4(d) and (e)) as well as the start time and the frequency of space utilization of RS (see Fig. 4(f)) by varying their values. Here is the summary of the results. The effect of the start batch size shows no similar pattern between different data sets. The time saving does not change much with different ending time except that a very small start batch size like 100 is used. Using the ending time determined by RC (indicated by the big cross symbols in the figures), RC saves at least 98% of the optimal time saving. The percentage of time saved may be lower if RS starts too early such as after processing 0% and 1% of the data set. It is also very sensitive to the frequency of filling the memory space. However, if RS starts later such as after processing 20% of the data set, the percentage of time saved is less sensitive to this frequency. In this paper, we proposed two algorithms RC and RS using the following two techniques respectively: (i) faster cutoff update, and (ii) space utilization after pruning. Both techniques complement each other in reducing execution time. Our experimental results show that the speed of our RCS algorithm, which combines these two techniques, is as fast as 1.4 to 2.3 times that of ORCA. We investigated the saving in total running time, CPU time and disk I/O time of these two techniques against different data sets and data sizes. The experimental result indicates that the improvement of RCS is relatively insensitive to the increase in the data size.
 This work has been partially supported by grant PolyU 5174/04E (B-Q05W) from Hong Kong RGC, A-PA5S and A-SA14 from PolyU.

