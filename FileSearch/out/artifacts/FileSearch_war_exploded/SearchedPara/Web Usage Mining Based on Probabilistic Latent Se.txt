 The primary goal of Web usage mining is the discovery of patterns in the navigational behavior of Web users. Stan-dard approaches, such as clustering of user sessions and dis-covering association rules or frequent navigational paths, do not generally provide the ability to automatically character-ize or quantify the unobservable factors that lead to common navigational patterns. It is, the refore, necessary to develop techniques that can automatically discover hidden semantic relationships among users as well as between users and Web objects. Probabilistic Latent Semantic Analysis (PLSA) is particularly useful in this context, since it can uncover la-tent semantic associations among users and pages based on the co-occurrence patterns of these pages in user sessions. In this paper, we develop a unified framework for the dis-covery and analysis of Web navigational patterns based on PLSA. We show the flexibility of this framework in charac-terizing various relationships among users and Web objects. Since these relationships are measured in terms of probabil-ities, we are able to use probabilistic inference to perform a variety of analysis tasks such as user segmentation, page classification, as well as predictive tasks such as collabora-tive recommendations. We demonstrate the effectiveness of our approach through experiments performed on real-world data sets.
 H.2.8 [ Database Management ]: Database Applications X  Data Mining ; I.2.6 [ Artificial Intelligence ]: Learning; I.5.1 [ Pattern Recognition ]: Models X  Statistical Algorithms Web usage mining, User profiling, PLSA Copyright 2004 ACM 1-58113-888-1/04/0008 ... $ 5.00.
Web users exhibit different types of behavior depending on their information needs and their intended tasks. These tasks are captured implicitly by a collection of actions taken by users during their visits to a site. For example, in a dynamic application-based e-commerce Web site, user tasks may be reflected by sequences of interactions with Web ap-plications to search a catalog or to make a purchase. On the other hand, in an information intensive site, such as a portal or an online news source, user tasks may be reflected in a series of user clicks on a collection of Web pages with related content.

The identification of intended user tasks can shed light on various types of user navigational behaviors. For exam-ple, in an e-commerce site, there may be many user groups with different (but overlapping) behavior types. These may include visitors who engage in  X  X indow shopping X  by brows-ing through a variety of product pages in different categories; visitors who are goal-oriented showing interest in a specific product category; or visitors who tend to place items in their shopping cart, but not purchase those items. Identi-fying these user tasks and behavior types may, for example, allow a site to distinguish between those who show a high propensity to buy versus whose who don X  X . This, in turn, can lead to automatic tools that can tailor the content of pages for those users accordingly.

Web usage mining techniques [7, 34], which capture Web users X  navigational patterns, have achieved great success in various application areas such as Web personalization [22, 24, 25, 27], link prediction and analysis [20, 29], Web site evaluation or reorganization [31, 33], Web analytics and e-commerce data analysis [13, 19], Adaptive Web sites [26, 21], and Web pre-fetching [30, 28]. Most current Web usage mining systems use different data mining techniques, such as clustering, association rule mining, and sequential pat-tern mining to extract usage patterns from user historical navigational data. Generally these usage patterns are stan-dalone patterns at the pageview level. They, however, do not capture the intrinsic characteristics of Web users X  activ-ities, nor can they quantify the underlying and unobservable factors that lead to specific navigational patterns.
Thus, to better understand the factors that lead to com-mon navigational patterns, it is necessary to develop tech-niques that can automatically characterize the users X  un-derlying navigational objectives and to discover the hidden semantic relationships among users as well as between users and Web objects. A common approach for capturing the latent or hidden semantic associations among co-occurring objects is Latent semantic analysis (LSA) [10]. It is mostly used in automatic indexing and information retrieval [2], where LSA usually takes the (high dimensional) vector space representation of documents based on term frequency as a starting point and applies a dimension reducing linear pro-jection, such as Singular Value Decomposition (SVD) to gen-erate a reduced latent space representation.

Probabilistic latent semantic analysis (PLSA) models, pro-posed by Hofmann [14, 16], provide a probabilistic approach for the discovery of latent variables which is more flexible and has a more solid statistical foundation than the stan-dard LSA. The basis of PLSA is a model often referred to as the aspect model [17]. Assuming that there exist a set of hidden factors underlying the co-occurences among two sets of objects, PLSA uses Expectation-Maximization (EM) algorithm to estimate the probability values which measure the relationships between the hidden factors and the two sets of objects. Due to its great flexibility, PLSA has been widely and successfully used in variety of application do-main, including information retrieval [15], text learning [3, 4, 12, 18], and co-citation analysis [5, 6].

In this paper, we propose a Web usage mining approach based on the PLSA model. In the Web usage scenario, as in information retrieval, we have co-occurrence data which, in this case, is comprised of Web users and Web objects. In this paper, we refer to the hidden factors that represent the latent relationships among these entities as tasks . Thisisto emphasize the fact that these factors generally represent the navigational objectives of users in a Web site, as reflected in their interaction with the Web objects.

By applying the PLSA model, we can effectively identify and characterize these hidden factors, thus quantitatively measuring the relationships between Web users and tasks, as well as between Web objects and tasks. These relation-ships are measured in terms of probabilities, which, in turn, allow for the discovery of a variety of usage patterns by us-ing probabilistic inference. In this way, the model enables different types of analysis including the characterization of a task by a group of most related pages; the identification of prototypical users who perform a certain task; the identifica-tion of underlying tasks present in a specific user X  X  activity; and the characterization of user groups (or segments) that perform a similar set of tasks.

The primary contributions of th is paper are two-fold: first, we develop a general framework for discovery and analysis of Web navigational patterns based on the PLSA model. Sec-ondly, we show, in detail, how this model can be used to gen-erate various usage pattern, such as those described above, and point out possible applications, including a specific ap-proach for Web personalization based on the discovered user segments. Furthermore, we illustrate many of these usage patterns by providing several illustrative examples based on real Web usage data, and we quantitatively evaluate the ef-fectiveness of derived user segments.

The paper is organized as follows. In Section 2 we pro-vide an overview of Probabilistic Latent Semantic Analysis model as applied to Web usage data. We present the details of deriving various usage patterns based on the PLSA model in Section 3. Finally, we present our experiments and inter-pretation of the result in Section 4 and conclude the paper in Section 5.
The overall process of Web usage mining consists of three phrases: data preparation and transformation, pattern dis-covery, and pattern analysis. The data preparation phase transforms raw Web log data into transaction data that can be processed by various data mining tasks. In the pattern discovery phase, a variety of data mining techniques, such as clustering, association rule mining, and sequential pat-tern discovery can be applied to the transaction data. The discovered patterns may then be analyzed and interpreted for use in such applications as Web personalization.
The usage data preprocessing phase [8, 32] results in a set of n pageviews, P = { p 1 ,p 2 ,...,p n } and a set of m user sessions, U = { u 1 ,u 2 ,...,u m } .A pageview is an aggregate representation of a collection of Web objects (e.g. pages) contributing to the display on a user X  X  browser resulting from a single user action (such as a click through, product pur-chase, or database query). The Web session data can be conceptually viewed as an m  X  n session-pageview binary matrix UP =[ w ( u i ,p j )] m  X  n ,where w ( u i ,p j )representsthe weight of pageview p j in a user session u i .Theweightscan be binary, representing the existence or non-existence of the pageview in the session, or they may be a function of the occurrence or duration of the pageview in that session.
PLSA is a latent variable model which associates hidden (unobserved) factor variable Z = { z 1 ,z 2 , ..., z l } with obser-vations in the co-occurences data. In our context, each observation corresponds to an access by a user to a Web resource in a particular session which is represented as an entry of the m  X  n co-occurrence matrix UP .

The probabilistic latent factor model can be described as the following generative model: 1. select a user session u i from U with probability Pr ( u 2. pick a latent factor z k with probability Pr ( z k | u 3. generate a pageview p j from P with probability Pr ( p
As a result we obtain an observed pair ( u i ,p j ), while the latent factor variable z k is discarded. Translating this pro-cess into a joint probability model results in the following: where summing over all possible choices of z k from which the ob-servation could have been generated. Using Bayes X  rule, it is straightforward to transform the joint probability into:
Now, in order to explain a set of observations ( U, P ), we need to estimate the parameters Pr ( z k ), Pr ( u i | z k while maximizing the following likelihood L ( U, P )oftheob-servations,
Expectation-Maximization (EM) algorithm [11] is a well-known approach to performing maximum likelihood param-eter estimation in latent variable models. It alternates two steps: (1) an expectation (E) step where posterior probabil-ities are computed for latent variables, based on the current estimates of the parameters, (2) a maximization (M) step, re-estimate the parameters in order to maximize the expec-tation of the complete data likelihood.

The EM algorithm begins with some initial values of Pr ( z Pr ( u i | z k ), and Pr ( p j | z k ). In the expectation step we com-pute:
Pr ( z k | u i ,p j )= Pr ( z k )
In the maximization step, we aim at maximizing the ex-pectation of the complete data likelihood E ( L C ),
E ( L C )= while taking into account the constraints, l k =1 Pr ( z k on the factor probabilities, as well as the following con-straints on the two conditional probabilities: and
Through the use of Lagrange multipliers (see [16] for de-tails), we can solve the constraint maximization problem to get the following equations for re-estimated parameters:
Iterating the above computation of expectation and max-imization steps monotonically increases the total likelihood of the observed data L ( U, P ) until a local optimal solution is reached.

The computational complexity of this algorithm is O ( mnl ), where m is the number of user sessions, n is the number of pageviews, and l is the number of factors. Since the us-age observation matrix is, in general, very sparse, the mem-ory requirements can be dramatically reduced using efficient sparse matrix representation of the data.
One of the main advantages of PLSA model in Web usage mining is that it generates probabilities which quantify rela-tionships between Web users and tasks, as well as Web pages and tasks. From these basic probabilities, using probabilistic inference, we can derive relationships among users, among pages, and between users and pages. Thus this framework provides a flexible approach to model a variety of types of usage patterns. In this section, we will describe various us-age patterns that can be derived using the PLSA model. As noted before, the PLSA model generates probabilities Pr ( z k ), which measures the probability of a certain task is chosen; Pr ( u i | z k ), the probability of observing a user session given a certain task; and Pr ( p j | z k ), the probability of a page being visited given a certain task. Applying Bayes X  rule to these probabilities, we can generate the probability that a certain task is chosen given an observed user session: and the probability that a certain task is chosen given an observed pageview:
In the following, we discuss how these models can be used to derive different kinds of usage patterns. We will provide several illustrative examples of such patterns, from real Web usage data, in Section 4.
Capturing the tasks or objectives of Web users can help the analyst to better understand these users X  preferences and interests. Our goal is to characterize each task, represented by a latent factor, in a way that is easy to interpret. One possible approach is to find the  X  X rototypical X  pages that are strongly associated with a given task, but that are not commonly identified as part of other tasks. We call each such page a characteristic page for the task, denoted by p This definition of  X  X rototypical X  has two consequences, first, given a task, a page which is seldom visited cannot be a good characteristic page for that task. Secondly, if a page is frequently visited as part of a certain task, but is also commonly visited in other tasks, the page is not a good characteristic page. So we define characteristic pages for a task z k as the set of all pages, p ch , which satisfy: where  X  is a predefined threshold.

By examining the characteristic pages of each task, we can obtain a better understanding of the nature of these tasks. Characterizing tasks in this way can lead to several appli-cations. For example, most Web sites allow users to search for relevant pages using keywords. If we also allow users to explicitly express their intended task(s) (via inputting task descriptions or choosing from a task list), we can return the characteristic pages for the specified task(s), which are likely to lead users directly to their objectives.
A similar approach can be used to identify  X  X rototypical X  user sessions for each task. We believe that a user session involving only one task can be considered as the characteris-tic session for the task. So, we define the characteristic user sessions, u ch , for a task, z k , as sessions which satisfy where  X  is a predefined threshold.

When a user selects a task, returning such exemplar ses-sions can provide a guide to the user for accomplishing the task more efficiently. This approach can also be used in the context of collaborative filtering to identify the closest neighbors to a user based on the tasks performed by that user during an active session.
Identifying Web user groups or segments is an important problem in Web usage mining. It helps Web site owners to understand and capture users X  common interests and prefer-ences. We can identify user segments in which users preform common or similar task, by making inferences based on the estimated conditional probabilities obtained in the learning phase.

For each task z k , we choose all user sessions with prob-ability Pr ( u i | z k ) exceeding a certain threshold  X  to get a session set C . Since each user sessions, u , can also be repre-sented as a pageview vector, we can further aggregate these users sessions into a single pageviews vector to facilitate in-terpretation. The algorithm of generating user segments is as follows: 1. Input: Pr ( u i | z k ), user session-page matrix UP and 2. For each z k , choose all the sessions with Pr ( u i | 3. For each z k , compute the weighed average of all the 4. For each factor z k , output page vector v k . This page
We can sort the weights so that the top items in the list correspond to the most frequently visited pages for the user segment.

These user segments provide an aggregate representation of all individual users X  navigational activities in the a partic-ular group. In addition to their usefulness in Web analytics, user segments also provide the basis for automatically gen-erating item recommendations. Given an active user, we compare her activity to all user segments and find the most similar one. Then, we can recommend items (e.g., pages) with relatively high weights in the aggregate representation of the segment.

In Section 4, we conduct experimental evaluation of the user segments generated from two real Web sites.
To better understand the preferences and interests of a single user, it is necessary to identify the underlying tasks performed by the user. The PLSA model provides a straight-forward way to identify the underlying tasks in a given user session. This is done by examining Pr ( task | session ), which is the probability of a task being performed, given the ob-servation of a certain user session.

For a user session u , we select the top tasks z k with the highest Pr ( z k | u ) values, as the primary task(s) performed by this user.

For a new user session, u new , not appearing in the histor-ical navigational data, we can adopt a  X  X olding-in X  method as introduced in [16] to generate Pr ( task | session )viathe EM algorithm. In the E-step, we compute andintheM-step,wefix Pr ( p | z ) and only update Pr ( z | Here, w ( u new ,p ) represents the new user X  X  visit frequency on the specified page p . After we generate these probabilities, we can use the same method to identify the primary tasks for the new user session.

The identification of the primary tasks contained in user sessions can lead to further analysis. For example, after identifying the tasks in all user sessions, each session u can be transformed into a higher-level representation, where z i denotes task i and w i denotes Pr ( z i | u ). This, in turn, would allow the discovery and analysis of task-level usage patterns, such as determining which tasks are likely to be visited together, or which tasks are most (least) popular, etc. Such higher-level patterns can help site owners better evaluate the Web site organization.
Recent studies [23, 1, 9, 13] have emphasized the ben-efits of integrating semantic knowledge about the domain (e.g., from page content features, relational structure, or domain ontologies) in the Web usage mining process. The integration of content information about Web objects with usage patterns involving those objects provides two primary advantages. First, the semantic information provides addi-tional clues about the underlying reasons for which a user may or may not be interested in particular items. Secondly, in cases where little or no rating or usage information is available (such as in the case of newly added items, or in very sparse data sets), the system can still use the semantic information to draw reasonable conclusions about user inter-ests. The PLSA model described here also provides an ideal and uniform framework for integrating content and usage information.

Each pageview contains certain semantic knowledge rep-resented by the content information associated with that pageview. By applying text mining and information re-trieval techniques, we can represent each pageview as an attribute vector. Attributes may be the keywords extracted from the pageviews, or structured semantic attributes of the Web objects contained in the pageviews.

As before, we assume there exists a set of hidden factors z  X  Z = { z 1 ,z 2 , ..., z l } , each of which represents a  X  X eman-tic X  group of pages. They can be a group of pages which have similar functionalities for users performing a certain task, or a group of pages which contain similar content information or semantic attributes. However, now, in addition to the set of pageviews, P , and the set of user sessions, U ,wealso specify a set of t semantic attributes, A = { a 1 ,a 2 ,...,a To model the user-page observations, we use and to model the attribute-page observation, we use
These models can then be combined based on the common component Pr ( p j | z k ). This can be achieved by maximiz-ing the following log-likelihood function with a predefined weight  X  .

L = where  X  is used to adjust the relative weights of two obser-vations. The EM algorithm can again be used to generate estimates for Pr ( z k ), Pr ( u i | z k ), Pr ( p j | z k By applying probabilistic inferences, we can measure the re-lationships among users, pages, and attributes, thus we are able to answer questions such as,  X  X hat are the most im-portant attributes for a group of users? X , or  X  X iven an Web page with a specified set of attributes, will it be of interest to a given user? X , and so on.
In this section, we use two real data sets to perform exper-iments with our PLSA-based Web usage mining framework. We first provide several illustrative examples of characteriz-ing users X  tasks, as introduced in the previous section, and of identifying the primary tasks in an individual user ses-sion. We then perform two types of evaluations based on the generated user segments. First we evaluate individual user segments to determine the degree to which they rep-resent activities of similar user. Secondly, we evaluate the effectiveness of these user segments in the context of generat-ing automatic recommendations. In each case, we compare our approach with the standard clustering approach for the discovery of Web user segments.

In order to compare the clustering approach to the PLSA-based model, we adopt the algorithm presented in [24] for creating  X  X ggregate profiles X  based on session clusters. In the latter approach, first, we apply a multivariate clustering technique such as k -means to user-session data in order to obtain a set of user clusters TC = { c 1 ,c 2 , ..., c k } Figure 1: An example of the characteristic pages for the  X  X nline Application X  task in the CTI data aggregate representation, pr c , is generated for each cluster c as a set of pageview-weight pairs: where the significance weight, weight ( p, pr c ), is given by of pageview p of the user session u  X  c . Thus, each segment is represented as a vector in the pageview space. In the fol-lowing discussion, by a user segment, we mean its aggregate representation as a pageview vector. In our experiments, we use Web server log data from two Web sites. The first data set is based on the server log data from the host Computer Science department. This Web site provide various functionalities to different types of Web users. For example, prospective students can obtain pro-gram and admissions information or submit online applica-tions. Current students can browse course information, reg-ister for courses, make appointments with faculty advisors, and log into the Intranet to do degree audits. Faculty can perform student advising functions online or interact with the faculty Intranet. After data preprocessing, we identified 21,299 user sessions ( U ) and 692 pageviews ( P ), with each user session consisting of at least 6 pageviews. This data set is referred to as the  X  X TI data. X 
The second data set is from the server logs of a local af-filiate of a national real estate company. The primary func-tion of the Web site is to allow prospective buyers to visit various pages and information related to some 300 residen-tial properties. The portion of the Web usage data during the period of analysis contained approximately 24,000 user sessions from 3,800 unique users. During preprocessing, we recorded each user-property pai r and the corresponding visit frequency. Finally, the data was filtered to limit the final data set to those users that had visited at least 3 properties. In our final data matrix, each row represented a user vector with properties as dimensions and visit frequencies as the corresponding dimension values. We refer to this data set as the  X  X ealty data. X 
Each data set was randomly divided into multiple training and test sets to use with 10-fold cross-validation.
By conducting sensitivity analysis, we chose 30 factors in the case of CTI data and 15 factors for the Realty data. To avoid  X  X vertraining X , we implemented the  X  X empered EM X  algorithm [14] to train the PLSA model.
Figure 1 depicts an example of the characteristic pages for a specific discovered task in the CTI data. The first 6 Figure 2: An example of the characteristic pages for three tasks in the Realty data pages have the highest Pr ( page | task )  X  Pr ( task | page )values, thus are considered as the characteristic pages of this task. Observing these characteristic pages, we may infer that this task corresponds to prospective students who are completing an online admissions application. Here  X  X haracteristic X  has two implications. First, if a user wants to perform this task, he/she must visit these pages to accomplish his/her goal. Secondly, if we find a user session contains these pages, we can claim the user must have performed online application.
Some page may not be characteristic pages for the task, but may still be useful for the purpose of analysis. An ex-ample of such a page is the  X /news/ X  page which has a rel-atively high Pr ( page | task ) value, and a low Pr ( task value. Indeed, by examining the at the site structure, we found that this page serves as a navigational page, and it canleaduserstodifferentsect ions of the site to perform dif-ferent tasks (including the  X  X nline application X ). This kind of discovery can help Web site designer to identify the func-tionalities of pages and reorganize Web pages to facilitate users X  navigation.

Figure 2 identifies three tasks in the Realty data. In con-trast to the CTI data, in this data set the tasks represent common real estate properties visited by users, thus reflect-ing user interests in similar properties. The similarities are clearly observed when property attributes are shown for each characteristic page. From the characteristic pages of each task, we infer that Task 4 represents users X  interest in newer and more expensive properties, while Task 0 reflects interest in older and very low priced properties. Task 5 represents interest in properties midrange prices.

We can also identify  X  X rototypical X  users corresponding to specific tasks. An example of such a user session is depicted in Figure 3 corresponding to yet another task in the realty data which reflects interest in very high priced and large properties (task not shown here).

Our final example is this section shows how the prominent tasks contained in a given user session can be identified. Fig-ure 4 depicts a random user session from CTI data. Here we only show the tasks IDs which have the highest proba-bilities Pr ( task | session ). As indicated, the dominant tasks for this user session are Tasks 3 and 25. The former is, in fact, the  X  X nline application X  task discussed earlier, and the latter is a task that represents international students who Figure 3: An example of a  X  X rototypical X  user ses-sion Figure 4: An example of a identifying the prominent tasks within a given session are considering applying for admissions. It can be easily observed that, indeed, this session seems to identify an in-ternational student who, after checking admission and visa requirements, has applied for admissions online.
We used two metrics to evaluate the discovered user seg-ments. The first is called the Weighted Average Visit Per-centage (WAVP) [24]. WAVP allows us to evaluate each segment individually according to the likelihood that a user who visits any page in the segment will visit the rest of the pages in that segment during the same session. Specifically, let T be the set of transactions in the evaluation set, and for a segment s ,let T s denote a subset of T whose elements contain at least one page from s . The weighted average simi-larity to the segment s over all transactions is then computed (taking both the transactions and the segments as vectors of pageviews):
Note that a higher WAVP value implies better quality of a segment in the sense that the segment represents the actual behavior of users based on their similar activities. Figure 5: Comparison of user segments in the CTI site based on the Weighted Average Visit Percent-age; PLSA model v. k -means clustering Figure 6: Comparison of user segments in the real estate site on the Weighted Average Visit Percent-age; PLSA model v. k -means clustering
For evaluating the recommendation effectiveness, we use a metric called Hit Ratio in the context of top-N recom-mendation. For each user session in the test set, we took the first K pages as a representation of an active session to generate a top-N recommendation set. We then compared the recommendations with the pageview ( K +1) in the test session, with a match being considered a hit . We define the Hit Ratio as the total number of hits divided by the total number of user sessions in the test set. Note that the Hit Ratio increases as the value of N (number of recommenda-tions) increases. Thus, in our experiments, we pay special attention to smaller number recommendations (between 1 and 20) that result in good hit ratios.

In the first set of experiments we compare the WAVP values for the generated segments using the PLSA model and those generated by the clustering approach. Figures 5 and 6 depict these results for the CTI and Realty data sets, respectively. In each case, the segments are ranked in the decreasing order of WAVP. The results show clearly that the probabilistic segments based on the latent factor factors pro-vides a significant advantage over the clustering approach. Figure 7: Accuracy of page recommendations based on PLSA segments versus k -means segments in the CTI site Figure 8: Accuracy of property recommendations based on PLSA segments versus k -means segments in the real estate site
In the second set of experiments we compared the rec-ommendation accuracy of the PLSA model with that of k -means clustering segments. In each case, the recommenda-tions are generated according to the recommendation algo-rithm presented in Section 3.2. The recommendation accu-racy is measured based on hit ratio for different number of generated recommendations. These results are depicted in Figures 7 and 8 for the CTI and Realty data sets, respec-tively.

Again, the results show a clear advantage for the PLSA model. In most realistic situations, we are interested in a small, but accurate, set of recommendations. Generally, a reasonable recommendation set might contain 5 to 10 rec-ommendations. Indeed, this range of values seem to repre-sent the largest improvements of the PLSA model over the clustering approach.
To understand Web users X  preference and interests, it X  X  necessary to develop techniques that can automatically char-acterize users X  objectives (tasks) and discover the seman-tic relationships among users, users X  tasks, and Web ob-jects (Web pages). In this paper, we have developed a uni-fied framework for the discovery and analysis of Web nav-igational patterns based on PLSA. We show the flexibil-ity of this framework in characterizing various relationships among users, user tasks and Web objects. Since these re-lationships are measured in terms of probabilities, we are able to use probabilistic inference to perform a variety of analysis tasks such as task identification and user segmenta-tion, as well as predictive tasks such as collaborative recom-mendations. We have demonstrated the effectiveness of our approach through experiments performed on two real-world data sets.

In our future work in this area, we plan to conduct more research on using the combined PLSA framework (as in-troduced in Section 3.4) to discover various usage patterns which involve users, pageviews, and semantic attributes, thus capturing users X  preferences and interests at a deeper semantic level. [1] C. Anderson, P. Domingos, and D. Weld. Relational [2] M. Berry, S. Dumais, and G. OBrien. Using linear [3] T. Brants, F. Chen, and I. Tsochantaridis.
 [4] T. Brants and R. Stolle. Find similar documents in [5] D. Cohn and H. Chang. Probabilistically identifying [6] D. Cohn and T. Hofmann. The missing link: A [7] R. Cooley, B. Mobasher, and J. Srivastava. Web [8] R. Cooley, B. Mobasher, and J. Srivastava. Data [9] H. Dai and B. Mobasher. Using ontologies to discover [10] S. Deerwester, S. Dumais, G. Furnas, T. Landauer, [11] A. Dempster, N. Laird, and D. Rubin. Maximum [12] E. Gaussier, C. Goutte, K. Popat, and F. Chen. A [13] R. Ghani and A. Fano. Building recommender systems [14] T. Hofmann. Probabilistic latent semantic analysis. In [15] T. Hofmann. Probabilistic latent semantic indexing. [16] T. Hofmann. Unsupervised learning by probabilistic [17] T. Hofmann and J. Puzicha. Unsupervised learning [18] Y. Kim, J. Chang, and B. Zhang. a empirical study on [19] R. Kohavi, L. Mason, R. Parekh, and Z. Zheng. [20] N.Kushmerick,J.McKee,andF.Toolan.Towards [21] B. Mobasher, R. Cooley, and J. Srivastava. Creating [22] B. Mobasher, R. Cooley, and J. Srivastava. Automatic [23] B. Mobasher, H. Dai, T. Luo, Y. Sun, and J. Zhu. [24] B. Mobasher, H. Dai, and M. N. T. Luo. Discovery [25] O. Nasraoui, R. Krishnapuram, A. Joshi, and [26] M. Perkowitz and O. Etzioni. Adaptive web sites: [27] D. Pierrakos, G. Paliouras, C. Papatheodorou, and [28] J. Pitkow and P. Pirolli. Mining longest repeating [29] R. Sarukkai. Link prediction and path analysis using [30] S. Schechter, M. Krishnan, and M. D. Smith. Using [31] M. Spiliopoulou. Web usage mining for web site [32] M. Spiliopoulou, B. Mobasher, B. Berendt, and [33] R. Srikant and Y. Yang. Mining web logs to improve [34] J. Srivastava, R. Cooley, M. Deshpande, and P. Tan.
