 At present, most major app marketplaces perform ranking and recommendation based on search relevance features or marketplace X  X opularity X  X tatistics. For instance, they check similarity between app descriptions and user search queries, or rank-order the apps according to statistics such as number of downloads, user ratings etc. Rankings derived from such signals, important as they are, are insufficient to capture the dynamics of the apps ecosystem. Consider for example the questions: In a particular user context, is app A more likely to be launched than app B ? Or does app C provide comple-mentary functionality to app D ? Answering these questions requires identifying and analyzing the dependencies between apps in the apps ecosystem. Ranking mechanisms that re-flect such interdependences are thus necessary.

In this paper we introduce the notion of interoperability ranking for mobile applications. Intuitively, apps with high rank are such apps which are inferred to be somehow impor-tant to other apps in the ecosystem. We demonstrate how interoperability ranking can help answer the above questions and also provide the basis for solving several problems which are rapidly attracting the attention of both researchers and the industry, such as building personalized real-time app rec-ommender systems or intelligent mobile agents. We describe a set of methods for computing interoperability ranks and analyze their performance on real data from the Windows Phone app marketplace.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Experimentation mobile, apps, ranking, recommender systems
The growing trend of users consuming information through their mobile as opposed to desktop devices requires rethink-ing some of our approaches to ranking, information retrieval and recommendation. Identifying relevant content on the web is by now a relatively well understood process. In mo-bile, however, content is primarily consumed through a spe-cial and very diverse proxy -mobile apps.

The question of how to provide better utility to users now shifts from discovering and ranking relevant informa-tion to discovering and ranking relevant apps that can in turn provide relevant information. This comes with a num-ber of challenges. Firstly, app marketplaces have very lim-ited knowledge of what users ultimately get from an app which they have installed. Apps often aggregate and dis-play content in custom formats which are not transparent to the underlying platform so that proper inference could be conducted. A news app, for instance, may be launched only for the entertainment news which it aggregates or only for the financial news. Secondly, the well established notion of  X  X ubs and authorities X  from the web world [4] is also not explicitly present in the case of mobile application.
Here we demonstrate that a simple idea of how we think about apps can alleviate many problems. The idea is that we should not treat apps independently , but rather recognize that there are implicit and explicit depen-dencies between them. Identifying these dependencies can help us approach the above challenges. Indeed, if we find out that many users who launch the news app from the example subsequently launch stock trading apps, then it is reasonable to assume that they have consumed financial news through the news app and are now looking for some complementary functionality. We can say that the news app is acting as a hub to multiple financial apps and also that recommending entertainment apps to its user-base will be less relevant than recommending financial apps.

With the above in mind, we introduce the concept of in-teroperability ranking . The interoperability rank of app A quantifies the importance it has to other apps in the ecosys-tem. We propose several methods for computing the rank and compare their performance on data from the Windows Phone marketplace (Section 4). The presented methods uti-lize a graph structure derived from user-app interaction logs which reflects the existence of potential connections between apps on the marketplace. We call this graph structure inter-operability graph (Section 3). First, however, we point out a number of important areas where interoperability ranking can provide valuable input.
Marketplace recommendation . In the previous sec-tions we gave examples for this application of interoperabil-ity ranking.

Personalized real-time recommendation . With many users having tens of apps, navigating and finding the one to launch can be time consuming [3]. We can build instead a special app -a personalized recommender, which in real time infers and organizes the apps that are most likely to be launched next by a user. Inference can take into account the user context, e.g. time of day, location, apps being launched or other context [3]. Among these features, we find that what apps were launched is very predictive of what is to be launched next, which suggests inherent dependencies be-tween the apps which users utilize. Interoperability ranking can help identify these dependencies.

Intelligent agents . More generally, we can have an in-telligent agent system which predicts not only the next app to launch but a whole sequence of apps which are relevant and should be launched in a given context. This is similar to the  X  X ero-query mobile IR X  systems discussed in [5] which predict what mobile users might need next without such need to be explicitly expressed. Another such system with growing popularity is Google Now. Through a set of rules it decides when to launch one or more special apps ( cards ), e.g. driving to the airport can activate the X  X raffic card X  X hen the  X  X light status X  card. Again, interoperability ranking can help the agent identify and trigger the right apps for a task.
Dependencies between web pages are explicitly expressed through web links which allows for intuitively building the link graph . The graph is then utilized in different ranking algorithms such as PageRank [2] and HITS [4]. Identifying dependencies between apps, however, is harder as there are usually no explicitly declared links. There are emerging ef-forts that try to catalog apps which provide means for other apps to  X  X eep launch X  them [1]. Yet, the number of apps in these catalogs is very limited and one needs to resolve to proxies from which to implicitly infer such links.
A proxy that we explore here are user-app interaction logs and in particular launch patterns of the form ( A i ; A j A j has been launched immediately after app A i within a session. By session here we denote the time span between a user turning on their device until they turn it off. Listing (1) shows session logs for two users. The logs show user 1 having three sessions in which the launch patterns ( A 1 ; A 2 ) and ( A 2 ; A 3 ) appear twice ( session and session 12 ), and ( A 1 ; A 3 ) which appears once ( session From user 2 we obtain ( A 1 ; A 2 ) and ( A 2 ; A 1 ). Aggregating across the users results in the graphs from Figure 1.
In the interoperability graph each app A i is represented by a node and each pattern ( A i ; A j ) with a directed edge pointing to app A j . The edge carries the implicit notion of dependence -the user may have launched A j to complement functionally what they obtained from A i . In the first graph Fi gure 1: Interoperability graphs for the sessions from List-ing (1). Frequency based -each pattern is counted, user based -a pattern is counted only once per user. (frequency based) we count all patterns when computing the weights of the edges. Often though a user would launch A j after A i randomly or while habitually checking the same sequence of apps. We therefore construct the second graph (user based) where each pattern is counted only once per user. In the evaluation we work with this type of graphs.
We exemplify the idea with Figure 2. It shows the interop-erability graph for sessions observed over a week. Patterns ( A i ; A j ) which appear less than a predefined threshold have been removed. Zooming in the dense area of the graph re-veals many patterns. Some are clearly resulting from the vast adoption of certain apps (e.g. social clients) which makes seeing them sequentially in a session also common. Other patterns, however, are more interesting and reveal-ing. We show several among many which we could identify 1 Thicker edges indicates higher weights for the patterns.
Th e real name of the apps have been obfuscated and re-placed with names suggestive of their functionality.
The evaluation is done on four weeks of user-app in-teractions data from the Windows Phone app market-place. We train on week1 and test on week2, then we shift the window, train on week2 and test on week3 and so on. We have removed apps and app patterns which appear less than a predefined number of times in training. After thresh-olding the data still covers tens of thousands of apps from the marketplace. The comparison here is done primarily with the personalized recommender systems from Section 2 in mind. A user is recommended k = [1 ; 10] apps ( x -axis on Figure 3 and 4). We compute the click-through-rate (CTR) among the top-k recommendations, e.g. if we recommend three apps ( k = 3) in what percentage of the cases the user actually launched one of them. We report the mean and standard deviation of the results across the three test weeks.
This set of methods compute statistics and recommend apps from the entire marketplace . We assume that we do not know what apps each particular user has installed. These methods are good for suggesting new apps to users which others have found useful in a similar context. The performance is summarized in Figure 3.

Static Frequency Based (SFB) . In this method we simply compute the top-k most frequently launched apps during the training week and we always predict these apps. I.e. we compute the probability p ( A j ) of launching A j rank-order the apps based on that probability. This is sim-ilar to the  X  X op free/paid/popular X  etc. recommendations that every marketplace offers. In predicting that the user will click A j the method does not take into account what app was launched prior to that. Figure 3 shows that if we recommend ten apps we can achieve 9 : 8% accuracy, or in other words every one in ten launched apps is among the top ten most frequently used apps on the marketplace.
Static AppRank (SAR) . Similar to above the method computes a static rank for all apps on the marketplace and every time predicts the top-k apps with highest rank. The rank is computed similarly to PageRank [2] with a few differ-ences which we found suitable for the purpose of interoper-ability ranking. In particular, the rank is computed over the interoperability graph which has well defined edge weights while PageRank collapses multiple links from one page to another into a single link.

If M is the transition matrix of the interoperability graph, where m ji is the weight for a pattern ( A i ; A j ), then the rank is given by the solution of R = (1 d ) P + d  X  M R , where is the column stochastic matrix derived from M by normal-izing the columns to sum to one and replacing zero columns with the probability vector P , which defines how the residual flow gets distributed among all apps. In the classic PageR-ank, P is uniformly randomly distributed over all nodes. Here we notice that people often start their sessions with some preferred apps, e.g. social clients or games. To avoid overemphasizing such apps we assign a hundred times lower probability in P to apps which appear too frequently (above some threshold) in the start of sessions. The method im-proves slightly the SFB method (2 : 3% lift at k = 10, Fig-ure 3).

Conditioned Frequency Based (CFB) . Here if a user has launched A i we check the graph and predict A j for which the edge ( A i ; A j ) has the highest weight among all edges starting from A i . The method performs a maximum like-lihood estimate of p ( A j j A i ) -the conditional probability of launching A j given that A i has been launched prior to that.
The performance of the method (Figure 3) and its large superiority over the non-conditional SFB and SAR methods clearly demonstrates the ideas emphasized through the text -apps should not be treated independently as there is valuable predictive signal embedded in the interdependencies.
Conditioned AppRank (CAR) . The method takes the top-k predictions of the CFB method and reorders them ac-cording to their AppRank, computed as explained for the SAR method. As we can see from Figure 3 the static global rank does not improve on top of the maximum likelihood es-timate of CFB. For k = 10 both CFB and CAR are identical because CAR simply reorders the results of CFB. Therefore a click on any of the top ten apps recommended by CFB will also in the case of CAR too.

In this set of methods we are aware of the apps that a user has installed and the top-k recommendations are selected among them . While these methods pro-duce much higher ctr (Figure 4) and seem very suitable for the personalized recommender system discussed in Section 2 they would not select new apps which others have discovered to be useful in similar context. Hence they are not suitable for diversifying the portfolio of already installed apps which might be important while aiming at better user engagement.
User-User (UU) For each app A i that a user launches the method predicts the top-k most likely to be launched next apps based on training set frequencies for that same user. If we do not have an estimate for A i (the app was not launched during the training period) then we predict the top-k apps which the user has launched most frequently on train data. Ties are resolved by random selection. Every week there is a number of new users joining the marketplace. For these users we have no frequency estimates from the training data. For them predictions are made by choosing uniformly at random among their installed apps.

The method does not utilize the interoperability graph or any information from the marketplace. It is fully per-sonalized. One can correctly argue that instead of interde-pendence patterns it detects patterns where users habitually check the same apps. Its results are significantly better than the user agnostic methods -83 : 3% ctr among the top ten recommendations compared to 27 : 4% for the CFB method. User-Marketplace (UM) . Similar to the CFB method UM uses the interoperability graph to select the top-k A which maximize p ( A j j A i ) from marketplace perspective with the additional constraint that all selected A j should be among the installed apps by the user. The improvement compared to the results for the UU method (ctr 87 : 2% vs 83 : 3% for k = 10, Figure 4) comes to demonstrate again that app in-teroperability patterns do exist on the marketplace and can be very useful when there is no user history to compute user specific launch probabilities from.

User-UserMarketplace (UUM) . The method is a com-bination of the UU and UM methods. For a particular user who has launched A i , if we have an estimate of the top-k apps A j which maximize p ( A j j A i ) then we select them as prediction. Otherwise, we resolve to estimates from the marketplace similar to UM. The method as seen on Figure 4 improves between 1% 4% over the UM method, where the improvement is especially notable for small values of k .
The results remain stable across the three test weeks with the static user agnostic methods naturally showing less vari-ance than the conditional and the user aware methods.
We studied the concept of interoperability rank -a mea-sure of the importance of apps to other apps on the market-place. A number of methods were presented for computing the rank. Their performance was demonstrated on a large real world data set of user-app interaction logs. The results reveal that app interdependencies can provide valuable pre-dictive signal in identifying apps with complementary func-tionality which is in turn essential when aiming for better user utility and engagement. [1] http://handleopenurl.com/ . [2] S. Brin and L. Page. The anatomy of a large-scale [3] P. Coppola, V. Della Mea, L. Di Gaspero, D. Menegon, [4] J. M. Kleinberg. Authoritative sources in a hyperlinked [5] T. Sakai. Towards zero-click mobile ir evaluation:
