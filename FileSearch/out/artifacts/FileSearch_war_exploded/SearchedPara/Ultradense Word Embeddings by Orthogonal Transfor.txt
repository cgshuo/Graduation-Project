 Embeddings are useful for many tasks, including word similarity (e.g., Pennington et al. (2014)), named entity recognition (NER) (e.g., Collobert et al. (2011)) and sentiment analysis (e.g., Kim (2014), Kalchbrenner et al. (2014), Severyn and Moschitti (2015)). Embeddings are generic representations, containing different types of information about a word. Statistical models can be trained to make best use of these generic representations for a specific ap-plication like NER or sentiment analysis (Ebert et al., 2015).

Our hypothesis in this paper is that the informa-tion useful for any given task is contained in an ul-tradense subspace E u . We propose the new method D
ENSIFIER to identify E u . Given a set of word em-beddings, D ENSIFIER learns an orthogonal transfor-mation of the original space E o on a task-specific training set. The orthogonality of the transformation can be considered a hard regularizer.

The benefit of the proposed method is that embed-dings are most useful if learned on unlabeled cor-pora and performance-enhanced on a broad array of tasks. This means we should try to keep all informa-tion offered by them. Orthogonal transformations  X  X eorder X  the space without adding or removing in-formation and preserve the bilinear form, i.e., Eu-clidean distance and cosine. The transformed em-beddings concentrate all information relevant for the task in E u .

The benefits of E u compared to E o are (i) high-quality and (ii) efficient representations. (i) D ENSI -FIER moves non-task-related information outside of E u , i.e., into the orthogonal complement of E u . As a result, E u provides higher-quality representations for the task than E o ; e.g., noise that could result in overfitting is reduced in E u compared to E o . (ii) E u has a dimensionality smaller by a factor of 100 in our experiments. As a result, training statistical models on these embeddings is much faster. These models also have many fewer parameters, thus again helping to prevent overfitting, especially for complex, deep neural networks.

We show the benefits of ultradense representa-tions in two text polarity classification tasks (Sem-Eval2015 Task 10B, Czech movie reviews).

In the most extreme form, ultradense representa-tions  X  i.e., E u  X  have a single dimension. We ex-ploit this for creating lexicons in which words are annotated with lexical information, e.g., with senti-ment. Specifically, we create high-coverage lexicons with up to 3 million words (i) for three lexical prop-erties: for sentiment , concreteness and frequency ; (ii) for five languages: Czech , English , French , Ger-man and Spanish ; (iii) for two domains, Twitter and News , in a domain adaptation setup.

The main advantages of this method of lexicon creation are: (i) We need a training lexicon of only a few hundred words, thus making our method effec-tive for new domains and languages and requiring only a minimal manual annotation effort. (ii) The method is applicable to any set of embeddings, in-cluding phrase and sentence embeddings. Assum-ing the availability of a small hand-labeled lexicon, D
ENSIFIER automatically creates a domain depen-dent lexicon based on a set of embeddings learned on a large corpus of the domain. (iii) While the in-put lexicon is discrete  X  e.g., positive (+1) and nega-tive (-1) sentiment  X  the output lexicon is continuous and this more fine-grained assessment is potentially more informative than a simple binary distinction.
We show that lexicons created by D ENSIFIER beat the state of the art on SemEval2015 Task 10E (deter-mining association strength).

One of our goals is to make embeddings more interpretable. The work on sentiment, concrete-ness and frequency we present in this paper is a first step towards a general decomposition of embed-ding spaces into meaningful, dense subspaces . This would lead to cleaner and more easily interpretable representations  X  as well as representations that are more effective and efficient. forms the original word embedding space into a space in which certain types of information are represented by a small number of dimensions. Concretely, we learn Q such that the dimensions D s  X  { 1 ,...,d } of the resulting space corre-spond to a word X  X  sentiment information and the { 1 ,...,d }\ D s remaining dimensions correspond to non-sentiment information. Analogously, the sets of dimensions D c and D f correspond to a word X  X  con-creteness information and frequency information, respectively. In this paper, we assume that these properties do not correlate and therefore the ultra-dense subspaces do not overlap, e.g., D s  X  D c =  X  . However, this might not be true for other settings, e.g., sentiment and semantic information.

If e w  X  E o  X  R d is the original embedding of word w , the transformed representation is Qe w . We use  X  as a placeholder for s , c and f and call d  X  = | D  X  | the dimensionality of the ultradense sub-space of  X  . For each ultradense subspace, we create P  X   X  R d  X   X  d , an identity matrix for the dimensions in D  X   X  { 1 ,...,d } . Thus, the ultradense represen-tation u  X  w  X  E u  X  R d  X  of e w is defined as: 2.1 Separating Words of Different Groups We assume to have a lexicon resource l in which each word w is annotated for a certain information as either l  X  ( w ) = +1 (positive, concrete, frequent) or l  X  ( w ) =  X  1 (negative, abstract, infrequent). Let l  X  ( v ) 6 = l  X  ( w ) holds. We want to maximize: Thus, our objective is given by: or, equivalently, by: subject to Q being an orthogonal matrix. 2.2 Aligning Words of the Same Group Another goal is to minimize the distance of two words of the same group. Let L  X   X  be a set of word index pairs ( v,w ) for which l  X  ( v ) = l  X  ( w ) holds. In contrast to Eq. 3, we now want to minimize each distance. Thus, the objective is given by: subject to Q being an orthogonal matrix.

The intuition behind the two objectives is graphi-cally depicted in Figure 1. 2.3 Training We combine the two objectives in Eqs. 3/5 for each subspace, i.e., for sentiment, concreteness and fre-quency, and weight them with  X   X  and 1  X   X   X  . Hence, there is one hyperparameter  X   X  for each subspace. We then perform stochastic gradient descent (SGD). Batch size is 100 and starting learning rate is 5 , mul-tiplied by . 99 in each iteration. 2.4 Orthogonalization Each step of SGD updates Q . The updated matrix reorthogonalize Q 0 in each step based on singular value decomposition: where S is a diagonal matrix, and U and V are or-thogonal matrices. The matrix is the nearest orthogonal matrix to Q 0 in both the 2-norm and the Frobenius norm (Fan and Hoffman, 1955). (Formalizing our regularization directly as projected gradient descent would be desirable. How-ever, gradient descent includes an additive operation and orthogonal matrices are not closed under sum-mation.)
SGD for this problem is sensitive to the learning rate. If the learning rate is too large, a large jump results and the reorthogonalized matrix Q basically is a random new point in the parameter space. If the learning rate is too small, then learning can take long. We found that our training regime of start-ing at a high learning rate (5) and multiplying by .99 in each iteration is effective. Typically, the cost initially stays about constant (random jumps in pa-rameter space), then cost steeply declines in a small number of about 50 iterations (sweet spot); the curve flattens after that. Training Q took less than 5 min-utes per experiment for all experiments in this paper. For lexicon creation, the input is a set of embed-dings and a lexicon resource l , in which words are annotated for a lexical information such as senti-ment, concreteness or frequency. D ENSIFIER is then trained to produce a one-dimensional ultra-dense subspace . The output is an output lexicon . It consists of all words covered by the embedding set, each associated with its one-dimensional ultra-dense subspace representation (which is simply a real number), an indicator of the word X  X  strength for that information.

The embeddings and lexicon resources used in this paper cover five languages and three domains (Table 1). The Google News embeddings for En-publicly available. We use word2vec to train 400-dimensional embeddings for English on a 2013 Twitter corpus of size 5.4  X  10 9 . For Czech, German and Spanish, we train embeddings on web data of sizes 3.3, 8.0 and 3.8  X  10 9 , respectively. We use the following lexicon resources for sentiment: SubLex 1.0 (Veselovsk  X  a and Bojar, 2013) for Czech; WHM for English [the combination of MPQA (Wilson et al., 2005), Opinion Lexicon (Hu and Liu, 2004) and NRC Emotion lexicons (Mohammad and Tur-ney, 2013)]; FEEL (Abdaoui et al., 2014) for French; German Polarity Clues (Waltinger, 2010) for Ger-man; and the sentiment lexicon of P  X  erez-Rosas et al. (2012) for Spanish. For concreteness, we use BWK, a lexicon of 40,000 English words (Brysbaert et al., 2014). For frequency, we exploit the fact that word2vec stores words in frequency order; thus, the ranking provided by word2vec is our lexicon re-source for frequency.

For a resource/embedding-set pair ( l,E ) , we in-tersect the vocabulary of l with the top 80,000 words of E to filter out noisy, infrequent words that tend to have low quality embeddings and we do not want them to introduce noise when training the transfor-mation matrix.

For the sentiment and concreteness resources, l  X  ( w )  X  { X  1 , 1 } for all words w covered. We cre-ate a resource l f for frequency by setting l f ( w ) = 1 for the 2000 most frequent words and l f ( w ) =  X  1 for words at ranks 20000-22000. 1000 words ran-domly selected from the 5000 most frequent are sions D s , D c and D f to represent sentiment, con-creteness and frequency, respectively, and arbitrar-ily set (i) D c = { 11 } for English and D c =  X  for the other languages since we do not have concrete-ness resources for them, (ii) D s = { 1 } and (iii) D f = { 21 } . Referring to the lines in Table 1, we then learn six orthogonal transformation matrices Q : for CZ-web (1), DE-web (2), ES-web (3), FR-web (4, 9), EN-twitter (5) and EN-news (6, 7, 8). 4.1 Top-Ranked Words Table 2 shows the top 10 positive/negative words (i.e., most extreme values on dimension D s ) when we apply the transformation to the corpora EN-twitter, EN-news and DE-web and the top 10 con-crete/abstract words (i.e., most extreme values on di-mension D c ) for EN-news. For EN-twitter (leftmost double column), the selected words look promising: they contain highly domain-specific words such as hashtags (e.g., #happy). This is surprising because there is not a single hashtag in the lexicon resource WHM that D ENSIFIER was trained on. Results for the other three double columns show likewise ex-treme examples for the corresponding information and language. This initial evaluation indicates that our method effectively learns high quality lexicons for new domains. Figure 3 depicts values for se-lected words for the three properties. Illustrative ex-amples are  X  X rother X  /  X  X rotherhood X  for concrete-ness and  X  X ate X  /  X  X ove X  for sentiment. 4.2 Quality of Predictions Table 1 presents experimental results. In each case, we split the resource into train/test, except for Twit-ter where we used the trial data of SemEval2015 Task 10E for test. We train D ENSIFIER on train and compute Kendall X  X   X  on test. The size of the lexicon resource has no big effect; e.g., results for Spanish (small resource; line 3) and French (large resource; line 4) are about the same. See Section 5.2 for a more detailed analysis of the effect of resource size.
The quality of the output lexicon depends strongly on the quality of the underlying word embeddings; e.g., results for French (small embedding train-ing corpus; line 4) are worse than results for En-glish (large embedding training corpus; line 6) even though the lexicon resources have comparable size.
In contrast to sentiment/concreteness,  X  values for frequency are low (lines 8-9). For the other three languages we obtain  X   X  [ . 34 ,. 46] for frequency (not shown). This suggests that word embeddings represent sentiment and concreteness much better than frequency. The reason for this likely is the learning objective of word embeddings: modeling the context. Infrequent words can occur in frequent contexts. Thus, the frequency information in a sin-gle word embedding is limited. In contrast negative words are likely to occur in negative contexts.
The nine output lexicons in Table 1  X  each a list of words annotated with predicted strength on one of three properties  X  are available at www.cis.lmu. de/  X  sascha/Ultradense/ . 4.3 Determining Association Strength We also evaluate lexicon creation on SemEval2015 Task 10E. As before, the task is to predict the sen-timent score of words/phrases. We use the trial data of the task to tune the hyperparameter,  X  s = .4. Out-of-vocabulary words were predicted as neu-tral (7/1315). Table 3 shows that the lexicon com-puted by D ENSIFIER (line 5, Table 1) has a  X  of .654 (line 6, column all ), significantly better than all other systems, including the winner of SemEval 2015 (  X  = .626, line 1). D ENSIFIER also beats Sen-timent140 (Mohammad et al., 2013), a widely used semi-automatic sentiment lexicon. The last column is  X  on the intersection of D ENSIFIER and Senti-ment140. It shows that D ENSIFIER again performs significantly better than Sentiment140. 4.4 Text Polarity Classification We now show that ultradense embeddings decrease model training times without any noticeable de-crease in performance compared to the original em-beddings. We evaluate on SemEval2015 Task 10B, classification of Twitter tweets as positive, nega-tive or neutral. We reimplement the linguistically-informed convolutional neural network (lingCNN) of Ebert et al. (2015) that has close to state-of-the-art performance on the task. We do not use sentence-based features to focus on the evaluation of the em-beddings. We initialize the first layer of lingCNN, the embedding layer, in three different ways: (i) 400-dimensional Twitter embeddings (Section 3); (ii) 40-dimensional ultradense embeddings derived from (i); (iii) 4-dimensional ultradense embeddings derived from (i). The objective weighting is  X  s = . 4 , optimized on the development set.

The embedding layer converts a sentence into a matrix of word embeddings. We also add linguistic features for words, such as sentiment lexicon scores. The combination of embeddings and linguistic fea-tures is the input for a convolution layer with filters spanning 2-5 words (100 filters each). This is fol-lowed by a max pooling layer, a rectifier nonlinear-ity (Nair and Hinton, 2010) and a fully connected softmax layer predicting the final label. The model is trained with SGD using AdaGrad (Duchi et al., 2011) and ` 2 regularization (  X  = 5  X  10  X  5 ). Learn-ing rate is 0 . 01 . Mini-batch size is 100. We follow the official guidelines and use the Sem-Eval2013 training and development sets as train-ing set, the SemEval2013 test set as development set and the SemEval2015 test set to report final scores (Nakov et al., 2013; Rosenthal et al., 2015). We report macro F 1 of positive and negative classes (the official SemEval evaluation metric) and accu-racy over the three classes. Table 4 shows that 40-dimensional ultradense embeddings perform almost as well as the full 400-dimensional embeddings (no significant difference according to sign test). Train-ing time is shorter by a factor of 21 (85/4 exam-ples/second). The 4-dimensional ultradense embed-dings lead to only a small loss of 1.5% even though the size of the embeddings is smaller by a factor of 100 (again not a significant drop). Training time is shorter by a factor of 44 (178/4).

We perform the same experiment on CSFD, a Czech movie review dataset (Habernal et al., 2013), to show the benefits of ultradense embeddings for a low-resource language where only one rather small lexicon is available. As original word embed-dings we train new 400 dimensional embeddings on a large Twitter corpus (3.3  X  10 9 tokens). We use D ENSIFIER to create 40 and 4 dimensional embeddings out of these embeddings and SubLex 1.0 (Veselovsk  X  a and Bojar, 2013). Word polarity features are also taken from SubLex. A simple bi-nary negation indicator is implemented by searching for all tokens beginning with  X  X e X . Since that in-cludes superlative forms having the prefix  X  X ej X , we remove them with the exception of common negated words, such as  X  X ejsi X   X   X  X ou are not X . We randomly split the 91,000 dataset instances into 90% train and 10% test and report accuracy and macro F 1 score over all three classes.

Table 4 shows that what we found for English is also true for Czech. There is only a small perfor-mance drop when using ultradense embeddings (not significant for 40 dimensional embeddings) while the speed improvement is substantial. In this section, we analyze two parameters: size of ultradense subspace and size of lexicon resource. We leave an evaluation of another parameter, the size of the embedding training corpus, for future work, but empirical results suggest that this corpus should ideally have a size of several billion tokens. 5.1 Size of Subspace With the exception of the two text polarity classifi-cation experiments, all our subspaces have dimen-sionality d  X  = 1 . The question arises: does a one-dimensional space perhaps have too low a capacity to encode all relevant information and could we fur-ther improve our results by increasing the dimen-sionality of the subspace to values d  X  &gt; 1 ? The lexicon resources that we train and test on are all bi-nary; thus, if we use values d  X  &gt; 1 , then we need to map the subspace embeddings to a one-dimensional scale for evaluation. We do this by training, on the train part of the resource, a linear transformation from the ultradense subspace to the one-dimensional scale (e.g., to the sentiment scale).

Figure 2 compares different values of d s for three different types of subspaces in this setup, i.e., the setup in which the subspace representa-tions are mapped via linear transformation to a one-dimensional sentiment value: (i) Random : we take the first d s dimensions of the original embeddings; (ii) PCA : we compute a PCA and take the first d s principal components; (iii) Ultradense subspace of dimensionality d s . We use the word embeddings and lexicon resources of line 6 in Table 1. For ran-dom, the performance starts dropping when the sub-space is smaller than 200 dimensions. For PCA, the performance is relatively stable until the subspace becomes smaller than 100 . In contrast, ultradense subspaces have almost identical performance for all values of d s , even for d s = 1 . This suggests that a single dimension is sufficient to encode all senti-ment information needed for sentiment lexicon cre-ation. However, for other sentiment tasks more di-mensions may be needed, e.g., for modeling differ-ent emotional dimensions of polarity: fear, sadness, anger etc.

An alternative approach to create a low-dimensional space is to simply train low-dimensional word2vec embeddings. The following experiment suggests that this does not work very well. We used word2vec to train 60-dimensional twitter embeddings with the same settings as on line 5 in Table 1. While the correlation for 400-dimensional embeddings shown in Table 1 is .661, the correlation of 60-dimensional embeddings is only .568. Thus, even though we show that the information in 400-dimensional embeddings that is relevant for sentiment can be condensed into a single dimension, hundreds of dimensions seem to be needed if we use word2vec to collect sentiment information. If we run word2vec with a small dimensionality, only a subset of available sentiment information is  X  X arvested X  from the corpus. 5.2 Size of Training Resource Next, we analyze what size of training resource is required to learn a good transformation Q . Labeled resources covering many words may not be available or suffer from lack of quality. We use the settings of lines 6 (sentiment) and 7 (concreteness) in Table 1. Figure 2 shows that a small training resource of 300 entries is sufficient for high performance. This sug-gests that D ENSIFIER can create a high quality out-put lexicon for a new language by hand-labeling only 300 words; and that a small, high-quality re-source may be preferable to a large lower-quality re-source (semi-automatic or out of domain).
 To provide further evidence for this, we train D
ENSIFIER on only the trial data of SemEval2015 task 10E. To convert the continuous trial data to bi-nary  X  1 / 1 labels, we discard all words with sen-timent values between  X  0 . 5 and 0 . 5 and round the remaining values, giving us 39 positive and 38 neg-ative training words. The resulting lexicon has  X  = .627 (Table 3, line 8). 4 This is worse than  X  = .654 (line 6) for the setup in which we used sev-eral large resources, but still better than all previ-ous work. This indicates that D ENSIFIER is espe-cially suited for languages or domains for which lit-tle training data is available. To the best of our knowledge, this paper is the first to train an orthogonal transformation to reorder word embedding dimensions into ultradense subspaces. However, there is much prior work on postprocess-ing word embeddings.

Faruqui et al. (2015) perform postprocessing based on a semantic lexicon with the goal of fine-tuning word embeddings. Their transformation is not orthogonal and therefore does not preserve dis-tances. They show that their approach optimizes word embeddings for a given application, i.e., word similarity, but also that it worsens them for other ap-plications like detecting syntactic relations. Faruqui et al. (2015) X  X  approach also does not have the bene-fit of ultradense embeddings, in particular the benefit of increased efficiency.

In a tensor framework, Rothe and Sch  X  utze (2015) transform the word embeddings to sense (synset) embeddings. In their work, all embeddings live in the same space whereas we explicitly want to change the embedding space to create ultradense embed-dings with several desirable properties.

Xing et al. (2015) restricted the work of Mikolov et al. (2013) to an orthogonal transformation to en-sure that normalized embeddings stay normalized. This transformation is learned between two embed-ding spaces of different languages to exploit simi-larities. They normalized word embeddings in a first step, something that did not improve our results.
As a reviewer pointed out, our method is also related to Oriented PCA (Diamantaras and Kung, 1996). However in contrast to PCA a solution for Oriented PCA is not orthogonal.

Sentiment lexicons are often created semi-automatically, e.g., by extending manually labeled seed sets of sentiment words or adding for each word its syno-/antonyms. Alternatively, words frequently cooccurring with a seed set of manually labeled sen-timent words are added (Turney, 2002; Kiritchenko et al., 2014). Heerschop et al. (2011) used Word-Net together with a PageRank-based algorithm to propagate the sentiment of the seed set to unknown words. Scheible (2010) presented a semi-automatic approach based on machine translation of sentiment lexicons. The winning system of SemEval2015 10E (Amir et al., 2015) was based on structured skip-gram embeddings with 600 dimensions and support vector regression with RBF kernels. Hamdan et al. (2015), the second ranked team, used the average of six sentiment lexicons as a final sentiment score, a method that cannot be applied to low resource lan-guages. We showed that the lexicons created by D
ENSIFIER achieve better performance than other semi-automatically created lexicons.

Tang et al. (2014b) train sentiment specific em-beddings by extending Collobert &amp; Weston X  X  model and Tang et al. (2014a) X  X  skip-gram model. The first model automatically labels tweets as posi-tive/negative based on emoticons, a process that can-not be easily transferred to other domains like news. The second uses the Urban Dictionary to expand a small list of 350 sentiment seeds. In our work, we showed that a training resource of about the same size is sufficient without an additional dictionary. D
ENSIFIER differs from this work in that it does not need a text corpus, but can transform existing, pub-licly available word embeddings. D ENSIFIER is in-dependent of the embedding learning algorithm and therefore extensible to other word embedding mod-els like GloVe (Pennington et al., 2014), to phrase embeddings (Yu and Dredze, 2015) and even to sen-tence embeddings (Kiros et al., 2015). We have introduced D ENSIFIER , a method that is trained to focus embeddings used for an application to an ultradense subspace that contains the informa-tion relevant for the application. In experiments on SemEval, we demonstrate two benefits of the ultra-dense subspace. (i) Information is preserved even if we focus on a subspace that is smaller by a fac-tor of 100 than the original space. This means that unnecessary noisy information is removed from the embeddings and robust learning without overfitting is better supported. (ii) Since the subspace is 100 times smaller, models that use the embeddings as their input representation can be trained more effi-ciently and have a much smaller number of parame-ters. The subspace can be learned with just 80  X  300 training examples, achieving state-of-the-art results on lexicon creation.

We have shown in this paper that up to three or-thogonal ultradense subspaces can be created. Many training datasets can be restructured as sets of simi-lar/dissimilar pairs. For instance, in part-of-speech tasks verb/verb pairs would be similar, verb/noun pairs dissimilar. Hence, our objective is widely ap-plicable. In future work, we will explore the possi-bility of factoring all information present in an em-bedding into a dozen or so orthogonal subspaces. This factorization would not change the information embeddings contain, but it would make them more compact for any given application, more meaningful and more interpretable.

The nine large D ENSIFIER lexicons shown in Ta-
Acknowledgments. We gratefully acknowledge the support of DFG: grant SCHU 2246/10-1.
