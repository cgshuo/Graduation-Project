 Many important search tasks require multiple search sessions to complete. Tasks such as travel planning, large purchases, or job searches can span hours, days, or even weeks. Inevitably, life interferes, requiring the searcher either to recover the  X  X tate X  of the search manually (most common), or plan for interruption in advance (unlikely). The goal of this work is to better understand , characterize , and automatically detect search tasks that will be continued in the near future. To this end, we analyze a query log from the Bing Web search engine to identify the types of intents , topics , and search behavior patterns associated with long-running tasks that are likely to be continued. Using our insights, we deve l-op an effective prediction algorithm that significantly outperforms both the previous state-of -the-art method, and even the ability of human judges, to predict future task continuation. Potential appl i-cations of our techniques would allow a search engine to pre-emptively  X  X ave state X  for a searcher (e.g., by caching search r e-sults), perform more targeted personalization, and otherwise better support the searcher experience for interrupted search tasks. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  search process; selection process Search session analysis; Search behavior; Personalization. As Web search becomes increasingly important for planning and decision making, the complexity and scope of search tasks per-formed on search engines is increasing. Search engines are now often used for tasks such as travel planning, job hunting, or real estate searching. However, these tasks require significantly more effort and time to complete [10] [21] [24] [25] , potentially spanning days, weeks, or even months. While existing commercial Web search engines such as Bing and Google now provide tools to help users maintain and manage their search histories, the support they provide is not sufficient and the tools are not specifically designed to allow searchers to resume tasks that may been interrupted. A challenge for search engines is to detect when a searcher is performing a long-running search task and predict whether they will continue it in the future. To this end, we analyze a query log from Bing to understand the types of intents , motivations , topics , and search behaviors associated with long-running tasks that are likely to be continued. Specifically, we try to understand search task continuation by analyzing tasks that were and were not co n-tinued by over a thousand Web searchers. For example, consider the task of planning a wedding. The search er might begin by checking recommended venues and their availabilities. However, at that point the task could be interrupted , as it requires checking dates and venues with the immediate fam i-ly. When the task is continued the next day, the searcher has to re-start from the beginning, unless the user planned for this event, and manually saved the most promising intermediate results. I n-deed, there has been previous work on system support that lets users explicitly record promising content [10] [27] . However, a perfect search engine could save the user the trouble if it could reliably detect that a suspended search session is likely to be co n-tinued at a later time. While previous studies have considered long running tasks spa n-ning multiple sessions (e.g., [10][21] [24] [25] ), we dive deeper into the problem of task continuation to analyze the intent, mot i-vation, and topics of these tasks. The more extensive analysis we perform allows for a fuller understanding of which tasks are most commonly resumed, in turn resulting in more accurate task co n-tinuation prediction. Potential applications include pre-emptively  X  X aving state X  for a searcher (e.g., by caching search results), more targeted personalization, and otherwise better supporting the searcher experience for long-running searches. More formally, our problem is predicting task continuation : This problem is challenging, since it requires a search engine to make predictions about the kinds of tasks that tend to be conti n-ued, which intuitively would require substantial knowledge about the world. Yet, this work presents techniques to make these pr e-dictions automatically as well as, and often better than, exper i-enced human annotators. Our contributions are threefold:  X  A large-scale characterization of the intents, motivations, and topics associated with long-running search tasks (Section 3).  X  Novel features to effectively capture these characteristics for automated prediction of task continuation (Section 4).  X  Techniques for accurate prediction of continuation that outpe r-form both a state-of -the-art automatic baseline and human pr e-dictions, coupled with the analysis of the most effective fe a-tures used by the predictive algorithms (Section 5). Next, we present related work to put our contributions in context. Prior research that relates to what we describe in this paper falls into four main areas: (i) behavioral analysis and modeling of search, (ii) understanding search intent, (iii) analysis of cross-session tasks, and (iv) task switching and interruptions. Search behavior has been studied intensely in recent years. Log data from search engines have proven to be extremely valuable in studying how people search in naturalistic settings across a wide variety of different search intents. Most previous work has focused on search behavior analysis and prediction within a single search session [1] [7] [42] , and related queries within a session can be part of a search goal [16] [19] , which try to represent the more abstract concept of search intent given only observable events. However, there is growing interest in using long-term search log data to build models of users X  interests [39] and improve search result ranking [34] . An important part of representing search intent is understanding the various types of search tasks and the different motivations that searchers may have for pursuing their information goals. Earlier work on understanding search behavior focused on classifying queries into high-level search goals, such as informational, navigational and transactional [6] [8] [32] . Kellar et al. [20] con-ducted a field study in which they logged detailed Web usage and asked participants to provide task categorizations of their Web usage based on the following categories: fact finding, information gathering, browsing, and transactions. They showed differences in search behavior per task type. In particular, information gathering tasks were the most complex; participants spent more time co m-pleting this task, viewed more pages, and used the Web browser functions most heavily during this task. Li and Belkin [23] review and discuss previously-proposed task classifications and develop a faceted classification that can be used to describe searchers X  work tasks and information search tasks. They identify essential facets and categorize them into generic task facets (e.g., source, product, and goal) and common task attributes (e.g., task characteristics and user perceptions). Rather than characterizing the nature of the search intent, Radlinski et al. [30] model search intent from qu e-ries and clicks in a way that could be directly consumed by search engines . Goals and related constructs (such as search intent) have also been widely studied in psychological research. Austin and Vancouver [4] review the theoretical development of the structure and properties of goals, goal establishment and striving processes, and goal-content taxonomies, which we use to motivate the sele c-tion of task dimensions to analyze. In fact, to our knowledge, our research is the first attempt to bring theory of motivation from psychology to bear on search intent analysis. In this paper we focus on tasks extending across multiple sessions. Search behavior can be analyzed over time to identify queries that express the same underlying information need. Previous work has tried to automatically identify queries on the same task. Mei et al . [26] proposed a framework to study sequences of search activities and focused on simple prediction and classification tasks, ranging from predicting whether the next click will be on an algorithmic result to segmenting the query stream into goals and missions. Teevan et al . [37] showed, via query log analysis, that nearly 40% of queries were attempts to re-find previously encountered results. Aula et al . [3] studied the search and information re-access strategies of experienced Web users using a survey. They found that people often have difficulty remembering the queries they used originally to discover information of interest. MacKay and Watters [25] explored a variety of Web-based information seeking tasks and found that almost 60% of complex information gathering tasks continued across sessions. Liu and Belkin [24] examined the structure (parallel or dependent) of tasks that extend across different search sessions. Jones and Klinker [19] proposed methods to partition a query stream into research missions and goals, where each mission corresponds to a set of related information needs and may include multiple search goals. Morris et al. [27] developed SearchBar , a system that proactively and persistently stores query histories, browsing histories, and users X  notes and ratings. SearchBar supports multi-session investigations by assisting with task context resumption and information re-finding. Donato et al. [10] developed SearchPad , a system that automatically identifies research missions and presents a search workspace comprising previous queries and results related to the mission. SearchPad uses measures of topic coherence between pairs of consecutive queries and user engagement to identify such research missions. This work was further extended by Aiello et al. [2] to group queries into mission-coherent clusters based on searcher behavior. However, none of the research described so far specifically addressed the important challenge of predicting search task continuation. The most similar research to this paper is that of Kotov et al. [21] . In that paper, the authors describe research on modeling cross-session information needs, and address the challenge of identifying all previous queries in a user X  X  search history on the same task as the current query, and predicting whether a user will return to the task in future sessions. Kotov et al. develop ed classifiers for these two tasks and through evaluation using labeled data from search logs showed that their classifiers can perform both tasks effectively. We use these classifiers as a baseline for some of the analysis presented later in the paper. Also relevant to this work is previous research on task switching and interruptions. Multi-tasking and external factors such as interruptions have been previously associated with prolonged search tasks. Spink [35] studied the multi-tasking behavior of a single searcher in a public library using diary, observation and interviews and found that switching between tasks was common. On the basis of th at study, she then developed a model of info r-mation multi-tasking and information task switching. Czerwinski et al. [9] present the findings of a week-long diary study of task interleaving admist interruptions, following eleven information workers in a non-search setting. They show tha t task complexity, task duration, length of absence, interruption count, and task type influence the perceived difficulty of switching back to tasks with participants reporting that it was most difficult to recommence complex tasks. The features we devise to represent tasks adapt and operationalize these ideas. The research presented in this paper extends previous work in a number of ways. First, we perform a detailed descriptive analysis of the cross-session search tasks that maps task intents and motivations derived from the information science and psychology literatures to evidence of task continuation mined from search logs and labeled by trained human annotators. Second, we propose new features to model characteristics of cross-session search tasks, focusing on future task continuation, using features of search behavior mined from annotated log data. Third, we show that these features can improve continuation modeling and prediction over a previously-reported state-of -the-art baseline, and even over experienced human annotators attempting to perform the same prediction task. This section describes the data collection (Section 3.1), the human data annotation for the dimensions hypothesized to be related to search task continuation (Section 3.2), and presents analysis of task characteristics (Sections 3.4-3.5) based on both manual ann o-tation of the tasks and an extended set of searc h log data. The data were gathered from the Microsoft Bing commercial Web search engine by sampling a set of sessions over a one-week per i-od for more than 1,000 users. Similar to [21] , we study what have been previously defined as  X  X arly dominant X  tasks identified for each user. An early dominant task is defined as having at least two distinct queries issued within a two-day period at the beginning of the week of interest. Some of these tasks are continued later du r-ing the week, while others are not . The data that we used for our study are summarized in Table 1. Additionally, the data above were augmented by extracting up to an additional two weeks of prior history for each user in the sa m-ple, from the two weeks immediately before the week of interest. This history contained search sessions determined based on a 30-minute inactivity timeout [40] , as well as the queries and URLs issued and visited. This allowed us to study the potential for utili z-ing additional profile information to predict task continuation. We annotated the characteristics of early dominant search tasks (defined above) according to a range of dimensions derived from information and cognitive science literatures, following the proc e-dure in Section 3.3. Our goals were: (1) to analyze the relationship of task characteristics to task continuation and (2) to learn to a u-tomatically identify these characteristics for better search contin u-ation modeling and prediction. In particular, we wished to invest i-gate how task intent and motivation , as well as other contextual factors such as task urgency, relate to the likelihood of continuing a search task (within the one-week horizon that we used in our study). In the remainder of this subsection we define the dime n-sions on which we annotated tasks.
 Intent Type : The type of the task, derived from previous studies in the information science literature (e.g., [20] [23] ). The hypoth e-sis is that some task types, such as information gathering or tran s-actions, are associated with task continuation. The specific intent types chosen for labeling were:  X  Fact finding (focused) : Find specific piece(s) of information (e.g., a query such as  X  mc gilvery oil wolsey  X ).  X  Information gathering (exploration) : Find information on a topic rather than for a specific fact (e.g.,  X  X nglish comedy X ).  X  Undirected browsing : Explore a site or the Web without an obvious goal (e.g.,  X  X ortland craigs list X ).  X  Transaction : Accomplish a task or perform a transaction online ( e.g.,  X  X ay discover card bill X  )  X  Communication (social) : Read or interact in online social sites such as forums .  X  Information maintenance or update : Monitor information on a running topic and possibly update a Web resource.
 Motivation : The cognitive or affective motivation inferred to be behind the task, derived and simplified from cognitive science and psychology literature [4] . Our intuition was that some motivations are more likely to associate with task continuation than others . The motivations selected for labeling were:  X  Affective : Based on emotion or feeling, with sub-categories of Arousal (e.g., adult content), Tranquility (e.g., viewing art), 
Happiness , and Physical well-being (e.g., verifying health i n-formation).  X  Cognitive : Learning about the world or about the self, with sub-cat egories of Exploration , Understanding , and Positive self-evaluation .  X  Self-assertive : Individual relationship between person and the environment, with sub-categories of Individuality, Self-
Determination, Superiority, and Approval (e.g., posting on a support forum).  X  Social : Integrative social relationships, with sub-categories of 
Belongingness (maintaining social relationships), Social R e-sponsibilities , or providing Social Support. If none of the specific subtypes seemed appropriate, the annot a-tors had an opt ion to pick a generic motivation (e.g.,  X  X ocial X ). Complexity : The complexity of the task, measured by the number of goals required to find the needed information. We hypothesized that more complex tasks, with multiple goals, are more likely to be continued. The options for complexity were:  X  Single goal : A task that can be theoretically satisfied by a single web page (e.g.,  X  X omen X  X  suffrage 1922 X ).  X  Multiple goals : A task that is expected to require aggregating information from multiple web pages (e.g.,  X  X heap flights X ).  X  Undirected : No evident goal (may be undirected exploration). We asked annotators to specify the number of goals (if the task was not labeled as  X  X ndirected X ) based on their estimates of the number of Web pages required to fulfill the searcher X  X  info r-mation need (one=single goal, many=multiple goals). WorkOrFun : Does the task appear to be necessary for work or life or is it more for fun? We hypothesized that fun-related tasks are more likely to be continued than those considered to be work-related . Time Sensitivity: How urgent or time sensitive is the information need, and is it likely to disappear/expire in a short time? Natura l-ly, we hypothesized that highly time-sensitive tasks are less likely to be continued. Continue or Not?: Finally, we asked the annotators to predict how likely they think a task is to continue within the week X  X  data horizon. The following four response options were available : [ very likely, likely, unlikely, very unlikely ] . We hypothesized that human judges would be able to use their world knowledge and intuition to reasonably estimate the likelihood of task continu a-tion, given the information available to them from the first two days of search behavior (e.g., all of the queries that users had issued, the URLs they had clicked, and the time of these events). These manually-generated estimates serve as a baseline for the performance of the predictive models developed in this paper. The human annotations were performed at the task level, where each task was previously identified as  X  X arly dominant X  by a h u-man annotator (defined in Section 3.1 above ), using a separate manual annotation process described in detail in reference [ 21]. For each of these tasks, the annotators were shown the sequences of queries, clicks, and date/times, with corresponding session identifiers, as well as all other search actions of that user (regar d-less of the task)The actua l labeling was performed only for the early-dominant tasks. The four annotators reviewed the guidelines for the above intents and motivations and worked through more than 20 example search tasks together, to ensure consistent inte r-pretation and application of the guidelines. Annotators labeled an average of nearly 300 search tasks each , with three of them con-tributing over 90% of the labels. An additional sample of 100 tasks was labeled by the three ann o-tators responsible for the bulk of the labeling, for the purposes of computing inter-annotator agreement statistics. The average ann o-tator agreement and the free-marginal Fleiss Kappa statistic [31] are reported in Table 2 1 . The agreement ranges from 0.65 to 0.71, with Kappa values b e-tween 0.52 for  X  X orkOrFun X  to 0.59 for  X  X ntent X . These values are acceptable for such a difficult and potentially-subjective task. The majority of tasks were labeled as information gathering (e x-ploratory) (56%), examples of which included research, school work, shopping, and travel planning. The other tasks were labeled as fact finding (focused) (20%), and transaction (13%), with the remainder of the search intents comprising 2-4% each. The task continuation statistics for these intents are reported in Figure 1. Information maintenance tasks were most likely to be continued (85%), followed by undirected browsing (78%). Both of these may reflect hobbies and other longer term interests of the users in our study. Interestingly, transaction and communication were also likely to be continued (both over 70-75%) . One possibl e confound is that transaction tasks include a small fraction of nav i-gational re-finding , even though by requiring at least two unique queries we attempted to filter out navigational queries. With 52% and 48% return rates, information gathering and fact finding tasks were less likely to be continued, perhaps because most these tasks were fairly simple and could be completed within a single session. Figure 2 reports the task continuation statistics for different mot i-vations for the tasks, in decreasing order by the likelihood of co n-tinuation . It appears that affectively motivated tasks are more likely to be continued, with arousal (typically, adult content) the most likely to be continued. Interestingly, self-assertive and social motivations were almost equally likely to result in task continu a-tion, while tasks motivated by cognitive: understanding and affe c-tive: physical wellbeing were the least likely tasks to be conti n-ued. Tasks with these motivations do not typically persist over time , presumably because they involved episodic lookups of facts or health-related information that does not require follow-up. In addition to analyzing variations in task continuation likelihoods associated with different intents and motivations, we were also interested in the impact of task complexity on the likelihood that users would continue. Figure 3 shows the relationship between the number of goals identified and the task continuation likelihood. Interestingly, the number of task goals (Figure 3) is not strongly associated with task continuation. In fact, the tasks that appear to be undirected (e.g., without a clear goal page or information nu g-get), are more likely to be continued. These include browsing employment opportunities, real estate listings, or adult content . Furthermore, tasks judged to be time-sensitive (Figure 4a), are more likely to be continued, compared to tasks judged to be not time-sensitive. Also tasks being attempted for pleasure (fun) ra-ther than necessity (work-related) are also slightly more likely to be continued (Figure 4b). While this seems counter-intuitive, one explanation could be that when searching by necessity, users are more likely to satisfice once the (minimum) sufficient information is found, whereas curiosity-or pleasure-driven exploration are less likely to be satisfied as quickly, and is likely to be more aligned with the searcher X  X  long -term interests. We explore this observation further in the next section. We have seen in this section that several factors are associated with the likelihood of task continuation. In particular, tasks that give searchers pleasure and align with the users X  interests are more likely to be continued, at least within the one-week period analyzed in this study, as we explore in more detail next. We hypothesized that certain topical categories of tasks are more likely to be resumed than others (see also [10] ). To identify top i-cal category, we use automatic query classification into the top two levels of the Open Directory Project (ODP, dmoz.org) hiera r-chy. The classifier has a micro-averaged F1 value of 0.60 and is described more fully in reference [5] . To obtain a topic represe n-tation for queries labeled as belonging to the task of interest, we obtained the top ten results for each query from Bing and categ o-rized each result by running the text classifier on its content. The result is a vector of topic probabilities, which we restricted to the three most probable classes. For each task, we obtained the most probable ODP categor y by merging the distributions for all ass o-ciated queries. Figure 5 reports that search tasks in some ODP categories, such as  X  X dult X ,  X  X ids and teens X  and  X  X ews X  , are very likely to be conti n-ued, while search involvement w ith other topics, such as  X  X ome X ,  X  X ealth X , and  X  X cience X  appear to be more episodic and less likely to be continued over time . Note that the search topic is distinct from the search intent (e.g., a task associated with  X  X ews X  topics may be either information maintenance , or fact finding ). Im-portantly, this demonstrates that the ODP category labels may be useful for automatically predicting task continuation. We explore the utility of this representation for prediction later in the paper. The potential utility of the ODP category labels for task continu a-tion prediction is not surprising, and indeed we observed anecd o-tally in our data that some topics were more likely to be repeated over time. These topic repeatability statistics could be considered as a  X  X rior X  for the ta sk continuation likelihood, and could be exploited in the absence of any other information about the user. To examine this observation in more detail we analyzed the pro b-ability that a given topical category will be observed in a future session for the same user within a week (similar to the setting used for this study). To do this, we used a separate set of Bing search logs for a period of three weeks that did not overlap with the one week of data used for our study. From these logs we e x-tracted over 100 million search sessions for over five million unique users. Search sessions were defined using a 30-minute inactivity timeout [40] . The results are summarized in Table 3 , and show that topics such as Computers/Internet , Arts/Television , and Adult/Computers are the most likely to be observed in subs e-quent search sessions, while topics such as Sports/Tennis or Re f-erence/Museums are likely to be used in one session but not to appear in future sessions for the same user within the following week. The former set of categories may be more likely to reflect users X  longer-term , persistent interests, whereas the latter may be more transient and affected by immediate social responsibilities e.g., specific events such as a museum visit or a tennis tourn a-ment. In the previous section, we analyzed the task continuation data with a focus on the characteristics of the search tasks that are a s-sociated with task continuation. We now turn to modeling and automatically predicting task continuation . As described earlier, this is an important area for search providers trying to help users perform cross-session searching. We first describe the features used for task representation and then describe the algorithms and training procedure that we adopted in this study (Section 4.2). We represent a task using topical , user engagement , user history profile , and topic and query priors feature groups, described in more detail below and shown in Table 4 . We use these features to predict task continuation. Baseline features. We began by re-implementing the most i m-portant features reported in [21] , which forms our baseline system in the prediction experiments. These features capture the basic lexicographic and behavior properties of the search session, such as query overlap, number of clicks on results returned by the search engine, and time between queries. Reference [21] provides more detailed descriptions of these features. In addition to the baseline features, we also added four groups: Search topic. These new features aim to capture the topical cat e-gories of the task derived from the automated classifier trained on ODP data and described in Section 3.5. Additional measures i n-clude the entropy of the topic distribution (for both the first-and second-level categories of the ODP hierarchy) to capture the d e-gree of topical focus in the task. We conjectured that tasks that span fewer distinct ODP topics are more likely to be continued User engagement. These new features aim to capture the searc h-er X  X  level of engagement in the task they are performing, going far beyond the baseline features described above. Features of note include the estimated satisfaction and dissatisfaction with the results (based on estimates of the amount of time that users spent dwelling on clicked results, per [13] ), the span of time and effort invested in the task, the amount of  X  X ulti -tasking X  interspersed with the task, as well as other metrics of effort and user activity. We hypothesized that if a user is heavily engaged with a task and that effort is focused , they will be more likely to continue. User profile history. In addition to analyzing the current search task, we also aim to capture historical information about the user . To do this we used two weeks of log data from the time period before the week of interest for each of the users in our study. Fea-tures generated from this profile include the topic distribution of previous search sessions, queries, overlap with the current task, and other profile information such as the time of the day and day of the week when the task was started. We hypothesized that to p-ics or query terms that interested the user in the past, are more likely to be continued in the future.
 Repeat priors on topic and query repetition. In addition to the random sample of the nearly 1,200 users under study, we make use of global query and ODP category statistics computed over the query log described in Section 3.5. We hypothesize d that to p-ics and query terms that tend to re-appear globally could provide additional evidence for task continuation. We experimented with two different classifiers for the problem of predicting task continuation. The two classifiers used were L o-gistic Regression [15] (which was shown to be effective for task continuation prediction in reference [21] ). We refer to this method as Baseline in subsequent experiments. Our main experiments were performed using a gradient-B oosted Decision T ree classifier, based on the MART algorithm [14] , with a logistic penalty, so that we can evaluate the importance of richer feature combinations. We refer to this classifier as BT (for B oo st-ed Tree) in subsequent experiments, typically listed in combin a-tion with either all the features in Table 4 ( X  X T: All X ) or feature subsets. The classification task is to predict whether a search task, previously identified to be early-dominant for a user, will be co n-tinued in the future (positive class) or not (negative class). All experimental results reported below were performed using 5 runs of 10 -fold cross validation, randomized for each method. To compare the performance of the classification methods we use the following standard performance measures: (1) accuracy , (2) precision and recall for the positive class (task continuation), and (3) area under the receiver-operator-characteristic curve (AUC ). Statistical significance between performance values was calcula t-ed using two-tailed independent sample t-tests where appropriate. This section first reports the human performance on the task, to indicate that predicting continuation is challenging even for h u-man judges (Section 5.1). We then report the results of automatic continuation predictors (Section 5.2), followed by extensive ana l-ysis of the feature groups and individual features that are most strongly predictive of task continuation (Section 5.3). Finally, we present the findings of a failure analysis which suggests future improvements to our predictive models. Table 5 reports the performance of the classifier trained on ind i-vidual task dimensions (manually labeled as described above), as well as on the explicit human judgment of task continuation, and a classifier trained on the combination of all of the manual annot a-tions. In other words, we attempt to create the best  X  X ybrid X  h u-man and machine prediction possible, by using the labels provided by the human judges as features. These labels were expected to augment the explicitly labeled  X  X ontinue or not X  prediction (which was considered positive when the response was  X  X ery likely X  or  X  X ikely X  , and negative otherwise). Recall , that the annotators were able to see the first two days of the user X  X  search history, but did not have access to the longer-term User Profile features above). So, the humans  X  predictions were performed based on two days of data as well as world knowledge and intuition about the nature of search tasks. As Table 5 indicates, humans can definitely predict continuation more accurately than the na X ve Majority baseline that always picks  X  X ontinue X  , or than any individual intent or motiv a-tion label. However, there is an even stronger signal in the comb i-nation of the manual dimension labels, resulting in the best pre-diction possible based on the human judgments data. In addition to computing the predictive value of the task dime n-sions and their combination, we were also interested in the rel a-tionship between the nature of the human judges X  estimation of continuation likelihood and whether users were observed to be continuing the search task. We did this to help us to understand their ability to make the explicit prediction (rather than using their prediction as a feature for learning). Figure 6 shows the predicted versus actual outcomes for each of the four rating options. As Figure 6 indicates, when human judges were sure of the pr e-diction (i.e., rated a task to be  X  X ery likely X  or  X  X ery unlikely X  to be continued), their prediction accuracy was 80% and 75%, r e-spectively. However, for the majority of cases, the annotators provided more tentative labels ( X  X ikely X  and  X  X nlikely X ), and in those cases the predictions had substantially lower accuracy.. We now focus on the predictive performance of the trained mo d-els, comparing them with the human predictive performance. Table 6 reports the performance of a human prediction of task continuation , against our implementation of the state-of -the-art baseline described in [ 21], and our extended method BT :All (using all classifier features and two weeks of prior history as described above). Both classifiers substantially outperform the human ann o-tators; furthermore, BT :All substantially and significantly outpe r-forms the baseline in terms of accuracy , precision, and AUC me t-rics with p &lt; 0.01. We augment the quantitative analysis in Table 6 by plotting the precision-recall curve for each of the methods in Figure 7 . As Figure 7 indicates, both the automated Baseline and our BT:All Features classifier substantially outperform human predictions. Furthermore, BT:All provides the biggest lift in AUC over the Baseline, at precision of at least 0.8 and remains acceptably high (  X  0.75) until nearly 0.8 recall levels. One factor that may affect the performance of the BT:All classifier is the availability of user history information. This information was not available to humans (although they can draw upon ge n-eral world knowledge and their own search experiences) or Base-line. To quantify the contribution of user X  X  history (profile) info r-mation to use for prediction , Table 7 reports the results of varying the amount of history data for each user included in the model from None (i.e., no user profile information prior to the two days at the beginning of prediction), to one week and two weeks. While adding one week of prior history improves performance slightly on all metrics, the improvements are not significant. However, the effects of adding an additional week of history (for two weeks total) are striking, providing substantial and significant improvements (with p &lt; 0.01 for accuracy, precision, and AUC metrics, and p &lt; 0.05 for Recall), compared to the same method with no prior user history. Having multiple weeks may more e f-fectively capture the users X  long -term interests or allow for recu r-ring tasks (e.g., those that happen biweekly) to be observed and used to make predictions for the current week. More research is needed to determine whether such gains consistently increase with history length. Also note that even when we remove the history features, the performance of BT:All: No History is still substa n-tia lly better than the performance of both Baseline and the human annotators. We discuss the differences between human and m a-chine performance in more detail later in Section 5.4. An important question in understanding the success of BT:All model is determining which feature groups contributed the most toward its strong performance. To this end, we now present some feature ablation analysis. In order to determine the contribution provided by each of the feature groups, we perform feature ablation experiment, by star t-ing with the full set of features (Table 4), and then systematically removing feature groups from the set, one group at a time. The results are reported in Table 8, averaging over five runs of ran-do mized 10 -fold cross validation. Surprisingly, removing text features such as the most frequently used query terms, has negligible effect on performance. In co n-trast, removing the user profile features computed over the user history degrades performance significantly on the accuracy , prec i-sion, and AUC metrics. While other feature groups also appear to contribute, the single most valuable feature group is the user e n-gagement effort and focus (listed in Table 4). Removing the se features degrades performance significantly to roughly that of the original baseline. It appears that the more engaged the user is with the search task, the more likely they are to continue it in the one-week time span of our study. With the importance of the engagement features apparent in the feature-group ablation study, we set out to investigate how each of the individual engagement features correlates with task continu a-tion . We computed the Pearson X  X  correlation coefficient ( ) b e-tween each of the engagement feature values and the task contin u-ation label. The values for the most and least correlated features are shown below in Table 9. Table 9 reports that the most strongly correlated engagement fe a-tures (from Table 4) include TaskSpanTime , NumDomTaskSessions (the number of on-task domain sessions), features such as the ratio of on-task vs. off-task sessions, and the number of SAT clicks (defined as clicks with dwell time  X  60). These features indicate that searchers tend to be more strongly focused on tasks that would be continued , and less involved in multi-tasking. In co n-trast, the lack of focus (e.g., increased multi-tasking during search as measured by the NumTaskSwitchSess feature), correlates nega-tively with task continuation. Interestingly, there is also a mode r-ate correlation between search satisfaction (measured by the nu m-ber of SAT clicks (defined in Table 4) and task continuation, and slightly negatively correlate d to the number of  X  X lick -backs X  or bounce-backs ( AvgDomClickBacks ), which are indicative of u n-satisfying results. Indeed, Hu et al. [18] found a positive correl a-tion between searcher satisfaction and search engine re-use on a week by week basis over a six-month period, although th ey just studied query volume and not task continuation. To understand how the individual features contribute in combin a-tion with the other features for the BT classifier, Table 10 reports the individual features that contributed the most to reducing error during training  X  those with highest average relative reduction in residual squared error (averaged across the cross validation folds). The single strongest predictor of continuation is TaskSpanTime  X  the amount of time the searcher already has spent on the task in the first two days. More interestingly, the next strongest indicator is DomQueriesPriorHist , the number of queries in the dominant task that were observed in the prior history for that user (hence pulling in information about the longevity of the task) . The prev i-ously studied feature BASE_NumQueryChars (the number of characters in the query) is another strong indicator, perhaps b e-cause longer queries say something about the nature of the task or are suggestive of users  X  knowledge of a particular domain of i n-terest. Previous studies have shown that users issue longer queries when searching in a domain about which they are knowledgeable [40] . Another new indicator of continuation is the TermRepeatPrior  X  the distribution of term repeat probabilities computed over more than five million users. This suggests that prior likelihoods of repetition computed independent of user may also be useful for predicti ng task continuation. Other strong ind i-cators include the task start time of day, as well as the task focus and engagement features discussed above. To gain additional insights into the task continuation prediction problem, we compared the task continuation predictions made by human annotators (using the labeling procedure described in Se c-tion 3.3), with those of the automated classifier, by focusing on the cases where the human and the classifier predictions differed. In these cases, humans were able to predict task termination more accurately (by about 25%) than task continuation. We conjecture that this could be partly explained to the data collection co n-straints, in that we required for a task to be resumed within one week in order to be marked  X  X ontinued X . Furthermore, humans were able to predict continuation more accurately (by about 30%) for tasks labeled as difficult, compared to those labeled as easy or moderate. We also found that humans were most accurate in pr e-dicting task continuation within a day, which intuitively covers many transaction and planning tasks. Most remarkably, in cases where a task query previously occurred in a user X  X  prior history, the classifier was 73% more likely to be correct than human jud g-es, who did not have access to the long-term user profiles and had to rely on their intuition and world knowledge instead. At this point it is appropriate to point out limitations of this study . As discussed earlier, to make this study feasible, the time horizon of task continuation was limited to one week. It is possible that some tasks are continued more than a week later. Another limit a-tion is methodological: without asking the users directly, it was not possible for us to investigate the underlying causes of the search interruptions, i.e., whether the searchers were waiting for external input or simply ran out of time and had to switch to a n-other task first. To get that information we will need to work with users directly and employ in-situ or retrospective methods to be t-ter understand the factors affecting their observed behaviors. Many search tasks such as travel planning, making large purcha s-es or job searches can span multiple search session extending over hours, days, or even weeks. The research reported in this paper is aimed at better understanding , characterizing, and automatically predicting search tasks that will be continued in the future  X  a task complementary to that of identifying previous related search se s-sions after the fact. We first annotated a query log from Bing to identify the types of search intents, motivations, and topics ass o-ciated with search tasks that are continued. We then developed new features of search engagement and focus in search sessions, which we use as input for prediction. Finally we developed effe c-tive prediction algorithms that significantly outperform both the previous state-of -the-art method, as well as the ability of human judges to predict search continuation. Our analysis of the task continuation yielded effective features that allow our prediction method to significantly outperform both a state-of -the-art baseline and predictions based on human jud g-ments. We identified the groups of features (user prior history and task engagement) that most strongly predict task continuation. We also identified individual task characteristics, such as time span, user focus, and task start time that are most strongly correlated with continuation. Future research directions include studying task continuations occurring beyond the one -week time frame studied here, as well as looking back further in time to build more complete historic profiles of users X  search interests given the observed value of such historic information for task continuation prediction. We are also interested in applying task continuation predictions to improve search result ranking (e.g., by personalizing the results to the a c-tive task and not to the whole user X  X  profile); and in better su p-porting searchers in resuming tasks (e.g., if a task is likely to be continued, saving the results already found for more convenient access once the task is continued). Thus, the work presented here forms an important advance towards developing a more persona l-ized and intelligent solution for long-running, complex tasks  X  a key challenge for the information retrieval community. We thank Alexander Kotov and Jaime Teevan for collecting and labeling earlier data for this paper, and Dan Liebling and Shane Williams for assistance with data processing. We also thank Filip Radlinski for insights on quantifying complexity of search tasks. [1] E. Agichtein, E. Brill and S. Dumais. Improving Web search [2] L. M. Aiello, D. Donato, U. Ozertem, and F. Menczer. Behavior-[3] A. Aula, N. Jhaveri and M. K X ki. Information search and re-[4] J.T. Austin and J.B. Vancouver. Goal constructs in psychology: [5] P.N. Bennett, K. Svore and S.T. Dumais. Classification-[6] A. Broder. A taxonomy of Web search. SIGIR Forum , 32(2): [7] H. Cao, D. Jiang, J. Pei, Q. He, Z. Liao, E. Chen and H. Li. [8] Y.-S. Chang, K.-Y. He, S. Yu and W.-H. Lu. Identifying user [9] M. Czerwinski, E. Horvitz and S. Wilhite. A diary study of task [10] D. Donato, F. Bonchi, T. Chi and Y. Maarek. Do you want to [11] D. Downey, S.T. Dumais, D. Liebling and E. Horvitz. Unde r-[12] S. Dumais, G. Buscher and E. Cutrell. Individual differences in [13] S. Fox, K. Karnawat, M. Mydland, S. T. Dumais and T. White. [14] J. Friedman. MART boosted decision trees: Greedy function [15] J. Friedman, T. Hastie and T. Tibshirani. Additive logistic [16] A. Hassan, R. Jones and K. Klinkner. Beyond DCG: User [17] A. Hassan, Y. Song and L. He. A task level metric for measuring [18] V. Hu, M. Stone, J. Pedersen and R.W. White. Effects of search [19] R. Jones and K. Klinkner. Beyond the session timeout: [20] M. Kellar, C. Watters and M. Shepherd. A field study [21] A. Kotov, P.N. Bennett, R.W. White, S.T. Dumais, and J. [22] U. Lee, Z. Liu and J. Cho. Automatic indentification of user [23] Y. Li and N.J. Belkin. A faceted approach to conceptualizing [24] J. Liu and N.J. Belkin. Personalizing information retrieval for [25] B. MacKay and C. Watters. Exploring multi-session Web tasks. [26] Q. Mei, K. Klinkner, R. Kumar and A. Tomkins. An analysis [27] D. Morris, M. Ringel Morris and G. Venolia. SearchBar: A [28] B. Piwowarski and H. Zaragoza. Predictive user click models [29] F. Radlinski and T. Joachims. Query chains: Learning to rank [30] F. Radlinski, M. Szummer and N. Craswell. Inferring query [31] J.J. Randolph. Free-marginal multirater Kappa (multirater K [32] D.E. Rose and D. Levinson. Understanding user goals in Web [33] M.D. Smucker, J. Allan and B. Carterette. A comparison of [34] D. Sontag, K. Collins-Thompson, P.N. Bennett, R.W. White, [35] A. Spink. Multitasking information behavior and information [36] B. Tan, X. Shen and C. Zhai. Mining long-term search history to [37] J. Teevan, E. Adar, R. Jones and M.A.S. Potts. Information re-[38] E.G. Toms, L. Freund, R. Kopak and J.C. Bartlett. The effect of [39] R.W. White, P. Bailey and L. Chen. Predicting user interests [40] R.W. White and S.M. Drucker. Investigating behavioral [41] R.W. White, S.T. Dumais, and J. Teevan. 2009. Characterizing [42] B. Xiang, D. Jiang, J. Pei, X. Sun, E. Chen, and H. Li. Context-
