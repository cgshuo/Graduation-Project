 DNA microarrays have provided the opportunity to measure the expression lev-els of thousands of genes simultaneously. One of the most common application of microarray is to classify the samples such as healthy versus diseased by com-paring the gene expression levels. Microarray data which is characterized by high dimension and small sample size suffers from curse of dimensionality[1]. For better classification there is a need to reduce the dimension. In general, among thousands of genes(features) which are monitored simultaneously only a fraction of them are biologically relevant. Therefore, efficient feature selection methods are needed to identify a set of discriminatory genes that can be used for effective class prediction and better clinic al diagnose. In literature, various fea-ture selection methods have been proposed. These methods broadly fall into two categories[2]: filter and wrapper methods. Most filter methods independently measure the importance of features without involving any classifier. So, they may not select the most relevant set of features for the learning algorithm. Also, the features set selected by filter methods may contain correlated(redundant) features which may degrade the performance of classifier. On the other hand, wrapper methods directly use the classification accuracy of some classifier as the evaluation criteria. They tend to find features better suited to the predetermined learning algorithm resulting in better performance. But, they are computation-ally more expensive . The conventional wrapper methods are hard to apply directly to high dimensional datasets as they require large computation time. Reducing the search space for wrapper methods will decrease the computation time. This can be achieved by first selecting a reduced set of non-redundant features from the original set of features without losing any informative feature.
In this paper, a novel two-stage approach is proposed to determine a subset of relevant and non-redundant genes for better cancer classification. Our approach first groups correlated genes and then select one informative gene from each one of these groups to reduce redundancy. This requires partitioning of the original gene set into some distinct clusters so that the genes within a cluster are highly similar(correlated) while those in different clusters are dissimilar. At the second stage a Sequential Forward Feature Sel ection(SFFS) method is applied to select a smaller set of discriminatory genes which can provide maximum classification accuracy.

This paper is organized as follows. Sect ion 2 describes related work. In sec-tion 3 we present our proposed algorithm for selecting a set of informative and non-redundant genes. Experimental results on some well-known datasets are presented in Section 4. Sectio n 5 contains conclusions. In order to achieve better classification of high dimensional microarray data, we need to determine a smaller set of discriminatory genes from a given set of genes without loosing any information. In literature, many gene selection methods have been proposed which are based on a gene ranking that assigns a score for each gene which approximates the relative strength of the gene. These methods return a set of top ranked genes and classifier is built on these genes. Among them, Golub et. al.[3] selected top genes using measure of correlation which emphasizes that a discriminatory gene must have close expression levels in samples within a class, but significantly different expression levels in samples across different classes. Other approaches that adopt the same principle with modifications and enhancements include[4] and [5]. Using ranking method, one cannot select a smallest set o f discriminatory genes as the selected subset may contain many correlated genes. Few wrapper based approaches are also suggested in literature which works better for small and middle dimensional data. How-ever they cannot be applied directly on high dimensional microarray dataset as it is computationally expensive. We can ove rcome this by determining a smaller set of genes for wrapper approach. This is possible if we can group correlated or similar genes into clusters and then s elect a gene from each cluster which can provide us a reduced set of independent and informative genes.

In literature clustering h as been employed for grouping correlated or simi-lar genes. Many diverse clustering tec hniques have been suggested in literature. The most widely used techniques include hi erarchical[6], k-means clustering[7] and Self-organized-maps(SOM)[8]. Each one of them is associated with advan-tages and disadvantages. Shi and Malik[9] have proposed an efficient normalized cut(NCUT) method based on graph theoretic approach for image segmentation. The normalized cut criterion measures both the total dissimilarity between the different groups as well as total similarity with in the groups. This can also be used for clustering of correlated gen es in microarray data. In NCUT a given graph G=(V, E), where v i V represents a gene and e ( v i ,v j ) Erepresentssim-ilarity between two genes v i and v j , is divided into two disjoint sets A and B. For partitioning of the genes into A and B, the capacity of the normalized cut, Ncut is defined as where cut ( A, B )=  X  uA,vB w ( u, v )and assoc ( A, V )=  X  uA,tV w ( u, t )
To determine a better partition of a cluster the value of Ncut should be minimized which is a NP-hard problem. Shi and Malik[9] have shown that this problem can be reformulated as eigenvalue problem which is given by It has been shown by Shi and Malik[9] tha t second smallest eigenvector of the above generalized eigenvalue system is the real valued solution to our minimum normalized cut problem. Hence, the seco nd smallest eigenvector can be used to partition the original cluster into two clusters.

In general euclidean distance and Pearsons correlation are used as the distance or similarity measure for clustering. However, euclidean distance is not suitable to capture functional similarity such as positive and negative correlation, and interdependency[10]. It is also pointed out that it is suitable only for a data which follows a particular distribution[11]. On other hand, Pearson coefficient is not robust to outliers and it may assign a high similarity score to a pair of dissimilar genes[12]. Also both these meas ures are sensitive to scaling and rota-tion. A similarity measures called maximal information compression index[13] is suggested in literature for measuring redundancy between two features. Given two random variables x 1 and x 2 , the maximal information compression index  X  ( x 1 ,x 2 ) is defined as where  X  1 ,  X  2 are the variance of x 1 ,and x 2 respectively and  X  ( x 1 ,x 2 )isthe correlation between x 1 and x 2 .
The value of  X  2 is zero when the features are linearly dependent and increases as the amount of dependency decreases. The measure  X  2 possesses several desir-able properties such as symmetry, sensitivity to scaling and invariance to rotation which are not present in the commonly used euclidean distance and correlation coefficient.

Further splitting of a cluster, from a set of available clusters, can be decided on the basis of representative entropy measu re. Representative entropy measures the amount of redundancy among genes in a given cluster. For a cluster containing p genes with covariance matrix  X  , representative entropy, H R of a cluster is given by
H R attains a minimum value(zero) when all the eigenvalues except one are zero, or in other words when all the information is present along a single di-rection. If all the eigenvalues are equal, i.e. information is equally distributed among all the genes, H R is maximum. High value of H R represents low redun-dancy in the cluster. Since we are interested in partitioning the original subspace into homogeneous clusters, each cluster should have low H R . So we split a clus-ter which has maximum H R among a given set of clusters as it contains more non-redundant genes. Here we propose a two stage algorithm to select a set of discriminatory genes to achieve better classification. Our proposed algorithm consists of two phases. The first phase involves partitioning of the original gene set into some distinct clusters so that the genes within a cluster are highly correlated to each other while those in different clusters are less correlated. The similarity measure used in NCUT clustering algorithm is maximal information compression index. We have used a hierarchical clustering in which we start with a single cluster. We split the original cluster into two clusters such that the normalized cut value is minimized. To determine which candidate cluster to further partition from the existing set of clusters, we have used re presentative entropy. The cluster with the maximum H R (low redundancy) is partitioned. This process is repeated till we get the required number of clusters. R epresentative gen efromeachcluster is chosen using t-statistics. In the second phase a Sequential Forward Feature selection(SFFS) method is applied to select a smaller set of discriminatory genes which provides maximum accuracy. The criterion used in the SFFS is the accu-racy of the classifier. The outline of the proposed algorithm is the following: Proposed Algorithm Input : Initial Set of genes, Class Labels C, Classifier M, PHASE 1 // to determine a subset of relevant and independent 1. Intialization : Set G=initial set of genes ; 2. S = empty set; No of clusters=2; /*Set of Selected Attributes*/ 3. Calculate the Similarity Matrix W using Maximal information compression index. 4. Define D where D ( i )=  X  j w ( i, j ) 5. Solve eigenvalue problem D  X  1 / 2 ( D  X  W ) D  X  1 / 2 x =  X  6. Use the eigenvector with second smallest eigenvalues to divide the original cluster C into two clusters. 7. While (no of clusters  X  Cluster Size) 8. Begin 9. For each cluster calculate the representative entropy H R 10. Choose the Cluster C i having the maximum entropy 11. Repeat step (3)-(6) for Cluster C i 12. No of clusters=No of clusters+1 13. End 14. For each cluster 15. Find the informative gene g i from cluster C i using t-statistics 16. S=S U g i PHASE 2 // to determine subset of genes which provides max accuracy 1.Initialization R=empty set 2.For each x j  X  S calculate classification accuracy for classifier M. 3.[ x k ,max acc ]= max j Classif ication accuracy ( x j ); 4. R = R  X  x k ; S = S  X  x k ; R min = R 5. For each x j calculate classification accuracy of S  X  x j forclassifierM 6. [ x k ,max acc ]= max j Classif ication accuracy ( x j ); 7. R = R  X  x k ; S = S  X  x k 8. If new max acc  X  max acc then R min=R;max acc=new max acc; 9. Repeat 5-9 until max acc=100 or S = empty set 10. Retum R min, max acc To test the effectiveness of our proposed algorithm, we have carried out ex-periments on three well known datasets from Kent Ridge Biomedical Data Repository[14]. The details of these datasets are given in Table 1. Datasets are normalized using Z-score before carrying out experiments.

Genes are clustered using NCUT based on maximal information compression index as similarity measure. From each cluster the most informative gene is selected using t-statistics. After co llecting a pool of genes, a Forward Feature Selection method is applied to get a sub-optimal set of genes which provides maximum classification accuracy. Classificat ion accuracy is calculated using leave-one-out cross validation. The different classifiers used in our experiments are lin-ear discriminant classifier(LDC), quadratic discriminant classifier(QDC), k-nearest neighbor(KNN) and support vector machine(SVM). For KNN the op-timal value of k is chosen. Linear kernel is used in SVM. The experiment was conducted for different cluster sizes. The cluster sizes considered in our experi-ments are 30, 40, 50 and 60. Table 2 depicts the maximum classification accuracy along with the number of genes obtained by our proposed algorithm for different cluster sizes. We can observe the following from Table 2: 1. For Colon dataset a maximum accuracy of 98.38% is achieved with 32 genes 2. For SRBCT dataset maximum classification accuracy of 100% is achieved 3. For prostate dataset maximum classification accuracy of 99.01% is achieved 4. The performance of KNN is better in terms of number of genes in comparison It is observed that our proposed algorithm is able to achieve a high classification accuracy with small number of genes. I n Table 3, we have also compared per-formance of our proposed method in terms of classification and number of genes with some already existing gene selection methods in literature[15],[16],[17],[18], [19],[4]and [20]. From Table 3, it can be observed that the performance of our proposed algorithm is significantly better in terms of both classification accuracy and number of genes selected. In this paper, we have proposed a two stage algorithm for finding a small subset of discriminatory genes responsible for classification in high dimensional microarray datasets. The first stage involves partitioning of the original gene set into some distinct subsets or clusters so that the genes within a cluster are highly correlated to each other while those in different clu sters are less correlated. We have used NCUT clustering algorithm which is based on graph theoretic approach and requires computation of similarity measures between genes. We have used a novel similarity measure maximal information compression index which is not used for microarray datasets earlier. Most informative gene from each cluster is then selected to create a pool of non-redundant genes. The size of this set is significantly small which allows us to use a wrapper approach at the second stage. The use of wrapper method at the second stage gives a smaller subset of genes which provides better classificatio n accuracy. Experimental results show that our proposed method is able to achieve a better accuracy with a small number of genes. Comparisons with other state of art methods show that our proposed algorithm is able to achieve better or comparable accuracy with less number of genes with all the three datasets.

