 1. Introduction
During the past decades, a number of recommender systems have been developed to aid individual users in finding items 2011 ), and websites ( Cantador, Bellogin, &amp; Vallet, 2010 ), to name a few. These recommenders, however, are tailored only to the needs of individual users. As people are gregarious by nature, a variety of activities involve groups of people who par-ticipate either online or in an old-fashioned manner, i.e., in person. To meet the demands of groups of users, group recom-
Diaz-Agudo, 2009 ) have been proposed to identify items, such as vacation packages ( McCarthy et al., 2006 ), restaurants  X  a whole (rather than individual users). As claimed by Gartrell et al. (2010) , effective group recommendations can have a po-however, is a challenging task due to the diverse interests of group members, even more so when dealing with groups con-
One of the in-demand recommendation tasks is to suggest movies to a group. Movies offer a popular group activity for friends, families, and colleagues who gather to either see a movie at the cinema or watch a DVD at home. These people often turn to experts X  reviews to find movies that match their interests and/or reach a consensus on their own regarding the mov-ies to watch. Group recommenders on movies can streamline this process by directly suggesting movies appealing to a group. While the majority of the recommenders that have recently been introduced to make group recommendations on of our knowledge, none of them adopts the content-based strategy to exploit descriptive information on movies to perform the recommendation task. In this paper, we introduce GroupReM, a group recommender system on movies. Our proposed top-N recommender, which is based on a content-based strategy to identify a ranked set of N movies that best match the (content of movies of) interest to a group, differs from existing CF-based group recommenders that adopt various strategies for predicting (individual/group) ratings on movies and suggest the movies with the highest overall rating to a group ( Yu,
Lakshmanan, &amp; Amer-Yahia, 2009 ). These recommenders are restricted, since  X  X  X imilar-minded X  X  individuals at a movie web-are required to make recommendations. GroupReM, on the other hand, simply relies on data readily available on social web-sites, which are tags and their frequencies of occurrence, along with bookmarked movies, to suggest movies to a group.
GroupReM considers semantic information of movies, i.e., (personal) tags at a movie website, to capture both the (i) con-tent of movies and (ii) the preferences of members of a given group G on movies archived at the website. GroupReM applies a rank aggregation model on two different measures, group appealing and global popularity , computed for each candidate movie
M to be considered for recommendation. The former captures the content similarity between M and the group profile of G , whereas the latter reflects the popularity of M at the movie website. GroupReM anticipates that popular movies, which are to the members of G . In matching (the tags in) M and the profile of G , GroupReM does not impose an exact-match constraint.
Instead, GroupReM relies on pre-computed word-correlation factors ( Koberstein &amp; Ng, 2006 ) to determine inexact, but anal-ogous, tags in M and G , in addition to exact-matched tags, to more accurately capture the degree of appealing to M  X  G .
GroupReM is (i) simple , since it solely employs a standard measure to combine the aforementioned content-similarity and popularity scores, (ii) fast , since it takes on the average less than a second to make recommendations for a group (of up to eight members), and (iii) scalable , since GroupReM can identify movies that capture the common interests of a group regardless of its size and the degree of cohesiveness among group members. Moreover, GroupReM requires neither training nor domain-specific knowledge to select movies to be recommended and thus can directly be adopted to make recom-mendations on items other than movies. We have conducted an empirical study using more than 3000 groups of users from the MovieLens dataset ( GroupLens Research at University of Minnesota, 2011 ) and verified that GroupReM (i) gen-erates relevant recommendations on movies tailored to the needs of a group and (ii) significantly outperforms CF-based group recommenders.

The remaining of this paper is organized as follows: In Section 2 , we discuss existing group recommenders and compare their recommendation strategies with GroupReM. In Section 3 , we detail the design of GroupReM. In Section 4 , we present the empirical study conducted to assess (compare, respectively) the performance of GroupReM (GroupReM with existing CF-based group recommenders, respectively) and illustrate the effectiveness and efficiency of GroupReM. In Section 5 , we give a concluding remark and directions for future work. 2. Related work
In this section, we present a number of existing group recommenders that suggest different types of items, including Clemente, 2009 ), and music ( Crossen, Budzik, &amp; Hammond, 2002 ), and compare their recommendation approaches with
GroupReM. Thereafter, we introduce representative work on recently-developed group recommenders on movies ( Gartrell et al., 2010; Basu Roy et al., 2010 ) which differ from GroupReM in their design methodologies. An in-depth discussion on group recommenders can be found in Boratto and Carta (2011) and Jameson and Smyth (2007) . 2.1. Non-movie group recommenders
As defined in Berkovsky and Freyne (2010) , there are two strategies commonly-adopted for generating group recommen-dations: the aggregated models and aggregated predictions . The former combines individual user models, i.e., individual user profiles that capture the preferences of a group member, into a group model from where items to be recommended for the group are identified, whereas the latter generates predictions for individual group members and then aggregates the predic-tions to suggest items for the group. Empirical studies conducted and presented in Berkovsky and Freyne (2010) suggest that the aggregated models strategy (which is employed by GroupReM) generally outperforms the aggregated predictions strategy.

Flytrap Crossen et al. (2002) , which identifies musical tracks for a group, learns the music preferences of a user U based on the songs U has listened to and the numerical votes casted by U for the songs. Flytrap considers (i) relationships among musical genres, (ii) the influence artists have on one another, and (iii) the transitions in between songs people tend to make to perform the recommendation task. The recommender relies on domain-specific information and thus cannot be extended to suggest items other than songs, contrary to GroupReM which can directly be employed for recommending non-movie items.
 CATS (Collaborative Advisory Travel System) ( McCarthy et al., 2006 ) assists groups of friends in planning skiing vacations.
CATS relies on an incremental method which analyzes individual user X  X  critiques on the proposed recommendations to refine the recommendations generated for the group. Unlike GroupReM, CATS depends on user feedbacks to narrow the search
CATS has been designed to recommend items to a group of at most four users, which is a limitation, as opposed to GroupReM which does not impose a constraint on the number of group members in performing its recommendation task.
Berkovsky and Freyne (2010) recommend recipes to families through an eHealth portal. The proposed CF-based group recommender considers the (i) ratings assigned to recipes on the eHealth portal and (ii) weight, i.e., influence, of each indi-vidual group member computed according to his/her activities on the portal in making recommendations. Unlike the recom-mender introduced in Berkovsky and Freyne (2010) , GroupReM relies on the semantic content and popularity of movies to accurately perform the recommendation task.

Cantador and Castells (2011) introduce an ontology-based group recommendation strategy. The proposed approach identifies users that share similar tastes/preferences, i.e.,  X  X  X ommunities of interest X  X , according to individuals X  ontology-based profiles. These clusters of related users are then exploited to generate group profiles and perform the recommen-dation task. Similar to GroupReM, the strategy in Cantador and Castells (2011) is primarily content-based. However, the approach presented in Cantador and Castells (2011) is employed to suggests photos (instead of movies), relies on ontology concepts to determine the similarity among users/items (unlike GroupReM that depends on user-defined keywords, i.e., tags, to capture users X  preferences and items X  descriptions), and according to the authors  X  X  X  X ore sophisticated and statis-tically significative experiments need to be performed in order to properly evaluate X  X  the correctness of applying the clus-tering techniques presented in Cantador and Castells (2011) for group modeling and content-based collaborative filtering recommendation.
 Masthoff (2004) describes a number of recommendations strategies that merge individual user models in order to suggest TV shows that appeal to a group of users. Unlike GroupReM, (some of) the group recommendation strategies discussed in
Masthoff (2004) are inspired by Social Choice Theory. Boratto et al. (2009) recommend TV programming to a group by first employing a hierarchical clustering algorithm using the cosine similarity metric, which determines the similarity among preference of the members in G based on the average ratings given by members of G to programs. Based on the group profile, recommendations are generated. Unlike GroupReM which generates recommendations for groups regardless of the cohesiveness among group members, the group recommender in Boratto et al. (2009) makes recommendations for groups of similar-minded individuals only, which is a restriction, since in real life groups tend to include members that may not share similar interests in various TV programming. 2.2. Group recommenders on movies
A number of group recommenders that identify movies of interest to a group have been developed in the last few years, which include the systems introduced in Baltrunas et al. (2010), Gartrell et al. (2010), O X  X onnor et al. (2001), and Basu Roy et al. (2010) . Gartrell et al. (2010) claim that some members of a group are more capable than others to influence the remain-ing group members in making decisions (i.e., relevant or non-relevant) on items suggested to the group. The authors consider several group factors, which include social interactions among group members, degrees of expertise of the members in the group, and dissimilarity among group members, to identify movies of interest to the group. While empirical studies con-ducted using 10 groups have verified the effectiveness of the proposed recommender, it relies heavily on the interaction activities among group members that may not always exist or become available.

Baltrunas et al. (2010) conduct an empirical study to assess the effectiveness of alternative rank aggregation strategies, such as Spearman Footrule, Borda Count, Least Misery, and Average, for combining individual ranking predictions using a
CF-based algorithm to make group recommendations. Similar to the approach in Baltrunas et al. (2010) , the group recom-mender developed by O X  X onnor et al. (2001) adopts a Least Misery strategy to combine individual ratings predicted by a
CF-based algorithm. Basu Roy et al. (2010) prune and merge rating lists predicted for individual members of a group G using the popular Average and Least Misery aggregation strategies, in addition to considering pairwise disagreement lists of mov-ies, to recommend movies of interest to G . Unlike GroupReM, the group recommenders in Baltrunas et al. (2010), O X  X onnor et al. (2001), and Basu Roy et al. (2010) are based on the aggregated prediction strategy. According to the research work con-ducted in Berkovsky and Freyne (2010) , this strategy has been empirically determined to be less effective than the aggre-gated model, which GroupReM adopts.
 3. Our proposed group recommender
In this section, we present our proposed recommender, GroupReM, which suggests movies appealing (to a certain degree) to members of a group who are users of a movie website, such as Netflix (netflix.com) and MovieLens (movielens.umn.edu).
GroupReM relies on tags assigned to (represent the content of) movies and the popularity of each movie to make recommendations.

As group members of a movie website often have diverse preferences in movies, GroupReM first assesses the interest of each individual member U of a given group G based on the tags assigned by U to movies bookmarked in his/her profile. Tags and their frequencies of occurrence in group members X  profiles are combined to create the group profile of G which reflects the common interests of the group members (as detailed in Section 3.1 ). Thereafter, using word-correlation factors (intro-duced in Section 3.2 ), GroupReM determines the movies, among the ones available at the website which are not included in the profile of any member of G , that are similar (based on tags) to the ones bookmarked by the members of G to a certain aggregation function (as presented in Section 3.4.3 ), GroupReM computes the overall ranking score of each candidate movie score of M (as computed in Section 3.4.2 ). The former is calculated according to the number of tags assigned to (represent the content of) M that exactly-match or are analogous to the ones which characterize the profile of G , whereas the latter reflects the overall interest of the website users on M . The top-10 ranked candidate movies are recommended to G . The overall pro-cess of GroupReM is illustrated in Fig. 1 . 3.1. Creating a group profile
As the goal of group recommenders is to suggest movies of interest to a group, GroupReM analyzes the preference of each group member in movies and creates a group profile which reflects the types of movies preferred (to a certain degree) by the group as a whole. To construct the profile, GroupReM employs an aggregated model ( Berkovsky &amp; Freyne, 2010 ) that merges interest of the group members in movies.

GroupReM identifies the preference in movies of each individual member U of a group G by considering the movies book-marked by U and tags assigned by U to the movies. Personal tags, i.e., tags defined by an individual user, are employed to represent (the content of) a movie M of interest to a user, as opposed to tags in the tag cloud
GroupReM aims to capture U  X  X  description of M . Hereafter, GroupReM proceeds to create the group profile for G which includes all the personal tags (and their combined frequencies) assigned by the members of G to movies in their individual profiles. The since the high frequency of T reflects that T is more often used by members of G to describe movies they are interested in than other tags with lower frequencies.

Example 1. Consider a group G with three different MovieLens members. Fig. 2 shows (a portion of) the profile of each member U in G , which includes the personal tags assigned by U to movies bookmarked in his/her profile. By combining the tags (and cumulating the corresponding frequencies) in the individual group member profiles, GroupReM creates a group member of G , since the tags are included in the personal profile of each group member, as opposed to tags such as  X  X  X ermaid X  X  and  X  X  X ar X  X , which are preferred by one out of three group members.
 3.2. Word-correlation factors
GroupReM relies on the pre-computed word-correlation factors in the word-correlation matrix ( Koberstein &amp; Ng, 2006 )to determine the similarity between any two tags, which facilitates the task of identifying candidate movies to be considered for recommendation (as detailed in Section 3.3 ). Moreover, GroupReM takes advantage of the word-correlation factors in calcu-lating the group appealing score of a candidate movie with respect to a group profile (as discussed in Section 3.4.1 ). Word-correlation factors were calculated using a set of approximately 880,000 Wikipedia documents (wikipedia.org).
Each correlation factor indicates the degree of similarity of the two corresponding words co-occurrence and (ii) relative distances in each Wikipedia document. Wikipedia documents were chosen for constructing the word-correlation matrix, since they were written by more than 89,000 authors with different writing styles, and the documents cover a wide range of topics with diverse word usage and contents. Compared with synonyms/related words compiled by the well-known WordNet (wordnet.princeton.edu) in which pairs of words are not assigned similarity weights, word-correlation factors provide a more sophisticated measure of word similarity. Despite the existence of a number of measures that rely on
WordNet to determine the semantic similarity between pairs of words, such as Banerjee and Pedersen (2003) and LCH ( Leacock &amp; Chodorow, 1998 ), GroupReM depends on word-correlations, which have been successfully adopted to determine the similar-
Pera, &amp; Ng, 2011 ). 3.3. Identifying candidate movies to be recommended
As the number of movies available at a movie website W can be large, i.e., in the hundreds of thousands, it is inefficient to analyze each movie of W to identify those of interest to the members of a group G at W , since the comparisons would sig-nificantly prolong the processing time of GroupReM to make recommendations. To minimize the number of comparisons and thus reduce the processing time required in generating recommendations for G , GroupReM applies a blocking strategy denoted Candidate _ Movies , to be considered for recommendation.

The blocking strategy adopted by GroupReM first considers the personal tags assigned by a group member U of G for each of his/her bookmarked movies, uM . A movie M archived at W (to a certain degree) with the ones in the group profile for G .

To select movies to be included in Candidate _ Movies , GroupReM relies on a reduced version of the word-correlation ma-trix (introduced in Section 3.2 ) which contains 13% of the most frequently-occurred words (based on their frequencies of occurrence in the Wikipedia documents), and for the remaining 87% of the less-frequently-occurring words, only the exact-matched correlation factor, i.e., 1.0, is used ( Gustafson &amp; Ng, 2008 ). By adopting a reduced version of the word-correlation matrix to determine potentially similar movies, the overall processing time of GroupReM is significantly reduced without affecting its accuracy ( Pera, Lund, &amp; Ng, 2009 ).

Example 2. Consider the five movies archived at MovieLens, i.e., ML not bookmarked by any member of the group shown in Fig. 2 . To determine which one of the five movies should be treated as candidate movies for the group G introduced in Example 1 , GroupReM compares personal tags assigned to each movie shown in Fig. 2 with the tags (in the tag cloud) of each movie shown in Fig. 3 . Given that each of the personal tags assigned to M highly similar to at least a tag in the tag cloud of ML 1 and  X  X  X artoon X  X , respectively) can be found in the reduced version of the word-correlation matrix, ML candidate movie. Furthermore, each of the personal tags assigned to describe M counterpart in ML 4 ( ML 3 , respectively). Therefore, ML tag in M 7 exactly matches its counterpart in (the tag cloud of) ML highly similar to another tag in (the tag cloud of) ML 5 , i.e.,  X  X  X overty X  X , ML
M , M 6 , and M 7 include a tag, i.e.,  X  X  X rama X  X , which is also a tag in the tag cloud of ML assigned to describe the content of either M 2 , M 3 , M 6 treated as a candidate movie. 3.4. Generate group recommendations
Having identified the set of candidate movies to be considered for recommendation to a group, GroupReM proceeds to rank each of the candidate movies by relying on two different scores, the group appealing and popularity scores, presented in Sections 3.4.1 and 3.4.2 , respectively. The two scores are combined using an aggregation function , as defined in Sec-tion 3.4.3 , and the top-10 candidate movies with the highest combined scores are recommended to the group. 3.4.1. Appealing scores of movies
To determine the degrees of interests of members in a group G on a candidate movie M , GroupReM computes the group appealing score of M for G , denoted GrpApp ( M , G ), by accumulating the word correlation factors among the tags that capture puting the GrpApp score of M for G , GroupReM relies on the word-correlation matrix introduced in Section 3.2 , instead of the reduced word-correlation matrix employed in Section 3.3 , since the former provides a more accurate similarity measure be-tween (tags representing) M and G than the reduced matrix. The GrpApp score of M for G is defined as lation factor of g and m in the word-correlation matrix, freq respectively) in GP (the tag cloud of M , respectively), Max ( freq tag in GP (the tag cloud of M , respectively), and freq g senting (the content of) GP ( M , respectively), since it reflects the frequency in which group members (number of users at a ensures that exactly-matched (or highly-similar) tags between GP and M do not inflate the group appealing score of M if they are not significant/representative tags to G ( M , respectively).

Example 3. To illustrate the merit of using word-correlation factors in computing the GrpApp score of a candidate movie, consider the group profile shown in Fig. 2 and the candidate movies ML tag, i.e.,  X  X  X isney X  X  and  X  X  X rama X  X , respectively in their corresponding tag clouds that exactly matches its counterpart in the group profile of G (as shown in Fig. 2 ), which implies that the GrpApp score of ML account the remaining, i.e., non-exact-matched but analogous, tags in the tag clouds of the aforementioned movies in calculating their respective GrpApp scores, GroupReM computes a more accurate group appealing score for each candidate movie. GrpApp ( ML 1 , G ), computed using Eq. (1) , is 3.5, whereas GrpApp ( ML whole, is more interested in family , animated movies than dramatic movies, as captured in the group profile of G . 3.4.2. Popularity scores of movies
In addition to computing the GrpApp score of a candidate movie M for G , GroupReM also considers the global popularity lective interest in M expressed by users at the movie website of which members of G are users, and provides a higher ranking on M if it is more frequently bookmarked at the website than other candidate movies.
 Popular movies which attract the attention of users at a movie website are more likely to be bookmarked by the users.
GroupReM weights the fact that frequently-bookmarked movies may also be of interest to members of G . While solely rely-ing on the popularity of an item in performing the recommendations task (which does not apply to GroupReM) can lead to less diverse and useless recommendations ( Zheng, Wang, Zhang, Li, &amp; Yang, 2010 ), Adomavicius and Kwon (2012) claim that the accuracy of the recommendations can be enhanced by considering the popularity of an item during the recommendation process.

GlbPop , which is considered by GroupReM as an additional decision factor besides GrpApp to rank M to make recommen-dations, is computed as the total number of users at W who have bookmarked M . 3.4.3. Rank aggregation
Having determined the group appealing and global popularity scores of each movie M in Candidate _ Movies , GroupReM computes the ranking score of M by applying a popular linear combination measure, called CombMNZ Lee (1997) , which is frequently used in fusion experiments Cormack, Clarke, and Buettcher (2009) . CombMNZ considers multiple existing lists of rankings on an item I to determine a joint ranking of I , a task known as rank aggregation or data fusion. where N is the number of ranked lists to be fused, i.e., the number of input ranked lists, I ranked list c , and j I c &gt;0 j is the number of non-zero, normalized scores of I in the lists to be fused.
Prior to computing the ranking score of M , it is necessary to transform the original scores in each individual ranked list into a common range , which can be accomplished by applying Eq. (3) to each score in each ranked list so that it is within the range [0,1], a common range Lee (1997) .
 where S I is the score of item I in the ranked list c prior to be normalized, I respectively) score available in c , and I c is the normalized score for I in c .

GroupReM normalizes the group appealing and global popularity scores of M computed in Sections 3.4.1 and 3.4.2 , respectively using Eq. (3) . Thereafter, using CombMNZ, GroupReM (i) sets N = 2 (in Eq. (2) ), which is the number of (input) ranked lists of normalized scores with the original ones computed in Sections 3.4.1 and 3.4.2 , respectively, (ii) determines the overall ranking score of each movie M in Candidate _ Movies using Eq. (2) , and (iii) recommends the top-10 ranked movies to (the members of) G .

By adopting this fusion strategy, GroupReM considers the strength of each evidence, i.e., the GrpApp and GlbPop scores, as opposed to simply positioning higher in the ranking movies with a high GrpApp or GlbPop score.

Example 4. Consider the candidate movies ML 1 , ML 3 , ML (normalized) group appealing and global popularity scores as shown in Table 1 . Using CombMNZ as a rank aggregation measure, GroupReM identifies the most relevant movies, i.e., movies of interest, for G . Even though the (normalized) global popularity score of ML 1 is slightly lower than the global popularity score of ML the ranking of movies to be recommended. This is because ML tag cloud of ML 1 and the tags in the group profile of G that depict the movie preferences of (the members of) G .
As shown in Table 1 , the global popularity score of ML 5 significantly lower in comparison with the group appealing scores of the remaining candidate movies. As a result, GroupReM positions ML 5 lower in the ranking of movies to be recommended than the remaining candidate movies in Fig. 3 . 4. Experimental results In this section, we first introduce the dataset (in Section 4.1 ) employed for assessing the performance of GroupReM.
Thereafter, we present the evaluation protocol and group formation strategy adopted for creating the groups used for the evaluation purpose (in Sections 4.2 and 4.3 , respectively). We define the metric which quantifies the accuracy and ranking approach of GroupReM (in Section 4.4 ). We detail the empirical study conducted for verifying the effectiveness and efficiency of GroupReM and compare its performance with existing group recommenders on movies (in Section 4.5 ). 4.1. Dataset
To evaluate GroupReM in recommending movies appealing (to a certain degree) to the members of a group, we consider the MovieLens dataset GroupLens Research at University of Minnesota (2011) , a dataset released by the ACM HetRec Con-ference in 2011. Statistical information on MovieLens is shown in Table 2 . (See detailed information on the dataset at group-lens.org/system/files/hetrec2011-movielens-readme.txt.) Note that the MovieLens dataset was not developed for assessing the performance of group recommenders, since pre-defined groups of users are not provided in the dataset. For this reason, we create our own groups of users for the evaluation purpose (see details in Section 4.3 ). 4.2. Evaluation protocol
To assess the relevancy of group recommendations suggested by GroupReM, we have adapted a standard approach to par-tition the movies bookmarked by each user in the MovieLens dataset into two subsets and employed the five-fold cross val-idation approach Manning and Schutze (2003) . In evaluating the recommendations made by GroupReM for a given group G , in each of the five repetitions, 80% of the movies bookmarked in MovieLens by each member U of G were treated by Grou-relevance of the recommendations generated for ( U in) G . A recommendation made by GroupReM is treated as relevant for ( U in) G , if the recommended movie is included in the 20% of the movies (bookmarked by U ) withheld for the testing purpose, a commonly-employed protocol for assessing recommendation systems ( Bellogin, Cantador, &amp; Castells, 2010; Guan et al., 2010 ). 4.3. Group formation
To the best of our knowledge, there are no benchmark datasets available for assessing the performance of group recom-menders, needless to say group recommenders on movies. For this reason, we employ a popular strategy for generating groups (of users in the MovieLens dataset introduced in Section 4.1 ) for evaluation purpose.

In creating groups for evaluating the recommendations generated by GroupReM, we consider two important factors: the difficulty in reaching consensus among members of small versus large groups. We consider groups with 2 X 8 members, which are comparable to the group sizes defined in Amer-Yahia et al. (2009) and Baltrunas et al. (2010) , to demonstrate the effec-tiveness of GroupReM in recommending movies for small, as well as large, groups.

Besides group size, group cohesiveness is another important criterion ( Amer-Yahia et al., 2009 ) in evaluating group recom-menders. By using groups that include members with various degrees of cohesiveness, i.e., different degrees of user-to-user similarity, we can verify the correctness of GroupReM in generating recommendations for groups of users that may or may not share common preferences in movies, since the latter is more challenging than the former in terms of satisfying their groups are formed by randomly selecting users from MovieLens, regardless of their preferences on movies. Highly-similar groups include members with common interests in the same types of movies, whereas dissimilar groups reflect groups of people that are different in terms of their preferences in movies. To determine the users who should be included in highly-similar and dissimilar groups, we adapted the strategy employed in Baltrunas et al. (2010) , which calculates the user-to-user similarity, denoted User _ Sim , on each pair of users in MovieLens. The User _ Sim metric is introduced in Amer-Yahia et al. (2009) and computed as constraints a movie M to be treated as  X  X  X hared X  X  between u and u 0 if they both rated M within two units of each other on tively) similar ratings provided by u and u 0 on i indicates that u and u 0 share the same preference on i .
In computing the User _ Sim score between any two users (as defined in Eq. (4) ), only pairs of users who have rated at least five common items are considered, a common practice among CF-based recommenders which ensures that the correlation of items, i.e., less than five movies in our case, by the two users ( Baltrunas et al., 2010 ).

We follow the strategy proposed by the authors in Baltrunas et al. (2010) , who consider the distribution of user pairs in a given dataset (based on their user-to-user similarity) and treat the 33% of user-pairs with the highest user-to-user similarity who achieve the highest user-to-user similarity. Hence, a group of MovieLens users whose user-to-user similarity among each other is higher or equal to 0.11 is treated as a highly-similar group. Applying the same strategy to determine highly-with a User _ Sim score less than or equal to 0.06 constitute the 33% of user-pairs in MovieLens with the lowest user-to-user similarity (as shown in Fig. 4 ), and these users are treated as members of dissimilar groups.

Based on the group formation protocol defined above, we created 3,150 distinct groups, which are uniformly distributed among highly-similar, dissimilar, and random groups. In addition, each set of the 1,050 groups that share the same degree of cohesiveness is uniformly distributed based on the pre-defined group sizes, i.e., 2 X 8 members. Thus, for each distinct group size there are 150 groups in which group members share the same (pre-determined) degree of cohesiveness. 4.4. Metrics To assess the overall performance and ranking strategy of GroupReM, we employ the Normalized Discounted Cumulative Gain ( nDCG )( Croft, Metzler, &amp; Strohman, 2010 ) measure, which is a standard IR metric often used for evaluating group ommendations generated by GroupReM for a given group G of a particular size that includes members (without) sharing the same degree of cohesiveness, we calculate the nDCG for G as the average of the nDCG value computed for each of the group members in G , following the experimental setting adopted by Amer-Yahia et al. (2009) . nDCG 10 , as defined in Eq. (5) for evaluating the relevance of each batch of top-10 recommendations generated by Grou-pReM, penalizes relevant movies ranked lower . The penalization is based on a relevance reduction, which is logarithmically proportional to the relative position of each relevant movie in a ranked list of recommended movies (as shown in Eq. (6) ).
The higher the nDCG 10 score is, the better the ranking strategy adopted by the corresponding recommender system RS is, since a high nDCG 10 score on a list of recommendations L indicates that relevant recommendations generated by RS are positioned high in L . where N (which is 150 in our case) is the number of groups with a pre-defined number of group members such that the members share the same pre-determined degree of cohesiveness (as detailed in Section 4.3 ), i is the i th group for which GroupReM generates movie recommendations, M is the number of group members in i , k is the k th group member in i ,
IDCG 10, k (in Eq. (5) ) is the best possible DCG 10, k value for the recommendations generated by GroupReM for k , where rel j is the binary relevant judgment of the recommended movie at the j th ranking position and is assigned a value of  X  X 1 X  X  if the movie is a relevant recommendation for k (as defined in Section 4.2 ) and is assigned a  X  X 0 X  X , otherwise. 4.5. The effectiveness and efficiency of GroupReM
In this section, we first verify the correctness of relying on word-correlation factors and the popularity of movies to gen-erate group recommendations (as presented in Section 4.5.1 ). Thereafter, we compare the performance of GroupReM with existing CF-based group recommenders (in Section 4.5.2 ) and assess the efficiency of GroupReM and CF-based group recom-menders in performing the recommendation task (in Section 4.5.4 ). 4.5.1. The correctness of GroupReM
As stated in Section 3.4 , GroupReM depends on the group appealing (based on word-correlation factors) and global pop-ularity scores to generate recommendations of interest to a group. To verify the effectiveness of GroupReM in making group recommendations on movies, we conducted an empirical study in which we compared two alternative implementations of
GroupReM. The first alternative, denoted GroupReM _ Exact , relies solely on the group appealing score computed on exactly-matched tags for generating movie recommendations for a group G . In this case, the group appealing score of a candidate acterizing (the group profile of) G . The second alternative, denoted GroupReM _ WCF , relies on the word-correlation factors and considers analogous, besides exactly-matching, tags. GroupReM_WCF computes the group appealing score of each can-didate movie using Eq. (1) .

As illustrated in Fig. 5 , regardless of the degree of cohesiveness among group members in groups of any size, Grou-pReM_WCF consistently improves the accuracy of the recommendations generated by GroupReM_Exact. The 3% overall improvement on the (average) nDCG achieved by GroupReM_WCF over GroupReM_Exact, using the MovieLens dataset and the groups introduced in Section 4.3 , indicates that relaxing the exact-matching constraint by adopting word-correlation factors enhances the accuracy of movies recommended to a group by GroupReM_WCF. In addition, at least 8% overall improvement on the (average) nDCG scores achieved by GroupReM over GroupReM_WCF, using the aforementioned dataset, validates the fact that the global popularity score (as defined in Section 3.4.2 ) further increases the accuracy of group rec-ommendations than simply using the group appealing scores of movies to perform the group recommendation task (as illus-trated in Fig. 5 a X  X ). Note that the differences between GroupReM_WCF and GroupReM_Exact with respect to GroupReM, in terms of nDCG , are statistically significant, as determined using a Wilcoxon Rank Sum Test ( p &lt; 0.05). 4.5.2. Comparing the performance of GroupReM with existing group recommenders
To further verify and demonstrate the effectiveness of GroupReM, we compare its performance with two well-known CF recommenders on movies, which are based on Average (CF_AVG) and Least Misery (CF_LM) aggregation strategies ( Amer-
Yahia et al., 2009; Baltrunas et al., 2010 ), respectively. Given that GroupReM adopts an aggregated model approach to make recommendations, we also compare its performance with a CF recommender that employs an average aggregated model strategy (CF_AVG_AM). We have chosen CF-based recommenders for comparisons, since to the best of our knowledge there is no group recommender on movies that depends primarily on content descriptions to make recommendations.
Given a group G , both CF_AVG and CF_LM first generate movie recommendations for individual members of G by employ-ing the well-known CF strategy. Thereafter, the recommenders proceed to merge the recommendations generated for indi-vidual group members to create the list of movies to be recommended to G . While CF_AVG computes the score of a movie M for G by averaging the ratings of M predicted for each individual group member in G , CF_LM defines the score of M for G as the smallest predicted rating of M among all the rating predictions of M determined for each of the individual members of G . The top-10 movies with the highest ratings are recommended to G . (A more in-depth discussion on CF_AVG and CF_LM can be found in Amer-Yahia et al. (2009), Baltrunas et al. (2010) .) The CF_AVG_AM approach, on the other hand, generates a single group profile by averaging the ratings of each movie bookmarked by each individual member of G . Thereafter, the well-known CF approach is employed to generate a list of the top-10 highest ranked movies for (the profile of) G .
Prior to comparing the performance of the aforementioned recommenders with GroupReM, we have determined the rel-evance of each movie recommended by CF_AVG, CF_LM, and CF_AVG_AM for each of the groups constructed in Section 4.3 using the MovieLens dataset, evaluation protocol, and metric detailed in Sections 4.1, 4.2, and 4.4 , respectively.
Fig. 5 a X  X  show the nDCG scores achieved by GroupReM, CF_LM, CF_AVG, and CF_AVG_AM for highly-similar, dissimilar, and random groups of different sizes, respectively. The average nDCG score of GroupReM computed for groups with highly-similar users is 0.28, which is at least 12% higher than the average nDCG scores achieved by either CF_LM, CF_AVG, or CF_AV-
G_AM, which are 0.07, 0.16, and 0.14, respectively. The average nDCG score achieved by GroupReM for groups with dissimilar ( random , respectively) users is 0.24 (0.27, respectively), which also outperforms the average nDCG scores achieved by CF_LM,
CF_AVG, and CF_AVG_AM on the same groups, which are 0.07, 0.12, and 0.11 (0.08, 0.15, and 0.14, respectively). All of these nDCG values achieved by GroupReM are statistically significant over CF_LM, CF_AVG, and CF_AVG_AM (as verified using a Wilcoxon Rank Sum Test for p &lt; 0.05).

A higher nDCG value indicates that GroupReM is more effective than CF_LM, CF_AVG, and CF_AVG_AM in detecting and ranking higher in the list of recommended movies the ones that are relevant, i.e., of interest, to a group, regardless of the number of members in the group or the similarity among group members in terms of their preferences in movies. 4.5.3. Observations
Since only movies reserved for the testing purpose (as detailed in Section 4.2 ) are considered relevant, it is not possible to account for the potentially relevant movies that the users have not bookmarked. As a result, the nDCG scores in our empirical study are underestimated, which is a well-known limitation of the evaluation protocol (introduced in Section 4.2 ) applied to ers, i.e., (alternative implementations of) GroupReM, CF_AVG, CF_LM, and CF_AVG_AM, the nDCG values are consistent for the comparative evaluations ( Bellogin et al., 2010 ).
 Regardless of the degrees of cohesiveness among group members, the nDCG scores computed for GroupReM (CF_AVG,
CF_LM, and CF_AVG_AM, respectively) consistently decrease when the group size increases . This decrease in nDCG score is expected as more users are involved in a group, the harder it is to reach consensus among members in terms of choosing movies that represent the collective interests of the group. Moreover, regardless of the size of the groups under evaluation, the nDCG scores computed for GroupReM (CF_AVG, CF_LM, and CF_AVG_AM, respectively) are slightly higher when consid-ering groups with highly-similar users. This is anticipated, since the more similar the group members are with one other in terms of their preferences in movies, the more likely they will treat each recommendation the same, i.e., as (non-)relevant.
The results of the analysis on the performance of GroupReM (and other recommenders used for comparison purposes), in terms of the degree of cohesiveness among group members, correlates with the empirical study conducted in Amer-Yahia et al. (2009) and Baltrunas et al. (2010) .

Note that the fact that CF_AVG_AM and CF_AVG outperform CF_LM is anticipated, since the latter adopts a least misery strategy which favors the  X  X  X east happy X  X  group member in making recommendations. Furthermore, CF_AVG, CF_LM, and
CF_AVG_AM rely on identifying  X  X  X imilar-minded X  X  users within a movie community, i.e., a movie website, to generate movie recommendations. The search is applied to each member of a given group G . In doing so, CF_AVG, CF_LM, and CF_AVG_AM solely consider users of a movie website who rate the same movies as the ones that have been bookmarked and rated by members of G . Hence, the less  X  X  X imilar-minded X  X  the users are (with respect to a member U of G ), the less reliable are the ratings predicted for movies to be recommended to U (and G ). GroupReM, on the other hand, does not require locating  X  X  X imilar-minded X  X  users to perform the recommendation task. Instead, GroupReM, relies on content-similarity on tags and the popularity scores of the candidate movies. 4.5.4. Efficiency of GroupReM
Besides assessing the effectiveness of GroupReM, CF_AVG, CF_LM, and CF_AVG_AM on making movie recommendations to a group (in Section 4.5.2 ), we have also validated the overall efficiency of (the variations of) GroupReM, CF_AVG, CF_LM, and CF_AVG_AM in suggesting movies of interest to a group.

Fig. 6 shows the average time (in seconds) required for (the alternative implementations of) GroupReM, CF_AVG, CF_LM, and CF_AVG_AM to generate recommendations for the 1050 groups of various sizes, such that group members share the same degree of cohesiveness among one another, using the 5-fold evaluation strategy detailed in Section 4.2 . While
GroupReM_Exact achieves the shortest processing time, which is 68 s, the additional processing time required by GroupReM, which is 66(=134 68) s, is relatively insignificant, compared with the degree of accuracy achieved by GroupReM in generating recommendations of interest to a group, as shown in Section 4.5.1 .

GroupReM and CF_AVG_AM require similar processing time to generate recommendations. When compared with CF_AVG and CF_LM, however, GroupReM requires significantly less time, i.e., as illustrated in Fig. 6 , the processing time of CF_AVG and CF_LM increases by at least 8 min in comparison with the processing time of GroupReM.

To further assess the efficiency of GroupReM, we consider 450(=3 15) groups (regardless of the degree of cohesiveness among the members of the group) of MovieLens users of each pre-defined size, i.e., 2 X 8, for evaluation purpose (as detailed in
Section 4.3 ). We computed the average processing time of GroupReM in generating recommendations for each one of the 450 groups of pre-defined size. As illustrated in Fig. 7 , the (average) time (in milliseconds) required by GroupReM to generate group recommendations does not exponentially increase when the number of group members increases. Instead, as deter-mined by the curve created using the Microsoft Excel Trend/Regression tool (also shown in Fig. 7 ), the increase in processing time of GroupReM when the number of group members increases follows a linear trend, which demonstrates the scalability of GroupReM.

We have also evaluated whether the total number of movies bookmarked by the members of a group can significantly affect the group recommendation processing time of GroupReM. To draw a conclusion, we considered the 3150 groups de-fined Section 4.3 and calculated the processing time of GroupReM in generating recommendations for each of the groups, regardless of the size of the groups or the degree of cohesiveness among group members. As anticipated, the processing time (in milliseconds) required for GroupReM to generate recommendations increases as the total number of movies bookmarked by group members increases , as illustrated in Fig. 8 . However, even though the total number of movies bookmarked by group members is in the thousands, the processing time of GroupReM in suggesting movies of interest to a group is at most 2.5 s, which is a relatively short period of time. Furthermore, the increase in processing time follows a polynomial trend, as deter-mined by the curve created using Microsoft Excel Trend/Regression tool and as shown in Fig. 8 .

Note that independently of the 3150 groups introduced in Section 4.3 , we have empirically evaluated GroupReM on gen-erating recommendations for groups of up till 100 members. Based on the conducted experiments, we have observed that (i) the total number of movies bookmarked by group members remains in the thousands and (ii) the processing time of
GroupReM is at most 5 s, even when considering groups of approximately 100 members with thousands of movies book-marked among them. 4.6. Limitations of the current implementation of GroupReM
GroupReM, as currently developed, adopts a Top-N strategy and suggests a list of N movies to a group of users at a given time ( Deshpande &amp; Karypis, 2004 ). The current design of GroupReM does not consider the dynamic preferences of group members that may evolve over time. Moreover, the satisfaction of a group member U on the recommended items, i.e., movies in our case, may depend on other group members. As stated in Baltrunas et al. (2010) and Masthoff and Gatt (2006) , U can be influenced by other group members through emotional contagion and conformity. The former claims that U  X  X  satisfaction may be increased if other group members are satisfied with the recommendations, whereas the latter states that the opin-ions of other users may influence U  X  X  opinions. The recommendation strategy adopted by GroupReM, however, does not con-sider that some members of a group are more capable than others to influence the remaining group members in making decisions on the (non-)relevance of movies suggested to the group, an issue to be addressed as future work. 5. Conclusions and future work
With the popularity of social activities in which groups of people are involved, either online or in person, group recom-menders that are designed for identifying items of interest to a group play a significant role in social networking. One of the item domains that predominates on group recommenders is movies. Groups of friends, family members, and acquaintances, who gather to watch a movie at home or at the cinema, can use the service of a group recommender to find movies pertain-ing to their interests. Identifying movies to be recommended that appeal a group, however, is a non-trivial task due to the personal (and often diverse) preferences of group members in movies. We have introduced GroupReM, a group recom-mender on movies, which advances the current technology in solving the problem.

To suggest movies for members of a given group G at a movie website W , GroupReM first constructs a group profile for G , which captures the collective interests of members of G in movies. Hereafter, GroupReM relies on a simple aggregation model to determine the ranking score of each candidate movie M archived at W , which has not been bookmarked by members of G M at W so that the top-10 ranked movies are recommended to G .

Unlike existing group recommenders on movies, which are based on the collaborative-filtering (CF) strategy and rely so-lely on the ratings assigned to movies to perform the recommendation task, GroupReM takes the advantage of the richness of semantic information, i.e., (personal) tags, which are available at any movie website. Considering the content-similarity of movies and a group profile, GroupReM is not constrained to find users at a movie website who are  X  X  X imilar-minded X  X  based on ratings assigned to the same movies to suggest movies to a group, as CF-based group recommenders do. In addition, Grou-pReM employs word-correlation factors and considers non-exact-matched, but analogous, tags to more adequately deter-mine the degree of appeal of a movie to a group, which in turn enhances the accuracy of the recommendations.
We have conducted an empirical study using more than 3,000 groups of various sizes and degrees of cohesiveness among group members, who are users in the MovieLens dataset, to verify the effectiveness and efficiency of GroupReM. The exper-imental results indicate that GroupReM is highly accurate in suggesting movies appealing (to a certain degree) to the mem-bers of a group. We have compared the performance of GroupReM with three well-known CF-based recommenders and verified that GroupReM outperforms the aforementioned recommenders by a large margin, and the average processing time of GroupReM is significantly shortened in comparison to its counterparts.

GroupReM relies on personal tags assigned to movies that have been bookmarked by group members to create group pro-files and identify movies to be recommended. Occasionally, personal tags may not be available or they may be too broad in describing (the content of) a movie. We plan to investigate strategies that can be applied to infer tags that adequately rep-resent the content of movies, if personal tags are missing or too general, which can further enhance the accuracy of the rec-ommendations made by GroupReM. We also intent to enhance the recommendation strategy of GroupReM by considering the fact that some members of a group may influence the remaining group members in making decisions on (non-)relevant items suggested to the group.
 References
