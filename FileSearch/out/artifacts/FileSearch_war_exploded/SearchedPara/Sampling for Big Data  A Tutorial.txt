 One response to the proliferation of large datasets has been to de-velop ingenious ways to throw resources at the problem, using mas-sive fault tolerant storage architectures, parallel and graphical com-putation models such as MapReduce, Pregel and Giraph. However, not all environments can support this scale of resources, and not all queries need an exact response. This motivates the use of sam-pling to generate summary datasets that support rapid queries, and prolong the useful life of the data in storage. To be effective, sam-pling must mediate the tensions between resource constraints, data characteristics, and the required query accuracy. The state-of-the-art in sampling goes far beyond simple uniform selection of ele-ments, to maximize the usefulness of the resulting sample. This tutorial reviews progress in sample design for large datasets, in-cluding streaming and graph-structured data. Applications are dis-cussed to sampling network traffic and social networks.
Research in big data draws from many disciplines, both from ar-eas of analysis (including probability, data analysis, machine learn-ing and algorithms) and also systems (including distributed sys-tems, database architectures, and programming models). This tu-torial addresses several target audiences, all of whom may want to benefit from a greater understanding of the use of probabilistic methods for sampling and estimation in big data: Researchers in big data who background is not primarily concerned with probabilistic methods, but wish to learn about these as a com-plement to their current expertise.
 Researchers with a more classical background in statistics who wish to refocus to apply their expertise to problems in big data. Researchers in big data methodologies who wish to learn more about current applications of sampling in big data in the field.
Supported in part by a Yahoo! Research FREP award. Address from September 2014: Department of Electrical and Computer Engineering, Texas A&amp;M University, College Station, TX 77843-3128 Fundamentals of Sampling. Summarization as a means to control resource usage, sampling as special case of summariza-tion. General comparison of sampling with other summarization techniques, including aggregation and sketching: different trade-offs between scalability, flexibility, and ability for post-hoc explo-ration. Sampling as a mediator between data characteristics. Un-biased estimation and the Horvitz-Thompson approach. Variance and Covariance Estimators. General purpose samples vs. targeted estimators.
 and generalizations to weighted streams, via Reservoir Sampling, Sample and Hold and Counting Samples. Inclusion Probability Proportional to Size Sampling, including Threshold Sampling, Pri-ority Sampling, Variance Optimal Sampling, and Bottom-k Sam-pling Sketch guided sampling, Structure Aware Sampling, Sam-pling from the distinct objects via ` 0 sampling, and ` p a source of Permanent Random Numbers (PRN). Hashing algo-rithms, including Universal Hashing and implementations. Ad-vanced topics may include Minwise hashing, Consistent Weighted Hashing, Hashing with Timeouts, Coordinated Sampling, or Tra-jectory Sampling Graph Sampling. Network sampling, ego-net sampling, and why simple edge or node centric sampling fails for more complex properties. Topics may include Biased sampling methods, Ran-dom walks, Snowball and respondent driven sampling, forest fire sampling, Graph stream sampling, Horwitz-Thompson estimation in graphs, or Triangle counting.
 Sampling Applications. Network Traffic Measurement (Net-flow, IPFIX, PSAMP); Sampling and Approximate Database Queries (Priority Sampling in Databases, BlinkDB); Social Networks (So-cial activity streams, Sampling and information diffusion). Graham Cormode is a Professor in Computer Science at the Uni-versity of Warwick in the UK.
 Nick Duffield is a Research Professor at Rutgers University/DIMACS, New Jersey, USA.
