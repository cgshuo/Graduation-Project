 MARTIN POTTHAST, BENNO STEIN, FABIAN LOOSE, and STEFFEN BECKER, Numerous Web sites invite visitors to comment on their content. To this end, com-ment boards are provided at the bottom of Web pages where submitted comments are shown in chronological order. Comments are one of the first kinds of user-generated Web content, and virtually all types of items are commented, be they texts, images, songs, videos, products, ideas, or personal profiles. Comment boards have not changed much since their debut in Web-based guest books. Their very purpose is to collect user feedback, but they provide practical value to visitors as well. Hence we con-sider comment boards as a form of social software that create a community centered around the commented item on the respective Web page. Comment boards serve as a paradigm to exploit the wisdom of the crowds since, ideally, commenters share their opinion, their criticism or extraneous information. Unlike tagging, blogging and  X  X iki-ing, X  commenting may not be considered work. In practice, however, comment boards appear less useful to the naked eye: popular Web pages get flooded with up to thousands of comments, an amount that cannot browsed by an individual user. Moreover, many comments are utterly irrelevant, spam or replications, which is why comments are often neglected as a source of useful information. It is all of these obser-vations that form the starting point of our research.

The article presents a study of the commentsphere, beginning with a discussion of the peculiarities of comment retrieval in Section 1.1. A survey of existing comment retrieval research is given in Section 2: we identify filtering, ranking, and summariza-tion as important comment retrieval tasks, which in turn can be classified as comment-targeting or comment-exploiting. Within this framework, Sections 3 X 6 report on four case studies: The original contributions of the case studies include: a comment filtering model based on writing style features, a comment ranking model based on novelty detection by similarity reduction, a comment summarization model based on a sentiment word cloud visualization, and a retrieval model for measuring cross-media similarity using comments. In the following, comment retrieval is compared to information retrieval. Let d q be a Web page or an item (e.g., a text, an image, a video) about a particular topic.  X  Information Need . In comment retrieval, a user X  X  interest in d q is understood as an information need q that targets comments on d q and d q  X  X  topic. It is not assumed that d q covers a topic exhaustively.  X  Query . Formulating a keyword query that targets comments (in the sense of  X  X e-trieve all comments that contain more information on d q  X  X  topic. X ) is hardly pos-sible. However, a good characterization of d q  X  X  topic X  X nd hence the information need X  X s d q itself, which hence should be used as query document.  X  Relevance . A relevant comment on d q is a comment that complements the infor-mation of d q in some respect. It is an information retrieval challenge to develop retrieval models that capture this kind of relevance.

Why is it unreasonable to formulate keyword queries against comment sets, just as we do against the Web? The answer is not straightforward: a well-known principle of information retrieval states that nothing can be retrieved about an interesting topic without having a-priori knowledge about it [Belkin et al. 1982]. Within Web search tasks, users formulate queries based on their incomplete knowledge about the topic for which they seek documents. The same holds true for retrieving comments, but at a much finer granularity. Since comments are short, the amount of knowledge in a comment is limited to a few facts. For example, to retrieve a comment, the user requires at least partial knowledge of the facts within. There is a (fuzzy) bound for the amount of a-priori knowledge about a fact x to be exceeded before users will start to search for x , and comments may be considered well below this bound. Another practical aspect prevents users from searching comments manually: why bother to search for a fact in a small, arbitrary set of comments when a Web search engine is only one click away?
A good description of what users actually want to find in a set of comments on d q is some kind of  X  X urprise. X  This can be complementary information but also a joke. Either way the invariant is that the comments to be retrieved are on the same topic as d q . Classical retrieval models (e.g., algebraic models like the vector space model, la-tent semantic indexing, explicit semantic analysis or probabilistic models like binary independence, the unigram language model or latent Dirichlet allocation) are not well-suited to measure such connections: when computing the relevance between d q and some comment d , term overlap or concept overlap is measured in first place, which is obviously inappropriate for our purpose. In its extreme form, the classical mod-els consider a comment as most relevant that duplicates d q , although it contributes nothing. An information retrieval system organizes a set of real-world documents for efficient retrieval and provides a user interface to specify information needs. The article in hand denotes a real-world document as d , an information need is denoted as q ,and sets of documents and information needs are denoted as D and Q respectively. The heart of an information retrieval system is a retrieval model R , which provides both the means and the rationale to compute document representations D for the real-world documents D , formalized information needs Q for their real-world counterparts Q , and a relevance function  X  : Q  X  D  X  [0 , 1].  X  estimates the relevance of some d with respect to some q as  X  ( d , q ).

The term  X  X ommentsphere X  is derived from the term  X  X logosphere X ; that is, the com-mentsphere is made up of all user-generated comments on Web items. The term was coined in a comment on a blog post asking how blogs may be improved, in which one commenter suggested  X  permalinks in the commentsphere  X  [Wolff 2004]. To the best of our knowledge, the first scientific paper mentioning the term is [Mishne and Glance 2006]; a variant is the term  X  X ommentosphere X  [Schuth et al. 2007]. This section presents a survey of research related to retrieval tasks in the comment-sphere. Based on an analysis of 59 relevant papers, we identify three main retrieval tasks where comments are successfully utilized: filtering, ranking and summarization. Other tasks that have attracted less attention include discourse extraction and popu-larity prediction. We distinguish the mentioned tasks with respect to their retrieval targets, which are either the comments themselves or the commented items. We term the underlying paradigms as comment-targeting and comment-exploiting. Research that targets comments is organized in Tables I and II. Special emphasis is placed on the underlying retrieval models. Two-thirds of the surveyed papers address only one specific comment type , namely reviews of products or movies. Reviews play an important role in decision making when buying something online, which renders this comment type particularly interesting. Also, reviews are rated by other users, which means that they can be used for evaluation purposes. Research on reviews is known as opinion mining, and it relies on technologies developed for sentiment analysis. We emphasize the following distinction: while the very purpose of sentiment analysis is the identification of subjectivity and polarity, opinion mining employs sentiment as a single feature among many for the analysis of reviews.

All comment retrieval models rely on a feature-based vector representation, where a feature is a possibly elaborate function that quantifies a certain aspect of a com-ment. Table I organizes the variety of the found features according to nine research fields. Table II shows an overview of the analyzed papers, whereas the retrieval model (multi-)column illustrates the feature usage by referring to the nine research fields of Table I: the  X  -symbol indicates the most important feature group of a model, which can be extended by features from other feature groups (indicated by the  X  -symbol). We also analyzed the related work with respect to the different approaches for relevance quantification.  X  In comment filtering, the quality of a comment is quantified, where  X  X ood quality X  refers to good writing style, the absence of vandalism and the absence of extreme sentiments. Also, the reputation of a commenter is taken into account, presuming that reputable commenters will not write low-quality comments in the future.  X  In comment ranking, the relevance of a comment is put on a level commensurate with its helpfulness as it is perceived by other users. The existing retrieval models try to capture the concept of helpfulness from the human-labeled product reviews in order to predict the degree of helpfulness of a new review. This approach is related to the learning-to-rank paradigm.  X  In comment summarization, the prevalent approach is the extraction of sentences that express an opinion. The relevance of a full comment is not considered, but the importance of sentences in describing the content of a comment or all comments is.
It can be observed that the commenters on comment boards begin to discuss or even argue about the commented item d q . Most comment boards, however, do not support discussion threading or record the reply-to structure. In this regard, Mishne and Glance [2006] tried to analyze whether a dispute is taking place on a comment board D . They represent the comment set D as a feature vector using features from almost all groups mentioned in Table I, train a decision tree classifier based on 500 an-notated comment boards and achieve a classification accuracy of 0.88. Dit et al. [2008] and Schuth et al. [2007] go one step further and attempt to extract the discussion hierarchy from D .

Remarks. The wide range of features that are employed within the comment re-trieval models shows the different views that can be taken on the data and makes comment-retrieval an interdisciplinary research field. A special case is comment filter-ing where many authors employ user-centered features and domain knowledge about reviews. These features are not applicable to other comment types since comments are often posted anonymously and do not necessarily review something.

Most of the comment ranking models use a comment X  X  helpfulness as relevance mea-sure, which is a reasonable choice for comment ranking. It must be considered, though, that helpfulness is hardly ever defined but derived from the ground truth of the eval-uation corpora. If, for example, a model is trained on Amazon reviews it is an open question whether a  X  X omain transfer X  to other comment types will work. Moreover, it stands to reason that the initiating (query) document d q for a set of comments D introduces a bias in the relevance assessments, which may not be crucial for reviews but for comments in general. The existing models measure the relevance of comments based on a static, predefined information need, since they do not consider d q .
Comment summarization is done by extracting the  X  X mportant X  sentences from comments. Again, this can only be done reliably for reviews but not for comments in general: unlike reviews, comments tend to be short and messy, rendering sen-tence extraction and the quantification of their importance difficult. Altogether we ob-serve that, while being an active research field, comment-targeting retrieval currently focuses too much on reviews. That is, most likely the proposed models are not adequate for the wider commentsphere. When retrieving Web items for which a sufficient number of comments are available, the comments can be used to raise the retrieval recall. This was first observed by Mishne and Glance [2006], who performed a large-scale analysis on the importance of comments within the blogosphere. They found that comments account for up to 30% of its size and that the use of comments improves the recall of blog search by 5%-15%, indicating that comments are a vital part of the blogosphere. This work is also the first to assess the intrinsic value of comments. In blog retrieval, in order for a the query, which was also the modus operandi of the TREC blog track [Ounis et al. 2008]. Recently, the same idea has been applied to video retrieval, in which comments provide a rich text resource as well X  X ignificantly larger than user-supplied tags or video titles [Cunningham and Nichols 2008; Yamamoto et al. 2008; Yee et al. 2009]. There is no doubt that this idea can be applied to image retrieval, music retrieval, or other types of Web items as well. In Section 6, we show that comments can be used to compare Web items across media.

Comments are also exploited for the summarization of Web items [Delort 2006; Hu et al. 2007, 2008; Park et al. 2008]: given an item d q and comments D on that item, the task is to generate a summary of d q . The comments on d q are evaluated to find the often-referred-to parts of d q . These parts are then used for the summary, circumvent-ing the problem of identifying them solely based on d q .

Similarly, filtering by comment exploitation is a viable monitoring and maintenance technology. Given a Web item d q about which commenters are outraged, which may be detected using the aforementioned dispute classification approach of Mishne and Glance [2006], the Web item could be automatically selected for reexamination by a site administrator. Though we have not found research addressing this, it is very likely that such measures are already being taken on sites such as YouTube, for example.

Another important task in this regard is the prediction of the popularity of a Web item based on its comments [Jamali and Rangwala 2009; Kaltenbrunner et al. 2007; Mishne and Glance 2006; Szab  X  o and Huberman 2008; Tsagkias et al. 2009; Yano et al. 2009]. Here, the comments are treated like time series data, using features, such as the increase of comments per time frame, to predict whether the commented item will become popular.

Remarks. Research on comment-exploiting retrieval is more diverse compared to the research targeting comments. This is in the nature of things as the ways in which comments can be exploited for different purposes cannot be enumerated. Approaches to comment-exploiting retrieval cannot be compared across different retrieval tasks. Within each comment-exploiting task, however, the literature is few and far between, which indicates that comments have not yet been adopted as a source of valuable in-formation about the commented item. By highlighting comment-exploiting retrieval as a paradigm, we hope to foster research in this direction. The fifth column in Table II shows places where comments can be found for evaluation purposes. Since most of the research is about reviews, Amazon is used most often as a corpus. However, there are plenty of other Web sites that may be useful in this respect (e.g., YouTube, Flickr, Last.fm, Digg, Picasa, or newspaper sites). Although there is no lack of comments in general, comments with human annotations are rare; exceptions include Amazon, Digg, and Slashdot. For our studies, we have compiled two evaluation corpora based on comments from Slashdot and YouTube; both are available to other researchers upon request.

Slashdot Corpus. Slashdot is a news Web site for publishing and commenting technology-related news articles. The publishing process is based on a moderation sys-tem in which users can submit an article d q , and Slashdot X  X  editors decide whether or not d q will be published. For each published article a comment board D is available, many of whose comments are categorized by Slashdot X  X  comment moderators. Eight predefined comment categories are used: four of which are considered  X  X ositive X  and four  X  X egative X  (see Table III for a short characterization). Based on the categories assigned by different moderators, an integer score is computed for each comment. The accounting of all assessments is mapped onto a range from  X  1 (negative) to +5 (max-imum positive). Unfortunately, the scores do not reflect how many moderators are involved in an assessment.

We have downloaded all Slashdot articles from January 2006 to June 2008, includ-ing all comments. In total, 17,948 articles were published during this period, and about 3.8 million comments were posted. Comments are organized as discussion threads, which means that a large fraction of the comments are not direct responses to an arti-cle, but responses to other comments. Only a small fraction of all comments has been categorized by moderators. Our experiments are based on the 311,167 categorized, direct responses. Together, the second and third quartile of the articles get between 16 to 41 direct comments, while the second and third quartile of the comment lengths range from 1 to 45 words. With respect to the distribution of the comments on the categories, there seem to be only very few low-quality comments on Slashdot (see Ta-ble III). However, one should be careful to consider this result as an accurate picture, considering most comments are not categorized and Slashdot policies encourage mod-erators to categorize positive rather than negative (i.e., moderators may spend time finding good comments instead of wasting time reading bad ones).

YouTube Corpus. YouTube is a video sharing Web site for homemade videos. The comments on videos are typically very short, and quite often thousands of comments per single video can be found. Since only a single video is associated per YouTube page, and since most comments are very short, we assume that most of them are some kind of opinion expression regarding the respective video. Explanations or discussions are less frequently observed than on Slashdot, for example. This makes YouTube comments especially interesting for opinion summarization. We downloaded 9.8 mil-lion comments from YouTube that were posted on 64,830 videos that appeared on several YouTube feeds at the end of 2008. Due to limitations of the YouTube API, only up to 1,000 comments per video could be retrieved, and it was not possible to adjust the time frame in which a comment or a video has been posted.

The following sections present four exploratory studies on the corpora that relate to the retrieval tasks discussed previously, the filtering of low-quality comments on Slashdot (Section 3), the ranking of comments on Slashdot (Section 4), the summa-rization of mass opinion in YouTube comments (Section 5) and the retrieval of Web items across media between Slashdot and YouTube by exploiting the associated com-ments (Section 6). Given a set of comments D , the task is to filter all comments of extremely low qual-ity, particularly comments from spammers and vandals. The case study investigates whether comment filtering on Slashdot can be done on the basis of a writing style anal-ysis. This analysis is interesting since existing retrieval models for this task depend primarily on user modeling [Veloso et al. 2007]. We assess a comment X  X  quality by its readability, which, in turn, depends much on its writing style. User-generated content on the Web particularly lacks in this respect since users tend to use common speech, they do not revise their writing for grammati-cal and spelling errors, and they often neglect punctuation and capitalization. On the contrary, many users seem to prefer good writing over bad writing, since comments with better style achieve consistently higher ratings on Slashdot. For this reason, as well as to ensure generalizability, our feature selection comprises features from lin-guistic stylometry and vandalism detection. The latter is an especially important fea-ture class targeting low-quality and ill-intentioned comments and was proposed for use in the detection of vandalism on Wikipedia [Potthast et al. 2008]. We use the following features (see Table I).  X  NLP. The frequency of prepositions and interjections indicates common speech.  X  Style1 . The comment length indicates whether or not a commenter puts effort in her writing. This feature achieves a remarkable performance in discriminating high-quality Wikipedia articles [Blumenstock 2008].  X  Style2 . Readability formulas, such as the DC Formula [Chall and Dale 1995; Dale and Chall 1948], the FK Grade [Flesch 1948; Kincaid et al. 1975] and the GF Index [Gunning 1969] indicate the sophistication of language use:  X  Vand1 The compression rate of a comment X  X  text.
  X  Vand2 The deviation of a comment X  X  letter frequency distribution from the expecta-tion. This as well as the first vandalism feature indicate bad writing or nonwriting (e.g., when a commenter hits the keyboard randomly).  X  Vand3 The normalized frequency of vulgar words found in a comment. Based on a set D of categorized comments from the Slashdot corpus, a dichotomous classifier c : D  X  X  0 , 1 } is trained on the feature representations D of D .Na  X   X ve Bayes is used as the classification technology. We have also experimented with SVM clas-sifiers but despite their otherwise good performance, Na  X   X ve Bayes could not be out-performed. The performance is measured as precision and recall with respect to each class  X  X  0 , 1 } , indicating the negative and the positive comment category on Slashdot. The training is based on a tenfold cross-validation; Experiment 1 in Table IV shows the achieved performance results. At the bottom of the table, a baseline is given in which all comments are classified as positive.

Two issues render our classification approach particularly difficult: the class im-balance and the short length of the comments. Keeping these problems in mind, the results of the first experiment are promising but not overwhelming: only a small portion of negative comments are classified as such. Another issue that needs to be addressed in this regard is the category  X  X unny. X  Funny comments are a vital part of Slashdot. We presume that neither of our features nor any of those we analyzed for our survey is capable of capturing funniness. One of the few publications targeting hu-mor retrieval is Mihalcea and Pulman [2007], wherein the authors try to distinguish humorous texts from other texts. But the particular case where a humorous text is a response to another not necessarily humorous text has not been studied. The Slashdot corpus appears as a valuable resource for humor retrieval.

We have conducted three additional experiments in which the  X  X unny X  category was either swapped from positive to negative, dropped or considered as a third class alto-gether. The results are also shown in Table IV. As is evident, when considering funny comments as negative the classification performance is significantly improved, which may be an indication that funny comments look similar to negative comments. Note that dropping funny comments results in a better performance as well. Considering funny comments as a third class does not work since they cannot be significantly sep-arated from negative comments using our retrieval model. In Reyes et al. [2010] we investigate this issue further.

Compared to the results of [Veloso et al. 2007], who study the same classification task, we achieve a similar classification accuracy. Note however, that we employ an entirely different feature set: the retrieval model of Veloso et al. is specific to Slashdot since it is based primarily on a user model, whereas our model can be considered as domain-independent, more robust and applicable to anonymous comments. Finally, Experiment 5 measures the classification performance of single features. In contrast to the findings of Blumenstock [2008], the comment length feature Style1 does not improve over the baseline, which is also the case for the vandalism feature Vand1. Given a document d q and a set of comments D , the task is to rank the comments according to their novelty with respect to d q . Ordinary novelty detection identifies sentences in a document stream that complement facts already known to the user [Soboroff and Harman 2005]. Analogously, d q encodes a user X  X  a-priori knowledge be-fore exploring D . The case study investigates the applicability of novelty to comment ranking. This analysis is interesting since none of the existing approaches include d q in their retrieval models, which may be acceptable for review ranking but not for general comment ranking. We analyze whether the well-known maximal marginal relevance (MMR) model works for Slashdot comments. Moreover, we propose a new model, called ESA , and compare it to MRR. Both MMR and ESA are metamodels, since they employ a generic retrieval model in a sophisticated fashion in order to boost their performance. Here, the generic retrieval model is a tf -weighted vector space model, which hence is also a good baseline for comparison.

Maximal Marginal Relevance. Under the MMR model, the most relevant comment that complements a given query document d q is computed iteratively from the com-ments D on d q , based on a subset S  X  D that the user already knows [Carbonell and Goldstein 1998]. In the i -th step, the comment d i at rank i is computed as follows: where S = { d q , d 1 ,..., d i  X  1 } are the a-priori known comments,  X  is the cosine similarity, and  X  adjusts the trade-off between d i  X  X  similarity to d q and the novelty of d i in D . Initially, S contains only d q . Note that the relevance of d i to d q is quantified as the value that maximizes the right-hand side of the equation. In accordance with the literature, we chose  X  =0 . 8.

Similarity-reduced Explicit Semantic Analysis. ESA is based on the explicit se-mantic analysis paradigm [Gabrilovich 2006; Gabrilovich and Markovitch 2006, 2007]. The original ESA model represents a document d as a vector d | D similarities of d to documents from a collection D I , referred to as index collection. The similarities of d to D I are computed using the vector space model. Each document from D I is considered as the description of a particular concept, and documents from Wikipedia have been successfully applied in this respect. The supposed rationale of ESA is to represent d in a concept space that is defined by the index collection. Within ESA, two documents d q and d are compared by computing the cosine similarity be-tween the concept vectors d q | D
The ESA model introduces a level of indirection to the similarity computation in the form of the index collection D I . This way, connections between d q and d may become apparent that are not obvious when looking only at vocabulary overlap. In ESA we intend to extract exactly this portion of similarity by first reducing the overlap be-tween d q and d : all terms from d that appear in d q are removed. What remains in the reduced comment d = d \ d q is considered as the comment X  X   X  X isible novelty. X  To quantify the relatedness of d q with regard to d , the ESA vectors d q | D compared. Two experiments were conducted on the Slashdot corpus.

Experiment 1: Score Ranking. The comment scores on Slashdot define a ranking that can be used to evaluate a retrieval model with respect to its ability to capture relevance. For the comments D of each article d q , the average relevance value of all comments D i  X  D with score i  X  X  X  1 , 0 ,..., 5 } to d q was computed. The left plot in Figure 1 shows the results. The standard deviations for the models are as follows:  X  model show a comparable similarity distribution in which medium scores are ranked highest on average, while MMR places the comments in a natural order. However, the ESA model achieves significantly higher similarity values than does the vector space model, while the relevance values computed with MMR appear to be rather small. The latter is not a problem as long as the desired ordering of the comments is achieved. To determine whether this is indeed the case, we have also computed the graded relevance scores NDCG, ERR, and Kendall X  X   X  , both on the entire rankings produced by the three models and restricted to the respective top 10 comments only. In addition, these scores have also been computed for a random ranking as a second baseline. Table V shows the results. As can be seen for the complete rankings, neither of the models clearly outperforms the other, and what is more, neither of the models outperforms the random baseline. On the top 10 comments, at least the latter is achieved, while the three models remain almost indistinguishable.

Experiment 2: Category Ranking. This experiment follows the design of Experi-ment 1, but computes the average relevance value of a comment to d q per category. Hence, based only on positive and negative judgments, a less fine-grained ranking is demonstrated (e.g., if a user wants negative comments to be presented after the pos-itive comment). The right plot in Figure 1 shows the results. Observe the difference in the distributions of the vector space model and MMR to the ESA model: the latter achieves significantly higher similarities for the positive categories (except for  X  X unny X ) than for the negative categories, whereas the vector space model and MMR show al-most no discriminative behavior.

Remarks. The results of this study should be interpreted with caution: the models induce a sensible ranking only in terms of averaged values while their graded rele-vance scores are inconclusive. Undesired rankings occur with a nonnegligible proba-bility. We conclude that the task of ranking comments is in its infancy and should be subject to further research. Given a set of comments D , the task is to generate a short text or a visualization that overviews the contents or the opinions of D : users will quickly get an idea about the comments without having to read everything. The case study investigates the use of word clouds for opinion summarization of comments on YouTube. 1 This analy-sis is interesting since existing comment summarization approaches rely on sentence extraction and are prevalently applied to distill customer product reviews. The re-spective technology cannot directly be applied to comments, which are significantly shorter than reviews X  X ost of the comments found on media sharing sites do not even contain one sentence. It is unlikely that relevant information can be found in such comments except the opinion of the commenters. Short comments in particular are tedious to read, which is why a suitable summarization for them is desired. While a single opinion may not be very useful (especially if no argument is provided), the fact that popular items inspire thousands of people to share their opinions allows us to generate a representative opinion summary. The summarization of a comment set D divides into an offline step and an online step. Suppose that two dictionaries V + and V  X  are given, comprising human-annotated terms that are commonly used to express positive and negative opinions respec-tively [Stone 1966].

In the offline step, the well-known sentiment analysis approach described by Tur-ney and Littman [2003] is used to extend V + and V  X  to the application domain. The extension is necessary in order to learn terms that are not covered by the dictionaries, and it is not feasible to do this manually. The semantic orientation SO of an unknown word w is measured by the degree of its association with known words from V + and V  X  : where assoc( w, w ) maps two words to a real number that indicates their association strength. If SO( w ) is greater than a threshold  X  (less than  X   X  ), w is added to V + ( V  X  ); otherwise w is considered as neutral. The point-wise mutual information statistic is applied as an association measure: where p ( w  X  w ) denotes the probability of observing w together with w ,and p ( w )is the probability of observing w .

In the online step, when a set of comments D is observed, two summary term vec-tors s + D and s  X  D are constructed in which the absolute frequencies of the positive and negative terms occurring in D are counted, based on the dictionaries V + and V  X  .Ifa word does not occur in the dictionaries, it is considered neutral. We visualize s + D and s  X  D as word clouds. Figure 2 shows examples where s + D and s  X  D are contrasted. Word clouds are arrangements of words in 2 or 3 dimensions in which im-portant words are highlighted [Seifert et al. 2008]. Here, the words are colored accord-ing to their sentiment polarity, and they are scaled according to their term frequency. The font size of a word is computed as follows: where max size is a predefined maximum font size and tf ( w ) is the term frequency of w as counted in s + D or s  X  D ,and V = V +  X  V  X  . For smoothing purposes, the logarithm of the numerator and the denominator may be taken.

A few more issues need to be addressed in practice: emoticons and exclamations (such as  X :-) X  or  X  X ol X ) require an additional set of detection rules since commenters often vary their spelling, spelling errors (which are abundant in comments) need to be corrected on the fly, and negations in front of opinion words need to be detected as a heuristic to determine the orientation of a word in context.

An important part of the visualization is the percentages of positive and negative words found on top of the word cloud (see Figure 2). For a quick overview these numbers are sufficient; however, when a user wants to know more about what other commenters thought, a click on any word in the cloud produces a list of comments containing it. As mentioned before, we have implemented a browser add-on that sum-marizes YouTube comments and Flickr comments on-the-fly. A lot of user feedback was obtained this way; from which it became clear that users find the summary interesting and useful, yet they criticize that sometimes words from comments are considered neg-ative (positive) although they have been used in a positive (negative) way. For future developments, it is planned to incorporate sentiment classification of whole comments.
Finally, it is noteworthy that the word cloud shown on the right in Figure 2 inspired us to investigate cross-media retrieval by exploiting comments (see the next section). In this particular case, the top 3 neutral words perfectly explain what the video is all about: a cat playing the piano. Our final case study investigates whether a set of comments D can be used for retrieval purposes (i.e., whether the combined knowledge of D tells us something about the commented item d q ), thus allowing for the comparison of items across media. To the best of our knowledge, we are the first to analyze this possibility. 2
Cross-media retrieval is a subproblem of multimedia information retrieval, which again divides into various subtasks. Here, we consider the following task: given a set of items of different media types, the task is to pair those items that are similar with respect to their topic, regardless of their media type. A primary goal of cross-media retrieval is the construction of retrieval models that bridge the gap between different media types by means of identifying correlations between low-level features and se-mantic annotations. We approach this problem from a different perspective through the use of comments in lieu of the commented item. This way, model construction is not an issue since well-known text retrieval models can be directly applied. Although the text surrounding a nontextual item has always been used to extract annotations in multimedia information retrieval [Deschacht and Moens 2008; Inoue 2004; Lew et al. 2006], comments in particular have not been considered in this respect.

A premise of our approach is that comments have to describe the commented item to some extent, which is analyzed in Potthast [2009]. In short, we found that comments on text are descriptive: 10 comments are sufficient to reach a considerable similar-ity between a text and its comments that is not rooted in duplication, while 100 X 500 comments contain a significant contribution of the commenters beyond the commented text. This case study proceeds in this direction. A standard vector space model with tf  X  idf term weighting is used as our retrieval model. Given a Web item d q and its associated set of comments D , d q is represented as a term vector d q based on the index terms found in D , while applying stop word reduc-tion and stemming. In the case that d q is a text document, as in the Slashdot corpus, the index terms found in d q are also included in d q . The representations of two items, d q and d q , are compared using the cosine similarity. Though nearly every retrieval model can be employed for this task, we resort to a simple vector space model in order to determine how robust a cross-media similarity assessment can be accomplished. Given the evaluation corpora we have described, 6,000 videos from the YouTube corpus were sampled and compared to each of the 17,948 Slashdot articles. This resulted in about 107.7 million similarities being computed. Slashdot and YouTube are similar in that both are community-driven Web sites, so that at least some topical overlaps can be expected. However, since both corpora have been compiled independently, we were not aware of existing overlaps. Figure 3(a) shows the obtained similarity distribution as a percentage of similarities over similarity intervals with an interval resolution of 0.01.
From all compared pairs of articles and videos, we have sampled a total of 150 for manual inspection by means of stratified sampling from similarity intervals of range 0.1. The sampled pairs were then classified into categories of topical match, namely pairs with equal topic, related topics, and unrelated topics. Figure 3(b) shows the obtained results. Topic overlaps start appearing at a similarity of 0.3, which may already be considered a  X  X igh X  cross-media similarity for its considerable positive de-viation from the expectation. As the comment-based cross-media similarity increases, more and more items with related or equal topics can be observed. Within high similar-ity ranges, pairs of articles and videos with equal topic appear most often. In addition to this manual inspection, the top 100 most similar pairs were evaluated with respect to their topical match. Table VI gives a detailed overview of these item pairs, and Ta-ble VII shows a selection of matching item pairs: 91% of the top item pairs have in fact equal or related topics. The similarity values in the table give an idea about the measured similarities and their standard deviation (stdev). Few false positives achieve high similarities, but are based on a lot more comments on the side of Slashdot. Ob-serve that the number of comments appears to correlate with the similarity, and that more comments possibly result in a  X  X opic drift. X  Often, the title of a YouTube video is descriptive, and hence we have determined the percentage of pairs where the video title overlaps with the Slashdot article. On average, this is the case in 60% of the ex-amined item pairs, which means in turn that in this experiment 40% of the top 100 matching item pairs would not have been identified based only on their titles. This article studies the commentsphere from an information retrieval perspective. We present a comprehensive survey of related work and, based on this survey, identify the most relevant retrieval tasks with respect to research effort and user impact: the filtering, the ranking and the summarization of comments as well as their exploita-tion for the same tasks on Web items. In addition, there are a number of secondary retrieval tasks that are no less exciting, including the prediction of a Web item X  X  popu-larity based on comments. We conducted a case study for four of the tasks mentioned previously. For this purpose we compiled adequate corpora that are available to other researchers in order to foster the research activities in this field. Within the case stud-ies, special attention was paid to the used retrieval models: our objective was to point out differences to existing retrieval models and to provide a better understanding of the challenges for retrieval tasks in the commentsphere. Moreover, we developed new retrieval model variants to address part of these challenges. Our achievements from a retrieval model perspective are:  X  Feature Overview. We compile an overview of features used to represent comments.  X  Retrieval Models for Filtering and Ranking. We propose a domain-independent model to filter low-quality comments that competes with other models. With ESA we present a new retrieval model to measure novelty for comment ranking.  X  Cross-media Retrieval. We introduce a new cross-media similarity analysis paradigm.
 From a research perspective, we consider the following directions as promising:  X  Retrieval Models. Current research focuses on product and movie reviews, but Web comments in general are much more diverse and require new tailored retrieval models.
  X  Humor Retrieval. If surprises are what people expect from comments, they may be funny at the same time: humor retrieval has not recognized comments as a research subject.  X  Multimedia Annotation. If comments capture a commented item X  X  topic well, it should be possible to extract tags and annotations for the commented item from its comments.  X  Ranking and Summarization. Comment boards show comments in chronological order, but with increasing comment number the overview gets lost. Hence, ranking comments by their relevance and novelty, as well as summarizing comments will continue to be an important direction for research.

