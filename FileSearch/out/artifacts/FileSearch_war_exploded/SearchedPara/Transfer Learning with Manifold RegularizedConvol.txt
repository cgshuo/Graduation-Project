 Recently, deep learning shows great success for learning robust representation and outperforms conventional state-of-the-art methods in computer vision ap-plications. Convolutional neural networks (CNNs), wins the ImageNet challenge which is a contest based on a large scale data sets with over 1 million images since 2012 [15, 23], and the key of this success is that the substantially increased depth enlarge the capacity of CNNs and then enable CNNs to fit the data sets well. The works [17], [24] reinforced this result by showing significant improvement over shallow models when applying a very deep neural network architecture. models increases, which further requires more data to avoid overfitting during training process. The problem is that most of data sets are not large enough for training, thus the performance degrades. To take advantage of both huger capacity of deep models and less needed data of shallow ones, a training technique called fine-tuning is proposed. Fine-tuning adopts pretrained models, which are trained on large scale data sets, e.g. ImageNet, on new tasks with only sightly modifying the parameters of the pretrained models. Several studies have reported that fine-tuning obtains outstanding performance and reduces training time from 2 or 3 weeks to few days [18, 19].
 performance drops significantly when directly applied to transfer learning with insufficient target domain data. Transfer learning aims to improve learning per-formance in target domain with little or without any label information by lever-aging knowledge from auxiliary source domain. Yosinski et al. [20] pointed out that deep feature must transition from general to specific by higher layers of the networks for transfer learning. Hence, only integrating with source domain data will lead to the learned presentation go too specific to source domain. neural network (MRCNN) framework for transfer learning, which aims to use manifold learning approach to regularize fine-tuning progress. Manifold learn-ing approaches are widely adopted in semi-supervised or unsupervised learning, which assumes that data points within a same local structure are likely to have the same label [2]. Therefore, the goal to use the unlabeled data in target domain is to preserve such structure in higher layer or output layer by imposing mani-fold based constraints. By coupling manifold regularization and fine-tuning, we expect that the learned representation in higher layers go more general or more specific to target domain, and thus the knowledge from auxiliary source domain is successfully transferred.
 1. We propose a unsupervised learning framework that collaborates fine-tuning 2. We conduct extensive experiments on several data sets and statistical evi-3. Furthermore, we investigate the impact of fine-tuning and manifold regular-In this section, we first review convolutional neural networks architecture, fine-tuning technique and manifold regularization, which serve as preliminary knowl-edge of this paper.
 2.1 Convolutional Neural Network Deep learning approaches have been widely adopted in the last decade [3]. Par-ticularly, convolutional neural networks (CNN) is proposed to learn robust rep-resentation and achieve satisfying results in computer vision [15, 17]. lutional ( conv ) layers, pooling layers (max-pool or average-pool), fully connected ( fc ) layers and a classifier on top of them. Both conv and fc layers learn non-linear mapping h l in the l th layer with slight difference. The mapping of conv and fc layers can be formulized by respectively, where w l is the weight matrix (kernel in conv layers) and b l is the bias of l th hidden layer,  X * X  denotes convolution operation and  X  (  X  ) is an non-linear activation function, e.g., Rectified Linear Unit (ReLU)  X  ( x ) = max (0 , x ) [8] for hidden layers or softmax function  X  ( x ) = e x P n ing layers perform a downsampling operation along the spacial dimension. It is useful for reducing computational cost and providing robustness for learning representation [26].
 where n is the size of data set X and l is the depth of model, h l is the learned representation of l th hidden layer formulized by Eq. (1) or (2) and C (  X  ,  X  ) is the cross entropy loss function. 2.2 Fine-tuning According to [20], the representation in earlier layers of deep CNNs which are trained on large scale data sets are general to different tasks. Hence, it would be beneficial for both performance boost and time-saving to use those weights as either initialization or feature extractor. Fine-tuning is a training procedure that aims to adopt pretrained models on new tasks.
 match the dimension, since the sizes of input data sets differ between tasks. And for the last conv layers, there are two major strategies: (1) Fix some earlier layers and only fine-tune the higher layers when data is very limited [18]; (2) Back propagate through all the layers when we have enough data [19].
 this paper, we mainly follow the first fine-tuning strategy and the details will be presented in later sections. 2.3 Manifold regularization Manifold learning is a graph based semi-supervised or unsupervised method. It attempts to preserve the manifold structure in a data set. In this paper, we incorporate manifold learning as a regularization term by enforcing neighbors located in the same local structure on embedding space.
 where k is a hyper-parameter and NN ( k, x ) denotes the k nearest neighbors of x . The similarity between x i and x j can be measured by cosine distance where x i and x j are column vectors and &gt; denotes matrix transpose. Let D = diag ( P j M [ i,j ] ) and L = D  X  M be the Laplacian matrix, then the manifold regularized term  X  ( X ) can be written as In this paper, we focus on unsupervised transfer learning, i.e., only labeled data are available in source domain and unlabeled data in target domain. Thus, we domain as D t = { x ( t ) i } n t i =1 with n t unlabeled instances.
 VGG19 Network architecture [17], which is composed of 16 conv , 5 max-pooling, 3 fc and 1 softmax layers. Following the notation in [17], the 16 conv layers are divided into 5 blocks by max-pooling layers, i.e., conv 1 block consists of conv layers before first max-pooling layer, conv 2 block consists of conv layers between first and second max-pooling layer, and so on.
 layers and randomly initialize the fc layers, which are shared for both source and target domains. Then we further extend VGG Net by integrating manifold regularization, which is imposed on target domain to enforce similar instances to have the same labels, and cross entropy loss on source domain data to incor-porate label information. By preserving manifold structure in this manner, we hope the learned representation in higher layers can be well generalized. Fig. 1 intuitively illustrates MRCNN framework. Note that conv 1  X  conv 5 each denotes a convolutional block, not a single layer. Due to the limitation of data and the learned representation transitioning from general to specific along the network, we adopt following three different strategies: (1) randomly initialize 3 fc layers because fc layers require strict dimensional matching while the sizes of input data sets are different. (2) conv 5 block, containing 4 conv layers, is carefully finetuned since the representation in this block is more transferable and needs only sightly tuning. In other word, we apply a smaller learning rate on this block. (3) the conv 1-conv 4 blocks consisting of 12 conv layers are fixed and used as feature extractor since the representation of these blocks are general. denotes Laplacian matrix of target domain, the overall objective is written as below: is the manifold regularization as described in Eq. (6) imposed on target domain and k is the number of nearest neighbors. The last term of Eq. (7) is L2 norm of weights and biases matrix, which controls the complexity of the network struc-ture and is defined as where l is the depth of the neural network, i.e., 19 in this paper.  X  ,  X  are hyper-parameters that balance the importance of manifold regularization and model complexity in the entire framework.
 Since we implement MRCNN by Deep Learning Library TensorFlow [25], which can automatically compute the gradients and derive the solution. Therefore, we will not formalize the update rules for parameters here. To evaluate the effectiveness of our proposed framework MRCNN, we conduct experiments on two image data sets and compare our model with several state-of-the-art baseline methods. 4.1 Data sets and Data Processing CIFAR-100 CIFAR-100 data set 4 has 100 classes, which are grouped into 20 superclasses [5], and each contains 600 images. Among these 20 superclasses, we randomly choose two of them  X  fruit and vegetables  X  and  X  household electri-cal devices  X  and take  X  fruit and vegetables  X  as positive examples and  X  household electrical devices  X  as negative one. Each superclass of  X  fruit and vegetables  X  and  X  household electrical devices  X  has 5 classes. To construct transfer learning classi-fication problems, we randomly choose one class from  X  fruit and vegetables  X  and one from  X  household electrical devices  X  as source domains, and then choose an-other one class of  X  fruit and vegetables  X  and another one of  X  household electrical devices  X  from the remaining classes to construct target domain. In this way, we can obtain 400 ( P 2 5  X  P 2 5 ) classification problems.
 Corel Corel data set 5 consists of two different top categories,  X  flower  X  and  X  traf-fic  X  [9]. Each top category further includes four subcategories. We take  X  flower  X  as positive class and  X  traffic  X  as negative one. Then by following the same pro-cessing procedure, we can construct 144 ( P 2 4  X  P 2 4 ) transfer learning classification problems.
 except subtracting mean for CNN based methods and normalizing features to [0, 1] for the other compared competitors. 4.2 Baseline methods We compare MRCNN with a variety of baselines, -Logistic regression (LR), one of the most widely applied supervised learning -Transductive Support Vector Machine (TSVM) [1], a transductive learning -Transfer Component Analysis (TCA) [12], which aims at learning a low--Transfer Learning with Deep Autoconders (TLDA) [22], which uses deep -Standard VGG Net [17], which finetunes on source domain but without 4.3 Implementation Details For LR, we perform grid search for L2 regularization item, and for TSVM, we use SVM lin 6 and sample the hyper-parameter in [10  X  5 , 10 1 ]. For TCA, the number of latent dimension is carefully tuned, e.g., for Corel data set, the number varies between 10 and 100 with the step size 10. For TLDA, we adopt author X  X  source code and use the default parameters.
 [25]. And for VGG, the learning rate r and weight decay strength  X  are set as 10  X  2 . 5 and 10  X  2 for CIFAR-100, 10  X  4 and 10  X  2 for Corel. For MRCNN, r and  X  are set as 10  X  2 . 5 and 10  X  2 for CIFAR-100, 10  X  2 . 5 and 10  X  1 for Corel. Moreover, the trade-off parameter  X  is set as 10  X  5 for CIFAR-100, 10  X  3 for Corel, and the number of nearest neighbors k is set as 3 for both CIFAR-100 and Corel. Note that, we use Adam [21] as an optimizer to minimize the objective function for both CNN based models. 4.4 Results In total, we construct 400 classification tasks for CIFAR-100 and 144 classifica-tion tasks for Corel. To make comprehensive comparison, we further divide the classification tasks into two groups for each data set according to the accuracy of LR. Specifically, we first conduct LR model on all classification tasks, and then group them into two groups, i.e., the first group of classification tasks with accuracies from LR lower than 70%, while the other one with accuracies from LR higher than 70%. Finally, we report the average results of two groups, and all the results are shown in Table. 1. Left and Right respectively denote the average performance of classification tasks whose accuracies lower and higher than 70%, and Total means the average results over all tasks. Note that lower accuracy from LR indicates more difficult to make transfer, and vice versa.
 -TSVM is better than LR, which indicates the importance to consider un--CNN based methods, i.e., VGG and MRCNN, significantly outperform all -Among the deep learning methods, MRCNN achieves considerable improve--Overall, the incorporation of manifold regularization leads to the success of 4.5 Analysis and Discussion Although we show the average results in Table 1, you maybe more interest in look for more detailed results. Here we try our best to present more detailed results, although it is not easy to show all detailed ones of totally 544 (400+144) classification tasks. Taking CIFAR-100 as an example, we average the accuracy of problems with the same target domain ( P 1 5  X  P 1 5 = 25 instances). The accuracies of LR, VGG and MRCNN are presented in Fig. 2(a) in an increasing order of LR X  X  accuracy.
 time. However, MRCNN can not achieve satisfying results at some points. We conjecture that the proposed model heavily depends on the local relationship of target domain data, so the bad quality of nearest neighbors would lead to the poor performance. Motivated by [6], we use to measure the confusion of nearest neighbors, and it can be formalized as where 1 { X } is the indicator function, # nn i is the number of nearest neighbors x . Intuitively, the higher is, the more confusing knowledge are introduced by imposing manifold regularization. We sort the classification problems according to the values of on target domains and show how influence the classification accuracy in Fig. 2(b). Moreover, we group the tasks into 3 groups according to consists of those with 0 . 1  X   X  0 . 15 and the rest form the third one. The average accuracy of each group is presented in Table. 2.
 substantially drops as grows for all methods. The reason may be that the positive and negative instances are similar for these classification tasks, and they are not easy to be separated. The above results also reveal that the confusion of nearest neighbors is the key factor that influences the performance of MRCNN. Hence, to further generalize MRCNN on transfer learning tasks, one crucial problem to be enhanced is how to obtain correct nearest neighbors. 4.6 Parameter Sensitivity In this section, we will discuss how the parameters r, X , X  and k in Eq. (7) impact on the experiments. To tune the hyper-parameters, we randomly select 10 problems from CIFAR-100 as validation problems. We sample the learning nearest neighbors is selected from [1, 3, 5, 10, 20, 40]. From these results from Fig. 3, we then set r, X , X  and k to 10  X  2 . 5 , 10  X  5 , 10  X  1 and 3 CIFAR-100. For the Corel data set, we randomly select 6 problems, and the parameters are similarly tuned. Transfer learning is the improvement of learning in a new domain by transferring the knowledge from auxiliary source domain [7, 11]. It has drawn much attention in past decades for its potential to ease the pain of manual labeling. Feature based approaches are one of the most widely proposed, which aim to learn a good feature representation for both source domain and target domain by reduce the difference between domains or integrating regularization [10, 12, 14, 16]. Among feature based transfer learning methods, several methods have been proposed to reduce the domain discrepancy explicitly. For example, transfer component analysis (TCA) [12] aims to minimize the difference of distributions between domains in a kernel Hilbert space, [10] is trying to find a subspace where training and testing samples are approximately i.i.d. by integrating Bregman divergence-based regularization between distributions of domains. One crucial problem of these methods is that most of them only adopt shallow representation models to reduce the domain discrepancy, which limits their ability to generalize for various tasks.
 resentation in recent years. To enjoy such benefit, several frameworks have been introduced. Stacked Denoising Autoencoders (SDAEs) [13] aims to improve the effectiveness of learned representation in Denoising Autoencoders (DAEs) [4] by extending the depth of DAEs, i.e. stack multiple DAEs within the framework. [22] further couple SDAEs and feature based transfer learning approach together where they explicitly minimize the KL divergence between distributions of source and target domains in embedding space.
 brought by large scale data sets. Researches like [18, 19] directly adopt the pre-trained models on new tasks and deliver state-of-the-art results. [20] investigates the generality of deep neural network and reveals that features must eventually transition from general to specific by the last layer of the network. Motivated by these works, we proposed a manifold regularized convolutional neural networks (MRCNN) framework and enjoy the generality of deep learning models. In this paper, we propose a novel manifold regularized convolutional neural net-work (MRCNN) framework for transfer learning. This framework adopts a very deep CNN architecture and incorporates manifold regularization component. By imposing manifold based constrains, we enforce the manifold structure of tar-get domain to be preserved while the cross entropy loss of source domain is minimized. We confirm that this manifold regularization can help improve gen-eralization performance and thus successfully leverage knowledge cross domains. Moreover, in order to ease the pain of training such deep CNN, we apply fine-tuning technique to our framework. Finally, we conduct a series of experiments against several competitors to demonstrate the effectiveness of our framework. This work is supported by the National Natural Science Foundation of China (Nos. 61473273, 91546122, 61573335, 61602438), Guangdong provincial science and technology plan projects (No. 2015 B010109005), the Youth Innovation Pro-motion Association CAS 2017146.

