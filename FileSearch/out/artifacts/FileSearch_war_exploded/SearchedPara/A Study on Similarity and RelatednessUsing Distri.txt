 Measuring semantic similarity and relatedness be-tween terms is an important problem in lexical se-mantics. It has applications in many natural lan-guage processing tasks, such as Textual Entailment, Word Sense Disambiguation or Information Extrac-tion, and other related areas like Information Re-trieval. The techniques used to solve this problem can be roughly classified into two main categories: those relying on pre-existing knowledge resources (thesauri, semantic networks, taxonomies or ency-clopedias) (Alvarez and Lim, 2007; Yang and Pow-ers, 2005; Hughes and Ramage, 2007) and those in-ducing distributional properties of words from cor-pora (Sahami and Heilman, 2006; Chen et al., 2006; Bollegala et al., 2007).

In this paper, we explore both families. For the first one we apply graph based algorithms to Word-Net, and for the second we induce distributional similarities collected from a 1.6 Terabyte Web cor-pus. Previous work suggests that distributional sim-ilarities suffer from certain limitations, which make them less useful than knowledge resources for se-mantic similarity. For example, Lin (1998b) finds similar phrases like captive-westerner which made sense only in the context of the corpus used, and Budanitsky and Hirst (2006) highlight other prob-lems that stem from the imbalance and sparseness of the corpora. Comparatively, the experiments in this paper demonstrate that distributional similarities can perform as well as the knowledge-based approaches, and a combination of the two can exceed the per-formance of results previously reported on the same datasets. An application to cross-lingual (CL) sim-ilarity identification is also described, with applica-tions such as CL Information Retrieval or CL spon-sored search. A discussion on the differences be-tween learning similarity and relatedness scores is provided.

The paper is structured as follows. We first present the WordNet-based method, followed by the distributional methods. Section 4 is devoted to the evaluation and results on the monolingual and cross-lingual tasks. Section 5 presents some analysis, in-cluding learning curves for distributional methods, the use of distributional similarity to improve Word-Net similarity, the contrast between similarity and relatedness, and the combination of methods. Sec-tion 6 presents related work, and finally, Section 7 draws the conclusions and mentions future work. WordNet (Fellbaum, 1998) is a lexical database of English, which groups nouns, verbs, adjectives and adverbs into sets of synonyms (synsets), each ex-pressing a distinct concept. Synsets are interlinked with conceptual-semantic and lexical relations, in-cluding hypernymy, meronymy, causality, etc.
Given a pair of words and a graph-based repre-sentation of WordNet, our method has basically two steps: We first compute the personalized PageR-ank over WordNet separately for each of the words, producing a probability distribution over WordNet synsets. We then compare how similar these two dis-crete probability distributions are by encoding them as vectors and computing the cosine between the vectors.

We represent WordNet as a graph G = ( V,E ) as follows: graph nodes represent WordNet concepts (synsets) and dictionary words; relations among synsets are represented by undirected edges; and dictionary words are linked to the synsets associated to them by directed edges.

For each word in the pair we first compute a per-sonalized PageRank vector of graph G (Haveliwala, 2002). Basically, personalized PageRank is com-puted by modifying the random jump distribution vector in the traditional PageRank equation. In our case, we concentrate all probability mass in the tar-get word.

Regarding PageRank implementation details, we chose a damping value of 0 . 85 and finish the calcula-tion after 30 iterations. These are default values, and we did not optimize them. Our similarity method is similar, but simpler, to that used by (Hughes and Ra-mage, 2007), which report very good results on sim-ilarity datasets. More details of our algorithm can be found in (Agirre and Soroa, 2009). The algorithm and needed resouces are publicly available 1 . 2.1 WordNet relations and versions The WordNet versions that we use in this work are the Multilingual Central Repository or MCR (At-serias et al., 2004) (which includes English Word-Net version 1.6 and wordnets for several other lan-guages like Spanish, Italian, Catalan and Basque), and WordNet version 3.0 2 . We used all the rela-tions in MCR (except cooccurrence relations and se-lectional preference relations) and in WordNet 3.0. Given the recent availability of the disambiguated gloss relations for WordNet 3.0 3 , we also used a version which incorporates these relations. We will refer to the three versions as MCR16, WN30 and WN30g, respectively. Our choice was mainly moti-vated by the fact that MCR contains tightly aligned wordnets of several languages (see below). 2.2 Cross-linguality MCR follows the EuroWordNet design (Vossen, 1998), which specifies an InterLingual Index (ILI) that links the concepts across wordnets of differ-ent languages. The wordnets for other languages in MCR use the English WordNet synset numbers as ILIs. This design allows a decoupling of the rela-tions between concepts (which can be taken to be language independent) and the links from each con-tent word to its corresponding concepts (which is language dependent).

As our WordNet-based method uses the graph of the concepts and relations, we can easily compute the similarity between words from different lan-guages. For example, consider a English-Spanish pair like car  X  coche . Given that the Spanish Word-Net is included in MCR we can use MCR as the common knowledge-base for the relations. We can then compute the personalized PageRank for each of car and coche on the same underlying graph, and then compare the similarity between both probabil-ity distributions.

As an alternative, we also tried to use pub-licly available mappings for wordnets (Daude et al., 2000) 4 in order to create a 3.0 version of the Span-ish WordNet. The mapping was used to link Spanish variants to 3.0 synsets. We used the English Word-Net 3.0, including glosses, to construct the graph. The two Spanish WordNet versions are referred to as MCR16 and WN30g. In this section, we describe the distributional meth-ods used for calculating similarities between words, and profiting from the use of a large Web-based cor-pus.

This work is motivated by previous studies that make use of search engines in order to collect co-occurrence statistics between words. Turney (2001) uses the number of hits returned by a Web search engine to calculate the Pointwise Mutual Informa-tion (PMI) between terms, as an indicator of syn-onymy. Bollegala et al. (2007) calculate a number of popular relatedness metrics based on page counts, like PMI, the Jaccard coefficient, the Simpson co-efficient and the Dice coefficient, which are com-bined with lexico-syntactic patterns as model fea-tures. The model parameters are trained using Sup-port Vector Machines (SVM) in order to later rank pairs of words. A different approach is the one taken by Sahami and Heilman (2006), who collect snip-pets from the results of a search engine and repre-sent each snippet as a vector, weighted with the tf  X  idf score. The semantic similarity between two queries is calculated as the inner product between the cen-troids of the respective sets of vectors.

To calculate the similarity of two words w 1 and w 2 , Ruiz-Casado et al. (2005) collect snippets con-taining w 1 from a Web search engine, extract a con-text around it, replace it with w 2 and check for the existence of that modified context in the Web.
Using a search engine to calculate similarities be-tween words has the drawback that the data used will always be truncated. So, for example, the numbers of hits returned by search engines nowadays are al-ways approximate and rounded up. The systems that rely on collecting snippets are also limited by the maximum number of documents returned per query, typically around a thousand. We hypothesize that by crawling a large corpus from the Web and doing standard corpus analysis to collect precise statistics for the terms we should improve over other unsu-pervised systems that are based on search engine results, and should yield results that are competi-tive even when compared to knowledge-based ap-proaches.

In order to calculate the semantic similarity be-tween the words in a set, we have used a vector space model, with the following three variations:
In the bag-of-words approach , for each word w in the dataset we collect every term t that appears in a window centered in w , and add them to the vector together with its frequency.

In the context window approach , for each word w in the dataset we collect every window W cen-tered in w (removing the central word), and add it to the vector together with its frequency (the total number of times we saw window W around w in the whole corpus). In this case, all punctuation symbols are replaced with a special token, to unify patterns like , the &lt; term &gt; said to and  X  the &lt; term &gt; Throughout the paper, when we mention a context window of size N it means N words at each side of the phrase of interest.

In the syntactic dependency approach , we parse the entire corpus using an implementation of an In-ductive Dependency parser as described in Nivre (2006). For each word w we collect a template of the syntactic context. We consider sequences of gov-erning words (e.g. the parent, grand-parent, etc.) as well as collections of descendants (e.g., immediate children, grandchildren, etc.). This information is then encoded as a contextual template. For example, the context template cooks &lt; term &gt; delicious could be contexts for nouns such as food, meals, pasta , etc. This captures both syntactic preferences as well as selectional preferences. Contrary to Pado and Lap-ata (2007), we do not use the labels of the syntactic dependencies.

Once the vectors have been obtained, the fre-quency for each dimension in every vector is weighted using the other vectors as contrast set, with vectors is used to calculate the similarity between each pair of terms.

Except for the syntactic dependency approach, where closed-class words are needed by the parser, in the other cases we have removed stopwords (pro-nouns, prepositions, determiners and modal and auxiliary verbs). 3.1 Corpus used We have used a corpus of four billion documents, crawled from the Web in August 2008. An HTML parser is used to extract text, the language of each document is identified, and non-English documents are discarded. The final corpus remaining at the end of this process contains roughly 1.6 Terawords. All calculations are done in parallel sharding by dimen-sion, and it is possible to calculate all pairwise sim-ilarities of the words in the test sets very quickly on this corpus using the MapReduce infrastructure. A complete run takes around 15 minutes on 2,000 cores. 3.2 Cross-linguality In order to calculate similarities in a cross-lingual setting, where some of the words are in a language l other than English, the following algorithm is used: 1. Replace each non-English word in the dataset 2. The vector corresponding to each Spanish word 3. Once the vectors are generated, the similarities 4.1 Gold-standard datasets We have used two standard datasets. The first one, RG, consists of 65 pairs of words collected by Rubenstein and Goodenough (1965), who had them judged by 51 human subjects in a scale from 0.0 to 4.0 according to their similarity, but ignoring any other possible semantic relationships that might ap-pear between the terms. The second dataset, Word-Sim353 5 (Finkelstein et al., 2002) contains 353 word pairs, each associated with an average of 13 to 16 hu-man judgements. In this case, both similarity and re-latedness are annotated without any distinction. Sev-eral studies indicate that the human scores consis-tently have very high correlations with each other (Miller and Charles, 1991; Resnik, 1995), thus val-idating the use of these datasets for evaluating se-mantic similarity.

For the cross-lingual evaluation, the two datasets were modified by translating the second word in each pair into Spanish. Two humans translated simultaneously both datasets, with an inter-tagger agreement of 72% for RG and 84% for Word-Sim353. 4.2 Results Table 1 shows the Spearman correlation obtained on the RG and WordSim353 datasets, including the in-terval at 0.95 of confidence 6 .

Overall the distributional context-window ap-proach performs best in the RG, reaching 0.89 corre-lation, and both WN30g and the combination of con-text windows and syntactic context perform best on WordSim353. Note that the confidence intervals are quite large in both RG and WordSim353, and few of the pairwise differences are statistically significant.
Regarding WordNet-based approaches, the use of the glosses and WordNet 3.0 (WN30g) yields the best results in both datasets. While MCR16 is close to WN30g for the RG dataset, it lags well behind on WordSim353. This discrepancy is further ana-lyzed is Section 5.3. Note that the performance of WordNet in the WordSim353 dataset suffers from unknown words. In fact, there are nine pairs which returned null similarity for this reason. The num-ber in parenthesis in Table 1 for WordSim353 shows the results for the 344 remaining pairs. Section 5.2 shows a proposal to overcome this limitation.
The bag-of-words approach tends to group to-gether terms that can have a similar distribution of contextual terms. Therefore, terms that are topically related can appear in the same textual passages and will get high values using this model. We see this as an explanation why this model performed better than the context window approach for WordSim353, where annotators were instructed to provide high ratings to related terms. On the contrary, the con-text window approach tends to group together words that are exchangeable in exactly the same context, preserving order. Table 2 illustrates a few exam-ples of context collected. Therefore, true synonyms and hyponyms/hyperonyms will receive high simi-larities, whereas terms related topically or based on any other semantic relation (e.g. movie and star ) will have lower scores. This explains why this method performed better for the RG dataset. Section 5.3 confirms these observations. 4.3 Cross-lingual similarity Table 3 shows the results for the English-Spanish cross-lingual datasets. For RG, MCR16 and the context windows methods drop only 5 percentage points, showing that cross-lingual similarity is feasi-ble, and that both cross-lingual strategies are robust.
The results for WordSim353 show that WN30g is the best for this dataset, with the rest of the meth-ods falling over 10 percentage points relative to the monolingual experiment. A closer look at the Word-Net results showed that most of the drop in perfor-mance was caused by out-of-vocabulary words, due to the smaller vocabulary of the Spanish WordNet. Though not totally comparable, if we compute the correlation over pairs covered in WordNet alone, the correlation would drop only 2 percentage points. In the case of the distributional approaches, the fall in performance was caused by the translations, as only 61% of the words were translated into the original word in the English datasets. In this section we present some analysis, including learning curves for distributional methods, the use of distributional similarity to improve WordNet sim-ilarity, the contrast between similarity and related-ness, and the combination of methods. 5.1 Learning curves for distributional methods Figure 1 shows that the correlation improves with the size of the corpus, as expected. For the re-sults using the WordSim353 corpus, we show the results of the bag-of-words approach with context size 10. Results improve from 0.5 Spearman correla-tion up to 0.65 when increasing the corpus size three orders of magnitude, although the effect decays at the end, which indicates that we might not get fur-ther gains going beyond the current size of the cor-pus. With respect to results for the RG dataset, we used a context-window approach with context radius 4. Here, results improve even more with data size, probably due to the sparse data problem collecting 8-word context windows if the corpus is not large enough. Correlation improves linearly right to the end, where results stabilize around 0.89. 5.2 Combining both approaches: dealing with Although the vocabulary of WordNet is very ex-tensive, applications are bound to need the similar-ity between words which are not included in Word-Net. This is exemplified in the WordSim353 dataset, where 9 pairs contain words which are unknown to WordNet. In order to overcome this shortcoming, we could use similar words instead, as provided by the distributional thesaurus. We used the distribu-tional thesaurus defined in Section 3, using context windows of width 4, to provide three similar words for each of the unknown words in WordNet. Results improve for both WN30 and WN30g, as shown in Table 4, attaining our best results for WordSim353. 5.3 Similarity vs. relatedness We mentioned above that the annotation guidelines of WordSim353 did not distinguish between simi-lar and related pairs. As the results in Section 4 show, different techniques are more appropriate to calculate either similarity or relatedness. In order to study this effect, ideally, we would have two ver-sions of the dataset, where annotators were given precise instructions to distinguish similarity in one case, and relatedness in the other. Given the lack of such datasets, we devised a simpler approach in order to reuse the existing human judgements. We manually split the dataset in two parts, as follows.
First, two humans classified all pairs as be-ing synonyms of each other, antonyms, iden-tical, hyperonym-hyponym, hyponym-hyperonym, holonym-meronym, meronym-holonym, and none-of-the-above. The inter-tagger agreement rate was 0.80, with a Kappa score of 0.77. This anno-tation was used to group the pairs in three cate-gories: similar pairs (those classified as synonyms, antonyms, identical, or hyponym-hyperonym), re-lated pairs (those classified as meronym-holonym, and pairs classified as none-of-the-above, with a hu-man average similarity greater than 5), and unrelated pairs (those classified as none-of-the-above that had average similarity less than or equal to 5). We then created two new gold-standard datasets: similarity (the union of similar and unrelated pairs), and relat-edness (the union of related and unrelated) 7 .
Table 5 shows the results on the relatedness and similarity subsets of WordSim353 for the different methods. Regarding WordNet methods, both WN30 and WN30g perform similarly on the similarity sub-set, but WN30g obtains the best results by far on the relatedness data. These results are congruent with our expectations: two words are similar if their synsets are in close places in the WordNet hierarchy, and two words are related if there is a connection between them. Most of the relations in WordNet are of hierarchical nature, and although other rela-tions exist, they are far less numerous, thus explain-ing the good results for both WN30 and WN30g on similarity, but the bad results of WN30 on related-ness. The disambiguated glosses help find connec-tions among related concepts, and allow our method to better model relatedness with respect to WN30.
The low results for MCR16 also deserve some comments. Given the fact that MCR16 performed very well on the RG dataset, it comes as a surprise that it performs so poorly for the similarity subset of WordSim353. In an additional evaluation, we at-tested that MCR16 does indeed perform as well as MCR30g on the similar pairs subset. We believe that this deviation could be due to the method used to construct the similarity dataset, which includes some pairs of loosely related pairs labeled as unrelated.
Concerning the techniques based on distributional similarities, the method based on context windows provides the best results for similarity, and the bag-of-words representation outperforms most of the other techniques for relatedness. 5.4 Supervised combination In order to gain an insight on which would be the up-per bound that we could obtain when combining our methods, we took the output of three systems (bag of words with window size 10, context window with size 4, and the WN30g run). Each of these outputs is a ranking of word pairs, and we implemented an or-acle that chooses, for each pair, the rank that is most similar to the rank of the pair in the gold-standard. The outputs of the oracle have a Spearman correla-tion of 0.97 for RG and 0.92 for WordSim353, which gives as an indication of the correlations that could be achieved by choosing for each pair the rank out-put by the best classifier for that pair.

The previous results motivated the use of a su-pervised approach to combine the output of the different systems. We created a training cor-pus containing pairs of pairs of words from the datasets, having as features the similarity and rank of each pair involved as given by the differ-ent unsupervised systems. A classifier is trained to decide whether the first pair is more simi-lar than the second one. For example, a train-ing instance using two unsupervised classifiers is meaning that the similarities given by the first clas-sifier to the two pairs were 0.001364 and 0.327515 respectively, which ranked them in positions 31 and 64. The second classifier gave them similarities of 0.084805 and 0.109061 respectively, which ranked them in positions 57 and 59. The class negative in-dicates that in the gold-standard the first pair has a lower score than the second pair.

We have trained a SVM to classify pairs of pairs, and use its output to rank the entries in both datasets. It uses a polynomial kernel with degree 4. We did not have a held-out set, so we used the standard set-tings of Weka, without trying to modify parameters, e.g. C. Each word pair is scored with the number of pairs that were considered to have less similar-ity using the SVM. The results using 10-fold cross-validation are shown in Table 6. A combination of all methods produces the best results reported so far for both datasets, statistically significant for RG. Contrary to the WordSim353 dataset, common prac-tice with the RG dataset has been to perform the evaluation with Pearson correlation. In our believe Pearson is less informative, as the Pearson correla-tion suffers much when the scores of two systems are not linearly correlated, something which happens often given due to the different nature of the tech-niques applied. Some authors, e.g. Alvarez and Lim (2007), use a non-linear function to map the system outputs into new values distributed more similarly to the values in the gold-standard. In their case, the empirically. Finding such a function is dependent on the dataset used, and involves an extra step in the similarity calculations. Alternatively, the Spearman correlation provides an evaluation metric that is in-dependent of such data-dependent transformations.
Most similarity researchers have published their complete results on a smaller subset of the RG dataset containing 30 word pairs (Miller and Charles, 1991), usually referred to as MC, making it possible to compare different systems using differ-ent correlation. Table 7 shows the results of related work on MC that was available to us, including our own. For the authors that did not provide the de-tailed data we include only the Pearson correlation with no confidence intervals.

Among the unsupervised methods introduced in this paper, the context window produced the best re-ported Spearman correlation, although the 0.95 con-fidence intervals are too large to allow us to accept the hypothesis that it is better than all others meth-ods. The supervised combination produces the best results reported so far. For the benefit of future re-search, our results for the MC subset are displayed in Table 8.

Comparison on the WordSim353 dataset is eas-ier, as all researchers have used Spearman. The figures in Table 9) show that our WordNet-based method outperforms all previously published Word-Net methods. We want to note that our WordNet-based method outperforms that of Hughes and Ram-age (2007), which uses a similar method. Although there are some differences in the method, we think that the main performance gain comes from the use of the disambiguated glosses, which they did not use. Our distributional methods also outperform all other corpus-based methods. The most similar ap-proach to our distributional technique is Finkelstein et al. (2002), who combined distributional similar-ities from Web documents with a similarity from WordNet. Their results are probably worse due to the smaller data size (they used 270,000 documents) and the differences in the calculation of the simi-larities. The only method which outperforms our non-supervised methods is that of (Gabrilovich and Markovitch, 2007) when based on Wikipedia, prob-ably because of the dense, manually distilled knowl-edge contained in Wikipedia. All in all, our super-vised combination gets the best published results on this dataset. This paper has presented two state-of-the-art dis-tributional and WordNet-based similarity measures, with a study of several parameters, including per-formance on similarity and relatedness data. We show that the use of disambiguated glosses allows for the best published results for WordNet-based systems on the WordSim353 dataset, mainly due to the better modeling of relatedness (as opposed to similarity). Distributional similarities have proven to be competitive when compared to knowledge-based methods, with context windows being better for similarity and bag of words for relatedness. Dis-tributional similarity was effectively used to cover out-of-vocabulary items in the WordNet-based mea-sure providing our best unsupervised results. The complementarity of our methods was exploited by a supervised learner, producing the best results so far for RG and WordSim353. Our results include confidence values, which, surprisingly, were not in-cluded in most previous work, and show that many results over RG and WordSim353 are indistinguish-able. The algorithm for WordNet-base similarity and the necessary resources are publicly available 8 .
This work pioneers cross-lingual extension and evaluation of both distributional and WordNet-based measures. We have shown that closely aligned wordnets provide a natural and effective way to compute cross-lingual similarity with minor losses. A simple translation strategy also yields good results for distributional methods.
