 sum for each query point q smoothing parameter h (the 'bandwidth'). This form of summation is ubiquitous in man y statis-points, yielding a brute-force computational cost scaling quadratically (that is O ( jRj 2 ) ). Err or bounds. Due to its expensi ve computational cost, man y algorithms approximate the Gaus-values. The follo wing error bound criteria are common in literature: Definition 1.1. An algorithm guar antees absolute err or bound , if for eac h exact value ( q for q Definition 1.2. An algorithm guar antees relati ve err or bound , if for eac h exact value ( q for q moti vation will be discussed shortly .
 exact value ( q 0 &lt; 1 &lt; 1 , e ( q i ; R ) ( q i ; R ) j ( q i ; R ) j .
 Pr evious work. The most successful class of acceleration methods emplo y  X higher -order divide and conquer X  or gener alized N -body algorithms (GN A) [4]. This approach can use any spatial performs a simulataneous recursi ve descent on both trees.
 ical performance around suboptimally small and lar ge bandwidths. [11, 10] extended GN A-based Gaussian summations with series-e xpansion which pro vided tighter bounds; it sho wed enormous ber of required terms in series expansion increases exponentially with respect to D . by bounding box es with normal-based condence interv al from Monte Carlo sampling. [9] demon-require per -query estimates, such as those required for kernel density estimation. performs a local dimension reduction to a localized subset of the data in a bottom-up fashion. This paper . We propose a new fast Gaussian summation algorithm that enables speedup in higher in [14 ], a pioneering paper in this area.
 numbers i; j 2 N , and denoted q R ; an internal node N has the child nodes N L and N R . Denition 1.3. The main routine for the probabilistic kernel summation is sho wn in Algorithm 1. The function MCMM tak es the query node Q and the refer ence node R (each initially called with value which controls the probability guarantee that each kernel sum is within relati ve error). Algorithm 1 The core dual-tree routine for probabilistic Gaussian kernel summation.
MCMM ( Q; R; ) 5: S UMMARIZE MC ( Q; R; ; ) 10: else 15: else The idea of Monte Carlo sampling used in the new algorithm is similar to the one in [9], except less variance than a pure Monte Carlo approach used in [9]. Algorithm 1 rst attempts approxima-tions with hard error bounds, which are computationally cheaper than sampling-based approxima-S UMMARIZE E XACT functions in any general dimension.
 pair -wise evaluations, which is determined by the condition: m controls the minimum number of refer ence points needed for Monte Carlo sampling to proceed. routine samples m in estimating ( q; R ) by e ( q; R ) = j R j Limit Theorem, given enough m samples, j point for the contrib ution of R is: where want to compute, we instead enforce the follo wing: where l ( q; R ) is the currently running lower bound on the sum computed using exact methods and j R j tar get error the right side of the inequality in Equation 2 with at least probability of 1 is: If the given query node and refer ence node pair cannot be pruned using either non-state the probablistic error guarantee of our algorithm as a theorem.
 Theor em 2.1. After calling MCMM with Q = Q root , R = R root , and = , Algorithm 1 appr oximates eac h ( q; R ) with e ( q; R ) suc h that Definition 1.3 holds. compute estimates for q 2 Q such that e ( q; R ) ( q; R ) &lt; ( q; R ) j R j least 1 &gt; 1 . By Equation 2, S UMMARIZE MC computes estimates for q 2 Q such that e ( q; R ) ( q; R ) &lt; ( q; R ) j R j jRj with probability 1 .
 pute estimates that satisfy e ( q; R L ) ( q; R L ) ( q;R ) j R L j e satisfying Denition 1.3. computed with exact bounds (S UMMARIZE E XACT and MCMMB ASE ) is not used. This portion beneted by this simple modication. each node N : N: = ( ; U; ; d ) where is the mean, U is a D d matrix whose columns consist of d d lower -dimensional coordinate using the orthogonal basis set of N : x norm reconstruction error is given by: jj x Monte Carlo sampling using a subspace tree. Consider C AN S UMMARIZE MC function in Algo-Algorithm 2 Monte Carlo sampling based approximation routines.
 S
AMPLE ( q; R; ; ; S; m ) for k = 1 to m do R where q be precomputed (which tak es d dot products between two D -dimensional vectors) and re-used for the computational cost is O ( d ( D + m )) &lt;&lt; O ( D m ) for each query point . Increased variance comes at the cost of ine xact distance computations, howe ver. Each dis-tance computation incurs at most squared L point plus the ine xactness due to dimension reduction range of bandwidths. This experiment is moti vated by man y kernel methods that require comput-other kernel methods that require efcient Gaussian summation.
 to build the trees. Codes are in C/C++ and run on a dual Intel Xeon 3GHz with 8 Gb of main memory . The measurements in second to eigth columns are obtained by running the algorithms at the bandwidth kh where 10 3 k 10 3 is the constant in the corresponding column header . The last columns denote the total time needed to run on all seven bandwidth values. Algorithm 3 PCA tree building routine.
 B if C AN P ARTITION ( P ) then else retur n N percentage of the query points that did not satisfy the relati ve error . pared to our approach that uses Monte Carlo sampling. Multipole moments are an effecti ve form of compression in low dimensions with analytical error bounds that can be evaluated; our Monte Carlo-based method has an asymptotic error bound which must be  X learned X  through sampling. better than the algorithm using exact bounds ( p = 1 ) by at least a factor of two. Compared to only 50K points, and the speedup will be more dramatic as we increase the number of points. We presented an extension to fast multipole methods to use approximation methods with both hard on high-dimensional datasets. Our future work will include possible impro vements inspired by a recent work done in the FMM community using a matrix-f actorization formulation [13 ]. Figure 1: Left: A PCA-tree for a 3-D dataset. Right: The squared Euclidean distance between a given query point and a refer ence point projected onto a subspace can be decomposed into two components: the orthogonal component and the component in the subspace. Refer ences
