 The problem of text classification has been examined for a long time but most of were treated in isolation, with no consideration about the relationship among them. For this problem, one-against-the-rest and pairwise classification methods ([ 11], [ 14]) have been widely applied. Based on the clas sification result of each binary classifier, those methods provided different ways to determine the categories to which a document should be assigned. 
Meanwhile, categories are usually organized in a hierarchical structure, usually like subsumption relationship among categories was taken into account for classification. Using the top-down level-based approach, a hierarchical classification method the root category, the classifiers at one leve l determined if the documents presented to classification process. 
Some methods have been developed for solving the classification problem on hierarchical categories ([ 2], [ 3], [ 8], [ 13]). In [ 13], the classification method assigned a document to only one category, which was a leaf category, assuming that the categories. Then the child category with the highest probability was selected for leaf category was reached. In order to classify a document to more than one leaf category, [ 8] improved [ 13] by defining a threshold for each level of the category tree, for further classification, where SVMs were used as binary classifiers. 
That a document was assigned to only leaf categories was not satisfactory when, in category instead. In [ 3], classification was viewed as a mapping from a document into classified into both leaf and internal categor ies of the original tree. However, that did not allow a document to have both a category and some of its ancestors as its labels. Meanwhile, in [ 2] a document could be labeled by any subset of categories in a category tree of discourse. 
However, all the above-mentioned methods were for tree-structured categories only, where a category had at most one parent category. Ontologically, a category paper we introduce three approaches that can solve the multi-label text classification problem for DAG-based categories and analyze their performances. The first approach is called DAG-based , which manipulates the category DAG directly. The second approach is called tree-based , which transforms the category DAG into an equivalent tree and adap ts the approach in [ 2] for it. The third approach is called flat , which converts the problem into a flat classification one. 
The paper is organized as follows. Firstly, Section 2 reviews the hierarchical for category DAGs. Section 4 shows experi mental results using SVMs as binary classifiers. Finally, Section 5 concludes the paper and suggests future work. The approach proposed in [ 2] was hierarchical classification for category trees where documents were assigned to both leaf and in ternal categories. All involved classifiers were binary ones. The subsumption relationship among categories was used during the training and classification phases of those binary classifiers. Building Classifiers o For each internal category C i , a binary classifier called subtree-classifier is built Training Phase Appropriate positive and negative training documents, respectively denoted by +ve and  X  X e , are selected for each kind of the above-mentioned classifiers. o Subtree classifier of an internal category C i : o Local classifier of an internal category C i : o Local classifier of a leaf category C i : Classification Phase phase. It is a top-down level-based classification process in which a document is presented to the classifiers of a category. If the classification process at that category cannot go further down, the classification along that branch will stop without document d j presented to a category C i , do the followings: o If C i is an internal category: 
The approach was realized using SVM binary classifiers. The experiment results training documents. However, the method cannot be used directly for categories organized as a DAG, in which a category may have more than one parent. We have adapted it for DAG-structured categories as presented in the next section. 3.1 Tree-Based Approach The approach proposed in [ 2] can be adapted for a category DAG by transforming the different nodes whose number is equal to the number of branches from the root leading to that category. The tree can be cr eated by traversing the graph in the depth-first order. Each time a category is visited, a copy of its is made and indexed by the of a category are physically separated, they are logically treated as the same label in the training and classification phases. 
However, this adapted classification appro ach has two main drawbacks. Firstly, the resulted tree may be very large if the original DAG contains cascaded nodes with multiple parents as exemplified in Figure 3.2. Secondly, such a category tree has completely similar classifiers. For example in Figure 3.1, the classifiers of F1, F2 are similar because they are trained with the same training set. That wastes computation and time because, when a document is classifi ed into the subtree rooted at F1, it does not need to be considered by any classifier in the subtree rooted at F2, and vice versa. 3.2 DAG-Based Approach We propose another approach also adapted from [ 2] but manipulates directly on a category (or leaf category) with n parents is equipped with n subtree classifiers (or local one subtree classifier at an internal category (and more than one local classifier at a leaf category). A document is also classified into both leaf and internal categories. Building Classifiers o For each internal category C i and a parent category C k of C i , a binary classifier Training Phase Appropriate positive and negative training documents, respectively denoted by +ve and  X  X e , are selected for each kind of the above-mentioned classifiers. o Subtree-classifier ik of an internal category C i and its parent category C k : o Local classifier of an internal category C i : o Local-classifier ik of a leaf category C i and its parent category C k : Classification Phase Information about the DAG structure of categories is used during the classification phase. Not as in the tree-based approach, th e classification along the branch leading to a category will stop in two cases: (1) it cannot go further down; or (2) that category steps. For each document d j presented to a category C i from its parent category C k , do the followings: o If C i is an internal category: o If C i is a leaf category: 3.3 Flat Approach document of a category, it is not necessarily classifiable into any of its child categories. For each internal category, one can add in a dummy child category to categories of a category collectively cover it, and all the leaf categories, including the discourse. 
For example, in Figure 3.3 the nodes in dashed ovals represent supplemented ones. If a document is classified as  X  X xpert System X  and  X  X ata Mining c  X , for instance, Information Extraction. 
The advantage of this approach is its simplicity. However, when a category DAG category has a very large number of negative training examples in comparison with considered at the same level is large, it is hard to distinguish them among each other, reducing classification accuracy. There have been various learning algorithms for text classification, among which SVMs ([ 21], [ 22]) has been shown to be one of the fastest and most effective above using SVM classifiers. We have implemented the SVM classifiers using the library LIBSVM provided by [ 5]. We have also employed the automatic model selection proposed in [ 12] for choosing C and kernel parameters of the SVM classifiers. 
Two data sets have been used to test and compare the performance of the three implemented methods, namely tree-based SVM, DAG-based SVM, and flat SVM. One data set is the commonly used Reuters-21578 ([ 18]) with 1327 documents. The other data set comprises 1,000 research papers in Artificial Intelligence (AI) based on the category DAG designed in [4]. Those AI papers have been retrieved from CiteSeer ([ 6]), MIT library ([ 17]), ACM Portal ([ 1]) and some technical reports of the Computer Science Department, Stanford University ([ 20]). We have extracted the abstracts of those papers and manually labeled them as our training set. The preprocessing phase (removing stop words and stemming) has been implemented using the BOW library ([ 15]). Based on the experiment of [ 9], the Document Frequency feature selection method has been used to build the feature vectors for our training set. 
In order to evaluate the performance of the presented methods, we have used the those parameters as follows: Pr where: | C | is the number of categories TP i is the number of documents positively classified into category C i FP i is the number of documents negatively classified into category C i
FN i is the number of documents belonging to category C i but negatively classified  X  is the user-defined importance of precision and recall. In our experiments, for the same importance of precision and recall, we set  X  = 1. Then F 1 is given below: 
For the Reuters-21578, as in other research works, we have chosen 6 categories that have the highest numbers of documents. The tree structure of these categories is depicted in Figure 4.1. Since the tree-based SVM is a special case of the DAG-based SVM, which perform the same on a tree, we compare only the DAG-based SVM and the flat SVM as reported in Table 4.1. The result shows that the DAG-based SVM performs well, and a bit better than the flat SVM. 
For the AI paper data set, we have tested and compared the three methods on three of its subsets with different selected category labels. The subsets comprise 1000 documents on 57 categories, 610 documents on 9 categories (Figure 4.2), and 502 documents on 6 categories (Figure 4.3). Since the number of documents is quite small, we have used cross validation to evaluate the methods X  performance. Following we have run the cross validation for several times (1, 10, 50, 100). The performances of the three methods on the three data subsets are illustrated in Figures 4.4, 4.5, and 4.6, respectively. 
Figure 4.4 shows that the flat SVM has the poorest performance, with F 1 being about 0.396, and the tree-based and DAG-based performances are nearly the same, with F 1 being about 0.434. However, all the three methods have quite low F 1 values, due to a large number of categories involved. As shown in Figures 4.5 and 4.6, when the number of categories is reduced, the performances of the methods are improved and the gap between the flat SVM and structure-based SVM (i.e., tree-based or DAG-based) is also reduced. We have presented three approaches to t ackle the text classification problem where categories are organized as a DAG. The tr ee-based approach adapts the method for original DAG. The DAG-based approach modifies the method in [ 2] to manipulate directly a category DAG. The flat approach adds in dummy leaf categories and reduces the problem to flat classification. We have conducted experiments on the Reuters-21578 data set and our constructed AI paper data sets, using SVMs as classifier s. The results show that the flat approach, which is simple, has a comparable performa nce to the hierarchical approaches when the number of categories involved is small. The tree-based and DAG-based approaches have nearly the same classification accuracy, but the former tends to produce large trees. 
The performance of a hierarchical approach depends on the performance of internal lower levels. We are researching a way to minimize the error at each internal category classifier. Also, we need to do experiments on larger data sets with more complicated hierarchical structures such as Yahoo or Google web pages. Those are among the topics that are being investigated. 
