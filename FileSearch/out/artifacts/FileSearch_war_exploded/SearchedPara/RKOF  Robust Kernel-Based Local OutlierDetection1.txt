 Compared with the other knowledge disco very problems, outlier detection is ar-guably more valuable and effective in finding rare events and exceptional cases from the data in many applications such as stock market analysis, intrusion detection, and medical diagnostics. In general, there are two definitions of the outlier detection: Regression outlier and Hawkins outlier. Regression outlier de-fines that an outlier is an observation which does not match the predefined metric model of the interesting data [1]. Hawkins outlier defines that an outlier is an observation that deviates so much from other observations as to arouse suspicion that this observation is generated by a d ifferent mechanism [2]. Compared with Regression outlier detection, Hawkins outlier detection is more challenging work because of the unknown generative mechanism of the normal data. In this paper, we focus on the unsupervised methods for Hawkins outlier detection. In the rest of this paper, outlier detect ion refers particularly to H awkins outlier detection.
Over the past several decades, the res earch on outlier detection varies from the global computation to the local analysis, and the descriptions of outliers vary from the binary interpretations to probabilistic representations. Breunig et al. propose a density estimation based Local Outlier Factor (LOF) [4]. This work is so influential that there is a rich body of the literature on the local density-based outlier detection. On the one hand, plen ty of local density-based methods are proposed to compute the outlier factors, such as the local correlation integral [5], the connectivity-based outlier factor [8], the spatial local outlier measure [9], and the local peculiarity factor [7]. On the other hand, many efforts are committed to combining machine learning methods with LOF to accommodate the large and high dimensional data [10,14].

Although LOF is popular in use in the literature, there are two major dis-advantages restricting its applications. First, since LOF is based on the local density estimate theory, it is obvious that the more accurate the density esti-mate, the better the detection performance. The local reach-ability density used in LOF is the reciprocal of the average of reach-distances between the given object and its neighbors. This density estimate is an extension of the nearest neighbor density estimate, which is defined as where n is the total number of the objects, and d k ( p ) is the distance between object p and its k th nearest neighbor. As shown in Fig. 1, the heavy tails of the density function and the discontinuities in the derivative reduce the accuracy of the density estimate. This dilemma indicates that with the LOF method, an outlier is unable to deviate substantially from the normal objects in the complex and large databases. Second, like all other local density-based outlier detection methods, the performance of LOF depends on the parameter k which is defined as the least number of the nearest neighbors in the neighborhood of an object [4]. However, in LOF, the value of k is determined based on the average density estimate of the neighborhood, which is statistically vulnerable to the presence of an outlier. Hence, it is hard to determine an appropriate value of this parameter to ensure the acceptable performance in the complex and large databases. In order to address these two disadvantages of LOF, we propose a Robust Kernel-based Outlier Factor (RKOF) in t his paper. Specifically, the main con-tributions of our work are as follows:  X  We propose a kernel-based outlier detection method which brings the vari- X  We propose the weighted density estimate of the neighborhood of a given  X  We keep the same framework of local den sity-based outlie r detection with The remainder of this paper is organized as follows. Section 2 introduces our RKOF method with a novel kernel function, named the Volcano kernel, and an-alyzes the special property of the Volca no kernel. Section 3 shows the robustness and computational complexity of RKOF. Section 4 reports the experimental results. Finally, Section 5 concludes the paper. A density-based outlier i s detected by comparing its density estimate with its neighborhood density estimate [4]. Hence, we first introduce the notions of the local kernel density estimate of object p , the weighted density estimate of p X  X  neighborhood . Then, we introduce the notion of the robust kernel-based outlier factor of p , which is used to detect outliers. Besides, we analyze the influences of different kernels to the performance of our method, and propose a novel kernel function named the Volcano kernel with its special proper ty in outlier detection.
To make this work self-contains, we introduce the notions of the k-distance of an object p ,and the k-distance neighborhood of p , which are defined in LOF. Definition 1. Given a data set D, an object p , and any positive integer k ,the k-distance(p) is defined as the distance d ( p, o ) between p and an object o  X  D , such that:  X  for at least k objects o  X  D \{ p } , it holds that d ( p, o )  X  d ( p, o ) .  X  for at most k  X  1 objects o  X  D \{ p } , it holds that d ( p, o ) &lt;d ( p, o ) . Definition 2. Given a data set D, an object p , and any positive integer k ,the k-distance neighborhood of p ,named N k ( p ) , contains every object whose distance k -distance ( p ) } , where any such object q is called a k-distance neighbor of p . |
N k ( p ) 2.1 Robust Kernel-Based Outlier Factor (RKOF) Let p =[ x 1 ,x 2 ,x 3 ,...,x d ] be an object in the data set D ,where d is the number of the attributes. | D | is the number of all the objects in D .
 Definition 3. (Local kernel density estimate of object p ) The local kernel density estimate of p is defined as wherehisthesmoothingparameter,  X  is the sensitivity parameter, K ( x ) is the multivariate kernel function and  X  o is the local bandwidth factor. f ( x ) is a pilot density estimate that satisfies f ( x ) &gt; 0 for all the objects,  X  is the sensitivity parameter that satisfies 0  X   X   X  1 ,and g is the geometric mean of f ( x ) . kde ( p ) is an extension of the variabl e kernel density estimate [3]. kde ( p )not only retains the adaptive kernel window width that is allowed to vary from one object to another, but also is computed locally in the k -distance neighborhood of object p . The parameter  X  equals the dimension number d in the original variable kernel density estimate [3]. For the local kernel density estimate, the larger  X  , themoresensitive kde ( p ). However, the high sensitivity of kde ( p ) is not always a merit for the local outlier detection i n high dimensional data. For example, if  X  o is always very small for all the objects in a sparse and high dimensional data set, (  X  o )  X   X  always equals infinity. This makes kde ( p ) lack of the capacity to discriminate between outliers and normal data. We give  X  a default value 2 to obtain a balance between the sensitivity and the robustness.
In this paper, we compute the pilot density function f ( x ) by the approximate nearest neighbor density esti mate according to Equation 1.
Substituting Equation 2 into kde ( p ) in Definition 3, we obtain Equation 3, where the default values of C and  X  are 1. In the following experiments, we estimate the local kernel density of object p as follows: Definition 4. (Weighted density estimate of object p  X  X  neighborhood) The weighted density estimate of p X  X  neighborhood is defined as where  X  o is the weight of object o in the k-distance neighborhood of object p ,  X  is the variance with the default value 1 ,and min k =min o  X  N In the majority of local density-based methods, outlier factor is computed by the ration of the neighborhood X  X  density estimate to the given object X  X  density esti-mate. The neighborhood X  X  density is generally measured by the average value of all the neighbors X  local densities in the neighborhood. In this estimate approach, the detection performance is sensitive to the parameter k . The larger the value of k , the larger the scale of the neighborhood. When k is large enough that the majority in the neighborhood are normal objects, outliers have the chance to be detected. In the weighted neighborhood density estimate, the weight of the neighbor object is a monotonically decreasing function of its k -distance. The neighbor object with the smallest k -distance has the largest weight 1. Compared with the average neighborhood density estimate, the weighted neighborhood density estimate makes that outliers ca n be detected accurately even if the num-ber of outliers in the neighborhood equals the number of normal objects. This means that the interval of the acceptable k in the weighted neighborhood density estimate is much larger than that of the average neighborhood density estimate, and our method is more robust to the variations of the parameter k . Definition 5. (Robust kernel-based outlier factor of object p ) The robust kernel-based outlier factor of p is defined as where wde ( p ) is the density estimate of the k-distance neighborhood of p, and kde ( p ) is the local density estimate of p.
 RKOF is computed by dividing the weigh ted density estimate of the neighbor-hood of the given object by its local kernel density estimate. The larger RKOF, the more probable to be an outlier the given object. It is obvious that the smaller the object p  X  X  local kernel density, and the larger the weighted density of its neighborhood, the larger its outlier factor. 2.2 Choice of Kernel Functions In LOF method, for most objects in a cluster, their outlier factors are approxi-mately equal to 1; for most outliers isolated from the cluster, their outlier factors are much larger than 1 [4]. This property makes it easy to distinguish between outliers and normal objects.

The multivariate Gaussian and Epanechnikov kernel functions are commonly used in the kernel density estimation, whose formulations are defined as follows: where x denotes the norm of a vector x and it can be used to compute the distances between objects.

Our RKOF method with the Gaussian kernel cannot ensure that outlier fac-tors of the normal objects in a cluster are approximately equal to 1. Then, we need to determine the threshold value of outlier factors in addition. The Epanech-nikov kernel function equals zero when x is larger than 1. Hence, for most of outliers and normal objects lying in the border of clusters, their outlier factors equal infinity.

In order to achieve the same property with LOF, we define a novel kernel function called the Volcano kernel as follows: Definition 6. The Volcano kernel is defined as where  X  assures that K ( x ) integrates to one, and g ( x ) is a monotonically de-creasing function, lying in a close interval [0 , 1] and equal to zero at the infinity. Unless otherwise specified, we use g ( x )= e  X  X  x | +1 as the default function for our experiments.
 Fig. 2 shows the curve of the Volcano kernel for the univariate data. When x is not larger than 1, the kernel value equals a constant value  X  . This generates that outlier factors of the objects deeply in the cluster are approximately equal to 1. When x is larger than 1, the kernel value is the monotonically decreasing function of x and less than 1. This not only makes outlier factors continuous and finite, but also makes outlier factors of outliers much larger than 1. Hence, RKOF method with the Volcano kernel ca n capture outliers much easier, and sort all the objects according to their RKOF values. In this section, we first analyze the robustness of RKOF to the parameter k . Then, we analyze the computation complexity of RKOF in detail.

Compared with the average neighborhood density estimate used in LOF, the weighted neighborhood density estimate defined in Definition 4 is more robust to the parameter k and it substantially helps impro ve the detection performance. As shown in Theorem 1, if the weighted n eighborhood density estimate replaces the average neighborhood density estimate in the computation of outlier factors, any local density-based outlier detection method following the framework of LOF can be less sensitive to the parameter k .
 Theorem 1. Let N k ( p ) be the neighborhood of object p, and p be an outlier in a data set D . Let r be the proportion of the outliers in N k ( p ) . Suppose that these outliers have the same local density estimate (DE)  X  and k-distance  X  with p. Also suppose that the normal data in N k ( p ) have the same local density estimate  X  and k-distance  X  ,with  X &lt; X  and  X  &gt; X  . The Outlier Factor (OF) can be computed based on any local density-based outlier detection method that follows the framework of LOF. Then for the average density estimate, it holds that: For the weighted density estimate, it holds that: where  X  =  X / X  and  X  is the weight of the outlier in N k ( p ) .
 Proof: For the average density estimate: For the weighted density estimate: Let  X  o i and  X  o j be the weights of the outlier and the normal object, respectively. According to Definition 4,  X  o i &lt; 1 and  X  o j =1 because  X  &gt; X  . Replacing  X  o i with  X  , we have According to Theorem 1, OF ( p ) is a function of the parameter r while  X  has afixedvalue. r is determined by the parameter k . As shown in Fig. 3, for the average neighborhood density estimate, OF ( p ) is a monotonica lly decreasing linear function when r increases. For the weighted neighborhood density esti-mate, OF ( p ) is a quadratic curve of r .When r  X  [0 , 1], OF ( p ) of the average method is always much less than that of the weighted method. Fig. 3 shows that OF ( p ) of the weighted method is larger than  X  %ofthemaximumof OF ( p ) when r  X  [0 ,  X  ].  X  depends on  X  and the weights of the outliers in the neighbor-hood of p . More importantly, OF ( p ) is approximately a cons tant in the interval [0 ,  X  ]. This property indicates that the weighted method makes the local outlier detection more robust to the variations of the parameter k .

Since RKOF shares the same framework with LOF, RKOF has the same computational complexity as that of LOF. To compute the RKOF values with the parameter k , the RKOF algorithm includes two steps. In the first step, the k -distance neighbors for each object need to be found with their distances to the given object computed in the data set D of n objects. The computational complexity of this step is O ( n log n ) by using the index technology for k-nn queries, which has been used in LOF [4]. In the second step, the kde ( p ), wde ( p ), and RKOF ( p ) values are computed by scanning the whole data set. Since both kde ( p )and wde ( p ) are computed in the k -distance neighborhood of the given object, the computational complexity of this step is O ( nk ). Hence, the total computational complexity of the RKOF algorithm is O ( n log n + nk ). Clearly, the larger k is, the more the running time is consumed. In this section, we evaluate the outlier detection capability of RKOF based on different kernel functions and compare RKOF with the state-of-the-art outlier detection methods on several s ynthetic and real data sets. 4.1 Synthetic Data As shown in Fig. 4, the Synthetic-1 data set consists of 1500 normal objects and 16 outliers with two attributes. The normal objects distribute in three Gaussian clusters with 500 normal objects in each cluster with the same variance, respec-tively. Fifteen outliers lie in a dense Gaussian cluster, and the other outlier is isolated from the others. The Synthetic-2 data set consists of 500 normal objects uniformly in the annular region, 500 normal objects in a Gaussian cluster, and 20 outliers in two Gaussian clusters.
 Table 1 exhibits the outlier detection results of LOF and RKOF on the Synthetic-1 data set, respectively, where  X  is the parameter of the weight in RKOF. Top-16 objects are the sixteen obj ects that have the largest outlier fac-tors in the synthetic data set. Obviously, if all top-16 objects are outliers, the detection rate is 100% and the false alarm rate is zero. coverage is the ratio of the number of the detected outliers to the 16 total outliers. RKOF(  X  =0 . 1) can identify all the outliers when k  X  27. RKOF(  X  = 1) can detect all the out-liers when k  X  31. Clearly, the parameter  X  directly relates to the sensitivity of the outlier detection for RKOF. LOF is unable to identify all the outliers until k = 60. Table 1 indicates that the available k interval of RKOF is larger than that of LOF, which means that RKOF is less sensitive to the parameter k .
AsshowninFig.5,RKOFwith k = 14 captures all the outliers in Top-20 objects. LOF obtains it s best performance with k = 20, whose detection rate is 85%. Compared with RKOF, LOF can not detect all the outliers whatever the value of k is. It is obviously that the annular cluster and the Gaussian cluster pose an obstacle to the choice of k . This result indicates that RKOF is more adapted to the complex data sets than LOF. 4.2 Real Data We compare RKOF with sever al state-of-the-art methods, including LOF [4], LDF [6], LPF [7], Feature Bagging [10], Active Learning [11], Bagging [12], and Boosting [13], on the real data sets. The performance of RKOF with the Gaus-sian, Epanechnikov, and Volcano kernels is also compared. In the real data sets, the features of the original data include discrete features and continuous fea-tures. All the data are processed using the standard text processing techniques following the original steps of the methods [7,11,10].

These real data sets consist of the KDD Cup 1999, the Mammography data set, the Ann-thyroid data set, and the Shuttle data set, all of which can be downloaded from the UCI database except the Mammography data set 1 .The KDD Cup 1999 is a general data set condensed for the intrusion detection re-search. 60593 normal records and 228 U2R attack records labeled as outliers are combined as the KDD outlier data set. All the records are described by 34 continuous features and 7 categorical features. The Mammography data set in-cludes 10923 records labeled 1 as normal data and another 260 records labeled 2 as outliers; all the records consist of 6 continuous features. The Ann-thyroid data set consists of 73 records labeled 1 as outliers and 3178 records labeled 3 as normal data. There are 21 attributes where 15 attributes are binary and 6 attributes are continuous. The Shuttle data set consists of 11478 records with label 1, 13 records with label 2, 39 records with label 3, 809 records with label 5, 4 records with label 6, and 2 records with label 7. We divide this data set into 5subsets:label2,3,5,6,7recordsvslab el 1 records, where the label 1 records are normal, and others are outliers.

All the comparing outlier detection methods are evaluated using the ROC curves and the AUC values. The ROC cu rve represents the trade-off between the detection rate as y-axis and the false alarm rate as x-axis. The AUC value is the surface area under the ROC curve. Clearly, the larger the AUC value, the better outlier detection method.

The AUC values for RKOF with different kernels and all other comparing methods are given in Table 2. Also shown in Table 2 are the running time data for RKOF with different kernels as well as those of the other three local density-based methods; since the AUC values fo r other comparing m ethods are directly obtained from their publications in the literature, the running time data for these methods are not available and thus are not included in this table.
From Table 2, we see that different RKOF methods using different kernels receive similar AUC values on all the data sets, especially the Volcano and Gaus-sian kernels. The k values with the best detection performance for all the three kernels on all the data sets are shown in Fig. 6(a). Clearly, the k values for the Volcano kernel are always smaller than those of the other kernels, and the k values for the Epanechnikov kernel are the largest among three kernels. This experiment supports one of the contributions of this work that the proposed Volcano kernel achieves the least computation time among the existing kernels. It indicates that different kernels used in RKOF do not significantly influence the detection performance, but they dramatically change the minimal k value with the acceptable performance and consequently the running time.

Fig. 6(b) shows the ROC curves of RKOF based on the Volcano kernel for the KDD data set ( k = 320) and the Mammography data set ( k = 110). Fig. 7 shows the AUC values of RKOF based on the Volcano kernel with different k values for the KDD and Mammography data sets. The AUC values for the KDD data set are larger than 0.941, when k varies from 280 to 700; the AUC values for the Mammography data set are larger than 0.824, when k changes from 40 to 460. Clearly, the detection performance of RKOF for any k in these interval is better than that of the other comparing methods except LPF. For the Mammography data set, RKOF is more effective than the other comparing methods with k = 110, compared with k = 11183 for LPF. For the KDD data set, RKOF achieves the second best performance with k = 320. The best AUC value is achieved by LPF, but this AUC value is obtained when k = 13000. The complexity of RKOF is O ( n log n + nk ), compared with O ( nd log n + ndk )for LPF, where d is the dimensionality of the data. It is clear that under the same circumstances LPF takes much longer time than RKOF while the AUC value of RKOF is very close to this best value. For the Ann-thyroid data set, RKOF achieves the acceptable performance that is very close to the best performance. TheAUCvalueoftheShuttledatasetistheaverageAUCofallthefivesubsets, where the AUC values of the subsets with the label 5, label 6, and label 7 are all approximately equal to 1. RKOF also obtains the acceptable performance that is very close to the best performance for the Shuttle data set. Overall, while there is no winner for all the cases, RKOF always achieves the best performance or is close to the best performance in all the data sets with the least running time. In particular, RKOF achieves the b est performance or is close to the best performance for the KDD and the Mammography data sets with much less running time, which are the two large data sets of all the four data sets. This demonstrates the high scalability of the RKOF method in outlier detection. Specifically, in all the cases RKOF always has less running time than LOF, LDF and LPF. Though the running time data for the other comparing methods are not available, from the theoretic complexity analysis it is clear that they would all take longer running time than RKOF. We have studied the local outlier detection problem in this paper. We have proposed the RKOF method based on the v ariable kernel den sity estimate and the weighted density estimate of the neighborhood of an object, which have ad-dressed the existing disadvantages of LOF and other density-based methods. We have proposed a novel kernel function named the Volcano kernel, which is more suitable for outlier detection. Theoretical analysis and empirical evaluations on the synthetic and real data sets dem onstrate that RKOF is more robust and effective for outlier detection at the sa me time taking less computation time.
