 Query-biased search result summaries, or  X  X nip pets X , help users decide whether a result is relev ant for their informa-tio n n eed, and h ave become increasingly important for help-ing searchers with difficult or ambiguous search tasks. Pre-viously pub lished snippet generation algo rithms have been primarily based on selecting document fragments most sim-ilar to the query, which d oes not t ake into acc oun t which parts of the docu ment the searchers actually fo und u se-ful. We present a new app roach to improving result sum-maries by incorporating post-click searcher behavior data, such as mouse cursor movements and scrolli ng o ver the re-sult documents. To achieve this aim, we develop a method for collecting behavioral data with p recise association b e-tween searcher intent, document examinatio n b ehavior, and the correspond ing docu ment fragments. In turn, this allows us to incorporate page examination b ehavior signals into a novel Behavior-Biased Sn ipp et generatio n system (BeBS). By mining searcher examination data, BeBS infers docu-ment fragments of most interest t o users, and combines this evidence with text-based features to select t he most promising fragments for inclusion in the result summary. Our extensive e xperiments and analysis demonstrate that our method improves the quality of result summaries com-pared to existing state-of-the-art methods. We believe that this work opens a new direction for improving search re-sult presentation, and we make available the code and the search b ehavior data used in this stud y to encourage further research in this area.
 H.4 [ Informat ion Systems Applications ]: Miscellaneous Result summary generatio n, searcher behavior, mouse cur-sor movement
While web search engines have been rapidly evolving, one constant in the search result pag es has been the presence of some form of a document summary, or  X  X nippet X , pro-vided to help a user to select t he best result documents. For some queries, the generated snippets already contain the de-sired information, either by design (e.g., Goog le X  X   X  X nstant Answers X  1 or Wolfram Alpha 2 ) or by serend ipity. Search en-gines have been increasingly succ essful at this, as evidenced by work on  X  X oo d aband onment X  [25 ] and others. Neverthe-less, for a large class of queries where the desired informatio n need is either ambiguous or cann ot be answered succ inctly, result snipp ets play a crucial role in guiding the users to the needed d ocu ments.

Our aim is to improve snipp et generation for informa-tional queries [7]. Sp ecifically, our goa l is twofold: If pos-sible, the snipp et text should include the desired informa-tio n d irectly. Otherwise, the snipp et should p rovide suffi-cient information for the user to distinguish the useful from the non-useful results. Fortunately, millions of users search daily, clic king o n results and examining the documents. Our key insight is that by capturing such examinatio n d ata, we could relate the areas of interest in the document to the search intent , subsequently generating more useful snippets for future searchers.

As a concrete example, consider an information n eed, specified as  X  X ow many pixels must be dead on a iPad 3 before Apple will replace it. X . After issuing the search query  X  X ow many dead p ixels ipad 3 replace X , the user examined the document t o find the answer, as s hown in Figure 1 (the eye ga ze positions were collected b y using infra-red eye track-ing). Note that while the user examined both u seful and non-useful docu ment fragments, she eventually focused on the best fragment containing the answer to the question. Hence, for this search intent and this docu ment, an ideal snipp et would in clud e the text fragment where the user fo-cused her attention. In contrast, if a document was not rel-evant, a goo d snipp et should also include the area of most interest, as it could help users with the same intent t o a void visiting this result in the future.
 This example intuitively motivates our goa l to generate Behavior-Biased Sn ipp ets, or BeBS, which, as we show experimentally, can significantly improve snipp et quality, which h as been shown in eye-tracking studies to signifi-cantly affect search b ehavior (e.g. [10]). Whil e pu tting a n eye-tracker on every desk is not yet feasible, we can still infer searcher interest in p articular pag e regions from their mouse cursor and scrolli ng behavior (e.g., following the ideas of [18] and [24]), and then incorporate this evidence to make the snippets more useful. To the best of our knowledge, our work is the first to successfully incorporate such b ehavior data into result snipp et generatio n. http:/ /goog leblog.blogspot.com/2010/09/goog le-instant-behind -scenes.html http:// www .wolframalph a.com/ mouse activity such as text tracing, link p ointing, link click-ing and text selection enable more acc urate extraction of key words of interest t han using the whole text of the page. More re cently, White and Buscher [39] proposed a method that uses text selections as implicit feedback, while Guo a nd Agichtein [19 ] proposed a Post Click Behavior (PCB) model to estimate the  X  X ntrinsic X  document relevance by modeling post-clic k behavior such as mouse cursor movements and scrolli ng. Our method extends this app roach to identify spe-cific regions, or fragments of the pag e, of particular interest to the users for including in the result summary.

In summary, our work extends previous s tate-of-the-art in snippet generatio n b y considering a radically new source of evidence, namely the user examinatio n d ata, in order to bias the resulting summaries towards the most  X  X nteresting X  or  X  X seful X  parts of the document, as we describe next.
First, we formalize the problem of generating  X  useful X  snipp ets. Then, we describe the key parts of our app roach (Section 3.2) , and the infrastructure we developed to acc om-plish the required data collectio n (Sectio n 3.3).
Following the literature on snipp et quality [31], snipp ets must satisfy the aspects of Representativeness , Reada bility , and Judgeability : 1. Representativeness : measures how w ell the snipp et 2. Reada bility measures the e ase with which the text of 3. Judgeability measures how w ell the snippet helps a user
Our primary go al is to optimize the Representativeness and the Judgeabili ty criteria by biasing the selected snip-pets towards the regions of most interest to the user, as inferred from the pag e e xaminatio n d ata. That is, our goa l is not t o replace the existing text-based snippet generation app roaches, bu t rather to a dd add itional evidence (when available) abou t t he parts of the document t o privilege.
Our app roach operationalizes the snipp et quality criteria above by incorporating both textual and b ehavioral evidence using a robu st machine learning-based app roach. Sp ecifi-cally, we combine tog ether the traditio nal text-based snippet generatio n features, and the inferred u ser interest in specific parts of a document.

First, following [27 ], a fragment scoring system is trained based on text-based features, using hu man judges, resulting in a strong text-only baseline that generates candidate frag-ments to be included into the snipp et (Section 4.1). Sepa-rately, examination b ehavior data is collected over the land -ing pag es, using o ur logg ing infrastructure described in the next section. Then, a behavior model is trained to infer the document fragments of interest to the user, based on user examinatio n d ata (Section 4.3). Finally, the behavior-based p rediction of interest in each cand idate fragment is combined with the original (t ext-based) fragment score, in order to g enerate the fin al behavior-biased snipp et cand i-date ranking (Section 4.4). Note that by decoup ling the behavior modeling from the cand idate generatio n method, our app roach can b e used with any other snipp et generatio n app roach that provides s cores for the cand idate fragments, which could b e combined with the behavior scores for the fin al ranking step.

While general and fl exible, our app roach makes three key assumptions. First, our method is primarily targeted (and evaluated for) informatio nal queries  X  that is, queries for which the user expects to find an answer in the text of the pag e, and optimizes the snippets acc ordingly. Second , we as-sume that document visits can b e group ed b y q uery intent, so that behavior features on the land ing pag es can b e agg re-ga ted tog ether for all the searchers with the same informa-tio n n eed. While a nu mber of methods have been proposed to cluster queries (and results clicks) by intent ( e.g., [34]), we acknowledge that t hese techn iques are not perfect, and may introdu ce noise in p ractice. Finally, we assume that user interactio ns on land ing pages can b e collected b y a search engine or a third party. While this naturally introdu ces po-tentia l privacy concerns, this assumptio n is not far-fetched: already, browser plug-ins and too lbars collect user interac-tio ns on web p ages; major organizatio ns can (and often do) use proxies for external web access; and common pag e wid-gets like bann er ads and visit counters inject JavaScript code to monitor basic user interactio ns and can b e e asily extended to collect more detailed data. While the privacy and secu-rity consideratio ns of these methods are beyond the scope of this paper, we merely point out t hat these behavior ga th-ering too ls already exist and are widely deployed. We will discuss these issues and p otentia l solutio ns in more depth in Sectio n 7.
A key component of our system is a mechanism for col-lecting searcher interactio ns on web pages, and tying them precisely to the page content at t he word level. As a start-ing point, we adapt the pub licly available EMU too lbar for the Firefox browser [17], that is able to collect mouse cursor movements over any visited webpage. Unfortun ately, out-of-the-box EMU fun ctionality is not sufficient, as the user interactio ns are not conn ected to the und erlying pag e con-tent: the available JavaScript API does not provide the text positio n und er the cursor, which could depen d on screen res-olution, size of browser wind ow, browser version, and p er-sonal browser settings.

To a ssociat e the tracked mouse cursor positions with cor-respond ing text fragments we e mployed the following tech-nique. After the HTML pag e is rendered in the browser win-dow, our JavaScript code modifies the document DOM tree, so that each word is wrapp ed by a separate DOM element tag s. Then for each DOM Elem ent, the window coo rdinates of that element are evaluated and saved in the Element X  X  attribu tes. Then, the processed HTML pag e with the coo r-dinates of each DOM Element is s aved to the server by an asynchronous request. The saved co ordinates are upd ated if the pag e layout is changed du e to a resize wind ow event or an AJAX actio n.

Thus, for each p age visit we know the searcher X  X  intent (question), the search engine query that t he user issued, the URL, the contents of the document, the bound ing boxes of each word in the HTML text, and the log of behavior actions: mouse cursor coo rdinates, mouse clicks, scrolling, and an answer to the questio n that t he user found in a page and sub mitted in the ga me interfa ce. Next, we show how to use this information to infer patt erns of browsing behavior that capture portions of document t hat are of most interest to the user.
We now present t he details of our Behavior-Biased snipp et generatio n system (BeBS). First, we describe the text-only snipp et generatio n system (Sections 4.1 and 4.2). Then, we introdu ce the method for inferring the most interesting or useful parts of the document from user behavior (Sec-tio n 4.3), to incorporate into the combined snippet genera-tio n p rocess (Section 4.4) .
In order to generate snipp ets, we extend the app roach presented in Metzler and Kanun go [33 ]. The downloaded HTML pag es are pre-processe d and indexed with Natu-ral Languag e Too l Kit (NLTK [3]). Extracted text is di-vided into sentences using Punk un supervised sentence split-ter [28]. We ind ex the text of the web p ag e exclud ing the &lt; sc ript &gt; and &lt; st yle &gt; tag s.

For a g iven query we first select all the sentences that have at least one match of query terms for further snipp et frag-ment generatio n. Once the set of sentences is selected, our system generates all possible snippet fragment cand idates by app lying a sliding wind ow moving a long each sentence. We vary fragment length from 3 words to a maximum character length provided as an inpu t parameter. We discard all frag-ments that do not contain any query term matches. Along with fragment generatio n our system scores each fragment using T extS cor e functio n d escribed in Section 4.2. This score is used to generate the fin al snippet.

The problem of summary generation h as been stud ied ex-tensively in n atural languag e processing a nd summarizatio n research commun ities. It has been shown in [36] that t his problem is equivalent t o a weighted set cover problem which is, i n turn, known to be NP-Hard. There are several pos-sible app roaches, i nclud ing g reedy weighted set cover and relaxatio ns, primarily based on integer linear programming. We resort to a g reedy algorithm, du e to relative simplicity of implementation. Our system can easily be extend ed with more advanced set cover solver if needed. As the set cover algorithm requires score compu tation for the set of selected fragments, we recompu te the scoring functio n for the un ion of selected cand idate fragments to find a set that greedily maximizes the score for the entire snipp et.
The fragment scoring required for snippet generation re-lies on a machine learning a pp roach b ased on set of text fea-tures representing various quality aspects of fragment candi-date. We extend the method of [33] by adding a dd itional fea-tures capturing relevance of the fragment ( relevance group ), properties of query match (query match group ) and read-ability of the fragment ( readability group). These features are summarized in Table 1. For T extS cor e score compu ta-tio n we used the Gradient Boo sting Regression Tree model [15] (GBRT). GBRT is a powerful family of models that has been successfully used in many app lications including sentence selection for search result summarizatio n [33] and search result snippet readabil ity assessment [2 7]. We train a GBRT model on a subset of training query-URL pairs to predict snippet fragment scores.
 Gradient Boo sting Regress ion Tree performs a nu mer-ical optimization in function space instead of parameter space. We provide a brief overview of the algo rithm and refer to the original paper for detailed informatio n [15]. A regression tree model f ( x ) , x  X  R n , partitions the space of covariates into disjoint intervals R k , k = 1 , 2 , ...., K associ-ated with leaf nodes of the tree. Each interval is assigned a value  X  k , such that f ( x ) =  X  k if x  X  R k . Thus, the tree model can b e written in where  X  = { R k ,  X  k } K k =1 , and I is an ind icator functio n. For a g iven loss function L ( y i ,  X  i ) t he parameters  X  are result of the following o ptimization problem: In our experiments we employ the squared loss functio n to train the regression trees. A gradient boo sted regression tree is an ensemble model [ 15] t hat incorporates a series of regression trees, and can b e written as: where at each stag e m,  X  m is estimated to fit t he residu als from the m  X  1th stag e: and M is the nu mber of stages (regression trees) in the model. In p ractice, one add s T ( x ;  X  m ) multiplied by  X   X  the learning rate inpu t parameter specified for the algorithm, resulting in the fin al predictor: In our implementation we used M = 200 regression trees, and  X  = 0 . 01 to train the GBRT model for fragment scoring.
To infer the text fragment importance from the user X  X  browsing behavior, we ag ain app ly supervised machine learning, namely the Gradient Boo sting Regression Tree (GBRT) algorithm [15], trained to identify X  X nteresting X  text fragments. Sp ecifically, for each p ag e visit of a user, the doc-ument is represented as a set of short text fragments . We consider a particular fragment to be  X  X nteresting X  (attrac-tive), if the user submitted an answer in the current session, Feature Descript io n Feature Grou p ExactMatch 1 if fragment contains query as a su bstring, otherwise 0 TermOve rlap Overlap of query terms an d the fragment LanguageMo delScore Fragment score u nd er the languag e model as in [33] Length Total n u mber of terms Location Rela t ive locatio n of the fragment in the document BM25ScoreFragment BM2 5 score of the fragment NumMatches Absolute cou nt of query terms matched in the fragment NumDistinctTerms Number of distinct terms in the fragment NumPunctC har Number of p un ctuatio n characters PercentPu nctChar Percent of p un ctuatio n characters NumLetterChar Number of letter ([a-zA-Z]) characters NumWordsCap Number of words with first letter capitalized PercentWo rdsCap Percent of words with first letter capitalized PunctPerWord Number of p un ctuatio n characters per word in the fragment an d the answer shares words with the fragment ( after stem-ming a nd stopword removal). Other fragments are labeled to be not interesting.

For each fragment we create a set of behavior features that could capture fragment interestingness. One key feature is the du ratio n of time when the mouse cursor was placed over the text fragment, or very close to the fragment. We also adapt t he features to measure scrollbar and event activity from referen ces [11] and [19], in order to detect  X  X eading X  vs.  X  X kimming X  behavior. The complete list of the fragment behavior features is presen ted in Table 2. Note that these features are exclusively focused on capturing user X  X  behav-ior associated with focused attention, and , by design, do not contain any docu ment or query information, in order for the learned b ehavior model to be app licable to un seen documents and queries.

The feature generation algorithm joins a sequence of be-havior events and a set of bound ing boxes for each word and DOM Element of a pag e. To speed up this join, our imple-mentation uses a spatial R-Tree index of element bound ing boxes, which allows for each logg ed event t o be efficiently matched to the matching DOM Elem ents in the specified coo rdinate range.

We then train the GBRT model on the training set of the labeled document fragments, each represented using the behavior features described above. The data is s tratified by the original document URLs, so that t he URLs of the fragments in the training a nd test sets are disjoint. The training set is created from only those page visits where the document text has a non-empty intersection with the user X  X  answer, and the answer is correct. The trained p rediction model is then applied to all pag e visits in the test set. Note that when the predictor is app lied on the test set, it has no informatio n about the user X  X  intent, answer, or the current query, and u ses only the behavioral features of the current pag e visit. The predicted behavior-based fragment interest -ingness score, is then used as a key ev idence for the fin al snipp et generatio n algo rithm, described next.
 Feature Descript io n
MouseOve rTime Time d u ration when the mouse cursor
MouseNearTime Time d u ration when the mouse cursor MouseOve rEvents The n u mber of mouse events du ring MouseNearEvents The n u mber of mouse events du ring
DisplayTime Time d u ration when the text fragment
DispMiddleTime Time d u ration when the text fragment
The fi nal step in our app roach is to combine the text-based score T extS cor e ( f ) for a candidate fragment (Section 4.1) with the behavior-based interestingness score BScor e ( f ) (Section 4.3) , inferred from the examination d ata. In our current implementatio n we combine these scores by linear combination: Note that T extS cor e ( f ) is not normalized, and could have values in the range [1 , 5], while BScor e ( f ) is normalized to be between 0 a nd 1.

The parameter  X  affects two characteristics of the algo-rithm: snippet coverage and quality . Sn ippet coverage is de-fin ed as the ratio o f the snippets produ ced by the behavior-biased algo rithm that differ from the snippets produ ced by the text-only baseline. Sn ipp et quality is measured by judge-ability, readabi lity, and representativeness metrics, via man-ual assessments. As  X  app roaches zero, coverage would also app roach zero (as text-based features would d ominate cand i-date selectio n), and the algo rithm effectively backs off to the baseline. In contrast, when  X  is large, snippet quality might decrease by weighing the behavior-based score too highly compared to the text-based score. We performed manual as-sessments for five different parameter values of  X   X  [0 , 1] to select t he best value. Other more soph isticated ways to com-bine text and b ehavior evidence are possible, such as jointly learning o ver both text and b ehavior features, as could b e explored in the future. However, we chose to follow the sim-pler linear app roach for bette r interpretability of the results (e.g., by analyzing the results of varying the  X  parameter).
This sectio n p resents the methodology used for acquiring search b ehavior data for training our system (Section 5.1) , describes the resulting behavioral data (Sectio n 5.2), the explicit snipp et judgments dataset used for training a nd validating the text-based snippet generatio n b aseline (Sec-tio n 5.3) . Note that t he code and the data used for exper-iments are available from http://ir.mathcs.emory.edu/ intent/ .
To collect the search b ehavior data, we used the infras-tructure created and pub lished by [1], and modified it for our task. The participants played a search contest  X  X a me X  consisting of 12 search tasks (questio ns) to solve. The stated goa l of the ga me was to sub mit the highest possible nu m-ber of correct answers within the allott ed time. After the searcher decided that t hey found the answer, they were in-structed to type the answer tog ether with the supp orting URL, into the correspond ing fields in the game interface. Each search session (for one question) was completed b y ei-ther sub mitting a n answer, or by clicking the X  X kip question X  bu tton to pass to the next question.

Participants were recruited through the Amazon Mechan-ical Turk (MTurk) website. As a first step, the workers had to solve a ReCaptcha pu zz le to verify that they are hu man and n ot an automated  X  X ot X . A browser verification check was performed to verify that t he browser is compatible with our JavaScript t racking code. During the data postprocess-ing stag e, we filtered out t he users who did n ot answer even the easy, trivial questions, as it indicated either poo r und er-stand ing o f the ga me rules, or an attempt t o make a quick bu ck without effort.

To capture all of the participants X  search actions, they were instructed to use only our search interface. Our search interfa ce performs web search u sing the pub lic API of a popu lar web search engine (we used Bing for our experi-ments), and d isplays the result pages using the original pag e design, l ayout and stylesheets, so the user X  X  s earch experi-ence is not affected. The Apache Web server proxy func-tio nality was used b y confi guring the modu les mod proxy , mod proxy html , an d mod sed so that the users could search an d b rowse the Web as usual, while the URLs in the HTML links were automatically replaced to request the document through our proxy. As the requested documents were re-turned through our proxy , JavaScript logg ing code was in-jected into the document, as described in S ectio n 3.3. A total of 109 MTurk participants fin ished their tasks. After filtering o ut t he users who did not follow the game rules, we obtained 1175 search sessions, performed b y 98 users. Our data for these users consists of 3,294 queries, 1,598 un ique queries, and 2,997 SERP clicks on 662 distinct URLs. Of these, the document behavioral data was collected for 2,289 pag e visits (76%) and 508 distinct URLs, compris-ing the fin al dataset for our experiments described below For each p ag e view there were on average 400 atomic brows-ing events (mouse movements, scrolli ng, key pressing) on av-erage. Note that a document might be visited from different queries, and for each query-URL pair, the snipp et genera-tio n algo rithm would p rodu ce a snippet independ ently. So the comparative e xperiments for snippet quality evaluatio n were performed on a set of 707 different query-URL pairs.
The set of the documents with collected behavior data was divided rand omly into equal-sized training a nd test set, stratified by the document URLs to ensure no snipp ets de-rived from the same document t o be includ ed in both th e training a nd test set. The training set was used to train the regression algorithm for predicting fragment interest ingness, and the test set was used to evaluate the generated snipp ets.
In order to train the text-based baseline snippet genera-tio n algorithm (Sectio n 4.1) , we require a set of labeled text fragments. For this, we collected 949 text fragment quality judgments through the Amazon MTurk service. The asses-sors were asked to re-rank 10 text fragments rand omly cho-sen b y our fragment generator system to o btain a reliable training set. Each fragment was judged b y 3 a ssessors. The fragments used in for this data collection were generated from query-URL instances taken from our training set, and do not overlap with our test set. We specifically asked as-sessors to re-rank fragments to a void p otentia l inacc uracies caused by using a n absolute scale. In order to train the frag-ment scorer, we performed rank agg regation b y compu ting the average rank of the fragment among the rankings pro-du ced by different judges.
We now present the main results of our stud y. First, we report the interm ediate result of using the behavior data to infer the interesting (useful) fragments in the document. Then, we report the main results of the paper where the quality of the generated snipp ets, with and without us-ing behavior data, compared using hu man judgments (Sec-tio n 6.2).
This experiment evaluates how w ell we can p redict inter-esting fragments by observing the user X  X  document exami-natio n b ehavior. We defin e the fragment t o be interesting if it is related to the answer for the question. For each visited pag e we collect t he user X  X  answer (if submitted), and all the correct answers from all the users who a nswered this ques-tio n. Then we automatically compare those answers to each text fragment in the document.
For the remaining 24% of the pag e visits, the behavioral data was missing du e to conflicts between our JavaScript tracking code and other JavaScript code already present on the pag e, and these documents were omitted from the dataset. Figure 2: Illustration of the cros s-validation setup: the training and test sets are disjoint by both users and URLs.

For this experiment, the document t ext is represented as a set of short text fragments, each consisting o f five words. Those 5-word sequences are, on one hand , almost un ique in a typical web d ocument, and thu s could b e used as an identifier of a text position in a document, and on the other hand are short enough to match to local behavior patterns. For each fragment a set of behavior features is compu ted as described in sectio n 4.3.
 The cross-validatio n experiment set up as ill ustrated in Figure 2. The set of users and URLs are divided into a training set and a test set, so that t he training set and the test set are disjoint for both sets of users and URLs. 10 -fold cross-validation was performed , so that each u ser-URL pair app eared in a test set for some cross-validation split.
In the training set, a fragment X  X  label is set to label ( f ragment i ) = 1 if the user sub mitted an answer in the current session, the answer is correct, and the answer has commo n words with the fragment. If the user sub mit-ted a correct answer, bu t t he answer shares no words with a document fragment, then label ( f ragment i ) = 0. If the user did not sub mit an answer, or the answer is incorrect, we ex-cluded the fragment from the training set. The answer and the fragment were compared after stemming a nd stopword removal. Similarly, in the test set we use for evaluatio n only those fragments that share words with the submitted search query.

The Gradient Boo sting Regression Tree algorithm was trained on the training set of fragments, and app lied to the test set. So each fragment in the test set receives a fragment interestingness BScor e ( f ragment i )  X  R .

We evaluate the interestingness of fragments by compar-ing the fragment X  X  text with user X  X  answer (if it exists) , and to a ll the correct answers submitted b y all users for the same question. We use the stand ard ROUGE-1 and ROUGE-2 metrics [32] for evaluation of the fragment intersectio n with answers, as these metrics are commonly used for evaluatio n of automatic summarization and ann otatio n algorithms.
ROUGE-N metric for a fragment and a set of answers A , is computed as the recall of the answer set covered by the fragment word N-grams: ROUGE-N ( f ragment, A ) =
Figure 3 shows the relationship between the interesting-ness of a fragment an d the behavior score. The graph shows that when the score is high (  X  0 . 5) , the average intersec-tio n b etween the fragment and u ser X  X  answer is much h igher than those when the fragment score is low. All ROUGE-N metrics increase when the behavior score increases, bu t the ROUGE-2 values over all correct answers are always very small (changing from 0 . 003 to 0 . 007) . We note that Figure 3: Fragment ROUGE-1 and ROUGE-2 values vs. behavior-based interestingnes s score BScor e . ROUGE-1 is much greater than ROUGE -2 for high scores, as the interesting fragment might contain useful i nformatio n for the answer, bu t t he user reformulat es the obtained infor-mation and sub mits reformulat ed answer. The ROUGE-N values for a user X  X  answer are much greater than those for all correct answers, as other users might obtain valuable in-formation from other documents, and some questions have distinct correct answers.

This experiment shows that we can p redict fragment in-terestingness by using behavior features only. In the next section, we app ly the computed behavior scores for a prac-tical task of improving search result summaries. E valuation S etup : Our evaluatio n follows the snippet qual-ity desiderata o utlined in Section 3.1. Sp ecifically, the snip-pet quality is evaluated by perform ing blind p aired p ref-erence tests. For each query and URL, a pair of snippets produ ced by two different algorithms were evaluated by an assessor. A pair of snipp ets were presented on a pag e in ran-dom order, so the assessors did not know which algorithm produ ced which snipp et.

Each assessor was asked to examine the web p ag e, the search query, and the pair of snippets, and to a nswer three questions that corresp ond to o ur snipp et quality criteria: For each question there were three possible answers:  X  X nip-pet 1 is better X , X  X oth snippets are similar for this criterion X , and  X  X n ippet 2 is better X .

We hired 14 pre-qualified Amazon MTurk workers, who have previously shown accurate results and h igh agreemen t with our  X  X o ld stand ard X  subset of snipp ets labeled b y the authors. As an add itional test, we also included a small portion of exactly the same snipp ets to check the quality of MTurk workers, to verify that for the same snipp ets the worker submitted  X  X oth snippets are similar X  label for each criterion. As a result, we collected pairwise preferen ce Feature Grou p NDCG @10 Single Feature Group Metzler-Kan un go 0.71 9 Relevance 0.74 1 Readability 0.74 3 QueryMatch 0.71 9 All Except One Feature Grou p All-Metzler-Kan un go 0.72 5 All-Relevance 0.73 6 All-Readabili ty 0.74 3 All-QueryM atch 0.71 7 All 0.76 4 Table 3: Feature ablation results for fragment text scoring (10-fold cross validation) labels for 2 959 snipp et pairs, 8525 a tomic judgements (t hree judgements for each snipp et pair, exclud ing  X  no a nswer X  re-sponses). The total amoun t paid to the MTurk workers was $217 . One half of the obtained judgements were used for de-velopment and d ebugg ing pu rposes, and the other half were used for testing and reporting the results in the next section. Evaluation Metrics : As a main evaluation metric we use the fraction of labels that give preference to the BeBS sys-tem, compared to the baseline. The preferen ce ratio metric is evaluated for each criterion in (judgeability, readability, representativeness). The reason is that we found that the snipp et quality criteria a re difficult for assessors to judge ab-solutely, bu t can b e easily compared as relative prefere nces.
Before studying the benefi ts of behavior-based snipp et generatio n, we optimized the text-based baseline method (Section 4.1) by v arying the combinatio n of features used. The results of the feature ablatio n experiments are re ported in Table 3. As this table shows, using all of the text-based features achieves highest judge preference for the resulting snipp ets, thu s, we use all of the features described in Table 1 for the baseline system performance in subsequent experi-ments.
This experiment compares the text-based baseline with our BeBS system. Recall, that the  X  parameter (t he rela-tive weight of the behavior score) affects two characteristics of the algorithm: coverage and snippet quality, as described in Section 4.4. To explore the resulting trade-offs, Figure 4 reports the judgeabili ty, readability, representativeness, and coverage for five values of  X  . The binomial distributio n two-sided statistical significance test with confid ence level 0 . 9 was compu ted, and the correspond ing confid ence intervals are presented on the graph . The graph shows that setting of  X  = 0 . 7 provides significant improvement on all three snipp et quality metrics. For this value, the coverage is 40 %, which means that t he behavior features provide improve-ment in representativeness for 0 . 4  X  0 . 68 = 27% of all snip-pets, and p rodu ce worse snipp ets for 0 . 4  X  (1  X  0 . 68) = 13% of all snipp ets. When  X  = 0 . 5, judgeability and readabili ty also improve, bu t the improvement in representativeness is small and n ot statistically significant. When  X  = 0 . 9, cover-ag e grows up to 53 %, bu t results in more noise and d egrades snipp et quality. When  X  is set to a low value, the coverage drops, and we have too few modified snipp ets from the base-line to observe statistically significant differences in snippet Table 4: Pairwise preference tests for snippets with behavior features added, number of judgements quality. The graph shows that for  X   X  { 0 . 1 , 0 . 3 } , the confi -dence intervals cross y = 0 . 5 a xis, and this means that the differen ce in snippet quality is not statistically significant. The Table 4 reports the detailed assessment data for the two best runs.

Finally, we show two examples that demonstrate how in-corporating behavior features can affect the resulting snip-pets. The first example (Figure 5, l eft) compares two snip-pets, the first produ ced by the baseline algo rithm (t op), and the second b y using BeBS (bottom), for the query  X  X ports invented in australia X , corresp ond ing to the questio n  X  X hat sports did Britain get from Australia? X . The example shows that t he snippet produ ced by BeBS is sub stantia lly more useful than the baseline, in clearly ind icating to the searcher that t he answer is present in the document. Note that this document is a difficult case for traditional snipp et genera-tio n app roaches: the land ing page contains 8 matches for the query word  X  X port X , and more than 50 matches for each of words  X  X nvent X , and  X  X ustralia X . Furthermore, the relev ant fragment is located near the bottom of the document ( three scrolli ng screens down), yet it is includ ed into the snipp et because several users scrolled near the e nd of the pag e and inspected the text near this fragment t horoughly, resulting in a high b ehavioral score.

The second example shows a case where the BeBS p ro-du ced an inferior snipp et compared to the baseline. The searcher issued a query  X  X etals less dense than water X , and the baseline algorithm produ ced a goo d snipp et that con-tains the correct answer  X  X otassium and Lithium X , relevant to both search query and search intent. But many users did not found the answer on the land ing pag e, and instead ex-amined the attractive section of the pag e correspond ing to  X  X opu lar Searches X , with the list of sugg ested queries. That resulted in a high behavior scores for that fragment, and consequently BeBS p rodu ced a snippet with poo rly readable and irrelevant t ext. Add itional behavior data, and further tun ing o f the behavior score predictio n may alleviate these errors, as we plan to explore in future work.
To estimate relat ive importance of behavior features for snipp et generation, we analyzed the Gini importance in-dex [5] for each b ehavior feature from the table 2. The table 5 shows that t he most important features are DispMid-of  X  .
 Table 5: Feature importance for behavioral features dleTime -the time d uration when the text fragment was vis-ible in the midd le of the browser wind ow, and MouseOver -Time , the time du ratio n when the mouse cursor hovered over the text fragment. While the first feature has been previously shown [11 ] to be beneficial for re-ranking search results, we are encouraged to find it t o be also helpful for snipp et generation. The MouseOve rTime feature has been shown to be correla ted with user interest [18], thu s confi rm-ing o ur hypothesis that searcher interest can b e helpful for generating better snippets.
We presented a novel way to g enerate behavior-biased search snippets that privilege the document fragments ex-amined by the users. To our knowledge, our work is the first to incorporate search examinatio n b ehavior data into snipp et generation result summary generatio n. To a ccom-plish this, we developed a robu st methodology to a cquire precise docu ment examination d ata a t scale using mouse cursor movement and scrolling, and to associat e these in-teractions with the fragments of the und erlying docu ment. Our results s how significant quantitative improvements over a strong text-based snipp et generation b aseline. We com-plemen t t he results with qualitative observations about the cases where examination b ehavior can b e helpful or harmful to the generated snippets. Our code is based on the freely available EMU plug-in for the Firefox browser [17 ], and the search b ehavior data used for experiments is shared with the research commun ity to encourage further work in this area.
While our work makes significant advances to search snip-pet generatio n, it also o pens many directions for future im-provements and extensions. While our method was primar-ily targeted (and evaluated for) informatio nal queries, an important consideratio n is how to generalize our app roach to a wider class of queries. For other query classes (e.g., naviga tional), snipp ets might be optimized using a different criteria, bu t snippet generatio n could still take advantag e of the search examination d ata. We have also a ssumed that document visits can b e group ed by query intent. A nu mber of methods have been proposed to cluster queries (and result clicks) by intent ( e.g., [34, 30 , 38]), and one future directio n is to extend our method to work with automated query clus-tering techn iques, which might introdu ce add itional noise. Another key assumptio n is that user interactions on land ing pag es can b e collected by a search engine. As we pointed out earlier, commerc ially deployed methods already exist t o collect interactio n d ata on land ing pag es, for app lications such as advertising. While add ressing the privacy issues inh erent in search b ehavior logg ing is beyond the scope of this paper, research in p rivacy-preserving data mining [2] could potentially alleviate these concerns. Furthermore, as efficient machine learning methods an d computatio nal capa-bili ties of personal and mobile devices continu e to advance, mining searcher interactions could b e performed directly on the user X  X  device, only sharing the less detailed predictions with the search engine. Finally, our app roach is not nec-essarily limited to desktop-based compu ters with a mouse. Modeling interactio ns on mobile and touch-enabled devices also a llows to infer searcher interest and att entio n [20], and incorporating this richer searcher behavior data into result summary generation is another promising future re search direction.
 This work was supp orted by the Natio nal Science Found a-tio n grant II S-1018321 , the DARPA grant D11AP00269 , and by the Russian Found atio n for Basic Research Grant 12 -07 -31225 . [1] M. Ageev, Q. Guo, D. Lagun , and E. Agichtein. Find [2] C. C. Agga rwal and S . Y. Philip. A general survey of [3] S. Bird. Nltk: the natural l angu ag e too lkit. In Proc. of [4] S. Blair-Goldensohn and K. McKeown. Integrating [5] L. Breiman and L. Breiman. Bagg ing predictors. In [6] E. Brill, S. Dumais, and M. Banko. An analysis of the [7] A. Broder. A taxonomy of web search. SIGIR Forum , [8] G. Buscher, E. Cutrell, and M. R. Morris. What do [9] G. Buscher, A. Dengel, and L. van Elst. Query [10] G. Buscher, S. T. Dumais, and E. Cutrell. The goo d, [11] G. Buscher, L. van Elst, and A. Dengel. Segment-level [12] H. Dang, D. Kelly, and J. Lin. Overview of the trec [13] H. Daum  X e III and D. Marcu. Bayesian query-focused [14] S. Fisher and B. Roark. Query-focused summarization [15] J. H. Friedman. Greedy function app roximatio n: A [16] J. Goldstein, M. Kantrowitz, V. Mittal, and [17] Q. Guo a nd E. Agichtein. Exploring mouse movements [18] Q. Guo a nd E. Agichtein. Towards predicting web [19] Q. Guo a nd E. Agichtein. Beyond d well time: [20] Q. Guo, H. Jin, D. Lagun , S. Yuan, and E. Agichtein. [21] S. Harabag iu and F. Lacatusu. Using topic themes for [22] Y. Hijikata. Implicit user profiling for on d emand [23] J. Huang, R. White, and G. Buscher. User see, user [24] J. Huang, R. W. White, G. Buscher, and K. Wang. [25] J. Huang, R. W. White, and S . Dumais. No clic ks, no [26] T. Kanun go , N. Ghamrawi, K. Y. Kim, and L. Wai. [27] T. Kanun go a nd D. Orr. Predicting the readabili ty of [28] T. Kiss and J. Strunk. Unsup ervised multilingual [29] J. Kup iec, J. Pedersen, and F. Chen. A trainable [30] X. Li, Y.-Y. Wang, and A. Acero. Learning query [31] S. Liang, S. Devlin, and J. Tait. Evaluating web [32] C.-Y. Lin. ROUGE: A Packag e for Automatic [33] D. Metzler and T. Kanun go . Machine learned sentence [34] F. Radlinski, M. Szummer, and N. Craswell. Inferrin g [35] K. Rodd en, X. Fu, A. Aula, and I. Sp iro. Eye-mouse [36] H. Takamura and M. Okumura. Text summarization [37] A. Tombros and M. Sand erson. Advantages of query [38] J.-R. Wen, J.-Y. Nie, and H.-J. Zhang. Clustering user [39] R. W. White and G. Buscher. Text selectio ns as
