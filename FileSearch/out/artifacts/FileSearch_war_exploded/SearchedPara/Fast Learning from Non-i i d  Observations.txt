 In the past, most articles investigating statistical properties of learning algorithms assumed that the observed data was generated in an i.i.d. fashion. However, in many applications this assumption cannot be strictly justified since the sample points are intrinsically temporal and thus often weakly dependent. Typical examples for this phenomenon are applications where observations come from (suitably pre-processed) time series, i.e., for example, financial predictions, signal processing, sys-tem observation and diagnosis, and speech or text recognition. A set of natural and widely accepted notions for describing such weak dependencies 1 are mixing concepts such as  X  -,  X  -, and  X  -mixing, processes including Markov chains and many time series models, and b) they quantify the depen-dence in a conceptionally simple way that is accessible to various types of analysis. Because of these features, the machine learning community is currently in the process of appreciat-ing and accepting these notions as the increasing number of articles in this direction shows. Prob-ably the first work in this direction goes back to Yu [20], whose techniques for  X  -mixing processes inspired subsequent work such as [18, 10, 11], while the analysis of specific learning algorithms probably started with [9, 5, 8]. More recently, [7] established consistency of regularized boosting algorithms learning from  X  -mixing processes, while [15] established consistency of support vector machines (SVMs) learning from  X  -mixing processes, which constitute the largest class of mixing processes. For the latter, [21] established generalization bounds for empirical risk minimization (ERM) and [19, 17] analyzed least squares support vector machines (LS-SVMs).
 In this work, we establish a general oracle inequality for generic regularized learning algorithms and  X  -mixing observations by combining a Bernstein inequality for such processes [9] with localization we then use it to show learning rates for some algorithms including ERM over finite sets and LS-SVMs. In the ERM case our results match those in the i.i.d. case if one replaces the number of observations with the  X  X ffective number of observations X , while, for LS-SVMs, our rates are at least quite close to the recently obtained optimal rates [16] for i.i.d. observations. However, the latter difference is not surprising, when considering the fact that [16] used heavy machinery from empirical process theory such as Talagrand X  X  inequality and localized Rademacher averages, while our results only use a light-weight argument based on Bernstein X  X  inequality. Let X be a measurable space and Y  X  R be closed. Furthermore, let ( X  , A , X  ) be a probability that is distributed according to the first n components of Z . Throughout this work, we assume that Z is stationary , i.e., the ( X  X  Y ) n -valued random variables ( Z i (and thus all) Z i , i.e., for all measurable A  X  X  X  Y , we have To learn from stationary processes whose components are not independent, [15] suggests that it is necessary to replace the independence assumption by a notion that still guarantees certain concen-tration inequalities. We will focus on  X  -mixing, which is based on the  X  -mixing coefficients  X  ( Z , X ,n ) := sup n  X  ( A  X  B )  X   X  ( A )  X  ( B ) : i  X  1 , A  X  X  i 1 and B  X  X   X  i + n o , n  X  1 , tively. Throughout this work, we assume that the process Z is geometrically  X  -mixing , that is b, X  &gt; 0 . Moreover, several time series models such as ARMA and GARCH, which are often used to describe, e.g. financial data, satisfy (2) under natural conditions [4, Chapter 2.6.1], and the same is true for many Markov chains including some dynamical systems perturbed by dynamic noise, see e.g. [18, Chapter 3.5]. An extensive and thorough account on mixing concepts including stronger mixing notions such as  X  -and  X  -mixing is provided by [3].
 Let us now describe the learning algorithms we are interested in. To this end, we assume that we have a hypothesis set F consisting of bounded measurable functions f : X  X  R that is pre-compact with respect to the supremum norm k X k  X  , i.e., for all  X  &gt; 0 , the covering numbers space `  X  ( X ) of bounded functions f : X  X  R . Moreover, we assume that we have a regularizer , that is, a function  X  : F  X  [0 ,  X  ) . Following [13, Definition 2.22], we further say that a function L : X  X  Y  X  R  X  [0 ,  X  ) is a loss that can be clipped at some M &gt; 0 , if L is measurable and and  X  t := M if t &gt; M . Various often used loss functions can be clipped. For example, if Y := { X  1 , 1 } and L is a convex, margin-based loss represented by  X  : R  X  [0 ,  X  ) , that is L ( y,t ) = Lemma 2.23]. In particular, the hinge loss, the least squares loss for classification, and the squared hinge loss can be clipped, but the logistic loss for classification and the AdaBoost loss cannot be clipped. On the other hand, [12] established a simple technique, which is similar to inserting a small amount of noise into the labeling process, to construct a clippable modification of an arbitrary convex, margin-based loss. Moreover, if Y := [  X  M,M ] and L is a convex, distance-based loss represented by some  X  : R  X  [0 ,  X  ) , that is L ( y,t ) =  X  ( y  X  t ) for all y  X  Y and t  X  R , then L can be clipped whenever  X  (0) = 0 , see again [13, Lemma 2.23]. In particular, the least squares loss and the pinball loss used for quantile regression can be clipped, if the space of labels Y is bounded. Given a loss function L and an f : X  X  R , we often use the notation L  X  f for the function ( x,y ) 7 X  L ( x,y,f ( x )) . Moreover, the L -risk is defined by and the minimal L -risk is R  X  L,P := inf {R L,P ( f ) | f : X  X  R } . In addition, a function f  X  L,P Given a regularizer  X  : F  X  [0 ,  X  ) , a clippable loss, and an accuracy  X   X  0 , we consider learning methods that, for all n  X  1 , produce a decision function f D n ,  X   X  X  satisfying Note that methods such as SVMs (see below) that minimize the right-hand side of (4) exactly , satisfy (4), because of (3). The following theorem, which is our main result, establishes an oracle inequality for methods (4), when the training data is generated by Z .
 Theorem 2.1 Let L : X  X  Y  X  R  X  [0 ,  X  ) be a loss that can be clipped at M &gt; 0 and that satisfies L ( x,y, 0)  X  1 , L ( x,y,t )  X  B , and for all ( x,y )  X  X  X  Y and t,t 0  X  [  X  M,M ] , where B &gt; 0 is some constant. Moreover, let Z := ( Z i ) i  X  1 be an X  X  Y -valued process that satisfies (2), and P be defined by (1). Assume that there exist a Bayes decision function f  X  L,P and constants  X   X  [0 , 1] and V  X  B 2  X   X  such that where F is a hypothesis set and L  X  f denotes the function ( x,y ) 7 X  L ( x,y,f ( x )) . Finally, let  X  : F  X  [0 ,  X  ) be a regularizer, f 0  X  F be a fixed function and B 0  X  B be a constant such that learning method defined by (4) satisfies with probability  X  not less than 1  X  3 Ce  X   X  : Before we illustrate this theorem by a few examples, let us briefly discuss the variance bound (6). For example, if Y = [  X  M,M ] and L is the least squares loss, then it is well-known that (6) is satisfied for V := 16 M 2 and  X  = 1 , see e.g. [13, Example 7.3]. Moreover, under some assumptions on the distribution P , [14] established a variance bound of the form (6) for the so-called pinball if Tsybakov X  X  noise assumption holds for q , see [13, Theorem 8.24]. Finally, based on [2], [12] established a variance bound with  X  = 1 for the earlier mentioned clippable modifications of strictly convex, twice continuously differentiable margin-based loss functions.
 One might wonder, why the constant B 0 is necessary in Theorem 2.1, since appearently it only adds further complexity. However, a closer look reveals that the constant B only bounds functions of the form L  X   X  f , while B 0 bounds the function L  X  f 0 for an unclipped f 0  X  X  . Since we do not assume that all f  X  X  satisfy  X  f = f , we conclude that in general B 0 is necessary. We refer to Examples 2.4 and 2.5 for situations, where B 0 is significantly larger than B .
 Let us now consider a few examples of learning methods to which Theorem 2.1 applies. The first one is empirical risk minimization over a finite set.
 Example 2.2 Let the hypothesis set F be finite and  X ( f ) = 0 for all f  X  F . Moreover, assume that k f k  X   X  M for all f  X  F . Then, for accuracy  X  := 0 , the learning method described by (4) is ERM, and Theorem 2.1 provides, by some simple estimates, the oracle inequality R Besides constants, this oracle inequality is an exact analogue to the standard oracle inequality for ERM learning from i.i.d. processes, [13, Theorem 7.2]. C Before we present another example, let us first reformulate Theorem 2.1 for the case that the involved covering numbers have a certain polynomial behavior.
 Corollary 2.3 Consider the situation of Theorem 2.1 and additionally assume that there exist con-stants a &gt; 0 and p  X  (0 , 1] such that Then there is c p, X  &gt; 0 only depending on p and  X  such that the inequality of Theorem 2.1 reduces to For the learning rates considered in the following examples, the exact value of c p, X  is of no impor-Corollary 2.3 can be applied to various methods including e.g. SVMs with the hinge loss or the rates in the i.i.d. case and to [7] for a consistency result in the case of geometrically  X  -mixing observations. Unfortunately, a detailed exposition of the learning rates resulting from Corollary 2.3 for all these algorithms is clearly out of scope this paper, and hence we will only discuss learning rates for LS-SVMs. However, the only reason we picked LS-SVMs is that they are one of the few methods for which both rates for learning from  X  -mixing processes and optimal rates in the i.i.d. case are known. By considering LS-SVMs we can thus assess the sharpness of our results. Let us begin by briefly recalling LS-SVMs. To this end, let X be a compact metric space and k be a continuous kernel on X with reproducing kernel Hilbert space (RKHS) H . Given a regularization parameter  X  &gt; 0 and the least squares loss L ( y,t ) := ( y  X  t ) 2 , the LS-SVM finds the unique solution To describe the approximation properties of H , we further need the approximation error function Example 2.4 (Rates for least squares SVMs) Let X be a compact metric space, Y = [  X  1 , 1] , and Z and P as above. Furthermore, let L be the least squares loss and H be the RKHS of a continuous kernel k over X . Assume that the closed unit ball B H of H satisfies where a &gt; 0 and p  X  (0 , 1] are some constants. In addition, assume that the approximation error function satisfies A (  X  )  X  c X   X  for some c &gt; 0 ,  X   X  (0 , 1] , and all  X  &gt; 0 . We define Then Corollary 2.3 applied to F :=  X   X  1 / 2 B H shows that the LS-SVM using  X  n := n  X   X  X / X  learns of  X  . However, a closer look shows that it depends on the confidence level 1  X  3 Ce  X   X  by a factor of e rather than by the factor of  X  appearing in our analysis, and hence these rates are not comparable. for sufficiently smooth kernels, see e.g. [13, Theorem 6.26]. Moreover, [19] has recently established the rate as the Gaussian RBF kernels, where p can be chosen arbitrarily close to 0 , their rate is never faster. Moreover, [19] requires knowing  X  , which, as we will briefly discuss in Remark 2.6, is not the case for our rates. In this regard, it is interesting to note that their iterative proof procedure, see [13, Chapter 7.1] for a generic description of this technique, can also be applied to our oracle inequality. Finally, both [19] and [17] only consider LS-SVMs, while Theorem 2.1 applies to various learning methods. C Example 2.5 (Almost optimal rates for least squares SVMs) Consider the situation of Example 2.4, and additionally assume that there exists a constant C p &gt; 0 such that with rate compared to the optimal rate n  X   X   X  + p in the i.i.d. case, see [16]. In particular, if H = W m ( X ) is a Sobolev space over X  X  R d with smoothness m &gt; d/ 2 , and the marginal distribution P X is absolutely continuous with respect to the uniform distribution, where corresponding density is bounded away from 0 and  X  , then (7) and (9) are satisfied for p := d 2 m . Moreover, the assumption on the approximation error function is satisfied for  X  := s/m , whenever f  X  L,P  X  W s ( X ) and s  X  ( d/ 2 ,m ] . Consequently, the resulting learning rate is that this difference can be made arbitrarily small by picking a sufficiently large m . Unfortunately, we do not know, whether the extra term 2 ds/m is an artifact of our proof techniques, which are relatively light-weighted compared to the heavy machinery used in the i.i.d. case. Similarly, we do not know, whether the used Bernstein inequality for  X  -mixing processes, see Theorem 3.1, is better version of this inequality, our oracle inequalities can be easily improved, since our techniques only require a generic form of Bernstein X  X  inequality. C Remark 2.6 In the examples above, the rates were achieved by picking particular regularization sequences that depend on both  X  and  X  , which in turn, are almost never known in practice. Fortu-nately, there exists an easy way to achieve the above rates without such knowledge. Indeed, let us assume we pick a polynomially growing n  X  1 /p -net  X  n of (0 , 1] , split the training sample D n into two (almost) equally sized and consecutive parts D (1) n and D (2) n , compute f D (1) and pick a  X   X   X   X  n whose f D (1) 2.2 with the oracle inequality of Corollary 2.3 for LS-SVMs shows that the learning rates of the Examples 2.4 and 2.5 are also achieved by this training-validation approach. Although the proof is a straightforward modification of [13, Theorem 7.24], it is out of the page limit of this paper. C smallest integer n satisfying n  X  t .
 The key result we need to prove the oracle inequality of Theorem 2.1 is the following Bernstein type inequality for geometrically  X  -mixing processes, which was established in [9, Theorem 4.3]: Theorem 3.1 Let Z := ( Z i ) i  X  1 be an X  X  Y -valued stochastic process that satisfies (2) and P be defined by (1). Furthermore, let h : X  X  Y  X  R be a bounded measurable function for which there exist constants B &gt; 0 and  X   X  0 such that E P h = 0 , E P h 2  X   X  2 , and k h k  X   X  B . For n  X  1 we define Then, for all n  X  1 and all  X  &gt; 0 , we have Before we prove Theorem 2.1, we need to slightly modify (10). To this end, we first observe that satisfying n  X  n 0 := max { b/ 8 , 2 2+5 / X  b  X  1 / X  } , we have In addition, we will need the following simple and well-known lemma: Proof of Theorem 2.1: For f : X  X  R we define h f := L  X  f  X  L  X  f  X  L,P . By the definition of f Now L  X  f 0  X  L  X   X  f 0  X  0 implies h f 0  X  h  X  f Inequality (11) applied to h := ( h f 0  X  h  X  f holds with probability  X  not less than 1  X  Ce  X   X  . Moreover, using and consequently we have with probability  X  not less than 1  X  Ce  X   X  that In order to bound the remaining term in (13), that is E D n h  X  f implies k h  X  f and b := (2  X   X  1 E P h  X  f Since E P h  X  f with probability  X  not less than 1  X  Ce  X   X  . By combining this estimate with (14) and (13), we now obtain that with probability  X  not less than 1  X  2 Ce  X   X  we have i.e., we have established a bound on the second term in (12).
 us first consider the case n  X  &lt; 3 c B (  X  + ln |C| ) . Combining (16) with (12) and using B  X  B 0 , B  X   X ( f 0 ) + 2 E P h f  X   X ( f 0 ) + 2 E P h f  X  3 X ( f 0 ) + 3 E P h f To establish a non-trivial bound on the term E P h  X  f  X  = 0 . Now, (11) together with a simple union bound yields and consequently we see that, with probability  X  not less than 1  X  C |C| e  X   X  , we have assumed Lipschitz continuity of L the latter implies for all ( x,y )  X  X  X  Y . Combining this with (18), we obtain with probability  X  not less than 1  X  C e  X   X  . By combining this estimate with (12) and (16), we then obtain that holds with probability  X  not less than 1  X  3 Ce  X   X  . Consequently, it remains to bound the various terms. To this end, we first observe that for In addition, V  X  B 2  X   X  , c  X   X  3 c B , 6  X  36 1 / (2  X   X  ) , and n  X   X  3 c B (  X  + ln |C| ) imply Using these estimates together with 1 / 6 + 1 / 9  X  1 / 3 in (19), we see that holds with probability  X  not less than 1  X  3 Ce  X   X  . Consequently, we have i.e. we have shown the assertion.
 Proof of Corollary 2.3: The result follows from minimizing the right-hand side of the oracle in-equality of Theorem 2.1 with respect to  X  .
 [1] P. L. Bartlett, O. Bousquet, and S. Mendelson. Local Rademacher complexities. Ann. Statist. , [2] G. Blanchard, G. Lugosi, and N. Vayatis. On the rate of convergence of regularized boosting [3] R. C. Bradley. Introduction to Strong Mixing Conditions. Vol. 1-3 . Kendrick Press, Heber City, [4] J. Fan and Q. Yao. Nonlinear Time Series . Springer, New York, 2003. [5] A. Irle. On consistency in nonparametric estimation under mixing conditions. J. Multivariate [6] W. S. Lee, P. L. Bartlett, and R. C. Williamson. The importance of convexity in learning with [7] A. Lozano, S. Kulkarni, and R. Schapire. Convergence and consistency of regularized boosting [8] R. Meir. Nonparametric time series prediction through adaptive model selection. Mach. Learn. , [9] D. S. Modha and E. Masry. Minimum complexity regression estimation with weakly dependent [10] M. Mohri and A. Rostamizadeh. Stability bounds for non-i.i.d. processes. In J.C. Platt, [11] M. Mohri and A. Rostamizadeh. Rademacher complexity bounds for non-i.i.d. processes. In [12] I. Steinwart. Two oracle inequalities for regularized boosting classiers. Statistics and Its [13] I. Steinwart and A. Christmann. Support Vector Machines . Springer, New York, 2008. [14] I. Steinwart and A. Christmann. Estimating conditional quantiles with the help of the pinball [15] I. Steinwart, D. Hush, and C. Scovel. Learning from dependent observations. J. Multivariate [16] I. Steinwart, D. Hush, and C. Scovel. Optimal rates for regularized least squares regression. In [17] H. Sun and Q. Wu. Regularized least square regression with dependent samples. Adv. Comput. [18] M. Vidyasagar. A Theory of Learning and Generalization: With Applications to Neural Net-[19] Y.-L. Xu and D.-R. Chen. Learning rates of regularized regression for exponentially strongly [20] B. Yu. Rates of convergence for empirical processes of stationary mixing sequences. Ann. [21] B. Zou and L. Li. The performance bounds of learning machines based on exponentially
