 Online hotel searching is a daunting task due to the wealth of online information. Reviews written by other travelers replace the word-of-mouth, yet turn the search into a time consuming task. Users do not rate enough hotels to enable a collaborative filtering based rec-ommendation. Thus, a cold start recommender system is needed.
In this work we design a cold start hotel recommender system, which uses the text of the reviews as its main data. We define con-text groups based on reviews extracted from TripAdvisor.com and Venere.com. We introduce a novel weighted algorithm for text min-ing. Our algorithm imitates a user that favors reviews written with the same trip intent and from people of similar background (na-tionality) and with similar preferences for hotel aspects, which are our defined context groups. Our approach combines numerous ele-ments, including unsupervised clustering to build a vocabulary for hotel aspects, semantic analysis to understand sentiment towards hotel features, and the profiling of intent and nationality groups.
We implemented our system which was used by the public to conduct 150 trip planning experiments. We compare our solution to the top suggestions of the mentioned web services and show that users were, on average, 20% more satisfied with our hotel recom-mendations. We outperform these web services even more in cities where hotel prices are high.
 Categories and Subject Descriptors: H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval -Information filtering General Terms: Algorithms.
 Keywords: Recommender systems, opinion/text mining, context-aware recommender systems, common traits, sentiment analysis.
The Internet has overtaken word of mouth as the primary medium for choosing destinations [1]; 63% of consumers plan travel by  X 
Part of this work was done while Asher Levi was visiting Techni-color lab in Palo Alto.  X 
Corresponding author searching the Internet, visiting an average of 22 sites before de-ciding on a destination.

Producing recommendations for travel is inherently difficult, as an individual rarely rates more than a small number of hotels and thus rich profiles cannot be built. Having limited or no information about the user translates to a user cold start recommendation [2, 3, 4]. An intelligent cold start recommender will minimize a new user X  X  effort while still learning enough to recommend the user a product that is likely to be of her interest.

In this paper we design a context-based search recommender sys-tem. We show that context-based search can be facilitated for the construction of cold start recommender systems. We further show that contextual information can be mined from review texts, and analyzed for common traits 1 per context group. Contextual infor-mation has been newly recognized as an important feature when the consumer decides to make a purchase [5, 6, 7]. A lot of re-search has already been performed in the area of recommender systems and information retrieval. However, most recommender systems focus on recommending the most relevant items to users without taking into account any additional contextual information. Most existing information retrieval systems base their retrieval de-cisions solely on queries collections, whereas information about search context is often ignored [8].

Users X  search patterns are context-based.Among the plethora of reviews, readers opt for recommendations from travelers with com-parable needs. A single traveler may share the same needs as other single travelers. A user traveling with her family has different needs from a user traveling on a business trip, i.e. the user context infor-mation is an important factor in choosing a hotel. When a user reads reviews she can metaphorically be seen as wearing person-alized glasses. Reviews are read through those glasses, and par-ticular words or comments will resonate, positively or negatively, with the reader based upon her needs for her upcoming trip and her personal preferences. Special attention is often given to reviews written with the same intent, or by reviewers from a comparable background. Hence, we define three types of context information. The first is intent , or purpose of the trip. We include 5 categories of intent, namely business trip, single traveler on vacation, family, group, couple . The second is nationality . The third context is user preferences for the different hotel aspects. These were mined from the text using an unsupervised clustering algorithm. We tagged the different clusters found in the text as location, service, food, room, price-value quality and the facilities (pool, spa, etc). Thus, a user using our system is asked to provide her trip intent, nationality, and preferences for these aspects.
We obtained data from Venere.com and TripAdvisor.com. The database contains details for each hotel: the hotel X  X  general infor-mation, reviews and ratings. In a pre-processing phase, we mined the text and found common traits for each context group. These are found in the form of typical words that appear more in text written within that context but are not common for other contexts. A clus-tering was used to group words that refer to each aspect. Thus, at the end of this pre-processing phase, we have significant words per context, be it intent, nationality, or hotel aspect.

Our recommender system mines the text of the reviews similarly to the user wearing personalized glasses. The user is prompted for her trip intent, nationality and preferences per hotel aspect. We introduce a novel weighted algorithm for context-based text min-ing. The core idea of the algorithm is to give more importance to reviews of people with the same contexts as the user X  X . Common traits per the user X  X  context groups and words that describe favor-able hotel aspects are given a higher score than other words. We further find the sentiment expressed in the review per context (i.e., positive or negative) and give a corresponding score. Thus, the final score for each review corresponds to that of a user X  X  with compara-ble needs and preferences and coming from a similar background.
We implemented our system and published it for use, presenting to the users results from our system combined with the top sugges-tions of the above mentioned web sites. We had over 150 evalua-tions by friends and colleagues who looked for hotels in four major European cities. Hotels recommended by our system were favored (60.2%) compared to TripAdvisor X  X  and Venere X  X  top suggestions (50.8%). More significant is the fact that our raters said they would not stay in 26.4% of the top hotels recommended by these sites, whereas with our context-based recommender, their dissatisfaction was much lower at 15.9%.

The contributions of our paper are the following: We designed a hotel recommender system that outperforms current leading web sites top suggestions; We define context-based search as a method to overcome the cold start problem for users; Our system is the first we know of that relies mainly on the text of reviews for a cold start recommendation. Hotel ratings and groups X  bias are used as tie breakers; We devise a weighted text mining algorithm that lever-ages common traits found per context group to enable the process-ing and evaluation of text per users X  needs; To find hotel aspects we use a community detection algorithm that leverages the spin glass theory, and changed its distance function to account for the extra clustering overlapping exhibited in the text. This enabled us to find the different hotel aspects in an unsupervised fashion; We held ex-periments with Mechanical Turk workers and showed that reviews are perceived differently than ratings given by the reviewers, sug-gesting that sentiment analysis of the text cannot rely on the ratings, although commonly used.
One of the common and difficult problems for recommender sys-tem is the cold-start problem, a situation in which the system needs to recommend a product to a new user that has no past information or a new item with very few or no rating [2, 4, 3]. We build a model for domains that by nature don X  X  have a lot of history (or not at all) information about the user, and user cold start recommender system is required.

Despite the abundance of studies targeted at solving the new item problem [4, 9], there has been little work in solving the new user problem. The dominant approach is using a learning phase, in which a user is asked to provide a set of ratings for selected items, in a way that gathers as much information about the user as possible [10]. Another approach presented in [11] exploits the significance of users X  implicit feedback for alleviating the new user problem. In this approach the user has to express interest in items, or organize the items in relative order, without providing explicit ratings for those items. Those approaches use only ratings or rel-ative rankings on items and thus are bounded under a rating rec-ommender system limitations. Moreover, they are missing all the information that can be extracted from the text.

Another approach is the "Metadata" approach; here the metadata of an item is used to create content-based recommender systems. This method relies on systems where the user needs to provide some demographic data. The solution presented in [12] is utilizing the strength of the vector aspect model with user information; they used the demographic information of the user (age, gender and job) as the user X  X  features. A model of relationships between a user X  X  demographic information and an item X  X  metadata was presented by Park et al. [13]. Those solutions use only ratings at their model; They use the user X  X  context information as a feature for building their recommendation; We extend the usage of contextual informa-tion. We are not considering only the user general information (e.g, age or gender) and simply profile the user, but rather we are trying to build a more complete behavioral profile that attempts to capture the expectations of the user from our recommendation;
By using a context information that relevant to the current ses-sion of search (e.g, for hotel recommendation we use the intent of the trip as one of the context features), we are capturing a more accurate and efficient profile.
A hotel recommender system typically won X  X  have sufficient his-torical information to build profiles for individuals. It does, how-ever, have additional data in the form of reviews that is sufficient to enable the characterization of context groups. We give here an overview of our system, which determines common traits for groups that share the same context. The core idea of our system is to give more importance to reviews of people with the same con-text. Our system brings greater importance to the topics those re-viewers focus on frequently and also focuses on topics that are as-sociated with the user X  X  stated preferences.

In the hotel arena people can be categorized by their trip intent (such as those who travel as a  X  X ouple X , or a  X  X amily X , etc.) and nationality, which we refer to as context groups. Using the text reviews from multiple people within a single context group, we can essentially find the common traits of groups such as  X  X amily X  travelers (and so on for the other categories). We additionally process the corpus of reviews to identify the vocabulary that is used to describe a particular aspect of a hotel. Once a person using our system specifies her intent, nationality and preferences our system evaluates reviews with accordance with the traits and preferences, and gives a recommendation.

We now give a brief overview of the components and steps of our method, depicted in Figure 1. The top 3 boxes on the left corre-spond to the pre-processing phase in which we define the common traits of intent and nationality groups, and define the different ho-tel aspects referenced in reviews, correspondingly. To find common traits for each context group, we extract the nouns and noun phrases (called features ) from all reviews and find those that are more com-mon for that group. These features are then assigned a weight per each context according to their relative frequency in reviews within that context. The higher the weight, the more important a feature. The common traits of context groups are the higher weight features for that group. Hence, the common traits of Italians consist of a set of features and their weights, while these of Germans may contain largely the same features but with different weights. Common traits of hotel aspects are constructed differently. Here we carried out a clustering task to cluster features based upon co-occurrence in the same sentence. Each feature can only occur in one cluster, and thus each cluster contains the most relevant vocabulary for that aspect. The fourth component of the preprocessing consists of building an opinion lexicon which will allow us to analyze adjectives associ-ated with features, and to give each feature an orientation score depending upon how positive or negative is the sentiment of any associated adjectives.

While the base weight of each feature is one in our system, fea-tures that are distinctive of several context groups may have differ-ent weight per group. The specific set of weights used in response to a user hotel search will be chosen once the user declares her context and preferences. For example, if a user specifies  X  X usiness traveler X  as her intent and her nationality, then the set of feature weights used will be those in the  X  X usiness traveler X  group and the corresponding national group. In our figure, this step corresponds to the "select relevant feature weight for intent" and ".. for nation-ality" boxes. Similarly, corresponding weights are given to features of important aspects. This implies, for example, that the feature  X  X ir conditioning X  will get one weight depending upon its importance for business travelers, a second weight depending upon its impor-tance for the given nationality, and a third weight depending upon its importance per the user preference for the aspect it belongs to. The final weight for each feature is done by combining these three weights (depicted as "build feature score" in the figure).
Next we use our opinion lexicon to give each feature an orien-tation score. We subsequently combine the features, their weights and orientations to build a score for each sentence. The sentence scores are then combined to give an overall score for each review. This score should reflect the relative importance of the given re-view for the user. Reviews that are both important and positive are deemed most relevant thereby receiving the highest scores. The final score for each hotel is an average of all of its reviews, each of which is scored from the user X  X  perspective (i.e., based on her context and preferences), and an adjustment bias calculated per the context given.

The main idea of our algorithm for context based text analysis is to assign weights to common traits per context. Thus, at the end of the process, each review is mapped to a score number, based on system X  X  perception of the user X  X  perspective.
We find common traits for each context group by mining the text from reviews on a sentence level. Our approach is to extract key features (i.e., words) that are important for each group. It has been shown that a reviewer X  X  vocabulary when commenting on an item was found to converge, in the sense that the most frequently used nouns and noun phrases used correspond to genuine and impor-tant features [14]. Similar to [15] we extract features and remove redundant and meaningless items from the candidate features we found.

The basic building block of our algorithm is the trait based weight assigning. For each review written we extract the features and as-sign each feature a weight that reflects its importance for each con-text group. Let c denote a general context that can be either an intent (or purpose) p or a nationality n (i.e., c 2 { p } let freq frequency of a feature per context is the relative number of occur-rences of feature f in sentences appearing in reviews that belong to context c . For example, the frequency of the feature  X  X iFi X  for Americans is calculated as the ratio of the number of times this feature appeared in sentences written by Americans, divided by the total number of sentences written by Americans. Similarly, avg the average frequency of feature f , stdv f is its standard deviation, and dev f = avg of a feature f for a given context as follows:
The majority of features will either be assigned a 1; however those whose frequency is larger than average plus or minus one standard deviation, are assigned values between 1 and 3 or 0.1 and 1 respectively. Hence each feature is assigned a weight in the range [0 . 1 , 3] per context.
Recall that we ask the user to input their preferences on six as-pects. These aspects were not selected at random, but were instead the result of a word clustering analysis we performed on the text. Often in reviews, different words may be used to refer to the same general aspect of a hotel. For example, words like  X  X rea X ,  X  X treet X , and  X  X etro X  may all refer to aspects of a hotel X  X  location. There are many approaches to clustering, hierarchical clustering, partition clustering (e.g k-means) etc. The number of clusters, k , is usually either an input parameter or found by the clustering procedure it-self [16]. In our case, clustering would yield the different hotel aspects and therefore should not be supervised but determined by the clustering algorithm over the text itself.

To account for the sparsity and the overlapping characteristics in the network of word features, we build upon an unsupervised a network graph in which each node corresponds to a feature and each community will correspond to an hotel aspect. Trying to find the maximal modularity is defined as finding a partition that will minimize the energy of the features network graph. The Hamilto-nian, denoted in equation 2 is defined in the following way: exist-ing internal edges and non-existing external links (between formed communities) minimize the Hamiltonian, while existing external links and non-existing internal links increase its value. The algo-rithm tries to find a partition that minimizes the Hamiltonian, based on the spin glass model for finding a partition that minimizes the energy of the spin glass with the spin states being the community indices.
 H ( { } )= Where A ij is a boolean adjacency matrix, i 2 1 , 2 ,...q the indices of the communities, with q the number of maximal com-munities. [19] showed that the division does not depend on large initial q values.

In [19] a ij and b ij where chosen as a function of the probability of two graph nodes to be adjacent under the assumption that when this probability is high the nodes are more likely to belong to same group, or community. In our case, this translates to the probabil-ity of two features to appear in a sentence together. However, we found in reviews that very frequent features are often found in sen-tences together. For example, it is common to find sentences of the following structure: The location was great and the room was very clean.
 Clearly, location and room belong to different hotel aspects, and should therefore belong to different communities. To account for this tendency we instead use the PMI-Pointwise mutual informa-tion weight, which measures the information overlapping between two random variables [20], described in (3).
 Where, p ( i ) is the probability that the feature i appears in a sen-tence. Then, a ij =  X  PMI ij , where is a parameter expressing the relative contribution to the energy from existing and missing edges. In our case we chose =1 .

Over the corpus of reviews, our PMI-pointwise improvement of the spinning glass community detection algorithm produced six clusters of different sizes (note that the number of clusters is un-supervised). The identification of these 6 clusters is important as it determined the particular hotel aspects that we chose to ask users their preferences for. Each cluster and the set of features it contains can be thought of, intuitively, as the common traits for the aspect associated with this cluster. These clusters are useful as follows. Suppose for example that a user specifies that location is of utmost importance to her. The room cluster identifies a large number of features (or words) that are often used to discuss things inside a hotel room; thus reviews in which these words occur frequently are more important to a user who cares about the room than one who cares about food. After studying the words that ended up in each cluster, we selected the cluster names as indicated in Table 4.2. These clusters can be computed ahead of time as part of the sys-tem X  X  preprocessing.

Our weight assigning algorithm for aspect related features relates to the user X  X  preference and is calculated online as follows: Let u pref ( k ) denote user u  X  X  preference for aspect (i.e. cluster) feature f is in cluster k , then we calculate the weight for the feature according to the users preferences as follows: where, W f ing to her preference u pref ( k ) . For example, if the user sets their preference for location to 5, and the feature is train , then the weight of train for this user is 2. Another user that specifies that location is of importance 1, would have the feature train assigned a weight of 1.2. When determining the weight for the feature train we only use the user X  X  preference for location (and not for room or food) because the feature  X  X rain X  is in the location cluster and cannot be in any other cluster.
Next we determine the polarity of the opinion expressed in the review on each feature, whether positive or negative, to assign a corresponding sign to a feature X  X  weight. To infer the opinion po-larity per feature we use an opinion lexicon. An opinion lexicon is a dictionary of words and word phrases that express positive or negative sentiments. In this work, we consider sentiment words to be adjectives the reviewers use to express opinions on product features, as in [21, 22] . To collect the opinion word list we use a corpus-based approach similar to the approach described in [14, 23]. We extract all the adjectives that appear in the same sentence for each feature.

We then find the semantic orientation of the extracted opinion words. When the reviewer uses a word that expresses a desirable state, then the word is classified as having a positive semantic ori-entation. Similarly, an undesirable state translates to a negative semantic orientation. We use a bootstrapping lexicon-based ap-proach as in [14]. Manually, we create a set of seed adjectives from the opinion lexicon list with semantic orientation. Then for each adjective in the seed list, we search for a synonym and an antonym in WordNet [24]. Each found adjective in the opinion lexicon is assigned an orientation, and is added to the seed list. The seed list grows in the process. A recent work [25] suggests to consider the influence of aspects on sentiment polarity. However, given that we give the weight per feature and that the aspect counts only for a fraction of the total weight we left the orientation per feature as before.

We used common opinion rules as described in [23]. One is the negation rule, words or phrases like  X  X o X ,  X  X ot X , etc. take the op-posite orientation expressed by the opinion phrase. The other is the But clause rules, a sentence containing  X  X ut X  also needs spe-cial treatment. The opinions before and after a  X  X ut X  are usually opposite of each other. First we try to determine the semantic ori-entation of the feature in  X  X ut X  clause. If we cannot get the orien-tation of the phrase we take the opposite orientation of the clause before the  X  X ut X  clause. Phrases such as  X  X ith the exception of X ,  X  X x-cept for X  etc. behave similarly to  X  X ut X  and are handled in the same way. For example, in the sentence "The room was clean except for the bathroom", the opinion about the feature room is positive and the feature bathroom gets the inverse opinion which is negative. There are also some phrases that contain negation and but words, yet do not change the orientation of the opinion. For example in the phrase "I do not only like the size of the room, but also its style", the  X  X ot X ,  X  X ut X  words do not change the orientation of the opinion words  X  X ike X  and  X  X tyle X .

Using these rules and our lexicon, we assign an orientation score to each feature f in a given sentence s , denoted score ( f, s ) should be clear that the same feature, in two different sentences, could receive different orientations. When many opinion words surround a single feature, they are aggregated as indicated in equa-tion (5). Here op is an opinion word in sentence s , d ( op, f ) is the distance (word count) between feature f and opinion word op in sentence s . Also, or op is the orientation ( 1 , +1) of the opinion word Dividing by the distance between the feature and the opinion word is used to give lower weights to opinion words that are farther away from f . When the final score is positive, then the overall opinion of feature f in s is positive, and similarly the reviewer X  X  opinion of the feature is negative when the final feature score is negative.
We now have a set of weights and their orientation per the user X  X  context for each feature in a review. We combine these elements to produce a single score for a review as follows. Given the user X  X  input on their context, each feature has 3 weights, one for intent, W u p , one for nationality W f u n , and one based on aspect preferences W u pref . The final weight W f u assigned to feature f for user multiplication of these three weights, namely: The weights for each context are multiplied because that allows fine grained differentiation of people within our various groups (such as intent and nationality). Consider a Japanese person who uses our system. Based upon our nationality profiling, we see that the feature  X  X ath X  is important. If that person also marks  X  X oom X  as a hotel aspect that is very important to them (i.e. a preference of 5), then the quality of the bathroom is more important for this user than for a second Japanese person who marks  X  X oom X  as low priority and  X  X ood X  as high priority. This allows us to differentiate within nationalities by using the intent and preferences (or to differentiate within an intent group by their nationality and preferences).
To produce a score for each sentence, we multiply each feature by its orientation score and sum up the weight scores of all features in a sentence s , namely P up the scores of all the sentences in a review to produce a score for a review v , as follows:
Where score ( v, u ) is the score of review v for user u . The review score captures how important a particular review is for the user based upon their context and preferences.
Next we produce a score for each hotel so that hotels can be ranked and presented to the user in order from highest score to lowest. The major factor in the score of a hotel in our system is the score calculated for reviews based on user context groups and preferences. We term this the hotel orientation score, ho ho u = avg v 2 R ( h ) [ score ( v, u )] and R ( h ) denotes the set of reviews for hotel h . The second argument is a bias adjustment, denoted b hsn , which captures the bias of a user with intent p and nationality n , as well as any hotel bias h . (The bias term is explained below.) Thus our final hotel score is given by:
Bias Adjustment to Hotel Score: In our hotel score, the ori-entation score coming from the text analysis of the reviews is the dominant component of the score, as these values will range from -40 to 80 approximately. Our bias terms range from 0 to 5 and are included primarily to break ties, or to differentiate hotels when their scores are very close. The process of using the star ratings needs to be adjusted for bias because there are systematic tendencies for some traveler groups to rate higher than others. For example, our data analysis shows that reviewers from Spain tend to rate lower in star rating systems than reviewers from the USA.

We compute the bias b hpn for hotel h from traveler with both intent p and nationality n , as follows. Let  X  denote the overall av-erage star rating of all hotels in the system. The parameter ifies the observed deviations of hotel h from the overall average. We use b hp to denote the observed deviations that travelers with intent p have for hotel h , (and similarly for b hn ). These deviations are with respect to the average score of hotel h .
The average deviations are shrunk towards zero by using the nor-malization parameters, 1 , 2 , 3 , which are determined by valida-tion on the test set. For each hotel h we set: where R ( h ) is a set of reviews for hotel h , and 1 =30 of intent group p for hotel h is: where R ( hp ) is a set of reviews for p and h , and where The bias of a nationality n for hotel h is given by: where, R ( hn ) is the set of reviews from nationality n for 3 =5 .
The dataset used in this study was extracted from two well-known travel search engines, namely, Tripadvisor.com and Venere.com. For each hotel the data contains general information about the hotel (e.g, name, address, average rating, stars, price etc.) and a list of re-views written by hotel guests. The reviews include: travel intent of the reviewer, nationality, rating, review text, and additional meta-data. The data was collected for 4 cities in Europe: Munich and Berlin in Germany, and Milan and Rome in Italy. The data includes reviews that were written before January 2011 . 84 , 968 reviews were collected for 1 , 930 hotels from TripAdvisor, and 52 , 266 views for 1 , 845 hotels from Venere. For each intent group we ob-tained thousands of reviews, from 6 , 541 reviews written by people on business trips to 60 , 113 reviews written by couples. Overall we collected information for 3 , 775 hotels corresponding to reviews.

Recent researchers have turned to text analysis as there is poten-tially a great deal more information that could be extracted from reviews then star ratings[26, 27, 28]. In these solutions, the star rating is taken as the overall product rating, and additional informa-tion is obtained by analyzing the review text to extract opinions on specific aspects of each item, and thus improve a personalized rec-ommendation. In [29] the authors incorporate a notion of personal scale, that is based on the observation that different users give dif-ferent values to their describing words to improve sentiment anal-ysis for personal recommendations. Our work differs from these because they use text reviews to build individual profiles and per-sonal scales, thereby not focusing on the cold start problem. In the absence of user profiles, we choose instead to use the text to find the common traits of intent groups and nationalities, and further differentiate hotels by personal preferences.

The underlying assumption of [29] and many others, is that a user X  X  ratings and text correlate. This assumption has not been pre-viously quantified. We further wanted to quantify whether the per-ception of the reviews differs across context groups.
 To this end we designed the following online experiment with Mechanical Turk workers. Each worker was given the text of five different hotel reviews. We used reviews from 50 different hotels X  taken from venere.com, where ratings are on a scale of [1 10] We then asked the workers, based on the text, to estimate the rate that the reviewer gave. To get meaningful results, we filtered out manually cases in which it was clear the workers did not read the text before estimating the rating 3 . We obtained 50 75 estimates of the star rating for each review, yielding a total of 3715 estimates.
First, we checked whether the workers estimated the ratings were indeed similar to those given by the person writing the text. For each estimate, we computed the difference between the estimate rating and the actual rating, depicted in Table 5 . We averaged all these differences in cases when the estimates where higher than the actual, and computed a second average across all cases when the estimates were lower than the actual ratings. There is a clear skew between the estimated rate and the real rate. The skew is more significant when reviews were perceived as negative by the workers ( 1 . 67 more negative vs. 0.94 more positive). Two conclu-sions are thus possible. The first claims that users give an accurate star ratings while signifying negative aspects when writing down reviews. The other suggests that users are generous with the star ratings while expressing their real opinion in writing. In either case, these result indicate that star ratings do not consistently capture the sentiment in text reviews, and thus the correlation between text and reviews is weak.
 To validate the assumption that context matters, we return to our Mechanical Turk experiment and ask whether the perception of the reviews differs across intent groups. For example, we saw that sin-gle travelers tend to rank almost the same as others but their text was perceived as much worse by the Mechanical Turk workers. worker did not seem to read the reviews at all. We had one dispute that was ruled in our favor.
 The average difference between the perceived rate and the real rate was 1 . 93 on the average for single travelers. For people traveling in groups, the average difference between perceived and real rate was 1 . 54 . This means that the text reviews of single travelers gives the perception to others of being far more negative than their cor-responding rates; whereas for group travelers, the text reviews are Table 2: Mechanical Turk results, estimating the review X  X  rate.
To validate our algorithmic approach that takes into account con-text we asked ourselves whether different context groups rank ho-tels differently, whether the intent groups emphasize different things, and whether the tone of the reviews differs across context groups. In our case the different context groups are the different intent groups and different nationalities. The analysis was performed on reviews taken from TripAdvisor, where the hotel ratings are in the range of [1 .. 5] . Table 3 shows the average rating for each context group, whether an intent group or nationality. Indeed we see differ-ences across the groups, with the difference between intent groups varying to up to 0 . 55 (on a five star rating system). Next we com-puted frequent words used by each of the groups over both datasets, TripAdvisor and Venere. We removed words that are used by all groups. This left us with examples of words that were frequent to one group but not others. Table 3 also shows examples of words that are frequently used by one group, but infrequently or never, appear in the text of other groups. This indicates that the intent of a trip influences the content of reviews that get written, and that the top words (i.e. topics) that interest reviewers also differ by country or culture.
The effect of considering the intent of a trip is clear both intu-itively and from our results. We wanted to further verify the use-fulness of using the nationality, as well as the bias factor used for the final hotel score. The following evaluations were done on our implemented system. mate reviews X  ratings when knowing the intent of the trip. The additional information did not affect the estimations, indicating the above results could not be predicted.
 Context Rate Typical words Family 4.15 Air condition, Car, Space, Shuttle, Breakfast Couple 4.08 Coffee, View, Balcony, Breakfast Group 4.02 Bar, Money, Bus stop, Shopping, Party Single 3.8 WiFi, TV, Price, Supermarket Business 3.6 Internet, Buffet, Park, Bar, Shopping, TV U.S.A 4.11 Hotel staff, Train station, Lobby, Shuttle Russia 4.07 Furniture, Style, Bus stop, Air conditioner Australia 3.97 Food, wifi, Supermarket, Area, Pillow Netherlands 3.94 Toilet, Hotel front, Coffee, Hotel breakfast Japan 3.86 Bath, Bed, Room shower, Sightseeing Table 3: Average ratings given by each intent group/nationality and corresponding distinguishing words We ran the following experiments with our implemented system. We issued numerous pairs of queries, one with nationality specified and one without. (Similarly for bias.) For each query in the pair, we recorded the top 10 (or top 20) hotels recommended and then compared the two lists. Let S 1 denote the list of top-10 hotels for the query without nationality, and S 2 denote the top-10 list for the same query with a nationality specified. We quantify the difference between these two lists using the Jacard distance:
Nationality The queries in the experiment are constructed as follows: for each city (4 options), and for each user X  X  travel intent (5 options), we randomly select five different user preference values for each aspect. We calculate the distance Diff ( S 1 ,S 2 pair in the formed lists, and compute the average distance across all pairs of queries. This captures the average influence on the search results of including nationality. The total number of queries exe-cuted was 2500. We found that the nationality parameter affects 16 . 6% of the search results, thus, we believe that this piece of con-text is important to include in our method. We executed the same experiment with 20 hotels in the results sets, and found similar re-sults; in this case we observed that the nationality context affected 15% of hotels recommended.

Bias adjustment. Similarly, we may wonder to what extent the bias adjustment plays a role in affecting the order of hotels pre-sented to a user. Hence, we ran similar experiments. For each city, for each user travel intent, and for each nationality, we randomly se-lect five different user preference values for each aspect. We com-pare the two obtained sets using our Diff ( S 1 .S 2 ) metric. We ran 2500 such experiments. Since the bias parameter is intended to be used as a sort of tiebreaker, to differentiate very closely ranked ho-tels, we don X  X  expect it to have a large impact; however if it plays no role then it could be eliminated from our method. We found that the bias affected 9% of the search results. Executing the same ex-periment, but recording the top-20 recommended hotels after each query, we found similar results -the bias influenced 8% of the rec-ommendations. We believe this is a sufficiently influence to warrant retaining the bias parameter in our solution.
Evaluation of a recommender system has to measure whether real people are willing to act based on the recommendations. User satisfaction with a recommender system results is well gauged with an on-line evaluation methodology. We use such a methodology as described in [30]; this methodology doesn X  X  measure absolute user satisfaction but only relative user satisfaction with one system over another.

We implemented our system and made it available on the public web for use. We asked numerous friends and colleagues to eval-uate our system and obtained 150 evaluations. Each experiment consisted of the following. The user inserts her search parameters: intent, nationality, aspect preferences, and a price range. Then we present the user a list of six hotels. Some are from our system, and some are the highest star ratings choices from Venere and Tripadvi-sor. In order to avoid biasing the user, these six hotels are presented in random order and thus the user is unaware of the source of the recommendations. Raters were shown links to the full text reviews to further explore the recommended hotels. For each one of the hotels, raters were asked to express their satisfaction by answering the question "Would you select this hotel?" with three optional an-swers: Yes, Maybe, No. In addition, the raters were asked to rate all the recommended hotels on the scale of [1 5] to indicate whether they felt the recommendation had met their search criteria and was to their satisfaction. They were also asked to indicate which as-pect was the one that most influenced their decision. Raters were specifically instructed to only select  X  X ntents X  that were realistic for them (e.g., if you don X  X  have kids, do not select the  X  X amily X  as the intent).

First we look at the overall satisfaction, namely the user X  X  re-sponse to the question "would you stay in this hotel?". We aver-aged the responses over all raters. We see that for 60 . 2% hotels recommended by our system, users stated they would stay there, as compared to 50 . 8% from the rating systems. Moreover, users stated they would not stay in 26.4% of hotels recommended by the rating systems, compared with a much lower 15.9% dissatis-faction with hotels recommended by our system. To examine this in more detail than just averages, we plot the empirical histogram of the ratings given by our raters in Figure 2. The hotels recom-mended by our method received more 4 and 5 ratings then those the other method, and similarly our recommended hotels received fewer 1 and 2 ratings than the other method.

Interestingly, we observed that the satisfaction/dissatisfaction re-sults varied by country (Germany versus Italy in our data), as can be seen in Table 6. The Italian hotels have higher average price as well as higher price variance than those in Germany. This makes producing a good recommendation in Italy harder, because when the price is reasonable (as in our German hotels) people are more easily satisfied. Satisfaction with the star rating recommendations in Italy was at 47 . 4% , whereas for our context-based system it rose to 58 . 8% . Similarly, our system has a more dramatic affect in terms of lowering dissatisfaction for the Italian hotels (dissatisfaction is lowered from 30 . 1% to 15 . 4% ), than for the German ones.
An important issue in understanding the performance of a cold-start context-based recommender is to assess user consistency. We looked at the reason each rater stated for making their decisions (to stay or not, and their rating of our proposals). We compared that to their preference markings for hotel aspects. In Table 6, we show that our users are indeed very consistent; their decisions were con-sistent 78% 97% of the time (depending upon the case). For the case where users made consistent decisions, we show in 63 72% those cases, the user marked that they were satisfied. This indicates that we showed them hotels whose reviews resonated positively for them because the focused on the aspects user care about and make decisions on.
 Germany Ours 83, 40 62 . 2% 21 . 1% 16 . 7% Germany Stars 83, 40 55 . 6% 23 . 0% 21 . 4% Type / Reason Location Service Room Price Food
Next, we checked whether our recommendations resonate well for users with different trip intents. Table 6 details users satisfac-tion by intent. Satisfaction from our results was higher than for the star rating sites by 13% on average. Single and business travel-ers were considerably more satisfied with our suggestions than by those of the star ratings systems, and showed a considerable lower dissatisfaction. Interestingly, users who planned to travel in a group were dramatically more satisfied with our system, with 21.4% pre-ferring our systems X  suggestions.

We have demonstrated that common traits for groups can be found by preprocessing large samples of text. This is a powerful result, as identifying group traits can later be used for classifying whether unknown individuals belong to the group. Additionally, if common traits of a group are known, text of reviews can be mined to identify the typical crowd of a restaurant or a hotel, for example.
Additionally, an interesting outcome of our Mechanical Turk ex-periments suggests that there is no strict correlation between how a review is perceived and the corresponding rating given by its au-thor.
 Online hotel searching is a daunting task due to the wealth of online information. Reviews written by other travelers replace the word-of-mouth, yet turn the search into a time consuming task. Users do not rate enough hotels to enable a collaborative filtering based rec-ommendation. Thus, a cold start recommender system is needed.
This demo describes briefly our cold start hotel recommender system, which uses the text of the reviews as its main data. We define context groups based on reviews extracted from TripAdvi-sor.com and Venere.com. We introduce a novel weighted algorithm for text mining.

We implemented our system which was used by the public to conduct 150 trip planning experiments. We compare our solution to the top suggestions of the mentioned web services and show that users were, on average, 20% more satisfied with our hotel recom-mendations. We outperform these web services even more in cities where hotel prices are high.
 Categories and Subject Descriptors: H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval -Information filtering General Terms: Algorithms.
 Keywords: Recommender systems, opinion/text mining, context-aware recommender systems, common traits, sentiment analysis.
Producing recommendations for travel is inherently difficult, as an individual rarely rates more than a small number of hotels and thus rich profiles cannot be built. Having limited or no information about the user translates to a user cold start recommendation [1]. An intelligent cold start recommender will minimize a new user X  X  effort while still learning enough to recommend the user a product that is likely to be of her interest.

A lot of research has already been performed in the area of rec-ommender systems and information retrieval. However, most rec-ommender systems focus on recommending the most relevant items to users without taking into account any additional contextual in-formation. Most existing information retrieval systems base their retrieval decisions solely on queries collections, whereas informa-tion about search context is often ignored [2, 3].

Users X  search patterns are context-based.Among the plethora of  X 
Part of this work was done while Asher Levi was visiting Techni-color lab in Palo Alto.
 reviews, readers opt for recommendations from travelers with com-parable needs. A single traveler may share the same needs as other single travelers. A user traveling with her family has different needs from a user traveling on a business trip, i.e. the user context infor-mation is an important factor in choosing a hotel. When a user reads reviews she can metaphorically be seen as wearing person-alized glasses. Reviews are read through those glasses, and par-ticular words or comments will resonate, positively or negatively, with the reader based upon her needs for her upcoming trip and her personal preferences. Special attention is often given to reviews written with the same intent, or by reviewers from a comparable background. Hence, we define three types of context information. The first is intent , or purpose of the trip. We include 5 categories of intent, namely business trip, single traveler on vacation, family, group, couple . The second is nationality . The third context is user preferences for the different hotel aspects. These were mined from the text using an unsupervised clustering algorithm. We tagged the different clusters found in the text as location, service, food, room, price-value quality and the facilities (pool, spa, etc). Thus, a user using our system is asked to provide her trip intent, nationality, and preferences for these aspects.

We obtained data from Venere.com and TripAdvisor.com. The database contains details for each hotel: the hotel X  X  general infor-mation, reviews and ratings. In a pre-processing phase, we mined the text and found common traits for each context group. These are found in the form of typical words that appear more in text written within that context but are not common for other contexts. A clus-tering was used to group words that refer to each aspect. Thus, at the end of this pre-processing phase, we have significant words per context, be it intent, nationality, or hotel aspect.

The core idea of the algorithm is to give more importance to re-views of people with the same contexts as the user X  X . Common traits per the user X  X  context groups and words that describe favor-able hotel aspects are given a higher score than other words. We further find the sentiment expressed in the review per context (i.e., positive or negative) and give a corresponding score. Thus, the final score for each review corresponds to that of a user X  X  with compara-ble needs and preferences and coming from a similar background.
We implemented our system and published it for use, presenting to the users results from our system combined with the top sugges-tions of the above mentioned web sites. We had over 150 evalua-tions by friends and colleagues who looked for hotels in four major European cities. Hotels recommended by our system were favored (60.2%) compared to TripAdvisor X  X  and Venere X  X  top suggestions (50.8%). More significant is the fact that our raters said they would not stay in 26.4% of the top hotels recommended by these sites, whereas with our context-based recommender, their dissatisfaction was much lower at 15.9%.
Figure 1 gives a brief overview of the components and steps of our method. The top 3 boxes on the left correspond to the pre-processing phase in which we define the common traits of intent and nationality groups, and define the different hotel aspects ref-erenced in reviews, correspondingly. To find common traits for each context group, we extract the nouns and noun phrases (called features ) from all reviews and find those that are more common for that group. These features are then assigned a weight per each context according to their relative frequency in reviews within that context. The higher the weight, the more important a feature. The common traits of context groups are the higher weight features for that group. The fourth component of the preprocessing consists of building an opinion lexicon which will allow us to analyze adjec-tives associated with features, and to give each feature an orienta-tion score depending upon how positive or negative is the sentiment of any associated adjectives.

While the base weight of each feature is one in our system, fea-tures that are distinctive of several context groups may have differ-ent weight per group. The final weight for each feature is done by combining these three weights (depicted as "build feature score" in the figure).

Next we use our opinion lexicon to give each feature an orien-tation score. We subsequently combine the features, their weights and orientations to build a score for each sentence. The sentence scores are then combined to give an overall score for each review. This score should reflect the relative importance of the given re-view for the user. Reviews that are both important and positive are deemed most relevant thereby receiving the highest scores. The final score for each hotel is an average of all of its reviews, each of which is scored from the user X  X  perspective (i.e., based on her context and preferences), and an adjustment bias calculated per the context given.

Evaluation of a recommender system has to measure whether real people are willing to act based on the recommendations. User satisfaction with a recommender system results is well gauged with an on-line evaluation methodology. We use a methodology as in [4] that measures relative user satisfaction with one system over an-other. We implemented our system and made it available on the public web for use. We asked numerous friends and colleagues to evaluate our system and obtained 150 evaluations. Each experiment consisted of the following. The user inserts her search parameters: intent, nationality, aspect preferences, and a price range. Then we present the user a list of six hotels. Some are from our system, and some are the highest star ratings choices from Venere and Tripadvi-sor. In order to avoid biasing the user, these six hotels are presented in random order and thus the user is unaware of the source of the recommendations. Raters were shown links to the full text reviews to further explore the recommended hotels. For each one of the hotels, raters were asked to express their satisfaction by answering the question "Would you select this hotel?" with three optional an-swers: Yes, Maybe, No. In addition, the raters were asked to rate all the recommended hotels on the scale of [1 5] to indicate whether they felt the recommendation had met their search criteria and was to their satisfaction. They were also asked to indicate which as-pect was the one that most influenced their decision. Raters were specifically instructed to only select  X  X ntents X  that were realistic for them (e.g., if you don X  X  have kids, do not select the  X  X amily X  as the intent).

Table 3 presents the overall satisfactory results. We see that for 60 . 2% of the hotels recommended by our system, users stated they would stay there, as compared to 50 . 8% from the rating systems.
To examine this in more detail than just averages, we plot the empirical histogram of the ratings given by our raters in Figure 2. The hotels recommended by our method received more 4 and 5 rat-ings then those the other method, and similarly our recommended hotels received fewer 1 and 2 ratings than the other method. [1] A. Schein, A. Popescul, L. Ungar, and D. Pennock,  X  X ethods [2] G. Akrivas, M. Wallace, G. Andreou, G. Stamou, and S. Kollias,  X  X ontext-sensitive semantic query expansion, X  in
AIS, 2002.(ICAIS 2002). IEEE, 2002. [3] G. Adomavicius and A. Tuzhilin,  X  X ontext-aware recommender systems, X  Recommender Systems Handbook , 2011. [4] C. Hayes and P. Cunningham,  X  X n on-line evaluation framework for recommender systems, X  2002.
