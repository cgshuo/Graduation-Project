 Data Mining often suffers from the curse of dimensional-ity. Huge numbers of dimensions or attributes in the data pose serious problems to the data mining tasks. Tradition-ally data dimensionality reduction techniques like Principal Component Analysis have been used to address this prob-lem. However, the need might be to remain in the original attribute space and identify the key predictive attributes in-stead of moving to a transformed space. As a result feature subset selection has become an important area of research over the last few years.

With the advent of network technologies data is sometimes distributed in multiple locations and often with multiple par-ties. The biggest concern while sharing data is data privacy. Here, in this paper a secure distributed protocol is proposed that will allow feature selection for multiple parties without revealing their own data. The proposed distributed feature selection method has evolved from a method called virtual dimension reduction used in the field of hyperspectral im-age processing for selection of subset of hyperspectral bands for further analysis. The experimental results with real life datasets presented in this paper will demonstrate the effec-tiveness of the proposed method.
 Algorithms H.2.8 [ Database Applications ]: Data Mining; C.2.4 [ Distributed Systems ]: Distributed applications, Distributed databases Privacy preserving distributed data mining, feature selec-tion,virtual dimensionality reduction
The curse of dimensionality is referred to the scenario where huge amount of attributes present in the dataset makes statistical analysis of the data extremely difficult. Various dimensionality reduction techniques are used to address this problem like Principal Component Analysis, Singular Value Decomposition etc. These methods have been widely used in privacy preserving data mining because they provide data transformation as well as the orthogonality of the resul-tant data eliminates the risk of correlation attack. How-ever, there might be a need to retain the original attribute space. In this case feature selection is done using some sys-tematic approach to retain only the attributes that will lead to accurate statistical analysis and all other attributes are eliminated. The most appropriate feature subset selection method would be to try all combinations of the attributes by construction of the powersets of the attributes and then select the subset that is most effective. But, this method of exhaustive search is impractical with high dimensional-ity, e.g., in case of n attributes the number of combination would be 2 n .

In this paper we will use a feature subset selection method called virtual dimensionality reduction [1] which is used in hyperspectral image processing to select hyperspectral bands that are relevant for further analysis and eliminates all other redundant bands. The details of this method are presented later in this paper in section 2. Here, we use this method in a privacy preserving distributed data setting.

The greatest concern while sharing data is privacy. The privacy and security risks associated with data mining often thwarts the utility of data mining. Data can be distributed either horizontally or vertically. When the data is horizon-tally partitioned each party involved in data sharing has information about all the attributes but for different sets of objects whereas when the data is vertically partitioned each party has partial information about all the objects. An example of horizontal partitioning can be where more than one banks have data from their corresponding sources about fraud detection and will benefit from collaborative analysis but is unwilling or not allowed to reveal their business data. The data at different banks have the same attributes but contain different records. An example of vertical partition-ing can be where a hospital and the environmental depart-ment wish to collaborate in order to analyze outbreak of certain diseases in certain areas without actually revealing their data. Here, both data sets are about the same locations or areas, but have different attributes.

There exists a rich body of work in privacy preserving distributed data mining based on Secure Multi-Party Com-putation techniques. Surveys can be found in [7]. The secure distributed protocols are often derived from various cryptographic techniques [6,10]. The privacy preserving dis-tributed data mining techniques develop algorithms that will work in a distributed scenario and will not reveal critical in-formation of the parties involved. There has been extensive research in classification, clustering etc. for a privacy pre-serving distributed data setting [5,8,9]. However to the best of our knowledge not many research in this area handle fea-ture selection. In this paper we address this shortcoming by proposing an algorithm for feature selection that deals with both horizontally and vertically partitioned data. As the method is unsupervised, it can be used as a preprocessing step for any data mining task that will follow. The pro-posed algorithm explores a very different approach for fea-ture subset selection called virtual dimensionality reduction. Extensive experiments have been conducted to validate the effectiveness of the proposed algorithm.

The rest of the paper is organized as follows. Section 2 presents the virtual dimensionality reduction technique. Sec-tion 3 provides details about our methodology. Section 4 presents the extensive experimental results. Section 5 con-cludes the paper.
This section presents the virtual dimensionality reduc-tion method briefly. Harsanyi,Farrand and Chang in recent past developed an eigen thresholding method referred to as Harsanyi-Farrand-Chang(HFC) to determine the number of spectral endmembers of a hyperspectral data. The concept of this method is based on the Neyman Pearson detection theory and can be briefly described as follows.

Since the data dimensionality is equal to the total number of its eigenvalues, each eigenvalue specifies a data dimension with a certain level of significance provided by that partic-ular attribute in terms of energy or variance. If there is no effective value contained in an attribute, its correspond-ing correlation-eigenvalue and covariance-eigenvalue should reflect only noise energy, in which case, both correlation-eigenvalue and covariance-eigenvalue must be equal. This fact allows to formulate the difference between the correlation-eigenvalue and the covariance-eigenvalue as a binary com-posite hypothesis testing problem where the null hypothesis represents the case of the zero difference while the alterna-tive hypothesis being the case that the difference is greater than zero. When the Neyman-Pearson test is applied to each pair of a correlation-eigenvalue and its corresponding covariance-eigenvalue, the number of times the test fails in-dicates how many significant attributes are present in the data. The cardinality of this reduced set of attributes is termed as the virtual dimension of the data.

HFC method, which is conceptually described above can be mathematically represented as follows. Let  X  c 1  X   X  c  X  ... X  c L and  X  v 1  X   X  v 2  X   X  v 3 ... X  v L be two sets of eigen-values generated by CORR L  X  L and COV L  X  L , called cor-relation eigenvalues and covariance eigenvalues respectively. Assume that the significant attributes have positive energies and noise variance in attribute l is given by  X  2 l . and  X  c l  X   X  v l =0 for l=VD+1,VD+2,........ L (2) where  X  c l =  X  v l +  X  2 l for l =1 , 2 ...VD and  X  c l =  X  for l = VD +1 ,VD +2 ...L
In order to determine significant dimensionality, [3] formu-lated the problem outlined by equations 3 and 4 as a binary hypothesis problem as follows.
 for l =1 , 2 ...L and where H 0 is the null hypothesis and the alternative hypothesis is H 1 . Algorithm 1 Algorithm: Virtual Dimensionality Reduction Input: Dataset D , threshold  X  Output: Retained attributes A 1: Compute the covariance of D; cov = covariance ( D ) 2: Compute the correlation of D; cor = correlation ( D ) 3: Compute the eigen values of covariance matrix; eigcov = 4: Compute the eigen values of correlation matrix; 5: A= {} 6: for each attribute i do 7: if eigcor ( i )  X  eigcov ( i ) &gt; X  then 8: Retain i ; A = A  X  i 9: else 10: Discard i 11: end if 12: end for 13: return A
The generic algorithm of the feature selection method us-ing virtual dimensionality reduction [1] is presented in al-gorithm 1. Algorithm 1 basically computes the covariance matrix and correlation matrix of the datasets followed by the eigen value decomposition of both the covariance and the correlation matrices. Then the eigen values of the co-variance and the correlation matrices are compared with a predetermined threshold  X  for each attribute to identify the significance of that attribute. The threshold is basically used to regulate the number of attributes filtered. From algo-rithm 1 it is apparent that if we can compute the following statistics from the data for horizontal as well as vertical par-titioning in a secure way we can compute both covariance and correlation of the whole dataset. Then the eigen value decomposition can be done locally by each party which will reduce the communication cost drastically. However, the parties will have to agree on a threshold value  X  beforehand to compute the filtering locally. More details on the algo-rithm for horizontal and vertical partitioning is presented below.The statistics that needs to be computed are as fol-lows: The covariance between attributes i and j is:
COV ( i, j )= SS ij ( D ) /N ( D )  X  FS i ( D )  X  FS j ( D ) /N ( D ) The correlation between attributes i and j is: Horizontally Partitioned Data: The algorithm for hor-izontal partitioning is presented in algorithm 2. In case of horizontal partitioning computing these statistics is rela-tively simple as each party p can compute the share of their local data N p ( D ), FS p ( D )and SS p ( D ) and then engage in a secure sum protocol to compute the global statistics for all parties involved. The next step is to compute the standard deviation of each attribute. Once N ( D )and FS ( D )are known to each party, mean of each attribute j ,  X  j ( D )can be computed locally. Each party p then computes their lo-cal share of the standard deviation  X  p ( D ) and then engages in a secure sum protocol to compute the global standard deviation  X  ( D ).
 Algorithm 2 Algorithm: Secure Feature Selection for Hor-izontally Partitioned Data Input: Dataset D ( n rows and m columns) distributed as D 1 ,D 2 ....D p in p participating parties; predetermined threshold  X  1: Each party p computes local share N p ( D )andthenall 2: Each party p computes local share FS j p ( D )where1  X  3: Each party locally computes the mean of each attribute 4: Each party computes local share of standard deviation 5: Each party p computes local share SS ij p ( D )where1  X  6: Each party then locally computes cov ( D ), corr ( D ), Vertically Partitioned Data: In case of vertical parti-tioning the number of data elements in the whole dataset N ( D ) is known to each party and FS j ( D )and sigma j ( D ) for each attribute j where 1  X  j  X  m and m is the number of attributes in the dataset can be computed locally. The computation of the second order sum SS ij ( D )foreachpair of attributes i, j , is relatively complex as attributes are dis-tributed among p parties. If attribute i and j belong to the same party SS ij ( D ) can be computed locally. However, if i and j belong to two different parties p and t ,then p and t will have to engage in a secure dot product protocol [5] in order to compute the second order sum for attribute pair i, j . The algorithm for secure feature selection for vertical data partitioning is presented in algorithm 3.
 Security Analysis for Horizontal and Vertical Parti-tioning: We follow the semi-honest adversarial model as pre-sented in [7]. Therefore, our main concern is to restrict the amount of information revealed in the intermediate steps. We also assume that individual data is sensitive but global statistics is not. We also follow the  X  X omposition theorem X  as mentioned in [7] which states that if each sub protocol is secure, the entire protocol is secure.
 Algorithm 3 Algorithm: Secure Feature Selection for Ver-tically Partitioned Data Input: Dataset D ( n rows and m columns) distributed as D 1 ,D 2 ....D p in p participating parties, predetermined threshold  X  1: N ( D ) is known to each party 2: FS j ( D )where1  X  j  X  m can be computed locally by 3: Each party locally computes the standard deviation for 4: for each attribute pair i, j where 1  X  j  X  m ,1  X  j  X  m 5: if i and j belong to same party p then 6: Party p computes the second order sum SS ij ( D ) 7: else 8: i and j belong to party p and party t respectively 9: end if 10: end for 11: All parties share their part of the statistics with
In case of horizontally partitioned data Step 1, 2, 3 and 5 in algorithm 2 uses secure sum protocol to compute the required statistics. Since, secure sum protocol is secure all these steps are secure. For vertically partitioned data in the algorithm 3 the only step where data is being shared by mul-tiple parties is step 8. Secure dot product protocol [5] is used to compute step 8 and since this is proved to be secure, step 8 can be considered secure. In step 11 all the parties share their part of the statistics with all other parties but since the only information shared is the statistics the individual data elements are not divulged. If the parties are unwilling they can use more stringent cryptographic techniques for this part with increased communication cost.
 Communication Cost Analysis for Horizontal and Vertical Partitioning: Let n be the number of records, m be the number of attributes and p be the number of parties. The only secure protocol used in case of horizontal parti-tioning is secure sum protocol. Each party is computing a vector of it X  X  local share for m attributes and using secure sum protocol to compute the global statistics. Therefore, the communication cost for horizontal partitioning is in the order of O ( mp ). Whereas, the only secure protocol used in case of vertical partitioning is secure dot product protocol. If a pair of attribute i, j is located in two different parties, the parties engage in secure dot product protocol to compute the global second order sum for attribute i and j .There-fore, the communication cost for vertical partitioning is in the order of O ( m 2 np ).
This section presents experimental evaluation of the pro-posed method. The experiments were conducted on a ma-chine with Pentium Dual Core, 2.0 GHz CPU, 3.0 GB of RAM, and running Windows Vista. All algorithms were implemented using Matlab 7.0. Figure 1: Classification Accuracy for Pendigits and Breast Cancer after Feature Selection Datasets: The experiments were run over four real life datasets. Three of the datasets are from UCI Machine learn-ing Repository namely Pendigits, Breast Cancer Wiscon-sin and Ionosphere [2]. A real life dataset for Agriculture data [4] has been used as well. The Agriculture dataset is a time series data consisting of annual yield of grain on Broadbalk field at Rothamsted from 1852 to 1925 for 34 neighboring plots.
 Metrics: We measure the quality of mining. We ran the linear regression in WEKA 3.6 with 10 fold cross validation for Agriculture data after feature subset selection (where we tried to predict the yield of grain for the year of 1925 based on the previous years) and reported the correlation coefficient. For Pendigits, Breast Cancer and Ionosphere we randomly selected 10% of the data as test data and ran K-nearest neighbor classification in MATLAB after applying feature selection method.
 Results: We implemented algorithm 1 and tested with the above mentioned four datasets. The threshold  X  is varied between 0 to 1 to gradually increase the number of selected attributes, with zero selecting no attributes and 1 select-ing all attributes. The results are presented in figure 1 for Pendigits and Breast Cancer datasets and figure 2 for Iono-sphere and Agriculture datasets. The number of attributes is presented in the X-axis while the mining quality is pre-sented in the Y-axis.

As apparent from the figures for all the datasets mining accuracy comparable with mining on the original data can be achieved for much lower number of attributes. The rea-son for Pendigits requiring more number of attributes can be since the number of class labels in Pensigits is 10. The results are pretty convincing for Breast Cancer and Iono-sphere, each requiring approximately 10% of their actual number of attributes. Also, for Breast Cancer dataset the mining accuracy after feature selection improves when the number of attributes is equal to 8. The result is proba-bly most satisfactory for the time series data of Agriculture where we are trying to predict the production of grain for the 74th year from the previous 73 consecutive years using linear regression. The figures for Breast Cancer and Agri-culture datasets also show that the mining accuracy is often decreasing with increasing the number of selected features which can be explained as including of not so significant at-tributes can often decrease the mining accuracy. Therefore, the threshold  X  can be used to find the ideal number of se-lected attributes.
In this paper we have proposed a distributed privacy pre-serving method to perform feature subset selection that han-dles both horizontal as well as vertical data partitioning. Figure 2: Classification Accuracy for Ionosphere and Agriculture Data after Feature Selection Our research is inspired from a method that is used for band selection in hyperspectral image processing. The ex-perimental results prove the efficiency of this method for real life datasets including time series data. Therefore, our contributions can be summarized as exploring a new tech-nique for feature selection and applying this feature selection technique in a privacy preserving distributed data setting.
As the future direction of this research we would like to try more real life datasets, specifically more time series datasets. We would also like to explore various noise whitening meth-ods as a preprocessing step to the feature selection method to see if the feature subset selection method can be im-proved. [1] C. I. Chang, Hyperspectral Imaging: Techniques for [2] A. Frank, A. Asuncion, UCI machine learning [3] J. Harsanyi, W. Farrand, C.-I. Chang, Detection of [4] R. Hyndman, Time series data library (Accessed on [5] G. Jagannathan, R. N. Wright, Privacy-preserving [6] Y. Lindell, B. Pinkas, Privacy preserving data mining, [7] J. Vaidya, C. Clifton, M. Zhu, Privacy Preserving [8] J. Vaidya, M. Kantarcioglu, C. Clifton, [9] J. S. Vaidya, C. Clifton, Privacy preserving association [10] A. C. Yao, How to generate and exchange secrets, in:
