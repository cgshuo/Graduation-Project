 REGULAR PAPER Y. Fa t a i c h a  X  M. Cheriet  X  J. Y. Nie  X  C. Y. Suen Abstract A significant portion of currently available docu-ments exist in the form of images, for instance, as scanned documents. Electronic documents produced by scanning and OCR software contain recognition errors. This paper uses an automatic approach to examine the selection and the ef-fectiveness of searching techniques for possible erroneous terms for query expansion. The proposed method consists of two basic steps. In the first step, confused characters in erroneous words are located and editing operations are ap-plied to create a collection of erroneous error-grams in the basic unit of the model. The second step uses query terms and error-grams to generate additional query terms, iden-tify appropriate matching terms, and determine the degree of relevance of retrieved document images to the user X  X  query, based on a vector space IR model. The proposed approach has been trained on 979 document images to construct about 2,822 error-grams and tested on 100 scanned Web pages, 200 advertisements and manuals, and 700 degraded images. The performance of our method is evaluated experimentally by determining retrieval effectiveness with respect to recall and precision. The results obtained show its effectiveness and indicate an improvement over standard methods such as vectorial systems without expanded query and 3-gram over-lapping.
 Keywords Document processing  X  Optical character recognition (OCR)  X  Information retrieval (IR)  X  Error-grams  X  Query expansion 1 Introduction During the past 20 years, scientists have conducted exten-sive research on various aspects of electronic document pro-cessing. In practice, much information is still stored in paper documents, including technical reports, government files, newspapers, books, journals, magazines, letters, and bank checks, to name just a few. Many of these collections have been scanned, indexed, OCRed, and placed in corporate in-tranets to be used to gain competitive advantage. Retrieving them requires that their contents be recognized. Despite all the research that has been done in document image process-ing, several problems are still commonly encountered in this field [ 3 ]. Electronic documents produced by scanning and OCR software contain recognition errors, and the rate of er-rors increases significantly with the degradation of the docu-ment image. Such documents may then become inaccessible using conventional retrieval methods that affect the retrieval results significantly.
 relevant documents from a collection of documents based on a query presented by the user. Research has been conducted on the interaction between OCR and IR since 1980 and has consistently shown that the results of operations based on the exact matching of string attributes are often of lower quality than expected. For example, consider a corporation main-taining various image databases. A specific customer name might be present in more than one image. In one image, a customer name might be recorded as  X  X iemannian X , while in other image databases the same name may be recognized as  X  X icmanuinn X  or as  X  X icmamian X . A request to correlate these images and create a unified view of users will fail to produce the desired output if exact string matching is used in the retrieval process.
 quential confused characters within each erroneous word) tool that could be used to expand queries, and hence to OCR document images of varying quality. By confused charac-ter we mean characters from a document image that are wrongly recognized by an OCR system. In this study, we will take into account the generation of short erroneous substrings of n -grams of the confused characters in words and process them using standard methods available in IR. We have incorporated edit distance to determine possible OCR errors, which have occurred by confronting the OCR text with that provided by the ground truth, to collect fre-quent error-grams and construct their corresponding correc-tion rules. Our method takes advantage of dynamic program-ming ability to generate error-grams derived from erroneous substrings, which are introduced to extend query terms. In addition, error-grams are weighted depending on their fre-quencies. These weights are used in an IR vector space model in which each document is represented by a vector and where each element reflects the importance of a particu-lar term in the document and the collection. We compare the user X  X  query to these document images through basic vector operations and rank the retrieved images in decreasing order of their similarity to the query.
 dations are considered. Degraded images are obtained from ideal images through a degradation model, or by physically degrading (printing, scanning, faxing, etc.) a hard copy. The quality of the original document can be a problem for the following reasons. 1. The original is old and has suffered physical degradation. 2. The original is produced by a manual typewriter so the in-3. The original is a low-quality photocopy and shows varia-document images (article, newspaper, advertisement, busi-ness card, manual, form, and degraded images). Three sets were processed, with the first one considered for training and the next two for testing. Error-grams and correction rules were first generated using the training set and then combined to extend query terms. The experiments were performed and showed a marked improvement in retrieval performance when compared to standard methods such as vectorial sys-tems without expanded query and 3-gram overlapping, as defined in Sect. 5.4.
 retrieval performance on OCR data obtained from document images of different levels of quality. The remainder of the paper is organized as follows. The next section presents pre-vious studies. In Sect. 3, we define a framework of the re-trieval process based on OCR errors. Section 4 categorizes various OCR errors and uses matching algorithms to con-struct error-grams. Section 5 presents the retrieval process and performance measures. Section 6 shows experimental results comparing the most efficient algorithms applied to three independent sets: training, test, and degraded-test sets of document images. The conclusions, discussion, and fu-ture work directions are presented in Sect. 7. 2 Related work Information retrieval serves to search large textual databases and return documents the system considers relevant to the user X  X  queries [ 6 ]. Smeaton [ 14 ] uses the approximate shape of words in a text to refine the retrieval process; however, this approach cannot disambiguate recognition errors. Text retrieval from document images is difficult because OCR er-rors derive from editing operations such as character substi-tution, deletion, and insertion [ 5 , 7 , 8 , 22 ]. rection steps. Most approaches to the correction of scanning errors make use of the lexicon. Errors are detected by search-ing the text for words that do not appear in a lexicon [ 6 , 17 ]. This leads to many false alarms, since a lexicon cannot pos-sibly cover everything. Many studies in this area [ 5 , 7 , 8 , 22 ] show that three common mistakes  X  character substitution, deletion, and insertion  X  make up 80 X 90% of all typing er-rors.
 stitute (ISRI) at the University of Nevada, Las Vegas con-ducted many experiments to study OCR accuracy and re-trieval effectiveness from OCR-generated texts [19 X 22]. They showed the effects of OCR errors on ranking and feed-back using the vector space model. Feedback is provided by an automatic process that uses information derived from known relevant and non-relevant documents to reformulate queries, but cannot be used to compensate for OCR errors caused by degraded documents. Taghva and Stofsky [ 22 ] developed OCRSpell, which uses a special parser, domain-specific dictionaries, and a statistical tool mapping word generator to create a list of word candidates to replace in-correct terms. OCRSpell is used in [ 20 ] to deal with typ-ical OCR errors in texts, to avoid the extreme variability in ranked sets, and to improve retrieval effectiveness from poorly recognized document collections.
  X  X rror-grams X  for improving document retrieval of OCR texts have not been studied sufficiently. Automatic query expansion is a way of evaluating the potential usefulness of correcting OCR errors. Several algorithms are avail-able in the current literature for automatic query expansion [ 10 , 11 , 16 ], and our goal is to include erroneous words which can have a relationship with the terms of a user X  X  query. We believe that query expansion improves retrieval effectiveness, especially when erroneous terms appear as proper nouns or in short documents, because of a lack of redundancy. Croft et al. [ 5 ] extend query terms by using n -grams contained in query words. This method needs better closeness measures in order to eliminate spurious terms in the expansion. Ohta et al. [ 8 ] present a probabilistic text re-trieval method for carrying out full-text searches of English documents containing OCR errors. The validity of retrieved terms is determined based on the occurrence of confused characters and the connection with their preceding and suc-ceeding characters. All possible error information included in the confusion matrices increases the recall rate but signif-icantly decreases the precision rate. Suen [ 18 ] tabulated the growth in the number of distinct n -grams as a function of vocabulary size, their word-positional dependence, and the influence of the selected corpus.
 with a Boolean retrieval system. An error-gram refers to the part of the word which was not correctly recognized. The method needed a test collection and validation using degraded images. A survey of document image degrada-tion models proposed in the existing literature can be found in [ 1 ]. Furthermore, as shown in [ 13 ], the vector space IR model improves the Boolean representation by synthesizing a document X  X  content not only through a set of terms but also by considering the importance of the terms in documents and their specifics in the collection. After creating a set of aug-mented and weighted query terms with the correction rules, a SMART [ 12 ], vector-based retrieval system developed at Cornell University is used to retrieve relevant documents and to evaluate retrieval performance.
 is based on a recent work by Souza et al. [ 15 ] in which differ-ent commercial OCRs were studied with the goal of assess-ing criteria for filter selection using a variety of image qual-ities. We used the best OCR because it obtained the highest recognition rate. 3 System architecture of the proposed approach The system architecture is shown in Fig. 1 . The approach is described by the following three stages. 1. In the first step, we start from the three sets of images 2. The second step uses query terms, error-grams, ASCII 3. Finally, we measure the performance of the retrieval sys-user: 1. Locates and extracts text objects in it. 2. Compares OCR-recognized text with the original text 3. Measures the effectiveness of retrieval systems using the following sections. 4 Matching OCR errors A commercial OCR was used to read the located text in the training set to perform character recognition. The OCR en-gine made mistakes (Table 1 shows examples).
 quency between the original input (ASCII text in ground-truth set) and recognized OCR output texts would indicate that the n -gram substring in question was incorrectly recog-nized. Since counting errors by hand is time consuming, a simple error measure  X  edit-distance  X  was adopted. 4.1 Edit-distance The edit-distance algorithm is based on dynamic program-ming and matches strings without lexicons or a priori in-formation [ 2 ]. The distance between two words equals the number of editing operations required to transform one of the words into the other.
 document, M ocr the set of words contained in the recognized document, and s = e 1 ; e 2 ; ... ; e n a sequence of edit oper-ations for transforming a string x into another string y .The costs c ( s ) of this sequence are given by c ( s ) = n i where c ( e i ) is the cost of the i th edit operation. operation which may be required to transform x into y ,we define the distance between x and y by d ( x , y ) ={ min { c ( s ) }: s is a sequence of edit operations In set notation, we have correctly recognized words M and remaining words M
M error-grams and correction rules in document images. The algorithm [ 23 ] used to compute the edit-distance () is based on dynamic programming. It fills the matrix D 0 ... | where D i , j represents the minimum number of operations to match strings x 1 ... i to y 1 ... j , x is a string, | and x i is the i th character of x . The costs relating to the editing operations are set to 1 in this study. The algorithm below calculates the distance gradually in order to match two strings:  X   X   X   X   X  4.2 Error-grams and correction rules Error-grams are confused portions in erroneously recog-nized words (sequential confused characters inside each er-roneous word). For example, if the word  X  X chultz X  is recog-nized as  X  X ehnltz X , one error-gram considered will be  X  X hu X . It can be replaced in OCR text by  X  X hn X . Our algorithm pro-cesses the words which appear only in the remaining origi-nal words M remo . It uses the edit-distance to find the nearest word in M remr , to locate the erroneous substrings in the rec-ognized word called error-grams, and to define the correc-tion rules based on the corresponding substrings in the orig-inal word. We then verify the pairing, extract the immediate predecessor and successor for each confused character, and classify the n -grams extracted in order of occurrence. The algorithm works as follows: 1. For each word x i  X  M remo 2. Locate errors and verify the matching accuracy between 3. Calculate weights to quantify the importance of the er-5 Retrieval process Information retrieval is about finding the relevant informa-tion in a large text collection, with string matching being one of its basic tools. However, exact string matching is not good enough for document image retrieval because a word, when recognized incorrectly in the database, can no longer be retrieved. When data are noisy or corrupted, as is the case with OCR texts, exact string matching becomes inappropri-ate, and another measure is needed to facilitate information retrieval from the collections of OCR text.
 images and to expand the query into other possible terms. The document processor prepares, processes, and inputs the documents that users are searching for. It identifies potential indexable elements in documents, deletes stop words, stems terms, extracts index entries, computes weights, and creates and updates the main inverted file against which the search engine searches in order to match queries to documents. The expanded, weighted query is searched against the inverted file of documents obtained by the M  X  N document matrix, where M is the number of documents in the collection and N is the number of unique terms in the collection. The sim-ilarity of each document is calculated in the subset of docu-ments, and the system presents an ordered list to the user. submodules which: 1. Add to the query words generated by initial query words, 2. Extract document images relevant to the user X  X  query. An 3. Employ a retrieval component that operates on query and Finally, we measure the performance of the retrieval system and compare different methods. 5.1 Query expansion and selection For every query word, we add the words generated by substituting all error-grams contained in the term with their corresponding correction rules. Let us first give an example of the query expansion. Suppose that an original query contains the word  X  X ight X . It is statistically uncertain because OCR confuses  X  X  X  with  X  X  X  and  X  X  X  with  X  X  X , etc. Through the error-grams we know that the words  X  llght  X ,  X  lighl  X ,  X  right  X , etc. (32 words) are strongly related to  X  X ight X . Then, the expanded query with a probability higher than 0.001 will be light ; llght ; ligit ; lighd ; lieht ; iight ; lighl ; ligbt and the weights of the expanded query are 1 ; 0 . 096 ; 0 . 0092 ; 0 . 0086 ; 0 . 009 ; 0 . 0036 ; 0 . fusion in the answers. For example, the word  X  X ight X  above is used as an extended term, and its use harms the meaning of the user X  X  request. This can, however, also help in iden-tifying the documents in which  X  X ight X  has been mistakenly recognized as  X  X ight X . As we use the vectorial model, the affected weight 0.001 with the word  X  X ight X  in our exam-ple will influence the order of relevance for the documents which contain it. 5.2 Indexing component Documents are usually described through a set of terms. A common automatic indexing strategy is to take the set of all words found in the document as terms, remove the most common words, such as  X  X he X  and uninteresting terms such as  X  X hing X , and stem the remaining terms to get  X  X mage X  from  X  X mages X  and  X  X maging X . The remaining terms consti-tute the set of index terms.
 uments in a database and queries. A vector is obtained for each document and query from sets of terms with associated weights. The weights could be the frequency of occurrences of the word in the document. We could also assign a more sophisticated weighting scheme for each term. One of the most popular ways of creating weighting vectors is through the tf*idf family of weighting schemes. The term frequency component (tf) of a term t i for a document d j is calculated according to tf where Max l frequency lj is the frequency of the most com-mon term in the document. The inverse document frequency idf is computed as follows: idf where N is the total number of documents in the database and n i is the number of documents that contain the term t the tf*idf weighting scheme, the component of the weighting vector for the document d j at position i (i.e. for term t d 5.3 Similarity calculation With the above measures we can use cosine similarity sim ( q , d i ) between a query q = q 1 ; q 2 ; ... ; q t ument d i = d i 1 ; d i 2 ; ... ; d it to determine how close they are to each other geometrically. The cosine similarity is cal-culated using the following formula: sim ( q , d i ) = where d ij is the weight of the term t j in the document d q j is a query term determined as follows: q In the above formula, s is the number of error-grams used in the expanded query term and Pr( q j ) corresponds to the probability of the error-gram used to construct q j . larity to the query. Documents whose similarities exceed a certain threshold are retained in the response list, while all others are rejected as being irrelevant. 5.4 Performance measures Performance is determined by the retrieval of randomly se-lected queries. The lists of relevant document images on the basis of original texts are compared with those obtained by using OCR texts. The evaluation of the different methods is based on the retrieval effectiveness using average values of the rated recall and precision, which are calculated from the following equations: (i) RECALL is a measure of the ability of the system to (ii) PRECISION is a measure of the ability of the system (iii) Quality-distance (QD) is used to measure the per-(iv) The precision averages (at 11 standard recall levels  X  6 Experimental results 6.1 Data collection Three sets of document images were chosen. The first was a training set which was used to construct error-grams; the document corpus was the technical document database of the University of Washington [ 9 ], with 979 journal pages from a wide variety of journals covering diverse subject areas and publishers. The average size per page was 510 words.
 and printed from the Web (the average size per page was 410 words). This corpus was then degraded using an im-age processing model and by repeated photocopying. We used a combination of noise and blurring to form an addi-tional set of degraded images. Noise degradation was ap-plied first, using three different standard deviations, and then blurred using average operations in the convolution of the input image with two window sizes [the value of each pixel in the output image is based on a comparison of the cor-responding pixel in the input image with its 8 (24) neigh-bors for a 3  X  3(5  X  5) window size]. Repeated photo-copying caused character breaks and added black and white areas at random throughout the page. The new degraded-test set contained 700 images obtained from the above test set of 100 Web-page images and seven types of degrada-tion. Figure 2 shows an original Web page and three types of degradations.
 the Media Team document image database provided by the University of Washington. They came from various fields (business cards, advertisements, manuals, and forms). The average size per page was 44 words for advertising and 304 words for manual and form images. This corpus was used to test the robustness and sensitivity with images containing names of people or places and short texts.
 selected from the content of documents. Each query con-tained an average of three words.
 uments returned by SMART for each query and based on electronic texts provided by the U-W-1 database as relevant. The documents retrieved by different systems were then compared with the supposed relevant documents to determine, for each query, whether or not they were relevant. 6.2 OCR recognition 6.2.1 Training phase The results obtained using an edit-distance are presented in Table 2 . In the original images, we had 614 non-text fields, which explains the higher number of words present in the OCR texts than the original texts: 499,123 words were present in the original documents while the OCR extracted 528,315 words. Only 468,619 words out of the 499,123 were correctly recognized. The dynamic programming algorithm with distance 2 matched 5,185 words, and we used them to build the error-grams. Note that we can improve recognition by reducing noise and using features acquired to distinguish text that is considered as noise.
 2,319 insertions. The output of the edit-distance algorithm will serve as the input for the error rule-building algo-rithm to construct the error-grams and the correction rules. The algorithm constructed 2,822 error-grams and correction rules.
 P that error-gram A i in the original image can be regarded as B j in the OCR texts. 6.2.2 Test phase Ta b l e s 4 and 5 show the results of the recognition of the test and the degraded-test collections. We note the decrease in the performance of the recognition on the degraded im-ages and observe that added Gaussian noise does not affect recognition accuracy. Figure 3 shows that the recognition by OCR resists noise, but the performance falls as the blur of the characters grows.
 that in the training set (93%) because the resolution and the quality of the printed Web images are worse.
 tion rate when it is submitted to OCR software. Many fac-tors such as font size, broken character, touching characters, and white speckles are used to indicate the image quality. Experimental results on the test and degraded sets exhibit a significant decrease in the recognition rate from 93.8% on the training set to 72.26% with the test collection and to about 10% with a high blur degradation. Two-pass photo-copying produces a recognition rate of about 38%, which has the same effect as degradation blurred by eight neigh-bour pixels. 6.3 Retrieval effectiveness The recall-precision graph is the most commonly used method for comparing systems. The plots of different runs can be superimposed on the same graph to determine which system is superior. Comparisons are best made in three dif-ferent recall ranges: 0 X 0.2, 0.2 X 0.8, and 0.8 X 1. These ranges characterize low-recall, mid-recall, and high-recall perfor-mance, respectively. 6.3.1 On training set In the retrieval process, performance is determined by the retrieval of 50 randomly selected queries. Each query con-tains several words. Our method is compared with the SMART-based vectorial model without expanded query (called Smart in different figures) as well as with Ukko-nen X  X  [ 23 ] Q-gram for quality distance = 1 and 4. Figure 4 shows that, in the training phase, our approach achieves an improvement in terms of recall and precision.
 query (Smart), the average precision is between 97.81 and 99.84% for the low-recall, between 83.62 and 96.95% for the middle-recall, and between 23.53 and 76.08% for the high-recall performance. The  X 3-gram overlap X  technique extracts a broad range of words and results in a fall of pre-cision at high recall levels. For 3-gram overlaps with a large QD distance, the recall increases but the precision decreases. In addition, the average precision for all relevant documents (averaged over queries) is 87.68% for our approach but 86.53% for a vectorial search without expanded query and does not exceed 65.99% for 3-gram overlap methods. of the required word likely to be incorrectly recognized. It presents the best performance at high recall. We observe that our approach achieves better overall retrieval effective-ness compared with other methods. This is due to the statis-tical characteristics of extracting and classifying expanded words based on their importance, relative to the confusions, in training. An example with the query input  X  X s schultz in some journals? X : after removing common words and stem-ming, query terms become  X  X chultz journ X  and the expanded query using 3-gram overlap is  X  X chultz journ sch chu hul ult ltz jou our urn X . If the word  X  X chultz X  is recognized in a doc-ument as  X  X ehnltz X , the QD distance between these words is 4. But with our method the expanded query contains the terms  X  X ehnltz X  with 0.002 probability and the document im-age is ranked in the relevant list.
 should mention that in the second range (middle recall), our system outperforms Smart by 4%, and it goes up to 13.08% in the third range (high recall). This will help to understand the importance of modelling errors and to exploit this mis-recognition in the retrieval process. 6.3.2 On Test 1, Test 2, and degraded-test sets It is interesting to extend these experiments to a wider range of document images. To do this, we tested our system and compared the results for original and degraded collections based on retrieval performance. The Test 1 and Test 2 col-lections used to obtain the results presented in Fig. 5 and Ta b l e 7 show the same tendency as that of the training set, except for the 3-gram overlap approach, with a small dis-tance, which is able to perform well in the high precision field. This is due to the smaller number of test document images we have compared with the training set.
 vant documents (averaged over queries) is 87.90% for our approach, but 85.33% for the vectorial search without ex-panded query, and it does not exceed 65.99% for other methods.
 that our approach has the best overall precision and concurs with the recall of the best 3-gram overlaps. The explanation for this is obtained from weak frequency words in the doc-ument, such as people or place names, and which are badly recognized. Only through our method the example  X  X aube-rian X , which appears twice in a document and is recognized as  X  X auoenan X , can be retrieved with a 0.0009 probability. We mention that query expansion improves retrieval effec-tiveness, especially when erroneous terms appear as proper nouns or in short documents due to a lack of redundancy. The influence of the added terms starts when the recall be-comes high.
 tion accuracy is very low. We can see in Figs. 6 and 7 that the precision is maintained with an upper limit of 65.54% for recalls lower than 50%. Beyond that the performance de-creases and all the methods follow the same tendency. We note that the performance rate is better with our expanded queries at high-recall performance for photocopied sets. The average precision for all relevant documents over all queries is 63.16% for our method, but decreases to 60.44% for 3-gram overlaps with large distances and to 60.66% for stan-dard vectorial searches without expanded queries. We see the same tendency for degraded images, where the aver-age precision for all relevant documents over all queries is 59.86% for our method and 55.62% with the standard vec-torial system. However, the rate decreases to 50.74% for 3-gram overlaps with wide distances. This is due to the power of 3-gram overlaps in extracting any parts of words which appear in the text and to the low number of images in the test collection. graded set can be seen in Fig. 8 , which shows a drop in the precision rate. We note that the  X 3-gram overlaps X  remain better in the middle recall. The problem with the 3-gram ap-proach is the fast drop in precision when the quality dis-tance (QD) increases. Between the high-precision and high-recall fields, the average precision becomes lower than 50% for the middle-recall and less than 3% for the high-recall performance, except for our method, which is maintained at 6.69%. The average precision for all relevant documents over all queries is 38.46% for our method but decreases to 33.64% for 3-gram overlaps with a small quality distance and 28.48% for standard vectorial search.
 that our algorithm performs better than other methods on degraded-quality images. We have a better closeness method for eliminating spurious terms in the expansion query, and it has improved the retrieval performance in document images. It is interesting to see how the image quality affects retrieval performance. All approaches yield almost the same results for very degraded images. 7 Conclusion and perspectives This work presents an approach to processing textual infor-mation contained in document images and for performing effective retrieval. String processing in a textual corpus is a very fertile and useful research area. Current OCRs do not work well on poor-quality or scanned document images. Three different sets of document images were used for train-ing, testing, and validating. The proposed method collects frequent error-grams and correction rules that can be used to extend query terms and to improve retrieval performance. We have shown that an n -gram and its corresponding prob-ability can greatly influence the results returned by the stan-dard cosine measure. Furthermore, investigating the OCR of poor-quality documents is important for document images generated from archives of originals created before the dawn of the digital age. We have discussed the use of one OCR engine and how the nature of the degradation can affect the accuracy of the resulting OCR effort.
 ages and 200 advertising and manual images were tested in order to validate any increase in retrieval effectiveness by using error-grams to expand query terms. Experimental results indicate a noticeable improvement in the retrieval ef-fectiveness as compared to exact, partial, and 3-gram over-lap matching. Further research is currently being undertaken to outperform our approach. The aim is to investigate the ability of this approach to improve retrieval effectiveness. Possible techniques include:  X  Additional image preprocessing, which may increase the
OCR recognition rate.  X  An iterative edit-distance technique with various cost functions to match more words and to increase the recog-nition rate.  X  Use of different training sets of document images to create other kinds of error-grams and hence correction rules to increase information retrieval performances.  X  Possible changes to some of the correction rules on which we based our approach. A potential solution is to rebuild these  X  X rror-grams X  for very degraded images in order to produce better results.  X  Use of a combination of a number of OCR engines and the derivation of a voting system for erroneous words to increase the quality of results.
 References
