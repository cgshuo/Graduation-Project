 Document classification presents difficult challenges due to the sparsity and the high dimensionality of text data, and to the complex semantics of the natural language. The tradi-tional document representation is a word-based vector (Bag of Words, or BOW), where each dimension is associated with a term of the dictionary containing all the words that ap-pear in the corpus. Although simple and commonly used, this representation has several limitations. It is essential to embed semantic information and conceptual patterns in or-der to enhance the prediction capabilities of classification algorithms. In this paper, we overcome the shortages of the BOW approach by embedding background knowledge de-rived from Wikipedia into a semantic kernel, which is then used to enrich the representation of documents. Our empir-ical evaluation with real data sets demonstrates that our ap-proach successfully achieves improved classification accuracy with respect to the BOW technique, and to other recently developed methods.
 I.5.3 [ Pattern recognition ]: Clustering X  algorithms, sim-ilarity measure ; I.7.0 [ Document and Text Processing ]: General Algorithms Text Classification, Wikipedia, Kernel Methods, Semantic Kernels
Text categorization represents a challenging problem to data mining and machine learning communities due to the growing demand for automatic information retrieval sys-tems. Traditionally, document classification is based on a  X  X ag of Words X  approach (BOW): each document is repre-sented as a vector with a dimension for each term of the dic-tionary containing all the words that appear in the corpus. The value associated to a given term reflects its frequency of occurrence within the corresponding document (Term Fre-quency, or tf ), and within the entire corpus (Inverse Doc-ument Frequency, or idf ). This technique has three major drawbacks: (1) it breaks multi-word expressions, like  X  X ext Classification X , into independent features; (2) it maps syn-onymous words into different components; and (3) it consid-ers polysemous words (i.e., words with multiple meanings) as one single component. Although traditional preprocess-ing of documents, such as eliminating stop words, pruning rare words, stemming, and norm alization, can improve the representation, its effect is still limited. It is therefore essen-tial to further embed semantic information and conceptual patterns to be able to enhance the prediction capabilities of classification algorithms.

We overcome the shortages of the BOW approach by em-bedding background knowledge constructed from Wikipedia into a semantic kernel, which is used to enrich the rep-resentation of documents. Our semantic kernel is able to keep multi-word concepts unbroken, it captures the seman-tic closeness of synonyms, and performs word sense disam-biguation for polysemous terms.

Attempts have been made in the literature to construct se-mantic kernels from ontologies, such as WordNet . Alhough empirical results have shown improvements in some cases, the applicability of WordNet to improve classification accu-racy is very limited. This is because the ontology is manually built, and its coverage is far too restricted. For this reason, we make use of Wikipedia, the world largest electronic ency-clopedia to date. In Wikipedia, a concept is illustrated with an article, and each concept belongs to at least one category (e.g., the concept  X  X aguar X  belongs to the category  X  X elines X ). A concept may redirect to another concept if the two are synonyms. If a concept is polysemous, Wikipedia provides a disambiguation page, which lists all possible meanings of the polysemous concept. Each meaning is again illustrated with an article.
 Thus, Wikipedia is a rich source of linguistic information. However, Wikipedia is not a structured thesaurus like Word-net. In [21], the authors constructed an informative the-saurus from Wikipedia, which explicitely derives synonymy, polysemy, hyponymy, and associative relations between con-cepts. The resulting thesaurus offers a much broader cov-erage than any manually built one, such as WordNet, and surpasses them in accuracy it can achieve [15]. In this pa-per, we leverage the thesaurus derived from Wikipedia [21] to embed semantic information in our document representa-tion, and therefore achieve improved classification accuracy based on documents X  content.

The paper is organized as follows. Section 2 discusses related work. Section 3 briefly describes the structure of Wikipedia, and how the authors in [21] build a thesaurus from Wikipedia. In Section 4, we introduce our technique for building semantic kernels. Experimental results are pre-sented and discussed in Section 5. Finally, Section 6 provides conclusions and ideas for future work.
Research has been done to exploit ontologies for content-based categorization of large corpora of documents. In par-ticular, WordNet has been widely used. Siolas et al. [19] build a semantic kernel based on WordNet. Their approach can be viewed as an extension of the ordinary Euclidean metric. Jing et al. [12] define a term similarity matrix using WordNet to improve text clustering. Their approach only uses synonyms and hyponyms. It fails to handle polysemy, and breaks multi-word concepts into single terms. Hotho et al. [11] integrate WordNet knowledge into text cluster-ing, and investigate word sense disambiguation strategies and feature weighting schema by considering the hyponymy relations derived from WordNet. Their experimental evalua-tion shows some improvement compared with the best base-line results. However, considering the restricted coverage of WordNet, the effect of word sense disambiguation is quite limited. The authors in [6, 20] successfully integrate the WordNet resource for document classification. They show improved classification results with respect to the Rocchio and Widrow-Hoff algorithms. Their approach, though, does not utilize hypernyms and associate terms (as we do with Wikipedia). Although [5] utilized WordNet synsets as fea-tures for document representation and subsequent cluster-ing, the authors did not perform word sense disambiguation, and found that WordNet synsets actually decreased cluster-ing performance.

Gabrilovich et al. [8, 9] propose a method to integrate text classification with Wikipedia. They first build an auxiliary text classifier that can match documents with the most rel-evant articles of Wikipedia, and then augment the BOW representation with new features which are the concepts (mainly the titles) represented by the relevant Wikipedia articles. They perform feature generation using a multi-resolution approach: features are generated for each docu-ment at the level of individual words, sentences, paragraphs, and finally the entire document. This feature generation procedure acts similarly to a retrieval process: it receives a text fragment (such as words, a sentence, a paragraph, or the whole document) as input, and then maps it to the most relevant Wikipedia articles. This method, however, only leverages text similarity between text fragments and Wikipedia articles, ignoring the abundant structural infor-mation within Wikipedia, e.g. internal links. The titles of the retrieved Wikipedia articles are treated as new features to enrich the representation of documents [8, 9]. The au-thors claim that their feature generation method implicitly performs words sense disambiguation: polysemous words within the context of a text fragment are mapped to the concepts which correspond to the sense shared by other con-text words. However, the processing effort is very high, since each document needs to be scanned many times. Further-more, the feature generation procedure inevitably brings a lot of noise, because a specific text fragment contained in an article may not be relevant for its discrimination. Further-more, implicit word sense disambiguation processing is not as effective as explicit disambiguation, as we perform in our approach.

Milne et al. [15] build a professional, domain-specific the-saurus of agriculture from Wikipedia. Such thesaurus takes little advantage of the rich relations within Wikipedia ar-ticles. On the contrary, our approach relies on a general thesaurus, which supports the processing of documents con-cerning a variety of topics. We investigate a methodology that makes use of such thesaurus, to enable the integration of the rich semantic information of Wikipedia into a kernel.
Wikipedia (started in 2001) is today the largest encyclope-dia in the world. Each article in Wikipedia describes a topic (or concept), and it has a short title, which is a well-formed phrase like a term in a conventional thesaurus [15]. Each article belongs to at least one category, and hyperlinks be-tween articles capture their semantic relations, as defined in the international standard for thesauri [11]. Specifically, the represented semantic relations are: equivalence ( synonymy ), hierarchical ( hyponymy ), and associative.

Wikipedia contains only one article for any given con-cept (called preferred term ). Redirect hyperlinks exist to group equivalent concepts with the preferred one. Figure 1 shows an example of a redirect link between the synonyms  X  X uma X  and  X  X ougar X . Besides synomyms, redirect links han-dle capitalizations, spelling variations, abbreviations, collo-quialisms, and scientific terms. For example,  X  X nited States X  is an entry with a large number of redirect pages: acronyms (U.S.A., U.S., USA, US); Spanish translations (Los Esta-dos, Unidos, Estados Unidos); common mispellings (Untied States); and synonyms (Yankee land) [2].

Disambiguation pages are provided for a polysemous con-cept. A disambiguation page lists all possible meanings asso-ciated with the corresponding concept, where each meaning is discussed in an article. For example, the disambiguation page of the term  X  X uma X  lists 22 associated concepts, includ-ing animals, cars, and a sportswear brand.

Each article (or concept) in Wikipedia belongs to at least one category, and categories are nested in a hierarchical or-ganization. Figure 1 shows a fragment of such structure. The resulting hierarchy is a directed acyclic graph, where multiple categorization schemes co-exist [15].

Associative hyperlinks exist between articles. Some are one-way links, others are two-way. They capture different degrees of relatedness. For example, a two-way link exists between the concepts  X  X uma X  and  X  X ougar X , and a one-way link connects  X  X ougar X  to  X  X outh America X . While the first link captures a close relationship between the terms, the second one represents a much weaker relation. (Note that one-way links establishing strong connections also exist, e.g., from  X  X ata Mining X  to  X  X achine Learning X .) Thus, mean-ingful measures need to be considered to properly rank as-sociative links between articles. Three such measures have been introduced in [21]: Content-based , Out-link category-based ,and Distance-based . We briefly describe them here. In Section 4.2 we use them to define the proximity between associative concepts. Figure 1: A fragment of Wikipedia X  X  taxonomy
The content-based measure is based on the bag-of-words representation of Wikipedia articles. Each article is mod-eled as a tf-idf vector; the associative relation between two articles is then measured by computing the cosine similarity between the corresponding vectors. Clearly, this measure (denoted as S BOW ) has the same limitations of the BOW approach.

The out-link category-based measure compares the out-link categories of two associative articles. The out-link cat-egories of a given article are the categories to which out-link articles from the original one belong. Figure 2 shows (a frac-tion of) the out-link categories of the associative concepts  X  X ata Mining X ,  X  X achine Learning X , and  X  X omputer Net-work X . The concepts  X  X ata Mining X  and  X  X achine Learning X  share 22 out-link categories;  X  X ata Mining X  and  X  X omputer Network X  share 10;  X  X achine Learning X  and  X  X omputer Net-work X  share again the same 10 categories. The larger the number of shared categories, the stronger the associative relation between the articles. To capture this notion of sim-ilarity, articles are represen ted as vectors of out-link cat-egories, where each component corresponds to a category, and the value of the i -th component is the number of out-link articles which belong to the i -th category. The cosine similarity is then computed between the resulting vectors, and denoted as S OLC . The computation of S OLC for the concepts illustrated in Figure 2 gives the following values, which indeed reflect the actual semantic of the correspond-ing terms: S OLC ( Data Mining , Machine Learning )=0 . 656, S OLC ( Data Mining , Computer Network )=0 . 213, S OLC ( Machine Learning , Computer Network )=0 . 157.
The third measure is a distance measure (rather then a similarity measure like the first two). The distance between Figure 2: Out-link categories of the concepts  X  X a-chine Learning X ,  X  X ata Mining X , and  X  X omputer Network X  two articles is measured as the length of the shortest path connecting the two categories they belong to, in the acyclic graph of the category taxonomy. The distance measure is normalized by taking into account the depth of the taxon-omy. It is denoted as D cat .

A linear combination of the three measures allows to quan-tify the overall strength of an associative relation between concepts:
S where  X  1 , X  2  X  (0 , 1) are parameters to weigh the individual measures. Equation (1) allows to rank all the associative articles linked to any given concept.
As mentioned before, the  X  X ag of Words X  (BOW) ap-proach breaks multi-word expressions, maps synonymous words into different components, and treats polysemous as one single component. Here, we overcome the shortages of the BOW approach by embedding background knowledge into a semantic kernel, which is then used to enrich the rep-resentation of documents.

In the following, we first describe how to enrich text doc-uments with semantic kernels, and then illustrate our tech-nique for building semantic kernels using background knowl-edge constructed from Wikipedia. The BOW model (also called Vector Space Model, or VSM) [18] of a document d is defined as follows:  X  : d  X   X  ( d )=( tf ( t 1 ,d ) ,tf ( t 2 ,d ) ,...,tf ( t where tf ( t i ,d ) is the frequency of term t i in document d , and D is the size of the dictionary.

The basic idea of kernel methods is to embed the data in a suitable feature space, such that solving the problem (e.g., classification or clustering) in the new space is easier (e.g., linear). A kernel represents the similarity between two ob-jects (e.g., documents or terms), defined as dot-product in this new vector space. The kernel trick [17] allows to keep the mapping implicit. In other words, it is only required to know the inner products between the images of the data items in the original space. Therefore, defining a suitable kernel means finding a good representation of the data ob-jects.

In text classification, semantically similar documents should be mapped to nearby positions in feature space. In order to address the omission of semantic content of the words in VSM, a transformation of the document vector of the type  X   X  ( d )=  X  ( d ) S is required, where S is a semantic matrix. Different choices of the matrix S lead to different variants of VSM. Using this tra nsformation, the correspond-ing vector space kernel takes the form in feature space can be computed efficiently directly from the original data items using a kernel function.

The semantic matrix S can be created as a composition of embeddings, which add refinements to the semantics of the representation. Therefore, S can be defined as: where R is a diagonal matrix containing the term weight-ings or relevance, and P is a proximity matrix defining the semantic similarities between the different terms of the cor-pus. One simple way of defining the term weighting matrix R is to use the inverse document frequency ( idf ).
P has non-zero off diagonal entries, P ij &gt; 0, when the term i is semantically related to the term j . Embedding P in the vector space kernel corresponds to representing a document as a less sparse vector,  X  ( d ) P , which has non-zero entries for all terms that are semantically similar to those present in document d . There are different methods for ob-taining P [22, 1]. Here, we leverage the external knowledge provided by Wikipedia.

Given the thesaurus built from Wikipedia, it is straight-forward to build a proximity (or similarity) matrix P .Here is a simple example. Suppose the corpus contains one doc-ument d 1 that talks about pumas (the animal). A second document d 2 discusses the life of cougars. d 1 contains in-stances of the word  X  X uma X , but no occurrences of  X  X ougar X . Vice versa, d 2 containts the word  X  X ougar X , but  X  X uma X  does not appear in d 2 . Fragments of the BOW representations of d 1 and d 2 aregiveninTable1,wherethefeaturevalues are term frequencies. The two vectors may not share any features (e.g., neither document contains the word  X  X eline X ).
Table 2 shows a fragment of a proximity matrix computed from the thesaurus based on Wikipedia. The similarity be-tween  X  X uma X  and  X  X ougar X  is one since the two terms are synonyms. The similarity between  X  X uma X  and  X  X eline X  (or  X  X ougar X  and  X  X eline X ) is 0.4, as computed according to equa-tion (1). Table 3 illustrates the updated term vectors of documents d 1 and d 2 , obtained by multipling the original term vectors (Table 1) with the proximity matrix of Table 2. The new vectors are less sparse, with non-zero entries not only for terms included in the original document, but also for terms semantically related to those present in the docu-ment. This enriched representation brings documents which are semantically related closer to each other, and therefore it facilitates the categorization of documents based on their content. We now discuss the enrichment steps in detail.
The thesaurus derived from Wikipedia provides a list of concepts. For each document in a given corpus, we search for the Wikipedia concepts mentioned in the document. Such concepts are called candidate concepts for the correspond-ing document. When searching for candidate concepts, we adopt an exact matching strategy, by which only the con-cepts that explicitly appear in a document become the can-didate concepts. (If an m -gram concept is contained in an n -gram concept (with n&gt;m ), only the last one becomes a candidate concept.) We then construct a vector represen-tation of a document, which contains two parts: terms and candidate concepts. For example, consider the text fragment  X  X achine Learning, Statistical Learning, and Data Mining are related subjects X . Table 4 shows the traditional BOW term vector for this text fragment (after stemming), where feature values correspond to term frequencies. Table 5 shows the new vector representation, where boldface entries are candidate concepts, and non-boldface entries correspond to terms.

We observe that, for each document, if a word only ap-pears in candidate concepts, it won X  X  be chosen as a term feature any longer. For example, in the text fragment given above, the word  X  X earning X  only appears in the can-didate concepts  X  X achine Learning X  and  X  X tatistical Learn-ing X . Therefore, it doesn X  X  appear as a term in Table 5. On the other hand, according to the traditional BOW ap-proach, after stemming, the term  X  X earn X  becomes an entry of the term vector (Table 4). Furthermore, as illustrated in Table 5, we keep each candidate concept as it is, without per-forming stemming or splitting multi-word expressions, since multi-word candidate concepts carry meanings that cannot be captured by the individual terms. Table 5: Vector of candidate concepts and terms
When generating the concept-based vector representation of documents, special care needs to be given to polysemous concepts, i.e., concepts that have multiple meanings. It is necessary to perform word sense disambiguation to find the specific meaning of ambiguos concepts within the corre-sponding document. For instance, the concept  X  X uma X  is an ambiguous one. If  X  X uma X  is mentioned in a document, its actual meaning in the document should be identified, i.e., whether it refers to a kind of animal, or to a sportswear brand, or to something else. In Section 4.2.1 we explain how we address this issue.

Once the candidate concepts have been identified, we use the Wikipedia thesaurus to select synonyms, hyponyms, and associative concepts of the candidate ones. The vec-tor associated to a document d is then enriched to include such related concepts:  X  ( d )=( &lt; terms &gt; , &lt; candidate con-cepts &gt; , &lt; related concepts &gt; ). The value of each component corresponds to a tf-idf value. The feature value associated to a related concept (which does not appear explicitely in any document of the corpus) is the tf-idf value of the corre-sponding candidate concept in the document. Note that this definition of  X  ( d ) already embeds the matrix R as defined in equation (3).

We can now define a proximity matrix P for each pair of concepts (candidate and related). The matrix P is repre-sented in Table 6. For mathematical convenience, we also include the terms in P . P is a symmetrical matrix whose elements are defined as follows. For any two terms t i and t , P ij =0if i = j ; P ij =1if i = j . For any term t i and any concept c j , P ij = 0. For any two concepts c i and c
P S overall is computed according to equation (1). depth represents the distance between the corresponding cate-gories of two hyponym concepts in the category structure of Wikipedia. For example, suppose c i belongs to category A and c j to category B .If A is a direct subcategory of B , then depth =1. If A is a direct subcategory of C ,and C is
Concepts Table 7: Cosine similarity between the Reuters doc-ument #9 and the Wikipedia X  X  articles correspond-ing to the different meanings of the term  X  X tock X  a direct subcategory of B ,then depth =2.  X  is a back-off factor, which regulates how fast the proximity between two concepts decreases as their category distance increases. (In our experiments, we set  X  =2.) By composing the vector  X  ( d ) with the proximity matrix P , we obtain our extended vector space model for docu-ment d :  X   X  ( d )=  X  ( d ) P .  X   X  ( d ) is a less sparse vector with non-zero entries for all concepts that are semantically sim-ilar to those present in d . The strength of the value asso-ciated with a related concept depends on the number and frequency of occurrence of candidate concepts with a close meaning. An example of this effect can be observed in Ta-ble 3. Let us assume that the concept  X  X eline X  is a related concept (i.e., did not appear originally in any of the given documents).  X  X eline X  appears in document d 1 with strength 0.8, since the original document d 1 contains two occurrences of the synonym concept  X  X uma X  (see Table 1), while it ap-pears in d 2 with a smaller strength (0.4), since the original document d 2 contains only one occurrence of the synonym concept  X  X ougar X  (see Table 1). The overall process, from building the thesaurus from Wikipedia, to constructing the proximity matrix and enriching documents with concepts, is depicted in Figure 3.
If a candidate concept is polysemous, i.e. it has multi-ple meanings, it is necessary to perform word sense disam-biguation to find its most proper meaning in the context where it appears, prior to calculating its proximity to other related concepts. We utilize text similarity to do explicit word sense disambiguation. This method computes docu-ment similarity by measuring the overlapping of terms. For instance, the Reuters-21578 document #9 talks about stock splits, and the concept  X  X tock X  in Wikipedia refers to sev-House stock Bucket shop Treasury stock Stock exchange Market capitalization Board of directors Business organizations Corporation Fiduciary Stock Chief executive officer Shareholder Fiduciary Corporate governance Corporation Figure 3: The process that derives semantic kernels from Wikipedia eral different meanings, as listed in Table 7. The correct meaning of a polysemous concept is determined by compar-ing the cosine similarities between the tf-idf term vector of the text document (where the concept appears), and each of Wikipedia X  X  articles (corresponding tf-idf vectors) describ-ing the different meanings of the polysemous concept. The larger the cosine similarity between two tf-idf term vectors is, the higher the similarity between the two corresponding text documents. Thus, the meaning described by the arti-cle with the largest cosine similarity is considered to be the most appropriate one. From Table 7, the Wikipedia article describing  X  X tock X  (finance) has the largest similarity with the Reuters document #9, and this is indeed confirmed to be the case by manual examination of the document (docu-ment #9 belongs to the Reuters category  X  X arn X ).

As mentioned above, document #9 discusses the stock split of a company, and belongs to the Reuters category  X  X arn X . The document contains several candidate concepts, such as  X  X tock X ,  X  X hareholder X , and  X  X oard of directors X . Ta-ble 8 gives an example of the corresponding related concepts identified by our method, and added to the vector represen-tation of document #9 of the Reuters data set. The evaluation was performed using the Wikipedia XML Corpus [7]. The Wikipedia XML Corpus contains processed Wikipedia data parsed into an XML format. Each XML file corresponds to an article in Wikipedia, and maintains the original ID, title and content of the corresponding Wikipedia article. Furthermore, each XML file keeps track of the linked article ID, for every redirect link and hyperlink contained in the original Wikipedia article.
We do not include all concepts of Wikipedia in the the-saurus. Some concepts, such as  X  X ist of ISO standards X  or  X 1960s X , do not contribute to the achievement of improved discrimination among documents. Thus, before building the thesaurus from Wikipedia, we remove concepts deemed not useful. To this end, we implement a few heuristics. First, Table 9: Number of terms, concepts, and links after filtering Terms in Wikipedia XML corpus 659,388 Concept After Filtering 495,214 Redirected Concepts 413 Categories 113,484 Relations in Wikipedia XML corpus 15,206,174 Category to Subcategory 145,468 Category to Concept 1,447,347
Concept to Concept 13,613,359 all concepts of Wikipedia which belong to categories related to chronology, such as  X  X ears X ,  X  X ecades X , and  X  X enturies X , are removed. Second, we analyze the titles of Wikipedia ar-ticles to decide whether they correspond to useful concepts. In particular, we implement the following rules: 1. If the title of an article is a multi-word title, we check 2. If the title is one word title, and it occurs in the article 3. Otherwise, the article is discarded.

After filtering Wikipedia concepts using these rules, we obtainedabout500,00 0conceptstobeinclud edinthethe-saurus. Table 9 provides a break down of the resulting num-ber of elements (terms, concepts, and links) used to build the thesaurus, and therefore our semantic kernels. In particu-lar, we note the limited number of redirected concepts (413). This is due to the fact that redirect links in Wikipedia often refers to the plural version of a concept, or to misspellings of a concept, and they are filtered out in the XML Corpus. Such variations of a concept, in fact, should not be added to the documents, as they would contribute only noise. For ex-ample, in Table 8, the synonyms associated to the candidate concepts  X  X hareholder X  and  X  X oard of visitors X  correspond to their plural versions. Thus, in practice they are not added to the documents.
We used four real data sets (Reuters-21578, OHSUMED, 20 Newsgroups, and Movies) to evaluate the performance of our approach for document classification. In the following, a description of each data set is provided. 1. Reuters-21578 [3] 1 . This collection of documents is
Available at http://www.daviddlewis.com/resources/ testcollections/reuters21578/ 2. OHSUMED [10] 2 . The OHSUMED test collection is 3. 20 Newsgroups (20NG) [14] 3 .The20News-4. Movie Reviews (Movies) [16] 4 . This collection con-
We used a Support Vector Machine (SVM) to learn models for the classification of documents. This choice was driven by the fact that SVMs have provided state-of-the-art results in the literature for text categorization. We conducted all the experiments using the software LIBSVM [4] 5 , and a lin-ear kernel. We compare the performance of three methods: 1. Baseline . The traditional BOW ( tf-idf )representation 2. Wiki-Enrich. This is the method proposed in [21].
Available via anonymous ftp from medir.ohsu.edu in the directory /pub/ohsumed
Available at http://people.csail.mit.edu/jrennie/ 20Newsgroups/
Available at http://www.cs.cornell.edu/people/pabo/movie-review-data
Software available at http://www.csie.ntu.edu.tw/  X  cjlin/ libsvm 3. Wiki-SK . This is our proposed approach. SK is for
We measured the performance of each method using the precision ratio , as defined in [23]: For the Reuters and OHSUMED data sets, we report both the micro-averaged and the macro-averaged precisions, since these data sets are multi-labeled, and the categories differ in size substantially. The micro-averaged precision operates at the document level, and is primarily affected by the cat-egorization accuracy of the larger categories. On the other hand, the macro-averaged precision averages results over all categories; thus, small categories have more influence on the overall performance.
For both methods Wiki-Enrich. and Wiki-SK the param-eters  X  1 and  X  2 of equation (1) were tuned according to the methology suggested in [21]. As a result, the values  X  1 =0 . 4 and  X  2 =0 . 5 were used in our experiments.

When retrieving the related concepts in our approach ( Wiki-SK ), for building the proximity matrix P (and the vector  X   X  ( d )), we consider the direct hyponyms, the syn-onyms, and the 10 closest associative concepts for each Wikipedia (candidate) concept found in a document. How-ever, not all candidate concepts present in a document are allowed to introduce related concepts. In order to identify the eligible candidate concepts, we calculate the cosine sim-ilarity between the tf-idf vector representation of the docu-ment containing the candidate concept, and the tf-idf vector representation of the Wikipedia article describing the same concept. Such similarity is computed for each candidate concept in a document. Only the top three to five candidate concepts that provide the highest similarities become the el-igible ones. The specific number (between three and five) is chosen based on the length of the document. This process can be seen as an extension, to all concepts, of the procedure for the disambiguation of concept senses (introduced in Sec-tion 4.2.1). This refinement proved to be effective in pruning concepts which do not express the focus of the topic being discussed in a document. Thus, it is successful in avoiding the expansion of the vector of terms with noisy features.
Table 10 shows the micro-averaged and the macro-averaged precision values obtained for the three methods on the four data sets. Our method Wiki-SK provides higher micro and macro precision values on all data sets. The im-provement with respect to the Baseline is significant for all four data sets. The largest improvements with respect to Wiki-Enrich. are obtained for the OHSUMED and 20NG data sets. In comparison with Wiki-Enrich. ,ourkernel-based method is capable of modeling relevant multi-word concepts as individual features, and of assigning meaning-ful strength values to them via our proximity matrix. Fur-thermore, our heuristic to filter concepts, and our selection mechanism to identify eligible candidate concepts success-fully avoid the introduction of noisy features. Overall, our results demonstrate the benefit and potential of embedding semantic knowledge into document representation by means of Wikipedia-based kernels.
To the best of our knowledge, this paper represents a first attempt to improve text classification by defining concept-based kernels using Wikipedia. Our approach overcomes the limitations of the bag-of-words approach by incorporat-ing background knowledge derived from Wikipedia into a semantic kernel, which is then used to enrich the content of documents. This methodology is able to keep multi-word concepts unbroken, it captures the semantic closeness to syn-onyms, and performs word sense disambiguation for polyse-mous terms.

We note that our approach to highlight the semantic con-tent of documents, from the definition of a proximity matrix, to the disambiguation of terms and to the identification of el-igible candidate concepts, is totally unsupervised, i.e. makes no use of the class labels associated to documents. Thus, the same enrichment procedure could be extended to enhance the clustering of documents, when indeed class labels are not available, or too expensive to obtain.

On the other hand, for classification problems where class labels are available, one could use them to facilitate the dis-ambiguation process, and the identification of crucial con-cepts in a document. Furthermore, class labels can be exploited to measure the correlation between terms and classes, and consequently define proper term weightings for the matrix R in equation (3). We plan to explore these avenues in our future work. This work was in part supported by NSF CAREER Award IIS-0447814. [1] L. AlSumait and C. Domeniconi. Local Semantic [2] R. Bunescu and M. Pasca. Using encyclopedic [3] Carnegie Group, Inc. and Reuters, Ltd. Reuters-21578 [4] C.-C. Chang and C.-J. Lin. LIBSVM: a library for [5] K. Dave, S. Lawrence, and D. M. Pennock. Mining the [6] M. de Buenega Rodriguez, J. M. Gomez-Hidalgo, and [7] L. Denoyer and P. Gallinari. The Wikipedia XML [8] E. Gabrilovich and S. Markovitch. Feature generation [9] E. Gabrilovich and S. Markovitch. Overcoming the [10] W. Hersh, C. Buckley, T. Leone, and D. Hickam. [11] A. Hotho, S. Staab, and G. Stumme. Wordnet [12] L. Jing, L. Zhou, M. K. Ng, and J. Z. Huang. [13] T. Joachims. Text categorization with support vector [14] K. Lang. Newsweeder: Learning to filter netnews. In [15] D. Milne, O. Medelyan, and I. H. Witten. Mining [16] B. Pang, L. Lee, and S. Vaithyanathan. Thumbs up? [17] J. Shawe-Taylor and N. Cristianini. Support Vector [18] J. Shawe-Taylor and N. Cristianini. Kernel Methods [19] G. Siolas and F. d X  X lch  X  e Buc. Support vector machines [20] L. A. Urena-Lopez, M. Buenaga, and J. M. Gomez. [21] P.Wang,J.Hu,H.-J.Zeng,L.Chen,andZ.Chen.
 [22] S. K. M. Wong, W. Ziarko, and P. C. N. Wong. [23] Y. Yang and J. Pedersen. A comparative study on
