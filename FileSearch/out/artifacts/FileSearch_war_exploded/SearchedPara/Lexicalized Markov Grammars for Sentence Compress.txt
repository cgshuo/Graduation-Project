 Sentence compression addresses the problem of re-moving words or phrases that are not necessary in the generated output of, for instance, summa-rization and question answering systems. Given the need to ensure grammatical sentences, a num-ber of researchers have used syntax-directed ap-proaches that perform transformations on the out-put of syntactic parsers (Jing, 2000; Dorr et al., 2003). Some of them (Knight and Marcu, 2000; Turner and Charniak, 2005) take an empirical ap-proach, relying on formalisms equivalent to proba-bilistic synchronous context-free grammars (SCFG) (Lewis and Stearns, 1968; Aho and Ullman, 1969) to extract compression rules from aligned Penn Tree-bank (PTB) trees. While their approach proved suc-cessful, their reliance on standard maximum like-lihood estimators for SCFG productions results in considerable sparseness issues, especially given the relative flat structure of PTB trees; in practice, many SCFG productions are seen only once. This problem is exacerbated for the compression task, which has only scarce training material available.
 In this paper, we present a head-driven Markovization of SCFG compression rules, an approach that was successfully used in syntactic parsing (Collins, 1999; Klein and Manning, 2003) to alleviate issues intrinsic to relative frequency estimation of treebank productions. Markovization for sentence compression provides several benefits, including the ability to condition deletions on a flexible amount of syntactic context, to treat head-modifier dependencies independently, and to lexicalize SCFG productions.

Another part of our effort focuses on better align-ment models for extracting SCFG compression rules from parallel data, and to improve upon (Knight and Marcu, 2000), who could only exploit 1.75% of the Ziff-Davis corpus because of stringent assump-tions about human abstractive behavior. To alleviate their restrictions, we rely on a robust approach for aligning trees of arbitrary document-abstract sen-tence pairs. After accounting for sentence pairs with both substitutions and deletions, we reached a reten-tion of more than 25% of the Ziff-Davis data, which greatly benefited the lexical probabilities incorpo-rated into our Markovized SCFGs.

Our work provides three main contributions: (1) Our lexicalized head-driven Markovization yields more robust probability estimates, and our compressions outperform (Knight and Marcu, 2000) according to automatic and human evaluation. (2) We provide a comprehensive analysis of the im-pact of different Markov orders for sentence com-pression, similarly to a study done for PCFGs (Klein and Manning, 2003). (3) We provide a framework for exploiting document-abstract sentence pairs that are not purely compressive, and augment the avail-able training resources for syntax-directed sentence compression systems. One successful syntax-driven approach (Knight and Marcu, 2000, henceforth K&amp;M) relies on syn-chronous context-free grammars (SCFG) (Lewis and Stearns, 1968; Aho and Ullman, 1969). SCFGs can be informally defined as context-free grammars (CFGs) whose productions have two right-hand side strings instead of one, namely source and target right-hand side. In the case of sentence compres-sion, we restrict the target side to be a sub-sequence of the source side (possibly identical), and we will call this restricted grammar a deletion SCFG . For in-stance, a deletion SCFG rule that removes an adver-bial phrase (ADVP) between an noun phrase (NP) and a verb phrase (VP) may be written as follows:
In a sentence compression framework similar to the one presented by K&amp;M, we build SCFGs that are fully trainable from a corpus of document and reduced sentences. Such an approach comprises two subproblems: (1) transform tree pairs into syn-chronous grammar derivations; (2) based on these derivations, assign probabilities to deletion SCFG productions, and more generally, to compressions produced by such grammars. Since the main point of our paper lies in the exploration of better probability estimates through Markovization and lexicalization of SCFGs, we first address the latter problem, and discuss the task of building synchronous derivations only later in Section 4. 2.1 Stochastic Synchronous Grammars The overall goal of a sentence compression system is to transform a given input sentence f into a concise and grammatical sentence c  X  C , which is a sub-sequence of f . Similarly to K&amp;M and many suc-cessful syntactic parsers (Collins, 1999; Klein and Manning, 2003), our sentence compression system is generative , and attempts to find the optimal com-pression  X  c by estimating the following function: 1  X  c = arg max If  X  ( f , c ) is the set of all tree pairs that yield ( f , c ) according to some underlying SCFG, we can esti-mate the probability of the sentence pair using: We note that, in practice (and as in K&amp;M), Equa-tion 2 is often approximated by restricting  X  ( f , c ) to a unique full tree  X   X  f , the best hypothesis of an off-the-shelf syntactic parser. This implies that each possible compression c is the target-side yield of at most one SCFG derivation.

As in standard PCFG history-based models, the probability of the entire structure (Equation 2) is fac-tored into probabilities of grammar productions. If  X  is a derivation  X  = r 1  X   X   X   X   X  r j  X   X   X   X  r J , where r j denotes the SCFG rule l j  X   X   X  j
The question we will now address is how to esti-mate the probability p (  X  j f , X  j c | l j ) of each SCFG pro-duction. 2.2 Lexicalized Head-Driven Markovization of A main issue in our enterprise is to reliably estimate productions of deletion SCFGs. In a sentence com-pression framework as the one presented by K&amp;M, we use aligned trees of the form of the Penn Tree-bank (PTB) (Marcus et al., 1994) to acquire and score SCFG productions. However, the use of the PTB structure faces many challenges also encoun-tered in probabilistic parsing.
Firstly, PTB tree structures are relatively flat, par-ticularly within noun phrases. For instance, adjec-tive phrases (ADJP) X  X hich are generally good can-didates for deletions X  X ppear in 90 different NP-rooted SCFG productions in Ziff-Davis, 2 61 of which appear only once, e.g., NP  X   X  DT ADJP JJ NN NN, DT JJ NN NN  X  . While it may seem ad-vantageous to maintain many constituents within the same domain of locality of an SCFG production, as we may hope to exploit its large syntactic context to condition deletions more accurately, the sparsity of such productions make them poor candidates for rel-ative frequency estimation, especially in a task with limited quantities of training material. Indeed, our base training corpus described in Section 4 contains only 951 SCFG productions, 593 appearing once.
Secondly, syntactic categories in the PTB are par-ticularly coarse grained, and lead to many incorrect context-free assumptions. Some important distinc-tions, such as between arguments and adjuncts, are beyond the scope of the PTB annotation, and it is often difficult to determine out of context whether a given constituent can safely be deleted from a right-hand side.

One first type of annotation that can effectively be added to each syntactic category is its lexical head and head part-of-speech (POS), following work in syntactic parsing (Collins, 1999). This type of an-notation is particular beneficial in the case of, e.g., prepositional phrases (PP), which may be either complement or adjunct. As in the case of Figure 1 (in which adjuncts appear in italic), knowing that the PP headed by  X  X rom X  appears in a VP headed by  X  X ell X  helps us to determine that the PP is a com-plement to the verb  X  X ell X , and that it should pre-sumably not be deleted. Conversely, the PP headed by  X  X ecause X  modifying the same verb is an adjunct, and can safely be deleted if unimportant. 3 Also, as discussed in (Klein and Manning, 2003), POS an-notation can be useful as a means of backing off to more frequently occurring head-modifier POS oc-currences (e.g., VBD-IN) when specific bilexical co-occurrences are sparsely seen (e.g.,  X  X ell X - X  X rom X ). At a lower level, lexicalization is clearly desirable for pre-terminals. Indeed, current SCFG models such as K&amp;M have no direct way of preventing highly improbable single word removals, such as deletions of adverbs  X  X ever X  or  X  X owhere X , which may turn a negative statement into a positive one. 4
A second type of annotation that can be added to syntactic categories is the so-called parent annota-tion (Johnson, 1998), which was effectively used in syntactic parsing to break unreasonable context-free assumptions. For instance, a PP with a VP parent is marked as PP X VP. It is reasonable to assume that, e.g., that constituents deep inside a PP have more chances to be removed than otherwise expected, and one may seek to increase the amount of vertical context that is available for conditioning each con-stituent deletion.

To achieve the above desiderata for better SCFG probability estimates X  X .e., reduce the amount of sister annotation within each SCFG production, by conditioning deletions on a context smaller than an entire right-hand side, and at the same time in-crease the amount of ancestor and descendent an-notation through parent (or ancestor) annotation and lexicalization X  X e follow the approach of (Collins, 1999; Klein and Manning, 2003), i.e., factor-ize n -ary grammar productions into products of n right-hand side probabilities, a technique sometimes called Markovization .

Markovization is generally head-driven, i.e., re-flects a decomposition centered around the head of each CFG production: where H is the head, L 1 ,...,L m the left modi-fiers, R 1 ,...,R n are right modifiers, and  X  termi-nation symbols needed for accurate probability es-timations (e.g., to capture the fact that certain con-stituents are more likely than others to be the right-most constituent); for simplicity, we will ignore  X  in later discussions. For a given SCFG production l  X   X   X  f , X  c  X  , we ask, given the source RHS  X  f that is assumed given (e.g., provided by a syntactic parser), which of its RHS elements are also present in  X  c . That is, we write: where k h ,k i l ,k j r ( X  X  X  for keep) are binary variables that are true if and only if constituents H,L i ,R j (re-spectively) of the source RHS  X  f are present in the target side  X  c . Note that the conditional probabil-ity in Equation 5 enables us to estimate Equation 3, rely on a state-of-the-art probabilistic parser to ef-fectively compute either p (  X  f | l ) or the probability of the entire tree  X  f , and need not worry about esti-mating this term. In the case of sentence compres-sion from the one-best hypothesis of the parser, we can ignore p (  X  f | l ) altogether, since  X  f is the same for all compressions.

We can rewrite Equation 5 exactly using a head-driven infinite-horizon Markovization: where  X  = ( k 1 l ,  X   X   X  ,k m l ) is a term needed by the chain rule. One key issue is to make linguistically plausible assumptions to determine which condi-tioning variables in the terms should be deleted. Fol-lowing our discussion in the first part of this section, we may start by making an order-s Markov approx-imation centered around the head, i.e., we condi-tion each binary variable (e.g., k i r ) on a context of up to s sister constituents between the current con-stituent and the head (e.g., ( R i  X  s ,...,R i ) ). In or-der to incorporate bilexical dependencies between the head and each modifier, we also condition all modifier probabilities on head variables H (and k h ). These assumptions are overall quite similar to the ones made in Markovized parsing models. If we as-sume that all other conditioning variables in Equa-tion 6 are irrelevant, we write: p (  X  c |  X  f ,l ) = p h ( k h | H,l ) (7) Note that it is important to condition deletions on both constituent histories ( R i  X  s ,...,R i ) and non-deletion histories ( k i  X  s r ,...,k i  X  1 r ) ; otherwise we would be unable to perform deletions that must op-erate jointly, as in production S  X   X  ADVP COMMA NP VP, NP VP  X  (in which the ADVP should not be deleted without the comma). Without binary his-tories, we often observed superfluous punctuation symbols and dangling coordinate conjunctions ap-pearing in our outputs.

Finally, we label l with an order-v ancestor anno-tation, e.g., for the VP in Figure 1, l = for v = 0 , l = VP X S for v = 2 , and so on. We also replace H and modifiers L i and R i by lexicalized entries, e.g., H = (VP,VBD, fell ) and R i = (PP,IN, from ). Note that to estimate p l ( k i l |  X   X   X  ) , we only lexicalize L and H , and none of the other conditioning modifiers, since this would, of course, introduce too many con-ditioning variables (the same goes for p r ( k i r |  X   X   X  ) ). The question of how much sister and vertical ( s and v ) context is needed for effective sentence compres-sion, and whether to use lexical or POS annotation, will be evaluated in detail in Section 5. To acquire SCFG productions, we used Ziff-Davis, a corpus of technical articles and human abstractive summaries. Articles and summaries are paired by document, so the first step was to perform sentence alignment. In the particular case of sentence com-pression, a simple approach is to just consider com-pression pairs ( f , c ), where c is a substring of f . K&amp;M identified only 1,087 such paired sentences in the en-tire corpus, which represents a recall of 1.75%.
For our empirical evaluations, we split the data as follows: among the 1,055 sentences that were taken to train systems described in K&amp;M, we selected the first 32 sentence pairs to be an auxiliary test corpus (for future work), the next 200 sentences to be our development corpus, and the remaining 823 to be our base training corpus (ZD-0), which will be aug-mented with additional data as explained in the next section. We feel it is important to use a relatively large development corpus, since we will provide in Section 5 detailed analyses of model selection on the development set (e.g., by evaluating different Markov structures), and we want these findings to be as significant as possible. Finally, we used the same test data as K&amp;M for human evaluation pur-poses (32 sentence pairs). We now describe methods to train SCFG models from sentence pairs. Given a tree pair ( f , c ) , whose respective parses (  X  f , X  c ) were generated by the parser described in (Charniak and Johnson, 2005), the goal is to transform the tree pair into SCFG derivations, in order to build relative frequency es-timates for our Markovized models from observed SCFG productions. Clearly, the two trees may sometimes be structurally quite different (e.g., a given PP may attach to an NP in  X  f , while attach-ing to VP in  X  c ), and it is not always possible to build an SCFG derivation given the constraints in (  X  f , X  c ) . The approach taken by K&amp;M is to analyze both trees and count an SCFG rule whenever two nodes are  X  X eemed to correspond X , i.e., roots are the same, and  X  c is a sub-sequence of  X  f . This leads to a quite restricted number of different productions on our base training set (ZD-0): 823 different pro-ductions were extracted, 593 of which appear only once. This first approach has serious limitations; the assumption that sentence compression appropri-ately models human abstractive data is particularly problematic. This considerably limits the amount of training data that can be exploited in Ziff-Davis (which contains overall more than 4,000 documents-abstract pairs), and this makes it very difficult to train lexicalized models.

An approach to slightly loosen this assumption is to consider document-abstract sentence pairs in which the condensed version contains one or more substitutions or insertions. Consider for example the tree pair in Figure 2: the two sentences are syn-tactically very close, but the substitution of  X  X om-puter X  with  X  X nit X  makes this sentence pair unus-able in the framework presented in K&amp;M. Arguably, there should be ways to exploit abstract sentences that are slightly reworded in addition to being com-pressed. To use sentence pairs with insertions and substitutions, we must find a way to align tree pairs in order to identify SCFG productions. More specif-ically, we must define a constituent alignment be-tween the paired abstract and document sentences, which determine how the two trees are synchronized in a derivation. Obtaining this alignment is no triv-ial matter as the number of non-deleting edits in-creases. To address this, we synchronized tree pairs by finding the constituent alignment that minimizes the edit distance between the two trees, i.e., mini-mize the number of terminals and non-terminals in-sertions, substitutions and deletions. 5 While criteria other than minimum tree edit distance may be effec-tive, we found X  X fter manual inspections of align-ments between sentences with less than five non-deleting edits X  X hat this method generally produces good alignments. A sample alignment is provided in Figure 2. Once a constituent alignment is available, it is then trivial to extract all deletion SCFG rules available in a tree pair, e.g., NP  X   X  DT JJ NN, DT JJ NN  X  in the figure.

We also exploited more general tree productions known as synchronous tree substitution grammar (STSG) rules, in an approach quite similar to (Turner and Charniak, 2005). For instance, the STSG rule rooted at S can be decomposed into two SCFG pro-ductions if we allow unary rules such as VP  X  VP to be freely added to the compressed tree. More specif-ically, we decompose any STSG rule that has in its target (compressed) RHS a single context free pro-duction, and that contains in its source (full) RHS a single context free production adjoined with any number of tree adjoining grammar (TAG) auxiliary trees (Joshi et al., 1975). In the figure, the initial tree is S  X  NP VP, and the adjoined (auxiliary) tree is VP  X  VP CC VP. 6 We found this approach quite helpful, since most useful compressions that mimic TAG adjoining operations are missed by the extrac-tion procedure of K&amp;M.

Since we found that exploiting sentence pairs con-taining insertions had adverse consequences in terms of compression accuracies, we only report experi-ments with sentence pairs containing no insertions. We gathered sentence pairs with up to six substi-tutions using minimum edit distance matching (we will refer to these sets as ZD-0 to ZD-6). With a limit of up to six substitutions (ZD-6), we were able to train our models on 16,787 sentences, which rep-resents about 25% of the total number of summary sentences of the Ziff-Davis corpus. All experiments presented in this section are per-formed on the Ziff-Davis corpus. We note first that all probability estimates of our Markovized gram-mars are smoothed. Indeed, incorporating lexical dependencies within models trained on data sets as small as 16,000 sentence pairs would be quite fu-tile without incorporating robust smoothing tech-niques. Different smoothing techniques were eval-uated with our models, and we found that interpo-lated Witten-Bell discounting was the method that performed best. We used relative frequency es-timates for each of the models presented in Sec-tion 2.2 (i.e., p h ,p l ,p r ), and trained p l separately from p r . We interpolated our most specific models (lexical heads, POS tags, ancestor and sister annota-tion) with lower-order models. 7
Automatic evaluation on development sets is per-formed using word-level classification accuracy, i.e., the number of words correctly classified as being either deleted or not deleted, divided by the to-tal number of words. In our first evaluation, we experimented with different horizontal and vertical Markovizations (Table 1). First, it appears that ver-tical annotation is moderately helpful. It provides gains in accuracy ranging from .5% to .9% for v = 1 over a simpler models ( v = 0 ), but higher orders ( v&gt; 1 ) have a tendency to decrease performance. On the other hand, sister annotation of order 1 is much more critical, and provides 4.1% improvement over a simpler model ( s = 0 ,v = 0 ). Manual exami-nations of compression outputs confirmed this anal-ysis: without sister annotation, deletion of punctu-ation and function words (determiners, coordinate conjunctions, etc.) is often inaccurate, and compres-sions clearly lack fluency. This annotation is also helpful for phrasal deletions; for instance, we found that PPs are deleted in 31.4% of cases in Ziff-Davis if they do not immediately follow the head con-stituent, but this percentage drops to 11.1% for PPs that immediately follow the head. It seems, how-ever, that increasing sister annotation beyond s&gt; 1 only provide limited improvements.

In our second evaluation reported in Table 2, we assessed the usefulness of lexical and POS anno-tation (setting s and v to 0). In the table, we use M to denote any of the modifiers L i or R i , and c , t , w respectively represent syntactic constituent, POS, and lexical conditioning. While POS annota-tion is clearly advantageous compared to using only syntactic categories, adding lexical variables to the model also helps. As is shown in the table, it is es-pecially important to know the lexical head of the modifier we are attempting to delete. The addition of w m to conditioning variables provides an improve-ment of 1.3% (from 66.5% to 67.8%) on our op-timal Ziff-Davis training corpus (ZD-6). Further-more, bilexical head-modifier dependencies provide a relatively small improvement of .5% (from 69.8% to 70.3%) over the best model that does not incor-porate the lexical head w h . Note that lexical con-ditioning also helps in the case where the training data is relatively small (ZD-0), though differences are less significant, and bilexical dependencies actu-ally hurt performance. In subsequent experiments, we experimented with different Markovizations and lexical dependency combination, and finally settled with a model ( s = 1 and v = 1 ) incorporating all conditioning variables listed in the last line of Ta-ble 2. This final tuning was combined with human inspection of generated outputs, since certain modi-fications that positively impacted output quality sel-dom changed accuracies.

We finally took the best configuration selected above, and evaluated our model against the noisy-channel model of K&amp;M on the 32 test sentences se-lected by them. We performed both automatic and human evaluation against the output produced by Knight and Marcu X  X  original implementation of their noisy channel model (Table 3). In the former case, we also provide Simple String Accuracies (SSA). 8 For human evaluation, we hired six native-speaker judges who scored grammaticality and content (im-portance) with scores from 1 to 5, using instructions as described in K&amp;M. Both types of evaluations fa-vored our Markovized model against the noisy chan-nel model.

Table 4 shows several outputs of our system (Markov) that significantly differed from the output of the noisy channel model (NoisyC), which con-firms our finding that Markovized models can pro-duce quite grammatical output. Our compression for the first sentence underlines one of the advantages of constituent-based classifiers, which have the ability of deleting a very long phrase (here, a PP) at once. The three next sentences display some advantages of our approach over the K&amp;M model: here, the lat-ter model performs deletion with too little lexico-syntactic information, and accidentally removes cer-tain modifiers that are sometimes, but not always, good candidates for deletions (e.g., ADJP in Sen-tence 2, PP in sentences 3 and 4). On the other hand, our model keeps these constituent intact. Finally, the fifth and last example is one of the only three cases (among the 32 sentences) where our model produced a sentence we judged clearly ungrammatical. After inspection, we found that our parser assigned par-ticularly errorful trees to those inputs, which may partially explain these ungrammatical outputs. A relatively large body of work addressed the prob-lem of sentence compression. One successful recent approach (McDonald, 2006) combines a discrimi-native framework with a set of features that cap-ture information similar to the K&amp;M model. Mc-Donald X  X  features include compression bigrams, as well as soft syntactic evidence extracted from parse trees and dependency trees. The strength of McDon-ald X  X  approach partially stems from its robustness against redundant and noisy features, since each fea-ture is weighted proportionally to its discriminative power, and his approach is thus hardly penalized by uninformative features. In contrast, our work puts much more emphasis on feature analysis than on efficient optimization, and relies on a statisti-cal framework (maximum-likelihood estimates) that strives for careful feature selection and combination. It also describes and evaluates models incorporating syntactic evidence that is new to the sentence com-pression literature, such as head-modifier bilexical dependencies, and n th-order sister and vertical an-notation. We think this work leads to a better un-derstanding of what type of syntactic and lexical ev-idence makes sentence compression work. Further-more, our work leaves the door open to uses of our factored model in a constituent-based or word-based discriminative framework, in which each elemen-tary lexico-syntactic structure of this paper can be discriminatively weighted to directly optimize com-pression quality. Since McDonald X  X  approach does not incorporate SCFG deletion rules, and conditions deletions on less lexico-syntactic context, we believe this will lead to levels of performance superior to both papers. We presented a sentence compression system based on SCFG deletion rules, for which we defined a head-driven Markovization formulation. This Markovization enabled us to incorporate lexical con-ditioning variables into our models. We empirically evaluated different Markov structures, and obtained a best system that generates particularly grammati-cal sentences according to a human evaluation. Our sentence compression system is freely available for research and educational purposes.
 We would like to thank Owen Rambow, Michael Collins, Julia Hirschberg, and Daniel Ellis for their helpful comments and suggestions.

