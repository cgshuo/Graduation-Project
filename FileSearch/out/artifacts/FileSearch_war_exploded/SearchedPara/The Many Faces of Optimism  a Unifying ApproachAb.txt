 Istv  X an Szita szityu@gmail.com Andr  X as L  X orincz andras.lorincz@elte.hu Reinforcement learning (RL) is the art of maximizing long-term rewards in a stochastic, unknown environ-ment. In the construction of RL algorithms, the choice of exploration strategy is of central significance. We shall examine the problem of exploration in the Markov decision process (MDP) framework. While simple methods like  X  -greedy and Boltzmann explo-ration are commonly used, it is known that their be-havior can be extremely poor (Koenig &amp; Simmons, 1993). Recently, a number of efficient exploration al-gorithms have been published, and for some of them, formal proofs of efficiency also exist. We review these methods in Section 2. By combining ideas from several sources, we construct a new algorithm for efficient ex-ploration. The new algorithm, optimistic initial model ( OIM ), is described in Section 3. In Section 4, we show that many of the advanced algorithms, including ours, can be treated in a unified way. We use this fact to sketch a proof that OIM finds a near-optimal policy in polynomial time with high probability. Section 5 provides experimental comparison between OIM and a number of other methods on some benchmark prob-lems. Our results are summarized in Section 6. In the rest of this section, we review the necessary prelimi-naries, Markov decision processes and the exploration task. 1.1. Markov Decision Processes (MDPs) Markov decision processes are the standard framework for RL, and the basis of numerous extensions (like continuous MDPs, partially observable MDPs or fac-tored MDPs). An MDP is characterized by a quintuple ( X, A, R , P,  X  ), where X is a finite set of states; A is a finite set of possible actions; R : X  X  A  X  X  X  X  R is the reward distribution, R ( x, a, y ) denotes the mean value of R ( x, a, y ), P : X  X  A  X  X  X  [0 , 1] is the transition function; and finally,  X   X  [0 , 1) is the discount rate on future rewards. We shall assume that all rewards are A (stationary) policy of the agent is a mapping  X  : X  X  A  X  [0 , 1]. For any x 0  X  X , the policy of the agent and the parameters of the MDP determine a stochastic process experienced by the agent through the instantiation x 0 , a 0 , r 0 , x 1 , a 1 , r 1 , . . . , x The goal is to find a policy that maximizes the expected value of the discounted total re-ward. Let us define the state-action value function (value function for short) of  X  as Q  X  ( x, a ) := E optimal value function as for each ( x, a )  X  X  X  A . Let the greedy action at x w.r.t. value function Q be a Q x := arg max a Q ( x, a ). The greedy policy of Q deterministically takes the greedy action in each state. It is well-known that the greedy policy of Q  X  is an optimal policy and Q  X  satis-fies the Bellman equations: Q  X  ( x, a ) = 1.2. The Exploration Problem In the classical reinforcement learning setting, it is as-sumed that the environment can be modelled as an MDP, but its parameters (that is, P and R ) are un-known to the agent, and she has to collect information by interacting with the environment. If too little time is spent with the exploration of the environment, the agent will get stuck with a suboptimal policy, without knowing that there exists a better one. On the other hand, the agent should not spend too much time vis-iting areas with low rewards and/or accurately known parameters.
 What is the optimal balance between exploring and exploiting the acquired knowledge and how could the agent concentrate her exploration efforts? These ques-tions are central for RL. It is known that the optimal exploration policy in an MDP is non-Markovian, and can be computed only for very simple tasks like k -armed bandit problems. Here we give a short review about some of the most important exploration methods and their properties. 2.1.  X  -greedy and Boltzmann Exploration The most popular exploration method is  X  -greedy ac-tion selection. The method works without a model, only an approximation of the action value function Q ( x, a ) is needed. The agent in state x selects the greedy action a Q x or an explorative move with a ran-dom action with probabilities 1  X   X  and  X  , respectively. Sooner or later, all paths with nonzero probability will have been visited many times, so, a suitable learning algorithm can learn to choose the optimal path. It is known, for example, that Q-learning with nonzero ex-ploration converges to the optimal value function with probability 1 (Littman &amp; Szepesv  X ari, 1996), and so does SARSA (Singh et al., 2000), if the exploration rate diminishes according to an appropriate schedule. Boltzmann exploration selects actions as follows: the probability of choosing action a is exp where  X  X emperature X  T ( &gt; 0) regulates the amount of explorative actions. Convergence results of the  X  -greedy method carry through to this case.
 Unfortunately, for the  X  -greedy and the Boltzmann method, exploration time may scale exponentially in the number of states (Koenig &amp; Simmons, 1993). 2.2. Optimistic Initial Values (OIV) One may boost exploration with a simple trick: the initial value of each state action pair can be set to some overwhelmingly high number. If a state x is vis-ited often, then its estimated value will become more exact, and therefore, lower. Thus, the agent will try to reach the more rarely visited areas, where the esti-mated state values are still high. This method, called  X  X xploring starts X  or  X  X ptimistic initial values X , is a popular exploration heuristic (Sutton &amp; Barto, 1998), sometimes combined with others, e.g., the  X  -greedy ex-ploration method. Recently, Even-Dar and Mansour (2001) gave theoretical justification for the method: they proved that if the optimistic initial values are suf-ficiently high, Q-learning converges to a near-optimal solution. One apparent disadvantage of OIV is that if initial estimations are too high, then it takes a long to fix them. 2.3. Bayesian Methods We may assume that the MDP (with the unknown values of P and R ) is drawn from a parameterized distribution M 0 . From the collected experience and the prior distribution M 0 , we can calculate succes-sive posterior distributions M t , t = 1 , 2 , . . . by Bayes X  rule. Furthermore, we can calculate (at least in prin-ciple) the policy that minimizes the uncertainty of the parameters (Strens, 2000). Dearden (2000) approx-imates the distribution of state values directly. Ex-act computation of the optimal exploration policy is infeasible and Bayesian methods are computationally demanding even with simplifying assumptions about the distributions, e.g., the independencies of certain parameters. 2.4. Confidence Interval Estimation Confidence interval estimation algorithms are between Bayesian exploration and OIV. It assumes that each state value is drawn from an independent Gaussian distribution and it computes the confidence interval of the state values. The agent chooses the action with the highest upper confidence bound. Initially, all con-fidence intervals are very wide, and shrink gradually towards the true state values. Therefore, the behavior of the technique is similar to OIV. The IEQL+ method of Meuleau and Bourgine (1999) directly estimates confidence intervals of Q -values, while Wiering and Schmidhuber (1998) calculate confidence intervals for P and R , and obtain Q -value bounds indirectly. Strehl and Littman (2006) improve the method and prove a polynomial-time convergence bound. Both algorithms are called model-based interval estimation . To avoid confusion, we will refer to them as MBIE(WS) and MBIE(SL).
 Auer and Ortner (2006) give a confidence interval-based algorithm, for which the online regret is only logarithmic in the number of steps taken. 2.5. Exploration Bonus Methods The agent can be directed towards less-known parts of the state space by increasing the value of  X  X nteresting X  states artificially with bonuses. States can be interest-ing given their frequency , recency , error , etc. (Meuleau &amp; Bourgine, 1999; Wiering &amp; Schmidhuber, 1998). The balance of exploration and exploitation is usually set by a scaling factor  X  , so that the total immediate reward of the agent at time t is r t +  X   X  b t ( x t , a t where b t is one of the above listed bonuses. The bonuses are calculated by the agent and act as intrin-sic motivating forces. Exploration bonuses for a state can vary swiftly and model-based algorithms (like pri-oritized sweeping or Dyna) are used for spreading the changes effectively. Alas, the weight of exploration  X  needs to be annealed according to a suitable schedule. Alternatively, the agent may learn two value functions separately: a regular one, Q r t which is based on the rewards r t received from the environment, and an ex-ploration value function Q e t which is based on the ex-ploration bonuses. The agent X  X  policy will be greedy the exploration mechanism may remain the same, but several advantages appear. First of all, the changes in  X  take effect immediately . As an example, we can immediately switch off exploration by setting  X  to 0. Confidence interval estimation can be phrased as an exploration bonus method: see IEQL+ (Meuleau &amp; Bourgine, 1999) or MBIE-EB (Strehl &amp; Littman, 2006). Even-Dar and Mansour (2001) have shown that  X  -greedy and Boltzmann explorations can be formu-lated as exploration bonus methods although rewards are not propagated through the Bellman equations. 2.6. E 3 and R-max The Explicit explore or exploit ( E 3 ) algorithm of Kearns and Singh (1998) and its successor, R-max (Brafman &amp; Tennenholtz, 2001) were the first algo-rithms that have polynomial time bounds for finding near-optimal policies. R-max collects statistics about transitions and rewards. When visits to a state enable high precision estimations of real transition probabili-ties and rewards then state is declared known . R-max also maintains an approximate model of the environ-ment. Initially, the model assumes that all actions in all states lead to a (hypothetical) maximum-reward absorbing state. The model is updated each time when a state becomes known. The optimal policy of the model is either the near-optimal policy in the real en-vironment or enters a not-yet-known state and collects new information. Our agent starts with a simple, but overly optimistic model. By collecting new experiences, she updates her model, which becomes more realistic. The value function is computed over the approximate model with (asynchronous) dynamic programming. The agent al-ways chooses her action greedily w.r.t. her value func-tion. Exploration is induced by the optimism of the model: unknown areas are believed to yield large re-wards. Algorithmic components are detailed below. Separate exploration values. Similarly to the ap-proach of Meuleau and Bourgine (1999), we shall sep-arate the  X  X rue X  state values from exploration values. Formally, the value function has the form for all ( x, a )  X  X  X  A , where Q r and Q e will summarize external and exploration rewards, respectively.  X  X arden of Eden X  state. Similarly to R-max, we introduce a new hypothetical  X  X arden of Eden X  state x
E , and assume an extended state space X 0 = X  X  { x
E } . Once there, then, according to the inherited model, the agent remains in x E indefinitely and re-ceives R max reward for every step, which may ex-ceed R 0 max =: max x,a,y R ( x, a, y ), the maximal reward of the original environment.
 Model approximation. The agent builds an approx-imate model of the environment. For each x, y  X  X note the number of times when a was selected in x up to step t , the number of times when transition x a  X  y was experienced, and the sum of external rewards for x  X  y transitions, respectively. With these notations, the approximate model parameters are  X  P ( x, a, y ) = Suitable initializations of N t ( x, a ), N t ( x, a, y ) and C ( x, a, y ) will ensure that the ratios are well-defined everywhere. The exploration rewards are defined as for each x, y  X  X  X  X  x E } , a  X  A , and are not modified during the course of learning.
 Optimistic initial model. The initial model as-sumes that x E has been reached once for each state-action pairs: for each x  X  X  X  X  x E } , y  X  X and a  X  A , Then, the optimal initial value function equals Q 0 ( x, a ) = Q r 0 ( x, a )+ Q e 0 ( x, a ) = 0+ for each ( x, a )  X  X 0  X  A , analogously to OIV. Dynamic programming. Both value functions can be updated using the approximate model. For each x  X  X , let a x be the greedy action according to the combined value function, i.e., The dynamic programming equations for the value function components are Q t +1 ( x, a ):= Q t +1 ( x, a ):=  X  Episodic tasks can be handled as usual way; we intro-duce an absorbing final state with 0 external reward. Asynchronous update. The algorithm can be on-line, if instead of full update sweeps over the state space updates are limited to state set L t in the  X  X eigh-borhood X  of the agent X  X  current state. Neighborhood is restricted by computation time constraints; any asyn-chronous dynamic programming algorithm suffices. It is implicitly assumed that the current state is always updated, i.e., x t  X  L t . In this paper, we used the im-proved prioritized sweeping algorithm of Wiering and Schmidhuber (1998).
 Putting it all together. The method is summarized as Algorithm 1. In the first part of this section, we analyze the similari-ties and differences between various exploration meth-ods, with an emphasis on OIM . Based on this analy-sis, we sketch the proof that OIM finds a near-optimal policy in polynomial time. Details of the proof can be found in (Szita &amp; L  X orincz, 2008). 4.1. Relationship to Other Methods  X  X ptimism in the face of uncertainty X  is a common point in exploration methods: the agent believes that she can obtain extra rewards by reaching the unex-plored parts of the state space.
 Note that as far as the combined value function Q is concerned, OIM is an asynchronous dynamic program-ming method augmented with model approximation. Optimistic initial values. Apparently, OIM is the model-based extension of the OIV heuristic. Note however, that optimistic initialization of Q -values is not effective with a model: the more updates are made, the less effect the initialization has and it fully dimin-ishes if value iteration is run until convergence. There-fore, naive combination of OIV and model construction is contradictory: the number of DP-updates should be kept low in order to save the initial boost, but it should be as high as possible in order to propagate the real rewards quickly.
 OIM resolves this paradox by moving the optimism into the model. The optimal value function of the initial model is Q 0  X  V max , corresponding to OIV. However, DP updates can not, but only model updates may lower the exploration boost.
 Note that we can set the initial model value as high as we like, but we do not have to wait until the initial boost diminishes, because Q r and Q e are separated. R-max. The  X  X arden of Eden X  state x E of OIM is identical to the fictitious max-reward absorbing state of R-max (and E 3 ). In both cases, the agent X  X  model tells that all unexplored ( x, a ) pairs lead to x E R-max , however, updates the model only when the transition probabilities and rewards are known with high precision, which is only after many visits to ( x, a ). In contrast, OIM updates the model after each single visit, employing each bit of experience as soon as it is obtained. As a result, the approximate model can be used long before it becomes accurate.
 Exploration bonus methods. The extra reward offered by the Garden of Eden state can be un-derstood as an exploration bonus: for each visit of the pair ( x, a ), the agent gets the bonus b t ( x, a ) = this formula with those of the other methods like the frequency-based bonus b t =  X   X   X  N t ( x, a ) or the error-based bonus b t =  X   X  Model-based interval exploration. The explo-ration bonus form of the MBIE method of Strehl and Littman (2005) sets b t =  X  N an ad-hoc method: the form of the bonus comes from Algorithm 1 The Optimistic initial model algorithm Model initialization: t := 0;  X  x, y  X  X,  X  a  X  A : repeat ) := N ( x t , a t , x t +1 ) + 1; N ( x t , a t ) := N ( x )  X 
P ( x, a, y ) Q e t ( y, a y ) . until Bellman-error &gt;  X  confidence interval estimations. The comparison to MBIE-EB will be especially valuable, as it converges in polynomial-time and the proof can be transported to OIM with slight modifications. 4.2. Polynomial-time Convergence Theorem 4.1 There exists a constant C so that OIM converges almost surely to a near-optimal policy in polynomial time if started with R probability 1  X   X  , the number of timesteps where O  X  For the sketch of the proof, we shall follow the tech-nique of Kearns and Singh (2002) and Strehl and Littman (2006), and will use the shorthands [KS] and [SL] for referring to them. See ( ? ) for the detailed proof with a slightly better polynomial bound. A pair ( x, a ) is declared known, if it has been visited at constant C . OIM preserves the optimism of the value function: Lemma 4.2 Let Q t be the sequence of Q -functions generated by OIM . Then, it holds with probability 1  X   X / 2 that for any t , Q t ( x, a )  X  Q  X  ( x, a )  X   X  Proof: According to [SL], with probability 1  X   X / 2,
X where  X  := R 0 max / (1  X   X  ) We will show that
R max / ( N t ( x, a )(1  X   X  )) +  X  2  X   X / l.h.s. and we can omit the second term (and prove the stricter inequality). If the relation is reversed, then the first term can be omitted. In both cases, we arrive at the requirement R max  X  3  X  which holds by assumption.
 At step t , a number of DP updates are carried out. We proceed by induction on the number of DP-updates.
P  X   X  Q  X  ( x, a )  X   X /  X  Q  X  ( x, a )  X   X  X  1  X   X  2 = Q  X  ( x, a )  X   X  1 , where we applied (1), (2), the induction assumption and the definition of  X  2 .  X  Let M denote the true (and unknown) MDP, let  X  M be the approximate model of OIM , and define  X  M so that it is identical to M for known pairs, and equals  X  M for unknown pairs. The parameters of  X  M and  X  M are nearly identical: if ( x, a ) becomes known, then the local values of  X  P and  X  R are O (  X  | X | HR 0 approximations of P and R with probability 1  X   X / 2 (Lemma 5 of [KS]). Therefore, by Lemma 4 of [KS], Define the H -step truncated value function of policy  X  as Q  X  ( x, a, H ) := E  X  any ( x, a ),  X  and any MDP M with discount factor  X  . Consider a state-action pair ( x 1 , a 1 ) and a H -step long trajectory generated by  X  . Let A M be the event that an unknown pair ( x, a ) is encountered along the tra-jectory. Then, by Lemma 3 of [SL], To conclude the proof, we separate two cases (following the line of thoughts of Theorem 1 in [SL]). In the first case, an exploration step will occur with high prob-ability: Let V 0 max := R 0 max / (1  X   X  ). Suppose that Pr( A M ) &gt;  X  1 /V 0 max , that is, an unknown pair ( x, a ) is visited in H steps with high probability. This can happen at most m | X || A | times, so by the Hoeffding-Azuma bound, with probability 1  X   X / 2, all ( x, a ) will steps.
 On the other hand, if Pr( A M )  X   X  1 /V 0 max , then the policy is near-optimal with probability 1  X   X  : where we applied (in this order) the property that truncation decreases the value function; Eq. (4); our assumption; the  X  1 -horizon property of H ; Eq. (3); Lemma 4.2 and the definition of  X  1 . To assess the practical utility of OIM , we compared its performance to other exploration methods. Ex-periments were run on several small benchmark tasks challenging exploration algorithms.
 For fair comparisons, benchmark problems were taken from the literature without changes, nor did we change the experimental settings or the presentation of ex-perimental data. It also means that the presentation format varies for different benchmarks. 5.1. RiverSwim and SixArms The first two benchmark problems, RiverSwim and SixArms , were taken from Strehl and Littman (2006). The RiverSwim MDP has 6 states, representing the position of the agent in a river. The agent has two possible actions: she can swim either upstream or downstream. Swimming down is always successful, but swimming up succeeds only with a 30% chance and there is a 10% chance of slipping down. The low-ermost position yields +5 reward per step, while the uppermost position yields +10000.
 The SixArms MDP consists of a central state and six  X  X ayoff states X . In the central state, the agent can play 6 one-armed bandits. If she pulls arm k and wins, she is transferred to payoff state k . Here, she can get a reward in each step, if she chooses the appropriate action. The winning probabilities range from 1 to 0.01, while the rewards range from 50 to 6000 (for the exact values, see Strehl &amp; Littman, 2006).
 Data for E 3 , R-max , MBIE and MBIE-EB are taken from Strehl and Littman (2006). Parameters of all four algorithms were chosen optimally. Following a coarse search in parameter space, the R max parameter for OIM was set to 2000 for RiverSwim and to 10000 for SixArms . State spaces are small and value iteration instead of prioritized sweeping was completed in each step.
 On both problems, each algorithm ran for 5000 time steps and the undiscounted total reward was recorded. The averages and 95% confidence intervals are calcu-lated over 1000 test runs (Tables 5.1 and 5.1). 5.2. 50  X  50 Maze with Subgoals Another benchmark problem, MazeWithSubgoals , was suggested by Wiering and Schmidhuber (1998). The agent has to navigate in a 50  X  50 maze from the start position at (2 , 2) to the goal (with +1000 re-ward) at the opposite corner (49 , 49). There are sub-
Method 95% 99% 99.8%  X  -greedy,  X  = 0 . 2  X  (0)  X  (0)  X  (0)  X  -greedy,  X  = 0 . 4 43k (4) 52k (4) 68k (4) Recency-bonus 27k (19) 55k (18) 69k (9) Freq.-bonus 24k (20) 50k (16) 66k (10) MBIE(WS) 25k (20) 42k (19) 66k (18)
OIM 19k (20) 29k (20) 31k (20) optimal goals (with +500 reward) at the other two corners. The maze has blocked places and punishing states (  X  10 reward), set randomly in 20-20% of the squares. The agent can move in four directions, but with a 10% chance, its action is replaced by a random one. If the agent tries to move to a blocked state, it gets a reward of  X  2. Reaching any of the goals resets the agent to the start state. In all other cases, the agent gets a  X  1 reward for each step.
 Each algorithm was run on 20 different mazes for 100,000 steps. After every 1000 steps, we tested the learned value functions by averaging 20 test runs, in each one following the greedy policy for 10,000 steps, and averaging cumulated (undiscounted) rewards. We measured the number of test runs needed for the algo-rithms to learn to collect 95%, 99% and 99.8% of the maximum possible rewards in 100,000 steps, and the number of steps this takes on average, if the algorithms can meet the challenge.
 The algorithms that we compared were the recency based and frequency based exploration bonus meth-ods, two versions of  X  -greedy exploration, MBIE(WS) and OIM . All exploration rules applied the improved prioritized sweeping of Wiering and Schmidhuber (1998). OIM  X  X  R max was set to 1000. The results are summarized in Table 3. 5.3. Chain, Loop and FlagMaze The next three benchmark MDPs, the Chain , Loop and FlagMaze tasks were investigated, e.g., by Meuleau and Bourgine (1999), Strens (2000) and Dear-den (2000). In the Chain task, 5 states are lined up along a chain. The agent gets +2 reward for being in state 1 and +10 for being in state 5. One of the actions advances one state ahead, the other one resets the agent to state 1. The Loop task has 9 states in Method Phase 1 Phase 2 Phase 8 QL+var.-bonus  X  2570 1  X  QL+err.-bonus  X  2530 1  X  QL  X  -greedy 1519 1611 1602 QL Boltzmann 1606 1623  X  IEQL+ 2344 2557  X  Bayesian QL 1697 2417  X  Bayesian DP 2 3158 3611 3643
OIM 3510 3628 3643 two loops (arranged in a 8-shape). Completing the first loop (using any combination of the two actions) yields +1 reward, while the second loop yields +2, but one of the actions resets the agent to the start. The FlagMaze task consists of a 6  X  7 maze with several walls, a start state, a goal state and 3 flags. Whenever the agent reaches the goal, her reward is the number of flags collected.
 The following algorithms were compared: Q-learning with variance-based and TD error-based exploration bonus (model-free variants),  X  -greedy exploration, Boltzmann exploration, IEQL+, Bayesian Q-learning, Bayesian DP and OIM . Data were taken from Meuleau and Bourgine (1999), Strens (2000) and Dearden (2000). According to the sources, parameters for all algorithms were set optimally. OIM  X  X  R max parame-ter was set to 0.5, 10 and 0.005 for the three tasks, respectively.
 Each algorithm ran for 8 learning phases. The total cumulated reward over each learning phase was mea-sured. One phase lasted for 1000 steps for the first two tasks and 20,000 steps for the FlagMaze task. We carried out 256 parallel runs for the first 2 tasks and 20 for the third one. We proposed a new algorithm for exploration and rein-forcement learning in Markov decision processes. The algorithm integrates concepts from other advanced ex-ploration methods. The key component of our al-gorithm is an optimistic initial model. The optimal policy according to the agent X  X  model will either ex-plore new information that helps to make the model knowledge (the list of successor states).
 Method Phase 1 Phase 2 Phase 8 QL+var.-bonus  X  179 1  X  QL+err.-bonus  X  179 1  X  QL  X  -greedy 337 392 399 QL Boltzmann 186 200  X  IEQL+ 264 293  X  Bayesian QL 326 340  X  Bayesian DP 2 377 397 399 OIM 393 400 400 Method Phase 1 Phase 2 Phase 8 QL  X  -greedy 655 1135 1147 QL Boltzmann 195 1024  X  IEQL+ 269 253  X  Bayesian QL 818 1100  X  Bayesian DP 2 750 1763 1864
OIM 1133 1169 1171 more accurate, or follows a near-optimal path. The ex-tent of optimism regulates the amount of exploration. We have shown that with a suitably optimistic initial-ization, our algorithm finds a near-optimal policy in polynomial time. Experiments were conducted on a number of benchmark MDPs. According to the exper-imental results our novel method is robust and com-pares favorably to other methods.
 Acknowledgments We are grateful to one of the reviewers for his help-ful comments. This research has been supported by the EC FET  X  X ew Ties X  Grant FP6-502386 and NEST  X  X ERCEPT X  Grant FP6-043261. Opinions and errors in this manuscript are the author X  X  responsibility, they do not necessarily reflect those of the EC or other project members.
 Auer, P., &amp; Ortner, R. (2006). Logarithmic online re-gret bounds for undiscounted reinforcement learning algorithms. NIPS (pp. 49 X 56).
 Brafman, R. I., &amp; Tennenholtz, M. (2001). R-MAX -a general polynomial time algorithm for near-optimal reinforcement learning. Proc. IJCAI (pp. 953 X 958). Dearden, R. W. (2000). Learning and planning in structured worlds . Doctoral dissertation, University of British Columbia.
 Even-Dar, E., &amp; Mansour, Y. (2001). Convergence of optimistic and incremental Q-learning. NIPS (pp. 1499 X 1506).
 Kearns, M., &amp; Singh, S. (1998). Near-optimal rein-forcement learning in polynomial time. Proc. ICML (pp. 260 X 268).
 Kearns, M., &amp; Singh, S. (2002). Near-optimal rein-forcement learning in polynomial time. Machine Learning , 49 , 209 X 232.
 Koenig, S., &amp; Simmons, R. G. (1993). Complexity analysis of real-time reinforcement learning. Proc. AAAI (pp. 99 X 105).
 Littman, M. L., &amp; Szepesv  X ari, C. (1996). A gener-alized reinforcement-learning model: Convergence and applications. Proc. ICML (pp. 310 X 318). Mor-gan Kaufmann.
 Meuleau, N., &amp; Bourgine, P. (1999). Exploration of multi-state environments: Local measures and back-propagation of uncertainty. Machine Learning , 35 , 117 X 154.
 Singh, S. P., Jaakkola, T., Littman, M. L., &amp;
Szepesv  X ari, C. (2000). Convergence results for single-step on-policy reinforcement-learning algo-rithms. Machine Learning , 38 , 287 X 308.
 Strehl, A. L., &amp; Littman, M. L. (2005). A theoretical analysis of model-based interval estimation. Proc. ICML (pp. 856 X 863).
 Strehl, A. L., &amp; Littman, M. L. (2006). An analysis of model-based interval estimation for Markov decision processes. Submitted.
 Strens, M. (2000). A Bayesian framework for reinforce-ment learning. Proc. ICML (pp. 943 X 950). Morgan Kaufmann, San Francisco, CA.
 Sutton, R. S., &amp; Barto, A. G. (1998). Reinforcement Learning: An Introduction . MIT Press, Cambridge. Szita, I., &amp; L  X orincz, A. (2008). The many faces of opti-mism  X  extended version (Technical Report). E  X otv  X os Lor  X and University, Hungary.
 Wiering, M. A., &amp; Schmidhuber, J. (1998). Efficient model-based exploration. Proc. SAB: From Animals
