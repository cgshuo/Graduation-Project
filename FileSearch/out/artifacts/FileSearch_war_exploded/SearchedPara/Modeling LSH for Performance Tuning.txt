 Although Locality-Sensitive Hashing (LSH) is a promising approach to similarity search in high-dimensional spaces, it has not been considered practical partly because its search quality is sensitive to several parameters that are quite data dependent. Previous research on LSH, though obtained in-teresting asymptotic results, provides little guidance on how these parameters should be chosen, and tuning parameters for a given dataset remains a tedious process.

To address this problem, we present a statistical perfor-mance model of Multi-probe LSH, a state-of-the-art vari-ance of LSH. Our model can accurately predict the average search quality and latency given a small sample dataset. Apart from automatic parameter tuning with the perfor-mance model, we also use the model to devise an adaptive LSH search algorithm to determine the probing parameter dynamically for each query. The adaptive probing method addresses the problem that even though the average perfor-mance is tuned for optimal, the variance of the performance is extremely high. We experimented with three different datasets including audio, images and 3D shapes to evaluate our methods. The results show the accuracy of the proposed model: the recall errors predicted are within 5% from the real values for most cases; the adaptive search method re-duces the standard deviation of recall by about 50% over the existing method.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Performance similarity search, locality sensitive hashing
The K nearest neighbors ( K -NN) problem is a common formulation of many similarity search tasks such as content-based retrieval of multimedia data[14]. Although the prob-lem has been studied extensively for several decades, no sat-isfactory general solution is known. The exact K -NN prob-lem suffers from the curse of dimensionality , i.e. either the search time or space requirement is exponential in D , the number of dimensions [6, 16]. Both theory and practice have shown that traditional tree based indexing methods, e.g. [9, 3, 11], degenerate into linear scan in sufficiently high dimensions[21]. As a result, various approximate algorithms have been proposed to trade precision for speed. One of the most promising approximate algorithms is Locality Sensitive Hashing (LSH)[10, 8, 4].

The key insight behind LSH is that it is possible to con-struct hash functions such that points close to each other in some metric space have the same hash value with higher probability than do points that are far from one another. Given a particular metric and corresponding hash family, LSH maintains a number of hash tables containing the points in the dataset. An approximate solution to the K -NN query may be found by hashing the query point and scanning the buckets which the query point is hashed to.
 As originally proposed, LSH suffers from two drawbacks. First, it requires significant space, usually hundreds of hash tables to produce good approximation. The recently pro-posed Multi-probe LSH algorithm[15] addresses this prob-lem in practice, showing a space reduction of more than 90% in experiments. The basic idea (building on [18]) is to probe several buckets in the same hash table  X  additional buckets probed are those with addresses close to the hash value of the query. The second significant drawback of LSH is that its performance is very sensitive to several parame-ters which must be chosen by the implementation. A pre-viously proposed scheme, LSH Forest[2], partially addressed this problem by eliminating the need to fix one of the pa-rameters; however, the implementation is still left with the issue of finding good values for others; and there is a similar problem, as in Multi-probe LSH, to determine how many nodes in the trees to visit. The process of parameter tun-ing is both tedious and a serious impediment for practical applications of LSH. Current research on LSH and its vari-ants provides little guidance on how these parameter values should be chosen.
 This paper presents a performance model of Multi-probe LSH. Given a particular data type, a small sample dataset and the set of LSH parameters, the model can accurately predict the query quality and latency, making the parameter tuning problem easy. The optimal setting of parameters depends on the dataset of interest, which, in practice, migh t not be available at implementation time. Our method does not require the full dataset. We identify relevant statisti cal properties of the data, infer them from a sample dataset and extrapolate them to larger datasets. We show that our model is an accurate predictor of empirical performance.
In addition, we use our performance model to devise an adaptive version of Multi-probe LSH with superior prop-erties. Both our analysis and experiments show that the performance of LSH on a query point depends not only on the overall distribution of the dataset, but also on the loca l geometry in the vicinity of the particular query point. The fixed number of probes used in the original Multi-probe LSH may be insufficient for some queries and larger than neces-sary for others. Our adaptive probing method only probes enough buckets to achieve the required search result qualit y.
Our evaluation with three different datasets  X  images, audio, and 3D shapes  X  shows that our analytical model is accurate for predicting performance and thus reliable for p a-rameter tuning. Furthermore, our adaptive probing method not only reduces the variance in search performance between different queries, but also potentially reduces the query la -tency.
The K Nearest Neighbor ( K -NN) problem is as follows: given a metric space h M ,d i and a set S  X  M , maintain an index so that for any query point v  X  M , the set I ( v ) of K points in S that are closest to v can be quickly identified. In this paper we assume the metric space is the D -dimensional Euclidean space R D , which is the most commonly used met-ric space.

Indyk and Motwani introduced LSH as a probabilistic technique suitable for solving the approximate K -NN prob-lem [10, 8]. The original LSH hash function families were suitable only for Hamming space, but more recent families based on stable distributions and suitable for L p , p  X  (0 , 2] have been devised [4].

In our case of R D with L 2 distance, the LSH family is defined as follows [4]: where a i  X  R D is a vector with entries chosen independently from the Gaussian distribution N (0 , 1) and b i is drawn from the uniform distribution U [0 ,W ). For different i , a i b are sampled independently. The parameters M and W control the locality sensitivity of the hash function. The index data structure is L hash tables with independent hash functions, and the query algorithms is to scan the buckets which the query point is hashed to. As a data point might collide with the query point in more than one hash tables, a bitmap is maintained for each query to record the points scanned, so each data point is scanned at most once.
One drawback of the basic LSH scheme is that in practice it requires a large number of hash tables ( L ) to achieve good search quality. Panigrahy [18] proposed an entropy-based LSH scheme which reduced the required number of hash tables by using both the original query point and randomly perturbed, nearby points as additional queries. Lv, et al. [15] used a similar perturbation-based approach to develop Multi-probe LSH, which achieves the best results known so far. The study of this paper is based on Multi-probe LSH, which we review briefly below.
Multi-probe LSH[15] is the current state of the art in LSH based schemes for nearest neighbor search. This scheme seeks to make better use of a smaller number of hash tables ( L ). To accomplish this goal, it not only considers the main bucket, where the query point falls, but also examines other buckets that are  X  X lose X  to the main bucket.

For a single hash table, let v be the query point and H ( v ) its hash. Recall that H ( v ) consists of the concatenation of M integral values, each produced by an atomic hash function on v . Buckets corresponding to hash values that differ from H ( v ) by  X  1 in one or several components are also likely to contain points near the original query point v . Buckets corresponding to hash values that differ from H ( v ) by more than 1 in certain component are much less likely to contain points of interest[15], and are not considered.

Multi-probe LSH is to systematically probe those buckets that are closest to main bucket. For concreteness, consider the scenario shown in Figure 1 where M = 3. In this exam-ple, the query point is hashed to h 5 , 3 , 9 i . In addition to ex-amining this main bucket, the algorithm also examines other to the main bucket. Note that the closer the query point X  X  hash value is to the boundary of the bin, the more likely it is that the bin bordering that boundary contains nearest neighbors of the query. In the above example, h 5 , 2 , 9 i is the most promising of the additional buckets and the first of them to be examined.

In general, given a query point v and the main bucket  X  0 h h 1 ,...,h M i , let the probing sequence be {  X  0 , X  1 ,..., X  where  X  t = h h 1 +  X  t, 1 ,...,h M +  X  t,M i and h  X  t, 1 called the perturbation vector for step t . Because the chance of K th nearest neighbor falling into buckets with |  X  t,i is very small, we restrict  X  t,i to the set { X  1 , 0 , +1 } in or-der to simplify the algorithm. The buckets in the probing sequence are arranged in increasing order of the following query dependent score:  X  (  X  1) is the distance from i th projection to the right/left window boundary for each i perturbed, and 0 for others, as illustrated in Figure 1.

Generating the probing sequence for each query is itself time consuming and so Multi-probe LSH uses a pre-calculated template probing sequence generated with the expected val-ues of  X  i (  X  1) to approximate the query dependent probing sequence. For a specific query, its hash function compo-nents are ranked according to the  X  i (  X  1) values, and then adjusted according to the template probing sequence to pro-duce the actual probing sequence used for the query. The precise details can be found in [15].

The length of the probing sequence T used by the algo-rithm is a further parameter to be tuned. Larger value of Figure 1: Illustration of LSH. The hash function consists of three components, each calculated by random projection and quantization. The point v is hashed to H ( v ) = h 5 , 3 , 9 i .
 T allow us to achieve the same quality of results with fewer hash tables. However, the likelihood of finding more relevan t results falls rapidly as T increases. A very large value of T will only increase the candidate set size without significan tly improving the quality of the results returned.

In summary, to achieve good performance from Multi-probe LSH, one needs to carefully tune four parameters: the window size W , the number of hash function components M , the number of hash tables L and the length of probing sequence T .
A closely related work is LSH Forest [2], which represents each hash table by a prefix tree such that the number of hash functions per table can be adjusted. As new data arrive, the hash tables can grow on the fly. For a leaf node in the LSH forest, the depth of the node corresponds to parameter M in the basic LSH scheme. The method was designed for ham-ming distance. Although the idea may apply to L 2 distance with p -stable distribution-based hash functions, it must tune other parameters. Our data distribution modeling approach could be useful for this purpose.

The idea of intrinsic dimension has been used to ana-lyze the performance of spatial index structures such as R-Tree [17]. The concepts in the paper have inspired our data model work, especially in the parameters of the gamma dis-tribution and power law for modeling the distribution of distances.
This section presents the performance model of Multi-probe LSH assuming fixed parameters, i.e. W,M,L and T . This model requires fitting a series of distributions that ar e dataset specific. Although there is not a universal family of such distributions, our experience indicates that the gamm a distribution is commonly followed by multimedia data. For this special case, we then show how to extract statistical parameters from a small sample dataset to plug into the performance model.

Table 1 summarizes the notations we use in this paper. The first step is to formalize the performance measures. There are two aspects of performance  X  quality and cost. We use the percentage of true K -NNs found, or recall , to capture the quality of the query result. Formally, let v be a query point, I ( v ) be the set of true K -NNs and A ( v ) be the candidate set, which is the union of all the buckets probed. The approximate query result is the K elements of A ( v ) closest to v (or the entire A ( v ) if there are less than K points). Recall  X  is the percentage of I ( v ) included in A ( v ), or the following: We rank A ( v ) and only return the best K points, thus pre-cision is exactly the same as recall.

For cost, we are only interested in the online query time, as the space overhead and offline construction time are both linear to the dataset size and the number of hash tables ( L ). The main part of query processing is scanning through the candidate set and maintaining a top-K heap. Because most candidate points, being too far away from the query, are thrown away immediately after distance evaluation, heap updates rarely happen. Therefore, the query time is mostly spent on computing distances, and is thus proportional to the size of the candidate set. As a result, the percentage of the whole dataset scanned by LSH is a good indicator of query time. This is further justified by experiments in Sec-tion 5. We thus define selectivity  X  ( v ) = | A ( v ) | /N , the size ratio between the candidate set and the whole dataset, as the measure of query time. We use the average recall and se-lectivity as overall performance measure. Our performance model is to estimate these two values by their mathematical expectation:  X  = E [  X  ( v )] and  X  = E [  X  ( v )]. If not otherwise stated, expectations in this paper are taken over the ran-domness of the data as well as the random parameters of the LSH functions, i.e. a i and b i in (2).

The probability that an arbitrary point u in the dataset is found in the candidate set A ( v ) is determined by its dis-tance to the query point v . We represent this distance with random variable X , and define the probability as a function of X , which we also call recall and use the same symbol  X  :
With the above definition of  X  ( . ) as a function of distance, the expected selectivity can be written as The same reasoning applies to the nearest neighbors. Let X k = || I k ( v )  X  v || be the distance between v and its k th nearest neighbor, then  X  ( X k ) is the recall of v on its k th nearest neighbor, and the overall expected recall can be ob-tained by taking the average So far the modeling problem is reduced to finding the dis-tributions of X and X k , which we will address in the next subsection, and finding the detailed expression of  X  ( ), as explained below.

To obtain the expression of  X  ( ), we need to decompose the candidate set A ( v ) into the individual buckets probed. Let B l,t ( v ) , 1  X  l  X  L, 1  X  t  X  T be buckets in the prob-ing sequences of the L hash tables. Because the hash tables are maintained by the same algorithm in parallel, with hash functions sampled from the same family, the subscript l actu-ally does not matter when only probabilities are considered , thus we use B t ( v ) to denote the t th bucket for arbitrary l . It is obvious that A ( v ) = S l,t B l,t ( v ). Let u be an arbitrary point such that X = k u  X  v k , then  X  ( X ) = 1  X  Pr[ u /  X  [ Recall that the corresponding hash function of the B t in the probing sequence is h h 1 ( v ) +  X  t, 1 ( v ) ,...,h M ( v ) +  X  Thus and
Pr[ u /  X  B t ( v )] = 1  X  Y The probability Pr[ h i ( u ) = h i ( v ) +  X  t,i ( v )] depends on the perturbation value  X  t,i ( v ) as well as v  X  X  distance to the cor-responding window boundary on the i th projection. The de-tailed expression depends on the specific atomic hash func-tion, which is (2) in our case for L 2 distance. We directly give the formula here: where  X  W, X  ( d,z ) = where  X  ( ) is the probability density function of the standard Gaussian distribution. Because  X  W, 0 ( d,z ) occurs frequently, we use the following approximation to simplify computation : The values  X  i (  X  ) are functions of the query v and the hash function parameters. We make some approximations to sim-plify calculations by taking advantage of the template prob -ing sequence. First, note that the actual order of the M components of the hash function does not affect the eval-uation of (10). Without loss of generality, we can assume that the components are ordered by their minimal distance to window boundary, as in the template probing sequence. Second, the value of h i ( u ) and h i ( v ) do not appear in (11) and only  X  i (  X  ) is interesting. Finally, we use the expected values of  X  i (  X  ) instead of their actual values: These assumptions would allow us to calculate the probabil-ities directly from the template probing sequence.
The performance model of both recall and selectivity is given by (7 X 14).
To apply the performance model to real datasets, we still need to determine a series of distributions: the distributi on of the distance X between two arbitrary points, and the dis-tributions of the distance X k between an arbitrary query point and its k th nearest neighbor. These distributions are dataset specific and there is not a universal family that fits every possible dataset. However, we show that many ex-isting multimedia datasets do fit a common family  X  the gamma distribution. In this subsection, we show how to ex-tract statistical parameters from a small sample dataset an d do parameter estimation specific to the gamma distribution.
A previous study [20] has shown with multiple datasets that the distribution of L 1 distance between two arbitrary points follows the log-normal distribution without giving any intuitive explanation. Actually a log-normal variable is c on-ceptually the multiplicative product of many small indepen -dent factors, which can not be easily related to distance distributions. In this paper, we propose to fit the squared L 2 distances, both X 2 and X 2 k , by the gamma distribution, whose probability density function is where  X  is the shape parameter,  X  is the scale parameter, as it always appears as the denominator under x , and  X  is the normalizing coefficient such that the integral of the functio n over R + is 1. The gamma distribution and log-normal dis-tribution have similar skewed shapes and are usually consid -ered as alternatives. However, for our purpose, the gamma distribution has the following advantages.

First, it fits many multimedia datasets. Figure 2 and 3 show that both X 2 and X 2 k can be accurately fitted (see Section 5.1 for detailed description of the datasets). Vari ous other examples exist, but are not shown due to the page limit.

Second, the gamma distribution has an intuitive explana-tion which is related to the dataset X  X  intrinsic dimensiona lity. Assume that the feature vector space can be embedded into R
D , D being the intrinsic dimensionality. For two points u and v , assume the difference of each dimension, u i  X  v follows an identical but mutually independent Gaussian dis -tribution, then || u  X  v || 2 = P D i =1 ( u i  X  v i ) 2 is the sum of D squared Gaussian variables, which can be proved to fol-low  X  2 distribution, a special case of gamma distribution. The parameter  X  in the distribution is determined by the dimensionality of the space. When D is integer, we have the relationship D = 2(  X  + 1). Because gamma distribu-tion does not require  X  to be integer, we extend this to the non-integer case, and call 2(  X  + 1) the degree of freedom of the distribution, which we expect to capture the intrinsic d i-mensionality of the datasets. A dataset with higher degree
Figure 3: The squared distance between the query point and k th NN ( X of freedom will be harder to index. The relative order of the three dataset with regard to degree of freedom matches the experimental results in Section 5.

Finally, there exists an effective method to estimate the distribution parameters. The parameters of gamma distri-bution can be estimated by Maximum Likelihood Estima-tion (MLE), and depends only on the arithmetic mean E and geometric mean G of the sample, which can be easily extracted from the dataset. Given E and G ,  X  and t can be solved from the following set of equations. where  X  ( x ) =  X   X  ( x ) /  X ( x ) is the digamma function. To obtain the estimation of the distributions of X 2 and X k , we need to calculate from the sample set the corre-sponding arithmetic and geometric means of these random variables. For X 2 , the means E and G can be obtained sim-ply by sampling random pairs of points. For X 2 k , we need E and G k for each k under consideration. As the total number K of nearest neighbors can be big, maintaining K pairs of means is not practical. Further more, the distribution of X depends on the size N of the dataset. At the performance modeling stage, there is usually only a small subset of the whole dataset available. In such case, we need to extrapo-late the parameters according to the available data. Pagel et al. [17] studied the expected value of X k as a function of k and N , and proved the following relationship: where D 1 and D 2 are the embedding dimensionality and intrinsic dimensionality, respectively, and N is the size of dataset. Based on this result, we propose to empirically model both E k and G k of X 2 k as power functions of both k and N : To extract the parameters, a subset of the dataset is sampled as anchor points, and subsets of various sizes are sampled from the rest of points to be queried against. The K -NNs of all the anchor points are found by sequential scan, resultin g in a sample set of X k for different values of k and N . We then obtain the six parameters of (18) via least squares fitting. As shown by Figures 4 and 5 (for now, disregard the curve  X  X rith. mean + std X ) this power law modeling is very precise.
The fact that for fixed k , X k is a power function of N is very important for practical system design. It indicates th at X k changes very slowly as dataset grows, the optimal LSH parameters should also shift very slowly as data accumulate This allows us to keep high performance of LSH by only re-constructing the hash tables when dataset doubles, and this kind of reconstruction could be achieved with low amortized cost.

With the above method, 8 parameters are collected in total. Given the full dataset size to be indexed, we can obtain via MLE the distribution function f ( . ) of X 2 , and distribution functions f k ( . ) of X 2 k . The performance of LSH is then calculated by
This section presents two applications of the performance model. First is parameter tuning for optimal average perfor -mance and second is adaptive probing, which dynamically determines for each query how many buckets to probe at runtime. Figure 4: Arithmetic and geometric means of squared k -NN distance ( X to k . Figure 5: Arithmetic and geometric means of squared k -NN distance ( X to N . Here we use k = 50 . There are four parameters related to LSH. Among them, W , M and L need to be fixed when creating the hash tables, and T is related to the query processing algorithm and can either be fixed offline, or change from query to query. Ac-cording to our model, larger L results in higher recall with the same selectivity, thus L should be tuned to the max-imal affordable value, as limited by the storage available. Note that in practice if L is really high ( L &gt;&gt; 10, which is not likely to happen for large datasets), query time will again start increasing as the cost to generate the probing sequences becomes dominant.

We would like to tune W and M for optimal when creat-ing the hash tables, while having T adaptively determined at query time. However, our model requires a fixed T value to predict the performance, and thus to tune W and M . As a workaround, we choose a fixed T value of medium size (adding an equation T = M is a good choice, as the first few buckets are the most fruitful[15]) to tune for (near) optima l W and M , and again determine the T value for each query online (to be explained in next subsection). The optimiza-tion problem is as follows: In our implementation, we use the following simple method to solve this problem. Assume M is fixed, the relationships between W and both recall and selectivity are monotonic (see Figure 7A-C), and the optimal W can be found using binary search. We then enumerate the value of M from 1 to some reasonably large value max (30 for our datasets) to obtain the optimal. On a machine with Pentium 4 3.0GHz CPU, our code runs for no more than one minute for each of the datasets we have.
In the previous subsection, we tuned M and W for op-timal average performance. However, for the following rea-son, we do not want to determine the T value offline as first proposed in [15]: even if we tune for optimal average perfor-mance, the actual performance can be different from query to query. That is because recall and selectivity are both determined by the local geometry of the query point. The  X  X rith. mean + std X  X urves in Figure 4 show a large standard deviation, which means that the local geometry of different query points can be very different. For each specific query, probing the default T buckets can be either too few or too many to achieve the required recall. In this subsection, we address this problem by the adaptive probing method.
The basic idea of adaptive probing is simple: maintain an online prediction of the expected recall of current query , and keep probing until the required value is reached. If we knew the true K -NN distances precisely, we can predict the expected recall with our performance model. The problem is then turned to predicting the true K -NN distances by the partial result when processing the query, and refine the prediction iteratively as new buckets are probed. A natural approach is to use the partial results themselves as approx-imations of the true K -NNs. This approximation actually has an advantage that the estimated values are always larger then the real values, and the estimated expected recall is thus always lower than the real value. Also, as the probing sequence goes on, the partial K -NNs will quickly converge to the real ones. In practice, if one is to tune for 90% recall, the estimated value should be more or less the same as the true one.

Adaptive probing requires calculating the current expecte d recall after each step of probing and this computation is tim e consuming. To prevent this from slowing down the query process, a small lookup table is precomputed to map K -NN distances to expected recalls, for each different T value. Our experiments show that the run time overhead of using this lookup table is minimal.
In this section, we are interested in answering the followin g two questions by experimental studies: 1. How accurate is our model in predicting LSH perfor-2. How does our adaptive probing method improve over The evaluation of our methods with three real-life datasets gives satisfactory answer to these questions.
We employ three datasets to evaluate our methods: im-ages, audio clips and 3D shapes. Table 2 provides a sum-mary of them. These datasets are chosen to reflect a variety of real-life use cases, and they are of different number of di-mensions, from tens to hundreds. Experiments with a couple of other datasets have shown equally good results, but are not shown here due to the space limit.
 Image Data: The image dataset is drawn from the Corel Stock Photo Library, a dataset for evaluating content-base d image retrieval algorithms. For feature extraction, we use JSEG [5] to segment the images into regions and use the method in [13] to extract a feature vector from each region. The feature vector is of 14 dimensions, among which, 9 is for color moments and 5 is for shape and size information. There are 66,000 images in the dataset, and each image is segmented into roughly 10 regions, resulting in 662,317 fea -ture vectors in total.
 Audio Data: The audio dataset is drawn from the DARPA TIMIT collection [7]. The TIMIT collection is an audio speech database that contains 6,300 English sentences spo-ken by 630 different speakers with a variety of regional ac-cents. We break each sentence into smaller segments and extract features from each segment with the Marsyas li-brary [19]. There are 54,387 192-dimensional feature vecto rs in total.
 Shape Data: The third dataset we use in our study con-tains about 29,000 3D shape models, which is a mixture of 3D polygonal models gathered from commercial viewpoint models, De Espona Models, Cacheforce models and from the Web. Each model is represented by a single Spher-ical Harmonic Descriptor(SHD) [12], yielding 28,775 544-dimensional feature vectors in total.
First of all, we need to justify modeling query latency with selectivity  X  under the assumption that the most of the query time is spent scanning the candidate data points. Figure 6: Latency vs. selectivity. The different se-lectivities in the two figures are obtained by varying T and W respectively. The matching of the two fig-ures confirms the reliability of selectivity as a proxy of latency.
 Since  X  is not a tunable input parameter, we conduct two different experiments by varying different input parameters for the image dataset. With all other parameters fixed, one experiment changes  X  by using different window sizes ( W ), and the other by using different probing sequence lengths ( T ). For each configuration of parameters, 1,000 queries for 50-NN are executed, with latency and selectivity  X  recorded. These numbers are then binned by  X  , and the average and the standard deviation of the latencies in each bin are cal-culated and plotted. Our results are shown in Figure 6, the height of the error bar representing two standard deviation s. The linear relationship between query time and  X  is obvious.
Strictly speaking, apart from the time spent on scanning the candidate set, there is a tiny cost to initialize the quer y data structure, and a tiny cost to locate the bucket to scan at each probing step. If these costs are not small enough, they should be seen in the plots. The former should create a non-zero y-intercept, and the latter will make the slopes of the two curves different. Figure 6 shows a minimal initial cost and a very small divergence between the slopes of the two curves which can be safely ignored in practice. As a result, it is safe to use selectivity as the machine-independent tim e cost instead of latency which is machine-dependent. We then go on to evaluate the accuracy of the model itself. As the model involves four input parameters, i.e. W , M , L and T , and two output parameters, i.e. recall  X  and selec-tivity  X  , it is hard to evaluate an overall accuracy. Also, not every point in the input parameter space is equally interest -ing because only the parameter configurations that result in high recall and low selectivity are of practical importance . We thus design our experiments in the following way. First, a set of baseline parameters are chosen for each dataset, which achieves about 90% recall with a reasonably low se-lectivity. Then each time we fix all but one of the four pa-rameters and change it around the baseline value. For each configuration of the parameters, we build LSH data struc-ture with the whole dataset, and run 50-NN queries for 1,000 randomly sampled query points. The average of the recall and selectivity are recorded and plotted. We also predict th e average recall and selectivity with our performance model. The data parameters are extracted from one 10th the whole dataset according to the method discussed in Section 3.2.
The effects of the four parameters on the three datasets are shown in Figure 7A-L, with error bars representing two standard deviations. According to the figures, the predicte d values are close to the real experimental values, and most of the errors are within one standard deviation. Our predictio n of recall values is especially accurate. The error is within 5% the actual recall value for most cases. When M &gt; 15, the error ratio is a little high, but these cases have very low re-call, and are not interesting in practice. Further more, our predictions correctly follow the trends of the actual curve s. This implies that our model is reliable for real system de-sign and the automatically tuned parameters are close to optimal.

By experimenting with the parameters one by one while leaving the others fixed, we also see the performance impact of each parameter.
In this subsection, we first conduct an experiment to show the impact of local geometry around query points on the performance, and demonstrate the advantage of adaptive method over the original method with fixed T , which we call the fixed method . We then show by another experiment that the adaptive method is capable of working with different K values at query time. Note that so far our discussion has been assuming a fixed K value.

The local geometry is best described by local K-NN dis-tances ( X k as in the model), and we want to see how LSH performs for query points with different K-NN distances. For each dataset, we sample 1,000 queries at random, and group them into about 30 bins according to the distances to the 50th nearest neigbhor ( X 50 ). We then run both fixed and adaptive versions of the query algorithm, and plot the average recall and selectivity for each bin. The parame-ters M and W are tuned for 90% recall of 50-NN queries, assuming T = M . For adaptive method, T is again dy-namically determined at query time. The results are shown in Figure 8, and there is an obvious difference between the behaviors of the two methods. These results are better un-derstood when compared with Figure 3, which shows the probability distribution of K-NN distance. The average and standard deviations of the recall and selectivity values ca n also be found in the K = 50 rows of Table 3.

Both recall and selectivity of the fixed method drop dra-matically as K-NN distance grows beyond certain point. This is because LSH is tuned to perform well for average K -NN distance, which is relatively small. For queries with larger K -NN distance, both the nearest neighbors and the background points have smaller chances of falling into the probed buckets, and thus the fixed method gets excessively low recall. To compensate this effect, the fixed method has to achieve exceptionally high recall, close to 100%, with th e easy cases. Because selectivity grows faster at higher reca ll values (see Figure 7), such compensation will result in high er overall cost.

The adaptive method, however, is able to probe more buckets for those difficult points and achieve a high recall to meet the requirement. As a result, the adaptive method significantly reduces the variance of recall and better meet s the quality requirements for individual queries. For the di f-ficult queries on the right end of the curves, it costs more than average for the adaptive method to achieve the required recall.
 Table 3: Adaptive vs. fixed method on dealing with different K requirement. To achieve the same recall, the fixed method needs to have T tuned for differ-ent K s, while the adaptive method does not. We can also see a significant reduction of recall standard de-viation by the adaptive method.

Our second experiment is to show that the adaptive method works with different K values better compared to the fixed method, which is tuned for a single K value. To compare the two methods, we use the same hash tables constructed with the parameters tuned for 50-NN queries, the same as those used in Figure 8. And we use these tables to answer queries for 10, 50, 100-NNs to see how the two methods behave. For the fixed method, we use our model to pre-calculate the T needed for each of the three cases so as to achieve 90% recall on average (as shown in the third column of Table 3) , and for the adaptive method, T of each query is dynami-cally determined. For each configuration of parameters, we run 1,000 queries and take the average recall and selectivit y as well as the standard deviation. The results are shown in Table 3.

As we can see from the results, the adaptive method signif-icantly reduces the performance variation between differen t queries. The standard deviation reductions for recall are 50%, 65% and 40% for three datasets respectively, around 50% on average. Also in most cases, the adaptive method produces a higher recall than the fixed method, with a lower selectivity.

The above two experiments allow us to demonstrate the effectiveness of our adaptive query process method on re-ducing the variation of performance among different queries as well as the average cost to achieve the same recall. Our study shows that it is possible to model Multi-probe LSH and data distribution accurately with small sample datasets and use the models for automatic parameter tuning in real implementations.
 We have proposed a performance model for multi-probe LSH and a data model to predict the distributions of K -NN distances in a dataset. Our experiments with three datasets show that the models fitted with small sample datasets are accurate; the recalls are within 5% of the real average value s for most cases.

We have derived an adaptive search method based on the performance model to reduce performance variance between different query points. Our experimental results show that the adaptive method can reduce the standard deviation of recalls by about 50%, while achieving the same recall with lower latency.

We have implemented the automatic tuning in the toolkit which will be made available to the public domain[1]. This work is supported in part by NSF grants EIA-0101247, CCR-0205594, CCR-0237113, CNS-0509447, DMS-0528414 and by research grants from Google, Intel, Microsoft, and Yahoo!. Wei Dong is supported by Gordon Wu Fellowship. [1] http://www.cs.princeton.edu/cass . [2] M. Bawa, T. Condie, and P. Ganesan. Lsh forest: [3] J. L. Bentley. K-d trees for semidynamic point sets. In [4] M. Datar, N. Immorlica, P. Indyk, and V. S. Mirrokni. [5] Y. Deng and B. Manjunath. Unsupervised [6] D. Dobkin and R. J. Lipton. Multidimensional [7] J. S. Garofolo, L. F. Lamel, W. M. Fisher, J. G. [8] A. Gionis, P. Indyk, and R. Motwani. Similarity [9] A. Guttman. R-trees: a dynamic index structure for [10] P. Indyk and R. Motwani. Approximate nearest [11] N. Katayama and S. Satoh. The sr-tree: an index [12] M. Kazhdan, T. Funkhouser, and S. Rusinkiewicz. [13] Q. Lv, M. Charikar, and K. Li. Image similarity search [14] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and [15] Q. Lv, W. Josephson, Z. Wang, M. Charikar, and [16] S. Meiser. Point location in arrangements of [17] B.-U. Pagel, F. Korn, and C. Faloutsos. Deflating the [18] R. Panigrahy. Entropy based nearest neighbor search [19] G. Tzanetakis and P. Cook. Marsyas: a framework for [20] Z. Wang, W. Dong, W. Josephson, Q. Lv, [21] R. Weber, H.-J. Schek, and S. Blott. A quantitative experimental results.
