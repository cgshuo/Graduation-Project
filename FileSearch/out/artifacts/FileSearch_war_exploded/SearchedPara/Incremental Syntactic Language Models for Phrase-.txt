 Early work in statistical machine translation viewed translation as a noisy channel process comprised of a translation model, which functioned to posit ad-equate translations of source language words, and a target language model, which guided the fluency of generated target language strings (Brown et al., 1990). Drawing on earlier successes in speech recognition, research in statistical machine trans-lation has effectively used n -gram word sequence models as language models.

Modern phrase-based translation using large scale n -gram language models generally performs well in terms of lexical choice, but still often produces ungrammatical output. Syntactic parsing may help produce more grammatical output by better model-ing structural relationships and long-distance depen-dencies. Bottom-up and top-down parsers typically require a completed string as input; this requirement makes it difficult to incorporate these parsers into phrase-based translation, which generates hypothe-sized translations incrementally, from left-to-right. 1 As a workaround, parsers can rerank the translated output of translation systems (Och et al., 2004).
On the other hand, incremental parsers (Roark, 2001; Henderson, 2004; Schuler et al., 2010; Huang and Sagae, 2010) process input in a straightforward left-to-right manner. We observe that incremental parsers, used as structured language models, pro-vide an appropriate algorithmic match to incremen-tal phrase-based decoding. We directly integrate in-cremental syntactic parsing into phrase-based trans-lation. This approach re-exerts the role of the lan-guage model as a mechanism for encouraging syn-tactically fluent translations.

The contributions of this work are as follows:  X  A novel method for integrating syntactic LMs  X  A formal definition of an incremental parser for  X  Integration with Moses (  X  5) along with empiri-Neither phrase-based (Koehn et al., 2003) nor hierar-chical phrase-based translation (Chiang, 2005) take explicit advantage of the syntactic structure of either source or target language. The translation models in these techniques define phrases as contiguous word sequences (with gaps allowed in the case of hierar-chical phrases) which may or may not correspond to any linguistic constituent. Early work in statisti-cal phrase-based translation considered whether re-stricting translation models to use only syntactically well-formed constituents might improve translation quality (Koehn et al., 2003) but found such restric-tions failed to improve translation quality.
Significant research has examined the extent to which syntax can be usefully incorporated into sta-tistical tree-based translation models: string-to-tree (Yamada and Knight, 2001; Gildea, 2003; Imamura et al., 2004; Galley et al., 2004; Graehl and Knight, 2004; Melamed, 2004; Galley et al., 2006; Huang et al., 2006; Shen et al., 2008), tree-to-string (Liu et al., 2006; Liu et al., 2007; Mi et al., 2008; Mi and Huang, 2008; Huang and Mi, 2010), tree-to-tree (Abeill  X  e et al., 1990; Shieber and Schabes, 1990; Poutsma, 1998; Eisner, 2003; Shieber, 2004; Cowan et al., 2006; Nesson et al., 2006; Zhang et al., 2007; DeNeefe et al., 2007; DeNeefe and Knight, 2009; Liu et al., 2009; Chiang, 2010), and treelet (Ding and Palmer, 2005; Quirk et al., 2005) techniques use syntactic information to inform the translation model. Recent work has shown that parsing-based machine translation using syntax-augmented (Zoll-mann and Venugopal, 2006) hierarchical translation grammars with rich nonterminal sets can demon-strate substantial gains over hierarchical grammars for certain language pairs (Baker et al., 2009). In contrast to the above tree-based translation models, our approach maintains a standard (non-syntactic) phrase-based translation model. Instead, we incor-porate syntax into the language model.

Traditional approaches to language models in speech recognition and statistical machine transla-tion focus on the use of n -grams, which provide a simple finite-state model approximation of the tar-get language. Chelba and Jelinek (1998) proposed that syntactic structure could be used as an alterna-tive technique in language modeling. This insight has been explored in the context of speech recogni-tion (Chelba and Jelinek, 2000; Collins et al., 2005). Hassan et al. (2007) and Birch et al. (2007) use supertag n -gram LMs. Syntactic language models have also been explored with tree-based translation models. Charniak et al. (2003) use syntactic lan-guage models to rescore the output of a tree-based translation system. Post and Gildea (2008) investi-gate the integration of parsers as syntactic language models during binary bracketing transduction trans-lation (Wu, 1997); under these conditions, both syn-tactic phrase-structure and dependency parsing lan-guage models were found to improve oracle-best translations, but did not improve actual translation results. Post and Gildea (2009) use tree substitution grammar parsing for language modeling, but do not use this language model in a translation system. Our work, in contrast to the above approaches, explores the use of incremental syntactic language models in conjunction with phrase-based translation models.
Our syntactic language model fits into the fam-ily of linear-time dynamic programming parsers de-scribed in (Huang and Sagae, 2010). Like (Galley and Manning, 2009) our work implements an in-cremental syntactic language model; our approach differs by calculating syntactic LM scores over all available phrase-structure parses at each hypothesis instead of the 1-best dependency parse.

The syntax-driven reordering model of Ge (2010) uses syntax-driven features to influence word order within standard phrase-based translation. The syn-tactic cohesion features of Cherry (2008) encour-ages the use of syntactically well-formed translation phrases. These approaches are fully orthogonal to our proposed incremental syntactic language model, and could be applied in concert with our work. Parsing is the task of selecting the representation  X   X  (typically a tree) that best models the structure of the president  X   X  2  X   X  2  X   X  2 . . .
 gram state, and syntactic language model state  X   X  . Hypothesis combination is also shown, indicating sentence e , out of all such possible representations  X  . This set of representations may be all phrase structure trees or all dependency trees allowed by the parsing model. Typically, tree  X   X  is taken to be:
We define a syntactic language model P ( e ) based on the total probability mass over all possible trees for string e . This is shown in Equation 2 and decom-posed in Equation 3.
 3.1 Incremental syntactic language model An incremental parser processes each token of in-put sequentially from the beginning of a sentence to the end, rather than processing input in a top-down (Earley, 1968) or bottom-up (Cocke and Schwartz, 1970; Kasami, 1965; Younger, 1967) fashion. After processing the t th token in string e , an incremen-tal parser has some internal representation of possi-ble hypothesized (incomplete) trees,  X  t . The syntac-tic language model probability of a partial sentence e ...e t is defined:
In practice, a parser may constrain the set of trees under consideration to  X   X  t , that subset of analyses or partial analyses that remains after any pruning is per-formed. An incremental syntactic language model can then be defined by a probability mass function (Equation 5) and a transition function  X  (Equation 6). The role of  X  is explained in  X  3.3 below. Any parser which implements these two functions can serve as a syntactic language model.

P ( e 1 ...e t )  X  P (  X   X  t ) = X 3.2 Decoding in phrase-based translation Given a source language input sentence f , a trained source-to-target translation model, and a target lan-guage model, the task of translation is to find the maximally probable translation  X  e using a linear combination of j feature functions h weighted ac-cording to tuned parameters  X  (Och and Ney, 2002).
Phrase-based translation constructs a set of trans-lation options  X  hypothesized translations for con-tiguous portions of the source sentence  X  from a trained phrase table, then incrementally constructs a lattice of partial target translations (Koehn, 2010). To prune the search space, lattice nodes are orga-nized into beam stacks (Jelinek, 1969) according to the number of source words translated. An n -gram language model history is also maintained at each node in the translation lattice. The search space is further trimmed with hypothesis recombination, which collapses lattice nodes that share a common coverage vector and n -gram state. 3.3 Incorporating a Syntactic Language Model Phrase-based translation produces target language words in an incremental left-to-right fashion, gen-erating words at the beginning of a translation first and words at the end of a translation last. Similarly, incremental parsers process sentences in an incre-mental fashion, analyzing words at the beginning of a sentence first and words at the end of a sentence last. As such, an incremental parser with transition function  X  can be incorporated into the phrase-based decoding process in a straightforward manner. Each node in the translation lattice is augmented with a syntactic language model state  X   X  t .

The hypothesis at the root of the translation lattice is initialized with  X   X  0 , representing the internal state of the incremental parser before any input words are processed. The phrase-based translation decoding process adds nodes to the lattice; each new node contains one or more target language words. Each node contains a backpointer to its parent node, in which  X   X  t  X  1 is stored. Given a new target language word e t and  X   X  t  X  1 , the incremental parser X  X  transi-tion function  X  calculates  X   X  t . Figure 1 illustrates Figure 2: Sample binarized phrase structure tree. Figure 3: Sample binarized phrase structure tree af-ter application of right-corner transform. a sample phrase-based decoding lattice where each translation lattice node is augmented with syntactic language model state  X   X  t .

In phrase-based translation, many translation lat-tice nodes represent multi-word target language phrases. For such translation lattice nodes,  X  will be called once for each newly hypothesized target language word in the node. Only the final syntac-tic language model state in such sequences need be stored in the translation lattice node. Having defined the framework by which any in-cremental parser may be incorporated into phrase-based translation, we now formally define a specific incremental parser for use in our experiments.
The parser must process target language words incrementally as the phrase-based decoder adds hy-potheses to the translation lattice. To facilitate this incremental processing, ordinary phrase-structure trees can be transformed into right-corner recur-. . . . . . . . . . . .
 Figure 4: Graphical representation of the depen-dency structure in a standard Hierarchic Hidden Markov Model with D = 3 hidden levels that can be used to parse syntax. Circles denote random vari-ables, and edges denote conditional dependencies. Shaded circles denote variables with observed val-ues. sive phrase structure trees using the tree transforms in Schuler et al. (2010). Constituent nontermi-nals in right-corner transformed trees take the form of incomplete constituents c  X  /c  X  X  consisting of an  X  X ctive X  constituent c  X  lacking an  X  X waited X  con-stituent c  X  X  yet to come, similar to non-constituent categories in a Combinatory Categorial Grammar (Ades and Steedman, 1982; Steedman, 2000). As an example, the parser might consider VP/NN as a possible category for input  X  X eets the X .

A sample phrase structure tree is shown before and after the right-corner transform in Figures 2 and 3. Our parser operates over a right-corner trans-formed probabilistic context-free grammar (PCFG). Parsing runs in linear time on the length of the input. This model of incremental parsing is implemented as a Hierarchical Hidden Markov Model (HHMM) (Murphy and Paskin, 2001), and is equivalent to a probabilistic pushdown automaton with a bounded pushdown store. The parser runs in O( n ) time, where n is the number of words in the input. This model is shown graphically in Figure 4 and formally defined in  X  4.1 below.

The incremental parser assigns a probability (Eq. 5) for a partial target language hypothesis, using a bounded store of incomplete constituents c  X  /c  X  X  . The phrase-based decoder uses this probability value as the syntactic language model feature score. 4.1 Formal Parsing Model: Scoring Partial This model is essentially an extension of an HHMM, which obtains a most likely sequence of hidden store states,  X  s 1 ..D 1 ..T , of some length T and some maxi-mum depth D , given a sequence of observed tokens (e.g. generated target language words), e 1 ..T , using HHMM state transition model  X  A and observation symbol model  X  B (Rabiner, 1990):  X  s The HHMM parser is equivalent to a probabilis-tic pushdown automaton with a bounded push-down store. The model generates each successive store (using store model  X  S ) only after considering whether each nested sequence of incomplete con-stituents has completed and reduced (using reduc-tion model  X  R ):
Store elements are defined to contain only the active ( c  X  ) and awaited ( c  X  X  ) constituent categories necessary to compute an incomplete constituent probability:
Reduction states are defined to contain only the complete constituent category c r d pute an inside likelihood probability, as well as a place (to end a sequence of incomplete constituents):
The model probabilities for these store elements and reduction states can then be defined (from Mur-phy and Paskin 2001) to expand a new incomplete constituent after a reduction has taken place ( f r d 1 ; using depth-specific store state expansion model  X 
S-E ,d ), transition along a sequence of store elements s s s e r r r right-corner tree structure of Figure 3. if no reduction has taken place ( f r d specific store state transition model  X  S-T ,d ): 2  X   X   X  and possibly reduce a store element (terminate a sequence) if the store state below it has re-where r  X  is a null state resulting from the failure of an incomplete constituent to complete, and constants are defined for the edge conditions of s 0 t and r D +1 t Figure 5 illustrates this model in action.

These pushdown automaton operations are then refined for right-corner parsing (Schuler, 2009), distinguishing active transitions (model  X  S-T -A ,d , in which an incomplete constituent is completed, but not reduced, and then immediately expanded to a new incomplete constituent in the same store el-ement) from awaited transitions (model  X  S-T -W ,d , which involve no completion): These HHMM right-corner parsing operations are then defined in terms of branch-and depth-specific s 3 ; likewise  X   X  5 1 contains s  X  for expansions:  X  for awaited transitions:  X  for active transitions:  X  for cross-element reductions:  X  for in-element reductions:
We use the parser implementation of (Schuler, 2009; Schuler et al., 2010). The phrase-based decoder is augmented by adding additional state data to each hypothesis in the de-coder X  X  hypothesis stacks. Figure 1 illustrates an ex-cerpt from a standard phrase-based translation lat-tice. Within each decoder stack t , each hypothe-sis h is augmented with a syntactic language model state  X   X  t a random variable store, containing a slice of ran-dom variables from the HHMM. Specifically,  X   X  t contains those random variables s 1 ..D t that maintain distributions over syntactic elements.

By maintaining these syntactic random variable stores, each hypothesis has access to the current language model probability for the partial transla-tion ending at that hypothesis, as calculated by an incremental syntactic language model defined by the HHMM. Specifically, the random variable store at hypothesis h provides P (  X   X  t where e h 1 ..t is the sequence of words in a partial hy-pothesis ending at h which contains t target words, and where there are D syntactic random variables in each random variable store (Eq. 5).

During stack decoding, the phrase-based decoder progressively constructs new hypotheses by extend-ing existing hypotheses. New hypotheses are placed in appropriate hypothesis stacks. In the simplest case, a new hypothesis extends an existing hypothe-sis by exactly one target word. As the new hypothe-sis is constructed by extending an existing stack ele-ment, the store and reduction state random variables are processed, along with the newly hypothesized word. This results in a new store of syntactic ran-dom variables (Eq. 6) that are associated with the new stack element.

When a new hypothesis extends an existing hy-pothesis by more than one word, this process is first carried out for the first new word in the hypothe-sis. It is then repeated for the remaining words in the hypothesis extension. Once the final word in the hypothesis has been processed, the resulting ran-dom variable store is associated with that hypoth-esis. The random variable stores created for the non-final words in the extending hypothesis are dis-carded, and need not be explicitly retained.
Figure 6 illustrates this process, showing how a syntactic language model state  X   X  5 1 in a phrase-based decoding lattice is obtained from a previous syn-tactic language model state  X   X  3 1 (from Figure 1) by parsing the target language words from a phrase-based translation option.

Interpolated WSJ 5-gram + HHMM 209.13 225.48
Interp. Giga 5-gr + WSJ HHMM 222.39 123.10
Interp. Giga 5-gr Figure 7: Average per-word perplexity values. HHMM was run with beam size of 2000. Bold in-dicates best single-model results for LMs trained on WSJ sections 2-21. Best overall in italics .
Our syntactic language model is integrated into the current version of Moses (Koehn et al., 2007). As an initial measure to compare language models, average per-word perplexity, ppl , reports how sur-prised a model is by test data. Equation 25 calculates ppl using log base b for a test set of T tokens.
We trained the syntactic language model from  X  4 (HHMM) and an interpolated n -gram language model with modified Kneser-Ney smoothing (Chen and Goodman, 1998); models were trained on sec-tions 2-21 of the Wall Street Journal (WSJ) tree-bank (Marcus et al., 1993). The HHMM outper-forms the n -gram model in terms of out-of-domain test set perplexity when trained on the same WSJ data; the best perplexity results for in-domain and out-of-domain test sets 4 are found by interpolating Figure 8: Mean per-sentence decoding time (in sec-onds) for dev set using Moses with and without syn-tactic language model. HHMM parser beam sizes are indicated for the syntactic LM.
 HHMM and n -gram LMs (Figure 7). To show the effects of training an LM on more data, we also re-port perplexity results on the 5-gram LM trained for the GALE Arabic-English task using the English Gi-gaword corpus. In all cases, including the HHMM significantly reduces perplexity.

We trained a phrase-based translation model on the full NIST Open MT08 Urdu-English translation model using the full training data. We trained the HHMM and n -gram LMs on the WSJ data in order to make them as similar as possible. During tuning, Moses was first configured to use just the n -gram LM, then configured to use both the n -gram LM and the syntactic HHMM LM. MERT consistently as-signed positive weight to the syntactic LM feature, typically slightly less than the n -gram LM weight.
In our integration with Moses, incorporating a syntactic language model dramatically slows the de-coding process. Figure 8 illustrates a slowdown around three orders of magnitude. Although speed remains roughly linear to the size of the source sen-tence (ruling out exponential behavior), it is with an extremely large constant time factor. Due to this slowdown, we tuned the parameters using a con-strained dev set (only sentences with 1-20 words), and tested using a constrained devtest set (only sen-tences with 1-20 words). Figure 9 shows a statis-tically significant improvement to the BLEU score when using the HHMM and the n -gram LMs to-gether on this reduced test set. This paper argues that incremental syntactic lan-guages models are a straightforward and appro-Figure 9: Results for Ur-En devtest (only sentences with 1-20 words) with HHMM beam size of 2000 and Moses settings of distortion limit 10, stack size 200, and ttable limit 20. priate algorithmic fit for incorporating syntax into phrase-based statistical machine translation, since both process sentences in an incremental left-to-right fashion. This means incremental syntactic LM scores can be calculated during the decoding pro-cess, rather than waiting until a complete sentence is posited, which is typically necessary in top-down or bottom-up parsing.

We provided a rigorous formal definition of in-cremental syntactic languages models, and detailed what steps are necessary to incorporate such LMs into phrase-based decoding. We integrated an incre-mental syntactic language model into Moses. The translation quality significantly improved on a con-strained task, and the perplexity improvements sug-gest that interpolating between n -gram and syntactic LMs may hold promise on larger data sets.

The use of very large n -gram language models is typically a key ingredient in the best-performing ma-chine translation systems (Brants et al., 2007). Our n -gram model trained only on WSJ is admittedly small. Our future work seeks to incorporate large-scale n -gram language models in conjunction with incremental syntactic language models.

The added decoding time cost of our syntactic language model is very high. By increasing the beam size and distortion limit of the baseline sys-tem, future work may examine whether a baseline system with comparable runtimes can achieve com-parable translation quality.

A more efficient implementation of the HHMM parser would speed decoding and make more exten-sive and conclusive translation experiments possi-ble. Various additional improvements could include caching the HHMM LM calculations, and exploiting properties of the right-corner transform that limit the number of decisions between successive time steps.
