 Name ambiguity arises from the polysemy of names and causes uncertainty about the true identity of entities ref-erenced in unstructured text. This is a major problem in areas like information retrieval or knowledge management, for example when searching for a specific entity or updating an existing knowledge base.

We approach this problem of named entity disambigua-tion (NED) using thematic information derived from Latent Dirichlet Allocation (LDA) to compare the entity mention X  X  context with candidate entities in Wikipedia represented by their respective articles. We evaluate various distances over topic distributions in a supervised classification setting to find the best suited candidate entity, which is either covered in Wikipedia or unknown. We compare our approach to a state of the art method and show that it achieves signif-icantly better results in predictive performance, regarding both entities covered in Wikipedia as well as uncovered en-tities.

We show that our approach is in general language inde-pendent as we obtain equally good results for named en-tity disambiguation using the English, the German and the French Wikipedia.
 I.2.7 [ Natural Language Processing ]: Text analysis; I.5.2 [ Design Methodology ]: Classifier design and evaluation, Feature evaluation and selection; H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation named entities, named entity disambiguation, topic model-ing, named entity resolution, classification
Recently several analysts have estimated that more than 70%-80% of all data in organizations is unstructured infor-mation [24]. The enrichment of knowledge bases from un-structured text can hence gain from reliable information ex-traction methods. Among those are text mining techniques, such as entity recognition, relation extraction or entity dis-ambiguation.

Named entity recognition identifies name phrases in a text as named entities by labeling the words with type labels, such as person or location. Subsequently named entity dis-ambiguation (NED) or name disambiguation aims to assign a name mention in a text to the underlying real-world en-tity , e.g., to a unique entity description. This is a critical step in the construction of semantic knowledge bases from unstructured text, because it enables entity-based retrieval instead of keyword search.

It is well known that names are not unique. The name mention John Taylor , for example, can denote a South Car-olina governor, an athlete, a racing driver, a jazz musician, a bass guitarist and so on. Note that the last two entities also have a similar artistic profession (both are musicians), which makes their distinction even more difficult. Addition-ally, polysemy of names spans across entity types. The term Bush , for example, may be used to refer to a large number of persons, a shrub, to undeveloped landscape, to a number of locations, brands, as well as to two rock bands.
The effects resulting from name ambiguity can easily be seen when carrying out web searches or retrieving articles from an archive of newspaper texts. For example, the top ten hits of a Google search for the string John Taylor con-tain ten different entities, among them a professor, a jazz guitarist, and a college. It may be clear to a human that these mentions do not refer to the same real-world entity. However, it is difficult to draw this conclusion automatically, for example using natural language processing techniques.
Miller and Charles observed that words with similar mean-ings are often used in similar contexts [17]. Hence, most approaches to name disambiguation estimate the identity of a name mention by comparing the words in the neighbor-hood of the mention with the words in the descriptions of real-world entities. While the comparison of (bag-of-) word vectors for NED is quite successful [1, 8, 5, 7] it is not ro-bust if different words with a similar meaning are used in the neighborhood of a name mention and the corresponding descriptions of the real-world entity.

Figure 1 depicts examples for some contexts mentioning the names Taylor and John Taylor . Consider for example T2 Figure 1: Context examples for the name Taylor referring to three different entities (extracted from Wikipedia). and T3: while the thematic information is similar, the over-lap of common terms is very low, even though both contexts refer to the same person. A pure word comparison could not detect that pampleteer and poet are thematically related to a topic such as writing . Additionally, disambiguation ap-proaches that are based on word comparisons are also often unable to reason on previously unseen attributes (e.g., new terms), and thus are likely to suffer from a lower recall for new data.

There have been several approaches to overcome the prob-lem of synonymy. Srinivasan et al., for example, generate probable words according to a topic model of a document [19]. In this paper, instead of representing topics by some words, we used topic model probabilities directly to describe the document content that a name mention appears in. This is motivated by the idea that topic models automatically disambiguate terms based on the co-occurrences with other terms. If LDA is trained on a sufficiently large corpus, most words of a new document will already be present in the LDA model. Thus, we assume that in a new document we can infer a topic probability distribution with sufficient quality, which may be used to assess semantic similarity to Wikipedia articles.

Our approach is able to assign name mentions in new doc-uments to unseen entity descriptions (Wikipedia articles). The topic models may be applied to new documents as well as to new Wikipedia articles and yield useful semantic char-acterizations even if some words are not covered by the topic model. Hence it is not necessary to train the topic model with all Wikipedia articles as long as a sufficiently large sample is used.

As topic models do not require labeled training data, the approach presented in this paper can be applied to any lan-guage as long as unique descriptions of named entities are available (currently Wikipedia is available in 269 languages). To demonstrate that our approach is in general language in-dependent, we apply the proposed method to the English, the German, and the French version of Wikipedia, the cur-rently three largest versions. We also show that we achieve quite similar performance figures for all three languages.
In the next section we describe prior work in name disam-biguation, which is also often referred to as entity resolution or as record linkage in the field of databases. Then we outline the properties of Wikipedia as a reference for unique named entities (Sec. 3) and the construction of a disambiguated dataset of corresponding entity mentions (Sec. 3.1). Next, we describe the classification approach used in this paper (Sec. 4). Subsequently, we give a short overview of LDA (Sec. 5) and outline the different topic distances used for the-matic context comparison (Sec. 5.3). Then we present the experimental setup and discuss the obtained results (Sec. 6).
In this work, we study the task of name disambiguation using Wikipedia as a knowledge base with describing con-texts. Given a name mention m with surrounding text T ( m ), we want to assign this mention to the correspond-ing true entity e ( m ) described by a Wikipedia article. By matching the surface forms of names (e.g., first and last name) m we get a set of candidate entities or candidate ar-ticles E ( m ) = { e 1 ,...,e |E| } from Wikipedia, such that for all e j  X  X  ( m ) we have m = name ( e j ) in the case of an exact name match or m  X  name ( e j ) in the case of a partial name match. We denote the Wikipedia text around entity e j as T ( e j ). The task is to select one of the entities in E ( m ) as the correct entity ( e ( m ) = e j ) or to decide that e ( m ) is not covered by Wikipedia ( e ( m ) 6 X  X  ( m )).
Name disambiguation is closely related to the task of word sense disambiguation, which aims at resolving the ambiguity of words in a text. Both rely on the assumption that the meaning of a mention is strongly dependent on the context it appears in [17]. However, studies in the fields of word sense disambiguation and entity resolution do often not assume the presence of a reference knowledge base.

One of the first studies in the field of name disambigua-tion is that of Bagga and Baldwing [1]. They propose to cre-ate context vectors for each target name mention m , where each vector contains exactly the words that occur within a fixed-sized window around the ambiguous name. To cluster contexts referring to different entities, the authors measure the similarity among these vectors by the cosine measure. This algorithm was extended to a large corpus by Dill et al. [8].

Chen and Martin [6] combined lexical context features and extracted information in a vector space model: non-stop words appearing within a fixed window around any mention m of the entity, noun phrases in the sentences containing the ambiguous name, and named entities (persons other than the ambiguous name, organizations, etc.) in the entire doc-ument. The authors reported better performance compared to previously published results.

The outputs of relation extractors or named entity recog-nizers are certainly useful features for name disambiguation, but unfortunately they cannot be used out of the box for other languages, such as German or French. Creating such modules is often a laborious task, as it can require anno-tated training data or rely on language specific character-istics, such as capitalization and word order. For example, the German language has a free word order and a different capitalization scheme. Thus, both entity recognizers as well as relation extraction methods can be harder to construct. Because we wanted to apply our approach simultaneously to different languages without extensive manual annotation, we did not use these features.

To describe an entity [19] use a bag of features similar to [6]. Additionally, the authors employ entity profiles that combine attributes of the entity (links from the entity to a value), relations (to or from another entity), and events that this entity is involved in. As a third feature the authors es-timate a topic model with 50 topics and determined the top 10 words with highest probability for the document accord-ing to the topic model. Disambiguation is then performed via single-link hierarchical agglomerative clustering. Exper-iments show an improvement over the results presented in [6]. Srinivasan et al. generate words according to a topic model and then add these to the respective feature vector to overcome the synonymity problem [19]. In contrast, our approach relies on the overall topic probability distribution of a document, thus using a completely different feature vec-tor representation based on topic clusters instead of words.
Other approaches use relation evidence for entity reso-lution. Bhattacharya and Getoor [2] resolve ambiguity in the context of citations, considering the mutual relations between authors, paper titles, paper categories, and con-ference venues. For example, we may conclude that the names R. Srikant and Ramakrishnan Srikant refer to the same person, because both denote a coauthor of another author. Bhattacharya and Getoor argue that the joint res-olution of the identity of authors, papers, etc. leads to a better result than considering each type seperately.
Similarly, Hassel et al. [9] disambiguate researcher names in citations by exploiting relational information contained in an ontology derived from the DBLP database. Attributes, such as affiliations, topics of interests, or collaborators are extracted from the ontology and matched against the text surrounding a name occurrence. The results of the match are then combined in a linear scoring function that ranks all possible senses of the name of interest. The scoring func-tion is trained on a set of disambiguated name queries that are automatically extracted from Wikipedia articles. The method is also able to detect when a name denotes an en-tity that is not covered in Wikipedia.

Both of the last mentioned approaches require relational information, for example provided by a relation extraction system or a well-maintained ontology. However, this rela-tional information is currently not available for arbitrary persons and in multiple languages. Therefore we did not include these features into our analysis.

Cucerzan [7] presents a large-scale system for the recog-nition and semantic disambiguation of named entities based on information extracted from Wikipedia and Web search results. The system uses co-reference analysis to associate different surface forms of a name in a text, e.g., George W. Bush and Bush . Again, context words are combined with Wikipedia categories to describe entities. The pro-posed method then assigns entities by maximizing the non-normalized scalar products for the contexts of entities and name phrases.

Bunescu and Pasca [5] disambiguate entities using a Rank-ing SVM [12], which generates a ranked list of plausible en-tities for a given context T ( m ) of the target name mention m . Features are words in a window around m in combina-tion with Wikipedia categories of the articles describing the corresponding real-world entities. To apply this method for uncategorized knowledge bases, one has to estimate classi-fiers to predict the Wikipedia categories for the text T ( m ), which requires a considerable effort, especially if needed for several languages.

As we find the problem formulation in [5] most similar to ours, we re-implemented this method (outlined in Sec-tion 4.2.2) and use it as a baseline for comparison with our approach. Similar to our work, the reference corpus is ex-tracted from Wikipedia (for details, see Section 3.1).
In the last years, Wikipedia 1 has received much attention in the scientific community. Many projects and scientific studies build upon this free, constantly updated repository of world knowledge. Due to its sheer size, however, handling this semi-structured knowledge resource for data mining or information retrieval purposes is a challenge.

Most of the textual content in Wikipedia is stored in ar-ticles. The English, German and French versions currently contain about 3.6 million, 1.2 million, and 1.0 million articles respectively. About 15% of its articles describe biographies and persons and 14% of the articles cover geography and places [13].

Each article in Wikipedia is uniquely identified through its title, which is typically the most common name of the described subject. Ambiguous names are further qualified with additional terms, for example affiliations or professions, e.g., Jason Taylor (athlete) , Jason Taylor (jazz) , or Jason Taylor (racing driver) .

Articles usually hold information focused on one specific concept or entity, describing it in a concise but comprehen-sive way. The information is substantiated and enhanced by many links to other articles and citations using external doc-uments. In the following we consider the Wikipedia article as the definition of the real-world entity it refers to.
Additionally, each Wikipedia article is labeled with one or more categories. These categories can represent content topics but also more general attributes, such as gender in the case of persons or the founding year in case of organizations. Aside from very general categories there exist very specific categories that apply to only very few articles. In our version of the English Wikipedia, which was downloaded on 15th January 2011, we found 600k different categories.
 Relations between articles (entities) are expressed by links. When referring to an entity or concept with an existing ar-ticle page, contributing authors are supposed to link at least the first mention of the related entity to its correspond-ing article. Inspired by [5] we exploit Wikipedia X  X  internal link structure to generate an automatically disambiguated dataset of entity mentions, which is described in Section 3.1. http://www.wikipedia.org
To create a disambiguated training dataset, we assume the correctness of links in Wikipedia, and extract all arti-cles referencing an entity of interest. As the link provides the true entity, this results in a set of disambiguated entity mentions.

Note that this corpus is not really a gold-standard dataset, as the linking is done by different Wikipedia contributors and thus depends on their individual taste. Sometimes links are rather conceptional links, i.e., link to a thematically re-lated article, and do not mean identity. For example, the term client can be linked to the article Lawyer . Unfortu-nately, we are not aware of other publicly available bench-mark datasets (with links to Wikipedia), especially for Ger-man and French. Still, we assume that the model evaluated on this dataset can generalize to other corpora.
 In the following, we use the definitions of Section 2.1. Let L ( e j ) be the set of Wikipedia articles containing a mention m which is linked to a different article describing e j . The articles T ( m ) in L ( e j ) are also called query documents . Note that a pair ( T ( m ) ,T ( e j )) with T ( m )  X  L ( e j ) can be used to train the relation between the entity e j and its mention m . As we observed rather few articles without ingoing links we get positive examples for nearly all entities e j in Wikipedia. Negative examples may be constructed by relating an entity e to a mention m not referring to it, i.e., e ( m ) 6 = e j
Following [5] this set of disambiguated queries can be used for the training as well as the evaluation of a disambiguation model.

To construct such a disambiguated training data set, we first selected a set of entities e j for which referencing docu-ments (e.g., links in other articles) are to be found. Here, we consider persons and other entities with ambiguous names. As we did not focus on Wikipedia X  X  category hierarchy, we use YAGO [20] to determine articles referencing persons. YAGO is build jointly over Wikipedia and WordNet and endows Wikipedia articles with attributes, such as person, group, location, etc.. Each entity article, which is ambigu-ous, i.e., whose surface name corresponds to more than one Wikipedia article, is added to the entity dataset A .
Next we define a dataset Q containing the mentions cor-responding to A . Note that there are articles with more than 1000 incoming links while other articles have only one incoming link. To balance this inequality we added at most ten randomly selected referencing documents from L ( e j ) as examples to Q , also storing a reference to the the linked entity e j . This restriction allows a better balanced model over all entities in A . Otherwise, we would run in danger to introduce a bias towards frequently referenced entities. We ignored references outgoing from documents in A , such that Q  X  A =  X  .

As the text T ( m ) corresponding to a mention m we used the complete Wikipedia article containing m as this yields better results than just using a small window around m . We experimented with different context width configura-tions and found that an additional boost on the local context can improve disambiguation performance (see 5.3.1). We apply the same configuration for English, German and French, remove stop words from respective lists and use the stemmed word forms obtained by the Snowball algorithm [18] (again for the respective language).
Ignoring the possibility of different spellings of the name of an entity, we consider an entity e j only as a candidate for some name mention m , if its title matches the name mention in the query document. The assignment to one of the can-didate entities in E ( m ) = { e 1 ,...,e |E| } can be considered as a multi-class classification problem, with each e j one dis-tinctive class. Note that with increasing number of entities this classification problem rapidly becomes intractable, if we would want to learn one model per name (or per entity).
As an alternative, we can formulate this task as a binary classification problem, deciding if e ( m ) = e or e ( m ) 6 = e . Following [12], we create the classifier input using groups of feature vectors { ~  X  ( m,e j ) } e j  X  X  ( m ) , where each feature vector describing the pair ( m,e j ). To each ~  X  ( m,e we assign a label y For at most one e j we get a positive label. In the special case that e ( m ) is an uncovered entity and e ( m ) 6 X  X  ( m ), none of the describing feature vectors ~  X  ( m,e j ) receives a positive label.

This binary classification problem can be tackled with many different classifiers. We use the SV M Light 2 mentation by Thorsten Joachims [11] as learning framework employing different kernels. This classifier has shown ex-cellent performance in many text mining problems. It is trained with the feature vectors ~  X  ( m,e j ) constructed from Wikipedia.

For a new mention  X  m we can construct new feature vectors ~  X  (  X  m,e j ) for e j  X  X  (  X  m ). Subsequently, the estimated SVM is applied to all ~  X  (  X  m,e j ). Then the entity which receives the highest score is selected as the best match. Note that it is possible that two entities come from a very similar field (i.e., two politicians may be member of the same party). Then it is likely that both entities receive high scores for a given name mention m . The difference between the scores of the best and the second best match can be used as an indicator of the uncertainty of the decision.
As already discussed, there are millions of entities not present in Wikipedia, many of them appearing for example in the local parts of a news paper. To account for this, we simulate non-covered entities similar to [5]. In particular, we remove the Wikipedia article and consequently all provided information for a fixed fraction of entities e j . Consequently all mentions m previously linked to these e j now correspond to an entity not covered in Wikipedia and all training exam-ples get assigned to class y =  X  1. During the application of the estimated SVM to new data, we consider a mention m as uncovered , if the model predicts no score higher than the threshold 0 for any of the candidates ~  X  (  X  m,e j ): Note that for future work it would also be possible to fine tune this threshold on a validation set. Alternatively, [5]
The SV M Light package is available at http://svmlight. joachims.org/ have shown that using a linear kernel, this threshold can be learned automatically from the weight of an indicative feature.
We compare our proposed method to previous approaches to the task of name disambiguation and describe these at-tributes in the following.
Cosine similarity was used by [1] as one of the first ap-proaches to assess the similarity between two documents. [5] and [7] both evaluated experimentally a ranking function based on the cosine similarity between the text of the query document T ( m ) and the text of the entity X  X  article T ( e ). Specifically, this denotes cosine similarity as Here V ( T ( m )) is the standard vector space model of T ( m ), where each component corresponds to a term in the vocab-ulary and each entry to the word count in the respective context.

In the formulation in Eq. 3, cosine similarity is an un-weighted but normalized summation over common words (terms appearing both in the query document and the ar-ticle text). The larger this number the more similar are T ( m ) and T ( e ) and hence the more similar are the entities denoted.

Measuring the similarity between contexts in this way has one major drawback: if alternative terms for one concept are used in T ( m ) and T ( e ), the similarity will be low even if the contexts denote the same entity. Additionally, entities in similar context will be difficult to distinguish using such an aggregated measure.

We use the feature  X  cos as a baseline feature in the feature vectors ~  X  ( m,e j ) in all of the following experiments, as it evaluates directly matching words in the contexts of e j and m .
Bunescu and Pasca ([5]) used the categories of Wikipedia articles as particularly indicative features, to learn the mag-nitude of semantic correlations between words w and cat-egories c . In particular, the authors assume that common words are indicative, and create the following feature vector representation:  X  wc ( m,e ) = In this approach, T ( m ) is reduced to a context window of width 25 around the entity mention.

Here, the maximal dimension of ~  X  wc is restricted by | W | X  | C | , where | W | is the number of all possible words and | C | the number of all possible categories.

We re-implement this approach using the Ranking SVM included in SV M Light (and denote it as Bun 06 in the follow-ing), with the slight modification that we use only directly assigned categories, instead of analyzing the category hier-archy to extract top-level categories. Note that analyzing Wikipedia X  X  category hierarchy is not a trivial task, as we can encounter loops and other inconsistencies.

In this paper we present an approach that is applicable for different languages and does not rely on category infor-mation.
LDA recently achieved very much attention in a grow-ing number of research fields, ranging from text analysis to computer vision. One very attractive point of LDA is that it effectively generates low-dimensional representations from sparse high-dimensional data, representing documents by a low-dimensional vector of topics . In this section, we briefly review the most important aspects of Latent Dirichlet Allo-cation, for more details the interested reader is referred to [4] and references therein.

LDA is a Bayesian probabilistic model that describes doc-ument corpora in a fully generatively way [4]. It assumes a fixed number K of underlying topics in a document collec-tion D , where each document d is a mixture of topics t According to LDA, the observable variables, i.e., the words in a document, are generated as follows: First, for each doc-ument d , document-specific topic proportions  X  d are drawn according to  X  d  X  Dir (  X  ). The parameter  X  is the concen-tration parameter of the Dirichlet prior, which is a conve-nient conjugate to the multinomial distribution. Then for each word i in d a topic z di is randomly chosen accord-ing to z di  X  Mult (  X  d ). Finally the observed word w randomly drawn from the distribution of the selected topic, w di  X  Mult (  X  z di ). This overall topic distribution is assumed to be drawn randomly beforehand from a Dirichlet distribu-tion  X  k  X  Dir (  X  ), where beta is the prior vector on the per-topic word distribution.

The nature and influence of the priors  X  and  X  has been studied for example by Wallach et al. [22]. We use the Mal-let implementation [16], which automatically optimizes the prior  X  according to the underlying collection.

The resulting word distributions p ( w n | z n , X  ) for each topic have high probabilities for words that often co-occur in doc-uments. Topic models alleviate two main problems arising in natural language processing: synonymy and polysemy. Syn-onymy refers to the case where two different words (say car and automobile) have the same meaning. Synonyms usually will co-occur in similar contexts and hence belong to simi-lar topics. Polysemy, on the other hand, refers to the case where a term, such as plant, has multiple meanings (indus-trial plant, biological plant). Depending on the context at hand (e.g., the document is mostly about industry or biol-ogy) different topics will be assigned to the word plant.
We build topic models for each the languages treated in this paper. We extracted 100k random documents from the respective versions of the English, French, and Ger-man Wikipedia and use them as training documents for LDA. Most of these documents describe persons. We use the trained models to infer the probability of a topic t i each word w in an article d . The average probability of topic t for document d is the average of the probabilities of topic t for each word w in d . These yield topic distributions both for the query document with name mention m as well as the candidate articles e and define new attributes for both documents: The distributions P ( e ) and P ( m ) and functions thereof may be used as features in ~  X  ( m,e j ), as they are able to represent the proportions of semantically similar words.

Table 1 shows exemplary topics for Wikipedia articles with name John Taylor together with the titles (stemmed terms) for topics generated by Mallet 3 . For each topic t associated probability p ( t i ) is the probability of the given topic for the respective article text. For example, the most prominent topic ( t 80 ) derived for the athlete John Taylor describes his sportive success in the Olympic Games. The topic with lower probability ( p ( t 135 ) is only about 10%) can be interpreted as an indicator for his nationality. We see that most articles can be described sufficiently by one or two topic clusters which is a good example for the dimensional-ity reduction provided by LDA. One phenomenon in LDA is that depending on the document length the mixture of top-ics varies. While in very short documents we often find one very prominent topic (for example the racing driver), longer documents have a higher variety (e.g., the article describing the bass guitarist is the longest one in this example).
For each mention-entity pair ( m,e j ) we compare the topic distribution P ( m ) of the query document containing m with the distributions P ( e j ) for the candidates e j  X  E ( m ) com-puted over the corresponding article texts T ( e j ).
One of the simplest ways to compare the topic probabili-ties is a concatenation of the respective probabilities: Hence each of the probabilities is used as a separate feature and the classifier has the task to evaluate the differences.
Alternatively, we may compute distance terms between corresponding topic probabilities directly. There are a num-ber of distance measures for probability distributions that propose different weighting factors. Instead of just sum-ming differences in probability values to a scalar difference value we use the difference terms separately as features. Then the classifier can evaluate correlations between these terms/features to improve performance.

The symmetric Kullback-Leibler divergence[14] or relative entropy is very popular in the topic model literature:  X  Here we use the symmetric version as the T ( e ) and T ( m ) are interchangeable with respect to similarity. Each term  X  sKLD ( m,e ) i can be used as a scalar feature in ~  X  ( m,e The Jensen-Shannon distance [15], is derived from the Kullback-Leibler divergence and adds an additional factor The topics are taken from a model over a subset of Wikipedia with K = 200, | D | = 100 k .
 Table 2: F1 (micro) performance for name disam-biguation using different topic distance representa-tions and kernel types to (6):  X  where r = 0 . 5( p i ( m ) + p i ( e )).

The Hellinger distance [3] is another alternative measure for the similarity of probability distributions:
The maximum distance 1 is achieved when p i ( m ) assigns probability zero to every set to which p i ( e ) assigns a positive probability, and vice versa.

Except for  X  T C all feature representations yield a maxi-mum dimension of  X  (  X  )  X  R K , with K the number of topics.
We ignore topic indices if both p i ( m ) and p i ( e ) are less than 0.01. This is based on the assumption that we don X  X  need to spend modeling effort for uninfluential topics. Note that this has the side effect that the overall number of non-sparse features will be rather low, which speeds up the kernel computation.

We evaluated all of the above distances using five-fold cross-validation on a small dataset (similar to Q 1, see Table 3) with different kernels using SV M Light standard parame-ters to find the most appropriate distance and kernel for the task at hand.

As depicted in Table 2, using  X  T C yields the weakest re-sults with a linear kernel. A linear kernel can not model the interactions between the topics for e and m . Better re-sults can be achieved using a quadratic or an RBF kernel. The Hellinger distance also improves results over the Jensen-Shannon distance (significantly with p &lt; 0 . 05 only for the quadratic kernel). To summarize, we find that basically all of the above distances yield similar results, which we assume is based on their similar origin. Still, even though the su-periority of the symmetric Kullback-Leibler distance is not striking, we found it to be significant with p &lt; 0 . 05 for the linear kernel and p &lt; 0 . 06 for the quadratic kernel. Thus, we used this distance for all the experiments described in Section 6.
Both [1] and [5] observed that the direct context around an entity mention often contains most of the disambiguating information. Thus, we performed some initial experiments to evaluate the optimal context width. We found that re-ducing the context window to the 25 left and 25 right tokens around the entity mention (as done by [5], and also reported for the corresponding experiments here), yields a slight de-crease in predictive performance for our approach. We be-lieve that the reason behind this is that the inferred topic distributions are less smooth and thus also less reliable.
Thus, we propose to use the complete document of the entity mention m with an additional boost on the local con-text. We found that a context window of [10,10] around the mention yields the best result. The terms from this win-dow are added five times to the overall tokens of the query document m . We found that this overweighting of the local context increases the performance significantly ( p &lt; 0 . 05) in comparison to the unboosted version. Hence, we use the local boosting for all of the following experiments based on topic information.
In preliminary experiments, we also evaluated topic mod-els with different values of K for the task of entity disam-biguation. That is, we varied the number of topics clusters K from 50 to 500 on the same training corpus and found no major difference in the predictive performance when in-creasing the number of topics above 200.

Note that the Mallet implementation of LDA automat-ically optimizes the  X  parameter, such that topic models with the same number of topics but different initial values for  X  yield the same (or very similar) performance. For all topic models we used  X  = 0 . 01 as prior on the word-topic distribution. The approach proposed in this paper using symmetric Kullback-Leibler distance over topic distributions is denoted as sKLD . We compare our approach to [5] on the English version of Wikipedia. In the respective tables, we refer to this method as Bun06 .

We first describe the employed datasets, the properties of the different data sets are summarized in Table 3.
The first dataset, denoted Q 1 , constitutes of references for a random selection of persons 4 with an ambiguous name. The reference dataset A contains 6213 different entities, from which we randomly selected each fifth entity as an uncovered entity. After the removal of these uncovered entities, we have a ratio of 1242 uncovered vs. 4971 covered entities. We then
The information that an article refers to a person is ex-tracted from YAGO.
 Table 3: Statistics on the disambiguation datasets for different languages. | A | is the size of the reference data set, | Q | the size of the query dataset. extracted 16582 queries with 2.06 candidates each (e.g., we build 34197 feature vectors).

Note that in application data, for example news articles, entities are often referenced merely by the surname, which makes their distinction even more difficult. To account for this difficulty, we create an additional dataset denoted Q which we allow candidates on partial name matches as well. In contrast to the other datasets, a candidate is selected if the surface name is also partially contained in the candi-date X  X  title (without disambiguation term). For example, the surface name Jones can match Bruce Jones , Adam Jones , Catherine Zeta-Jones , but also Jones Soda or Jones, Okla-homa . This way we get more than 26 candidates per query mention and thus a highly ambiguous data set with more than 400k feature vectors representing all possible (name, entity)-pairs. Uncovered entities are modeled as above.
To show that our approach is in general language inde-pendent, we also report results for disambiguation using the German and the French Wikipedia. For the German version, we extracted 22211 articles (again persons 5 with ambiguous names), and 44332 query documents referencing the respec-tive entities. 4442 of the available entities were modeled as default entities. Here, we had in average 2.91 candidates per query, resulting in 129091 feature vector instances.
For the French version, we extracted 7201 articles (again persons with ambiguous names), and 15149 query docu-ments. 1440 of the entities were modeled as default entities. Here, we had in average 1.88 candidates per query, resulting in 28430 instances.

For both datasets, we trained a topic model with 200 top-ics on a random selection of articles from the respective ver-sion of Wikipedia. We used the same preprocessing tech-
We used language links from the English version to extract persons in other languages. Figure 2: Exemplary document and entity based splitting for cross-validation niques as for the English version, adapting only the stop word lists and the language of the employed stemmer.
We report results obtained on five-fold cross-validations for each of the datasets. Additionally, we propose two split-ting strategies for cross-validation: one based on documents, one based on the identity of the referenced entities.
The two splitting policies are depicted in Figure 2 for an exemplary set of documents and entities. The document based splitting (upper part of Figure 2) is analogous to the standard procedure for cross-validation, documents (exam-ples) are distributed i.i.d. over the buckets. The entity based splitting (lower part of Figure 2), in contrast, considers the true entities denoted in the query documents, and creates a splitting over the overall entity set.

Even though this can result in unbalanced bucket sizes (note that the number of query documents per entity varies and uncovered entities all fall into one bucket), this strategy allows for additional interpretation of the model X  X  ability to generalize, as the testing instances only contain previously unseen entities (together with context). The intuition be-hind the entity based splitting is that with new documents new entities described by previously unseen terms may ap-pear. Previously unseen terms constitute new features that can not be considered by a trained SVM model based only on words.

We use several performance indicators for evaluation that are commonly used in classification scenarios. We report on micro and macro performance (for more details see [23]), to asses both document and class (entity) based performance. Macro measurements constitute the averaged Precision (P), Recall (R) and F1-measure (F1) per entity. This is often used when the number of instances is unbalanced over the classes. In contrast, the formulation of micro performance computes the averaged Precision, Recall and F1 per docu-ment.
In Table 4 we report results for Q 1 using a linear as well as quadratic kernel for the topic based approach. As shown in Table 2 the RBF-kernel did not perform better, so we omitted experiments using this kernel. To emphasize the performance for uncovered entities, we report the accuracy for these mentions separately.

In the entity based splitting, the baseline Bun 06 approach completely failed to predict uncovered entities. As uncov-ered entities are observed only in one fold, Bun06 never got the chance to learn the appropriate decision threshold. Hence, we observe a rather low recall in micro performance, while in the macro performance the correct predictions for covered entities compensate this. In contrast, in the docu-ment based splitting (where a threshold could be learned), the method achieved an accuracy of 70.74% for uncovered entities.

We observe that using thematic context distance mea-sured by symmetric Kullback-Leibler distance ( sKLD ) yields better results in the prediction of uncovered entities for both splitting policies, even though we did not learn an adapted threshold 6 = 0. Considering macro performance, both sKLD approaches, using either a quadratic or a linear kernel, show significantly better results ( p &lt; 0 . 001) compared to the base-line approach. Thus, our method increases recall and preci-sion simultaneously in the disambiguation across all entities. The increase in recall compared to the baseline approach is significant in all experiments (document/entity split).
We found that even though we achieve higher precision as well, the improvement is only significant in the document based splitting. The entity based splitting has a higher vari-ance, which is both an effect of the differing bucket sizes as well as the distribution of uncovered entities.

Note that although the entity based splitting seems diffi-cult, the precision of all approaches is higher compared to the document based splitting. The document based split-ting has to treat the difficult uncovered entities in each fold, resulting in a lower precision but also a lower variance com-pared to the entity splitting.

Table 4 also shows the averaged SVM X  X  computation time (cpu-sec). These learning times are relative fractions, where 1 indicates the shortest time and all others are multiples. For example, a topic model with 200 topics and a quadratic kernel requires a more than ten times longer computation time than a model with the same number of topics but a linear kernel.

Comparing the explicit increase in learning time to the rather low increase in performance when using a quadratic kernel instead of a linear one (92.33% vs. 91.90% F1 in the entity splitting, resp. 90.44% vs. 88.28% F1 in the document splitting), we chose to use the linear kernel in the remainder of the experiments. Note that sKLD using a linear kernel is also about four times faster than Bun06 .
Especially for the highly ambiguous dataset Q 2 an SVM training run using a quadratic kernel is computationally ex-pensive. Table 5 shows the results for this dataset using a linear kernel. Even though we increased the SVM X  X  cost ratio for false negative predictions (as we have many more negative than positive examples), Bun06 does not achieve satisfying results for this dataset as the F-measure is always below 22%. We believe that this is because the approach is more focussed on the disambiguation of persons, while in this dataset many other entities (or entity types) appear. In contrast, our approach keeps up the good performance reported for the smaller corpus. Even though precision and recall are notably lower, we conclude that the proposed the-matic context distance is a very good measure for the dis-ambiguation of name phrases. We suspect that for larger datasets a more sensitive topic model (i.e., with more topics over more documents) might be needed. Recent approaches Table 5: Results for name disambiguation on the dataset Q2 Table 6: Name disambiguation using thematic con-text distance sKLD on the German dataset to speed up LDA on very large corpora (for example [10] or [21]) provide a useful tool to be explored.
Tables 6 and 7 show results for the disambiguation model using the German and the French dataset. In either sce-nario, the obtained F1-measure is well above 80%, with low derivation, which is a promising result. We find that al-though we did not spend additional efforts on the specific characteristics of these languages, we can very accurately assign name phrases to the corresponding articles.

On the relatively small French dataset, we also evaluated the quadratic kernel and found that performance could be increased mostly in the document splitting setting, by up to 3% in F-measure.

As LDA is a language independent method, these results are not surprising but nevertheless new: we have shown that the same approach to measure thematic context dis-tance works for various source languages and yields very good results. Note that apart from training the LDA model, which is unsupervised, no other language specific adapta-tions needed to be made.
 Table 7: Name disambiguation using thematic con-text distance sKLD on the French dataset
In this paper we approach the problem of name ambigu-ity using thematic distances over describing documents. We compare our approach to a state of the art method that ex-ploits word-category information extracted from Wikipedia. Our approach relies on semantic topics provided by LDA and is thus able to exploit more information than other, rather restrictive, word-matching methods. We significantly improve the assignment of name mentions to the underly-ing articles in Wikipedia. We also treat names that are not covered by an article in Wikipedia and show that our method can handle this problem very accurately, superior to the baseline approach. This is a crucial aspect: When we retrieve information for a known entity, we don X  X  want to assign false facts to it.

In future work we will apply the proposed method to the automatic updating of knowledge bases, which is a very in-teresting line of research. It opens the possibility to auto-matically generate articles in Wikipedia for entities that are currently not covered.
 The work presented here was funded by the German Federal Ministry of Economy and Technology (BMWi) under the THESEUS project. We would like to thank the reviewers for their constructive comments and suggestions on this paper. [1] A. Bagga and B. Baldwin. Entity-based [2] I. Bhattacharya and L. Getoor. Relational clustering [3] D. Blei and J. Lafferty. Topic models. In A. Srivastava [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] R. C. Bunescu and M. Pasca. Using encyclopedic [6] Y. Chen and J. Martin. Towards robust unsupervised [7] S. Cucerzan. Large-scale named entity disambiguation [8] S. Dill, N. Eiron, D. Gibson, D. Gruhl, R. Guha, [9] J. Hassel, B. Aleman-Meza, and I. B. Arpinar.
 [10] M. Hoffman, D. M. Blei, and F. Bach. Online learning [11] T. Joachims. Learning to Classify Text Using Support [12] T. Joachims. Optimizing search engines using [13] A. Kittur, E. H. Chi, and B. Suh. What X  X  in [14] S. Kullback and R. A. Leibler. On information and [15] J. Lin. Divergence measures based on the shannon [16] A. McCallum. Mallet: A machine learning for [17] G. Miller and W. Charles. Contextual correlates of [18] M. F. Porter. Snowball: A language for stemming [19] H. Srinivasan, J. Chen, and R. Srihari. Cross [20] F. Suchanek, G. Kasneci, and G. Weikum. Yago -a [21] M. Wahabzada, K. Kersting, C. Bauckhage, and [22] H. Wallach, D. Mimno, and A. McCallum. Rethinking [23] Y. Yang. An evaluation of statistical approaches to [24] N. Yuhanna. Today X  X  challenge in government: What
