 Spreadsheets contain valuable data on many topics. How-ever, spreadsheets are difficult to integrate with other data sources. Converting spreadsheet data to the relational model would allow data analysts to use relational integration tools.
We propose a two-phase semiautomatic system that ex-tracts accurate relational metadata while minimizing user effort. Based on an undirected graphical model, our sys-tem enables downstream spreadsheet integration applica-tions. First, the automatic extractor uses hints from spread-sheets X  graphical style and recovered metadata to extract the spreadsheet data as accurately as possible. Second, the interactive repair identifies similar regions in distinct spread-sheets scattered across large spreadsheet corpora, allowing a user X  X  single manual repair to be amortized over many possible extraction errors. Our experiments show that a hu-man can obtain the accurate extraction with just 31% of the manual operations required by a standard classification based technique on two real-world datasets.
 H.2.8 [ Database Management ]: Database applications-Data Mining Information extraction; spreadsheets; graphical model
Spreadsheets are a critical data management tool that are diverse and widely used: Microsoft estimates the number of worldwide Excel users at more than 400 million, and For-rester Research estimates 50 to 80% of businesses use spread-sheets 1 . Moreover, there is a large amount of data on the Web that is, practically speaking, only available via spread-sheets. For example, the United States government for many years published a compilation of thousands of spreadsheets about economic development, transportation, public health, Figure 1: A spreadsheet about smoking rates, from the Statistical Abstract of the United States. and other important social topics; a spreadsheet was the only data format used.

However, there is at least one area where spreadsheet functionality badly trails the relational world: data inte-gration. For example, imagine that a policy expert wants to see whether the strength of the connection between smoking and lung cancer is consistent across all U.S. states. Different branches of the government have published data relevant to his task, and he is likely to obtain the data via two down-loadable spreadsheets, one for the smoking statistics and one for the lung cancer statistics. Unfortunately, the policy ex-pert may have to write tedious customized code to combine the two spreadsheets for the analysis.

If spreadsheet data could be easily converted to the rela-tional model, many researchers  X  in public policy, public health, economics, and other areas  X  could benefit from so-ciety X  X  huge investment in relational integration tools. There has been a substantial amount of work in converting grid-style data to the relational model much of it in connection to Web HTML tables [4, 14], rather than spreadsheets. How-ever, projects to date [1, 2, 12, 18] have either been ex-tremely labor-intensive, or they have ignored data layouts that are very typical of spreadsheets.

For example, Figure 1(a) shows a portion of a spreadsheet downloaded from the government X  X  Statistical Abstract of the United States. 2 A human reader can easily tell that the data value 28.7 is annotated by the annotations 1990 , Male , White , and 45 to 64 years . We call this implicit re-lationship between annotations and data a mapping . By repeatedly finding such mappings, a human could eventu-ally reconstruct the relational tuples seen in Figure 1(b).
This annotation-to-data mapping is common in tabular data such as Web HTML tables and financial reports, but is especially common in spreadsheets. We manually examined 200 randomly selected spreadsheets from the Web and found that more than 32% of the spreadsheets in a general English-language Web crawl contain an implicit mapping between annotations and data. When examining the top ten Internet domains that publish the greatest number of spreadsheets, more than 60% of spreadsheets do so [7].
 Our Goal  X  This paper is to study a critical problem in spreadsheets: recovering mappings between annotations and data accurately and with low effort . Doing so opens up the opportunities for an ad-hoc spreadsheet integration tool [9], which a variety of people in society can use for data analysis.
Finding all the accurate mappings is important to avoid misleading results in our downstream spreadsheet integra-tion tool. But even a high-quality automatic extractor will eventually make a mistake, and obtaining fully accurate mappings is hard to achieve without incorporating user in-teractions into our extraction system. Thus, our goal is to extract fully accurate mappings with low human effort. Technical Challenges  X  Unfortunately, we face a number of challenges. First, the annotation relationship shown in Figure 1 is clear to a human because of the textual format-ting, but many other spreadsheets use different or contra-dictory formatting; methods based on formatting heuristics will be quite poor at reconstructing these relationships.
Second, many implicit mappings rely on human under-standing of domain-specific metadata. Consider a spread-sheet of US states that does not use any stylistic cues to distinguish Michigan from Midwest ; the human reader X  X  do-main knowledge is what makes the annotations recoverable.
Finally, our extraction system is designed to be interac-tive; the user will likely give the system very little labeled data. The system must be able to repair extraction errors and obtain accurate annotation mappings with an unusually small amount of user input.
 Our Approach  X  We propose a new two-phase semiauto-matic approach based on an undirected graphical model to extracting spreadsheet annotation-to-data mappings accu-rately and with little human effort.

First, the automatic extractor receives spreadsheets as in-put and computes a mapping without human interaction. Based on an undirected graphical model, it exploits single-spreadsheet graphical style hints, such as the font and ty-pographic alignment, that are obvious to a human observer. It also identifies and exploits correlated extraction decisions; these correlated decisions can appear within one spreadsheet or between two unrelated spreadsheets. Our resulting auto-matic extractor obtains accuracy that beats a baseline ap-proach by up to 91% on a large workload of spreadsheets.
Second, our system offers an interactive repair phase, in which a human repeatedly reviews and corrects the auto-matic extractor X  X  output until no errors remain. We expect a human will review the automatic extractor X  X  output. But Figure 2: Our user interface for repairing mappings. our interactive repair is more than simply asking a human to fix every single extraction error. We again exploit the cor-relations among different extraction decisions to make more effective use of each user repair operation. A user X  X  single re-pair can be silently and probabilistically applied to multiple possible errors, allowing us to amortize the user X  X  effort over many likely extractor mistakes. Building a model that can perform this amortization, and managing the inadvertent er-rors that such an approach might introduce (a problem we call backtracking ), is one of this paper X  X  core contributions.
Figure 2 shows an example of the user interface for ap-plying repairs. (We discussed this interface in more detail previously [9].) The left side of the diagram indicates the initial hierarchy obtained by the automatic extractor for Fig-ure 1. The dashed arrow shows that a user performs a repair by clicking and dragging White so that it becomes a child of Male , indicating that Male annotates White . This one re-pair operation triggers multiple error fixes, including setting Male to also annotate Black . By making our system part of the user X  X  natural review-and-repair loop, we can reduce the number of manual repairs by up to 71% when compared to our already-effective automatic extractor.

A critical component of both the automatic extractor and interactive repair is the detection of similar extraction de-cisions. By automatically constructing our own domain-specific metadata resource, we can more effectively detect these decisions than when using no metadata or when using an off-the-shelf resource such as Freebase [3].
 Background  X  In our previous work, we described the Web spreadsheet corpus and a basic form of the extractor [7]; we compared against and beat that basic extractor (see Sec-tion 7 for more detail). We also demonstrated the ad hoc in-tegration application [9]. We have not previously described the graphical model based extraction and repair technique that forms our core technical contribution in this paper. Contributions  X  In this paper, we focus on extracting the annotation-to-data mapping in spreadsheets. To the best of our knowledge, we are the first to present the semiautomatic extraction approach. Our contributions include:
We cover related work in Section 2, and define the annotation-to-data mapping extraction task in Section 3. Finally, we conclude with a discussion of future work in Section 8. There are three main areas of related work: Spreadsheet Management  X  Existing approaches for trans-forming spreadsheet data into databases fall into a few broad categories. First, rule-based approaches [2, 16, 18, 25] often require users to learn a newly defined language to describe the transformation process. The approaches are flexible but often require explicit conversion rules that are difficult and time-consuming for the user to compose. Second, there is a range of visualization systems [27] that help the user nav-igate and understand spreadsheets with visualization tech-niques, but the mechanisms are not able to extract relational data from spreadsheets. Finally, automated approaches are the most similar to ours. Abraham and Erwig [1] attempted to recover spreadsheet tuples, and Cunha et al. [12] primar-ily focused on the problem of data normalization. But their work assumes a simple type of spreadsheets and they did not address the hierarchical structures that are key to un-derstanding a huge portion of the online spreadsheet data. Tabular Data Extraction  X  There has been a large amount of work centered on extracting tabular data on the Web [4, 5, 14]. Most of these projects have focused on the details of identifying data-centric tables or on applications that can be built on top of them. HTML tables likely contain hierarchical-style data examples, but we are not aware of any research to date focused on this problem.
 Programming By Demonstration  X  The interactive re-pair component of our work is part of an intellectual thread that ties programming by demonstration [15, 20, 21, 28], mixed-initiative systems [17], and incorporation of user feed-back into extraction systems [6]. Many of these systems are driven by a programming language that the user must learn; our system does not require the user to learn a language, just to use a  X  X rag-and-release X  interface. Our solution X  X  de-sign, which alternates automatic and human-driven effort, is similar in spirit to Wrangler [15, 21] and mixed initiative systems [17, 19, 20, 28]. However, Wrangler-style techniques cannot be applied to our situation directly, as they gener-ally process data with standard textual cues that are often missing from real-world spreadsheets.
In this section, we briefly describe the spreadsheet data model and provide a short summary of a graphical model.
In its most generic incarnation, a spreadsheet is simply an M  X  N grid of cells, in which each cell can contain a string, a number, or nothing. In practice, most spreadsheets, especially the high-quality ones that carry data that we want to extract, have substantially more structures. We make two assumptions about the spreadsheets we will process without seriously compromising our approach X  X  generality.
 Data Frames  X  First, we focus on a prototypical form of spreadsheet that we call a data frame . Figure 3 shows the three components that make up a data frame: two rectangu-lar annotation regions ( left and top ) and a single rectangular data region . We previously addressed the problem of finding data frames in spreadsheets using a linear chain CRF [7]. Hierarchies  X  Second, we focus on hierarchical spread-sheets. We assume a spreadsheet is hierarchical if the anno-tations in the top or left annotation region exhibit a hierar-Figure 3: The three primary components of a data frame spreadsheet. chical tree structure of at least two layers. For example, the left annotation region of the spreadsheet in Figure 3 shows a hierarchical structure of three layers. In this paper, we primarily discuss left annotation hierarchies, but hierarchies also exist in top . However, top annotation hierarchies are generally easier to recover than left , as the row and column number in top are very strong and reliable clues [7]. Problem Statement  X  Thus, we now formally describe our implicit mappings recovery task.

Let A = { a 1 ,...,a N ,root } be a set of annotations in an annotation region, where root is a synthetic node as the root of every hierarchical tree. Given a p ,a c  X  A , we say ( a a ParentChild pair if a p is the parent of a c in the annotation hierarchy. For example, in Figure 1, (row-20, row-26) (the strings (Male, White) ) is a ParentChild pair, while (row-25, row-26) (the strings (65 years and over, White) ) is not.
The spreadsheet annotation-to-data mapping task, thus, amounts to recovering all the ParentChild pairs for its an-notation regions. For example in Figure 1, the solution for mappings in left is a set of all its ParentChild pairs { (row-19, row-20), ..., (row-32, row-37) } . We also call this the hierarchy extraction task.
A graphical model G [22] describes a joint distribution over a set of n random variables x = { x 1 ,...,x n } , where each variable x i takes a label l i from a set of labels L . The model captures properties of each variable and dependencies among variables in the graph by defining potential functions on cliques of correlated variables.

A common method to define the potentials is as a dot function between the weight parameters and a feature vec-tor [26]. A node potential captures the features that cor-respond to a single variable. The node potential is usu-ally defined on a variable x i as  X  ( x i ) = w 1 T f ( x f ( x i ,l i ) is a feature vector and w 1 is the associated weight parameters. Similarly, the edge potential is usually defined on pairwise variables x i and x j to describe their correlation domain knowledge via the feature vectors f , while the pa-rameters w = { w 1 , w 2 } are trained from labeled data. In the training stage, the feature vector is derived from a set of labeled data in order to obtain the optimal value for the weight parameters w . In the inference stage, the optimal la-beling can be obtained by finding the maximum joint prob-ability. As our model is conditionally trained, it belongs to the class of general graph conditional random fields [24].
In this section, we describe how to exploit the different sources of information and how to encode the automatic extraction as an undirected graphical model. We now formally describe our problem and observations. The task of hierarchy extraction is to detect all of the Par-entChild pairs P = { ParentChild ( a i ,a j ) } in an annotation region A . One way to model this problem is to create a Boolean variable x to represent a ParentChild pair candidate ( a p ,a c ) for every annotation pair a p ,a c  X  A . Each variable x takes a label l  X  L = { true,false } , and x holds true if a is the parent of a c . For example, Figure 4 shows a portion of the created variables for Figure 1 X  X  left metadata. Each oval node corresponds to a single boolean ParentChild decision. For example, setting the node (18 to 24 years, Male) to true indicates that 18 to 24 years is the hierarchy parent of Male .
However, simply enumerating all pairs in a region A can yield thousands of variables. In practice, it is possible to greatly reduce the set of ParentChild candidates with a few heuristics. 3 Failing to create a node for a true ParentChild relationship is bad, but not catastrophic: the user can still describe the correct relationship during interactive repair.
A true ParentChild variable may be indicated by the sur-rounding style and layout information. For example, a vari-able that describes annotations which are physically close is likelier to be true than a variable that describes annotations that are physically distant. We formulated 32 features for evaluating a ParentChild variable. The full set of features can be refered in our technical report version [8].
ParentChild decisions can be correlated; knowing the as-signment of one ParentChild variable sheds light on some others. We found the following four types of correlations. Correlation (i)  X  Stylistic Affinity. When two Par-entChild variables in the same spreadsheet have identical visual style for parents and for children, it is likely that the two variables should be decided together. For exam-ple in Figure 5 (a), the two ParentChild variables ( (White, College) and (Male, 18 to 24 years) ) should be decided to-gether because the parents ( White and Male ) share the same typographic style, as do the children ( College and 18 to 24 years ). We say that two variables have stylistic affinity when the parents and children share a range of visual qualities: alignment, indentation, capitalization, typeface, type size, type style ( i.e. , bold or italicized), and use of certain spe-cial strings ( i.e. , a colon, a number, or the word  X  X otal X ). Note that stylistic affinity only makes sense when testing ParentChild pairs within a single spreadsheet ; different Figure 5: An example of stylistic affinity shown in (a) and metadata affinity shown in (b). spreadsheets may have different or contradictory ways of vi-sually indicating the ParentChild relationship.
 Correlation (ii)  X  Metadata Affinity. If we have a metadata resource available, we can use it to find additional correlations between ParentChild variables both within and between spreadsheets. For example in Figure 5 (b), the two ParentChild candidates, (White, Female) and (Black, Male) , should be decided together because the parents ( White and Black ) belong to the same semantic category race ; similarly the children ( Female and Male ) belong to gender . The details of how to test whether two variables have metadata affinity are discussed in Section 6.
 Correlation (iii)  X  Adjacent Dependency. If we con-sider the ParentChild pairs of a single spreadsheet as a sequence, adjacent variables often follow a transition pat-tern of the labels.
 Correlation (iv)  X  Aggregate Design. There are two further constraints that reflect typical overall spreadsheet design and ensure that the resulting variable assignment yields a legal hierarchy ( i.e. , a tree).

The first is the orientation constraint . Spreadsheets tend to have an  X  X pward X  or  X  X ownward X  orientation; that is, par-ents do not appear above their children in some cases and below their children in other cases. For example in Figure 4, the pairs (Male, 18 to 24 years) and (25 to 34 years, 18 to 24 years) cannot simultaneously be true .

The second is the one-parent constraint . We enforce our assumption that ParentChild relationships genuinely form a tree; one annotation can only have one parent. Put another way, for all of the variables sharing the same child, only one of them is true and the rest are false . For example, in Figure 4, (Male, 25 to 34 years) and (18 to 24 years, 25 to 34 years) could not both be true.
Now we describe how we encode the ParentChild pair properties and their correlations into the graphical model. Node Potentials  X  Each variable x in the graphical model represents a ParentChild decision, which takes a label l  X  L = { true,false } . We define the node potential  X  ( x,l ) on each variable x assigned the label l . The node potentials depend on Boolean feature functions { f k ( x,l ) } (The 32 fea-tures mentioned in Section 4.1) and trained weights { w associated with the feature functions: Edge Potentials  X  The correlations (i) (ii) and (iii) men-tioned in Section 4.2 are encoded in the graphical model as pairwise edge potentials . The edge potential  X  ( x,l,x is defined on two variables x and x 0 in the graphical model on their assignments l and l 0 if the variables x and x 0 found to be correlated in one of the three ways mentioned above. We define, where J l = l 0 K takes the value 1 when l = l 0 and 0 other-wise; { w e } are the associated weights. The edge features { f e ( x,x 0 ) } test which type of correlation x and x 0 belong to and whether x and x 0 have the same child/parent.
 Global Potentials  X  Finally we encode the correlation (iv) mentioned in Section 4.2 as global potentials . Let x = ( a and x 0 = ( a 0 p ,a 0 c ) be two arbitrary variables in the graphical model with the assigned labels l and l 0 , and R ( a ) repre-sents the row number of an annotation a . We now define two global potentials :  X  a ( x , l ) to encode the orientation constraint and  X  b ( x , l ) to encode the one-parent constraint : where J C K value 2 value 1 takes the value 1 when condition C is true and value 2 otherwise.
The interactive repair phase allows the user to check and fix any ParentChild decision mistakes made by the system. The goal of interactive repair is to save user effort by using each explicit user-given repair to fix not just the error in question, but also additional extraction errors that the user never directly inspects. In this section, we describe the in-teractive repair workflow in more detail, plus how to modify the graphical model to support the repair process. Finally, we describe the training and inference methods.
During interactive repair, we assume a user always fixes extraction errors correctly. We do not focus on the problem of noisy human-labeled data, and there is crowdsourcing lit-erature on how to ensure trustworthy answers [13].
 We now discuss our model workflow for interactive repair. The system starts by presenting to the user the initial ex-traction results computed by the automatic extraction and then enters the interactive repair interaction loops (shown in Figure 6). For each loop, the system takes two steps: 1. Review and Repair  X  A user is able to repair an error in the extracted hierarchy by dragging and releasing an annotation node on the interface. One user repair action changes an annotation X  X  parent from one to another. For example in Figure 2, a user changes the parent annotation of White from Root to Male .

A user repair operation has two implications. First, the variable x that represents the new correct ParentChild rela-tionship is set to true . In the case of Figure 2, the variable (Male, White) is true . Second, all the other variables that represent ParentChild relationships sharing the same child with x are set to false . In the case of Figure 2, variables (Root, White) and (Total smoker, White) are false .
As a result, a user X  X  repair to an extraction error yields a set of label assignments to some ParentChild variables. 2. Spread Repairs  X  The system now aims to save user effort by repairing other similar extraction errors.
Of course, the system has already given its best extraction estimate in the automatic extraction phase, so it does not know where any latent extraction errors are. But we have Figure 6: Interaction cycle for interactive repair. Algorithm 1 SpreadRepair 1: From user repairs R , create repair-induced variables x already used different kinds of affinity to connect two Par-entChild decisions that are highly likely to share the same label; it is a shame to forget about this information just when the user is providing a new source of correct labels.
It is appealing to spread each user-repaired label on a vari-able to other variables that are identified by affinity correla-tions (i) and (ii). But simply propagating assignments might introduce errors where none previously exist, which we call the backtracking problem. We want to leverage the graphi-cal model to integrate probability information with the node, edge, and global correlations to prevent backtracking.
Here, we describe how to encode the user repair interac-tion to the graphical model. Algorithm 1 shows the Spread-Repair function that is invoked after each user repair op-eration (described in step 2 of the previous section). First, when a new repair arrives, we translate this new repair and all the previous repairs to the assignments on a set of vari-ables x r = { x r 1 ,...,x r n } with labels l r = { l r 1 ond, we generate a new graphical model G 0 by adding the re-pair potentials to the original automatic extraction graphical model G . The repair potentials capture the pairwise correla-tion between variables, and we describe the repair potentials later. Finally, we condition on the known variables x r and infer labels for the variables of G 0 . The inferred labels are returned as the updated answer.

Note that by adding repair potentials only to nodes that we also condition on, we add information to the inference process without increasing any inference-time computational complexity. The conditioning process essentially removes the observed nodes and their edges prior to the inference [22].
There is nothing in principle that prevents our system from backtracking , unless we can find heuristics to prop-agate the assignments fully correctly, which is often hard especially on real-world datasets. However, our mechanism is designed to prevent it. First, we only probabilistically propagate known variable assignments to others, via the re-pair potentials. Second, this probabilistic repair informa-tion is combined with all our previous information sources: the node potentials, edge potentials and global potentials. The hope is that adding high quality new information to the automatic extraction graphical model (instead of treat-ing spreading repairs as a non-probabilistic post-processing stage) will yield better outcomes overall.
 We now discuss how to generate the repair potentials. Repair Potentials  X  The repair potential  X  ( x,l,x r ,l r describes the likelihood that the repaired node X  X  label should be spread to a similar ParentChild node. A repair potential exists between an observed variable x r  X  x r and a variable x  X  x if x r and x exhibit either stylistic affinity or metadata affinity. In other words, repair potentials do not introduce any novel edges to the graphical model: the edges of repair potentials are a subset of the edges derived from correlations (i) and (ii). The repair potentials are defined as: J C K takes the value 1 when condition C is true; otherwise 0. Stylistic ( x,x r ) and Metadata ( x,x r ) test whether x and x have stylistic or metadata affinity. The two feature functions f s and f m weigh the strength of influence from observed variables to unobserved ones. They characterize how similar the unobserved variables are to the observed ones. To be precise, we define f s ( x,l,x r ,l r ) = logP s ( x = l | x where P s ( x = l | x r = l r ) represents the probability of a variable x taking the label l once we observe a variable x r with the label l r . This probability can be derived from training data. For example, in the training data, among 1000 stylistic affinity edges detected, 900 of them connect two variables with the same assignment. We then set P s ( x = true | x r = true ) = 0 . 9 and P s ( x = false | x r = true ) = 0 . 1. The f m potentials are defined in the same way.
 Summary  X  We can now formally define the spreadsheet annotation hierarchy extraction framework, which supports both automatic extraction and interactive repair.

Let G be a graphical model that has a set of variables x = { x 1 ,...,x n } where each x i  X  x represents a ParentChild candidate in an annotation region and takes a label l i from L = { true, false } . Let l r be the set of repair-induced labels on variables x r . We define node potentials (Equation 1), edge potentials (Equation 2), global potentials (Equation 3 and 4), and repair potentials (Equation 5) in G . The joint distribution of the graphical model G is: P ( l | l r , x ) = 1
In this section, we discuss how to train model parameters and infer assignments to variables in the graphical model.
In the graphical model, we only have unknown parameters for node and edge potentials . Assuming that no user repairs are involved, we can write the joint probability as, 1 Z ( w )
Let w = { w } be the set of parameters for node and edge potentials . Given training data D = { x , l } that describes hand-labeled correct hierarchies of the training spreadsheets, we estimate w for node and edge potential functions,  X  ( x,l ) and  X  ( x,l,x 0 ,l 0 ). A common choice of regularization to avoid overfitting is to add a penalty on weight vectors, based on the Euclidean norm of w and on a regularization parameter 2  X  2 . The goal is to maximize the regularized log likelihood: max Algorithm 2 EnforcedTreeInference 1: P  X  X } , conf idence  X  0 6: Obtain the probability cprob that x = true 7: if cprob &gt; maxprob then 9: end if 10: end for 12: conf idence  X  conf idence + log ( maxprob ) 13: end for where C is a constant. This is a standard form for param-eter estimation, and known techniques, such as conjugate gradient and L-BFGS, can be used to find the optimal pa-rameters for this formula. Previous work [22, 24] discusses this optimization problem and its solution in more detail.
The graphical model described poses a serious computa-tional challenge. Inference is NP-hard if no assumptions are made about the structure of the graph [11], yet our appli-cation requires that we infer labels after each user repair to redisplay the updated hierarchy. In order to infer variables in interactive time, we first simplify the graphical model. Model Simplification  X  The potential stumbling blocks to efficient inference are the edge and global potentials. (The repair potentials do not complicate the inference because the conditioning algorithm [22] erases observed variables along with all the repair potential edges.) The edge potentials alone can yield more than a million edges on a graph with 37,386 nodes derived from just 100 randomly-chosen WEB spreadsheets (see Table 3 for details).

We considered two methods for conducting inference in a limited amount of time: running the tree-reweighted be-lief propagation algorithm [23] on the full graph, or running an exact inference method on a simplified tree-structured model. Our experiments show that when running on a model derived from 100 random SAUS spreadsheets and repeating this process 10 times, tree-reweighted belief prop-agation is 48 times slower and 5.4% worse on F1 than the tree-structured model. Thus, at inference time we convert our graphical model into a tree-structured model.

It is not easy to find the tree-structured graphical model that yields the highest-quality results. Exhaustively enu-merating all the possible trees in a graph with more than a million edges and 37,000 nodes is impractical. We simply randomly sample edges from each type of pairwise correla-tion (stylistic, metadata, and adjcency), rejecting any edge that would induce a cycle. We terminate when all nodes are connected. We add all possible metadata edges before adding any stylistic edges, and add all stylistic edges before adding any adjacency edges. We found experimentally that this ordering helped slightly, though different orderings do not change F1 very much: testing on 100 random spread-sheets of SAUS , different orderings changed F1 from 0.8808 to 0.8867 and from 0.8237 to 0.8363 when testing on WEB . Inference  X  We can now present our method for approx-imating the graphical model X  X  optimal assignment. First, we build the model with node potentials, tree-structured edge potentials , and all the repair potentials if there exist any. Given a set of observed variables x r with labels l translated from users X  repairs (we assume x r is empty if no repairs are observed), the conditioning algorithm yields a forest-structured model.

Second, we run a standard inference algorithm on this new model to obtain the assignment to all the variables. Because the model is now a forest-structured, a variety of existing algorithms, such as belief propagation, can perform exact inference on such a structure.

Finally, we treat the global potentials as a post-processing stage to ensure that the inferred variable assignment yields legal hierarchical trees for the input annotation regions. The goal of global potentials is to handle the orientation and one-parent constraints. Thus, we first enumerate all of the Par-entChild candidates of each orientation,  X  X pward X  or  X  X own-ward, X  and compute two separate annotation hierarchies with EnforcedTreeInference , seen in Algorithm 2. For all the ParentChild candidates with a given annotation as the child, the algorithm selects the one with the maximal proba-bility (derived from the graphical model), thereby handling the one-parent constraint . We obtain two possible hierar-chies, one  X  X pward X  and one  X  X ownward, X  each with com-puted confidence . We select the one with the higher con-fidence to handle the orientation constraint . Therefore, our algorithm yields legal annotation hierarchies.
A critical part of both automatic extraction and interac-tive repair is detecting metadata affinity. As described in Section 4.2, ParentChild variables might be correlated be-cause they describe data belonging to one semantic category. This information is useful for examining annotations within a single spreadsheet, and is the only way to tie ParentChild decisions across multiple spreadsheets.

General-purpose schema resources, such as Freebase [3], can be used to detect metadata affinity between two annota-tions. But spreadsheet domains can be quite narrow. Fortu-nately, we are able to synthesize a domain-specific metadata resource from a corpus of spreadsheets. Our central observa-tion is that any useful category of annotations  X  whether a general-purpose one like gender or a hyper-specific one such as chemicalPrecursor  X  will likely appear in many datasets. Further, annotations drawn from the same category (such as Male and Female ) often appear as siblings in an extracted annotation hierarchy. We measure whether two annotations belong to the same category by testing how strongly the an-notations appear as siblings in a large number of extracted hierarchies. We perform the test as follows: 1. Extract all annotation hierarchies from a corpus of 2. Count the number of sibling sets where an annotation 3. Count the number of sibling sets where the annotation
Table 1: Basic statistics of our eight test sets.
We can then measure the extent to which two annotations a and a j are observed as siblings (and thus are likely to be in the same category) by computing the pointwise mutual
Let x 1 = ( a p 1 ,a c 1 ) and x 2 = ( a p 2 ,a c 2 ) be two variables in the CRF. The two variables x 1 and x 2 have metadata affinity if and only if PMI ( a p 1 ,a p 2 ) &gt;  X  and PMI ( a  X  , where  X  is a predefined threshold.
We now evaluate the performance of automatic extrac-tion (Section 4) and interactive repair (Section 5). We also evaluate the quality of our metadata resource (Section 6).
Our experiments are based on two spreadsheet corpora 4 :
From each of the two datasets, SAUS and WEB , we ran-domly selected 200 hierarchical spreadsheets . We call these test sets SAUS R200 and WEB R200 . We constructed them by randomly sampling from SAUS or WEB and re-taining only the hierarchical ones ( i.e. , ones that have either hierarchical left or top annotations). In addition, we con-structed a series of topic-specific test sets. For SAUS , we used government-provided category labels to identify spread-sheets for each of three topic areas: health , finance , and transportation ; we chose 10 random hierarchical spread-sheets from each topic. For WEB , we used URL domain names as a rough proxy for the category label, choosing 10 random hierarchical spreadsheets from each of bts.gov , usda.gov , and nsf.gov . We asked a human expert to manu-ally examine the above spreadsheets and create ground truth hierarchies. Details about the test sets are shown in Table 1.
We used the Python xlrd library to access data and for-matting details of spreadsheet files. Our graphical model was implemented with UGM [29].
In this section, we evaluate the performance of the auto-matic extraction phase. We evaluate the automatic extrac-Table 2: Performance of the automatic extractor on SAUS and WEB R200 datasets. tion X  X  accuracy in predicting correct ParentChild relation-ships by using standard metrics of Precision , Recall , and F1 . We trained and tested automatic extraction using SAUS R200 and WEB R200 . We randomly split each of the two datasets equally for training and testing. We trained pa-rameters on the training set and constructed one graphical model for the test set. We repeated the split-and-test pro-cess 10 times, computing average Precision , Recall and F1 .
We have previously discussed the simple classification based approaches for automatic extraction [7], and the approaches proposed are equivalent to AutoLR and AutoGlobal as below. Automatic Models  X  A naive method AutoBasic to solve the hierarchy extraction problem is to use simple features (i.e. local alignment and indentation information) to classify two annotations as having a ParentChild relationship or not and assigns the most probable parent to each child.
We compared four different configurations of the auto-matic extraction graphical model with AutoBasic to demon-strate the power of each component of our automatic ex-tractor: AutoLR uses node potentials only (with no edge or global potentials, the model is equivalent to the logistic re-gression, or LR, method) 5 . AutoEdge uses node potentials and edge potentials. AutoGlobal uses node potentials and global potentials. Finally, AutoFull uses all three potential types and reflects the entire contents of Section 4.3. 6
Table 2 shows the performance of the five methods. We can see that all of our four graphical models significantly out-performed the baseline AutoBasic . Both partial models  X  AutoEdge and AutoGlobal  X  performed better than AutoLR , indicating that both edge and global potentials independently helped to improve the performance of automatic extraction. AutoFull , the model that includes all three potential types, is the best of all (though AutoFull  X  X  margin is small in the case of
SAUS ). We noticed that many extraction errors are due to contradictory spreadsheet formatting; designers of differ-ent spreadsheets may have conflicting designs, but even the format within one spreadsheet may not be consistent. Training Data  X  We wanted to know if our supply of training data was limiting the automatic extractor X  X  accu-racy. We conducted a test in which we artificially con-strained the training set size derived from SAUS R200 and WEB R200 , building a series of automatic extraction mod-els with varying amounts of training data. Figure 7 shows the F1 of the ParentChild pairs for AutoFull as we change the size of the training set. The growth in both SAUS and Figure 7: Performance for automatic extractor using different amounts of training data.
 WEB accuracy plateaus after a certain size. This analysis does not mean more training data cannot help, but does indicates that additional gains will likely be expensive. Domain Sensitivity  X  We also examined whether the WEB automatic extractor X  X  accuracy varies with the qual-ity of the spreadsheet. It is difficult to precisely describe a spreadsheet X  X  quality, so as a proxy we use the rank of the spreadsheet URL X  X  Internet domain, when sorted in de-scending order of the number of spreadsheets hosted by the domain. Figure 8 shows the average F1 within each Internet domain X  X  spreadsheets. We followed the same training and testing procedure as in the Automatic Models part above. The figure shows that the publisher X  X  rank (or the quantity of spreadsheets it publishes) does not correlate with extrac-tion performance. However we did find that spreadsheets from lower ranked domains are less likely to pass our initial  X  X ierarchical data frame spreadsheet X  filter.

In summary, our system shows substantially better per-formance than the baseline AutoBasic method, a 91% im-provement in F1 on SAUS and a 76% improvement in F1 on WEB . We now turn to interactive repair to shrink the user X  X  burden even further.
We now evaluate the performance of the interactive repair phase. We use the eight datasets described in Section 7.1. For each R200 of SAUS and WEB , we again randomly split the dataset into 100 training spreadsheets and 100 test-ing spreadsheets. We further randomly split the 100 testing spreadsheets into 10 subgroups with 10 spreadsheets in each, as R10 ; we then averaged the performance over the 10 sub-groups. We created one model for each test set (health, finance, etc ), except R10 , where we created one model for each subgroup. Table 3 shows basic statistics for the inter-active repair graphical models constructed for our test sets.
The metric of success for interactive repair is the amount of user work reduced when compared to simply fixing all the errors made by automatic extractor. We evaluate the amount of user effort by counting the required number of drag-and-drop repair operations to fix all the extraction er-rors in an annotation hierarchy, via our visual repair tool (seen in Figure 2). In the experiments, we simulated a user who randomly chooses extraction errors to repair, and who never makes a mistake. The user repairs errors until no er-rors remain. For each dataset, we ran this process 20 times and counted the average number of repairs performed. No-tice that the maximum number of possible repair operations for a given hierarchy is the number of annotations in it.
For each result shown in Figure 9, Figures 10 and 11, we normalize the number of required repairs by the maximum possible number of repairs in that dataset ( i.e. , the number Figure 9: The normalized repair number for inter-active repair on SAUS and WEB test sets.
 Table 3: Basic statistics for each test set X  X  interac-tive repair model. of annotations). Thus, smaller bars are better, and results should be comparable across datasets.
 Repair Models  X  A baseline method RepairBasic to incor-porate interactive repair is to tie the ParentChild variables in one spreadsheet if the parents share the same formatting and so do the children: if a user changes one decision, the system automatically applies the change to the tied ones.
We also evaluated six different versions of our extraction system. AutoLR and AutoFull are the automatic extractors described in the above section; we assume a user simply fixes all of their extraction errors one after another. RepairLR, Re-pairEdge, RepairGlobal and RepairFull are created by adding repair potentials to the previous four automatic extraction models. RepairFull is the full system described in Section 5.
Figure 9 shows the normalized number of repair opera-tions of different interactive repair systems. RepairFull per-formed the best of all, requiring just 7.2% of the maximum number of possible repairs when averaged over all test sets. In contrast, AutoFull (itself a dramatic improvement over the automatic extraction baseline) requires 15.4% of the maxi-mum; our exploitation of user repairs thus allows us to re-duce the user burden by an additional 53%. AutoLR , an automatic extractor without joint inference, yields an even worse average of 23.3% ; we improve by 69%. The absolute number of user repairs is reasonable: RepairFull requires be-tween 2 and 3.5 repairs per sheet for SAUS , and between 1.38 and 2.94 repairs per sheet for WEB .

Note that applying user repair information naively yields terrible results: RepairBasic requires 60.2% of the maximum possible number of repairs, much worse than even AutoLR . Figure 10: The normalized repair number for four interactive repair configurations.
 F igure 11: The normalized repair number required by different configurations of metadata links.
 In all the datasets, RepairFull always improves or matches AutoFull , which indicates that our repair mechanism is gen-uinely beneficial to users; we managed to prevent backtrack-ing and did not create more work for users. The same is not true for AutoLR vs RepairLR , which backtracks in the cases of SAUS /health and WEB /usda.

We further investigated interactive repair by considering different possible configurations of the interactive repair model on different test sets (shown in Figure 10). The Figure shows that both edge and global potentials are useful in reducing user burden, and using all of them helps the most.
 Spreadsheet Grouping  X  We also investigated the in-fluence of two spreadsheet grouping methods on interactive repair performance. (1) By topic : We group spreadsheets according to their human-given topic labels (such as finance and health) or their URL hostnames (such as bts.gov and nsf.gov); and (2) By Jaccard similarity : We compute the clusters by creating a graph in which each spreadsheet is a node, and edges exist when two spreadsheets have Jaccard similarity (computed over the non-numeric strings from each spreadsheet) greater than a threshold of 0.6. We find all weakly connected components in the graph as the spread-sheet groups. Note that grouping spreadsheets should only impact metadata affinity, as metadata affinity is the only way to connect ParentChild decisions across spreadsheets.
For both SAUS and WEB , we ran each grouping tech-nique, then randomly selected 3 groups of size 2, 3 groups of size 5, and 3 groups of size 10. For each group, we first built one RepairFull on this group of spreadsheets and com-puted the number of repairs required to eliminate all the extraction errors. We then compared against the sum of repairs needed by RepairFull when running on each spread-sheet of the group in isolation. We found that grouping by topic only reduces repairs up to 5.8% on SAUS and 2.1% on WEB , while grouping by Jaccard similarity reduces repairs by up to 64.0% in SAUS and 84.6% in WEB .

Thus, Jaccard similarity grouping yields a massive reduc-tion in necessary user repairs when compared to topic group-ing. We did not present these results in Figures 9 and 10 because we believe that highly coherent clusterings will only be possible in certain situations. Shared spreadsheet tem-plates is one such situation; another is when the metadata resource is of especially high quality (perhaps even curated by hand), allowing interactive repair to find otherwise invis-ible connections among independent spreadsheets.
 Metadata Resources  X  The quality of our metadata re-source clearly impacts metadata affinity. Figure 11 shows the normalized number of repairs required by different meta-data resource configurations of RepairFull , when run on Jaccard-clustered spreadsheets mentioned above in Spreadsheet Group-ing . We compared the approach based on our metadata resource from Section 6 ( RepairFull-Metadata ) against a no-metadata technique ( RepairFull-Style ) and a technique that uses Freebase to discover metadata affinity ( RepairFull-Freebase ). (In that last case, two annotations have metadata affinity if they share the same Freebase topic.) The figure shows that in all cases, our RepairFull-Metadata technique performs the best, usually followed by RepairFull-Freebase . On average, our induced metadata resource reduces user effort by 34.4% when compared to the Freebase resource. Note that some of the spreadsheets we process are on extremely technical topics (such as currency trading, health care, and minerals processing) that are unlikely to be captured in a general-purpose metadata resource such as Freebase.
 Runtime Performance  X  After each user repair opera-tion, users have to wait for the model to recompute the new result. In our experiments, each repair X  X  inference took 0.7s on R10 in SAUS and 3.2s on R10 in WEB on average. All other test datasets took less than 0.7s, except for nsf (at 4.7s). The results indicate that interactive repair is compu-tationally feasible, at least for relatively small datasets.
Overall, we have demonstrated that our RepairFull extrac-tion system can extract accurate spreadsheet hierarchies us-ing just 7.2% of the maximum possible human effort, a re-duction of 53% compared to AutoFull , our automatic ex-traction system (itself a significant improvement over previ-ous automatic extraction techniques). These numbers apply to real-world datasets; in certain cases where spreadsheets share a large amount of metadata, we can improve the factor even further. Moreover, our system works well on domain-specific datasets with no explicit user-provided metadata.
We have described a semiautomatic framework for ex-tracting data from spreadsheets. This system can derive accurate extractions with dramatically lower user effort than required by a traditional system. It should enable individu-als and organizations to better exploit the large amount of data currently locked away in spreadsheet files.

In the future work, we would like to automatically and preemptively integrate spreadsheets with the data resources in an organization: relational databases, unstructured docu-ments, even data-centric images, such as plots. The resulting cross-type integrated database could be used as the basis of a general query tool that can ignore distracting details of how each data item happened to be stored.
The authors are grateful for feedback from Michael An-derson, Dolan Antenucci, Matthew Burgess, Jun Chen, H. V. Jagadish, Barzan Mozafari, Yongjoo Park, Alex Roper, Jian Tang, and especially Bob Vogel. This work was sup-ported by National Science Foundation grants IIS-1054913 and IIS-1064606, and gifts from Dow, Google, and Yahoo!.
