 Learning Vector Quantization (LVQ) [1] is a supervised learning technique for nearest prototype classification. Duri ng recent years, many variants [2] [3] of LVQ have been proposed. These methods have been used as effective pattern recognition tools in a variety of applications.

However, LVQ and LVQ-based methods have some shortcomings. In the first place, it needs a user to initialize the value of the prototypes. Therefore, the learning performance will be affected by the initial values. What X  X  worse, the online learning can not be indeed realized since many people often initialize them with part of training samples. Additionally, the number of prototypes for every class is arbitrarily predetermined without any valuable prior knowledge. Last but not least, these methods can X  X  handle the incremental data set.
As described herein, we propose an autonomous learning method named in-cremental learning vector qu antization (ILVQ) for supervised classification. The goal is to develop a network that can operate autonomously in a non-stationary data environment. We need not predetermine the number or value of prototypes before learning. The proposed ILVQ can grow dynamically, learning the number of prototypes for each class needed to solve a current issue and adjusting the number and value of prototypes during training based on a given application. Ac-tually, ILVQ can realize an incremental learning task in the meanings of within-class incremental learning and between-c lass incremental learning. Within-class incremental learning means that the net work learns new knowledge incremen-tally within the same class, whereas betw een-class incremen tal learning means that it incrementally learns new knowledge that comes from a new incoming class. In other words, ILVQ not only adds prototypes within a class (within-class incremental learning); it also incrementally adds the number of classes during training (between-class in cremental learning). Furth ermore, ILVQ can eliminate the noise from input data. Consequently, the sequence of input patterns in the training set will impart little influence on the ILVQ performance. In this section, we introduce the basic idea of the proposed incremental learning vector quantization (ILVQ), and give the detailed algorithm.
 Shown in Figure 1 is the overall structure of ILVQ. It X  X  a three-layer network. The first layer is input layer that is used to input patterns x into the network. The second layer is competitive layer. Prototypes are stored in this layer to conduct competitive learning with each other. The competitive result within a class i is: where n i is the number of prototypes in class i and w i j is the value of the jth node in class i . The last layer is used to output the result of the network. The competitive result s that records the winner pro totype in each class is where s i is the prototype in class i that is nearest to x And we obtain the computed class label x of the input pattern x
In this network, not only the number of prototypes n i for each class i but also the class number c are incrementally learned and adjusted. The number and value of prototypes for each class will be reported when the learning is finished. The complete algorithm of ILVQ is given in Algorithm 1.
 To fulfill the incremental task, the network should be dynamically growing. Thus we need to sequentially insert proper nodes into the network. However, permanent insertion policy may result in interminable growth of network and overfitting. Therefore, we must decide when and how to insert a new prototype and when insertion is to be stopped. Here, we adopt a threshold method like this: a pattern from new classes that haven X  X  been learned before should be obviously inserted into the network. But for patterns from learned classes, firstly we find the two prototypes  X  X inner X  and  X  X unner-up X  that are nearest to the input pattern x . Then the insertion operation can be conducted when the distance between x and w winner is larger than a certain thre shold or distance between x and w runnerup is larger than threshold of runner-up. The network may be interrupted by probable noises if we simply take the distance between  X  X inner X  and x into account. So the location of  X  X unner-up X  is also considered. This scheme ensures ILVQ to automatically generate proper number of prototypes needed for each class.

However, if the threshold T is too small, all samples will form an isolated prototype. To the contrary, each input pattern will be attributed to only one prototype. To determine T needs some prior knowledge, but we often can not avail them. Furthermore, the threshold should not be constant during training. Otherwise the prototypes in the network may increase endlessly. According to the above discussion, we propose a new method to dynamically compute the threshold distance T in Algorithm 2. We set the average distance between i and other prototypes in the same label with i as i  X  X  within-class distance, and set the distance between i and prototypes whose label is different from i  X  X  as its between-class distance.

From the discussion in [4], establishing reasonable topological structure is ben-eficial for us to detect probable noise in the network. We apply the topological representing rule to connect two prototypes when they become the winner and the runner-up of an incoming pattern. In an online environment, the prototypes in the network can not remain their neigh bor relationship constantly. The pro-totypes that are neighboring at an early stage won X  X  be neighboring at a more advanced stage. Thus it is necessary to re move connections which have not been updated recently.
 We improve the competitive learning rule to update the value of prototypes. First, we divide the prototypes in the neighbor region of winner into two sets C 1 and C 2 according to their label values. The prototypes that have the same label as x are put into C 1 . And the remaining prototypes are put into C 2 .And we update the value of prototypes in this way: when label winner i = label x ,we
Algorithm 2. Compute the distance threshold T i of prototype vector i update winner and prototypes in set C 2 . On the contrary, we update winner and those prototypes in C 1 . In this case, the directions that these prototypes move are exactly different from the winner: if the winner is moved towards  X  , these prototypes will be moved far from  X  . So the distance between each of the prototype in C 2 and winner will be larger. It is beneficial for us to distinguish those patterns from different classes.

Noise may be interfused in the input data. We need to take some efforts to delete those prototypes in the network that are likely to be noise. Here, we take the strategy in [4]: for every several epochs of learning, we try to remove those nodes with only one or no topological neighbor. In [4], this strategy works well for removing nodes caused by noise without adding computing load.

In Algorithm 1, the learning rate  X  1 and  X  2 affect the extent the  X  X inner X  and its neighbors move towards or far away from the input pattern. In this study, we adopt a method used in [5] to adapt the learning rate over training epoch varying and we can make the position of prototype more stable when it becomes a winner for more and more times during training. Due to the divergence of the series  X  n =1 1 n and the convergence of  X  n =1 1 n 2 , the learning speed of our network satisfies stochastic approximation conditions. 3.1 Artificial Data Set In this section, we conduct our experiment on the two dimensional data set as shown in Fig. 2. Details of the artificial data set can be found in [5]. We apply Fig. 2 to simulate real-world data: 50000 patterns are randomly picked up from all classes, and we add some noise (20% of useful data) into training set. For ILVQ, we set parameters as  X  = AgeOld = 16. During the training, the number and value of prototypes are automatically determined by the algorithm itself. When the training is finished, each class has different number of prototypes: class 1, 2, 3, 4 and 5 have 8, 9, 11, 6 and 6 prototypes respectively. It means ILVQ can use fewer prototypes to represent a simple class, and use relative more prototypes to represent a complex class. Such prototypes are connected by edges to represent topological structure of input data, as shown in Fig. 3. We can see not only within-class connections but between-class connections are learned. We highlighted between class conn ections by black line in Fig. 3.
 3.2 Handwritten Digits Recognition Now we use Optdigits data sets from UCI Machine Learning Repository [6] to test the proposed algorithm. For ILVQ, we apply three times ten-fold cross-validation policy to tune the parameters:  X  = AgeOld = 450. Traditional LVQ and Generalized-Learning V ector Quantization(G-LVQ)[7], an improved version of LVQ, are used to make comparison. For stationary environment and incremen-tal environment, we repeat the algorithms eight times to obtain average results respectively, as shown in Table 1 and Table 2.
Table 3 lists recognition rate and compression ratio (the number of proto-types dividing data set size) of ILVQ under different parameters. It means the recognition rate and compression ratio of ILVQ is not sensitive to its parameters. Moreover, we find that in a non-stationary environment LVQ and G-LVQ can only recognize the patterns from the very class (class 10) that are lastly input into the network. The learned patterns ( from class 1 to class 9) were destroyed when the LVQ or G-LVQ learned patterns from class 10. But ILVQ can deal with the incremental data environment well because the learned patterns can be always soundly preserved in the network. 3.3 Other UCI Data Sets Now we sequentially use some other data sets from UCI Machine Learning Repos-itory to test the proposed algorithm. To better evaluate the proposed method, be-sides LVQ and G-LVQ, we compare it to so me other supervised classifiers such as KMC, NNC, and Nearest Subclass Classifier(NSC)[8]. We apply ten-fold cross-validation procedure three times to estimate recognition performance. We list the results of ILVQ in both stationary and incremental environment in Table 4. Be-cause all the above classifiers except NNC can not cope with incr emental learning task, we only list the results in stationary environment. The parameters obtained by ten-fold cross-validation procedure and compression ratio ( r c )areshownin Table 5. We can find that ILVQ realize a best compromise between recognition performance and compression ratio. And the performance of ILVQ in incremental environment is as well as it in stationary environment. In this paper, we propose an online incremental algorithm for supervised learn-ing. The proposed ILVQ applies a threshold-based insertion criterion to grow the network. An innovative method is proposed to adjust the value of the threshold dynamically. Due to the very criterion, the number of prototypes for each class can be generated automati cally and adjusted adaptively during learning. Thus users no longer need to predetermine it . ILVQ network can grow gradually and store the learned patterns perfectly so th at it can realize the in cremental learning well. A proper deletion policy is taken to ensure ILVQ to successfully eliminate the possible noise that comes into the network during learning.

In the experiments, we compare ILVQ with LVQ and other typical prototype-based classifiers. The results show that the recognition rate of ILVQ is better under most conditions. In the respective of compression ratio, the performance of ILVQ is nice as well. Although some algorithms such as KMC and BTS achieve a better compression ratio, we should note ILVQ fulfill the best compromise between classification perfo rmance and storage efficien cy. Moreover, the experi-ments demonstrate ILVQ can handle the in cremental learning task perfectly.
