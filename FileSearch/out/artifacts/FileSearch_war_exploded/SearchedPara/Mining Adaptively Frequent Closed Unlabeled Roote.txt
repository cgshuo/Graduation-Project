 Closed patterns are powerful representatives of frequent pat-terns, since they eliminate redundant information. We pro-pose a new approach for mining closed unlabeled rooted trees adaptively from data streams that change over time. Our approach is based on an efficient representation of trees and a low complexity notion of relaxed closed trees, and leads to an on-line strategy and an adaptive sliding win-dow technique for dealing with changes over time. More precisely, we first present a general methodology to identify closed patterns in a data stream, using Galois Lattice The-ory. Using this methodology, we then develop three closed tree mining algorithms: an incremental one IncTreeNat , a sliding-window based one, WinTreeNat , and finally one that mines closed trees adaptively from data streams, Ada-TreeNat . To the best of our knowledge this is the first work on mining frequent closed trees in streaming data varying with time. We give a first experimental evaluation of the proposed algorithms.
 H.2.8 [ Database applications ]: Database Applications X  Data Mining Algorithms Data streams, closed mining, concept drift, patterns, trees
Tree-structured representations are a main key idea per-vading all of Computer Science; many link-based structures may be studied formally by means of trees. From the pars-ing structures in Compiler Design and Natural Language Processing, to the B+ indices that make our commercial Database Management Systems useful, through search-tree or heap data structures, tree automata, the decision tree structures in Artificial Intelligence and Decision Theory, or the now-ubiquitous XML, they often represent an optimal compromise between the conceptual simplicity and process-ing efficiency of strings and the harder but much richer knowledge representation formalisms based on graphs. Ac-cordingly, a wealth of slight variations of the basic notions, both of the structures themselves (binary, bounded-rank, unranked, ordered, unordered) or of their relationships (in-duced or embedded, top-down or bottom-up subtree rela-tions) have been proposed for study and motivated applica-tions. In particular, mining frequent trees is becoming an important task, with broad applications including chemical informatics, computer vision, text retrieval, bioinformatics, and web analysis. We focus on finding navigation patterns in web sites and web logs, so we are interested on unlabeled induced rooted trees, thus our relevant information is the root and the link structure. Unlabeled trees are also a pre-vious step to mine labeled trees, a more powerful pattern in applications.

Closure-based mining on purely relational data, that is, itemset mining, is, by now, well-established, and there are interesting algorithmic developments. Sharing some of the attractive features of frequency-based summarization of sub-sets, it offers an alternative view with advantages; first, by imposing closure, the number of frequent sets is heavily re-duced and, second, the possibility appears of developing a mathematical foundation that connects closure-based min-ing with lattice-theoretic approaches like Formal Concept Analysis.

Data streams are defined as data sequences that arrive at high speed. They are so large that we may not be able to store all of what we see, and we do not have too much time to process each item. Several applications naturally generate data streams, a prime example being log records or click-streams in web tracking and personalization. The unlabeled rooted tree is an interesting pattern to obtain from this data. The most frequent way to deal with continuous data streams evolving on time, is to keep in memory a window of examples and refresh its model every time change is detected.
We propose a general methodology to identify closed pat-terns in a data stream, using Galois Lattice Theory. Using this methodology, we develop three closed tree mining al-gorithms: IncTreeNat , an incremental closed tree mining algorithm; WinTreeNat , a sliding window closed tree min-ing algorithm; and finally AdaTreeNat , an adaptive closed tree mining algorithm.
AdaTreeNat is a new algorithm that can adaptively mine from data streams that change over time, with no need for the user to enter parameters describing the speed or na-ture of the change. We take a recently proposed algorithm ( ADWIN ) [4] for detecting change and keeping updated statis-tics from a data stream, and use it as a black-box in place or counters or accumulators. Since ADWIN has rigorous per-formance guarantees, this opens the possibility of extending such guarantees to the new algorithm.

The rest of the paper is organized as follows. We discuss related work in Section 2. Sections 3 and 4 give background and introduce our closure operator and its properties needed for our algorithms. Section 5 introduces the general mining framework and Section 6 shows how to adapt this framework to deal with concept drift. Section 7 shows its application to tree structures. Experimental results are given in Section 8, and some conclusions in Section 9. There is a large body of work done on itemset mining. An important part of the most recent work is related to data streams; see the survey [12] and the references there. We can divide these data stream methods in two different classes depending on whether they use a landmark window or a sliding window. Only a small part of these methods deal with frequent closed mining. Moment [5], CFI-Stream [13] and IncMine [11] are the state-of-art algorithms for mining frequent closed itemsets over a sliding window. CFI-Stream stores only closed itemsets in memory, but it must main-tain all closed itemsets as it does not implement a minimum support threshold. Moment stores much more information besides the current frequent closed itemsets, but it has a minimum support threshold to reduce the quantity of pat-terns found. IncMine proposes a notion of semi-FCIs that consists in increasing the minimum support threshold for an itemset as it is retained longer in the window.

There have been subsequent efforts in moving towards closure-based mining on structured data, particularly se-quences, trees and graphs. One of the differences with closed itemset mining stems from the fact that uniqueness of set-theoretic intersection no longer holds: whereas the intersec-tion of two sets is a set, the intersection of two sequences or two trees is not one sequence or one tree. This makes it nontrivial to justify the word  X  X losed X  in terms of a stan-dard closure operator. Many papers resort to a support-based notion of closeness of a tree or sequence [6]; others (like [1]) choose a variant of trees where a closure operator between trees can be actually defined (via least general gen-eralization). In some cases, the trees are labeled, and strong conditions are imposed on the label patterns (such as non-repeated labels in tree siblings [16] or nonrepeated labels at all in sequences [9]). Chi et al. proposed CMTreeMiner [6], the first algorithm to discover all closed and maximal fre-quent labeled induced subtrees without first discovering all frequent subtrees. CMTreeMiner shares many features with CloseGraph [18].

A lot of research work exist on XML pattern mining. Asai et al. [2] present StreamT, a tree online mining algorithm that uses a forgetting model and is able to maintain a slid-ing window, but it extracts only frequent trees, not closed ones. Hsieh et al. [10] propose STMer, an alternative to StreamT to deal with frequent trees over data streams, but without using a sliding window. In [8], Feng et al. present SOLARIA*, a frequent closed XML query pattern mining al-gorithm, but it is not an incremental method. Li. et al [14] present Incre-FXQPMiner, an incremental mining algorithm of frequent XML query patterns, but it does not obtain the closed XML queries, neither it uses a sliding window.
As we are interested in web link structure we focus on unlabeled trees. Labeled trees are trees in which each vertex is given a unique label. Unlabeled trees are trees in which each vertex has no label, or there is a unique label for all vertices. A comprehensive introduction to the algorithms on unlabeled trees can be found in [17].

To the best of our knowledge this is the first approach defined for mining frequent closed trees in streaming data that evolve with time.
Patterns are graphs, composed by a labeled set of nodes (vertices) and a labeled set of edges. The number of nodes in a pattern is called its size . Examples of patterns are itemsets, sequences and trees [20].

Given two patterns t and t 0 , we say that t is a subpattern of t 0 , or t 0 is a super-pattern of t , denoted by t t 0 exists a 1-1 mapping from the nodes in t to a subset of the nodes in t 0 that preserves node and edge labeling. As there may be many mappings with this property, we will define for each type of pattern a more specific definition of subpattern. Two patterns t , t 0 are said to be comparable if t t 0 or t Otherwise, they are incomparable. Also t  X  t 0 if t is a proper subpattern of t 0 (that is, t t 0 and t 6 = t 0 ).

The (infinite) set of all patterns will be denoted with T , but actually all our developments will proceed in some finite subset of T which will act as our universe of discourse.
The input to our data mining process, now is a given finite dataset D of transactions, where each transaction s  X  D consists of a transaction identifier, tid , and a pattern. Tids are supposed to run sequentially from 1 to the size of D . From that dataset, our universe of discourse U is the set of all patterns that appear as subpattern of some pattern in D .
Following standard usage, we say that a transaction s sup-ports a pattern t if t is a subpattern of the pattern in trans-action s . The number of transactions in the dataset D that support t is called the support of the pattern t . A subpattern t is called frequent if its support is greater than or equal to a given threshold min sup . The frequent subpattern mining problem is to find all frequent subpatterns in a given dataset. Any subpattern of a frequent pattern is also frequent and, therefore, any superpattern of a nonfrequent pattern is also nonfrequent (the antimonotonicity property).

We define a frequent pattern t to be closed if none of its proper superpatterns has the same support as it has. Gen-erally, there are much fewer closed patterns than frequent ones. In fact, we can obtain all frequent subpatterns with their support from the set of frequent closed subpatterns with their supports. So, the set of frequent closed subpat-terns maintains the same information as the set of all fre-quent subpatterns.

Itemsets are subsets of a set of items. Let I = { i 1 ,  X  X  X  ,i be a fixed set of items. All possible subsets I 0  X  I are item-sets. We can consider itemsets as patterns without edges, and without two nodes having the same label. In itemsets the notions of subpattern and super-pattern correspond to the notions of subset and superset.

Sequences are ordered list of itemsets. Let I = { i 1 ,  X  X  X  ,i be a fixed set of items. Sequences can be represented as  X  ( I 1 )( I 2 ) ... ( I n )  X  , where each I i is a subset of I , and I before I j if i  X  j . Without loss of generality we can assume that the items in each itemset are sorted in a certain order (such as alphabetic order). In sequences we are interested in a notion of subsequence defined as following: a sequence s =  X  ( I 1 )( I 2 ) ... ( I n )  X  is a subsequence of s 0 i.e. s s 0 , if there exist integers 1  X  j 1 &lt; j 2 ... &lt; j such that I 1  X  I 0 j 1 ,...,I n  X  I 0 j n .
 Trees, viewed as patterns, are discussed in more detail in Section 7.
Song et al.[15] introduced the concept of relaxed frequent itemset and we adapt it to pattern mining. The support space of all subpatterns can be divided into n = d 1 / intervals, where r is a user-specified relaxed factor, and each interval can be denoted by I i = [ l i ,u i ), where l ( n  X  i )  X  r  X  0, u i = ( n  X  i + 1)  X  r  X  1 and i  X  n . Then a subpattern t is called a relaxed closed subpattern if and only if there exists no proper superpattern t 0 of t such that their suports belong to the same interval I i .

Relaxed closed mining is a powerful notion that reduces the number of closed subpatterns in data streams where ap-proximation is acceptable.

We can define Relaxed support as a mapping from all pos-sible dataset supports to the set of relaxed intervals. We can apply it to our mining algorithms, replacing the calls to support values, to relaxed support values.

We introduce the concept of logarithmic relaxed frequent of c generating n intervals. Depending on the closed pat-tern distribution on the dataset, and the scale of supports of interest, the notion of logarithmic support may be more appropiate than the linear one.
In this section we develop our approach for closed pattern mining based on the use of closure operators. Most previous approaches defined  X  X losed X  patterns in terms of support. This essentially leaves antimonotonicity as the only math-ematical property to be exploited. Our approach relies on much richer mathematics, which, as usual, leads to more interesting algorithmics.

The following concept is standard in mathematics. If X is a set with a partial order  X  , a closure operator on X is a function C : X  X  X such that x  X  C ( X ), x  X  y implies C ( x )  X  C ( y ), and C ( X ) = C ( C ( X )). A Galois connection is defined by two functions, relating two lattices in a certain way. Here our lattices are plain power sets of the transactions, on the one hand, and of the corresponding subpatterns, in the other. On the basis of the binary relation t t 0 , the following definition and proposition are rather standard.

Definition 1. The Galois connection pair:
Proposition 1. The composition  X  D =  X   X   X  D is a clo-sure operator on the subsets of D .

We point out the following easy-to-check properties: 1. t  X   X  D ( { t } ) 2.  X  D 1  X  X  2 ( { t } ) = { t 1  X  t 2  X   X  t 1  X   X  D 1 ( { t } ) ,t
We can relate the closure operator to the notion of clo-sure based on support, as previously defined, as follows: t is closed for D if and only if:  X  D ( { t } ) = { t } .
Proposition 2. Adding a pattern transaction to a dataset of patterns D does not decrease the number of closed patterns for D .

Proof. All previously closed patterns remain closed. A closed pattern will become unclosed if one of its superpat-terns reach the same support, but that is not possible be-cause every time the support of a pattern increases, the sup-port of all its subpatterns also increases. 2
Proposition 3. Adding a transaction with a closed pat-tern to a dataset of patterns D does not modify the number of closed patterns for D .
 Proof. Suppose s is a subpattern of a closed pattern t . If s is closed then  X  D ( { s } ) = { s } . If s is not closed, then  X 
D ( { s } )  X   X  D ( { t } ) = { t } . Increasing the support of the closed pattern t will increase the support of all its subpat-terns. The subpatterns that are closed will remain closed, and the ones that are non-closed, will remain non-closed be-cause the support of its closure will increase also. 2
Proposition 4. Deleting a pattern transaction from a dataset of patterns D does not increase the number of closed patterns for D .

Proof. All the previous unclosed patterns remain un-closed. A condition for an unclosed pattern to become closed is that its superpatterns with the same support modifies their support, but this is not possible because every time we decrease the support of a superpattern we decrease also the support of this pattern. 2
Proposition 5. Deleting a pattern transaction that is re-peated in a dataset of patterns D does not modify the number of closed patterns for D .

Proof. Adding a transaction with a previously closed pattern to a dataset of patterns D does not modify the num-ber of closed patterns for D . So deleting it does not change the number of closed patterns. 2
Proposition 6. Let D 1 and D 2 be two datasets of pat-terns. A pattern t is closed for D 1  X  D 2 if and only if  X  { t } .
 Proposition 6 follows from the definition of closed pattern. We use it as a closure checking condition when adding a set of transactions to a dataset of patterns.

Corollary 1. Let D 1 and D 2 be two datasets of pat-terns. A pattern t is closed for D 1  X  X  2 if and only if Closed Subpattern Mining Add ( T 1 ,T 2 ,min sup,T ) 1 T  X  T 1 2 for every t in T 2 in size-ascending order 3 do if t is closed in T 1 4 do support T ( t )+ = support T 2 ( t ) 5 for every t 0 that is a subpattern of t 6 do if t 0 is in T 1 7 then if t 0 is not updated 8 then insert t 0 into T 9 support T ( t 0 )+ = support T 2 ( t 0 ) 10 else 11 skip processing t and all its subpatterns 12 do if t is not closed in T 1 13 do insert t into T 14 for every t 0 that is a subpattern of t 15 do if t 0 is not updated 16 then if t 0 is in T 1 17 then support T ( t 0 )+ = support T 2 ( t 0 ) 18 if { s  X  t  X   X  s  X   X  T 1 ( { t 0 } ) } = { t 0 } 19 then insert t 0 into T 20 support T ( t 0 )+ = support T 2 ( t 0 ) 21 else 22 skip processing t 0 and all its subpatterns 23 return T
Proposition 7. Let D be a dataset of patterns. A pat-tern t is closed for D if and only if the intersection of all its closed superpatterns is t .

Proof. Suppose that the intersection of all its closed su-perpatterns is t 0 and that t 0 6 = t , then t is not closed because it exists a superpattern t 0 with the same support. Also, sup-pose the intersection of all its closed superpatterns is t and that t is not closed. Then t 0  X   X ( { t } ) has the same sup-port as t , and it must be in the intersection of all the closed superpatterns of t . 2
We use Proposition 8 as a closure checking condition when deleting a set of transactions from a pattern set.
In this subsection we propose a new method to do incre-mental closed pattern mining. Every time a new batch of patterns D T 2 arrives we compute the closed pattern set of the batch D T 2 , and then we update the closed pattern set T using Closed Subpattern Mining Add as is shown in Figure 1.
 In words, let T be the existing set of closed patterns, and T 2 those coming from the new batch. For each closed pat-tern in D T 2 , we check whether the pattern is closed in T . If it is closed, we update its support and the support of all closed, as it is closed for T 2, we add it to the closed pat-tern set, as justified by Corollary 2, and we check for each of its subpatterns whether it is closed or not. In line 18, we use Proposition 6 to do the closure-check  X  T 1  X  T 2 { t 1  X  t 2  X   X  t 1  X   X  T 1 ( { t 0 } ) ,t 2  X   X  T 2 ( { t 0 the fact that  X  T 2 ( { t 0 } ) = { t } . Here  X  T 2 ( { t pattern in T 2. As we check all the subpatterns of T 2 in size-ascending order, we know that all closed subpatterns of t have been checked before, and therefore we can suppose that  X  T 2 ( { t 0 } ) = { t } .

The best (most efficient) data structure to do this task will depend on the pattern. In general, a lattice is the default option, where each lattice node is a pattern with its support, and a list of its closed superpatterns and a list of its closed subpatterns: We can use the lattice structure to speed up the closure check  X  T 1  X  T 2 ( { t 0 } ) = { t 0 } .
Adding a method to delete a set of transactions, we can adapt our method to use a sliding window of pattern trans-actions.

Figure 2 shows the Closed Subpattern Mining Delete pseudocode. We check for every t pattern in T 2 in ascending order if its subpatterns are still closed or not after deleting some transactions. We can look for a closed superpattern with the same support or use the closure checking condition given by Proposition 8: a pattern t is closed if the intersec-tion of all its closed superpatterns is t . The lattice structure supports this operation well. We can delete a transaction one by one, or delete a batch of transactions of the slid-Closed Subpattern Mining Delete ( T 1 ,T 2 ,min sup,T ) 1 T  X  T 1 2 for every t in T 2 in size-ascending order 3 do for every t 0 that can be reduced from t 4 do if t 0 is not updated 5 then if t 0 is in T 1 6 then if t 0 is not closed 7 then delete t 0 from T 8 else support T ( t 0 )  X  = support T 2 ( t 0 ) 9 else 10 skip processing t 0 and all its subpatterns 11 return T ing window. We delete transactions one by one to avoid recomputing the frequent closed patterns of each batch of transactions.
In this section we present a new method for dealing with concept drift in pattern mining, using ADWIN [4], an algo-rithm for detecting change and dynamically adjusting the length of a data window. First we briefly review the AD-WIN algorithm and then we describe our method combining the previous sliding window pattern mining algorithms and ADWIN . Recently, we proposed an algorithm termed ADWIN (for Adaptive Windowing) that solves in a well-specified way the problem of tracking the average of a stream of bits or real-valued items. ADWIN keeps a variable-length window of re-cently seen items, with the property that the window has at all times the maximal length statistically consistent with the hypothesis  X  X here has been no change in the average value inside the window X .

More precisely, an older fragment of the window is dropped if and only if there is enough evidence that its average value differs from that of the rest of the window. This has two con-sequences: one, that change reliably declared whenever the window shrinks; and two, that at any time the average over the existing window can be reliably taken as an estimation of the current average in the stream (barring a very small or very recent change that is still not statistically visible). A formal and quantitative statement of these two points (a theorem) appears in [4].

ADWIN is parameter-and assumption-free in the sense that it automatically detects and adapts to the current rate of change. Its only parameter is a confidence bound  X  , indicat-ing how confident we want to be in the algorithm X  X  output, inherent to all algorithms dealing with random processes.
Also important for our purposes, ADWIN does not main-tain the window explicitly, but compresses it using a variant of the exponential histogram technique in [7]. In particular, it keeps a window of length W using only O (log W ) memory rather than the O ( W ) one expects from a na  X   X ve implemen-tation. The processing time per item is also O (log W ).
We propose two strategies to deal with concept drift: 1. Using a sliding window, with an ADWIN estimator de-2. Maintaining an ADWIN estimator for each closed set in In both strategies we use Closed Subpattern Mining Add to add transactions. In the first strategy we use Closed Subpattern Mining Delete to delete transactions as we maintain a sliding window of transactions.

In the second strategy, we do not delete transactions. In-stead, each ADWIN monitors its support and when a change is detected, then the support may
In this section we apply the general framework above specifically by considering the tree pattern. Trees are con-nected acyclic graphs, rooted trees are trees with a vertex singled out as the root, and unranked trees are trees with unbounded arity. We say that t 1 ,...,t k are the components of tree t if t is made of a node (the root) joined to the roots of all the t i  X  X . We can distinguish betweeen the cases where the components at each node form a sequence (ordered trees) or just a set ( unordered trees ). We will deal with rooted, un-ranked trees. We do not assume the presence of labels on the nodes.

An induced subtree of a tree t is any connected subgraph rooted at some node v of t that its vertices and edges are subsets of those of t . An embedded subtree of a tree t is any connected subgraph rooted at some node v of t that does not break the ancestor-descendant relationship among the vertices of t . We are interested in induced subtrees. Formally, let s be a rooted tree with vertex set V 0 and edge set E 0 , and t a rooted tree t with vertex set V and edge set E . Tree s is an induced subtree (or simply a subtree ) of t (written t 0 t ) if and only if 1) V 0  X  V , 2) E 0  X  E , and 3) the labeling of V 0 is preserved in t . This notation can be extended to sets of trees A B : for all t  X  A , there is some t  X  B for which t t 0 .
 We represent each tree using natural representations [3]. The natural representation of a tree is a sequence over a countably infinite alphabet, namely, the set of natural num-bers. This encoding basically corresponds to a preorder traversal of t , where each number of the sequence repre-sents the depth of the current node in the traversal. As an example, the natural representation of the tree ple, the subsequence (1 , 2 , 2 , 3) corresponds to the bottom-up subtree rooted at the left son of the root.

The input to our data mining process is a given finite dataset D of transactions, where each transaction s  X  D consists of a transaction identifier, tid , and an unlabeled rooted tree. Figure 3 shows a finite dataset example. The closure operator defined for trees uses the following Galois connection pair:
The main results of Section 4 may be established for un-labeled trees as: Corollary 2. Let D 1 and D 2 be two datasets of trees. A tree t is closed for D 1  X  X  2 if and only if
Proposition 8. Let D be a dataset of trees. A tree t is supertrees is t .

The closed trees for the dataset of Figure 3 are shown in the Galois lattice of Figure 4.
 Figure 4: Example of Galois Lattice of Closed trees
In [3] the authors presented an algorithm for computing frequent and closed trees from a dataset of trees, in a non-incremental way. They represent the potential subtrees to be checked as frequent and closed on the dataset in such a way that extending them by one single node, in all pos-sible ways, corresponds to a clear and simple operation on the representation. The completeness of the procedure is assured, that is, all trees can be obtained in this way. This allows them to avoid extending trees that are found to be already nonfrequent.

The pseudocode of this method, Closed Subtrees Mining , is presented in Figures 5 and 6. Note that the first line of the algorithm is a canonical representative checking, a check that is used frequently in tree mining literature. In [3] the authors selected one of the ordered trees corresponding to a given unordered tree to act as a canonical representative: by convention, this canonical representative has larger trees always to the left of smaller ones.
We propose three tree mining algorithms adapting the Closed Subtree Mining ( t,D,min sup,T ) 1 if t 6 = Canonical Representative ( t ) 2 then return T 3 for every t 0 that can be extended from t in one step 4 do if support( t 0 )  X  min sup 5 do T  X  Closed Subtree Mining ( t 0 ,D, 6 do if support( t 0 ) = support( t ) 7 then t is not closed 8 if t is closed 9 then insert t into T 10 return T Figure 5: The Closed Subtree Mining algorithm Closed Mining ( D,min sup ) 1 t  X  2 T  X  X  X  3 T  X  Closed Subtree Mining ( t,D,min sup,T ) 4 return T Figure 6: The Closed Unordered Mining algorithm general framework for patterns presented in Section 5:
The batches are processed using the non-incremental al-gorithm explained in Subsection 7.1. We use the relaxed closed tree notion to speed up the mining process.
We tested our algorithms on synthetic and real data, com-paring the results with CMTreeMiner [6].
 All experiments were performed on a 2.0 GHz Intel Core Duo PC machine with 2 Gigabyte main memory, running Ubuntu 7.10. As far as we know, CMTreeMiner is the state-of-art algorithm for mining induced frequent closed trees in databases of rooted trees. CMTreeMiner and our algorithms are implemented in C++. The main difference with our approach is that CMTreeMiner is not incremental and works with labeled nodes, and we deal with unlabeled trees.
On synthetic data, we use the same dataset as in [6] and [19] for rooted ordered trees restricting the number of dis-tinct node labels to one. We call this dataset TN1, and is generated by the tree generation program of Zaki [19] avail-able from his web page. This program generates a mother
Time (sec.) Figure 7: Data experimental time results on ordered trees on TN1 dataset
Time (sec.) Figure 8: Time used on unordered trees, TN1 dataset tree that simulates a master website browsing tree. Then it assigns probabilities of following its children nodes, includ-ing the option of backtracking to its parent, such that the sum of all the probabilities is 1. Using the master tree, the dataset is generated creating subtrees by randomly picking subtrees according to these probabilities.

In the TN1 dataset, the parameters are the following: the number of distinct node labels is N = 1, the total number of nodes in the tree is M = 10 , 000, the maximal depth of the tree is D = 10, the maximum fanout is F = 10. The average number of nodes is 3.

The results of our experiments on synthetic data are shown in Figures 7, 8, 9 and 10. We changed the dataset size from 100 , 000 to 8 milion, and we observe that as the dataset size increases, IncTreeNat time increases linearly, and CMTreeM-iner does much worse than IncTreeNat . After 6 milion samples, in the unordered case, CMTreeMiner runs out of main memory and it ends before outputing the closed trees.
Figure 11 shows the result of the second following exper-iment: we take a TN1 dataset of 2 milion trees, and we introduce artifical concept drift changing the dataset trees from sample 500,000 to 1,000,000 and from 1,500,000 to 2,000,000, in order to have a small number of closed trees. We compare IncTreeNat , WinTreeNat with a sliding window of 500 , 000 and 1 , 000 , 000, and with AdaTreeNat We observe that AdaTreeNat detects change faster, and it quickly revises the number of closed trees in its output. On the other hand, the other methods have to retain all the
Memory Figure 9: Data experimental memory results on or-dered trees on TN1 dataset
Memory Figure 10: Memory used on unordered trees, TN1 dataset data stored in its window, and they need more samples to change its output number of closed trees.

To compare the two adaptive methods, we perform a third experiment. We use a data stream of 200 , 000 trees, with a trees and 20 different closed trees on the last 100 , 000 trees. The number of closed trees remains the same. Figure 12 shows the difference between the two methods. The first one, which monitors the number of closed trees, detects change at sample 111,480 and then it reduces the window size im-mediately. In the second method there are ADWIN s monitor-ing each tree support; they notice the appearance of new closed trees quicker, but overall the number of closed trees decreases more slowly than in the first method.

Finally, we tested our algorithms on the CSLOGS Dataset, available from Zaki X  X  web page [19]. It consists of web logs files collected over one month at the Department of Com-puter Science of Rensselaer Polytechnic Institute. The logs touched 13 , 361 unique web pages, and the CSLOGS dataset contains 59 , 691 trees. The average tree size is 12. Figure 13 shows the number of closed trees detected on the CSLOGS dataset, varying the number of relaxed intervals. We see that on this dataset support values are distributed in such a way that the number of closed trees using loga-Figure 11: Number of closed trees detected with artificial concept drift introduced Figure 12: Number of closed trees maintaining the same number of closed datasets on input data rithmic relaxed support is greater than using linear relaxed support. When the number of intervals is greater than 1,000 the number of closed intervals is 249, the number obtained using the classical notion of support.
We have presented efficient algorithms for mining ordered and unordered frequent unlabeled closed trees on evolving data streams. If the distribution of the tree dataset is sta-tionary, the best method to use is IncTreeNat , as we do not need to delete any past transaction. If the distribution may evolve, then a sliding window method is more appropi-ate. If we know which is the right size of the sliding window, then we can use WinTreeNat , otherwise AdaTreeNat would be a better choice, since it does not need the window size parameter.

Future work will be to do more experiments varying other tree parameters, and comparing it to other incremental meth-Figure 13: Number of closed trees detected on CSLOGS dataset varying Number of relaxed inter-vals ods as StreamT, if they are available. Most importantly, we want to apply the methodology explained in this paper to labeled trees, a pattern that has more applications than un-labeled trees, and in particula compare our methodology with CMTreeMiner using the same type of trees.
Partially supported by the EU PASCAL2 Network of Ex-cellence, and by the DGICYT MOISES-BAR project, TIN2005-08832-C03-03. Albert Bifet is supported by a Formaci  X o d X  X nvestigadors (FI) grant through the Grups de Recerca Consolidats (SGR) program of Generalitat de Catalunya.
The authors would like to thank Jos  X e L. Balc  X azar for his unconditional support and inspiring ideas and discussions. [1] H. Arimura and T. Uno. An output-polynomial time [2] T. Asai, H. Arimura, K. Abe, S. Kawasoe, and [3] J. L. Balc  X azar, A. Bifet, and A. Lozano. Mining [4] A. Bifet and R. Gavald`a. Learning from time-changing [5] Y. Chi, H. Wang, P. S. Yu, and R. R. Muntz.
 [6] Y. Chi, Y. Xia, Y. Yang, and R. Muntz. Mining closed [7] M. Datar, A. Gionis, P. Indyk, and R. Motwani. [8] J. Feng, Q. Qian, J. Wang, and L.-Z. Zhou. Efficient [9] G. C. Garriga and J. L. Balc  X azar. Coproduct [10] M. C.-E. Hsieh, Y.-H. Wu, and A. L. P. Chen.
 [11] Y. K. James Cheng and W. Ng. Maintaining frequent [12] Y. K. James Cheng and W. Ng. A survey on [13] N. Jiang and L. Gruenwald. CFI-Stream: mining [14] H.-F. Li, M.-K. Shan, and S.-Y. Lee. Online mining of [15] G. Song, D. Yang, B. Cui, B. Zheng, Y. Liu, and [16] A. Termier, M.-C. Rousset, and M. Sebag. DRYADE: [17] G. Valiente. Algorithms on Trees and Graphs . [18] X. Yan and J. Han. CloseGraph: mining closed [19] M. J. Zaki. Efficiently mining frequent trees in a [20] M. J. Zaki, N. Parimi, N. De, F. Gao,
