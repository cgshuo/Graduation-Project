 Short-length random walks on the bipartite user-item graph have recently been shown to provide accurate and diverse recommendations. Nonetheless, these approaches suffer from severe time and space requirements, which can be allevi-ated via random walk sampling, at the cost of reduced rec-ommendation quality. In addition, these approaches ignore users X  ratings, which further limits their expressiveness. In this paper, we introduce a computationally efficient graph-based approach for collaborative filtering based on short-path enumeration. Moreover, we propose three scoring func-tions based on the Bayesian paradigm that effectively exploit distributional aspects of the users X  ratings. We experiment with seven publicly available datasets against state-of-the-art graph-based and matrix factorization approaches. Our empirical results demonstrate the effectiveness of the pro-posed approach, with significant improvements in most set-tings. Furthermore, analytical results demonstrate its effi-ciency compared to other graph-based approaches.
  X  Information systems  X  Recommender systems; Top-k retrieval in databases; collaborative filtering; item recommendation; Bayesian statis-tics
The main objective of Recommender Systems (RS) is to guide users in a personalized way to interesting products to maximize user satisfaction and improve merchant rev-enue. These systems have become largely utilized in multi-ple business niches; the most relevant applications are rec-ommendation of products in online shopping websites like Amazon, movies in video portals like YouTube and friends in social networks like Facebook. RS fundamentally take one of two approaches, namely, Collaborative Filtering (CF) or Content-based Filtering, or show a combination of them. Loosely speaking, Collaborative Filtering uses the prefer-ences of similar users to select items, and Content-based Filtering uses features of items to find similar content.
The representation of users and the choice of a measure of similarity between them are two fundamental questions in the context of CF systems. In memory-based CF ap-proaches, n users are represented as vectors embedded in an m -dimensional vector space where each dimension corre-sponds to an item, and measures of similarity are often based on operations defined over this vector space. Thus, data are represented as an n  X  m user-item matrix where rows cor-respond to users, columns to items and each entry usually represents a rating. On the one, hand such approaches pro-vide a simple representation, but, on the other hand, it is computationally expensive to compute similarities between all pairs of users as the dataset size increases.

As an attempt to overcome the computational expense previously pointed out, the user-item matrix can be regarded as an adjacency matrix of a user-item undirected bipartite graph, giving rise to so-called graph-based approaches. As a result, measures of similarity between users are based on graph statistics such as commute or hitting time between nodes [4]. For instance, users that possess the same taste will be connected by a large number of short paths [4]. Page et al. [15] propose to treat an entity graph as a Markov chain whose long-term stationary distribution can be used as a global ranking scoring. There exists several works in the literature [1, 3, 4, 5, 9, 20] that make use of ran-dom walks in the context of RS. To this end, transition prob-abilities are defined so that they exploit the user-item graph structure. For instance, the transition probability between two items is proportional to the number of users that rated both items [9]. The authors in [1, 3] take a step further and show that it is possible to improve recommendation qual-ity by obtaining the distribution within three or five steps instead of incurring the computational burden of obtaining the long-term stationary distribution as in [4, 5, 9, 20].
Due to the size of the transition matrix, the methods pre-sented in [1, 3, 4, 5, 9, 20] suffer from memory limitation as the dataset size increases. In addition, these methods are computationally intensive as they require matrix mul-tiplications. To overcome these issues, Christoffel et al. [1] and Cooper et al. [3] propose to approximate the final dis-tribution by a sampling process. The proposed methods are applicable to medium or large size datasets, but recommen-dation quality depends on the sample size adopted in the sampling process. Thus, we claim that there is a lack of less resource demanding methods in the context of graph-based approaches that do not degrade the quality of recommenda-tions.

As a concrete example, given two items where one has 80 positive out of 100 ratings while the other has 20 positive out of 20 ratings, which item is likely to maximize user satisfac-tion in face of this information? In this scenario, we believe that the set of ratings given by users to items contains a valuable source of latent information that should be taken into account for recommendation. Thus, in this work we hypothesize that graph-based RS can benefit from the ex-ploitation of the set of ratings for recommendation and then improving user satisfaction. To the best of our knowledge, such hypothesis has not been investigated in the context of graph-based RS. Furthermore, the current state-of-the-art graph-based approaches do not extract any further informa-tion from the set of ratings to leverage recommendation. To fulfill this gap, we propose a CF method that not only relies on the user-item bipartite graph, but it also takes advantage of ratings given by the users to make recommendations.
In this work, we propose three scoring functions based on the Bayesian paradigm that are at the core of our recommen-dation approach. Similar to the works presented in [1, 3], we exploit three-step paths in the user-item bipartite graph starting from the target user with the advantage that we resort neither to matrix multiplications nor sampling pro-cesses. Instead, we propose the enumeration of all such paths hence overcoming the computational burden incurred by the allocation and multiplication of transition matrices. We analytically show that the enumeration process is more efficient than the approaches proposed in [1, 3].

We carried out experiments on seven datasets freely avail-able. The results show that our method clearly outperforms the approaches proposed in [1, 3] in all metrics used for com-parison. It is common sense in the RS literature that ma-trix factorization approaches are the state-of-the-art meth-ods for item recommendation in explicit feedback domains. Thus, we compare our method against matrix-factorization approaches, and results show that our method is able to im-prove their results in some datasets. The contributions of this paper are three-fold: a) we propose a more computa-tionally efficient graph-based approach than those presented in [1, 3]; b) we propose three Bayesian scoring functions that take advantage of the set of ratings for recommenda-tion; c) we present a comprehensive empirical evaluation of our method against state-of-the-art RS approaches across several datasets.
Fouss et al. [4, 5] propose a CF approach that relies upon random walks over user-item bipartite graph. They con-sider four similarity measures, namely, average first passage time, average commute time, Euclidean commute time dis-tance and pseudo inverse of the Laplacian matrix C + , where C = B  X  A , A represents the adjacency matrix, B is a diago-nal matrix where each entry represents vertex degrees associ-ated to the user-item bipartite graph. Computational results show C + provided the best results for a movie dataset.
ItemRank [9] is a random walk based scoring algorithm for recommendation. It makes use of an item correlation graph where edge weights represent item correlations. The correlation between two items amounts to the number of users that rated both items. In turn, the transition ma-trix is given by the normalized correlation matrix of items. Experiments show that ItemRank performs better than the methods proposed by Fouss et al. [4, 5].

Singh et al. [20] propose an approach to CF that combines social relationships and ownership data to make recommen-dations. To this end, they model user-item relations as a bipartite graph and augment it with user-user social links. The algorithm is based on random walks with absorbing states that induces a distribution per user over all items. A walker begins from a target user from where it may transi-tion to a friend or to an item. Once the walker reaches an item, he cannot be transitioned out of it. The authors eval-uate the proposed method using data from an online game service to suggest items the user might buy, and from text corpus to suggest words to papers.
 Cooper et al. [3] propose three scoring algorithms called P , P 5 and P 3  X  , which are based on random walks on the graph representing associations between users and items. Let G = ( U  X  I,E ) be an undirected bipartite graph of users and items, where U is the set of users, I is the set of items and there exists an edge between user u and item i if u rated i . Define the transition matrix P = B  X  1 A , where A is the adjacency matrix associated to G and B is a diagonal ma-trix where each entry equals the degree of the corresponding vertex. P 3 and P 5 are based on the distribution of random walks of three and five steps, respectively, starting from the target user vertex. In turn, P 3  X  generalizes P 3 in the sense that its transition matrix is raised to the power of  X  . Exper-iments show that P 3  X  provides better results than P 3 , which in turn provides better results than P 5 . Due to the memory burden of the proposed methods, the authors resort to esti-mating the distribution per user over all items via random walk sampling. They show that direct simulations of ran-dom walks for P 3 and P 5 are more memory efficient when compared to methods based on matrix calculations, so ran-dom walking sampling can be applied to larger datasets at the cost of recommendation quality. As a conclusion, the authors show P 3 and P 3  X  provide better results than the methods proposed in [4, 5, 9].

Christoffel et al. [1] introduce an algorithm called RP which is based on P 3 to optimize accuracy and diversity. RP 3  X  compensates for the influence of popular items by tak-ing into account item popularity in the ranking given by P Let P 3 ui be the original score of item i for target user u as the outcome of P 3 . RP 3  X  re-weights the score with  X  P 3 ui where b ii represents the degree of vertex i and  X   X  R + . Ex-periments show that RP 3  X  increases accuracy and diversity when compared to P 3 . Similarly to Cooper et al. [3], due to memory limitations, the authors resort to a sampling ap-proximation of the proposed method where distributions per user over all items are estimated by using 5 million random walks.

A naive implementation of the methods proposed in [1, 3] results in time complexity  X (( | I | + | U | ) 3 ) and space com-plexity  X (( | I | + | U | ) 2 ). Thus, the authors in [3] propose an approach that split the matrices in blocks and compute P 3 by a sequence of multiplications and additions of these blocks which results in time complexity  X ( | I | 2  X | U | space complexity  X ( | I | X | U | ).

As we move to medium/large size datasets, the methods discussed here have computational limitations since the size of the transition matrix may become too large to fit in mem-ory. As an alternative, Cooper et al. [3] propose approximat-ing transition probabilities by sampling random walks of a fixed length. However, random walk sampling may require a large number of simulations to converge. Thus, there is a lack of a method that provides exact calculations and de-creases memory burden without degrading recommendation quality. To fulfil this lack, we also propose a strategy to compute P 3 , P 5 , P 3  X  and RP 3  X  without resorting to matrix calculations, so bypassing the memory burden due to matrix allocation.

The methods discussed in this section do not take into account for recommendation any latent information present on the set of ratings. In fact, these graph-based approaches solely make use of the set of user-item pairs to define the structure of the user-item bipartite graph. Thus, the rating system is completely discarded in these approaches. For instance, in these methods, there is no distinction between a scenario where a user gives the maximum possible rating for an item and a scenario where a different user gives the minimum possible rating for the same item.
The Bayesian paradigm provides a principled way of in-corporating prior information to data and, then, allows the assignment of non-zero probabilities to unseen events. These probabilities represent our beliefs in those events. For in-stance, the Bayesian paradigm naturally allows us to answer questions like which item should we buy where one item has 2 up-votes out of 2 ratings while the other has 35 up-votes out of 40 ratings. Thus, the scoring functions we propose re-lies on the Bayesian paradigm to leverage recommendation.
Let U be the set of users and I be the set of items. We define the set of evaluations D = { ( u,i,r ui ) | u  X  U,i  X  I,r ui  X  R } , where R represents the set of possible rating values. Thus, ( u,i,r ui )  X  D represents the evaluation made by user u where item i received rating r ui . We assume that R = { 0 , 1 } , where, given u  X  U,i  X  I , r ui = 1 represent-ing a positive assessment while r ui = 0 representing a neg-ative assessment. Such assumption is not unrealistic, for instance YouTube adopts a thumbs up/down rating scale so that users can provide explicit feedback. Also, according to the media Netflix 1 and Uber 2 may replace their 5-star rating system.

Given u  X  U , I u = { i  X  I | ( u,i,r ui )  X  D } represents the set of items evaluated by user u . Let U i = { v  X  U | ( v,i,r D } be the set of users who evaluated item i . Similarly, let R j = { r vj  X  R | ( v,j,r vj )  X  D } be the set of ratings given to item j . Thus, we define the set R + j = { r vj  X  R j | r positive assessments and the set R  X  j = { r vj  X  R j | r of negative assessments received by item j .

Let G = ( U  X  I,E ) be an undirected bipartite graph, where E = { ( u,i ) | ( u,i,r ui )  X  D } , that is, there exists an edge between user u and item i if u rated i . We define  X  ( x ) = { i | ( x,i )  X  E } as the neigborhood of a vertex x ,  X 
U = max u  X  U |  X  ( u ) | as the maximum vertex degree in U http://www.businessinsider.com/netflix-wants-to-ditch-5-star-ratings-2016-1 http://qz.com/574033/uber-may-replace-its-five-star-driver-rating-system-with-emoji/ average degree of vertices in U and  X  I is defined similarly for I . Given u  X  U , we define P u = { X  u,v,w,x  X  X  v  X  I U,x  X  I, ( u,v ) , ( v,w ) , ( w,x )  X  E } as the set of all three-step paths starting from vertex u  X  U . Let P =  X  u  X  U P u be the set of all three-step paths in the graph starting from users.
Given j  X  I , let Y j be a binary random variable that as-sumes 1 if j receives a positive assessment and 0 otherwise, where P ( Y j = 1) =  X  j . We place a Beta distribution as the prior of  X  j , where  X  j  X  Beta( X  a,  X  b ). Intuitively,  X  the unknown reliability of item j within the range (0 , 1). As | R j | increases, the Beta distribution shape tends to concen-trate around its mean, then such notion of reliability turns out to be more precise. After observing R j , we update our knowledge about  X  j , so that where a =  X  a + | R + j | e b =  X  b + | R  X  j | .

The method we propose in this work makes a recommen-dation list for a target user u  X  U based on P u . To this end, we define a ranking function f u : I  X  R + defined as f a scoring function. Intuitively, f u ( x ) represents the sum of the weights of all three-step paths connecting u and x . input : G = ( U  X  I,E ), u  X  U , scoring function s output: f u for v  X   X  ( u ) do 2 for w  X   X  ( v ) do 3 for x  X   X  ( w ) do 4 f u ( x ) := f u ( x ) + s (  X  u,v,w,x  X  ) 5 end 6 end end
Algorithm 1 illustrates the method we propose in this work. The algorithm receives as input the user-item undi-rected bipartite graph G , target user u and a scoring func-tion s as previously defined. In lines 1-3, we systemati-cally iterate over each neighbor of the corresponding vertex. Thus, we enumerate all three-step paths starting from u . In line 4 we compute the value function f u for the final vertex x in the path  X  u,v,w,x  X   X  P . The algorithm outputs the ranking function f u . In this work, we propose three scoring functions that are based on the Bayesian paradigm. These scoring functions are discussed next.
Posterior Inequality Scoring . We propose a scoring function dubbed Posterior Inequality Scoring (PIS) where given  X  u,v,w,x  X   X  P u . We can obtain P (  X  i &gt;  X  from (1). Intuitively, PIS represents the probability of the reliability of candidate item x being greater than the relia-bility of item v in the user history.

Posterior Prediction Scoring We propose a scoring function called Posterior Prediction Scoring (PPS) where s (  X  u,v,w,x  X  ) = given  X  u,v,w,x  X   X  P u . We can obtain P ( Y j = 1 | R a + b +1 from (1). Intuitively, PPS represents the probability of both v and x receiving positive assessments where we assume that Y v and Y x are independent.

Posterior Odds Ratio Scoring We propose a scoring function denominated Posterior Odds Ratio Scoring (PORS) where given  X  u,v,w,x  X   X  P u . We define  X  i = P ( Y i =1 | R I . Intuitively, PORS represents how large the odds of x receiving a positive assessment is when compared to the odds of v receiving a positive assessment.

The scoring functions we propose are global in the sense that they do not depend on the target user. However, the ranking function f u is in fact customized for each user since the final result depends on the paths starting from vertex u .
Complexity Analysis . Our method incurs space com-plexity O ( | U | X   X  U ) and time complexity O ( | U | X   X  at worst case scenario considering the set of target users U . Thus, both space and time complexity of our method is lower than those presented in Section 2.

Implementation Details . We represent G as an ad-jacency list and obtain P u by systematically enumerating according to G all three-step paths starting from vertex u . Thus, our implementation for P 3  X  and RP 3  X  makes use of such enumeration instead of relying on matrix calculations or sampling as proposed by Christoffel et al. [1] and Cooper et al. [3]. Cook [2] proposes an analytical approach to com-pute Beta inequalities. However, this approach is computa-tionally expensive since it relies on recursive function calls. Thus, we resort to numerical integration methods to com-pute P (  X  i &gt;  X  j | R i ,R j ). To this end, we make use of GNU Scientific Library [6] as the numerical integration library for PIS and memoization technique for storing results and, then, avoid recomputing such values. To avoid the occurrence of probabilities equal to zero in the proposed scoring functions, we make use of pseudo counts. For each item, we add one positive and one negative pseudo-assessment as if they both were given by a dummy user. The reason to add one for each end of the scale is to balance out the analysis so we do not induce bias.
Our experiments are designed to answer the following ques-tions: i) can we improve the accuracy of recommendations by taking into account the latent information present in the set of ratings by means of the scoring functions we propose in this work? ii) how do our proposed scoring methods com-pare to one another? To answer these questions we make use of several datasets and baselines in our experiments.
The BookCrossing (BX as an abbreviation) dataset [21] contains data about book lovers that exchange books all around the world. The original dataset contains 278,858 anonymous users providing 1,149,780 ratings about 271,379 books spanning a period of time from August to September 2004. Ratings are given by users in a ten-star rating scale. In our experiments, we removed ratings related to implicit feedback.

The MovieLens datasets [10] are widely used in the Rec-ommender Systems literature and comprise four datasets of increasing sizes. Each dataset consists of users X  preferences for movies expressed in a five-star rating scale. In this work we consider the MovieLens 1M (ML 1M for short) dataset that comprises 1 million ratings from 6,000 users on 4,000 movies.

The Amazon dataset [14] contains 142.8 million product reviews spanning May 1996 -July 2014 and product meta-data from Amazon. Ratings are given by users in a five-star rating scale. In this work, we select three representative product categories from that dataset, namely, Cds &amp; Vynil (Cds as an abbreviation), Electronics and Kindle.

The FilmTrust dataset [8] contains 35,497 movie ratings crawled in June 2011 from 1,508 users on 2,071 movies. This dataset also provides ratings among users denoting how much a user trusts another in the social network. In turn, the Epinions dataset [13] contains 664,824 product ratings from 49,290 users on 139,738 products.

Since we do not focus on cold start issues, we keep users and items with at least 10 ratings. As a result, the data used for experiments differ from the original datasets and are summarized in Table 1. A common sense in the RS literature is that most users consume a small fraction of the items present in the catalogue and many items have few ratings [16]. One can see that except for MovieLens 1M dataset the maximum and average degree for items and users are significantly less than the corresponding total number in the dataset which turns out to favor our enumeration process.
We consider P 3  X  , RP 3  X  , Most Popular (MP), Bayesian Per-sonalized Ranking Matrix Factorization (BPRMF) [17] and Weighted Regularized Matrix Factorization (WRMF) [11] as recommendation baselines in our investigations. MP sorts items according to their global popularity and so the rec-ommendation list does not depend on the target user. In turn, WRMF and BPRMF are two representative matrix-factorization approaches for recommendation. WRMF ex-tracts latent factors from implicit feedback and introduces regularization to prevent overfitting. BPRMF optimizes a maximum posterior estimator derived from a Bayesian anal-ysis of the problem and learning relies on stochastic gradi-ent descent with bootstrap sampling. In our experiments, we make use of the WRMF and BPRMF implementations provided in the Java port MyMediaLite [7] recommender system framework. 3
The datasets are not compliant with the assumption R = Given a dataset and user u present on it, we calculate the average  X  r u of all ratings given by u . Assume that item i was graded  X  r ui by u . We define r ui equals 1 if  X  r ui  X   X  r erwise. We replace ( u,i,  X  r ui ) in D by ( u,i,r ui ). As a result, https://github.com/jcnewell/MyMediaLiteJava we make a transformation of the original set of ratings into a dichotomized set where the user bias is taken into account.
We carried out experiments on the datasets described by using 5-fold cross validation. To this end, we randomly par-titioned D into 5 subsets, where a subset T is retained for testing, and the remaining 4 subsets are used as training data. We repeated this process 5 times and report the aver-age result across all the test folds in 5 trials.

Given u  X  U , we define the set T u  X  I u of items rated by u and used for testing. We partition T u = T + u  X  T  X  u T u = { i  X  I u | ( u,i,r ui )  X  T,r ui = 1 } is the set of items used for testing that u rated positively and T  X  u , in turn, is the set of items used for testing that u rated negatively. To express our lack of information about data we adopt an uninformative prior distribution by setting  X  a and  X  b to one so that Beta distribution reduces to Uniform distribution.
To answer the questions we raised, we make use of met-rics largely adopted in the RS literature [18]. The metrics we report are computed by averaging over all the users in testing. Given u  X  U , let L u be the recommendation list ordered according to f u . For a given N  X  N , let L the recommendation list composed of the N highest scored items according to f u . Given L u , L u [ i ] represents the item at position i and the same applies for L N u . Finally, let A be the set of all | I | length ordered lists composed of items in I where repetition is not allowed.
 Precision . Given L N u , precision is defined as Mean Average Precision (MAP) . Given L u , Mean Average Precision (MAP) is defined as where 1 T + Mean Reciprocal Rank (MRR) . In the Information Retrieval literature Mean Reciprocal Rank is defined as the average of the multiplicative inverse of the rank position of the first correct answer for a set of queries. Shi et al. [19] define RR for a given recommendation list of a user by how early the first relevant recommended item is in the list and MRR as the average of the RR across all the recommenda-tion lists for individual users. Thus, given L u we define where rank : A  X  N , rank( L u ) = min { i | 1  X  i  X | L u T u } .
 Normalized Discounted Cumulative Gain (NDCG) .
 It is a commonly used measure of the performance of web search engines in Information Retrieval. In the context of RS, NDCG measures the quality of a ranking based on the graded relevance of the recommended entities with respect to an ideal ranking of entities. It is defined as DCG @ N is defined as where rel : I  X P ( I ) \ X  X  X { 0 , 1 , 2 } is a function that indicates the relevance of an item and we define it as In turn, IDCG @ N is the maximum possible DCG @ N where L u is sorted in non-increasing order by item relevance. In this work, we consider items in T + u as highly relevant, items in T  X  u as fairly relevant and those not in T u as irrelevant items for recommending to a target user u . For further in-formation we refer the reader to [12].
In this section we assess the performance of our approach and the baselines. We use the symbols M ( O ) and N ( H ) to denote statistically significant increase (decrease) at p &lt; 0 . 05 and p &lt; 0 . 01 levels, respectively, while the symbol  X  is used to denote no significant difference according to Stu-dent X  X  t-test. Since PPS shows the best results among the scoring functions we propose, as can be seen in Table 2, we present the results for the other methods when compared to PPS.

Table 2 provides the results for the methods and datasets considered in this work. The first columns presents the dataset name. The second column shows the method con-sidered in the experiments. The next two columns give the results for MAP and MRR, respectively. The next two columns present results for P@N respectively for N = 5 and N = 10. Finally, the last two columns show results for NDCG@N for N = 5 and N = 10, respectively. Each result entry presents the proper symbol representing the t-test re-sult, metric value and in parenthesis the mean percentage improvement of PPS over the considered method.
In the BX dataset, PPS provides statistically significant increase at the p &lt; 0 . 01 level over the other methods for all metrics. When compared to P 3  X  and RP 3  X  , all the methods we propose provide better results, where PPS respectively shows results at least 219.8% and 296.5% superior than P 3 and RP 3  X  . When compared to the matrix factorization ap-proaches, PPS respectively shows results at least 140.3% and 13.5% superior than BPRMF and WRMF. Finally, an inter-esting finding here is that MP provides results better than P , RP 3  X  and even BPRMF in all metrics.

In the Cds dataset, PPS provides statistically significant increase at the p &lt; 0 . 01 level over the other methods for all metrics. When compared to P 3  X  and RP 3  X  , all the methods we propose provide better results, where PPS respectively shows results at least 280.6% and 626.9% superior than P 3 and RP 3  X  . The matrix factorization approaches provide re-sults inferior when compared to PPS, namely, the latter re-spectively shows results at least 143.8% and 44.8% better than BPRMF and WRMF.

In the Electronics dataset, PPS provides statistically sig-nificant increase at the p &lt; 0 . 01 level over the other meth-ods for all metrics. When compared to P 3  X  and RP the methods we propose provide better results, where PPS respectively shows results at least 299.7% and 829% better than P 3  X  and RP 3  X  . When compared to the matrix factor-ization approaches, PPS respectively shows results at least 80.4% and 20% superior than BPRMF and WRMF. Finally, MP provides results better than P 3  X  , RP 3  X  and even BPRMF in all metrics.

In the Kindle dataset, PPS provides statistically signifi-cant increase at the p &lt; 0 . 01 level over the other methods for all metrics. When compared to P 3  X  and RP 3  X  , all the methods we propose provide better results, where PPS re-spectively shows results at least 284.4% and 441% superior than P 3  X  and RP 3  X  . The matrix factorization approaches provide results inferior when compared to PPS, namely, the latter respectively shows results at least 291.3% and 209.1% better than BPRMF and WRMF.

In the ML 1M dataset, PPS provides statistically signif-icant increase at the p &lt; 0 . 01 level over MP, P 3  X  and RP for all metrics. When compared to P 3  X  and RP 3  X  , all the methods we propose provide better results, where PPS re-spectively shows results at least 50.3% and 492.1% superior than P 3  X  and RP 3  X  . In turn, MP provide better results than P  X  and RP 3  X  for all metrics. When compared to BPRMF and WRMF, our methods do not show any statistically sig-nificant increase. In fact, except to MRR, PPS respectively show results at most 29.4% and 38.8% inferior than BPRMF and WRMF. Even though PPS shows results 4.3% better than BPRMF in MRR, there is no significance difference according to Student X  X  t-test.

In the FilmTrust dataset, PPS provides statistically sig-nificant increase at the p &lt; 0 . 01 level over P 3  X  and RP all metrics. When compared to P 3  X  and RP 3  X  , all the methods we propose provide better results, where PPS respectively shows results at least 23.3% and 128.8% superior than P 3 and RP 3  X  . When compared to BPRMF and WRMF, PPS provides statistically significant increase at the p &lt; 0 . 01 level for MAP, MRR and P @5, but no significance differ-ence for the other metrics. In fact, the most striking im-provements are found in MRR, where PPS provides results 103% and 159.1% better than BPRMF and WRMF, respec-tively. In turn, PPS provides no significance difference for MAP, P @5 and P @10 when compared to MP. However, MP provides statistically significant increase at the p &lt; 0 . 05 level in NDCG @5 and NDCG @10 over PPS. Thus, the most interesting finding here is that MP provides results at least as good as PPS except for MRR.

In the Epinions dataset, PPS provides statistically signif-icant increase at the p &lt; 0 . 01 level over BPRMF, MP, P and RP 3  X  for all metrics. PPS respectively provides results at least 54.7%, 270.4% and 635.6% better than BPRMF, P  X  and RP 3  X  . One can see that MP provides better re-sults than P 3  X  and RP 3  X  for all metrics. When compared to WRMF, PPS provides statistically significant increase at the p &lt; 0 . 01 level for MAP, MRR, P @5 and P @10, where PPS shows results at most 38.2% superior. However, WRMF pro-vides statistically significant increase at the p &lt; 0 . 05 and the p &lt; 0 . 01 in NDCG @5 and NDCG @10, respectively, over PPS.

Among the methods we propose in this work, PPS clearly outperforms PIS and PORS in all datasets and metrics, ex-cept for P @10 in the FilmTrust dataset where there is no statistically significant difference. PIS systematically pro-vides results as good as PORS in all datasets and metrics except for the FilmTrust dataset where the latter outper-forms the former. However, PIS is the most computationally demanding method we propose as it makes use of numerical integration.

In summary, PPS provides the best results in the BookCross-ing and all Amazon datasets. In the MovieLens dataset, WRMF provides the best results. In the FilmTrust dataset, due to its simplicity, we advocate that MP is the best choice even though PPS provides a significant improvement in MRR. Finally, in the Epinions dataset, PPS provides the best re-sults for all metrics except for NDCG .
In this paper, we studied ranking algorithms for personal-ized recommendation in the context of explicit feedback. We have presented three scoring methods based on the Bayesian paradigm that take advantage of the set of ratings given by the users. Also, we have proposed a strategy to cope with the memory and computational burden incurred from graph-based approaches in the literature, and analytically showed that our strategy is more efficient. The method we propose exploit three-step paths starting from the target user in the user-item graph to score items by means of the proposed scoring methods.

In our evaluation, we carried out experiments in seven datasets to assess the performance of the recommendation methods. We empirically showed that our PPS method clearly outperforms the graph-based approaches considered in all datasets and metrics. We also compared our methods against two representative matrix factorization methods and the results show that PPS clearly outperforms those meth-ods in four datasets and is competitive with them in the other two; among the methods we propose, PPS clearly out-performs PIS and PORS. Thus, we confirm our hypothesis that graph-based approaches can take advantage of the set of ratings for improving the user satisfaction in recommender systems.
 We plan to investigate the relaxation of the assumption R = { 0 , 1 } , so that our method is able to deal with a multiple scale rating system. In this way, we can have a better idea of how effective our scoring methods are in such contexts and, BX Cds Electronics Kindle ML 1M FilmTrust Epinions eventually, propose further improvements. Also, we plan to carry out experiments to explain why our approach fails in improving results in MovieLens dataset when compared to the matrix factorization methods. Furthermore, we intend to assess how the path length affects the performance of our method. Finally, we are currently investigating the cus-tomization of the scoring methods so that probabilities are conditioned on the target user.
 This work was partially funded by projects InWeb (MCT/ CNPq 573871/2008-6) and MASWeb (FAPEMIG/PRONEX APQ-01400-14), and by the authors X  individual grants from CNPq and FAPEMIG.
 [1] F. Christoffel, B. Paudel, C. Newell, and A. Bernstein. [2] J. Cook. Exact calculation of beta inequalities. Techni-[3] C. Cooper, S. H. Lee, T. Radzik, and Y. Siantos. Ran-[4] F. Fouss, A. Pirotte, and M. Saerens. A novel way of [5] F. Fouss, A. Pirotte, J.-m. Renders, and M. Saerens. [6] M. Galassi et al. GNU Scientific Library Reference [7] Z. Gantner, S. Rendle, C. Freudenthaler, and [8] J. Golbeck and J. Hendler. Filmtrust: movie recom-[9] M. Gori and A. Pucci. Itemrank: A random-walk based [10] F. M. Harper and J. A. Konstan. The movielens [11] Y. Hu, Y. Koren, and C. Volinsky. Collaborative filter-[12] K. J  X  arvelin and J. Kek  X  al  X  ainen. Cumulated gain-based [13] P. Massa and P. Avesani. Trust-aware recommender [14] J. McAuley, R. Pandey, and J. Leskovec. Inferring net-[15] L. Page, S. Brin, R. Motwani, and T. Winograd. The [16] Y. Park and A. Tuzhilin. The long tail of recommender [17] S. Rendle, C. Freudenthaler, Z. Gantner, and [18] G. Shani and A. Gunawardana. Recommender Sys-[19] Y. Shi, A. Karatzoglou, L. Baltrunas, M. Larson, [20] A. P. Singh, A. Gunawardana, C. Meek, and A. C. Su-[21] C. Ziegler, S. M. McNee, J. A. Konstan, and G. Lausen.
