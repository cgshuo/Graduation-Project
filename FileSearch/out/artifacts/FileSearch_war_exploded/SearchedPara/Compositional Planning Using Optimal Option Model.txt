 David Silver d.silver@cs.ucl.ac.uk Kamil Ciosek k.ciosek@cs.ucl.ac.uk Classical planning algorithms make extensive use of temporal abstraction to construct high-level chunks of useful knowledge (Amarel, 1968; Sacerdoti, 1975; Korf, 1985; Laird et al., 1986). They are typically provided with a set of primitive planning operators as inputs. These are then composed together into macro-operators : open-loop sequences of planning operators. Macro-operators jump directly from an initial state to the outcome state that would result from following the sequence, without having to execute the intermediate operators. Macro-operators can themselves be com-posed together into more abstract operators, allowing planning to take place at a much more abstract level. Macro-operators can be thought of as building blocks of knowledge, which can be combined together into more abstract knowledge. Powered by this knowledge, the path to the goal can often be found in a small num-ber of high-level planning operations, even when the path is composed of thousands of primitive actions. In Markov Decision Processes (MDPs), the outcome of an action may be stochastic. An open-loop sequence does not capture the contingencies that can arise as a result of each intermediate action. Instead, a closed-loop policy, which maps states to actions, can respond to each particular situation as it arises. A closed-loop policy that is followed for some number of steps, and stops according to a termination condition that also depends on the state, is known as an option (Sutton et al., 1999). An option model describes the distribu-tion of outcome states that would result from follow-ing the option (Sutton, 1995). Option models are the stochastic analogue of macro-operators: they jump di-rectly from initial state to outcome, without having to execute the intermediate actions. Option models can also be composed together into more abstract option models (Precup et al., 1998). Option models thus pro-vide basic building blocks for compositional knowledge in general MDPs.
 However, prior work on planning with options has been restricted to shallow hierarchies. Option models are either constructed from primitive actions, in an ap-proach known as intra-option model learning; or they are used to compute a value function, in an approach known as inter-option (or SMDP) planning (Sutton et al., 1999). Although these steps are sometimes com-bined, they are typically combined in two stages: first constructing the option models without using them; and then using the option models without changing them. In both stages, the planning operators are fixed. In this paper we focus explicitly on compositional plan-ning : the multi-level composition of option models. Each option model is both constructed (intra-option) and used (inter-option). It is constructed from lower-level option models, so as to maximise progress to-wards a given subgoal. It may also be used to com-pose higher-level option models. As soon as an option model has been created, it can be used to construct other option models. As a result, the set of planning operators improves dynamically, providing longer and more purposeful jumps as planning proceeds.
 Our approach is based on a major generalisation of the Bellman equation along four dimensions. First, we provide a recursive relationship between state proba-bilities as well as between rewards. Second, we com-pose over options rather than primitive actions. Third, we generalise from the overall goal of maximising to-tal reward, to any given subgoal. Fourth, we optimise over termination conditions as well as policies. Several of these dimensions have been partially ex-plored by prior work. First, Sutton et al. (1995, 1999, Section 5) developed a Bellman expectation equation for state probabilities, but this work was restricted to Markov reward processes without actions (Sut-ton, 1995) or to fixed policies without control (Sut-ton et al., 1999). We present a Bellman optimality equation for state probabilities in Markov decision pro-cesses, including actions and control. Second, Precup et al. (1998) provided Bellman equations for compos-ing option models into policies, but not into options. Our framework constructs both policies and termina-tion conditions, so that we can compose option models into other option models  X  a crucial step for composi-tional planning. Third, Sutton et al. (1999, Section 7) defined optimal options with respect to a given sub-goal and termination condition, and suggested the ex-istence of a corresponding Bellman optimality equa-tion. We define this Bellman optimality equation, and also extend to the case when neither, either, or both the policy and termination condition are specified. No prior work has considered the state probabilities as-sociated with Bellman optimality equations. Without knowledge of these state probabilities, it is not pos-sible to jump directly to the outcome of an optimal option. Our approach to compositional planning is built directly on this knowledge, so as to build ab-stract macro-operators that can jump from one state directly to a distant state.
 The Bellman optimality equation gives rise to impor-tant planning methods such as value iteration. Simi-larly, we use our generalised Bellman equation to de-rive a compositional planning algorithm, which simul-taneously and recursively constructs the optimal op-tion model for multiple subgoals, including the overall goal as a special case. We prove that this algorithm converges to optimal option models for all subgoals, including the optimal policy.
 The options framework is agnostic about the source of the options, and does not commit to any particu-lar algorithm for their construction. However, several other approaches to hierarchical reinforcement learn-ing have been proposed, based on samples from an unknown MDP. These architectures, including Diet-terich X  X  MAXQ (2000), and Parr and Russell X  X  HAMs (1997; 2002), do construct the solution to one subprob-lem from the solution to other subproblems. However, these architectures are not directly applicable to com-positional planning, where the MDP is known rather than sampled. By focusing on planning with known models, we develop a sound theoretical framework for compositional planning, based on the generalised Bell-man equation. This work can be viewed as a bridge between the generality of options, and the composi-tional construction algorithms used by architectures such as MAXQ.
 We illustrate our approach on two well-known bench-mark problems: hierarchical path planning and the Tower of Hanoi. Both problems have been extensively studied using classical planning approaches. In both problems, planning directly with primitive operators (e.g. using value iteration) requires computation time that is exponential in the problem size, whereas algo-rithms based on compositions of macro-operators (e.g. Jonsson 2009) can solve these problems in polynomial time. Unfortunately, classical planning approaches do not generalise to stochastic planning problems. In con-trast, our compositional planning algorithm can solve both deterministic and stochastic variants of these problems in a polynomial number of iterations. An MDP is defined by a set of n states S , a set of actions A , action transition matrices P a and action reward vectors R a for each action a  X  A , and a dis-count factor 0  X   X  &lt; 1. Each component of the action transition matrix P a ss 0 is the discounted probability of next state s 0 given that action a was selected in state s , P a ss 0 =  X  Pr ( s t +1 = s 0 | s t = s,a t = a ). Each component of the action reward vector R a s is the ex-pected reward given that action a was selected in state s , R a s = E [ r t +1 | s t = s,a t = a ]. The discount factor can be viewed as a chance of exiting to an absorbing terminal state with probability 1  X   X  . The discounted probability P a ss 0 can be interpreted as the probability of reaching state s 0 without exiting.
 A policy  X  ( s,a ) is the probability of selecting action a given state s ,  X  ( s,a ) = Pr ( a t = a | s t = s ). The value function , V  X  ( s ), is the expected total reward from state s when following policy  X  , V  X  ( s ) = E [ r t +1  X r t +2 + ... | s t = s, X  ]. The optimal value function V  X  ( s ) and optimal action value function Q  X  ( s,a ) are the maximum achievable value and action value that can be achieved by any policy, V  X  ( s ) = max V An optimal policy  X   X  ( s,a ) is any policy that achieves the optimal value function.
 The optimal value function obeys a recursive rela-tionship: the Bellman optimality equation , V  X  ( s ) = max R tion is the unique fixed point of this equation, and can be found by turning the Bellman optimality equa-tion into an iterative update, V k +1 ( s )  X  max R P (Bellman, 1957).
 An option o =  X   X , X   X  is an extended behaviour or macro-action that combines a policy  X  ( s,a ) with a termination condition  X  ( s ) giving the probability that the option will stop in state s . We assume that options can be initiated from all states. Primitive actions are options: they can be represented by a policy that de-terministically selects that action, and a termination condition that stops with probability 1. We denote the set of all policies by  X  and the set of all termi-nation conditions by B . An option model comprises an option transition matrix P o and an option reward vector R o . Each component R o s is the expected total reward given that option o was executed from state s , R  X  is the random variable for the duration of option o . Each component P o ss 0 is the probability of terminat-ing in state s 0 given that option o was executed from state s , discounted by the total duration of the option, P be interpreted as the probability of option o terminat-ing in s 0 without exiting. Informally, a model is a stochastic mapping from state to state, combined with the reward accumulated along the way. Applying a model to a state results in a distri-bution over outcome states, and an expected reward. To compose models together, we apply a second model to this outcome distribution and expected reward, and arrive at a new state distribution and reward. We now formalise these ideas, following Sutton (1995). We define a rasp (reward and state probabilities), [ r | p ], to be a 1  X  (1 + n ) row vector, where n = |S| , r is a scalar reward, and p is a 1  X  n row vector rep-resenting a discounted probability distribution p over states in S . We use s s to denote the deterministic rasp that is in state s with probability 1, and has a reward component of zero; we shorten to s when there is no ambiguity. Rasps are ordered by their reward components, [ r 1 | p 1 ]  X  [ r 2 | p 2 ] if and only if r A model is a transformation from rasp to rasp. For-mally, a model matrix containing an n  X  1 reward vector R and an n  X  n transition matrix P . This block matrix nota-tion for models and block vector notation for rasps are known as homogeneous coordinates (Sutton, 1995). To compose two models together, we multiply their homo-geneous coordinates, Similarly, to compose a rasp and a model, we again multiply their homogeneous coordinates, To aid readability, homogenous matrices and vectors are denoted by boldface letters, e.g x , M and M for rasps, models and model sets respectively. Sequences of compositions are best understood by reading left to right, e.g. sAB is the model composition that starts in state s , applies model A and then applies model B . 3.1 Model Sets Models can represent the outcomes of actions, options and policies. An option model O o  X  O represents the outcome on termination of a corresponding option o  X  O . It combines an option transition matrix with an option reward vector, O o = tion model A a  X  A represents the outcome of a prim-itive action a  X  A , where A a = models are option models, A  X  O , corresponding to options that terminate with probability 1. A policy model  X   X   X   X  , where  X   X  = the outcome of executing policy  X  forever. Policy mod-els are also option models,  X   X  O , corresponding to options that terminate with probability 0. Finally, we define an identity model I corresponding to zero re-ward and the identity transition matrix, I = this can be viewed as a null model without any dis-counting. Note that action/option/policy subscripts may be dropped when there is no ambiguity. 3.2 Value Models A value model V  X  V , where V = transition matrix of zero ( i.e. it always exits) and a reward vector given by the components of V ( s ) as its reward vector ( i.e. total reward before exiting). Policy models are value models,  X   X  V , where the reward vector contains the values V  X  ( s ), and the transition matrix is zero due to infinite discounting.
 Value models can be used to express several famil-iar value functions. A state value function V ( s ) can be represented by composition with the corresponding value model, sV ; an action value function can be rep-resented by sAV ; and an inter-option value function (Sutton et al., 1999) can be represented by sOV . The true value model G  X  = overall goal of maximising total reward. It is defined to have a value function V  X  ( s ) that is a lower bound on S , X   X   X . This definition ensures that a termination condition of  X  ( s ) = 0 is always optimal, and that pol-icy models dominate over terminating option models, with respect to the true value, s X   X  G  X  = s X   X  = 3.3 Expectation Models An expectation model E  X  ( M ) is the expected model under some distribution  X  ( s,  X  ) over models. For ex-ample, an action expectation model E  X  ( A ) averages all action models A a  X  A according to policy  X  ( s,a ). Specifically, each row of E  X  ( A ) contains the expected rasp from state s after one action has been executed Composing a model with a deterministic rasp s picks out the row corresponding to state s , 3.4 Maximising Models A max model max V value models W  X  V . Each reward component is the maximum value of sV from state s . An argmax model argmax MV models in set M , with respect to value model V . Each row of argmax MV the value sMV from state s . Composing an argmax model with a deterministic rasp s picks out the maximising row, We now explore recursive relationships between com-positions of models. For didactic purposes we be-gin with compositions of primitive actions into policy models, and develop a model equation that is analo-gous to the Bellman equation. We then extend this approach to compositions of option models into policy models; to compositions of action models into option models; and finally to compositions of option models into other option models. We provide proofs of unique fixed points in the supplementary material. 4.1 Action-Policy Model Composition We begin by rewriting the Bellman expectation equa-tion as a model composition, We call this equation the action-policy model expec-tation equation . It rewrites the Bellman expectation equation in homogeneous coordinates. This equation has fixed point V =  X   X  , i.e. composing the action ex-pectation model E  X  ( A ) with policy model  X   X  results in the same policy model  X   X  .
 We also consider the model max AV the state-action value sAV from every state s . We can then rewrite the Bellman optimality equation in homogeneous coordinates, We call this equation the action-policy model optimal-ity equation .
 The optimal policy model is the max model max  X  over all policy models over the set of primitive actions, s max  X  optimal value function. The optimal policy model V = max  X  optimality equation.
 4.2 Option-Policy Model Composition We now compose option models into a policy model. We assume we are given a base set of options  X   X  O and a corresponding set of option models  X   X  O . We consider the option expectation model E  X  ( O ) that averages the base option models O o  X   X  according to hierarchical policy  X  ( s,o ) = Pr ( o | s ). Similarly to Equations 3 and 4, each row of E  X  ( O ) contains the expected rasp from state s after one option has been executed by  X  , This gives the option-policy model expectation equa-tion , with fixed point V =  X   X  , Next, we consider the model max OV the composed value sOV (the inter-option value func-tion). This leads to the option-policy model optimality equation , Given only a base set of option models  X  , which does not necessarily include all primitive actions, it is not in general possible to construct all policy mod-els. Instead, we consider the hierarchical policy model set {  X   X  | supp(  X  )  X   X  } , which is the set of all policy models corresponding to hierarchical policies over  X . The hierarchically optimal policy model max  X  is the max model over this set; it is analogous to a hi-erarchically optimal value function (Dietterich, 2000), i.e. the best that can be achieved under the hier-archical constraints imposed by the choice of base options. 1 The hierarchically optimal policy model V = max  X  option-policy model optimality equation. If the base set includes all primitive actions, A  X   X  , then all policy models can be represented and the hierarchi-cally optimal policy model is the optimal policy model, max  X  4.3 Action-Option Model Composition Primitive actions can also be composed together into option models, to give intra-option model learning. This requires a mechanism to incorporate option ter-mination into model compositions.
 We represent the termination condition  X  ( s ) by a ter-mination model E  X  ( I , M ). This is an expectation model over { I , M } that selects each row from the iden-tity model I with probability  X  ( s ), or from model M with probability 1  X   X  ( s ), sE  X  ( I , M ) = s (  X  ( s ) I + (1  X   X  ( s )) M ) ,  X  s  X  S (13) Composing an action model A with termination model E  X  ( I , M ) selects between A (termination) or AM (continuation). In particular, we consider the compo-sition of expectation model E  X  ( A ) with termination model E  X  ( I , M ). This gives the action-option model expectation equation , with fixed point M = O  X   X , X   X  , We now consider the optimality of option models. We define optimality with respect to a subgoal value model G that represents the value on termination of the op-tion, e.g. whether a given subgoal has been achieved. An optimal option model argmax OG model, with respect to subgoal value model G , over all options, i.e. it maximises over both policies and ter-mination conditions. We will consider option models that maximise over policies or termination conditions in a subsequent section.
 We represent optimal termination by an argmax model over B  X  { I , M } , which maximises the binary choice between termination, represented by identity model I , and continuation, represented by model M . For ex-ample, argmax ABG model A or from the composed model AM , depending on whether sAG (termination) or sAMG (continua-tion) gives more reward from state s . We only optimise over deterministic termination conditions, because an optimal deterministic termination condition must exist (analogous to optimal policies). We can now define the option-option model optimality equation , for which any optimal option model argmax OG 4.4 Option-Option Model Composition We now present the most general case in which op-tion models are composed into other option models. This combines intra-option model learning with inter-option model learning, a key step towards our goal of compositional planning. As in option-policy model composition, we assume that we are given a base set  X 
Option Model Equation Fixed point  X   X , 0  X  V = E  X  ( A ) V  X   X   X  X  X  , 0  X  V = max AV  X   X , X   X  M = E  X  ( A ) E  X  ( I , M ) O  X   X , X   X   X  X  X  , X   X  M = argmax A  X  BG  X   X ,  X  X  M = argmax  X  ABG  X  X  X  ,  X  X  M = argmax ABG of options, and a corresponding set  X  of option models to compose together. As in action-option model com-position, we consider termination conditions as well as policies. Combining these ideas together gives the option-option model expectation equation , with fixed point M = O  X   X , X   X  , and the option-option model optimality equation , It is not in general possible to construct all option models, due to limitations of the base set  X  . In-stead, we consider the hierarchical option model set
O  X   X  ,  X   X  | supp(  X  )  X   X  ,  X   X  X  , which is the set of option models O  X   X  ,  X   X  where  X  is restricted to op-tions in  X . The hierarchically optimal option model , set, with respect to subgoal value model G . A hier-archically optimal option model is a fixed point of the option-option model optimality equation. 4.5 Optimal  X  -and  X  -Option Models There are in fact two dimensions of optimality for op-tion models: optimality of the policy  X  and optimality of the termination condition  X  . The previous sections dealt with jointly optimal option models, which max-imise over both policies and termination conditions. We now consider option models that optimise just one of these two dimensions.
 An optimal  X  -option model argmax OG argmax model over the set of options with termina-tion condition  X  , i.e. it maximises over policies for a given termination condition  X  . Similarly, an optimal  X  -option model argmax OG
Opt. Model Equation Fixed point  X   X , 0  X  V = E  X  ( O ) V  X   X   X  X  X  , 0  X  V = max OV  X   X , X   X  M = E  X  ( O ) E  X  ( I , M ) O  X   X , X   X   X  X  X  , X   X  M = argmax O  X  BG  X   X ,  X  X  M = argmax  X  OBG  X  X  X  ,  X  X  M = argmax OBG over the set of options with policy  X  , i.e. it maximises over termination conditions for a given policy  X  . We can now define action-option model optimality equations for optimal  X  -option models, where the ter-mination condition is given; and for optimal  X  -option models, where the policy is given, These equations have respective fixed points: optimal  X  -option model M = argmax OG option model M = argmax OG For option-option model composition of  X  -options, we restrict option models to elements of the hierarchical option model set that also match a given termina-tion condition  X  , O  X   X  ,  X   X  | supp(  X  )  X   X  ,  X  =  X  . The hierarchically optimal  X  -option model is the argmax model over this restricted set, argmax OG The option-option model optimality equations for  X  -options and  X  -options respectively are, The fixed points of these equations are the hierar-chically optimal  X  -option model M = argmax OG and the optimal  X  -option model M = argmax OG Table 1 and 2 summarise the various model equations and their fixed points. In the supplementary material, we prove that each fixed point satisfies the correspond-ing equations, and furthermore that the subgoal value of each fixed point is unique.
 The Bellman optimality equation forms the basis of a wide variety of MDP planning algorithms (Sutton &amp; Barto, 1998). Similarly, the model optimality equa-tions can be used to derive a wide variety of MDP planning algorithms. In particular, the option-option model equations can be used to derive algorithms for compositional planning in MDPs. We focus here on a dynamic programming algorithm that uses the option-option model optimality equation (Equation 17) as an iterative update. This algorithm, which we call option-option model iteration (OOMI), can be viewed as a generalisation of value iteration to option models for multiple subgoals.
 We assume that we are given a base set  X  of option models, and also m subgoal value models { G 1 ,..., G m } for m different subgoals. At each iteration k , the al-gorithm updates a set of m option models M k =
M k 1 ,..., M k m , containing one option model for ev-ery subgoal. Each option model is initialised to the true value model, M 0 g = G  X  . At each iteration k , for every subgoal j , option model M k +1 g is updated by the option-option model optimality equation (Equa-tion 17). Maximisation is performed over the base set  X  and the current set of option models M k , OOMI imposes no explicit hierarchy: any option model may be composed with any other option model. When updating the option model M g for subgoal value model G g , all current option models are considered. In particular, the option model M g itself is considered; this allows option models to be repeatedly squared, so that a single model may be efficiently applied as many times as required. As a result, even if OOMI is restricted to primitive actions,  X  = A , and only a sin-gle subgoal, G 1 = G  X  , it may still converge in signifi-cantly fewer iterations than value iteration. We prove in the supplementary material that OOMI converges to a hierarchically optimal option model for each sub-goal value model G g . If OOMI includes the true value model in its set of value models, G g = G  X  , then the corresponding option model M g will converge to the hierarchically optimal policy model  X   X   X  . Option-option model iteration can similarly be extended to  X  -options, where the termination condition is given; or  X  -options, where the policy is given, by using Equa-tions 20 and 21 respectively as iterative updates. Fi-nally, the option-option expectation model equation (Equation 16) can be used as the basis for an iterative update, analogous to policy iteration, that interleaves option evaluation with option improvement. We illustrate our framework for compositional plan-ning using two hierarchical MDPs: the Tower of Hanoi problem, and the Nine Rooms problem. The N -disc Tower of Hanoi problem has a discount factor is  X  = 1, each action receives a reward of  X  1, and episodes ter-minate upon reaching the goal state ( N discs stacked on right peg). The level-1 Nine Rooms gridworld is a 3  X  3 grid. The N -level Nine Rooms gridworld con-tains a 3  X  3 grid of instances of level N  X  1 problems; neighbouring instances are connected by a width 3 N  X  2 doorway; and there is a single goal state in one corner. The discount factor is  X  = 0 . 9, rewards are 1 in the goal state, and 0 elsewhere. We also use stochastic variants in which each action causes the intended move with probability 1  X  p , or with probability p randomly selects another legal move (Tower of Hanoi, p = 0 . 4), or re-mains in the current state (Nine Rooms, p = 0 . 05). For the Tower of Hanoi, we use m = 3 N + 1 subgoal value models. This set includes the true value model G  X  and a subgoal value model G d,e = 1 0 V on each disc d on top of each peg e . Each subgoal value function is defined by V on d,e ( s )  X  on ( s,d,e ), where the predicate on ( s,d,e ) has a value of 1 if disc d is on peg e in state s and 0 otherwise. 2 For the Nine Rooms, we use 12( n  X  1) subgoal value models. This set includes a subgoal value model G l,j for each of the j  X  [1 , 12] doorways at each level l of the hierarchy. Each subgoal value function is defined by V doorway l,j ( s )  X  in ( s,l,j ), where the predicate in ( s,l,j ) has a value of 1 if state s is in the j th level-l doorway, and 0 otherwise. In this problem, initiation sets were used to restrict the states considered to relevant doorways within the neighbour-hood of the subgoal. All subgoal values are designed to be achieved  X  X t any cost X , by choosing a large con-stant of proportionality. We use the primitive actions (moving a disc in Tower of Hanoi; moving N, E, S, W in Nine Rooms) as the base set  X  .
 We compare three solution methods. Action-policy model iteration (APMI) is a one-level planning al-gorithm that plans over primitive action models. It iteratively applies the action-policy model optimality equation (Equation 9), and is equivalent to value iter-ation. Action-option-policy model iteration (AOPMI) is a two-level planning algorithm, with fixed planning operators. It first performs intra-option learning, con-structing option models from primitive action models by iteratively applying the action-option model op-timality equation (Equation 15). It then fixes the set of option models, and performs inter-option plan-ning. Finally, it constructs a value function from op-tion models, by iteratively applying the option-policy model optimality equation (Equation 12). Option-option model iteration is the compositional planning algorithm (OOMI) described in Section 5. For each algorithm we measured the total number of iterations (applications of the corresponding model equation) re-quired; and also the mean number of backups (up-dates to an individual state) to each state. 3 The re-sults are shown in Table 3. In larger instances of both problems, the compositional approach required significantly fewer iterations and backups than either flat planning (APMI) or two-level planning (AOPMI), where options are first created and then used. In the Tower of Hanoi, APMI and AOPMI required a number of iterations that grew exponentially with the number of discs, whereas OOMI required just 1 additional it-eration per disc in the deterministic case, and 8 ad-ditional iterations per disc in the stochastic case. In the Nine Rooms, the total iterations for APMI and AOPMI again grows exponentially with the level, but polynomially for OOMI. The Bellman optimality equation has motivated the development of a wide variety of MDP planning al-gorithms. We have generalised the Bellman equation in several important dimensions, enabling an anal-ogous variety of compositional planning algorithms. We have illustrated one such approach, using option-option model iteration. This is the first MDP planning algorithm to dynamically create its own planning oper-ators. These operators are composed together to give increasingly deep and purposeful jumps through state space. Like value iteration, option-option model iter-ation applies full-width backups over complete sweeps of the state space. In principle, the model equations could also be solved by sample backups over sam-ple trajectories, leading to compositional algorithms for hierarchical reinforcement learning. In this pa-per we have focused on planning with table lookup models; however, similar to MAXQ (Dietterich, 2000), HAMs (Andre &amp; Russell, 2002) or skills (Konidaris &amp; Barto, 2009), substantial efficiency improvements may be generated when each option model is provided with its own state abstraction.

