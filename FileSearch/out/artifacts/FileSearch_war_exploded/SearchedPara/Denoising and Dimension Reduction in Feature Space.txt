
Kekul  X  estr. 7, 12489 Berlin Kernel machines use a kernel function as a non-linear mapping of the original data into a high- X  X rotect X  against the high dimensionality of the feature space.
 a function can be reconstructed using only a few kernel PCA components asymptotically, then the same already holds in a finite sample setting, even for small sample sizes. dimensional feature spaces. Furthermore we can use de-noising techniques in feature space, much Let us consider an example. Figure 1 shows the first six kernel PCA components for an example combined to construct the decision boundary. components suffice to represent the optimal decision boundary.
 class labels, making the learning process much more stable. This combination of dimension and noise estimate allows us to distinguish among data sets showing weak performance which might either be complex or noisy.
 principal components under appropriate conditions. (2) We propose an algorithm which estimates the data set, and thus to perform model selection among different kernels. (3) We show how the analyze some well-known benchmark data sets and evaluate the performance as a de-noising tool in Section 5. Note that we do not claim to obtain better performance within our framework when scalar products with the eigenvectors of the kernel matrix. Let us start to formalize the ideas introduced so far. As usual, we will consider a data set ( X feature space F via the feature map  X  . Scalar products in F can be computed by the kernel k in (normalized) kernel matrix K with entries k ( X We wish to summarize the information contained in the class label vector Y = ( Y observed class label vector can be written as Y = G  X  N with N = G  X  Y denoting the noise in following lemma relates projections of G to the eigenvectors of the kernel matrix K : Lemma 1 The k th kernel PCA component f of the kernel matrix K : ( f Y  X  R n to the leading d kernel PCA components is given by  X  d ( Y ) = P d k =1 u k u &gt; k Y. Proof The kernel PCA directions are given as (see [10]) v [ u tors of the kernel matrix K . Thus, the k th PCA component for a point X The sum computes the j th component of K u Since the u the first d kernel PCA components is given by P d products are linked to the decay rate of the kernel PCA principal values. It is clear that we cannot expect G to generally locate favorably with respect to the kernel PCA components, but only when there is some kind of match between G and the chosen kernel. This eigenspaces converge. Their asymptotic limits are given as the eigenvalues  X   X  marginal measure of P occur in the well-known Mercer X  X  formula: By Mercer X  X  theorem, k ( x,x 0 ) = P  X  The asymptotic counterpart of G is given by the function g ( x ) = E ( Y | X = x ) . We will encode fitness between k and g by requiring that g lies in the image of T to saying that there exists a sequence (  X  the scalar products decay as quickly as the eigenvalues, because  X  g, X  of the known convergence of spectral projections, we can expect the same behavior asymptotically obtain the following bound on u &gt; Theorem 1 Let g = P  X  with high probability. where r balances the different terms ( 1  X  r  X  n ), c around l eigenvalues smaller than  X  space spanned by the first r eigenfunctions.
 The bound consists of a part which scales with l terms). Typically, the bound initially scales with l l will typically be small: for smooth kernels, the eigenvalues quickly decay to zero as r  X  X  X  . The related quantities  X  r for larger i .
 sions depends on the asymptotic coefficients  X  k smooth kernels whose leading eigenfunctions  X  between a kernel function and a data set in a practical way.
 explained above. Since G is not known, we can only observe the contributions of the kernel PCA components to Y , which can be written as Y = G + N (see Section 2). The contributions u &gt; will thus be formed as a superposition of u &gt; G equally distributed over all coefficients. Therefore, the kernel PCA coefficients s = u &gt; the shape of an evenly distributed noise floor u &gt; information protrude (see Figure 2(b) for an example).
 by fitting a two component model to the coordinates of the label vector. Let s = ( u &gt; Then, assume that given number of leading kernel PCA components. (b) shows the negative log-likelihood of the two only the first four components.
 We select the d minimizing the negative log-likelihood, which is proportional to Model Selection for Kernel Choice For different kernels, we again use the likelihood and select the two component model will not work very well, leading to large values of ` . vector  X  G =  X   X  err = 1 set, and therefore an estimate for the noise level in the class labels. PCA components do not depend on Y , the noise N contained in Y is projected to a random subspace of dimension d . Therefore, 1 estimated dimension d ). components are relevant. This behavior of the algorithm can also be seen from the training and minimum. Finally, in Figure 2(c), the resulting fit is shown.
 Benchmark data sets We performed experiments on the classification learning sets from [7]. For selected from 20 logarithmically spaced points between 10  X  2 and 10 4 for each data set. For the dimension estimation task, we compare our RDE method to a dimensionality estimate based on cross-validation. More concretely, the matrix S = P d respect to the eigenvector basis u Table 3 shows the resulting dimension estimates. We see that both methods perform on par, which shows that the strong structural prior assumption underlying RDE is justified. space (kPCR) against kernel ridge regression (KRR) and support vector machines (SVM) on the although there is a tendency to under-estimate the true error.
 set seems to be a good candidate to benefit from more data. discards some void projected kernel PCA directions and thus provides a regularized model. Figure 3: Estimated dimensions and error rates for the benchmark data sets from [7].  X  X im X  shows original error rates from [7]. error for the learning problem. Compared to common cross-validation techniques one could argue show the usefulness of our RDE algorithm.
 An interesting future direction lies in combining these results with generalization bounds which class.
 X 1 ,...,X n kernel k is approximated by the truncated kernel matrix is k kernel matrix is denoted by K by E of eigenvalues of K r eigenfunctions  X  functions converge to either 0 or 1 . The error is measured by the matrix C let  X  Next, we collect definitions concerning some function f . Let f = P  X  ` P The proof of Theorem 1 is based on performing rough estimates of the bound from Theorem 4.92 in [1]. The bound is where the three terms are given by It holds that k  X  + larger than 1  X   X  ([1], Lemma 3.135) that k C large constant, especially, if  X  Now, Lemma 3.135 in [1] bounds k E assuming that K will be reasonably small (for example, for rbf-kernels, K = 1 ). Combining this with our rate for k  X  + Finally, we obtain  X  If we assume that  X 
