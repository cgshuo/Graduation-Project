 Information retrieval has been examined from many different angles. There have been many methods of retrieval designed to obtain precise results in a fast time from a simple key word based query. Of all of the methods, the most used during the last half century was the vector space method. It X  X  simplicity provides it with great speed and little storage needed and its precision has been used as the baseline for many experiments. The vector space method of document retrieval suffers from two main problems: 1) Its disregard of term positions. 2) Its assumption of term independence. Term positions should be taken into account during the document scoring process. As a simple example, we will examine the documents: Given a query of  X  X akery trucks X  each of these documents would be given the same score using the vector space method. The simplest method of distinguishing between the two is by observing the term positions. Only then can we see that document 2 is better suited to the query.
 documents are converted into vectors, we only examine the term occurrences though the document rather than the term positions. There have been many efforts to try to include term positions [4, 1, 5, 8]. These term proximity meth-ods calculate the document scores based on the distances between the query terms within the document. These methods provide high precision results for specific cases, but have trouble trying to incorporate the term occurrence in the document scores and have also resulted in an increase in the query time and data storage.
 ment retrieval system which is able to use term position information and present the results to the user in a time comparable to the vector space method. creation. When creating our term space, we choose to assign the count of each unique term to a separate dimension, resulting in a vector space of M dimensions, where M is the number of unique terms in the document set. Our document score is based on the inner product of the query and document vectors, therefore all of the terms are treated as independent entities, which they are not. If our query is given as  X  X ed vehicles X , neither of the previously examined documents would be retrieved, simply because they do not contain the query terms. If we were to somehow remove the term independence, we would want document 1 obtaining a higher score than document 2. Document 1 has terms related to both of the query terms ( X  X aroon X  and  X  X ruck X ), while document 2 has terms related to only one of the query terms ( X  X iesel X  and  X  X ruck X ). There have been many attempts to remove this term independence assumption. Manual and automatic thesauruses have been used [7], many versions of latent semantic analysis have been tried [2, 6], and more recently language models have been constructed [16, 3]. Each of these have their own method of removing the independence assumption but they all add to the query time and the document storage.
 term independence assumption which is a combination between the thesaurus method and the latent semantic analysis method and also allows us to retrieve documents in a time comparable to the vector space method.
 document retrieval method and discuss such aspects as term signal creation and use of phase in document scoring; Section 3 will follow by introducing our query mapping structure and how we can perform fast query term expansions using latent semantic analysis and speed ups used in the vector space method. To make use of the positions of query terms in document, many have tried observing the features such as the query term proximities. If query terms occur frequently through the document, we must make many comparisons and take into account many positions in the document. Once we have made calculations based on the term proximity, we are left with the problem of how to incorporate the count of each term in the document. Relative to the vector space method, each of these calculations increases the query time and storage of the term positions increases the data storage.
 tions rather than individual positions. The patterns are set by our choice of spec-tral transformation. If we apply the spectral transformation to a document, we move from the term position domain to the term spectral domain. Each spectral component is independent of the others, therefore we only have to compare the one spectral component for each term in a document to obtain a score based on the query term positions. Therefore, by comparing query term spectra rather than query term positions we reduce the amount of comparison that need to be made. of its elements and provide experimental results showing the benefits over the vector space method. 2.1 Term Signals The vector space method assigns a single number to each term-document ele-ment, the term occurrence in the document. This value contains no information about the position of the term in the document, only the amount of times it ap-pears. Our spectral based retrieval method uses term signals in the place of the term occurrence value. A term signal is a sequence of numbers that represent the occurrence of the associated term in particular sections of the document. This is similar to providing the term count for each paragraph, but in our case we find the term count within a certain range of terms depending on the desired term signal length. If we choose to have term signals of length B (containing B elements) then the b th element of the term signal  X  f d,t will be the count of term t in document d from words N b/B to N ( b +1) /B  X  1, where N is the document length in words. In other words, we split the document into B equal portions and term signal element b is the count of term t in document d  X  X  b th portion. The term signals are shown as: ues that represent the approximate positions of each term in a document. The greater the value of B , the higher the accuracy of the term positions 1 . Since each term signal is a sequence, we are able to apply signal processing transfor-mation to them to map them into a spectral domain. Transforms which we have investigated are the Fourier transform, the cosine transform, and the Wavelet transform using the Haar wavelet and Daubechies-4 wavelet. 2.2 The Spectral Domain Once we have our set of term spectra for each document (  X   X  d,t ), we can now pro-ceed with the query. When a query is given, the user generally wants the retrieval system to return documents which have many occurrences of the query terms and the query terms should be within a small proximity within the document. Now that we have a set of term spectra, we cannot measure term proximity directly. We must take advantage of the complex nature of each spectrum. If we used the Fourier transform to obtain our term spectra, the resulting complex signal could be split into magnitude and phase signals. The magnitude of the term spectrum corresponds to the term count in the corresponding term signal. The phase of the term spectrum corresponds to the relative position of the term in the term signal.
 Magnitude. The magnitude of the term spectrum  X  f d,t is related to the oc-currence of the t th term in the d th document, so it is important that we use this information in our document score calculations. The spectral transformation that we use is a linear transform, therefore each of the spectral component mag-nitudes is equally as important as the other. If a certain component magnitude is dominant in the spectrum, it implies that the term signal followed a certain pattern which is represented in the used transform. We take the magnitude of the term spectrum to be the sum of each of the spectral component magnitudes: Phase Precision. We have mentioned that we would like to give a high score to documents that have high magnitudes and similar phase for each spectral component of each query spectrum. If we have a set of query terms Q and each query term spectrum contains the elements: where  X  d,t,b is the b th spectral component of the term spectrum from term t in document d , H d,t,b is its magnitude,  X  d,t,b is its phase and i = for any given magnitude, we would want the highest score to be attributed to the document that has the same phase in each spectral component for all of the query terms. So the highest score is given in the case: As the phase of each component shifts, we want the score to reduce. Therefore if the spectral components are totally out of phase, we should assign a low score. each of the elements are similar. Unfortunately, we cannot use variance because phase is a radial value (2  X  = 0). Therefore we must look towards phase precision. magnitude of the average vector. If the vectors are facing the same direction, the magnitude of the average will be large. If the vectors are facing in different directions, the averaging will cause the vectors to cancel each other, providing a small magnitude of the average. We can use this concept with our phases. If we attach a unit magnitude to each of the phases, we can average them and take the magnitude of the average, called phase precision. If all of the phases are the same, the phase precision will be 1. If all of the phases are different, the phase precision will be close to zero (shown in figure 1). The phase precision equation is: where  X  d,b is the phase precision of spectral component b in document d . Combining Magnitude and Phase Precision. The magnitude and phase precision values are obtained for each spectral component, therefore we must combine them to obtain a single document score. The magnitude represents the occurrence of the query terms in the document and the phase precision is a measure of how similar query term positions are. We use the phase precision as a weighting factor to the magnitude calculation. This implies that is the query terms are in approximately the same positions, we will use the full magnitude values (since the phase precision weight will be 1) and as the query term position difference grows the phase precision weight will reduce and we will be using only a fraction of the magnitude values. We give the document score as: 2.3 Experimental Results Some experimental results are shown in table 1. The precision obtained after 5, 10, 15 and 20 documents is shown for the vector space model and our spectral based method for three high ranking weighting schemes [17]. The experiments were run on the AP2WSJ2 document set from TREC using the titles of queries 51 to 200. We can see that the spectral based method provides significantly better precision results for each weighting scheme. To remove the assumption of term independence, methods such as thesauruses have been employed which expand the users query to include terms related to the original query terms. Other methods, such as latent semantic indexing, map the documents and query from the terms space into a reduced dimensional topic space. Documents and queries are compared as usual (using the inner product) in this topic space.
 expansion using latent semantic analysis. The query expansion data is placed in a mapping which occurs before the documents and query are compared. In this section we will describe the mapping creation process and display results of experiments of the vector space method with and without the query expansion. 3.1 Latent Semantic Analysis If a word has latent semantics, it implies that there is a hidden meaning behind it. Latent semantic analysis (LSA) is the process which we follow in order to find the hidden meanings. LSA was first performed using singular value decomposition (SVD) [2] of the document-term index and later performed using maximum likelihood methods [6]. In each of the methods, we receive a mapping matrix which is able to map a vector from the term domain to the latent topic domain. If we focus on the SVD method, the decomposition gives us: where A is our document-term index, U is the set of orthonormal left singular vectors,  X  is the set of singular values, and V is the set of orthonormal right singular vectors. From this equation, we take our new set of document vectors to be U X  and our term to topic mapping as the matrix V . We can show that the mapping of the document-term index give us the set of mapped document vectors in the topic space: where  X  A is the set of document vectors in the topic space. Queries are mapped into the topic space in the same way: where  X  q is the query in the topic space. Once all of our vectors are in the topic space, we compare them for similarity using the inner product: where s is the set of document scores. We know that V is an orthonormal matrix, therefore VV = I (the identity matrix).
 can ignore the last few elements of each vector and still obtain similar results. If we take only the first n elements of the document and query vectors mapped into the topic space (  X  A and  X  q ) we are obtaining the best least squares estimate of the vectors in n dimensions. The magnitude of the singular values give us insight into the importance of the corresponding dimension of the singular vectors. If the singular value is zero, the corresponding dimension can be ignored with no change to A . 3.2 Latent Semantic Space By reducing the dimensionality of the topic space, we are reducing the number of elements that we need to store and use in calculations. A typical reduction would take a document vector from its 100,000 dimension term space to a 100 dimensional topic space. But as the saying goes  X  X here is no free lunch X . The doc-ument vectors in the term space are very sparse, we can see that if there were only 100 unique terms in a document, 99.9% of the document vector elements would contain zeros and the majority of non-zero elements would contain the value of 1. This statistic leads to very high compression and also allows the sys-tem designer to implement fast scoring algorithms based on this knowledge. The query vector is also very sparse, usually containing only two or three non-zero elements. The document scores are based on the inner product of the document and query vector, therefore, the most number of multiplications required to ob-tain the score will be equal to the number of non-zero elements in the query vector. By mapping the document and query vectors to the latent topic space, we are mapping our sparse high dimensional vectors in to dense low dimensional vectors. By obtaining a dense query vector, we have to perform many more mul-tiplications during the query process and hence, receive a longer query time. The compression of the stored document vectors in the reduced topic space would not be as compact as it was in the term space due to the unexpected data patterns. space has caused an increase in query time and storage. 3.3 Latent Semantic Query Map If we examine the equation where we have used the dimension reduced singular vectors to map our documents and query into the topic space (equation 10), we can see that it is possible to obtain the document scores by using the original sparse document vectors: where M = VV . This equation shows that we are able to leave the document vectors in the term space and apply a simple mapping to the query to obtain the related query terms in the term space. By combining the term-topic mapping ( V ) with itself, we are creating a mapping which takes data from the term space to the topic space and then back to the term space. This will take the users query and map it to a set of weighted terms (shown in figure 2) where the weight reflects the query term X  X  relevance to the query. This combined mapping benefits the retrieval process by: 1) leaving the documents in the sparse term space which allows us to use fast querying techniques. 2) providing us with an expanded query containing weighted terms, not unidentifiable topics. The second point is an important one, because it allows the user to review the expanded query and select terms which are appropriate for the query. The query can also be pruned automatically by setting a cutoff weight or choosing a set number of query terms. By reducing the number of query terms, we are also reducing the calculations required during the query process and the time required to complete the query. ment vectors, but we must also store the mapping. 3.4 Storing the Query Map The query map ( M )isa m  X  m matrix, where m is the number of unique query terms in the document set. It created by multiplying the term to topic mapping by the transpose of itself, therefore it is a dense square matrix, which is as compressible as  X  A .
 we go back to the construction of the V matrix, we can recall that it is through the SVD of A . This leads to the following set of equations: The last line shows that V is the set of eigenvectors for the matrix A A which is the covariance matrix of A or the covariance of the terms found in the document set. To establish a good estimate of the covariance of two variables, we need to take many samples. Therefore, if there are only a few occurrences of a term in the document set, its estimated covariance with other terms would not be accurate. This implies that we should remove the under sampled terms from the document term matrix before calculating the SVD.
 well. If they are included in the calculation of V , we will find that they would be about equally distributed amongst all topics and hence related to all terms. By including terms that appear in most documents into the query would not benefit the query since the term would simply add to the score of most documents (just as if we added a constant value to the document scores). Therefore we can reduce the query mapping size by excluding the terms that appear in many documents before the calculation of the SVD. 3.5 Fast Mapping If we review the scoring process using the query mapping, we will observe that the mapping stage is exactly the same as the document score calculation stage. The two stages are: 1) Multiply the query vector with the mapping to receive the expanded query vector. 2) Multiply the expanded query vector with the document index to receive the document scores. This implies that all of the compression and fast querying techniques that are found while calculating the top document scores (such as quantisation and early termination [9]) can also be used to obtain the top weighted query terms related to the original query. 3.6 Experimental Results We have provided a few experimental results showing the query mapping method and the Lnu.ltu vector space method using the titles of queries 51 to 200 from the TREC document set. Figure 3 shows results from three sets of experiments. The top four plots have precision on the y-axis and the number of documents a term must appear in to be included in the mapping (e.g. 200 implies that only terms that appeared in more that 200 documents were included in the mapping) as the x-axis. The first two plots compare the precision at 10 documents and the average precision obtained by adjusting the number of expansion terms in the query. We can see that the 1000 term expansion provides only a slight improve-ment over the 50 term expansion. The second two plots provide the precision at 10 documents and the average precision obtained when changing the number of topic dimensions chosen. We can see that the 100 dimensional topic space provides a greater precision with a smaller mapping when compared to the 400 dimensional topic space.
 accumulators on the x-axis. They show the precision at 10 documents and the average precision obtained when the number of query term accumulators are varied. We can see that the Continue method provides higher precision, and if we choose 400 or more accumulators we achieve the best precision.
 We have presented two separate methods of extending the vector space model in information retrieval and addressing the problems of including term position information and using term dependencies.
 easily utilise the term positions by taking into account their term spectra. By combining query term spectra, we can produce greater document scores for those documents that have query terms within a smaller proximity when compared to those that span a larger proximity.
 query expansions on the users query based on the term relationships in the doc-ument set. The term relationships are found using singular value decomposition. This mapping can be stored in a fast retrieval index such as those found in the vector space model.
 generalised vector model of information retrieval. We have presented an extensive investigation of these methods in the papers [11, 15, 12, 14, 13, 10].
