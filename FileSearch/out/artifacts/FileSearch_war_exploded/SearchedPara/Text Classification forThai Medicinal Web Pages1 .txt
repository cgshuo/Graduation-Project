 The fast growth and dynamic change of online information, have provided us a very large amount of information. Text Categorization (TC) is an important tool for organizing documents into classes by applying statistical methods or artificial intelligence (AI) techniques. With this organization, the utilization of the documents is expected to be more effective. A variety of learning techniques for TC, have been developed, such as Bayesian approaches [1], linear classifiers [2] and support vector machines [3]. Most research works have used standard data sets which is published in English, a language with word boundary delim-iter such as Reuters, OHSUMED, 20-Newsgroups for evaluating the experiment. Recently, some research works [4,5] have applied TC techniques to medical doc-uments (e.g., OHSUMED). However, there is no systematic research work on text categorization in other language, especially for Thai medicinal collection. Like other Asian languages such as Chinese, Korean and Japanese, Thai lan-guage also has no explicit word boundary delimiter. For languages without word boundary, word segmentation plays an important role to construct a set of terms for classification process. One more feature in most Thai medicinal web pages is that they usually include a plentiful of technical terms in both Thai and English. This property implies a large number of unique terms in the web collection. Most of technical terms in Thai medicinal web pages are compound terms which can be captured by the concept of n-gram (i.e., bigram or trigram). Moreover, such technical web pages also contain transliterates, and typo errors in both Thai and English, due to their technical aspect.

This paper presents an evaluation of several techniques in handling these prob-lems. Taken into account are three different factors; classification algorithm, word segmentation algorithm and term modeling. In the rest of this paper, Section 2 presents a number of well-known classification algorithms. Some characteristics of Thai medicinal web pages and the ways to construct representative models for Thai medicinal web pages are given in S ection 3. The data sets and experimen-tal settings are described in Section 4. In Section 5, a number of experimental results are given. A conclusion is made in Section 6. This section gives a brief introduction to three well-known algorithms that were widely used for text classification i.e. na  X   X ve Bayes, centroid-based algorithms and the support vector machine (SVM). 2.1 Na  X   X ve Bayes Algorithm As a statistical-based algorithm, the na  X   X ve Bayes classifier (NB) first calculates the posterior probability P ( c k | d j )ofclass c k that the document belongs to differ-ent classes, and assigns it to the class with the highest posterior probability. Ba-sically, a document d j can be represented by a bag of words { w 1 j ,w 2 j ,...,w nj } in that document (i.e., a vector of occu rrence frequencies of words in the doc-ument). NB assumes that the effect of a word X  X  occurrence on a given class is independent of other words X  occurrence. With this assumption, a NB classifier finds the most probable class c k  X  X  , called a maximum a posteriori (MAP) c
MAP for the document which is determined by arg max c k As a preliminary experiment, occurrence f requency for calculating the posterior probability P ( w ij | c k ) outperforms the binary frequency. Therefore, this method will be used in this work. 2.2 Centroid-Based Algorithm The centroid-based algorithm is a linear classification algorithm. Only positive documents are taken into account for co nstructing a centroid vector of a class. The vector is normalized with the docum ent length to a unit-length vector (or prototype vector). In the classification stage, a test document is compared with these prototype vectors by dot product (cosine measure) in order to find the nearest class. Normally, a centroid-based classifier (CB) obtained high classifi-cation accuracy with small time compl exity. The techniques to improve the CB by introducing term-distribution factors to term weighting, in additional to the standard tf  X  idf was proposed [6]. In this work, we also use term distributions (TD) with CB. The term weighting formula is tf ik  X  idf i  X  tf ik and csd ik are average class term frequency and class standard deviation of a term t i in class c k , respectively. The idf i and sd i are inverse document frequency and standard deviation of term t i , respectively. Normally, combination of TD and TFIDF in an appropriate way, outperforms TFIDF. 2.3 Support Vector Machines Support vector machines (SVMs) are based on the structure risk minimization principle [7]. It has been shown in previous works [3] to be effective for text categorization. SVM divides the term space into hyperplanes or surface separat-ing the positive and negative training samples. An advantage of SVM is that it can work well on very large feature spaces, both in terms of the correctness of the categorization results and the efficiency of training and categorization algo-rithm. However, a disadvantage of SVM training algorithm is that it is a time consuming process, especially training with a large corpus. The WWW permitted the general public to navigate easily across the global In-ternet and view a variety of information. In Thailand, most of web documents are conducted in Thai. However, in practical situation, English language is frequently contributed to Thai web pages, especia lly educational web pages. There are a large number of medical information rel ated to disease, drug, herbal medicine and so on. Most of them are written in Thai with English terms attached. So far, several research works have developed methods for classifying medical document written in English. Up to present, there is no systematic work for classifying Thai medicinal web collection. From our preliminary study, we found that this type of documents has some special proper ties. These documents usually contain a lot of technical terms in both Thai and English. Most technical terms are com-pound noun and they are easily mistyped. From these properties, the system for classifying the collection are investigated in a systematic way. In order to construct a set of terms (features) in the s tep of learning and classifying phases, two factors are considered. Firstly, word segmentation is an essential component in creating features (wor ds) from a sentence. Lastly , higher-grams seem to be a good representative for constructing f eatures for medicinal documents. The detail of these factors are described. 3.1 Thai Word Segmentation Algorithms In Thai language, a word segmentation plays as an important role to construct a set of terms for classification process. Most researchers had implemented their Thai word segmentation system based on using a dictionary. Currently, three al-gorithms: longest matching, maximal matching and n -gram are well-known and widely used. Most of early works in Thai word segmentation are based on a longest matching algorithm [8]. The algorithm scans an input sentence from left to right, and selects the longest match with a dictionary entry at each point. In case that the selected match cannot l ead the algorithm to find the rest of the wards in the sentence, the algorithm will backtrack to find the next longest one and continue finding the rest and so on. It is obvious that this algorithm will fail to find the correct the segmentation in m any cases because of its greedy charac-teristics. The maximal matching algorithm was proposed to solve the problem of the longest matching algorithm [9]. This algorithm first generates all possible seg-mentations for a sentence and then select the one that contain the fewest words, which can be done efficiently by using dynamic programming technique. Because the algorithm actually finds real maximal matching instead of using local greedy heuristics to guess, it always outperforms the longest matching method. Besides the two algorithms, another popular statistical model is n -gram. The n -gram assumes that the probability of the event depends on n previous events. For a word segmentation, n -gram is applied as a measure of whether a word bound-ary is likely to locate between a current character and its pr eceding characters where c is a character in a word. It is well known that to obtain a good estima-tion for this statistic, a large corpus is required. While up to three contiguous characters (3-gram) is taken into account, this work apply bigram ( n =2) due to computation issue. 3.2 Representation Basis The frequently used document representation in IR and TC is the so-called bag of words (BOW) [10] where words in a document (or a class) are used as basics for representing that document. In this representation, each element (or feature) in the vector is equivalent to a unique word with a weight. In a more general framework, the concept of n -gram can be applied. Instead of a single isolated word, a sequence of n words will be used as representation basis. In several applications, not specific for classification, the most popular n -grams are 1-gram (unigram), 2-gram (bigram) and 3-gram (trigram). Alternatively, the combination of different n -grams, for instance the combination of unigram and bigram, can also be applied. Although a higher-gram provides more information and this may effect in improving classification accuracy, more training data and computational power are required. Ther efore, unigram and bigram are considered in this work. The dataset used for evaluation is a se t of web documents which are collected from several Thai medicinal websites. Composed of eight categories (i.e. Educa-tion, Disease, Drug, Food, Herbal, Toxic, Organization and Dental), the numbers of documents for these classes are 1298, 1615, 716, 426, 761, 323, 1492 and 258, re-spectively. This collection was constructed under the research in a project named  X  X esearch and Development of Resources for Processing Very Large-Scaled In-formation on the Internet  X  X nformation Retrieval and Data Mining X  X . The total number of pages is 6889. In this collection, several web pages contain English technical terms. Without any term selection, the number of terms in the unigram model after passing any Thai word segmentation algorithm is approximately sev-enty thousand. In details, there are a lot of errors triggered by mis-typing, mis-segmenting and so on. To partially solv e this problem, we select a set of terms which appear in a collection at least thre e times. Focusing on unique words, the number of English terms is nearly the same as Thai terms. The total numbers of terms using longest matching, maximal matching and bigram algorithms in a unigram model are 27945, 28120 and 28587. They are 221376, 220960 and 224643 in bigram models, respectively.

Three models of features are investigat ed. The features of the first model (non-segmented model) are constructed from English unigram terms, Thai unigram terms and Thai phrases. Due to the fact that Thai word segmentation is not applied, the number of features in Thai for non-segmented model is almost as twice as that of unigram model. In the second and third models, three Thai word segmentation algorithms are used for the preprocessing step before starting the training process. The second model, all features are unigram term in both Thai and English. For the third model, we add the bigram terms into the second model and it is so-called bigram model. As a preprocessing, some stopwords (e.g., a, eliminate the affect of these common words and typographic words. This may be helpful to make classification processes not depend on any specific format.
Three experiments are performed. In the first experiment, four types of classi-fiers are evaluated i.e. na  X   X ve Bayes classifier (NB), cen troid-based classifier with tf  X  idf term weighting (TFIDF), centroid-based classifier with tf  X  idf and term distributions (TFIDF*TD, see detail in 2.2) and support vector machine (SVM). The query weighting is tf  X  idf for centroid-based classifiers. For SVM, the linear function is applied and the term weighting is tf  X  idf .ThreeThai word segmentations are considered i.e. longest matching, maximal matching and bigram. The second and last experiments investigate the effect of our focused four classifiers and three Thai word segmentation algorithms in unigram and bigram models, respectively . All experiments were performed using 90% for the training set and 10% randomly for the test set. We performed 10 trials for each experiment. The performance was measured by classification accuracy defined as the ratio between the number of documents assigned with correct classes and the total number of test documents. Due to the fact that one factor and two fac-tors are involved for each experiment, one-way and two-way analysis of variance (ANOVA) are used as a statistical method for evaluating the difference of mean with the significant level of 0.05. The diff erence of average clas sification accuracy between methods for each factor is compared by Scheff  X  e X  X  test, a method which is suitable for both multiple comparison and range test. 5.1 Effect of Classifiers In the first experiment, performance of four classifiers, i.e. NB, TFIDF, TFIDF*TD and SVM is explored. Table 1 showed the result in forms of the average classification accuracy  X  standard error of the mean (SEM).

According to 1-way ANOVA between classi fiers, the average accuracies of the four classifiers are significantly different ( p &lt; 0.05). NB is an intermediated per-formance classifier. Standard TFIDF classifier achieves the lowest performance among the four classifiers. However, term distributions improve its performance and outperforms SVM classifier. As describe in Section 4, the number of unique terms in non-segmented collection is g reater than segmented unigram model collection. Term distributions efficien tly utilizes these t erms (or phrases). 5.2 Effect of Thai Word Segmentation Algorithms In the second experiment, the effects of thr ee Thai word segmentation algorithms i.e. longest matching (Longest), maximal matching (Maximal) and bigram (Bi-gram), and four classifiers are explored. The result is shown in Table 2.
According to 2-way ANOVA between cla ssifiers and Thai word segmentation algorithms, there is no significantly diff erence between the average accuracies of the three Thai word segment algorithms. This indicates that the types of word segmentation has no effect on performance and then we can apply any of them for classifying Thai medicinal Web pages. On the contrary, the average accuracies of the four classifie rs are significantly different ( p &lt; 0.05). Thai word segmentations segment Thai sentences in to separate words. In this case, all fea-tures are represented with unigram model. The number of unique features in unigram model is less than non-segment ed collection. However, the number of features in a document vector increases ( longer document vector). The result is that all classifiers except TFIDF*T D, achieves higher performance. 5.3 Effect of Bigram Models In the last experiment, bigram models of features are used instead of unigram models in previous experiment. The result is shown in Table 3.

According to 2-way ANOVA between cla ssifiers and models, the average ac-curacies of the four classifiers are significantly different ( p &lt; 0.05). There is also significantly difference ( p &lt; 0.05) between the average accuracies on uni-gram and bigram models. The bigram models usually achieve higher performance than unigram models for all classifiers except only TFIDF. The SVM on bigram model is still the best classifier in ter m of accuracy. However, time for learn-ing phase increases a lot while a little bit improvement over unigram model. The TFIDF*TD is the second classifier i n terms of performance. However, its performance in bigram models is comparable to SVM. This paper investigated a set of methods to classify Thai medicinal Web doc-uments in a systematic way and analyzed the results by statistical methods. Three factors are taken into account i.e classification algorithm, word segmen-tation algorithm and term modeling. Normally, This collection has some special properties i.e. a large number of terms in both Thai and English, several terms are represented in higher-gram and a lot of typing errors. From the experimental results, classification performance depe nds on the three factors especially clas-sification algorithm. In a model without Thai word segmentation, the number of unique features is greater than those of a unigram model with Thai word segmentation. For this case, TFIDF with term distributions efficiently utilize these unique features. It outperformed other classifiers including SVM. When Thai word segmentations were applied in both unigram and bigram models, SVM was superior to the others. However, the performance of TFIDF with term distributions in the bigram model was higher than the unigram and only less than the gap of 2% from the state-of-art algorithm, SVM. The results suggested that the bigram model of TFIDF with term distributions was a good model but we need to accept trade-off between time spent in both training phase and the accuracy for classifying Thai medicinal Web collection. It was the classifier of choice when Thai word segmentation was not applied or in the adaptive learn-ing environment. The SVM is the classifier of choice when accuracy is the most important and any of Thai word segmentation algorithms can be applied. In the future, we will consider the Thai medicinal Web documents with less English terminology and the other types of Thai Web documents. We will also consider other feature selection techniques.
 This work was funded by the National Electronics and Computer Technology Center (NECTEC) via research grant NT-B-22-I4-38-49-05.

