 In computational linguistics and information re-trieval, Vector Space Models (Salton et al., 1975) and its variations  X  such as Word Space Models (Sch X tze, 1993), Hyperspace Analogue to Lan-guage (Lund and Burgess, 1996), or Latent Se-mantic Analysis (Deerwester et al., 1990)  X  have become a mainstream paradigm for text represen-tation. Vector Space Models (VSMs) have been empirically justified by results from cognitive sci-ence (G X rdenfors, 2000). They embody the distri-butional hypothesis of meaning (Firth, 1957), ac-cording to which the meaning of words is defined by contexts in which they (co-)occur. Depending on the specific model employed, these contexts can be either local (the co-occurring words), or global (a sentence or a paragraph or the whole doc-ument). Indeed, VSMs proved to perform well in a number of tasks requiring computation of seman-tic relatedness between words, such as synonymy identification (Landauer and Dumais, 1997), auto-matic thesaurus construction (Grefenstette, 1994), semantic priming, and word sense disambiguation (Pad X  and Lapata, 2007).

Until recently, little attention has been paid to the task of modeling more complex concep-tual structures with such models, which consti-tutes a crucial barrier for semantic vector models on the way to model language (Widdows, 2008). An emerging area of research receiving more and more attention among the advocates of distribu-tional models addresses the methods, algorithms, and evaluation strategies for representing compo-sitional aspects of language within a VSM frame-work. This requires novel modeling paradigms, as most VSMs have been predominantly used for meaning representation of single words and the key problem of common bag-of-words-based VSMs is that word order information and thereby the structure of the language is lost.

There are approaches under way to work out a combined framework for meaning representa-tion using both the advantages of symbolic and distributional methods. Clark and Pulman (2007) suggest a conceptual model which unites sym-bolic and distributional representations by means of traversing the parse tree of a sentence and ap-plying a tensor product for combining vectors of the meanings of words with the vectors of their roles. The model is further elaborated by Clark et al. (2008).

To overcome the aforementioned di ffi culties with VSMs and work towards a tight integra-tion of symbolic and distributional approaches, we propose a Compositional Matrix-Space Model (CMSM) which employs matrices instead of vec-tors and makes use of matrix multiplication as the one and only composition operation.

The paper is structured as follows: We start by providing the necessary basic notions in linear al-gebra in Section 2. In Section 3, we give a for-mal account of the concept of compositionality, introduce our model, and argue for the plausibil-ity of CMSMs in the light of structural and cogni-tive considerations. Section 4 shows how common VSM approaches to compositionality can be cap-tured by CMSMs while Section 5 illustrates the capabilities of our model to likewise cover sym-bolic approaches. In Section 6, we demonstrate how several CMSMs can be combined into one model. We provide an overview of related work in Section 7 before we conclude and point out av-enues for further research in Section 8. In this section, we recap some aspects of linear algebra to the extent needed for our considerations about CMSMs. For a more thorough treatise we refer the reader to a linear algebra textbook (such as Strang (1993)).
 Vectors. Given a natural number n , an n -dimensional vector v over the reals can be seen as a list (or tuple) containing n real numbers r ,..., r n  X  R , written v = ( r 1 r 2  X  X  X  r n ). Vectors will be denoted by lowercase bold font to the i th entry of vector v . As usual, we write R n to denote the set of all n -dimensional vectors with real entries. Vectors can be added entry-wise, i.e., ( r 1  X  X  X  r n ) + ( r 0 1  X  X  X  r 0 n ) = ( r 1 r 1  X  X  X  r n uct (also known as Hadamard product) is defined by ( r 1  X  X  X  r n ) ( r 0 1  X  X  X  r 0 n ) = ( r 1  X  r 0 1  X  X  X  r Matrices. Given two real numbers n , m , an n  X  m matrix over the reals is an array of real numbers with n rows and m columns. We will use capital letters to denote matrices and, given a matrix M we will write M ( i , j ) to refer to the entry in the i th row and the j th column:
M = The set of all n  X  m matrices with real num-ber entries is denoted by R n  X  m . Obviously, m -dimensional vectors can be seen as 1  X  m matri-ces. A matrix can be transposed by exchanging columns and rows: given the n  X  m matrix M , its transposed version M T is a m  X  n matrix defined by M T ( i , j ) = M ( j , i ).
 Linear Mappings. Beyond being merely array-like data structures, matrices correspond to certain type of functions, so-called linear mappings , hav-ing vectors as in-and output. More precisely, an n  X  m matrix M applied to an m -dimensional vec-tor v yields an n -dimensional vector v 0 (written: v M = v 0 ) according to
Linear mappings can be concatenated, giving rise to the notion of standard matrix multiplica-tion: we write M 1 M 2 to denote the matrix that corresponds to the linear mapping defined by ap-plying first M 1 and then M 2 . Formally, the matrix product of the n  X  l matrix M 1 and the l  X  m matrix M 2 is an n  X  m matrix M
Note that the matrix product is associative (i.e., ( M 1 M 2 ) M 3 = M 1 ( M 2 M 3 ) always holds, thus parentheses can be omitted) but not commutative ( M 1 M 2 = M 2 M 1 does not hold in general, i.e., the order matters).
 Permutations. Given a natural number n , a per-mutation on { 1 ... n } is a bijection (i.e., a map-ping that is one-to-one and onto)  X  : { 1 ... n }  X  { 1 ... n } . A permutation can be seen as a  X  X eorder-ing scheme X  on a list with n elements: the element at position i will get the new position  X  ( i ) in the reordered list. Likewise, a permutation can be ap-plied to a vector resulting in a rearrangement of the entries. We write  X  n to denote the permutation corresponding to the n -fold application of  X  and  X   X  1 to denote the permutation that  X  X ndoes X   X  .
Given a permutation  X  , the corresponding per-
Then, obviously permuting a vector according to
 X  can be expressed in terms of matrix multipli-cation as well as we obtain for any vector v  X  R n :
Likewise, iterated application (  X  n ) and the in-verses  X   X  n carry over naturally to the correspond-ing notions in matrices. The underlying principle of compositional seman-tics is that the meaning of a sentence (or a word phrase) can be derived from the meaning of its constituent tokens by applying a composition op-eration. More formally, the underlying idea can be described as follows: given a mapping [[  X  ]] :  X   X  S from a set of tokens (words)  X  into some semantical space S (the elements of which we will simply call  X  X eanings X ), we find a semantic com-position operation ./ : S  X   X  S mapping sequences of meanings to meanings such that the meaning of a sequence of tokens  X  1  X  2 ... X  n can be obtained by applying ./ to the sequence [[  X  1 ]][[  X  2 ]] ... [[  X  This situation qualifies [[  X  ]] as a homomorphism between (  X   X  ,  X  ) and ( S ,./ ) and can be displayed as follows: [[  X  1 ]]
A great variety of linguistic models are sub-sumed by this general idea ranging from purely symbolic approaches (like type systems and cate-gorial grammars) to rather statistical models (like vector space and word space models). At the first glance, the underlying encodings of word seman-tics as well as the composition operations di ff er significantly. However, we argue that a great vari-ety of them can be incorporated  X  and even freely inter-combined  X  into a unified model where the semantics of simple tokens and complex phrases is expressed by matrices and the composition op-eration is standard matrix multiplication.
 More precisely, in Compositional Marix-Space Models, we have S = R n  X  n , i.e. the semantical space consists of quadratic matrices, and the com-position operator ./ coincides with matrix multi-plication as introduced in Section 2. In the follow-ing, we will provide diverse arguments illustrating that CMSMs are intuitive and natural. 3.1 Algebraic Plausibility  X  Most linear-algebra-based operations that have been proposed to model composition in language models are associative and commutative. Thereby, they realize a multiset (or bag-of-words) seman-tics that makes them insensitive to structural dif-ferences of phrases conveyed through word order.
While associativity seems somewhat acceptable and could be defended by pointing to the stream-like, sequential nature of language, commutativity seems way less justifiable, arguably.

As mentioned before, matrix multiplication is associative but non-commutative, whence we pro-pose it as more adequate for modeling composi-tional semantics of language. 3.2 Neurological Plausibility  X  From a very abstract and simplified perspective, CMSMs can also be justified neurologically.

Suppose the mental state of a person at one spe-cific moment in time can be encoded by a vector v of numerical values; one might, e.g., think of the level of excitation of neurons. Then, an external stimulus or signal, such as a perceived word, will result in a change of the mental state. Thus, the external stimulus can be seen as a function being applied to v yielding as result the vector v 0 that corresponds to the persons mental state after re-ceiving the signal. Therefore, it seems sensible to associate with every signal (in our case: token  X  ) a respective function (a linear mapping, represented by a matrix M = [[  X  ]] that maps mental states to mental states (i.e. vectors v to vectors v 0 = v M ).
Consequently, the subsequent reception of in-puts  X  ,  X  0 associated to matrices M and M 0 will transform a mental vector v into the vector ( v M ) M 0 which by associativity equals v ( M M 0 ). Therefore, M M 0 represents the mental state tran-sition triggered by the signal sequence  X  X  0 . Nat-urally, this consideration carries over to sequences of arbitrary length. This way, abstracting from specific initial mental state vectors, our semantic space S can be seen as a function space of mental transformations represented by matrices, whereby matrix multiplication realizes subsequent execu-tion of those transformations triggered by the in-put token sequence. 3.3 Psychological Plausibility  X  A structurally very similar argument can be pro-vided on another cognitive explanatory level. There have been extensive studies about human language processing justifying the hypothesis of a working memory (Baddeley, 2003). The men-tal state vector can be seen as representation of a person X  X  working memory which gets transformed by external input. Note that matrices can per-form standard memory operations such as storing, deleting, copying etc. For instance, the matrix M applied to a vector v , will copy its k th entry to the l th position. This mechanism of storage and inser-tion can, e.g., be used to simulate simple forms of anaphora resolution. In VSMs numerous vector operations have been used to model composition (Widdows, 2008), some of the more advanced ones being related to quantum mechanics. We show how these com-mon composition operators can be modeled by CMSMs. 1 Given a vector composition operation ./ : R n  X  R n  X  R n , we provide a surjective function  X  resentation into a matrix representation in a way such that for all v 1 ,... v k  X  R n holds of the matrices assigned to v i and v j . 4.1 Vector Addition As a simple basic model for semantic composi-tion, vector addition has been proposed. Thereby, tokens  X  get assigned (usually high-dimensional) vectors v  X  and to obtain a representation of the meaning of a phrase or a sentence w =  X  1 ... X  k , the vector sum of the vectors associated to the con-stituent tokens is calculated: v w = P k i = 1 v  X 
This kind of composition operation is subsumed by CMSMs; suppose in the original model, a token  X  gets assigned the vector v  X  , then by defining (mapping n -dimensional vectors to ( n + 1)  X  ( n + 1) matrices), we obtain for a phrase w =  X  1 ... X  k  X  Proof. By induction on k . For k = 1, we have v = = 4.2 Component-wise Multiplication On the other hand, the Hadamard product (also called entry-wise product, denoted by ) has been proposed as an alternative way of semantically composing token vectors.
 By using a di ff erent encoding into matrices, CMSMs can simulate this type of composition op-eration as well. By letting we obtain an n  X  n matrix representation for which 4.3 Holographic Reduced Representations Holographic reduced representations as intro-duced by Plate (1995) can be seen as a refinement of convolution products with the benefit of pre-serving dimensionality: given two vectors v 1 , v 2  X  R , their circular convolution product v 1 ~ v 2 is again an n -dimensional vector v 3 defined by v ( i + 1) = for 0  X  i  X  n  X  1. Now let  X  ~ ( v ) be the n  X  n matrix M with In the 3-dimensional case, this would result in  X  ~ ( v (1) v (2) v (3)) = Then, it can be readily checked that  X  4.4 Permutation-based Approaches Sahlgren et al. (2008) use permutations on vec-tors to account for word order. In this approach, given a token  X  m occurring in a sentence w =  X  1 ... X  k with predefined  X  X ncontextualized X  vec-tors v  X  vector v w , m for  X  m by which can be equivalently transformed into Note that the approach is still token-centered, i.e., a vector representation of a token is endowed with contextual representations of surrounding tokens. Nevertheless, this setting can be transferred to a CMSM setting by recording the position of the fo-cused token as an additional parameter. Now, by assigning every v  X  the matrix we observe that for we have Now we will elaborate on symbolic approaches to language, i.e., discrete grammar formalisms, and show how they can conveniently be embedded into CMSMs. This might come as a surprise, as the ap-parent likeness of CMSMs to vector-space models may suggest incompatibility to discrete settings. 5.1 Group Theory Group theory and grammar formalisms based on groups and pre-groups play an important role in computational linguistics (Dymetman, 1998; Lambek, 1958). From the perspective of our com-positionality framework, those approaches employ a group (or pre-group) ( G ,  X  ) as semantical space S where the group operation (often written as multi-plication) is used as composition operation ./ .
According Cayley X  X  Theorem (Cayley, 1854), every group G is isomorphic to a permutation group on some set S . Hence, assuming finite-ness of G and consequently S , we can encode group-based grammar formalisms into CMSMs in a straightforward way by using permutation matri-ces of size | S | X | S | . 5.2 Regular Languages Regular languages constitute a basic type of lan-guages characterized by a symbolic formalism. We will show how to select the assignment [[  X  ]] for a CMSM such that the matrix associated to a token sequence exhibits whether this sequence be-longs to a given regular language, that is if it is accepted by a given finite state automaton. As usual (cf. e.g., Hopcroft and Ullman (1979)) we define a nondeterministic finite automaton A = ( Q set of states,  X  the input alphabet,  X   X  Q  X   X   X  Q the transition relation, and Q I and Q F being the sets of initial and final states, respectively.
Then we assign to every token  X   X   X  the n  X  n matrix [[  X  ]] = M with Hence essentially, the matrix M encodes all state transitions which can be caused by the input  X  . Likewise, for a word w =  X  1 ... X  k  X   X   X  , the matrix M w : = [[  X  1 ]] ... [[  X  k ]] will encode all state transitions mediated by w . Finally, if we define vectors v I and v F by v ( i ) = then we find that w is accepted by A exactly if v 5.3 The General Case: Matrix Grammars Motivated by the above findings, we now define a general notion of matrix grammars as follows: Definition 1 Let  X  be an alphabet. A matrix grammar M of degree n is defined as the pair  X  [[  X  ]] , AC  X  where [[  X  ]] is a mapping from  X  to n  X  n matrices and AC = { X  v 0 1 , v 1 , r 1  X  ,...,  X  v 0 m , v with v 0 1 , v 1 ,..., v 0 m , v m  X  R n and r 1 ,..., r is a finite set of acceptance conditions . The lan-guage generated by M (denoted by L ( M ) ) con-tains a token sequence  X  1 ... X  k  X   X   X  exactly if v [[  X  1 ]] ... [[  X  k ]] v T i  X  r i for all i  X  { 1 ,..., will call a language L matricible if L = L ( M ) for some matrix grammar M .

Then, the following proposition is a direct con-sequence from the preceding section.
 Proposition 1 Regular languages are matricible.
However, as demonstrated by the subsequent examples, also many non-regular and even non-context-free languages are matricible, hinting at the expressivity of our grammar model.
 Example 1 We define M X  [[  X  ]] , AC  X  with [[ b ]] = Then L ( M ) contains exactly all palindromes from { a , b , c }  X  , i.e., the words d 1 d 2 ... d n  X  1 d n d Example 2 We define M =  X  [[  X  ]] , AC  X  with
 X  = { a , b , c } [[ a ]] = [[ b ]] =
Then L ( M ) is the (non-context-free) language { a m b m c m | m &gt; 0 } .

The following properties of matrix grammars and matricible language are straightforward. Proposition 2 All languages characterized by a set of linear equations on the letter counts are ma-tricible.
 Proof. Suppose  X  = { a 1 ,... a n } . Given a word w , let x i denote the number of occurrences of a i in w . A linear equation on the letter counts has the form k 1 x 1 + ... + k n x n = k k , k 1 ,..., k n  X  R
Now define [[ a i ]] =  X  + ( e i ), where e i is the i th unit vector, i.e. it contains a 1 at he i th position and 0 in all other positions. Then, it is easy to see that w will be mapped to M =  X  + ( x 1  X  X  X  x n ). Due to the fact that e n + 1 M = ( x 1  X  X  X  x n 1) we can enforce the above linear equation by defining the acceptance conditions Proposition 3 The intersection of two matricible languages is again a matricible language.
 Proof. This is a direct consequence of the con-siderations in Section 6 together with the observa-tion, that the new set of acceptance conditions is trivially obtained from the old ones with adapted dimensionalities. q.e.d.
Note that the fact that the language { a m b m c m | m &gt; 0 } is matricible, as demonstrated in Ex-ample 2 is a straightforward consequence of the Propositions 1, 2, and 3, since the language in question can be described as the intersection of the regular language a + b + c + with the language characterized by the equations x a  X  x b = 0 and x count of the expressivity of matrix grammars by showing undecidability of the emptiness problem. Proposition 4 The problem whether there is a word which is accepted by a given matrix gram-mar is undecidable.
 Proof. The undecidable Post correspondence problem (Post, 1946) is described as follows: given two lists of words u 1 ,..., u n and v 1 ,..., v n over some alphabet  X  0 , is there a sequence of num-bers h 1 ,..., h m (1  X  h j  X  n ) such that u h v
We now reduce this problem to the emptiness problem of a matrix grammar. W.l.o.g., let  X  0 = { a 1 ,..., a k } . We define a bijection # from by Note that this is indeed a bijection and that for w , w 2  X   X  0 X  , we have Now, we define M as follows:
 X  = { b Using the above fact about # and a simple induc-tion on m , we find that [[ a h Evaluating the two acceptance conditions, we find them satisfied exactly if #( u h #( v h case if and only if u h fore M accepts b h h ,..., h m is a solution to the given Post Corre-spondence Problem. Consequently, the question whether such a solution exists is equivalent to the question whether the language L ( M ) is non-empty. q.e.d.

These results demonstrate that matrix grammars cover a wide range of formal languages. Never-theless some important questions remain open and need to be clarified next: Are all context-free languages matricible? We conjecture that this is not the case. 3 Note that this question is directly related to the question whether Lambek calculus can be modeled by matrix gram-mars.
 Are matricible languages closed under concatena-tion? That is: given two arbitrary matricible lan-guages L 1 , L 2 , is the language L = { w 1 w 2 | w 1  X  L , w 2  X  L 2 } again matricible? Being a property common to all language types from the Chomsky hierarchy, answering this question is surprisingly non-trivial for matrix grammars.

In case of a negative answer to one of the above questions it might be worthwhile to introduce an extended notion of context grammars to accom-modate those desirable properties. For example, allowing for some nondeterminism by associating several matrices to one token would ensure closure under concatenation.
 How do the theoretical properties of matrix gram-mars depend on the underlying algebraic struc-ture? Remember that we considered matrices con-taining real numbers as entries. In general, ma-trices can be defined on top of any mathemati-cal structure that is (at least) a semiring (Golan, 1992). Examples for semirings are the natural numbers, boolean algebras, or polynomials with natural number coe ffi cients. Therefore, it would be interesting to investigate the influence of the choice of the underlying semiring on the prop-erties of the matrix grammars  X  possibly non-standard structures turn out to be more appropri-ate for capturing certain compositional language properties. Another central advantage of the proposed matrix-based models for word meaning is that several matrix models can be easily combined into one. Again assume a sequence w =  X  1 ... X  k of tokens with associated matrices [[  X  1 ]] ,..., [[  X  k according to one specific model and matrices ([  X  1 ]) ,..., ([  X  k ]) according to another.
Then we can combine the two models into one { [  X  ] } by assigning to  X  i the matrix By doing so, we obtain the correspondence { [  X  1 ] } ... { [  X  k ] } = In other words, the semantic compositions belong-ing to two CMSMs can be executed  X  X n parallel. X  Mark that by providing non-zero entries for the up-per right and lower left matrix part, information exchange between the two models can be easily realized. We are not the first to suggest an extension of classical VSMs to matrices. Distributional mod-els based on matrices or even higher-dimensional arrays have been proposed in information retrieval (Gao et al., 2004; Antonellis and Gallopoulos, 2006). However, to the best of our knowledge, the approach of realizing compositionality via matrix multiplication seems to be entirely original.
Among the early attempts to provide more com-pelling combinatory functions to capture word or-der information and the non-commutativity of lin-guistic compositional operation in VSMs is the work of Kintsch (2001) who is using a more so-phisticated addition function to model predicate-argument structures in VSMs.

Mitchell and Lapata (2008) formulate seman-tic composition as a function m = f ( w 1 , w 2 , R , K ) where R is a relation between w 1 and w 2 and K is additional knowledge. They evaluate the model with a number of addition and multiplication op-erations for vector combination on a sentence sim-ilarity task proposed by Kintsch (2001). Widdows (2008) proposes a number of more advanced vec-tor operations well-known from quantum mechan-ics, such as tensor product and convolution, to model composition in vector spaces. He shows the ability of VSMs to reflect the relational and phrasal meanings on a simplified analogy task. Giesbrecht (2009) evaluates four vector compo-sition operations ( + , , tensor product, convolu-tion) on the task of identifying multi-word units. The evaluation results of the three studies are not conclusive in terms of which vector operation per-forms best; the di ff erent outcomes might be at-tributed to the underlying word space models; e.g., the models of Widdows (2008) and Giesbrecht (2009) feature dimensionality reduction while that of Mitchell and Lapata (2008) does not. In the light of these findings, our CMSMs provide the benefit of just one composition operation that is able to mimic all the others as well as combina-tions thereof. We have introduced a generic model for compo-sitionality in language where matrices are associ-ated with tokens and the matrix representation of a token sequence is obtained by iterated matrix mul-tiplication. We have given algebraic, neurological, and psychological plausibility indications in favor of this choice. We have shown that the proposed model is expressive enough to cover and combine a variety of distributional and symbolic aspects of natural language. This nourishes the hope that ma-trix models can serve as a kind of lingua franca for compositional models.

This having said, some crucial questions remain before CMSMs can be applied in practice: How to acquire CMSMs for large token sets and specific purposes? We have shown the value and expressivity of CMSMs by providing care-fully hand-crafted encodings. In practical cases, however, the number of token-to-matrix assign-ments will be too large for this manual approach. Therefore, methods to (semi-)automatically ac-quire those assignments from available data are re-quired. To this end, machine learning techniques need to be investigated with respect to their ap-plicability to this task. Presumably, hybrid ap-proaches have to be considered, where parts of the matrix representation are learned whereas oth-ers are stipulated in advance guided by external sources (such as lexical information).

In this setting, data sparsity may be overcome through tensor methods: given a set T of tokens together with the matrix assignment [[]] : T  X  R n  X  n , this datastructure can be conceived as a 3-dimensional array (also known as tensor) of size n  X  n  X | T | wherein the single token-matrices can be found as slices. Then tensor decomposition tech-niques can be applied in order to find a compact representation, reduce noise, and cluster together similar tokens (Tucker, 1966; Rendle et al., 2009). First evaluation results employing this approach to the task of free associations are reported by Gies-brecht (2010).
 How does linearity limit the applicability of CMSMs? In Section 3, we justified our model by taking the perspective of tokens being functions which realize mental state transitions. Yet, us-ing matrices to represent those functions restricts them to linear mappings. Although this restric-tion brings about benefits in terms of computabil-ity and theoretical accessibility, the limitations in-troduced by this assumption need to be investi-gated. Clearly, certain linguistic e ff ects (like a-posteriori disambiguation) cannot be modeled via linear mappings. Instead, we might need some in-between application of simple nonlinear func-tions in the spirit of quantum-collapsing of a "su-perposed" mental state (such as the winner takes it all, survival of the top-k vector entries, and so forth). Thus, another avenue of further research is to generalize from the linear approach.
 This work was supported by the German Research Foundation (DFG) under the Multipla project (grant 38457858) as well as by the German Fed-eral Ministry of Economics (BMWi) under the project Theseus (number 01MQ07019).

