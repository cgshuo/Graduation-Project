 Sergey Levine svlevine@stanford.edu Vladlen Koltun vladlen@stanford.edu Reinforcement learning is a powerful framework for controlling dynamical systems. Direct policy search methods are often employed in high-dimensional ap-plications such as robotics, since they scale gracefully with dimensionality and offer appealing convergence guarantees (Peters &amp; Schaal, 2008). However, it is often necessary to carefully choose a specialized pol-icy class to learn the policy in a reasonable number of iterations without falling into poor local optima. Substantial improvements on real-world systems have come from specialized and innovative policy classes (Ijspeert et al., 2002). This specialization comes at a cost in generality, and can restrict the types of be-haviors that can be learned. For example, a policy that tracks a single trajectory cannot choose different trajectories depending on the state. In this work, we aim to learn policies with very general and flexible rep-resentations, such as large neural networks, which can represent a broad range of behaviors. Learning such complex, nonlinear policies with standard policy gra-dient methods can require a huge number of iterations, and can be disastrously prone to poor local optima. In this paper, we show how trajectory optimization can guide the policy search away from poor local optima. Our guided policy search algorithm uses differential dynamic programming (DDP) to generate  X  X uiding samples, X  which assist the policy search by exploring high-reward regions. An importance sampled variant of the likelihood ratio estimator is used to incorporate these guiding samples directly into the policy search. We show that DDP can be modified to sample from a distribution over high reward trajectories, making it particularly suitable for guiding policy search. Fur-thermore, by initializing DDP with example demon-strations, our method can perform learning from demonstration. The use of importance sampled pol-icy search also allows us to optimize the policy with second order quasi-Newton methods for many gradient steps without requiring new on-policy samples, which can be crucial for complex, nonlinear policies. Our main contribution is a guided policy search algo-rithm that uses trajectory optimization to assist pol-icy learning. We show how to obtain suitable guiding samples, and we present a regularized importance sam-pled policy optimization method that can utilize guid-ing samples and does not require a learning rate or new samples at every gradient step. We evaluate our method on planar swimming, hopping, and walking, as well as 3D humanoid running, using general-purpose neural network policies. We also show that both the proposed sampling scheme and regularizer are essen-tial for good performance, and that the learned policies can generalize successfully to new environments. Reinforcement learning aims to find a policy  X  to con-trol an agent in a stochastic environment. At each time step t , the agent observes a state x t and chooses an action according to  X  ( u t | x t ), producing a state transi-tion according to p ( x t +1 | x t , u t ). The goal is specified by the reward r ( x t , u t ), and an optimal policy is one that maximizes the expected sum of rewards (return) from step 1 to T . We use  X  to denote a sequence of states and actions, with r (  X  ) and  X  (  X  ) being the total reward along  X  and its probability under  X  . We focus on finite-horizon tasks in continuous domains, though extensions to other formulations are also possible. Policy gradient methods learn a parameterized policy  X   X  by directly optimizing its expected return E [ J (  X  )] with respect to the parameters  X  . In particular, like-lihood ratio methods estimate the gradient E [  X  J (  X  )] using samples  X  1 ,..., X  m drawn from the current pol-icy  X   X  , and then improve the policy by taking a step along this gradient. The gradient can be estimated using the following equation (Peters &amp; Schaal, 2008): E [  X  J (  X  )] = E [ r (  X  )  X  log  X   X  (  X  )]  X  where  X  log  X   X  (  X  i ) decomposes to P t  X  log  X   X  ( u t since the transition model p ( x t +1 | x t , u t ) does not de-pend on  X  . Standard likelihood ratio methods require new samples from the current policy at each gradient step, do not admit off-policy samples, and require the learning rate to be chosen carefully to ensure conver-gence. In the next section, we discuss how importance sampling can be used to lift these constraints. Importance sampling is a technique for estimating an expectation E p [ f ( x )] with respect to p ( x ) using sam-ples drawn from a different distribution q ( x ): Importance sampling is unbiased if Z = m , though variance. Prior work proposed estimating E [ J (  X  )] with importance sampling (Peshkin &amp; Shelton, 2002; Tang &amp; Abbeel, 2010). This allows using off-policy samples and results in the following estimator: The variance of this estimator can be reduced further by observing that past rewards do not depend on fu-ture actions (Sutton et al., 1999; Baxter et al., 2001): where  X   X  (  X  i, 1: t ) denotes the probabiliy of the first t steps of  X  i , and Z t (  X  ) normalizes the weights. To in-clude samples from multiple distributions, we follow prior work and use a fused distribution of the form q (  X  ) = 1 n P j q j (  X  ), where each q j is either a previous policy or a guiding distribution constructed with DDP. Prior methods optimized Equation (1) or (2) directly. Unfortunately, with complex policies and long rollouts, this often produces poor results, as the estimator only considers the relative probability of each sample and does not require any of these probabilities to be high. The optimum can assign a low probability to all sam-ples, with the best sample slightly more likely than the rest, thus receiving the only nonzero weight. Tang and Abbeel (2010) attempted to mitigate this issue by constraining the optimization by the variance of the weights. However, this is ineffective when the distri-butions are highly peaked, as is the case with long roll-outs, because very few samples have nonzero weights, and their variance tends to zero as the sample count in-creases. In our experiments, we found this problem to be very common in large, high-dimensional problems. To address this issue, we augment Equation (2) with a novel regularizing term. A variety of regularizers are possible, but we found the most effective one to be the logarithm of the normalizing constant:  X (  X  ) = This objective is maximized using an analytic gradient derived in Appendix A of the supplement. It is easy to check that this estimator is consistent, since Z t (  X  )  X  1 in the limit of infinite samples. The regularizer acts as a soft maximum over the logarithms of the weights, ensuring that at least some samples have a high proba-bility under  X   X  . Furthermore, by adaptively adjusting w , we can control how far the policy is allowed to de-viate from the samples, which can be used to limit the optimization to regions that are better represented by the samples if it repeatedly fails to make progress. Prior methods employed importance sampling to reuse samples from previous policies (Peshkin &amp; Shelton, 2002; Kober &amp; Peters, 2009; Tang &amp; Abbeel, 2010). However, when learning policies with hundreds of pa-rameters, local optima make it very difficult to find a good solution. In this section, we show how differential dynamic programming (DDP) can be used to supple-ment the sample set with off-policy guiding samples that guide the policy search to regions of high reward. 4.1. Constructing Guiding Distributions An effective guiding distribution covers high-reward regions while avoiding large q (  X  ) densities, which result in low importance weights. We posit that a good guid-ing distribution is an I-projection of  X  (  X  )  X  exp( r (  X  )). An I-projection q of  X  minimizes the KL-divergence D
KL ( q ||  X  ) = E q [  X  r (  X  )]  X  X  ( q ), where H denotes en-tropy. The first term forces q to be high only in regions of high reward, while the entropy maximization favors broad distributions. We will show that an approxi-mate Gaussian I-projection of  X  can be computed using a variant of DDP called iterative LQR (Tassa et al., 2012), which optimizes a trajectory by repeatedly solv-ing for the optimal policy under linear-quadratic as-sumptions. Given a trajectory (  X  x 1 ,  X  u 1 ) ,..., (  X  x and defining  X  x t = x t  X   X  x t and  X  u t = u t  X   X  u t namics and reward are approximated as r ( x t , u t )  X   X  x T t r x t +  X  u T r u t + Subscripts denote the Jacobians and Hessians of the dynamics f and reward r , which are assumed to exist. 1 Iterative LQR recursively estimates the Q-function: as well as the value function and linear policy terms: The deterministic optimal policy is then given by By repeatedly computing this policy and following it to obtain a new trajectory, this algorithm converges to a locally optimal solution. We now show how the same algorithm can be used with the framework of linearly solvable MDPs (Dvijotham &amp; Todorov, 2010) and the related concept of maximum entropy control (Ziebart, 2010) to build an approximate Gaussian I-projection of  X  (  X  )  X  exp( r (  X  )). Under this framework, the optimal policy  X  G maximizes an augmented reward, given by where p is a  X  X assive dynamics X  distribution. If p is uniform, the expected return of a policy  X  G is which means that if  X  G maximizes this return, it is an I-projection of  X  . Ziebart (2010) showed that the optimal policy under uniform passive dynamics is where V is a modified value function given by Under linear dynamics and quadratic rewards, it can be shown that V has the same form as the deriva-tion above, and Equation 4 is a linear Gaussian with the mean given by g ( x t ) and the covariance given by  X  Q  X  1 uu t . This stochastic policy corresponds approx-imately to a Gaussian distribution over trajectories. We can therefore sample from an approximate Gaus-sian I-projection of  X  by following the stochastic policy It should be noted that  X  G (  X  ) is only Gaussian un-der linear dynamics. When the dynamics are nonlin-ear,  X  G (  X  ) approximates a Gaussian around the nomi-nal trajectory. Fortunately, the feedback term usually keeps the samples close to this trajectory, making them suitable guiding samples for the policy search. 4.2. Adaptive Guiding Distributions The distribution in the preceding section captures high-reward regions, but does not consider the current policy  X   X  . We can adapt it to  X   X  by sampling from an I-projection of  X   X  (  X  )  X  exp( r (  X  ))  X   X  (  X  ), which is the op-timal distribution for estimating E  X  construct the approximate I-projection of  X   X  , we sim-ply run the DDP algorithm with the reward  X  r ( x t , u t then an approximate I-projection of  X   X  (  X  )  X  exp(  X  r (  X  )). In practice, we found that many domains do not re-quire adaptive samples. As discussed in the evaluation, adaptation becomes necessary when the initial samples cannot be reproduced by any policy, such as when they act differently in similar states. In that case, adapted samples will avoid different actions in similar states, making them more suited for guiding the policy. 4.3. Incorporating Guiding Samples We incorporate guiding samples into the policy search by building one or more initial DDP solutions and sup-plying the resulting samples to the importance sam-pled policy search algorithm. These solutions can be initialized with human demonstrations or with an of-fline planning algorithm. When learning from demon-strations, we can perform just one step of DDP start-ing from the example demonstration, thus construct-ing a Gaussian distribution around the example. If adaptive guiding distributions are used, they are con-structed at each iteration of the policy search starting from the previous DDP solution.
 Although our policy search component is model-free, DDP requires a model of the system dynamics. Nu-merous recent methods have proposed to learn the model (Abbeel et al., 2006; Deisenroth &amp; Rasmussen, 2011; Ross &amp; Bagnell, 2012), and if we use initial ex-amples, only local models are required. In Section 8, we also discuss model-free alternatives to DDP. One might also wonder why the DDP policy  X  G is not itself a suitable controller. The issue is that  X  G is time-varying and only valid around a single trajectory, while the final policy can be learned from many DDP solu-tions in many situations. Guided policy search can be viewed as transforming a collection of trajectories into a controller. This controller can adhere to any pa-rameterization, reflecting constraints on computation or available sensors in partially observed domains. In our evaluation, we show that such a policy generalizes to situations where the DDP policy fails. Algorithm 1 summarizes our method. On line 1, we build one or more DDP solutions, which can be ini-tialized from demonstrations. Initial guiding samples are sampled from these solutions and used on line 3 to pretrain the initial policy  X   X  ? . Since the samples are drawn from stochastic feedback policies,  X   X  ? can al-ready learn useful feedback rules during this pretrain-ing stage. The sample set S is constructed on line 4 from the guiding samples and samples from  X   X  ? , and the policy search then alternates between optimizing  X (  X  ) and gathering new samples from the current pol-icy  X   X  S k is chosen on line 6. In practice, we simply choose the samples with high importance weights under the current best policy  X   X  ? , as well as the guiding samples. The objective  X (  X  ) is optimized on line 7 with LBFGS. This objective can itself be susceptible to local optima: when the weight on a sample is very low, it is effec-Algorithm 1 Guided Policy Search 1: Generate DDP solutions  X  G 1 ,..., X  G n 2: Sample  X  1 ,..., X  m from q (  X  ) = 1 n P i  X  G i (  X  ) 3: Initialize  X  ?  X  arg max  X  P i log  X   X  ? (  X  i ) 4: Build initial sample set S from  X  G 1 ,..., X  G n , X   X  ? 5: for iteration k = 1 to K do 6: Choose current sample set S k  X  X  7: Optimize  X  k  X  arg max  X   X  S k (  X  ) 8: Append samples from  X   X  k to S k and S 9: Optionally generate adaptive guiding samples 10: Estimate the values of  X   X  k and  X   X  ? using S k 11: if  X   X  k is better than  X   X  ? then 12: Set  X  ?  X   X  k 13: Decrease w r 14: else 15: Increase w r 16: Optionally, resample from  X   X  ? 17: end if 18: end for 19: Return the best policy  X   X  ? tively ignored by the optimization. If the guiding sam-ples have low weights, the optimization cannot benefit from them. To mitigate this issue, we repeat the op-timization twice, once starting from the best current parameters  X  ? , and once by initializing the parameters with an optimization that maximizes the log weight on the highest-reward sample. Prior work suggested restarting the optimization from each previous policy (Tang &amp; Abbeel, 2010), but this is very slow with com-plex policies, and still fails to explore the guiding sam-ples, for which there are no known policy parameters. Once the new policy  X   X  from  X   X  ing samples, the adaptation is done on line 9 and new guiding samples are also added. We then use Equa-tion 2 to estimate the returns of both the new policy and the current best policy  X   X  ? on line 10. Since S k now contains samples from both policies, we expect the estimator to be accurate. If the new policy is better, it replaces the best policy. Otherwise, the regularization weight w r is increased. Higher regularization causes the next optimization to stay closer to the samples, making the estimated return more accurate. Once the policy search starts making progress, the weight is de-creased. In practice, we clamp w r between 10  X  2 and 10  X  6 and adjust it by a factor of 10. We also found that the policy sometimes failed to improve if the sam-ples from  X   X  ? had been unusually good by chance. To address this, we draw additional samples from  X   X  ? on line 16 to prevent the policy search from getting stuck due to an overestimate of the best policy X  X  value. We evaluated our method on planar swimming, hop-ping, and walking, as well as 3D running. Each task was simulated with the MuJoCo simulator (Todorov et al., 2012), using systems of rigid links with noisy motors at the joints. 3 The policies were represented by neural network controllers that mapped current joint angles and velocities directly to joint torques. The re-ward function was a sum of three terms: r ( x , u ) =  X  w u || u || 2  X  w v ( v x  X  v ? x ) 2  X  w where v x and v ? x are the current and desired horizon-tal velocities, p y and p ? y are the current and desired heights of the root link, and w u , w v , and w h deter-mine the weight on each objective term. The weights for each task are given in Appendix B of the supple-ment, along with descriptions of each simulated robot. The policy was represented by a neural network with one hidden layer, with a soft rectifying nonlinearity a = log(1 + exp( z )) at the first layer and linear con-nections to the output layer. Gaussian noise was added to the output to create a stochastic policy. The pol-icy search ran for 80 iterations, with 40 initial guiding samples, 10 samples added from the current policy at each iteration, and up to 100 samples selected for the active set S k . Adaptive guiding distributions were only used in Section 6.2. Adaptation was performed in the first 40 iterations, with 50 DDP iterations each time. The initial guiding distributions for the swimmer and hopper were generated directly with DDP. To illustrate the capacity of our method to learn from demonstra-tion, the initial example trajectory for the walker was obtained from a prior locomotion system (Yin et al., 2007), while the 3D humanoid was initialized with ex-ample motion capture of human running.
 When using examples, we regularized the reward with a tracking term equal to the squared deviation from the example states and actions, with a weight of 0 . 05. This term serves to keep the guiding samples close to the examples and ensures that the policy covariance is positive definite. The same term was also used with the swimmer and hopper for consistency. The tracking term was only used for the initial guiding distributions, and was not used during the policy search.
 The swimmer, hopper, and walker are shown in Fig-ure 1, with plots of the root position under the learned policies and under the initial DDP solution. The 3D humanoid is shown in Figure 5. 6.1. Comparisons In the first set of tests, we compared to three variants of our algorithm and three prior methods, using the planar swimmer, hopper, and walker domains. The time horizon was 500, and the policies had 50 hidden units and up to 1256 parameters. The first variant did not use the regularization term. 4 The second, referred to as  X  X on-guided, X  did not use the guiding samples during the policy search. The third variant also did not use the guiding samples in the policy objective, but used the guiding distributions as  X  X estart distri-butions X  to specify new initial state distributions that cover states we expect to visit under a good policy, analogously to prior work (Kakade &amp; Langford, 2002; Bagnell et al., 2003). This comparison was meant to check that the guiding samples actually aided policy learning, rather than simply focusing the policy search on  X  X ood X  states. All variants initialize the policy by pretraining on the guiding samples.
 The first prior method, referred to as  X  X ingle exam-ple, X  initialized the policy using the single initial exam-ple, as in prior work on learning from demonstration, and then improved the policy with importance sam-pling but no guiding samples, again as in prior work (Peshkin &amp; Shelton, 2002). The second method used standard policy gradients, with a PGT or GPOMDP-type estimator (Sutton et al., 1999; Baxter et al., 2001). The third method was DAGGER, an imita-tion learning algorithm that aims to find a policy that matches the  X  X xpert X  DDP actions (Ross et al., 2011). The results are shown in Figure 2 in terms of the mean reward of the 10 samples at each iteration, along with the value of the initial example and a shaded interval indicating two standard deviations of the guiding sam-ple values. Our algorithm learned each gait, matching the reward of the initial example. The comparison to the unregularized variant shows that the proposed reg-ularization term is crucial for obtaining a good policy. The non-guided variant, which only used the guiding samples for pretraining, sometimes found a good solu-tion because even a partially successful initial policy may succeed on one of its initial samples, which then serves the same function as the guiding samples by indicating high-reward regions. However, a successful non-guided outcome hinged entirely on the initial pol-icy. This is illustrated by the fourth graph in Figure 2, which shows a walking policy with 20 hidden units that is less successful initially. The full algorithm was still able to improve using the guiding samples, while the non-guided variant did not make progress. The restart distribution variant performed poorly. Wider initial state distributions greatly increased the variance of the estimator, and because the guiding distributions were only used to sample states, their ability to also point out good actions was not leveraged.
 Standard policy gradient and  X  X ingle example X  learn-ing from demonstration methods failed to learn any of the gaits. This suggests that guiding samples are cru-cial for learning such complex behaviors, and initializa-tion from a single example is insufficient. DAGGER also performed poorly, since it assumed that the ex-pert could provide optimal actions in all states, while the DDP policy was actually only valid close to the example. DAGGER therefore wasted a lot of effort on states where the DDP policy was not valid, such as after the hopper or walker fell. Interestingly, the GPS poli-cies often used less torque than the initial DDP solu-tion. The plot on the right shows the torque magni-tudes for the walking pol-icy, the deterministic DDP policy around the example, and the guiding samples. The smoother, more compli-ant behavior of the learned policy can be explained in part by the fact that the neural network can produce more subtle corrections than simple linear feedback. 6.2. Adaptive Resampling The initial trajectories in the previous section could all be reproduced by neural networks with sufficient train-ing, making adaptive guiding distributions unneces-sary. In the next experiment, we constructed a walking example that switched to another gait after 2 . 5s, mak-ing the initial guiding samples difficult to recreate with a stationary policy. As discussed in Section 4.2, adapt-ing the guiding distribution to the policy can produce more suitable samples in such cases. The plot on the right shows the results with and with-out adaptive samples. With adaptation, our algorithm quickly learned a success-ful gait, while without it, it made little progress. 5 The supplemental video shows that the final adapted example has a single gait that resembles both initial gaits. In practice, adaptation is useful if the examples are of low quality, or if the obser-vations are insufficient to distinguish states where they takes different actions. As the examples are adapted, the policy encourages the DDP solution to be more regular. In future work, it would be interesting to see if this iterative process can find trajectories that are too complex to be found by either method alone. 6.3. Generalization Next, we explore how our policies generalize to new en-vironments. We trained a policy with 100 hidden units to walk 4m on flat ground, and then climb a 10  X  in-cline. The policy could not perceive the slope, and the root height was only given relative to the ground. We then moved the start of the incline and compared our method to the initial DDP policy, as well as a simpli-fied trajectory-based dynamic programming (TBDP) approach that aggregates local DDP policies and uses the policy of the nearest neighbor to the current state (Atkeson &amp; Stephens, 2008). Prior TBDP methods add new trajectories until the TBDP policy achieves good results. Since the initial DDP policy already suc-ceeds on the training environment, only this initial pol-icy was used. Both methods therefore used the same DDP solution: TBDP used it to build a nonparametric policy, and GPS used it to generate guiding samples. As shown in Figure 3, GPS generalized to all positions, while the DDP policy, which expected the incline to start at a specific time, could only climb it in a small interval around its original location. The TBDP solu-tion was also able to generalize successfully. In the second experiment, we trained a walking policy on terrain consisting of 1m and 2m segments with vary-ing slopes, and tested on four random terrains. The results in Figure 4 show that GPS generalized to the new terrains, while the nearest-neighbor TBDP pol-icy did not, with all rollouts eventually failing on each test terrain. Unlike TBDP, the GPS neural network learned generalizable rules for balancing on sloped ter-rain. While these rules might not generalize to much steeper inclines without additional training, they in-dicate a degree of generalization significantly greater than nearest-neighbor lookup. Example rollouts from each policy are shown in the supplemental video. 6 6.4. Humanoid Running In our final experiment, we used our method to learn a 3D humanoid running gait. This task is highly dy-namic, requires good timing and balance, and has 63 state dimensions, making it well suited for ex-ploring the capacity of our method to scale to high-dimensional problems. Previous locomotion methods often rely on hand-crafted components to introduce prior knowledge or ensure stability (Tedrake et al., 2004; Whitman &amp; Atkeson, 2009), while our method used only general purpose neural networks.
 As in the previous section, the policy was trained on terrain of varying slope, and tested on four random terrains. Due to the complexity of this task, we used three different training terrains and a policy with 200 hidden units. The motor noise was reduced from 10% to 1%. The guiding distributions were initialized from motion capture of a human run, and DDP was used to find the torques that realized this run on each train-ing terrain. Since the example was recorded on flat ground, we used more mild 3  X  slopes.
 Rollouts from the learned policy on the test terrains are shown in Figure 5, with comparisons to TBDP. Our method again generalized to all test terrains, while TBDP did not. This indicates that the task required nontrivial generalization (despite the mild slopes), and that GPS was able to learn generalizable rules to main-tain speed and balance. As shown in the supplemental video, the learned gait also retained the smooth, life-like appearance of the human demonstration.
 Policy gradient methods often require on-policy sam-ples at each gradient step, do not admit off-policy samples, and cannot use line searches or higher order optimization methods such as LBFGS, which makes them difficult to use with complex policy classes (Pe-ters &amp; Schaal, 2008). Our approach follows prior meth-ods that use importance sampling to address these challenges (Peshkin &amp; Shelton, 2002; Kober &amp; Peters, 2009; Tang &amp; Abbeel, 2010). While these methods re-cycle samples from previous policies, we also introduce guiding samples, which dramatically speed up learning and help avoid poor local optima. We also regular-ize the importance sampling estimator, which prevents the optimization from assigning low probabilities to all samples. The regularizer controls how far the policy deviates from the samples, serving a similar function to the natural gradient, which bounds the information loss at each iteration (Peters &amp; Schaal, 2008). Unlike Tang and Abbeel X  X  ESS constraint (2010), our regular-izer does not penalize reliance on a few samples, but does avoid policies that assign a low probability to all samples. Our evaluation shows that the regularizer can be crucial for learning effective policies. Since the guiding distributions point out high reward regions to the policy search, we can also consider prior methods that explore high reward regions by using restart distributions for the initial state (Kakade &amp; Langford, 2002; Bagnell et al., 2003). Our restart dis-tribution variant is similar to Kakade and Langford with  X  = 1. In our evaluation, this approach suffers from the high variance of the restart distribution, and is outperformed significantly by GPS.
 We also compare to a nonparametric trajectory-based dynamic programming method. While several TBDP methods have been proposed (Atkeson &amp; Morimoto, 2002; Atkeson &amp; Stephens, 2008), we used a sim-ple nearest-neighbor variant with a single trajectory, which is suitable for episodic tasks with a single ini-tial state. Unlike GPS, TBDP cannot learn arbitrary parametric policies, and in our experiments, GPS ex-hibited better generalization on rough terrain. The guiding distributions can be built from expert demonstrations. Many prior methods use expert ex-amples to aid learning. Imitation methods such as DAGGER directly mimic the expert (Ross et al., 2011), while our approach maximizes the actual return of the policy, making it less vulnerable to suboptimal experts. DAGGER fails to make progress on our tasks, since it assumes that the DDP actions are optimal in all states, while they are only actually valid near the example. Additional DDP optimization from every visited state could produce better actions, but would be quadratic in the trajectory length, and would still not guarantee optimal actions, since the local DDP method cannot recover from all failures.
 Other previous methods follow the examples directly (Abbeel et al., 2006), or use the examples for super-vised initialization of special policy classes (Ijspeert et al., 2002). The former methods usually produce nonstationary feedback policies, which have limited ca-pacity to generalize. In the latter approach, the policy class must be chosen carefully, as supervised learning does not guarantee that the policy will reproduce the examples: a small mistake early on could cause dras-tic deviations. Since our approach incorporates the guiding samples into the policy search, it does not rely on supervised learning to learn the examples, and can therefore use flexible, general-purpose policy classes. We presented a guided policy search algorithm that can learn complex policies with hundreds of parame-ters by incorporating guiding samples into the policy search. These samples are drawn from a distribution built around a DDP solution, which can be initialized from demonstrations. We evaluated our method using general-purpose neural networks on a range of chal-lenging locomotion tasks, and showed that the learned policies generalize to new environments.
 While our policy search is model-free, it is guided by a model-based DDP algorithm. A promising avenue for future work is to build the guiding distributions with model-free methods that either build trajectory follow-ing policies (Ijspeert et al., 2002) or perform stochastic trajectory optimization (Kalakrishnan et al., 2011). Our rough terrain results suggest that GPS can gen-eralize by learning basic locomotion principles such as balance. Further investigation of generalization is an exciting avenue for future work. Generalization could be improved by training on multiple environments, or by using larger neural networks with multiple layers or recurrent connections. It would be interesting to see whether such extensions could learn more general and portable concepts, such as obstacle avoidance, pertur-bation recoveries, or even higher-level navigation skills. We thank Emanuel Todorov, Tom Erez, and Yuval Tassa for providing the simulator used in our experi-ments. Sergey Levine was supported by NSF Graduate Research Fellowship DGE-0645962.
 Abbeel, P., Coates, A., Quigley, M., and Ng, A. An ap-plication of reinforcement learning to aerobatic he-licopter flight. In Advances in Neural Information Processing Systems (NIPS 19) , 2006.
 Atkeson, C. and Morimoto, J. Nonparametric rep-resentation of policies and value functions: A trajectory-based approach. In Advances in Neural Information Processing Systems (NIPS 15) , 2002. Atkeson, C. and Stephens, B. Random sampling of states in dynamic programming. IEEE Transactions on Systems, Man, and Cybernetics, Part B , 38(4): 924 X 929, 2008.
 Bagnell, A., Kakade, S., Ng, A., and Schneider, J. Pol-icy search by dynamic programming. In Advances in
Neural Information Processing Systems (NIPS 16) , 2003.
 Baxter, J., Bartlett, P., and Weaver, L. Experi-ments with infinite-horizon, policy-gradient estima-tion. Journal of Artificial Intelligence Research , 15: 351 X 381, 2001.
 Deisenroth, M. and Rasmussen, C. PILCO: a model-based and data-efficient approach to policy search.
In International Conference on Machine Learning (ICML) , 2011.
 Dvijotham, K. and Todorov, E. Inverse optimal con-trol with linearly-solvable MDPs. In International Conference on Machine Learning (ICML) , 2010.
 Ijspeert, A., Nakanishi, J., and Schaal, S. Move-ment imitation with nonlinear dynamical systems in humanoid robots. In International Conference on Robotics and Automation , 2002.
 Kakade, S. and Langford, J. Approximately opti-mal approximate reinforcement learning. In Inter-national Conference on Machine Learning (ICML) , 2002.
 Kalakrishnan, M., Chitta, S., Theodorou, E., Pastor,
P., and Schaal, S. STOMP: stochastic trajectory optimization for motion planning. In International Conference on Robotics and Automation , 2011.
 Kober, J. and Peters, J. Learning motor primitives for robotics. In International Conference on Robotics and Automation , 2009.
 Peshkin, L. and Shelton, C. Learning from scarce ex-perience. In International Conference on Machine Learning (ICML) , 2002.
 Peters, J. and Schaal, S. Reinforcement learning of motor skills with policy gradients. Neural Networks , 21(4):682 X 697, 2008.
 Ross, S. and Bagnell, A. Agnostic system identification for model-based reinforcement learning. In Inter-national Conference on Machine Learning (ICML) , 2012.
 Ross, S., Gordon, G., and Bagnell, A. A reduction of imitation learning and structured prediction to no-regret online learning. Journal of Machine Learning Research , 15:627 X 635, 2011.
 Sutton, R., McAllester, D., Singh, S., and Mansour, Y.
Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems (NIPS 11) , 1999. Tang, J. and Abbeel, P. On a connection between importance sampling and the likelihood ratio policy gradient. In Advances in Neural Information Pro-cessing Systems (NIPS 23) , 2010.
 Tassa, Y., Erez, T., and Todorov, E. Synthesis and stabilization of complex behaviors through online trajectory optimization. In IEEE/RSJ International Conference on Intelligent Robots and Systems , 2012. Tedrake, R., Zhang, T., and Seung, H. Stochastic pol-icy gradient reinforcement learning on a simple 3d biped. In IEEE/RSJ International Conference on Intelligent Robots and Systems , 2004.
 Todorov, E., Erez, T., and Tassa, Y. MuJoCo: A physics engine for model-based control. In IEEE/RSJ International Conference on Intelligent Robots and Systems , 2012.
 Whitman, E. and Atkeson, C. Control of a walking biped using a combination of simple policies. In 9th IEEE-RAS International Conference on Humanoid Robots , 2009.
 Yin, K., Loken, K., and van de Panne, M. SIMBICON: simple biped locomotion control. ACM Transactions Graphics , 26(3), 2007.
 Ziebart, B. Modeling purposeful adaptive behavior with the principle of maximum causal entropy . PhD the-
