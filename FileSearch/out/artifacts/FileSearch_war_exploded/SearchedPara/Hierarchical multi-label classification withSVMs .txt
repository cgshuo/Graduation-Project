
Department of Computer Engineering, Chulalongkorn University, Bangkok, Thailand
Department of Electrical and Computer Engineering, University of Miami, Coral Gables, FL, USA Hydro Informatics Division, Hydro and Argo Informatics Institute, Bangkok, Thailand 1. Introduction
In its baseline problem statement, the field of classifier induction seeks to develop mechanisms capable of assigning an example to one (and only one) out of a set of mutually independent classes. Some recent applications, though, have generalized this scenario in two significant aspects: (1) an example is allowed to belong to two or more classes at the same time, and (2) the classes are hierarchically organized. In this paper, we refer to this task by the acronym HMC (H ierarchical M ulti-label C lassification). Among the applications of HMC, perhaps the most typical are web repositories/digital libraries (e.g., Dmoz, Wikipedia, 1 Yahoo [26,27], LookSmart [10], EUROVOC [34], Reuters [24], OHSUMED [16]) and image recognition [40]. Our own work deals with gene-function prediction . Here, genes represent examples, each gene function is a different class, and the class-to-class relations are specified by a directed acyclic graph (DAG) such as the one in Fig. 1 (note that each node in a DAG can have more than one parent).
Due to the relative novelty of the field, it is to be expected that existing induction systems for HMC domains are still far from perfect, and leave ample space for further research. In particular, we focus here on the top-down approach that begins by inducing a classifier for each class at the highest level of the DAG, and then proceeds downward, always employing the higher-level classifiers when creating the training sets for lower-level classifiers.

The performance of existing systems is limited by three problems. First, misclassifications committed at higher levels of the class hierarchy tend to get propagated downwards, making it hard to induce for each class often means that the training data for classifier induction tend to be imbalanced (negative examples outnumbering positive examples). Third, since different classes are characterized by different sets of attributes, it is necessary to run attribute-selection techniques separately for each of them.
To test the hypothesis that explicit treatment of these aspects will improve classification performance, we developed our own induction system. The baseline technique we used for the induction of the binary classifiers is R-SVM [45], our earlier version of a Support Vector Machine (SVM) [19,44,48], tailored to domains with imbalanced classes. Throughout the text, we refer to our new system by the acronym HR -SVM, where H stands for h ierarchical). Having experimented with several benchmark domains, we observed that HR -SVM compared favorably with the best of the existing  X  X ompetitors, X  including the hierarchical version of the traditional SVM ( H -SVM) 2 [12,13] and another well-known approach, Clus-HMC [3,35,47].

In the course of the work, we have realized that HMC X  X  performance has to be evaluated along some-what different criteria than those used in classical machine learning. Let x be an example, and let C be the set of classes to which x belongs. A perfect classifier will label x with all classes from C ,never suggesting any class from outside C ; moreover, HMC usually requires that any x that has been labeled requirements in performance evaluation, we developed novel crieria based on precision , recall ,and F 1 that are commonly used in information retrieval.

The rest of the paper is organized as follows. Section 2 defines the HMC task, and Section 3 briefly surveys related work. Section 4 discusses diverse aspects of HMC performance and develops novel crite-ria reflecting these aspects. Our own induction system, HR -SVM, is detailed in Section 5. Experimental data are summarized in Section 6, and experimental results are reported in Section 7. 2. Hierarchical classification: A formal problem statement
A graph consists of a set of nodes, N , and a set of edges, E , where an edge is an ordered pair of nodes, ( N that N 1 = N a and N n = N c .Ina directed acyclic graph (DAG) the existence of a path, N a  X  N c , guarantees the non-existence of the opposite-direction path, N c  X  N a . A node without any child is called a leaf node, and a node without any parent is called a root node.

In the task addressed by this paper, we consider a set of class labels, C , whose mutual relations are specified by a class hierarchy, H , that has the form of a DAG in which each node represents one and only that C s is a specialization of C g ).

Let X X  X  p be a finite set of examples, each described by p numeric attributes. We assume that each x hierarchy). From these data, we want to induce a classifier to carry out the mapping  X : X X  2 C in a way that maximizes classification performance. Moreover, an example belonging to class C c is expected to belong also to all C c  X  X  ancestor classes, C a . This property is called  X  hierarchical constraint  X .
Two versions of this task exist. In the mandatory leaf-node problem (MLNP), only the leaf-node classes are used. By contrast, in the non-mandatory leaf node problem (NMLNP), an example can be labeled with any class from the given class hierarchy. Considering the class hierarchy from Fig. 2, MLNP allows also the other class labels (e.g., C1 or C2.2). Our own research focuses on the more general NMLNP. 3. Related work
Surveying existing solutions to the HMC problem, [39] distinguishes three fundamental strategies: (i) flat classification, (ii) the top-down approach (local classifiers), and (iii) the  X  X ig-bang X  approach (global classifiers). 3.1. Flat classification
This ignores the class hierarchy altogether, and deals only with the leaf-node classes (as if the problem were MLNP), whether by a single multi-label classifier or by a set of binary classifiers (a separate one for each leaf node). The advantage is that this makes it possible to rely on traditional machine-learning techniques such as neural networks, decision trees, or SVM, and indeed such attempts have been reported by several authors [18,33,49].
This approach, of course, only makes sense if the leaf-node class label is known for each example, non-terminal classes. This is not the case of the domains addressed in this paper, and we will therefore not go into any further details. 3.2. Top-down approach (local classifier)
This is the most common approach in HMC induction. In the simplest case, a separate (local) classifier is induced for each node in the DAG-specified class hierarchy, starting at the top levels, and then pro-ceeding downwards, creating in the process a whole hierarchy of classifiers. The advantage is the relative simplicity. On the other hand, the approach tends to suffer from  X  X rror propagation X : misclassifications of the higher-level classes are propagated to lower levels.
 Koller and Sahami [22] were perhaps the first to experiment with local classifiers, choosing Naive Bayes as the baseline learner used to induce each individual class. The authors experimented with tree-structured class hierarchies (no more than one parent for any node) that were limited to just two levels.
Fagni and Sebastiani [12,13] compared four different policies to generate the binary training data from which to induce the local classifiers: Sibling, ALL, BestGlobal, and BestLocal. They used tree-structured hierarchical versions of boosting and SVM, called TreeBoost and TreeSVM, respectively, achieving the best results by the use of the Sibling policy in which the negative training examples of the i -th node are all positive examples of its sibling nodes in the hierarchy. In our paper, we refer to TreeSVM with the Sibling policy as  X  H -SVM X  because it can be regarded as a hierarchical algorithm that uses SVM.
Sun and Lim [41] applied this strategy to text classification where the class hierarchy was a plain tree structure. For each class, they induced two SVMs: a local classifier and a subtree classifier. An example passed to c i  X  X  subclassifiers. Nguyen et al. [28] extended the approach to domains with DAG-structured class hierarchies by transforming the DAG hierarchy into a set of tree hierarchies. Experimental results indicated high classification performance, but also high computational costs.

Seeking to further improve the performance, Secker et al. [36] used several different induction algo-rithms for each node of the hierarchy: Naive Bayes, SMO, 3-NN, etc. For each node, they trained ten classifiers, and then selected the one with the best classification results. This improved classification accuracy, but also further increased computational costs.

Addressing the problem of the very high number of classes in the hierarchy, Bi and Kwok [2] applied the kernel dependency estimation (KDE) to reduce the number of the classes during the training process. In particular, they proposed an algorithm called  X  X ondensing Sort and Selection Algorithm (CSSA) X  for the tree-structured hierarchies, and then extended it to the DAG-structured hierarchies. They did not report induction time or the amount of class-number reduction.

Recently, Alaydie et al. [1] proposed a framework called HiBLADE (Hierarchical multi-label Boost-ing with Label Dependency), applied to tree-structured hierarchies. The baseline classifier for each class is a boosting algorithm, such as ADABOOST, where the model for each boosting iteration is updated by a method utilizing Baysian correlation. 3.3. The  X  X ig-bang X  approach (global classifier)
Instead of inducing a separate binary classifier for each node, some authors prefer to induce one big (global) classifier for the entire class hierarchy. In this way, mutual interdependencies of the classes are more easily taken into account, and the global classifier is sometimes smaller than the sum of the local classifiers.

Clare and King [6] developed a hierarchical extension to the decision-tree generator C4.5 [31] and applied it to functional-genomics data (they called their system HC4.5). To give higher priority to more specific classes, they introduced a mechanism to weigh, accordingly, the entropy formula.
Another attempt to apply the decision-tree paradigm to HMC domains was reported by Blockeel et al. [3] whose Clus-HMC is a hierarchical version of the earlier  X  X redictive clustering tree X  (PCT) [4]. Ven et al. [47] improved Clus-HMC so that it could be used in DAG-specified class hierarchies. An ensemble version of the algorithm, Clus-HMC-ENS, was proposed by Schietgat et al. [35]. Unfortunately, although the ensemble concept does improve classification performance, its computational costs are much higher than those of the original Clus-HMC.

Pandey et al. [29] proposed a global-approach hierarchical framework based on the k -nearest neighbor classifier ( k -NN). There are many improvements in the system. First, the distance function is Lin X  X  semantic similarity measure. Second, the prediction function of the i -th class incorporates the inter-relationship score of the i -th class to other classes in the hierarchy. Finally, the mechanism to filter insignificant class inter-relationships was suggested. 4. Performance evaluation
How to evaluate performance in HMC is not an easy question, and the research community has not reached a consensus. In this section, we attempt to improve the situation by developing evaluation criteria that we believe are sufficiently objective and robust. 4.1. Classical approach
We begin with the two-class case where each example is either positive and negative. Classical ma-chine learning literature evaluated these classifiers by error-rate estimates obtained by the comparison of testing examples X  known class labels with those recommended by the classifier. Error rate, however, is inadequate in domains where one class significantly outnumbers the other [23]. For instance, if only 1% of the examples are positive, then a classifier that labels all examples as negative will achieve 99% accuracy, and yet it is virtually useless.

For this latter case, other criteria have been used; the most popular among them are precision and recall . Let us denote by TP the number of t rue p ositives, by FN the number of f alse n egatives, by FP the number of f alse p ositives, and by TN the number of t rue n egatives. Precision and recall are defined as follows:
In plain English, precision is the percentage of truly positive examples among those labeled as such by the classifier; recall is the percentage of positive examples that have been recognized as such ( X  X e-called X ) by the classifier. Which of the two is more important depends on the specific needs of the con-crete domain. Seeking to combine them in a single formula, [43] proposed F  X  , where the user-specified parameter,  X   X  [0 ,  X  ) , quantifies each component X  X  relative importance: It would be easy to show that  X &gt; 1 apportions more weight to recall while  X &lt; 1 emphasizes precision . Moreover, F  X  converges to recall if  X   X  X  X  ,andto precision if  X  =0 . If we do not want to give more weight to either of them, we use the neutral  X  =1 :
All this, however, applies only to domains where each example is labeled with one and only one class. For multi-label domains, [51] proposed two methods to average the above metrics over multiple classes: (1) macro-averaging ,where precision and recall are first computed separately for each class and then averaged; and (2) micro-averaging ,where precision and recall are obtained by summing over all individual decisions. Which of the two approaches is better depends on the concrete application. equal weight to each class. The formulas are summarized in Table 1 where Pr j , Re j ,and F 1 ,j stand for precision , recall ,and F 1 for the j -th class (from l classes). 4.2. Hierarchical classification
In domains with hierarchically organized classes, the above-defined classical metrics do not suf-fice. [7,41] surveyed several alternatives, such as distance-based, semantics-based, and hierarchy-based measures.

Among these, we prefer the metrics proposed by Kiritchenko et al. [21] as extended versions of, again, also to all ancestors of this class in the class hierarchy. The metrics for the i -th example are summarized in Table 2, where P i and T i represent sets of predicted and true classes respectively, and combination of hPr i and hRe i .
  X  P
Suppose we want to evaluate the performance measured on a data set with n examples labeled with l hierarchically organized classes. If we combine the performance of all examples by micro-averaging (Table 3), the value is biased towards examples with longer paths. To see why, consider the following be organized according to the hierarchy from Fig. 2. Suppose there are two classifiers,  X  1 and  X  2 , about the same, hPr  X  = hRe  X  = hF 1  X  = 1 2 .

To reasonably evaluate the classifiers in the previous example, we propose to apply macro-averaging to merge hierarchical measures of all examples as shown in Table 4. This averaging method then evalu-ates the performance of  X  2 higher than that of  X  1 .For  X  1 , hPr M = hRe M = hF 1 M = 1 3 , and, for  X  2 , 4.3. A more advanced approach
To see the limitation of the above criteria, consider the domain from Table 5 and the class hierarchy by the classifier.

This information is expressed in the matrix form in Table 6 (top) for the true classes, and in Table 6 (bottom) for the classes predicted by the classifier. Note that each row represents an example, and each column represents a class. The value of a given field is  X 1 X  if the example belongs to the class and  X 0 X  if it does not.

The hierarchical criterion from Section 4.2 evaluates the performance separately across each row of the matrix, and then averages the results. By contrast, the multi-label criterion from Section 4.1 evaluates the performance separately for each column , and then averages the results. We want to combine the two approaches.

Incidently, such combination was encouraged by the organizers of a recent competition to develop the best system on  X  X arge Scale Hierarchical Text Classification (LSHTC2) X . 4 In their notation, the multi-label criteria are referred to as  X  X  abel-b ased (Ma cro) X  ( LbMa ), 5 so that LbPr , LbRe ,and LbF 1 refer are denoted by EbPr , EbRe ,and EbF 1 , respectively.

Whether to prefer example-based criteria or label-based ones may be a matter of some dispute, but a good classifier should satisfy both. In line with this argument, we propose a simple way to accom-plish just that. Denoting the criterion by the acronym ELb (E xample-L abel-b ased), we define it by the following formula: where Eb is an example-based metric ( precision , recall ,or F 1 )and Lb is a class-label-based metric. For instance, ELbPr is our equivalent of precision , ELbRe is our equivalent of recall ,and ELb F 1 is our equivalent of F 1 .

Finally, let us remark that, in some domains, the closer a class is to the root of the class hierarchy, the more important it is deemed. In the work reported here, this factor is ignored. 5. HR-SVM : Induction of class hierarchies
Let us now proceed to the description of HR -SVM, our induction system designed for the needs of multi-label domains with classes organized in DAG-specified class hierarchies. In all illustrative exam-ples below, we will assume that the class hierarchy is the DAG from Fig. 2 that we reprint here for the reader X  X  convenience as Fig. 3. Each example can be labeled with leaf nodes as well as with the internal nodes of the DAG. Following the top-down approach, HR -SVM uses a  X  X aseline learner X  to induce a local classifier for each non-root node in the class hierarchy.

Care is taken to follow the hierarchical constraint . For instance, if classifier C 2 (associated with node  X 1 X  X nFig.3)labels x as negative, then x has to be labeled as negative also by the classifiers corresponding to the training sets from which the  X  X ode X  classifiers are induced, and it attempts to deal with the fact that errors committed by higher-level classifiers are propagated down the hierarchy.

As indicated in Fig. 4, HR -SVM consists of four modules; three of them for data pre-processing and the last for induction from imbalanced data. 5.1. Exclusive-Parent Training Policy (EPT)
For individual-node class induction, the first step is the generation of the corresponding (binary) train-ing set. Several methods have been proposed in the literature so far [11 X 13,39]. The one we use in HR -SVM is referred to as EPT (E xclusive P arent T raining Policy). 6 Let us now describe it in detail as follows.

Let Tr be the set of all training examples, let Tr ( C i ) denote the set of training examples used for the of the parents of C i . Finally,  X  \  X  is the set exclusion operator.
 HR -SVM X  X  way of choosing the training examples for the induction of C i is defined as follows: HR -SVM thus includes in this training set all positive examples of C i  X  X  parent class(es).
Let us illustrate the process by two examples.  X  Example 1: A one-parent node (e.g., C 2 . 2 ):  X  Example 2: A multiple-parents node (e.g., C 2 . 1 ):
The advantages of EPT are best illustrated by the comparison with another policy that has been used in the past, namely EAT (E xclusive A ll T raining Policy) [12,13] 7 where each node classifier is trained represents | C 0 | = | Tr | = 1000 examples, we have | C 2 | = 100 examples, | C 2 . 2 | = 50 examples, and | C 2 . 2 . 2 | = 10 examples. The sizes of the training sets generated by EPT and EAT, respectively, are given in Table 7. The reader can see that, at the lower-level classes, EPT generates smaller and more balanced training sets than EAT. For instance, when inducing C 2 . 2 , EPT creates a training set of size 100, whereas EAT uses 1000 examples.
 classifier, C 2 , has the responsibility to remove those C 1  X  X  examples. 5.2. Local Feature Selection (LFS)
In our domains, the examples are often described by thousands of attributes, which can lead not only to prohibitive induction costs, but also to performance degradation if many of the attributes are irrele-vant. Importantly, the relevance of the individual attributes can vary from class to class. Many scientists have studied attribute-selection techniques (see, e.g., [15]). Choosing from these, we need one that is computationally efficient, and we need to apply it separately to each class.

In our previous work [9,46], we made a good experience with ordering the attributes by their gain attributes is fixed and equal in every class. Experiments indicated that the best performance of multi-label classifiers was in our domains obtained when only the top 25% of attributes are retained. However, this strategy cannot be directly applied to the HMC domain from that preliminary experiment. Moreover, in terms of computational cost, the number of classes in the HMC domain can be much higher than in the multi-label domain, so it is more efficient to allow that the number of attributes be different for each class node.

In HR -SVM, we improved this (rather simplistic) approach by choosing those attributes that satisfy the following two conditions: the minimum accumulated gain ratio (or G % of total gain ratio) and the minimum number of attributes (or P % of the total number of attributes).

For illustration, suppose the user has set G = 95% and P = 25%; and let the gain ratios of a given represent more than the required 25% of the total 10 attributes. 5.3. False-Positive Correction (FPC)
HR -SVM X  X  next module seeks to correct the false positives ( FP )  X  negative examples that are incor-path errors .
 Let us illustrate. The EPT policy ensures that classifier C 2 . 2 is induced from examples belonging to errors from C 2 negatively affect the performance of C 2  X  X  subclasses.
 HR -SVM addresses this issue by our False-Positive Correction strategy (FPC). The idea is to add to C  X  X  negative training examples, Tr  X  ( C i ) ,alsoasetof FP examples at C i  X  X  superclass(es) (denoted in Note that FPC cannot be applied to classifiers at the top level; the FPC process begins after the SVM and the results are used to identify the FP examples. Finally, a subset of these examples is added to the set of negative training examples at C i , Tr  X  ( C i ) .

For instance, the potential errors propagated from C 2 . 2  X  X  parents, FP (  X  ( C 2 . 2)) = FP ( C 2) ,are FPC is determined as FP (  X  ( C 2 . 1)) = FP ( C 1 . 2) FP (2) .

We expect that the FPC strategy can potentially decrease the number of false positives, thus improving precision as well as F 1 . 5.4. R-SVM
The reader will recall that our system induces a separate binary classifier for each class. To induce the class is represented by only a small subset of the examples, which means that the corresponding training set is imbalanced. Traditional induction algorithms are in similar situations known to be biased towards the majority class, which results in many false negatives ( FN ).

In HMC-domains, the issue becomes even more serious due to the phenomenon called blocking :for be recognized as belonging to classes C 2 . 2 and C 2 . 2 . 2 .In HR -SVM, we mitigate the problem by using a mechanism borrowed from our earlier work [45]. Let us briefly summarize the essence.

SVM induces a hyperplane, h ( x )= w  X  x + b =0 , whose orientation is determined by w and offset by b . The expression can be used to calculate for each example, x , its  X  X VM score X :
In domains where one class outnumbers the other, SVM X  X  bias toward low error rate may result in high precision ,butverylow recall ( F 1 is then low too). This is rectified by threshold adjustment [5,14, 17,25,30,37,42,50], a process that translates the hyperplane (by the modification of b without changing w highest value of a user-defined performance criterion, perf (e.g., the F 1 metric), over the set of training data mapped to SVM scores, L = { ( s 1 ,C 1 ) ,..., ( s n ,C n ) } :
R -SVM is a threshold-adjustment algorithm shown by [45] to compare favorably with some earlier attempts, such as SVM F 1 [5], SVM CV [5], ScutFBR [24], and BetaGamma [30,37]. The essence is out-lined in Fig. 6. After inducing the initial SVM model, the first task is to identify a set of candidate thresholds,  X  . To maintain reasonable computational costs, the system orders the examples by their scores (Eq. (7)) and then defines candidate thresholds as the middle points between those pairs of neigh-boring examples that differ in their class labels:  X  opt = {  X  1 ,..., X  K } .

The best threshold from the candidate thresholds is found as follows. First, M auxiliary training 5.5. Complexity analysis hierarchical domains. Since HR -SVM uses a threshold-adjusted SVM ( R -SVM) as its baseline classifier, its complexity can be derived using the same kind of analysis.

Let M be the number of classes, let N be the number of training examples, let V be the number of attributes, and let L v be the average number of non-zero attribu tes. In multi-label ( non-hierarchical) classification, the training time of the traditional SVM is O ( MN c ) (where c  X  1 . 2  X  1 . 5 is a domain-specific constant), and its testing time is O ( ML v ) .
 The training time of R -SVM is given by Eq. (9), where c 1 and c 2 are constant times for Box 2.1 and Box 2.2 from Fig. 6, respectively. Since the process of threshold adjustment is applied after the model induction, the first term in the equation is the SVM induction time, and the second term is the evaluation time of the SVM model on training data.
 For the hierarchical classification system, the total complexity of the top-down approach, including HR -SVM, is given by Eq. (10), where h is the depth of the hierarchy, b is the number of branches at the level, j = { 1 ,...,m i } is an index for the class at the i -th level, n ij is the number of local training examples, N i is the total number of training examples at the i -th level, N 0 N i ,and  X  ij is defined as 6. Experimental data
We experimented with several real-world databases from the field of functional genomics available from the DTAI webpage 9 [35]. The task is to predict gene functions of three organisms: Saccharomyces cerevisiae (S. cerevisiae), Arabidopsis thaliana (A. thaliana), and Mus musculus (M. musculus). In this paper, we worked with 8 data sets annotated by the functional hierarchy in Gene Ontology (GO) whose structure forms a DAG. Each data set is described by different aspects (attributes) of the genes that may originate at diverse sources. The characteristics of the experimental data sets are summarized in Table 8.
Since the data contained nominal attributes, and many values were missing, some pre-processing was necessary. 1. Data cleaning . Assuming that rare classes cannot be reliably induced, we ignored all classes rep-2. Missing-value imputation . In the case of nominal attributes, we replaced a missing value with the 3. Nominal-to-numerical conversion . Some attributes were nominal, acquiring one out of m different 4. Attribute transformation . Some attributes were constrained to very small ranges  X  for instance, the 5. Normalizing the attribute values . We normalized the values of numeric attributes to the interval
Each domain provided by the DTAI website consists of 3 files that were originally intended for train-mance comparisons, we merged these three files into one, and then used 5-fold cross-validation. 7. Experiments
HR -SVM induces a hierarchical classifier by a mechanism built around the publicly available. svm-light 10 As for the kernel function, preliminary experiments indicated that the linear function gave better examples being described by thousands of attributes, the individual classes were easy to separate from each other.

The task of the experiments reported below is twofold. First, we want to show that our system out-performs earlier attempts. Since it is clearly not possible to compare HR -SVM with every single other system, we chose two systems that are regarded by the relevant literature as perhaps the most powerful: H -SVM and Clus-HMC. 11
Our second goal is to find out how each of HR -SVM X  X  modules (see Fig. 4) contributes to the overall performance. To this end, we added these modules one by one, obtaining the four variants listed in Table 9 where H -SVM is a hierarchical version of the traditional SVM [12,13], and HR -SVM-ALL is the complete system. The only exception was domain D 18 where only H -SVM-ALL could be experimented with  X  the high number of attributes made it impossible to use SVM-based systems without attribute selection (LFS).

All results are expressed in terms of the example-label based macro-averaging (ELbMa) version of the performance criteria from Section 4.3. 7.1. Comparing our system X  X  performance with that of its predecessors
A new technique is deemed useful if its performance compares favorably with that of other tech-niques. This is why we compare here HR -SVM-ALL with H -SVM (a hierarchical variant of SVM) and Clus-HMC (a global approach that relies on decision trees). Both are regarded as perhaps the best existing HMC-induction systems.
 reader can see that, in terms of F 1 , HR -SVM-ALL outperforms H -SVM in all domains, the results being particularly impressive in D 0 ; H -SVM achieved only F 1 = 0.0929, whereas HR -SVM-ALL scored F 1 = 0.4811 (an improvement of more than 400%). The new technique induced faster than H -SVM on all data sets except D 14 and D 17  X  in these, the efficiency of the employed attribute-selection technique was impaired by the additional time needed to process the FP data sets (FPC).

When compared with Clus-HMC along F 1 , HR -SVM-ALL was significantly better in six out of eight domains. The explanation for the unfavorable F 1 -results of HR -SVM-ALL in D 17 and D 18 is that both domains are described exclusively by categorical attributes with values from { 0 , 1 } . The input data of SVM must be numeric, while that of the decision tree can be either numeric or categorical. This means that this kind of data ( D 17 and D 18 ) is more suitable for decision trees than for SVM. Note that the F 1 -result of H -SVM and Clus-HMC in D 16 is not so conspicuous  X  the mechanisms proposed in HR -SVM-ALL can overcome the drawback of the traditional SVM on this kind of data. The induction time is comparable, HR -SVM-ALL being faster than Clus-HMC in D 0 , D 13 , D 16 ,and D 17 ,andslower in
D 14 , D 15 , D 18 ,and D 19 . 7.2. Contributions of HR-SVM X  X  modules In the next step, we wanted to find out how much each of the modules listed in Fig. 4 contributed to HR -SVM X  X  classification performance (see Table 11).

The induction times of the individual consecutive steps are summarized in Table 12. The following subsections discuss the classification-performance aspects. 7.2.1. The effect of R-SVM
In HR -SVM, SVM was replaced by R -SVM, with the intention to reduce SVM X  X  bias to the majority class (recall that our experimental domains were dominated by false negatives). The hypothesis that R -SVM can help, here is tested by the comparison of HR -SVM X  X  performance with that of H -SVM. The results in Fig. 8 show that HR -SVM exhibited better F 1 than H -SVM in all domains. In D 0 , HR -SVM X  X  F 1 improvement over the baseline SVM wa s more than 400% (fro m 0.0929 to 0.4693). As for the computational costs (Table 12), both systems needed about the same time. This indicates that the additional thresholding time in HR -SVM is very small (only at most 1% of the SVM induction time). Especially in D 19 , our system needed only 2.29 seconds to adjust the separation hyperplane in addition to the induction time of the traditional SVM, which was 23,334.07 seconds.

HR -SVM X  X  classification success was largely due to its significant improvement of recall (see Fig. 9), which was achieved at the cost of slightly reduced precision on some data sets, though the average of precision on all data sets still increased by 25%. It turns out that R -SVM properly adjusts the class-separation hyperplane, thus reducing the bias to the majority class, which in turn reduces the number of false negatives propagated down the class hierarchy. 7.2.2. The effect of FPC
Let us now compare HR -SVM-FPC with HR -SVM, thus finding out how the system X  X  performance benefits from the False-Positive Correction. Recall that false positives that occur at higher levels are propagated to lower levels in the class hierarchy. The total number of false positives can be high, which means lower precision and F 1 .
 The summary of the experimental results in Fig. 10 indicates that HR -SVM-FPC outperforms HR -SVM in almost all domains. In 5 out of 7 domains, the improvement is statistically significant. As seen in Fig. 11, this is due to FPC X  X  ability to increased precision (in all eight domains). This indicates that the number of FP -errors propagated throughout the system was indeed reduced.

Table 12 shows how much FPC increases the computational costs (on account of the extra false posi-tives added to the training data). Note that, in D 15 , FPC led to reduced induction time because the SVM model here converged faster in spite of the training data being larger (a phenomenon explained by [38]). 7.2.3. The effect of LFS
The results of the comparisons between HR -SVM-ALL and HR -SVM-FPC give us an idea of the contribution of the attribute-selection technique (LFS).

Recall that LFS relies on two user-defined parameters: (i) the minimum percentage of accumulated gain ratio and (ii) the minimum number of attributes. In the experiments reported below, the former threshold is set to 95% of the overall gain ratio, following the general threshold used in Principal Com-ponent Analysis (PCA) [20]. The latter threshold is set to 25% of the total number of attributes as suggested in [9,46].
Comparing to HR -SVM-FPC in terms of F 1 , Fig. 12 shows that HR -SVM-ALL won in D 0 , D 13 , indistinguishable. Figure 13 shows that the removal of less relevant attributes indeed improved precision on all data sets except for D 17 .

Table 12 shows that LFS reduced the training time in most domains, most remarkably in D 19 where the time was reduced by 56% while F 1 also improved by 2%. However, HR -SVM-ALL X  X  induction costs increased in D 15 and D 17 . This is caused by SVM X  X  inverse convergence [38]. LFS is especially useful in
D 18 where the number of attributes is so high that memory-resident algorithms cannot load the whole data set into the computer X  X  memory. 12
In conclusion, attribute selection seems beneficial, especially in our experimental data sets where ex-amples are described by great many attributes. LFS increases F 1 by improving precision ; it also reduces computational costs. 7.3. Performance at different hierarchical levels Let us now try to gain more insight by comparing the classification performance of HR -SVM-ALL, H -SVM, and Clus-HMC, at different levels of the class hierarchies.
 Note that the label-based metrics ( Lb ) are here better suited than the example-label based ones ( ELb ). The ELb -metrics evaluate the performance along the class paths ,suchas C 1  X  C 1 . 1  X  C 1 . 1 . 1 ,which does not make much sense when evaluating each class-level independently.

Three factors are known to affect an induced classifier X  X  performance: (i) the training set size, (ii) the number of positive examples, and (iii) the data distribution (e.g., the degree of imbalance). Hierarchical domains add one more factor: the  X  X ropagated error X   X  the accuracy of lower-level classifiers is related to that of the parent classifiers.

Table 13 summarizes the expected tendencies. Note that the accuracies of higher-level classifiers suf-fer from highly imbalanced training data, and the accuracies of lower-level classifiers suffer from the scarcity of positive training examples and from the propagated errors. As we go down the hierarchy, the impact of these factors increases and the performance of the lower-level classifiers declines.
Let us first take a closer look at one of the domains: D 0 . Figure 14 plots the classification performance at the different levels. We can see that HR -SVM-ALL outperforms H -SVM and Clus-HMC at all levels, and H -SVM always lags behind the other two. As seen in Table 14, the performance at the highest level is negatively affected by the highly imbalanced class representation. The percentage of positive examples at this level is only 0.07%.

Even more insight is gained from Table 15 which gives the number of classifiers that incorrectly label all examples as negative (which means that F 1 =0 ). Since the traditional SVM suffers from imbalanced class representation, most classifiers in H -SVM have F 1 =0 .However, R -SVM was designed in a way that improves its  X  X eaction X  to imbalanced classes, and this is why there are no classifiers with F 1 =0 in the case of HR -SVM-ALL. On the other hand, Clus-HMC fails to recognize examples of some classes (resulting in F 1 =0 ).
 In all other domains (apart from D 17 and D 18 ), the F 1 -results were quite similar to those shown in Fig. 15 for D 16 . As we proceed to lower levels, F 1 gradually decreases. This is caused by the decreasing numbers of training examples (Table 16) and by the errors propagated from higher levels (Table 17). F 1 is lower. Comparing the three hierarchical classification systems, we see that HR -SVM-ALL exhibits the best classification performance, whereas H -SVM is the worst.

In D 17 and D 18 , Clus-HMC showed the best F 1 -results at all levels due to their data characteristic as mentioned in Section 7.1.
Based on all these results, we conclude that HR -SVM-ALL handles the imbalanced training sets par-ticularly well at the upper levels of the class hierarchy where it clearly fares better than other systems. Our system also takes better care of the propagated errors at the lower levels, which (usually) prevents the creation of classifiers that fail to correctly predict any positive examples ( F 1 = 0). 8. Conclusion
The paper presented HR -SVM, a new top-down induction technique for multi-label classifiers in do-mains with hierarchically organized classes. In designing it, we followed the common strategy that induces a separate binary classifier for each node in the hierarchy, and employs higher-level classifiers when creating the training sets for the induction of lower-level classifiers. The paper described our sys-tem, and then reported experiments illustrating its performance as well as diverse aspects of its overall behavior.

Top-down approaches often suffer from two problems: imbalanced class representation and top-down error propagation. HR -SVM addresses them explicitly by the following four techniques: (i) exclusive-parent training sets (EPT), (ii) attribute-select ion module (LFS), (iii) a mechanism to correct false pos-itives (FPC), and (iv) the correction of the majority-class bias. The first three can be regarded as data pre-processing techniques that help generate smaller and not-so-imbalanced training sets,  X  X nriched X  by examples misclassified by parent classifiers. The task of the last module, R -SVM, is to induce unbiased SVM-hyperplanes.

Importantly, we argue that, in hierarchical classification, the usual metrics for performance evaluation label based macro-averaging, X  that uses the harmonic mean of macro-averaging performances in two dimensions: per example and per class .

We applied our system to eight real-world domains from the the field of gene-function prediction of three organisms: S. cerevisiae, A. thaliana, and M. musculus . These data are available at the DTAI website. In each domain, the data were annotated using their own functional hierarchy in Gene Ontology (GO), whose structure is a directed acyclic graph (DAG).

Experimenting with these domains, we observed that our system significantly outperformed alternative hierarchical-classification techniques such as H -SVM (a top-down hierarchical version of SVM) and Clus-HMC (a  X  X lobal X  approach based on decision trees). Especially in D 0 ,the F 1 -improvement of HR -SVM over H -SVM is about 400%, while the induction costs are much lower. The explanation is that the module EPT creates training sets that are less imbalanced, the module R -SVM reduces the number of false negatives (which resulted in better recall ), the module FPC decreases the number of false positives (which leads to higher precision ), and the module LFS removes irrelevant attributes (which saves a lot of induction time).
 References
