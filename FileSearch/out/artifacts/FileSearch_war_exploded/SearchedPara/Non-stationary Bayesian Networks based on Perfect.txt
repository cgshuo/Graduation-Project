 Non-stationary Dynamic Bayesian Networks (Non-stationary DBNs) are widely used to model the temporal changes of directed depen-dency structures from multivariate time series data. However, the existing change-points based non-stationary DBNs methods have several drawbacks including excessive computational cost, and low convergence speed. In this paper we proposed a novel non-stationary DBNs method. Our method is based on the perfect simulation model. We applied this approach for network structure inference from synthetic data and biological microarray gene expression data and compared it with other two state-of-the-art non-stationary DBNs methods. The experimental results demonstrated that our method outperformed two other state-of-the-art methods in both computa-tional cost and structure prediction accuracy. The further sensitivity analysis showed that once converged our model is robust to large parameter ranges, which reduces the uncertainty of the model be-havior.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms Dynamic Bayesian Networks, Markov Chain Monte Carlo, Perfect Simulation
Non-stationary Dynamic Bayesian Network methods are widely used to model the temporal changes of dependency structures from multivariate time series data [32, 10, 34, 17, 22, 5, 16, 11]. Com-paring to traditional DBNs modeling, non-stationary DBNs have advantages to capture the structural dynamics of networks in var-ious biological systems, such as Neural assemblies in response to  X  to whom correspondence should be addressed visual stimuli [34], morphogenesis in the organisms X  life cycle [32, 16], adaptive mammalian immune response against infection of virus [10], or circadian regulation dynamics of plants caused by dramatic changes of outside environment such as light intensity [10].

Several methods have been developed for constructing non-stationary models. For example, Robinson et al. proposed a discrete non-stationary DBNs method [32] using Reversible Jump Markov Chain Monte Carlo (RJMCMC) [8] to sample underlying changing net-work structures. Grzegorczy et al. proposed a non-homogeneous continuous Bayesian network method with a Gaussian mixture model based on the allocation sampler technique [27]. Grzegorczy et al. improved the convergence of their method using perfect sim-ulation modeling [6] and reduced the risk of overfitting and in-flated inference uncertainty [16] in their later work [11]. Both Robinson X  X  and Grzegorczyk X  X  methods perform change-point de-tection and we call them change-point based approaches. Song et al. [34] proposed a time-varying DBNs (TV-DBNs) method and used a kernel re-weighted l 1 -regularised auto-regressive ap-proach for learning the graph structures at each time step. Lebre et al. [22] proposed a more flexible auto-regressive time varying model called ARTIVA that allows gene-by-gene analysis. Hus-meier et al. [16] introduced inter-time segment information sharing schemes to address the over-flexibility issue in the ARTIVA ap-proach. Those three approaches are different from the change-point detection based approaches and fell into the category of structure learning of constantly varying network over time. In this paper, our work focuses on the change-point detection modeling for regula-tory network dynamics.

There are several limitations of the existing change-points based techniques. First, the mixture model used by Grzegorczy et al. [10, 11] assumed that the underlying network structures are invari-ant over time. Such an assumption is too rigid when changes of network structures are expected, for example, morphogenesis or embryogenesis [16]. Second, Grzegorczy X  X  method with the im-provement on convergence [11] mixed the structure sampling steps and the perfect simulation steps in the same RJMCMC procedure. The time complexity of each perfect simultion step is quadratic to the number of the observations [6]. This scheme brings extra computational costs on change-point simulation that are propor-tional to the number of sampling iterations even if genes are decom-posed into groups to alleviate the computational concern. Third, the RJMCMC sampling approach in Robinson X  X  work [32] converges slowly. For example, the results in [8] using RJMCMC did not con-verge as pointed out in the subsequent work in [9]. In addition, our experiments show that the structure prediction accuracy of Robin-son X  X  RJMCMC is low.

We posit that the key computational obstacle for efficient mod-eling of time series data with non-stationary DBNs methods is the interplay of change-point detection and structure inference for each identified time segment. To improve computational efficiency, we designed an algorithm called ReCursion Non-Stationary Dynamic Bayesian Networks ( RCnsDBNs ) to separate these two essential steps. Our method adopted Fearnhead X  X  perfect simulation model [6] for change-point detection. The perfect simulation model was originally developed for univariate time series data [6] and we mod-ified the algorithm to model our multi-variate time series data. In particular, we designed an iterative algorithm for the structure in-ference and change-point detection. Our method first used the point process [29] as the prior for the occurrences of change-points and directly simulated the change-points from the posterior distribu-tion. For each predicted segment, we then used a regular Markov Chain Monte Carlo (MCMC) method, a revised KNUT (Known Transition Number Unknown Transition Time) setting in Robin-son X  X  method [32]. Once the algorithm converges, we output the most likely change-points and a sequence of network structures corresponding to the separated segments.

There are several advantages for the novel non-stationary DBNs algorithm. First, by directly simulating the posterior distribution of transition time for graph structures, our method efficiently reduces the model space and improves the computational performance both on time and numbers of sampling iterations for covergence. Sec-ond, even if a negative binomial prior is adopted in our point pro-cess, the experiments showed that our experimental results are sta-ble within large parameter ranges, which reduces the uncertainty of the model behavior. Third, different from Grzegorczy X  X  method [11], our method only needs to simulate the change-points once for each round of our algorithm. It saves the computational time. Fourth, Our approach outperform Robinson X  X  RJMCMC approach on structural prediction accuracy. Even if our discrete model needs the discretization of the data, compared with Grzegorczyk X  X  con-tinuous approach, our method show ed the competitive performance for structure estimation.
The change-point detection problems have been extensively in-vestigated in time series models. Recent work could be found in: PCA-based singular-spectrum transformation models [26]; non-parametric online-style algorithm via direct density-ratio estima-tion [20]; two-phase linear regression model [23]; a hybrid algo-rithm that relies on particle filtering and Markov chain Monte Carlo [3]; the RJMCMC method [8]; The perfect simulation model based on product-partitio n model [6]; change-point detection by minimiz-ing a penalized contrast function [21]. These models are widely used in various applications, such as climate analysis [23], coal-mining disaster analysis [8, 6], well-log analysis [6], the analysis of abrupt economic agents X  behav iors [3], and asse t price volatility [21].

Researchers find that change-point modeling is a very promising way of dealing with the non-stationarity property [3]. Hence, the current non-stationary DBNs methods employed different change-point detection techniques to model the underlying change-point processes of network structures. Robinson et al. [32] applied RJM-CMC [8] and used a discrete model with the assumed multino-mial distributed data with the Dirichlet prior. Using the RJM-CMC technique, Lebre et al. [22] proposed a new time varying networks approach based on first-order auto-regression and Yao X  X  two-stage regime-SSM model [30]. Their method focuses on local structural changes and performs node-by-node analysis. Further, in order to address the structural overfitting problem in [22], Don-delinger et al. [5] and Husmeier et al. [16] introduces informa-tion sharing between segments into Lebre X  X  approach. Grzegorczy et al. [10, 11] applied the allocation sampler technique and in-troduced a continuous-valued DBNs method that approximates the non-stationary property with a Gaussian mixture model. Based on their work, Ickstadt et al. [17] further generalized this non-linear BGe mixture model into a broader framework of non-parametric Gaussian Bayesian networks. In this paper, we incorporate the per-fect simulation modeling into our dynamic bayesian framework and provide a computationally efficient non-stationary DBNs approach. We chose this change-point detection technique for the following reasons. First, perfect simulation is based on bayesian analysis and can be easily applied into our MCMC algorithm. Second, with an approximation in the recursion, the computational complexity of this method is approximately linear to the number of observations.
Fearnhead used the perfect simulation model to find change-points in the univariate time series data [6]. We adapted his method to the framework of our dynamic bayesian networks, and provided a non-stationary DBNs method to detect the change-points for net-work structures in multivariate time series data.
 We consider an observed time series data D = { y 1 ,  X  X  X  ,y spanning T time points, where each observation y i  X  R n :1  X  i  X  T is a n dimensional vector ( x 1 ,  X  X  X  ,x n ) . The time series data is subdivided to m segments D = { D 1 ,  X  X  X  ,D m } ,where m is unknown. We denote the change-points for these segments as L T =( l 0 ,l 1 ,...,l m  X  1 ,l m ) ,where l 0 =0 and l m =
We assume the change-points as a point process on positive in-tegers, which is characterized by a probability mass function g where t is the distance of two successive change-points. We choose the negative binomial distribution as the distribution for the dis-tance between two successive change-points and have g ( t p k (1  X  p ) t  X  k with the parameters k&gt; 0 ,p &gt; 0 and its corre-sponding accumulative distribution function G ( t )= t i =1 For the special case of the first change-point, we have g
Given the assumption of the independence between segments, we calculate the probability of a sequence of observations after one change-point l : Q ( t )= Pr ( y t : n | l = t  X  1) by using recursive function below: where 2  X  t  X  T ,and In Equation 1 and 2, P ( t, s ) is the simplified notation of P t  X  1) : 1  X  t  X  T,t  X  s  X  T , where the observations y t : the same segment between two change-points t  X  1 and s . Similarly, P (1 ,s ) is the simplified notation of P ( y 1: s | l =0):1  X  where the observations y 1: s are in the same segment between two change-points l 0 and s .

Further, based on Q ( t ) , we calculate the probability distribution of the first change-point below: where l 1 :1  X  l 1  X  T  X  1 is the first change point.

Given l i , we calculate the conditional probability P ( l l +1  X  l i +1  X  T  X  1 as the following: And the probability of no more change-point is given as: Finally, with the probability distribution of l 1 and conditional distri-bution of l i +1 given l i , we directly simulate the change-point sam-ples and compute the posterior probability distributions P and P ( m | T ) . Due to the limitation of the space, we omitted the mathematical derivation and proofs. More technical details are available on [6]. One of the key computations in the simulation procedure is to compute the probability P ( y t 1: t 2 | l t 1  X  T,t 1  X  t 2  X  T . By assuming the i.i.d. observations in a sin-gle segment and the conjugate priors  X  (  X  ) on the parameters  X  asso-ciated with each segment, Fearnhead X  X  work provides a closed form of solution for P ( y t 1: t 2 | l = t 1  X  1) = t 2 i = t In the analysis of the well-log data, he assumed the normally dis-tributed observations: y i  X  N (  X  i , X  2 ) with the fixed variance  X  and a normal prior for the mean  X  i . In the following we discuss our solutions both under the static and dynamic bayesian network frameworks.

Perfect simulation modeling in bayesian networks. Bayesian networks (BNs) are a special case of probabilistic graphic models. A static BN is defined by an acyclic directed graph G and a com-plete joint probability distribution of its nodes P ( X )= X n ) . The graph G : G = { X,E } contains a set of variables X = { X 1 ,...,X n } , and a set of directed edges E , defining the causal relations between variables. With a directed acyclic graph, the joint distribution of random variables X = { X 1 ,...,X decomposed as P ( X 1 ,...,X n )= i P ( X i |  X  i ) ,where  X  the parents of the node (variable) X i .

We assume that the observations inside one segment are inde-pendent. In each segment there is one graph G h :1  X  h  X  m that dominates the segment. We denote y t 1: t 2 as D  X  t 1: t P (  X 
G are the parameters associated with the data D t 1: t 2 correspond-ing to G .  X  ( X  G | G ) is the probability density function of
Under the assumption that the data are complete and multino-mially distributed with a Dirichlet prior on the parameters have the BDeu [13] solution to P ( y t 1: t 2 | l = t 1  X  1) r is the number of possible discrete values of x i . q i is the number of configurations of parents  X  i for the variable x i . N perparameters for Dirichlet distribution.  X  ijk isassumedtobeuni-formly distributed inside a segment and is set to  X  ijk = The equivalent sample size  X  is set to 1.
 a model space M for G . We use MCMC to simulate M .Given the collected sample size N M , we approximate the calculation of Equation 6 as follows: Perfect simulation modeling in dynamic bayesian networks. The topology of bayesian networks must be a directed acyclic graph and hence could not be used to model the case where two nodes may be dependent on each other. As an extension of BNs to model time series data, Dynamic Bayesian Networks (DBNs) lift the lim-itation of directed acyclic graph by incorporating temporal depen-dence in constructing bayesian networks. It is not straightforward to extend the solution in BNs to modeling DBNs mainly due to that neighboring observations are not independent given the model pa-rameters. Below we develop a heuristic and provide a solution for P (
We set the lag value  X  =1 and assume the segments are over-lapped. For each segment D h :1 &lt;h  X  m , the length of over-lapped area with the previous segments D 1 ,  X  X  X  ,D h  X  1 to the lag value  X  =1 .For D 1 , there are no previous segments and we add  X  =1 additional y 1 s at the beginning of D 1 .Given the transition time points L T , each segment of observations D { y { y and have D h = { D  X  h ,D  X  c h } . We take a heuristic to assume that D h is independent of G h ,andthat P 1. Similarly we denote y t 1: t 2 as D  X  t 1: t 2 , y t 1  X  D G h :1  X  h  X  m that dominates the segment.
 With the assumption that ,wehaveTheorem1 With Theorem 1, we have  X 
G are the parameters associated with the data D t 1: t 2 correspond-ing to G .  X  ( X  G | G ) is the probability density function of The assumption of the multinomially distributed data with the Dirichlet prior leads to the same solution (BDeu metric) of the closed form expression of the marginal likelihood P ( D t t 1  X  1 ,G ) in Equation 7.

Similarly as bayesian networks, we use MCMC to simulate M for { G } . Our experimental study shows that our methods in both BNs and DBNs versions output the similar results for the distribu-tions of change-points.
Given an observed time series data D , the structure learning problem of DBNs is equal to maximizing the posterior probabil-ity of the network structure G .

By the Bayes X  rule, the posterior probability is expressed as the following: Given a non-stationary time series data, we need to find a sequence of network structures G T =( G 1 ,...,G m ) , m segments, and a transition vector L T , the posterior probability in Equation 11 is replaced by Equation 12: P ( D | T ) is treated as a constant, and then In the following discussion, we specify the formula for calculating each component of Equation 13.

We are using the same assumption in [32] that the networks change smoothly over time. We use the exponential priors on the change of network structures. We transform the form of the se-quence of graph structures G T : G T =( G 1 ,...,G m ) into G G
T =( G 1 , G 1 ,..., G m  X  1 ) ,where G h :1  X  h  X  m  X  1 is the change of edges between G h and G h +1 . We calculate P ( G T | m, T ) as follows.
 ,where S : S = m  X  1 h =1 s h ,and s h is the number of edge change between G h +1 and G h . We have no prior knowledge on P ( and see the uniform distribution as the prior.

We assume that the data are complete and multinomially dis-tributed with a Dirichlet prior on the parameters. We calculate P ( D h | G h ,T ) of each segment by following Equation 7: We denote I h as the segment h ,  X  G h as the parameters corre-sponding to G h , r i as the number of possible values of x q ih as the number of configurations of parents  X  i in I h .Welet  X  ijk and  X  ij to be the hyperparameters for Dirichlet distributions applied in I h .  X  ijk is uniformly distributed inside I h  X  ijk =  X / ( r i q ih ) . We set the equivalent sample size  X  equal to 1. We denote N ijk ( I h ) as the times that x i had value k in I N
T HEOREM 2. With Theorem 1 and the Markov property, the marginal likelihood P ( D | G T ,m,T ) is expressed as below:
With Theorem 2 and Equation 15, we get the extended BDeu metric: We use the perfect simulation modeling to calculate the posterior probability distributions of P ( L T | m, T ) and P ( m | the most likely m , fix the number of segments, and have P 1 . We use the sampling method to collect { G T } and will discuss the details in the subsequent section.
Considering the fact that the gene expression data are usually sparse, which makes the posterior probability over structures to be diffuse [15], we choose sampling approaches rather than heuristic methods to search structural models, where a group of most likely structures could explain data better than a single one. In addition, the sampling methods also have the advantage to approximate the model space M for change-points simulation. We select MCMC as our sampling approach to collect G T samples and compute the posterior probabilities of edges { e s,i,j | 1  X  s  X  m, 1  X  in G T . We use every single G T sample to calculate the marginal y 1: t 2 based on Equation 17. With a simulated sample space by MCMC, we get P ( D t 1: t 2 | l = t 1  X  1) based on Equation 8 and further calculate the whole conditional probability distribution of change-points.
 We design our algorithm based on the following considerations. First, we choose the heuristic search instead of the MCMC simu-lation to initialize G with only a single segment at the beginning of the algorithm. With the non-stationary nature, the data consists of multiple segments. And the possible model space and its dis-tribution in each segment are different. In this case, MCMC may not provide a good approximation of M and is computationally ex-pensive. Hence, we use the heuristic search to initializes a single G to do the perfect simulation and such change does not affect the prediction performance. In general, we take much smaller number of heuristic steps compared with MCMC, and the number of steps is proportional to the size of nodes. The detailed configurations of heuristic steps could be found in Section 4.
 Second, we use KNUT move set instead of the KNKT (Known Transition Number Known Transition Time) move set [32] contain-ing six move types, MT1-MT6 . Induced by the limitation of the initialization of a single G at the beginning, the true distributions of P ( m ) and P ( L T ) are doubtful after the first round of perfect simu-lation. Simply using fixed change-points will distort the simulated model space. By bringing the move to shift the change-points into the move set, we allow MCMC not only to converge for L T but also to provide a model space approximately at every time point. With this method, we improve the quality of M and have our al-gorithm converged. The procedure for our method is shown in the Algorithm as follows.
 RCnsDBNs Algorithm Input: Time series Data D , parameters p , k and  X  s Output: P ( L T ) , P ( m ) ,and P ( { e s,i,j } ) Begin Use heuristic search and select a single graph G .
 Run perfect simulation to sample change-points.
 Calculate the distributions P ( m ) and P ( L T ) .

Select the most likely m and initialize G T with G. while P ( m ) , P ( L T ) and P ( { e s,i,j } ) not converged do end while
End
We performed all the experiments on Intel Xeon 3.2 Ghz EM64T processors with 4 GB memory. We implemented our method RC-nsDBNs in Java.

We compare three approaches: 1 ) our approach (RCnsDBNs), 2) reversible jump Markov chain Monte Carlo Non-Stationary Dy-namic Bayesian Networks (RJnsDBNs) [32], 3) Allocation Sam-pler Non-Stationary Dynamic Bayesian Networks (ASnsDBNs) [10, 11]. For RJnsDBNs, we use the default setting of unknown num-bers and times of transitions (UNUT) in all of the data sets. RJns-DBNs is implemented in Java. ASnsDBNs is implemented in Mat-lab. In addition, we show the results of our method in BNs ver-sion denoted as RCnsBNs ( ReCursion Non-Stationary Bayesian Networks). Both two versions of our method find very similar results on the posterior distributions of change-points. We grid-search the parameters for RCnsDBNs and RJnsDBNs for the best performance on change-point and structure estimation. For ASns-DBNs, we choose K max =10 for all experiments that we believe to satisfy the number of different components of the mixture vector in various data sets.

Our experimental study is based on three data sets: (i) Synthetic data set, (ii) Bone Marrow-derived Macrophages gene expression time series data (Macrophages data s et), and (iii) Circadian regula-tion in Arabidopsis Thaliana gene expression time series data (Ara-bidopsis data set ). We evaluate three methods from two aspects: computational performance on convergence and structure predic-tion accuracy.

Convergence Rate and Computational Time. ASnsDBNs with perfect simulation modeling (ASnsDBNs-PSM) [11] improves ASns-DBNs [10] on convergence. It selects parameters to give best ap-proximation to the outputs of ASnsDBNs. Hence, we choose ASns-DBNs -PSM for computational performance comparison. We fol-low Grzegorczyk X  X  work in [11] and evaluate ASnsDBNs-PSM and our method with the proportion of edges denoted by  X  for which potential scale reduction factors (PSRFs) [7] lies below the pre-defined threshold. PSRFs=1 shows perfect convergence and that PSRFs &lt; 1.1 is seen as the sufficient condition for convergence [11, 7]. 0  X   X   X  1 and higher  X  values indicate better convergence.
RJnsDBNs does not output graph samples. We use the varia-tion of edge posterior probabilities (VEPP) to measure the con-ously sampling in MCMC, and P ( e I s,i,j ) is the posterior probabil-ity of an edge e i,j in the graph G s that dominates the s th segment computed from I iterations. Once MCMC converges, | P ( e I P ( e I s,i,j ) | X  0 with I  X  +  X  . Hence, VEPP values close to 0 in-dicate that a MCMC chain converges to a stationary distribution. We use a pre-defined threshold  X  .When VEPP &lt; X  , we decide that MCMC converges and calculate the computational time.
Structure Prediction Accuracy. To compare the inferred struc-ture results from different data sets, we follow the evaluation method introduced in [15, 35, 10]. For the synthetic data set, we compare the inferred network structures with the true networks. For each real data set, we first collect gold standard reference networks as the ground truth. For the Macrophages data set, such reference net-works are available in [19, 31, 10]. For the Arabidopsis data set, we collect the network information from [24, 33, 4, 28]. In case where we have ground truth network structure (the Bone Marrow data set and Arabidopsis data set), we use the area under receiver operating characteristic curve (AUROC) values to evaluate the performance. 5 2 4 In addition, for each data set, we show the posterior distribution of the number of segments and the locations of change-points. Be-fore we discuss the details of experimental results, we present the characteristics of our data set first below.
We evaluate our method RCnsDBNs on a synthetic data and two gene expression data sets used in [32, 10]. We preprocess the orig-inal gene expression data sets by following Zhao X  X  work [36]. We set the values of a missed time point with the mean of its two neigh-values are at the beginning or end, simply set the same value as its neighbor; i.e., X i,t = X i,t +1 if t =1 or X i,t = X i,t In the following, we show the details of each data set.
Synthetic Data. We created a synthetic time series data with 80 time points and binary valued observations. It was generated by a sequence of 5 node networks with 3-4 edge changes between successive segments. The change-points of the graph structures happened at times 20, 40, 60. We showed the true networks in Figure 1.
 Bone Marrow-derived Macrophages Gene Expression Data.
 We use the Macrophage data sets previously investigated in [10]. feron regulatory factors (IRFs), proteins central to the mammalian innate immunity [14, 31]. The Macrophage data sets were sam-pled from different conditions: (I) Infection with Cytomegalovirus (CMV), (II) Treatment with Interferon Gamma ( IFN  X  ), and (III) Infection with Cytomegalovirus after pretreatment with IFN + IFN  X  ). Each data set has 25 time points collected with the in-terval 30 minutes. We follow Grzegorczyk X  X  work [10] and use Irf 2  X  Irf 1  X  Irf 3 as the gold standard. We assume that the network is invariant over time.
 Arabidopsis Thaliana Circadian Regulation Gene Expression Data. We use the Arabidopsis Thaliana Circadian data investigated in [10]. The data sets consist of 9 genes, LHY , CCA1 , TOC1 , ELF4 , ELF3 , GI , PRR9 , PRR5 ,and PRR3 . The group of genes create transcriptional feedback loops an d are critical to understand the in-ternal clock-signalling network of plant. The Arabidopsis data are sampled from two light-dark conditions: (I) 10h:10h light/dark cy-cle and (II) 14h:14h light/dark cyc le. Each data set contains 13 time points collected with the interval of 2 hours. We build a gold standard network based on the biological literatures [24, 33, 4, 28, 12, 25]. In this network, CCA1 and LHY proteins directly bind to the promoter of TOC1 to represses the expression of TOC1. The pseudo-response regulators PRR5 and PRR9 are activated by CCA1 and LHY and repress CCA1 and LHY subsequently. G1 im-proves the expression of TOC1. ELF4 is repressed by CCA1. For a detailed referred graph figure, please refer to our previous work [18].
We first compared the computational performance between our method RCnsDBNs and ASnsDBNs-PSM. The curves of fraction of edges with PSRFs &lt; 1.04 on two methods for Thaliana T20 data is showed in Figure 2 and the VEPP curves in Figure 3. We cal-culated the PSRFs and VEPP scores from 10 independent MCMC chains. We found that RCnsDBNs and ASnsDBNs-PSM have the similar convergence rate measured in terms of MCMC sampling iterations. However, for 250,000 iterations, it takes ASnsDBNs-PSM more than 350 hours while RCnsDBNs only needs less than 1 minute . Even considering the fact that two algorithms are imple-mented in different programming languages (RCnsDBNs in java and ASnsDBNs-PSM in Matlab), compared with ASnsDBNs-PSM, our method has much better computational efficiency. Figure 2: The curves of fraction of edges with PSRFs &lt; 1.04 on RCnsDBNs and ASnsDBNs-PSM for Thaliana T20 data. RC-nsDBNs: black solid line ; ASnsDBNs-PSM: blue dashed line. Figure 3: The VEPP curves on RCnsDBNs and ASnsDBNs-PSM for Thaliana T20 data. RCnsDBNs: black solid line ; ASnsDBNs-PSM: blue dashed line.

For the comparison between RJnsDBNs and our approach, we set  X  =0 . 05 for the convergence of VEPP values and listed the number of iterations and computational time in Table 1. In multi-ple data sets, RCnsDBNs converges much faster than RJnsDBNs. Compared with RJnsDBNs , RCnsDBNs got 6 folds computational improvement on CMV data, 6 folds on CMV + IFN  X  data, 9 folds on IFN  X  data. On Arobidopsis microarray data, RJnsDBNs took less time than RCnsDBNs. However, it failed to detect any meaningful change-point as RCnsDBNs and ASnsDBNs did on Arobidopsis data. In addition, we showed the VEPP curves of two approaches for CMV data in Figure 4.
We use k =1 for all the experiments because the gene ex-pression data usually has limited time points and larger k values eliminate short segments. The value of parameter p is adjusted for the purpose of the convergence of results for different domi-nant segment numbers m . We grid-search the values of p between 0 . 00001  X  0 . 5 for the effective range on the preferred segmenta-tion. For the synthetic data, it has four segments ( m =4 Macrophages data, we selected m =1 based on the assumption of a single IRFs network structure with varying parameters [10]. For Arobidopsis T20 data, most of the p range leads to m =1 . Finally, Figure 4: The VEPP curves for CMV data. RCnsDBNs ( p = 0 . 01 , X  s =2 ): black solid line ; RJnsDBNs (  X  m =0 . 65 , X  2 ): magenta dash-dot line. for Arobidopsis T28 data, we chose m =2 with the consideration of the external light/dark cycle condition.
 Synthetic (m=4) 0 . 02  X  0 . 032 CMV (m=1)  X  0 . 009 CMV+IFN  X  (m=1)  X  0 . 0006 IFN  X  (m=1)  X  0 . 0001 Arobidopsis T20 (m=1)  X  0 . 5
Arobidopsis T28 (m=2) 0 . 18  X  0 . 23
In the following, we will show the results of predicted structures and detected change-points.

The results on synthetic data. We compared two discrete mod-els, RCnsDBNs and RJnsDBNs, on synthetic data. RCnsDBNs totally runs 16 rounds to get converged, and each round uses 5,000 iterations for burn-in and then takes additional 20,000 iterations to collect samples ; RJnsDBNs runs 100,000 iterations for burn-in and then takes additional 400,000 iterations to collect samples. RCns-DBNs initializes G with additional 1000 heuristic search steps.
We showed the predicted posterior distributions on the numbers of segments and change-points in Figure 5. RCnsDBNs correctly identified 4 segments and its predicted change-points are close to the true times at 20, 40, and 60 while RJnsDBNs failed to identify meaningful change-points. The AUROC scores of predicted struc-tures by RCnsDBNs is showed in Table 3. When the equivalence class of bayesian network structures [2] were considered, the AU-ROC scores of all segments were increased, which were shown in thesametable.

The results on Macrophages data. On the CMV Macrophages data, RCnsDBNs totally runs 4 rounds to get converged, and each round uses 500 iterations for burn-in and then takes additional 2,000 iterations to collect samples ; RJnsDBNs runs 10,000 iterations for burn-in and then takes additional 40,000 iterations to collect sam-ples. On the CMV + IFN  X  Macrophages data, RCnsDBNs to-tally runs 5 rounds to get converged, and each round uses 1,000 iterations for burn-in and then takes additional 4,000 iterations to collect samples ; RJnsDBNs runs 26,000 iterations for burn-in and then take additional 104,000 iterations to collect samples. On the IFN  X  Macrophages data, RCnsDBNs totally runs 7 rounds to get converged, and each round uses 500 iterations for burn-in and then takes additional 2,000 iterations to collect samples ; RJnsDBNs runs 16,000 iterations for burn-in and then take additional 64,000 iterations to collect samples. In both data sets, ASnsDBNs runs Figure 5: Comparison of two methods on the synthetic data. Up: The 10,000 iterations for burn-in and then take additional 40,000 itera-tions to collect samples. RCnsDBNs initializes G with additional 100 heuristic search steps.

In Figure 6, 7, and 8, we show the posterior probabilities of the numbers of segments and change-points on Macrophages data sets.
For the CMV data, we observe that ASnsDBNs clearly identi-fies a dominant 3-segment in the data set while the posterior prob-abilities pr oduced by RJnsDBNs are almost flat. There is a con-sensus among three methods that the most probable change-point occurs around the location 5. The results of three methods are con-sistent with the biological phenomenon that the simultaneous re-sponses of Macrophages happen under the attack of Cytomegalovirus [10]. In order to assess the network prediction performance, we show the AUROC scores in Table 2. We find that all methods per-form well in the CMV data with the AUROC scores equal to 1.
For the CMV + IFN  X  data, both RJnsDBNs and ASnsDBNs methods identify 1 segment, which [10] explained as a coexistence state between virus and its host cell [1, 10]. And their posterior probabilities are flat. Differen t from these two methods, RCns-DBNs found two posterior peaks at 3 and 8. Such finding indicates the coexistence state may not happen at the beginning under both the IFN  X  treatment and invasion of virus. In Table 4, we find that RCnsDBNs and ASnsDBNs show a much better network predic-tion with the AUROC score equal to 0.6667 while in RJnsDBNs the AUROC score is equal to 0.2222.

For the IFN  X  data, there is a postulated transition with the im-mune activation under the treatment of IFN  X  . Both RJnsDBNs and ASnsDBNs infer 1 segments. RJnsDBNs and ASnsDBNs iden-tify a same posterior peak at the location around 5. RCnsDBNs finds two posterior peaks of transition time at 9 and 13. On the assessment of the predicted network structures, the AUROC scores are 0.7778 in RCnsDBNs and RJnsDBNs, and 0.6667 in ASns-DBNs.

For each Macrophages data set using RCnsDBNs and RJnsDBNs methods, we find that the posterior probability distributions of any edge do not change much across different segments. This finding is consistent with the assumption that the underlying network struc-ture does not change through the time.

The results on Arabidopsis data. On the Arabidopsis data, RC-nsDBNs totally runs 10 rounds to get converged, and each round uses 5,000 iterations for burn-in and then takes additional 20,000 iterations to collect samples ; RJnsDBNs runs 6,000 iterations for burn-in and then take additional 24,000 iterations to collect sam-RJnsDBNs 1 0 . 7778 0 . 2222 ASnsDBNs 1 0 . 6667 0 . 6667 RCnsBNs 1 0 . 5556 0 . 6667
RCnsDBNs 1 0 . 7778 0 . 6667
P(m) Figure 6: Comparison of four methods on CMV Macrophage data. Figure 7: Comparison of four methods on CMV + IFN  X  ples; ASnsDBNs runs 990,000 iterations for burn-in and then take additional 10,000 iterations to collect samples. Considering larger size of variables and thereafter the larger model space compared Figure 8: Comparison of four methods on IFN  X  Macrophage data. with other two data sets, RCnsDBNs takes more heuristic search steps and initializes G with additional 10000 heuristic iterations.
In Figure 9 and 10, we show the posterior distributions of the numbers of segments and changepoints on two Arabidopsis data sets. For the Arabidopsis T20 data, the dominant samples in RJns-DBNs and ASnsDBNs are respectively 2 and 3 segments. For the Arabidopsis T28 data, RJnsDBNs infers 1 segment and ASnsDBNs infers 5 segments. In both data sets, we find that the difference between the posterior peaks of changepoints and the time points nearby in RJnsDBNs are not noticeable. Hence, for this data set, we only use a single network in RJnsDBNs to compare with other methods. Using ASnsDBNs, the posterior peaks of change-points on T20 data are 1, 5 and those on T28 are 2, 7, 10. In [10], the results of ASnsDBNs are explained as a phase shift incurred by different dark/light cycles. Our method RCnsDBNs had the same finding by identifying the peaks at 5, 7, and 10 on T20 data and the peaks at 2, 6, and 9 on T28 data. And in addition, RCnsdBNs finds a peak around 10 on T20 data. This time point is exactly the beginning of the new light/dark cycle.

We evaluated the network reconstruction accuracy of three meth-ods by comparing with the reference network showed in Section 4.1. We show the AUROC scores in Table 5. Our method outper-forms RJnsDBNs in both datasets and has competitive performance on structure prediction accuracy against ASnsDBNs. Figure 9: Comparison of four methods on Arabidopsis T20 data. Up: Figure 10: Comparison of four methods on Arabidopsis T28 data. Up:
In this paper we introduced a new computationally efficient non-stationary DBNs method based on perfect simulation model. We applied this approach for the network inference on one synthetic data and two non-stationary time series microarray gene expres-sion data. The experimental results demonstrated that our method outperformed two other state-of-the-art methods in both computa-tional cost and structure prediction accuracy. The further sensitivity analysis showed that once converged our model is insensitive to the parameter, which reduces the uncertainty of the model behavior.
We are grateful to Dr. Marco Grzegorczy at TU Dortmund Uni-versity and Joshua W Robinson at Duke University to provide the data sets and softwares. This work is partially supported by NSF IIS award 0845951. [1] Chris A. Benedict, Theresa A. Banks, Lionel Senderowicz, [2] David Maxwell Chickering. Learning equivalence classes of [3] N. Chopin. Dynamic detection of change points in long time [4] Michael F. Covington, Satchidananda Panda, Xing Liang [5] Frank Dondelinger, Sophie Lebre, and Dirk Husmeier. [6] Paul Fearnhead. Exact and effi cient bayesian inference for [7] Andrew Gelman and Donald B. Rubin. Inference from [8] Peter J. Green. Reversible jump markov chain monte carlo [9] Peter J. Green. Trans-dimensional markov chain monte carlo. [10] Marco Grzegorczy, Dirk Husmeier, Kieron D. Edwards, [11] Marco Grzegorczyk and Dirk Husmeier. Improvements in [12] Anthony Hall, Laszlo Kozma-Bognar, RekaToth, Ferenc [13] David Heckerman, Dan Geiger, and David Maxwell [14] Kenya Honda, Akinori Takaoka, and Tadatsugu Taniguchi. [15] Dirk Husmeier. Sensitivity and specificity of inferring [16] Dirk Husmeier, Frank Dondelinger, and Sophie Lebre. [17] Katja Ickstadt, Bjorn Bornkamp, Marco Grzegorczyk, Jakob [18] Yi Jia and Jun Huan. Constructing non-stationary dynamic [19] JE Darnell Jr, IM Kerr, and GR Stark. Jak-stat pathways and [20] Yoshinobu Kawahara and Masashi Sugiyama. Change-point [21] M. Lavielle and G. Teyssi  X  l  X  lree. Adaptive detection of [22] Sophie Lebre, Jennifer Becq, Frederic Devaux, Michael [23] Robert Lund and Jaxk Reeves. Detection of undocumented [24] Paloma Mas. Circadian clock function in arabidopsis [25] Takeshi Mizuno and Norihito Nakamichi. Pseudo-response [26] V. Moskvina and A. Zhigljavsky. An algorithm based on [27] Agostino Nobile and Alastair T. Fearnside. Bayesian finite [28] Alessia Para, Eva M. Farre, Takato Imaizumi, Jose L. [29] A. Pievatolo and Peter J. Green. Bounary detection through [30] Arvind Rao, Alfred O. Hero III, David J. States, and [31] Sobia Raza, Kevin A Robertson, Paul A Lacaze, David Page, [32] Joshua W Robinson and Alexander J Hartemink.
 [33] Patrice A. Salome and C. Robertson McClung. The [34] Le Song, Mladen Kolar, and Eric Xing. Time-varying [35] Adriano V. Werhli, Marco Grzegorczyk, and Dirk Husmeier. [36] Wentao Zhao, Erchin Serpedin, and Edward R. Dougherty.
