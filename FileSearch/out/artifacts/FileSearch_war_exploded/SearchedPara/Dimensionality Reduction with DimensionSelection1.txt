 Recent years have been witnessing large i ncrease in studies on dimensionality reduction (DR). The purpose of DR is mainly to find the corresponding coun-terparts (or embeddings) of the input data of dimension M in a much lower dimensional space (so-called latent space, usually Euclidean) of dimension n and n M without incurring significant information loss. A number of new algorithms which are specially designed for nonlinear dimensionality reduction (NLDR) have been proposed such as Lapacian Eigenmaps (LE) [1], Isometric mapping (Isomap) [2], Local Tangent Space Alignment (LTSA) [3], Gaussian Process Latent Variable Model (GPLVM ) [4] etc. to replace the simple linear methods such as Principal Component Analysis (PCA) [5], Linear Discriminant Analysis (LDA) [6] in which the assumption of linearity is essential.

Among these NLDR methods, it is worth mentioning those which can handle highly structured or so-called non-vectorial data (for example proteins, which are not readily converted to vectors) dir ectly without vectorization. This cate-gory includes the  X  X ernelized X  linear methods. Typical methods are Kernel PCA (KPCA) [7], Generalized Discriminant Analysis (GDA or KLDA) [8] and so on. The application of the kernel function not only introduces certain nonlinearity implied by the feature mapping associated with the kernel which enables the algorithms to capture the nonlinear features, but also embraces much broader types of data including the aforementioned non-vectorial data. Meanwhile, ker-nels can also be regarded as a kind of similarity measurements which can be used in measurement matching algorithms like Kernel Laplacian Eigenmaps (KLE) [9] and Twin Kernel Embedding (TKE) [10]. Because these methods can directly use the structured data through kernel functions and hence bypass the vectoriza-tion procedure which might be a source of bias, they are widely used in complex input patterns like proteins, fingerprints, etc.

Although DR is proven to work well for some machine learning tasks such as classification [11], an inevitable question yet to be answered in applying DR is how to estimate the dimensionality which is the so-called intrinsic dimension estimation. Various methods have been presented in machine learning literature [12]. Nevertheless, a very simple way fo r dimension estimation is to reduce the dimension one at a time in a suitable space until significant information loss occurs and then stop. This procedure does the reduction and dimension estima-tion at the same time. There are two very important ingredients in this method: the proper space of the transformed da ta and the stop criterion of dimension reducing. These two should be combined seamlessly.

Interestingly, we can look at this problem the other way around. Instead of dropping dimensions, we can select dimensions in a suitable space. To do this properly, we refer to the variable selectio n framework. The nature of the variable selection problem is combinatorial optimization and hence NP hard. Neverthe-less, there is a recent trend of using spars e approximation to solve this problem which has attracted attention in statistics and machine learning society. The earliest work is from [13] called the LASSO. By adding an l 1 norm constraint on the coefficients of a linear regression model, the original combinatorial op-timization problem was converted to a convex optimization problem which is much easier to solve. The optimization is normally cast as a regularization prob-lem solved by lots of efficient algorithms such as coordinate descent [14], iter-ative shrinkage-thresholding [15] and etc. Several sparsity encouraging models have been proposed afterwards with various properties. For example, the group LASSO [16] has the group selection property by applying the l 2 /l 1normtothe group coefficients.

The above discuss leads to a novel method for dimensionality reduction which selects dimensions in trans formed space and the selectio n is carried out by sparse methods. What follows is how to choose the transformed space? The research in this decade on dimensionality reduction provides many solutions to this ques-tion. We choose Laplacian eigenmaps in this paper because it approximates the embedded manifold through a mapping function with proximity preserving prop-erty. However, It is very convenient to e xtend this choice to other methods such as LLE, TKE and etc. depending on the application at hand. Another variable in this idea is the sparse method normally by using sparsity encouraging norms. Since different sparsity encouraging norms come with different features, they provide us flexibility for various machine learning tasks, for example subspace learning, feature extraction etc. We use l 2 /l 1 norm here for its group selection capability which is suitable for our dimension selection purpose. In Section 3, we will briefly discuss some of its variants and show how this idea could be used for subspace learning. Since this method involves sparse models for dimensionality reduction, we call it sparse dimensionality reduction or SDR for short.
The most related work is [17] where a rank prior as a surrogate of data di-mensionality was imposed on GPLVM. In [17] the transformation of data was carried out by GPLVM, which converted the data to a space where a low rank representation (measured by rank prior ) was possible. The stopping criterion was the stationary point of the optimization process. The work in [18] is also similar to ours but it was built on a sparse linear regression model and a dictionary in high dimensional space is required.

The paper is organized as follows. Section 2 briefly reviews Laplacian eigen-maps. Section 3 explains the proposed SDR method in detail followed by a section for the optimization procedure. In Section 5, we present several experi-mental results using SDR on visualization to show its effectiveness. We conclude this paper in Section 6. In the following discussion, let y i  X  R M be the i -th data sample on a manifold embedded in M dimensional space. Laplacian eigenmaps (LE) [1] is a typical nonlinear method that belongs to the family of graph-based DR methods. It attempts to preserve proximity relations in the input data which is expressed by a weight matrix based on adjacency graph (or called neighborhood graph). This adjacency graph G is constructed by referring to  X  neighborhood or n nearest neighbor criterion. An edge will connect y j and y i if y i  X  y j 2 &lt; X  (  X  neighborhood), or if y j is among n nearest neighbors of y i and vice versa ( n nearest neighbor is more commonly used). The adjacency graph plays an important role in dimensionality reduction which leads to a series of graph based methods [19,20].

After the construction of the adjacency graph, the weights on the edges are evaluated that stand for the proximity relations among the input data. There are also two variations of setting the weights in LE: (a) exponential decay function: (b) binary (  X  =0): w ij =1if y i and y j are connected, and w ij =0otherwise. This simplification avoids the need to choose  X  .

The weight matrix W ( { w ij } ) containing the proximity information is then used to construct the Laplacian of the graph L = D  X  W where D is a diagonal matrix and its ii -th element [ D ] ii = N j =1 w ij . The reason for Laplican is that the optimal locality preserving maps of the data on manifold onto R K ( K is at most M  X  1 because of the removal of arbitrary translation) can be approximated by obtaining the smallest K eigen vectors (excluding the eigen vector corresponding to eigenvalue 0) from the following generalized eigendecomposition where X of size N  X  K is the matrix of maps of y i  X  X  in R K and I is the identity matrix with compatible dimensions. F or dimensionality reduction purpose, K is selected (somewhat arbit rarily) much less than M or by dimension estimation.
LE is a local method since it is based on the adjacency graph. Several variants have been derived from original LE such as the LPP (Locality Preserving Pro-jection) [21] and the KLE (Kernel LE) [9]. LPP introduces a linear constraint between input data and embeddings, i.e x i = Ay i while KLE replaces the weight matrix by a sparse kernel Gram matrix. We interpret the dimensionality reduction as a process of space transformation under the framework of Laplacian eigenmaps. The assumption is that the data lie on or near to a dimensional manifold embedded in M dimensional ambient space. The Laplacian eigenmaps is to unfold the manifold in a K dimensional subspace. As we do not know the intrinsic dimension of the manifold, We have to resort to other methods, which may be heterogeneous totally to LE, to estimate the dimensionality in advance.

However, if the unfolded manifold is indeed low dimensional, it should be  X  X ompressible X , i.e. we can drop redundant dimensions while maintain the struc-ture of the unfolded manifold in this transformed space. As discussed in Section 1, the force of compressing can be realized by introducing sparsity encouraging norms. The suitable one is the l 2 /l 1 norm [16] defined as where X i is the i -thcolumnof X and || X || 2 is l 2norm.When X is a row vector, l 2 /l 1 norm degenerates to normal l 1 norm. An outstanding feature of l 2 /l 1norm is its group selection capability meaning that the elements in some l 2norms (groups) will be compressed towards zero altogether if the norm is minimized, for example in the group variable selection in [16,22]. For our purpose of dimension selection, we use it to vanish the whole dimension, which is regarded as group in terms of l 2 /l 1 norm, if it is dispensable.

Our sparse dimensionality reduction (SDR) takes the following form where t  X  R + is the parameter to control the  X  X imension sparsity X . Following the previous discussion, the rationale is quite clear, that is we unfold the manifold in such a way that some of the dimensions can be reduced or in other words the most important dimensions can be selected. The algorithm starts with a generalized eigen decomposition, i.e. (2) without l 2 /l 1 norm constraint. Once the selection completes, we retain the selected dimensions only from the initialization only. Details about implementation will be presented in Section 4.

Interestingly, if we substitute the l 2 /l 1 norm by the nuclear norm in (2) as follows the idea would be unfolding the manifold such that the rank of the of the embed-dings, i.e. X , is restricted. This is equivalent to finding a subspace in R K that reveals the lower dimensional structure of the manifold. It suggests that poten-tially SDR can be used as a tool for subspace learning [23]. More generally, we can use other sparsity encouraging norms denoted as lq , which certainly brings different interpretation to this method.

Furthermore, we can extend this idea to LPP which is in line with LE. LPP has another layer on top of LE which is a linear mapping from input data to embeddings. In this case, we have the following objective in variable A where A  X  R K  X  M is the linear transformation matrix. q varies depending on the purpose of the application.

The flexibility of SDR enables it to embrace a lot of existing DR methods as well as sparse methods. In this paper, we focus only on (2) for its simplicity and direct understanding of dimension selection. We proceed to obtaining the solution fo r SDR in (2). Direct optimization of the objective of SDR in (2) is very d ifficult because the nonsmooth l 2 /l 1norm constraint and quadratic equality constraint, which makes it a quadratic pro-gramming with quadratic constraint (QPQC) problem with additional norm restriction. The quadratic equality constraint effectively excludes some popular alternating optimization schemes such as ADMM [24] since the augmented La-grange term from the equality has no close form solution. It also throws some trouble to second order optimizer such as Newton-Raphson method because the Hessian is a tensor. To maintain the convexity of the original problem and also to make it easier to solve, we relax the equality constraint by converting it to a regularization term in the objective so that the first order algorithms are appli-cable. As a result, the original SDR prob lem has been converted to the following form where || X || F denotes the F norm of a matrix and e is an all one column vector. Note that we add another regularizer to the l 2 /l 1normof X , z e T t ( t  X  R K ), and the i -th element of t is responsible for || X i || 2 . z has the same function as t in (2) controlling the dimension sparsity. The introduction of this additional regularization does not bring extra complexity; however, it enables us to use the efficient Euclidean projection explained in [25] with which (5) can be solved using Nesterov algorithm [26] easily. We will not go into too much detail of the algorithm but provide the necessary elements to make it work. Write supposing  X  and z are given. We have the derivatives and  X  X  ( X , t )  X  t = z e for this first order algorithm. The detailed optimization pro-cedures are shown in Table 1.

In Nesterov algorithm, there are two sequences of variables, the target in the optimization problem, X and t in this case, and assistant variables, S and h shown in Table 1 corresponding to X and t respectively. The assistant variable is a linear combination of current and previous estimation of the target, e.g. S projection in Line 6, where P C ( x ) is the Euclidean projection of x onto the feasible convex set C .Inourcase, C is the set of values satisfying || X || 2 / 1 t .The Euclidean projection of the given pair U and v is the solution to the following f  X , x ( y ) is the the linear approximation of the objective function f ( y )atthe point x regularized by the proximality The algorithm in Table 1 has incorporated the Nemirovski line search in Line 4 to 10 for better efficiency. The initialization of X can be obtained from the solution of the generali zed eigendecomposition LX = DX  X  where  X  is the diagonal matrix of the eigenvalues. Note that the last eigenvalue is 0 and its corresponding eigenvector should be removed to obtain translation invariant as in LE. The initial t can simply be the l 2 norm of each column of X .

The convergence is guaranteed by Nesterove algorithm. The computational complexity is mainly from matrix multiplications in the evaluation of the gra-dient in (6). The Euclidean projection in (7) is linear time. So the dominant complexity is O ( K 3 N 4 )since D in (6) is a diagonal matrix. It may look very high. But as it iterates, many columns of X become zero, which effectively brings down K . Our experience with computation t ime is that it completes in several minutes on uptodate personal computer for N = 2000, K = 600. We applied the SDR to several data sets: COIL data set, Frey faces and SCOP protein structure data where LE has difficulties. They are widely used for ma-chine learning and image processing tests. We mainly reduced the dimensionality to 2 so that we can plot the embeddings in 2D plane for interpretation. z is se-lected by bisection so that only require d dimensions were sel ected. To construct LE initialization for the SDR algorithm and LE itself, we set the number of nearest neighbors to be 5 and used simple binary weight for images data. We used KLE with Mammoth kernel for prote in data where the number of nearest neighbors was 8. All these parameters were frequently used or reported to be somewhat optimal. For other methods in protein experiment, we also set their parameters to their reported optimal if any. In SDR optimization procedure, we set the update tolerance to be 1e-10 and maximum number of iterations to be 100, whichever reaches first stops the algorithm. It turned out that these settings worked well on the data sets we have tested in this section. 5.1 COIL 3D Images We demonstrate SDR X  X  dimension selection capability against LE in this ex-periment. We took the first 20 objects from Columbia Object Image Library (http://www1. cs.columbia.edu/CAVE/software/softlib/coil-100.php). Each ob-ject has 72 greyscale images of size 128  X  128 taken from video frames. Since all the images have been perfectly aligned and noise free, traditional methods like PCA can achieve good embedding. However, we focus on dimension selection capability of SDR here. In regard to 2D display, we expected that the objects to line up in the 2D plane somehow with some overlap. As we can see from Fig. 1 (b) where shapes stand for objects, LE X  X  result is 3 isolated islands with heavy overlapping. However the 2D space sel ected by SDR reveals two cups classes clearly with overlap in the middle with o ther objets, which is closer to our ex-pectation. This suggests SDR X  X  subspace learning capability, which is further confirmed in the following experiment.

We further extended the target dimension from 1 to 10 and used the 1 nearest neighbor (1NN) classification errors rate as in [4] to compare the results quantita-tively. The smaller the error rate, the better the method. The 1NN classification error rates are plotted in Figure 1 (c). It turned out that dimensions selected by SDR are consistently better although they are not optimized for classification task. 5.2 Frey Faces In this subsection, the input data is 1,965 images (each 28  X  20 grayscale) of a single person X  X  face taken from a video sequence which was also used in [27]. It is widely accepted that two parameters control the images, the face direction and facial expression. Intuitively, there should be two axes in 2D plane for these two parameters fused together somehow. However, the understanding like this is somewhat artificial. This may not even close to the truth. But we hope our algorithms can shed some light on these two parameters. Very interestingly as shown in Figure 2 (a) corresponding to SDR results, three axes for happy, sad and plain expressions respectively with continuously changing face direction can be clearly observed. It turns out that SDR identified the major dimensions as facial expressions. The way that SDR axes are lined up once again pronounces its potential in subspace learning. The LE X  X  result smeared out the facial expression and direction, which is not really informative. 5.3 Protein Structure Data We move from image data (mainly video se quences) to protein structure data where KLE is more suitable because of the non-vectorial property of the pro-tein data. Experiment was conducted on the SCOP (Structural Classification Of Protein). This database is available at http://scop.mrc-lmb.cam.ac.uk/scop/. It provides a detailed description of the structural and evolutionary relationships of the proteins of known structure. 292 prot ein sequences from different superfami-lies and families were extracted for the test. The kernel we used is the Mammoth kernel, a so-called alignment kernel [28].
 Only the results of SDR and KLE are plotted in Figure 3 for limited space. However other methods were tested. Each point (denoted as a shape in the figure) represents a protein. The same shapes with the same colors are the proteins from the same families (shown in legend) while the same shapes with different colors represent the proteins from different families but the same superfamilies. Except for better scattered clusters in SDR resu lt, one noticeable difference is that one family dominates (diamonds) the horizontal axis in the middle in SDR result and others are projected to the other axis as 2D space is apparently not enough for this data set. We used  X  X urity X  [10] to quantify the clusters produced by different methods. It uses the fraction of the number of samples from the same class as given point in a neighborhood of size n . The purity is the average of the fraction over all points. The higher the purity, the better the quality of the clusters. As we can see from Figure 3 (c), SDR has the purest clusters when n&lt; 9. Although it drops below KLE when n  X  9, it is still better than other methods in this test. Note that for linear methods like PCA we used the vectorization method derived form the kernel introduced in [10]. We proposed a novel method called spars e dimensionality reduction (SDR) in this paper along with a practical optimization algorithm to minimize an approx-imated SDR objective. SDR projects input data to a space where the redundant dimensions are compressible, and therefore it is not necessary to specify the dimensionality of the target space. The dimension sparsity parameter z in (5) is determined empirically. Bisection can be used if the target dimensionality is clear as shown in the experiments. If the final dimensionality is tied up with some quantitative standard such as MSE in regression, we can optimize z against it. It exhibits subspace learning property and the interesting results in images from video sequences suggested that SDR may be suitable for, and not confined to, computer vision applications such as subspace identification etc.
 Acknowledgement. This project is sponsored by the Australian Research Council (ARC) under Discovery Proj ect grant DP130100364 and also partially supported by the Commonwealth of Australian under the Australian-China Sci-ence and Research Fund (ACSRF01222).
 In many real-world applications in data mining and machine learning, data are naturally described by multiple views [7]. For example, in document analysis, web pages can be represented by their content or the content o f the pages pointing to them; In bioinfor-matics, genes can be described with the feature space (corresponding to the genetic activity under the biological c onditions) as well as the term space (corresponding to the text information related to the genes); Im ages are represented by different kinds of low-level visual features, such as color histograms, bags of visual words, and so on.
Actually, different views describe different aspects of data. No one among them is absolutely better than others for describing the data [10]. Thus, a good alternative is to simultaneously employ multiple views to learn the data. This is well known as multi-view learning [2]. Multi-view learning has been shown to be more effective than single-view learning, particularly in the scenario where the weaknesses of a single view can be strengthened by others [14]. For example, in content analysis, color features have been shown to be sensitive to scaling, while SIFT features are robust to scaling. Combining color and SIFT features to perform multi-view learning can boost the performance by complementing each other X  X  robustness on different aspects of the data.
Meanwhile, existing multi-view learning me thods have several limitations. First, not all views of data are useful for some specifi learning tasks since some of them may be redundant. However, existing methods are often designed to learn from all views of the data, without taking the redundancy issue into acc ount. For example, canonical correlation analysis (CCA) and its kernel edition KCCA (e.g., [4,16]) were designed to learn a common latent feature space by learning from all views of the data. Second, multi-view data often contain noise, which easily affects the effectiveness of learning tasks while learning from all views of the data. Third, the intrinsic group structure of each individual view (i.e., view structure) in the data should also be preserved. Given multi-view data, each view of the data is a natural group to describe an aspect of the data. For example, a color histogram feature naturally forms a group for describing the color characteristics of image data.

Given that data are often represented by multiple views and associated with multiple object categories, this paper focuses on the problem of visual classificati n with multi-view multi-label (MVML) learning. In this pa per, we propose a novel mixed-norm joint sparse learning model, which aims to select representative views and remove noisy at-tributes for MVML classificati n. More specificall , we f rst employ a least square loss function measured via a Frobenius norm (or F -norm in short) in each view to pursue a minimal regression error across all the views. We then introduce a new mixed-norm regularizer (i.e., combining an F -norm with an 2 , 1 -norm) to avoid redundant views and preserve the intrinsic view structure via the F -norm regularizer, and remove noisy attributes via the 2 , 1 -norm regularizer. We further devise a novel iterative algorithm to eff ciently solve the objective function of the proposedmixed-normjoint sparse learning model, and then theoretically prove that the proposed algorithm enables the objective function to converge to its global optimum. After performing the iterative algorithm, the derived regression coeff cient matrix only contains a few non-zero rows in a few selected views due to the mixed-norm regul arizer. This makes the test process more eff cient. Finally, we conduct an extensive experimental study on real-life datasets to compare the effectiveness of the proposed learning model with state-of-the-art methods for MVML classificati n.

We summarize the contributions of this paper as follows:  X  We identify limitations in traditional multi-view learning, mainly caused by re- X  The proposed model focuses on embedding a mixed-norm regularizer into the ex- X  To perform MVML classif cation, the proposed model can be regarded as In this paper, p -norm of a vector v  X  R n is define as v p = n v  X  The transpose of X is denoted as X T ,theinverseof X is X  X  1 , and the trace operator of a matrix is denoted by the symbol  X  X r X . 2.1 Loss Function Given the g -th view X g of the training data X , we need to obtain its regression coeffi training label Y =[ Y T tion between X g and W g , i.e., G be def ned as follows: where X =[ X 1 ,..., X g ,..., X G ] . Obviously, Eq.1 meets our goal of minimizing the regression error across all views. 2.2 Mixed-Norm Regularizer Given a loss function (such as in Eq.1), during the optimization process we also de-sign a mixed-norm regularizer, aiming to meet other goals, such as removing redun-dant views and noisy attributes. In this paper, we achieve these goals by performing two types of feature selection, i.e., view-selection for removing redundant views and attribute-selection for deleting noisy attributes. To this end, we propose a new mixed-norm regularizer by integrating an F -norm regularizer with an 2 , 1 -norm regularizer.
More concretely, in the proposed joint sparse learning model, the F -norm regularizer generates the codes of redundant views as zeros and the others as non-zeros; the 2 , 1 -norm regularizer generates the codes of noisy attributes as zeros and the others as non-zeros. Then with the impact of sparse views and attributes, MVML classificatio can be effectively and efficientl performed. Mor eover, the mixed-norm regularizer enables us to avoid the issue of over-f tting.
 In this paper, the 2 , 1 -norm regularizer is define as: was designed to measure the distance of the attributes via the 2 -norm,while performing summation over all data points via the 1 -norm. Thus the 2 , 1 -norm regularizer leads to the row sparsity as well as to consider the correlations of all attributes. The F -norm regularizer is define as: where W g is the g -th block of matrix W (or the submatrix formed by all the rows belonging to the g -th view), and indicates the effect of the g -th block (i.e., there are sequential m g rows in the g -th view) to all data points. 2.3 Objective Function By considering three equations together, i.e., Eq.1, Eq.2 and Eq.3, we obtain the ob-jective function of the proposed mixed-norm joint sparse learning model as follows: where both  X  1 (  X  1 &gt; 0 )and  X  2 (  X  2 &gt; 0 ) are tuning parameters.
Similar to the mixed sparsity using the 1 -norm regularizer and the 2 , 1 -norm reg-ularizer together in separable sparse learning, the proposed mixed regularizer leads to the mixed joint sparsity. That is, it f rst discriminates redundant views via the F -norm regularizer, and then detects noisy a ttributes in the selected views via the 2 , 1 -norm regularizer.

Actually, some literatures have focused on the mixed sparsity, such as elastic net [19] and sparse group lasso [11] in separable sparse learning, adaptive multi-task lasso [8] in joint sparse learning, and so on. For example, an elastic net combines the 1 -norm regu-larizer with the 2 -norm regularizer for achieving the element sparsity (via the 1 -norm regularizer) and impact group effect (via the 2 -norm regularizer). Sparse group lasso achieves the mixed sparsity, i.e., the element sparsity via the 1 -norm regularizer as well as the group sparsity via the 2 , 1 -norm regularizer. As mentioned in Section 2, neither elastic net nor sparse group lasso benefit for feature selection. Recently, adaptive multi-task lasso combines the 1 -norm regularizer with the 2 , 1 -norm regularizer in multi-task learning to achieve feature selection (via the 2 , 1 -norm regularizer) and deletes noisy elements (via the 1 -norm regularizer). Obviously, existing literatures mentioned above were not designed to delete redundancy views and to perform feature selection at the same time, as the proposed method in this paper does.

Next we explain why the proposed mixed-norm regularizer leads to the mixed joint sparsity, i.e., simultaneously obtaining two types of sparsity. While the value of  X  2 is larger, the minimization process in Eq.4 drives the value of the F -norm (i.e., the third term in Eq.4) smaller. This tends to force the values of some blocks (e.g., the value of the g -th block is W g F ) with small values to be smaller. After several iterations, the values of these blocks in W are close to zero. Thus we obtain a sparse W with zero value in some blocks, e.g., the g -th block. This indicates that the corresponding views (e.g., the g -th view) of X are redundant views since the sparsity appears in those blocks (e.g., the g -th block) of W . The sparse blocks of W remove the corresponding views of
X from the test process. Meanwhile, we also notice that the larger the value of  X  2 , the more the block sparsity. With the same principle, while the value of  X  1 is larger, the minimization process in Eq.4 forces some rows in W to be zero, i.e., the attributes corresponding to the sparse rows in W are not involved the test process. Hence, the proposed mixed-norm regularizer leads to the mixed joint sparsity, which achieves the block sparsity as well as the row sparsity.

According to above analysis, Eq.4 can be us ed to select a few useful attributes from a few representative (or signif cant) views of the data for the visual classif cation. This has the following advantages. First, it benef ts for improving the eff ciency of the test process due to the sparse W . Second, these two kinds of feature selection help to avoid the impact of redundant views and noisy attributes in the test process, thus benef t for effectively performing the MVML classific tion. Third, it induces the mixed joint spar-sity as well as leads to a hierarchical coding model (i.e., non-sparse attributes generated from non-sparse views), which plays an important role in many applications where a feature hierarchy exists. Last but not the least, views-selection via the F -norm regu-larizer also preserves the individual view s tructures of the non-sparse views since each view is regarded as a block. 2.4 Classification By solving Eq.4, we obtain the optimal W . Given a test dataset X test , we obtain the corresponding label set Y test by Y test = X test W in the test process. Due to inducing by the proposed mixed-norm regularizer, only a few blocks in the derived W are non-zeros, and also only a few rows in these non-zero blocks are non-zeros. This makes the test process more eff cient to be performed.

After ranking Y test according to the label values, the top-k labels are assigned to the test data as the predicted labels. This rule is the same to existing multi-label methods, e.g., [18].
 Eq.4 is obviously convex since it consists of three norms, which have been shown to be convex [6]. Therefore, Eq.4 has the global optimum. However, its optimization is very challenging because both the W F -norm and the W 2 , 1 -norm in Eq.4 are convex but non-smooth. In this section we solve this problem by calculating sub-gradients of the mixed-norm regularizer, i.e., the W F -norm and the W 2 , 1 -norm respectively. 3.1 The Proposed Solver By setting the derivative of Eq.4 with respect to W as zero, we obtain: where C is a diagonal matrix with the i -th diagonal element: where ( W ) i denotes the i -th row of W , i =1 ,...,n . D = diag ( D 1 ,..., D G ) ,where the symbol  X  diag  X  is the diagonal operator and each D g ( g =1 ,...,G ) is also a diagonal matrix with the i -th diagonal element as: where j =1 ,...,m g .

By observing Eq.5, we f nd that both the matrix C and the matrix D depend on the value of matrix W . In this paper we design a novel iterative algorithm to optimize Eq.5 by alternatively computing the W and the C (with the D ). We fi st summarize the details in Algorithm 1, and then prove that in each iteration the updated W and the C (with the D ) make the value of Eq.4 decrease.
 Algorithm 1. The proposed method for solving Eq.4 With Algorithm 1, at each iteration, given the fi ed C and D ,the W is updated by Eq.5. Then the C and the D can be updated with the f xed W . The iteration process is repeated until there is no change on the value of Eq.4. 3.2 Convergence In this subsection we introduce Theorem 1 to guarantee that Eq.4 monotonically de-creases in each iteration of Algorithm 1. Following the literature in [9,17], we f rst give a lemma as follows: Lemma 1. For any positive values a i and b i , i =1 ,...,m , the following holds: Theorem 1. In each iteration, Algorithm 1 monotonically decreases the objective func-tion value in Eq.4.
 Proof. According to the fi th line of Algorithm 1, we denote the W [ t +1] as the results of the ( t +1) -th iteration of Algorithm 1, then we have: Then we can get: which indicates that:
Substituting b i and a i with ( W g ) [ t ] F ) in Lemma 1, we have: This indicates that the objective function v alue in Eq.4 monotonically decreases in each iteration of Algorithm 1. Therefore, due to the convexity of Eq.4, Algorithm 1 enables Eq.4 to converge to its global optimum. In order to evaluate the performance of the proposed mixed-norm joint sparse learning state-of-the-art methods on public datasets (e.g., MIRFLICKR [5] and NUS-WIDE [3]) for MVML classif cation, by evaluating t he average precision and Hamming loss. 4.1 Experiment Setup We use four datasets, including MIRFlickr, NUS-WIDE, SCENE and OBJECT in our experiments for MVML classif cation. The comparison methods include the method in [15] (denoted as F2F from its objective function, for simplicity) which only considers the block sparsity, the method in [12] (denoted as F2L21 ) which only considers the row sparsity, the MKCCA method in [1] which does not consider the feature redundancy and noise, and the single view method Wo r s t S (or BestS ) which has the worst (or best) classificatio performance from the data re presented by a single view via ridge regres-sion (i.e., all single views are tested). We use two popular evaluation metrics (i.e., the average precision (AP) and Hamming loss (HL)) in multi-label learning [13] to evaluate the effectiveness of all the methods in our experiments.

Given the ground true label matrix Y 1  X  X  0 , 1 } n  X  c (where n is the number of in-stances and c is the number of labels) and the predicted one Y 2  X  X  0 , 1 } n  X  c obtained by the algorithm for performing MVML learning, average precision (AP) is def ned as: where the symbol  X  X ard X  means the cardinality operation.
 HL measuring the recovery error rate is define as: where  X  is an XOR operation, a.k.a. exclusive disjunction. According to the literatures, e.g., [13,18], the larger (or smaller) the performance on AP (or HL) is, the better the method. 4.2 Experimental Results In this subsection, we report the results on MVML classif cation. First, we evaluate the convergence rate of the proposed F2L21F on all four datasets, for evaluating the efficie cy of our optimization algorithm, in terms of the objective function value in each iteration. Second, we test the parameters X  sensitivity of the proposed model on  X  1 and  X  2 , aiming at obtaining the best performance of the proposed F2L21F . Finally, we compare F2L21F with the comparison algorithms in terms of average precision and Hamming loss.
 Convergence Rate. We solve Eq.4 by the proposed Algorithm 1. In this experiment, we want to know the convergence rate of Algorithm 1. Here we report some of the results in Fig.1 and Fig.2 due to the page limit. Fig.1 shows the results on the objective function value while fi ing the value of  X  1 (i.e.,  X  1 =1 )andvarying  X  2 .Fig.2shows the results on the objective function value while fi ing the value of  X  2 (i.e.,  X  2 =1 ) and varying  X  1 . In both Fig.1 and Fig.2, the x-axis and y-axis denote the number of iterations and the objective function value respectively.
We can observefromboth Fig.1 and Fig.2 that: 1) the objectivefunction value rapidly decreases at the f rst few iterations; and 2) the objective function value becomes stable after about 30 iterations (or even less than 20 in many cases) on all datasets. This con-f rms a fast convergencerate of Algorithm 1 to solve the proposed optimization problem in Eq.4. Similar results are observed for other  X  1 and  X  2 values.
 Parameters X  Sensitivity. In this experiment, we test different settings on parameters  X  1 and  X  2 in the proposed F2L21F , by varying them as { 0 . 01 , 0 . 1 , 1 , 10 , 100 , 1000 The results on average prediction and Hamming are illustrated in Fig.3.
It is clear that the proposed F2L21F is sensitive to the parameters X  setting, similar to other sparse learning methods [11,18]. However, we f nd the worst performance is always obtained when both  X  1 and  X  2 have extremely large values. For example, when the values of parameters pair (  X  1 , X  2 ) are around (10,10), F2L21F achieves the best performance. Actually, in our experiments such a setting simultaneously leads to both the row sparsity (via the  X  1 ) and the block sparsity (via the  X  2 ).
 Comparison. In this experiment, we compare our proposed method with state-of-the-art methods for MVML classif cation. We set the values of parameters for the compari-son methods by following the instructions in their original papers. For all the methods, we randomly sample 60% of the original data as the training data, and leave the rest as the test data. We randomly generate ten runs, and report the average result and the standard deviation on the average precision and Hamming loss, as shown in Fig.4. Note that we do not use dataset MIRFlickr since it has only two views.

From Fig.4, we have the following observations: 1) The proposed F2L21F always achieves the best performance. Among six views in NUS-WIDE and f ve views in SCENE and OBJECT, the 64-D color histogram is detected as a redundant view in F2L21F . F2L21 and MKCCA use all the views to perform MVML classificati n and obtain worse performance than F2L21F . This conf rms that some views (e.g., the color histogram in the tested datasets) are not helpful in the learning process and may even degrade the performance, especially when many views are available. F2L21F is able to identify those redundant views and avoid their negative impact on the classif cation. Although F2F can also discover those redundantviews, it is not able to remove noisy at-tributes from the selected views, leading to worse performancethan F2L21F .This result provesthe effectivenessof F2L21F in removingredundantviews and noisy attributes by employing the proposed mixed-norm regularizer in MVML classif cation. 2) The per-formance of single view learning methods (i.e., BestS and Wo r s t S )isalwaysworsethan those multi-view learning me thods. This again confirm the advantages of using multi-views in visual classif cation. 3) The sparse learning methods (i.e., F2L21F , F2F ,and F2L21 ) consistently outperform MKCCA . This shows the superiority of sparse learn-ing which encodes negligible elements as zeros and only s elects important elements to perform MVML classif cation. Moreover, in our implementation the computational cost of F2L21F is about tens times faster than that of MKCCA , indicating much higher eff ciency than MKCCA .

In sum, more views can help improve the pe rformance of visual classificatio since more information can be utilized in the learning process. On the other hand, more views mayalsopotentiallyintroducehigherredundancyandmorenoisewhichcompromisethe performance.The proposed F2L21F is able to identify those redundant views and noisy attributesso thatMVMLclassificatio can beperformedformoreeffectiveperformance. In this paper we proposed a mixed-norm joi nt sparse learning model for multi-view multi-label (MVML) classification The p roposed method, powered by a mixed-norm regularizer, can effectively avoid the negative impact of redundant views and noisy attributes from the multi-view representatio n of a large amount of data. Extensive ex-perimental results have shown that the pr oposed method outperforms state-of-the-art learning methods for MVML classif cation. In the future, we will extend the proposed method into its kernel edition to project noise more clearly, and involve other learning models, such as semi-supervised MVML cla ssificatio and transfer MVML classifica tion, to leverage the widely available unlabeled data and heterogenous data.
