 Estimating the appropriate word order for a target language is one of the most diffi-cult problems in statistical machine translation (SMT). This is particularly true when translating between languages with widely different word orders, such as Japanese and English. To address this, a large body of research has been conducted on word reordering, for instance: lexicalized reordering model [Tillman 2004] for phrase-based SMT, hierarchical phrase-based SMT [Chiang 2007], syntax-based SMT [Yamada and Knight 2001], preordering [Xia and McCord 2004], and postordering [Sudoh et al. 2011b].

The preordering framework is useful for word reordering because it can use source-language syntactic structures in a simple way. Specifically, a preordering method using source-language syntactic structures for English-to-Japanese translation has been confirmed to be highly effective [Goto et al. 2011; Sudoh et al. 2011a]. Existing preordering methods that use source-language syntactic structures require a source-language syntactic parser. Unfortunately, high-quality syntactic parsers are not avail-able for many languages.

Preordering methods that do not require a parser are useful in cases where no source-language syntactic parser is available [DeNero and Uszkoreit 2011; Neubig et al. 2012]. Such methods produce preordering rules using word alignments. How-ever, these preordering rules do not use syntactic structures, which are an essential factor in deciding word order. Therefore, the use of syntactic structures is a major challenge for preordering methods that do not require a source-language syntactic parser.

In this article, we propose a novel preordering approach that uses syntactic struc-tures by employing a target-language syntactic parser without requiring a source-language parser. A high-quality target-language constituency parser will be useful for preordering. Source-language syntactic structures and corresponding target-language syntactic structures are expected to be similar in a parallel corpus [Hwa et al. 2005]. The proposed method relies on this expectation. We project target-language syntactic constituent structures in a parallel corpus onto their corresponding source-language sentences through word alignments, which produces partial syntactic structures where the words are from the source language but the phrase labels are from the target-language syntax. We then select parallel sentences with highly synchronized parallel syntactic structures based on the projection. We construct a probabilistic context-free grammar (CFG) model and a probabilistic model for unsupervised part-of-speech (POS) tagging using the partial syntactic structures of the selected paral-lel sentences and the Pitman-Yor process [Pitman and Yor 1997]. We then parse the source-language training sentences to produce full binary syntactic tree structures us-ing the produced probabilistic models with the projected partial syntactic structure constraints. A preordering model based on inversion transduction grammar (ITG) [Wu 1997] is learned using the full binary syntactic constituent structures of the source-language sentences and word alignments. Input sentences are parsed using the ITG-based preordering model, then their syntactic structures and reordered structures are identified jointly.

Our main contributions are (i) a new effective framework for preordering using a target-language syntactic parser that does not require a source-language syntac-tic parser, (ii) methods that facilitate the learning of ITG by producing highly syn-chronized parallel syntactic structures based on cross-language syntactic projection and sentence selection, (iii) a simple method for producing full binary syntactic con-stituent structures of source-language sentences from the constituent structures of the corresponding target-language sentences using the Pitman-Yor process, and (iv) an empirical confirmation of the efficacy of Japanese X  X nglish and Chinese X  X nglish patent translation.

There is a need for translation in situations where all the following are true: (1) a high-quality target-language parser is available, (2) a high-quality source-language parser is not available, and (3) the source-language word order and the target-language word order are largely different, such as in subject-object-verb (SOV) and subject-verb-object (SVO) languages. We propose a method that can be applied in such situations. In our experiments on Japanese X  X nglish and Chinese X  X nglish translation using the patent data from the NTCIR-9 and NTCIR-10 Patent Machine Translation Tasks [Goto et al. 2011, 2013a], we were able to significantly improve translation qual-ity, as measured by both RIBES [Isozaki et al. 2010] and BLEU [Papineni et al. 2002]. Our method is superior to phrase-based SMT, hierarchical phrase-based SMT, string-to-tree syntax-based SMT, an existing preordering method without a parser, and an existing preordering method that uses a source-language dependency parser.
The rest of this article is organized as follows: Section 2 describes the preordering framework and previous work; Section 3 provides an overview of the proposed method; Section 4 explains the training method; Section 5 describes the preordering method; Section 6 reports and discusses the experimental results; and Section 7 concludes the article. Machine translation is defined as the process in which a source-language sentence F is transformed into a target-language sentence E . During this process, word reordering is often necessary. More specifically, long-distance word reordering is necessary when translating between languages with widely different word orders.

The syntactic structure of F is useful for long-distance word reordering. Preordering is an SMT method in which the syntactic structure of F can be handled in a simple way. This is the approach that we take in this article. In the preordering approach, translation is performed as a two-step process, as shown in Figure 1. In the first part of the process, F is reordered to F , which is a source-language word sequence with almost the same word order as the target language. In the second part of the process, F is translated into E using an SMT method such as phrase-based SMT, which can produce accurate translations when only local reordering is required.

The preordering framework has been widely studied. In most preordering research, the reordering of words is conducted using reordering rules and the syntactic struc-ture of F that is obtained using a source-language syntactic parser. Reordering rules are produced automatically [Xia and McCord 2004; Li et al. 2007; Habash 2007; Dyer and Resnik 2010; Ge 2010; Genzel 2010; Visweswariah et al. 2010; Wu et al. 2011a, 2011b; Lerner and Petrov 2013] or manually [Collins et al. 2005; Wang et al. 2007; Ramanathan et al. 2008; Badr et al. 2009; Xu et al. 2009; Isozaki et al. 2012; Hoshino et al. 2013].

However, if a source-language syntactic parser is not available, then these methods cannot be applied. In such cases, preordering methods that do not require a parser are useful [Tromble and Eisner 2009; Visweswariah et al. 2011; DeNero and Uszkoreit 2011; Neubig et al. 2012; Khapra et al. 2013].

Methods that induce a parser deserve particular mention because they are similar to our approach. DeNero and Uszkoreit [2011] and Neubig et al. [2012] induce a nonsyn-tactic parser automatically using a parallel corpus with word alignments. The induced nonsyntactic parser is used to produce binary tree structures of input sentences. The input sentences are then preordered based on the binary tree structures and brack-eting transduction grammar (BTG) [Wu 1997]. The resulting binary tree structures are nonsyntactic structures. In contrast, our method utilizes syntactic structures for preordering via a target-language syntactic parser.

Compared with the nonsyntactic structures that are produced by a nonsyntactic parser based on BTG [Neubig et al. 2012], syntactic structures are thought to be supe-rior when making decisions about word reordering for the following two reasons.  X  In syntactic structures, a subtree span is expected to be consistent with the span of an expression that has cohesive meanings. For example, clauses are thought to be spans with cohesive meanings, and clauses are expressed by subtrees in syntactic structures. In contrast, in nonsyntactic structures produced by BTG, a subtree span is not always consistent with the span of an expression with cohesive meanings.  X  Syntactic structures are richer in terms of information than nonsyntactic structures produced by BTG. Syntactic structures have many phrase label types. In contrast, BTG has only one phrase label type.
 Therefore, syntactic structures are thought to be useful when performing word re-ordering for preordering methods.

Table I compares the necessity of syntactic parsers in existing preordering methods for source and target languages with that of the proposed preordering method. In some cases, a high-quality syntactic parser is not available for the source language, but a high-quality syntactic parser is available for the target language, while the source-language word order and the target-language word order are largely different, such as with SOV and SVO languages. Our method is applicable in these cases. In this section, we provide an overview of our preordering method.

Our preordering method processes syntactic structures using a target-language parser even when a source-language parser is not available. The syntactic structures of source-language sentences and the syntactic structures of the corresponding target-language sentences are expected to be similar in a parallel corpus [Hwa et al. 2005]. We used this expectation to produce syntactic constituent structures of source-language sentences that are similar to the syntactic constituent structures of the corresponding target-language sentences.

To effectively learn ITG or synchronous CFG [Aho and Ullman 1969], it is impor-tant that the level of synchrony between parallel syntactic structures is high. This is because ITG or synchronous CFG rules are learned from the synchronized parts of parallel syntactic structures, and such rules cannot be generated from nonsynchro-nized parts of parallel syntactic structures. That is, it is difficult to effectively learn ITG or synchronous CFG rules from parallel syntactic structures with a low level of synchrony.

Our proposed method facilitates the learning of ITG by producing highly synchro-nized parallel structures which are then used as training data for ITG, based on the following methods. (i) We produce source-language syntactic structures, which are maximally synchronized with the corresponding target-language syntactic structures, by cross-language syntactic projection. (ii) We select parallel sentences with highly synchronized parallel structures based on cross-language syntactic projection.
Figure 2 shows an overview of our method. Our preordering model is trained via the following steps: (1) parsing target-language sentences in the parallel training corpus using a syntactic (2) projecting the syntactic structures of the training target-language sentences (3) selecting parallel sentences with highly synchronized parallel structures based on (4) producing a probabilistic CFG model and a probabilistic model for unsupervised (5) parsing the training source-language sentences to produce full binary syntac-(6) learning the preordering model based on ITG using the full binary syntactic tree Input sentences are preordered by jointly parsing and identifying reordering using the ITG-based preordering model.

Our main contribution is an effective new framework for preordering using a target-language parser. Additionally, we propose a new parsing method for source languages that does not require a source-language parser or a source-language POS tagger.
Jiang et al. [2011] developed a method for projecting constituent structures between languages. There are two main differences between our method and theirs. One is the method for estimating CFG rule probabilities. They count the number of CFG rules appearing in tree candidates in each sentence for maximum likelihood estimation of CFG rule probabilities. In this process, they assume a uniform distribution over the projected tree candidates and then calculate the expected counts under this assump-tion. This looks like a single iteration of the EM algorithm. However, their assumption is incorrect. The expected counts of CFG rules in probable tree candidates should be larger than those of CFG rules in unlikely tree candidates. Our method solves this problem by simply engaging the Pitman-Yor process. The other difference between our method and that of Jiang et al. [2011] is in the requirements. Their method requires source-language POS tags that are produced by a POS tagger. In contrast, our method does not require source-language POS tags.
 In Section 4, we describe the training method of our preordering model in detail. In Section 5, we explain the methods for preordering input sentences and the training sentences. In this section, we will explain the five components of the training method for our preordering model, which is employed after the parsing of target-language training sentences is complete. Through word alignments, we project the binary syntactic constituent structures of the target-language sentences in the training parallel corpus onto the corresponding source-language sentences. Partial syntactic structures of the source-language sen-tences are then obtained. An example of this projection is shown in Figure 3.
The projection is conducted by (1) identifying the span in F corresponding to a subtree span in E through word alignments, and (2) adding the root phrase label of the subtree in E to the span in F .Aspanin F is the span from the leftmost position to the rightmost position of the source words that are aligned to the target word(s) in the subtree in E . The root phrase label of a projected subtree in E is added to the projected span in F . Note that if any nonaligned words are adjacent to the span in F , then there is a chance that these words should be contained in the span. That is, when there are nonaligned words adjacent to a span in F , there are ambiguities in the span. A pro-jected span that does not contain the adjacent nonaligned words, which is represented by a horizontal solid line in Figure 3, is called a minimal projected span . A phrase label is added to a minimal projected span. In Figure 3, a phrase label is not projected to the span covering kat ta because the corresponding target expression (bought) is a word instead of a phrase and does not have a phrase label.

To ensure that the projected structures can compose tree structures and consist solely of high-quality structures, we do not project any subtree spans in E when their corresponding spans in F conflict with one other. Here, the conflict is that two subtree spans that do not overlap in E do overlap, except for non-aligned words, when they are projected to F . That is, we only project the subtree spans of E whose corresponding spans of F are also continuous and do not conflict with one other. We choose to project none of the conflicted spans in E .
 We use the projected spans to select parallel sentences with highly synchronized par-allel structures. The selected sentences are then used to produce probabilistic mod-els for parsing and full binary tree structures. Let r 1 be the span coverage rate of the projected partial syntactic structures for a source-language sentence. The r each source-language sentence is calculated by dividing the number of projected spans by the number of words in the sentence minus one. 2 When r synchrony between parallel syntactic structures is high because the projected spans represent the parts that synchronize between the languages without conflict. The source-language training sentences are sorted based on r 1 unique source-language sentences with high r 1 as the data for the processing described in Sections 4.3 to 4.5. In our translation experiments in Section 6, we selected the top 0.1 million unique source-language sentences. The projected structures are usually partial structures. As full binary tree structures are required for learning our preordering model, we produce probabilistic models for parsing source-language sentences to produce full binary tree structures.
We will now discuss in detail our method for producing probabilistic models for pars-ing. The inputs are a source-language sentence F and projected partial syntactic struc-tures of F , described in Section 4.1. The following task characteristics enable the use of a simple model to produce full binary tree structures. (i) Partial structures are given. (ii) The set of phrase labels is predefined. We also predefine the number of types of POS tags. POS tags for each word are induced automatically.
We build a probabilistic context-free grammar (CFG) model for parsing source-language sentences. We use the Pitman-Yor process (PY) [Pitman and Yor 1997] build the model because it has a  X  X ich-get-richer X  property learning a model from partially annotated structures. This is because information con-tained in annotated structures can be used 6 to infer structures that are not annotated. We also build a probabilistic model for unsupervised POS tagging using the Pitman-Yor process.
 A probabilistic CFG is defined by the 4-tuple G = ( F , V , S , of terminals, which are source-language words in the training data, V is the set of nonterminals, S  X  V is a designated start symbol, and R is a set of rules. A CFG rule x  X   X   X  R used in this process consists of x  X  V and  X  , which consists of two elements of V . V is defined as V = L  X  T , where L is the set of phrase labels for the target-language syntax, T ={ 1,2,..., | T |} is the set of source-language POS tags represented by numbers, where | T | is the number of POS tag types, and Let f  X  F be a source-language word and F = f 1 f 2 ... f tree D is defined as the product of the probabilities of its component CFG rules and the probabilities of words given their POS tags, as follows.
 where c ( x  X   X  , D ) is the number of times x  X   X  is used for the derivation D , P ( X  the probability of generating  X  given its root phrase label x , t i of t indicates the position in F ,and P ( f | t ) is the probability of generating f given its POS tag t . The designated phrase label, S , is used for the phrase label of the root node of a tree.

Our PY models represent probability distributions over CFG rules or source-language words, as follows.
 where d cfg ,  X  cfg , d tag ,and  X  tag are hyperparameters for the PY models. The hy-perparameters are optimized via the auxiliary variable technique [Teh 2006]. The backoff probability distributions, P base ( X  | x ) and P follows.
 where | V | is the number of nonterminal types and | F | is the lexicon size of the source-language words in the training data. As our CFG rule has two leaf nodes, the number of pair nonterminal node types is | V | 2 .

Sampling for building the distributions is conducted according to Equation (1) with the following constraints. When projected spans are present, we constrain the sam-pling such that only the derivation trees that do not conflict with the projected spans are sampled. Here, the conflict is that both a subtree span in the tree derivation and a projected span partially overlap each other. When there is an ambiguity in a pro-jected span, which comprises the minimal projected span and any number of adjacent unaligned word(s), the constraints are as follows. If a sample (a subtree span in the tree derivation) does not conflict with the minimal projected span, then the minimal projected span is used as the constraint for the sample. Otherwise, the whole span (the minimal projected span and its adjacent unaligned word(s)) is used as the constraint for the sample. When the projected phrase label for a subtree span in a derivation tree is present, we constrain the sampling such that only the projected phrase label is sampled.

We use a sentence-level blocked Gibbs sampler based on a dynamic programming al-gorithm [Johnson et al. 2007]. The sampler consists of the following two steps: for each sentence, (1) inside probabilities [Lari and Young 1991] are calculated from the bottom up using the CYK algorithm, and (2) a tree is sampled from the top down according to the inside probabilities for each CFG rule. In the first step, when we calculate the inside probabilities for each phrase label in each cell of the triangular table of the CYK algorithm and save the inside probabilities, we also save the inside probabilities for each CFG rule. In the second step, we sample a CFG rule according to the inside prob-abilities for the CFG rules in each cell from the top down. To reduce computational costs, we only use N-best POS tags for each word when the inside probabilities are calculated. In our experiment in Section 6, we used five-best POS tags for each word.
The computational complexity of producing probabilistic models for parsing is lin-early proportional to the number of training sentences when the data properties are identical except for the data size. The computational cost depends on the amount of nonconstrained parts in the data. When the amount of nonconstrained parts becomes larger, the computational cost becomes larger. After the probability distributions of the PY models are built, we parse the source-language sentences to produce full binary tree structures that are maximally synchro-nized with the corresponding target-language structures. The parsing complements deficiencies and resolves ambiguities in the projected partial structures. The deficien-cies are insufficient spans or phrase labels in the projected spans and labels to con-struct a full binary tree structure. The ambiguities of spans are shown as horizontal dotted lines in Figure 3, which cover nonaligned words adjacent to minimal projected spans. We calculate the maximum likelihood full binary tree structures based on the CYK algorithm within the constraints of the minimal projected spans and their phrase labels, using the produced probabilistic CFG model and the produced probabilistic model for unsupervised POS tagging. The probability for a derivation tree is calculated using Equation (1). The constraints are the same constraints used for sampling when building the probabilistic models. The resulting full binary tree structures comprise the phrase labels of the target-language syntax. An example of the production of a full binary tree structure is shown in Figure 3.

Note that when the full binary trees are generated, all of the minimal projected spans are not necessarily included in the full binary trees if the projected spans have ambiguities. If nonaligned words are located adjacent to the projected spans, then there may be cases in which some minimal projected spans are not included in the full binary tree. For example, when a minimal projected span is ( f nonaligned word where parentheses denote a span, a full binary tree may be ( f This tree does not include the minimal projected span because there are ambiguities in the span when a nonaligned word is adjacent to the span, as explained in Section 4.1. Learning of our preordering model uses the full binary tree structures of source-language sentences and word alignments.

The preordering model is a model based on two fundamental frameworks [Goto et al. 2013b]: (i) parsing using probabilistic CFG and (ii) the inversion transduction gram-mar (ITG) [Wu 1997]. In this article, the model combining (i) and (ii) is called the ITG parsing model and parsing using ITG is called ITG parsing . We use the ITG parsing model for preordering while Goto et al. [2013b] used this model for postordering.
To obtain the training data for the preordering model, we first obtain the reordered structure that produces the word order of F most similar to the word order of the corresponding E using their word alignments. Figure 4 shows an example of the tree structure of F calculated from the tree structure of F and word alignments. Reordering is conducted by swapping child nodes in the binary tree structure of F so that Kendall X  X   X  is maximized between F and E . 8 For each node, we decide whether its child nodes are swapped or not. This decision is made deterministically from the bottom up. The algorithm of this maximization can be expressed as O ( m 2 m is a sentence length. This is because the computational complexity of Kendall X  X   X  can be expressed as O ( m log m ) [Knight 1966] for each node in a binary tree. When the scores for candidates are the same 9 , we retain the original order.

The nodes whose child nodes are swapped to transform F into F are then annotated with an  X  SW X  suffix (indicating  X  X wap/inversion X ), and other nodes with two child nodes are annotated with an  X  ST X  suffix (indicating  X  X traight X ) in the binary tree for F . Figure 5 shows an example of F and its binary tree structure annotated with the
ST and SW suffixes. The resulting binary tree syntactic structure of F is augmented with straight or swap/inversion suffixes, which can be regarded as a derivation of ITG between F and F .

Thus, an ITG model can be learned from the binary tree structures using a proba-bilistic CFG learning algorithm. This learned model is the ITG parsing model. In this study, we use the state split probabilistic CFG [Petrov et al. 2006] for learning the ITG parsing model. The learned ITG parsing model is our preordering model. This section explains how to preorder input sentences and training sentences. Input sentences are preordered using the ITG parsing model described in Section 4.5. The preordering process is shown in Figure 6. An input sentence F is parsed using the ITG parsing model. 10 When F is parsed, the reordered structure for F is jointly identified based on ITG. Each nonterminal node of a phrase label in the tree derivation is augmented by either an  X  ST X  suffix or an  X  SW X  suffix. The word order for F is determined by the binary tree derivation with the suffixes of the nonterminal nodes. We swap the child nodes of the nodes augmented with the  X  SW X  suffix in the binary tree derivation to produce F . After transforming the F of an input sentence into F , we use a phrase-based SMT to translate F into E . Therefore, a phrase-based SMT requires parallel F and E sen-tences to train its translation model. Now, we will explain how to produce F for the parallel sentences for training the SMT translation model.

If F in the training data is produced using the same method as for preordering input sentences, then the word order of F in the training data will be consistent with the word order of the preordered input sentences. However, the method for preordering input sentences is not always the best method for preordering the training data. This is because a corresponding E already exists in the training data. Thus, we also have to consider the consistency between F and E in the training data. Methods that do not consider the consistency between F and E will not be optimal.

It is important to consider the consistency between F and E . The objective of pre-ordering the training sentences is to build a phrase table. The phrase table is the SMT translation model, which consists of parallel phrase pairs between F and E and their probabilities. When a pair of corresponding expressions in E and F are both contin-uous, they can be extracted as a parallel phrase pair. A span in F that is projected by the method described in Section 4.1 indicates that the span in F and its corre-sponding span in E are both continuous. If the projected span in F is transformed into a noncontinuous expression in F by preordering, then a parallel phrase pair for the noncontinuous expression in F and the corresponding expression in E cannot be ex-tracted as a phrase pair. Therefore, it is optimal that F be reordered into F under the condition that this problem can be avoided, using (to the maximum possible extent) the same method used for preordering input sentences.

Thus, we preorder F in the training data into F as follows. Partial syntactic struc-tures are first projected onto the source-language sentences in the training data using the method described in Section 4.1. The source-language sentences are then parsed and reordered using the ITG parsing model (described in Section 5.1) within the con-straints of the projected spans. The constraining method is the same as that used in Section 4.4. Our main goal is to translate text between two languages with widely different word orders, such as a SOV and SVO, with a high-quality target-language parser. Therefore, we conduct a Japanese-to-English (JE) translation to test the quality of translation from an SOV language to an SVO language. In addition, we conduct a Chinese-to-English (CE) translation to test the quality of translation from an SVO language to another SVO language, as Chinese and English are more similar in terms of word order than Japanese and English. We investigate the efficacy of our method by comparing it with other methods. We used the patent data from the NTCIR-9 and NTCIR-10 Patent Machine Translation Tasks [Goto et al. 2011; Goto et al. 2013a] for the experiment. The training data and the development data from the NTCIR-9 and NTCIR-10 are the same, but the test data are different. The JE training data consists of approximately 3.18 million sentence pairs and the CE training data consists of 1 million sentence pairs. The development data consists of 2,000 sentence pairs. There are 2,000 test sentences for the NTCIR-9 and 2,300 for the NTCIR-10. The reference data for each test sentence is a single reference translation.

We use Enju [Miyao and Tsujii 2008], which outputs head-binarized trees, to parse the English sentences in the training data. We applied a parsing customization for patent sentences [Isozaki et al. 2012]. MeCab 11 is used for Japanese segmentation, and the Stanford segmenter 12 is used for Chinese segmentation. We adjust the tok-enization of alphanumeric characters in Japanese to be the same as for English.
The translation model is trained using sentences that are 40 words or less in length and English side sentences that could be parsed to produce binary syntactic tree struc-tures. Approximately 2.06 million sentence pairs are used to train the translation model for JE. Approximately 0.40 million sentence pairs are used to train the transla-tion model for CE. GIZA++ and grow-diag-final-and heuristics are used to obtain word alignments. To reduce word alignment errors, we remove the articles English and the particles { ga , wo , wa } in Japanese before performing word alignments, because these function words do not have corresponding words in the other languages between Japanese X  X nglish or Chinese X  X nglish. After word alignment, we restore the words that had been removed and shift the word alignment positions to the original word positions.
 We use 5-gram language models with modified Kneser-Ney discounting [Chen and Goodman 1998] using the SRILM toolkit [Stolcke et al. 2011]. The language models are trained using the English sentences from the bilingual training data.
The SMT weighting parameters are tuned via MERT [Och 2003] using the devel-opment data. To stabilize the MERT results, we tune the parameters three times via MERT using the first half of the development data. We then select the SMT weighting parameter set that performs the best on the second half of the development data based on the BLEU scores from the three SMT weighting parameter sets. Next, we describe how the proposed method (P ROPOSED ) was performed. As the train-ing data for our preordering model, we produce source-language full binary syntactic tree structures for 0.1 million source-language training sentences, which are selected using the method described in Section 4.2. 13 To produce the probabilistic CFG-model and the probabilistic model for unsupervised POS tagging, we use the Gibbs sam-pler for 100 iterations. 14 We use | T |= 50, which employs the same number of word classes used in the Moses default setting, where | T | is the number of POS tag types. The Berkeley parser [Petrov et al. 2006], which is an implementation of the state split probabilistic CFG-based parser, is used to train our preordering model and to parse using the preordering model. We perform six split-merge iterations as the same iter-ation of the parsing model for English [Petrov et al. 2006]. We use the phrase-based SMT system Moses [Koehn et al. 2007] to translate from F into E with a distortion limit of 6, which limits the moves of phrases for word reordering to six or less words. We used the following six comparison methods.  X  Phrase-based SMT with lexicalized reordering models (P BMT  X  Hierarchical phrase-based SMT (H PBMT ) [Chiang 2007].  X  String-to-tree syntax-based SMT (S BMT ) [Hoang et al. 2009].  X  Phrase-based SMT with a distortion model (P BMT D ) [Goto et al. 2014].  X  Preordering using a source-language dependency parser (S  X  Preordering without using a parser (L ADER ) [Neubig et al. 2012]. We use Moses [Hoang et al. 2009; Koehn et al. 2007] for P S
RCDEP ,andL ADER . We use an in-house standard phrase-based SMT decoder com-patible with the Moses decoder with a distortion model [Goto et al. 2014] for P
For P BMT L , we use the MSD bidirectional lexicalized reordering models [Koehn et al. 2005], which are built using all of the data used to build the translation model.
The distortion models for PBMT D are trained using 0.2 million source-language sentences 17 from the data used to build the translation model. This setting is the same as that in the experiments by Goto et al. [2014]. For PBMT POS tags produced by MeCab for Japanese and the Stanford tagger
S RCDEP requires a source-language dependency parser. We use CaboCha [Kudo and Matsumoto 2002] and POS tags produced by MeCab to obtain Japanese dependency structures 20 and use the Stanford parser 21 and POS tags produced by the Stanford tagger to obtain Stanford dependencies for Chinese [Chang et al. 2009]. Note that there are publicly available Japanese dependency parsers but there are no pub-licly available Japanese constituency parsers. The preordering rules of S built using all of the data used to build the translation model.
The preordering models for L ADER are trained using the same 0.1 million source-language sentences and their word alignments as the training data for the preordering models of P ROPOSED . We use source-language word classes produced by the Moses toolkit. Note that while the L ADER preordering method does not use a parser, the training data for L ADER is selected using a target-language parser. We perform 100 iterations to train the L ADER preordering model. 22
For P BMT L , we use distortion limits of 4 or 20 for JE translation and distortion limits of 4 or 10 for CE translation, because a limit of 20 is the best for JE translation and a limit of 10 is the best for CE translation among 10, 20, 30, and by Goto et al. [2014] and because Genzel [2010] uses a baseline phrase-based SMT capable of local reordering of up to 4 words. To distinguish between the distortion limits for P BMT L , we indicate the distortion limit as a subscript of P P BMT L -20 for a distortion limit of 20. For P BMT D , a distortion limit of 20 is used for JE translation and a distortion limit of 10 is used for CE translation. An unlimited max-chart-span is used for H PBMT and S BMT and a distortion limit of 6 is used for the preordering methods of S RCDEP and L ADER . The default values are used for the other system parameters. We evaluate the translation quality based on the case-insensitive automatic evalua-tion scores from the BLEU-4 [Papineni et al. 2002] and RIBES v1.01 [Isozaki et al. 2010]. RIBES is an automatic evaluation measure based on word-order correlation co-efficients between reference sentences and translation outputs. Our main results for JE translation are presented in Table II and those for CE translation are presented in Table III. In these tables, check marks indicate usage for that method. Bold numbers indicate that values are not significantly lower than the best result (i.e., nonbold num-bers indicate values that are significantly lower than the best result) in each test set and in each evaluation measure. 23 To assess this, we used the bootstrap resampling test at a significance level of  X  = 0.01 [Koehn 2004].

P ROPOSED achieved the best scores for both RIBES and BLEU in both the NTCIR-9 and NTCIR-10 datasets, and for both JE and CE translation. Because RIBES is sensitive to global word order and BLEU is sensitive to local word order, this confirms the efficacy of P ROPOSED for both global and local word ordering.

Now, we compare the effects of the differences in the approaches. First, we com-pare our method with three existing methods that do not use a parser and conduct word selection and reordering jointly. For both the NTCIR-9 and NTCIR-10 results for JE translation, the RIBES and BLEU scores for P ROPOSED for the standard phrase-based SMT (P BMT L -20 ), the hierarchical phrase-based SMT (H PBMT ), and the phrase-based SMT with a recent distortion model (P results confirm that preordering is effective compared with these methods, which do not use a parser and conduct word selection and reordering simultaneously, for JE patent translation. The tendencies of the CE translation results are the same as those of the JE translation results. These results confirm that preordering is also effective for CE patent translation.

Next, we compare our method with an existing method that uses a target-language syntactic parser, S BMT . The required resources are the same as those for P For both the NTCIR-9 and NTCIR-10 results for JE translation, the RIBES and BLEU scores for P ROPOSED are higher than those for the string-to-tree syntax-based SMT (S
BMT ). These results confirm that preordering is effective compared with a method that uses target-language syntactic structures and conducts word selection and re-ordering simultaneously for JE patent translation. The tendencies of the CE transla-tion results are also the same as those of the JE translation results.

We then compare our method with an existing method using a source-language de-pendency parser, S RCDEP . For both the NTCIR-9 and NTCIR-10 results for JE trans-lation, the RIBES and BLEU scores for P ROPOSED are higher than those for S These results confirm that our method is effective compared with a method that uses a source-language dependency parser [Genzel 2010] for JE patent translation. The tendencies of the CE translation results are the same as those of the JE translation results.

We confirm the effects of S RCDEP for JE (i.e., SOV to SVO) translation. produces BLEU scores that are about two BLEU points higher than those for P These results are consistent with the experimental results of Genzel [2010]. Genzel [2010] compares their method with their baseline phrase-based SMT, which is capa-ble of local reordering of up to four words. Although S RCDEP scores than those for P BMT L -4 and better RIBES scores than those for P P BMT L -20 , the BLEU scores for S RCDEP are lower than those for P dicates that even if a source-language dependency parser is used, it is not easy to improve JE translation quality by preordering. 25 One of the reasons that S unable to achieve scores on par with P ROPOSED is thought to be because when S changes the order of child nodes, the reordering rules consider only the local informa-tion. Reordering, however, should consider sentence-level consistency. For example, an SOV sentence in Japanese should be reordered into an SVO sentence for JE transla-tion. However, when the subject in a sentence is omitted in Japanese, an OV sentence in Japanese should not be reordered into a VO sentence. This is because such sentences are usually translated into sentences in the passive voice, and the objects in Japanese become subjects in the translated sentences. Because S RCDEP consider local information, a rule is unable to handle the difference between SOV and OV when the rule does not consider S, such as when swapping O and V. In contrast, P ROPOSED considers sentence-level consistency.

Finally, we compare our method with an existing preordering method that does not use a syntactic parser, L ADER . For both the NTCIR-9 and NTCIR-10 results for JE translation, the RIBES and BLEU scores for P ROPOSED are higher than those for L
ADER . 26 These results confirm that syntactic structures are effective for preordering in JE patent translation. 27 The tendencies of the CE translation results are also the same as those of the JE translation results.

Here, we confirm the effects of our method for selecting the training sentences de-scribed in Section 4.2 for JE translation. Because the sentence selection method is an indispensable element of our method, we compare L ADER with our sentence selection method, which is L ADER in Table II, to L ADER without our sentence selection method. We used 0.1 million source-language sentences from the training data as the train-ing data for the preordering model of L ADER without our sentence selection method. The size of the 0.1 million sentences is the same as the size of the training data for the preordering model of L ADER in Table II. The results are shown in Table IV. The RIBES and BLEU scores for L ADER with our sentence selection method are higher than those for L ADER without our sentence selection method. This comparison confirms the effi-cacy of our sentence selection method. This result also confirms that the learning of BTG from parallel sentences with highly synchronized parallel structures is effective compared with the learning from parallel sentences with less synchronized parallel structures.

Through the process described in Section 4.1, we assess the percentage of the spans in the source language that do not conflict in all of the spans that could be projected from the target language. As we explain in Section 4.1, to ensure that the projected structures can compose tree structures and consist solely of high-quality structures, we do not project any subtree spans in E when their corresponding spans in F conflict with one other. The nonconflict span rate in the source language is calculated by dividing the number of projected nonconflict spans in the source language by the number of spans in the source language that could be projected without consideration of conflict. The nonconflict span rates for the data selected to build the translation model are 0.782 for Japanese and 0.747 for Chinese.

We also check the average coverage rates of the projected spans, except for the sen-tence root spans, 28 in the process described in Section 4.1. Let r of the projected spans, except for the root span, for a source-language sentence. r is calculated by dividing the number of projected spans, except for the sentence root span, by the number of words in a sentence minus two. 29,30 selected to build the translation model (2.06 million Japanese sentences and 0.40 mil-lion Chinese sentences) is 0.560 for Japanese and 0.601 for Chinese. The average r for the 0.1 million sentences selected via our sentence selection method (described in Section 4.2) is 0.856 for Japanese and 0.828 for Chinese. structures, full binary tree structures were produced using the methods described in Sections 4.3, 4.4, and 5.2. 32 In these experiments, we have not compared our method with postordering methods. However, for the same NTCIR-9 test data, the RIBES score (76.35) and the BLEU score (33.83) for P ROPOSED are higher than the RIBES score (75.12) and the BLEU score (32.95) reported in Goto et al. [2013b], which were calculated in the same way as ours, for a postordering method [Goto et al. 2013b]. The postordering method of Goto et al. [2013b] used the same state split probabilistic CFG method for the ITG parsing model as our method for the ITG parsing model. In addition, P advantage over the postordering methods of Sudoh et al. [2011b], Goto et al. [2013b], and Hayashi et al. [2013]. These postordering methods use manually-defined, high-quality preordering rules of head-finalization for translation from English to Japanese [Isozaki et al. 2012], so it is not easy to apply these methods to other language pairs. In contrast, P ROPOSED does not require such manually-defined rules, and thus can be applied to other languages. To investigate the effects of our projection method, we compare the parsing quality produced by our method with that produced by the method of Jiang et al. [2011]. We use the same data and evaluation method as Jiang et al. [2011]. We use the same FBIS Chinese X  X nglish parallel corpus (LDC2003E14), which consists of 0.24 million sentence pairs, to obtain projected constituent structures. We evaluate our projected parser using the same test data as the subset of Chinese Treebank 5.0 (CTB 5.0; LDC2005T01), which consists of no more than 40 words after the removal of punc-tuation, just as in Jiang et al. [2011].

We use the same evaluation metric of unlabeled F 1 as Jiang et al. [2011], which is the harmonic mean of the unlabeled precision and recall. This is defined by Klein [2005, pp. 19 X 22]. The evaluation for unlabeled brackets differs slightly from the stan-dard PARSEVAL metrics: multiplicity of brackets is ignored, brackets with a span of one are ignored, and bracket labels are ignored. Previous research [Jiang et al. 2011; Klein 2005, p. 16] removed punctuation before conducting the evaluations. Followed this, we remove words that have PU punctuation tags in CTB 5.0 after parsing.
We use our method, described in Sections 4.1 to 4.4, 6.1, and 6.2 to obtain projected constituent structures. As a result of the process described in Section 4.1, the noncon-flict span rate in the source language for the training data of the FBIS corpus is 0.681. To reduce computational costs, we change one of the settings described in Section 6.2. To produce a probabilistic CFG model and full binary trees, we select the top 50,000 unique source-language sentences from the FBIS corpus, whereas we selected the top 0.1 million unique source-language sentences in Section 6.2. rate ( r 2 ) of the projected spans, except for the sentence root spans, for the 50,000 sen-tences selected to produce full binary tree structures is 0.795. We use the Berkeley parser, which was also used by Jiang et al. [2011] for the same purpose, to build the parsing model from the projected constituent structures and to parse the test data. Jiang et al. [2011] uses the gold POS tags from CTB 5.0 for parsing and a supervised Chinese POS tagger for tagging the FBIS corpus. In contrast, we do not use the gold POS tags from CTB 5.0 or a supervised Chinese POS tagger. Therefore, a comparison of our method with that of Jiang et al. [2011] would be unfair.

The evaluation results are given in Table V. Although our method does not require source-language POS tags, our method produced an F 1 higher than that of Jiang et al. [2011]. This confirms the efficacy of our projection method.

The quality of projected spans is affected by the quality of word alignments between languages as well as the quality of target-language parse trees. To check the quality of spans projected by the method described in Section 4.1, we use the English translation with annotations 34 (LDC2007T02) of a part of CTB 5.0 and the corresponding Chinese sentences in CTB 5.0. From the data, we extract parallel sentence pairs that have: (1) a one-to-one sentence correspondence, (2) sentence lengths of 40 words or less, and (3) English side sentences that could be parsed to produce binary syntactic tree struc-tures. The extracted Chinese X  X nglish parallel sentence pairs are called parallel CTB in this article. The parallel CTB comprise 2,477 sentence pairs. We obtain word align-ments using the parallel CTB and the FBIS corpus simultaneously. Then, we project spans using our method described in Section 4.1. We check the quality of the minimal projected spans for the parallel CTB. Error spans, which are also called cross brackets, are the projected spans that conflict with subtree spans of CTB 5.0. The conflict is that a projected span and a subtree span in the syntactic tree of CTB 5.0 for a sentence partially overlap each other. The error rate of the minimal projected spans is 0.217, which is calculated by dividing the number of error spans by the number of projected spans, except for the root spans. 35 For the Chinese sentences from the parallel CTB, the nonconflict span rate is 0.689, and the average coverage rate ( r spans, except for the root spans, is 0.608. We have presented a preordering method that uses a target-language parser to pro-cess syntactic structures without a source-language parser. This is achieved by pro-jecting source-language syntactic structures from the corresponding target-language constituency structures, as well as learning an ITG-based parsing model for the source-language using the projected syntactic structures. Our method, which is based on cross-language syntactic projection and sentence selection, facilitates the learning of ITG by producing highly-synchronized parallel syntactic structures. In the experi-ments on Japanese-to-English and Chinese-to-English patent translation, our method is significantly better in terms of translation quality as measured by both RIBES and BLEU when compared with phrase-based SMT, hierarchical phrase-based SMT, string-to-tree syntax-based SMT, an existing preordering method that does not use a parser, and an existing preordering method that uses a source-language dependency parser. As RIBES is sensitive to global word order and BLEU is sensitive to local word or-der, we conclude that our proposed method is better than the compared methods in terms of global and local word ordering. We also confirm the efficacy of our projection method compared with an existing projection method for constituent structures using the FBIS corpus and Chinese Treebank 5.0. Future work will involve cooperating with a source-language parser when one is available.

