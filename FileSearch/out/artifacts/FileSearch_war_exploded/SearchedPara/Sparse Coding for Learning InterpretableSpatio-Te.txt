 In recent years sparse coding has become a popular paradigm to learn dictionaries of natural images [10, 1, 4]. The learned representations have proven very effective in computer vision tasks such as image denoising [4], inpainting [10, 8] and object recognition [1]. In these approaches, sparse coding was formulated as the sum of a data fitting term, typically the Frobenius norm, and a regularization term that imposes sparsity. The ` 1 norm is typically used as it is convex instead of other sparsity penalties such as the ` 0 pseudo-norm.
 However, the sparsity induced by these norms is local; The estimated representations are sparse in that most of the activations are zero, but the sparsity has no structure, i.e., there is no preference to which coefficients are active. Mairal et al. [9] extend the sparse coding formulation of natural images to impose structure by first clustering the set of image patches and then learning a dictionary where members of the same cluster are encouraged to share sparsity patterns. In particular, they use group norms so that the sparsity patterns are shared within a group.
 Here we are interested in the problem of learning dictionaries of human motion. Learning spatio-temporal representations of motion has been addressed in the neuroscience and motor control liter-ature, in the context of motor synergies [13, 5, 14]. However, most approaches have focussed on learning static primitives, such as those obtained by linear subspace models applied to individual frames of motion [12, 15].
 One notable exception to this is the work of diAvella et al. [3] where the goal was to recover primi-tives from time series of EMG signals recorded from a set of frog muscles. Using matching pursuit [11] and an ` 0 -type regularization as the underlying mechanism to learn primitives, [3] performed matrix factorization of the time series. The recovered factors represent the primitive dictionary and the primitive activations. However, this technique suffers from the inherent limitations of the ` 0 regularization which is combinatorial in nature and thus difficult to optimize; therefore [3] resorted to a greedy algorithm that is subject to the inherent limitations of such an approach.
 In this paper we propose to extend the sparse coding framework to learn motion dictionaries. In particular, we cast the problem of learning spatio-temporal primitives as a tensor factorization prob-lem and introduce tensor group norms over the primitives that encourage sparsity in order to learn the number of elements in the dictionary. The introduction of additional diagonal constraints in the activations, as well as smoothness constraints that are inherent to human motion, will allow us to learn interpretable representations of human motion from motion capture data. As demonstrated in our experiments, our approach outperforms state-of-the-art matching pursuit [3], as well as recently developed sparse coding algorithms [7]. In this section we first review the framework of sparse coding, and then show how to extend this framework to learn interpretable dictionaries of human motion. 2.1 Traditional sparse coding Let Y = [ y 1 ,  X  X  X  , y N ] be the matrix formed by concatenating the set of training examples drawn i.i.d. from p ( y ) . Sparse coding is usually formulated as a matrix factorization problem composed of a data fitting term, typically the Frobenius norm, and a regularizer that encourages sparsity of the activations or equivalently where  X  and  X  sparse are parameters of the model. Additional bounding constraints on W are typi-cally employed since there is an ambiguity on the scaling of W and H . In this formulation W is the dictionary, with w i the dictionary elements, H is the matrix of activations, and  X  ( H ) is a regularizer that induces sparsity. Solving this problem involves a non-convex optimization. However, solving with respect to W and H alone is convex if  X  is a convex function of H . As a consequence,  X  is usually taken to be the ` 1 norm, i.e.,  X  ( H ) = P i,j | h i,j | , and an alternate minimization scheme is typically employed [7].
 If the problem has more structure, one would like to use this structure in order to learn non-local sparsity patterns. Mairal et al. [9] exploit group norm sparsity priors to learn dictionaries of natural images by first clustering the training image patches, and then learning a dictionary where members of the same cluster are encouraged to share sparsity patterns. In particular, they use the ` 2 , 1 norm defined as  X  ( H ) = P k || h k || 2 , where h k are the elements of H that are members of the k -th group. Note that the members of a group do not need to be rows or columns, more complex group structures can be employed [6].
 However, the structure imposed by these group norms is not sufficient for learning interpretable motion primitives. We now show how in the case of motion, we can consider the activations and the primitives as tensors and impose group norm sparsity on the tensors. Moreover, we impose additional constraints such as continuity and differentiability that are inherent of human motion data, as well as diagonal constraints that ensure interpretability. 2.2 Motion dictionary learning Let Y  X  &lt; D  X  L be a D dimensional signal of temporal length L . We formulate the problem of learning dictionaries of human motion as a tensor factorization problem where the matrix W is now a tensor, W  X  &lt; D  X  P  X  Q , encoding temporal and spatial information, with D the dimensionality of the observations, P the number of primitives, and Q the length of the primitives. H is now also defined as a tensor, H  X  &lt; Q  X  P  X  L , with L the temporal length of the sequence. For simplicity in the discussion we assume that the primitives have the same length. This restriction can be easily re-moved by setting Q to be the maximum length of the primitives and padding the remaining elements to zero. We thus define the data term to be Figure 1: Walking dataset composed of multiple walking cycles performed by the same subject. (left, center) Projection of the data onto the first two principal components of walking. This is the data to be recovered. (right) Training error as a function of the number of iterations. Note that our approach converges after only a few iterations where vec ( W )  X  &lt; D  X  P Q and vec ( H )  X  &lt; QP  X  L are projections of the tensors to be represented as matrices, i.e., flattening.
 When learning dictionaries of human motion, there is additional structure and constraints that one would like the dictionary elements to satisfy. One important property of human motion is that it is smooth. We impose continuity and differentiability constraints by adding a regularization term that encourages smooth curvature, i.e.,  X  ( W ) = P P p =1 || X  2 W p, : , : || F .
 One of the main difficulties with learning motion dictionaries is that the dictionary words might have very different temporal lengths. Note that this problem does not arise in traditional dictionary learning of natural images, since the size of the dictionary words is manually specified [4, 1, 9]. This makes the learning problem more complex since one would like to identify not only the number of elements in the dictionary, but also the size of each dictionary word. We address this problem by adding a regularization term that prefers dictionaries with small number of primitives, as well as primitives of short length. In particular, we extend the group norms over matrices to be group norms over tensors and define where W i,j,k is the k -th dimension at the j -th time frame of the i -th primitive in W . We will also like to impose additional constraints on the activations H . For interpretability, we would like to have only positive activations. Moreover, since the problem is under-constrained, i.e., H and W can be recovered up to an invertible transformation WH = ( WC  X  1 )( CH ) , we impose that the elements of the activation tensor should be in the unit interval, i.e., H i,j,k  X  [0 , 1] . As in traditional sparse coding, we encourage the activations to be sparse. We impose this by bounding the L 1 norm. Finally, to impose interpretability of the results as spatio-temporal primitives, we impose that when a spatio-temporal primitive is active, it should be active across all its time-length with constant activation strength, i.e.,  X  i,j,k, H i,j,k = H i,j +1 ,k +1 .
 We thus formulate the problem of learning motion dictionaries as the one of solving the following optimization problem where  X  train ,  X  and  X  are parameters of our model.
 When optimizing over W or H alone the problem is convex. We thus perform alternate minimizatio. Our algorithm converges to a local minimum, the proof is similar to the convergence proof of block coordinate descent, see Prop. 2.7.1 in [2]. Figure 2: Estimation of W and H when the number of primitives is unknown, using (top) matching pursuit without refractory period, (second row) matching pursuit with refractory period [3], (third row) traditional sparse coding and (bottom) our approach. Note that our approach is able to recover the primitives, their number and the correct activations. Matching pursuit is able to recover the number of primitives when using refractory period, however the activations and the primitives are not correct. When we do not use the refractory period, the recovered primitives are very noisy. Sparse coding has a low reconstruction error, but neither the number of primitives, nor the primitives and the activations are correctly recovered. We compare our algorithm to two state-of-the-art approaches in the task of discovering interpretable primitives from motion capture data, namely, the sparse coding approach of [7] and matching pursuit [3]. In the following, we first describe the baselines in detail. We then demonstrate our method X  X  ability to estimate the primitives, their number, as well as the activation patterns. We then show that our approach outperforms matching pursuit and sparse coding when learning dictionaries of walking and running motions. For all experiments we set  X  train = 1 ,  X  test = 1 . 3 ,  X  = 1 and  X  = 0 . 05 and use the ` 2 , 1 , 1 norm. Note that similar results where obtained with the ` 2 , 2 , 1 norm. For SC we use  X  = 0 . 01 and c is set to the maximum value of the ` 2 norm. The threshold for MP with refractory period is set to 0 . 1 . (walk,  X  2 = 50 , e 59 D ) (walk,  X  2 = 100 , e 59 D ) (walk,  X  2 = 50 , e P CA ) (walk,  X  2 = 100 , e P CA ) (run,  X  2 = 50 , e 59 D ) (run,  X  2 = 100 , e 59 D ) (run,  X  2 = 50 , e P CA ) (run,  X  2 = 100 , e P CA ) Figure 3: Error as a function of the dimension when adding Gaussian noise of variance 50 and 100 . (Top) Walking, (bottom) running.
 Matching pursuit (MP): We follow a similar approach to [3] where an alternate minimization over W and H is employed. For each iteration in the alternate minimization, W is optimized by minimizing ` data defined in Eq. (2) until convergence. For each iteration in the optimization of H , an over-complete dictionary D is created by taking the primitives in W , and generating candidates by shifting each primitive in time. Note that the cardinality of the candidate dictionary is |D| = P ( L + Q  X  1) if W has P primitives and the data is composed of L frames. Once the dictionary is created, a set of primitives is iteratively selected (one at a time) by choosing at each iteration the primitive with the largest scalar product with respect to the residual signal that cannot be explained with the already selected primitives. Primitives are chosen until a threshold on the scalar product is reached. Note that this is an instance of Matching Pursuit [11], a greedy algorithm to solve an ` 0 -type optimization. Additionally, in the step of choosing elements in the dictionary, [3] introduced the refractory period, which means that when one element in the dictionary is chosen, all overlapping elements are removed from the dictionary. This is done to avoid multiple activations of primitives. In our experiments we compare our approach to matching pursuit with and without refractory period.
 Sparse coding (SC): We use the sparse coding formulation of [7] which minimizes the Frobenius norm with an L 1 regularization penalty on the activations with  X  a constant trading off the relative influence of the data fitting term and the regularizer, and c a constant bounding the value of the primitives. Note that now  X  W and  X  H are matrices. Following [7], we solve this optimization problem alternating between solving with respect to the primitives  X  W and the activations  X  H . 3.1 Estimating the number of primitives In the first experiment we demonstrate the ability of our approach to infer the number of primitives as well as the length of the existing primitives. For this purpose we created a simple dataset which is composed of a single sequence of multiple walking cycles performed by the same subject from the CMU mocap dataset 1 . We apply PCA to the data reducing the dimensionality of the observations Figure 4: Error as a function of the Gaussian noise variance for 4D and 10D spaces learned from a dataset composed of a single subject. (Top) walking, (bottom) running. from 59D to 2D for each time instant. Fig. 1 depicts the projections of the data onto the first two principal components as a function of time. In this case it is easy to see that since the motion is periodic, the signal could be represented by a single 2D primitive whose length is equal to the length of the period.
 To perform the experiments we initialize our approach and the baselines with a sum of random smooth functions (sinusoids) whose frequencies are different from the principal frequency of the periodic training data, and set the number of primitives to P = 2 . One primitive is set to have approximately the same length as a cycle of the periodic motion and the other primitive is set to be 50 % larger. Note that a rough estimate of the length of the primitives could be easily obtained by analyzing the principal frequencies of the signal. Fig. 2 depicts the results obtained by our approach and the baselines. The first two columns depict the two dimensional primitives recovered (W1 and depicts the activations vec ( H )  X  &lt; ( Q 1 + Q 2 )  X  L recovered. We expect the successful activations to be diagonal, and to appear only once every cycle.
 Note that our approach is able to recover the number of primitives as well as the primitive them-selves and the correct activations. Matching pursuit without refractory period (first row) is not able to recover the primitives, their number, or the activations. Moreover, the estimated signal has high frequencies. Matching pursuit with refractory period (second row) is able to recover the number of primitives, however the activations are underestimated and the primitives are not very accurate. Sparse coding has a low reconstruction error, but neither the primitives, their number, nor the acti-vations are correctly recovered. This confirms the inability of traditional sparse coding to recover interpretable primitives, and the importance of having interpretability constraints such as the refrac-tory period of matching pursuit and our diagonal constraints. Note also that as shown in Fig. 1 (right) our approach converges in a few iterations. 3.2 Quantitative analysis and comparisons We evaluate the capabilities of our approach to reconstruct new sequences, and compare our ap-proach to the baselines [3, 7] in a denoising scenario as well as when dealing with missing data. We preprocess the data by applying PCA to reduce the dimensionality of the input space. We measure error by computing the Frobenius norm between the test sequences and the reconstruction given by Figure 5: Multiple subject error as a function of the dimension for noisy data with variance 100 and different numbers of primitives. As expected one primitive is not enough for accurate reconstruction. (smooth, Q/ 2 , e 59 D ) (random, Q/ 2 , e 59 D ) (smooth, 2 Q/ 3 , e 59 D ) (random, 2 Q/ 3 , e 59 D ) Figure 6: Missing data and influence of initialization: Error in the 59 D space when Q/ 2 and 2 Q/ 3 of the data is missing. The primitives are either initialize randomly or to a smooth set of sinusoids of random frequencies. the learned W and the estimated activations H test as well as the error in the original 59D space which can be computed by projecting back into the original space using the singular vectors. Note that W is learned at training, and the activations H test are estimated at inference time. To evaluate the generalization properties of each algorithm, we compute both errors in a denoising scenario, where H test is obtained using  X  V test = V test + , with i.i.d Gaussian noise, and the errors are computed using the ground truth data V test . For each experiment we use P = 1 ,  X  = 0 . 05 ,  X  train = 1 ,  X  test = 1 . 3 and a rough estimate of Q , which can be easily obtained by examining the principal frequencies of the data [16]. The primitives are initialized to a sum of sinusoids of random frequencies.
 We created a walking dataset composed of motions performed by the same subject. In par-dataset. We also performed reconstruction experiments for running motions and used motions the rest for testing, and report average results over 10 random splits. Fig. 3 depicts reconstruction error in PCA space and in the original space as a function of the noise variance. Fig. 4 depicts reconstruction error as a function of the dimensionality of the PCA space. Our approach outper-forms matching pursuit with and without refractory period in all scenarios. Note that out method outperforms sparse coding when the output is noisy. This is due to the fact that, given a big enough dictionary, sparse coding overfits and can perfectly fit the noise.
 We also performed reconstruction experiments for running motions performed by different subjects. In particular we use motions { 03 , 04 , 05 , 06 } of subject 9 and motions { 21 , 23 , 24 , 25 } of subject 35 . Fig. 5 depicts reconstruction error for our approach when using different numbers of primitives. As expected one primitive is not enough for accurate reconstruction. When using two primitives our approach performs comparable to sparse coding and clearly outperforms the other baselines. In the next experiment we show the importance of having interpretable primitives. In particular we compare our approach to the baselines in a missing data scenario, where part of the sequence is miss-ing. In particular, Q/ 2 and 2 Q/ 3 frames are missing. We use the single subject walking database. Figure 7: Influence of  X  and P on the single subject walking dataset as well as using soft constraints instead of hard constraints on the activations. (left) Our method is fairly insensitive to the choice of  X  . As expected the reconstruction error of the training data decreases when there is less regularization. The test error however is very flat, and increases when there is too much or too little regularization. For missing data, having good primitives is important, and thus regularization is necessary. Note that the horizontal axis depicts  X  log  X  , thus  X  decreases for larger values of this axis. (center) Error with (green) and without (red) missing data as a function of P . Our approach is not sensitive to the value of P ; one primitive is enough for accurate reconstruction in this dataset. (right) Error when using solft constraints | H i,j,k  X  H i,j +1 ,k +1 | X   X  as a function of  X  . The leftmost point corresponds As shown in Fig. 6 our approach clearly outperforms all the baselines. This is due to the fact that sparse coding does not have structure, while the structure imposed by our equality constraints, i.e.,  X  i,j,k H i,j,k = H i,j +1 ,k +1 , help  X  X allucinate X  the missing data. We also investigate the influence of initialization by using a random non-smooth initialization and the smooth initialization described above, i.e.,sinusoids of random frequencies. Note that as our approach, sparse coding is not sensitive to initialization. This is in contrast with MP which is very sensitive due to the ` 0 -type regularization. We also investigated the influence of the amount of regularization on W . Towards this end we use the single subject walking dataset, and compute reconstruction error for the training and test data with and without missing data as a function of  X  . As shown in Fig. 7 (left) our method is fairly insensitive to the choice of  X  . As expected the reconstruction error of the training data decreases increases slightly when there is too much or too little regularization. When dealing with missing data, having good primitives becomes more important. Note that the horizontal axis depicts  X  log  X  , thus  X  decreases for larger values of the horizontal axis. The test error is higher than the training error for large  X  since we use  X  train = 1 and  X  test = 1 . 3 . Thus we are more conservative at learning since we want to learn interpretable primitives. We also investigate the sensitivity of our approach to the number of primitives. We use the single subject walking dataset and report errors averaged over 10 partitions of the data. As shown in Fig. 7 (middle) our approach is very insensitive to P ; in this example a single primitive is enough for accurate reconstruction.
 We finally investigate the influence of replacing the hard constraints on the activations by soft con-straints | H i,j,k  X  H i,j +1 ,k +1 |  X   X  . Note that our approach is not sensitive to the value of  X  and that the hard constraints ( H i,j,k = H i,j +1 ,k +1 ), depicted in the leftmost point in Fig. 7 (right), are almost optimal. This justifies our choice since when using hard constraints we do not need to search for the optimal value of  X  . We have proposed a sparse coding approach to learn interpretable spatio-temporal primitives of hu-man motion. We have formulated the problem as a tensor factorization problem with tensor group norm constraints over the primitives, diagonal constraints on the activations, as well as smooth-ness constraints that are inherent to human motion. Our approach has proven superior to recently developed matching pursuit and sparse coding algorithms in the task of learning interpretable spatio-temporal primitives of human motion from motion capture data. In the future we plan to investigate applying similar techniques to learn spatio-temporal dictionaries of video data such as dynamic textures.
