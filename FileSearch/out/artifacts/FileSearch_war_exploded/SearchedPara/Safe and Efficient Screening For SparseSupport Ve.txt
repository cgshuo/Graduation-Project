 Sparse support vector machine (SVM) is a robust predictive model that can effectively remove noise and preserve signals. Like Lasso, it can efficiently learn a solution path based on a set of predefined parameters and therefore provides strong support for model selection. Sparse SVM has been success-cluding text mining, bioinformatics, and image processing. The emergence of big-data analysis poses new challenges for model selection with large-scale data that consist of tens of millions samples and features. In this paper, a novel screen-ing technique is proposed to accelerate model selection for ` -regularized ` 2 -SVM and effectively improve its scalability. This technique can precisely identify inactive features in the optimal solution of a ` 1 -regularized ` 2 -SVM model and re-move them before training. The technique makes use of the variational inequality and provides a closed-form solution for screening inactive features in different situations. Every feature that is removed by the screening technique is guar-when ` 1 -regularized ` 2 -SVM uses the features selected by the technique, it achieves exactly the same result as when it uses the full feature set. Because the technique can remove a large number of inactive features, it can greatly increase the efficiency of model selection for ` 1 -regularized ` 2 -SVM. Ex-perimental results on five high-dimensional benchmark data sets demonstrate the power of the proposed technique. H.2.8 [ Database Management ]: Database Applications X  data mining ; I.5.2 [ Pattern Recognition ]: Design Method-ology X  feature evaluation and selection Screening, sparse support vector machine, feature selection
Sparse predictive modeling algorithms provide powerful tools to analyze high-dimensional data and generate results that have high-degree of interpretability and robustness [5, 11]. In general, an ` 1 -regularized sparse predictive modeling algorithm can be formulated as min w loss( w )+  X  k w k 1 function, and  X   X  0 is the regularization parameter that bal-ances between the loss function and the regularizer. When tion, the resulting sparse model is the ` 1 -regularized support vector machine (SVM) [4, 18, 2, 6, 16]. An ` 1 -regularized SVM model can simultaneously perform model fitting by margin maximization and remove noisy features by soft-thresholding. It has been successfully applied in a variety of data mining applications that include text mining, bioinfor-matics, and image processing. Compared to other variances of sparse SVM model [15, 8, 1], ` 1 -regularized SVM enjoys two major advantages. First, it defines a convex problem; therefore, an optimal solution can always be achieved with-out any relaxation of the original problem. Second, its op-timization is simple, and a well implemented ` 1 -regularized SVM solver can readily handle large-scale problems with tens of millions samples and features [6].

The value of the regularization parameter  X  is crucial to the performance of an ` 1 -regularized SVM. To achieve good performance, model selection is often used to help choose an appropriate  X  value. For example, given a series of regular-lution can be chosen by using a prespecified criterion, such as the accuracy or the area under the curve (AUC) that is achieved by the resulting models on holdout samples.
Big-data analysis requires a higher standard of efficiency for predictive modeling. When data are huge, the computa-tional cost of model selection can be prohibitive. An intu-itive question is to ask whether the solution obtained in the k th step of model selection can be used in the ( k + 1)th step to speed up computation. For Lasso [11], the answer leads to the state-of-the-art screening techniques to accelerate model solution w  X  1 for  X  =  X  1 , one can identify many features that are guaranteed to have zero coefficients in w  X  2 when  X  =  X  By removing a large number of these inactive features, the cost for computing w  X  2 can be greatly reduced.

Although screening algorithms have been designed for Lasso, nique for ` 1 -regularized ` 1 -SVM. This paper presents a novel screening technique that is designed to speed up model se-lection for ` 1 -regularized ` 2 -SVM. The technique makes use of the variational inequality [9] for constructing a tight con-vex set, which can be used to compute bounds for screening features. Features that are removed by this technique are guaranteed to be inactive in the optimal solution. Therefore, dimensional benchmark data sets demonstrate that the pro-posed screening technique can dramatically speed up model selection for ` 1 -regularized ` 2 -SVM by efficiently removing a large number of inactive features.
Assume that X  X  IR m  X  n is a data set that contains n sam-ples, X = ( x 1 ,..., x n ), and m features, X = f &gt; 1 Assume also that y = ( y 1 ,...,y n ) contains n class labels, y i  X  { X  1 , +1 } , i = 1 ,...,n . Let w  X  IR m be the m -dimensional weight vector, let  X  i  X  0 ,i = 1 ,...,n be the n slack variables, and let b  X  IR and  X   X  IR + be the bias and the regularization parameter, respectively. The primal form of the ` 1 -regularized ` 2 -SVM is defined as:
Eq. (1) specifies a convex problem that has a non-smooth ` regularizer, which enforces that the solution is sparse. Let w ? (  X  ) be the optimal solution of Eq. (1) for a given  X  . All Let  X   X  IR n be the n -dimensional dual variable. By apply-ing the Lagrangian multiplier [3], the dual of the problem defined in Eq. (1) can be obtained as: Here,  X  f = Yf , and Y = diag ( y ) is a diagonal matrix. By defining  X  =  X   X  , Eq. (2) can be reformulated as:
In the primal formulation for the ` 1 -regularized ` 2 -SVM, the primal variables are b , w , and  X  . And in the dual for-mulation, the dual variables are  X  or  X  . When b and w are known,  X  ,  X  , and  X  can be obtained as: The relation between  X  and w can be expressed as:  X  &gt;  X  f j = sign ( w j )  X , if w j 6 = 0 [  X   X , +  X  ] , if w The relation between  X  and w can be expressed as:  X  &gt;  X  f j = sign ( w j ) , if w j 6 = 0 [  X  1 , +1] , if w  X  max is defined as the smallest  X  value that leads to w = 0 when it is used in Eq. (1). Given an input data set ( X , y ),  X  max can be obtained in a closed form as: where n + and n  X  denote the number of positive and negative samples, respectively. And when  X   X   X  max , the optimal solution of the problem defined in Eq. (1) can be written as: enter the model is the one that corresponds to the element that has the largest magnitude in m .
Eq. (6) shows that the necessary condition for a feature tive in the optimal solution. Given a  X  value, this condition can be used to develop a screening rule for removing inactive features to speed up training for the ` 1 -regularized ` 2 A feature can be safely removed if its upper bound value is less than 1. The cost of computing the upper bounds can be much lower than training ` 1 -regularized ` 2 -SVM. There-fore, screening can greatly lower the computational cost by removing many inactive features before training. To bound vex set K that contains  X  . The upper bound value can be then computed by maximizing |  X  &gt;  X  f | over K .
In the following, Eq. (3) and the variational inequality [9] are used to construct a closed convex set K to bound |  X  &gt;  X  convex optimization problem.

Proposition 3.1. Let  X  ? be an optimal solution of the following convex optimization problem: where g is continuously differentiable and K is closed and convex. Then the following variational inequality holds: The proof of this proposition can be found in [9].  X  max  X   X  1 &gt;  X  2 and its corresponding solution  X  1 is known The reason to introduce  X  1 is that when  X  1 is close to  X   X  1 is known,  X  1 can be used to construct a tighter convex set that contains  X  2 to bound the value of |  X  &gt; 2  X  f | .
When  X  1 =  X  max ,  X  1 is given in Eq. (4). Figure 1: The K in a two-dimensional (2D) space when different t values are used. The red circle cor-responds to t = 0 , and the blue circle corresponds to t = 1 + 1  X 
Let  X  1 and  X  2 be the optimal solutions of the problem defined in Eq. (3) for  X  1 and  X  2 , respectively. Assume that  X  1 &gt;  X  2 and that  X  1 is known. The following results can be obtained by applying Proposition 3.1 to the convex problem defined in Eq. (3) for  X  1 and  X  2 , respectively: By substituting  X  =  X  2 into Eq. (9), and  X  =  X  1 into Eq. (10), the following equations can be obtained: In the preceding equations,  X  1 ,  X  1 , and  X  2 are known. Therefore, Eq. (11) defines an n -dimensional halfspace and Eq. (12) defines an n -dimensional hyperball. Because  X  needs to satisfy both equations, it must reside in the inter-section of the halfspace and the hyperball. Obviously, this dimensional space. In the figure, Eq. (11) defines the area below the blue line, Eq. (12) defines the area in the red circle, and K is indicated by the shaded area.

Besides the n -dimensional hyperball defined in Eq. (12), it is possible to derive a series of hyperballs by combining Eq. (11) and Eq. (12). Assume that  X  ? is the optimal solu-tion of Eq. (3) and t  X  0. It is easy to verify that  X  ? the optimal solution of the following problem: By applying Proposition 3.1 to the problem defined in Eq. (13) for  X  1 , and  X  2 , the following results can be obtained: Eq. (14) and Eq. (15), respectively, and then combining the two inequalities, the following equations can be obtained: c = 1
As the value of t changes from 0 to  X  , Eq. (16) generates a series of hyperballs. When t = 0, c = 1 2 ( 1  X  by Eq. (12). The following theorems provide some insights about the properties of the hyperballs generated by Eq. (16):
Theorem 3.2. Let a = perball generated by Eq. (16) reaches it minimum when Let  X c be the center of the ball and l be the radius. When the minimum is reached, they can be computed as:  X c = 1 Here, P u ( v ) = v  X  v the null-space of u . Since k a k 2 = 1 , P a ( 1 ) = 1  X  a
Proof. The theorem can be proved by minimizing the r defined in Eq. (16).

Theorem 3.3. Let the intersection of the hyperplane  X  be P t . The following equation holds:
Proof. The theorem can be proved by showing that P t is independent of t .

This theorem shows that the intersection between the hy-perball B t and the hyperplane (  X  1  X  1  X  the same for different t values.
Theorem 3.4. Let the intersection of the halfspace (  X  be Q t . The following inequality holds:
Proof. The theorem can be proved by showing that  X  t 1 , t  X  0 and t 1  X  t 2 , if  X  2  X  Q t 1 , then  X  2  X  Q t 2 also holds.
This theorem shows that the volume of Q t becomes larger when t becomes larger. And Q t 1  X  Q t 2 if t 1  X  t 2 . corresponds to the one obtained by setting t 1 = 0 in Eq. (16). The blue circle corresponds to the one obtained by setting in Eq. (16). It can be observed that the intersections of the two circles pass the line (  X  1  X  1  X  consistent with Theorem 3.3. Also since t 1  X  t 2 , Q t 1 which is consistent with Theorem 3.4.

Thereom 3.4 suggests that Q t =0 should be used to con-struct K , because when t = 0, the volume of Q t is mini-mized. The equality  X  &gt; y = 0 in Eq. (3) of the dual formu-lation can also be used to further reduce the volume of K . a =
Theorem 3.3 shows that when t varies, the intersection of the hyperball B t and the hyperplane (  X  1  X  1  X  0 remains unchanged. This suggests that if the maximum matter which B t is used, the maximum value will be the same. This property can be used to simplify the computa-tion. Section 3.2.4 will show that when the maximum value perball B t =0 and the hyperplane (  X  1  X  1  X  the computation of the maximum value can be simplified by switching to B t with t = 1 + 1  X 
Given the convex set K defined in Eq. (17), the maximum value of  X  &gt; 2  X  f can be computed by solving the problem: s.t. a &gt; ( b + r )  X  0 , k r k 2  X  X  b k 2  X  0 , ( c + r ) and y are known. Since the following equation holds: max ( c + r ) &gt;  X  f can be decomposed to two subproblems: s.t. a &gt; ( b + r )  X  0 , k r k 2  X  X  b k 2  X  0 , ( c + r ) s.t. a &gt; ( b + r )  X  0 , k r k 2  X  X  b k 2  X  0 , ( c + r ) and
Eq. (19) and Eq. (20) suggest that the key to bound  X  &gt; is to solve the following problem: s.t. a &gt; ( b + r )  X  0 , k r k 2  X  X  b k 2  X  0 , ( c + r ) Its Lagrangian L ( r , X , X , X  ) can be written as: r &gt;  X  f +  X  a &gt; ( b + r ) + 1 And the Karush-Kuhn-Tucker (KKT) conditions are: is also bounded from below.

According to whether the inequality constraints are ac-tive, the problem can have different minimum values. This sections study these cases in detail. 3.2.1 The Case  X  = 0 ,  X  f +  X  a +  X  y 6 = 0 In this case, set r = t ( f +  X  a +  X  y ), and let t  X   X  X  X  . Then L ( r , X , 0 , X  )  X  X  X  X  . This contradicts the observation that min r L ( r , X , X , X  ) must be bounded from below. So when  X  f +  X  a +  X  y 6 = 0 ,  X  must be positive. 3.2.2 The Case  X  = 0 ,  X  f +  X  a +  X  y = 0 that  X P y ( a ) =  X  P y (  X  f ). This suggests that  X P P y  X  f are colinear. Also since  X   X  0, the following must hold: Given  X P y ( a ) =  X  P y  X  f ,  X  can be computed by: Similarly, the value of  X  can be computed by: By plugging  X  = 0 and the obtained value of  X  and  X  into Eq. (22), L ( r , X , 0 , X  ) can be written as: specified in Eq. (23) X  X q. (29) are satisfied. Since the prob-convex, Eq. (30) defines the minimum value of the problem. The following theorem summarizes the result when  X  = 0.
Theorem 3.5. When P y ( a ) its minimum value at  X  = 0 , which can be computed as: And in this case, the value of the dual variables are:  X  = the hyperplane defined by a &gt; ( b + r ) = 0.

Corollary 3.6. When its maximum value at  X  = 0 . In this case  X  min  X  &gt;  X  computed as:  X  min  X  &gt; 2  X  f =  X  min r &gt;  X  f  X  c &gt;  X  f = max  X  &gt; 2  X  f can be computed by replacing  X  f with  X   X 
In the computation, k P y  X  f k 2 are independent to  X  1 ,  X  and  X  1 . Therefore, it can be precomputed. k P y ( a ) k a  X  1 are shared by all features. These properties can be used to accelerate the computation. 3.2.3 The Case:  X  &gt; 0 ,  X  = 0
In this case, since  X  &gt; 0 and  X  = 0, the minimum value Figure 1, this corresponds to the arc of the red circle under the blue line. Plugging  X  = 0 in Eq. (22) results in:
L ( r , 0 , X , X  ) = r &gt;  X  f + 1 The dual function g (0 , X , X  ) can be obtained by setting  X  r L ( r , 0 , X , X  ) =  X  f +  X  r +  X  y = 0  X  r =  X  1 Plugging the obtained r and  X  into L ( r , 0 , X , X  ) leads to: g (  X  ) = min The dual function can be maximized by setting  X  X  (  X  )  X  X  Also since b &gt; y = c &gt; y , the following equation holds:
Squaring both sides of the equation and solving the ob-tained equation leads to the result: To obtain this equation, the following facts are used: the following equation must hold: And in this case,  X  can be written in the form:
To compute max Plugging Eq. (35) and Eq. (37) into Eq. (33) leads to: written in the following form: max It can be verified that in this case, all the KKT conditions specified in Eq. (23) X  X q. (24) and Eq. (26) X  X q.(29) are sat-isfied. The additional condition for Eq. (25) to be satisfied Eq. (22) to be zero leads to the following equation: Plugging this equation to a &gt; ( b + r )  X  0 results in: to the complementary slackness condition, a &gt; ( b + r ) = 0. contradicts the requirement that  X  = 0. On the other erwise,  X  &gt; 0 and  X  =  X  a &gt; b  X  a &gt; f  X   X  a &gt; contradiction. Therefore, to satisfy Eq. (25), the condition  X  a and Eq. (36) in  X  a &gt; b  X  a &gt; f  X   X  a &gt; y  X  0 leads to: Under this condition, the KKT condition a &gt; ( b + r )  X  0 must be satisfied. The following theorem summarizes the result for the case  X  &gt; 0 and  X  = 0.
 0 , r &gt;  X  f achieves its minimum value at  X  &gt; 0 and  X  = 0 : min In this case, the values of the dual variables are:  X  = 0 , X  = in this case,  X  min  X  &gt;  X  f can be computed as: max  X  &gt; 2  X  f can be computed by replacing  X  f with  X   X 
In the computation, P y  X  f and  X  1 . Therefore, it can be precomputed. P y ( a ) &gt; and k P y ( b ) k 2 , although rely on  X  1 ,  X  2 or  X  1 by all features and only need to be computed once. These properties can be used to accelerate computation. 3.2.4 The Case:  X  &gt; 0 ,  X  &gt; 0
In this case, the minimum value of r &gt;  X  f is achieved on the form solution for the problem specified in Eq. (19) is diffi-cult. Theorem 3.3 suggests that when the minimum value is achieved on the intersection of the hyperball and the hy-perplane, one could switch the hyperball used in Eq. (19) to simplify the computation. It turns out that a closed form solution can be obtained by using the hyperball B t t = 1 + 1  X  perball defined in Theorem 3.2. As proved in Theorem 3.3, the intersections of different B t and ( 1  X  are identical. Therefore, switching the hyperball B t does not change the maximum value of |  X  &gt;  X  f | .

When B t with t = 1 + 1  X  assuming that the minimum is achieved on the intersection of the hyperball and the hyperplane, the problem specified in Eq. (19) can be rewritten as: And its Lagrangian can be written as: L ( r , X , X , X  ) = r &gt;  X  f +  X  a &gt; r + 1 In the preceding equation, c is the center of the hyperfall, and l is the radius of the hyperfall. They are defined as:  X c = 1
The dual function g (  X , X , X  ) = min r L ( r , X , X , X  ) can be obtained by setting  X  r L ( r , X , X , X  ) = 0, which leads to: Since  X  6 = 0, k r k 2 = l . Therefore,  X  can be written as: Since  X  6 = 0, a &gt; r = 0. Therefore,  X  can be written as: Plugging the obtained r ,  X  , and  X  into L ( r , X , X , X  ) leads to: Squaring both sides of the equation and solving the resulting problem yields a closed-form solution for  X  :  X  =  X  To obtain this equation, the following facts are used: Since (  X c + r ) &gt; y = 0, P a (  X c ) + P a ( r ) fore,  X  = P a (  X  f ) positive, the following equation holds:  X  =  X  In this case,  X  can be written in the form:
To compute max l k P a  X  f +  X P a ( y ) k 2 = l 2 By plugging this equation and  X  into Eq. (42), max be written in the following form: 1 2 Since P a ( 1 ) &gt; P a ( f )  X  max 1 2 Theorem 3.9 summarizes the result when  X  &gt; 0 and  X  &gt; 0.
Theorem 3.9. When r &gt;  X  f achieves its minimum value at  X  &gt; 0 and  X  &gt; 0 , it can be computed as: min 1 2
Corollary 3.10. When  X  &gt;  X  f achieves its minimum value at  X  &gt; 0 and  X  &gt; 0 , it can be computed as:  X  min  X  &gt; 2  X  f =  X  min r &gt;  X  f  X   X c &gt;  X  f = (44) 1 2 max  X  &gt; 2  X  f can be computed by replacing  X  f with  X   X 
In the computation, P P a ( y ) P a ( 1 ) features and needs to be computed only once. This property can be used to accelerate computation.
Algorithm 1 shows the procedure of screening features for ` -regularized ` 2 -SVM. Given  X  1 ,  X  2 , and  X  1 , the algorithm returns a list L , which contains the indices of the features that are potentially active in the optimal solution that corre-sponds to  X  2 . The algorithm first weights all features using Line 4 and Line 5. If the value is larger than 1, it adds the index of the feature to L in Line 7. The function neg min ( computes  X  min  X  &gt; 2  X  f . Since P u (  X  v ) =  X  P u termediate results computed for neg min (  X  f ) can be used by neg min (  X   X  f ) to accelerate its computation.

The algorithm needs to be implemented carefully to en-to be decomposed to many small substeps, so that the in-termediate results obtained in the preceding substeps can be used by the following substeps to accelerate computa-tion. Second, the substeps need to be organized and ordered properly so that no redundant computation is performed. It turns out the procedure listed in Algorithm 1 is surprisingly  X  , and  X  2 . Therefore, they can been precomputed before training, and the cost is O ( mn ).  X  &gt; 1 y ,  X  &gt; 1 shared by all the features. So they can be computed at the begining of screening, and the cost is O ( n ). Given these in-termediate results, most substeps for computing max |  X  f can be obtained in O (1). The only expensive substep is to might have already computed  X  f &gt;  X  1 as an intermediate re-from the solver for screening features at no cost.
In summary, in the worst case of the proposed procedure, has m features and n samples is O ( mn ). And if  X  f &gt; f y , and f &gt; f can be obtained from the intermediate results generated by the ` 1 -regularized ` 2 -SVM solver, the total cost can decrease to just O ( m ) . The proposed screening method was implemented in the C language and compiled as a library that can be conve-niently accessed in a high-level programming language, such for accelerating model selection for ` 1 -regularized ` 2 Experiments are performed on a Windows Server 2008 R2 with two Intel Xeon r L5530 CPUs and 72GB memory. 1 L =  X  , i = 1, Y = diag ( y ); 2 for i  X  m do 3  X  f = Yf i ; 4 m 1 = neg_min(  X  f ) , m 2 = neg_min(  X   X  f ) ; 5 m = max { m 1 ,m 2 } ; 6 if m  X  1 then 7 L = L  X  X  i } ; 8 end 9 i = i + 1; 10 end 11 return L ; 12 Function neg_min(  X  f ) 14 compute m using Eq. (31); 15 return m ; 16 end 18 compute m using Eq. (40); 19 return m ; 20 end 21 compute m using Eq. (44); 22 return m ; 23 end Algorithm 1: Screening for ` 1 -regularized ` 2 -SVM.
Five benchmark data sets are used in the experiment. One is a microarray data set: gli 85. Three are text data sets: rcv1.binary(rcv1b), real-sim, and news20.binary (news20b). And one is a educational data mining data set: kdd2010 bridge-to-algebra (kddb). The gli 85 data set is downloaded from Gene Expression Omnibus, 2 and the other four data sets are downloaded from the LIBSVM data repository. 3 Ac-cording to the feature-to-sample ratio ( m/n ), the five data sets fall into three groups: (1) the m n group, including the gli 85 and news20b data sets; (2) the m  X  n group, in-cluding the rcv1b and kddb data sets; and (3) the m n tailed information about the five benchmark data sets. Table 1: Summary of the benchmark data sets www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE4412 www.csie.ntu.edu.tw/ cjlin/libsvmtools/datasets/
A solver based on the coordinate gradient descent (cgd) algorithm [13] is implemented in the C language for training the ` 1 -regularized ` 2 -SVM model. This solver improves the one that is implemented in the liblinear package [6]. In lib-linear, the bias term b is also penalized by the ` 1 regularizer and is inactive in most cases. In contrast, the improved one solves the problem specified in Eq. (1) exactly. Therefore,  X   X  f the solver as an intermediate result. Therefore, in screening,  X  f  X  1 values can be obtained from the solver at no cost.
For each given benchmark data set, the cgd solver is used to fit the ` 1 -regularized ` 2 -SVM model along a sequence of 20  X  values:  X  k = 1 k  X  max  X  ,k = 1 ,..., 20 , = 10  X  8 . When  X  =  X  max  X  , only one feature is active. Denote n + and n  X  as the number of positive and negative samples, respec-corresponds to the largest element in m .

For each given benchmark data set, the cgd solver runs in seven different configurations: (1) In org , the solver runs without any accelerating technique. (2) In warm , the solver runs with warm-start. In the k th iteration, the w for fitting the model. When  X  k and  X  k  X  1 are close, warm-start can effectively speed up training by reducing the num-ber of iterations for the solver to converge. (3) In shr , the solver runs with the shrinking strategy. During each itera-tion of the cgd solver run, if a feature X  X  current weight is 0 and its gradient is very small, the feature is set to be inac-tive [6]. (4) In warm shr , the solver runs with both warm-start and the shrinking strategy. (5) In scr , the solver runs with the screening technique. (6) In warm scr , the solver runs with both warm-start and the screening technique. (7) In scr shr , the solver runs with both the shrinking strategy and the screening technique.

Warm-start and screening are designed to speed up model These techniques can be combined for further performance improvement. The main purpose of running the ` 1 -regularized ` -SVM solver with different configurations is not only to compare different accelerating techniques, but also to pro-vide a sensitivity study for exploring how these techniques can be combined to achieve the best performance.

Both screening and shrinking reduce computational cost by removing inactive features. Their major differences in-clude the following: (1) Shrinking is performed in each it-eration of training to reduce the search space of the solver, whereas screening is performed only once before training. (2) Shrinking is a heuristic method for removing inactive features. Sometimes it might also remove active features; safe, because all the removed features are guaranteed to be inactive the optimal solution. (3) The introduced shrink-the proposed screening technique can be applied with any ` -regularized ` 2 -SVM solver to speed up model selection. Therefore, the proposed screening technique is more general. Table 2: Total run time of the ` 1 -regularized ` 2 -SVM solver when different combinations of accelerating techniques are used to speed up model selection.

Tech. gli 85 rcv1b real-sim news20b kddb org 328.7 17.92 20.81 943.67 9209.06 warm 376.6 10.30 13.48 682.08 7752.80 shr 2.78 4.49 7.25 62.44 3374.53 warm shr 3.10 2.31 4.32 32.62 2395.45 scr 0.78 3.35 6.67 25.53 1126.05 warm scr 0.74 1.78 4.30 17.84 831.87 scr shr 1.45 4.00 7.14 48.29 2603.94
Table 2 and Table 3 show the results of the total run time ` -SVM solver to converge when different combinations of accelerating techniques are used. The total run time and total number of iterations are obtained by aggregating the time and number of iterations used by the ` 1 -regularized ` -SVM solver when it fits models using different regular-ization parameters. In terms of total running time, screen-ing with warm-start ( warm scr ) provides the best perfor-mance. Compared to org , for the m n group, the speed-news20b data. For the m  X  n group, the speed-up ratio And for the m n group, the speed-up ratio is about 5 for the real-sim data. The result shows that warm scr is the number of samples. A similar trend is observed on scr and scr shr . In terms of the total iteration number, the best performance is achieved by warm and warm scr . This sug-gests that warm-start can effectively speed up convergence by providing a good start point for optimization. A similar trend is observed when shr is compared to warm shr .
When org is compared to scr , the result suggests that the proposed screening technique can significantly improve the performance of the ` 1 -regularized ` 2 -SVM solver. This justi-fies that screening can effectively reduce the computational cost of training by removing most inactive features. When shr is compared to scr , the result suggests that screening method for removing inactive features. Sometimes, it might remove active features during training. When this happens, violations can be detected by using the KKT conditions for the optimal solution. However, recovering the optimal solu-tion leads to extra cost. This is supported by the observa-tion that with shr the solver usually takes more iterations to converge than with org and src . When warm is compared to scr and shr , the results suggest that removing inactive features for training is more effective than providing a good starting point for optimization.

The results presented in Table 2 and Table 3 suggest that the performance of screening and shrinking can be further improved by combining them with warm-start. This is be-cause warm-start can effectively speed up convergence by providing a good starting point for optimization. However, combining screening with shrinking does not improve the performance of screening because that screening has already Table 3: Total number of iterations for the ` 1 -regula-rized ` 2 -SVM solver to converge when different com-binations of accelerating techniques are used.

Tech. gli 85 rcv1b real-sim news20b kddb org 15535 1062 568 2579 755 warm 14610 615 373 1898 628 shr 16046 1737 815 4995 2008 warm shr 14888 713 431 2157 1046 scr 15376 1059 596 2862 843 warm scr 14599 590 390 1999 569 scr shr 16150 1695 942 4908 1901 removed many inactive features before training is performed. When shrinking is used in training, its benefit for removing inactive features becomes insubstantial and is overwhelmed by the cost of recovering the optimal solution when it acci-dentally removes active features.

Figure 2 shows detailed information about how differ-ent combinations of accelerating techniques perform on the news20b data set when  X  decreases from  X  max to 1 20  X  max The result shows that screening with warm-start is effec-tive for accelerating and its performance is stable. It also shows that when  X  decreases, the proposed screening tech-nique can stably select a small set of features for reducing computational cost. Let k be the number of active fea-tures. On the news20b data set, when  X  decreases from  X  max to 1 20  X  max , the proposed screening technique retains about k +430 features for training the ` 1 -regularized ` model. This number is much smaller than the dimension-Similar trends are also observed on other data sets and are not presented in the paper because of the space limit. Table 4: Comparison of screening to training time
Tech. gli 85 rcv1b real-sim news20b kddb scr 0.03 0.06 0.04 1.91 41.65 tr 0.75 3.29 6.63 23.63 1084.40 ratio 0.04 0.02 0.01 0.08 0.04 scr 0.03 0.07 0.04 1.91 41.71 tr 0.70 1.72 4.26 15.93 790.15 ratio 0.05 0.04 0.01 0.12 0.05
Table 4 compares the time used by screening to the time used by training. Notice that for training, the solver uses only the features that are selected by screening. Compared to training, the time used by screening is marginal.
The results presented in this section indicate that the pro-posed screening technqiue is effective for removing inactive features to improve training efficiency. And with warm-start they form the most powerful combination for accelerating model selection for the ` 1 -regularized ` 2 -SVM.
Screening is an effective technique for improving model paper proposes a novel technique to screen features for ` form criteria to screen features for the ` 1 -regularized ` model in different situations. Empirical study shows that the proposed technique can greatly improve model selection efficiency by stably eliminating a large number of inactive. Our ongoing work will extend the technique to screen fea-tures for the ` 1 -regularized ` 1 -SVM model. The authors would like to thank Anne Baxter, Russell Albright, and the anonymous reviewers for their valuable suggestions to improve this paper. [1] M. T. abd Li Wang and I. W. Tsang. Learning sparse [2] J. Bi, M. Embrechts, C. M. Breneman, and M. Song. [3] S. Boyd and L. Vandenberghe. Convex Optimization . [4] P. S. Bradley and L. O. Mangasarian. Feature [5] E. Candes and M. Wakin. An introduction to [6] R.-E. Fan, K.-W. Chang, C.-J. Hsieh, X.-R. Wang, [7] L. Ghaoui, V. Viallon, and T. Rabbani. Safe feature [8] I. Guyon, J. Weston, S. Barnhill, and V. Vapnik. Gene [9] J. L. Lions and G. Stampacchia. Variational [10] J. Liu, Z. Zhao, J. Wang, and J. Ye. Safe screening [11] R. Tibshirani. Regression shrinkage and selection via [12] R. Tibshirani, J. Bien, J. H. Friedman, T. Hastie, [13] P. Tseng and S. Yun. A coordinate gradient descent [14] J. Wang and et al. Lasso screening rules via dual [15] J. Weston, A. Elisseff, B. Schoelkopf, and M. Tipping. [16] G.-X. Yuan and K.-L. Ma. Scalable training of sparse [17] J. X. Zhen, X. Hao, and J. R. Peter. Learning sparse [18] J. Zhu, S. Rosset, T. Hastie, and R. Tibshirani.
