 In this paper, we study the problem of object-level figure/gr ound segmentation in images and video sequences. The core problem can be defined as follows: Given a n image X with known figure/ground may want to extract a walking person in an image using the figur e/ground mask of the same person in another image of the same sequence. Our approach is based o n training a classifier from the appearance of a pixel and its surrounding context (i.e., an i mage patch centered at the pixel) to recognize other similar pixels across images. To apply this process to a video sequence, we also evolve the appearance model over time.
 A key element of our approach is the use of a prior segmentatio n to reduce the complexity of the segmentation process. As argued in [22], image segments are a more natural primitive for image modeling than pixels. More specifically, an image segmentat ion provides a natural dimensional reduction from the spatial resolution of the image to a much s maller set of spatially compact and relatively homogeneous regions. We can then focus on repres enting the appearance characteristics of these regions. Borrowing a term from [22], we can think of e ach region as a  X  X uperpixel X  which represents a complex connected spatial region of the image u sing a rich set of derived image fea-tures. We can then consider how to classify each superpixel ( i.e. image segment) as foreground or foreground-background segmentation we are interested in.
 ment X  X  color histogram [19], filter bank responses [22], ori ented energy [18] and contourness [18]. These features are effective for image segmentation [18], o r finding perceptually important bound-aries from segmentation by supervised training [22]. Howev er, as shown in [17], those parameters do not work well for matching different classes of image regi ons from different images. Instead, we propose using a set of spatially randomly sampled image pa tches as a non-parametric, statistical superpixel representation. This non-parametric  X  X ag of pa tches X  model 1 can be easily and robustly evolved with the spatial-temporal appearance information from video, while maintaining the model size (the number of image patches per bag) using adaptive sam pling. Foreground/background clas-sification is then posed as the problem of matching sets of ran dom patches from the image with these models. Our major contributions are demonstrating the effectiveness and computational sim -plicity of a nonparametric random patch representation for semantically labelling superpixels and a novel bidirectional consistency check and resampling str ategy for robust foreground/background appearance adaptation over time. We organize the paper as follows. We first address several ima ge patch based representations and the associated matching methods are described. In section 3 , the algorithm used in our approach is presented with details. We demonstrate the validity of the p roposed approach using experiments on real examples of the object-level figure/ground image mappi ng and non-rigid object tracking under dynamic conditions from videos of different resolutions in section 4. Finally, we summarize the contributions of the paper and discuss possible extensions and improvements. Building stable appearance representations of images patc hes is fundamental to our approach. There are many derived features that can be used to represent the ap pearance of an image patch. In this paper, we evaluate our algorithm based on: 1) an image patch X  s raw RGB intensity vector, 2) mean LDA and NDA (Nonparametric Discriminant Analysis) feature s [7, 3] on the raw RGB vectors. For completeness, we give a brief description of each of these te chniques.
 Texture descriptors : To compute texture descriptions, we first apply the Leung-Malik (LM) filter bank [13] which consists of 48 isotropic and anisotropic filters w ith 6 directions, 3 scales and 2 phases. Thus each image patch is represented by a 48 componen t feature vector. The Haralick texture descriptor [10] was used for image classification in [17]. Haralick feat ures are derived from the Gray Level Co-occurrence Matrix, which is a tabulation o f how often different combinations of pixel brightness values (grey levels) occur in an image re gion. We selected 5 out of 14 texture descriptors [10] including dissimilarity, Angular Second Moment (ASM), mean, standard deviation (STD) and correlation. For details, refer to [10, 17].
 Dimension reduction representations : The Principal Component Analysis (PCA) algorithm is used to reduce the dimensionality of the raw color intensity vectors of image patches. PCA makes no prior assumptions about the labels of data. However, reca ll that we construct the  X  X ag of patches X  appearance model from sets of labelled image patches. This s upervised information can be used to project the bags of patches into a subspace where they are bes t separated using Linear discrimi-nant Analysis (LDA) or Nonparametric Discriminant Analysis (NDA) algorithm [7, 3] by assuming Gaussian or Non-Gaussian class-specific distributions.
 Patch matching : After image patches are represented using one of the above m ethods, we must match them against the foreground/background models. Ther e are 2 methods investigated in this paper: the nearest neighbor matching using Euclidean dista nce and KDE (Kernel Density Estima-tion) [12] in PCA/NDA subspaces. For nearest-neighbor matc hing, we find, for each patch p , its nearest neighbors p F p = k p  X  p B n k as probability density values from the KDE functions KDE ( p,  X  F ) and KDE ( p,  X  B ) where  X  F | B are bags of patch models. Then the segmentation-level class ification is performed as section 3.2. We briefly summarize our labeling algorithm as follows. We as sume that each image of interest has been segmented into spatial regions. A set of random imag e patches are spatially and adap-tively sampled within each segment. These sets of extracted samples are formed into two  X  X ags of patches X  to model the foreground/background appearance re spectively. The foreground/background decision for any segment in a new image is computed using one o f two aggregation functions on the appearance similarities from its inside image patches t o the foreground and background models. Finally, for videos, within each bag, new patches from new fr ames are integrated through a robust bidirectional consistency check process and all image patc hes are then partitioned and resampled to create an evolving appearance model. As described below, th is process prune classification inaccu-racies in the nonparametric image patch representations an d adapts them towards current changes in foreground/background appearances for videos. We describ e each of these steps for video tracking of foreground/background segments in more detail below, an d for image matching, which we treat as a special case by simply omitting step 3 and 4 in Figure 2.

Non-parametric Patch Appearance Modelling-Matching Algo rithm Figure 3: Left: Segment adaptive random patch sampling from an image with kn own figure/ground labels. 3.1 Sample Random Image Patches in our experiments. A typical segmentation result is shown i n Figure 1. We use X to represent a sequence of video frames. Given an image segme nt, we formulate its representation as a distribution on the appearance variation over all possi ble extracted image patches inside the segment. To keep this representation to a manageable size, w e approximate this distribution by sampling a random subset of patches.
 We denote an image segment as S segment, where i is the index of the (foreground/background)image segment w ithin an image. Ac-cordingly, P respectively. The cardinality N to thousands. However small or large superpixels are expect ed to have roughly the same amount of uniformity. Therefore the sampling rate  X  with increasing N predefined threshold  X  , (typically 2500  X  3000 ), above which size ( P adaptivity is illustrated in Figure 3. Notice that large ima ge segments have much more sparsely sampled patches than small image segments. From our experim ents, this adaptive spatial sampling strategy is sufficient to represent image segments of differ ent sizes. 3.2 Label Segments by Aggregating Over Random Patches For an image segment S patches P distances d F models respectively as described in Section 2.
 The decision of assigning S appearance distribution of S tics) as its distances D F In terms of distances { d F segment X  X  foreground/background fitness is set as the inver se of the distances: M F and M B Median robust operator can also be employed in our experiments, wit hout noticeable difference in performance. Another choice is to classify each p  X  X  foreground/background decision for S 3.3 Construct a Robust Online Nonparametric Foreground/Ba ckground Appearance Model From sets of random image patches extracted from superpixel s with known figure/ground labels, 2 foreground/background  X  X ags of patches X  are composed. The bags are the non-parametric form of the foreground/background appearance distributions. Whe n we intend to  X  X rack X  the figure/ground model sequentially though a sequence, these models need to b e updated by integrating new image patches extracted from new video frames. However the size (t he number of patches) of the bag will be unacceptably large if we do not also remove the some redund ant information over time. More importantly, imperfect segmentation results from [6] can c ause inaccurate segmentation level fig-ure/ground labels. For robust image patch level appearance modeling of  X  bidirectional consistency check and resampling strategy t o tackle various noise and labelling uncer-tainties.
 More precisely, we classify new extracted image patches {P  X  no good contrast (simply, the ratio between d F further sort the distance list of the newly classified foregr ound patches {P F image patches on the top of the list which have too large dista nces and are probably to be outliers, and ones from the bottom of the list which have too small dista nces and contain probably redundant appearances compared with  X  F Then the filtered {P survival p Next, we cluster all image patches of  X  F  X  | B  X  bution and sampling for each mode. Ideally, the resampling r ate  X   X  should decrease with increasing ter resampling. If we perform resampling directly over patc hes without partitioning, some modes of the appearance distribution may be mistakenly removed. Thi s strategy represents all partitions with sufficient number of image patches, regardless of their diff erent sizes. In all, we resample image patches of  X  F | B wise sampling rate  X   X  , to generate  X  F | B number of image patches extracted from a certain frame X The problem of partitioning image patches in the bag can be fo rmulated as the NP-hard k-center problem. The definition of k-center is as follows: given a data set of n points and a predefined cluster number k , find a partition of the points into k subgroups P ters c is the index of clusters. Gonzalez [8] proposed an efficient g reedy algorithm, farthest-point clus-tering , which proved to give an approximation factor of 2 of the opti mum. The algorithm oper-ates as follows: pick a random point p for iterations i = 2 , ..., k , find the point p d ter and recompute the means of clusters in C . Compared with the popular k-means algorithm, this algorithm is computationally efficient and theoretically b ounded 6 . In this paper, we employ the Eu-clidean distance between an image patch and a cluster center , using the raw RGB intensity vector or the feature representations discussed in section 2. We have evaluated the image patch representations describe d in Section 2 for figure/ground mapping between pairs of image on video sequences taken with both sta tic and moving cameras. Here we summarize our results. 4.1 Evaluation on Object-level Figure/Ground Image Mappin g We first evaluate our algorithm on object-level figure/groun d mapping between pairs of images under eight configurations of different image patch representati ons and matching criteria. They are listed as follows: the nearest neighbor distance matching on the im age patch X  X  mean color vector ( MCV ); raw color intensity vector of regular patch scanning ( RCV ) or segment-adaptive patch sampling over image ( SCV ); color + filter bank response ( CFB ); color + Haralick texture descriptor ( CHA ); PCA feature vector ( PCA ); NDA feature vector ( NDA ) and kernel density evaluation on PCA features ( KDE ). In general, 8000  X  12000 random patches are sampled per image. There is no apparent difference on classification accuracy for the patch size ran ging from 9 to 15 pixels and the sample rate from 0.02 to 0.10. The PCA/NDA feature vector has 20 dime nsions, and KDE is evaluated on the first 3  X  6 PCA features.
 Because the foreground figure has fewer of pixels than backgr ound, we conservatively measure the classification accuracy from the foreground X  X  detection pr ecision and recall on pixels. Precision is the ratio of the number of correctly detected foreground p ixels to the total number of detected total number of foreground pixels in the image. The patch siz e is 11 by 11 pixels, and the segment-480 ) images with the labelled figure/ground segmentation, we co mpare their average classification accuracies in Tables 1.
 Table 1: Evaluation on classification accuracy (ratio). The first row is precision; the second row is recall.
 sity vector without any dimension reduction. MCV has the worst accuracy, which shows that pixel-color leads to poor separability between figure and ground in our data set. Four feature based rep-resentations, CFB, CHA, PCA, NDA with reduced dimensions, have similar performance, wherea s NDA is slightly better than the others. KDE tends to be more biased towards the foreground class because background usually has a wider, flatter density dist ribution. The superiority of SCV over RCV proves that our segment-wise random patch sampling strateg y is more effective at classifying image segments than regularly scanning the image, even with more samples. As shown in Figure 4 (b), some small or irregularly-shaped image segments do not have enough patch samples to produce stable classifications. 4.2 Figure/Ground Segmentation Tracking with a Moving Came ra From Figure 4 (h), we see KDE tends to produce some false positives for the foreground. Ho wever which is also formulated as a KDE function of image patch coor dinates. By considering videos with complex appearance-changing figure/ground, imperfect seg mentation results [6] are not completely avoidable which can cause superpixel based figure/ground la belling errors. However our robust bidirectional consistency check and resampling strategy , as shown below, enables to successfully track the dynamic figure/ground segmentations in challengi ng scenarios with outlier rejection, model rigidity control and temporal adaptation (as described in s ection 3.3).
 Karsten.avi shows a person walking in an uncontrolled indoor environmen t while tracked with a handheld camera. After we manually label the frame 1, the for eground/background appearance model starts to develop, classify new frames and get updated online. Eight Example tracking frames are shown in Figure 5. Notice that the significant non-rigid d eformations and large scale changes of the walking person, while the original background is comple tely substituted after the subject turned his way. In frame 258, we manually eliminate some false posit ives of the figure. The reason for this failure is that some image regions which were behind the subject begin to appear when the person is walking from left to the center of image (starting f rom frame 220). Compared to the online foreground/background appearance models by then, these ne wly appearing image regions have quite different appearance from both the foreground and the backg round. Therefore the foreground X  X  spatial prior dominates the classification. We leave this is sue for future work. 4.3 Non-rigid Object Tracking from Surveillance Videos We can also apply our nonparametric treatment of dynamic ran dom patches in Figure 2 into track-normally capture small non-rigid figures, such as a walking p erson or running car, in low contrast and low resolution format. Thus to adapt our method to solve t his problem, we make the follow-ing modifications. Because our task changes to localizing fig ure object automatically overtime, we can simply model figure/ground regions using rectangles and therefore no pre-segmentation [6] is needed. Random figure/ground patches are then extracted fro m the image regions within these two rectangles. Using two sets of random image patches, we train an online classifier for figure/ground classes at each time step, generate a figure appearance confid ence map of classification for the next frame and, similarly to [1], apply mean shift [4] to find the ne xt object location by mode seeking. In our problem solution, the temporal evolution of dynamic i mage patch appearance models are executed by the bidirectional consistency check and resamp ling described in section 3.3. Whereas [1] uses boosting for both temporal appearance model updati ng and classification, our online bi-nary classification training can employ any off-the-shelf c lassifiers, such as k-Nearest Neighbors (KNN), support vector machine (SVM). Our results are favora bly competitive to the state-of-the-art algorithms [1, 9], even under more challenging scenario. namic foreground-background extraction in images and vide os using non-parametric appearance models produces very promising and reliable results in a wid e variety of circumstances. For track-matting X  problem [15, 25] by robust and automatic learning. For surveillance video tracking, our results are very competitive with the state-of-art [1, 9] un der even more challenging conditions. Our approach does not depend on an image segmentation algori thm that totally respects the bound-aries of the foreground object. Our novel bidirectional con sistency check and resampling process has been demonstrated to be effectively robust and adaptive . We leave the explorations on super-vised dimension reduction and density modeling techniques on image patch sets, optimal random patch sampling strategy, and self-tuned optimal image patc h size searching as our future work. In this paper, we extract foreground/background by classif ying on individual image segments. It might improve the figure/ground segmentation accuracy by mo deling their spatial pairwise relation-ships as well. This problem can be further solved using gener ative or discriminative random field (MRF/DRF) model or the boosting method on logistic classifie rs [11]. In this paper, we focus on learning binary dynamic appearance models by assuming figur e/ground are somewhat distribution-wise separatable. Other cues, as object shape regularizati on and motion dynamics for tracking, can be combined to improve performance.

