 With the ever-growing amount of digital ima ge data in multimedia databases, there is a great requirement for algorithms that can provide e ff ective semantic indexing. Cate-gorizing digital images only using keywords is the quintessential, but not always ex-ecutable example in image classification tasks. Face recognition (FR) is one typical image classification problem. Sev eral aspects contribute to the di ffi culty of FR prob-lem including the large variability in variance, illumination, pose, occlusion and even disguise of di ff erent subjects.

To design realistic FR systems, researcher s usually focus on feature extraction of facial images and the generalization of cla ssifiers. The testing sample from the same mance. Although the testing sample might be corrupted, the training data sets are commonly assumed to be well taken in some desired conditions including reasonable illumination, pose, variations and without occlusion or dis guise. When applying exist-ing face recognition methods for practical s cenarios, we will need to throw away the corrupted training images, and we might thus encounter small sample size and over-fitting problems. Moreover, the disregard of corrupted training face images might give up some valuable information for recognition. Inspired by the sparse coding mechanism of human vision system [1][2], and with the rapid development of 1 -norm minimiza-tion techniques in recent years, the sparse repre sentation classification (SRC) ideas have been successfully used in various machine vision and pattern recognition applications [3][4][5][6]. Though interesting classification results have been reported in documen-tations, more investigations need to be made in order for a clearer understanding about the relationship between object representa tion and classification. Since SRC requires the training images to be well aligned for reconstruction purposes, [7] and [8] further extend it to deal with face misalignment a nd illumination variations. [5] also proposes modified SRC-based framework to handle outliers such as occlusions in face images. However, the above methods might not generalize well if both training and testing im-ages are corrupted.

To address this issue, we propose formulating the face recognition problem under a matrix completion framework fueled by th e recent advances in low-rank (LR) matrix recovery [9][10][11], together with the discriminant regularization denoted by within-class scatter and between-class scatter [12]. In this paradigm, low-rank matrix approxi-mation is solved in a supervised manner as the whole label information of the training database is accessible. That is, we regularize the representative basis derived from stan-dard LR matrix recovery using class-specific discriminant criterion which is motivated by Fisher criterion, and plays an important role in face recognition tasks [12][13][14]. By introducing this type of regularization, our matrix completion algorithm is able to capture discriminative portions extracted from di ff erent classes. 2.1 Discrimination in Face Recognition The face recognition literature is fairly dense and diverse and thus cannot be surveyed in its entirety in this limited space. In this pa per, we focus on the class of face recogni-tion approaches called subspace methods that are more closely related to our method. A prime instance of such methods is Eigenfaces [15], which attempts to group images by minimizing data variance. Fisherfaces [1 2], due to finding a subs pace that minimizes the within-class distances while maximizing the between-class distances at the same time, achieves much better classification p erformance than Eigenfaces in face recog-nition problem. Some other subspace met hods are geometrically inspired where the emphasis is on identifying a low dimensional sub-manifold on which the face images lie. The most successful of these methods include those which seek to project images to a lower dimensional subspace such that the local neighborhood structure present in the training set is maintained. These include Laplacianfaces [16], Locality Preserv-ing Projections (LPP) [17], Orthogonal Laplaci anfaces [18], Marginal Fisher Analysis (MFA)[19] etc.. Over time, improvements on discrimination of these methods have ap-peared in [20][21][22][23][24]. These generalizations seriously make the discriminant regularization as an indispensable part of their models, and therefore great improve-ments can be witnessed.
 2.2 Sparse Representation-Based Classification Recently, Wright et al [4] proposed a sparse rep resentation-based classification algo-rithm for face recognition. In SRC-based algorithms, each testing image is regarded as a sparse linear combination of the whole training data by solving an 1 minimization problem, and very impressive results were reported in [4]. Several works have been pro-posed to further extend SRC-based algorithms for improved performance. For example, [25] utilizes a LASSO type regularization for computing the joint sparse representa-sparse regularization for hierarchical sparse coding. Although promising face recogni-tion results were reported by SRC-based algorithm, it still requires clean face images for training and thus might not be preferable for real-world scenarios. If corrupted train-ing data is presented, SRC-based algorithms tend to recognize testing images with the same type of corruption and thus lead to poor performance. In the following section, we will introduce our proposed method for robus t face recognition, in which both training and testing data can be corrupted. 2.3 Matrix Recovery via Rank Minimization Low-rank matrix recovery is a procedure for reconstructing an unknown matrix with low-rank or approximately low-rank constraints from a sampling of its entries. This problem is motivated by the requirement of inferring global structure from a small num-ber of local observations. [10], a breakthrough in matrix completion algorithms, states that the minimization of the rank functio n under broad conditions can be achieved us-ing the minimizer obtained with the nuclear norm (sum of singular values). Since the natural reformulation of the nuclear norm gives rise to a semi-definite program, exist-ing interior point methods can only handle problems with a number of variables in the order of the hundreds. Recently, Robust PCA method [9] has been proved to achieve the state-of-the-art performance using Augmented Lagrange Multipliers (ALM) method [11]. The proposed algorithm is also solved within the framework of ALM due to its fast e ffi ciency. In the context of computer vision and pattern recognition, minimization of the nuclear norm in matrix completion has been applied to several problems: struc-ture from motion [27], RPCA [9][28], subspace alignment [29], subspace segmentation [30] and signal denoising [31] etc.. 3.1 Problem Setting Given the original dataset X = [ x 1 , x 2 ,..., x n ]  X  R D  X  n consists of n columns, each column denotes a sample. Low-rank matrix recovery decomposes X into the following form where A is a low-rank matrix, and E is a sparse matrix. The dimension of matrices A and E is the same as X . According to [10], the solution of eq(1) can be solved by ALM [11] method by optimizing the following model where  X  denotes nuclear norm, and 1 denotes 1 norm. 3.2 Within-Class and Between-Class Scatters Assume that all the labels of data X are available. Specifically, let x s i denote the i -th matrices in the following manner which is di ff erent from Fisherfaces [12].
Let w s denote the within-class scatter of class s . Define it as Let X s = [ x s 1 , x s 2 ,..., x s c in class s ,and e c Rewriting eq(3) shows where Tr denotes trace operator of matrix. Thus we have where D s = I s  X  2 c
Next, we can define the between-class scatter of s -th class with the other classes where c is the number of classes. Following similar formulations from eq(3) to (5), we can rewrite eq(6) as where B 1 = c  X  1 3.3 Low-Rank Matrix Rec overy Discrimination Although low-rank matrix recovery decomposes the original data X and produces a low-rank matrix A together with a sparse error matrix E for better representation purpose, as shown in eq(1), the derived low-rank matrix A might not contain su ffi cient discriminat-ing information. Assume that the original X represents face image data, we can rewrite it into class-wise form X = [ X 1 , X 2 ,..., X c ].

Based on the within-class scatter and between-class scatter matrices shown in eq(5) and (7), it is a natural idea of adding a discriminant regularization to the low-rank matrix recovery problem shown in eq(1) class scatter and between-class scatter of s -th class, respectively. Like LDA or Fish-expect that the samples within the same class cluster as close as possible and samples between classes separate as far as possible in the learned low-rank matrix A .Theterm
A class and between-class scatters, which is penalized by the parameter  X  balancing the low-rank matrix approximation and discrimi nation. We refer to eq(8) as low-rank ma-trix recovery with discriminant regularization.

Meanwhile, we can rewrite ( w s ( A s )  X   X  s ( A s )) into the following form where As b 3 is irrelevant to A s , the optimization of eq(8) can be rewritten as
The optimization of eq(11) can be solved by ALM [11]. The general method of ALM is introduced for solving the following constrained optimization problem The corresponding ALM function of eq(12) is defined as Algorithm 1 General Method of ALM where Y is a Lagrange multiplier matrix and  X  is a positive scalar. The solution to eq(13) is outlined as Algorithm1.

In the proposed eq(11), let X = ( A s , E s ), then respectively. The ALM function of our eq(11) is To solve eq(15), we can optimize A s , E s and Y s iteratively.  X  Updating A s :  X  Updating E s : Once we obtain A s and E s , Y s can be updated using the 4th iteration of Algorithm1. The whole method we proposed is described in Algorithm2.
 Algorithm 2 Low-rank Matrix Recovery with Discrimination 3.4 LR with Discrimination for Face Recognition Occlusion is a common challenging encounter ed in face recognition tasks, such as eye-glasses, sunglasses, scarves and some obj ects placed in front of the faces. Moreover, even in the absence of an occluding object, violations of an assumed model for face appearance may act like occlusions: e.g., shadows due to extreme illumination. Robust-ness to occlusion is therefore essential to practical face recognition system. If the face images are partially occluded, popular rec ognition methods based on holistic features such Eigenfaces [15], Fisherfaces [12] and L aplacianfaces [16] would lead to unaccept-able performance due to the corruption of the extracted features. Although SRC-based algorithm [28] achieves better results in r ecognizing occluded testing images, it still requires unoccluded face images for training and thus might not be preferable for real application scenarios.

Low-rank matrix recovery has been applied to alleviate the aforementioned problems by decomposing the collected data matrix into two di ff erent parts, one is a representa-tion basis matrix of low rank and the other is the corresponding sparse error, as shown in Fig.1.
We can find out from Fig.1 that when the standard low-rank matrix recovery is com-bined with discrimination, the face images within the recovered representation basis matrix tend to be more similar to each other for the same subject, which means more compactness exists within the same classes and dissimilarity between di ff erent classes. In addition, we also can conclude from Fig.1 that the sparse error with discrimination can remove more sparse noise. As a result, the representation basis matrix of low-rank recovery with discrimination has a better representative ability than the original version. Since the face images usually lie in high dimensional spaces, traditional dimensionality reduction techniques, like PCA or LDA, can be performed on the recovered representa-tion basis matrix. As a result, the derived subspace can be applied as the dictionary for training and the testing purposes. In the recognition stage, one can also use SRC-based classification strategy to identify the i nput image. Our scheme for face recognition is described as Algorithm3.
 Algorithm 3 LR with Discrimination for Face Recognition In this section, we perform the proposed method shown in Algorithm3 on publicly available databases for face recognition to demonstrate the e ffi cacy of the proposed classification algorithm. We will first examine the role of feature extraction within our framework, comparing performance across v arious feature spaces and feature dimen-sions, and comparing to several popular methods. Meanwhile, We will then demonstrate the robustness of the proposed algorithm to corruption and occlusion. Finally, the ex-perimental results demonstrate the e ff ectiveness of sparsity as a means of validating testing images.

Besides the standard low-rank matrix r ecovery without discrimination and our pro-posed method, we also consider Nearest Neighbor (NN), SRC [4], and LLC [32] for comparisons. Note that LLC can be regarded as an extended version of SRC exploiting data locality for improved sparse coding, and the classification rule is the same as that of SRC. To evaluate our recognition performance using data with di ff erent dimensions, we project the data onto the eigenspace derived by PCA using our LR with discrimination models. For the standard LR approach, the eigenspace spanned by LR matrices without discrimination is considered, while those of other SRC based methods are derived by thedatamatrix X directly. We vary the dimension of the eigenspace and compare the results in this section. 4.1 Two Databases  X  The Extended Yale B database consists of 2 , 414 frontal face images of 38 indi- X  The AR database consists of over 4 , 000 frontal images for 126 individuals [34]. 4.2 Results On the Extended Yale B Database. For each subject, we randomly select 10 , 20 and 30 images of each subject for training respect ively, and the left images for testing. Ran-domly choosing the training set ensures that our results and conclusions will not depend on any special choice of the training data. We vary the dimension of the eigenspace as 25, 50, 75, 100, 150, 200, 300 and 400 to compare t he recognition performance between di ff erent methods. All experiments run ten times and the average results are shown in Ta b l e 1-3.

It is clear from those Tables mentioned above that the proposed method consistently achieves higher recognition rates than other NN and SRC-based approaches. For exam-ple, at dimension = 100, our method achieves a better recognition rate at 96 . 2%, and those for LR, SRC, LLC, and NN are 95 . 4%, 94 . 5%, 89 . 7%, and 66 . 8%, respectively (see Table3). Repeating the above experiments using di ff erent training images for each person, we can confirm from these empirical results that the use of LR method allevi-ates the problem of severe illumination variations even when such noise is presented in both training and testing data. Furthermore, when discrimination is taken into account as proposed in the paper, LR method exhibits enhanced classifi cation capability and thus outperforms the standard LR algorithm.
 On AR Database. In the experiment, a subset of the dataset consisting of 50 male subjects and 50 female subjects was chosen. The images are cropped with dimension 165  X  120. Di ff erent from [4], for each subject, bot h neutral (four neutral faces with di ff erent lighting conditions and three faces with di ff erent expressions) and corrupted images (three faces with sunglasses and three faces with scarfs) taken at session 1 are used for training, and session 2 for testing. Specifically, we consider the following sam-ple selection for training: 7 neutral images plus 3 sunglass images; 7 neutral images plus 3 scarf images; 7 neutral images plus 3 sunglass images and 3 scarf images. We vary the dimension of the eigenspace as 25, 50, 75, 100, 150, 200, 300 and 400 to com-pare the recognition performance between di ff erent methods. The experimental results are visualized in Fig.3.
From these three figures, we see that the proposed method outperforms all other algorithms across di ff erent dimensions. It is worth noting that with the increase of oc-clusion (from sunglass ro scarf), the recognition r ates of all the approaches are severely degraded, which can be seen from Fig.3(a) and Fig.3(b). In addition, with the increase of occluded images in the training set, the performances of all the approaches are also severely degraded which can be seen from Fig.3(c). These two cases indicate that the direct use of corrupted training image data will remarkably make the recognition results worse. In this paper, a low-rank matrix recovery algorithm with discriminant regularization is proposed. The discrimination regularizer is motivated by Fisher criterion which plays an important role in classification tasks. The introduction of this kind of regularizer into low-rank matrix recovery promotes the discrimination power in the learned representa-tion basis. We also show that the proposed optimization algorithm can be formulated by augmented Lagrange multipliers. When app lied to face recognition problem, the pro-posed algorithm demonstrates robustness to severe occlusions of face images even in the training set. The experiments has shown that our method achieves the state-of-the-art recognition results.
 Acknowledgments. The authors confirm that the research was supported by National Natural Science Foundation (No.61170109, No.61100119 and No.61272468), and Sci-ence and Technology Planning Project of Zhejiang Province (No.2012C21021), China.
