 With advancement in science and technology, computing syst ems are becoming increasingly more complex with an increasing v ari-ety of heterogeneous software and hardware components. The y are thus becoming increasingly more difficult to monitor, manag e and maintain. Traditional approaches to system management hav e been largely based on domain experts through a knowledge acquisi tion process that translates domain knowledge into operating ru les and policies. This has been well known and experienced as a cumbe r-some, labor intensive, and error prone process. In addition , this process is difficult to keep up with the rapidly changing envi ron-ments. There is thus a pressing need for automatic and efficie nt approaches to monitor and manage complex computing systems . A popular approach to system management is based on analyzin g system log files. However, some new aspects of the log files hav e been less emphasized in existing methods from data mining an d machine learning community. The various formats and relati vely short text messages of log files, and temporal characteristi cs in data representation pose new challenges. In this paper, we will d escribe our research efforts on mining system log files for automatic man-agement. In particular, we apply text mining techniques to c atego-rize messages in log files into common situations, improve ca tego-rization accuracy by considering the temporal characteris tics of log messages, and utilize visualization tools to evaluate and v alidate the interesting temporal patterns for system management.
 System Log, Categorization, Temporal Information, Visual ization, Naive Bayes, Hidden Markov Model When problems occur, traditional approaches for trouble sh ooting rely on the knowledge and experience of domain experts to figu re out ways to discover the rules or look for the problem solutio ns laboriously. It has been estimated that, in medium and large com-panies, anywhere from 30% to 70% of their information techno logy resources are used in dealing with problems [22]. It is unrea listic and inefficient to depend on domain experts to manually deal w ith complex problems in ever-changing computing systems.
 Modern computing systems are instrumented to generate huge amounts of system log data. The data in the log files describe t he status of each component and record system operational chan ges, such as the starting and stopping of services, detection of n etwork applications, software configuration modifications, and so ftware execution errors. Analyzing log files, as an attractive appr oach for automatic system management and monitoring, has been enjoy ing a growing amount of attention. However, several new aspects of the system log data have been less emphasized in existing ana lysis methods from data mining and machine learning community and pose several challenges calling for more research. The aspe cts in-clude disparate formats and relatively short text messages in data reporting, asynchronism in data collection, and temporal c haracter-istics in data representation.
 First, the heterogeneous nature of the system makes the data more complex and complicated [10]. As we know, a typical comput-ing system contains different devices (e.g., routers, proc essors, and adapters) with different software components (e.g., opera ting sys-tems, middleware, and user applications), possibly from di fferent providers (e.g., Cisco, IBM, and Microsoft). These various com-ponents have multiple ways to report events, conditions, er rors and alerts. The heterogeneity and inconsistency of log formats make it difficult to automate problem determination. For example , there are many different ways for the components to report the star t up process. Some might log  X  X he component has started X , while o thers might say that  X  X he component has changed the state from star ting to running X . This makes it difficult to perform automated ana lysis of the historical event data across multiple components whe n prob-lems occur as one need to know all the messages that reflect the same status, for all the components involved in the solution [26]. To enable automated analysis of the historical event data ac ross multiple components, we need to categorize the text message s with disparate formats into common situations. Second, text mes sages in the log files are relatively short with a large vocabulary s ize [25]. Hence, care must be taken when applying traditional documen t pro-cessing techniques. Third, each text message usually conta ins a timestamp. The temporal characteristics provide addition al context information of the messages and can be used to facilitate dat a anal-ysis.
 In this paper, we describe our research efforts to address th e above challenges in mining system logs. In particular, we propose to mine system log files for computing system management by acquirin g the needed knowledge automatically from a large amount of hi stor-ical log data, possibly from different types of information sources such as system errors, resource performance metrics, and tr ouble ticket text records. Specifically, we will apply text mining tech-niques to automatically categorize the text messages into a set of common categories, incorporate temporal information to im prove categorization performance, and utilize visualization to ols to eval-uate and validate the interesting temporal patterns for sys tem man-agement. A preliminary version of this paper has been presen ted as a short paper [20] at The 2nd IEEE International Conferenc e on Autonomic Computing (ICAC-05) It should be note that our framework is complementary to the c ur-rent knowledge-based approaches, which are based on elicit ation of knowledge from domain experts. Automated log data analys is can be performed without much domain knowledge and its resul ts provide guidance for network managers to perform their jobs more effectively. Moreover, the available domain knowledge can be used to validate, improve, and refine data analysis.
 The rest of the paper is organized as follows: Section 2 appli es text mining techniques to categorize text messages into a set of c om-mon categories, Section 3 proposes two approaches of incorp o-rating temporal information to improve the categorization perfor-mance, Section 4 discusses visualization can be used to disc over, interpret and validate the temporal patterns/relationshi ps for sys-tem management, Section 5 presents our experimental result s, and finally Section 6 provides conclusions and discussions. The disparate logging mechanisms impede problem investiga tion because of no standard approach for classifying them [26]. T wo components experiencing the same problem may use different for-mats to report the same information. For instance, when comp onent A starts running, it reports  X  X  has started X  in log file. Howev er, when component B starts running, it may report  X  X  has begun ex e-cution X . Although both messages have the same content meani ng, they are reported differently. This makes it difficult to cor relate logs from across different components when problems occur. In order to create consistency across similar fields and impr oves the ability to correlate across multiple logs, it is necessa ry to trans-form the messages in the log files into a common base [3] (i.e., a set of common categories). In this paper, we first manually de ter-mine a set of categories as the basis for transformation. The set of categories is based on the CBE (Common Base Event) format established by IBM initiative [26]. CBE provides a finite set of canonical situations after analyzing thousands of log entr ies across multiple IBM and non-IBM products. Although we use CBE as an internal presentation here, the problem and the proposed ap proach are generic and can be extended for other data formats as well . Specifically, the set of categories 1 includes start , stop , dependency , create , connection , report , request , configuration , and other . The category start deals with the start-up process of the component. The category stop deals with the exit process. The category de-pendency handles messages which report components cannot find some features or other components. The category create handles the creation problems of components. The category connection is used to identify aspects about a connection to another compo nent. The category report deals with performance, task-relative reporting messages. The category request identifies the completion status of a request. The category other identifies the blank messages or mes-sages having vague meanings. Given the set of common categories, we can then categorize th e messages reported by different components into the prescri bed cat-egories. This can be viewed as a text categorization problem where the goal is to assign predefined category labels to unlabeled doc-uments based on the likelihood inferred from the training se t of labeled documents [9; 24]. investigating approaches to infer categories from the log d ata auto-matically.
 In our work, we use naive Bayes as our classification approach as it is among the most successful known algorithms for learning i n text categorization [23]. System log files usually have a large si ze of vo-cabulary and most of the system log messages contain a free-f ormat 1024-byte ASCII description of the event [25]. Naive Bayes i s a simple practical generative classification algorithm base d on Bayes Theorem with the assumption of class conditional independe nce. Basically it assumes that the text data is generated by a para metric model, and uses training data to calculate Bayes-optimal es timates of the model parameters [17]. Then, it classifies test data us ing the generative model by calculating the posterior probabil ity that a class would have generated the test data in question. The mo st probable class is then assigned to the test data [19]. Formally, suppose there are L categories, denoted by C , C 2 ,  X  X  X  C L , and each category is generated from some probability distribution. Each document d i is created by first selecting a category according to the prior P ( C j ) followed by the generation according to the distribution of the chosen cate gory P ( d i | C j ) . We can characterize the likelihood of a document with the sum of probability over all the categories Given a set of training samples S , the naive Bayes classifier uses to estimate P ( d i | C j ) and P ( C j ) . The estimated probabilities are typically calculated in a straightforward way from trainin g data. To classify a new sample, it uses Bayes rule to compute class pos terior The predicted class for the document d i is then just argmax bility model for document generation of each category. More details on Naive Bayes can be found in [19]. The classification performance achieved by the Naive Bayes c las-sifier in our practice is not satisfactory as will shown in Sec tion 5. In many scenarios, text messages generated in the log files us u-ally contain timestamps. The temporal characteristics pro vide ad-ditional context information of the messages and can be used to facilitate data analysis. As an example, knowing the curren t status of a network component would help forecast the possible type s of messages it will generate. These structural constraints ca n be natu-rally represented using naive Bayes algorithm and hidden Ma rkov models [11]. In this section, we describe two approaches of u ti-lizing the temporal information to improve classification p erfor-mance. Our experiments in Section 5 show that both approache s greatly improve the categorization accuracy. To facilitate the discussion, Table 1 gives an example of the sys-ample, four columns, i.e., Time , EventSource , Msg and State are listed. Time represents the generated timestamp of log messages, EventSource identifies the component which reports the message, \ invdelta.tmp. dependency \ invdelta.tmp. dependency Msg lists the content of text message, and State labels the current category.
 If a sequence of log messages are considered, the accuracy of cate-gorization for each message can be improved as the structure rela-tionships among the messages can be used for analysis. For ex am-ple, the components usually first start up a process, then sto p the process at a later time. That is, the stop message usually occurs af-ter the start message. For another example, as shown in Table 1, the component Inventory Scanner will report dependency messages if it cannot find some features or components. This happens especi ally just right after it generates the create message X   X  X annot create *  X  (* is the abbreviation of arbitrary words. It represents cer tain com-ponents or features here). Thus it is beneficial to take the te mporal information into consideration.
 In order to take the relationships between adjacent message s into account, we make some modifications to the naive Bayes algo-rithm. Suppose we are given a sequence of adjacent messages D = ( d 1 , d 2 ,  X  X  X  , d T ) . let Q i be the category labels for mes-sage d i (i.e., Q i is one of C 1 , C 2 ,  X  X  X  C L ). Now we want to classify d i +1 . Note that Assuming that d i +1 is conditionally independent of Q Q once Q i is determined, P ( d i +1 | Q i ) is then fixed. Hence maximizing P ( Q i +1 | d i +1 , Q i ) is equivalent to maximizing P ( Q i +1 | Q i ) P ( d i +1 | Q i +1 ) . Observe that P ( d in order to maximize P ( d i +1 | Q i +1 ) , it is enough to maximize P ( Q i +1 | d i +1 ) . Therefore, in summary, we assign d egory Q i +1 which is In other words, we aim at maximizing the multiplication of te xt classification probability P ( C j | d i +1 ) and state transition probabil-ity P ( C j | Q i ) . The transition probability P ( C j mated from training log files. Hidden Markov Model(HMM) is another approach to incorporat e the temporal information for message categorization. The t empo-ral relations among messages are naturally captured in HMM [ 13]. The Hidden Markov Model is a finite set of states, each of which is associated with a (generally multidimensional) probabi lity distri-bution. Transitions among the states are governed by a set of prob-abilities called transition probabilities [21]. In a parti cular state an outcome or observation can be generated, according to the as so-ciated probability distribution. The model describes a pro babilis-tic generative process whereby a sequence of symbols is prod uced by starting in some state, transitioning to a new state, emit ting a symbol selected by that state, transitioning again, emitti ng another symbol and so on until a designated final state is reached. Ass o-ciated with each of a set of states, S = { s 1 ,  X  X  X  , s n ability distribution over the symbols in the emission vocab ulary K = { k 1 ,  X  X  X  , k m } . The probability that state s j vocabulary item k is written P ( k | s j ) . Similarly, associated with each state is a distribution over its outgoing transitions. The proba-bility of moving from state s i to state s j is written P ( s is also a prior state distribution  X  ( s ) . Training data consists of several sequences of observed emissions, one of which would be written { o 1 ,  X  X  X  , o x } [5].
 In our experiment, the category labels we specified in the abo ve sections are regarded as states. The emitting observation s ymbols are the log messages corresponding to their state labels. HM M ex-plicitly considers the state transition sequence. It can al so be used to compare all state paths from the start state to the destina tion state, and then choose the best state sequence that emits a ce rtain observation sequence. So it works well in labeling continuo us log messages. When one log message has been assigned several com -petitive state labels by text classification, HMM selects a c ertain la-bel by traversing the best state sequence. The state transit ion prob-ability is calculated from the training log data sets, for ex ample, in our experiment the probability of state create to state dependency is 0.4470. The probability of emitting messages can be estim ated as the ratio of the occurrence probabilities of log messages to the occurrence of their states in the training data. Viterbi alg orithm is used to find the most possible state sequence that emits the gi ven log messages, that is, finding the most possible state labels to assign to these log messages [12].
 The advantage of HMM lies in the fact that it calculates all po ssi-ble state paths from the beginning state to the current state . As for some messages that have several competitive state labels, t he text classification probability and the occurrence probability are com-bined to calculate the observation probability. For exampl e, the message  X  X he Windows Installer initiated a system restart t o com-plete or continue the configuration of  X  X icrosoft XML Parser  X . X  has 0.4327 probability to be categorized into configuration state, and has 0.4164 probability to be labeled as stop state. This means that this message can be possibly emitted by two states. In this ca se, the observation probability not only needs to consider the occu rrence probability, but also the text classification probability. Once the messages in the log file are transformed into common c at-egories, it is then possible to analyze the historical event data across multiple components to discover interesting patterns embe dded in the data. Each message in log files usually has a timestamp and we will try to discover the interesting temporal patterns. Tem poral pat-terns of interest appear naturally in the system management appli-cations [7]. Specifically, a computer system problem may tri gger a series of symptom events/activities. Such a sequence of sy mp-tom events provides a natural signature for identifying the root cause. For example, when component A reports  X  X  is stopped X , how should we respond to this stop message? Is this a failure in the component A or just a natural exit after successful compl etion of tasks? In fact, the event  X  X  is stopped X  does not necessari ly in-dicates that an errors or a failure happened. We noticed that in our log data files, the stop message often occurs after the component reports dependency message (i.e., it cannot find certain features or components). In this case, the stop message signifies the failure of a task execution. However, if there is no dependency message before the stop message, the stop message usually indicates the comple-tion of a task. Thus to decide the proper actions in response t o the stop messages, we need to check what happens right before them. Thus knowing the temporal patterns can help us pinpoint the r oot cause and take proper actions.
 Visualization can help the users understand and interpret p atterns in the data [6; 27]. For example, a scatter plot can help netwo rk operators to identify patterns of significance from a large a mount of monitoring data [4]. Once we categorize each message into a set of categories (or event types), each message can then be r epre-sented as a three-tuple &lt; s, t, c &gt; where s is the category label or event type, t is the timestamp, and c is the source component o f this log message. We can use visualization tools to discover, int erpret and validate interesting patterns. We will describe our vis ualization effort in Section 5. In this section, we present our experimental results. Secti on 5.1 de-scribes the log data generation, Section 5.2 gives the exper imental results using Naive Bayes, Section 5.3 shows the results usi ng the two approaches for incorporating temporal information, Se ction 5.4 illustrates our visualization efforts on the log files. The log files used in our experiments are collected from sever al different machines with different operating systems in the School of Computer Science at Florida International University. W e use logdump2td (NT data collection tool) developed by Event Min ing Team at IBM T.J. Watson research center. The raw log files has 1 4 columns, which are sequence, time, EventType, ReportTime, Log-TypeName, LogTypeID, EventSourceName, EventSource, Host -Name, HostID, severity, Category, and Msg. To preprocess te xt messages, we remove stop words and skip html labels. We use ac -curacy, defined as the the proportion of messages that are cor rectly assigned to a category, to evaluate the classification perfo rmance and use the random 30-70 data split for training and testing. Our first experiment is on using naive Bayes classifier to cate gorize log messages. The classification tool is built on the Bow (A To olkit for Statistical Language Modeling, Text Retrieval, Classi fication training data are labeled with nine categories, i.e., configuration , connection , create , dependency , other , report , request , start , and stop . The accuracy of log categorization is about 0.7170. Table 2 lists the keywords and their probabilities in the cor respond-ing classes. These keywords have high occurrence probabili ty and information gain associated with a category and they can be u sed to describe the category.
 As we know, one difficulty for classifying log messages with v ar-ious formats is that different platforms (e.g., Windows and Linux) usually have different set of terminologies. The classifier model built from Windows Log files may not be able to achieve good per -formance on Linux log files due to the differences in vocabula ry. One of our future tasks is to map the keywords between differe nt platforms. In order to improve the classification accuracy of log messag es, we consider the time stamp of log messages and adjacent state tr an-sition as discussed in Section 3.1. Table 3 lists the state tr ansition probability P ( C j | C i ) derived from training data. The leftmost col-umn lists previous category labels C i . The top row lists current cat-egory labels C j . Every entry contains a transition probability from a row category to the corresponding column category.
 After considering the state transition probability, the ca tegoriza-tion accuracy is improved to 0.8232. We describe three examp les (shown in Table 4, Table 5, Table 6) taken from our experiment s to illustrate the effectiveness of our approach. The column s in the three tables are, from left to right, time stamps, category l abels ob-tained by incorporating transition probability (denoted a s TC ), cat-egory label without considering the transition probabilit y (denoted as Naive ), the reporting component, and the log message. In each table, the first row represents the previous message, and the second row represents the current message. By incorporating the tr ansition probability, the category of the previous message contribu tes to the decision on the category of the current message.
 In Table 7, each row corresponds to one current log message fr om the above three examples (the leftmost column identifies the time stamp of these messages). Each entry is the text classificati on prob-ability P r ( C j | d i ) , the probability of row message categorized into the column category. We can observe the labels obtained by na ive Bayes text classification on these three example messages ar e not correct, while the modified approach produces the correct la bel. For instance, in the first example, the transition probabili ty from configuration to start is 0.1869, the probability from configura-tion to configuration is 0.1111. Naive Bayes classification assigns this message to start with probability of 0.4430199. It assigns this 3 The tool can be downloaded at http://www-2.cs.cmu.edu/  X  mccallum/bow/ .
 message to configuration category with probability 0.4981456. By incorporating temporal information, our modified naive Bay es al-gorithm assign this message to start as 0 . 1869  X  0 . 4430199 &gt; 0 . 1111  X  0 . 4981456 . Our experiments show that the modified al-gorithm improves the accuracy of log categorization from 0. 7170 to 0.8232. Our implementation of HMM tool is based on the package transition probability is calculated from the training log data sets and the probability of emitting messages can be estimated as the ratio of the occurrence probabilities of log messages to the occur-rence of their states in the training model. We use Baum-Welc h procedure to learn the HMM parameters [2]. Viterbi algorith m can then be used to discover the best state sequence. Because of t he large number of emitting messages, the observation probabi lity ta-ble is omitted here. The accuracy of using HMM categorizatio n is 0.85. The experimental results on message categorization are sum ma-rized in Table 8. In summary, our experiments show that both t he modified Bayes algorithm and HMM enhance the classification a c-curacy and HMM achieves the best performance in our practice . However, constructing HMM requires more computational cos ts. 4 The package can be downloaded at http://www.cfar.umd.edu/  X  kanungo/software/software.html . Intuitively, the modified Bayes algorithm considers only ad jacent state transitions while HMM considers all possible state pa ths. The choice between the two approaches for incorporating tempor al in-formation is problem-dependent in practice.

Table 8: Performance Comparisons on Message Categorizatio n Once we categorize each message into a set of categories (or e vent types), we can use visualization tools to discover, interpr et and val-idate interesting patterns. Visualization can help the use rs under-stand and interpret patterns in the data [6; 27]. For example , a scatter plot can help network operators to identify pattern s of sig-nificance from a large amount of monitoring data [4]. We perfo rm visualization using the EventBrowser [15]. The visualizat ion of events makes it convenient for people to find interesting tem poral patterns. The temporal pattern found and validated can be us ed to provide better mechanisms for system management, especi ally when complex integrations are typically hard for humans to c ap-ture.
 Figure 1 is derived from a desktop machine named  X  X aven X  in th e graduate lab of the School of Computer Science at Florida Int erna-tional University. Different gray scales in Figure 1 corres pond to different components and different gray scales in Figure 2 r epresent different categories. In Figure 1, dotted lines, lying in th e ellipse range with the same gray scale, are generated from the same co m-ponent. We observe that these dotted lines in create and connection categories occur alternately. The dotted line in dependency situa-tion overlaps with that in create state. From the log file of  X  X aven X  we can easily verify that the component  X  X nventory Scanner X  corre-spond to these lines in the ellipse range. From Figure 1, we ob serve that the create problem of inventory scanner will trigger the depen-dency problem (i.e., it cannot find some features or components.). The continuing occurrences of the create and dependency problems will finally generate the connection problem. Using visualization tools, we note that the connection problems occur after the create problems for at least three days. However, the dependency prob-lems usually appear just several seconds after the create problems. The wide range of lags, from seconds to days, also demonstrat es the difficulty of using a predefined window for temporal data mini ng and shows the necessity of methods without a predefined windo w. In Figure 2, different gray scales represent different cate gories. The dotted lines lying in the ellipse range of component 6 act ually consists of create and dependency situations which appear alter-nately. The other lines in component 6 belong to the connection category. This scenario depicts the relationships among th ese three categories. Moreover, from this figure, the dotted line of co mpo-nent 6 is almost in parallel with that of component 5 X  X , with a small lag. This indicates that component 5 will report problems sy n-chronously whenever there are problems in component 6. In ad -dition, we may need to pay attention to component 7 which seem s to cause the problems in component 6 and component 5. The dott ed line in component 0 consists of start and report situations. Using the visualization tool, we observe that after component 0 st arts, it tends to keep generating report messages. Figure 3 is an example of 3D-plot view of the log data. 3D-plot will help users under stand and interpret complicated patterns. In this paper, we present our research efforts on mining log fi les for computing system management. We propose two approaches for incorporating temporal information to improve the perform ance of categorizing log messages into common categories. There ar e sev-eral natural avenues for future research. First, instead of manu-ally determining the set of common categories, we could deve lop techniques to automatically infer them from historical dat a. For in-stance, we could use clustering techniques to identify the s tructures Figure 1: The 2D plot of the  X  X aven X  data set. X axis is the time . Y axis is the state Figure 2: The 2D plot of the  X  X aven X  data set. X axis is the time . Y axis is the component from large amounts of log data [8]. Second,in practice, the n umber of different common categories for system management can be ex-temely large due to the complex nature of the system. To resol ve this issue, it would be useful to utilize the dependence rela tionships among different categories [18; 28]. Third, it would also be inter-esting to develop methods that can efficiently discover inte resting temporal patterns from the transformed log files [1; 14; 16]. [1] Rakesh Agrawal and Ramakrishnan Srikant. Mining sequen -[2] Dana Ballard. An introduction to natural computation . The [3] M. Chessell. Specification: Common [4] M. Derthick, J.A. Kolojejchick, and S.F. Roth. An intera ctive [5] Dayne Freitag and Andrew McCallum. Information extrac-[6] J. Goldstein, S.F. Roth, J. Kolojejchick, and J. Mattis. A [7] Joseph L. Hellerstein, Sheng Ma, and Chang shing Perng. [8] Anil K. Jain and Richard C. Dubes. Algorithms for Clustering [9] Thorsten Joachims. Text categorization with support ve c-[10] Jeffrey O. Kephart and David M. Chess. The vision of auto -[11] Nicholas Kushmerick, Edward Johnston, and Stephen [12] T. R. Leek. Information extraction using hidden markov mod-[13] C. Li and G. Biswas. Temporal pattern generation using h id-[14] Tao Li and Sheng Ma. Mining temporal patterns without pr e-[15] Sheng Ma and Joseph L. Hellerstein. Eventbrowser: A flex -[16] Heikki Mannila, Hannu Toivonen, and A. Inkeri Verkamo. [17] A. McCallum and K. Nigam. A comparison of event models [18] Andrew K. McCallum, Ronald Rosenfeld, Tom M. Mitchell, [19] Tom M. Mitchell. Machine Learning . The McGraw-Hill [20] Wei Peng, Tao Li, and Sheng Ma. Mining logs files for com-[21] L. R. Rabiner. A tutorial on hidden Markov models and [22] IBM Market Research. Autonomic computing core technol -[23] Irina Rish. An empirical study of the naive Bayes classi fier. [24] Fabrizio Sebastiani. Machine learning in automated te xt cate-[25] Jon Stearley. Towards informatic analysis of syslogs. In Pro-[26] Brad Topol, David Ogle, Donna Pierson, Jim Thoensen, [27] M.O. Ward. Xmdvtool: Integrating multiple methods for vi-[28] Andreas S. Weigend, Erik D. Wiener, and Jan O. Pedersen. Wei Peng received her BS degree in Computer Science from Xi X  X n Institute of Science and Technology, China in 2002. She is cu r-rently a doctoral student in the School of Computer Science a t Florida International University. Her research interests are data mining and GIS.
 Tao Li received his Ph.D. degree in Computer Science from Uni-versity of Rochester in 2004. He is currently an assistant pr ofessor in the School of Computer Science at Florida International U ni-versity. His primary research interests are data mining, ma chine learning, bioinformatics, and music information retrieva l. Sheng Ma received BS degree in electrical engineering from Ts-inghua University, Beijing China, in 1992. He received MS an d Ph.D with honours in electrical engineering from Rensselae r Poly-technic Institute, Troy NY, in 95 and 98, respectively. He jo int IBM T.J. Watson research center as a research staff member in 199 8. He became a manager of Machine Learning for Systems in 2001. His current research interests are machine learning, data mini ng, net-work traffic modeling and control, network and computer syst em management.

