 Cloud-based data management systems are emerging as scalable, fault-tolerant, and efficient solutions to manage large volumes of data with cost effective infrastructures, and more and more data analysis applications are migrated to the cloud. As an attractive so-lution to provide a quick sketch of massive data before a long wait of the final accurate query result, online processing of aggregate queries in the cloud is of paramount importance. This problem is challenging to solve because of the large block based data organiza-tion and distributed processing mode in the cloud. In this paper, we present COLA, a system for Cloud Online Aggregation to provide progressive approximate answers for both single tables and joined multiple tables. We develop an online query processing algorithm for MapReduce to support incremental and continuous computing of aggregations on joins which minimizes the waiting time before an acceptable estimate is achieved. We formulate a statistical foun-dation that supports block-level sampling for single-table online aggregations and effective estimation of approximate results and confidence intervals of statistical significance. We also develop a two-phase stratified sampling method to support multi-table aggre-gations to improve the approximate query answers and speed up the convergence of confidence intervals. We implement COLA in Hadoop, and our experiments demonstrate that COLA can deliver reasonable precise online estimates within a time period two orders of magnitude shorter than that used to produce exact answers. H.2.4 [ DATABASE MANAGEMENT ]: Systems X  Query process-ing Algorithms, Design, Performance online aggregation, MapReduce, cloud computing
In the past decade, massive data has been produced in various applications, including online transaction data, Web access logs, sensor data, scientific data, etc. Distilling the meaning from these massive data has never been in such urgent demand, for example, exploring users X  behaviors and interests will provide decision sup-port, and deep analysis of scientific data is commonly conducted to support scientific discoveries. These applications are difficult to support in traditional databases, due to the massive volumes of data, the complexity and diversities of queries. Meanwhile, cloud based data management systems provide highly efficient and cost effective solutions for massive data analytics, and applications are gradually migrated to the cloud environment.

Aggregations are among the most common query types for data analysis, which often scan enormous amount of tuples to generate summary and statistical results. Though query processing in the cloud is normally paralleled, due to the data volumes involved, an aggregation query often takes a long time to return the final result, and users may have to wait a long period of time before the whole query is executed to generate the precise answer. Since many large-scale aggregation queries are used to get a sketch or  X  X ig picture X  of the data, one promising approach to address this problem is online aggregation (OLA) [14], which can continuously provide  X  X arly re-turns X  with estimated confidence intervals. As more data is pro-cessed, the estimate is progressively refined and the confidence in-terval is narrowed until the precise result is produced. While OLA has been studied for RDBMS and streaming data management sys-tems [10, 11, 16], OLA emerges as a new research area for the cloud and is of paramount importance due to the unique charac-teristics of the cloud. First, OLA makes it possible to save cost on the pay-as-you-go cloud cost mode. Reducing query processing time will lead to immediate cost saving and is highly desired by users. For example, obtaining a result with 95% confidence level in much less time could be very attractive. Secondly, queries in the cloud is always composed of many sub-tasks executing on dif-ferent nodes, OLA could help to increase the parallelism degree and resource utilization by processing parallel subtasks in online mode. Thirdly, OLA could reduce or eliminate performance bot-tleneck caused by the slowest nodes. Since the cloud is typically a heterogeneous environment consisting of many nodes with diverse hardware setup and performance, the execution time of a query is significantly influenced by the sub-tasks in the slowest nodes. With an OLA system, the data flow between sub-tasks is pipelined and the estimate result is refined continuously, thus the performance  X  X ail X  effect could be alleviated or eliminated.

Though online aggregation techniques have been extensively stud-ied for single-site relational database, there are many challenges to adapt them to the cloud. First of all, the distributed environment of the cloud brings up the problems of processing concurrency, data distribution, data skew and other issues that must be accounted for during query processing and statistical computing. In particular, for aggregations over joined multiple tables in such environment, how to process queries in online mode to minimize the time be-fore a confident estimate is obtained requires sophisticated consid-erations. In addition, data in the cloud is typically organized and processed in blocks, which could be thousands times larger than those in traditional file systems [9, 1]. In the statistics estimate procedure in OLA, uniform-random sampling is of theoretical sig-nificance. However, taking a uniform-random sample can be in-efficient for distributed data in such organization. For example, if data is blocked on an attribute associated with the aggregation, in the worst case scenario, each sampling step can be no faster than a full scan of the data. To effectively build statistical estimators with blocked data, a major challenge is to guarantee the accuracy of the estimators while leveraging sampling as efficiently as pos-sible. Last but not least, the typical batch processing mode of the cloud does not match the requirements of online aggregation pro-cessing, where  X  X arly returns X  are generated before all the data is processed. For example, for MapReduce [8], the entire output of each map and reduce task is materialized to a local file before it can be consumed by the next stage. The operators cannot begin until their precursor operators finish.

Motivated by the requirements and challenges, we propose and develop a system COLA for Cloud Online Aggregation to bridge the gap of online aggregation and the cloud. Our contributions in-clude: 1. We propose a system architecture for online processing of 2. We develop an online query processing algorithm for MapRe-3. We formulate a statistical framework for supporting online 4. We propose a two-phase stratified sampling method for multi-5. We implement COLA in Hadoop, and our experiments demon-
The rest of this paper is organized as follows. In Section 2, we present the related work. In Section 3, we describe the system ar-chitecture and data flow of COLA. In Section 4, we analyze the sampling unit and the sampling mechanism for online aggregation on single tables, and describe the estimate and confidence interval algorithm over MapReduce. In Section 5, we describe the online processing algorithms of aggregate queries over multiple tables, in-cluding the two-phase stratified sampling method and the statisti-cal computing algorithms on table joins. Experiment results are presented in Section 6, followed by conclusions.
In general, our work in this paper is related to two fields: on-line aggregation and data sampling. Online aggregation was first proposed in [14], which focuses on single-table queries involving  X  X roup by X  aggregations. The work in [10] improves the approach in [14] by providing the large-sample and deterministic confidence interval computing methods in the case of single-table and multi-table queries. The query processing and estimate algorithms for OLA were studied in the context of joins over multi-tables [11, 16, 15]. A family of join algorithms called ripple joins were presented in [11]. Ripple joins are based on the traditional block nested-loops and hash joins, which are designed to minimize the time until an acceptably precise estimate result is available. The work in [16] extends the original ripple joins to speed up the convergence by parallelizing the query processing and sampling. However, it can not provide statistical guarantee when the exact data distribution is unknown or the memory overflows. The work in [15] tries to make query estimate and maintain probabilistic confidence bounds no matter whether the inputs fit in memory or not. The approach combines the traditional sort-merge join and ripple join algorithms, and adds a shrink phase to the query processing which runs concur-rently with the merge phase to update the estimate result. The on-line aggregation is extended in [21] to the distributed environment maintained in a distributed hash table network. All the work above is in the context of traditional databases. Nowadays, online aggre-gation research is renewed in the context of cloud computing, and some studies have been conducted based on MapReduce [7, 19]. Hadoop Online Prototye(HOP) [7] pipelines the MapReduce pro-cessing of Hadoop, which allows posterior operators consume the output of precursor operators before the precursor operators com-plete. HOP can provide the original snapshots of the MapReduce jobs at data dependent intervals, and it supports OLA by scaling up the snapshots with the job progress without any confidence bounds of the query estimate. A Bayesian framework based approach is used to implement OLA over MapReduce [19]. The approach con-siders the correlation between aggregate value of each block and the processing time, takes into account the scheduling time and pro-cessing time of each block as observed data during the estimate pro-cessing. The approach focuses on single table aggregate query con-taining one MapReduce job, without considering aggregate queries over joined multiple tables, which contain several MapReduce jobs.
Data sampling is important for online processing of aggregate queries, and much work has been done in the DBMS field. There are two levels of sampling unit in the existing sampling techniques: row-level sampling [17, 18, 3] and page-level or block-level sam-pling [5]. Row-level sampling provides true uniform-randomness, which is the basis of many approximate algorithms. However, row-level sampling can be very expensive because data is always clus-tered by blocks or pages. The block-level sampling is more effi-cient, but is prone to errors when generating statistics. The work in [5] analyzes the impact of block-level sampling on statistic esti-mation for histogram and distinct-value estimate, and proposes the corresponding statistical estimators with block-level samplings. A bi-level sampling scheme is proposed in [12], which combines the row-level and page-level sampling methods. All the above work is in the field of single-site DBMS. In the field of distributed DBMS, the work in [20] compares the accuracy and efficiency of different sampling methods for query size estimation in the parallel DBMS, by using stratified random sampling and simple random sampling with the unit of row-level and page-level. Stratified sampling is well used in the online aggregation of distributed environment [21, 16]. In the context of online aggregation in the cloud, existing work of OLA over MapReduce assumes random sampling [7, 19], and no special sampling techniques have been proposed.
In COLA, we are addressing two major challenges: how to pro-duce incremental and continuous aggregation computing to provide early returns in cloud computing architecture, and how to provide effective sampling and estimation of confidence of early returned results. We first present an overview of the architecture and work-flow of COLA, as shown in Figure 1. Data in the cloud is orga-nized into blocks and distributed over different nodes. COLA first retrieves samples continuously from the data nodes and then sends them to the online query processing executor. The query execu-tor processes the samples through MapReduce jobs and reports the intermediate results to the estimator. The estimator computes the approximate aggregation result and confidence interval based on the intermediate results, and presents it to the users. As the inter-mediate results are updated progressively, the estimator continues to refine the approximate results. Once users are satisfied with the early result, they can stop the query early before its completion.
Implementing online aggregation in the cloud requires exten-sions to the traditional MapReduce framework. First, online ag-gregation needs to provide  X  X arly returns X , so it requires that the intermediate results of all operators should be pipelined during the query processing. However, the traditional MapReduce framework is batch-oriented, which means that a consuming operator cannot start until its producing operators finish. HOP [7] is a modified version of the Hadoop MapReduce framework to allow data to be pipelined within a MapReduce job and between jobs. We imple-ment COLA based on extending the pipelining techniques of HOP, and set different pipeline granularities for different tasks. Secondly, in order to make efficient estimates, the online queries should pro-cess sample data instead of sequential data stream from data nodes. We add samplers in two phases of the MapReduce framework. The first sampler is arranged before the first job X  X  map functions in the job series, and it samples data directly from the source data. We also add samplers in the shuffle phase to allow reducers perform sampling on mappers X  outputs, and this sampler can be switched on or off depending on the query type.

COLA supports online aggregation processing over both single tables and multiple table joins. To our best knowledge, COLA is the first work on studying online aggregation for joins over MapRe-duce. During the online aggregation on single table which involves only one MapReduce job, we only adopt the sampler before map functions. To make more accurate estimates under certain data transmission and I/O cost, we take data block as the random sam-pling unit, and conduct the statistical computing based on aggre-gation results of the blocks. We implement the online processing of join aggregations through combining the repartition join [4] and ripple join [11] in the MapReduce framework. We propose a two-phase stratified sampling mechanism, which involves both the sam-plers before the map function and in the shuffle phase. The imple-mentation details of COLA are presented in the following sections.
In this paper, we focus our discussions on SUM and COUNT queries, while other aggregations such as AVG, VARIANCE and STD_DEV can be implemented through some extensions. For a table R containing |R| tuples, we consider aggregate queries of the form: SELECT op(exp( t ij )), col FROM R WHERE predicate GROUP BY col;
In the query, op is the operation of SUM or COUNT, exp is an arithmetic expression of the attributes in R, predicate is an arbitrary predicate involving the attributes, and col is one or more columns in R. When op equals to COUNT, we assume that exp reduces to the SQL  X * X  identifier. t ij represents the j  X  th tuple in block i , a random variable is constructed: X ij = | R | X  exp p ( t result group k , exp p ( t ij ) equals to exp ( t ij ) if t icate clause and belongs to group k, or equals to zero otherwise. If op is COUNT, then exp p ( t ij ) equals to 1 when t ij predicate clause, and the procedure is the same to SUM. We set X in table R to be the population, and the mean value of the popula-tion is the final aggregation result. Thus estimating the query result is transformed to the problem of estimating the mean value of the population. In the MapReduce framework, the aggregation query on single table requires one job. We present in this section the sam-pling method and statistical computation of online aggregation for single tables over MapReduce.
Sampling provides random access to the population data, and a majority of statistical estimates are based on the  X  X niform random sample X  of tuples in the table. The uniform random sampling (or simple random sampling in statistical terms) means that the possi-bility of every tuple to be selected is equal. However, the tuple-level simple random sampling is not efficient in the cloud environment. Data in the cloud is organized into blocks, which is the unit of data transmission between nodes and disk I/O in the local node. In ad-dition, a block is also the processing unit of MapReduce, which allocates one block to a mapper with consideration of locality. Re-trieving a simple random sample of size b for one mapper may cause the transmission of b blocks in the network at worst, so it will be very expensive to conduct the tuple-level simple random sampling for online aggregation in the cloud. The alternative is to adopt the block as the sampling unit. However, in this case it is less clear how to guarantee the quality of the resulting estimate, since tuples within one block may have some correlation if data is clustered by the sampling attribute. In this section we analyze the variance-error of random sampling on both tuple level and block level, and clarify in Theorem 1 that the block-level sampling can produce more accurate estimate than tuple-level sampling with the same data transmission cost in the cloud.

T HEOREM 1. Set  X   X  blk and  X   X  tpl are used to represent the mean values of simple random sample data on block level and tuple level respectively. VAR(  X   X  blk ) denotes the variance error of estimate ob-tained from block-level sampling of n block, and VAR(  X   X  the estimate variance error from tuple-level sampling of n tuples. Then both  X   X  blk and  X   X  tpl are unbiased estimate of the aggregation query, and under the same data transmission cost,VAR(  X   X  &lt;= VAR(  X   X  tpl ).

P ROOF .Set S 2 to be the variance of X ij in the population, and set S 2 b to be the variance among the blocks. The simple random sampling on block level is called cluster sampling in statistical terms, with the cluster size equal to block size B . We define two types of mean values: the mean value per block:  X  X =  X  x the mean value per tuple:  X   X  X =  X  x i / NB , where N represents the number of blocks in the population, and x i represents the sum of elements in block i . According to the characteristics of the clus-ter sampling, we can conclude that  X   X  blk is the unbiased estimate of the mean value of the population. The correlation of data in one block can be measured by the intra-cluster correlation coefficient  X  , which is computed from S and S b :  X  = S 2 b  X  S 2 ( B  X  error of  X   X  blk can be computed through the correlation coefficient: The variance error of  X   X  tpl is: Under the simple random sampling, the finite population correc-tions is negligible, thus we can compute the design effect based on equation (1) and (2):
Since ( x i  X   X  X )=( x i 1  X   X   X  X )+( x i 2  X   X   X  X
According to equation (3) and inequation (4), we can get the con-clusion: def f &lt;=1
COLA conducts the block-level random sampling in the first sampler mentioned in Section 3, it is implemented in the map task scheduling procedure of the first job. After the input file is split ac-cording to the block size, all the mappers corresponding to the file splits are added to the scheduling queue. Each time a map task is requested by the job scheduler, COLA retrieves a mapper from the scheduling queue through simple random sampling, and one block is sampled as the input data.
After the block-level sampling procedure, we can get n inde-pendent blocks with size B . Given a sampled block B i ,andset given by
According to the previous equation, the final aggregation query result can also be considered as the average of Y i ,where Y exp p ( B i ) .The sampled blocks are retrieved in random order, so ob-servations of Y i are identically distributed and independent to each other. According to the analysis of [14] based on CLT(central lim-ited theorem), the average of Y i in samples approximately obeys the normal distribution. We can obtain the corresponding formulas to compute the half-width interval with specified confidence level 100 p %:  X  n = z p  X  n / normal distribution, and  X  n is the standard deviation of n variables in the sample. Then the final 100 p % confidence interval of the aggregation result is [  X   X  blk  X   X  n ,  X   X  blk +  X  n ] When implementing the aggregation online processing over Map-Reduce, we have following considerations. First, we try to arrange more computation work on the mapper functions. Generally speak-ing, there are more mappers in the MapReduce job for aggregation queries, so arranging more work on mappers will help increase the degree of parallelism. Secondly, we design the data flow to reduce the transmission cost in the shuffle phase. Thirdly, we make the computation of aggregation estimation and confidence interval of each iteration incremental to reduce the repetitive work. The online aggregation processing on a single table involves one MapReduce job. In the map function shown in Algorithm 1, tu-ples of every block are filtered according to the predicate and trans-formed into key-value pairs. The key is the group column value of tuple t (2), and the value is equal to exp p ( t ) which is defined at the beginning of Section 4(3). In order to reduce the burden of reducers and the transmission cost in the shuffle phase, the combine function is executed after the map function as shown in Algorithm 2. The values belongs to the same group of this block are accumulated (1-4), and the output value is a structure containing two double data called "Twodouble". The first data is used to compute the variable X  X  mean value, and the second data is used to compute the variance in the reduce function (5-6).

Algorithm 1 : Map Function
After the combine function, all the values belonging to the same group are transmitted to the same reducer. The reduce function is executed each time the estimate is invoked, so it is important to make the computing process incremental and make use of the results of the previous reduce function. The reduce function is de-scribed in Algorithm 3. After the accumulation of new values in the iteration (4-8), we compute the total sum and variance by adding the sum and quadratic sum computed in the last iteration (9-11). At last, the aggregation estimate and the half-width of confidence interval of each group is computed (12-13).
Algorithm 2 : Combine Function
Algorithm 3 : Reduce Function
We propose the join algorithm that will support online process-ing for multi-table join aggregations in the form: SELECT op(exp( t 1 i , t 2 j , ..., t km )) FROM R1,R2,...,Rk WHERE predicate GROUP BY col;
In the query expression, op is the aggregate operator as COUNT or SUM. exp , predicate and col represent the same meanings with those in the single table aggregation query, except that they involve the attributes of multi-relations. In this paper, we focus on the on-line processing of equal joins, which are most commonly used in real applications. Suppose we want to perform an equijoin on two relations R and S on attributes R.c and S.d, and conduct an ag-gregation on the join results. Next we discuss the procedures on processing join aggregation, the sampling mechanism and the esti-mation algorithm.
Different from aggregation query on a single table which in-volves only one MapReduce job, the aggregation on joins involves multiple MapReddue jobs. There are several processing methods of implementing offline join queries over MapReduce, such as repar-tition join, broadcast join, semi-join, etc [4]. While offline query processing aims to minimize the time to query completion, online query processing attempts to provide precise estimate quickly, in a smooth and continuous way. The broadcast join and semi-join implemented on MapReduce have to compute global distribution information of tables before the join processing, which is adverse to the incremental computing of online aggregation. In the context of online aggregation, ripple join is proposed to support cumula-tive update of the join result in the single-node environment [11]. We design the online processing of join aggregation by combining repartition join and ripple join.

In the context of offline processing, the repartition join on the previous example involves two MapReduce jobs. The main work-flow of repartition join is depicted in the solidline rectangles of Figure 2. In Job 1, mappers conduct the filtering on two tables and arrange the join key as the key of output results. In the shuffle phase, all the tuples whose join keys belonging to the same hash bucket are sent to the same reducer. At last, the reducers execute the local joins. Every output file of Job 1issentto Job 2 as the input file of mappers. The mappers set the group-by column as the out-put key to the shuffle phase, and reducers execute the aggregation function.

In order to support online aggregation, we extend the traditional repartition join framework, which is depicted in the dotted rectan-gles of Figure 2. First, we add samplers before some of the oper-ators, which will provide random sampling data as the input. In addition, COLA allows the dataflow pipelined during the query processing, so in the reduce function of Job 1, data emitted con-tinuously from all the mappers are ripple joined. Thirdly, we add an estimate module in the reduce function of Job 2 to provide the estimate result and confidence interval of each group. The sam-pling mechanisms and the estimate algorithm are described in the following sections.
Ripple join is proposed to support join online aggregation in the single-node environment [11], and it is the mostly used method in online aggregation of joins until recently. Let exp p ( r it to zero. We use  X  to represent exp ( r i , s j ) , and the ripple join result can be signified by a matrix shown in Figure 3(a). In each sampling step, the ripple join retrieves random samples from both R and S, and joins them with the previously-seen tuples and with each other. Then it computes the estimate result and confidence interval based on these join results by taking all the elements in the two-dimensional matrix as the population. However, this sam-pling method will not be efficient in the context of MapReduce repartition join for the following reasons. First, the repartition join processing is distributed to different nodes, and conducting a cen-tralized sampling may introduce load imbalance for these nodes, which will result in inefficiency for both the query processing and estimation. Secondly, the ripple join sampling considers the whole space of two tables X  cartesian product as the statistical population. When the number of tuples satisfying the join predicate is small or there are many groups in the output, the estimation results will be less accurate and the interval convergence will be slow. In particu-lar, if the join key is the key or candidate key of one table, then there can only be one tuple satisfying the join predicate and contributing to the aggregation result in one column or one row of the matrix. Last, in the MapReduce repartition join, tuples that owing the same (a) The original ripple join sampling join key are partitioned to one reducer. However, the original ripple join does not efficiently utilize this data distribution.
Based on the previous analysis, we distribute the sampling proce-dure into different nodes, which has the similar spirit to the strati f i -ed sampling in the statistical term [6]. Set R h and S h represent the tuples sent from the two tables to the h th reducer of Job 1 respec-tively, and the join keys of R h and S h belong to the same range. We can get R S = R h S h . We define the tuples of each ta-ble sent to each reducer of Job 1asthe sampling stratum .Givena reducerID or a stratumID h , stratified sampling data from different tables is ripple joined, and we define these joined results as part h . Under this sampling mechanism, we consider the union of all the joined results of each part as the population X. As shown in Fig-ure 3(b), all the non-zero results are clustered together based on the join keys of the corresponding strata. The stratified sampling effi-ciently utilizes the data repartition in MapReduce join, and it can be easily distributed over multiple reducers. Also it improves the esti-mate accuracy and accelerates the convergence speed by excluding much of the zeroes in the cartesian product.
Stratified sampling provides a distributed sampling mechanism for join online aggregation over MapReduce. Suppose there are m reducers in Job 1, the population size is N =  X  m h = 1 We set the random variable X ij as X ij = | N | X  exp p ( r ( r , s j )  X  ( R h  X  S h ) , then the join aggregation result is the mean value of all the random variables in the population. However, ap-plying stratified sampling directly on the repartition join confronts a big challenge: the stratum to which a tuple belongs is unknown until all the data has been processed by the shuffle phase of Job 1. As a result, we cannot get the stratum distribution or stratum size before the completion of the join aggregation query, which is the base of conducting stratified sampling and computing the estima-tion.

Our general approach is to conduct a two -phase sampling mech-anism, also called double sampling [6]. As shown in Figure 2, the first phase of sampling is arranged before map functions of Job 1, and the first sampler mentioned in Section 4 is executed from R and S respectively. These sampled tuples are processed by the mappers and then repartitioned to reducers depending on the join keys. After the map functions we can get the first samples from R and S with size n r and n s respectively, which are used to estimate the stratum distribution of the population.

The second phase of sampling is conducted based on the first sample before the reduce function in Job 1, and it produces dis-tributed stratified random samples of size n r and n s from R and S respectively. After the map function, each mapper sorts the output
Algorithm 4 : Second-Phase Sampling key-value pairs and partitions them into different parts consumed by the following reducers. Algorithm4 illustrates the second-phase sampling from one table executed by reducer h before the reduce function. Each reducer executes random sampling from all the mappers X  output results, and the size of sample from each map-per is proportional to the cardinality of its output results partitioned to this reducer (5-6). The samples from each mapper X  X  output are collected by reducer h (7) and they produce a random sample of a stratum in the output of first-phase sampling from one table.
T HEOREM 2. In each step of the second phase sampling, after all the reducers of Job1 conduct Algorithm4 from one table, the collected samples produce a stratified random sample on the first samples of this table.

P ROOF . After the first-phase sampling and the mapper functions of Job 1, the output results of each mapper are partitioned into m parts, which correspond to the m reducers, and the samples from the first phase are divided into m disjoint strata. In the output re-sults of mappers, the size of data that should be consumed by re-ducer h is n hr ,and n hri represents the size of data consumed by reducer h from table r in the output of mapper i . Under the algo-rithm Second-Phase Sampling, reducer h takes random sampling from each mapper X  X  output, and each tuple of partition h in mapper i is retrieved by the probability 1 / n hri . Reducer h picks mapper i with probability n hri / n hr . So given the stratum h of table r ,every tuple has the same probability 1
After the two-phase sampling, we get n hr  X  n hs samples from part h of population X. Estimating the join aggregation result is equal to estimating the mean value  X  Y of all the variables in population X. Let  X  Y h to be the mean value of part h in population X, the mean value of part h in the first samples, and  X  y h value of part h in the second samples. The weight of each part estimated without bias by:  X  h = n hr  X  n hs /  X  m h = 1 n the following formulas to compute the estimate of  X  Y :
T HEOREM 3. The estimated mean  X  y =  X  m h = 1  X  h  X   X  y ased estimate of the join aggregation result  X  Y , that is E
P ROOF . According to Theorem 2, the second sampling is a strat-ified random sampling from the first sample. As analyzed in [11], the estimated mean of part h in the second sampling  X  y h ased estimate for the samples of part h in the first phase E (  X  y h )=  X  y
E (  X  y )= E 1 ( E 2 (  X  y ))= E 1 ( E 2 (
Under the two-phase stratified sampling, given stratum h , tuples from R h and S h are randomly retrieved. We assume that the sam-pling is performed with replacement, and the error of this assump-tion is negligible because we only sample a small fraction from the big table data. Then the samples from R h and S h can be viewed as independent and identically distributed observations. According to the analysis of [13], the conclusion of [11] can be extended to the repartition join situation to show that, as the sample size be-comes large, the estimation  X  y approximately obeys normal distri-bution with mean value  X  Y and variance  X  2 = var (  X  y dence interval is [  X  y  X   X  ,  X  y +  X  ], and  X  is computed by: where  X  2 n is the consistent estimator of  X  2 over the samples. We have computed the estimated mean value in the previous section, then the left work is to compute the variance.
 Set R h ( n hr ) and S h ( n hs ) represent the tuples sampled from R and Sinstratum h of the second sample, where n hr and n hs represent the sample size from R h and S h respectively. Note that the samples R ( n hr ) and S h ( n hs ) of each stratum are mutually independent, so the estimated mean value  X  y h is independent of  X  y l for h s represent the variance of part h in the second sample, then the consistent estimated variance of  X  y can be computed through the following formulas: Next we discuss how to compute the variance s 2 h . In fact, vari-ables in part h are not statistical independent, because they are the cross-products of tuples in R h ( n hr ) and S h ( n hs the argument [11], the variance of samples in part h is given by s =  X  2 ( R h ( n hr )) / n hr +  X  2 ( S h ( n hs )) / n hs the average of exp p ( r , s ) over all s  X  S h ( n hs ) ,then the variance of  X  ( r , R h ( n hr )) over all r  X  R h ( n the similar definition as  X  2 ( R h ( n hr )) .
As mentioned in Section 5.1, online join aggregation of two ta-bles involves two MapReduce jobs: the first job conducts the join, and the second job conducts the aggregation. After the shuffle phase of Job 1, all the tuples of the same join key are partitioned to one reducer. The reduce function processes samples of one stratum from all the tables. It not only implements the ripple join, but also computes the mean value and variance of the corresponding part, as shown in Algorithm 5. The input value of the reduce function is a structure called "Threedouble": tableMark identifies which table the value is retrieved from, exp _ col represents the column value associated with the arithmetic expression exp ,and group _ col is the group-by column value. We adopt a two-dimensional array outRes to store three statistical elements for each group in the stra-tum: the sum of exp p ( r , s ) , the quadratic sum of sum sum ( s , S h ) . sum ( r , R h ) represents the sum of exp s  X  S h and sum ( s , S h ) has the similar definition. The tuple val-ues from table R and S in one value list own the same join key.
Algorithm 5 : Reduce Function of Job1 Two buffers B R and B S are created based on tableMark ,andthey contain the values from table R and S respectively. The join is im-plemented by computing cartesian product of B R and B S through a nest loop (7-21). Each time tuple r and s are joined, the group which exp p ( r , s ) belongs to is retrieved (12), and the sum sum ( s , S h ) of this group is increased by exp p ( r , accumulation of sum ( r , B R ) is complete, the first values and sec-ond values in outRes of all the groups are updated (17-20). The third values of outRes[G][3] are updated when the nest loop ends (22-26). The part variance is computed based on formulas given in the previous subsection (29-31), and also the mean value of the part is computed (32). The output key of reduce function is set to the group key, and the mean value and variance of the part are encapsulated into the output value with partID (33-35).
The input key for mappers in Job 2 is the group key, and the input value is a Threedouble structure, which includes the partID h ,part mean value  X  y h and part variance S 2 h of each group. Map function in Job 2 does not process the records, and after the shuffle phase, all the statistics belonging to the same group are sent to one reducer. The result estimation and the interval computation are implemented in the reduce function of Job 2, as shown in Algorithm 6. The ag-gregation result is estimated through equation (6) in the previous subsection (5). The consistent estimated variance of aggregation result is computed according to equation (7) of the previous sub-section (6). Then the confidence interval half-width is computed based on the estimated variance (8).

Algorithm 6 : Reduce Function of Job2
We evaluate the performance of COLA in terms of estimate ac-curacy, convergence speed and scalability. We also study the influ-ence of data correlation and sampling unit on the performance. All the experiments are performed on Hadoop 0.19.2. We first conduct experiments over a real world dataset to evaluate COLA X  X  estimate accuracy and interval convergence speed, then we run experiments to evaluate the influences of data correlation and sampling unit on the performance. Last we evaluate the scalability of COLA.
The testbed is established on a cluster of 11 nodes connected by a 1Gbit Ethernet switch. One node serves as the namenode of HDFS and jobtracker of MapReduce, and the remaining 10 nodes act as slave nodes. Each node has a 2.33G quad-core CPU and 7GB of RAM, and the disk size of each node is 1.8TB. We set the block size of HDFS to 64MB, and configure Hadoop to run 2 mappers and 1 reducer per node.

In the experiment, we analyze the page traffic statistics of Wikipe-dia hits log. The dataset we use contains 7 months of hourly pageview statistics for all articles in Wikipedia, with 320GB of compressed data (1TB uncompressed) [2]. Based on the original traffic data of Wikipedia, we construct two tables: visit _ log and page _ size .Table visit _ log contains hits log of Wikipedia pages with three columns: pagename, language and pageviews, while table page _ size con-tains the page size information with three columns: pagename, lan-guage and pagesize. Data sizes of table visit _ log and page _ size are 300GB and 30GB respectively, and all the data files are stored in HDFS. We test aggregation queries on a single table and multi-tables with example queries shown next.
 Q1= SELECT SUM(pageviews),language FROM visit_log GROUP BY language Q2=
During the online processing of the above queries, COLA re-turns estimate result with confidence interval of every group con-tinuously. In the experiment, we set the confidence level to 95%, and update the confidence interval at every 2% increment of the query progress.
In this experiment, we run the two queries on the original wikipe-dia dataset. The accuracy of estimated aggregation result is mea-sured by relative _ error , which is computed post-hoc by: relative _ dence interval is reflected through avgResTime ,whichistheaver-age time elapsed to get the estimate result with relative _ error less than 1% and relative _ interval less than 5%. The relative _ interval
Figure 4 and Figure 5 illustrate the update of relative _ error and relative _ interval as the online aggregation of Q1 proceeds. There are ten kinds of languages analyzed in the experiment, and here we show the results of English and Polish. English is the most frequently visited language, while Polish belongs to the languages that are less frequently visited. We also show the estimate aggre-gation results of English without sampling on the source data. As Figure 5 shows, the confidence intervals become narrow as the on-line aggregation continues. Figure 4 shows that the relative errors of both English and Polish become less than 1.5% when the second estimate result is obtained at the query progress of 4%. However, the relative error of English without sampling on the source data is huge, and the average relative error of all the estimate results reaches 4.9%. So the estimate result and confidence interval com-puted without sampling is of no statistical significance. The relative interval of English reaches 4.4% when the query progress is at 6%, while the relative interval of Polish becomes less than 5% when its progress arrives at 12%, which is 2.8 times longer than that of En-glish. Among the source data, log records of English and Polish ac-count for 41% and 3.9% respectively, so the selectivity of English is much higher than that of Polish. The experiments demonstrate that COLA provides reasonable estimate within acceptable time period even for data with low selectivity.
 The performance of COLA on join aggregations is illustrated in Figure 6 and Figure 7. We show the results of English and Pol-ish with the two-phase stratified sampling, and also the results of English under the un-stratified sampling. Under the un-stratified sampling, the relative _ error keeps at least 1% larger than that of the two-phase stratified sampling before the progress reaches 75%. This is because two-phase stratified sampling utilizes the data dis-tribution after the repartition and excludes many zeroes from the population. The relative _ interval under two-phase stratified sam-pling also shrinks faster -it becomes less than 5% at the progress of 8% for English. The progress has to reach 24% for relative _ interval of English to be less than 5% under the unstratified sam-pling.

Table1 shows the avgResTime of COLA for English and Polish, and it also shows the time consumed to complete the query to pro-duce a precise result of COLA and HOP. Q1 runs for 1532.9s be-fore completing the query on HOP without online aggregation, and it takes 216.5s to obtain a reasonably precise result of Polish with low selectivity, the longest time among all the language groups. Re-turning the precise result of Q2 also saves a significant amount of time compared to completing it without online aggregation. We can also see that the time spent to complete queries on COLA is about 1% longer than the time spent on HOP without online aggregation, which demonstrates COLA has minimal overhead.
During the online aggregation of single tables, a data block is adopted as the sampling unit and the statistical computing unit. In this experiment, we evaluate the effect of data correlation on the es-timate of single table aggregation with COLA. For the real dataset, log records within the same hour are ordered by language. We
Figure 7: Relative Interval Width(Q2) time(s) English Polish COLA HOP avgResTime avgResTime Complete Complete change the data layout of real dataset to generate two extreme de-grees of correlation: records in the whole data file are laid randomly or are ordered by language. The real data layout falls between the two. Figure 8 and 9 illustrate COLA X  X  performance on estimat-ing the results of English over different data layouts. At 10% of the query process, COLA provides estimate with relative error less than 1.5% for the fully ordered data. And for real data and ran-dom data, the relative error becomes less than 1% after the query progress reaches 8%. However, the relative error on real data fluc-tuates more widely than that on random data. This is a consequence of the fact that the variance of block aggregation on real data is big-ger than the random data. The block aggregation on fully ordered data  X  the worst case of data layout  X  has the biggest variance, and the relative confidence interval becomes less than 5% after the query has processed 14%.
In this experiment, we test the performance of COLA with differ-ent data block sizes. A block is the sampling unit of the sampler be-fore map function, and it is also the processing unit in the MapRe-duce framework. We run Q1 and Q2 on COLA with four block sizes: 32MB, 64MB ,128MB and 256MB respectively, and com-pare the avgResTimes of the English group. As Figure 10 shows, queries running on the data with block size 64MB takes the short-est time to get the reasonable precise estimate. Since one block is processed by a single mapper task, a bigger block size will result in less mapper tasks of a MapReduce job. Also since every map task involves startup time, for large input data files, less number of map tasks will cost less startup time and help to reduce the total execution time of jobs. On the other hand, samples retrieved from bigger data blocks will introduce larger variances, and will result in slow interval convergence. Thus their is a tradeoff between the query execution speed and the convergence speed of different block sizes.
We evaluate the scalability of COLA by varying data sizes and node numbers, and the metric is also avgResTime of Q1 and Q2. Here node number represents the the number of slaves of the clus-ter. Figure 11 illustrates the performance of COLA running on the cluster with 10 nodes with different data sizes of table visit _ log : 75GB, 150GB, 225GB and 300GB. From the results we can see that the avgResTime of 150GB is slightly shorter than that of 75GB, and after that the avgResTime increases with low growth rate. We run Q1 and Q2 on 5-node, 10-node, 15-node and 20-node config-urations respectively, and the results are shown in Figure 12. As the number of nodes increases, the parallelism of tasks gets higher and more records are sampled at each step, thus the avgResTimes of both Q1 and Q2 decrease. This demonstrates the scalability of COLA for large datasets.
Online aggregation in the cloud makes it possible to save cost by taking acceptable approximate early answers. There are critical re-quirements to support online aggregation in the cloud, including the statistical methods to support sampling of data and confidence esti-mation of approximate results, and the query processing framework to support incremental and continuous computing of aggregations on single tables or multiple tables. There are major challenges due to the mismatch of the data storage and computing model of the cloud and the unique requirements of online aggregation. COLA provides a promising framework to bridge the gap to support on-line processing of aggregate queries in the cloud. Major contribu-tions of COLA include: a system architecture to support online ag-gregation by extending the HOP framework, statistical methods to support effective sampling and estimation of approximate results, online query processing algorithms to support aggregates on both single tables and joined multiple tables, and highly efficient perfor-mance based on the framework. To our best knowledge, COLA is the first work on studying online aggregation for joins in the cloud. Our experiments demonstrate that COLA can produce acceptable approximate answers within a time period two orders of magnitude shorter compared to those to produce exact results. We believe our methods are fundamental and can be extended to support other types of online analytical queries. This research was partially supported by the grants from the Natural Science Foundation of China (No. 91024032, 91124001, 61070055, 60833005), the Fundamental Research Funds for the Central Universities, and the Research Funds of Renmin Univer-sity of China (No. 11XNL010, 10XNI018), National Science and Technology Major Project (No. 2010ZX01042-002-003), the US National Library of Medicine (No. R01LM009239). [1] HDFS. Avaiable at http://hadoop.apache.org/hdfs/. [2] Wikipedia Page Traffic Statistics. Avaiable at [3] G. Antoshenkov. Random sampling from pseudo-ranked b+ [4] S. Blanas, J. M. Patel, V. Ercegovac, J. Rao, E. J. Shekita, [5] S. Chaudhuri, G. Das, and U. Srivastava. Effective use of [6] W.G.Cochran. Sampling Techniques . John Wiley and Sons, [7] T. Condie, N. Conway, P. Alvaro, J. M. Hellerstein, J. Gerth, [8] J. Dean and S. Ghemawat. Mapreduce: simplified data [9] S. Ghemawat, H. Gobioff, and S.-T. Leung. The google file [10] P. J. Haas. Large-sample and deterministic confidence [11] P. J. Haas and J. M. Hellerstein. Ripple joins for online [12] P. J. Haas and C. Koenig. A bi-level bernoulli scheme for [13] P. J. Haas, J. F. Naughton, S. Seshadri, and A. N. Swami. [14] J. M. Hellerstein, P. J. Haas, and H. J. Wang. Online [15] C. Jermaine, A. Dobra, S. Arumugam, S. Joshi, and A. Pol. [16] G. Luo, C. J. Ellmann, P. J. Haas, and J. F. Naughton. A [17] F. Olken and D. Rotem. Random sampling from b+ trees. In [18] F. Olken and D. Rotem. Random sampling from database [19] N. Pansare, V. R. Borkar, C. Jermaine, and T. Condie. Online [20] S. Seshadri and J. F. Naughton. Sampling issues in parallel [21] S. Wu, S. Jiang, B. C. Ooi, and K. L. Tan. Distributed online
