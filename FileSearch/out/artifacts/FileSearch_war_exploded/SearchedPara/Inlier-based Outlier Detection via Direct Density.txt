
We propose a new statistical approach to the problem of set based on the training set consisting only of inliers. Our key idea is to use the ratio of training and test data densi-ties as an outlier score; we estimate the ratio directly in a semi-parametric fashion without going through density es-timation. Thus our approach is expected to have better per-formance in high-dimensional problems. Furthermore, the applied algorithm for density ratio estimation is equipped with a natural cross-validation procedure, allowing us to objectively optimize the value of tuning parameters such as the regularization parameter and the kernel width. The al-gorithm offers a closed-form solution as well as a closed-form formula for the leave-one-out error. Thanks to this, the proposed outlier detection method is computationally very efficient and is scalable to massive datasets. Simula-tions with benchmark and real-world datasets illustrate the usefulness of the proposed approach.
 Keywords : outlier detection, density ratio, importance
The goal of outlier detection (a.k.a. anomaly detection , novelty detection , or one-class classification ) is to find un-common instances ( X  X utliers X ) in a given dataset. Outlier detection is useful in various applications such as topic de-tection in news documents [14], intrusion detection in net-work systems [24], and defect detection from behavior pat-terns of industrial machines [3, 9]. For this reason, outlier detection has been studied thoroughly in statistics, machine learning, and data mining communities for decades [7].
A standard outlier detection problem falls into the cate-gory of unsupervised learning due to lack of prior knowl-edge on the  X  X nomalous data X . In contrast, the papers [4, 5] addressed a semi-supervised outlier detection problem where examples of outlier and inlier are available as a training set. The semi-supervised outlier detection meth-ods could perform better than unsupervised methods thanks to additional label information, but such training samples are not always available in practice. Furthermore, the type of outliers may be diverse and thus the semi-supervised methods X  X earning from known types of outliers X  X re not necessarily useful in detecting unknown types of outliers.
In this paper, we address a problem of inlier-based out-lier detection where examples of inlier are available. More formally, the inlier-based outlier detection problem is to find outlier instances in the test set based on the training set con-sisting only of inlier instances. The setting of inlier-based outlier detection would be more practical than the semi-supervised setting since inlier samples are often available abundantly. For example, in defect detection of industrial machines, we already know that there is no outlier (i.e., a defect) in the past since no failure has been observed in the machinery. Therefore, it is reasonable to separate the mea-samples observed in the past and the test set consisting of recent samples from which we try to find outliers.
As opposed to supervised learning, the outlier detection problem is vague and it is not possible to universally define what the outliers are. In this paper, we consider a statistical framework and regard instances with low probability den-sities as outliers. In light of inlier-based outlier detection, samples. However, density estimation is known to be a hard problem particularly in high dimensions, so outlier detec-tion via density estimation may not work well in practice.
To avoid density estimation, we may use One-class Sup-port Vector Machine (OSVM) [19] or Support Vector Data Description (SVDD) [23], which finds an inlier region con-taining a certain fraction of training instances; samples out-these methods cannot make use of inlier information avail-able in the inlier-based settings. Furthermore, the solutions of OSVM and SVDD depend heavily on the choice of tun-ing parameters (e.g., the Gaussian kernel width) and there seems to be no reasonable method to appropriately deter-mine the values of the tuning parameters.

To overcome the weakness of the existing methods, we propose a new approach to inlier-based outlier detec-training and test data densities in a semi-parametric fash-ion. Among existing methods of density ratio estimation [1, 8, 10, 16, 21, 22], we adopt an algorithm called uncon-strained Least-Squares Importance Fitting (uLSIF) [10] for outlier detection. The reason for this choice is that uLSIF is equipped with a variant of cross-validation, so the values of tuning parameters such as the regularization parameter can be objectively determined without subjective trial and error. Furthermore, uLSIF-based outlier detection allows us to compute the outlier score just by solving a system of lin-ear equations X  X he leave-one-out cross-validation error can also be computed analytically. Thus, the proposed method is computationally very efficient and therefore is scalable to massive datasets. Through experiments using benchmark datasets and a real-world dataset of failure detection in hard disk drives, our approach is shown to compare favorably with existing outlier detection methods and other density ratio estimation methods with higher scalability.
In this section, we propose a new statistical approach to outlier detection.

Suppose we have two sets of samples X  X raining samples { x The training samples { x tr samples { x te outlier detection here is to identify outliers in the test set based on the training set consisting only of inliers. More formally, we want to assign a suitable inlier score for the the more plausible the sample is an outlier.

Let us consider a statistical framework of the outlier de-tection problem: suppose training samples { x tr dependent and identically distributed (i.i.d.) following a training data distribution with density p ples { x te with strictly positive density p cal framework, test samples with low training data densities are regarded as outliers. However, p ble in practice and density estimation is known to be a hard problem. Therefore, merely using the training data density as an inlier score may not be promising in practice.
In this paper, we propose using the ratio of training and test data densities, called the importance , as an inlier score: ing and test data densities are equivalent), the value of the importance is one. The importance value tends to be small in the regions where the training data density is low and the test data density is high. Thus samples with small impor-tance values are plausible to be outliers.

One may suspect that this importance-based approach is not suitable when there exist only a small number of outliers X  X ince a small number of outliers cannot increase the values of p drawn from a region with small p change in p For example, let the increase of p would be a suitable inlier score (see Section 4.3 for illustra-tive examples).
The values of the importance are unknown in practice, so we need to estimate them from the data samples. If we es-timate the training and test densities from the data samples, it can suffer from the curse of dimensionality . So we would like to directly estimate the importance values without go-ing through density estimation. In this section, we review such direct importance estimation methods which could be used for outlier detection.
The KMM method avoids density estimation and directly gives an estimate of the importance at training points [8].
The basic idea of KMM is to find b w ( x ) such that the mean discrepancy between nonlinearly transformed sam-ples drawn from p versal reproducing kernel Hilbert space [20]. The Gaussian kernel is an example of kernels that induce a universal reproducing kernel Hilbert space. It has been shown that the solution of the following optimization problem agrees with the true importance: min where  X  X  X  X  F denotes the norm in the Gaussian reproducing kernel Hilbert space.

An empirical version of the above problem is reduced to the following quadratic program: s.t.
 where  X  (  X  0) , B (  X  0) , and  X  (  X  0) are tuning parameters. The solution { b w points { x te
Since KMM does not require the density estimates, it is expected to work well. However, the performance of KMM is dependent on the tuning parameters B ,  X  , and  X  and they cannot be simply optimized, e.g., by cross-validation since points.
Another approach to directly estimating the importance is to use a probabilistic classifier. Let us assign a selector variable  X  = 1 to training samples and  X  =  X  1 to test samples, i.e., the training and test densities are written as
Application of the Bayes theorem yields that the impor-tance can be expressed in terms of  X  as follows [1]: The conditional probability p (  X  | x ) could be approximated by discriminating test samples from training samples using a LogReg classifier, where  X  plays the role of a class vari-able. Below we briefly explain the LogReg method.

The LogReg classifier employs the following parametric model for expressing the conditional probability p (  X  | x ) where m is the number of basis functions and {  X  are fixed basis functions. The parameter  X  is learned so that the negative regularized log-likelihood is minimized: min Since the above objective function is convex, the global op-timal solution can be obtained by standard nonlinear opti-mization methods such as Newton X  X  method, conjugate gra-dient, and the BFGS method. Then the importance estimate is given by
An advantage of the LogReg method is that model selec-tion (i.e., the choice of basis functions {  X  as the regularization parameter  X  ) is possible by standard cross-validation since the learning problem involved above is a standard supervised classification problem.
KLIEP [21, 22] also directly gives an estimate of the im-portance function without going through density estimation by matching the two distributions in terms of the Kullback-Leibler divergence.

Let us model the importance w ( x ) by the following lin-ear model: where {  X  functions such that  X  p ( x ) is given by In KLIEP, the parameters  X  are determined so that the Kullback-Leibler divergence from p mized: KL[ p tr ( x )  X  b p tr ( x )] = = Since b p tion, it should satisfy The KLIEP optimization problem is given by replacing the expectations in Eqs. (3) and (4) with empirical averages: max s . t . This is a convex optimization problem and the global solution X  X hich tends to be sparse X  X an be obtained, e.g., by simply performing gradient ascent and feasibility satis-faction iteratively. See [16] for the convergence proof.
Model selection of KLIEP is possible by a variant of like-lihood cross-validation (LCV) [6] as follows. We first di-vide the training samples { x tr a validation part, the model is trained based on the training part, and then its likelihood is verified using the validation part; the model with the largest estimated likelihood is cho-sen. Note that this LCV procedure corresponds to choosing the model with the smallest KL[ p
In uLSIF, the linear importance model (2) is used and the parameters are determined so that the following objective function is minimized [10]: 1 2
Z  X  = where the last term in the right-hand side is a constant and therefore can be safely ignored. By the empirical approxi-mation, the following optimization problem is obtained. where, for  X  ( x ) = (  X  c
H = analytically by where I ments of e  X  could be negative, it is modified as where 0 solution of uLSIF, which can be computed analytically.
Let us consider the leave-one-out cross-validation (LOOCV) score of uLSIF: using the well-known Woodbury inversion formula, b  X  can be expressed as b  X   X  =max where This expression implies that the matrix inverse needs to be computed only once (i.e., A  X  1 ) for computing the LOOCV score. Note that the size of A  X  1 is b  X  b , which is inde-pendent of the numbers of training and test samples. Thus LOOCV can be carried out very efficiently without repeat-ing the hold-out loop.
In this section, we discuss the characteristics of impor-tance estimation methods reviewed in the previous section and propose a practical outlier detection procedure based on uLSIF. For KMM, there is no objective model selection method. Therefore, model parameters such as the Gaussian width need to be determined by hand, which is highly subjec-tive in outlier detection. On the other hand, LogReg and KLIEP give an estimate of the entire importance function. Therefore, the importance values at unseen points can be estimated and CV becomes available for model selection. However, LogReg and KLIEP are computationally rather expensive since non-linear optimization problems have to be solved. uLSIF inherits the preferable properties of LogReg and KLIEP. Furthermore, the solution of uLSIF can be com-puted analytically through matrix inversion and therefore uLSIF is computationally very efficient. Thanks to the availability of the closed-form solution, the LOOCV score can also be analytically computed without repeating the hold-out loop, which highly contributes to reducing the computation time in the model selection phase.

Based on the above discussion, we decided to use uLSIF in our outlier detection procedure.
In uLSIF, a good model may be chosen by LOOCV, given that a set of promising model candidates is prepared. Here we propose using a Gaussian kernel model centered at the training points { x tr where K width  X  .

The reason why the training points { x tr sen as the Gaussian centers, not the test points { x te is as follows. By definition, the importance w ( x ) tends to take large values if the training density p the test density p be small (i.e., close to zero) if p is large. When a function is approximated by a Gaussian kernel model, many kernels may be needed in the region where the output of the target function is large; on the other hand, only a small number of kernels would be enough in the region where the output of the target function is close many kernels at high training density regions, which can be achieved by setting the Gaussian centers at the training points { x tr
Alternatively, we may locate ( n nels at both { x tr liminary experiments, this did not further improve the per-formance, but just slightly increased the computational cost. Since n points { x tr tionally rather demanding. To ease this problem, we practi-cally propose using a subset of { x tr ters for computational efficiency, i.e., where c { x
We use the above basis functions in LogReg, KLIEP, and uLSIF in the experiments.
Here, we illustrate how uLSIF behaves in outlier detec-tion. 4.3.1 Toy Dataset Let the dimension of the data domain be d = 1 , and let the training density be (a) p (b) p where N ( x ;  X ,  X  2 ) denotes the Gaussian density with mean  X  and variance  X  2 . We draw n 99 test samples from p tr ( x ) , and we add an outlier sample at x = 5 for the case (a) and at x = 0 for the case (b) in the test set; thus the total number of test samples is n The number of basis functions in uLSIF is fixed at b = 100 and the Gaussian width  X  and the regularization parameter  X  are chosen from a wide range of values based on LOOCV. The data densities as well as the importance values (i.e., the inlier scores) obtained by uLSIF are depicted in Figure 1. The graphs show that the outlier sample has the smallest inlier score among all samples and therefore the outlier can be successfully detected.

Since the solution of uLSIF tends to be sparse, it may be natural to have a Gaussian-like curve as the inlier score (see Figure 1 again). 4.3.2 USPS Dataset USPS is a dataset which contains images of hand-written digits provided by U.S. Postal Service. Each image consists of 256 (= 16  X  16) pixels, each of which takes a value between  X  1 to +1 representing its color in gray-scale. The class labels attached to the images are integers between 0 and 9 denoting the digits the images represent. Here, we try to find irregular samples in the USPS dataset by uLSIF.
To the 256 -dimensional image vectors, we append 10 ad-ditional dimensions indicating the true class to identify mis-labeled images. In uLSIF, we set b = 100 and  X  and  X  are
Figure 1. Illustration of uLSIF-based outlier detection. chosen from a wide range of values based on LOOCV. Fig-ure 2 shows the top 5 outlier samples in the original USPS test set (of size 2007 ) found by uLSIF where their original labels are attached next to the images. This result clearly shows that the proposed method successfully detects outlier samples that are very hard to recognize even by humans.
We also consider an inverse scenario: we switch the training and test sets and examine the original USPS train-ing set (of size 7291 ). Figure 3 depicts the top 5 outliers found by uLSIF, showing that they are relatively  X  X ood X  samples. This implies that the USPS training set consists only of high-quality samples.
In this section, we discuss the relation between the pro-posed density-ratio based outlier detection approach with existing outlier detection methods.

The outlier detection problem we are addressing in this paper is to find outliers in the test set { x te the training set { x tr isting methods reviewed here are solving is to find out-liers in the test set without the training set. Thus the set-ting is slightly different. However, the existing methods can also be employed in our setting by simply using the union of training and test samples as a test set: { x { x
KDE is a non-parametric technique to estimate a den-sity p ( x ) from samples { x kernel is expressed as where K
The performance of KDE depends on the choice of the kernel width  X  , but its value can be objectively determined based on LCV [6]: a subset of { x estimation and the rest is used for estimating the likelihood of the held-out samples. Note that this LCV procedure cor-responds to choosing  X  such that the Kullback-Leibler di-vergence from p ( x ) to b p ( x ) is minimized. The estimated density values could be directly used as an inlier score. A variation of the KDE approach has been studied in the pa-per [11], where local outliers are detected from multi-modal datasets.

However, density estimation is known to suffer from the curse of dimensionality , and therefore the KDE-based out-our experiment, we will use a global KDE-based outlier de-tection method since we do not assume the multi-modality of the datasets.
SVM is one of the most successful classification algo-rithms in machine learning. The core idea of SVM is to separate samples in different classes by the maximum mar-gin hyperplane in a kernel-induced feature space.

OSVM is an extension of SVM to outlier detection [19]. The basic idea of OSVM is to separate data sam-ples { x a Gaussian reproducing kernel Hilbert space. More specif-ically, the solution of OSVM is given as the solution of the following quadratic programming problem: where  X  ( 0  X   X   X  1 ) is the maximum fraction of outliers.
OSVM inherits the concept of SVM, so it is expected to work well. However, the OSVM solution is dependent on the outlier ratio  X  and the Gaussian kernel width  X  ; choos-ing these tuning parameter values could be highly subjective in unsupervised outlier detection. This is a critical limita-rectly obtained by OSVM; the distance from the separating hyperplane may be used as an inlier score, but its statistical meaning is rather unclear.

A similar algorithm named Support Vector Data De-scription (SVDD) [23] is known to be equivalent to OSVM if the Gaussian kernel is used.
The LOF is an outlier score suitable for detecting local outliers apart from dense regions [2]. The LOF value of an example x is defined using the ratio of the average distance from the nearest neighbors as where nearest x and lrd from the k nearest neighbors of x . If x lies around a high density region and its nearest neighbor samples are close to each other in the high density region, lrd become much smaller than lrd In such cases, LOF as a local outlier.

Although the LOF values seem to be a suitable outlier measure, the performance strongly depends on the choice is no systematic method to select an appropriate value of k . In addition, the computational cost of the LOF scores is expensive since it involves a number of nearest neighbor search procedures.
A formulation called learning from positive and unla-beled data has been introduced in the paper [13]: given pos-itive and unlabeled datasets, the goal is to detect positive samples contained in the unlabeled dataset. The assump-tion behind this formulation is that most of the unlabeled samples are negative (outlier) samples, which is different from the current outlier detection setup. In the paper [12], a modified formulation has been addressed in the context of text data analysis X  X he unlabeled dataset contains only a small number of negative documents. The key idea is to construct a single representative document of the negative butions of positive and unlabeled documents. Though the problem setup is similar to ours, the method is specialized in text data, i.e., the bag-of-words expression.

Since these above methods do not suit general inlier-based outlier detection scenarios, we will not include them in the experiments in Section 6.
In summary, the proposed density-ratio based approach with direct density-ratio estimation would be more advan-tageous than KDE since it can avoid solving an unnecessar-ily difficult problem of density estimation. Compared with OSVM and LOF, the density-ratio based approach with Lo-gReg, KLIEP, and uLSIF would be more useful since it is equipped with a model selection procedure. Furthermore, uLSIF is computationally more efficient than OSVM and LOF thanks to the analytic-form solution.
In this section, we experimentally compare the perfor-mance of the proposed and existing algorithms. For all ex-periments, we used the standard statistical language envi-ronment R [17]. We implemented uLSIF, KLIEP, LogReg, KDE, and KMM by ourselves. uLSIF and KLIEP are im-plemented following the pseudo codes provided in the pa-pers [10, 21, 22]. A package of the L-BFGS-B method called the optim is used in our LogReg implementation, and a quadratic program solver called the ipop contained in the kernlab package is used in our KMM implementation. We use the ksvm function contained in the kernlab package for OSVM and the lofactor function included in dprep package for LOF. We use 12 datasets available from R  X  atsch X  X  Benchmark Repository [18]. Note that they are originally binary clas-sification datasets X  X ere we regard the positive samples as inliers and the negative samples as outliers. All the negative samples are removed from the training set, i.e., the training set only contains inlier samples. In contrast, a fraction the negative samples are retained in the test set, i.e., the test set includes all inlier samples and some outliers.
When evaluating the performance of outlier detection al-gorithms, it is important to take into account both the de-tection rate (the amount of true outliers an outlier detection algorithm can find) and the detection accuracy (the amount of true inliers that an outlier detection algorithm misjudges as outliers). Since there is a trade-off between the detection rate and detection accuracy, we decided to adopt the Area Under the ROC Curve (AUC) as our error metric here.
We compare the AUC values of the density-ratio based methods (KMM, LogReg, KLIEP, and uLSIF) and other methods (KDE, OSVM, and LOF). All the tuning param-eters included in KDE, LogReg, KLIEP and uLSIF are cho-sen based on CV from a wide range of values. CV is not available to KMM, OSVM, and LOF; the Gaussian kernel width in KMM and OSVM is set as the median distance be-tween samples, which has been shown to be a useful heuris-b = 100 . Note that b can also be optimized by CV, but our preliminary experimental results showed that the per-formance is not so sensitive to the choice of b and b = 100 seems to be a reasonable choice. For LOF, we test 3 differ-ent values for the number of nearest neighbors k .
The AUC values as well as the normalized computation time are summarized in Table 1, showing that uLSIF and KLIEP work very well. Though the other methods per-form well for some datasets, they also exhibit poor perfor-mance in other cases. On the other hand, the performance of uLSIF and KLIEP is relatively stable. In addition, from the viewpoint of computation time, uLSIF is much faster than KLIEP and other methods. Thus, the proposed uLSIF-based method could be a reliable and computationally effi-cient alternative to existing outlier detection methods.
Finally, let us consider a real-world failure predic-tion problem in hard-disk drives equipped with the Self-Monitoring and Reporting Technology (SMART). The SMART system monitors individual drives and stores some data. We use the SMART dataset provided by a manufac-turer [15]. The dataset consists of 369 drives, where 178 failed. Each drive stores up to the last 300 records that are logged almost every 2 hours. Although each record origi-nally includes 59 attributes, we use only 25 variables cho-sen based on the feature selection test [15]. The sequence of records are converted into data samples in a sliding-window manner with window size  X  .

In practice, the training set may contain a few  X  X ad X  sam-ples. To simulate such realistic situations, we construct drives and adding a small fraction  X  of  X  before-fail  X  exam-ples taken from the 191 failed drives which are more than 300 hours prior to failure. The test set is made of the records of the good drives and the records of the 191 failed drives less than 100 hours prior to failure; the  X  X ail X  samples are regarded as outliers in this experiment.

First, we perform experiments for the window size  X  = 5 , 10 and evaluate the dependence of the feature dimen-sion on the outlier detection performance. The fraction  X  of before-fail examples in the training set is fixed to 0.05. Other settings including the fraction  X  of outliers and b the same as the previous experiments. The results are sum-marized in Table 2. Next, we change the fraction of before-and evaluate the effect of heterogeneousness of the training set on the outlier detection performance. The fraction  X  outliers in the test set is fixed to 0.05 and the window size is fixed to 10. The results are summarized in Table 3.
Overall, the density-ratio based methods work very well; among them, uLSIF has the lowest computational cost. The performance of OSVM tends to be degraded as the outlier fraction  X  increases and the performance of KDE rapidly gets worse as the feature dimension  X  increases. LOF works very well if the number of nearest neighbors k is large. However, a good choice of k may be problem-dependent and there seems no systematic way to determine k appropri-ately. The computation of LOF is very slow due to extensive nearest neighbor search, and the performance of LOF tends to be degraded if the fraction  X  of before-fail examples in the training set is increased.

These results indicate that our algorithm using the den-sity ratio is accurate and computationally efficient in real-world failure prediction tasks X  X n particular, the use of KLIEP and uLSIF seems promising.
We cast the inlier-based outlier detection problem as a problem of estimating the ratio of probability densities (i.e., the importance ), and proposed a practical outlier detection algorithm based on uLSIF. Our method is equipped with a model selection procedure, which allows us to obtain a purely objective solution. This is a highly valuable feature in ill-defined problems such as outlier detection. Further-more, the proposed method is computationally very effi-cient and therefore useful in practice. Through extensive simulations with benchmark and real-world datasets, the usefulness of the proposed approach was demonstrated.
