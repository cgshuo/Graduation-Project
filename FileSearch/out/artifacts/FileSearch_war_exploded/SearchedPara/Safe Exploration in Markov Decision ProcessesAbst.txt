 Teodor Mihai Moldovan moldovan@cs.berkeley.edu Pieter Abbeel pabbeel@cs.berkeley.edu University of California at Berkeley, CA 94720-1758, USA When humans learn to control a system, they natu-rally account for what we think of as safety. For exam-ple, when a novice pilot learns how to fly an RC heli-copter, they will slowly spin up the blades until the he-licopter barely lifts off, then quickly put it back down. They will repeat this a few times, slowly starting to bring the helicopter a little bit off the ground. When doing so they would try out the cyclic (roll and pitch) and rudder (yaw) control, while X  X ntil they have be-come more skilled X  X t all times staying low enough that simply shutting it down would still have it land safely. When a driver wants to become skilled at driv-ing on snow, they might first slowly drive the car to a wide open space where they could start pushing their limits. When we are skiing downhill, we are careful about not going down a slope into a valley where there is no lift to take us back up.
 One would hope that exploration algorithms for phys-ical systems would be able to account for safety and have similar behavior naturally emerge. Unfortunately most existing exploration algorithms completely ig-nore safety issues. More precisely phrased, most exist-ing algorithms have strong exploration guarantees, but to achieve these guarantees they assume ergodicity of the Markov decision process (MDP) in which the ex-ploration takes place. An MDP is ergodic if any state is reachable from any other state by following a suit-able policy. This assumption does not hold true in the exploration examples presented above as each of these systems could break during (non-safe) exploration. Our first important contribution is a definition of safety, which, at its core, requires restricting atten-tion to policies that preserve ergodicity with some well controlled probability. Imposing safety is, unfortu-nately, NP-hard in general. Our second important contribution is an approximation scheme leading to guaranteed safe, but potentially sub-optimal, explo-ration. 1 A third contribution is the consideration of uncertainty in the dynamics model that is correlated over states. While usually the assumption is that un-certainty in different parameters is independent X  X s this makes problem more tractable computationally X  being able to learn about state-action pairs before vis-iting them is critical for safety.
 Our experiments illustrate that our method indeed achieves safe exploration, in contrast to plain explo-ration methods. They also show that our algorithm is almost as computationally efficient as planning in a known MDP X  X ut then, as every step leads to an update in knowledge about the MDP, this computa-tion is to be repeated after every step. Our approach is able to safely explore grid worlds of size up to 50 100. Our method can make safe any type of explo-ration that relies on exploration bonuses, which is the case for most existing exploration algorithms, includ-ing, for example, the methods proposed in (Brafman &amp; Tennenholtz, 2001; Kolter &amp; Ng, 2009). In this ar-ticle we do not focus on the exploration objective and use existing ones.
 Safe exploration has been the focus of a large number of articles. (Gillula &amp; Tomlin, 2011; Aswani &amp; Bouf-fard, 2012) propose safe exploration methods for linear systems with bounded disturbances based on model predictive control and reachability analysis. They de-fine safety in terms of safe regions of the state space, which, we will show, is not always appropriate in the context of MDPs. The safe exploration for MDP meth-ods proposed by (Geramifard et al., 2011; Hans et al., 2008) gauge safety based on the best best estimate of the transition measure but they ignore the level of un-certainty in this estimate. As we will show, this is not sufficient to provably guarantee safety.
 Provably efficient exploration is a recurring theme in reinforcement learning (Strehl &amp; Littman, 2005; Li et al., 2008; Brafman &amp; Tennenholtz, 2001; Kearns &amp; Singh, 2002; Kolter &amp; Ng, 2009). Most methods, however, tend to rely on the assumption of ergodicity which rarely holds in interesting practical examples; consequently, these methods are rarely applicable for physical systems. The issue of provably guaranteed safety, or risk aversion, under uncertainty in the MDP parameters has also been studied in the reinforcement literature. In (Nilim &amp; El Ghaoui, 2005) they propose a robust MDP control method assuming the transition frequencies are drawn from an orthogonal convex set by an adversary. Unfortunately, it seems impossible to use their method to constrain some safety objec-tive while optimizing a different exploration objective. In (Delage &amp; Mannor, 2007) they present a safe ex-ploration algorithm for the special case of Gaussian distributed ambiguity in the reward and state-action-state transition probabilities, but their safety guaran-tees are only accurate if the ambiguity in the transition model is small.
 This is the 8-page (ICML format) version of this paper. All references to the appendix refer to the appendix of the full paper (Moldovan &amp; Abbeel, 2012), which is identical up to this sentence and the appendix. Due to space constraints, we will not give a general in-troduction to Markov decision processes (MDPs). For an introduction to MDPs we refer the readers to (Sut-ton &amp; Barto, 1998; Bertsekas &amp; Tsitsiklis, 1996). We use capital letters to denote random variables; for example, the total reward is: V := P  X  t =0 R S t ,A t . We represent the policies and the initial state distributions by probability measures. Usually the measure  X  will correspond to a policy and the measure s :=  X  ( s ), which puts measure only in state s , will correspond to starting in state s . With this notation, the usual value recursion, assuming a known transition measure, p , reads: We specify the transition measure as a superscript of the expectation operator rather than a subscript for typographical convenience; in this case, and in gen-eral, the positioning of indexes as subscripts or su-perscripts adds no extra significance. We will let the transition measure p sometimes sum to less than one, that is P s 0 p s,a,s 0  X  1. The missing mass is implicitly assigned to transitioning to an absorbing  X  X nd X  state, which, for example, allows us to model  X  discounting by simply using  X p as a transition measure.
 We model ambiguous dynamics in a Bayesian way, al-lowing the transition measure to also be a random variable. When this is the case, we will use P to de-note the, now random, transition measure. The belief , which we will denote by  X  , is our Bayesian probabil-ity measure over possible dynamics, governing P and R . Therefore, the expected return under the belief and policy  X  , starting from state s , is E  X  E P s, X  [ V ]. We allow beliefs under which transition measures and rewards are arbitrarily correlated. In fact, such correlations are usually necessary to allow for safe exploration. For compactness we will often use lower case letters to de-note the expectation of their upper case counterparts. Specifically, we will use the notations p := E  X  [ P ] and r := E  X  [ R ] throughout. 3.1. Exploration Objective Exploration methods, as those proposed in (Brafman &amp; Tennenholtz, 2001; Kolter &amp; Ng, 2009), operate by finding optimal policies in constructed MDPs with ex-ploration bonuses. The R-max algorithm, for exam-ple, constructs an MDP based on the discounted ex-pected transition measure and rewards under the be-lief, and adds a deterministic, belief dependent explo-ration bonus,  X   X  s,a := max s,a E  X  R s,a , to any transitions that are not sufficiently well known. Our method al-lows adding safety constraints to any such exploration methods. Henceforth, we will restrict attention to such exploration methods, which can be formalized as op-timization problems of the form: 3.2. Safety Constraint The issue of safety is closely related to ergodicity . Al-most all proposed exploration techniques presume er-godicity; authors present it as a harmless technical as-sumption but it rarely holds in interesting practical problems. Whenever this happens, their efficient ex-ploration guarantees cease to hold, often leading to very inefficient policies. Informally, an environment is ergodic if any mistake can be forgiven eventually. More specifically, a belief over MDPs is ergodic if and only if any state is reachable from any other state via some policy or, equivalently, if and only if: where B s 0 is an indicator random variable of the event that the system reaches state s 0 at least once: B s 0 = 1 { X  t &lt;  X  such that S t = s 0 } = min (1 , P t 1 S t = s 0 Unfortunately, many environments are not ergodic. For example, our robot helicopter learning to fly can-not recover on its own after crashing. Ensuring almost sure ergodicity is too restrictive for most environments as, typically, there always is a very small, but non-zero, chance of encountering that particularly unlucky sequence of events that breaks the system. Our idea is to restrict the space of eligible policies to those that preserve ergodicity with some user-specified probabil-ity,  X  , called the safety level . We name these policies  X  -safe . Safe exploration now amounts to choosing the best exploration policy from this set of safe policies. Informally, if we stopped a  X  -safe policy  X  o at any time T , we would be able to return from that point to the home state s 0 with probability  X  by deploying a return policy  X  r . Executing only  X  -safe policies in the case of a robot helicopter learning to fly will guarantee that the helicopter is able to land safely with probability  X  whenever we decide to end the experiment. In this example, T is the time when the helicopter is recalled (perhaps because fuel is running low), so we will call T the recall time . Formally, an outbound policy  X  o is  X  -safe with respect to a home state s 0 and a stopping time T if and only if: Note that, based on Equation (2), any policy is  X  -safe for any  X  if the MDP is ergodic with probability one under the belief. For convenience we will assume that the recall time, T , is exponentially distributed with parameter 1  X   X  , but our method also applies when the recall time equals some deterministic horizon. Un-fortunately, expressing the set of  X  -safe policies is NP-hard in general, as implied by the following theorem proven in the appendix.
 Theorem 1. In general, it is NP-hard to decide whether there exist  X  -safe policies with respect to a home state, s 0 , and a stopping time, T , for some be-lief,  X  . 3.3. Safety Counter-Examples We conclude this section with counter-examples to three other, perhaps at first sight more intuitive, def-initions of safety. First, we could have tried to define safety in terms of sets of safe states or state-actions. That is, we might think that making the non-safe states and actions unavailable to the planner (or sim-ply inaccessible) is enough to guarantee safety. Fig-ure 1 shows an MDP where the same state-action is used both by a safe and by an unsafe policy. The idea behind this counter-example is that safety de-pends not only on the states visited, but also on the number of visits, thus, on the policy. This shows that safety should be defined in terms of safe policies, not in terms of safe states or state-actions.
 Second, we might think that it is perhaps enough to ensure that there exists a return policy for each poten-tial sample MDP from the belief, but not impose that it be the same for all samples. That is, we might think that condition 3 is too strong and, instead, it would be enough to have: Figure 2 shows an MDP where this condition holds, yet all policies are naturally unsafe.
 Third, we might think that it is sufficient to simply use the expected transition measure when defining safety, as in the equation below. Figure 3 shows that this is not the case; the expected transition measure is not a sufficient statistic for safety. Although imposing the safety constraint in Equa-tion (3) is NP-hard, as shown in Theorem 1, we can efficiently constrain a lower bound on the safety objec-tive, so the safety condition is still provably satisfied. Doing so could lead to sub-optimal exploration since the set of policies we are optimizing over has shrunk. However, we should keep in mind that the exploration objectives represent approximate solutions to other Algorithm 1 Safe exploration algorithm Require: prior belief  X  , discount  X  , safety level  X  . Require: function  X  : belief  X  exploration bonus
M,N  X  new MDP objects repeat until  X   X  = 0, so there is nothing left to explore NP-hard problems, so optimality has already been for-feited in existing (non-safe) approaches to start out with. Algorithm 1 summarizes the procedure and the experiments presented in the next section show that, in practice, when the ergodicity assumptions are vi-olated, safe exploration is much more efficient than plain exploration.
 Putting together the exploration objective defined in Equation (1) and the safety objective defined in Equa-tion (3) allows us to formulate safe exploration at level  X  as a constrained optimization problem: The exploration objective is already conveniently for-mulated as the expected reward in an MDP with tran-sition measure  X p , so we will not modify it. On the other hand, the safety constraint is difficult to deal with as is. Ideally, we would like the safety constraint to also equal some expected reward in an MDP. We will see that, in fact, it takes two MDPs to express the safety constraint.
 First, we express the inner term, E P S expected reward in an MDP. We can replicate the be-haviour of B s 0 , that is counting only the first time state s 0 is reached, by using a new transition measure, P  X  (1  X  1 s = s 0 ) under which, once s 0 is reached, any further actions lead immediately to the implicit  X  X nd X  state. Formally, we express this by the identity: We now focus on the outer term, E P s Since the recall time, T , is exponentially distributed with parameter 1  X   X  , we can view S T as the final state in a  X  -discounted MDP starting at state s 0 , following policy  X  o . In this MDP, the inner term plays the role of a terminal reward. To put the problem in a standard form, we convert this terminal reward to a step-wise reward by multiplying it by 1  X   X  .
 E At this point, we have expressed the safety constraint in the MDP formalism, but the transition measures of these MDPs, P (1  X  1 s = s 0 ) and  X P , are still random. If we could replace these random transition measures with their expectations under the belief  X  that would significantly simplify the safety constraint. It turns out we can do this, at the expense of making the constraint more stringent. Our tool for doing so is Theorem 2 presented below, but proven in the appendix. It shows that we can replace a belief over MDPs by a single MDP with the expected transition measure, featuring an appropriate reward correction such that, under any policy, the value of this MDP is a lower bound on the expected value under the belief.
 Theorem 2. Let  X  be a belief such that for any policy,  X  , and any starting state, s , the total expected reward in any MDP drawn from the belief is between 0 and 1 ; i.e. 0  X  E P s, X  [ V ]  X  1 ,  X  -almost surely. Then the fol-lowing bound holds for any policy,  X  , and any starting state, s : where  X   X  s,a := X We first apply Theorem 2 to the outer term, yielding the following bound: E We, then, apply it again to the inner term: Combining the last two results allows us to replace the NP-hard safety constraint with a stricter, but now tractable, constraint. The resulting optimization prob-lem corresponds to the guaranteed safe, but poten-tially sub-optimal exploration problem: s.t.: E  X p s v s = E The term v s represents our lower bound for the inner term per Equation (4), and is simply the value function of the MDP corresponding to the inner term; i.e. the MDP with transition measure p (1  X  1 s = s 0 ) and reward the return policy,  X  r , does not appear anywhere else, we can split the safe exploration problem we obtained in Equation (5) into two steps: Step one: find the optimal return policy  X   X  r , and corresponding value function v  X  s , by solving the stan-dard MDP problem below: Step two: find the optimal exploration policy  X   X  o un-der the strict safety constraint, by solving the con-strained MDP problem below: The first step amounts to solving a standard MDP problem while the second step amounts to solving a constrained MDP problem. As shown by (Altman, 1999), both can be solved efficiently either by linear programming, or by value-iteration. In our exper-iments we used the LP formulation with the state-action occupation measure as optimization variable. Solutions to the constrained MDP problem will usu-ally be stochastic policies, and, in our experiments, we found that following them sometimes leads to random walks which explore inefficiently. We addressed the issue by de-randomizing the exploration policies in fa-vor of safety. That is, whenever the stochastic policy proposes multiple actions with non-zero measure, we choose the one among them that optimizes the safety objective. 5.1. Grid World Our first experiment models a terrain exploration problem where the agent has limited sensing capabil-ities. We consider a simple rectangular grid world, where every state has a height H s . From our Bayesian standpoint these heights are independent, uniformly distributed categorical random variables on the set move to any immediately neighboring state. Such move will succeed with probability one if the height of the destination state is no more than one level above the current state; otherwise, the agent remains in the current state with probability one. In other words, the agent can always go down cliffs, but is unable to climb up if they are too steep. Whenever the agent enters a new state it can see the exact heights of all immedi-ately surrounding states. We present this grid world experiment to build intuition and to provide an easily reproducible result. Figure 4 shows a number of ex-amples where our exploration method results in intu-itively safe behavior, while plain exploration methods lead to clearly unsafe, suboptimal behavior.
 Our exploration scheme, which we call adapted R-max , is a modified version of R-max exploration (Brafman &amp; Tennenholtz, 2001), where the exploration bonus of moving between two states is now proportional to the number of neighboring unknown states that would be uncovered as a result of the move, to account for the remote observation model. The safety costs for this exploration setup, as prescribed by Theorem 2 are: tempted move a succeeds in state s and the belief  X  de-scribes the distribution of the heights of unseen states. In practice we found that this correction is a factor of two larger than would be sufficient to give a tight safety bound.
 A somewhat counter intuitive result is that adding safety constraints to the exploration objective will, in fact, improve the fraction of squares explored in ran-domly generated grid worlds. The reason why plain exploration performs so poorly is that the ergodicity assumptions are violated, so efficiency guarantees no longer hold. Figure 6 in the appendix summarizes our exploration performance results. 5.2. Martian Terrain For our second experiment, we model the problem of autonomously exploring the surface of Mars by a rover such as the Mars Science Laboratory (MSL) (Lock-wood, 2006). The MSL is designed to be remote con-trolled from Earth but communication suffers a latency of 16.6 minutes. At top speed, it could traverse about 20m before receiving new instructions, so it needs to be able to navigate autonomously. In the future, when such rovers become faster and cheaper to deploy, the ability to plan their paths autonomously will become even more important. The MSL is designed to a static stability of 45 degrees, but would only be able to climb slopes up to 5 degrees without slipping (MSL, 2007). Digital terrain models for parts of the surface of Mars are available from the High Resolution Imaging Sci-ence Experiment (HiRISE) (McEwen et al., 2007) at a scale of 1.00 meter/pixel and accurate to about a quarter of a meter. The MSL would be able to obtain much more accurate terrain models locally by stereo vision.
 The state-action space of our model MDP is the same as in the previous experiment, with each state corre-sponding to a square area of 20 by 20 meters on the surface. We allow only transitions at slopes between -45 and 5 degrees. The heights, H s , are now assumed to be independent Gaussian random variables. Un-der the prior belief, informed by the HiRISE data, the expected heights and their variances are: where h are the HiRISE measurements, g is a Gaus-sian filter with  X  = 5 meters,  X   X   X  represents image convolution, D 20 is the sub-sampling operator and v 0 = 2  X  4 m 2 is our estimate of the variance of HiRISE measurements. We model remote sensing by assum-ing that the MSL can obtain Gaussian noisy measure-ments of the height at a distance d away with variance v ( d ) = 10  X  6 ( d + 1m) 2 .
 To account for this remote sensing model we use a first order approximation of the entropy of H as an exploration bonus: Figure 5 shows our simulated exploration results for a 2km by 1km area at  X  30 . 6 degrees latitude and 202 . 2 degrees longitude (PSP, 2008). Safe exploration at level 1 . 0 is no longer possible, but, even at a con-servative safety level of . 98, our method covers more ground than the regular (unsafe) exploration method which promptly get stuck in a crater. Imposing the safety constraint naively, with respect to the expected transition measure, as argued against at the end of Section 3.3, performs as poorly as unsafe exploration even if the constraint is set at . 98. 5.3. Computation Time We implemented our algorithm in Python 2.7.2.7, using Numpy 1.5.1 for dense array manipulation, SciPy 0.9.0 for sparse matrix manipulation and Mosek 6.0.0.119 for linear programming. The discount factor was set to . 99 for the grid world experiment and to . 999 for Mars exploration. In the latter experiment we also restricted precision to 10  X  6 to avoid numeri-cal instabilities in the LP solver. Table 1 summarizes planning times for our Mars exploration experiments. In addition to the safety formulation we discussed in Section 3.2, out framework also supports a number of other safety criteria that we did not discuss due to space constraints:  X  Stricter ergodicity ensuring that return is possible  X  Ensuring that the probability of leaving some pre- X  Ensuring that the expected total reward under the Additionally, any number and combination of these constraints at different  X  -levels can be imposed simul-taneously.

