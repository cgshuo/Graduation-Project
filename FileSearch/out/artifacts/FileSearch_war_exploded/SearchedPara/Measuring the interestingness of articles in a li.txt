 1. Introduction
An explosive growth of online news has taken place in the last few years. Users are inundated with thousands of news to read and analyze many news articles daily, such as financial analysts, government officials, and news reporters. Informa-information at the right time (VIRT) ( Hayes-Roth, 2006 ) strategy for information handling.
 relevant to keyword searches. For example, searching for  X  X  X an Francisco X  in Google News will yield about 135,000 articles if it discusses an event that the user has already read about in other articles.
Although it has been shown that collaborative filtering can aid in personalized recommendation systems ( Wang, de Vries, approach is taken by directly addressing what makes an article interesting, going beyond simple topic relevance. ditional understanding of relevance (i.e., system and topic relevance).

To simply train a separate model per user using standard topic relevance techniques is insufficient, as additional features, a need for some online feature selection method that is user-based. There has been much work in news recommendation systems, but none have yet addressed the question of what makes an article interesting and how to build such a system.
To address the news recommendation problem, an effective online news filtering agent is needed. More specifically, the news recommendation problem addressed here follows a similar evaluation model as in the TREC11 adaptive filter task article.

This paper presents a comprehensive summary of work in news recommendation for a limited user environment, from an following contributions to the problem: 1. Filtering based on only topic relevance is shown to be insufficient for identifying interesting articles. 2. Further discussion and tractable implementations of the relevance classes discussed by Saracevic (1996, 2007) . the interestingness of an article for a user. It is the combination of multiple features that can yield 50% higher quality results. For each user, these features have different degrees of usefulness for predicting interestingness. 4. Several classifiers are evaluated for combining these features to find an overall interestingness score. interestingness for the user.

In the next section, areas of related works are discussed and contrasted to the work done in iScore. The following sections final section discusses the experimental results, evaluating iScore with traditional information retrieval (IR) techniques. 2. Related words
In this section, related works are discussed and contrasted with iScore. iScore X  X  primary contributions, in relation to the following related works, are threefold: 1. iScore adapts existing online solutions, such as Rocchio and language models, that are well suited to some of the sub-problems of  X  X  X nterestingness. X  And instead of applying these solutions to their intended applications, iScore applies them to the broader problem of identifying interesting articles. in an online environment, such as multiple topic tracking, novelty detection, and online feature selection. And these solu-tions are also applied to the general problem of finding interesting articles. 3. iScore is an ensemble system that combines multiple techniques that address different aspects of  X  X  X nterestingness. X  2.1. Recommendation systems iScore is a recommendation system in a limited user environment, so the only available information is the article X  X  con-and Romani (2005) rank news articles and new sources based on several properties in an online method. They claim that important news articles are clustered. They also claim that mutual reinforcement between news articles and news sources cles are ranked based on various properties in an online method, but instead of ranking articles using mutual reinforcement liness that the most recent news articles are more important than older ones is taken into account.

Macskassy and Provost (2001) measure the interestingness of an article as the correlation between the article X  X  content online processing of new articles as they are published.

Other systems perform clustering or classification based on the article X  X  content, computing such values as term-fre-quency-inverse-document frequency (TF-IDF) weights for tokens. A near neighbor text classifier ( Billsus et al., 2000 ) uses a document vector space model. A personalized multi-document summarization and recommendation system by Radev,
Fan, and Zhang (2001) recommend articles by suggesting articles from the same clusters in which the past interesting arti-approach, MiTAP ( Damianos, Wohlever, Kozierok, &amp; Ponte, 2003 ) monitors infectious disease outbreaks and other global events. Multiple information sources are captured, filtered, translated, summarized, and categorized by disease, region, ing articles. 2.2. Adaptive filtering
The work in iScore is closely related to the adaptive filtering task in the Text Retrieval Conference (TREC), which is the olds and incremental profile updates.

Xu et al. (2002) use a variant of the Rocchio algorithm, in which they represent documents as a vector of TF-IDF values and maintain a profile for each topic of the same dimension. The profile is adapted by adding the weighted document vector of relevant documents and by subtracting the weighted vector of irrelevant documents. Since this approach performed the best in the task, this method is incorporated into iScore. Other methods explored in TREC11 include using a second-order
Walker, Zaragoza, &amp; Herbrich, 2002 ). 2.3. Ensembles
Other work, like iScore, have leveraged multiple existing techniques to build better systems for specific tasks. For exam-ple, Henzinger (2006) combines two popular webpage duplication identification methods to achieve better results. In an-other example, Lazarevic and Kumar (2005) combine the results from multiple outlier detection algorithms that are applied using different sets of features.

Yan and Hauptmann (2006) combine multiple ranking functions over the same document collection through probabilistic latent query analysis, which associates non-identical combination weights with latent classes underlying the query space. tion to a finite mixture of conditional probabilistic models. In the iScore experiments, two methods of a linear combination combined that are not necessarily ranking functions that can be used for ranking documents for interestingness by them-selves. Each function is a different aspect of interestingness, and the functions need to be combined together to generate meaningful scores for interestingness. 2.4. Topic detection and tracking Topic detection and tracking (TDT) identifies new events and groups news articles that discuss the same event. Formally, story segmentation ( NIST, 2004 ).

Many TDT systems, like Allan, Papka, and Lavrenko (1998), Franz, Ward, McCarley, and Zhu (2001), and Allan (2002) are event and a new profile is created using the document vector of the news story. Otherwise, the news story is used to update the existing profiles. Other work, such as Makkonen, Ahonen-Myka, and Salmenkivi (2004) , add simple semantics of loca-tions, names, and temporal information to the traditional term frequency vectors used in previous work.
Although a similar single-pass clustering algorithm is used in the multiple topic tracking (MTT) component of iScore, users interests, which are under continuous evolution. Additionally, MTT uses the interestingness of topics when evaluating changes over time. A topic that may have been interesting in the past may not be interesting in the future. Consequently,
MTT discards old profile vectors that are no longer of interest to reduce resource consumption, to speed up document eval-uation, and to improve the quality of results.
 documents are partitioned into possibly overlapping subcollections according to their publication time. The most prominent themes (or subtopics) are extracted from each subcollection. For themes in two different subcollections, an ETP solution de-cides whether there is an evolutionary transition from one theme to the other. The general ETP problem is not restricted to operation within an online and continuous environment, so the solution posed by Mei and Zhai (2005) is an offline data min-are published. Additionally, the solution posed by Mei and Zhai (2005) does not learn which themes or topics are of interest ering the most interesting articles for the user as they are published. 2.5. Feature selection
There has been a significant amount of work done in offline feature selection. Guyon and Elisseeff (2003) survey a variety of feature selection techniques, noting cases where feature selection would improve the results of classifiers. They show that noise reduction and better class separation may be obtained by adding features that are presumably redundant. Fea-tures that are independently and identically distributed are not truly redundant. Perfectly correlated features are truly redundant in the sense that no additional information is gained by adding them. However, very high feature correlation does not mean the absence of feature complementarity. A feature that is completely useless by itself can provide a signif-icant performance improvement when taken with others. In other words, two features that are useless by themselves can be useful together.

The feature selection method used by iScore is most similar to embedded methods, which perform feature selection dur-ing the process of training and are usually specific to given learning machines. Unfortunately, existing feature selection methods cannot be applied directly to the features used within iScore. Blum and Langley (1997) discuss feature-weighting methods such as Winnow ( Littlestone, 1988 ). However, the inputs and outputs of the Winnow algorithm are all binary and cannot be applied directly to continuous inputs, such as the feature scores generated by iScore X  X  feature extractors. Other
Winnow variants and Winnow-based online feature selection techniques studied by Carvalho and Cohen (2006) require that all inputs are weights of importance and must be values between 0 and 1, such as normalized term frequencies. However, in general, features may not necessarily be positive weights or even have the same semantic meaning. In the case of iScore, a feature X  X  correlation to interestingness may be positively correlated; whereas, another feature maybe negatively correlated.
Utgoff, Berkman, and Clouse (1997) address this problem with an incremental decision tree algorithm that makes use of an efficient tree restructuring algorithm. However, the drawback is that any numeric data must be stored and maintained in sorted order by value and the decision tree X  X  storage requirements will continually grow.

Another method for feature selection is to reduce the number of redundant features, which is different from our goal of reducing the number of irrelevant features. Nurmi and Floreen (2005) identify redundant features by performing pair-wise experiments, we assume a more general setup, where documents from different news sources that span multiple domains are aggregated together into a single document stream and are simply ordered by publication time. Consequently, an article in the document stream is not necessarily dependent upon the content of the article that immediately precedes it in the doc-ument stream.

The feature selection method used by Xing, Jordan, and Karp (2001) employs information gain ranking and Markov blan-tures based on their information gain, we use correlation rather than information gain due to correlation X  X  computability in an online environment. Information gain requires the discretization of feature values which requires examining the entire putationally intensive subset selection procedure, which is not ideal for an online setting either.

Because the importance of features for what makes an article interesting varies among users, are unknown a priori, and may change over time, no features can be discarded when constructing the overall classifier. The usefulness of each feature must be learned in an online fashion. And current online feature selection approaches are not general or efficient enough to handle the general features used by iScore. 2.6. Document classification
Since news articles are classified as interesting or uninteresting by iScore, it is important to note work in document clas-sification. However, most document classification methods have been used to classify documents into specific topics, which is a different problem from binning documents by their interestingness.

Wong, Kan, and Young (1996) classify documents, taking into account the term frequencies as well as the local relation-ships between available classes. They try to balance specificity, which measures the degree of precision with which the con-tents of a document is represented by the classification result, and exhaustivity, which measures the degree of coverage by the classification result on the domain found in a document. Liang (2004) uses a SVM for web-page classification. Al-Mubaid and Umair (2006) use distributional clustering and a logic-based learning algorithm to classify documents. Angelova and
Weikum (2006) combine the graph/network properties of documents along with traditional content classification methods to classify documents. Diaz and Metzler (2006) improve classification of documents by using multiple large external corpora. This is accomplished through a mixture of relevance models.

Latent Dirichlet Allocation (LDA) first proposed by Blei, Ng, and Jordan (2003) for document classification has been pop-represent a document. Newman, Chemudugunta, Smyth, and Steyvers (2006), Griffiths and Steyvers (2004), and Steyvers documents into topics not known a priori, similar to unsupervised learning methods such as traditional clustering methods.
Unlike most clustering methods, the clusters of documents may share documents (i.e., overlap). In their topic model, a topic is a multinomial probability distribution over unique words in the vocabulary of the corpus. Each document is a mixture of topics and is represented as a multinomial probability vector, one probability for each topic. Given this model for a set of documents and topics, Gibbs sampling is used to estimate the topic-word and document-topic distributions.
Yu, Zhai, and Han (2003) classify documents from positive and unlabelled documents; whereas, most classification schemes assume that the training data are completely labeled. They extend a support vector machine (SVM) for this task.
They show that when the positive training data is not too under-sampled, their approach outperforms other methods be-cause it exploits the natural gap between positive and negative documents in feature space. This situation is a scenario in which iScore mostly operates in, where most articles are unlabelled with a few documents that are positively labeled. Unfor-tunately, an SVM method is not applicable to iScore X  X  online operating environment. 3. iScore pipeline
In iScore, news articles are processed in a streaming fashion, much like the document processing done in the adaptive cation date. Once the system classifies an article, an interestingness judgment is made available to the system by the user. extractors generate a set of feature scores F  X  d  X  X  f 1  X  d  X  ; f score, or an iScore I  X  d  X  :
Next, the adaptive thresholder thresholds the iScore to generate a binary classification, indicating the interestingness of numbers bounded between 0 and 1, the efficacy of every threshold between 0 and 1 in increments of 0.01 is evaluated.
And in the case of ties between the utility measures, the threshold that provides the best separation between the average iScores of uninteresting and interesting articles is selected. The utility measure evaluated is FMeasure f task ( Robertson &amp; Soboroff, 2002 ). This measure is discussed in further detail in the Experimental Results section.
I larly for the next document in the pipeline. 4. Features for classification not determine the interestingness of an article for a user. 4.1. Rocchio for topic relevance the Rocchio adaptive learning method ( Rocchio, 1971 , chapter 14). Further discussion on the Rocchio algorithm is available in Joachims (1996) , in which the author compares the Rocchio relevance feedback algorithm with its probabilistic variant and the standard na X   X  ve Bayes classifier.

A document is represented as a TF-IDF vectors ~ d . The Rocchio algorithm maintains a profile vector ~ p and updates it as follows:
The parameters v and c are the weights that determine relative weighting of positively labeled articles and negatively algorithm of a document d is the cosine of the angle between the profile vector and the document vector: updates profiles as follows:
The first two conditions are satisfied by user taggings. The third condition is for pseudo-negative documents, which have and 0.6, respectively ( Xu et al., 2002 ). 4.2. Multiple topic tracking this problem. Although the cluster of all the interesting documents would contain interesting documents, it would also con-tain many uninteresting articles due to its size. If the user is interested in many orthogonal topics, then the encompassing cluster would be much larger and would also contain many more uninteresting articles as well.
 maintains smaller topic clusters instead of the larger encompassing cluster, improving classification precision. In other specific topics; at the same time, using multiple vectors keeps classification variance low.

In MTT, each document and profile vector is represented as a TF-IDF vector, like in Rocchio. A set of profiles P is main-tained, which is initially empty. Until an interesting article arrives on the document stream, each article is scored with a 0. When an interesting article does arrive, a new profile vector ~ p
P . For each subsequent article on the document stream, the closest profile in P (denoted as ~ p ness of the new document and ~ p max is low, then a new profile is generated using the new document. Otherwise, ~ p dated similarily as in Rocchio. 4.3. eRocchio
In many information retrieval algorithms, such as the Rocchio algorithm ( Rocchio, 1971 , chapter 14) and MTT ( Pon et al., 2007b ), parameters often must be fine-tuned to a particular dataset through extensive experimentation. For example, in Xu labeled articles. In MTT, performance greatly depended upon several parameters that needed to be fine-tuned. These param-eters are determined through extensive trial and error experiments. If there are many different datasets that must be eval-uated, this process is often tedious and expensive, leading many to simply fine-tune the parameters to one dataset and applying the parameters globally to all other datasets, which may not be optimal. In news recommendation, user reading behavior may vary from user to user, and would result in different parameters for recommendation algorithms. For example, lem is even more magnified if there are many users with different reading/learning behaviors. It is not feasible for a news recommendation engine to fine-tune parameters for every user because it is very rare that validation data is available for fine-tuning until a user begins reading articles recommended by the system. Even if such a validation data was available, the task would be too time-consuming for it to be tractable if done on every user.

Given the shortcomings of existing information retrieval (IR) algorithms, such MTT, Rocchio, and its variants, that require tively. However, because c is a weight relative to v , multiple c -values are evaluated simultaneously while holding v to 1.
Each document is evaluated by multiple instantiations of the Rocchio formulation in parallel, each with a different neg-intervals of 0.01, are evaluated. Because the cosine similarity between the query profile and the document is a real number bounded between 0 and 1, and a binary decision must be made, the similarity is thresholded such that articles with a high use a static threshold, the efficacy of every threshold between 0 and 1 in increments of 0.01 is evaluated. Each Rocchio own unique c and adaptive thresholder. After each adaptive thresholder has generated a binary score from its corresponding and its corresponding threshold are chosen by selecting the Rocchio instantiation and the threshold combination that has had the best utility measure up to that point. In evaluations, FMeasure F 4.4. Language models for topic relevance
Severalothermethodsformeasuringtopicrelevanceuselanguagemodels.An n -gramlanguagemodelingapproachhasbeen topicrelevancescores.Likena X   X  veBayesianclassifiers,language-basedmodelingclassifiersclassifydocumentsgiventhenumber pendently,languagemodelingclassifiersassumethatanoccurringgramisdependentuponthelast n 1grams.Inotherwords: where N is the number of grams in the document and g i is the i th gram in the document d . P  X  g with Jelinek X  X ercer smoothing ( Chen &amp; Goodman, 1996 ).

In iScore, the language models are updated as new documents are processed. However, the estimation of the probabilities is time-consuming, so the number of times the model is updated is minimized while still being able to produce meaningful avoid biasing the models from classifying articles as uninteresting (since there are an overwhelming number of uninteresting articles compared to interesting ones) and to reduce compilation time, the models are updated with all interesting articles, the number of interesting article seen.
 Using language models, several topic relevance measurements are extracted for each document. The first measurement is tween the language model of interesting past articles and the current article, using a 6-gram character model. This is commonly referred to as the binary language model classifier.

Language modeling classifiers that are trained and rebuilt in batches to reduce the consumption of computational re-complete article. So by using a much smaller training text to build the models, the models are more easily updateable. But instead of computing a very large product, which can approach 0 as the number of factors to be multiplied increase, the log-probability is taken instead, transforming the product of probabilities to a sum of probabilities:
For the title of the article, a language model built on bigrams, where the grams are words, is used. For the first paragraph of each interesting article, the language model built on the 6-grams, where grams are characters, is used. Using grams that are tokens for the bodies of articles would have resulted in too large and diverse of a language model to run efficiently so characters are used instead. 4.5. Clustering for anomaly detection
Articles that yield little new information compared to articles already seen may not be interesting. In contrast, an article that first reports a news event may be interesting. Anomalous articles that describe a rare news event may also be interest-ing. For example, Rattigan and Jensen (2005) hypothesize that interesting articles may be produced by rare collaborations icance of a putative signal that may be a small perturbation in a noisy experimental background. Work in network security, such as intrusion detection has already leveraged work in outlier detection. For example, Liao and Vemuri (2002) use a k-nearest-neighbor search (kNN) for intrusion detection. The occurrence of system calls is used to characterize program behav- X  X  X ocument. X 
The first anomaly measurement used is the dissimilarity of the current article with clusters of past articles. Each docu-ment is represented as a document vector, as in the Rocchio algorithm. At most maxCluster clusters are maintained, which are also represented by vectors. A count is maintained of documents that each cluster contains. The anomaly score is the weighted average dissimilarity score between the current document and each cluster, weighted by each respective cluster X  X  size (i.e., number of contained documents): which the document is similar to, then a new cluster is added to the list of clusters given the document X  X  vector. 4.6. Language models for anomaly detection
Two other methods for anomaly detection use language models. In the first model, compiled models trained on the doc-uments already seen are maintained, estimating the following: A 6-gram character model, and a bi-gram model, where grams are word stems, are experimented with.
 The second language model-based anomaly detection method measures the significance and the presence of new phrases.
A background model is maintained of all the documents previously seen and compare it with the language model of the cur-rent document. The sum of the significance of the degree to which phrase counts in the document model exceed their ex-pected counts in the background model is computed. Only the top-10 phrases that exceed their expected counts are considered. A tri-gram model, where grams are word tokens, is used. Significance of each n -gram is based on the z -score ( Alias, 2006 ): with P  X  success  X  defined by the n -gram X  X  probability estimate in the background model, the numSuccess variable being the count of the n -gram in the foreground model, and the numTrials variable being the total count in in the foreground model.
The probability of the opening paragraph (or title) being  X  X  X enerated X  from a language model of all seen first paragraphs (or titles) is measured as well. This serves as another measurement of uniqueness of an article. 4.7. Sliding anomaly detection
All previous measures of uniqueness examine how different the current article is with all articles seen previously. How-ever, it may be useful to measure the uniqueness of the article with the content of news from the last k days. A summary of the news from the last k days (where k is 30 days in the experiments) is maintained by summing the TF-IDF vectors of all articles published in the last k days: last k days and keeping a sum of the vectors as the ~ d summary sured by this measure is defined as follows: 4.8. Cluster movement
Previous techniques for measuring uniqueness aim at measuring how different an article is compared to previously seen articles. Instead, another method for measuring uniqueness will be to measure the impact an article has on its parent topic by measuring how much a cluster of articles has changed when an article is added to it. Articles are added to a cluster when is bootstrapped with a set of clusters to begin with.
 measured as follows: 4.9. Source reputation Source reputation estimates an article X  X  interestingness given the source X  X  past history in producing interesting articles.
Articles from a source known to produce interesting articles tend to be more interesting than articles from less-reputable may be its news agency or its author. In the experiments, the article X  X  author(s) are used for the Yahoo! News and the TREC as the average proportion of documents produced by the authors that were interesting in the past:
In otherwords, the source reputation value is estimated as the probability of the author of the article producing an inter-esting article. 4.10. Writing style Most work using the writing style of articles has mainly been for authorship attribution of news articles ( Li, Zheng, &amp; features over the course of a document have been used to segment documents as well ( Chase &amp; Argamon, 2006 ). Instead of author attribution and document segmentation, the same writing style features are used to infer interestingness. For exam-layman versus an expert). Also writing style features may help with author attribution, which can be used for classifying interestingness, where such information is unavailable.

A na X   X  ve Bayes classifier is used and trained on a subset of the features summarized by Chesley, Vincent, Xu, and Srihari in the topic relevance measurements, the number of positive and negative articles used to update the classifier is balanced.
The writing style score measured is:
The writing style contribution to interestingness is estimated as the probability of the article being interesting given the writing style features. 4.11. Freshness
Generally, articles about the same event are published around the time the event has occurred. This may also be the case esting articles and the current article:
The log of the temporal distance is measured between an interesting article and the current article since the order of mag-nificantly more interesting than an article published 100 days after the last interesting article. On the other hand, two esting article, even though they may have been published 1000 and 1500 days ago, respectively, after the last interesting 4.12. Topic driven freshness
The previous technique for measuring freshness measures the average log of the temporal distance of the last k interest-is closest to:
In the experiments, k is 10 articles, which seems to be a reasonable number of articles. Too large of a number would un-fairly weight topics that have existed longer. Too small of a number would yield inaccurate measurements. In summary, to-pic driven freshness measures the time difference between the current article and the last set of interesting articles that belong to the same topic as the current article. 4.13. Subjectivity and polarity
The sentiment of an article may also contribute to an user X  X  definition of interestingness. For example,  X  X  X ad news X  may be more interesting than  X  X  X ood news X  (i.e., the polarity of the article). Or, subjective articles may be more interesting than 2000 ). Others have looked at subjectivity tagging, using various natural language processing (NLP) techniques ( Wiebe, Wil-its subjectivity ( Wiebe, 2002 ) as well.

Four different features is maintained of this feature class: polarity, subjectivity, objective speech events, and subjective speech events. A speech event is a statement made by a person, such as a quotation. Using the Multi-Perspective Question
Answering (MPQA) Opinion corpus ( Wiebe, 2002 ) to train 6-g character language model classifiers, each sentence in the doc-ument is classified to determine its polarity, subjectivity, and the presence of objective or subjective speech events. The
MPQA corpus is a new article collection from a variety of news sources annotated for opinions and other states, such as be-liefs, emotions, sentiments, and speculations. For each document and each feature in this feature set, the following is measured: ions, speculation), the sentence contains an objective speech event, or the sentence contains a subjective speech event. 4.14. Phrase interestingness In the techniques used in the previous sections for measuring topic relevance, bags of words are mostly used, such as in
Rocchio and MTT. Language models make some effort to go beyond the bags of words approach but often examines phrases that do not make sense. For example, in the sentence,
The black dog jumped over the fence. the following tri-grams would be examined:  X  X  X he black dog, X   X  X  X lack dog jumped, X   X  X  X og jumped over, X   X  X  X umped over the, X  and  X  X  X ver the fence. X  However, if only noun phrases are looked at,  X  X  X he black dog X  and  X  X  X he fence X  would be the only can-didates. Using a noun-phrase extractor provided by OpenNLP (2006) , noun phrases are extracted and normalized (making all characters in the phrase lowercase, removing stop-words, and stemming each word in the phrase). The average probability of the interestingness of noun phrases is measured as: 5. Online feature selection and classification
The overall classifier computes the final iScore given all the features X  values generated by the feature extractors. Because the features are continually refined as more documents are seen, some of the feature values may be erroneous for early doc-uments. The ideal classifier must be able to adapt quickly to these changes.

Furthermore, the definition of interestingness varies from user to user. For example, the writing style of an article may be training period, the usefulness of the recommendation system suffers. Users of recommendation systems are less inclined as: where Z is a scaling factor dependent on f 1 ; ... ; f n , and Int is the interesting article class. The probability P  X  f which is equivalent to marginalizing over them.
 ciated with the subset of features with the highest FMeasure statistic. Because of the conditional independence of the fea-tures, only a single set of statistics is needed to be maintained, (in the form of kernel estimators) related to P  X  f the subset are essentially ignored when generating a classification score from the na X   X  ve Bayes classifier. interestingness for each feature.

Because statistics about each feature are continually maintained, a feature that was deemed useless earlier can be in-features are ignored for the overall document classification, statistics learned about the features are never forgotten.
Since only subsets of features with the highest correlations are considered for each document, as opposed to all possible would be expected to be very low performing for document classification; whereas, sets of features with high correlation would be expected to be higher performing. Because only the top-k most highly correlated features are considered, subsets consisting of only low correlated features are never considered. And from document to document, one would expect to see for that document.

The Pearsons correlation is used to evaluate the usefuless of features. Correlation is defined as: E is the expected value operator. l X and r X are the average and standard deviaton of the random variable X , respectively. able during evaluation (as in an online streaming environment). Consequently, correlation is used instead due to its simple online computability and its lack of a need for discretization.

For example in Fig. 4 , let there be three features that are ordered in terms of correlation with interestingness: f f . Using this order, the following subsets are evaluated for their classification effectiveness: f f with the correlation of each feature with interestingness and the FMeasure of each subset. The process repeats with the next article in the document stream. 6. Experimental results
In this section, the recommendation performance results are summarized for the Yahoo! News, tagger, and Digg datasets, comparing the iScore with feature selection, iScore with no feature selection, the Rocchio Variant (which performed the best in the last TREC Adaptive Filter task), a language modeling classifier, MTT, and eRocchio. 6.1. Datasets and other non-news material). However, there is currently no experimental dataset that matches the criteria perfectly so for the interesting article recommendation task, several datasets are used: Yahoo! News, tagger and Digg. 6.1.1. Yahoo! news a span of 3 months and 1 year, respectively. The smaller dataset is used in a few of the earlier experiments of iScore; community of users is determined by an interest-driven RSS feed from the Yahoo! News articles collection. The 43 interest-driven RSS feeds considered for labeling are feeds of the form:  X  X  X op Stories [category], X   X  X  X ost Viewed [category], X   X  X  X ost
Emailed [category], X  and  X  X  X ost Highly Rated [category], X  including category-independent feeds such as the  X  X  X op Stories, X   X  X  X ost Emailed, X   X  X  X ost Viewed, X  and  X  X  X ighest Rated X  feeds. For example, RSS feeds such as  X  X  X ost Viewed Technology X  is a good proxy of what the most interesting articles are for technologists. Other categories, such as  X  X  X op Stories Politics, X  a community of users. 6.1.2. Tagger
In the Yahoo! News dataset, the user is modeled after a community instead of as an individual interested in a particular category. Consequently, in addition to the Yahoo! RSS feeds, the second dataset consists of articles that are anonymously collected from volunteer news readers that tag articles as they read their daily news on the web. A user can tag an article using a Firefox plug-in or a Google RSS Feed GreaseMonkey script add-on for Firefox. The script and plug-in do not record any personal identifiable information but instead uses a unique randomly generated identifier to uniquely identify users. uninteresting enough to not click on. The webpages are downloaded every night. Webpages that are non-articles (e.g., adver-tisements, table of contents, videos) are manually removed by this author from the collection. There are only 16 users that user over only the documents that have been seen by a user as indicated by a user tagging or by existing on a referring page of a tagged article. 6.1.3. Digg
Due to the difficulties in recruiting volunteers to consistently read and tag articles, an alternative method is looked at, which is collected via the web service, Digg, to complement the user tagging collection. There have been 18,924 pages from (i.e., the front page) of Digg since the most active users on Digg are the most active users in the Digg community and are is much smaller and much noisier due to the heterogeneity of the type of pages being collected. Many of the webpages downloaded may not be news but images, videos, and other webpages beyond the scope of this paper. 6.2. Evaluation metrics
In this paper, FMeasure is primarily the metric that has been focused on. FMeasure encompasses both precision and recall, so a good FMeasure score will be a balance of both of the basic retrieval metrics. FMeasure may also be defined in terms of precision and recall as follows: FMeasure is 0 when the number of articles retrieved is 0.
 TREC11 X  X  T11SU is also used for comparing the performance of iScore with the work done in TREC11: For systems that retrieve no articles, the system would have a T11SU score of 0.33.

An ideal system would yield high FMeasure and T11SU scores overall and across time. In the experiments described in identify the best classifier. The follow tests are looked at: 1. The overall performance, averaged over all users in the collection, after the entire document collection is processed. the entire document collection is processed. The bottom-k and the top-k users are defined as the users for which iScore provided the best and worst recommendations. This test will show the performance of the classifiers for the very difficult and the very easy users to recommend for. 3. The cumulative average performance, averaged over all users in the collection, as documents are processed. This test will show any consistent overall performance differences among the classifiers, regardless of the collection size. 4. The average performance, averaged over all users in the collection, for the last 5000 documents. This test will show the current performance of the classifiers. 6.3. Yahoo! News
For the Yahoo! News collection, the best performing classifier seen is iScore with feature selection. Fig. 5 shows the aver-language modeling classfier, MTT, eRocchio, iScore with no feature selection, and iScore with feature selection. The figure This is 24% better than the best baseline classifier, the language modeling classifier.

Fig. 6 shows the average FMeasure of all, the top-10 and bottom-10 performing feeds/users. The figure shows that iScore with online feature selection can give much better performance for the worst performing feeds/users than all of the other baseline classifiers. It also shows that it has the best performance of the best performing feeds/users as well.
A similar result is seen in Fig. 7 , which shows the FMeasure performance of the classifiers over the 5000 most recent doc-most likely due to a long pause in the data collection for this time period. 6.4. Tagger For the tagger dataset, the iScore with online feature selection performs the best, much like the Yahoo! News collection. sifiers and iScore with no feature selection.

Fig. 9 shows that iScore with online feature selection performs better due to recommendation improvements for the eas-iest users and most difficult users and for most users in general. The overall performance improvement provided by iScore using online feature selection over iScore with no feature selection is 11.3%. The improvement over the best baseline clas-sifier (i.e., the Rocchio variant) is 50.7%. 6.5. Digg For the Digg dataset, the best classifier seen is iScore with online feature selection. Fig. 10 shows the average FMeasure, performs better in terms of FMeasure alone, iScore with feature selection has a much higher T11SU score and precision than core with no feature selection. Also, it is 5.2% better than the best baseline classifier (i.e., the Rocchio variant).
Fig. 11 shows the average FMeasure of all, the easiest, and the most difficult users to recommend for. The figure indicates that online feature selection has improved upon the recommendation performance of iScore with no feature selection.
Fig. 12 shows the classifiers X  FMeasure performance over the 5000 most recent documents at various time periods. The ods for iScore than the other classifiers, indicating better stability against noise and gaps in data collection. 6.6. Summary
In summary, iScore with online feature selection works generally well for all datasets. The Yahoo! News and tagger data-ating over several high-level  X  X  X nterestingness X -based features can generally recommend better articles than traditional information retrieval techniques. 7. Conclusion
The online recommendation of interesting articles for a specific user is a complex problem, having to draw from many to addressing this problem.

To address news recommendation in a limited user environment, where methods such as collaborative filtering would perform poorly, a news recommendation framework, called iScore, is introduced to analyze the reasons why an article is interesting. The iScore framework consist of several features, including Rocchio, multiple topic tracking, online parameter selection, clustering, language models, anomaly detection, source reputation, writing style, freshness, sentiment analysis, and phrase extraction. By incorporating these features and online feature selection, iScore can generally give better recom-mendation results than standard information retrieval techniques with three datasets with which iScore has been evaluated: recommendation results than traditional information retrieval techniques.
 Acknowledgement
This work was performed under the auspices of the US Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 (LLNL-JRNL-407783).
 References
