 Most statistical learning models make the assumption that data instances are IID samples from some fixed distribu-tion. This assumption is often violated in real-world sit-uations. In many cases, the data are collected from dif-ferent sources, at different times, locations and under dif-ferent circumstances. In such cases, the training data is often not representative of the data over which the classi-fier must operate. For example, in classifying news arti-cles, instances are usually organized chronologically. New events, people and places appear (and disappear) in bursts over time (Kleinberg, 2002). The training data might con-sist of articles taken over some time period; these are only somewhat representative of future articles. In a task of clas-sifying customers into categories, our training data might be collected from one geographical region, which may not represent the distribution in other regions.
 Traditionally, this distribution drift is either ignored or avoided by changing the data to conform better to the IID assumption  X  all examples are mixed together and training and test are selected randomly. Unfortunately, this homo-geneity cannot be ensured in real-world tasks, where only the (non-representative) training data is actually available for training.
 One aspect of the non-homogeneity phenomenon is that the test data may contain many features that were never or only rarely observed in the training data. These features may be very useful for classification. For example, in our news ar-ticle task, these features might include the names of places or people currently in the news. In the customer example, the features might include purchases of products that are specific to a region (e.g., snow suits). However, these fea-tures and their effect on the class label cannot be identified from the training data.
 In this paper, we propose a method for identifying and uti-lizing these unseen features. Our approach is to associate with each unseen feature a hidden variable that encodes in-fluence this feature has on the class label. The value of this variable is unknown, and must be  X  X earned X  from the test data, without knowledge of the labels in the test data. Our method uses probabilistic inference over a graphical model to infer these values. Probabilistic inference effec-tively  X  X ootstraps X  from instances that are classified with high confidence, identifying the interaction between these unseen features and the class, and thereby helping to clas-sify other new instances.
 So what, if anything, can we learn from the training data about these new features? Although the unseen features in the test data do not appear in the training data, the training data might contain other features that play a similar role. For example, in our news article domain, the phrase  X  X XX said today X  might appear in many places in the data, for different values of  X  X XX X . In many cases,  X  X XX X  is the name of a person in the news, and might be (temporarily) a useful feature for determining the topic of an article. In the customer example, we might have features of products that can help us predict their role as unseen features. Our algorithm learns a template model which predicts the effect of a unseen feature from from a set of meta-features . This template model is learned from seen features in the training data, and applied to unseen features in the test data. The remainder of this paper is structured as follows. In Section 2 we describe a general framework and probabilis-tic model for global and unseen features. In Section 3 we describe our algorithm for learning this model from data and in Section 4 we show how it can be used in classifi-cation. In Section 5 we provide experimental results on the Reuters news articles dataset and a University web site dataset, showing significant improvement in accuracy. We conclude in Section 6 with discussion and a comparison to related work. 2.1. Scopes To make our intuitions precise within the framework of a probabilistic model, we need the notion of a scope . We assume that data instances are sampled from some set of scopes, each of which is associated with some data distri-bution. Of course, the problem only makes sense as a co-herent learning problem if some aspects of the distributions are shared. We focus on cases where the different distri-butions share a probabilistic model for some set of global features , but can contain a different probabilistic model for a scope-specific set of local features . These local features may be rarely or never seen in the scopes comprising the training data.
 Let X denote global features, Z denote local features, and Y the class variable. We assume, for simplicity, a binary classification problem where Y  X  { X  1 , 1 } ; our model can be extended to multi-class classification. In scope-varying data, we assume that for two scopes S and S 0 P
S ( Y | X , Z ) and P S 0 ( Y | X , Z ) can be quite different, but the effect of the global features is preserved. To make this intuition precise, we fix a particular parametric model for P S ( Y | X , Z )  X  logistic regression. In this model, for each global feature X i , there is a parameter  X  i . Addi-tionally, for each scope and each local feature Z i , there is a parameter  X  S features and weights is Fig. 1 shows a representation of this probabilistic model in terms of plates. The global feature parameters  X  are the same across scopes, while the local feature parameters  X  S depend on the scope S . As is often the case, we assume that the global weights can be learned from the training data, so that their values are fixed when we encounter a new scope. The key issue that we try to address in this paper is that the local feature weights are unknown. 2.2. Probabilistic Model The main idea behind our approach is that these local fea-ture weights can be treated as hidden variables in a graph-ical model. In such a model, we would like evidence from global features for the labels of some of the instances to modify our beliefs about the role of the local features present in these instances to be consistent with the la-bels. By learning about the roles of these features, we can then propagate this information to improve accuracy on instances that are harder to classify using global features alone. To implement this idea, we define a joint distribution For reasons that we discuss below, we use the framework of undirected graphical models, or Markov networks (Pearl, 1988). Specifically, we define a hybrid Markov network (one containing both continuous and discrete variables), that specifies a single joint distribution over the class vari-ables and features of all of the instances in a scope, and the feature parameters.
 We begin by briefly reviewing the framework of Markov networks. We present the log-quadratic parameterization of a hybrid Markov network, as it is more directly suited to our needs. Let V = ( V d , V c ) denote a set of random variables, where V d are discrete and V c are continuous variables, respectively. A Markov network over V defines a joint distribution over V , assigning a density over for each possible assignment v d to V d . A Markov net-work M is an undirected graph whose nodes correspond to
V . It is parameterization by a set of potential functions  X  ( C 1 ) , . . . ,  X  ` ( C ` ) , such that each C  X  V is a fully con-nected subgraph, or clique, in M , i.e., each V i , V are connected by an edge in M . (Note that a single node is also considered a clique.) For our purposes, we assume that the function  X  ( C ) is log-quadratic in variables u each value u d of C d . The Markov network then represents the distribution: P ( V )  X  Q ` In our case, the Markov network for a given scope contains: the global and local features X 1 , . . . , X m and Z 1 , . . . , Z m for all instances; the labels for all instances Y 1 , . . . , Y m and the global and local feature weights  X  and  X  S . Our log-quadratic model contains three types of potentials: One type of potential has the form  X  (  X  i , Y j , X exp {  X  i Y j X j i } , and relates each global feature X stance i to its weight  X  i and the class variables Y j of the cor-responding instance i . A similar potential  X  (  X  i , Y j , Z exp {  X  i Y j Z j i } relates the local feature Z j i to its weight and the label Y j . Finally, as the local feature weights are assumed to be hidden, we introduce a prior over their val-ues, or the form  X  (  X  S Overall, our model specifies a joint distribution as follows: where  X  (  X  S ) = Q Viewed graphically, our Markov network contains edges between each Y j and all of its global features and corre-sponding parameters, and between Y j , and each of its local features and corresponding parameters associated with that local feature. Fig. 2(a) shows a simple example. The graph can be simplified considerably, when we account for vari-ables whose values are fixed. In this case, we can simply in-stantiate the various functions in our log-quadratic models with these values, and omit the variables from the model. As we discussed, we assume that the global feature weights  X  are learned from the training data, and hence their value is fixed. Furthermore, in a classification setting, the actual feature values are known. The resulting Markov network is a fully connected bi-partite graph over the set of unob-served variables { Y j } and {  X  i } . However, when the local features Z j nificantly. When Z j Y j and any of the variables  X  i . In this case, we can sim-ply omit the edge between  X  i and Y j . For example, if our instances are documents and our local features are words, our model would contain an edge only between the label of a document and the weights of the scope-specific words (local features) that it contains. An example of this condi-tioned Markov network is shown in Fig. 2(b).
 In this model, we can see that the labels of all of the in-stances are correlated with the local feature weights of fea-tures they contain, and thereby with each other. Thus, for example, if we obtain evidence (from global features) about the label Y 1 , it would change our posterior beliefs about the local feature weight  X  2 , which in turn would change our beliefs about the label Y 2 . Thus, by running probabilis-tic inference over this graphical model, we obtain updated beliefs both about the local feature weights and about the instance labels.
 Importantly, this flow of influence depends critically on our choice of single joint undirected graphical model. Suppose, for example, that we had chosen a directed model, where each class label variable Y j is simply a logistic conditional distribution of the instance features. In this model, illus-trated in Fig. 2(c), when the labels ( y  X  X ) are not observed, they d-separate the weights from each other, and no influ-ence flows from the observed global features, through the labels, to help infer the role of the local features. We now describe how the model described in the previous section can be learned from data. 3.1. Learning Global Feature Weights For the case of global features, the problem is an easy one. We simply learn their parameters from the training data, using standard logistic regression. Maximum-likelihood (ML) estimation finds the weights  X  that maximize the con-ditional likelihood of the labels given the global features, To help avoid overfitting, we assume a  X  X hrinkage X  prior over the weights (a zero-mean Gaussian), and use maxi-mum a posteriori (MAP) estimation. More precisely, we assume that parameters are a priori independent and define P (  X  ) = model using conjugate gradient, where the gradient of the log-posterior objective is given by: 3.2. Learning Local Feature Distributions The weights of the local features are unknown in a test scope. Hence, we use a distribution over their values. Most simply, we use a Gaussian distribution with zero mean and some variance  X  2 . In this case, we assume that we know nothing about the local features, and hence all of them have an identical prior distribution. More interesting, however, is the case where we can infer something about a local fea-ture, even if we have never seen it before.
 As we discussed in the introduction, we often have addi-tional cues that indicate the role of a local feature. For example, consider the problem of trying to distinguish be-tween news articles labeled grain and those labeled trade . Words such as  X  X orn X  or  X  X heat X  are very useful for dis-tinguishing these topics. These words often appear within the phrase  X  X ons of corn (wheat) X . Thus, we can learn that if a word  X  X XX X  appears in the context  X  X ons of XXX X , it is likely to have positive interaction with the label grain . If we now see the phrase  X  X ons of rye X  in the test data (where  X  X ye X  is rare in the training data), we can infer that  X  X ye X  probably has positive interaction with the label grain . This conclusion, in turn, will let us better classify documents containing the word  X  X ye X , even if that word does not ap-pear in the context  X  X ons of rye X .
 We can exploit such patterns by learning a model that pre-dicts the prior of the local feature weights using meta-features  X  features of features. More precisely, we learn a model that predicts the prior mean  X  i for  X  i from some set of meta-features m i . In our news classification exam-ple, these meta-features might consist of the words in the vicinity of the word (feature) Z i from all of the word X  X  occurrences in the scope. Other meta-features might in-clude capitalization, morphological information, presence in a list of proper names, or more. As our predictive model for the mean  X  i we choose to use a linear regression model, setting  X  i = w  X  m i .
 The parameters w are learned from the training data. We first learn a feature weight for each global feature, and for each feature local to the training scope(s). We then train the regression model weights w to predict the weight of feature i as w  X  m i . We used a standard linear regression with ridge penalty of 0 . 1 .
 Note that we train the meta-feature model using both global and local features. This decision reflects an assumption that the meta-features help predict the weights of any feature, local or global. In other words, features that have similar context tend to interact similarly with the class label. This assumption allows us to utilize the information from global features to learn our probabilistic model for local feature weights.
 The other parameter characterizing the distribution over lo-cal feature weights is the variance. Intuitively, if the vari-ance of a weight is small, its posterior distribution must stay fairly close to its original mean. The larger it is, the more flexibility there is for the test instances to change the posterior mean, updating our beliefs about the role that this feature plays in the test scope. We currently do not learn the variance parameter, but set it via cross-validation. Now that we have discussed both the basic model and how it is learned from data, we describe the overall use of the model in the context of an actual learning task.
 Given a training set, we first learn the model, as described in the previous section. We note that, in the training set, there local and global features are treated identically. When applying the model to the test set, however, our first deci-sion is to determine the set of local and global features. In some cases, such as the work of (Blei et al., 2002), prior knowledge helps us distinguish global and local features. When we have binary-valued features (such as words), we can use a simple heuristic, and select as local features those features that have zero or low frequency in the training set and high frequency in the test set. The intuition behind this rule is twofold: First, it is clear that the probabilistic model for these features is fairly different in the training and test set, so they are likely to be scope-specific. Second, the model learned for these features in the training data is likely to be quite poor, so a local model is often better. In general, the problem of distinguishing local features from global ones is far from trivial, and is an important topic for future work.
 Our next step is to generate the Markov network for the test set, as described in Section 2.2. This Markov network de-fines a joint probability distribution over the two types of hidden variables: the weights of the local features, and the instance labels. As we discussed, probabilistic inference over this model will precisely implement the bootstrapping process described above for inferring the effect of local fea-tures. It will also label the instances in a way that takes advantage of this information.
 The major difficulty is that this joint distribution is over a very high-dimensional space. However, the presentation as a graphical model reveals certain structure that can be ex-ploited for inference. In very simple cases, the graph may be sufficiently structured that exact (up to numerical error) inference can be used (Lerner et al., 2001). In our experi-ments, however, the graphs contain thousands of nodes and are quite densely connected. Exact inference is completely intractable in these cases. We therefore resort to approxi-mate inference.
 Several approximate inference methods can be used to solve this problem, including variational (Jordan et al., 1999; Blei et al., 2002), or Monte Carlo sampling. We chose to use expectation propagation for its simplicity and relative efficiency and accuracy. Expectation Propa-gation (EP) is a local message passing algorithm (Minka, 2001; Heskes &amp; Zoeter, 2003) akin to Belief Propagation (BP) (Pearl, 1988; Yedidia et al., 2000). Like BP, EP main-tains approximate beliefs (marginals) over nodes of the Markov network and iteratively adjusts them to achieve lo-cal consistency. In our networks, we maintain independent Gaussian beliefs over  X  variables, and discrete beliefs over the Y variables. The messages m (  X  ) and beliefs b (  X  ) tialized to uniform. Nodes then iteratively pass messages to each other, and update their local beliefs.
 In our network, each edge connects between a node repre-senting a local feature weight  X  i and a node corresponding to a label Y j . We therefore have two types of messages: m i ( Y j ) that goes from the weight to the label, and m j that goes the other way. These messages are computed as follows: where  X  is a normalizing constant. Computing the mes-sage m i ( Y j ) involves one-dimensional numerical integra-tion which can be done efficiently using Gaussian quadra-ture (Press et al., 1988).
 In BP, the beliefs at each node are computed as the prod-uct of the prior and the incoming messages. In hybrid net-works such as the one in our application, the resulting func-tion is a very complex multi-modal function, and is com-putationally prohibitive to maintain. In EP, we approximate these beliefs as a Gaussian, and update them using  X  X eak marginalization X , where the product of messages from Y  X  X  and the prior is approximated by matching its mean and variance: Again, we use Gaussian quadrature to compute the mo-ments of b 0 (  X  i ) . The beliefs of the label nodes b ( Y j ) = messages. We performed experiments on two data sets  X  Reuters, a news articles collection, and Webkb2, a collection of web pages from universities. 5.1. Reuters selected four categories in the data set that contain a sub-stantial number of documents (  X  500 ); these categories are grain , crude , trade , and money-fx . We eliminated docu-ments that are labeled with more than one of these four labels. We split the data into articles, tokenized it, and rep-resented each document as a bag of words.
 Using this data set, we created six experimental setups, by using all possible pairings of categories from the four cat-egories that we have chosen. For each pairing, we com-bined all documents with those two labels, and sorted them temporally. We divided the resulting sequence into nine time segments with roughly the same number of documents (  X  100 ) in each segment. Thus, documents from the same segment represent articles that are published at around the same period of time, and hence can be assumed to corre-spond to a scope. In each setup, we set aside segments 7 X 9 (  X  300 documents) as our test set, and segment 6 as a validation set. We then trained five models, using each of the segments 1 X 5 separately as a training set. We used the data in segment 6 to select the parameter  X  for the Gaussian prior used for regularization for all models, and evaluated the accuracy on the test set.
 Our baseline model Flat is a logistic regression model that uses words as features. We tried different variations for the baseline model, including one that learned using only  X  X lobal words X   X  words that appear in the training set with enough frequency ( &gt; 8 documents). However, our best model is a Flat model using all words, with each document normalized according to its length.
 Unseen is the basic model that leverages on unseen fea-tures. For seen features, it uses parameters learned from Flat . As for unseen features, it models the uncertainty over their weights with a zero mean Gaussian and variance set to  X  , which we set using cross-validation.
 Unseen-fc is almost identical to Unseen , except that we feature j appeared in the test set. Again, we set  X  using cross-validation.
 Context builds on top of Unseen-fc by allowing the prior means of unseen features to be changed by their meta-features, which in this case are words appearing in their contexts. We used the individual words in a window of  X  3 around the feature word as meta-features, as well as consecutive pairs of words within the same window. Fi-nally, we eliminated meta-features that appear too fre-quently ( &gt; 250 times) or too infrequently ( &lt; 10 as context words.
 Fig. 3 shows the results. Six sets of three columns rep-resent test error of the three models for each pair of cat-egories ( grain , crude , trade , and money-fx ) abbreviated by the first letter. Last set of columns is the average test er-ror. Each column is an average of 5 folds. As shown in the figure, both Unseen and Unseen-fc perform better than Flat . The relative reduction in error is 38.51% and 48.47%, respectively. Moreover, Context performs better than the stronger unseen feature model Unseen-fc , with a 15.19% relative reduction in error. The p-values from paired t-test for each of the models Unseen , Unseen-fc and Con-text with flat are 0.0000566, 0.0000192 and 0.00000749 respectively. Note that the meta-features of the Context model are also a part of the global features and are already used by the Flat model. Hence the improvement that comes from the Context model is due to the indirect impact of these meta-features, which provide the model with a rea-sonable guess for the weights of the unseen features. We also compared our models to transductive SVMs using of the SVM parameter C , transductive SVMs with a linear kernel achieve average error of 8 . 59% (averaged over all folds and pairings), which is significantly worse than our models.
 Fig. 4 shows how the performance of models varies with the temporal distance between training and test sets. As evident from the graph, the Flat model gradually performs worse with increasing temporal distance. This is because the distribution of words between training and test sets be-comes more and more different as the temporal distance be-tween them increases. While the Flat model learned words that are useful for prediction in one time period but not the other, the models utilizing unseen features are able to adapt to changing distributions, and thus do not suffer from the same problem.
 It is interesting to examine what meta-features had high impact on determining the function of the unseen fea-tures. Some examples with high regression weight include: tonnes of (wheat/rye/rice) , and export for the category grain , and kuwait , saudi and demand for (petroleum/oil) for the category crude . Moreover, the model successfully inferred that many features that were not seen on the train-ing were very useful in classification on the test set. Some examples with high posterior mean are texaco and chevron for the category grain and ventures and prosperity for the category trade . 5.2. Webkb2 This data set consists of hand-labeled web pages from Computer Science department web sites of four schools: Berkeley, CMU, MIT and Stanford. We chose four cate-gories based on the requirement that they each have a sub-stantial number of documents in each school (  X  100 per school). The categories are faculty , student , course and or-ganization . As in Reuters, we tokenized each web page and represented it as a bag of words.
 We created six experimental setups by using all possible pairings of categories from the four categories that we have chosen. In this dataset, each school naturally corresponds to a scope. Thus, for each pairing, we train a model us-ing each school, using one of the other three schools for validation and the remaining two for test.
 Fig. 5 shows the results of our runs on Webkb2 using two of the models described above, Flat and Context . Six sets of two columns represent test error for each pair of categories ( faculty , student , course , and organization ) abbreviated by the first letter. Last set of columns is the average test error. Each column is an average of 8 folds.
 As shown in the figure, Context performs better than Flat on all but one pairing, and is superior on average. The re-duction of error is 5.5%, and the p-value from paired t-test is 0.0368.
 Although our model outperforms the baseline, the improve-ments on this data set are not as large as in Reuters. There are several possible reasons. First, the words in web pages might not be as well suited as local features when compared to the case of news articles. Unlike words in news articles, which are closely associated with the topic of the article, web pages contain more heterogeneous text. Features of html formatting of web pages might serve as better local features; although html formatting changes from school to school, within a particular school, it may be a good indica-tor of the type of web page. A second reason for the reduc-tion in improvement is that the average performance of the Flat model in the case of Reuters is better. This improved base gives our models a better starting point for inferring the values of unseen features. Our work is not the first to address the task of learning with unseen features or scope. Recently, Blei et al. (2002) ad-dressed the problem of local features in the context of web-page classification and information extraction. There are some important conceptual differences between our work and theirs. In their work, local features were disjoint from global features, with local features consisting of html for-matting and global features of words on the page. This clean partition simplifies their setting. A second key differ-ence is that they learn very little concerning unseen features from the training data, whereas our approach learns the context characterizing features. At a more technical level, their work assumes that the global probabilistic model is naive Bayes, a model whose unrealistic independence as-sumptions often lead to suboptimal results for text. The problem of discovering regularities and adapting to the test set is related to several threads of work. Slattery and Mitchell (2000) designed an iterative procedure that ex-ploits web-specific unseen features (directory pages) in or-der to improve classification of web pages. However, their procedure is limited to using these very specific features and lacks a global underlying model (probabilistic or oth-erwise).
 Our approach is also related to the transductive learning setting (Vapnik, 1995), in which the learner is presented the (unlabeled) test set together with the training data, and can attempt to optimize its performance on just the examples in the test set. Joachims (1999) defines transductive support vector machines and shows improvements over purely in-ductive methods for text classification. However, transduc-tive learners assume the presence of both training and test set at training time and cannot adapt to multiple test sets. A probabilistic approach proposed by Nigam et al. (2000) uses the EM algorithm to combine labeled and unlabeled data to improve classification. However, the underlying as-sumption in that work is that the distribution in training and test are the same.
 In general, however, the problem of heterogeneous data distributions is a complex one, and none of the work done so far provides a comprehensive solution. We propose a probabilistic framework that explicitly accommodates for the notion of different scopes with varying data distribu-tions. We also describe a model that accounts for one source of variability between scopes  X  the appearance of new features. Clearly, however, this model addresses only part of the problem: Even a feature that is common in one scope can still take on a different meaning in another. Thus, we may want to relax the strict partition into local and global features, allowing the meaning of a feature to vary, to some degree, between scopes. In this case, we would need to construct both a mechanism that learns the role of a feature in one scope, but allows it to vary (to some extent) in others.
 Finally, our work assumes that we are given a partition of data into scopes. In many settings, this assumption is a reasonable one. In others, however, this partition may not be known, or (as in the temporal setting) may not even be a sharp partition, but rather a gradual shift. It would be very interesting to try and detect changes in the distribution automatically, and update the model accordingly.
 This work was supported by ONR Contract F3060-01-2-0564-P00002 under DARPA X  X  EELD program.
 Blei, D. M., Bagnell, J. A., &amp; McCallum, A. K. (2002).
Learning with scope, with application to information ex-traction and classification. Proceedings for Uncertainty in Artificial Intelligence (UAI) .
 Heskes, T., &amp; Zoeter, O. (2003). Generalized belief propa-gation for approximate inference in hybrid bayesian net-works. In Proc. AISTATS .
 Joachims, T. (1999). Transductive inference for text clas-sification using support vector machines. Proc. ICML99 (pp. 200 X 209). Morgan Kaufmann Publishers, San Fran-cisco, US.
 Jordan, M. I., Ghahramani, Z., Jaakkola, T., &amp; Saul, L. K. (1999). An introduction to variational methods for graphical models. Machine Learning , 37 , 183 X 233. Kleinberg, J. (2002). Bursty and hierarchical structure in streams. In Proc. 8th ACM SIGKDD .
 Lerner, U., Segal, E., &amp; Koller, D. (2001). Exact inference in networks with discrete children of continuous parents.
Proceedings of the 17th Annual Conference on Uncer-tainty in AI (UAI) (pp. 319 X 238).
 Minka, T. (2001). A family of algorithms for approximate bayesian inference . Doctoral dissertation, MIT Media Lab.
 Nigam, K., McCallum, A. K., Thrun, S., &amp; Mitchell, T. M. (2000). Text classification from labeled and unlabeled documents using EM. Machine Learning , 39 , 103 X 134. Pearl, J. (1988). Probabilistic reasoning in intelligent sys-tems . San Francisco: Morgan Kaufmann.
 Press, W., Teukolsky, S., Vetterling, W., &amp; Flannery, B. (1988). Numerical recipes in C . Cambridge University Press.
 Slattery, S., &amp; Mitchell, T. (2000). Discovering test set regularities in relational domains. Proc. ICML00 (pp. 895 X 902).
 Vapnik, V. (1995). The nature of statistical learning theory . New York, New York: Springer-Verlag.
 Yedidia, J., Freeman, W., &amp; Weiss, Y. (2000). Generalized
