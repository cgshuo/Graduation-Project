 This paper addresses the problem of long-term language change in information retrieval (IR) systems. IR research has often ignored lexical drift. But in the emerging domain of massive digitized book collections, the risk of vocabulary mismatch due to language change is high. Collections such as Google Books and the Hathi Trust contain text written in the vernaculars of many centuries. With respect to IR, changes in vocabulary and orthography make 14th-Century English qualitatively different from 21st-Century English. This challenges retrieval models that rely on keyword match-ing. With this challenge in mind, we ask: given a query writ-ten in contemporary English, how can we retrieve relevant documents that were written in early English? We argue that search in historically diverse corpora is similar to cross-language retrieval (CLIR). By considering  X  X odern X  English and  X  X rchaic X  English as distinct languages, CLIR techniques can improve what we call cross-temporal IR (CTIR). We fo-cus on ways to combine evidence to improve CTIR effective-ness, proposing and testing several ways to handle language change during book search. We find that a principled combi-nation of three sources of evidence during relevance feedback yields strong CTIR performance.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval; H.3.7 [ Digital Libraries ]: Systems Issues Information retrieval, temporality, digital libraries, book search
Today, digitized book collections are growing in size and scope. This raises new challenges for information retrieval (IR) research. Repositories such as Google Books 1 and the Hathi Trust 2 collect millions of scanned volumes with the aim of improving readers X  access to the range of informa-tion stored in academic libraries. To meet this goal, these collections must support appropriate search interactions.
This paper tackles a specific problem in book search: long-term language change. Though many books in, say, the Hathi Trust are written in English, 20th-Century authors used English very differently than authors did during the 14th-or even the 17th-Centuries. This presents a challenge to retrieval models that rank documents based on the dis-tribution of query words observed in documents. In Google Books, a query term work may imply a user X  X  interest in werk, woork, wyrke or even swyinkinge . All textual IR suf-fers from the vocabulary mismatch problem, where searchers and authors use different terms for similar ideas. But in his-torically diverse collections, this mismatch is exacerbated by centuries of linguistic evolution.

Our aim in this paper is to allow a query written in con-temporary English to retrieve documents written in the ver-nacular of older centuries. Thus a person interested in the pedigree of the proverb many hands make light work will be able to find a broad range of variants on this theme, from a range of historical periods using a single query. This task is similar to cross-language information retrieval (CLIR), and so we will refer to it as cross-temporal retrieval (CTIR). This paper X  X  main contribution is a novel approach to CTIR. We present a model that imagines contemporary queries as translations of archaic statements. Given a query in mod-ern English Q M we attempt to recover terms from the  X  X rue X  archaic version Q A . The model yields a distribution over terms that fits naturally into a structured query for the in-ference network retrieval model. While we describe and test several techniques, the paper X  X  main contribution (described in Section 6.4.2) is a relevance feedback method that com-bines information from a bilingual dictionary, orthographic evidence and feedback probabilities in a principled way.
Though book search has a strong research community  X  especially in the INEX book track  X  many issues that impact the effectiveness of IR in digitized book collections have yet to be articulated clearly. In particular, realistic use cases and their attendant demands on a system are non-obvious for search over digitized book collections. Who would use these collections, and for what purposes? What would realistic queries look like? What constitutes relevance in peoples X  interactions with a scanned and OCR X  X  library?
In this paper, we limit our attention to a fairly narrow hypothetical use case. Our imagined user  X  probably a hu-manities scholar, perhaps a general-interest reader  X  is in-terested in a particular phrase or historically durable idea. Examples of such interests include: Thus our user might be interested in finding variants of the opening lines of Chaucer X  X  Canterbury Tales . These lines vary from manuscript to manuscript and edition to edition, each with different spellings and vocabulary. They were also adapted into increasingly modern English as critics and writ-ers influenced by Chaucer echoed his text. To get a sense of the range of these lines X  deployment in literature over the years, a searcher would need to use a typical IR system very carefully, relying on a great deal of expert knowledge.
In other cases, there is no canonical text to retrieve. The proverb many hands make light work has changed over the years; it has no  X  X rue X  version. A communitie maketh the werk lesse is clearly related to this proverb, though neither rendition is in any sense more authoritative than the other.
We imagine that a user of our system is a person with an interest in finding historical variants of a textual seed which we call the searcher X  X  exemplar . We aim to retrieve items that contain renditions of that exemplar  X  appropriations, variants, in-line quotations, etc. This suggests that topical relevance is not a suitable criterion for optimizing our sys-tem. Searches like those described here seek variants of an exemplar, not necessarily documents that are about that ex-emplar. A critic X  X  quote of Chaucer X  X  opening lines would be relevant to this searcher. But so would a 15th-Century play by an author who borrows from The Canterbury Tales in the course of his own work. We will operationalize this notion of relevance when we describe our experimental evaluation.
Though this use case is narrow, we believe that it is re-alistic. It speaks to the diachronic focus that has driven much earlier work on information access in scanned book repositories, [3, 27, 28].
To the best of our knowledge, the cross-temporal IR prob-lem has not been studied before. However, cognate problems have seen a good deal of attention. Much of this earlier work relates to this paper, both in terms of the problems we ap-proach and the ways we propose solving them.

Historical linguists have lent sophistication and rigor to the study of language change. Most notable in our context is Edward Sapir X  X  theories on  X  X anguage drift, X  the process by which a language changes over time to such an extent that earlier texts become unintelligible to contemporary readers [30]. However, although historical linguistics could surely help with the problem of CTIR, its focus on the analysis of linguistic genealogies is only indirectly useful here.
In domains closer to IR, work on document date identifi-cation relates to our study. In [11], [17], and [5], variations on language modeling have been brought to bear on the problem of identifying when a given document was written. As in these studies, our approaches to CTIR are based on language models, resting on the assumption that resolving differences among distant centuries X  vernaculars is tractable via a high-level estimation of word probabilities.
From a conceptual and practical standpoint, CTIR is sim-ilar to two active IR research areas: cross-language IR and IR over noisy text (typically due to OCR errors).

If we consider archaic English and Modern English to be two distinct languages, then the CTIR problem as we have presented it is a cross-lingual retrieval problem. Cer-tainly, CLIR methods as surveyed in [10] have a role to play in CTIR. For instance, reliance on dictionary-based and corpus-based translation tools enter into our analysis. Addi-tionally, the approach to structuring queries that we adopt is common in the CLIR literature (cf. [4]). Efforts to solve the vocabulary mismatch problem have brought CLIR methods into monolingual retrieval, as well. Work such as [2] and [12] considers query-document matching as an English-to-English translation process.

The CTIR problem also echoes challenges faced by re-trieval over modern OCR X  X  text. Of course, much of the research on IR for scanned documents is situated in the do-main of cultural heritage [6], as is our work. But the similar-ity is more substantive that this. Foundational work such as [8] presents n-gram methods for supporting search over de-graded texts. Additionally, variants of the n-gram methods that followed this work (particularly in the TREC confu-sion track) will form one of our experimental baselines in Section 8. Of particular interest is [18], where Lam-Adesina et al. note that many IR methods are quite robust against OCR-introduced noise; but relevance feedback is more brit-tle. Our feedback technique presented in Section 6.4.2 was developed in part due to this observation.

Finally, the task of CTIR is not constrained to search over digitized books. But these collections do make the problem of language drift particularly keen. A good deal of work in the INEX book track has treated OCR-related problems in book search (cf. [13] for an overview). However, the focus of INEX topics have left linguistic change as a relatively minor problem in most previous studies of book search.

Perhaps the most important fact about the literature on book search is that while digitized book data is plentiful, what people will want to do with these data is still largely unknown. We intend this paper to complement recent ef-forts to expand the scope of interaction design for large book repositories [14, 15, 16].
Whatever the nature of users X  information needs, repos-itories of digitized books present a novel challenge for IR systems: massive language drift. A corpus such as Google Books or the Hathi Trust contains texts that are nominally written in the same language (e.g. English), but whose vo-cabularies are nonetheless dissimilar due to long-term lan-guage change. The first line of the famous poem Sir Gawain and the Green Knight [32] reads: Sithen the sege and the assaut watz sesed at Troye , which translates into contempo-rary English as Since the siege and the assault was ceased at Troy. Seeing this equivalence is easy for scholars of Mid-dle English, but it is difficult for many English speakers, let alone for a computational system.

In this paper, our goal is to support a query issued in con-temporary English, retrieving documents that are relevant but that are written in the vernacular of earlier centuries. For convenience, we refer to the user X  X  language as modern English, with all old forms of English called archaic . This is a simplification in the sense that  X  X odern X  has technical im-plications in certain disciplines of the humanities. Likewise, our umbrella term  X  X rchaic X  refers to many eras such as Mid-dle English and early modern English. While operationally, we have divided English into two classes  X  modern and ar-chaic  X  the fluidity between these classes will be apparent later in our discussion.
Though not strictly necessary, in this paper we rely on the inference network retrieval model implemented in the Indri search engine [24, 31] . Given a query Q and a document D , the inference network allows us to compute P ( I| Q,D,  X ) where I is the information need that generated Q and  X  con-tains the parameters of the underlying joint distribution over indexing features. The inference network approach is help-ful here because it admits operations on structured queries. The belief operators in the Indri query language have been widely used for cross-language IR in a way that we adopt di-rectly. Consider the user-supplied modern query Q M , april showers . For cross-temporal IR, we might rewrite this as Q #combine(#wsyn(0.5 april 0.3 aprile 0.2 aprylle) #wsyn(0.7 showers 0.2 shours 0.1 rain)) where the wsyn operator treats word variants as weighted synonyms. Of course, finding proper synonyms and weights is hard; it is the task that comprises the contribution of this paper.
Given a modern query Q M = { m 1 ,m 2 ,...,m n } , we will build a structured query Q S that consists of n wsyn clauses such that the i th clause contains k archaic translations of the i th modern term. For retrieval we use the  X  X oolean and X  ( band ) belief operator to calculate where we have k translations for each of our n observed terms and w ij is the weight of the j th translation for the i observed term. We use Indri X  X  default parameters such that the probabilities are estimated using Dirichlet smoothing of multinomial language models with a hyperparameter of  X  = 2500. In other words  X  = {  X P ( t 1 | C ) , X P ( t 2 | C ) ,..., X P ( t where P ( t | C ) is the probability of term t in the collection and V is the size of the vocabulary.
Given the retrieval model given by Eq. 1, a core challenge of CTIR lies in finding archaic synonyms for each modern query term and assigning weights to them. To accomplish this, we pose the problem as a variation on cross-language retrieval, where modern and archaic English are considered to be two distinct languages. We assume that a modern query Q M is generated by an underlying information need I . Our goal is to rephrase Q M as a query Q S that will retrieve documents useful to I but expressed in a way that will retrieve documents written in archaic English 3 .
Of course, a search engine would retrieve both archaic and modern documents relevant to I . However, retrieving mod-We begin by assuming that an observed modern query Q
M is a translation of an unknown archaic query Q A . Our goal is to recover Q A from Q M . To simplify this process we further assume that an observed query word m i translates to 0 or more archaic terms { a i 1 ,a i 2 ,...,a ik } . In this paper we estimate the probability of each archaic translation of a modern term m i in isolation from all other observed terms m j 6 = m i , leaving non-independencies for future work.
For a modern term m i , we can rank the V terms in the vocabulary { a 1 ,a 2 ,...,a V } in decreasing order of P ( a | m Those archaic terms whose P ( a | m i ) exceeds some threshold  X  are added as a synonym for m i in the final query, with P ( a | m i ) as their weight in the wsyn clause.

For the modern query Q M = { m 1 ,m 2 ,...,m n } we choose elements for the wsyn query clause associated with term m by taking the top k terms from P ( a | m i ). This gives the clause: #wsyn( P ( a i 1 | m i ) a i 1 P ( a i 2 | m i ) a
Our final query has an analogous clause for each of the n observed modern query terms. For simplicity we assume that all observed terms (i.e. all wsyn clauses) are given equal weight in the final query.
All of the models we present are based on a simple prob-abilistic form, which we present here. Let T be a body of text such as a document, a corpus or a dictionary. We de-fine P ( a | m,T ), the probability that in a text T , the term a acts as a translation of m . By the definition of conditional probability, we have: We can think of P ( . | m,T ) as a multinomial characterized by  X  m , the probability that m is a translation of { a 1 ,a 2 for a vocabulary of size V . To estimate  X  m , the maximum likelihood estimator is: where | a i ,m,T | is the number of times that term a i acts as a translation of m in T .

The following sections will define several approaches to de-termining if an occurrence of a in T is acting as a translation of m . Thus, Eq. 2 is generic. To make it useful we need a method of calculating the strength of the translational rela-tionship between a and m . For any source of evidence E , we will define an indicator function  X  E ( a,m ) that evaluates to 1 if criteria specific to E are met by a and m , or 0 otherwise, allowing us to obtain the counts specified in Eq. 3. The precise way to define  X  will depend on the type of evidence we are using.
Bilingual dictionaries are a mainstay of cross-language IR [1, 20]. Researchers in the humanities have created similar resources that are helpful for cross-temporal IR. Of course many machine-readable dictionaries of Old and Middle En-glish are available 4 . But these are often small, with commen-tary that makes using them in automated settings difficult. ern versions is a problem that is at least conceptually distinct from our focus here.
However, one resource stands out in this field. The dig-ital humanities MorphAdorner project 5 contains a list of  X  archaic,modern  X  word pairs. After case-folding this  X  X ic-tionary X  contains 202,285 pairs. The MorphAdorner lexicon is helpful for CTIR due to its high quality. In personal corre-spondences, project developers explained that compiling the list took substantial work by subject experts. Because it was built by human experts there are very few false positives in the dictionary. Likewise, from its inception, the dictionary was intended to support automated systems. Thus using it requires no disambiguation or other data cleaning.
But like any bilingual dictionary, the MorphAdorner word list presents problems for use during CTIR: 1. Translation weights. Word pairs in the list are sim-2. Out of vocabulary terms. Even a list of 202,285 Despite these issues, the structure and high quality of the MorphAdorner dictionary makes a simple translation indi-cating function obvious. Given a dictionary D of archaic-modern term pairs:
Plugging Eq. 4 into Eq. 3 we have the estimator: where | m,D | is the number of tuples in the dictionary with m as their modern entry, and | a,m,D | is the number of these | m,D | tuples that have a as their archaic entry.
Eq. 5 allows us to specify a  X  X ictionary X  query Q D . For an n -term modern query Q M , Q D has  X   X  n wsyn clauses. The elements of each wsyn clause are simply the archaic terms with non-zero  X  P ml D ( a | m,D ), with all archaic terms weighted uniformly, in accordance with Eq. 5.
Cross-temporal IR invites us to supplement dictionary-based evidence with a second source of information. Un-like, say, English and French, where cognates are rare, word spellings in 20th-Century English and Early Modern or even Middle English are often similar. Most English readers are familiar with the addition of a terminal e when translating a modern term to an archaic version, as in april  X  aprile . It is likewise easy to recognize that aperil is a  X  X ognate X  for april . These similarities are common, and thus provide obvious leverage for translating modern terms into archaic terms. We refer to this as orthographic , or spelling evidence.
Orthographic evidence is attractive because it relieves the problem of out-of-dictionary terms. Spelling evidence might http://morphadorner.northwestern.edu/morphadorner/ also be useful in determining translation weights for struc-tured queries. However, several problems impinge on using orthographic evidence in cross-temporal IR.
Regardless of these difficulties, we hypothesize that if han-dled properly, orthographic modern-archaic word similarity should improve the quality of query translations. Our goal in this regard is to induce a model that gives P S ( a | m ), the spelling-related probability that archaic word a is a viable translation of modern word m . Intuitively, P S ( aprile | april ) should be large, as should P S ( avril | april ). But P S should be small, with P S ( man | april ) still smaller.
The appeal of using orthography to identify translations is that it frees us from our dictionary X  X  limitations. Instead of D , we will now use the entire text of the corpus C as T in Eq. 2. That is, we simply consider C to be a very large document. Referring to Eq. 3 we have the spelling probability: where the number of translation events between a and m is determined by a function  X  S ( a,m ) which we define in the following subsection.
To assess whether a and m are  X  X lose enough X  to be archaic-modern translations with respect to orthography, we need a measure of string similarity. A good deal of prior work has treated this problem. Much of this work focuses on variants of the edit distance. Given given two strings a and m , a set of permissible edit operations, and a cost function ` , the edit distance d ( a,m ) between a and m is the minimum cost of transforming a into m via our allowed operations 6 .
If all costs equal 1, then this definition gives the well-known Levenshtein distance. But the Levenshtein distance has two drawbacks. First, it has no ready probabilistic in-terpretation that would help us integrate it into CTIR. Also, in CTIR, some operations intuitively merit different costs.
Several probabilistically motivated edit distances that ad-dress these issues have been proposed in the literature. We rely on results by Oncina and Sebban [26] and Ristad and Yianilos [29]. In particular, we make use of the model de-veloped by Oncina and Sebban which finds the maximum likelihood estimator of the cost matrix via the EM algo-rithm 7 . Since we use this algorithm without alteration, we omit explicating it here, referring readers to [26, Appendix
We use the common edit operations, insert, delete and re-place.
Our results rely on the open-source SEDIL software, http://labh-curien.univ-st-etienne.fr/SEDiL/ . A] for a full treatment. For our purposes, two details of the algorithm are important. First, it operates without any domain knowledge aside from the training tuples. Second, based on a corpus of archaic-modern term tuples, we obtain a translation model t ( a | m ) by normalizing the results of the EM algorithm such that t ( . | m i ) sums to one for all m
We trained t ( a | m ) on the MorphAdorner dictionary. This leads to the spelling indicator function: for an empirically determined threshold  X  S which governs how similar a term a must be to m to be considered a viable translation for m .
The previous two subsections introduced sources of ev-idence that might help cross-temporal IR. But combining these sources would presumably improve effectiveness of CTIR, much as evidence combination has aided CLIR [25]. This section presents two methods of combining dictionary and spelling evidence in the framework given by Eq. 2.
It is tempting simply to assume that strong evidence on both dimensions  X  dictionary and spelling  X  should increase our confidence in a translation. We refer to this approach as naive combination .

We can combine dictionary and spelling evidence by us-ing the dictionary model P D ( a | m,D ) as the basis for a prior to find the maximum a posteriori (MAP) estimate of the spelling model. Because P S ( a | m,C ) is a multinomial, we use the Dirichlet distribution (the multinomial X  X  conjugate prior) with parameters {  X  S P D ( a 1 | m,D ) , X  S P D ( a  X 
P D ( a V | m,D ) } as our prior, with a smoothing hyperpa-rameter  X  S .

The maximum a posteriori estimate is thus: where P N ( a | m,C ) is the  X  X aive X  translation model.
Eq. 8 alleviates the constraints of dictionary-based trans-lation, but at the cost of a huge increase in the size of the search space. Because Eq. 8 operates in a completely un-supervised way over the entire vocabulary, it is likely that a query populated with all terms with non-zero probability according to Eq. 8 will contain many false positives.
Thus, we hypothesize that weak orthographic similarity is a strong indicator that a is a bad translation of m . But it is not the case that high orthographic similarity implies a good translation. If this is true, while Eq. 8 allows us to ignore many low-similarity terms, it will still suffer from a high false positive rate.

To overcome this problem, a logical approach is to con-strain the search space of translation discovery. Relevance feedback accomplishes this. Given R , a set of k (pseudo-) relevant documents obtained by the dictionary-based query Q
D , we will limit consideration of translation to terms that occur in R , aiming for two effects: 1. Term identification. The set of relevant documents 2. Term weighting. The dictionary gives deterministic The central idea in our feedback approach is that we only allow terms that appear in the pseudo-relevant documents to alter our query model based on orthographic similarity to observed modern query terms.

This approach uses intuition similar to He X  X  work on CLIR [9]. However, our approach is unique in several senses. Aside from the obvious difference in focus (CTIR vs. CLIR) and the attendant differences in evidence, our approach differs from He X  X  mathematically and algorithmically. He used feed-back to improve a translation model learned from parallel corpora, linearly interpolating model weights with dictio-nary probabilities. We have no parallel corpus. This invites a Bayesian approach, considering the dictionary model as a prior over translations which we use to smooth the proba-bilities estimated during feedback.

In the remainder of this section, we assume that based on our initial, dictionary-built query, we have retrieved k = 20 pseudo-relevant documents. We concatenate these k docu-ments into a large pseudo-document R .

Continuing with our earlier notation, we have the rele-vance feedback probability of a given m : Again, we enumerate the translation events between a and m in R via the indicator function: where t ( a | m ) is the orthographic probability (i.e. string sim-ilarity) and  X  R is a threshold that governs how similar a term a in the feedback documents must be to m to be considered a viable translation for m .

As in the case of our naive combination model, in this case we update our feedback probabilities by considering the dic-tionary model as the basis for a Dirichlet prior: {  X  R P D ( a 1 | m,D ) , X  R P D ( a 2 | m,D ) ,..., X  R P D ( a ing: We have presented four models for building CTIR queries. Table 1 summarizes them and lists an abbreviated name for each. A few points about these models are worth stressing: background, with more novel models shown against a white background. form weights.
 Judges cut-and-paste a relevant passage to create a query. Queries formed via sequential dependency model [23].
Relevance model 3 of [19]. Feedback terms interpolated with Q D (Eq. 5).
 ability t ( a | m ) &lt;  X  S .
 improve the spelling model based on dictionary evidence.
Considers all terms in the collection. Weights terms by dictionary evidence and collection frequency.

Same as CNAIVE but the set of possible translations is constrained to the vocabulary of k feedback documents.
Weights terms by dictionary evidence and frequency in feedback documents. To evaluate our approaches, we built a new test collection. We did not use a pre-existing collection such as the INEX book track X  X  corpora due two considerations: 1. We needed to ensure a high degree of historical diver-2. The types of information needs (and queries) needed This section details our test collection. Documents came from two sources. Members of the Google Books team (not affiliated with the authors) made the text of 3,690 volumes available to us. These were a random sam-ple of out-of-copyright texts by the authors listed on three Wikipedia pages in October 2010 8 . These samples had many redundancies, with duplicate and near-duplicate pages be-ing common; these were retained. Documents resulted from OCR on scanned pages, leading to typographical noise.
Besides the Google Books data, we harvested 34,806 full-text books from Project Gutenberg in October 2010. These books were selected by requesting all titles labeled as being in English, Middle English, or Old English. http://en.wikipedia.org/wiki/List_of_English_writers, http://en.wikipedia.org/wiki/Classical_Latin, http://en.wikipedia.org/wiki/Classical_Greek
For our experiments, we set the page (as opposed to the book, chapter, etc.) as the unit of retrieval. For the Google Books data, page-breaks were already present. The Guten-berg data, however, were simply long text files. We split these files into non-overlapping passages of no more than 300 words to form retrievable  X  X ocuments. X  The 300-word window was intended to mimic the length of book pages, though it was chosen without rigorous estimation.

All metadata was removed from documents, and texts were indexed using Indri, with no stemming or stoplists applied at index time. This yielded an index containing 25,845,101 documents and a vocabulary size of 6,657,631 terms. We did not perform any language identification to learn which documents were indeed modern and which were archaic; we preferred to let our models tackle this problem.
Topics were developed by two hired subject experts in digital humanities  X  one masters and one doctoral student. The students self-identified as users of Google books. As they created queries, topic developers explored the collection using a simple web interface to the index described above.
Each topic developer was given a page-long description of the proposed CTIR use case. Based on this, they were asked to imagine exemplars that they would like to learn about. Developers were free to choose exemplars that are very com-mon (such as famous Biblical passages) or rarer ones. The main imperative was for them to identify exemplars that would plausibly be of interest to digital humanists in the context of CTIR.

For each exemplar, the developer was asked to write a single version of it in modern English. These were used as queries during experimentation. The developers were in-structed to choose a modern phrasing that would be a plausi-ble starting point for a searcher interested in the exemplar. Finally, topic authors wrote a description of the exemplar that their query referred to as a basis for relevance judging.
This process raises at least two objectionable points. First, in some sense, our topic development was done backwards from a more realistic scenario. Generating modern rendi-tions of a known archaism is artificial. Second, with no  X  X or-rect X  way to write an archaic exemplar as a modern query, subjectivity entered the process of query creation. However, we argue that the first point  X  artificial order of operations in query-building  X  is tolerable; developers were asked to consider the overarching problem, and their expertise in the field put them in a good position to handle this ambiguity. As for the latitude in query expression, this is little differ-ent than any other topic development, where an abstract information need must be couched in a particular phrasing.
Developers created a total of 53 topics. Two topics were later found to have zero highly relevant documents and were removed. Five topics were chosen with uniform probability to be used for model training. The remaining 46 topics were used for testing.

Queries ranged in length from 3 to 23 words, with a me-dian of 6 and mean 7.65 terms. Figure 1 shows that the Mor-phAdorner dictionary provides good coverage of the queries, with only six queries having more than one out-of-dictionary term. Based on this, we believe that the simple dictionary-based query is likely to perform well for these data. Figure 1: Length of Test Queries and each Query X  X  Number of Out-of-dictionary Terms.
Unfortunately, the topic authors were not available to serve as relevance assessors. So four Ph.D. students in Me-dieval Studies were hired to perform relevance judgments. These students were all self-identified experts in early En-glish. Assessors were shown the same topic development guidelines that query authors had read. In addition, as-sessors were given instructions on how they were to judge document relevance. They then completed an initial trial of 25 judgments, after which they raised any questions before moving on to the bulk of their work.

Judging was done on a three-point scale: 0=not relevant, 1=somewhat relevant, 2=relevant. Assessors were also al-lowed to say  X  X  don X  X  know, X  though this option was never used. In Section 8, all effectiveness measures except NDCG treat judgments of 1 and 2 as relevant. NDCG leaves the three-point scale intact.

The criteria for relevance in the context of CTIR are not obvious. Judges were instructed to consult query-document pairs and ask does the document contain language that in-tentionally echoes the query ? By intentionally , we did not mean that the author needed to know his source. Instead, he needed to know that he was using language that has the history referenced by the query.

Novelty and document quality were not taken into account during the judging process.
 Pooling was conducted by running methods DICT, RM3, CNAIVE, and CFB described in Table 1 over the queries (with parameters chosen empirically) . These results were then pooled at a depth of 50 documents per query per model, yielding a total of 8,791 judgments.

Because our assessors were paid domain experts, we only assigned one assessor per judgment, yielding more judg-ments than multiple assessments per pair would allow. Nev-ertheless, we wanted some sense of inter-rater reliability. Thus we sampled 50 query-document pairs: 5 queries taken uniformly from our 53, with 10 documents sampled uni-formly from those documents that were in at least three of our pools. All assessors rated these query-document pairs. On the three-point relevance scale, this gave a Fleiss kappa agreement of 0.638. Conflating ratings of 1 and 2 to a single relevant class, increased kappa to 0.752. Both of these statis-tics gave p &lt; 0 . 001, suggesting that our assessors largely agreed on the criteria for assigning relevance judgments.
Because CTIR is largely an unstudied problem, defining a reasonable baseline for comparing approaches is non-trivial. We define three baselines, shown in the gray rows of Table 1. The run DICT is the simple dictionary lookup described in Eq. 5. The USER run is in some sense an oracle con-dition; relevance assessors were asked to cut and paste a single, highly relevant passage from a document to form the query verbatim. This text was then represented using the sequential dependency variant of the Markov Random Field model [22]. The USER run is thus not strictly compara-ble to the others we report because it is based on explicit feedback. But it isn X  X  a pure oracle condition since USER queries stem from a single relevant document and are not expanded in any way. USER queries are high-quality but narrow.
Due to space constraints, we do not report results from several models because they were unsuccessful; we list them here for completeness. As additional baselines, we used the character n-gram model described in [8]. We also imple-mented several ad hoc models based on soundex query trans-formation (cf. [33]). These methods did not approach the effectiveness of the simple DICT run, so we did not pursue them further. Of course, this is not a defect in the ap-proaches themselves. Rather it simply means that they did not transfer from their original domains to CTIR. Finally, standard Rocchio and BM25 pseudo-feedback were less suc-cessful than the reported relevance models.
Table 2 summarizes the outcomes of our experimental as-sessment. Results are based on retrievals of 100 documents per query. In the table, statistically significant changes via a permutation test are shown as follows. 4 : improvement over DICT has p &lt; 0 . 05, N : improvement over DICT has p &lt; 0 . 01,  X  : improvement over USER has p &lt; 0 . 05,  X  : im-provement over USER has p &lt; 0 . 01,  X  : decline wrt USER has p &lt; 0 . 05,  X  : decline wrt USER has p &lt; 0 . 01. Table 2: Summary of Retrieval Effectiveness. Statis-tics are mean average precision (MAP), num-ber of relevant retrieved (Rel Ret), R-precision (Rprec) and normalized discounted cumulative gain (NDCG).

A few results are unsurprising: The dictionary-based queries (DICT) do seem like a reasonable baseline. The condition with known relevant text as queries (USER) performs very strongly. And in the bottom three rows, adding increasing amounts of structure to our dictionary-orthography combi-nations improves effectiveness.

The most obvious point of interest in Table 2 is the strong performance by our orthographically informed feedback method, CFB. Except for the semi-oracle USER run X  X  NDCG, CFB outperforms all methods on all effectiveness measures.
By comparing CFB to DICT and RM3 we can see another interesting result. Pseudo-relevance feedback helps when it is used to alter a query by combining feedback and ortho-graphic evidence via CFB. In contrast, standard feedback did not improve over the simple dictionary method. It is important to note that our implementation of RM3 inter-polated the feedback model with the dictionary model; i.e. RM3 it did not give up the query structure of the other mod-els. Also, the RM3 run used a custom stoplist with words added to the standard Indri stoplist to improve performance.
Figure 2 helps us assess the relationship between the two highest-performing runs  X  USER and CFB. In Figure 2 we see that both methods give higher MAP than the DICT run on most queries. However, CFB is  X  X afer X  than the USER run insofar as CFB X  X  queries whose effectiveness decline with respect to DICT do so less than comparable declines seen for USER. This is not surprising, as the USER run puts all of the query burden on the words obtained from a single relevant document. Thus USER is very good at retrieving some relevant documents. But it does this at the cost of overfitting the supplied relevant text. Not surprisingly, if we remove the documents from which the judges extracted the query text for USER, its MAP declines to 0.2096.
Though CFB performed well, Table 2 shows that using spelling evidence is risky. The purely orthographic SPELL model was significantly inferior to the simple dictionary lookup. And CNAIVE saw a small, though not significant decline as well. This suggests that there is information in orthographic information, but that using it well is difficult.
 Figure 2: Query -by-Query Difference in MAP between
The CFB feedback method aims to improve a simple DICT query in two ways: by discovering candidate translations that were not in the dictionary and by learning weights for query terms. The formalism given in Section 6.4.2 dictates how these goals are combined in CFB. But a logical ques-tion is: which aspect  X  term discovery or term weighting  X  is most responsible for the improvements over DICT seen in Table 2? To pursue this question, we created two variants of CFB that isolate these effects (at the expensive of becoming heuristic in motivation). CFB-W reweights terms from the dictionary query as Eq. 11 dictates but does not add any query terms. Conversely, CFB-T adds all feedback terms to the model that constitute  X  X ranslations X  according to Eq. 10. But in CFB-T, all terms from the dictionary receive a weight of 1.0, while added terms are weighted at 0.5. Table 3: Retrieval Effectiveness Measures for vari-ants of CFB Feedback Method. Statistically signif-icant changes via a permutation test are shown as follows. H : decline wrt CFB has p &lt; 0 . 01 .

Table 3 suggests that the crucial mechanism for CFB is its ability to discover new terms. If we disable this behavior (yielding CFB-W) performance declines significantly. But if we ignore the induced term weights of CFB (yielding CFB-T) we actually see an increase in effectiveness.
Our experiments suggest that methods for handling lan-guage change have a strong effect on the success of the types of queries studied in this paper. Without a canonical base-line, it is difficult to say if our methods are performing  X  X ell. X  However, the MorphAdorner dictionary is an unusually large and clean knowledge base by CLIR standards. Thus we ar-gue that the DICT model gives a reasonable baseline.
To improve on this performance, it is logical to enlist or-thographic evidence, since it often provides a clear path from a modern term to its historical variants. But our hypothesis that orthographic evidence can only help CTIR in a limited way was borne out experimentally. If a term a is ortho-graphically similar to m , that does not imply that a is a good substitute for m in a CTIR query. But it is also the case that if we fail to see orthographic similarity between a and m , it is unlikely that we have a match that we should trust without further corroboration.

We anticipated a benefit from using feedback to moderate string similarity X  X  influence during retrieval. The strength of the CFB model bears out this hypothesis. By constrain-ing the domain of possible orthographic matches to those in high-quality documents, we avoid the high false posi-tive problem that the purely string-based approach such as SPELLING encountered.

Several limitations of this study are worth noting. We omitted discussion of the sensitivity of the parameters  X  and  X   X  because sweeps on our training queries showed lit-tle interesting change. But the poor performance of the CNAIVE model suggests that closer attention to parame-terization would be helpful. More importantly, using only five training queries limited our ability to assess parameter-ization robustly.

Another limitation hinges on temporal diversity. Between  X  X rchaic X  and  X  X odern X  English there is a spectrum of ver-naculars. Effective CTIR should retrieve documents across this spectrum. But without a clear metric for assessing such diversity, we have not reported our success in finding docu-ments from diverse periods.

Finally, a fourth type of evidence could be used for CTIR: translation models learned from parallel corpora. We built such a model using Biblical translations, but found that the high number of out-of-vocabulary query terms made its use infeasible. Perhaps an approach based on the less restrictive notion of comparable corpora will allow us to incorporate such evidence. In future work we plan to pursue this. Two other avenues will inform our future work on CTIR. First, Table 3 suggests that our feedback model performs well but not optimally. Though probabilistically convenient, Eq. 11 is not an optimal way to combine dictionary, spelling and feedback information. In future work we will pursue how to exploit the strengths of CFB more fully.

Our second avenue for future work lies in expanding the domains in which we study cross-temporal IR. This paper has focused on book search. But techniques for handling language change have a role to play in other types of IR. In particular, social media such as microblogs see rapid shifts in discursive conventions [7]. While the temporal dynamics of relevance (e.g. [21]) have seen a good deal of attention recently, the problem of temporality as an invitation for vo-cabulary mismatch deserves increased scrutiny.
The growing opportunity for digitized book repositories to impact peoples X  use of information suggests that lan-guage evolution will be an increasingly important challenge for modern IR. Without prompting, all four of the Medieval Studies Ph.D. candidates who performed our relevance judg-ments said that they wished that a system capable of cross-temporal search were available to them.

In light of this change, this paper proposed techniques for handling vocabulary mismatch due to temporal shifts in language. From a conceptual standpoint, our contribution entails a generic way of considering the use of texts (dictio-naries, corpora, relevant documents) to inform CTIR. More pragmatically, the paper X  X  main contribution is a novel feed-back technique that combines several types of evidence to improve cross-temporal retrieval. This work was supported by a Google Digital Humanities Award and NSF Grant # 1217279. Any opinions, findings, and conclusions or recommendations expressed in this ma-terial are those of the author and do not necessarily reflect the views of the funders. The author thanks Jon Orwant for his help in the early stages of this research. [1] Lisa Ballesteros and W. Bruce Croft. Dictionary [2] Adam Berger and John Lafferty. Information retrieval [3] Dan Cohen. Is google good for history?, January 7, [4] Kareem Darwish and Douglas W. Oard. Probabilistic [5] F.M.G. de Jong, H. Rode, and D. Hiemstra. Temporal [6] Michael Droettboom. Correcting broken characters in [7] Miles Efron. Information search and retrieval in [8] S. Harding, W. Croft, and C. Weir. Probabilistic [9] Daqing He and Dan Wu. Enhancing query translation [10] Daniel Jurafsky and James H. Martin. Speech and [11] Nattiya Kanhabua and Kjetil N X rv  X ag. Improving [12] Maryam Karimzadehgan and ChengXiang Zhai.
 [13] Gabriella Kazai and Antoine Doucet. Overview of the [14] Gabriella Kazai, Carsten Eickhoff, and Peter [15] Marijn Koolen, Jaap Kamps, and Gabriella Kazai. [16] Marijn Koolen, Gabriella Kazai, Jaap Kamps, Michael [17] Abhimanu Kumar, Matthew Lease, and Jason [18] Adenike M. Lam-Adesina and Gareth J. F. Jones. [19] Victor Lavrenko and W. Bruce Croft. Relevance based [20] Gina-Anne Levow, Douglas W. Oard, and Philip [21] Xiaoyan Li and W. Bruce Croft. Time-based language [22] Donald Metzler and W. Bruce Croft. A markov [23] Donald Metzler and W. Bruce Croft. Latent concept [24] Donald Metzler, Victor Lavrenko, and W. Bruce [25] Jian-Yun Nie, Michel Simard, Pierre Isabelle, and [26] J. Oncina and M. Sebban. Learning stochastic edit [27] Marc Parry. The humanities go google. The Chronicle [28] Matjaz Perc. Evolution of the most common English [29] Eric Sven Ristad and Peter N. Yianilos. Learning [30] Edward Sapir. Language: And Introduction to the [31] Trevor Strohman, Donald Metzler, Howard Turtle, [32] J. R. R. Tolkien and E. V. Gordon, editors. Sir [33] Justin Zobel and Philip Dart. Phonetic string
