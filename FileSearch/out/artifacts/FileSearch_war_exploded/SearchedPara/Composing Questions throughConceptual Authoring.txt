 The Open University The Open University are risk-averse. We present a proof-of-concept system we have developed: a question-answering allows users to successfully and reliably compose complex queries with minimal training. 1. Introduction
Where early attempts to build natural language question-answering systems focused on accessing and presenting information held in (closed domain) databases (e.g., Hendrix et al. 1978; Templeton and Burger 1983; Kaplan 1984; Hafner and Godden 1985), the of texts. However, despite significant advances in open domain question answering since the simple pattern-matching systems of the first current systems are still largely restricted to simple questions. They can, for example, successfully find answers to questions like Which is the highest peak in Africa? or Who first climbed Kilimanjaro? but they cannot correctly answer more complex questions like: although the first question is very simple to interpret, a correct answer is unlikely to be
A question-answering system would thus have to first retrieve the heights of each of the top twelve highest peaks, probably from different documents, and apply some calculations to obtain their median height, and then generate a response that aggregates answers from multiple documents. The answer to the second question, on the other resolving the temporal information, correctly assuming that 55 refers to age at the time of death, and interpreting the negation but not as referring to the climbing of Everest only within the specified time span. For the third question, the difficulty comes from a combination of complex question and complex answer. Retrieving aggregated results from the World Wide Web also introduces issues of reliability because the sources may not all be trusted, and there is no guarantee that a different selection of sources would not yield a contrary result.
 and trusted answers is paramount X  X or example, in the medical, legal, and financial domains, or indeed in any research area X  X nd it is to this scenario that the work we present here applies. Our goal is to develop a general and intuitive method by which users can pose complex queries to data repositories; we are particularly concerned with scenarios where the users are domain experts (i.e., clinicians, lawyers, financiers, the method of posing questions should be easy to learn, and where the questions themselves should be transparent (i.e., clear and unambiguous) to both user and system.
 languages such as SQL . These languages are highly technical and require a great deal queries shown in the previous example. Successful query composition requires the user to be proficient in the query language and have detailed knowledge of the structure of the database to which the queries are being addressed. Users also need to be classifications, laws, bank codes). For example, in the medical domain alone there are a large number of clinical terminologies and classifications, used for different purposes:
Some classifications, such as ICD -9, ICD -10, and OPCS -4, are employed in summarizing such as CPT 4or ICD -9 CM , manage the process of billing patients. Each covers a large number of terms and associated codes: SNOMED -CT alone, to name the most widely used medical terminology, currently contains some 365,000 individual concepts, and is being updated continuously (College of American Pathologists 2004). Finally, because database languages are not transparent, mistakes in query formulation can be difficult to spot; so even where the system itself may be highly reliable, there is a reasonable chance that X  X xcept for very highly experienced database programmers X  X he returned answer may not be an accurate response to the intended question.
 systems, which make use of graphical devices such as forms, diagrams, menus, and pointers to communicate the content of a database to the user. They are also widely used in commercial applications, and research shows that they are much preferred over textual query languages like SQL , especially by casual and non-expert users (Capindale 106 and Crawford 1990; Bell and Rowe 1992; Catarci and Santucci 1995). However, visual experts using visual object-oriented modeling tools (Kim 1990), and a clear advantage of text over graphics for understanding nested conditional structures (Petre 1995). questions, but this is also highly problematic because queries expressed in free natural language are obviously very sensitive to errors of composition (e.g., misspellings, ungrammaticalities) or processing (at the lexical, syntactic, or semantic level). 2. Natural Language Interfaces In a typical natural language interface to a database (henceforth requests database records through a query expressed in natural language. The ques-tion is first parsed and analyzed semantically by a linguistic front-end, which trans-lates it into an intermediate meaning representation language (typically, some form of logic). The intermediate language expression is then translated into a database language (usually SQL ) that is supported by the underlying database management system.
 wide range of techniques. The general drawback of these systems understand only a subset of natural language. Casual users cannot always discern which constructions are valid or whether the lack of response from the system is due to the unavailability of an answer or to an unaccepted input construction. On the positive side, natural language is far more expressive than formal database query languages such as SQL , so it is generally easier to ask complex questions using natural language (NL) than a database language (a single natural language query will have to be trans-lated into multiple SQL statements). Natural language queries are not only more user-friendly for the non-expert user, but they also allow easier manipulation of temporal constructions.
 In order to recover from errors in any of these steps, most advanced incorporate some sort of cooperative user feedback module that will inform users when the system cannot construct their query, and ask for clarification. 2.1 Our Solution: A Quasi-NL Interface The solution that we propose partially overlaps with previous research in into the database query language. The difference lies in the nature of the NL interface, which in our case uses a method that we call Conceptual Authoring ; this replaces the traditional method of free text entry followed by automatic interpretation. representation, governed by a predefined ontology. Instead of typing in text, the user second key idea (captured by  X  X uthoring X ) is that the user interface presents the transparent to users X  X amely, natural language text, possibly supplemented by other familiar media; users therefore feel that they are performing a familiar activity, a kind of guided writing, rather than an unfamiliar activity akin to programming.
 edge encoding is edited by direct manipulation of a familiar presentation, the presen-tation being generated automatically from the underlying knowledge encoding, and updated every time knowledge is added (or removed) through an editing operation.
The user need not be aware of the underlying formalism any more than a person using a text editor need be aware of ASCII codes. Conceptual Authoring therefore depends entirely on language generation technology; it does not use language interpretation at all.
 nature of the underlying knowledge and the presentational medium. In the query editor described in this article, the underlying knowledge is a set of assertions (i.e., an
A-box), and the presentational medium is natural language text. Elsewhere, we have used the term WYSIWYM (What You See Is What You Meant) for various systems of this kind that we have developed (Power and Scott 1998): as well as query interfaces they include programs that generate technical documentation in multiple languages. We use  X  X onceptual Authoring X  as a more general term that would also cover applications in which the underlying knowledge included conceptual definitions and rules as well as assertions, and the presentation medium included diagrams as well as text X  X rovided, of course, that the diagrams were familiar to the relevant subject-matter experts (e.g., a molecular structure diagram if the user were an organic chemist).
 text is generated in order to present successive states of the underlying logical representation. This text includes generic phrases, called  X  X lace-holders X , which mark attributes that currently have no value. Place-holders serve as the locations where new objects may be added. By opening a pop-up menu on a place-holder, the user obtains a list of short (generated) phrases describing the types of objects that are permissible representation, including the attributes of the new object. As more information is added 108 about an object, it will be presented by longer spans of text, comprising sentences or associated semantic material can be cut or copied. The cutting operation removes the logical fragment that was previously the value of an attribute, and stores it in a buffer, where it remains available for pasting into another suitable location. The text associated with the fragment may or may not remain the same, depending on the context of the new location.
 naturally be expressed by the sentence The doctor examined the patient with a stethoscope .
The underlying logical structure could be an event object of type examined ,with attributes for actor , actee ,and instrument ; this will of course be only one among many events allowed by the ontology. To define this content using a Conceptual
Authoring interface, the user begins from a text containing a place-holder for any kind of event. By clicking on this place-holder, the user obtains a list of event patterns, each shown as a short phrase corresponding to a specific event type from the ontology:
FEEDBACK TEXT [Some event]
MENU OF OPTIONS ..... consulted examined treated visited .....

When the user selects examined from this list, an event object of type examined is added to the underlying semantic model, and the feedback text is regenerated to express the new event and its attributes (as yet unspecified), which are shown by short phrases in square brackets (the place-holders). A color code on place-holders indicates whether an attribute is obligatory (it must be specified) or optional (it can be left unspecified); here for convenience we use boldface for obligatory, and italics for optional. By clicking corresponding attribute (in this case the actor ).

FEEDBACK TEXT [Some person] examined [some person] [in some way]
MENU OF OPTIONS ..... doctor nurse patient .....
By making successive choices in this way, the user will complete the desired proposi-tion, perhaps through the following sequence: design feedback texts in a way that minimizes ambiguity. We might, for instance, prefer to avoid the more natural phrase  X  X ith a stethoscope X , which introduces the well-known
PP-attachment ambiguity, in favor of the slightly clumsy but unambiguous alternative employed herein.
 sentence reached in the last example, the user could select the span  X  X he patient X  and choose Cut , thus emptying the slot and reinstalling the place-holder: Having freed up the slot in this way, the user might next select  X  X he doctor X , choose
Copy , select the place-holder  X  X some person] X , and choose Paste .The Copy operation here applies to the actual instance selected: It does not create a new instance of the same type. Therefore, after the Paste operation, the doctor instance fills two slots, both the actor and the actee of the event. The resulting coreference is shown by the wording of the feedback text: knowledge content for multilingual generation of instruction manuals (Power and Scott 1998; Power, Scott, and Evans 1998; Scott, Power, and Evans 1998) and pharmaceutical leaflets (Bouayad-Agha et al. 2002). It has also been applied in a tool for posing queries to a knowledge base of legal and regulatory information about maritime shipping resembles early menu-based techniques like Tennant, Ross, and Thompson (1983) and
Mueckstein (1985); however, this resemblance is only superficial, because in these techniques the user edits a linguistic structure, whereas in Conceptual Authoring all editing operations are defined on an underlying logical structure. 3. A Test Application: Electronic Health Records
Typically, an individual X  X  medical record is a collection of documents held in his or her 110 hospitals or clinics they have attended, or specialists they have seen. These records are primarily textual, and the record of the average hospital patient will consist of a large number of documents X  X round 100 narratives, plus hundreds of items of structured data derived from laboratory, pharmacy, or other hospital subsystems. There is a significant move X  X ot just by medical providers, but by governments (e.g., the National
Programme for Information Technology in Medicine [NPfIT] in the UK, and various e-Health initiatives in other countries) X  X o replace or supplement the current form of patient records with electronic records; these are intended to be not simply electronic accessible at the point of care. One of the disadvantages of text-only health records is that the information contained within them, because it is  X  X ocked into X  the text, is not available for statistical manipulation and cannot be easily interrogated.
 analysis, show that free text queries written by medical professionals are mostly complex and often highly ambiguous. From this we conclude that when querying medical databases, such users need to be able to construct queries that are complex, both in volume of material and in the organization of this material (e.g., into temporal or conditional constructions). Traditionally, user interfaces to medical databases have been complex visual interfaces that are unsuitable for use by a casual user (Nadkarni and Brandt 1998; Shahar and Cheng 1999).
 answering systems but, for obvious reasons, the veracity of the results of any query is critical, making it doubly important that queries put to the system, and their resulting responses, are unambiguous and clearly understandable to the user. Because the users will be medical professionals, with great demands on their time, the ease of use of the question-answering system is also extremely important. We have applied our Conceptual Authoring question-answering method to one such application: the Clinical
E-Science Framework ( CLEF ). 3.1 The Clinical E-Science Framework
The Clinical E-Science Framework ( CLEF ) aims at providing a repository of well-organized clinical histories that can be queried and summarized both for biomedical research and clinical care (Rector et al. 2003). In this context, the purpose of the query interface is to provide efficient access to aggregated data for performing a variety of tasks: assisting in diagnosis or treatment, identifying patterns in treatment, selecting subjects for clinical trials, and monitoring the participants in clinical trials. Although currently being applied to cancer, in collaboration with the Royal Marsden Hospital in London, one of the primary centers for the treatment of cancer in Britain.
 a total of more than 400,000 database entries, some 3.5 million record components patient records modeled on an archetype for cancer developed by Kalra et al. (2001).
The information on each patient comes from hundreds of documents, and a single care episode or clinical problem is likely to be mentioned repeatedly in several documents. entry representing an instance of the cancer archetype. 3.2 Users and Extent
The CLEF query system is designed to answer questions relating to patterns in medical attribute-centric queries asking for aggregated results, such as: database, because they do not require inferences over the repository of patient records.
However, the query interface is also coupled with a data-mining module to provide answers to more complex queries, such as
The query interface can also be used for accessing information about individual patients.
 semantic domain of the repository (but not with its actual structure or encoding) and
Under this description come three primary types of users, each having a different goal in interrogating the repository: languages (e.g., SQL ) to be the main beneficiaries of the query interface, although in the
Evaluation (Section 6) we will show that even for SQL -aware users, the query interface represents an improved alternative to standard SQL . We also target the interface at users who are unfamiliar with medical encoding schemes, such as prefer to use natural language expressions instead of medical codes. 3.3 Previous Work on Querying Clinical Databases
There are a number of query systems for clinical databases, mostly designed for formulating patient-centric queries and typically using visual interfaces. For example: 112
The Columbia-Presbyterian Medical Center Query Builder works off the medical
Tr i a l DB (formerly ACT/DB) is a clinical study data management system that provides
KNAVE is a visualization and navigation model that enables clinicians to query a specific restricted tasks. For example, the HERMES system (Rivera and Cercone 1998) allows demographic information, patients X  personal details, visit information, and insurance coverage information. It is designed as an aid to hospital administration, and not to clinical care. 4. The CLEF Query Interface
The CLEF Query Tool has four components which are invoked in sequence whenever a query is posed by the user (Figure 1). The first is the Query Editor , a natural language this component is a logical representation underlying the query text that the user has created. The second component, the Query Transcoder , converts this logical represen-tation to a Java encoding accepted by the CLEF database management system ( In this form, the query is sent to the DBMS , which recodes it again into submits it to the database. The result of the query, usually a list of records for relevant patients, returns to the third component of the Query Tool, the Result Processor , which transforms the raw data into an aggregated representation defining the content of the answer. This representation then passes to the fourth and final component, the Answer
Renderer , which configures a convenient display for the user by combining fluent text with diagrams (tables and charts).
 4.1 Query Editor
The Query Editor allows the user to create a logical representation of the query by means of Conceptual Authoring. When beginning a new query, the user is shown a minimally specified feedback text based on a model of query structure in this domain; this model is described in Section 5. By inserting content in the initial place-holders, the user can minutes once the user has become accustomed to the editing process (for details, see Section 6). A query is potentially complete when all obligatory slots have been filled. This is easy for the user to verify because obligatory place-holders are shown in red:
Submit button, whereupon the current A-box is passed to the Query Transcoder. 4.2 Query Transcoder
The Query Transcoder takes as input an A-box from the Query Editor, and recodes it in the format expected by the DBMS . This conversion depends on a mapping between the ontology (or T-box) employed by the Query Editor, and the concepts of the database archetype. The T-box cannot be exactly the same as the archetype, because it has to serve a different purpose X  X hat of providing logical representations suitable for generating linguistic structures like clauses and nominals. 4.3 Result Processor
The Result Processor receives the data returned by the DBMS for relevant patients, and constructs the logical representation of an answer for the user. requirements of the query, and specify, for each patient, the features along with values for each of the query elements. For example, the query may yield the result set shown in Figure 2.
 patients are grouped according to the age/gender breakdown and the individual query terms. For each query term, the data are split into a dynamically determined number of age groups, and for each age group the patients are further subdivided by gender. 114 4.4 Answer Renderer
The data thus organized are presented to the user in three types of formats: tables, charts and text. Each individual chart is accompanied by an automatically generated caption that explains its content.
 by the same data that were used for generating the chart. For example, the results we
This is accompanied by a textual explanation in the form of a caption, a fragment of which reads: 5. Query Model
A controlled editing environment is most effective when based on a model of the kinds of queries that users will wish to make. There is a trade-off here between flexibility and ease of use. If we have no preconceptions about the general nature of queries, we have to provide users with a wide set of possible patterns, leaving them to search for the particular pattern they happen to want. If instead we can assume that the query will belong to a known set of patterns, the editor can help the user to get started by offering a manageable list of alternatives, so avoiding the  X  X lank page X  problem.
 generally ask for evidence-based advice for treatment decisions (Ely et al. 2000). For example, of 64 generic question types, the three most common are:
In contrast to these findings, our consultation with cancer clinicians revealed that and to be directed at groups of patients, searching for relationships rather than simple values: 116 guidance of the relevant experts X  X linicians and medical researchers in the area of cancer. 5.1 Elements of a Basic Query
The structure of a typical query (according to our experts) is shown by the following relatively simple example:
As can be seen, this query breaks down into three elements: the set of relevant patients, defined by a problem ; the partition of this set according to treatment ; and the further partition according to outcome , from which the percentage can be calculated. For maximum clarity, the Query Editor can format the query so that these three elements are marked explicitly and presented separately: Generalizing from this example, we can identify the following basic query pattern: be unambiguous X  X amely, that users should understand correctly how the outcome measure will be calculated. We assume that the calculation will proceed through the following steps. First, retrieve all the patients in the database who satisfy the conditions in the first two paragraphs (Relevant subjects and Treatment profile) X  X n this example, all patients with cancer of the pancreas who received a course of gemcitabine. Call this the subset of S also satisfying the outcome condition X  X n this example, the patients still C ( S ) and express the result as a percentage.
 which requests a comparison rather than a single value: Again this can be presented to the user using a separate paragraph for each element:
For a comparison question we need to compute two outcome measures, so the steps in the calculation have to be elaborated as follows. First, retrieve two sets of patients, S and S 2 , satisfying the conditions that we want to compare. In the example, S the set of patients with cancer of the pancreas who had a course of gemcitabine; S be the set of patients with the same type of cancer but no gemcitabine treatment. for each set, find the subset of patients still alive after five years: Call these subsets M and M 2 . Finally, compute the measures to be compared by dividing C ( M
C ( M 2 )by C ( S 2 ), and expressing the resulting ratios as percentages. 5.2 Complex Queries
Each element of a query can be made more complex in two ways. First, it can be replaced by a conjunction or disjunction, so that the query in a sense becomes several queries requiring several answers. Second, the content of the description can be elaborated, for example by adding more qualifications. Here is an example of the first kind:
This can be analyzed as a single relevance group, single treatment, and multiple outcome measures (survival at 1, 2, and 5 years). Separate answers for these measures will be needed. An example of the second kind is the following:
We assume this is a single rather than a multiple query, and that separate answers are not needed for the various conjunctions and disjunctions. The treatment profile (Taxol) and the outcome measure (survival rate) have a content that can be easily specified X  X  single choice from a menu would suffice. However, the set of relevant patients requires a very elaborate description because there are so many qualifications. 5.3 Multiple Relevance Sets
When the phrase describing a relevance set includes a conjunction or disjunction, there may be ambiguity over whether the intended query is single or multiple. Compare these three patterns: altogether while wording the query in a way that is reasonably natural, but at least we 118 example, bulleted lists for conjunctions (or disjunctions) that imply multiple queries, and discourse connectives ( and , or ) for ones that imply single queries. For example: versus
In the first we have two relevance sets; in the second we have only one. 5.4 Multiple Treatment Profiles
A similar ambiguity is found when several treatment profiles are mentioned. Either there are several queries, or there is a single query concerning a logical combination of the treatments. The following query text (written as an example by a medical researcher) could be interpreted either way: with allogeneic ones. Alternatively, it might not matter whether the transplant is autologous or allogeneic provided that it is one or the other, as suggested by the singular verb ( X  X hat is the survival X ). In the query interface, the ambiguity can be avoided in the same way as before, by using bullets to mark separate queries. For example: versus
In the first we have two treatment profiles and hence separate queries; in the second we have only one. 5.5 Multiple Outcome Measures two years, and five years. It makes no sense to combine these into a single query, so they are always interpreted as separate queries. 5.6 Comparison Queries
Comparison queries are those that ask for certain outcomes of separate groups of patients that do not share common diagnosis or treatment profiles.

We represent such queries as two independent queries, with separate profiles. 5.7 Elaborate Descriptions
Descriptions are boolean combinations of properties. A description can be elaborate either because it contains many boolean operators, or because the properties are both ways: prose without the scopes of the boolean operators becoming unclear. To avoid this problem, the feedback text generator formats complex boolean combinations using hierarchical layout: excerpt has two treatment profiles, the first using a combination of second using AND combined with temporal sequence (marked by 120 The corresponding part of the feedback text is laid out as follows: 5.8 Representing Time
Queries about patient records contain many references to events occurring at particular times: diagnoses, tests, treatments, deaths, and so forth. These time specifications are
First, the feedback text should express temporal relations naturally and unambiguously, using familiar devices like tenses and adverbials. Second, the resulting conceptual represented in the database.
 example, if a mastectomy was performed on 29th January 2000, the valid time would be some representation of this day, perhaps  X 29-01-2000 X , assuming a granularity calibrated moment when it was written down. Obviously this might differ from the valid time, concept of query time , the moment when a query was formulated by the this is needed in order to interpret deictic time references in the feedback text, based for example on tenses or on phrases like  X  X fter 1995 X , which can be interpreted to mean  X  X rom 1995 until now X .
 date. For events that last for longer intervals, like a whole course of treatment, two valid time stamps have to be given, one for the start time and one for the end time. Of course this distinction is related to granularity. With a granularity based on days, a week has to be treated as an interval with a start date and an end date; with a granularity based on weeks, the same week could be identified by a single time stamp (e.g.,  X  X 40-2000 X , meaning the 40th week of the year 2000).
 clear options, and map them to the time stamps used in the database. At present, the temporal modifiers offered during query editing are as follows: before 1999 ,or patients who received chemotherapy within 5 months of surgery . The interface allows Allen X  X  13 basic interval relationships to be expressed in natural language (Allen 1984).
 mentioned in a query; however, by imposing this further requirement on users we would pay a high price in usability, virtually doubling the number of operations needed in order to complete the query, and damaging the transparency of the resulting text.
In the CLEF query interface we have decided instead to associate default values to time descriptions and to make the time stamp anchors visible only on demand in the feedback text. The output text will contain all the time stamps, with the values either entered by the user or defaulted, so allowing the user to review the query and amend it where necessary. 6. Evaluation
The best evaluation of any question-answering system is one which looks at real users making information-seeking requests in real-life contexts. Because the complete system is not yet ready for deployment, this is impractical at this stage. However, we have been able to perform usability tests on the query interface in isolation from the the Query to SQL Translation and the Answer Retrieval components, which are part of the server components side of the query interface. This separation is not always possible in practice. For example, we cannot at this stage test the full range of queries that can be constructed in the interface, because some are not yet supported by the back-end.
Similarly, we can only assess the time necessary for editing queries, not for retrieving answers, because this is almost entirely dependent on the communication procedure and on the speed of the SQL translator.
 questions: 6.1 Experiment 1: Query Composition
As mentioned earlier (Section 1), one of the main desiderata behind the design of our implemented for CLEF , what this means is that medics and bio-informaticians should repository. This experiment tests the extent to which our querying method fulfills these requirements. 122
Subjects. Fifteen medics and bio-informaticians participated in the experiment. All had previously been granted clearance 6 to see the information in the confidential repository of patient records. All subjects were knowledgeable in the domain of cancer, and all but two had no knowledge of the representation language of the repository ( of how the data contained therein were structured; none had any prior experience with the query-formulation interface.

Methodology. Each subject was given a short (5 X 10 minute) introduction to the interface, which included a demonstration of the construction of a fairly simple query. Subjects interface. To increase the difficulty of the task, the questions presented to the subjects avoided, where possible, the wording required by the user interface, so that users were obliged to think about the meaning rather than to aim for particular target phrases. To avoid effects of practice, we varied randomly the order in which the questions in the set were presented to subjects. Subjects were allowed as much time as they needed to compose each query.
 number of operations used for constructing it.
 Materials. The materials for the experiment consisted of the following set of four queries:
These are representative of the query types that emerged from an earlier requirements analysis with oncologists and cancer bio-informaticians. They also vary in their levels of structural complexity and in the number of interface operations required to successfully complete them.
 posed to search engines or to most other interactive query engines (as described, for example in [Hovy, Hermjakob, and Ravichandran 2002]; [Soricut and Brill 2004]; [TREC 2005]).
 3.9 minutes (noting that subjects were under no time pressure to complete the individual queries). 7 Figure 4, which gives the average time to completion across all subjects, shows that subjects learned to use the interface quickly: they take much longer on their first
This effect is confirmed by an analysis of variance (ANOVA) differences were found between subjects X  performance on the first query they composed test). However, application of the same test showed no significant difference in subjects X  performance on the second versus third, second versus fourth, or the third versus fourth composed query.
 perform more interface actions than others, and so one would predict a difference in subjects X  performance (i.e., time to completion) on the individual queries; this was borne out by the analysis (ANOVA, F = 5.5015; p &lt; .0028).
 interface will increase fairly quickly as they move from the first query they encounter mean that subjects perform twice as many operations as are required; a value of 0 would emerges from this is one where, overall, subjects are very efficient, achieving an average score of 0.19 over their first four encounters with the method. They make a fair number the time they get to their second query, and near perfect by the time they get to the fourth. Analysis of variance 9 shows a highly significant effect of order of presentation (F = 7.4993; p &lt; .0004). Once again, the Tukey HSD Test shows a significant difference between the first query encountered and each of the subsequent ones (p &lt; . 01), and that the differences between the second and third, the second and fourth, and the third and fourth, were nonsignificant. 124 6.2 Experiment 2: Clarity of the Queries
Interfaces to databases based on natural language interpretation inevitably suffer from controlled language. Our method of composing queries avoids this problem altogether: because the natural language feedback text is generated by the system rather than the user, there is no need for the system to choose among alternative interpretations. Of course, this does not guarantee that the query text is equally transparent to the user: this will depend on the efficacy of our feedback text design X  X he point we wish to evaluate in the present experiment, which explores the extent to which composed queries, as presented in the feedback texts, can be clearly understood.

Subjects. Fifteen subjects participated in the experiment. Of these, ten had previously participated in Experiment 1; the new subjects had the same profile as those previously seen.

Methodology. Subjects were given a paper-based questionnaire containing 24 trials, each showing a completed complex query as presented in the interface (i.e., as a  X  X eedback text X ). Each query was associated with three alternative interpretations, presented as full natural language questions: only one of these represented the correct meaning; the other two represented plausible but incorrect meanings. Subjects were given a forced-choice task to identify which of the three alternatives corresponded to the meaning of the given feedback text.
 five presentation sets, each containing a different ordering of the options for each query, and these were randomly assigned to subjects. We suggested to subjects that a useful strategy might be to read the alternatives before looking at the associated feedback text. There was no time limit.
 Materials. The materials comprised four examples, each of six patterns of ambiguity:
Type 1: Attachment of temporal expression. Most events can have a temporal expression associated. When there is more than one event that could be subsumed by a temporal expression, the text may become ambiguous. For example: Options: 10
Type 2: Scope of conjunctions. Whenever a complex expression contains a combination of conjunctions and disjunctions, potential ambiguities may occur, especially when combined with negations or prepositional phrases. For example: Options:
Type 3: Scope of conjunctions plus attachment of temporal expression. This is an extension of the first two cases, where a temporal expression post-modifies an expression that is part of a conjunction of events. For example: Options: 126
Type 4: Combination of various query components. Events in a query can be linked to each other by various means, including temporal expressions, conjunctions, and disjunc-tions. Complex combinations may render the feedback text ambiguous. For example: Options:
Type 5: Complex queries, non-ambiguous components. We introduced this category in order components. Because most queries in the medical domain are likely to be very complex, can the sheer number of query components render the query ambiguous to the users? For example: Options
Type 6: Attachment/interpretation of outcome. The outcome section generally describes a condition holding between a reference and a target set of patients. If the query contains multiple features describing the patient set, it may be difficult to differentiate between features that contribute to the reference set and features that contribute to the target set. For example: Options:
Results. If the presented feedback text is incomprehensible, the probability that subjects a third of the time). Our results show that subjects X  precision is 0.84; that is, on average, The breakdown by type of ambiguity is shown in Table 1.
 128 6.3 Summarizing the Evaluation designed. Normally, this would be preceded by a bank of formal empirical studies under more controlled conditions. For a question-answering system like the one we are addressing in this article (which is but a small part of a much larger system), a formal controlled evaluation would ideally cover a large number of exemplars of each type of query supported by the system, and a large number of subjects. Given our constraints on the number of available subjects (and the concomitant effect this has on the possible design of any experiments), the evaluation reported here is necessarily more limited in scope. This is not an unusual situation in system development, where evaluation must proceed by gradual refinement through the application of rigor, wherever possible, but also applying along the way intuition, common sense, and past experience. The evaluation we have presented here shows what can be done during the early phases of the development of a large and complex system whose components are in different stages of completion, and where access to representative users is limited.
 encouraging. Our results suggest that our target users (medical researchers) can quickly relevant. Specifically: high degree of structural complexity are not difficult to understand. This is extremely important, as it means that users can be confident that they are obtaining an answer that pertains to the question that they think they are asking, as opposed to an answer to some other similar question.
 that the Conceptual Authoring method of query composition may be much more user-friendly than the traditional method of direct skilled SQL coders with a high level of familiarity with the database and the domain (Hallett, Scott, and Power 2006). Our tests showed that (an albeit small sample of) such
SQL codes, found it much easier to compose queries with the Conceptual Authoring average, to compose the query in SQL , but they were not able to produce the complete
SQL in that time. 7. Conclusion
Most question-answering systems make use of natural language understanding and allow users to pose simple questions to textual repositories. We have presented here a generic method for composing natural language questions within a question-answering system that avoids the well-know pitfalls of natural language understanding while allowing users to pose complex questions to data repositories. The method,
Conceptual Authoring, involves no natural language interpretation  X  X nly generation  X  and is particularly well-suited to query interfaces to closed-domain systems. We have elucidated the method through its use in the CLEF query tool, which has been designed of electronic health records by doctors and medical researchers. Similar requirements finance), as data are increasingly available in machine-usable electronic form; they can be summarized as follows:
Although not exactly a requirement, a practical consideration is that the queries should be frequent and important enough to justify the effort needed to meet the very stringent language interface like the CLEF query tool except in contexts where the query results are highly valuable.
 foremost on using a familiar medium X  X he medium the experts use in their normal work, which in this case means natural language. However, as argued in
Section 2, traditional natural language interfaces to database systems cannot meet the text can be achieved only if the text conforms strictly to a controlled language (which our users would not have time to learn). We therefore proposed a modification of the traditional approach, in which the semantic representation of the query is edited directly, through an interactive feedback text generated by the system. Otherwise the approaches are the same: once obtained, the semantic representation is transcoded to the database query language and passed to the database management system; when the answer is returned, it is organized to suit the purposes of users, and presented in a familiar display such as a text or diagram.
 evaluation study provides some evidence that our approach can meet the requirements. First, our studies were performed with the relevant users , in this case medical experts.
The training required for reaching a reliable level of performance was a matter of minutes X  X sually a single demonstration followed by a single trial. Thereafter, most 130 users could formulate fairly complex queries in a reasonably short time (3 X 4 minutes); in contrast, we have found in informal tests that expert SQL as long, while often failing to achieve a complete query (Hallett, Scott, and Power 2006).
The reliability achieved over 60 queries was 100%, in the sense that all users managed to formulate all their targets. Finally, a further experiment showed that the formulations of the queries in the feedback texts were transparent , with accuracy rates of 80 X 90% on a multiple-choice comprehension task with a random baseline of 33%.
 approach to this class of problems: we know of no alternative approach that can achieve similar success in meeting these requirements. However, there are several areas where improvement should be possible. First, we need to find ways of facilitating the development process: building and maintaining a system like the requires, at present, the hand-coding of linguistic and conceptual resources; as a step in this direction, we have developed a method of automatically inferring relevant types
Conceptual Authoring interface (Hallett 2006). Second, we cannot be sure yet that the wording of feedback texts is optimal X  X erhaps with more research the comprehension rates can be pushed higher. Finally, we have only begun to explore the possibilities for improving the GUI for Conceptual Authoring.
 References 132
