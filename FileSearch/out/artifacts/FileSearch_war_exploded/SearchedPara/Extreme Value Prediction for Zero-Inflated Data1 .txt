 The notion behind being able to foretell the occurrence of an extreme event in a time series is very appealing, especially in domains with significant ramifications associated with the occurrence of an ext reme events. Predicting pandemics in an epidemiological domain or forecasting natural disasters in a geological and climatic environment are examples of applications that give importance to detec-tion of extreme events. Unfortunately, the accurate prediction of the timing and magnitude of such events is a challenge giv en their low occurrence rate. More so, the prediction accuracy depends on the r egression method used as well as char-acteristics of the data. On the one hand, standard regression methods such as generalized linear model (GLM) emphasize estimating the conditional expected value, and thus, are not best suited for i nferring extremal values. On the other hand, methods such as quantile regression are focused towards estimating the confidence limits of the prediction, and thus, may overestimate the frequency and magnitude of the extreme events. Though methods for inferring extreme value distributions do exist, combining them with other predictor variables for prediction purposes remains a challenging research problem.

Standard regression methods typically assume that the data conform to certain parametric distributions (e.g., from an exponential family). Such methods are in-effective if the assumed distribution does n ot adequately model characteristics of the real data. For example, a common pro blem encountered especially in model-ing climate and ecological data is the excess probability mass at zero. Such zero-inflated data, as they are commonly known, often lead to poor model fitting using standard regression methods as they tend to underestimate the frequency of ze-ros and the magnitude of extreme values in the data. One way for handling such type of data is to identify and remove the excess zeros and then fit a regression model to the non-zero values. Such an approach, can be used, for example, to pre-dict future values of a precipitation time series [13], in which the occurrence of wet or dry days is initially predicted using a classification model prior to applying the regression model to estimate the amount of rainfall for the predicted wet days. A potential drawback of this approach is that the classification and regressions mod-els are often built independent of each other, preventing the models from gleaning information from each other to potentially improve their predictive accuracy. Fur-thermore, the regression methods used in modeling the zero-inflated data do not emphasize accurate predic tion of extreme values.

The paper presents an integrated framework that simultaneously classifies data points as zero-valued or not, and apply quantile regression to accurately predict extreme values or the tail end of the non-zero values of the distribution by focussing on particular quantiles.

We demonstrate the efficiency of the proposed approach on modeling climate data (precipitation) obtained from the Canadian Climate Change Scenarios Net-work website [1]. The performance of the approach is compared with four baseline methods. The first baseline is the general linear model (GLM) with a Poisson distribution. The second baseline used is the general linear model using an ex-ponential distribution coupled with a binomial distribution classifier(GLM-C). A zero-inflated Poisson was used as the third baseline method (ZIP). The fourth basesline was quantile regression. Empirical results showed that our proposed framework outperforms the baselines for majority of the weather stations inves-tigated in this study.

In summary, the main contributions of this paper are as follows:  X  We compare and analyze the performance of models created using variants  X  We present a approach optimized for modeling zero-inflated data that out- X  We successfully demonstrated the proposed approach to the real-world prob-The motivation behind the presented mod el is accurately pr edicting extreme values in the presence of zero-inflated data. Previous studies have shown that additional precautions must be taken to ensure that the excess zeros do not lead to poor fits [2] of the regression models. A typical approach to model a zero-inflated data set is to use a mixture distribution of the form P ( y | x )=  X  X  0 ( x )+(1 x and  X  is a mixing coefficient that governs the probability an observation is a zero or non-zero value. This approach assumes that the underlying data are generated from known parametric distributions, for example,  X  may be Poisson or negative binomial distribution (for discrete data) and lognormal or Gamma (for continuous data).

Generally, simple modeling of zero val ues may not be sufficient, especially in the case of zero-inflated climate data such as that of precipitation where extreme value observations, (that could indicate floods, droughts, etc) need to be accurately modeled. Due to the significance of extreme values in climatology and the increasing trend in extreme precipita tion events over the past few decades, a lot of work needs to be done in analysing the trends in precipitation, temperature, etc., for regions in United states, Canada, among others [3]. Katz et al. introduces the common approaches used in climate change research, especially with regard to extreme values[4].

The common approaches to modeling extreme events are based on general extreme value theory [5], Pareto distribution [10], generalized linear modeling [6], hierarchical Bayesian approaches [9], etc. Gumbel [8] and Weibull [12] are the more common variants of gen eral extreme value distribution used. There are also Bayesian models [11] that try augmenting the model with spatial information. Watterson et al. propose a model that also deals with the skewness of non-zero data/intermittency of precipitation using gamma distribution to interpret changes in precipitation extremes [7]. In contrast, the framework presented in this paper handles the intermittency of the data by coupling a logistic regression classifier to the quantile regression part of the model. Consider a multivariate time series L =( x t ,y t ), where t  X  X  1 , 2 ,  X  X  X  ,n } is a discrete-valued index for time, x t is a d -dimensional vector of predictor vari-ables at time t ,and y t is the corresponding value for the response (target) variable. Given an unlabeled sequence of multivariate observations x  X  ,where  X   X  X  n +1 ,  X  X  X  ,n + m } , our goal is to learn a target function f ( x ,  X  )thatbest estimates the values of the response variable by minimizing the expected loss E estimated from the training data L .

Multiple linear regression (MLR) is one of most widely used regression meth-ods due to its simplicity. It assumes f ( x ,  X  )=  X  T x (where x is a ( d +1)-dimensional vector whose first element x 0 =1and  X   X  d +1 is the weight vector) and the response variable y is related to f ( x ,  X  ) via the following equa-tion: As a result, P ( y | x )  X  N (  X  T x , X  2 )and E y | x [ y ]= predicted value of the response variable for a test data point x  X  is  X  T x  X  ,this implies that the predictions made by MLR focus primarily on the average value of y given x  X  . This explains the limitation of MLR in terms of inferring extreme values in a given time series. The parameter vector  X  in MLR can be estimated using the maximum likelihood (ML) approach to obtain where X is the n  X  ( d +1)designmatrixand y is an n  X  1 column vector for the observed values of the response variable.

The drawback of simple linear regression is that it is built on a strong assump-tion -namely, normality. Unfortunately, real world data may not always have a normal distribution and may be skewed to one side or may not cover the whole range of real numbers or may have a heavier tail than the normal distribution, etc. Hence, alternative approaches that are not constrained by such assumptions such as GLM may be used. 3.1 Generalized Linear Model(GLM) and 2-Step GLM (GLM-C) The generalized linear model is one of m ost widely used regression methods due to its simplicity. Generally, a GLM consists of three elements: 1. The response variable Y , which has a probability distribution from the exponential family. 2. A linear predictor  X  = X  X  3. A link function g (  X  ) such that E ( Y | X )=  X  = g  X  1 (  X  ) where, Y  X  X  n  X  1 is the response variables vector, X  X  X  n  X  d is the design matrix with all 1 in the last column.  X   X  X  p  X  1 is the parameter vector. Since the link function shows the relationship between the linear predictor and the mean of the distribution, it is very important to understand the detail about the data before arbitrarily using the canonical link function. In our case, since the precipitation data are always non-n egative and values represented using a millimeter scale, the non-zero data may be treated as count data allowing us to use Poisson distribution or an exponential distribution to describe the data. Hence, in our experiments we always choose log(  X  ) as the link function and choose to use Poisson distribution. We scale the Y used in the regression model to be 10  X  Y :
The histogram in Figure 1 is a representation of the data belonging to station-1. It is clear that the number of zero is t oo large. The second histogram which is without zero looks similar to a kind of Poisson or exponential distribution.
Considering the large number of zeros, one is motivated to perform classifi-cation first to eliminate the zero values before any regression. There are many classification methods available. But for the purpose of our experiments, we use logistic regression (which is also a variation of GLM) to do the classification. The response variable Y  X  of logistic regression is a binary variable defined as: The detail of the model is as follows: The link function is a logit link g ( p )= log( When we derive the fitted values, they will be transferred to be binary: The second part is a GLM with exponential distribution, the response variable Y is just those non-zero data, and the link function is g (  X  ) = log(  X  ): Then, we got fitted-value f for all X i Finally, we report the product of those two fitted-values  X  Y = f  X   X  f
To fit the GLM model, we use iteratively reweighted least squares(IRLS) method for maximum likelihood estimation of the model parameters.
 3.2 Zero Inflated Poisson Regression(ZIP) Differing from the methods above, zero i nflated poisson regression treats the zero as a mixture of two distributions: a Bernoulli distribution with probability  X  to get 0, and a Poisson distribution with parameter  X  (let Pr (  X  ;  X  )denotethe probability density function). In fact, the ZIP regression model is defined as: where 0 &lt; X  i &lt; 1, and where  X  1 ,  X  2 are all regression parameter. Both of them could be found by maximizing the likelihood function. For the purpose of the experiments, we used the R package  X  X scl X  to fit the model. 3.3 Quantile Linear Regression(QR) and 2-step QR(QR-C) Quantile regression was used to estimate the specified quantile of a population. Hence, if the objective of the regression is to estimate the conditional quan-tile(e.g., median) of Y instead of a conditional mean like MLR and Ridge regres-sion, one may use quantile regression. I ts loss function for the linear regression model is: where Let F Y ( y )= P ( Y  X  y ) be the distribution function of a real valued random variable Y. The  X  th quantile of Y is given by: It can be proved that the  X  y which minimizes E X   X  ( y  X   X  y ) should satisfy that F
Y ( X  y )=  X  . Thus, quantile regression will find the  X  th quantile of a random variable, for example: For the purpose of the experiments conducted, we always used  X  =0 . 95 to rep-resent extreme high value. Unlike the least squares methods mentioned above which could be solved by numerical linear algebra, the solution to quantile re-gression is relatively non-trivial. Linear programming is used to solve the loss function by converting the problem to the following form. For the same reason as mentioned in the Section 3.1, a classification method should be incorporated along with the regression model. We used logistic regres-sion for classification, and quantile regression on those nonzero Y . Finally, we report the product of those two fitted values. Quantile regression may return a negative value, which we force to 0. We do this because precipitation is always non-negative. Now that we have introduced quantile regression, which is an integral part of our objective function we will elaborate the motivation behind the various components of the proposed objective func tion. Since zero-inflated data is best described with the help of a classifier that help identify non-zero values and a regression component to address non-zero values, our framework consists of both components. For the classifier component we use least square support vec-tor machine and for the regression component, we use the intuition of quantile regression to help focus the regression of extreme values. Since the final predic-tion of the data point using this framework is a product of the regression and classification component, the quantile regression component is built to work on the eventual predicted return value, th ereby integrating both the classifier and regression components. 4.1 Integrated Classifier and Regression for Extreme Values(ICRE) The classification and regression models developed in this study are designed to minimize the following objective function: arg min  X  where n  X  is the number of nonzero y i . Then it can be expanded as follows:
The rationale for the design of our objective function is as follows. The first term which corresponds to the regression part of the equation represents quantile regression performed for only the observed non-zero values in the time series. The regression model is therefore biased towards estimating the non-zero extreme values more accurately and not be adversely influenced by the over-abundance of to the predicted output of our joint classification and regression model. The sec-ond term in the objective function, which is the main classification component, is equivalent to the least square suppor t vector machine. And the last two terms in the objective function are equivalent to the L 2 norm used in ridge regression models to shrink the coefficients in  X  1 and  X  2 .

We consider each data point to be a repr esentative reading at an instance of time t  X  X  1 , 2 ,  X  X  X  ,n } in the time series. Each predictor variable is standardized by subtracting its mean value and then dividing by its corresponding standard deviation. The standardization of the variables is needed to account for the varying scales.
 The optimization method used while performing experiments is  X  X -BFGS-B X , described by Byrd et. al. (1995). It is a limited memory version of BFGS methods. This method does not store a Hessian matrix, just a limited number of update steps for it, and then it uses derivative information. Since our model includes a quantile regression component, which is not differentiable, this method of optimization is well suited to our objective function.

To solve the objective function, we used the inverse logistic function of x T i  X  2 instead of sign(( x T i  X  2 +1) / 2)). The decision was motivated by the fact that the optimizer tries to do a line search along the steepest descent direction and finds the positive derivative along this line, which would result in a nearly flat surface for the binary component. Hence conversion of the binary report to an inverse logistic function of x T i  X  2 was used to address this issue. During the prediction stage, we use the binary-fitted values from the SVM component. In this section, the climate data that are used to downscale precipitation is de-scribed. This is followed by the experiment setup. Once the dataset is introduced, we analyzed the behavior of baseline models and contrasted them with ICRE in terms of relative performance of the various models when applied to this real world dataset to forecast futu re values of precipitation. 5.1 Data All the algorithms were run on climate data obtained for 29 weather stations in Canada, from the Canadian Climate Change Scenarios Network website [1]. The response variable to be regressed (down scaled), corresponds to daily precipita-tion values measured at each weather station. The predictor variables correspond to 26 coarse-scale climate variables derived from the NCEP Reanalysis data set and the H3A2a data set(computer generated simulations), which include mea-surements of airflow strength, sea-level pressure, wind direction, vorticity, and humidity. The predictor variables used for training were obtained from the NCEP Reanalysis data set while the predictor variables used for the testing were ob-tained from the H3A2a data set. The data span a 40-year period, 1961 to 2001. The time series was truncated for each weather station to exclude days for which temperature or any of the predictor values are missing. 5.2 Experimental setup The first step was to standardize the predictor variables by subtracting its mean value and then dividing by its corresponding standard deviation to account for their varying scales. The training size used was 10yrs worth of data and the test size, 25yrs. During the validation p rocess, the selection of the parameter  X  was done using the score returned by RMSE-95. Also, to ensure the experiments replicated the real world scenario wher e the prediction for a future timeseries needs to be performed using simulated va lues of the predictor variables for the future time series, we used simulated values for the corresponding predictor variables obtained from H3A2a climate scenario as X U , while X L are values obtained from NCEP. All the experiments were run for 37 stations. 5.3 Baseline Algorithm We compare the performance of ICRE with b aseline models created using general linear model(GLM), general linear model with classification (GLM-C), quantile regression(QR), quantile regression with classification and zero-inflated Pois-son(ZIP). Further details about the baselines are provided below.
 General Linear Model (GLM). The baseline GLM refers to the generalized linear model that uses a Poisson distribution as a link function, resulting in the regression function log(  X  )= X X  ,where E ( Y | X )=  X  General Linear Model with Classification (GLM-C). Unlike the previous baseline (GLM), GLM-C refers to a two st ep generalized linear model that uses a Binomial distribution, for the classifier with the model described as logit ( p )= X X  ,and E ( Y =1 | X )= p which Y =1when Y&gt; 0and Y =0when Y =0 and a second step that uses a generali zed linear model with an exponential distribution that is built only on non-zero response data points. The regression function is log(  X  )= X X  ,which E ( Y | X )=  X  . The eventual predicted value for each data point is the product of the two respective fitted values. Quantile Regression (QR). The baseline QR refers to the regular quantile regression described earlier in the preliminary section 3 Quantile Regression with Classification(QR-C). The baseline QR-C refers to a two step model that has a GLM that uses a binomial distribution that acts as a classifier and a regular quantile regression model that is built on non-zero valued data points as described earlier in the preliminary section. These two models that comprise QR-C are built independent of each other and the eventual predicted value for each data point is the product of the two respective fitted values.
 Zero Inflated Poisson(ZIP). Zero Inflation Poisson model used as a baseline and is similar to the ZIP model described in Section 3. 5.4 Evaluation Criteria The motivation behind the selection of the various evaluation metrics was to evaluate the different algorithms in terms of predicting the magnitude and the timing of the extreme events.The followi ng criteria to evalua te the performance of the models are used:  X  Root Mean Square Error (RMSE), which measures the difference between  X  RMSE-95, which we use to measure the difference between the actual and  X  Confusion matrices will be computed to visualize the precision and recall of To summarize, RMSE-95 is used for measuring magnitude and F-measure mea-sures the correctness of the timing of the extreme events. 5.5 Experimental Results The results section consists of two main sets of experiments. The first set of experiments evaluates the impact of zero-inflated data on modeling extreme values. The second section compares the performance of ICRE with the baseline methods which are followed . Impact of Zero-Inflated Data on Extreme Value Prediction. Unlike regular data which may be modeled usin g regression, modeling zero-inflated data usually involves a classifier and a regression component. The classifier is used to identify zero and no n-zero values, which is followed by regression for the non-zero values. But since the focus of the paper is on extreme data points within zero-inflated data, the impact of the classifier is unclear. In this section, we compare the impact of including the classifier in modeling extreme values of zero-inflated data. We compared QR with QR-C and GCM with GCM-C and show the results in Table 1. Note that the percentage of wins for F-measure, recall, precision may not total to 100 in the case of a tie.

As shown in the Table 1, it isn X  X  clear that using an independent classifier along with regression for modeling extreme values among zero inflated data is preferred. But the results do indicate that the inclusion or exclusion of a classi-fier with the regression model built independent of each other may compromise either RMSE-95 (by overestimating the magnitude) or F-measure (mistiming predicting an extreme value), without necessarily compromising both together. Comparison of ICRE to Baseline Methods. Table 2 shows the relative per-formance of ICRE to all the baseline methods in terms of percentage of stations outperformed against the baseline method in terms of RMSE-95 values calcu-lated on extreme rain days. In terms of RMSE of extreme rain days, as shown in Table 2, ICRE outperformed the baselines (except QR) in almost every one of the 37 stations. But QR was the best across all methods for RMSE-95 of extreme days. In terms of F-measure that was co mputed based on recall and precision of identifying extreme events, ICRE again outperformed the baselines(except QR-C) in majority of the 37 stations. But ICRE was only able to outperform QR-C in 16 or the 37 stations in terms of F-measure. Although QR performed the best in terms of estimating magnitude for those extreme events, it over-estimate the timing of the events as seen by the relatively lower F-measure score. QR-C did the reverse, it did reasonably well in terms of modeling the timing, but performed very poorly in terms of the magnitude of the events by overestimating. This paper compare and analyze the perfo rmance of models created using vari-ants of GLM, quantile regression and ZIP approaches to accurately predict values for extreme data points that belong to a zero-inflated distribution. An alter-nate framework(ICRE) was present that outperforms the baseline methods and the effectiveness of the model was demons trated on climate data to predict the amount of precipitation at a given station. For future work, we plan to extend the framework to a semi-supervised setting.

