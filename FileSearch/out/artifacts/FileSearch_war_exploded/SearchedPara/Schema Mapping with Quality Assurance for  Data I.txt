 Independently developed producers for example Amazon, eBay etc. bring about sequently Schema-Mapping targeting at the uniformity of data schema is a primary operation. To guarantee the quality of generated data is the main part in data exchange and information integration. We have a very good understanding of mappings between relational schemas (see, e.g., recent SIGMOD and PODS keynotes on the subject [1, 2]); several advanced prototypes for specifying and managing mappings have been developed and incorporated into commercial systems [3, 4]. There are techniques for using such mappings in data integration and exchange, and tools for handling mappings themselves, for example, for defining various operations on them [2, 5, 6, 7, 8, 9]. But much less is known about mappings between XML schemas. 
However, more and more of today X  X  data are represented in non-relational form. In particular, XML is increasingly popular, especially for data published on the Web and data exchanged between organizations. [10] XML data has become the standard of information conversion and integration on the Internet because of the usability of XML. geneous XML information, so information cannot be extracted and collected effectually. For that reason, Schema-Mapping is required to integrate originally heterogeneous XML information into same Schema. Since XML data is semi-structured, traditional Schema-Mapping operation on the heterogeneous XML data cannot guarantee the Mapping with Quality Assurance for XML Data Exchange. The problem we faced has some properties: Heterogeneous Representation (HR) and Inconsistency Hidden (IH) in the semi-structure. 1. HR means that different sources for example Amazon, eBay etc. have different schemas to describe their data. As a semi-structured data description language, XML allows the same data is organized various forms through attributes and elements. 2. IH means that inconsistent data appear in the generated result after the Schema-Mapping operation. 
For example, two people, Jack Baker and Jason Brown, are obvious two persons. In the circumstances of writing the full name, they can be distinguished automatically by computer. As a matter of fact, data are not invariably standard as our example. In some case, they both are written as J. B. and to computer, J. B. is one person. Otherwise, Jack Baker is written as Jack B. and computer cannot identify that both names refer to the same person. The former phenomenon is called Homonymy-Name-Problem (HNP), and the latter phenomenon is called Synonym-Name-Problem (SNP). 
In general, there are some traditional methods to solve the information integration problem. [11-14] carry out a technique about Schema-Mapping between relational database and XML data. In fact, data integration or data Exchange is an operation of high CPU and Memory cost in database. The predecessor X  X  contribution cannot be simply used in the XML database. Even if the technology of predecessor is applied to implement a system, the system cannot guarantee the quality of data. In this paper, we are determined to address HNP and SNP to improve data quality. Since entity in the XML data is not correctly identified, HNP and SNP emerge. This will cause that we cannot get complete and accurate information about one entity on the whole. Solving HNP raises the rate of precise (accurate) and solving SNP raises the rate bring on Schema-Mapping based on entity. 
In this paper, we first summarize the predecessor X  X  works about Schema-Mapping data quality assurance which addresses HNP and SNP based on entity. Afterward a strategy comes up to automatically locate the node with sensitive quality. Ultimately, we prove our algorithm is practicable, feasible and valid through experiment. The rest of the paper is organized as follows. Section 2 presents a motivating example Section 3 describes the model and criterion. Section 4 gives the detailed methods. Section 5 is the implement of automatically positioning with quality problem mechanism. Section 6 uses experiments to verify our methods. Section 7 makes a conclusion. And references are in Section 8. Schema. We can abstract a simple problem as Figure 1 from Schema-Mapping. For the reason that we mainly focus on XML data which can be represented as node labeled meaning of label in our example is described in Table 1. 
In the source, R S shows three different paper which can be distinguished by title of paper or paper X  X  authors. R T shows the author information from original data (after data integration based value ). However in the target data, except for the same name (L.Y.), the author of t1 and the author of t2 could not be identified one person, and this is HNP. description of author, but also tree structure and lable value are the descriptions. In this section, we introduce a new model based on entity to solve Schema-Mapping, and propose a criterion to validate that our solution is able to achieve high precision and Schema-Mapping. 
We use precision and recall as metrics to evaluation. In order to calculate precision team data are generated, and every team contains dozens of authors. Then paper data person-name might appear in one or more teams, and the aim is to produce HNP. When using person-name in paper, the string of person-name might be mixed into noise information, and the aim is to produce SNP. To evaluate precision, we compare output set with  X  X old-Answer X  from process of generation. Let O means the set of object-pairs which describe one entity in both output set and Gold-Answer. Let P means the set of Gold-Answer. Let Q means the set of object-pairs which describe different entities in broken down by the Entity Extraction process into separate entity. One or more objects are come into being from every entity by the Data Exchange process. And the Output set is organized by new entities generated from the Entity Recognition process. 4.1 Entity Extraction process ensures that the entity agrees with the source schema. Considering example in procedures are shown in Operation 1. 4.2 Data Exchange The Data Exchange component is used to exchange the old data into new data. And this detailed operation is various because of different source and target schemas. For our example, the procedures are shown in Operation 2. 
In our example, structure information is CoAuthor, and this is stored in list authors. 4.3 Entity Recognition The Entity Recognition component is used to distinguish new entity and locate it. And this process aims at solving SNP and HNP through entity recognition. The basic plan is shown in Figure 4. At entrance, edit-distance between strings [16] and string trans-formations [17] are commendable to find out the hidden objects which are the potential which describe different entity. The procedure is shown in Operation 3. 
In the circumstances of data integration and schema mapping, the entity recognition considers the simple but significant structure information will obtain a high precise result within reasonable time. In the example, CoAuthor is chosen as structure information. 
In operation 3, the entity_set stores all entities and their structure information, dy-namic managed in the process of handling object one by one. Similarly the list stores all entities that are candidates of the object, author element. In real application, we have millions of objects that do not necessarily fit in the memory and need to be stored on disk. In putAuthor(), content of entity is stored in entity_set, which can be put in disk. This problem can be solved through adding a mapping structure between entity_set in memory and disk. 4.3.1 Broad-Entrance Broad-entrance is realized by the sentence: output is the list of similar entities. The method introduced in [16] and [17] certainly authors. 
In the process of Broad-Entrance, the purpose is to guarantee all entities which are information hidden carries main ability of distinguish. So the value of name becomes the foundation of Broad-Entrance. Approach in [19], originally proposed in [20], takes as signature all values having a hash value that is divisible by a constant m. When the value of object matches several entities, those entities are returned. This can work well based on clean data where different values mean different entity. Actually there is not clean data in specific applications. One way to deal with this is to replace each string by a set of chunks -e.g. words, q-grams[21] or v-grams[22] occurring in it, and then to compare these sets of chunks. A method for converting strings into sets of chunks is chunks is to form for each string the set of chunks occurring in any of its values, and then compare these sets. Such an approach has been suggested in [23]. Technically this can be done exactly as before the only difference is that we are now comparing sets of since information held by structure is not taken into account [24]. 
By means of chunks, the original value is broken into more short strings. Then the the signature of the set of chunks and the entity. The signature of object obtained as the same method matches entities, then the candidate entities is discovered by filter. 4.3.2 Narrow-Exit Narrow-exit is realized by the sentence: Coauthor is the description of the detected object which means variable author in in-stance, and stores all coauthors organized as one set. Through comparing the descrip-is discovered. By utilizing Jaccard Measure [25], compare method determines the relation between the object and the entity. 
Our approach to schema mapping based on entity is separated into above three op-erations. To derive the complexity of our method, assume that each of N articles has exactly A authors, and each of M authors has B coauthors. Then the time to process the scale of article and author. The model proposed in above section solves the quality problem with given position. In quality problem cannot be discovered immediately. The way exploiting programmer X  X  experience can help to manually distinguish the position with quality problem. But it X  X  not a general method. In consequence, the mechanism of automatical positioning with quality problem is proposed in this section. 
To realize the auto-mechanism, DTD document, the description of XML data, is detailed analyzed. There are some discoveries: organized as an XML tree, the attribute can be seen as attribute node. only 1 or 0. which retains main nodes is reorganized. This trick can cut down many weak branches in order to minish the structure of main entity. Based on this idea, the method of weak branch converged is proposed. 5.1 An Example of Weak Branch Converged The example in motivation is too simple; factually data have more complex structure. from common sense, article has one publisher and one conference; author has one age and one gender. Through weak branch converged, S is reformed into S X . processes of data exchange and entity recognition work. 5.2 The Strategy of Weak Branch Converged As the target, a reduced structure is the form which makes the Data Exchange simple schema. Some definitions are given as below: Central-node: is a relation between two or more nodes which satisfy many-to-many relationship. In Figure 5, article and author are central-node by common sense. central-node ship or designated by user. 
The reduction process is that Kernel-nodes absorb non-Kernel-nodes . And target structure is made up of Kernel-nodes . 5.3 An Implement of Weak Branch Converged A common-sense fitted assumption: if A and B have non-kernel-node relationship, they can be detected in local information. This is suitable in mostly circumstances, because information is gathered locally in the pro cess of schema exchange between schema S and schema T. Therefrom three rules for recognition is given as below: 1. If A is B's father in both schema and A:B=1:1, node A absorbs node B and inherits all son nodes under B. 2. If A is B's father in one schema, A:B=1:1 and B is A's father in the other, B:A=1:1, A absorbs node B and inherits all son nodes under B in both schema under the A:B=1:1 in one schema. B is A X  X  father, B:A=1:more, and the type of A is the only type of B X  X  son nodes. Under this circumstances node A absorbs B. 
Apply the above rules iteratively until all father-son pairs dissatisfy adsorbility. We use convergence ratio (CR for short) to judge the effect of reduction. CR=U/O, where U means node number of schema T and O means node number of schema S. experiments on an Intel dual core 2.4 Ghz Windows machine with 1G RAM. Our code was implemented by Code::Blocks 8.02. 6.1 Precision and Recall The result on real data is shown in table 2. 
From the results, we can draw some conclusions: 1. Different sources have different described the same entity, the better result. 
The diagrams which explain trend along with different parameters are illustrated in the following Figure 6. The data is artificial which are mentioned in Section 3. 
For precision measure, it has nothing with the number of groups and the number of recall measure, there is no explicit relationship between recall and number of groups. It increases with the number of every group X  X  papers and decreases with the number of every group X  X  authors. 6.2 Convergence Ratio The result on real data is shown in table 3. 
It is very common that schema can be converged. And from real data, the converged operation brings about a striking effect (see the small ratio). with quality assurance. The new model settles HNP and SNP, and the method is based on entity. Meanwhile, a strategy is given to find nodes with sensitive quality. Finally, we prove our technique through experiment. 
