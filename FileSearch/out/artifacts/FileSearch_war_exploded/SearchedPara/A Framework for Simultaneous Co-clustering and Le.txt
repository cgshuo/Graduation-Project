 For difficult classification or regression problems, practition-ers often segment the data into relatively homogenous groups and then build a model for each group. This two-step pro-cedure usually results in simpler, more interpretable and actionable models without any loss in accuracy. We con-sider problems such as predicting customer behavior across products, where the independent variables can be naturally partitioned into two groups. A pivoting operation can now result in the dependent variable showing up as entries in a  X  X ustomer by product X  data matrix. We present a model-based co-clustering (meta)-algorithm that interleaves clus-tering and construction of pre diction models to iteratively improve both cluster assignment and fit of the models. This algorithm provably converges to a local minimum of a suit-able cost function. The framework not only generalizes co-clustering and collaborative filtering to model-based co-clustering, but can also be viewed as simultaneous co-segment-ation and classification or regression, which is better than independently clustering the data first and then building models. Moreover, it applies to a wide range of bi-modal or multimodal data, and can be easily specialized to ad-dress classification and regression problems. We demon-strate the effectiveness of our approach on both these prob-lems through experimentation on real and synthetic data. H.2.8 [ Database Management ]: Database Applications X  Data Mining Algorithms Co-clustering, classification, regression, prediction models, multimodal data Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00.
While it is common practice to develop a single learned model (e.g., a classification or regression model, or a single ensemble of multiple base-learners) to characterize a given dataset, for many problems it is practically advantageous to partition the population into multiple, relatively homoge-neous segments and then develop separate models for each segment [2, 7, 20, 18]. For example, an e-tailor may attract different types of browsers, from casual shoppers to bulk purchasers, and one may want to model their purchasing inclinations separately. Similarly while forecasting electric load usage, it is advisable to build separate predictive models for weekdays, weekends and holidays. Advantages of such divide-and-conquer approaches include not only improved accuracy and reliability in general, but also improved inter-pretability as well, since the component models are often far simpler [24]. Typically the partitioning is done apri-ori based on domain knowledge or a separate segmentation routine [2, 7].

This paper is concerned with situations where the inde-pendent variables can be naturally partitioned into two (or more) groups that are associated with their corresponding modes. We then simultaneously cluster along each mode, as well as fit a learned model to each co-cluster. The approach can alternatively be viewed as a model-based generalization of biclustering or co-clustering , which is a technique that si-multaneously clusters along multiple axes and has been suc-cessfully applied in several domains like text clustering and microarray data analysis [4, 3, 6]. Co-clustering, is tradi-tionally applied to a matrix of data values, where the rows are data points and the columns are features, e.g. in mi-croarray data the rows are genes and columns are experi-ments, in recommender systems the rows are customers and the columns are products. Co-clustering exploits the duality between the two axes to improve on single-sided clustering. In the alternative viewpoint, along with the data matrix, a set of variables is associated with the rows, and another set with the columns. For each co-cluster, a model is learned to predict a matrix cell value given the corresponding row and column attributes. Generalization refers to predicting the missing values in the data matrix, as well as values when new rows/columns are added.

Example: To concretize the above discussion, consider the problem of predicting customer purchase decisions (rec-ommending products to customers). The dataset in this case is a matrix of customers by products, where the cell val-ues are class labels representing whether a customer buys a certain product or not. This matrix will have missing values where the corresponding customer-product choice is unknown. Each customer is described by a set of attributes, for example, demographics and each product also has at-tributes, which could include the price, market share, qual-ity, etc. The problem is to predict the choices for the missing customer-product combinations, as well as behavior of new customers, or choices made for new products. Note that collaborative filtering approaches for this problem will make use only of the matrix entries and ignore customer/product attributes [10, 8]. On the other extreme, a typical classifica-tion model will form a map between the feature vector for a given customer-product pair and the corresponding matrix entry, but will not consider nearby customers or products in this process. For the classifier, the dependent variable values are nothing but the matrix entries, and the indepen-dent variables are grouped into variables associated with the rows and variables associated with the columns. For a di-verse population of customers and a wide range of different products, it is unlikely that all the customer-product pref-erences can be well explained by a single model. It is more natural for a model to closely represent the preferences of only a subset of the customers for a subset of the products.
A co-clustering approach will simultaneously cluster the customers and products based on the matrix entries. It will then use the entries of the corresponding co-cluster to pre-dict a missing value. If done properly, this gives better re-sults than standard recommender systems [8], however this approach still ignores the customer and product attributes, i.e., the prediction is solely based on the value of the de-pendent variable in a suitably identified neighborhood. Our approach exploits both neighborhood information as well as the available customer/product attributes. The idea is to co-cluster the entire data matrix into blocks of customers and products such that each block can be well characterized by a single predictive model. Note that the similarity between two data entries is now determined not by the similarity between the values themselves, but rather between the cor-responding predictive models. Moreover, our model based co-clustering-cum-learning algorithm achieves this by inter-leaving clustering and construction of classification models to iteratively improve both cluster assignment and fit of the models. This simultaneous approach is better than inde-pendently clustering the data first and then building clas-sification models. We also exhibit a cost function that is steadily decreased in both steps of the iterative process, till one reaches a local minimum, thereby guaranteeing conver-gence.

The above approach is not restricted to classification. To highlight this, we also consider the problem of building mul-tiple regression models, one within each co-cluster, for the application of predicting the number of items of a given product purchased by a customer, using a formidable, real dataset in Section 8.2. The overall approach remains the same, only the specifics of the learned models and of the measure of fit change.

In the rest of the paper we refer to the data points as customers and axes as products, based on our motivating applications. Our approach is however not restricted to a customer-product matrix, and is applicable to any bi-modal dataset. It can also be extended to multi-modal data, e.g. a 3-D tensor (data cube) with sets of variables associated with one or more of the axes.

Notation : Small letters represent scalars e.g. a , z , small, bold face letters represent vectors e.g. b , c ,  X  , capital letters like Z, W represent matrices. Individual elements of a ma-trix e.g. Z are represented as z ij ,where i and j are the row and column indices respectively.
There are several examples of the use of localized predic-tion models in load forecasting systems, where clustering is used to distinguish smaller, homogenous groups of data and a prediction model is then fitted for each cluster in a two step sequential process [2, 7]. Clustering based prediction models have also been widely used in economics [20, 18]. Sfetsos et al. [23] propose an iterative algorithm to cluster time se-ries data such that each cluster consists of data points with similar linear models. This is followed by a heuristic to iden-tify a single cluster to be used for future predictions. This clustering algorithm is one sided and linear model based, a special case of our model based co-clustering algorithm.
In the bioinformatics domain, clustering of genes is often used as a preprocessing step for the classification of exper-iments (samples) in microarray data analysis [17, 13]. A cluster is represented by the mean of the expression profiles across all its member genes, which acts as a dimensional-ity reduction step for the classification process. This helps to reduce gene redundancies and constructs parsimonious and more interpretable classification models. A simultane-ous clustering and classification algorithm is proposed by Zhang et al. [27], which uses a voting based classifier ensem-ble to improve a clustering solution. The labels assigned by an initial clustering are used to train a set of diverse classi-fiers. Data points that lie on cluster boundaries are relabeled by combining the classifier predictions using a majority vote. This process is iterated to refine the clustering solution.
The mixture-of-experts frame work [12, 21] simultaneously partitions the input space while learning models for each partition. The partitioning is soft however, i.e., multiple models are involved in varying amounts for producing any particular input-output map, which makes the system less interpretable or actionable as compared to our proposed ap-proach. Moreover our approach is able to smooth over the joint input-output space which is not achieved by a mixture-of-experts.

Co-clustering (also known as biclustering) has been used in several diverse data mining applications like clustering microarray data [3, 4], text mining [6] and marketing ap-plications [25]. Most clustering or co-clustering approaches cannot handle missing data and assume a full data matrix. However the formulation by Banerjee et al. readily handles missing data [1], and has been shown to perform significantly better than traditional collaborative filtering techniques in a recommender system setting [8], where the data is a ma-trix of customer-movie ratings. The known ratings are used to simultaneously cluster customers and movies and com-pute summary statistics for the co-clusters, which are then used to predict unknown ratings, using an instance of the Bregman co-clustering algorithm [1]. Recently Ying et al. proposed a recommendation model that emphasizes the im-portance of using missing recommendation ratings as part of the model rather than ignoring them completely [26]. They demonstrate that by jointly modeling whether and how an item was rated (selection and rating respectively), the ac-curacy of existing recommendation systems can be substan-tially improved.

The idea of simultaneous clustering and regression was in-troduced in the marketing literature by Wedel and Steenkamp, who proposed a generalized fuzzy clusterwise regression tech-nique to find both customer segments and market struc-ture [25]. Each cluster include s fractional membership from all customers and products and is hence a fuzzy co-cluster. Each cluster has a regression model that predicts the prefer-ences as a linear combination of the product attributes. The cluster memberships and models are estimated so as to re-duce the total squared error between the actual preferences and the predicted preferences. However, the hard version of this method corresponds to diagonal co-clustering [19], where only a subset of products is associated with each customer group. In contrast, this paper is concerned with partitional co-clustering, which covers the entire customer-product matrix.
We now describe the problem formulation for the classi-fication setting. Let m be the total number of customers and n the total number of products. The data can be rep-resented as an m  X  n matrix Z of customers and products, with cells z ij representing the corresponding class labels, e.g. whether customer i buys product j or not. Throughout the following discussion we assume that we are dealing with a 2 class problem and z ij  X  X  X  1 , +1 } , however the algorithm can easily be generalized to deal with multiclass settings. The problem formulation and solution for regression mod-els is given in Section 5. A weight w ij is associated with each cell z ij . The weights of the known (training) matrix cell values are set to 1. The missing cell values, that are to be predicted are given a weight of 0. In general, the weight is not restricted to 0 or 1 and can take other values. This formulation allows the predi ction framework to deal with data uncertainties, where less certain values can be given comparatively lower but non-negative weights.

Acustomer i has attributes C i , and product j has at-tributes P j . It is assumed that each class label z ij is pri-marily determined by the attributes of the corresponding customer-product pair and is generated by a certain model involving these attributes. We assume this model to be a logistic regression model 1 where the log odds is modeled as a linear combination of the customer and product attributes given by where x ij T =[1 , C i T , P j T ] is a vector consisting of the cus-tomer and product attributes, and f ( x ij )=  X  T x ij is a linear model with parameters  X  T =[  X  0 ,  X  c T ,  X  p T ]. The similar-ity of the cell values is now defined based on the similarity of their underlying logistic regression models. The aim is to simultaneously cluster the rows (customers) and columns (products) into a grid of k row clusters and l column clus-ters 2 , such that the class labels within each co-cluster are
The focus of this paper is on simultaneous co-clustering and classification, rather than obtaining the best possible classifier, therefore we have chosen a standard, fairly flex-ible classifier rather than experiment with a multitude of classifier options.
This form of co-clustering is often called partitional co-clustering [19] predicted by a single, common classification model. The co-cluster assignments along with the classification models for the co-clusters can be used to predict the class labels for missing customer-product combinations.

Formally, let  X  be a mapping from the m rows to the k row clusters and  X  be a mapping from the n columns to the l column clusters. We want to find a co-clustering defined by (  X ,  X  ) and associated set of classification models {  X  gh } that minimize the following objective function
X where z uv is the original value (class label) in row u ,column v of the matrix, with associated weight w uv .Here  X  gh de-notes the vector of coefficients of the model associated with the co-cluster that the cell value z uv is assigned to. Since the weights for the missing z uv values are 0, the objective function essentially ignores them and is simply the log loss summed only over all the known elements of matrix Z .Min-imizing this objective function is equivalent to maximizing the log-likelihood of the data.
A co-clustering (  X ,  X  ), that reduces the cost function (1) can be obtained by a simple iterative algorithm. Since the objective function is the log loss summed over all the ele-ments of the matrix, it can be expressed as a sum of row or column losses. If row u is assigned to row cluster g (i.e.  X  ( u )= g ), the row error is Since any missing values in the row u will have a weight 0, the error E u ( g ) is effectively computed only over the known values in row u . For a given column clustering and model parameter sets {  X  gh } , the best choice of the row cluster assignment for row u is the g that minimizes this error, i.e., Each row is hence assigned to the row cluster that minimizes the row error. A similar approach is used to (re)-assign columns to column clusters. Such row and column cluster updates hence decrease the objective function and improve the clustering solution. Note that updating column cluster assignments could cause the best row assignments to change and vice versa. Thus optionally, the row and column clus-ter reassignment steps can be repeated several times and in arbitrary order until both row and column cluster member-ships converge.

Given the current row and column cluster assignments, the co-cluster models need to be updated, i.e. the co-efficient vector  X  has to be updated for each co-cluster. To update the model for a row cluster g of size r and column cluster h of size c , train a logistic regression model with the r  X  ues within the co-cluster, weighted by their corresponding weight values. The missing values present in the co-cluster have weights of 0 and are essentially ignored. The logis-tic regression model is hence trained using only the known training samples ( x uv ,z uv ). In the more general case of arbi-trary valued weights, this step involves updating a weighted logistic regression model [16] rather than a simple logistic regression model. The output will be an updated vector  X  gh of coefficients that minimizes the model log loss given by The model update step is hence guaranteed to decrease the objective function (1).

The resulting algorithm is a simple iterative algorithm de-scribed in Figure 1. Step 1 minimizes the objective function due to the property of logistic regression, steps 2(a) and 2(b) directly minimize the objective function. The objec-tive function hence decreases at every iteration. Since this function is bounded from below by zero, the algorithm is guaranteed to converge to a local minimum. This algorithm could also be extended to classification models other than logistic regression. In this case the loss function and the up-date models step will be modified, but the overall approach will still be the same.

Predicting missing class labels: After the co-cluster assignments and the co-clusterwise classification models are obtained by the algorithm, the missing class labels can be predicted easily. Let z uv be a missing cell value that has been assigned to row cluster g and column cluster h . x uv the vector of attributes of row u and column v and  X  gh rep-resents the model parameters of the logistic regression model of the assigned co-cluster. The logistic regression model is used to obtain the probability of z uv of belonging to the positive class as follows A suitable threshold t is used to convert the probabilities into class labels i.e. z uv =1if P ( z uv =1) &gt;t,z uv  X  1otherwise.
In the regression setting, Z is an m  X  n matrix of  X  X us-tomers X  and  X  X roducts X , with cells representing the cor-responding customer-product preference values, ratings or choice probabilities. Here we assume the generative model to be a linear model, where the preference value z ij  X  R modeled as a linear combination of the corresponding cus-tomer and product attributes. The preference value is esti-mated as  X  z ij =  X  0 +  X  c T C i +  X  p T P j . Similar to the problem definition in section 3, the aim is to simultaneously cluster the customers and products into a grid of k row clusters and l column clusters, such that preference values within each co-cluster have similar linear models and can be represented by a single common model. We want to find a co-clustering defined by (  X ,  X  ) and the associated k  X  l regression models that minimize the following objective function where  X  z uv =  X  gh T x uv . The only difference here as com-pared to the classification case is the loss function, which is now squared loss rather than log loss. A co-clustering (  X ,  X  ), Algorithm
Input: Z m  X  n , W m  X  n , C =[ C 1 .. C m ], P =[ P 1 .. P
Output: Co-clustering (  X ,  X  ) and co-cluster models  X   X  X  1. Begin with a random co-clustering (  X ,  X  ) 2. Repeat 3. Update co-cluster models 4. for g =1to k do 5. for h =1to l do 6. Train a logistic regression model with 7. all the training samples ( x uv ,z uv )in 8. co-cluster ( g, h ), with associated weights 9. w uv , to obtain an updated  X  gh . 10. end for 11. end for 12. Update  X  -assign each row to the 13. row cluster that minimizes the row error 14. for u =1to m do 15.  X  ( u )= argmin g 16. exp (  X  z uv  X  gh T x uv )) 17. end for 18. Update  X  -assign each column to the 19. column cluster that minimizes the column error 20. for v =1to n do 21.  X  ( v )= argmin h 22. exp (  X  z uv  X  gh T x uv )) 23. end for 24. Optional: repeat steps 2(a) and 2(b) 25. until convergence
Until Convergence 24. return (  X ,  X  )and  X   X  X  Figure 1: Pseudo-code for simultaneous co-clustering and classification that minimizes the objective function can be obtained by an algorithm similar to the one described in section 4. The clus-ter reassignment steps assign each row or column to the row or column cluster that minimizes the row or column error.
The model for row cluster g of size r and column cluster h of size c is updated by finding the  X  that minimizes In case of 0/1 weights, this is equivalent to ignoring miss-ing values and updating the  X  for each co-cluster by least squares regression using only the non-missing (training) val-ues within the co-cluster. In case of a general set of weights, the  X  is a solution to a weighted least squares problem. Least squares regression finds a solution for  X  that mini-mizes the sum of the squared e rrors between the original values and the predicted values. The model update step is hence guaranteed to decrease the objective function.
After the algorithm converges, the co-cluster assignments and co-clusterwise regression models can be used to pre-dict unknown customer-product preference values. A miss-ing value z uv is predicted as  X  z uv =  X  gh T x uv .
The simultaneous co-clustering and prediction approach constructs k  X  l independent models, one per co-cluster, re-quiring a total of (1 + | C | + | P | )  X  kl parameters, where and | P | are the number of customer and product attributes respectively. This model will have many parameters for large values of k and l and may overfit in cases where training data is limited. The other extreme is a single prediction model for all the data, with (1 + | C | + | P | ) parameters, which might not be adequate. We propose an intermediate approach, which constructs k  X  l models but with smoothing or regu-larization achieved by sharing parameters across certain sets of models. The co-cluster models are constructed in such a way that the customer coefficients of all models for the same row cluster and the product coefficients of all models for the same column cluster are constrained to be identical. This reduced setting has (1 + | C | )  X  k +(1+ | P | )  X  l parameters, which will be considerably lower than (1 + | C | + | P | ) large k and l . We now describe how the model parameters can be obtained for the regression problem.
 If z uv is the original value in row u ,column v of the matrix Z that is assigned to row cluster g and column cluster h ,the predicted value  X  z uv is now given by where  X  g c 0 and  X  h p 0 are the customer and product intercepts and  X  g c and  X  h p are the customer and product coefficient vec-tors for the row cluster g and column cluster h respectively.
We still want to find a co-clustering defined by (  X ,  X  )and associated regression models that minimize the objective function (2). The row and column cluster assignment steps remain the same. The update models step now involves solv-ing a constrained optimization problem. Instead of updating k  X  l linear models independently, this step now updates the k row cluster models, such that the product coefficients are fixed, and the customer coefficients are updated, and then the l column cluster models, in which the customer coeffi-cients are fixed and the product coefficients are updated.
To update the model for row cluster g with r rows and n columns solve where  X  represents elementwise multiplication. X c is an ( r  X  n )  X  (1 + | C | ) matrix of the customer attributes, with the first column set to 1 for the intercept. The response variable y is a vector with r  X  n elements, given by where z is a vector of all the r  X  n preference values in the row cluster g with associated weights w ,[  X  h p 0 ,  X  h p T ]isavector of the product coefficients of the corresponding column clus-ters and X p is a matrix of size ( r  X  n )  X  (1+ | P | )representing the product attributes corresponding to the preference val-ues. The column cluster models are updated similarly. This update ensures that all the customer coefficients of models within the same row cluster and the product coefficients of models within the same column cluster are updated simul-taneously and are identical.
The algorithm described in section 4 was first evaluated on a number of synthetic datasets. We used synthetic data for experimentation as an initial sanity check before working with real data. These experiments also indicate the amount of improvement localized classification models provide when the model assumptions match the generative model for the data. The synthetic dataset is created by generating k  X  blocks of data, each corresponding to a true cluster of class labels generated by a logistic regression model. Each block has a different logistic regression generative model, with its coefficient vector (  X  ) set randomly. The blocks are then appended together in the form of a grid to form a data matrix, whose rows and columns are then randomly shuffled. To assign a class label to a cell z ij within each block, we begin by taking a linear combination of randomly generated customer and product attributes with the co-efficient vector y ij =  X  T x ij . We then add random Gaussian noise with variance  X  2 to all the y values. We obtain the probability of a cell belonging to the positive class as P ( z ij =1)= 1 1+ A threshold of 0.5 is used to convert the probabilities into class labels.

Table 1 describes the synthetic datasets that were used for experimentation. Dataset 1 and 2 are very similar, dataset 2 has more noise and hence a weaker relationship with the un-derlying generative model as compared to dataset 1. Dataset 3 and 4 are larger, also with a substantial amount of noise. The datasets along with details of their generative models can be accessed at http://www.ece.utexas.edu/  X  deodhar/ modelCCData .
We evaluate the ability of co-clustering with clusterwise classification models (Model CC) to classify unknown ma-trix values in the synthetic datasets. The data is split as 90% training and 10% test (missing) and the technique in section 4 is used to predict the class label of the missing values. The predicted labels are compared to the true labels and the classification quality is evaluated using precision, recall, F-measure and classification error. We compare this approach to the partitional co-clustering algorithm, Breg-man co-clustering [1], which uses only the matrix Z with-out any attribute information. The Bregman co-clustering algorithm is very flexible and can work with several dis-tance measures and co-cluster definitions. The special case of Bregman co-clustering that we compare with uses squared Euclidean distance as the distance measure and tries to find uniform co-clusters that minimize the distance of the data points within the co-cluster to the co-cluster mean 3 ,since this case best matches the data generation process.
In order to apply co-clustering (CC) to this problem, in matrix Z , we encode positive class labels by the value 1 and negative by 0. The co-clustering algorithm approximates the cell values within each co-cluster by the co-cluster mean  X  gh . If a missing cell z ij is assigned to row cluster g and column cluster h ,withco-clustermean  X  gh , we assign a class label to z ij using the rule z ij =1if  X  gh &gt; threshold , z ij =  X  1 otherwise. If the threshold is selected to be 0.5 this rule can be interpreted as assigning a missing cell the majority class label within its co-cluster. We also compare our approach with a single logistic regression classification model (Global Model), which is Model CC with k =1and l =1.

Table 2 displays the precision, recall, F-measure and clas-sification error for simultaneous co-clustering and classifica-tion (Model CC), co-clustering (CC) and a single logistic regression model (Global Model). The results are averaged over 5 random 90-10% splits of the data. The values in parentheses are the standard e rrors. The threshold is set to 0.5 for all these experiments since the same threshold was used to obtain class labels from probability values while gen-erating the synthetic data. One can observe that on all the synthetic datasets Model CC does significantly better than CC and Global Model in terms of both the F-measure and the classification error.

On these datasets we also evaluate the ability of the si-multaneous co-clustering and classification algorithm to re-construct the original data matrix. We find that on all the datasets this approach is consistently able to recover a close approximation of the original data matrix. Additionally, the cluster assignments made by the algorithm closely match the true underlying cluster labels.
This application deals with data based on course choices made by masters students at a large Midwestern University. The objective is to use the information of previous known course choices of students to predict unknown choices. These predictions can be used to recommend the right courses for students to take in the future. The data includes a matrix of students vs. courses with class labels = 1 if the student took the course and -1 otherwise. Each student has attributes in-cluding the student X  X  career aspiration and undergraduate degree. The course attributes include the department offer-ing the course, the course evaluation score and a binary vari-able indicating whether the course is quantitative. Around 25% of the student course choices are positive and the rest negative.
In order to test the classification capability of Model CC on this problem, the data is split as 90% training and 10% test and the class labels in the test set, i.e. the unknown student-course choices, are predicted using the co-cluster models. Results are obtained by averaging over 10 random 90-10% data splits. We compare the Model CC results with CC and Global Model. For assigning class labels to the miss-ing matrix entries we use a thre shold that is varied from 0.1 to 0.9 to get a range of precision-recall tradeoffs.
This corresponds to scheme 2 of the Bregman co-clustering algorithm [1] with squared Euclidean distance
Figures 2(a), 2(b), 2(c) display the precision-recall curves, the F-measure and the classification error of the 3 algorithms at different values of the threshold. Beyond a certain thresh-old, both CC and Global Model classify all the data points as belonging to the negative class, causing the F-measure to be undefined. Such points are excluded from the Precision-Recall curve and the F-measure plot.
 Model CC is significantly better than CC and Global Model in terms of precision and recall as can be seen in the Precision-Recall curves and hence its F-measure is con-sistently better than the other approaches at all values of the threshold. The classification error of Model CC is also lower than CC and Global Model. At threshold value 0.2 how-ever, CC has a lower error as it has a much smaller number of false positives.
The algorithm described in section 5 was first evaluated on a number of synthetic datasets. Each synthetic dataset is created by generating k  X  l blocks of data, each correspond-ing to a true co-cluster of real values generated by a linear regression model. Each block has a different linear regres-sion model, with its co-efficient vector  X  set randomly. The blocks are then appended together in the form of a grid to form the data matrix Z . The cell value z ij within each block is obtained by taking a linear combination of randomly gen-erated customer and product attributes with the co-efficient vector i.e. z ij =  X  T x ij . We then add random gaussian noise to all the values in Z . Due to the added noise, the cluster-wise linear models do not have a perfect fit. We quantify the linear relationships existing in the noisy data in terms of the average R 2 of the linear models, where R 2 of a linear model is a measure for assessing the model fit.

Table 3 describes the synthetic datasets 4 that were used for experimentation. Dataset 1 and 2 are very similar, dataset 2 has more noise and a weaker linear relationship as com-pared to dataset 1. Dataset 3 is larger, also with a substan-tial amount of noise. Dataset 5 is similar to dataset 4, but 0.1% of the data is perturbed to create outliers with large positive values.
The data is split as 90% training and 10% test (miss-ing) and the technique in section 5 is used to predict the missing values. The quality of the predictions is compared using mean squared error. Table 4 displays the prediction errors for a single linear regression model (Global Model), co-clustering, where no attribute information is used (CC),
Datasets available at http://www.ece.utexas.edu/ deodhar/modelCCData co-clustering with clusterwise models (Model CC) and the reduced parameter model (Reduced Model). The results are averaged over 5 random 90-10% splits of the data. Training Err. is the mean squared error on the training data and Test Err. is the mean squared error on the test data. The values in parentheses are the standard errors. Avg. R 2 is the average R 2 of the linear regression models constructed on the training data. One can observe that on the synthetic datasets Model CC does significantly better than the other approaches when the noise level is reasonable and there are no outliers (datasets 1-4), since the generative model of the data matches most closely with the data model assumed by Model CC. Under these conditions Reduced CC also does better than CC and Global Model but not as well as Model CC. However, in the presence of outliers (dataset 5) Model CC tends to overfit and Reduced CC does slightly better. On all the datasets, the average R 2 of the models recon-structed by Model CC is very close to that of the original models, whereas if we try to fit a single linear model its R is substantially lower.

Figure 3 shows the plot of the predicted matrix values vs the actual values on synthetic dataset 1. For perfect prediction the plot will be a straight line at 45 degrees. The plot for simultaneous co-clustering and regression is closer to the ideal than that for co-clustering. In case of co-clustering the 6 co-cluster means are used to predict all the missing values and hence the predicted values are one of the 6 means. In simultaneous co-clustering and regression, however, even though the set of linear co-efficients is common for all the values in a co-cluster, each predicted value could be different due to different row and column attributes.

Additionally, we find that the simultaneous co-clustering and regression approach does well at reconstructing the orig-inal data and recovering the original clusters even at high levels of added noise.
We applied the simultaneous co-clustering and regression approach to a challenging marketing application. Given pur-chase information for a set of customers and products along with customer and product attributes, we simultaneously clustered customers and products and used the co-clustering solution to predict unknown customer-product purchase in-formation. The obtained co-clusters also provide informa-tion about customer segments in the market and equivalent product groups, achieving simultaneous market segmenta-tion and structure, an important problem in marketing re-search [9]. The dataset that we use is the publicly available ERIM dataset 5 consisting of household panel data collected by A.C. Nielsen, which is well known in the marketing com-munity and has been used by several researchers [14, 15, 22]. This dataset has purchase information for six product categories over a period of 3 years from 1985-1988 for house-holds in Sioux Falls, South Dakota. The dataset includes household demographics and product characteristics.
URL: http://www.gsb.uchicago.edu/kilts/research/ db/erim/
The data preprocessing steps we took are similar to the data selection procedure by Seetharam et al. [22]. We have 6 product categories (ketchup, tuna, sugar, tissue, margarine and peanut butter) with a total of 121 products. Brands with very low market share in each product category are omitted. We select households that made at least 2 pur-chases in each product category, resulting in a set of 1714 households. We select 6 household attributes -income, num-ber of residents, male head employed, female head employed, total visits and total expense and 3 product attributes -mar-ket share, price, number of times the product was advertised. The data can be represented by a data matrix of households and brands where the cell values are the number of units of a brand purchased by a household, aggregated over the time the household was tracked. The number of units purchased can be used as an indicator of brand preference.
The data matrix is extremely sparse, with 74.86% of the values being 0. The distribution of the number of units purchased is very skewed. 99.12% of the values are below 20, while the remaining values are very large and range upto around 200. These few, large values that form the tail in the histogram of the matrix entries, can be considered as outliers with respect to the rest of the values.
The dimensions (items) in this application are products from 6 different product categories. The product attributes such as price and extent of advertising could vary from one category to another. When we construct a linear model for a co-cluster we weigh the attributes of all the products in the co-cluster by the same set of co-efficients. However, the products in the co-cluster could be from different categories with very different ranges of attribute values. We hence need to standardize the product attributes to make them com-parable across categories. We transform each product at-tribute value a to a = a  X   X  c  X  c ,where  X  c and  X  c are the mean and standard deviation within the corresponding product category c . This problem does not arise in case of the cus-tomer attributes since they are relatively comparable. The matrix cell values, which are the number of units purchased could also be very different across categories and have to be standardized. The cell values z ij within each sub-matrix of all the products belonging to a specific product category the mean and standard deviation of the all the values in the sub-matrix. Since the standardization of the data is a linear transformation, the dataset properties described in section 8.2.1 continue to hold.
Table 5 displays the mean squared error of the approx-imated data matrix obtained by the different algorithms on the entire standardized dataset, averaged over 10 runs.  X  X ow Clustering X  is Model CC with the number of column clusters set to 1 and  X  X olumn Clustering X  is Model CC with the number of row clusters set to 1. Note that Model CC obtains the best reconstruction of the original matrix as compared to the other approaches in terms of MSE (mean squared error). Table 5 also shows the average R 2 of the linear models constructed within each co-cluster. The R 2 values are actually quite low, indicating that a strong lin-ear relationship does not really exist in the data, which is to the disadvantage of the simultaneous co-clustering and regression algorithm.
The prediction error of the different approaches on this problem is computed by averaging over 10 random 90-10% training and test data splits. Here we additionally compare with a two-step sequential approach (Cluster Models) that first clusters the customers based on their attributes and Table 5: Reconstruction error on entire ERIM dataset. then fits regression models in each customer cluster. Table 6 shows the training error (Trai ning Err.), the t est set error (Test Err.) on the standardized data and the test set error on the original, unstandardized dataset obtained by back transforming the standardized data (Test Err. Original).
As mentioned in section 8.2.1 the data is skewed with a few very large outliers. In the presence of outliers the cluster-wise models overfit the training data and do not generalize well to the test data. Model CC is the most susceptible to overfitting since it is the most complex and involves the most number of parameters. Model CC does better than Global Model and Cluster Models but slightly worse than CC. Re-duced Model does better than Model CC in this scenario since it has fewer parameters and a simpler overall model, which generalizes better. Reduced Model does slightly bet-ter than CC as well, indicating that using the attribute in-formation in the prediction process helps. However, this improvement is small, which can be explained by the fact that the data does not show a very strong linear relation.
Linear  X  X east squares X  regression is very sensitive to out-liers and a few outliers could skew the model results. Hence on this dataset, for a fair comparison of linear model based techniques with respect to prediction of missing values, we need some way of dealing with outliers. It is unreasonable for a model based on linear regression to capture both small as well as extremely large values simultaneously and a more suitable approach would be to separate out these two very different sets of values and model them independently. A threshold of 20 for the number of units purchased was used to separate the bulk of the matrix entries (99.12%) from the tail of high values. This can easily be handled by the co-clustering algorithm by setting the weight of the outlier points to 0. We now focus on the prediction problem and the model for the bulk of the entries, the results of which are illustrated in Table 7.

Results on the low valued matrix entries: Table 7 shows the training mean squared error and the test set error for the different approaches on this problem. Model CC and Reduced Model do significantly better than Global Model and Cluster Models on the test set. Model CC and Reduced Model also do slightly better than CC. Avg. R 2 is the average R 2 of the regression models. One can observe that the average R 2 in case of Model CC is better than that of Global Model, Cluster Models and Reduced Model.
Our complete model for the prediction problem consists of the model constructed for the bulk of the matrix entries as described above and a linear model for the outliers. A classifier is trained to appropriately select one of the two models to predict each unknown matrix value. An alterna-tive way of dealing with outliers is to reduce the influence of the extremely high values on the constructed models, al-lowing them to generalize better. This is achieved by giving high valued matrix entries a very small fixed weight, en-abling the linear models to focus on the bulk of the values and rather than fit a few high values. Through experiments conducted for both these cases [5] , we observe that here as well, Model CC and Reduced Model do better than the other approaches.

Summary of Empirical Studies: Based on the re-sults in Sections 7 and 8 we observe that the simultane-ous co-clustering and prediction approach holds promise for both classification and regression problems, at least for the datasets examined so far. One should be able to further improve the results by using alternative prediction mod-els within each co-cluster that more closely conform to the data characteristics. This is particularly true for the ERIM dataset which is known to have significant outliers, but we still used linear least squares regression as it is widely adopted and understood. Note that overfitting of the models to out-liers is addressed to some extent by the reduced parameter model. Further improvements can be achieved by using a more robust error func tion rather than squared error [11]. Moreover, non-linear models would help because the limi-tations of linear models for this dataset is quite evident by the low R 2 values obtained by several researchers who pre-viously applied such models to ERIM.
Simultaneous co-clustering and modeling is a promising framework that generalizes co-clustering, collaborative fil-tering and traditional segment-wise modeling. Note that this approach is not limited to the algorithms presented in Sections 4 and 5, but forms a broad framework for solv-ing difficult classification and regression problems in general. The logistic regression models, for instance, could easily be replaced by other classifiers by modifying the objective func-tion to optimize the corresponding loss function.
While this paper concentrated on synthetic and marketing data for illustration, there are many other domains charac-terized by data matrices supplemented by annotated row and column entities. For example, this approach can be used to analyze microarray data with gene and experiment annotations, social network settings with sets of attributes attached to persons (rows) and relationships (columns), and clustering of web documents annotated by link as well as semantic information. It will be worthwhile to investigate specific instances of this framework (with specific choices of the co-clustering model used as well as of the classi-fier/predictor used) in terms of their suitability for different problems within this wide range of application domains.
The research was supported by NSF grants IIS 0325116 and IIS 0307792. We would like to thank Prof. McAlister and Andrea Godfrey of the McCombs School of Business at the University of Texas at Austin for the MBA dataset and insightful discussions. [1] A. Banerjee, I. Dhillon, J. Ghosh, S. Merugu, and [2] T. Baumann and A. Germond. Application of the [3] Y. Cheng and G. M. Church. Biclustering of [4] H. Cho, I. S. Dhillon, Y. Guan, and S. Sra. Minimum [5] M. Deodhar and J. Ghosh. A framework for [6] I. Dhillon, S. Mallela, and D. Modha.
 [7] M. Djukanovic, B. Babic, D. Sobajic, and Y. Pao. [8] T. George and S. Merugu. A scalable collaborative [9] R. Grover and V. Srinivasan. A simultaneous approach [10] J. Herlocker, J. Konsta n, A. Borchers, and J. Riedl. [11] P. J. Huber. Robust Statistics . Wiley, New York, 1981. [12] M. Jordan, R. Jacobs, S. J. Nowlan, and G. E. [13] R. Jornsten and B. Yu. Simultaneous gene clustering [14] B. Kim and P. Rossi. Purchase frequency, sample [15] B. Kim and M. Sullivan. The effect of parent brand [16] W. Lee and B. Liu. Learning with positive and [17] X. Liu, A. Krishnan, and A. Mondry. An [18] L. Lokmic and K. A. Smith. Cash flow forecasting [19] S. C. Madeira and A. L. Oliveira. Biclustering [20] K. Oh and I. Han. An intelligent clustering forecasting [21] V. Ramamurti and J. Ghosh. On the use of localized [22] P. Seetharaman, A. Ainslie, and P. Chintagunta. [23] A. Sfetsos and C. Siriopoulos. Time series forecasting [24] A. Sharkey. On combining artificial neural networks. [25] M. Wedel and J. Steenkamp. A clusterwise regression [26] Y. Ying, F. Feinberg, and M. Wedel. Leveraging [27] S. Zhang, D. Neagu, and C. Balescu. Refinement of
