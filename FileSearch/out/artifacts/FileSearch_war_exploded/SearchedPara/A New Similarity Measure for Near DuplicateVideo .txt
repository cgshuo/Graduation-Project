 Searching for the near-duplicates of a given video clip is an important research issue in content-based video search. Co nsider an application of NDVC detection in TV broadcast monitoring. A company that contracts TV stations for certain commercials would like to contract a market survey company to monitor whether their commercials are actually broadca sted as contracted, and how much their commercials has been edited. While the applications of NDVC detection have become widespread, effective NDVC detection approaches are high demanded to handle this task. Defining a suitable similarity measure for detecting similar videos is the first step towards effective NDVC detection.

A video is defined as a sequence of frames which represent high dimensional feature vectors of specific images over a particular time. NDVCs are those from the same original video source but possibly compressed at different qualities, reformatted to different sizes and frame-rates, or undergone different editing in either spatial or temporal domain. In NDVC detection, two factors, information loss and computation cost , are required to be considered, i.e., the information loss is minimized in the similarity measure, and the search is fast enough. Due to the complexity of video data, using original data to compare clips is not ap-preciable for large data set. To address this problem, an appropriate way is to represent each video in compact summari es, on which the inter-video similar-ity is measured. Although many approaches have been proposed to further the response of video matching, they suffer from the drawback of information loss, thus producing undesirable search results.

A typical approach for video matching is measuring the similarity by the num-ber of similar frames [4,9]. Meanwhile, m any compact representations are used to further the similarity search processing. A common approach is to extract several key frames from the segmented vi deo shots, and perform the similarity match between two videos by comparing the key frame sets of them [11]. How-ever, matching videos over the selected key frames incurs a heavy information loss, and also neglects the sequence information of videos, thus producing poor query results. In [5], each video is repr esented by a sequence of single values, each of which describes the change in color from one image to the next. Video matching proceeds by using local alignment to find sequence of similar values in video clips. This approach is robust to color variance. However, it can not discriminate the similarity between near-duplicates with a desirable ranking.
In this paper, we believe NDVCs have similar sequence context, and also those with both similar sequence context and similar visual features are more similar. Based on this, we propose a new similarity measures that takes the temporal order, inter-frame similarity and sequence context into consideration. To improve the efficiency of the matchin g, the effective estimation of it based on the symbolization and probability is utilized. The main innovation of the proposed measure can be presented in three aspects. (1) We derive the traditional edit distance into video matching; (2) The sequence context is embedded into the measure to not only solve the problem of feature variation, but also effectively compensate the informati on loss caused by compact r epresentation; (3) With this complementary information compensation scheme, key symbols approach is employed. Experiments on real video show the promising results of our approach. Several related video matching approac hes [7,9,3] are proposed in recent years for effective video retrieval. However, they only consider part of properties of video data that can not capture the flexible similarity of NDVCs in a manner suitable to human perception. In [9,4], the inter-video similarity is measured by the number of similar frames shared by two videos. The distance between two videos is defined as the ratio of the number of similar frames shared by them to the total number of their frames. The main disadvantage of this approach is that, since each video is considered as a set of frames by this approach, it does not capture the information of sequence.
Taking temporal order into consideration, there are a wealth of papers [7,3,2,8] working on sequence matching. Edit distance variants [3,2,8] are the most robust approaches, since they preserve temporal order in a flexible manner. ERP is the first one proposed to combine the edit distance with L1 norm distance together. In [8], a symbolization representation has been proposed based on dimension-wise quantization, called vString . The real-valued feature values are mapped into some discrete classes. Each dimension of the feature is transformed to a symbol that represents a class. Accordingly, ea ch video is represented by a multidimen-sional video string. Then, vstring edit distance is utilized. This work also does not reduce the dimensionality of video features, thus the representation is not com-pact. Therefore, the existing edit distance variants incur the high computation cost. Moreover, in the real applications, visual features may be variant largely among the NDVCs due to the various forms of quality degradation or video editing, thus these features that use visual features directly are not workable.
To solve the problem of color shift, a Signature Alignment ( SA )basedvideo matching approach is proposed in [5]. Signature Alignment first transforms each frame into a single value sequence by computing the similarity from one image to the next. This approach uses the local context of a video and is robust to feature variation. However, in real applications, except for the cases of shot transition, neighboring frames are always very similar with each other. Therefore, the single videomatchingby Signature Alignment is not discriminative. In this section, we present a novel compl ementary information compensation scheme based video matching method, VED, that not only considers the tempo-ral and spatial information of sequences, but also the relationship of neighboring frames in videos, for NDVC detection. The visual feature based video similar-ity, ED, is first defined on original sequences. Then, the complexity reduction strategy based on video frame summarization and estimation is described in detail, including how a video sequence is compactly represented, and how the inter-video similarity is estimated on the summaries. Finally, VED is proposed by embedding video context information into ED measure. 3.1 Video Similarity Based on Spatial and Temporal Information Edit distance is widely used in string matching and pattern recognition. We ex-tend it to the inter-video similarity measure by redefining the match or mismatch of different frames. Given two video se quences, A and B, the edit distance be-tween them, ED(A,B), is the number of insertion, deletion or substitution opera-tions that are required to transform A into B. To formally define ED(A,B), it is crucial to decide whether two compared frames are matched or not by measuring the similarity of them. Thus, two suitable distance functions, for measuring the inter-frame similarity and the inter-sequence similarity respectively, are essential.
Generally, the distance between frames is defined by the Lp-norm in a d-dimensional space. Given a matching threshold, ,whethertwoelementsare matched or not is judged by the Lp-norm distance between them. If the Lp-norm distance between two frames is no more than , they are matched; otherwise, they are mismatched. Given two videos (m  X  n), the edit distance between them, ED(S 1 ,S 2 ), is defined as follows:
Here,  X  is the cost for substitution operation. If s m and f n are matched,  X  =0; otherwise  X  =1. The definition of ED is very similar to that of EDR, but more suitable to the similarity match between videos, since the comparison between high dimensional data is based on the distance over all dimensions rather than the difference over ea ch single dimension. 3.2 Video Similarity with Sequence Information When the edit distance variants like ERP, EDR, and our proposed ED are used for sequence matching, practically, two main problems are required to be solved. Since visual features are subjective to various forms of quality degradation, the visual features do not capture enough information. Also, frame based measure with these distance functions suffers from the high complexity of high dimen-sional distance computations. Although key frames and other video representa-tions are very effective for reducing the cost of measures, they incurs considerable information loss that reflects human pe rception for effective NDVC detection. A desirable approach for solving these two problems is to compensate the infor-mation loss, which are from the video repr esentation and the video recording as well, by using the context information of sequences. With this consideration, the distance between two videos is re-defined by embedding the sequence context difference between them.
 frames, the sequence context informati on is formed by the similarity between the neighboring key frames that can be represented as X c = &lt; x 1 ,..x i ...x n &gt; ,where x denotes the distance between the i th and (i-1) th key frame, and X c is called as the context vector of X. Suppose that X c and Y c are the context vectors of video X and Y respectively, we define the context difference between them as d(X c , Y ). Given two videos X and Y, the distance between them, VED, is defined as: where w 1 and w 2 are the weights of the visual feature difference and context difference in the distance function. VED considers the difference of visual fea-tures between two videos and that of their sequence relationship. This measure scheme compensates the information los s from video representations, thus key frames can be effectively utilized in th is measure to reduce the complexity of the measure significantly. To further sa ve the cost of video matching, the key frames can be summarized and symboli zed. Accordingly, VED is computed over the summarized symbol space. This part will be described in 3.3. 3.3 Complexity Reduction A major step in computing the VED bet ween two videos is to decide whether each frame pair is matched or not by the distance between them. The complexity of the frame distance computation is proportional to the dimension of the feature vector, which is usually quite high in video applications. For the video sequence matching, the number of frame distance computations is exponential to the length of them, usually several hundreds to thousands. Clearly, for the real video applications, reducing the complexity of video matching is a crucial task.
We propose two schemes, frame symbolization and key symbol representation, to reduce the complexity of video matching. The basic idea of frame symboliza-tion is to transform each video frame into a symbol, which is a process of aggres-sive dimensionality reduction. Then, key symbols are selected to represent the video clip, thus reducing the length of compared sequences. Turning a longer video sequence into a shorter symbol string, one may think that the match-ing can not work because of the severe information loss. However, this is not necessarily true, since the information can be fully compensa ted by the comple-mentary scheme which has been introdu ced in 3.2 and some effective strategies such as multi-symbol representation and optimal probability selection(T=0.5), whichhasbeendescribedin[10].
 Video representation. We have introduced the technique of frame symboliza-tion in [10]. As a video symbol sequence usually contains same symbols which occurs sequentially, in this paper, we rep resent the symbol sequence by selecting the key symbols at equal intervals. Beca use we only use the selected key frames of video sequences in the sequence match ing, the process of summarization and symbolization is not performed over the whole frame dataset, but only the se-lected key frames of each video sequences are utilized. Given a video dataset and avalve,  X  (0, each videos are selected first, and the vid eo symbolization is performed by first clustering over this key frame dataset and then mapping each key frame to its corresponding cluster id. To ensure the high similarity of frames in the same cluster, the maximal cluster radius that is equal to 2 is usually set very small.
With the traditional key frame selection methods, the similarity between two videos may vary due to the change of sequence lengthes. The increased difference of the sequence lengthes increases the d issimilarity between them, accordingly, producing the inaccurate measure results. Suppose that we have two videos of equal lengthes, A and B. After key frame s election and symbolization, they are transformed into (aba) and (a) respectively. Obviously, the dissimilarity between them is increased because they are transf ormed into symbol sequences of different lengthes. To simplify the issue of key frame selection and eliminate the effect of length changing, we select the key fram es by simply sampling video frames with equal intervals. As such, the ratio of the sequence lengthes is maintained.
Many clustering methods have been reviewed in [6]. We adopt a recursive 2-mean algorithm that recursively performs the binary clustering algorithm until the radius of the cluster is no more than . As such, a set of clusters, each containing similar frames, can be produ ced. We represent each cluster by a four-tuplet { id,O,r,n } ,where id is the cluster identifier; O is the cluster centre which indicates the position of the cluster in the original high dimensional space; r is the radius of boundary hypersphere of the cluster. n is the number of frames in the cluster. The information of the clusters kept in the cluster data set is used as a video dictionary to map a key frame into an id during the preprocessing procedure of the similarity query.

By looking up the video dictionary and representing each video key frame with its cluster id , each video is symbolized as a digital string which consists of the cluster id s of its key frames. With this approach, the similarity compari-son between query and video data is sim plified as the issue of string matching, and the comparison between each frame pair is transformed into that between clusters. Although this key symbol repr esentation may leak certain important information, this information loss can be effectively compensated by utilizing the context between neighboring key symbols in the video matching.
 Probability Measure on Clusters. As each summary obtained from video sequence symbolization is not only a symbol, but corresponds to a cluster having a set of frames, traditional string matching is not suitable for measuring the similarity of transformed sequences th at are series of cluster ids. A cluster id has two features: (1) one id represents a set of frames within a high dimensional space; (2) subspaces to different ids may have certain overlapping. Based on this, we proposed probability based approach for the inter-cluster comparison in [10], i.e., comparing two frames by the probability that neither of them falls into the intersection of the clusters to their ids. For two symbols, the similarity between them is constructed by a probability value, P  X  [0,1]. The value of P can be obtained by the probability function, which is defined as follows:
Here, | O i  X  O j | is the number of frames in cluster i but out of cluster j . |
O between different clusters. The data distributed in the small clusters is uniform, thus the | O i  X  O j | can be estimated by the ratio of the volume of the part of the cluster O i outside of the intersection to that of the whole cluster, which is shown as follows:
V( O i  X  O j ) is the volume of a hyper concavo-convex. When performing a sim-ilarity match on the symbols, for the purpose of maintaining more information of frames in the original high dimensional space, the inter-symbol distance is not determined by whether they are same or not, but by their probability distance. This probability value shows the extent of overlapping between two clusters and can be obtained before the sequence comparison. 4.1 Experimental Set-Up Our experiments are conducted on a real video collection which consists of 1083 real-world commercial videos captured from TV stations and recorded using the Virtual Dub at the PAL frame rate of 25fps [9]. Each video frame is compressed using PLCVideo Mjpegs and the resolution is 192  X  144 pixels. A video in the dataset is a 60s-clip, which is represented as a 32-dimensional feature vector in the RGB color space.

Six video clips are selected as queries. Th e selection criteria is that the selected clips are not near-duplicates with each other and, for each query, at least one near-duplicate can be found in the video collection. The major parameters and their default values used in the experiments are summarized in Table 1. We run experiments with different ,T, w 1 and w 2 , and the default values of them in Table 1 are chosen according to the best performance of them. 4.2 Evaluation Criteria The standard evaluation method in IR that has been used for evaluating the VideoQ system [1] is applied to measure th e effectiveness of the proposed video matching approach. The evaluation is based on two factors: P recision and Recall . Given a query Q, let rel be the set of the relevant video clips to the query and | rel | be the size of the set; let ret be the set of top 30 results returned by the system. P recision and Recall are defined as below:
For each query, we find its top 30 ranked nearest neighbors. The precision is calculated after each relevant clip is retrieved. If a relevant clip is not retrieved, its precision is 0.0. A precision -recall curve is then produced by measuring precisions at 11 evenly spaced recall points (0,...1.0). All precision values are then averaged together to get a single number for the performance of a query. The values are then averaged over all queries, leading t o the average precision of a search system.
Three sets of experiments are conducted to evaluate the effectiveness of pro-posed approach. Our objectives of this evaluation are: (1) to study the effect of the sequence context information compensation; (2) to study the effect of the number of key symbols selected; (3) to study the superiority of VED with the existing video matching approaches. 4.3 Effect of Information Compensation We performed experiments to evaluate the impact of context information com-pensationintermsof P recision and Recall during the similarity search by com-paring VED against ED. In this experiment, all parameters described in Table 1 are fixed as default values. Figure 2 show s the precision-recall curve, and Table 2 reports the average precision of these two measures.

From Figure 2 and Table 2, we found that VED approach achieves the much better average precision (0.9037) as well as the higher precision values at all recall levels, since sequence context are utili zed to effectively compensate the informa-tion loss, thus enhancing the quality of NDVC detection. Taking the average precision of each individual query into consideration, VED always outperforms ED, leading to much better average precision of the system. 4.4 Effect of Key Symbol Interval Then, we examine effectiveness of the VEDbyvaryingkeysymbolintervals from 10 to 50, with other parameters in Table 1 to default values. Figure 3 shows the average precision of VED at different key symbol interval levels. From this figure, it can be seen that, from 10 t o 20, the average precision of VED keeps steady due to the information compensati on of sequence context. Consequently, the information loss from key symbol representation affects the search results very slightly. When the key symbol interval reaches to 30, with the increasing of key symbol interval, the average precision degrades at an obvious rate for the sequence representation containing only few key symbols incurs too heavy information loss that can not be well compensated. 4.5 Comparison of VED and Existing Measures Having shown that the information loss originated from the video representation can be compensated effectively, and key symbols can be used to represent video sequences more compactly in VED under the limited symbol intervals, we will 10 also demonstrate that VED is more effective by comparing with the existing competitors, including ERP and Signature Alignment (SA). For each of these three approaches, the precision at each r ecall level and the average precision over each individual query and all queries are reported in Figure 4 and Table 3.
From Table 3 and Figure 4, we can see that VED has the best average precision and the best precision at each recall level, with the ERP following it, and the Signature Alignment performs much worse than the other two. This is caused by the effective complementa ry information compensation scheme in VED measure. Since ERP only captures the information of frame similarity and the alignment of different video sequences, the NDVCs having much visual feature variation can not be retrieved with this measure. While Signature Alignment with the relationship of neighboring frames considered neglects the visual features of each clip, the spatial information of each c lip can not be captur ed, thus leading to worse matching results. VED overcomes the weakness of ERP and Signature Alignment by introducing a complementary information compensation scheme into the measure, which produces gr eat improvement of effectiveness. To visualize the superiority of VED, we give the results of a query sample in Figure 5 and 6. To save the space, only the top 10 results, produced by VED and ERP, of the first query Q 1 that is the No 1 clip in the results are shown in the figures. Clearly, 10 correct results are obtained by VED, while ERP only finds 8 relevant clips with non-relevant clips o ccurring at position 7 and 10 respectively. Clearly, VED is more robust for NDVC detection. In this paper, we proposed a new video similarity measure, VED, which is based on a complementary information compensation scheme for NDVC detection. VED takes in consider not only the visual information of a video clip, but also the relationship of neighboring frames in video matching based on similarity. With this measure, compact video representation using frame symbolization and key symbols can be deployed effectively, and thus efficient similarity match is performed over the summaries. The extens ive experimental results have shown that the proposed measure is hi gh effective for NDVC detection.

