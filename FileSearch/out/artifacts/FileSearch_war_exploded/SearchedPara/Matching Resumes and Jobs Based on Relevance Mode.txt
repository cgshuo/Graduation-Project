 } We investigate the difficult problem of matching semi-struc-tured resumes and jobs in a large scale real-world collection. We compare standard approaches to Structured Relevance Models (SRM), an extension of relevance-based language model for modeling and retrieving semi-structured docu-ments. Preliminary experiments show that the SRM ap-proach achieved promising performance and performed bet-ter than typical unstructured relevance models.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval General Terms: Algorithms, Experimentation Keywords: Relevance Models, Resume, Job Matching
We are interested in finding resumes that are appropri-ate matches to a job description, where appropriate means that a prospective employer would be interested in reading the retrieved resumes. We carry out a series of experiments on a dataset consisting of over a million resumes, almost a quarter million job descriptions, and a large number of rele-vance judgments that indicate which resumes are potentially interesting for a particular job description.

Prospective employees or employers usually submit their resume or job information through online forms that contain many free text fields such as job title , biography , etc. This information is typically maintained by a relational database engine. An ideal system would retrieve candidate resumes for a job or a list of jobs potentially suitable for a candidate. However using a relational engine for this matching task will run into two major obstacles. First, many fields are input as free form text by users rather than a set of agreed upon keywords from a closed vocabulary. That means that the contents cannot be reliably predicted; the problem is more of a classic information retrieval one. A second obstacle is that many fields are missing: users often do not input all the fields in an online form. For example, in our collection, 23% of the resumes do not have a ResumeBody field and 90% are missing the Summary field.

Our primary approach for this problem is Structured Rele-vance Models (SRM) [3], a retrieval model for semi-structured documents based on the idea that plausible values for a given field could be inferred from the context provided by the other fields in the record. For instance, if two jobs have  X  X atabase Administrator X  in the JobTitle fields, it is likely that appropriate candidates X  resumes for both jobs should have  X  X QL server X  or  X  X ySQL X  in the ResumeBody fields. We will formally describe this matching resume/job task and then present different approaches for it. Then we will describe some preliminary experimental results by applying these different approaches.
Given a collection of semi-structured resumes R , a collec-tion of semi-structured jobs J , and some known matched resume/job pairs &lt; r , j &gt; . The task is to retrieve a list of related resumes for any existing or new job j , or retrieve a list of related jobs for any existing or new resume r . We will focus on the former this task here; the other direction could be done in similar way and has similar performance.
Our two baseline runs ignore the structure of the docu-ments. To do that, we strip the structure from the resume and job records, flattening the data by concatenating the free form text in all the fields. We use a query likelihood approach, with the flattened job record as the query and the flattened resumes as the documents. We call this sim-ple language modeling approach  X  X LM X . We expect its per-formance to be weak because it does not have any way to bridge the vocabulary divide between job descriptions and resumes X  X .g., the DB administrator and SQL server example above.

To address that problem, our second baseline run ignores the structure but leverages past judged pairs in a type of supervised query expansion. This approach is a variation of Relevance Models [2] where the relevance model is built from known relevant documents (resumes) rather than from highly ranked ones. We call this approach tRM for  X  X rue relevance model. X  It runs in three steps: (1) we run the flattened job record as a query against the flattened job collection, and retrieve a list of similar jobs; (2) we utilize the resumes are known (by our relevance judgments) to be related to those retrieved jobs, and build a relevance lan-guage model from them; and (3) we run the relevance model against the flattened resume collection and retrieve a list of similar resumes. Note that this approach has the opportu-nity to bring resume-specific language that is related to the job into the query.

Our final model is the SRM approach [3]. It uses relevance information in the same way that tRM does, but it also uses the structure of the fields as well as their inter-dependence. It follows roughly the same three steps, but operates slightly differently because of the multiple fields. For SRM we first run each field of a given job j as a query against the corre-sponding field of the semi-structured job collection J , and merge the field-specific retrieved jobs using weighted cross-entropy [1]. We retain only the top k most highly ranked jobs. As with tRM, we now have a set of jobs that are sim-ilar to the query job; in contrast to tRM we used the field structure of the jobs to find them. In our second step, we use the resumes known to be related to the retrieved jobs and build relevance models, but this time we build one model per field in the resumes. In the third step, we run each of those field-specific models as a query and then rank all resumes according to their similarity, again weighted cross-entropy.
The resume/job matching experiments are performed on a challenging large scale real-world semi-structured collection. Each resume or job is represented as a record that may be missing some fields X  information X  X .e. some fields are NULL. Fields can be numeric or textual. In total, the collection contains 1,276,573 resume records (spanning 90 fields, 12 of them textual), 206,393 job records (spanning 20 fields, 9 of them textual) and 1,820,420 resume/job pairs annotated by implicit feedback from job agents. Table 1 shows some statistics for resume fields.

In experiments we first select a set of 300 jobs that had 60-80 annotated matching resumes. We split that set into two halves, one of which is used for training (e.g., tuning the Dirichlet smoothing parameters) and the other half is used for testing. In addition, we split the set of resumes equally into training and test sub-collections. We used the train-ing resumes for building relevance models and searched for target resumes in the test resumes. When we incorporated structure for the SRM approach, we used the title and body fields from both resumes and jobs (even though they have the same name, the content is rarely similar).

Table 2 shows the performance of SRM against the two other approaches. We are matching 150 test jobs against the test resume collection. The upper half of Table 2 shows precision at fixed recall levels; the lower half shows preci-sion at different ranks. The %change column shows relative difference between SRM and tRM. The improved column shows the number of matches where SRM exceeded tRM vs. the number of matches where performance was differ-ent. Bold figures indicate statistically significant differences (according to the sign test with p &lt; 0 . 05).

The results show that a classic retrieval approach such as sLM performs very poorly for this task, suggesting that we cannot directly use text from job fields to find matching resumes. The Relevance Model approach achieves promis-ing performance by incorporating a form of true relevance feedback. However, when structure is also provided, SRM outperforms tRM, beating tRM X  X  mean average precision by almost 14%. R-precision and precision at 10 are improved by 17% and 19% respectively.

We note that performing this resume/job matching task Table 2: Performance of matching 150 test jobs to the test resume collection. Evaluation is based on retrieving 1000 resumes. Across all 150 test jobs, there a total of 5173 matched resumes.

Table 3: Counts broken down by P @10 ranges. on a large-scale real-world semi-structured database is very difficult. At 5 documents retrieved, the precision of SRM is less than 20% while on average there are 35 annotated train-ing resumes per job (half of the 60-80): that means that on average only 1 of the 35 relevant documents is found in the top five. To explore each test job X  X  matching result fur-ther, we categorized the 150 jobs into 3 groups according to precision at 10; the size of each group is shown in Table 3. For some jobs, the relevance-based approaches find more than 5 matched resumes in the top 10 listed (i.e., P@10 is more than a half). By looking into the text of some failed matching cases directly we observe that judgments based on implicit feedback are still not good enough. Although still more analysis is needed, these preliminary results demon-strate that supervised feedback and structure are promis-ing techniques for this difficult semi-structured documents matching task.
We are indebted to Monster Worldwide X  X  research lab for their continued support of this research. This work was also supported in part by the Center for Intelligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under contract number HR0011-06-C-0023. Any opinions, findings and conclusions or rec-ommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsors.
