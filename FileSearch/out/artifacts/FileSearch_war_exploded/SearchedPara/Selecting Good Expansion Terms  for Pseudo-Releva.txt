 Pseudo-relevance feedback assumes th at most frequent terms in the pseudo-feedback documents are useful for the retrieval. In this study, we re-examine this assumption and show that it does not hold in reality  X  many expansion terms identified in traditional approaches are indeed unrelated to the query and harmful to the retrieval. We also show that good expansion te rms cannot be distinguished from bad ones merely on their distributions in the feedback documents and in the whole collection. We then propose to integrate a term classification process to predict th e usefulness of expansion terms. Multiple additional features can be integrated in this process. Our experiments on three TREC collections show that retrieval effectiveness can be much improved when term classification is used. In addition, we also demonstrate that good terms should be identified directly according to their possible impact on the retrieval effectiveness, i.e. using supervised learning, instead of unsupervised learning. H.3.3 [Information Storage and Retrieval]: Retrieval models Design, Algorithm, Theory, Experimentation Pseudo-relevance feedback, Expans ion Term Classification, SVM, Language Models User queries are usually too short to describe the information need accurately. Many important terms can be absent from the query, leading to a poor coverage of the relevant documents. To solve this problem, query expansion has been widely used [9], [15], [21], [22]. Among all the approaches, pseu do-relevance feedback (PRF) exploiting the retrieval result has been the most effective [21]. The basic assumption of PRF is that the top-ranked documents in the discriminate relevant documents fro m irrelevant ones. In general, the expansion terms are extracted either according to the term distributions in the feedback documents (i.e. one tries to extract the most frequent terms); or according to the comparison between the term distributions in the feedback documents and in the whole document collection (i.e. to extract the most specific terms in the feedback documents). Several additional criteria have been proposed. For example, idf is widely used in vector space model [15]. Query length has been consid ered in [7] for the weighting of expansion terms. Some linguistic feat ures have been tested in [16]. However, few studies have directly examined whether the expansion terms extracted from ps eudo-feedback documents by the existing methods can indeed help retrieval. In general, one was concerned only with the global impact of a set of expansion terms on the retrieval effectiveness. A fundamental question often overlooked at is whether the expansion terms extracted are truly related to the query and are useful for IR. In fact, as we will s how in this paper, the assumption that most expansion terms extracted from the feedback documents are useful does not hold, even when the global retrieval effectiveness can be improved. Among the extracted terms, a non-negligible part is either unrelated to the query or is harmful, instead of helpful, to retrieval effectiven ess. So a crucial question is: how can we better select useful expansion terms from pseudo-feedback documents? In this study, we propose to use a supervised learning method for term selection. The term selection problem can be considered as a term classification problem  X  we try to separate good expansion terms from the others directly accor ding to their potential impact on the retrieval effectiveness. This method is different from the existing ones, which can typically be considered as an unsupervised learning. SVM [6], [20] will be us ed for term classification, which uses not only the term distribution criteria as in previous studies, but also several additional criteria such as term proximity. This approach proposed has at least the following advantages: 1) Expansion terms are no longer se lected merely based on term distributions and other criteria indirectly related to the retrieval effectiveness. It is done directly according to their possible impact on the retrieval effectiveness. We can expect the selected terms to have a higher impact on the effectiveness. 2) The term classification process can naturally integrate various criteria, and thus provides a framework for incorporating different sources of evidence. We evaluate our method on three TREC collections and compare it to the traditional approaches. The e xperimental results show that the retrieval effectiveness can be improved significantly when term classification is integrated. To our knowledge, this is the first attempt trying to investigate the direct impact on retrieval effectiveness of individual expansion terms in pseudo-relevance feedback. The remaining of the paper is organized as follows: Section 2 reviews some related work and the state-of-the-art approaches to query expansion. In section 3, we examine the PRF assumption used in the previous studies and s how that it does not hold in reality. Section 4 presents some experiments to investigate the potential usefulness of selecting good terms for expansion. Section 5 describes our term classification method and reports an evaluation of the classification process. The integration of the classification results into the PRF methods is desc ribed in Section 6. In section 7, we evaluate the resulting retrieval method with three TREC collections. Section 8 concludes this paper and suggests some avenues for future work. Pseudo-relevance feedback has been widely used in IR. It has been implemented in different retrieval models: vector space model [15], probabilistic model [13], and so on. Recently, the PRF principle has also been implemented within the language modeling framework. Since our work is also carried out using language modeling, we will review the related studies in this framework in more detail. The basic ranking function in language modeling uses KL-divergence as follows: where V is the vocabulary of the whole collection, and are respectively the query model and the document model. The document model has to be smoothed to solve the zero-probability problem. A commonly used smoothing method is Dirichlet smoothing [23]: of w within d , P ( w|C ) the probability of w in the whole collection C estimated with MLE (Maximum Likelihood Estimation), and the Dirichlet prior (set at 1,500 in our experiments). The query model describes the user X  X  information need. In most traditional approaches using lan guage modeling, this model is estimated with MLE without smooth ing. We denote this model by w P  X  . In general, this query model has a poor coverage of the relevant and useful terms, especi ally for short queries. Many terms probability in) the model. Pseudo-re levance feedback is often used to improve the query model. We mention two representative approaches here: relevance model and mixture model. The relevance model [8] assumes that a query term is generated by a relevance model ) | ( the relevance model without any re levance information. [8] thus exploits the top-ranked feedback documents by assuming them to be samples from the relevance model. The relevance model is then estimated as follows: Where F denotes the feedback documents. On the right side, the relevance model Applying Bayesian rule and making some simplifications, we obtain: That is, the probability of a term w in the relevance model is determined by its probability in the feedback documents (i.e. P ( w|D )) as well as the correspondence of the latter to the query (i.e. P ( Q|D )). The above relevance model is used to enhance the original query model by the following interpolation: where is the interpolation weight (set at 0.5 in our experiments). Notice that the above interpolation can also be implemented as document re-ranking in practice, in which only the top-ranked documents are re-ranked according to the relevance model. The mixture model [22] also tries to build a language model for the query topic from the feedback documents, but in a way different from the relevance model. It assumes that the query topic model to be extracted corresponds to the part that is the most distinctive from the whole document collection. This distinctive part is extracted as follows: Each feedback document is assumed to be generated by the topic model to be extracted and the collection model, and the EM algorithm [3] is used to extract the topic model so as to maximize the likelihood of the feedback documents. Then the topic model is combined with the original qu ery model by an interpolation similarly to the relevance model. Although the specific techniques used in the above two approaches are different, both assume that the strong terms contained in the feedback documents are related to the query and are useful to improve the retrieval effectiveness. In both cases, the strong terms are determined according to their distributions. The only difference is that the relevant model tries to extract the most frequent terms from the feedback documents (i.e. with a strong P ( w | D )), while the mixture model tries to extract those that are the most distinctive between the feedback documents and the general collection. These criteria have been generally used in other PRF approaches (e.g. [21]). Several additional criteria have been used to select terms related to the query. For example, [14] proposed the principle that the selected terms should have a higher probab ility in the relevant documents than in the irrelevant documents. For document filtering, term selection is more widely used in order to update the topic profile. For example, [24] extracted terms from true relevant and irrelevant documents to update the user profile (i.e. query) using the Rocchio method. Kwok et al. [7] also made use of the query length as well as the size of the vocabulary. Smeaton and Van Rijsbergen [16] examined the impact of determining expansion terms using minimal spanning tree and some simple linguistic analysis. Despite the large number of studie s, a crucial question that has not been directly examined is whether the expansion terms selected in a way or another are truly useful fo r the retrieval. One was usually concerned with the global impact of a set of expansion terms. Indeed, in many experiments, improvements in the retrieval effectiveness have been observed with PRF [8], [9], [19], [22]. This might suggest that most expansion te rms are useful. Is it really so in reality? We will examine this question in the next section. Notice that some studies (e.g. [11]) have tried to understand the effect of query expansion. Howeve r, these studies have examined the terms extracted from the whole collection instead of from the feedback documents. In addition, they also focused on the term distribution aspects. The general assumption behind PRF can be formulated as follows: Most frequent or distinctive terms in pseudo-relevance feedback documents are useful and th ey can improve the retrieval effectiveness when added into the query. To test this assumption, we will consider all the terms extracted from the feedback documents using the mixture model. We will test each of these terms in turn to see its impact on the retrieval effectiveness. The following score f unction is used to integrate an expansion term e : where t is a query term, ) | ( described in section 2, e is the expansion term under consideration, and w is its weight. The above expression is a simplified form of query expansion with a single term. In order to make the test simpler, the following simplifications are made: 1) An expansion term is assumed to act on the query independently from other expansion terms; 2) Each expansion term is added into the query with equal weight -the weight w is set at 0.01 or -0.01. In practice, an expansion term may act on the query in dependence with other terms, and their weights may be different. Despite these the expansion terms. Good expansion terms are those that improve the effectiveness when w is 0.01 and hurt the effectiveness when w is -0.01; bad expansion terms produce the opposite effect. Neutral expansion terms are those that produce similar effect when w is 0.01 or -0.01. Therefore we can generate three groups of expansion terms: good, bad and neutral. Ideally, we w ould like to use only good expansion terms to expand queries. Let us describe the identification of the three groups of terms in more detail. Suppose MAP(q) and MAP(qU e) are respectively the MAP of the original query a nd expanded query (expanded with e ). We measure the performance change due to e by the ratio 0.005 i.e., good and bad expansion terms should produce a performance change such that |chg(e)|&gt;0.005 . In addition to the above performance change, we also assume that a term appearing less than 3 times in the feedback documents is not an important expansion term. This a llows us to filter out some noise. The above identification produces a desired result for term classification. Now, we will examine whether the candidate expansion terms proposed by the mixture model are good terms. Our verification is made on thre e TREC collections: AP, WSJ and Disk4&amp;5. The characteristics of these collections are described in Section 7.1. We consider 150 queries for each collection and 80 expansions with the largest probabilities for each query. The following table shows the proportion of good, bad and neutral terms for all the queries in each collection. As we can see, only less than 18% of the expansion terms used in the mixture model are good terms in all the three collections. The proportion of bad terms is higher. This shows that the expansion process indeed added more bad terms than good ones. We also notice from Table 1 that a large proportion of the expansion terms are neutral terms, which have little impact on the retrieval effectiveness. Althoug h this part of the terms does necessarily not hurt retrieval, adding them into the query would produce a long query and thus a heavier query traffic (longer evaluation time). It is then desirable to remove these terms, too. The above analysis clearly shows that the term selection process used in the mixture model is insufficient. Similar phenomenon is observed on the relevance model and can be generalized to all the methods exploiting the same criteria . This suggests that the term selection criteria used -term distributions in the feedback documents and in the whole docume nt collection, is insufficient. This also indicates that good and bad expansion terms may have similar distributions because the mixture model, which exploits the difference of term distribution between the feedback documents and the collection, has failed to distinguish them. To illustrate the last point, let us look at the distribution of the expansion terms selected with the mixture model for TREC query #51  X  X irbus subsidies X . In Figure 1, we place the top 80 expansion terms with the largest probabilities in a two-dimensional space  X  one dimension represents the loga rithm of its probability in the pseudo-relevant documents and another dimension represents that in the whole collection. To make the illustration easier, a simple normalization is made so that the final value will be in the range [0, 1]. Figure 1 shows the distribution of the three groups of expansion terms. We can observe that the neutral terms are somehow isolated from the good and the bad terms to some extent (on the lower-right corner), but the good expansion terms are intertwined with the bad expansion terms. This figure illustrates the difficulty to separate good and bad expansion terms according to term distributions solely. It is then desirable to use additional criteria to better select useful expansion terms. Before proposing an approach to select good terms, let us first examine the possible impact with a good term selection process. Let us assume an oracle classifier th at separate correctly good, bad and neutral expansion terms as determined in Section 3. In this experiment, we will only keep the good expansion terms for each query. All the good terms are integrated into the new query model in the same way as either relevance model or mixture model. Table 2 shows the MAP (Mean Average Precision) for the top 1000 results with the original query model ( LM ), the expanded query models by the relevance model ( REL ) and by the mixture model ( MIX ), as well as by the oracle expansion terms ( REL+Oracle and MIX+Oracle ). The superscript,  X  L  X ,  X  R  X  and  X  M  X  indicates that the improvement over LM, REL and MIX is statistically significant at p&lt;0.05. 
Figure 1. Distribution of the expansion term s for  X  X irbus subsidies X  in the We can see that the retrieval effectiveness can be much improved if term classification is done perfect ly. The oracle expansion terms can generally improve the MAP of the relevance model and the mixture model by 18-30%. This shows the usefulness of correctly classifying the expansion terms and the high potential of improving the retrieval effectiveness by a good term classification. The MAP obtained with the oracle expansion terms represents the upper bound retrieval effectiveness we can expect to obtain using pseudo-relevance feedback. Our problem now is to develop an effective method to correctly classify the expansion terms. Any classifier can be used for term classification. Here, we use SVM. More specifically, we use the SVM variant C-SVM [2] because of its effectiveness and simplicity [20]. Several kernel functions can be used in SVM. We use the radial-based kernel function (RBF) because it has relativ ely fewer hyper parameters and has shown to be effective in previ ous studies [2],[5]. This function is defined as follows: where  X  is a parameter controlling the shape of the RBF function. The function gets flatter when is larger. Another parameter C&gt;0 in C-SVM should be set to control the trade-off between the slack variable penalty and the margin [2 ]. Both parameters are estimated with a 5-fold cross-validation to maximize the classification accuracy of the training data (see Table 7). In our term classification, we are interested to know not only if a term is good, but also the extent to which it is good. This latter value is useful for us to measure the importance of an expansion term and to weight it in the new query. Therefore, once we obtain a classification score, we use the method described in [12] to transform it into a posterior probability: Suppose the classification score calculated with the SVM is s ( x ). Then the probability of x belonging to the class of good terms (denoted by +1 ) is defined by: where A and B are the parameters, which are estimated by minimizing the cross-entropy of a portion of training data, namely the development data. This process has been automated in LIBSVM [5]. We will have P ( +1 |x )&gt;0.5 if and only if the term x is classified as a good term. More details about this model can be found in [12]. Note that the above probabili stic SVM may have different classification results from the simple SVM, which classifies instances according to sign( s ( x )). In our experiments, we have tested both probabilistic and simple SVMs, and found that the former performs better. We use the SVM implementation LIBSVM [5] in our experiments. Each expansion term is represented by a feature vector vector. Useful features include those already used in traditional approaches such as term distribution in the feedback documents and term distribution in the whole collection. As we mentioned, these features are insufficient. Theref ore, we consider the following additional features: -co-occurrences of the expansion term with the original query -proximity of the expansion terms to the query terms. We will explain several groups of features below. Our assumption is that the most useful feature for term selection is the one that makes the largest difference between the feedback documents and the whole collection (similar to the prin ciple used in the mixture model). So, we will define two sets of features, one for the feedback documents and another for the whole collection. However, technically, both sets of features can be obtained in a similar way. Therefore, we will only describe the features for the feedback documents. The others can be defined similarly.  X  Term distributions The first features are the term di stributions in the pseudo-relevant documents and in the collection. The feature for the feedback documents is defined as follows: where F is the set of feedback documents. f 2 ( e ) is defined similarly on the whole collection. These features are the traditional ones used in the relevance model and mixture model.  X  Co-occurrence with single query term Many studies have found that the terms that co-occur with the query terms frequently are often related to the query [1]. Therefore, we define the following feature to capture this fact: where C(t i ,e|D) is the frequency of co-occurrences of query term t and the expansion term e within text windows in document D . The window size is empirically set to be 12 words.  X  Co-occurrence with pairs query terms A stronger co-occurrence relation for an expansion term is with two query terms together. [1] has shown that this type of co-occurrence relation is much better than the previous one because it can take into account some query contexts. The text window size used here is 15 words. Given the set of possible term pairs, we define the following feature, which is slightly extended from the previous one: Weighted term proximity The idea of using term proximity has been used in several studies [18]. Here we also assume that two terms that co-occur at a smaller distance is more closely related. There are several ways to define the distance between two terms in a set of documents [18]. Here, we define it as the minimum number of words between the two terms among all co-occurrences in the documents. Let us denote this distance between t i and t j among the set B of documents by dist(t i ,t j |B) . For a query of multiple words, we have to aggregate the distances between the expansion term and all query terms. The simplest method is to consider the average distance, which is similar to the average distance defined in [18]. However, it does not produce good results in our experiments. Instead, the weighted average distance works better. In the latter, a distance is weighted by the frequency of their co-occurrences. We then have the following feature: where C(t i , e) is the frequency of co-occurrences of t text windows in the collection. The window size is set to 12 words as before.  X  Document frequency for query terms and the expansion The features in this group model the count of documents in which the expansion term co-occurs with all query terms. We then have: where I(x) is the indicator function whose value is 1 when the Boolean expression x is true, and 0 otherwise. The constant 0.5 here acts as a smoothing factor to avoid zero value. To avoid that a feature whose values varies in a larger numeric range dominates those varying in smaller numeric ranges, scaling on feature values is necessary [5]. The scaling is done in a query-by-query manner. Let e  X  GEN(q) be an expansion term of the query q , and f i (e) is one feature value of e . We scale f becomes a real numb er in [0, 1]. In our experiments, only the above features are used. However, the general method is not limited to them. Other features can be added. The possibility to integrate arbitrary features for the selection of expansion terms indeed represents an advantage of our method. Let us now examine the quality of our classification. We use three test collections (see Table 7 in Section 7.1), with 150 queries for each collection. We divide these queries into three groups of 50 queries. We then do leave-one-out cross validation to evaluate the classification accuracy. To generate training and test data, we use the method described in section 3 to label possible expansion terms of each query as good terms or non-good terms (including bad and neutral terms), and then represent each expansion with the features described in section 5.2. The candidate expansion terms are those that occur in the feedback docum ents (top 20 documents in the initial retrieval) no less than three times. Table 3 shows the classification results. In this table, we show the percentage of good expansion terms for all the queries in each collection  X  around 1/3. Using th e SVM classifier, we obtain a classification accuracy of about 69%. This number is not high. In fact, if we use a na X ve classifier that always classifies instances into non good class, the accuracy (i.e. one minuses the percentage of good terms) is only slightly lower. However, such a classifier is useless for our purpose because no expansion term is classified as good term. Better indicators are recall, and more particularly precision. Although the classifier only identifies about 1/3 of the good terms (i.e. recall), around 60% of the identified ones are truly good terms (i.e. precision). Comparing to Table 1 for the expansion terms selected by the mixture model, we can see that the expansion terms select by the SVM classifier are of much higher quality. This shows that the additional features we considered in the classification are useful, although they could be further improved in the future. In the next section, we will describe how the selected expansion terms are integrated into our retrieval model. The classification process performs a further selection of expansion terms among those proposed by the relevance model and the mixture model respectively. The selected terms can be integrated in these models in two different ways: hard filtering, i.e. we only keep the classification score to enhance the weight of good terms in the final query model. Our experiments show that the second method performs better. We will make a comparison between these two methods in Section 7.4. In this section, we focus on the second method, which means a redefinition of the models relevance model and ) | ( are redefined as follows: For a term e such that P(+1|e)&gt;0.5 where Z is the normalization factor, and is a coefficient, which is estimated with some development data in our experiments using line search [4]. Once the expansion terms are re-weighted, we will retain the top 80 terms with the highest probabilities for expansion. Their weights are normalized before being interpolated with the original query model. The number 80 is used for a fair comparison with the relevance model and the mixture model. We evaluate our method with three TREC collections, AP88-90, WSJ87-92 and all documents on TREC disks 4&amp;5. Table 4 shows the statistics of the three collections. For each dataset, we split the available topics into three parts: the training data to train the SVM classifier, the development data to estimate the parameter in equation 9, and the test data. We only use the title for each TREC topic as our query. Both documents and queries are stemmed with Porter stemmer and stop words are removed. The main evaluation metric is Mean Average Precision (MAP) for top 1000 documents. Since some previous studies showed that PRF improves recall but may hurt precision, we also show the precision at top 30 and 100 documents, i.e., P@30 and P@100. We also show recall as a supplementary measure. We do a query-by-query analysis and conduct t-test to determine whether the improvement on MAP is statistically significant. The Indri 2.6 search engine [17] is used as our basic retrieval system. We use the relevance model implemented in Indri, but implemented the mixture model following [22] since Indri does not implement this model. In the experiments, the follo wing methods are compared: LM : the KL-divergence retrieval mo del with the original queries; REL : the relevance model; REL+SVM : the relevance model with term classification; MIX : the mixture model; MIX+SVM : the mixture model with term classification. These models require some parameters, such as the weight of original model when forming the final query representation, the Dirichlet prior for document model smoothing and so on. Since the purpose of this paper is not to optimize these parameters, we set all of them at the same values for all the models. Tables 5, 6 and 7 show the results obtained with different models on the three collections. In the tables, imp means the improvement rate over LM model, * indicates that the improvement is statistically significant at the level of p&lt;0.05, and ** at p&lt;0.01. The superscripts  X  X  X  and  X  X  X  indicate that the result is statistically better than the relevance model and mixture model respectively at p&lt;0.05. From the tables, we observe that both relevance model and mixture model, which exploit a form of PRF, can improve the retrieval effectiveness of LM significantly. This observation is consistent with previous studies. The MAP we obtained with these two models represent the state-of-the-art effectiveness on these test collections. Comparing the relevance model and the mixture model, we see that the latter performs better. The reason may be the following: The mixture model relies more on the difference between the feedback documents and the whole collection to select the expansion terms, than the relevance model. By doing this, one can filter out more bad or neutral expansion terms. On all the three collections, the model integrating term classification performs very well. When the classi fication model is used together with a PRF model, the effectiveness is always improved. On the AP and Disk4&amp;5 collections, the improvements are more than 7.5% and are statistically significant. The improvements on the WSJ collection are smaller (about 3.5%) and are not statistically significant. About the impact on precision, we can also see that term classification can also improve the precision at top ranked documents, except in the case of Disk4&amp;5 when SVM is added to REL. This shows that in most cases, adding the expansion terms does not hurt, but improves, precision. Let us show the expansion terms for the queries  X  X achine translation X  and  X  X atural language processing X , in Table 8. The stemmed words have been restored in this table for better readability. All the terms contained in the ta ble are those suggested by the mixture model. However, only part of them (in italic ) is useful expansion terms. Many of them are general terms that are not useful, for example,  X  X ood X ,  X  X ake X ,  X  X ear X ,  X 50 X , and so on. 
The classification process can help identify well the useful expansion terms (in bold ): although not all the useful expansion terms are identified, those identified (e.g.  X  X  rogram X ,  X  X ictionary X ) are highly related and useful. As the weight of these terms is increased, the relative weight of the other terms is decreased, making their weights in the final query model smaller. These examples illustrate why the term classification process can improve the retrieval effectiveness. 
Compared to the relevance model and the mixture model, the approach with term classification made two changes: it uses supervised learning instead of unsupervised learning; it uses several additional features. It is then impo rtant to see which of these changes contributed the most to the increase in retrieval effectiveness. 
In order to see this, we design a method using unsupervised learning, but with the same additional features. The unsupervised learning extends the mixture model in the following way: 
Each feedback document is also considered to be generated from the topic model (to be extracted) and the collection model. We try to extract the topic model so as to maximize the likelihood of the feedback documents as in the mixture model. However, the difference is that, instead of defining the topic model ) | ( model, we define it as a log-linear model that combines all the features: where F ( w ) is the feature vector defined in section 5.2, vector and Z is the normalization factor to make probability.  X  is estimated by maximizing the likelihood of the feedback documents. To avoid overfitting, we do regularization on by assuming that it has a zero-mean Gaussian prior distribution [2]. Then the objective function to be maximized becomes: where  X  is the regularization factor, which is set to be 0.01 in our experiments.  X  is the parameter representing how likely we use the topic model to generate the pseudo-relevant document. It is set at a fixed value as in [22] (0.5 in our case). Since L(F) is a concave function w.r.t.  X  , it has a unique maximum. We solve this unconstrained optimization problem with L-BFGS algorithm [10]. 
Table 9 shows the results measured by MAP. Again, the superscript,  X  M  X  and  X  L  X  indicate the improvement over MIX and Log-linear model is statistically significant at p&lt;0.05. 
From this table, we can observe th at the log-linear model outperforms the mixture model only slightly. This shows that an unsupervised learning method, even with additional features, cannot improve the retrieval effectiveness by a large margin. The possible reason is that the objective function, L(F) , does not correlate very well with MAP. The parameters maximizing L(F) do not necessarily produce good MAP. 
In comparison, the MIX+SVM model outperforms largely the log-linear model on all the three collec tions, and the improvements on AP and Disk4&amp;5 are statistically significant. This result shows that a supervised learning method can more effectively capture the characteristics of the genuine good expansion terms than an unsupervised method. We mentioned two possible ways to use the classification results: hard filtering of expansion term s by retaining only the good terms, section, we compare the two methods. Table 10 shows the results obtained with both methods. In the table,  X  X  X ,  X  X  X , and  X  X  X  indicate the improvement over MIX, REL and HARD are statistically significant with p&lt;0.05 From this table, we see that both hard and soft filtering improves the effectiveness. Although the improvements with hard filtering are smaller, they are steady on all the three collections. However, only the improvement over MIX model on the AP and Disk4&amp;5 data is statistically significant. In comparison, the soft filtering method performs much better. Our explanation is that, since the classification accuracy is far from perfect (actually, it is less than 70 % as shown in Table 3), some top ranked good expansion terms, which could improve the performance significantly, can be removed by the hard filtering. On the other hand, in the soft filtering case, even if the top ranked good terms are misclassified, we will only reduce their relative weight in the final query model rather than removing them. Therefore, these expansion terms can still contribute to improving the performance. classification errors. A critical aspect with query expansion is that, as more terms are evaluation, becomes larger. In the previous sections, for the purpose of comparison with previous methods, we used 80 expansion terms. In practice, this number can be too large. In this section, we examine the possibility to further reduce the number of expansion terms. In this experiment, after a re-weigh ting with soft filtering, instead of keeping 80 expansion terms, we only select the top 10 expansion terms. These terms are used to construct a small query topic model w P  X  . This model is interpolated with the original query model as before. The following table describes the results using the mixture model. As expected, the effectiveness with 10 expansion terms is lower than with 80 terms. However, we can still obtain much higher effectiveness than the traditional language model LM, and all the improvements are significantly significant. The results with 10 expansion terms can also be advantageously compared to the mixture model with 80 expansion terms: for both AP and Disk4&amp;5 collections, the effectiveness is higher than the mixture model. The effectiveness on WSJ is very close. This experiment shows that we can reduce the number of expansion terms, and even with a reasona bly small number, the retrieval effectiveness can be greatly increased. This observation allows us to control query traffic within an acceptable range, and make the method more feasible in the search engines. Pseudo-relevance feedback, which adds additional terms extracted from the feedback documents, is an effective method to improve the query representation and the retrieval effectiveness. The basic assumption is that most strong terms in the feedback documents are useful for IR. In this paper, we re-examined this hypothesis on three test collections and showed that the expansion terms determined in traditional ways are not all useful. In reality, only a small proportion of the suggested expansion terms are useful, and many others are either harmful or useless. In addition, we also showed that the traditional criteria for the selection of expansion terms based on term distributions are insufficient: good and bad expansion terms are not distinguishable on these distributions. Motivated by these observations, we proposed to further classify expansion terms using additional features. In addition, we aim to select the expansion terms directly according to their possible impact on the retrieval effectivene ss. This method is different from the existing ones, which often rely on some other criteria that do not always correlate with the retrieval effectiveness. Our experiments on three TREC collections showed that the expansion terms selected using ou r method are significantly better than the traditional expansion terms. In addition, we also showed that it is possible to limit the query traffic by controlling the number of expansion terms, and this still l ead to quite large improvements in retrieval effectiveness. This study shows the importance to examine the crucial problem of usefulness of expansion terms before the terms are used. The method we propose also provides a general framework to integrate multiple sources of evidence. This study suggests several intere sting research avenues for our future investigation: The results we obtained with term classification are much lower than with the oracle expansion terms. This means that there is still much room for improvement. In particular, improvement in classi fication quality could directly result in improvement in retrieva l effectiveness. The improvement of classification quality could be obtained by integrating more useful features. In this paper, we have limited our investigation to only a few often used features. More discriminative features can be investigated in the future. [1] Bai, J. Nie, J., Bouchard, H. an d Cao, G. Using query contexts [2] Bishop, C. Patten recognition a nd machine learning. Springer, [3] Dempster, A. , Laird, N. and Rubin, D. Maximum likelihood [4] Gao, J., Qi, H., Xia, X., and Nie, J. Linear discriminant model [5] Hsu, C. Chang, C. and Lin, C. A practical guide to support [6] Joachims, T. Text categorization with support vector machines: [7] Kwok, K.L, Grunfeld, L., Chan, K., THREC-8 ad-hoc, query [8] Lavrenko, V. and Croft, B. Re levance-based language models. [9] Metzler, D. and Croft, B. La tent Concept Expansion Using [10] Nocedal, J. and Wright, S. Numerical optimization. Springer, [11] Peat, H.J. and Willett, P., The limitations of term co-[12] Platt, J. Probabilities for SV Machines. Advances in large [13] Robertson, S., and Sparck Jone s, K. Relevance weighting of [14] Robertson, S.E., On term selection for query expansion, [15] Rocchio, J. Relevance feedback in information retrieval. In [16] Smeaton, A. F. and Van Rijsbergen , C. J. The retrieval effects [17] Strohman, T., Metzler, D. and Turtle, H., and Croft, B. (2004). [18] Tao, T. and Zhai, C. An exploration of proximity measures [19] Tao, T. and Zhai, C. Regularized estimation of mixture models [20] Vapnik, V. Statistical Learning Theory. New York: Wiley, [21] Xu, J. and Croft, B. Query expansion using local and global [22] Zhai, C. and Lafferty, J. Model-based feedback in the KL-[23] Zhai, C. and Lafferty, J. A study of smoothing methods for [24] Zhang, Y., Callan, J., The bais problem and language models 
