 LinkedIn Groups provide a platform on which professionals with similar background, target and specialities can share content, take part in discussions and establish opinions on industry topics. As in most online social communities, spam content in LinkedIn Groups poses great challenges to the user experience and could eventually lead to substantial loss of active users. Building an intelligent and scalable spam de-tection system is highly desirable but faces difficulties such as lack of labeled training data, particularly for languages other than English. In this paper, we take the spam (Span-ish) job posting detection as the target problem and build a generic machine learning pipeline for multi-lingual spam detection. The main components are feature generation and knowledge migration via transfer learning. Speci cally, in the feature generation phase, a relatively large labeled data set is generated via machine translation. Together with a large set of unlabeled human written Spanish data, unigram features are generated based on the frequency. In the sec-ond phase, machine translated data are properly reweighted to capture the discrepancy from human written ones and classi ers can be built on top of them. To make effective use of a small portion of labeled data available in human written Spanish, an adaptive transfer learning algorithm is proposed to further improve the performance. We evaluate the proposed method on LinkedIn's production data and the promising results verify the efficacy of our proposed al-gorithm. The pipeline is ready for production.
 H.2.8 [ Database Management ]: Database applications-Data Mining Algorithm
Ma jor part of this work was done when Qian Sun interned at LinkedIn.
 c  X  transfer learning, classi cation, text mining, NLP
Spam content posted in group discussion at LinkedIn will jeopardize other user's return visit on the site. Figure 1 is one of the spam examples in group discussion 1 , where A.A puts a job post in reply to S.S's pro le update. A.A's response would probably annoy S.S since her reply is not related to S.S's update. Simply blocking A.A's account is not a smart solution for LinkedIn, as she may be a legiti-mate user and may also put informative posts somewhere else on the site. A better solution is to examine each piece of content and lter those deemed to be spammy. With the development of globalization in social network, LinkedIn is available in more than 20 languages. Therefore such spam content may be written in different languages, such as En-glish, Spanish, Chinese, etc. Efficient tools for spam detec-tion in multiple languages are highly desired at LinkedIn. The multilingual text categorization problem has gained its attention recently [1, 13, 15, 23, 26].
 Fi gure 1: A sample of spam. S.S posted a status up-date at LinkedIn. A.A replied with a post searching for jobs, which is clearly a spam in this case.

One may solve this multi-lingual spam detection prob-lem by enriching data representation with machine trans-lated features in different languages [2, 21], as illustrated in Figure 2. If we are dealing with multilingual spam de-tection problem in English, Spanish and French, we can
F or the purpose of privacy protection, we anonymize the name and pro le pictures in all examples. tra nslate any content into the other two languages. In this way, we will have bag-of-words features concatenated from three languages, and each piece of content needs to be trans-lated into multiple languages in order to apply the uniform model. However, the machine translation to multiple lan-guages leads to higher latency and consumes more budget. Therefore, the uniform model is not the best choice for an online product.
 Fi gure 2: Multi-lingual uniform model. Messages are translated into other languages at rst. The keywords for all the languages are then extracted. Frequency based values are assigned to each entry of the data matrix.

Considering the problem from an online product view, we propose to build a generic pipeline for each language. Thus we will keep a model pool in our system which contains mod-els for different languages. The main idea of the pipeline is illustrated in Figure 3. When a piece of content is posted and transferred to the server, we rst detect the language in which it is written, then apply the corresponding model to do the spam detection. If it is a spam, the system takes ap-propriate action. We will start with Spanish spam detection rst, as Spanish is the second most widely used language in the US. The pipeline can be petentially well generalized to many other languages, such as French, German, etc. Problem Setup : In this paper, we aim to develop a frame-work to deal with the Spanish spam detection problem with-out training data at LinkedIn. The challenge of Spanish spam detection is that we do not have enough labeled Span-ish data in our database, so we cannot train an accurate model from real Spanish data. Instead we have a lot of la-beled English content available. Asking human experts to help us label the Spanish data is both time and labor con-suming. It is more efficient to translate Spanish content with the emergence of machine translation tools [19, 7].
There are two ways to do the translation: the rst one is to translate the features (unigram) in English to Spanish, and the second one is to translate the whole content from English to Spanish and then generate the unigram features based on the machine translated Spanish. To simply trans-late the features in English to Spanish will lose the intended meaning of the unigrams due to the loss of the context sur-rounding them. For example, Python can be translated into one kind of programming language, and it can also be translated into snake, depending on its surrounding text. Furthermore, for certain languages there is no one-to-one correspondence in translation. For example, one unigram feature in English may be translated into several characters in Chinese. Therefore, we will follow the second way to do the translation.
 Fi gure 3: High-level idea for multilingual spam de-tection. Given a piece of content, we rst detect which language it is written in. The corresponding model is then extracted from the model pool to do spam detection. Action will be taken thereafter. Contributions : In this paper, we propose a generic pipeline to solve the multi-lingual spam detection problem. Job post-ing is the most common category of spam under group dis-cussion, therefore we target at the job classi cation here. Starting with Spanish, we utilize the machine translation to obtain the training content in the target language. Google translator is applied in translating the labeled English con-tent into Spanish content. We also collected job posts in Spanish that were originally written by human for model evaluation. It can be shown that the machine translated Spanish and the human written Spanish have different dis-tributions. We therefore propose a two-stage transfer learn-ing algorithm to migrate the knowledge learnt from machine translated Spanish to human written Spanish. In the rst stage, we apply the Kernel Mean Matching (KMM) to re-weight the instances in the machine translated Spanish to match the human written Spanish. In the second stage, we adaptively update the model if a small amount of label in-formation is available in the human written Spanish domain. Our experiments on LinkedIn's production data show that the proposed two-step transfer learning algorithm achieves much better performance in terms of precision and recall compared with baseline algorithms.
 Organization : The rest of the paper is organized as follows. In Section 2, we brie y review the spam detection problem in group discussion module at LinkedIn. In Section 3, we introduce our generic pipeline to solve multi-lingual spam detection problems in detail. Next, we propose our two-step transfer learning algorithm for the bilingual spam de-tection problem in Section 4, which consists of an instance re-weighting algorithm as step one and an adaptive learning approach as step two. In Section 5, we evaluate our pro-posed algorithm on LinkedIn's production data. Section 6 discusses some related work and the differences from ours. Finally, we conclude the paper in Section 7.
LinkedIn is the world's largest professional network with more than 332 million members in over 200 countries and territories. LinkedIn provides a wide variety of products and services that allow members to connect with one an-other in order to be more productive and successful in their professional lives. LinkedIn Groups in particular is a prod-uct which allows professionals in the same industry or with si milar interests to gather, share content, nd answers to questions, post and view jobs, make business contacts, and establish themselves as industry experts. LinkedIn mem-bers are able to freely join public groups, or may be invited to private groups by a group manager. So far, there are more than 2 million groups at LinkedIn, and 154 ; 000 pieces of content are being posted every day. Members who use Groups are 5 times more likely to get pro le views from people outside their network. Figure 4 shows a LinkedIn group for professional interior designers, and a snapshot of discussions amongst designers sharing expertise and knowl-edge about their industry. The best way to participate in LinkedIn groups in general is to focus on intelligent, mean-ingful posts that add value to a discussion and that will bene t other members.

There are rules for participating in LinkedIn Groups dis-cussions. First, different groups have different norms and expectations around discussions, established by the partici-pants in the group and the group manager(s). For example, different groups have different tolerance for promotional con-tent in group discussions. Some groups encourage members to freely share links to their own websites, blog posts, or other promotional materials, whereas other groups ask their members to con ne this sort of content and sharing to a Promotions area within the group. Some groups provide a Jobs area for posting job openings, talking about the current job market, asking job related questions, or starting career discussions. Other groups allow neither promotions nor job postings. In other words, users posting to a group should understand the community norms and expectations for that group, and use good judgement when making a post. Con-tent posted to a group that does not adhere to the group's norms and expectations is considered spam.

The motivation of this work is to develop a computational method to automatically maintain the order of group dis-cussion, i.e., to detect and lter the unwanted posts. For each group, spam detection is conducted based on its spe-ci c rules. For example, in the groups which do not allow job posting, any posts related to job annoucement are con-sidered as spam and appropriate actions need to be taken. Users who post spam content under group discussion are not necessarily spamers, instead they may be legitimate users but unfamiliar with the rules of the group. For this reason, we choose to build a content classi er rather than to block the users' accounts.

Users can post their updates in up to 20 different lan-guages (Figure 5). Thus, to maintain the order of group discussion, spam detection is required to work in different languages. From the perspective of production, we build one content classi er for each language and keep a classi er pool in our system.

In summary, the key contributions of this paper are as follows: (1) We design a generic pipeline for spam detection in different languages. (2) Starting with Spanish, we develop a two-step adaptive learning approach to solve the cross-language spam detection problem. (3) We test and validate the proposed algorithm on real data from LinkedIn. The resulting pipeline is production ready.
In this section, we introduce the proposed pipeline to tackle the multi-lingual spam detection problem at LinkedIn. Speci cally, we design a generic pipeline to do the spam de-Fi gure 5: Supported languages at LinkedIn. The site is available in 20 languages, including Chinese, English, French, German, Italian, Portuguese, etc. tection for different languages. Here, we start with the spam detection in Spanish, as Spanish is the second most widely used language in the US. The pipeline can be easily gen-erated to any other language than Spanish by substituting the target language of the machine translator. The pipeline Fi gure 6: Proposed pipeline for Spanish spam detec-tion. Machine Translator is employed to translate the English labeled content into Spanish. We adopt transfer learning to build a spam detection model. Evaluation is done on human written Spanish. contains four steps, as shown in Figure 6. To make use of labeled English samples in the server, we collect the English labeled data for translation in the rst step. During the collection step, we rst apply the language detector [12] to make sure that the selected samples are written in English. After collecting a large set of labeled English content, we use machine translation tools to translate the English con-tent into Spanish. We select the Google translate API in the second step as it is one of the most popular machine translators. The second step results in a large set of ma-chine translated Spanish content, which can be considered as the training samples for building a Spanish spam detec-tor. Due to the distribution difference between the machine translated Spanish and the human written Spanish, we pro-pose a two-step transfer learning algorithm to generate a Spanish spam classi er. Finally, we evaluate our model on hunman written Spanish content.
In the aforementioned pipeline, a collection of machine translated Spanish data is available after the machine trans-lation. We generate tf-idf features to represent both machine translated Spanish and human written Spanish data. The features are then used for building transfer learning models.
Term frequency-inverse document frequency (tf-idf) is a numerical statistic that is intended to re ect how important a word is to a document in a collection of corpus. The tf-idf value increases proportionally to the number of times a word appears in the document, but is offset by the frequency of the word in the corpus. The offset is necessary since some words are generally more common than others.

Tf-idf is the product of two statistics, including term fre-quency (tf) and inverse document frequency (idf). Various ways for determining the exact values of both statistics ex-ist. We apply the Scikit-learn ( http://scikit-learn.org/ stable/ ) [16] package to generate the tf-idf features, where tf, idf and tf-idf are de ned as follows: A high weight in tf-idf is reached by a high term frequency (in the given document) and a low document frequency of the term in the whole collection of documents; the weights hence tend to lter out common terms.
The vocabulary of the corpus is large, and the data matrix for the tf-idf features is sparse. To study the distribution of machine translated Spanish and human written Spanish, we rst select the top features using the labels.
 Here, we apply Lasso [22] to do the feature selection. Lasso is essentially a linear regression problem with an l norm regularizer. It is often adopted as an efficient tool to perform the feature selection and its extension remains as a hot topic in recent research [27, 25]. Denote the feature matrix as A , the label as y , and the model as x . Lasso can be formulated as the following optimization problem: where is a parameter that controls the balance between the loss function and the penalty. The Lasso problem can be solved efficiently by FISTA algorithm [3]. After we get the solution x , the non-zero entries in x correspond to the selected features in A .

We apply Lasso to both machine translated Spanish and human written Spanish to investigate the characteristics of these two domains. The top 20 selected features are listed in Table 1. It can be veri ed that most of the features re-late to jobs. We can see there are some top features shared by machine translated Spanish documents and human writ-ten Spanish documents, but about half of the features are different between the two domains. This is not surprising: the machine translated jobs and the manually constructed jobs were taken from non-overlapping data sources. The for-mer was translated job posts from English jobs on LinkedIn, while the latter was obtained by crawling the web and nd-ing job posts written in Spanish. While the top 20 features in both data sets are related to the broad topic of jobs, e.g. business, information, employee, opportunity, development, team, management (Table 1), we cannot expect the features to be 100% identical over both data sets. We only expect stop words to be invariant across different data sets, not nouns, verbs, or adjectives. Furthermore, some of the top 20 features in the manually constructed and the machine trans-lated sets are morphological cognates such as buscamos = we look and buscando = looking ; cliente = client and clientes = clients , which further illustrates that the top 20 features are semantically very close in both data sets.
Despite the above issues, the machine translated content has the same overall intended meaning as the human writ-ten content. In addition, as we select more unigram features, we nd larger overlaps of selected features between the two domains. Thus, transfer learning is a viable approach to solve the problem. Consider the machine translated Span-ish as the source domain, and the human written Spanish as the target domain, transfer learning aims to borrow the kn owledge learnt from source domain and apply it to tar-get domain. The details of our two-step transfer learning algorithm are presented in the next section.
In this paper, we propose a two-step transfer learning al-gorithm to learn the content classi er from machine trans-lated Spanish and apply it to human written Spanish. To our best knowledge, this is the rst work that transfers the knowledge from machine translation corpus to human writ-ten corpus in social network. This paper is different from a related two-stage transfer learning algorithm [20] in the sense that the number of domains are different. The pre-vious work [20] focuses on multiple source domain transfer learning, in which the second stage utilized the smoothness assumption among hypotheses from multiple sources. How-ever, this paper focuses on single source domain transfer learning where the smoothness assumption cannot be estab-lished. The proposed algorithm is a general algorithm that can be adopted to any other languages as well as any binary classi cation problem.

Denote the machine translated Spanish as the source do-main T , and the human written Spanish as the target do-main H . We have shown in Section 3 that the distributions of the T and H could be different while the support can be the same with the increase of feature dimension. There-fore, we make an assumption that the distribution difference mainly lies in the marginal probability rather than the con-ditional probability. Based on the assumption, we propose our rst step of transfer learning: instance re-weighting. The main idea of instance re-weighting is to calculate the weights for the data in the source domain, and re-weight the source domain for matching the marginal distribution with the tar-get domain.

After the rst step, we train a logistic regression model on the re-weighted source data, then apply it to the target domain. In addition, if there are some labels available in the target domain, we propose an adaptive learning algorithm as the second step to update our model learnt in the rst step. The adaptive learning algorithm penalizes the mis-classi cation errors in the target domain while minimizes the modi cations of the model. The second step will also make up the conditional distribution difference between T and H if our assumption in the rst step does not hold.
The rst step in the proposed transfer learning frame-work is the Kernel Mean Matching (KMM) algorithm [9], which is a nonparametric method to directly infer weights for source domain without distribution estimation. KMM utilizes the unlabeled data to build a bridge for connecting two domains. In general, the estimation problem with two different distributions P r ( x; y ) and P r  X  ( x; y ) is unsolvable, as the two terms could be arbitrarily far apart. In partic-ular, for arbitrary P r ( y j x ) and P r  X  ( y j x ), there is no way to infer a good estimator based on the available training samples in the source domain. Hence a simpli ed version makes assumption that P r ( x; y ) and P r  X  ( x; y ) only differ via P r ( x ) and P r  X  ( x ), i.e., the conditional probability re-mains unchanged: P r ( y j x ) = P r  X  ( y j x ). This particular case of transfer learning has been termed as covariate shift.
The basic assumption behind KMM approach is that be-tween the two domains, the key difference lies in the marginal distribution P r ( x ) rather than the conditional distribution P r ( y j x ). The idea of KMM is to nd the appropriate weights for T which minimize the discrepancy between the mean val-ues of T and H in the common projected space. Denote x i as the i-th sample in T and x  X  i as the i-th sample in H . The mean differences after projection can be formulated as: wh ere i ( i = 1 ; : : : ; m ) is the weight we want to learn, m is the number of samples in T , m  X  is the number of samples in H , and is a mapping which maps the raw features into a latent space.
 The formulation is actually a constrained QP (Quadratic Programming) problem. After weighting, we still want to ensure P r ( x ) is close to a distribution, so there are two constraints that need to be added: i 2 [ LB; U B ] and and the upper bound of i respectively. Denote K ij = k ( x i ; x j ) and i = m m  X  formulation (2) as: It can be solved by many optimization algorithms, such as interior point methods or any other successive optimization procedure. We simply apply the optimize module in SciPy to calculate the solution.

After we obtaining , we multiply each sample in T with the corresponding i , then build a regularized logistic re-gression model w 0 as follows: If there is no labeled data available in target domain, we apply the model learnt in Equation (4) to the target domain for Spam detection. There are many different classi cation models can be adopted to solve this problem, such as Naive Bayes [11] and SVM [28]. The reason of choosing the logis-tic regression to build our classi cation model lies in three folds: (1) Logistic regression has achieved great success in many real-world applications; (2) The processing time for an online product should be as small as possible, and logistic regression is a perfect match; (3) It is easy to be generalized to a distributed version.
KMM targets at the scenario that there is no labeled data available in the target domain. So far, we have not touched any label information in the target domain. There are scenarios under which a small portion of labeled target samples may be labeled by crowdsourcing. Next, we show how to improve our model w 0 learnt from KMM by using additional labeled instances in H . We propose an adap-tive learning strategy, which aims to learn a perturbation  X  f ( x ) =  X  w T  X  ( x ) on the basis of w 0 to further compen-sate for domain mismatch. Denote the classi er we learnt through KMM as w 0 , the target classi er we aim to learn as w , we have the following relation: w = w 0 +  X  w .
Based on the aforementioned assumption that the differ-ence between the distributions of machine translated Span-ish and human written Spanish mainly lies in the marginal distribution, the intuition of the proposed adaptive learn-ing is that the difference of w and w 0 should be as small as possible. There are related previous work [6, 8], which were proposed to learn a perturbation of the source hyperplane, Fi gure 7: Illustration of adaptive learning. Model w 0 is borrowed from the domain T , and the model w for the domain H should be similar to w 0 .  X  w is a small perturbation we aim to learn. by minimizing the classi cation error on labeled examples in the target domain. The pertubation can also be considered as new feature representations that correct for the domain change. The intuition of our adaptive learning algorithm is consistent with the online learning, thus this step can be put into online learning system.

Our adaptive learning step aims to learn a perturbation  X  w such that the difference between prediction f ( x ) = w and f 0 ( x ) = w T 0 x is small (Figure 7). The following equation holds: f ( x ) = f 0 ( x ) +  X  f ( x ) = w T 0  X  ( x ) +  X  w  X  w is the difference to be learnt here. Therefore, we for-mulate the adaptive learning as the following optimization problem: =min where the logistic loss is utilized with the penalty that en-courages the perturbation  X   X  w  X  to be small. Here is a tuning parameter to control the balance between the loss function and the penalty. Problem (5) involves a convex smooth function, which can be solved efficiently by gradient-based method. Adaptive learning can make up for the po-tential issue that there are also conditional differences in the distributions between the machine translated Spanish and human written Spanish. It can also serve as an online update algorithm when more and more labeled target data become available.

Finally, we obtain the model w = w 0 +  X  w after the pro-posed two-step transfer learning algorithm. We summarize our two-step transfer learning algorithm in Algorithm 1.
In this section, we demonstrate the effectiveness of our proposed machine learning pipeline using production data at LinkedIn.

Following the pipeline 6, we collect 12000 pieces of English content in group discussions at LinkedIn. The content is col-lected from the groups where job posts are not allowed, i.e., job related content are considered as spam in these groups. The English content is kept balanced for the two classes: job-related posts and non-job posts. It is used for translation A lgorithm 1 A two-step transfer learning algorithm for bilingual spam detection Re quire: a large set of labeled samples in T: f x i ; y i large set of unlabeled samples in H: f x  X  i ; y  X  i g , (optional) a small set of labeled samples in H: f x  X  X  i ; y  X  X  i g ,  X  w = 0 Ensure: w 1: Calculate weights by solving 2: Com pute w 0 by solving 3: (Optional) Compute  X  w by solving 4: return w = w 0 +  X  w in to Spanish to form the source domain. We translated all the collected human labeled content from English to Spanish with Google translate API.
 In addition, we also collect 12000 pieces of human written Spanish content (balanced) by crowdsourcing, which serves as the target domain. Inside the target domain, there are 50 pieces of comments considered as labeled samples used in our adaptive learning step and the remaining ones are treated as test data for evaluation. We generate tf-idf fea-tures for both machine translated Spanish and human writ-ten Spanish, and it turns out that the feature spaces of the two domains differ from each other, as shown in Table 1.
Based on the source domain corpus and the target domain corpus, we rst generate the tf-idf features for each domain, and then use the intersection of selected features to build the Spanish spam detector.

Scikit-learn ( http://scikit-learn.org/stable/ ) [16] is a powerful machine learning package in python which pro-vides feature generation, feature selection as well as classi-cation functions. We make use of scikit-learn to carry out the experiment: rst, we generate the tf-idf features for both domains by use of feature ex traction.text.T dfTransformer class; then we build the classi cation model via linear mo del-.LogisticRegression class.

In our experiment, it turns out that stemmer would not help in classi cation tasks, as it reduced the feature dimen-sion of the problem. There are overlapped unigram fea-tures between machine translated Spanish and human writ-ten Spanish, but the feature spaces of these two domains are not identical. We simply choose the intersection as our feature space to conduct the transfer learning.

We implement our proposed two-step transfer learning al-gorithm in Python, and we solve the optimization prob-lem in both KMM and adaptive learning by calling the scipy.optimize module ( http://docs.scipy.org/doc/scipy/ reference/optimize.html ). We increase the tf-idf feature dimension to leverage the loss of information introduced by performing intersection. To verify the efficacy of the pro-posed two-step transfer learning algorithm, we compare it with four other algorithms: We choose the above four algorithms for comparison be-cause they are both straight forward and practically viable algorithms for an online product. To further test the robust-ness of different approaches, we conduct our experiments by varying the dimension of feature space as follows: From our experience, the more features we generated, the better it will generalize to unseen data.
The results of all ve algorithms are shown in 8 and Ta-ble 2. To encourage the model to achieve the desired per-formance for production, we set the threshold of logistic re-gression so as to make the precision xed at 0 : 95.
The experimental results exhibit the efficacy of our pro-posed algorithm in the following aspects: S LT 0. 95 0 .42 0 .63 0. 93 S LT 0. 95 0 .51 0 .66 0. 93 S LT 0. 95 0 .52 0 .67 0. 93 S LT 0. 95 0 .58 0 .72 0. 93
From the results, we conclude that each of the two steps (Instant Weighting and Adaptive Learning) improves the performance compared to the baseline method, while com-bining the two steps works the best. As the feature di-men sion increases, the performance also improves. To avoid over tting and ensure the efficiency as an online product, we stop at 2000 features.

The results also verify our assumption about the distribu-tions of two domains, that is, the main difference lies in the marginal distribution rather than the conditional difference. The proposed two-step transfer learning algorithm achieves the required performance for production.
In real applications, we may not have enough labeled sam-ples in H . In addition, we may not have enough unlabeled samples in H in advance for KMM. As the mean value of projected distribution for the target domain cannot be cap-tured, KMM may fail to match the difference between the source domain and target domain.

To verify whether the KMM algorithm works with a small sample size in the target domain, we further design a series of experiments with varied sizes of samples in the target do-main. Speci cally, We vary the amounts of unlabeled sam-ples from 20 to 1000 in human written Spanish to test the KMM algorithm, and the results can be seen in Table 3. Table 3: KMM results for a small number of samples in the target domain sa mple size in H pr ecision reca ll f1-score A UC
W e want to emphasize that even with a small sample size (100) in H , we can get the desired performance for produc-tion. The explanation behind the experiment results lies in the fact that as long as the target samples are balanced, KMM can capture the mean value of the distribution in the projected space even with small sample size. This is critical for an online product: as in the real world, we want to en-sure the model to work even with small amounts of human written content. As more and more human written Spanish data become available, we can apply the second step to in-crementally update the model, which can also be considered as online learning.
Cross-lingual content classi cation is an important topic in natural language processing area, and it gains more at-tention as the social network becomes global [1, 17, 14, 13, 15, 18, 24, 4]. The transfer learning research on text mining [5, 10] mainly focus on transferring the knowledge from different topics in the same language. The work done by Vinokourov [23] can be considered as the unsupervised cross-lingual learning, where the correlation between two languages are learnt by Canonical Correlation Analysis. In this work, the high correlation between English and French words indicates that they represent the same semantic in-formation. It is different from our work as we deal with su-pervised learning problems. There is related research that focus on modeling the multi-view representations in which each language is considered as a separate source, and a joint loss is minimized while the consistency between languages is ensured [1]. One related work [15] also represents the data from two languages views and proposes a non-negative ma-trix tri-factorization (BNMTF) model for the cross-lingual sentiment classi cation problem. These methods aim to uti-lize the labeled data in two languages and build a joint model for multi-view learning, which differs from our work as we do not have labeled data in both languages, and for an on-line product, it is not practical to translate every piece of message into the other language.

The most relevant work was done by Ling et al. [13]. In their work, they translated the Chinese web pages into En-glish, and the common parts of two languages are extracted and used for classi cation. Speci cally, the KL divergence between the feature distribution and the label distribution is minimized to generate the classi cation model. This work bridges the translated English and label set by common fea-tures, and uses a parametric algorithm to build the classi er. In our work, we make use of non-parametric transfer learn-ing algorithm to migrate the information learnt from the machine translated Spanish and apply it to the human writ-ten Spanish. To the best of our knowledge, this is the rst attempt to take care of the distribution difference between the machine translated corpus and human written corpus in social network. Moreover, we propose an adaptive learning algorithm to update the model online.
In this paper, we develop a machine learning pipeline to tackle the multilingual spam detection problem at LinkeIn. We propose to build a model pool in which one model works for one language, and we start with Spanish as it is widely used in US. Due to the lack of labeled data in Spanish, we rst translate the English labeled content into Spanish con-tent via machine translation. Since the distributions of ma-chine translated Spanish and human written Spanish are not identical, we propose a two-step transfer learning algorithm to transfer the knowledge learnt from machine translated Spanish to human written Spanish. In the rst step, KMM is applied in order to match the marginal distribution dif-ference between the two domains. If there is a small portion of labeled data available in the human written Spanish, we propose a second step to incrementally update the model obtained from the rst step. Results show that our pro-posed pipeline achieves the requirements in Spanish spam detection for industrial products.

The proposed pipeline is a generic system which can be applied to any other languages by substituting the target lanaguage in machine translation. We plan to extend our proposed algorithms to solve the French and Chinese spam detection problems in the future. In addition, the classi -cation pipeline can deal with different kinds of classi cation problems as long as there are labeled samples available. We also plan to apply the pipeline to other text classi cation problems at LinkedIn. Furthermore, the adaptive learning algorithm we propose in the second step can serve as an online learning model. When more labeled data in Spanish become available, the model will be adaptively updated. [1] M.-R. Amini and C. Goutte. A co-classi cation [2 ] C. Banea, R. Mihalcea, and J. Wiebe. Multilingual [3] A. Beck and M. Teboulle. A fast iterative [4] W. De Smet, J. Tang, and M.-F. Moens. Knowledge [5] C. Do and A. Y. Ng. Transfer learning for text [6] L. Duan, I. W. Tsang, D. Xu, and S. J. Maybank. [7] G. Foster, C. Goutte, and R. Kuhn. Discriminative [8] J. Hoffman, E. Rodner, J. Donahue, K. Saenko, and [9] J. Huang, A. Gretton, K. M. Borgwardt, B. Sch  X  olkopf, [10] J. Jiang. Domain adaptation in natural language [11] A. M. Kibriya, E. Frank, B. Pfahringer, and [12] D. Kurokawa, C. Goutte, and P. Isabelle. Automatic [13] X. Ling, G.-R. Xue, W. Dai, Y. Jiang, Q. Yang, and [14] X. Meng, F. Wei, X. Liu, M. Zhou, G. Xu, and [15] J. Pan, G.-R. Xue, Y. Yu, and Y. Wang. Cross-lingual [16] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, [17] J. C. Platt, K. Toutanova, and W.-t. Yih.
 [18] P. Prettenhofer and B. Stein. Cross-language text [19] J. Slocum. A survey of machine translation: its [20] Q. Sun, R. Chattopadhyay, S. Panchanathan, and [21] J. Tang, X. Wang, H. Gao, X. Hu, and H. Liu. [22] R. Tibshirani. Regression shrinkage and selection via [23] A. Vinokourov, N. Cristianini, and J. S. Shawe-taylor. [24] C. Wan, R. Pan, and J. Li. Bi-weighting domain [25] J. Wang, J. Zhou, P. Wonka, and J. Ye. Lasso [26] P. Wang and C. Domeniconi. Towards a universal text [27] S. Xiang, T. Yang, and J. Ye. Simultaneous feature [28] J. Yang, R. Yan, and A. G. Hauptmann.

