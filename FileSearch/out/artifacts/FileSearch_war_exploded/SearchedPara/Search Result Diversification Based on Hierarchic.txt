 A large percentage of queries issued to search engines are broad or ambiguous. Search result diversification aims to solve this problem, by returning diverse results that can ful-fill as many different information needs as possible. Most existing intent-aware search result diversification algorithm-s formulate user intents for a query as a flat list of subtopics. In this paper, we introduce a new hierarchical structure to represent user intents and propose two general hierarchical diversification models to leverage hierarchical intents. Ex-perimental results show that our hierarchical diversification models outperform state-of-the-art diversification methods that use traditional flat subtopics.
 Search result diversification; hierarchical diversification; hi-erarchical intents
In web search, the majority of short queries are ambigu-ous or broad when it comes to specifying a user X  X  information need [16, 18, 25, 33, 34]. For example, by issuing an ambigu-ous query [apple] , one user might be searching for infor-mation about the IT company Apple, whereas another user might be looking for information about the fruit. By issu-ing a broad query [harry potter] , a user may want to seek contents covering various aspects, such as [harry potter movie] , [harry potter book] ,or [harry potter charac-ters] within this broad topic. Traditional search may fail to cover these different intents in the top ranks.
As an effective way to solve this problem, search result di-versification, which aims to return diverse search results that cover as many user intents as possible, has received a lot of attention in recent years. Many search result diversification algorithms [1, 2, 5, 14, 23, 25, 26, 31, 32, 36, 37, 42] and evaluation metrics [1, 4, 8, 9, 28, 39] have been developed to Corresponding Author c improve and evaluate search result diversity. Some public search result diversification evaluation tasks, including the TREC Web Track Diversity task [11] and the NTCIR In-tent/IMine task [22, 27], have been organized to evaluate diversification approaches via public test collections.
In diversification, most existing algorithms generalize user intents in a flat list of independent subtopics, such as topical categories [1, 35], query reformulations by search engine [12, 13, 30], words or phrases extracted from top retrieved docu-ments [3, 13], or combined subtopics from multiple external resources [14, 17]. For instance, Santos et al. [30] repre-sented different query intents as a set of Google suggestions or related queries. Dang and Croft [13] extracted words or phrases from top retrieved documents. Dou et al. [14] mined subtopics from different types of resources.

In typical search result diversification tasks (such as TREC and NTCIR), the intents of a query are predefined by hu-man labellers. To achieve good performance, it is criti-cal to automatically mine subtopics for a query and the mined subtopics should properly match the predefined in-tents. However, as the intents and subtopics are indepen-dently obtained, it is not easy to match them, especially when a flat intent list is used.
 Let us take the query  X  X efender X  (topic number 20) in TREC 2009 [6] as an example. Table 1 shows six man-ually defined intents for it, including  X  X indows Defender Homepage X ,  X  X and Rover Defender X ,  X  X efender Marine Sup-ply X ,  X  X efender Arcade Game Online X ,  X  X indows Defend-er Reports, X  and  X  X hicago Defender Newspaper X . To mine subtopics for the query, we use the approach proposed in [12, 13, 30]. We send the query  X  X efender X  to a commer-cial search engine and get back query suggestions  X  X efend-er windows X ,  X  X efender arcade game, X  and  X  X efender land rover X  (The first-level subtopics in Figure 1). When we take them as subtopics for the query X  X efender X , we find that they are too coarse to distinguish the user intents. For instance, the subtopic  X  X efender windows X  ( t 1 ) covers both the user intents  X  X indows Defender homepage X  ( s 1 ) and  X  X indows Defender Reports X  ( s 5 ). So the diversity algorithm has the risk to only select documents from subtopic t 1 with respect to the intent s 1 without covering subtopic s 5 ,orviceversa.
An easy remedy to the above problem is to use fine-grained intents in diversification. By further sending the three subtopic-s as queries to the search engine, we can get their query sug-gestions as fine-grained subtopics (the second-level subtopics in Figure 1). With the fine-grained subtopics, we can do a better job to distinguish user intents. For example,  X  X e-fender windows home X  ( t 1 , 1 ) and  X  X efender windows prob-Table 1: Subtopics of query  X  X efender X  in TREC 2009 Web Track. no. subtopic description s 1 I X  X  looking for the homepage of Windows Defend-s 2 Find information on the Land Rover Defender s 3 I want to go to the homepage for Defender Marine s 4 I X  X  looking for information on Defender, an arcade s 5 I X  X  like to find user reports about Windows De-s 6 Take me to the homepage for the Chicago Defender lems X  ( t 1 , 3 ) can be easily matched to user intents  X  X indows Defender homepage X  ( s 1 ) and  X  X indows Defender Report-s X  ( s 5 ). However, the fine-grained subtopics bring in a new problem that multiple subtopics are matched to the same us-er intent. For example,  X  X efender land rover for sale X  ( t  X  X efenderlandroverusa X ( t 3 , 2 ), and  X  X efender land rover parts X  ( t 3 , 3 ) are all related to the user intent  X  X and Rover Defender X  ( s 2 ). When simply combining the fine-grained subtopics in a flat list, the diversity algorithm may first se-lect one document from subtopic t 3 , 1 ( X  X efender land rover for sale X ), and then select another document from subtopic t 2 ( X  X efender land rover usa X ), unaware of the fact that the selected documents match the same user intent s 2 ( X  X and Rover Defender X ).

To solve the above problems, we explore a new way of organizing subtopics in a hierarchical structure. Figure 1 shows the two-level hierarchical subtopic structure for the query  X  X efender X . We can see that the six user intents in Ta-ble 1 are mapped to the subtopics in both two levels. This hierarchical structure maintains user intents with differen-t granularity levels and the relationships between different levels. It provides the flexibility of applying and balancing different levels of intent granularity in diversification, which ultimately increases the probability of correctly matching more diverse intents.

In this paper, we propose a diversification framework to explicitly leverage the hierarchical intents. In particular, we extend the state-of-the-art diversification algorithms xQuAD and PM2, and propose a Hierarchical xQuAD model (HxQuAD) and a Hierarchical PM2 model (HPM2). These hierarchical models select documents that maximize diversity in the hi-erarchical structure. For the example in Figure 1, if the pre-viously selected documents have already cover the subtopic  X  X efender windows home X , the next selected document for the intent  X  X efender windows X  should better come from the subtopic X  X efender windows download X  X r X  X efender windows problems X . Without the hierarchical structure, it is dif-ficult for traditional diversification methods to distinguish and balance these second-level subtopics. Moreover, after selecting documents for the second-level subtopics  X  X efend-er windows home X  and  X  X efender windows problems X , which belong to the same first-level subtopic  X  X efender windows, X  Figure 1: Two-level hierarchical subtopics of query  X  X efender X  from query suggestions of a commercial search engine. the next document is better to be related to another first-level subtopic such as  X  X efender arcade game X  or  X  X efender land rover X . This situation is not considered in traditional diversification methods when only the flat intent list is used.
We argue that real user intents are in a hierarchical struc-ture sometimes. Recent work [15, 19] understands queries by query facets and represents query facets by multiple groups of facet items, where these query facets and their facet items can be viewed as a two-level hierarchical explanation of the query. In Table 1, real user intents of the query also lie in a two-level hierarchy. It contains five first-level subtopics  X  X indows Defender X ,  X  X and Rover Defender X  ( s 2 ),  X  X efend-er Marine X ( s 3 ), X  X efender Arcade Game X ( s 4 ), and X  X hicago Defender Newspaper X ( s 6 ). The subtopic X  X indows Defend-er X  contains two second-level subtopics  X  X indows Defender Homepage X  ( s 1 ) and  X  X indows Defender Reports X . To mea-sure the real user satisfaction on the query, it would be ideal to evaluate diversity based on hierarchical intents. Unfor-tunately we do not have such diversity judgment data and evaluation metrics. In this paper, we still use existing met-rics to evaluate our algorithms.

Our experiments are conducted on two-level hierarchical subtopics which are automatically extracted from a com-mercial search engine. The algorithms are evaluated on the public TREC [7] dataset and the NTCIR [22] dataset. Ex-perimental results show that by using hierarchical subtopics, our hierarchical algorithms (HxQuAD and HPM2) outper-form most state-of-the-art models including xQuAD [30], P-M2 [12], TxQuAD, and TPM2 [13], in terms of ERR-IA [4],  X  -NDCG [8], NRBP [9], and D -nDCG [28]. Even when only using single-level subtopics in the hierarchical structure, our hierarchical algorithms still outperform their corresponding algorithms which represent the same subtopics in a flat list, in terms of ERR-IA,  X  -NDCG, and NRBP. The results show that exploiting the hierarchical intent structure can benefit search result diversification.
In early diversification algorithms, query intents were im-plicitly considered to promote diversity by selecting docu-ments with little content similarity. Maximal Marginal Rel-evance (MMR) [2] measured documents with cosine simi-larities in vocabulary, and attempted to reduce redundan-cy while maintaining query relevance in re-ranking. Some work estimated document similarity in different ways, such as using Kullback-Leibler divergence [38], ranking sentences by random walks in an absorbing Markov chain [42], and modeling directed graphs based on the document link struc-ture [40]. These approaches assume that similar documents will cover similar query intents, without considering exactly which query intents are being covered.

Many researchers noticed the above problem and explicit-ly considered query intents for diversification. IA-Select [1] developed an intent-aware diversifying method by classified topical categories for queries and documents based on ODP taxonomy. xQuAD [30] used a greedy algorithm to maxi-mize the coverage of query aspects. RxQuAD [35] explic-itly provided a relevance formulation for query aspects. P-M2 [12] divided diversification into two processes: finding the best unsatisfied topic-by-topic proportionality, and then choosing the best document based on the selected subtopic. Intrinsic diversity [26] predicted the successor queries for initiator query to seek which content to cover. Yu and Ren [36] formulated diversification as a 0-1 multiple subtopic knapsack problem. Fusion diversification [21] inferred laten-t subtopics based on topic modeling. Some work promoted diversity by leveraging subtopics from multiple external re-sources, such as involving user clicks to help query aspect-s [24], combining subtopics from different data types [14, 17]. Although these approaches generate query intents from var-ious sources, combinations or models, they commonly rep-resent intents in a flat list. In contrast, our work utilizes hierarchical intents and provides hierarchical frameworks to promote diversity of search results.

Some researchers use machine learning techniques to di-versify search results, such as Structural SVMs [37] and R-LTR [43]. In this paper, we focus on the unsupervised intent-aware diversification. More specifically, we extend xQuAD to hierarchical novelty-based model, and adapt PM2 for hi-erarchical proportionality-based model.

There are a few prior art that are somewhat similar to ours. Term level diversification [13] represented a subtopic by a set of key terms, which is similar to our idea of denot-ing a subtopic with a group of child subtopics in hierarchical subtopic structure. Unlike considering hierarchical informa-tion as we do, term level diversification integrated all terms to build subtopics. We implement their basic models as our baselines in our experiments. Concept hierarchy based diversification [41] considered subtopic relations in result di-versification. Instead of providing hierarchical frameworks to handle hierarchical subtopics, it exploited concept hier-archies to extract query subtopics in a flat list, utilized hier-archical relations of subtopics to propose a structural simi-larity function for subtopics, and incorporated this function into the traditional xQuAD framework. This function it-eratively selected documents covering important subtopics that are less structurally similar to the subtopics covered by the selected documents. This work was done in enterprise search domain, as it is hard to build high-quality concept hierarchies in web search. We implement this method by adopting our hierarchical subtopics used in our hierarchical models as their input concept hierarchy.

There have been some approaches [15, 19, 20, 29] that mine hierarchical information for a query. In the present Figure 2: An example of hierarchical subtopic tree. study, we use Google suggestions as the source of hierarchi-cal subtopics, following previous work [12, 13, 30]. We will explore our own subtopic mining methods in future work.
As mentioned earlier, most intent-aware diversification al-gorithms model user intents as a group of subtopics and boost diversity based on them. In this paper, we propose to present subtopics in a hierarchical structure.

Formally, we use T q = { t 1 ,t 2 , ... } to indicate a set of first-level subtopics { t i 1 } for query q where i 1 =1 , 2 ... is the position of subtopic t i 1 in T q . For subtopic t i 1 ,weuse T { t i 1 , 1 ,t i 1 , 2 , ... } to denote a set of its child subtopics For each subtopic t i 1 ,i 2  X  T i 1 , i 2 istherelativeindexof we use t i 1 ,.,i j to denote a subtopic at level j and use T to denote a set of its child subtopics. t i 1 ,.,i j +1  X  t
Figure 2 shows an example of the hierarchical subtopics in two levels. The first level contains two subtopics t 1 and the second level has four subtopics t 1 , 1 ,t 1 , 2 ,t t 1 and t 1 , 2 are child subtopics of t 1 ,and t 2 , 1 and t child subtopics of t 2 . Here exists T 1 = { t 1 , 1 ,t 1 { t
Let us reuse the example topic  X  X efender X  in Figure 1 to illustrate the process. The subtopic X  X efender windows X  X on-tains three child subtopics:  X  X efender windows home X  shows users want the homepage of the software,  X  X efender windows download X  indicates the download requirement of the soft-ware, and  X  X efender windows problems X  denotes users are interested in the problems of the software. Similarly, the first-level subtopics  X  X efender arcade game X  and  X  X efender land rover X  have their own second-level child subtopics.
It is worth noting that term level diversification [13] also indicates a subtopic by a set of terms t i = { t 1 i ,t 2 They treat each t j i as an independent subtopic, and inte-grate them together to build a larger flat term level subtopics { t m is not aware of the relationship between t j i and t i , and the relationship between two terms. In contrast, we maintain the subtopics in hierarchy structure and diversify search re-sults based on these hierarchical subtopics.

Foragivenquery q ,weuse R = { d 1 ,d 2 , ..., d m } to de-note its initial ranked documents set. For traditional di-versification algorithms that use a flat list of subtopics, we use T = { t 1 ,t 2 , ..., t n } to denote subtopics of the query. Let P ( d | q ) be the probability that document d is relevan-ttoquery q , P ( d | t ) indicate the probability that d satisfies subtopic t ,and P ( t | q ) denote the importance of subtopic t for query q . Existing diversification algorithms use T , ments D out of R . Similarly, for hierarchical diversification, we define the following probabilities: a subtopic t i 1 ,.,i j +1 with respect to its parent subtopic t We assume that t i 1 ,.,i j is fully covered by its child subtopic set T i 1 ,.,i j and each of the child subtopic is independent to each other. Hence we have: In Figure 2, we have P ( t 1 , 1 | t 1 )+ P ( t 1 , 2 | t P ( t 2 , 2 | t 2 )=1.
 with respect to the query q .Thewayofcalculating P ( t i 1 ,.,i j may vary in different applications. If the importance of leaf subtopics is known (for example, when parent subtopics are generated by clustering child subtopics), we can update the importance of their ancestors by iteratively summing up the importance of child subtopics, i.e., we have:
In some other cases, we may just know the importance of the first-level subtopics. For example, when building the hierarchy subtopics based on Google suggestion, we need to first retrieve first-level subtopics, and then retrieve second-level subtopics by issuing the first-level subtopics as queries. In this case, we may calculate the weight of each child subtopic by using Bayes X  X  formula: In Figure 2, if P ( t 1 | q )isgiven, P ( t 1 , 1 | q )= P ( t P ( t 1 , 2 | q )= P ( t 1 | q )  X  P ( t 1 , 2 | t 1 ). If P ( t known, P ( t 1 | q )= P ( t 1 , 1 | q )+ P ( t 1 , 2 | q ). sume P ( q | t i 1 ,.,i j ) = 1. We assume that leaf subtopics t are usually represented as words or phrases, and we can di-model or other retrieval model. For non-leaf subtopics, in-stead of words or phrases, they may be organized as groups of their child subtopics (e.g., when second-level subtopics are Google suggestions, and the first-level subtopics are cluster-s of these suggestions). In this case, we use a bottom-up method to recursively calculate P ( d | t i 1 ,.,i j ) for a subtopic t satisfy t i 1 ,.,i j +1 . The product denotes the probability that d fails to satisfy every child subtopic for t i 1 ,.,i j . One minus that product equals the probability that d will satisfy at
The topic novelty model, inspired by MMR [2], is a widely used framework in diversification. It considers both the rel-evance between the document and the query, and the topic diversity of the document among the selected documents. It iteratively selects a next best document d that is rele-vant to query q and can maximize the diversity of selected documents D . The formulation of the model is as below.
Different probabilistic models are proposed to measure the diversity  X ( d, D ) in previous work [1, 3, 30]. We select xQuAD, one of the state-of-the-art diversification method-s, to adapt a diversity model to use hierarchical structured subtopics. xQuAD explicitly estimates the diversity of a document by calculating the coverage of matched subtopics. In the above equation, (1  X  P ( d | t )) indicates the probability that an existing document d does not satisfy t . The prod-uct shows the probability that all the selected documents D fail to satisfies t . Summing up over all subtopics, weighted by P ( t | q ), the diversity is the probability that d covers the subtopics while the existing document list D fail to satisfy.
However, Equation (4) is designed for the subtopics formed as a flat list, which may fail when real user intents are hi-erarchical. We take the query intents shown in Figure 2 as an example. Assume there are four documents: d 1 and d 2 relevant to t 1 , 1 , d 3 relevant to t 1 , 2 ,and d 4 relevant to t One of the ideal rank lists is ( d 1  X  d 4  X  d 3  X  d 2 )and the diversity is maximized within the top three results. If we just use the first-level subtopics in xQuAD, the returned diversified rank list might be ( d 1  X  d 4  X  d 2  X  d 3 ). In the third iteration, it fails to distinguish the difference between d and d 3 because both are relevant to t 1 .Ifwejustusethe second-level subtopics in xQuAD, the resulting list might be ( d 1  X  d 3  X  d 4  X  d 2 ). In this case, it assumes that d and d 4 are equally important because both can offer a new subtopic, but in fact, d 4 is better because both d 3 and d belong to subtopic t 1 .

To solve the above problem, we adapt xQuAD so that it can handle hierarchical subtopics. We propose HxQuAD ,a hierarchical xQuAD model, to explicitly model result diver-sity based on the hierarchical subtopics. Specifically, at each level j of the hierarchical subtopic tree, HxQuAD estimates result diversity by:
 X ( d, D, j )= Here | i 1 ,.,i j | = j means t i 1 ,.,i j is a subtopic at level j . which is the same as Equation (2). This model evaluates the importance of document d basedonwhetheritcanimprove overall diversity in terms of the subtopics at level j .
We then combine these components, and evaluate the overall importance of a document in terms of all levels with-in the hierarchy. A parameter  X  is introduced to control the granularity of subtopics that the diversification tends to optimize for. We have:
 X ( d, D )=  X   X   X ( d, D, 1) + (1  X   X  )  X   X ( d, D, 2)+ Here  X   X  (0 , 1] and  X  =0 . 5 indicates that all the subtopic levels are equally weighted. A value larger than 0.5 means that the algorithm tends to diversify result based on coarse subtopics; whereas a value lower than 0.5 indicates that it provides fine-grained diversify. Specially, if  X  = 1, then it only uses the first-level subtopics; whereas, if  X  is close to 0, the model tends to take the leaf subtopics. Note that  X  could be 0 if there are only two levels in hierarchy.
In summary, HxQuAD extends xQuAD to hierarchical subtopics by redefining a multi-level diversity function. Be-sides balancing the relevance and diversity by parameter  X  , we use a parameter  X  to control the impact of subtopics at different depth.
Instead of considering diversity of subtopics and docu-ments at the same time, the topic proportionality based di-versification model selects subtopics and documents sepa-rately. At each iteration, the algorithm first selects the best subtopic based on the proportionality strategy, and then finds the most relevant document optimized for the selected subtopic.

PM2 [12] is one of the state-of-the-art topic proportional-ity based diversification algorithms. It considers the diver-sification problem as assigning seats to members of compet-ing political parties and follows a highest quotient (Sainte-Lague) method to select subtopics as allocating seats. In the beginning, PM2 computes the quotient qt i for each subtopic t by the Sainte-Lague formula 1 .
To maintain the proportionality of the subtopic distribu-tion, PM2 assigns the subtopic with the largest quotient as function  X ( d, D, t  X  ) to find the document d  X  that is most where
 X ( d, D, t  X  )=  X   X  qt i  X   X  P ( d | t i  X  )+(1  X   X  )  X  After document d  X  is selected, to punish its highly relevant subtopics, PM2 increases the  X  X ortion X  of occupied seats s for each subtopic t i by its normalized relevance to d  X  .
Sainte-Lague: http://www.elections.org.nz/voting/ mmp/sainte-lague.html The algorithm repeats the above process to iteratively select next best documents from R to D .

In this paper, we modify the framework of PM2 to adap-t hierarchical subtopics and propose the hierarchical PM2 model ( HPM2 ). HPM2 maintains the basic idea of find-ing the best document based on the preselected subtopic by proportionality. Moreover, since each level of hierarchical subtopics may contain different diversity information, HP-M2 selects one best subtopic for each level of the subtopic tree, and combine them together to find the best document. Considering Figure 2 as an example, we may select t 1 with max quotient from the first-level and t 1 , 1 with max quotient for the second-level, and find the best document based on them. Notethatwemaychoose t 1 and t 2 , 1 sometimes, as the best subtopics are selected independently.

First of all, HPM2 computes the quotient values for the subtopics in each level, respectively. For the subtopic t at level j , the quotient is similarly formulated as Equa-tion (7). Note that P ( t i 1 ,.,i j | q ) is the probability that t satisfies q . Comparing all the quotients of the subtopics at level j , HPM2 selects the best subtopic t  X  i 1 ,.,i j with max quotient respectively chosen from level 1 , 2 , ..., n in hierarchy. HPM2 calculates the document diversity for each level of hierarchi-cal subtopics and combines all to select the best document.
For level j in the hierarchical subtopics, since t  X  i 1 ,.,i j preselected subtopic at level j , according to the diversity definition in PM2, a document is more diverse if it is relevant Therefore, we define  X ( d, D, t  X  i 1 ,.,i j )as: t and the selected subtopic t  X  i 1 ,.,i j . We use it because treat-ing all the unselected subtopics equally is not fair in the hierarchy. A close subtopic t , which may share common parent, grandparent, or ancestor with t  X  , is more related to t than other subtopics. For instance, if t 1 , 1 is the select-ed subtopic, t 1 , 2 is usually more semantically related to t than t 2 , 1 and t 2 , 2 . So we should assign a higher weight to t 2 than t 2 , 1 and t 2 , 2 in Equation (11). We use the following function to evaluate the weight of a subtopic based on its distance to the selected subtopic.
 where dis ( t, t  X  ) is the length of the path for moving from t to t . Since both subtopics are at level j , the maximal distance between t and t  X  is 2 j . It is used to normalize the distance. Considering the example in Figure 2, we have P ( t 2 | t 1 1  X  2+1) / (2  X  1) = 0 . 5, P ( t 1 , 1 | t 1 , 2 )=(2  X  2  X  2+1) / (2 and P ( t 2 , 1 | t 1 , 2 )=(2  X  2  X  4+1) / (2  X  2) = 0 . 25.
Based on Equation (11), we further combine all levels, and find the best document d  X  by the following formula. d  X  = arg max (1  X   X  ) 2 Similar to Equation (6), parameter  X   X  (0 , 1] is used to control the impact of subtopic granularity.

At last, HPM2 updates the occupied seat s i 1 ,.,i j for the subtopic t i 1 ,.,i j based on the selected document d  X  as follows. Note that the update is respectively done for each level in hierarchy, as the best subtopic is selected at each level.
In short, HPM2 selects each best subtopic by proportion-ality, finds the document based on selected subtopics and updates the occupied seats by the chosen document, follow-ing the steps of PM2. In addition, HPM2 performs subtopic selecting and seats updating at each level of the hierarchi-cal subtopics and finds the best document by considering n selected subtopics at the same time. It uses a distance function to control the influence of the unselected subtopics by considering their distances to selected subtopics.
We experiment with the proposed algorithms on four topic sets provided by TREC Web Tracks from 2009 [6] to 2012 [7]. Every topic set contains 50 topics, each of which includes three to eight subtopics. Topics 95 and 100 in the TREC 2010 topic set were removed as they lack diversity relevance judgments. Following the official task definitions, we use the ClueWeb09 [10] document collection for the four topic sets. We merge the four datasets into one and name it as TREC in this paper.

For each topic, we retrieve top 1,000 documents using the batch search service provided by Lemur project 2 . Similar to existing approaches [12, 13], we remove the documents with spam score larger than 70 using the Waterloo Spam Filter 3 for ClueWeb09. We take these filtered documents as our initial non-diversified ranking results, and conduct our ex-perimental results on top 50 documents as existing work has found that both xQuAD and PM2 achieve their best perfor-mance when using 50 documents [12]. We estimate the rele-vance between documents and topics by the language model used in Lemur 2 . And we use the same model to calculate the relevance between documents and subtopics.

In addition, we use the dataset provided by the IMine (Intent Mining) task in NTCIR-11 [22]. We use the Chi-nese topic set which is comprised of 18 clear queries and 32 ambiguous or broad queries, and name this dataset as NT-CIR in this paper. We evaluate the algorithms using the 32 ambiguous or broad queries, as diversity relevance judg-ments are not provided for the clear queries. We use the
Batch Service Clueweb09: http://boston.lti.cs.cmu. edu/Services/clueweb09_batch/
Waterloo Spam Filter: http://plg.uwaterloo.ca/ ~gvcormac/clueweb09spam/ official non-diversified baseline retrieval results from the So-gouT2008 4 corpus provided by the organizers. Note that, in contrast to the TREC data, IMine task provides two-level human annotated subtopics and we can evaluate diversity using both coarse-grained and fine-grained search intents.
We use ERR-IA [4],  X  -NDCG [8], and NRBP [9], which are official evaluation metrics at TREC Web Track, to eval-uate result diversity. They measure the diversity of a result list by explicitly rewarding novelty and penalizing redun-dancy observed at every rank. We use the same parameters as those used in official TREC tasks, and hence per-subtopic graded relevance assessments are treated as binary. In addi-tion, we use D -measures [28], the primary metric that used in NTCIR IMine task, which actually utilizes graded diver-sity relevance judgments. All metrics are computed based on the top 20 ranking results, consistent with the official tasks in TREC 2010-2012. Moreover, we use the two-tailed paired t-test for statistically significance testing and report a significant difference if the p-value is lower than 0.05.
Similar to previous work [12, 30], we use query suggestions extracted from Google search engine as subtopics (as shown in Figure 1). To avoid Google X  X  personalized suggestions, we clean the cookies and set the location to United States before query suggestion crawling.

For each topic, we collect its query suggestions from Google as the first-level subtopics. To generate subtopic hierar-chy, we further issue the first-level subtopics as queries to Google and retrieve their query suggestions as the second-level subtopics. Sometimes Google fails to provide sugges-tions for some queries. When a topic has no suggestion, we view this topic as invalid and omit it. If a first-level subtopic from a valid topic has no suggestion, we add itself as its second-level subtopic to ensure the two-level struc-ture. Finally, we collect 1,696 first-level subtopics and 10,527 second-level subtopics for 194 queries. We only consider two-level hierarchical subtopics in this paper, and leave the investigation of using third and deeper levels to future work.
Consistent with existing research [30, 12], we assume a u-niform probability distribution for all the first-level subtopic-subtopics. We also assume a uniform probability distribu-tion for the second-level subtopics with respect to their par-ent subtopics. We use Equation (1) to calculate the impor-tance of a second-level subtopic with respect to the query. For example, for the j th second-level subtopic t i,j of the i th first-level subtopic t i , its importance P ( t i,j | q )is where | T i | is the count of second-level subtopics of t
We compare our proposed models with the following base-line approaches: the non-diversified baseline ranking (Base-line), xQuAD, PM2, TxQuAD, TPM2, and ConceptH. We already introduced xQuAD and PM2 in Section 3.2 and we recall that our hierarchical diversification models are ex-tended from them. Term level diversification models, viz., TxQuAD and TPM2 [13], split the original subtopics (used by xQuAD and PM2) into terms and use these terms as
SogouT2008: http://www.sogou.com/labs/dl/t-e.html Table 2: Performance comparison on TREC 2009-2012. The best result is in bold. Statistically signif-icant differences between the hierarchical methods (HxQuAD and HPM2) and the baseline methods (xQuAD, TxQuAD, PM2, TPM2, and ConceptH) are marked with  X  , , ,  X  ,  X  ,respectively.

Baseline .2630 .3610 .2238 .4124 xQuAD  X  .2842 .3822 .2465 .4109 TxQuAD .2792 .3835 .2396 .4189 PM2 .2952 .3990 .2548 .4289 TPM2  X  .2805 .3895 .2385 .4256 ConceptH  X  .3002 .4064 .2607 .4366 HxQuAD .3206  X   X   X  .4229  X   X  .2845  X   X   X  .4378  X   X 
HPM2 .3235  X   X   X  .4234  X   X  .2880  X   X   X  .4381  X   X  their subtopics. In order to maintain the consistency of the subtopics, we did not use the DSPApprox method intro-duced in [13] that extracts terms from search results. Con-ceptH [41] exploits concept hierarchies to find flat query subtopics and infers subtopic relations in traditional diversi-fication by hierarchical similarities of subtopics. We directly take our subtopic hierarchy (See Subsection 4.3) as the con-cept hierarchy in ConceptH, which means that ConceptH shares the same subtopic hierarchy with HxQuAD and HP-M2. All the baseline methods have a parameter  X  that re-quires tuning. Our hierarchical models have two parameters to tune: the traditional parameter  X  and the hierarchical weight parameter  X  .Weusea 5-fold cross validation to tune these parameters in terms of ERR-IA@20 on TREC data and NTCIR data, respectively.

The hierarchical subtopics and all runs can be found on the website: http://www.playbigdata.com/dou/hdiv .
We compare our models with the baseline approaches. For the baseline models, we use the first-level subtopics which is a common approach in existing work [12, 13, 30]. The results are shown in Table 2 and Table 3. We find that: (1) The hierarchical diversification models, i.e., HxQuAD and HPM2, outperform all baseline methods on the TREC 2009-2012 dataset. Table 2 shows that HxQuAD and HPM2 have statistically significant improvements in terms of ERR-IA,  X  -nDCG, NRBP, and D -nDCG (p &lt; 0.05 with two-tailed paired t-tests). Specifically, HPM2 outperforms xQuAD, TxQuAD, and TPM2 by more than three hundredths; out-performs PM2 by more than two hundredths; outperforms ConceptH by more than one hundredth, in terms of ERR-IA,  X  -nDCG, and NRBP. (2) As mentioned before, the diversity on the NTCIR IMine dataset can be evaluated either on the coarse-grained intents or on the fine-grained intents. Table 3 shows that HxQuAD and HPM2 outperform all baseline algorithms in terms of ERR-IA,  X  -nDCG, and NRBP at either the coarse-grained level or the fine-grained level, though their improve-ments are not statistically significant. Please note that there are only 32 topics on NTCIR data. The results indicate that the hierarchical models provide more diverse results than the Table 3: Performance comparison on NTCIR data.
 (a) Evaluated by using coarse-grained intents
Baseline .3044 .4508 .2644 .3952 xQuAD  X  .3146 .4666 .2682 .4079 TxQuAD .3282 .4915 .2852 .4266 PM2 .3200 .4865 .2702 .3596 TPM2  X  .3239 .5115 .2699 .4493 ConceptH  X  .3203 .4807 .2731 .4185 HxQuAD .3436 .4901 .3064 .4094
HPM2 .3449 .5150 .2975 .4507 (b) Evaluated by using fine-grained intents
Baseline .1596 .3497 .1338 .2782 xQuAD  X  .1669 .3616 .1396 .2853 TxQuAD .1717 .3836 .1458 .2929 PM2 .1641 .3695 .1348 .2825 TPM2  X  .1609 .3929 .1222 .3135 ConceptH  X  .1684 .3652 .1403 .2837 HxQuAD .1831 .3823 .1598 .2881
HPM2 .1831 .3994 .1543 .3081 baseline models when users are interested in either coarse-grained subtopics or fine-grained subtopics. (3) Both HxQuAD and HPM2 significantly outperform their corresponding models xQuAD, TxQuAD, PM2, and TPM2 on TREC data, and outperform their corresponding models by more than one to three hundredths on both levels of NTCIR data, in terms of ERR-IA,  X  -nDCG, and NRBP. This indicates that utilizing hierarchical subtopics, even just containing two levels, could help promote result diversity than traditional subtopics formed as a flat list. (4) Recall that ConceptH leveraged subtopic hierarchies to calculate subtopic dependencies for traditional flat subtopic-s. The results show that ConceptH outperforms all the oth-er baseline models on TREC data. However, both HxQuAD and HPM2 outperform ConceptH in terms of all metrics. They significantly outperform ConceptH in terms of ERR-IA and NRBP on TREC data, and outperform ConceptH by more than one to three hundredths on both levels of NTCIR data, in terms of ERR-IA,  X  -nDCG, and NRBP. Therefore, when using subtopic hierarchies in diversification, propos-ing hierarchical frameworks for hierarchical subtopics works better than improving traditional frameworks on subtopic relations for flat subtopics. (5) Consistent with the recent work [12], PM2 performs slightly better than xQuAD in terms of all metrics on the TREC dataset in Table 2. And they perform similarly in the coarse-grained intent of the NTCIR data in Table 3. Ter-m level models TxQuAD and TPM2 perform differently on different datasets. On the TREC data, they underperfor-m their corresponding models xQuAD and PM2 in terms of most metrics. But on NTCIR IMine data, they outper-form their corresponding models in terms of most metrics, and are the top performers in some cases. Based on the study made by Dang et al. [13],theyworkverycloselyto their corresponding models. After analyzing the results, we found that term level models do not work well on topics containing phrases. For instance, considering topic number 165  X  X lue throated hummingbird X  in TREC 2012, it has a subtopic  X  X lue throated hummingbird picture X  meaning to find the pictures of the specific bird. Term level models in our experiments use the split terms  X  X lue X ,  X  X hroated X ,  X  X ummingbird, X  and  X  X icture X  as the subtopics, which may select some noisy irrelevant documents relevant to  X  X lue X  or  X  X icture X . Dang et al. [13] claimed that they could identify phrases within the subtopics (for example, split the above example subtopic to  X  X lue throated hummingbird X  and  X  X ic-ture X ), which may help improve the performance of term-level diversification algorithms. However, we used the Stan-ford Tokenizor 5 and it failed to detect such kinds of phras-es. This might be one of reasons why TxQuAD and TPM2 do not work well in our experiments. For the NTCIR da-ta, we use the ICTCLAS Chinese tokenizer 6 . The tokenizer can recognize named entities and phrases, and may generate better term-level subtopics than on TREC data.
The analysis so far has shown that the proposed hierarchi-cal models using hierarchical subtopics outperform the base-line models with flat-formed subtopics (xQuAD, TxQuAD, PM2, and TPM2). There are at least two possible explana-tions for it. The first is that our two-level hierarchical ap-proach for diversification is indeed effective. The second is that our approach outperformed the baseline models simply because we used extra subtopics (i.e., second-level subtopic-s) while the baseline models only used first-level subtopics.
To clarify this problem, we experiment with baseline mod-els using different levels of subtopics. We want to investi-gate whether our models still outperform baseline models with the same subtopics in a flat list. For fair compari-son, we provide baseline models exactly the same subtopics as hierarchical models, by organizing hierarchical subtopic-s into a flat list, and set a uniform weight for them. We use xQuAD 1st , xQuAD 2nd , and xQuAD all to denote using first-level subtopics, second-level subtopics, and all subtopic-s (merged by the obtained suggestions in two levels) for xQuAD model. Similar symbols are used for other models.
Recall that, in hierarchical models, the impact of first-level subtopics and second-level subtopics are controlled by a parameter  X  according to Equation (6) and Equation (11). When  X  = 1, only the first-level subtopics are used in hi-erarchical models. When  X  = 0, hierarchical models only use the second-level subtopics. Due to space limitation of the paper, we only report the results on the TREC dataset. TheresultsareshowninTable4andTable5.Thebestre-sults are in bold, statistically significant differences between hierarchical methods and their corresponding methods are respectively marked in the upper right corner, and all related parameters are tuned by 5-fold cross validations.
Table 4 and Table 5 show that, by utilizing first-level subtopics, hierarchical models outperform all their corre-sponding models in terms of ERR-IA,  X  -nDCG, and NRBP. Specifically, HxQuAD 1st significantly outperforms xQuAD 1st in terms of all metrics.

The difference between hierarchical models and their coun-terparts is the relevance estimation P ( d | t i ) between the document d and the first-level subtopic t i . In tra-ditional diversification models, i.e., xQuAD 1st and PM2 1st
Stanford Tokenizor: http://nlp.stanford.edu/ software/tokenizer.shtml ICTCLAS: http://www.ictclas.org/ Table 4: Performance comparison of HxQuAD and its corresponding methods using different subtopics. (a) Using first-level subtopics only xQuAD  X  1st .2842 .3822 .2465 .4109 TxQuAD 1st .2792 .3835 .2396 .4189
HxQuAD  X  1st .3054  X  .4041  X  .2683  X  .4259  X  (b) Using second-level subtopics only xQuAD 2nd .2956 .3940 .2609 .4207 TxQuAD  X  2nd .2850 .3903 .2452 .4171 (c) Using all subtopics (both two levels) xQuAD all .2948 .3930 .2572 .4193 TxQuAD  X  all .2898 .3930 .2508 .4185 the probability that document d satisfies subtopic t i , i.e., P ( d | t i ), is calculated by language model [30]. For hierar-chical models, only the relevances between documents and second-level (leaf) subtopics are directly computed as tra-ditional methods. The relevances between documents and first-level (non-leaf) subtopics are estimated by P ( d | 1  X  This design makes our hierarchical models more flexible to handle different subtopic mining algorithms, which may con-tain virtual subtopics at the non-leaf levels in hierarchy.
The results indicate that using the relevance estimation between documents and subtopics in hierarchical models, makes the results more diverse than the traditional way. One of the possible reasons is that, by involving the child subtopics into estimation, finding a relevant document for the subtopic becomes finding a relevant document for its child subtopics. The document related to more child second-level subtopics will be viewed as more relevant to the par-ent first-level subtopic. For example, a first-level subtopic t has two relevant documents d 1 , d 2 . Traditional models find that the documents X  relevances are P ( d 1 | t 1 )=0 . 8and P ( d 2 | t 1 )=0 . 7, and think d 1 is more relevant to t chical models check the child second-level subtopics of t find that d 1 is related to one child subtopic P ( d 1 | t and d 2 is related to two child subtopics P ( d 2 | t 1 , P ( d 2 | t 1 , 2 )=0 . 4. Then hierarchical models treat d relevant to t 1 as P ( d 2 | t 1 )=1  X  (1  X  0 . 6)  X  (1  X  P ( d 1 | t 1 )=1  X  (1  X  0 . 7)=0 . 7. In fact, since d 2 child subtopics, it contains more diversity information than d and should be valued more in diversification. By utilizing the child subtopics to find relevant documents for the first-level subtopics, hierarchical models increase the relevances for the documents covering more child subtopics, and pro-vide better result diversity than traditional models.
When adopting second-level subtopics, hierarchical mod-els still outperform all their corresponding models in terms of ERR-IA,  X  -nDCG,andNRBP,asshowninTable4andTa-ble 5. In particular, HxQuAD 2nd significantly outperforms xQuAD 2nd in terms of ERR-IA and  X  -nDCG, and HPM2 2nd outperforms PM2 2nd by more than one hundredth in terms of ERR-IA,  X  -nDCG, and NRBP.

The difference between hierarchical models and their coun-terparts is the relevance probability P ( t i,j | q ) between Table 5: Performance comparison of HPM2 and its corresponding methods using different subtopics.
 (a) Using first-level subtopics only PM2  X  1st .2952 .3990 .2548 .4289 TPM2 1st .2805 .3895 .2385 .4256
HPM2  X  1st .3070 .4055 .2693 .4280 (b) Using second-level subtopics only PM2 2nd .3054 .4104 .2670 .4380 TPM2  X  2nd .2847 .3925 .2435 .4206 (c) Using all subtopics (both two levels) PM2 all .3022 .4072 .2620 .4353 TPM2  X  all .2879 .3948 .2485 .4211 the query q and the second-level subtopic t i,j .Re-call that, in subtopic hierarchy, we set a uniform proba-bility distribution for the first-level subtopics of the query, subtopics (See Subsection 4.3). And we set a uniform weight for each second-level subtopic with respect to its parent first-level subtopic, i.e., P ( t i,j | t i )= 1 | T i | , where of the child subtopics in the first-level subtopic t i .Then we calculate the relevance probabilities between queries and second-level subtopics by Equation (1) in Subsection 3.1, i.e., P ( t i,j | q )= P ( t i,j | t i )  X  P ( t i | q )= other side, in traditional models, since their subtopics are in a flat list, they set a uniform relevance probability dis-tribution for the second-level subtopics of the query, i.e., P ( t i,j | q )= 1 | T i | ,where | T i | indicates the total number of all the second-level subtopics. For example, consider top-ic number 3  X  X etting organized X  in TREC 2009, which has 9 first-level subtopics and 52 second-level subtopics in to-tal. In traditional models, all second-level subtopics share the same relevance to the query P ( t i,j | q )= 1 52 =0 . 019. In hierarchical models, for a first-level subtopic t 1 ,  X  X etting or-ganized at work X , it has 9 child second-level subtopics and their relevance probabilities to the query are P ( t 1 ,j  X  1 9 =0 . 012; for another first-level subtopic t 2 , X  X etting organized for college X , it has 3 child second-level subtopics whose relevance probabilities to the query are P ( t 2 ,j  X  1 3 =0 . 037.

The results indicate that hierarchical models assign bet-ter relevance probabilities for second-level subtopics with re-spect to queries than traditional models with flat subtopic-s. One possible reason is that, many second-level subtopics may come from some coarse first-level subtopics, so the doc-uments related to these coarse subtopics may be overvalued in traditional models who treat all second-level subtopic-s as equally important. On the contrary, by passing the relevance of queries uniformly from first-level subtopics to second-level subtopics, hierarchical models control the total contribution of second-level subtopics from coarse first-level subtopics in diversification. Continue the upper example, as-sume that document d 1 is highly related to all second-level subtopics from t 1 , P ( d 1 | t 1 ,i )=1 ( i =1,2,.,9), and documen-t d 2 is highly related to all second-level subtopics from t 2 P ( d 2 | t 2 ,j )=1 ( j =1,2,3). Traditional models think d diverse as d 1 is related to more subtopics. P ( d | t 1 ,i P ( t 1 ,i | q )=9  X  1  X  0 . 019=0.171 &gt; P ( d | t 2 ,j 1  X  0 . 019=0.057. Hierarchical models find that d 2 is also important because d 2 is related to more relevant subtopics.
P ( d | t 1 ,i )  X  P ( t 1 ,i | q )=9  X  1  X  0 . 012=0.11 = P ( d P ( t 2 ,j | q )=3  X  1  X  0 . 037=0.11. Accurately, d 1 and d equally important in diversity since either d 1 or d 2 is only re-lated to one first-level subtopic ( t 1 or t 2 ). Therefore, involv-ing first-level subtopics in assigning relevances for second-level subtopics is a fair choice in hierarchical models. The ex-perimental results show that considering first-level subtopics in estimating the relevances between subtopics and queries is very helpful in result diversification.
Table 4 and Table 5 show that, by using all the subtopics, our hierarchical models significantly outperform their corre-sponding models in terms of ERR-IA,  X  -nDCG, and NRBP. This indicate that, when considering two level subtopics in diversification, our hierarchical models with two-level hierar-chy are better than traditional diversification models which merge two-level subtopics in a flat list.

Moreover, for HxQuAD and HPM2, using subtopic hi-erarchy is always better than using single-level subtopics, and the improvements are significant in terms of ERR-IA,  X  -nDCG, and NRBP. This means that incorporating the en-tire hierarchy is better than the sole use of a single-level of subtopics in hierarchical models. Each level plays its own role on diversifying search results.

The results also show that all traditional models with the second-level subtopics outperform their counterparts with the first-level subtopics in terms of most metrics. The im-provements are mostly not significant, but they indicate that the traditional models perform better by using the second-level (fine-grained) subtopics than by using the first-level (coarse) subtopics. In a preliminary study in TREC da-ta, we found that there are more fine-grained or specific subtopics than coarse or general subtopics. For example, for the  X  X efender X  example shown in Section 1, many predefined subtopics (including s 1 , s 4 ,and s 5 ) correspond to second-level subtopics we extracted from the commercial search en-gine. This means that exploiting more fine-grained subtopics could help improve result diversity. xQuAD and PM2 per-form worse when using all subtopics than using second-level subtopics. One possible reason is that the merged subtopics include many overlapped subtopics, which may involve re-dundant documents in search results. So the second-level subtopics is good enough for the flat-subtopic based models, and involving the all subtopics is unnecessary and risky.
In short, our hierarchical models with subtopic hierarchy significantly outperform all their corresponding models with different levels of subtopics in terms of ERR-IA,  X  -nDCG, and NRBP. This indicates that our two-level hierarchical models are indeed effective in diversification.
In this paper, we argued that the user intents covered by a query can be hierarchical. We leveraged hierarchical intents and proposed hierarchical diversification models to promote search result diversification. Specifically, hierarchical diver-sification calculated the document diversity on each level of hierarchical subtopics and combined these diversity scores together to help select the best document. We conducted our experiments with two-level hierarchical subtopics generated automatically from Google suggestions. The experimental results showed that our approaches based on hierarchical subtopics outperformed their counterparts with the tradi-tional subtopics in a flat list. Even when using single-level subtopics, hierarchical diversification also provided reason-able benefits, as these single-level subtopics from hierarchi-cal tree implicitly utilize hierarchical information to help diversify search results, while the traditional diversification algorithms use pure subtopics in a flat list without addition-al information. This work was partially supported by the National Key Basic Research 973 Program of China under grant No. 2014CB340403 and the Fundamental Research Funds for the Central Universities, the Research Funds of Renmin Univer-sity of China No. 14XNLF05 and No. 15XNLF03.
