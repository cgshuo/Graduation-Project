
The tutorial will give an overview of the state of the art
The primary learning objectives of the tutorial are:  X  Provide participants a thorough understanding of the full workflow and the methodological decisions involved in a human judge based (offline) measurement process.  X  Provide a review of the state of the art relating to each step of the evaluation process, covering topics such as sampling methods, experiment and metric design, hu-man (judge) resource management, judging interface design, and methods for the evaluation of judgments and metrics.  X  Provide participants with further insights on offline measurement by means of real world examples and practical considerations both from the literature and from industry.  X  Enable participants to make informed decisions on ex-periment design choices, metric selections, sampling methods and judging system designs based on their own needs.

Evaluation methodologies have played a significant role in the development of information retrieval and knowledge management systems. Human judgments, and evaluation metrics based on human judgments are critical to the ad-vancement of a wide range of applications that rely on la-belled data, such as those that make use of machine learning techniques, e.g., search, recommendation, etc. The labels and evaluation metrics guide the design of a technique, or directly become an objective function on which algorithms are trained.

However, designing suitable metrics, labelling systems and designing complete evaluation workflows and experiments from which reasonable and reliable conclusions can be drawn is no trivial matter. There are many decisions that need to be made when the quality of a system is evaluated, such as which metrics to use, how to obtain a representative sample of tasks, how to collect high-quality human judgments, etc. Erroneous or biased labels or unsuitable metrics can not only invalidate the evaluation of a system, but they can also cause serious issues when used in the training of machine learning components of the system.

In this tutorial, we aim to cover the theory and practice of human judgment based evaluation, including topics such as experiment and task design, human label collection, metric selection and evaluation. We review the end-to-end process of judgment-based evaluation and detail each step of the workflow. We first illustrate the basic principles in designing an offline evaluation experiment. We then describe the key stages, such as metric selection, query (or task) sampling, judging interface design, judge training, evaluation of the adopted metrics and the experiments. This tutorial should be of interest to a wide range of SI-GIR attendees. Many (if not most) SIGIR participants work on areas where the use of human judgments is indispensable for evaluation, including information retrieval (IR), infor-mation extraction, recommender systems, and so on. IR in particular has a long and rich tradition of label-based eval-uation, and many of the techniques for IR evaluation are applicable to other fields.

This tutorial will provide a broad overview of the state-of-the-art, both in the theory and practice of human label based evaluation of search and other types of intelligent al-gorithms. Through this tutorial, those new to the concept of evaluation will come away with a solid understanding of how to design human judgment based evaluation experiments, while those with intermediate knowledge will gain deeper insights and further understanding of the mechanisms used in different stages of the evaluation process.  X  Designing Offline Measurements (20 mins) [23, 24, 25, 26, 27, 33, 34]  X  Metrics for Offline Measurement (50 mins)  X  Break (20 minutes)  X  Human Judgments for Offline Measurement (60 min-utes)  X  Practical Guide (30 minutes)
The target audience includes any researchers and practi-tioners who have a need for human judge based evaluation, for example, to train or evaluate an IR system. Since we also cover many of recent papers in IR evaluation, this tutorial will be also helpful for those who wants to follow up on state-of-the-art research in this area. A college-level background in statistics is required and a basic domain knowledge in IR is helpful but not mandatory.

After attending the tutorial, we expect the audience to design, execute and evaluate human judge-based measure-ment for their need in an effective and efficient manner. They would be understand basic procedure, key decisions and trade-offs to make, and know where to look for when they need more guidance. Also, they will have some code to start with if they want to use some of the metrics explained.
Jin Young Kim graduated from UMass Amherst with Ph.D in Computer Science at 2012. His thesis focused on the re-trieval and evaluation techniques for personal information. He is currently an Applied Researcher at Relevance Mea-surement team in Microsoft Bing, where he spends most of his time in improving the measurement of search quality, consulting on challenging measurement issues, establishing best practice on measurement across the company. He pub-lished dozens of papers in the area of ranking model, user modelling, and evaluation for IR. He was a lecturer for of-fline measurement in recent Microsoft internal course for new employees, and gave numerous talks in conferences in-cluding SIGIR, ECIR, CIKM, WSDM and WWW. Emine Yilmaz is a lecturer (assistant professor) at the Computer Science Department of University College Lon-don. She is also working as a research consultant for Mi-crosoft Research Cambridge. She obtained her Ph.D. from Northeastern University in 2008. Her main interests are information retrieval and applications of information the-ory, statistics and machine learning. She has published research papers extensively at major information retrieval venues such as SIGIR, CIKM and WSDM. She has previ-ously given several tutorials on evaluation at the SIGIR 2012 and SIGIR 2010 Conferences and at the RuSSIR/EDBT Summer School in 2011. She has also organized several workshops on Crowdsourcing (WSDM2011, SIGIR 2011 and SIGIR 2010) and User Modeling for Retrieval Evaluation (SIGIR 2013). She has served as one of the organizers of the ICTIR Conference in 2009 and as the demo chair for the ECIR Conference in 2013. The relationship between IR effectiveness measures and user satisfaction. SIGIR  X 07. E. The good and the bad system: does the test collection predict users X  effectiveness? SIGIR  X 08. result page context, X  in IIiX, 2010. user utility: a conceptual framework for investigation. SIGIR  X 11. Chickering, and Susan T. Dumais,  X  X ere or There, X  in ECIR, 2008. assessor error on IR system evaluation. SIGIR  X 10. preference judgments for novel document retrieval. SIGIR  X 12. Evaluation Measures for Novelty and Diversity.
 SIGIR X 13. Pierre Grinspan. Expected reciprocal rank for graded relevance. CIKM  X 09. Cormack, Olga Vechtomova, Azin Ashkan, Stefan B  X  uttcher, Ian Mackinnon. Novelty and Diversity in Information Retrieval Evaluation. SIGIR  X 08. Ramsey. An experimental comparison of click position-bias models. WSDM  X 08. Dumais, and Thomas White. 2005. Evaluating implicit measures to improve web search. ACM Trans. Inf.
 Syst. 23, 2 (April 2005), 147-168. Increasing evaluation sensitivity to diversity, Information Retrieval, v.16 n.4, p.530-555, August 2013. gain-based evaluation of IR techniques. ACM TOIS, 20(4):422-446, October 2002. sparse data for the language model component of a speech recogniser. IEEE Transactions on Acoustics, Speech, and Signal Processing, 35(3), 400 X 401. Relevance Dimensions in Preference-based IR Evaluation. SIGIR  X 13. precision for measurement of retrieval effectiveness. ACM Transactions on Information Systems, 27(1):2:1-2:27, December 2008. human judgment, X  Trends in cognitive sciences, vol. 11, no. 1, pp. 37-43, 2007. search results using per-intent graded relevance. SIGIR  X 11. [20] Mark Sanderson, Monica Lestari Paramita, Paul Clough, and Evangelos Kanoulas, Do user preferences and evaluation measures line up? SIGIR  X 10. [21] Falk Scholer, Diane Kelly, Wan-Ching Wu, Hanseul S. Lee, and William Webber. 2013. The effect of threshold priming and need for cognition on relevance calibration and assessment. SIGIR  X 13. [22] Paul Thomas and David Hawking, Evaluation by comparing result sets in context, CIKM X  06. [23] Jean Tague-Sutcliffe. The pragmatics of information retrieval evaluation. In Information Retrieval Experiment: Experiment, pages 59 X 102.
 Butterworth-Heinemann, 1981. [24] Jean Tague-Sutcliffe. The pragmatics of information retrieval experimentation, revisited. Inf. Process. Management., 28(4):467 X 490, 1992. [25] Jean Tague-Sutcliffe. The pragmatics of information retrieval experimentation, revisited. Readings in Information Retrieval. lnformation retrieval, pages 205 X 216, 1997. [26] Ellen M. Voorhees. The philosophy of information retrieval evaluation. In CLEF  X 01: Revised Papes from the Second Workshop of the Cross-Language Evaluation Forum on Evaluation of Cross-Language Information Retrieval Systems, pages 355 X 370, London, UK, 2002. Springer-Verlag. [27] Ellen M. Voorhees and Donna K. Harman. TREC: Experiment and Evaluation in Information Retrieval. MIT Press, 2005. [28] Javed A. Aslam, Virgil Pavlu, and Emine Yilmaz. A statistical method for system evaluation using incomplete judgments. In SIGIR  X 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 541 X 548. ACM Press, August 2006. [29] Ben Carterette, James Allan, and Ramesh Sitaraman. Minimal test collections for retrieval evaluation. In SIGIR  X 06: Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, pages 268 X 275, 2006. [30] Ben Carterette and Rosie Jones. Evaluating search engines by modeling the relationship between relevance and clicks. In NIPS  X 07 : Proceedings of Advances in Neural Information Processing Systems, 2007. [31] Ben Carterette, Virgil Pavlu, Evangelos Kanoulas, Javed A. Aslam, and James Allan. If i had a million queries. In Advances in Information Retrieval: 31st European Conference on IR Research, Lecture Notes in Computer Science. Springer-Verlag, April 2009. [32] Matteo Cattelan and Stefano Mizzaro. IR evaluation without a common set of topics. In ICTIR  X 09: Proceedings of the 2nd International Conference on Theory of Information Retrieval, pages 342 X 345.
 Springer-Verlag, 2009. [33] Filip Radlinski, Madhu Kurup, and Thorsten Joachims. How does clickthrough data reflect retrieval quality? In CIKM  X 08: Proceeding of the 17th ACM conference on Information and knowledge management, pages 43 X 52, New York, NY, USA, 2008. ACM. Pskip: estimating relevance ranking quality from web search clickthrough data. In KDD  X 09: Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 1355 X 1364, New York, NY, USA, 2009. ACM. average precision with incomplete and imperfect judgments. In Philip S. Yu, Vassilis Tsotras, Edward Fox, and Bing Liu, editors, Proceedings of the Fifteenth ACM International Conference on Information and Knowledge Management, pages 102 X 111. ACM Press, November 2006. Aslam. A simple and efficient sampling method for estimating AP and NDCG. In Sung-Hyon Myaeng, Douglas W. Oard, Fabrizio Sebastiani, Tat-Seng Chua, and Mun-Kew Leong, editors, SIGIR  X 08: Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 603 X 610. ACM Press, July 2008. J. Cox. Topic (query) selection for IR evaluation. In SIGIR  X 09: Proceedings of the 32nd international ACM SIGIR conference on Research and development in information retrieval, pages 802 X 803, New York, NY, USA, 2009. ACM. Milic-Frayling, Milad Shokouhi, Emine Yilmaz: An uncertainty-aware query selection model for evaluation of IR systems. SIGIR 2012: 901-910 Yilmaz. Evaluating Web Retrieval Effectiveness X . In Web Search Engine Research, chapter Evaluating Web Retrieval Effectiveness, Emerald Library and Information Science Book Series, 2011 Thorsten Joachims: Beyond binary relevance: preferences, diversity, and set-level judgments. SIGIR Forum 42(2): 53-58 (2008) maximum entropy method for analyzing retrieval measures. SIGIR 2005: 27-34 the bootstrap. SIGIR 2006: 525-532 Set Size on Retrieval Experiment Error, ACM SIGIR 2002 Proceedings, pp. 316-323, 2002. System Evaluation: Effort, Sensitivity, and Reliability, ACM SIGIR 2005 Proceedings, pp. 162-169, 2005. evaluation measure stability. In SIGIR  X 00: Proceedings of the 23rd annual international ACM SIGIR conference on Research and development in information retrieval, pages 33 X 40, 2000. Grinspan: Expected reciprocal rank for graded relevance. CIKM 2009: 621-630 [47] Emine Yilmaz, Milad Shokouhi, Nick Craswell, Stephen Robertson: Expected browsing utility for web search evaluation. CIKM 2010: 1561-1564 [48] Mark D. Smucker, James Allan, and Ben Carterette. A comparison of statistical significance tests for information retrieval evaluation. In CIKM  X 07: Proceedings of the sixteenth ACM conference on Conference on information and knowledge management, pages 623 X 632, New York, NY, USA, 2007. ACM. [49] David Banks, Paul Over, and Nien-Fan Zhang. Blind men and elephants: Six approaches to TREC data. Information Retrieval Journal, 1(1-2):7 X 34, 1999. [50] Mihai Georgescu and Xiaofei Zhu. 2014. Aggregation of Crowdsourced Labels Based on Worker History. In Proceedings of the 4th International Conference on Web Intelligence, Mining and Semantics (WIMS14). [51] Matteo Venanzi, John Guiver, Gabriella Kazai, Pushmeet Kohli, and Milad Shokouhi. 2014.
 Community-based bayesian aggregation models for crowdsourcing. In Proceedings of the 23rd international conference on World wide web (WWW  X 14). [52] Falk Scholer, Alistair Moffat, and Paul Thomas. 2013. Choices in batch information retrieval evaluation. In Proceedings of the 18th Australasian Document Computing Symposium (ADCS  X 13). [53] Gabriella Kazai, Emine Yilmaz, Nick Craswell, and S.M.M. Tahaghoghi. 2013. User intent and assessor disagreement in web search evaluation. In Proceedings of the 22nd ACM international conference on Conference on information &amp; knowledge management (CIKM  X 13). [54] Luca Busin and Stefano Mizzaro. 2013. Axiometrics: An Axiomatic Approach to Information Retrieval Effectiveness Metrics. In Proceedings of the 2013 Conference on the Theory of Information Retrieval (ICTIR  X 13). [55] Jinyang Gao, Xuan Liu, Beng Chin Ooi, Haixun Wang, and Gang Chen. 2013. An online cost sensitive decision-making method in crowdsourcing systems. In Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data (SIGMOD  X 13). [56] Djellel Eddine Difallah, Gianluca Demartini, and Philippe Cudr  X e-Mauroux. 2013. Pick-a-crowd: tell me what you like, and i X  X l tell you what to do. In Proceedings of the 22nd international conference on World Wide Web (WWW  X 13). [57] Omar Alonso. 2013. Implementing crowdsourcing-based relevance experimentation: an industrial perspective. Inf. Retr. 16, 2 (April 2013), 101-120. [58] Gabriella Kazai, Jaap Kamps, and Natasa Milic-Frayling. 2013. An analysis of human factors and label accuracy in crowdsourcing relevance judgments. Inf. Retr. 16, 2 (April 2013), 138-178. [59] Carsten Eickhoff and Arjen P. Vries. 2013. Increasing cheat robustness of crowdsourcing tasks. Inf. Retr. 16, 2 (April 2013), 121-137.
