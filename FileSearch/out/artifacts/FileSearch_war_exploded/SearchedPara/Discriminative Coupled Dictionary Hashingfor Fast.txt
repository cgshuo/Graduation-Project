 Cross-media hashing, which conducts cross-media retrieval by embedding data from different modalities into a common low-dimensional Hamming space, has attracted intensive at-tention in recent years. The existing cross-media hashing approaches only aim at learning hash functions to preserve the intra-modality and inter-modality correlations, but do not directly capture the underlying semantic information of the multi-modal data. We propose a discriminative cou-pled dictionary hashing (DCDH) method in this paper. In DCDH, the coupled dictionary for each modality is learned with side information (e.g., categories). As a result, the cou-pled dictionaries not only preserve the intra-similarity and inter-correlation among multi-modal data, but also contain dictionary atoms that are semantically discriminative (i.e., the data from the same category is reconstructed by the similar dictionary atoms). To perform fast cross-media re-trieval, we learn hash functions which map data from the dictionary space to a low-dimensional Hamming space. Be-sides, we conjecture that a balanced representation is cru-cial in cross-media retrieval. We introduce multi-view fea-tures on the relatively  X  X eak X  modalities into DCDH and extend it to multi-view DCDH (MV-DCDH) in order to en-hance their representation capability. The experiments on two real-world data sets show that our DCDH and MV-DCDH outperform the state-of-the-art methods significantly on cross-media retrieval.
 H.3.3 [ Information Search and Retrieval ]: Information Search and Retrieval Coupled dictionary learning; Cross-media retrieval; Hashing
With the rapid development of Internet and social net-work, it has attracted increasing attention to study the cor-relations among multi-modal data. For example, an up-loaded image on the Flickr web site is always tagged with some related descriptions or labels; a microblog may consist of a short text and correlative images. The relevant data from different modalities may have semantic correlations. Therefore, it is desirable to support cross-media retrieval across the data of different modalities, e.g., the retrieval of semantically-related textual documents in response to a query image and vice versa. Due to the large-scale nature of the existing multimedia data over the Internet, efficient retrieval of cross-media is particularly important.
An effective way to speed up the similarity search is the hashing-based method, which makes a tradeoff between ac-curacy and efficiency by approximate nearest neighbor search. The principle of hashing method is to map the high dimen-sional data into compact hash codes and generate the same or similar hash codes for similar data.

The motivation of hashing is to solve the approximate nearest neighbor (ANN) search problem. However, in the cross-media retrieval, the NN cannot be directly obtained as the data may come from different modalities. Therefore, most of the existing hashing approaches are not applica-ble to cross-media retrieval and cross-media hashing method should be specifically studied.

Generally speaking, the existing hashing approaches can be classified into three categories:
Most of the existing hashing approaches are uni-modal hashing. One of the well-known uni-modal hashing method is Locality Sensitive Hashing (LSH) [2], which uses random projections to obtain the hash functions. However, due to the limitation of random projection, LSH usually needs a quite long hash code and hundreds of hash tables to guar-antee good retrieval performance. To make the hash codes compact, several learning based approaches are proposed. Weiss et al. proposed Spectral Hashing (SH) [23] which utilizes the distribution of training data and uses eigenfunc-tion to obtain the hash functions. Compared with LSH, SH achieves better performance since the hash functions cap-ture the manifold structure of the data. Since then, many extensions of SH have been proposed [27, 11, 20, 21, 5, 12].
However, in the real world applications, we can extract heterogenous features from the data and some multi-view hashing approaches are therefore leveraged to boost the re-trieval performance[26, 17]. The principal idea of them is to learn the hash functions while preserving the local struc-tures of each individual feature and globally considering the consistency of multi-view features.

Cross-media hashing is a new research area and there has been only limited research efforts focusing on it so far [3, 9, 28, 29, 15, 30, 25]. Most of the existing cross-media hash-ing approaches share the common idea of learning different hash functions individually for each modality and map the data from different modalities to a shared low-dimensional Hamming space. However, such a binary embedding strate-gy often results in poor indexing performance for the shared embedding space is not semantically discriminative, which is significantly important for cross-media retrieval.
In this paper, we propose a cross-media hashing frame-work titled Discriminative Coupled Dictionary Hashing (D-CDH). Firstly, data from different modalities along with their classes or categories are jointly utilized to learn the both discriminative and coupled dictionaries. The discrimi-native capability indicates that data from same category will have similar sparse representation (i.e., sparse codes), and the coupling means not only intra-modality similarity but also inter-modality correlation will be preserved. As a re-sult, DCDH assigns an explicit semantic meaning (i.e., top-ic) to each dictionary atom in multi-modal dictionaries and thus makes the sparse representation for the multi-modal data interpretable . Secondly, the obtained sparse codes for the data over their corresponding dictionary are exploited to learn the hash functions and further transform the sparse codes to compact binary hash codes.

Furthermore, we find that the representation capability of the dictionaries from different modalities varies and an  X  X n-balanced X  representation may adversely influence the per-formance of cross-media hashing. To address this problem, we additionally incorporate multi-view features into DCDH to enhance the representation capability of the dictionaries from the relatively  X  X eak X  modalities. This extended version of DCDH is named Multi-View DCDH (MV-DCDH).

The main contributions of this paper are three-fold:
The rest of the paper is organized as follows: In Section 2, we review the related work of dictionary learning and cross-media hashing approaches. In Section 3, we give out the detailed explaination of our DCDH and its multi-view extension MV-DCDH. The complexity of DCDH is analyzed in Section 4. Experimental results and comparisons on two real-world data sets are demonstrated in Section 5. Finally, the conclusions are given.
Beyond the traditional dictionary learning approaches [24, 1], coupled or semi-coupled dictionary learning approaches [6, 22] attempt to learn dictionaries for multi-modal data by minimizing the reconstruction error of each dictionary and preserving the pairwise correspondence across differen-t modalities. However, these approaches are unsupervised so that the class or category information is not exploited and can not significantly boost the performance of learned coupled dictionaries. Zhuang et al. proposed a supervised semi-coupled dictionary learning approach which introduces the category side information into multi-modal dictionary learning via a  X  2 , 1 -norm regularization term.

Our proposed DCDH bears some resemblance to submod-ular dictionary learning (SDL) [7] that takes advantage of the submodularity to learn dictionary efficiently. We extend the idea from uni-modal data into multi-modal data in order to learn discriminative coupled dictionaries .
Cross-media retrieval is a hot research focus in recent years [16, 32, 31]. With the rapid advance of hashing, some cross-media hashing approaches have been proposed [3, 9, 28, 29, 15, 30, 25].

The problem of cross-media hashing was first proposed by Bronstein et al. in CMSSH [3]. Specifically, given two modalities of data sets, CMSSH learns two groups of hash functions to ensure that if two data points (with differen-t modalities) are relevant, their corresponding hash codes are similar and otherwise dissimilar. However, CMSSH on-ly preserves the inter-modality correlation but ignores the intra-modality similarity. Kumar et al. extended Spectral Hashing [23] from the traditional uni-modal setting to the multi-modal scenario and proposed CVH [9]. CVH attempt-s to generate the hash codes by minimizing the distance of hash codes for the similar data and maximizing the distance for the dissimilar data. The inter-view and intra-view sim-ilarities are both preserved in CVH. LCMH [30] adopts a  X  X wo-stage X  strategy to learn the cross-media hash functions: First, the data within each modality are low-rank represent-ed using the anchor graph[11]. Then, hash functions for each modality are learned to project the data from each anchor graph space into a shared Hamming space. MLBE employs a probabilistic generative model to encode the intra-similarity and inter-similarity of data across multiple modalities. Ac-cording to the estimation of maximum a posteriori, the bi-nary latent factors can be obtained and then be taken as the hash codes in MLBE. However, the hash codes generated by MLBE do not require the independency between different hash bits, and may obtain highly redundant hash bits.
Wu et al. introduced dictionary learning into cross-media hashing [25]. By the joint modeling of the intra-modality similarity and inter-modality correlation among multi-modal data with a hypergraph, the coupled dictionaries with a hy-pergraph laplacian regularizer are learned in an iterative manner. The learned dictionary for each modality is then adopted as the hash functions, and the sparse code of each data point over its corresponding dictionary is regarded as the hash code to perform cross-media retrieval.

Inspired by the effectiveness of using the coupled dictio-nary space to represent the data points from different modal-ities, we additionally emphasize discrimination when learn-ing coupled dictionaries in order to make the shared dic-tionary space interpretable. Furthermore, unlike [25] that directly exploits the sparse codes as the hash codes, we fur-ther learn hash functions to map the sparse codes to binary hash codes.
In this section, we introduce the detail of DCDH. Figure 1 illustrates the algorithmic flowchart of our DCDH. For the sake of illustrative simplicity, we assume that only two kinds of data (e.g., images and texts) are available in Figure 1. The proposed DCDH mainly consists of the following two stages: 1. Discriminative coupled dictionary learning : In Figure 2. Unified hash functions learning : Based on the learned The notations used in this paper are listed in Table 1. Symbols Explanation p 1 , ..., p M the dimensionality of each modality
X 1 , ..., X M data set X m = [ x m 1 , x m 2 , ..., x m N D 1 , ..., D M dictionary D m = [ d m 1 , d m 2 , ..., d m
Z 1 , ..., Z M sparse codes Z m  X  R K  X  N o f X m w.r.t. D
To well model the intra-modality similarity and the inter-modality correlation of M data sets, we resort to the unified graph G ( V, E, w ) similar to [19]. The vertex set V denotes the data from all the data sets, and the edge set E models the pairwise intra-modality similarity or the inter-modality correlation between data points. The weight of an edge is measured by some similarity functions which we will discuss in the following.

To model the intra-modality similarity within the same modality, we adopt the local similarity metric with a Gaus-sian kernel. The intra-modality similarity w m i,j of two data points x m i and x m j from modality m is defined as: w where N K ( x ) represents the set of k -nearest neighbours of x and  X  = 1 N P i ,j | x m i  X  x m j | 2 is the expectation over all the pairwise distance in X m .

It takes O ( N 2 p m ) time to compute an intra-modality sim-ilarity matrix. When N is large, we can use some approxi-mated methods such as the anchor graph structure to con-struct this similarity matrix efficiently [11]. In this paper, we simply use the exact k -NN graph in Eq.(1).

To model the inter-modality correlation of the data from two modalities (we name them as modality a and b , a 6 = b and a, b  X  X  1 , 2 , ..., M } ), the similarity function w data x a i and x b j is defined as:
Moreover, to better understand the semantics of data, we additionally exploit the category information. Let C be the category-labels set indicating the category label of each da-ta point (i.e., each vertex in G ), the final category-labeled unified graph is denoted as G ( V, E, w, C ).
Given the category-labeled graph G ( V, E, w, C ), we at-tempt to jointly learn the discriminative coupled dictionar-ies D 1 ,..., D M for the data from each modality. The cou-pling of the dictionaries indicates that these dictionaries have the same number of atoms (i.e, K ) and the dictionary atoms from M modalities have a one-to-one correspondence codes to binary hash codes. (paired dictionary atoms), since the paired dictionary atoms have their different intrinsic power to characterize the multi-modal data. Moreover, the paired dictionary atoms are dis-criminative in terms of semantics (i.e., category ) and is consistent with only one category label. That is to say, the data from different modalities are semantically aligned in a shared coupled dictionary space .

Inspired by the efficiency and effectiveness of submodular dictionary learning approach [7], we formulate our discrimi-native coupled dictionary learning as a graph partition prob-lem on G ( V, E, w, C ). Learning coupled dictionaries with size K is equal to partitioning the category-labeled graph G into K subgraphs which can be further regarded as a prob-lem of selecting a subset A of the edge set E (i.e., A  X  E ) [7, 10]. We can formulate an objective function with respec-t to A and maximize it to obtain the optimal partitions. Our objective function has the property of submodularity and thus can be approximately optimized with an efficient greedy algorithm.

Our objective function consists of three parts which cor-responds to the following requirements: 1) each subgraph should be compact so that the obtained dictionaries have a good representative capability; 2) each subgraph is encour-aged to be discriminative so that the sparse representation of the data over learned dictionary (i.e. using the centroid to represent subgraphs), from the same category to be simi-lar; 3) to avoid the over-fitting on the subgraphs X  size (some subgraphs may be extremely large while the others are so tiny), the size of each subgraph is in balance (nearly equal).
Compact Function: The entropy rate of the random walk over the graph G is exploited to obtain the compact subgraphs. The entropy rate measures the uncertainty of a stochastic process S = { S t | t  X  T } where T is an index set. For a discrete random process, the entropy rate is defined as an asymptotic measure as: H ( S ) = lim t  X  X  X  H ( S t | S t  X  1 which is the conditional entropy of the last random variable given the past. In the case of a stationary 1st-order Markov chain, the entropy rate is: H ( S ) = lim t  X  X  X  H ( S lim t  X  X  X  H ( S 2 | S 1 ) = H ( S 2 | S 1 ).

We define the random walk model on graph G as S = { S t | t  X  T } . The transition probability from the vertex v the vertex v j is defined as p i,j = P r ( S t +1 = v j | S w weights of the vertex v i , and the stationary distribution is defined as: where w all = P | V | i =1 w i is the sum of incident weights of all vertices. The entropy rate of the random walk is defined as:
Leaving in Eq.(3) intact, the set functions for the tran-sition probability P i,j : 2 E  X  R w.r.t. A are defined as:
Consequently, the compact function with respect to A can be defined as the entropy rate of the random walk on G :
Given the entropies of the transition probabilities, maxi-mizing the entropy rate in Eq.(6) encourages the edges with large weights (small distance) to be selected [7]. Hence the compact function H ( A ) can generate compact subgraphs.
Discriminative Function: T o encourage the discrimi-nation of subgraphs which further guarantees the sparse rep-resentation of the data from the same category to be similar, a discriminative function on G is proposed [7].

Let A be the selected edge set, N A be the number of sub-graphs with respect to A , the partition of graph G with selected edge set A is G A = { G 1 , ..., G N A } where each G subgraph. We construct a count matrix N = [ N 1 , ..., N N R signed to each subgraph and c is the number of the categories of the multi-modal data set. Each N i = [ N i 1 , ..., N where N i c is the number of data points from the c -th catego-ry assigned to i -th subgraph. It is worth noting that the size of the count matrix N is dynamic since N A changes when new edges are added to the selected edge set A .

The purity for each subgraph G i is defined as: P ( G i ) = m ax y N i y where y  X  X  1 , 2 , ..., c } is the category label, C P signed to subgraph G i . The overall purity of G A is: where C total = P i C i = | V | is the sum of the count of all subgraphs. The discriminative function is defined as:
D ( A ) measures the discriminative capability of the sub-graphs. Maximizing D ( A ) encourages each subgraph to have a consistent category label, i.e., the data within each sub-graph are expected to have the same category label.
Balancing Function: If we only use the compact and discriminative functions, there may exist some extreme cas-es where the majority of data belong to one subgraph and the other data are sporadically dispersed. This makes the learned dictionary suffer from over-fitting. Therefore, a bal-ancing function is used to regularize the subgraphs of similar sizes.

Denote p A as the distribution of the subgraph member-ship, p A is formulated as:
The balancing function is defined using the entropy max-imum theory:
The aforementioned three functions are proved to be mono-tonically increasing and submodular with respect to A [10][7]. Furthermore, It has been proved that the linear combination of submodular function is still submodular [14]. Therefore, we define an overall function F = H ( A ) +  X  D ( A ) +  X  B ( A ) which is also monotonically increasing and submodular. The optimal solution of F ( A ) is achieved by maximizing the ob-jective function with best A as: where  X  and  X  control the contribution of the three terms. defined parameters. N A  X  K is a constraint on the number of subgraphs which enforces exactly K subgraphs since the objective function is monotonically increasing.

Directly maximizing Eq.(11) is a NP-hard problem. How-ever, since F ( A ) is a submodular function, we can obtain an approximate solution by a simple greedy algorithm, which gives a 1 2 -approximation lower bound on the optimality of the solution [14]. When the optimal K subgraphs are ob-tained, we simply use the center of the data within each subgraph as the corresponding dictionary atom. Since each subgraph consists of the data from M modalities respective-ly, M coupled dictionary atoms are obtained. The coupled dictionaries of the common size K are generated based on all the subgraphs. The overall algorithm of the discriminative coupled dictionary learning is summarized in Algorithm 1.
Note that the weights of the intra-modality edges w m i,j (0 , 1] are not larger than the inter-modality edges w a,b and the two vertices connected by the inter-modality edges are within the same category. Therefore, adding an inter-modality edge satisfies the discriminative function and the compact function at the same time and each inter-modality edge has a high probability to be selected out at the early iterations in the step 4 of Algorithm 1. This observation is valuable which ensures that within each subgraph, there is at least one data point for every modality and the trivial zero-value dictionary atoms is avoided.
 Algorithm 1 D iscriminative Coupled Dictionary Learning Input: d ata sets X 1 ,..., X M ,  X   X  ,  X   X  , K Output: The learned coupled dictionaries D 1 ,..., D M 1: Construct unified graph with labeled multi-modal data 2: Initialization: A  X  X  X  , D 1 , ..., D M  X  X  X  3: while N A &gt; K do 4: e  X  = argmax 5: A  X  A  X  X  e  X  } 6: end while 7: for each subgraph G i in G do 8: for m = 1 to M do 9: V m i = { v j | v j  X  G i and v j from modality m } 1 1: end for 12: end for
A s the coupled dictionary for each modality is learned, the data from each modality can be encoded as sparse codes using its corresponding learned dictionary.

For the data points in data set X m , their K -dimensional sparse codes Z m can be efficiently computed using the dic-tionary D m as follows: The non-negative constraint on Z m is needed for the fol-lowing hash functions learning step. Eq.(12) is a simple non-negative LASSO problem [18], and we use the efficient LARS [4] solver to solve this problem. Moreover, despite of different choices of  X  , the sparsity (maximum number of the zero-elements) of Z can be well controlled by LARS. This is helpful since we expect the sparsity of each sparse code to be equivalent. The sparsity is set to 0.9 (i.e., 90% elements of a sparse code are 0) throughout the paper.

By solving the Eq.(12) for the data set of each modality, the sparse codes Z 1 , ..., Z M are correspondingly generated. Denote Z = [ Z 1 , ..., Z M ]  X  R K  X  MN as the joint sparse codes for all M data sets , we intend to further learn hash functions which linearly projects each sparse code z i  X  Z onto L -dimensional compact binary hash codes ( L &lt; K ).
The commonly used hash function learning strategy is based on graph-laplacian [9, 27], etc. The hash functions are learned by solving an eigenvalue-decomposition problem of a laplacian matrix which takes O ( N 3 ) time. However, it is infeasible to learn hash functions when N is large. There-fore, we adopt the hash function learning strategy based on the sparse characteristic of sparse codes.

Since Z is non-negative, we can use Z (each column has been  X  1 normalized) to construct an approximate adjacency matrix  X  W = Z T  X   X  1 Z  X  R N  X  N where  X  = diag( Z 1 )  X  R
K  X  K [11]. The approximate adjacency matrix  X  W is: 1) nonnegative and sparse; 2) low-rank (the rank is at most K ); 3) double stochastic, i.e., has unit row and column sum. Afterwards, the laplacian matrix is formulated as  X  L = I  X  where I is the identity matrix.

The optimal hash functions can be acquired as the L eigenvectors with smallest eigenvalues of the approximated laplacian matrix  X  L (removing the trivial eigenvector corre-sponds to eigenvalue 0), which is equal to L eigenvectors with largest eigenvalues of  X  W . Due to the low-rank prop-erty of  X  W , a smaller matrix Q =  X   X  1 / 2 ZZ T  X   X  1 / 2 is substituted for eigenvalue-decomposition problem on By solving the eigen-system of Q , L largest eigenvector-eigenvalue pairs { ( v k ,  X  k ) } L k =1 where 1 &gt;  X  are obtained. Denote V = [ v 1 , v 2 , ..., v L ]  X  R K  X  L diag(  X  1 ,  X  2 , ...,  X  L )  X  R L  X  L , the hash functions are defined as follows: where P = projection matrix, z  X  R K is a sparse code and sign( ) is the binary function.

When given a new data point, its hash code can be con-sequently generated using a two-stage mechanism: for ex-ample, given a data point x m from modality m , it is first nonlinearly transformed to a sparse code z m using its cor-responding dictionary D m similar with Eq.(12). After that, with the learned projection matrix P , z m is linearly trans-formed to a L -dimensional compact binary hash code using the learned hash functions in Eq.(13).
It is natural that the representation capability of the dic-tionaries for different modalities varies widely. For exam-ple, the representing capability of the dictionary for a text modality is much stronger than the one for an image modal-ity. This  X  X nbalanced X  representation may lead to an un-satisfying cross-media retrieval performance. Therefore, we incorporate the multi-view representation into our coupled dictionary learning to enhance the representing capability of the relatively  X  X eak X  modalities.

Without loss of generality, assuming we have two modal-ities a and b , for modality a , we have a single-view feature X a ; for modality b , we extract multi-view (e.g., 2 views) unified graph G ( V, E, w, C ) is similar with the aforemen-tioned methods. The size of vertex set does not change since each vertex represents one data point. The edge set E is expanded as some relations between the vertices in V are added with the introduction of multi-view features.
The multi-view discriminative coupled dictionary learning method is similar to the DCDH in Algorithm (1) except for the generation of coupled dictionaries. For the single-view modality a , its corresponding dictionary D a is learned; for the multi-view modality b , two dictionaries D b 1 and D b learned, respectively.

For modality a , we use the dictionary D a to generate s-parse codes Z a for the data set X a by Eq.(12). For modality sparse codes Z b as follows: where Z b is the sparse codes over the multi-view dictionar-R two views. The optimization of Eq.(14) is similar to Eq.(12).
Our DCDH approach consists of an off-line stage to learn the discriminative coupled dictionaries and unified hash func-tions; an on-line stage to encode an out-of-sample data point into a binary hash code. We detail the time complexity for each part respectively.
Discriminative coupled dictionary learning: Leav-ing out the time for constructing the graph G ( V, E, w, C ) which takes O ( P M m =1 N 2 p m ) time, the complexity of dis-criminative coupled dictionary learning can be implemented efficiently. Using a well designed heap structure, the ideal time complexity is O ( | V | log | V | ) (i.e., O ( MN log MN )) [7].
Unified hash functions learning: To learn the u-nified hash functions V , we first learn the sparse codes of Z , ..., Z M using Eq.(12). which can be solved in O ( P M m =1 NKp m ) time using the LARS algorithm [4]. How-ever, the generation of each sparse code is independent which can be solved in O ( p m K ) time, some parallel implementa-tion can be adopted to solve the problem efficiently 1 . After the sparse codes for all training data are obtained, an eigen-system of a small matrix Q  X  R K  X  K is solved in O ( K time to obtain the projection matrix W and corresponding hash functions. Therefore, the overall unified hash functions learning step can be very efficient.
The on-line hash encoding step should be fast enough to support the cross-media retrieval over the large scale data set. The time for encoding a new data point is two-fold: h ttp://spams-devel.gforge.inria.fr/ (a) Off-line training time F igure 2: The time cost of the training and test-ing stages for DCDH, MV-DCDH and other cross-media hashing approaches. The experiments are conducted on Wiki-Potd data set with dictionary size K = 100 and MV-DCDH uses two views of fea-tures.

Sparse Coding: Given a new data point x m from modal-ity m , its sparse code z m is obtained similar as Eq.(12). Therefore, the time complexity is O ( p m K ).

Binary Embedding: The linear transformation from a sparse code z m to a binary hash code is achieved by the hash functions in Eq.(13) which takes O ( KL ) time.

An intuitive comparison of DCDH and other state-of-the-art cross-media hashing methods on off-line training and on-line testing time are demonstrated in Figure 2. We can see that our DCDH and MV-DCDH require the least time in the training stage and is also very efficient in the testing stage. In our experiments, we evaluate the performance of our D-CDH. We first introduce the data set, evaluation criteria and the parameter setting we used in the experiments. Then, we compare our DCDH with other state-of-the-art methods and analyze the results. Finally, we further investigate the learned coupled dictionary space to explain why our DCDH and MV-DCDH achieve the superior performance. We use two real-world data sets  X  X ikipedia-Picture of the Day X (abbreviated as Wiki-Potd) 2 and NUS-WIDE 3 . Both data sets are bi-modal containing images and texts.
The Wiki-Potd data set consists of 2866 Wikipedia doc-uments. Each document contains one text-image pair. All documents are labeled by one of 10 semantic categories. For the image modality, we extract 1000-D Bag of visual words (BoVW) and 512-D GIST descriptors for each image. For the text modality, we calculate the frequency of all words in the data set and select the most representative words to quantize all texts into 5,000-D Bag-of-Words (BoW).
The NUS-WIDE data set contains 269,648 labeled images and is manually annotated with 81 categories. Each image with its annotated tags in NUS-WIDE can be taken as a pair of image-text data. To guarantee that each category has abundant training samples, we select those pairs that belong to one of the 10 largest categories (e.g.,  X  X ky X ,  X  X uildings X ,  X  X erson X ) with each pair exclusively belonging to one of the 10 categories (discrimination on concepts are required when learning coupled dictionaries.). For the image modality, over three types of visual features are extracted for each image h ttp://www.svcl.ucsd.edu/projects/crossmodal/ http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm Table 2: The details of the data sets used in the experiments Validation set size* 2000 /10000 866/866 (i.e., 500-D BoVW, 255-D Color Moments, 128-D Wavelet Texture). For the text modality, the corresponding labels of each image are represented by a 1,000-D BoW.
 The details of the two data sets are shown in Table 2.
To evaluate the performance of the cross-media retrieval results, we adopt the mean average precision (MAP) and mean average top-R precision (MAP@R) defined in [13].
We perform three types of retrieval schemes in the experi-ments : 1) Image-query-Texts: use image queries to retrieve relevant texts. 2) Text-query-Images: use text queries to retrieve relevant images. 3) Image-query-Images: use image queries to retrieve relevant images. For the first two re-trieval schemes, we compare with the state-of-the-art cross-media hashing methods CMSSH [3], CVH [9], MLBE [28], LCMH [30]; for the third retrieval scheme, we additional-ly compare with some uni-modal hashing approaches: SH [23], KLSH [8], AGH [11] and a multi-view hashing approach MFH [17]. The reason why we don X  X  compare DCDH with the approaches in [31] and [25] is that they can not generate compact and binary hash codes, thus their performance can not be fairly evaluated under the same settings.

Our DCDH method and its multi-view enhancement are denoted as DCDH and MV-DCDH, respectively.

For the Image-query-Texts and Text-query-Images retrieval schemes, the performances of CMSSH, CVH, MLBE, L-CMH, DCDH, MV-DCDH are compared. Except for our MV-DCDH which induces multi-view features, the remain-ing methods take the BoVW descriptors for the image modal-ity and BoW for text modality; for MV-DCDH, the multi-view features are utilized for image modality.

For the Image-query-Images retrieval scheme, we compare with all the counterparts aforementioned. It is notable that for MFH, the multi-view features are exploited.
There are four parameters in DCDH: the k -NN of the intra-modality;  X   X  ,  X   X  in Eq.(11) when learning the coupled dictionaries; the size of the coupled dictionaries K .
Following the prior settings in [7, 10],  X   X  and  X   X  are set to 10 and 1 respectively throughout the experiments.

We fix the code length L = 24 and evaluate the aver-age MAP variations (the average MAP scores of Image-query-Texts and Text-query-Images) in terms of K and k -NN on the validation set. The tested combinations are K = The optimal combination on NUS-WIDE is k -NN = 5, K = Table 3: The MAP performance comparison on the N US-WIDE data set with code length L varying from 8 to 40. The items in bold are the two best results, and the results with asterisk are the best. Task Methods Hash code length L = 8 L = 24 L = 40
Image q uery Texts Task Methods Hash code length L = 8 L = 24 L = 40
Text q uery Images Table 4: The MAP performance comparison on the W iki-Potd data set.
 Task Methods Hash code length L = 8 L = 24 L = 40
Image q uery Texts Task Methods Hash code length L = 8 L = 24 L = 40
Text q uery Images 100 and k -NN = 20, K = 300 on Wiki-Potd. These settings are adopted in the following experiments.
We compare our DCDH and its extension MV-DCDH with the three following methods: CMSSH [3], CVH [9] and MLBE [28]. We evaluate the cross-media retrieval perfor-mance with code length varying from 8 to 40 and report results in terms of MAP in Table 3 and 4, respectively. Moreover, we report the Image-query-Images retrieval per-formance in terms of MAP@50 in Figure 3. The reason why we choose MAP@50 rather than MAP for the Image-query-Images task is that the differences of MAP scores, especial-ly for the counterparts, are not statically significant, which makes it difficult to illustrate them explicitly in a line graph.
The MAP scores on Wiki-Potd data set is generally low-er than that on NUS-WIDE even if the Wiki-Potd data set contains rich textual information. This may be explained as that the categories of Wiki-Potd data set (e.g.,  X  X rt X ,  X  X iol-ogy X ) is too general, so the feature vector can not precisely capture its corresponding semantic meaning. F igure 3: The performance comparison of Image-query-Images on two data sets.

It can be noted that DCDH significantly outperforms the counterparts over different code lengths: 2%  X  7% on Wiki-Potd data set and 2%  X  5% on NUS-WIDE, respectively. The improvement is due to the effectiveness of the sparse representation over the discriminative coupled dictionaries. All the counterparts simply project the data from different modalities into a shared Hamming space using the learned hash functions. However, the meaning of the shared Ham-ming space is ambiguous and cannot well clarify the seman-tic information of the data. By contrast, our DCDH exploits the category information when learning the coupled dictio-naries, thus making the coupled dictionary space semantic interpretable. By utilizing the distribution of the sparse codes, the manifold structure of the dictionary space is also preserved in the embedding Hamming space. Therefore, the binary hash codes represent the semantic information of the data and lead to superior cross-media retrieval performance. Moreover, we find that the incorporation of the multi-view features on the image modality, i.e., MV-DCDH, produces a significant improvement over DCDH on both of the Image-query-Texts and the Text-query-Images tasks. This obser-vation is explained as that exploiting multiple features over the  X  X eak X  modality (image modality) gives better under-standing of the semantics of the images. This observation also verifies our hypothesis that a balanced representation is important in cross-media retrieval.

Although our main goal is cross-media retrieval, we can readily use the hash functions we learned to perform uni-modal retrieval. That is to say, all the cross-media hashing approaches can be adapted to uni-modal hashing. We con-duct the task of Image-query-Images and report the perfor-mance of the aforementioned cross-media hashing approach-es. Besides, we add some state-of-the-art uni-modal hashing and multi-view hashing approaches to perform fair experi-mental comparison. The results are shown in Figure 3.
From Figure 3, we find that all the cross-media hashing approaches except CMSSH achieve reasonable performance. This is due to the fact that the learned hash functions for im-age modality can borrow strength from text modality. The observation for the poor performance of CMSSH in this task may be explained as the lack of intra-modality preservation when learning hash functions in CMSSH.

In addition, since MV-DCDH induces multi-features, we also add MFH [17] into comparison which also exploits multi-features when learning hash functions. The results show that MFH outperforms the other uni-modal hashing ap-proaches and most of the cross-media hashing approaches, which demonstrates the effectiveness of multi-views in im-age understanding. Our MV-DCDH slightly outperforms Table 5: Topic words and corresponding category la-b els of some selected dictionary atoms on the Wiki-Potd data set.
 Categories Topic Words
Geography Creek Tree Parks Ridge Forest the MFH and achieves the overall best performance in the I mage-query-Images task.
The results over both the data sets above outline the supe-rior performance of DCDH over the other cross-media hash-ing approaches. This superiority mainly owes to the aptitude of the discriminative capability of the coupled dictionary s-pace in DCDH. To verify our hypothesis, we investigate the coupled dictionary space. We choose the Wiki-Potd data set since it has rich textual information and is convenient for il-lustrations. To show the effect of discriminative capability of the coupled dictionary space, we design a non-discriminative version of DCDH by simply setting  X   X  = 0. The rest settings are same as the ones used aforementioned ( K = 300 and k -NN = 20).

Denote the learned coupled dictionaries for image and text modalities as D x amd D y , respectively. Z x and Z y are the sparse codes of the test data set in two modalities by the corresponding dictionaries. To measure the discrimination of the sparse codes, we define a metric called Discriminative Degree (abbreviated as DD) as follows: where N is the number of the testing samples. P ( z i ) = 1 if at least one of the selected dictionary atoms of the sparse code z i indicates the true category label of the i -th data point and 0 otherwise.

Moreover, DD can also be used to measure the coupling degree of the sparse codes from different modalities. De-note the dot product of Z x and Z y as Z xy and DD ( Z xy reflects the one-to-one correspondence of the paired dictio-nary atoms .

Figure 4 shows the comparison results. The  X  X andom X  method indicates that the dictionaries are randomly gen-erated; The  X  X on-discriminative X  method is the aforemen-tioned DCDH with  X   X  = 0. The  X  X iscriminative X  and  X  X V-Discriminative X  methods correspond to our DCDH and MV-DCDH. From the results, we get four observations: 1) the sparse codes of the text modality is more semantically dis-criminative than the ones from the image modality (even when we do not impose the discriminative constraints); 2) The introduction of side information improves the discrimi-native capability especially for the image modality (without the category side information, the DD score of the image modality is almost equal to the random method); 3) The Figure 4: The discriminative capability comparison c oupling degree is significantly improved when imposing the discriminative constraint. This is mainly due to the better representation of the image modality; 4) Exploiting multi-view features further improves the performance on both the understanding of the image modality and the coupling de-gree.

To give an intuitive illustration of the learned dictionary, we give an insight into the textual dictionary D y (the image dictionary D x is learned from the BoVW, which is difficult to illustrate). Since each dictionary atom d y k is obtained by clustering a portion of BoW features, the values of the ele-ments in d y k measure the occurrence frequency of the words. Naturally, we can use the elements with largest values in d k to represent the topic words of this dictionary atom d Moreover, each d y k is learned with a discriminative constrain-t and has a dominated category label, we demonstrate the relation between the category label and topic words of the dictionary atoms in Table 5 (topic words correspond to the 5 largest elements of the selected dictionary atoms). From the results, we can find that the topic words for each dictionary atom indeed reflect some certain semantic information and are consistent with its belonging category. Besides, two dic-tionary atoms belong to the same category have an explicit disparity in semantics. e.g., the two atoms from the cat-egory  X  X iology X  describe different topics of  X  X iology X : the first topic is about dinosaurs and the second one is about whale killing. This observation can be explained as the col-laborative effect of the compact function and discriminative function. The discriminative function encourages the topics to be classified into the correct categories and the compact function encourages each topic to reflect an individual aspect of its corresponding category.
In this paper, we propose a discriminative coupled dic-tionary hashing (DCDH) approach for fast cross-media re-trieval. Our DCDH is two-stage in that we first learn cou-pled dictionary for each modality discriminatively with the side information of category labels, so that the data from different modalities are represented as the sparse codes in a shared semantically discriminative dictionary space. After-wards, the sparse codes are mapped to binary hash codes by the learned unified hash functions to support fast cross-media retrieval. Extensive experiments on two real-world data sets demonstrate the superior performance of DCDH over the existing state-of-the-art hashing approaches.
Moreover, we conjecture that a balanced cross-media rep-resentation benefits the cross-media retrieval performance. Therefore, we extend DCDH to MV-DCDH which intro-d uces multi-view features on the relatively  X  X eak X  modal-ities (i.e., the image modality in our experiments) to obtain a balanced representation. The experimental results verify the effectiveness of MV-DCDH. This work is supported in part by National Basic Research Program of China (2012CB316400), NSFC (No.61128007), 863 program (2012AA012505), the Fundamental Research Funds for the Central Universities and Chinese Knowledge Center of Engineering Science and Technology (CKCEST) and Program for New Century Excellent Talents in Univer-sity. Dr.Qi Tian is also supported by ARO grant W911NF-12-1-0057, Faculty Research Award by NEC Laboratories of America, and 2012 UTSA START-R Research Award re-spectively. Dr. Jiebo is also supported by Google Faculty Research Awards. [1] M. Aharon, M. Elad, and A. Bruckstein. K-svd: An [2] A. Andoni and P. Indyk. Near-optimal hashing [3] M. Bronstein, A. Bronstein, F. Michel, and [4] B. Efron, T. Hastie, I. Johnstone, and R. Tibshirani. [5] Y. Gong and S. Lazebnik. Iterative quantization: A [6] K. Jia, X. Tang, and X. Wang. Image transformation [7] Z. Jiang, G. Zhang, and L. S. Davis. Submodular [8] B. Kulis and K. Grauman. Kernelized [9] S. Kumar and R. Udupa. Learning hash functions for [10] M.-Y. Liu, O. Tuzel, S. Ramalingam, and [11] W. Liu, J. Wang, S. Kumar, and S. Chang. Hashing [12] Y. Liu, F. Wu, Y. Yi, Y. Zhuang, and A. Hauptman. [13] X. Lu, F. Wu, S. Tang, Z. Zhang, X. He, and [14] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [15] M. Ou, P. Cui, F. Wang, J. Wang, W. Zhu, and [16] N. Rasiwasia, J. Costa Pereira, E. Coviello, G. Doyle, [17] J. Song, Y. Yang, Z. Huang, H. Shen, and R. Hong. [18] R. Tibshirani. Regression shrinkage and selection via [19] C. Wang and S. Mahadevan. A general framework for [20] J. Wang, S. Kumar, and S. Chang. Semi-supervised [21] Q. Wang, D. Zhang, and L. Si. Semantic hashing using [22] S. Wang, L. Zhang, Y. Liang, and Q. Pan.
 [23] Y. Weiss, A. Torralba, and R. Fergus. Spectral [24] J. Wright, A. Yang, A. Ganesh, S. Sastry, and Y. Ma. [25] F. Wu, Z. Yu, Y. Yang, S. Tang, Y. Zhang, and [26] D. Zhang, F. Wang, and L. Si. Composite hashing [27] D. Zhang, J. Wang, D. Cai, and J. Lu. Self-taught [28] Y. Zhen and D. Yeung. A probabilistic model for [29] Y. Zhen and D.-Y. Yeung. Co-regularized hashing for [30] X. Zhu, Z. Huang, H. T. Shen, and X. Zhao. Linear [31] Y. Zhuang, Y. Wang, F. Wu, Y. Zhang, and W. Lu. [32] Y. Zhuang, Y. Yang, and F. Wu. Mining semantic
