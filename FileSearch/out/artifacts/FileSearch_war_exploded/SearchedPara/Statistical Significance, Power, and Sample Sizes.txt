 We conducted a systematic review of 840 SIGIR full papers and 215 TOIS papers published between 2006 and 2015. The origi-nal objective of the study was to identify IR effectiveness experi-ments that are seriously underpowered (i.e., the sample size is far too small so that the probability of missing a real difference is ex-tremely high) or overpowered (i.e., the sample size is so large that a difference will be considered statistically significant even if the actual effect size is extremely small). However, it quickly became clear to us that many IR effectiveness papers either lack signifi-cance testing or fail to report p -values and/or test statistics, which prevents us from conducting power analysis. Hence we first report on how IR researchers (fail to) report on significance test results, what types of tests they use, and how the reporting practices may have changed over the last decade. From those papers that reported enough information for us to conduct power analysis, we identify extremely overpowered and underpowered experiments, as well as appropriate sample sizes for future experiments. The raw results of our systematic survey of 1,055 papers and our R scripts for power analysis are available online. Our hope is that this study will help improve the reporting practices and experimental designs of future IR effectiveness studies.
 effect sizes; evaluation; power analysis; sample sizes; statistical power; statistical significance; systematic review
In experiments for retrieval effectiveness evaluation, the de facto standard is the practice of comparing the mean evaluation mea-sure scores across topics by means of statistical significance tests such as the (paired and unpaired) t -test and Analysis of Variance (ANOVA). While alternative approaches to these standard tests ex-ist (e.g., Carterette X  X  Baysian inference [3], Robertson and Kanoula X  X  view of documents as the source of variance [12], and Killeen X  X  probability of replication for experiments in psychology [10]), so far they have not yet enjoyed the same popularity as the classical significance tests in the IR community.

In standard signifiance testing, the test statistic (e.g., the tic for the t -test and the F statistic for ANOVA) is a function of the effect size (i.e., the actual magnitude of the difference between systems) and the sample size (i.e., the number of topics, users etc.), even though what we are usually interested in is the effect size: is the difference  X  X ubstantial X ? However, due to the above rela-tionship, increasing the sample size increases the value of the test statistic, which in turn decreases the p -value (i.e., the probability of observing the obtained result or something more extreme under the null hypothesis). Since we conclude a result to be statistically significant if, for a predetermined significance level  X  , we observe that p -value  X   X  holds, we can make anything statistically signif-icant by using a large enough sample. Conversely, if the sample size is too small, we might be missing differences that we really should be detecting. While  X  is the predetermined probability of Type I error (detecting a nonexistent difference),  X  is the probabil-ity of Type II error (missing a real difference), and the ability to detect a real diffrence, given by (1  X   X  ) ,isthe statistical power . The combination (  X ,  X  )=(0 . 05 , 0 . 20) , known as Cohen X  X  five-eighty convention (where  X  X ighty X  refers to 80% power), is often used as a standard setting for determining the sample size for an experiment [5, 15].

Recent statistically motivated studies have suggested that topic sets used in IR test collections should be substantially larger than they currently are in order to meet a clear set of statistical require-ments. For example, Sakai X  X  topic set size design results show that if researchers use the paired t -test for ad hoc IR and want to en-sure Cohen X  X  five-eighty convention for any system difference of 0.05 (or higher) in mean average precision, about 300 topics are required [15] 1 . Urbano, Marrero and Mart X n conclude that  X  in most cases a couple hundred  X  topics are required for stable system rank-ings from their study based on the Generalisability Theory [20]. While these studies do not imply that all system comparison exper-iments based on 50 topics are invalid, they do suggest that some of the experiments may be underpowered : we may be missing a lot of real differences due to small sample sizes. On the other hand, the advent of web search engines brought with it the practice of using thousands or even millions of queries from their query log data for averaging; we suspected that some of such studies may be overpowered : even if the effect size is very small, statistically significant differences can be obtained due to large sample sizes. The original objective of this study was to identify IR effectiveness experiments that are extremely underpowered or overpowered.
We conducted a systematic review of 840 ACM SIGIR full pa-pers and 215 ACM TOIS papers published between 2006 and 2015. SIGIR is considered by many to be the premier conference in IR,
Over 700 topics are required if unstable measures such as ex-pected reciprocal rank (ERR) is used [15]. while TOIS enjoys a similar status in the journal domain; this jour-nal explicitly instructs authors as follows:  X  When reporting statis-tics, the name of the statistic, the degrees of freedom, the value obtained, and the p -value should be reported, e.g., F (3 , 65) = 4 . 83 ,p &lt; 0 . 01  X  2 . However, it quickly became clear to us that many IR effectiveness papers published in SIGIR and TOIS either lack significance testing or fail to report p -values and/or test statis-tics, which prevents us from conducting power analysis. Hence we first report on how IR researchers (fail to) report on significance test results, what types of tests they use, and how the reporting practices may have changed over the last decade. From those papers that reported enough information for us to conduct power analysis, we identify extremely overpowered and underpowered experiments, as well as appropriate sample sizes for future experiments. The raw results of our systematic survey of 1,055 papers are available on-line. We hope that this study will help improve the reporting prac-tices and experimental designs of future IR effectiveness studies.
Two papers directly inspired us to conduct the present systematic review. The first is the study by Armstrong et al. [1]: they examined SIGIR papers in the period 1998-2008 and CIKM papers in the pe-riod 2004-2008 to investigate whether IR systems have improved over the years. A total of 106 papers, which contained IR effec-tiveness results using TREC test collections, were analysed in their survey. One of their main findings was that researchers often claim statistically significant results but use low baselines for compari-son. The second is the study by Kelly and Sugimoto [9]: they anal-ysed 127 journal and conference papers selected from 2,791 papers in the period 1967-2006 and investigated the evaluation practices in interactive information retrieval (IIR), including experimental de-signs, corpora and measures used. In the present study, we tried to follow Kelly and Sugimoto X  X  description of systematric reviews to the best of our ability:  X  Researchers articulate a plan for gather-ing and analyzing studies and attempt to be exhaustive with their coverage of the literature. Researchers also take a neutral position during analysis and attempt to create generalizations from findings. Systematic reviews adhere to strict scientific guidelines to minimize potential selection and interpretation biases to ensure replicability (and hence reliability).  X  Like Armstrong et al. [1], we are primarily interested in system effectiveness studies that typically rely on aver-aging across a set of topics. However, as we shall see, some of the findings from the present study partially overlap with one of Kelly and Sugimoto X  X  main finding from their systematic review of IIR studies:  X  Because of the basic goals and design of these studies, the majority of researchers used either ANOVA or t -tests to ana-lyze the results. [...] In some cases, the type of test conducted was not reported although statistically significant results were claimed and/or p -values were presented.  X 
Sanderson and Zobel [16] report on a small-scale survey which is also relevant to our present study: they examined 26 system ef-fectiveness papers from SIGIR 2003 and 2004 to see which sig-nificance tests and evaluation measures were used. Their findings, which are also in line with ours, are perhaps also worth quoting:  X  We found that si gnificance was not explicitly reported in 14 of the papers. In two it was implied such tests had been tried, but outcomes were not given. In three or four of these papers, the im-provements were large and arguably a significance test was unnec-essary. However, in at least six papers (23% of the sample) the reported improvements were small, sometimes no more than a few percent in relative MAP.  X ;  X  Among the 12 papers with significance https://tois.acm.org/aut hors.cfm visited April 12, 2016. Table 1: Statistics of the ACM papers examined in this study. tests, one used both ANOVA and the t -test, five each used either the t -test or Wilcoxon X  X  test, and in one, the test was not identified.  X  In the present study, we manually examined 1,055 SIGIR and TOIS papers, and found that at least 862 of them appear to deserve statistical significance testing for performance differences; we then analysed these 862 papers further. This number is substantially higher compared to the above studies, although the numbers are not directly comparable due to differences in objectives and methods of analysis. More importantly, unlike prior art, we conducted power analysis for 133 papers that reported the p -value and/or the test statistic, and computed the achieved power as well as appropriate sample sizes for future experiments. The primary purpose of this systematic survey is to examine how IR researchers, especially those working on improving IR effective-ness , use statistical significance tests, and to conduct power analy-sis wherever possible. We are interested, for example, in whether the topic set sizes are appropriate in test collection-based studies, and whether the number of participants or observations is appropri-ate in user-based studies. Table 1 shows the number of papers per year that we examined 3 . First, we created two lists of DOIs, one for SIGIR and the other for TOIS, on separate sheets in an Excel file, and we created columns in the Excel sheets according to a strict coding scheme as described below. Then, the author of this paper manually examined each paper at least twice. The coding started in February 2015, and was completed in October 2015. The pdf file of each paper was downloaded from the ACM digital library and was viewed on computer screen; for some papers that required careful interpretations of significance test results, hard copies were also used 4 . Each paper was coded in an Excel file as follows.

Step 1 Does the paper contain a table or figure of mean effective-
Step 2 Does the paper conduct a significance test? Assign exactly TOIS published Volume 34 Issue 1 (6 papers) in October 2015. These papers are outside the scope of this study.
The pdf files of three SIGIR papers were not searchable by Ctrl-F; for these cases also, hard copies were used.
Table 3: Subcategories for papers that compare two systems
Step 3 If categorised as (I) in Step 2, check if the paper claims
Step 4 If categorised as (A)-(G) in Step 2, record whether the
Step 5 If the test statistic for categories (A)-(E) is obtained in Step 4
Step 6 In addition, copy and paste sentences from the paper that
In Step 2, if the paper mentioned a t -test but did not indicate two sample sizes, we assumed that the test is paired (Category (B)) as this is the typical setting in test collection-based studies; if the paper did not indicate whether the test is two-sided or one-sided, we interpreted it as the more conservative two-sided test when con-ducting power analysis. Category (G) was chosen if the paper men-tioned a significance level (  X  ), a p -value, or just statistical signifi-cance, but did not specify the significance test used; Category (H) was chosen if the paper did not mention any significance testing but used CIs, boxplots, (often unexplained) error bars, or reported stan-dard errors/deviations. Category (I) was chosen if the paper did not mention significance testing at all, even if it claimed  X  X ignificant X  improvements (without using the word  X  X tatistical X ).

In Step 4, we considered the p -value as  X  X eported in the paper X  if the exact p -value was reported. We did not regard  X  p&lt; 0 . 1 ,p&lt; 0 . 05 ,p &lt; 0 . 01  X  as exact p -values; rather, we regarded them as the predetermined significance level  X  , which may or may not be substantially larger than the actual p -values. On the other hand, we count  X  p&lt; 0 . 001  X  and smaller values as  X  X eported in the pa-per X  as such values should be accurate enough for power analysis. Whenever the test statistic t was not reported but the p -value and the sample sizes were, we deduced the test statistic as: where t inv (  X  ; P ) is the critical t value for  X  degrees of freedom and probability P , obtained with Microsoft Excel as T . INV . 2T ( P, X  ) for two-sided tests, or T . INV (1  X  P, X  ) for one-sided tests paired t -tests with sample size n ,welet  X  = n  X  1 ; for unpaired t -tests with sample sizes n 1 and n 2 ,welet  X  = n 1 + n 2 Similarly, for ANOVA results where p -values were reported but the F values were not, we computed them as follows whereover possible: where F inv (  X  1 , X  2 ; P ) is the critical F value for (  X  of freedom and probability P ,or F . INV . RT ( P, X  1 , X  2 crosoft Excel. The degrees of fr eedom are computed in accordance with ANOVA.

After coding each paper as described above, we focussed on pa-pers labelled  X (F) Other tests X  and assigned a subcategory to each of them, for the purpose of examining the popularity of different significance tests for comparing two systems. The subcategories are shown in Table 3; (A)-(F5) are used in the analysis in Sec-tion 4.3. Category (F6) includes papers that used nonparametric tests for more than two systems.

Admittedly, even though our coding scheme is systematic, it is impossible to completely rule out the possibility that the present author has misinterpreted some of the papers. To accommodate correction, we have created a tw itter account solely for this pur-pose 6 : if an assessment of a paper needs to be corrected, the author of that paper can point this out by using either TOIS or SIGIR followed by the last seven digits of the paper X  X  DOI as the hash-tag (e.g., #SIGIR1148261 ). Recall that our intepretation of each paper is available as a raw Excel file and therefore that it is easy to correct it. However, it is highly unlikely that these minor  X  X ug reports X  will affect the main conclusions of the present paper.
We conducted a power analysis for every paper in Categories (A)-(E) where the test statistic and actual sample sizes were ob-tained, based on modified versions of t -test and ANOVA power analysis tools provided by Toyoda [19]. The original R scripts of Toyoda, which simply rely on R X  X  library pwr 7 , are available at the publisher X  X  website 8 ; our own versions, which differ from Toy-oda X  X  only in input and output specifications 9 , are available from our website 10 , along with our raw systematic review results. We provide a function for each significant test type, for computing the
For one SIGIR 2014 paper, the t statistic was computed by divid-ing the mean difference by the reported standard error of the mean. This paper is discussed in Section 4.6 (Table 7 Entry #12). http://twitter.com/IRsysrev https://cran.r-project.org/web/packages/pwr/pwr.pdf http://www.tokyo-tosho.co.jp/download/DL02065.zip
The present author is solely responsible for the modifications and any errors introduced thereby. http://www.f.waseda.jp/tetsuya/data.html effect size, achieved power and recommended future sample sizes. In what follows, we adhere to Toyoda X  X  notations when we refer to effect sizes (  X  es for t -tests;  X  f and  X  f 2 for ANOVA).
Our R function for unpaired t -tests is called future.sample.unpairedt , whose arguments are the t statis-tic ( t ), two sample sizes ( n 1 ,n 2 ), whether the test is two-sided or one-sided (default: two-sided),  X  , and power ( 1  X   X  ) (default: Co-hen X  X  five-eighty convention). The script computes first the sam-ple effect size  X  es = | t | ( n 1 + n 2 ) / ( n 1 n 2 ) (which expresses the between-system difference in standard deviation units) and then the achieved power and the recommended sample size per group n future experiments, by calling the function power.t.test (if n n
Our R function for paired t -tests is called future.sample.pairedt , whose arguments are the t statistic ( t ),thesamplesize( n ), whether the test is two-sided or one-sided (default: two-sided),  X  , and power ( 1  X   X  ) (default: Cohen X  X  five-eighty convention). The script computes first the sample effect size  X  es = | t | / size n , by calling power.t.test .
Our R function for one-way ANOVA tests is called future.sample.1wayanova , whose arguments are the F statis-tic ( F ), number of groups being compared ( m ), number of obser-vations per group ( n ),  X  , and power ( 1  X   X  ) (default: Cohen X  X  five-eighty convention). The script computes first the sample effect size  X  f =  X  A F/ X  E (where  X  A = m  X  1 , X  E = m ( n  X  1) and then the achieved power and the future sample size per group n , by calling pwr.anova.test . Here,  X  f is an estimate of how large the between-group population standard deviation is compared to the within-group population standard deviation.
Our R function for two-way ANOVA (without replication) tests is called future.sample.2waynorep , whose arguments are thesameas future.sample.1wayanova . The script first com-putes the sample effect size  X  f 2 =  X  A F/ X  E (where  X  A 1 , X  E =( m  X  1)( n  X  1) ) and then the achieved power and the future sample size per group n , by calling pwr.f2.test .Note that this tool outputs  X  f 2 rather than  X  f as the effect size, simply be-cause that is what pwr.f2.test requires as an argument 11 equivalent way to express  X  f 2 would be  X  f 2 =  X  2 p / (1  X   X  2 expresses how much of the total variance (after removing other effects) is due to factor A ; it can be computed as  X  2 p Also,  X  p is known as the partial correlation ratio .
Our R function for two-way ANOVA tests is called future.sample.2wayanova , whose arguments are the F statis-tics ( F A ,F B ,F AB ), number of groups ( m ), number of cells per group ( n ), total number of observations ( N ),  X  , and power ( (default: Cohen X  X  five-eighty convention). For example, A, B, AB could represent the system and topic effects and the interaction be-tween them, respectively. Let the degrees of freedom for A, B, AB and the residual E be  X  A = m  X  1 , X  B = n  X  1 , X  AB =( m  X  http://127.0.0.1:25552/library/p wr/html/pwr.f2.test.html Figure 1: Number of papers concerning results reported in Sec-tions 4.1, 4.2, 4.3, and 4.6 (SIGIR+TOIS). 1)( n  X  1) , X  E = N  X  mn . Then the sample effect size for given by  X  f A =  X   X  2 pA / (1  X   X   X  2 pA ) ,where  X   X  2 pA and  X  f AB are computed similarly. The achieved power and the to-tal sample size for future experiments N are computed by calling pwr.anova.test .
In Step 1 of the coding scheme described in Section 3.1, a rep-resentative table was selected for 700 out of the 840 SIGIR papers (83%), and for 162 papers out of the 215 TOIS papers (75%). Thus, our view is that at least 862 papers out of the 1,055 papers that we examined may deserve statistical significance testing for comparing mean effectiveness scores and the like. Hereafter, we focus our at-tention to these 862 papers. Figure 1 provides an overview of some of the results reported in this section, in terms of paper counts. Figure 2 shows the distributions of the aforementioned 700 SI-GIR and 162 TOIS papers over the seven categories shown in Ta-ble 2. Paper counts are shown in addition to percentages. It can be observed that the distributions for SIGIR and TOIS are similar: 35-37% of the papers use the paired t -test (Category (B)); about 28-30% do not report significance test results even though these pa-pers have a representative tab le (Category (I)); and about 18-24% use tests other than the t -test or ANOVA (Category (F)) 12 examine Catgories (I) and (F) more closely in Sections 4.2 and 4.3, respectively.

Figure 3 shows the distribution of papers over the categories and across the timeline; the top graph shows the results for SIGIR; the bottom one shows the results obtained by summing the SIGIR and TOIS statistics. The number of TOIS papers alone per year is con-sidered too small for our analysis and are not shown here. For each year, the number of papers in each category is divided by the total number of papers with a representative table for that year; the latter is shown below the x axis for each year. It appears that the the use of the paired t -test is now more common than a decade ago, and that we are also seeing fewer papers without statistical significance tests. In Section 4.4, we shall compare the results for 2006 and those for 2015 from this viewpoint.
Recall, however, that we had to select exactly one significance test type per paper even if both t -tests and ANOVA were used in the same paper. Figure 2: Distributions of papers with a representative table over categories.
 Table 4: Category (I) papers (no significance tests): Column (a) shows the number of papers that claim  X  X ignificant X  improve-ments and the like; Column (b) shows the number of papers that make no such claims. For the SIGIR and SIGIR+TOIS columns, the percentages are also shown, where the denomina-tors are the paper counts shown in Figure 3.

Why are there so many papers without significance tests despite explicit instructions such as the ones from TOIS (Section 1)? Re-call that all of these Category (I) papers have a representative table that appears to deserve significance testing. Possible good reasons include: (i) the authors consider statistical significance testing to be of limited or no value (e.g. [7, 8]); (ii) the authors judge that significance testing is unnecessary in their particular situations be-cause either the sample size and/or the effect size is very large, or because the difference is not the central point the study. For ex-ample, one SIGIR 2012 paper (DOI: 10.1145/2348283.2348301) states:  X  Since the present study does not aim to prove one retrieval method better than another, we report the findings without tests on significance of statistical differences.  X  [2]. In Category (I), how-ever, we did find some papers that use expressions such as  X  X ignif-icant improvement X  and  X  X ignificantly outperform X  in the context Figure 3: Distributions of papers with a representative table over categories across 10 years. of discussing effectiveness even though statistical significance tests are never mentioned. Some of these claims are even made in the paper abstracts and conclusions. We argue that such practices are quite misleading and that the use of ambiguous expressions such as those mentioned above should be avoided. Table 4 breaks down the Category (I) papers from each year into papers that claim  X  X ig-nificant X  improvements and the like, and those that do not. It is worrying that 167 papers (19%) out of 862 papers with a represen-tative table say  X  X ignificant X  without conducting significance tests, even if in some cases it may be clear from the context that the word is being used in the non-statistical sense.
As was mentioned in Section 3.1, we used the subcategories shown in Table 3 to examine which significance tests are popu-lar for comparing two systems. Historically, IR researchers in the 20th century were relatively reluctant to use parametric tests [14], but nowadays the robustness of the t -test (which is parametric) is recognised and the test is widely used. Savoy [17] and Sakai [13] advocated the use of the bootstrap test in 1997 and 2006, respec-tively, while Smucker, Allan and Carterette [18] advocated the ran-domisation test in 2007. Are these computer-based, distribution-free tests used more often now? Figure 4 breaks down 365 SIGIR papers and 99 TOIS papers that used a significance test for compar-ing two systems by test type. Again, the overall pictures are very similar for these two venues: 61-66% of these papers use the paired t -test; 20-23% use the Wilcoxon signed rank test 13 ;4-5%usethe randomisation test; 3-4% use the sign test; only 1% use the boot-strap test. Recall, however, that we picked one primary test from each paper even if it utilised multiple test types. Figure 5 shows how the popularity of these tests have changed over the last decade; note that the bottom graph aggregates the statistics from SIGIR and TOIS as before. It appears that the paired t -test is now more popu- X  X ilcoxon test X  was always interpreted as the Wilcoxon signed rank test, not as the Wilcoxon rank sum test. Figure 4: Distributions of papers over significance test types for comparing two systems. lar than it was 10 years ago, and that the Wilcoxon signed rank test is less so. In Section 4.4, we shall compare the results for 2006 and 2015 from this viewpoint.

We came across a few papers that explicitly discussed why a particular test type was chosen, although we did not conduct an ex-haustive search for such comments. In a TOIS paper from 2012 (10.1145/2382438.2382445), the authors cite Sanderson and Zo-bel X  X  SIGIR 2005 paper [16] and state:  X  We report statistical sig-nificance test results using the nondirectional paired t-test at a con-fidence level of 0.01, since this test has been shown to be more reli-able than the Wilcoxon and signed tests  X  [6]; A SIGIR 2008 paper (10.1145/1390334.1390407) provides a similar argument. In a SI-GIR 2015 paper (10.1145/2766462.2767700), the authors cite the aforementioned CIKM 2007 paper by Smucker et al. [18] as well as Sanderson and Zobel [16], and choose the randomisation test. Figures 3 and 5 suggest the following four clear trends: (1) In Figure 3, the proportion of Category (I) (no significance tests) pa-pers is decreasing; (2) Similarly, the proportion of Category (B) (paired t -test) papers is increasing; (3) In Figure 5, the proportion of Category (F1) (Wilcoxon signed-rank test) papers is decreasing; (4) Similarly, the proportion of Category (B) (paired t -test) papers is increasing. Recall that Figure 3 considers all papers with a rep-resentative table, while Figure 5 only considers papers that used a significance test for comparing two systems. Ideally, these obser-vations deserve a time series analysis, where an observation from a given year is modelled as a function of an observation from previ-ous years along with some noise. However, this would not be very useful with our data with only ten data points. An obvious alterna-tive is to regard the observation from each year as an independent sample to conduct standard significance testing, but in practice the independence assumption is highly unlikely to hold: for example, if many SIGIR paper authors use the t -test in 2014, those who sub-Figure 5: Distributions of papers over significance test types for comparing two systems across 10 years. mit a paper to SIGIR 2015 are also likely to do so, perhaps after reading some of the SIGIR 2014 papers. Due to the above consid-erations, we focus our attention on the comparison between 2006 and 2015, i.e., the leftmost and rightmost results in Figures 3 and 5, and conduct significance testing for two samples, each from a dif-ferent population, under an independence assumption. Below, we describe our (two-sided) significance testing procedure for compar-ing two proportions (one from 2006 and the other from 2015) [11].
From the first population, we draw a sample of size n 1 ,andfind testing); the observed proportion is x 1 /n 1 . The observed propor-tion for the second population is defined similarly as x 2 P 2 = P .Let which are the estimates of the true proportions P 1 ,P 2 ,P tinuity corrections, respectively. Under H 0 , it is known that the distribution of  X  P  X  1  X   X  P  X  2 can be approximated by N (0 ,P (1  X  P )( 1 n the following test statistic: The point estimate for the true difference between the two propor-tions is given by  X  P  X  1  X   X  P  X  2 , whereas the margin of error ( computing the 95% CI is given by: where z  X / 2 is the critical z value for probability  X / 2=0 . 025 (since we want 95% confidence).

Our four null hypotheses, which correspond to the aforemen-tioned trends (1)-(4), state that t he population proportion computed for 2015 is equal to that for 2006. We denote them as H 1 Table 5 summarises the results of th e significance tests; Part (a) Table 5: Statistical significance test results: comparing the pro-portions from 2006 and those from 2015. uses data from the SIGIR papers only, while Part (b) uses both SI-GIR and TOIS papers. It can be observed that all the p -values are below  X  =0 . 05 , and that all of the above null hypotheses are re-jected. More importantly, the 95% CIs for H 1 0 and H 3 0 zero, while those for H 2 0 and H 4 0 are below zero. Hence, we can conclude, for both Parts (a) and (b), as follows: 1. The proportion of Category (I) (no significance tests) papers 2. The proportion of Category (B) (paired t -test) papers among 3. The proportion of Category (F1) (Wilcoxon signed-rank test) 4. The proportion of Category (B) (paired t -test) papers among
Next, we focused on papers (with a representative table) that did conduct significant tests, regardless of the test type. For 453 SI-GIR papers and 112 TOIS papers in Categories (A)-(G), Step 4 described in Section 3.1 further categorised them into four classes: (i) both exact p -values and test statistics are reported; (ii) only exact p -values are reported (from which test statistics may be deduced if the sample sizes are known); (iii) only test statistics are reported; and (iv) neither is reported. Note that Class (iv) includes papers that specify a significance level  X  but does not report the exact p -values. Researchers (who accept and rely on classical signifi-cance tests) should report p -values and effect sizes. Saying  X  X ig-nificant at  X  =0 . 05  X  leads to dichotomous thinking ( X  X ignificant or not? X ) [5], and is not very informative for the reasons discussed in Section 1. Figure 6 visualises the proportion of Classes (i)-(iii) against Class (iv) papers for each year. It also contains a table of the actual number of papers. It can be observed that over one half of the papers with significance tests report neither p -values nor test statistics, and that the situation does not seem to be improving. Figure 6: Proportion of papers (with a representative table) that report either p -values or test statistics (or both). Table 6: Number of papers (with a representative table) with (a) an overpowered experiment (power  X  0 . 99 ); (b) an under-powered experiment (power  X  0 . 50 ); (c)  X  X bout right X  experi-ment ( 0 . 50 &lt; power &lt; 0 . 99 ).

Among the 200  X  X ood practice X  papers indicated in Figure 6, we were able to conduct a power analysis for 99 SIGIR papers (75 with t -tests and 24 with ANOVA tests) and 34 TOIS papers (31 with tests and 3 with ANOVA test); the other 67 papers used other signif-icance test types. Table 6 shows the paper counts for overpowered, underpowered, and  X  X bout right X  pap ers for Categories (A)-(E). It can be observed that as many as 38 (29%) out of the 133 papers that went through power analysis are overpowered: the achieved power is 99% or higher for these papers. Whereas, only 9 (7%) papers were found to be underpowered: the achieved power is 50% or lower.

Hereafter, we focus on the power analysis results for SIGIR pa-pers. Table 6  X  X IGIR (a) X  column shows that we found a total of 19  X  X verpowered X  and 5  X  X nderpowered X  papers with t -tests (Cat-egories (A) and (B)), as well as 10  X  X verpowered X  and 2  X  X nder-powered X  papers with ANOVA tests (Categories (C)-(E)). Tables 7 and 8 provide complete power analysis results for these papers. Re-call that n and n denote the actual and future sample sizes per group , while N and N denote the actual and future total sam-ple sizes (Section 3.2). To quantify the gap between actual and future sample sizes, let us define sample size ratio as the actual sample size divided by the future sample size. Figure 7 plots sam-ple size ratios against the achieved power; the baloons in the figure indicate which paper in Table 7 or 8 each dot corresponds to. For example, the top right balloon in Figure 7(a) indicates the third en-try in Table 7, for which the effect size is  X  es =1 . 864 effect [4]) and the achieved power is 100%; although the actual sample size was n = 192 , in fact n =5 is sufficient. The left-most balloon in Figure 7(a) indicates the 24th entry in Table 7, for which  X  es =0 . 180 (a  X  X mall X  effect [4]) and the achieved power is only 15.2%; although the actual sample size was n =28 , in fact n = 244 is needed to ensure 80% power. Similar relationships can be observed between Figure 7(b) and Table 8. power, then by effect size. achieved power.
There are a few extremely large sample sizes in the  X  X verpow-ered X  section of Table 7. The 12th entry in Table 7 (SIGIR 2014, 10.1145/2600428.2609617), a paper on personalisation from a search engine company, reported the difference in mean average preci-sion (MAP) together with the standard error of the mean (SEM), which enabled us to compute the t -statistic by simply dividing the former with the latter. We chose the result with the largest dif-ference ( MAP =0 . 0224 , SEM =0 . 00140 ) from one of their tables, which resulted in t =16 . 0 . The actual sample size (the number of impressions for averaging) was n = 5352460 ,buta recommended future sample size is n = 164107 . This situation is also visualised in Figure 7(a) (third balloon from the top). Note that the effect size is extremely small:  X  es =0 . 007 , although even such a small effect may possibly translate to a subtantial increase in profit for search engine businesses. The authors of the 17th en-try in Table 7 (SIGIR 2015, 10.1145/2766462.2767712) are from academia, but they utilise commercial search engine logs, one of which contains 11,813,260 search sessions. As authors indicate that they used 30% of the above sessions for computing (mean) click entropy, we assumed that the sample size was n = 3543978 even though this exact number is not indicated in the paper. The authors report p -value &lt; 10  X  5 ,sothe t statistic was computed as t = t inv ( n  X  1; 10  X  5 )=4 . 42 , and the future sample size obtained is n = 1425634 . Again, the effect size is extremely small:  X  es =0 . 002 . As for the 11th entry in Table 7 (SIGIR 2014, 10.1145/2600428.2609602), one of its authors works for the afore-mentioned search engine company. The authors use the unpaired t -test with large sample sizes ( n 1 + n 2 = 193524 , from which we assume that n 1 = n 2 = 96762 ) and report a p -value of 1 . 5  X  10  X  12 However, the purpose of employing this test in their study was to quantify the association between two different user search be-haviour features rather than to compare retrieval effectiveness: one feature is used to split the data into two samples, and then the other is used to compare the two samples in terms of the unpaired
There were also two SIGIR paper s (not included in the aforemen-tioned 75 papers that went through power analysis) for which the sample sizes for unpaired t -tests were too large for our power anal-ysis tool: A SIGIR 2010 paper (10.1145/1835449.1835537) used a sample of some 95.8M query-URL pairs per group; A SIGIR 2007 (10.1145/1277741.1277771) used two samples where the total sam-ple size was approximately 60M. These papers are from two differ-ent search engine companies: the latter paper is from the same com-pany as the aforementioned 11th and 12th entries in Table 7. All of the above papers should be commended (not condemned!) for pro-viding enough information in their papers for post hoc power anal-Figure 7: Summary of power analysis results: sample size ratio vs. achieved power. ysis; however, recall that reporting p -values from an experiment with extremely large sample sizes without discussing effect sizes is generally not very informative, as was discussed in Section 1.
Next, let us examine a few overpowered ANOVA cases, using Ta-ble 8 and Figure 7(b). More specifically, we examine two papers in-dicated by the two top right balloons in Figure 7(b): The second and seventh entries in Table 8 (SIGIR 2015, 10.1145/2766462.2767746 and SIGIR 2010, 10.1145/1835449.1835484). These papers have a common first author, and the experiments in both papers utilise a commercial social media application suite for the purpose of item recommendation. In the second entry, a one-way ANOVA result is reported, where the number of groups is m =3 (items rated  X  X ery interesting X  vs.  X  X nteresting X  vs.  X  X ot interesting X ), the degree of freedom is  X  = 7650 and the F statistic is 243 . 42 . Since our power analysis tool for one-way ANOVA recommends a per-group future sample size, we assume uniform group size for the analysis and let  X  = m ( n  X  1) = 7650 , which gives us the average group size n = 2551 . As shown in Table 8 and Figure 7(b), this sample size is much larger than the required sample size n =52 . Similarly, in the seventh entry, the authors report on a one-way ANOVA result with m =5 (five recommendation systems),  X  = 5496 .Again,if we assume equal group size for the purpose of power analysis, we obtain n = 5496 / 5 + 1 = 1100 . 2 , whereas the future sample size is n =51 . It can be observed in Table 8 that the effect size for the latter experiment is much smaller (  X  f =2 . 252 vs.  X  Again, let us emphasise that these are good papers which, unlike many other SIGIR papers, provide enough information for us to conduct post hoc power analysis. However, it is probably fair to say that many highly overpowered experiments come from indus-try, where data is abundant.
Arguably, extremely underpowered experiments may be more problematic than extremely overpowered experiments. Extremely overpowered experiments may conclude that some very small ef-fects are statistically significant, and the small effects may or may not be practically significant. On the other hand, extremely un-derpowered experiments may hide away very important real differ-ences forever.

First, let us have a look at two extremely underpowered paired t -test results indicated by the two leftmost balloons in Figure 7(a): the 24th and 23rd entries in Table 7. The 24th entry (SIGIR 2012, 10.1145/2348283.2348343) reports on many statistical significance results including ANOVA, but what we have selected for power analysis was a statistically insignificant result with a paired where n =28 participants were involved (within-subjects design) and two systems were compared in terms of a user experience sub-scale called  X  X ocused attention. X  As Table 7 shows, the ef-fect size is  X  X mall X  (  X  es =0 . 180 ) and the future number of partic-ipants is n = 244 , which is quite demanding for a user study. For this particular paper, we c onducted additional power analy-ses for the other paired t -test results reported:  X  X elt involvement X  (  X  es =0 . 026 , power =0 . 052 ,n = 12061 ),  X  X ndurability X  ( 0 . 061 , power =0 . 061 ,n = 2083 ),  X  X earch effectiveness X  ( 0 . 111 , power =0 . 111 ,n = 396 ) 14 . It can be observed that the actual sample size ( n =28 ) was too small regardless of what user experience sub-scale is used, as the effect sizes are very small. The 23rd entry (SIGIR 2009, 10.1145/1571941.1571947) reports on a user study with 24 participants for comparing two algorithms (LAIR2 vs. Buckshot), but since each participants performed two tasks with each algorithm, we assumed that the sample size was n =48 when the two algorithms were compared in terms of the F 1 measure. As Table 7 shows,  X  es =0 . 198 , power =0 . 269 ,n = 203 .(Evenif n =24 , the experiment is still underpowered: 0 . 280 , power =0 . 259 ,n = 103 .) We would like to emphasise, however, that these papers are also examples of good papers, which provide enough details even for results that did not turn out to be statistically significant. This is what enables us to conduct post hoc studies, and the important question is how to design future experi-ments for similar studies.

Finally, we discuss the 12th and the 11th entry in Table 8, the two extremely underpowered cases indicated as the two leftmost balloons in Figure 7(b). The 12th entry (SIGIR 2015, 10.1145/2766462.2767719) reports on a statistically nonsignificant repeated-measures ANOVA result, where the relationship between four levels of search latency ( m =4 ) and Skin Conductance Re-sponses (SCRs) were examined. This can be regarded as a two-way ANOVA without replication case, with  X  E =( m  X  1)( n  X  1) = 48 and therefore the number of participants per group is n =17 As can be seen at the bottom of Table 8, the effect size is 0 . 039 , power =0 . 183 ,n =75 . Thus this experiment would have required 75 participants. The same paper also reports on statistically nonsignificant ANOVA results for examining the re-lationship between search latency and self-reported measures of engagement: if we apply the same R script to these results with  X 
E =( m  X  1)( n  X  1) = 54 ,m =4 ,n =19 , we obtain 0 . 042 , power =0 . 215 ,n =71 for CSUQ (Computer System Usability Questionnaire),  X  f 2 =0 . 044 , power =0 . 222 ,n =68 for FA (Focused Attention),  X  f 2 =0 . 052 , power =0 . 257 ,n = 59 for post-NAS (Negative Affect), and  X  f 2 =0 . 068 , power = 0 . 332 ,n =46 for post-PAS (Positive Affect). Thus, regardless of which ANOVA result we choose, relying on only n =17 par-ticipants results in very low power, unfortunately. For the 11th en-try (SIGIR 2008, 10.1145/1390334.1390362), we selected statis-tically nonsignificant one-way ANOVA results for comparing the
For  X  X erceived usability X , the t statistic was zero, which suggests that there is no effect. demographic characteristics of m =3 participant groups, each with n =12 subjects. As Table 8 shows, we would have wanted n =43 participants per group based on these particular results. However, these results do not constitute the main part of their study.
All of the  X  X nderpowered X  papers discussed above are user study papers. This is not so surprising, as hiring good participants for experiments can always be difficult. Many such papers use statisti-cal tests and report the results appropriately, but we argue that we should learn from past experiments as we have done in this study so that we can conduct better-designed experiments in the future.
We conducted a systematic review of 840 SIGIR full papers and 215 TOIS papers published between 2006 and 2015. Our main findings are as follows:
To recap, p -values alone are not very informative as results of statistical significance testing, because one can obtain arbitrarily small p -values for any experiment by using a large enough sam-ple [5, 14]. Hence, whenever researchers report on statistically significant differences based on overpowered experiments, it is vi-tal that they report the effect sizes in addition to p -values. As for underpowered user experiments, researchers should conduct pilot studies first, or learn from similar studies in prior art about effect sizes and appropriate sample sizes.

One original question that is left unanswered in the present study is: are existing test collections with 50-100 topics good enough? Statistical requirements for comparing any systems suggest that test collection require many more topics [15, 20], but are current test collections actually serving the purpose for comparing existing systems? The question is left unanswered because, as we have seen above, many system effectiveness papers do not provide enough information for post hoc power analysis: recall that either an ex-act p -value or a test statistic (with a clearly stated sample size) is required for this. To make matters worse, there is the publication bias problem: researchers are ofte n tempted not to report on statis-tically nonsignificant results. We already see some good reporting practices in interactive IR papers, complete with test statistics and effect sizes even for statistically nonsignificant results; we believe that similar practices are in order for the rest of the IR community as well.
 We thank Professor Hideki Toyoda (Waseda University) for letting us modify his R code and distribute it.
