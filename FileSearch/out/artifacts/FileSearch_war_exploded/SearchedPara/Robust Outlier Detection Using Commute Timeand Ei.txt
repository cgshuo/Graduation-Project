 Unlike other data mining techniques whic h extract common or frequent patterns, the focus of outlier detection is on finding abnormal or rare observations in the data. Standard techniques for outlier detection include statistical [7,14], distance based [2,10] and density based [3] approaches. However, standard statistical and distance based approaches can only find global outliers which are extremes with respect to all observations in the dataset. On the other hand local outliers are extremes with respect to their neighbo rhood observations, but may not be ex-tremes with respect to all other observat ions in the dataset [16]. A well-known method for detecting local outliers is the Local Outlier Factor (LOF), which is a density based approach [3]. The downside of LOF is the outlier score of each observation only considers its local neighborhood and does not have the global view over all the dataset. Recentl y, Moonesinghe and Tan [13] proposed a method called OutRank to detect outlier using a random walk on graph. The outlier score is the connectivity of each node which is computed from a station-ary random walk. This method cannot find outlying clusters where the node connectivities are still high. An excellent survey by Chandola et. al [4] provides a more detailed view on out lier detection techniques.

In this paper, we present a new method to find outliers using a measure called commute time distance, or commute distance for short (CD) 1 .CDisa well-known measure derived from a random walk on graph [11]. The CD between two nodes i and j in the graph is the number of steps that a random walk, starting from i will take to visit j and then come back to i for the first time. Indeed CD is a Mahalanobis distance in the space spanned by eigenvectors of the graph Laplacian matrix. Unlike traditional Mahalanobis distance, CD between two nodes can capture both the distance between them and their local neighborhood densities so that we can capture both global and local outliers using distance based methods such as methods in [2,10]. Moreover, the method can be applied directly to graph data.

To illustrate, consider a graph of five data points shown in Figure 1, which is built from a dataset of five observations. Denote d ED ( i, j )and d CD ( i, j )as an Euclidean distance and a CD between observations i and j , respectively. The distances between all pairs of observations are in Table 1.
 , i = j ) even though d ED ( i, j )havethesameorlargerEu clidean distances than d
ED (1 , 2). The CD from an observation outside the cluster to an observation inside the cluster is significantly larger than the CDs of observations inside the cluster. Since CD is a metric, a distance based method can be used to realize that point 1 is far away from other points using CD. Therefore, the use of CD is promising for identifying outliers. The contributions of this paper are as follows:  X  We prove that CD can naturally capture the local neighborhood density and  X  We propose an outlier detection metho d using the CD metric to detect global  X  We accelerate the computation of CD u sing a graph component sampling The remainder of the paper is organized as follows. Section 2 reviews the theory of random walk on graph and CD. In Section 3, we introduce the method to detect outliers with the CD measure. S ection 4 presents a w ay to approximate CD and accelerate the algorithm. In Sect ion 5, we evaluate our approach using experiments on real and synthetic dat asets. Section 6 is the conclusion. 2.1 Random Walk on Graph and Stationary Distribution The random walk on a graph is a sequence of nodes described by a finite Markov chain which is time-reversible [11]. The probability that the random walk on node i at time t selects node j at time t + 1 is determined by the edge weight on the adj ( i )isasetofneighborsofnode i .

Let P be the transition matrix with entry p ij , A is the graph adjacency matrix, and D is the diagonal matrix with entries d ii .Then P = D  X  1 A .Denote  X  i ( t )as state probability distribution at time t , the state on transforming is  X  ( t +1)= P T  X  ( t ) and thus  X  ( t )=( P T ) t  X  (0) where  X  (0) is an initial state distribution. The distribution  X  ( t ) is stationary if  X  ( t )=  X  (0) for all t&gt; 0. 2.2 Commute Distance This section reviews two measures of a random walk called hitting time h ( i, j ) and commute time c ( i, j ) [11]. The hitting time h ( i, j ) is the expected number of steps a random walk starting at i will take to reach j for the first time: The commute time, which is known to be a metric and that is the reason for the term  X  X ommute distan ce X  [6], is the expected number of steps that a random walk starting at i will take to reach j once and go back to i for the first time: The CD can be computed from the Moore-Penrose pseudoinverse of the graph Laplacian matrix [9,6]. Denote L = D  X  A and L + as the graph Laplacian matrix and its pseudoinverse respectively, the CD is: where V G = n i =1 d ii is the volume of the graph and l + ij is the ( i, j )elementof L + .Equation3canbewrittenas where e i is the i -thcolumnofan( n  X  n )identitymatrix I [15]. Consequently, c ( i, j ) 1 / 2 is a distance in the Euclidean space spanned by the e i  X  X . 3.1 A Proof of Commute Distance Property for Outlier Detection We now show that CD is a good metric for local outlier detection.
 Lemma 1. The expected number of steps that a random walk which has just visited node i will take before returning back to i is V G /d ii .
 Proof. For the proof of this Lemma, see [11].
 Theorem 1. Given a cluster C and a point s outside C connected to a point t on the boundary of C (Fig. 2a). If C becomes denser (by adding more points or edges), the CD between s and t increases.
 Proof. From Lemma 1, the expected number of steps that a random walk which has just visited node s will take before returning back to s is V G /d ss = V G /w st . Since the random walk can only move from s to t , V G /w st = h ( s, t )+ h ( t, s )= c ( s, t ) (Fig. 2b). If cluster C becomes denser, there a re more edges in cluster C . As a result, V G increases while w st is unchanged. So the CD between s and t (i.e c ( s, t )) increases. * + As shown in Theorem 1, the denser the cluster, the larger the CD between a point s outside the cluster to a point t in the cluster. That is the reason why we can effectively detect local outliers using CD. 3.2 Outlier Detection Using Commute Distance This section introduces a method based on CD to detect outliers. As CD is a met-ric and captures both the distance between nodes and their local neighborhood densities, we can use a CD based method to find global and local outliers. First, a mutual k 1 -nearest neighbor graph is constructed from the dataset. The mutual k 1 -nearest neighbor graph connects nodes u and v if u belongs to the k 1 nearest neighbors of v and v belongs to the k 1 nearest neighbors of u .The reason for choosing mutual k 1 -nearest neighbor graph is that this graph tends to connect nodes within cluster of similar densities, but does not connect nodes from clusters of different densities [12]. Therefore, outliers are isolated and data clusters form graph components in mutual k 1 -nearest neighbor graph. Moreover, the mutual k 1 -nearest neighbor graph with n nodes ( k 1 n ) is usually sparse, which has an advantage in computation. If the data has coordinates, we can use kd -tree to avoid O ( n 2 ) searching of nearest neighbors. The edge weights are inversely proportional to their Euclid ean distances. However, it is possible that the mutual k 1 -nearest neighbor graph is n ot connected so that we cannot apply random walk on the whole graph. One approach to make the graph con-nected is to find its minimum spanning tree and add the edges of the tree to the graph.
 Then the graph Laplacian matrix L and its pseudoinverse L + are computed. After that the pairwise CDs between any two observations are calculated from L + . Finally, the distance based outlier detection using CD with pruning tech-nique proposed by Bay and Schwabacher [2] is used to find the top N outliers. The main idea of pruning is that an observation is not an outlier if its average dis-tance to k 2 current nearest neighbors is less th an the score of the weakest outlier among top N found so far. Using this approach, a large number of non-outliers can be pruned without carrying out a full database scan. The outlier score used is the average distance of an observation to its k 2 nearest neighbors. Suitable values for k 1 (for building the nearest neighbor graph) and k 2 (for estimating the outlier score) will be presented in the experiments. While CD is a robust measure for detecting both global and local outliers, its main drawback is its computational time. The direct computation of CD from L + is proportional to O ( n 3 ) which is not feasible for large graphs ( n is the number of nodes). In this work, the graph components are sampled to reduce the graph size and then eigenspace approximation in [15] is applied to approximate the CD on the sampled graph.
 4.1 Graph Sampling An easy way to sample a graph is selecting nodes from it uniformly at random. However, sampling in this way can break the graph geometry structure and outliers may not be chosen in sampling. To resolve this, we propose a sampling strategy called component sampling. After creating the mutual k 1 -nearest neigh-bor graph, the graph tends to have many connected components corresponding to different data clusters. Outliers are either isolated nodes or nodes in very small components. For nodes in normal components (we have a threshold to dis-tinguish between normal and outlying components), they are uniformly sampled with the same ratio p =50 k 1 /n , which is chosen from experimental results. For nodes in outlying components, we sample all of them. Then we rebuild a mu-tual k 1 -nearest neighbor graph for the sampled data. Sampling in this way will maintain the geometry of the original graph and the relative densities of normal clusters. Outliers are also not sampled in this sampling strategy. 4.2 Eigenspace Approximation Because the Laplacian matrix L ( n  X  n ) is symmetric and has rank n  X  1 [5], it can be decomposed as L = VSV T ,where V is the matrix containing eigenvectors of L as columns and S is the diagonal matrix with the corresponding eigenvalues  X  1 =0 &lt; X  2 &lt; ... &lt;  X  n on the diagonal. Then L + = VS + V T where S + is the  X  n = 0. Equation 4 can be written as c ( i, j )= V G ( x i x i = S +1 / 2 V T e i [15]. Therefore, the CD between nodes on the graph can be viewed as the Euclidean distance in the space spanned by eigenvectors of the graph Laplacian matrix.

Denote  X  V ,  X  S as a matrix containing m largest eigenvectors of L + and its The CD c ( i, j )inan n dimensional space is transformed to the CD  X  c ( i, j )in an m dimensional space. Therefor e, we just need to compute the m smallest eigenvectors with non zero eigenvalues of L (i.e the largest eigenvectors of L + ) to approximate the CD. This approximation is bounded by c ( i, j )  X   X  c ( i, j )  X  V 4.3 Algorithm The proposed method is outlined in Algorithm 1. We create the sampled graph from the data using graph components sampling. Then the graph Laplacian L of the sampled graph and matrix  X  V ( m smallest eigenvectors with nonzero eigenvalues of L ) are computed. Since we use the pruning technique, we do not need to compute the approximate CD for all pairs of points. Instead, we compute and v jk are entries of matrix  X  V . 4.4 The Complexity of the Algorithm The k -nearest neighbor graph with n nodes is built in O ( nlogn )using kd -tree with the assumption that the dimensionality of the data is not very high. The average degree of each node is O ( k )( k n ). So the graph is sparse and thus finding connected components take O ( kn ). After sampling, the size of graph is O ( n s )( n s n ). The standard method for eigen decomposition of L is O ( n 3 s ). Since L is sparse, it would take O ( Nn s )= O ( kn 2 s )where N is the number of nonzeros. The computation of just the m smallest eigenvectors ( m&lt;n s )isless expensive than that.

The typical distance base d outlier detection takes O ( n 2 s ) for the neighborhood search. Pruning can scale it nearly linear. We only need to compute the CDs O ( n s ) times each of which takes O ( m ).

So the time needed for two steps is proportional to O ( nlogn + kn + kn 2 s + mn s )= O ( nlogn )as n s n .
 Algorithm 1. Commute Distance Based Outlier Detection with Graph Com-ponent Sampling and Eigenspace Approximation.
 In this section, the effectiveness of C D as a measure for outlier detection is evaluated. Firstly, the ability of the distance based technique using CD (de-noted as CDOF) in finding global, local outliers, and outlying clusters was tested in a synthetic dataset. The distance based technique using Euclidean distance [2] (denoted as EDOF), LOF [3], and OutRank [13] (denoted as ROF and the same graph of CDOF was used) were also used to compare with CDOF. Secondly, the effectiveness of CDOF was evaluated in a real dataset. Thirdly, we have shown that CDOF is more resistant to small perturbations to data than LOF. Finally the performance an d effectiveness of approximate CDOF were evaluated. The experiments were conducted on a workstation with an 3GHz Intel Core2 Duo processor and 2GB of main memory in Windows XP.
 5.1 Synthetic Dataset Figure 3a shows a 2-dimensional synthetic dataset. It contains one dense cluster of 500 observations ( C 1 ) and one sparse cluster of 100 observations ( C 2 ). More-over, there are three small outlying clusters with 12 observations each ( C 3  X  5 ) and four outliers ( O 1  X  4 ). All the clusters were generated from a Normal distri-bution. O 2 , O 3 , O 4 are global outliers which are far away from other clusters. O 1 is a local outlier of dense cluster C 1 .

In the following experiments for this dataset, the numbers of nearest neigh-bors are k 1 = 10 (for building the graph), k 2 = 15 (for estimating the outlier score. Since the size of outlying clusters is twelve, fifteen is a reasonable number to estimate the outlier scores), and the number of top outliers is N = 40 (the total observations in three outlying clusters and four outliers). The results are shown in Figure 3. The  X  X  X  signs mark the top outliers found by each method. The figure shows that EDOF cannot detect local outlier O 1 .BothEDOFand LOF cannot find two outlying cluster C 3 and C 4 . The reason is those two clus-ters are near each other with similar den sities and consequently for each point in the two clusters the average distance to its nearest neighbors is small and the relative density is similar to that of its neighbors. Moreover, ROF outlier score is actually the node probability distribution when the random walk is stationary capture nodes in the outlying clusters where d ii is large. For degree one outlying nodes, ROF and CDOF have similar scores. The result in Figure 3b shows that CDOF can identify all the outliers and o utlying clusters. The key point is in CD, inter-cluster distance is significantly larger then intra-cluster distance even if the two clusters are near i n the Euclidean distance. 5.2 Real Dataset In this experiment, CDOF was used to find outliers in an NBA dataset. The dataset contains information of all the players in the famous basketball league in the US in year 1997-1998. There were 547 players and six attributes were used: position, points per game, rebounds per game, assists per game, steals per game and blocks per game. Point and assist reflect the offensive ability of a player while steal and block show how good a player is in defending. Rebound can be either offensive or defensive but total rebound is usually an indicator of defensive ability. The results are shown in Table 2 with the ranking and statistics of top five outliers. The table also shows the maximums, averages, and standard deviations for each attribute over all players.

Dikembe Mutombo was ranked as the top outlier. He had the second highest blocks (5.6 times of standard deviation away from mean), the highest rebounds for center players, and high points. It is rare to have good scores in three or more different statistics and he was one of the most productive players. Dennis Rodman and Michael Jordan took the second and third positions because of their highest scores in rebound and point (4.6 and 3.7 times of standard deviation away from mean, respectively). Dennis Rodman was a rare case because his points were quite low among high rebound players as well. The next was Shaquille O X  X eal who had the second highest points and high rebounds. He was actually the best scoring center and is likely a local outlier among center players. Finally, Jayson Williams had the second highest rebounds. It is interesting to note that except for Dennis Rodman because of his bad beh aviour in the league, the other four players were listed in that year a s members of All-Stars team [1]. 5.3 Sensitivity to Data Perturbation In this section LOF and CDOF were compared on their ability to handle  X  X oise X  perturbations in data. Recall that LOF ( p ) is the ratio between the average rela-tive density of the nearest neighbors q of p over the relative density of p . LOF ( p ) is high (i.e p is outlier) if p  X  X  neighborhood area is sparse and q  X  X  neighborhood area is dense. Suppose that noise is uniformly distributed in the data space, it is obvious that the noise will have more effect on outliers than points in clus-ters. The noise data can be neighbors of outliers and their neighborhood are also sparse. Thus the numerator in LOF ( p )formulawhere p is outlier reduces considerably while the denominator increases. As a result, LOFs of outliers may reduce significantly but they does not change much for points inside the clus-ters. Therefore, the relative rankings of data may not be preserved. On the other hand, uniform noise changes the nearest neighbor graph for CDOF in the way that degrees of outliers will increase but are still much smaller than degree of points inside the clusters. Thus there will still be a big difference between inter-cluster and intra-cluster CDs. That maint ains the higher scores for outliers than the points inside the cluster.

To show this, we randomly added 10% noise from a uniform distribution to the synthetic dataset in Section 5.1 and applied LOF and CDOF in the new dataset. Then noise data was removed from the ranking. Two criteria were used to compare two methods: Spearman rank test for the ranking of the whole dataset and the similarity between the sets of top outliers before and after adding noise. The results were averaged over ten tria ls. Spearman rank test in LOF was 0.01 while the it was 0.48 for CDOF. It shows the relative ranking by LOF changes significantly due to noise effect. After adding noise, there were 62% of the original outliers still in the top outlier list for LOF while it was 92% for CDOF. CDOF is less sensitive as it combines local and global views of the data.
Since noise is a kind of outlier, we cannot distinguish between outliers and noise but the preliminary results for noi se effect show that the proposed method is more resistance to noise than the local outlier detection method. 5.4 Performances of the Proposed Method In the following experiments, we comp ared the performan ces of EDOF, LOF, and approximate CDOF mentioned in Sect ion 4. The experiment was performed using five synthetic datasets, each of which contained different clusters gener-ated from Normal distributions and a number of random points. The number of clusters, the sizes, and the locations of the clusters were also chosen randomly. The results are shown in Figure 4a where the horizontal axis represents the dataset sizes in the ascending order and the vertical axis is the corresponding computational time. The result of approximate CDOF was averaged over ten trials of data sampling. It is shown that approximate CDOF is faster than LOF and slower than EDOF. This reflects the complexities of O ( n ), O ( nlogn ), and O ( n 2 ) for EDOF, approximate CDO F, and LOF, respectively.

In order to validate the effectivenes s of approximate CD, we used CD and approximate CD to find outliers in five synthetic datasets generated in the same way as the experiment noted above with smaller sizes due to the high computa-tion of CDOF. The results were averaged over ten trials. The results in Figure 4b shows approximate CDOF (aCDOF) is much faster than CDOF but still preserves a high percentage (86.2% on average) of top outliers found by CDOF. 5.5 Impact of Parameter k In this section, we investigate how the number of nearest neighbors affects CDOF. Denote k min as the maximum number of nodes that a cluster is an outlying cluster and k max as the minimum number of nodes that a cluster is a normal cluster. There are two situations. If we choose the number of nearest neighbors k 2 &lt;k min , nodes in an outlying cluster do not have neighbors outside the cluster. As a result, their outlier factors are small and we will miss them as outliers. On the other hand, if we choose k 2 &gt;k max , nodes in a normal clus-ter have neighbors outside the cluster. And it is possible that some nodes in the cluster will be falsely recognized as outliers. The value of k min and k max can be considered as the lower and upper bounds for the number of nearest neighbors. They can be different depending on the application domains. In the experiment in Section 5.1, we chose k 2 = 15, which is just greater than the sizes of all outlying clusters (i.e 12) and is less than the size of the smallest normal cluster (i.e 100). The same result can be obtained with 15 &lt;k 2 &lt; 100 but it re-quires longer computational time. k 2 is also chosen as a threshold to distinguish between normal and outlying clusters.

Note that k 2 mentioned in this section is the number of nearest neighbors for estimating the outlier scores. For building mutual k 1 -nearest neighbor graph, if k 1 is too small, the graph is very sparse and may not represent the dataset densities properly. In the experiment in Section 5.1, if k 1 = 5, the algorithm misclassifies some nodes in the smaller normal cluster as outliers. If k 1 is too large, the graph tend to connect togeth er clusters whose sizes are less than k 1 and are close to each other. Then some o utlying clusters may not be detected if they connect to each other a nd form a normal cluster. k 1 = 10 is found suitable for many synthetic and real datasets. We have proposed a method for outlier detection using  X  X ommute distance X  as a metric to capture global, local outliers, and outlying clusters. The CD cap-tures both distances between observatio ns and their local neighborhood densi-ties. We observed and proved a property of CD which is useful in capturing local neighborhood density. The experiments have shown the effectiveness of the pro-posedmethodinbothsyntheticandreald atasets. Moreover, graph component sampling and eigenspace approximation used to approximate CD and the use of pruning rule can accelerate the algorithm significantly while still maintaining the accuracy of the detectio n. Furthermore preliminary experiments suggest that CDOF is less sensitive to perturbations in data than other measures. The authors of this paper acknowledge the financial support of the Capital Mar-kets CRC.

