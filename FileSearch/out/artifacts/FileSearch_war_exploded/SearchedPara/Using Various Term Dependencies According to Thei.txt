 In this paper, we propose a model to integrate term dependencies. Different from previous studies, ea ch pair of terms is assigned a different weight of dependency ac cording to their utility to IR. The experiments show that our model can significantly outperform the previous dependency models using fixed weights. H.3.3 [ Information storage and retrieval ]: Information Search and Retrieval  X  Retrieval Models Algorithms, Performance, Experimentation, Theory. Discriminative Model, Language Model, Term Dependency, Dependency Strength. Traditional Information Retrieval (IR) approaches assume independence between terms, l eading to the so-called bag-of-words models. However, terms in a natural language sentence are often dependent. For example, phrases such as  X  X pace program X  and  X  X lack Monday X  have drasti cally different meanings from those of their constituent words. The bag-of-words models are unable to consider their specific meanings. This problem has been the subject of a large number of studies. Within the language modeling framework, the typical a pproach is to create one or more additional models for documents to capture term dependencies of certain types. Th ese models are combined with the traditional unigram model so that the documents in which the required dependencies between query terms are present are ranked higher. However, we observe that most previous approaches used fixed weights in such a combination. This is equivalent to say that any term dependency of the same type, say adjacency, has equal importance in the retrieval process. This is obviously untrue. For example, the adjacency between  X  X lack X  and  X  X onday X  in the expression  X  X lack Monday X  corr esponds to a strong dependency, while the one in  X  X omputer game X  is less critical  X  even if the dependency between  X  X omputer X  and  X  X ame X  is ignored, the retrieval effectiveness using th e unigram model would not be much degraded. The difference between the two cases lies in their utility for IR. On the one hand, if the meaning of the dependent terms together is compositional, then the omission to consider the dependency is not problematic. On the other hand, if the meaning consideration of the dependency in IR is crucial. The variable strength of dependency between terms has not been correctly coped with in previous approaches. adjacent words only. However, de pendencies can span over more distant terms. For example, in  X  X rocessor specifically designed for laptop computers X , there is a strong dependency between the distant words  X  X rocessor X  and  X  X ap top X . Moreover, the strength of dependency between distant terms is not necessarily weaker than closer terms. For example, in  X  X omputer aided crime X , the relation between  X  X omputer X  and  X  X rime X  is much stronger and useful for IR than the adjacent pairs  X  X omputer aided X  and  X  X ided crime X . Therefore, the strength of the dependency is not only a function of their distance. In this paper, we propose a new dependency model to account for the above two aspects. In this model, each term dependency is weighted according to its stre ngth and possible impact on the retrieval effectiveness. An important task is to determine such a strength and impact. We will propos e a learning process for it using a set of features. Our experiments will show higher effectiveness of the approach than those in the literature. An obvious extension to a bag-of-words model is to combine it with higher-order n-gram models such as bigram and trigram models. However, the combination with bigram and trigram models leads to less improvement in retrieval effectiveness than one would expect [13]. The reason is twofold: (1) bigrams and trigrams only capture the depende ncies between adjacent terms while many useful dependencies are between more distant terms. (2) a large number of the bigrams and trigrams do not correspond to true dependencies. To deal wi th the first problem, [14] proposes a less strict biterm model, whic h does not require the terms to occur in the specific order. To deal with dependencies between more distant terms, proximity models further consider the proximity of query terms in a document, such as [6][15][7][18]. The proximity language model (PLM) proposed in [18] performs empirically better than previous proximity models. Proximity information is integrated into the document model as follows: where  X   X   X   X   X ;  X  is the count of word w i in D ,  X  X  X  X  proximity centrality of term  X   X  , and  X  X  X   X  language model for smoothing. The best method to compute  X  X  X  X   X   X   X   X   X  turned out to be the following one : where  X   X   X   X   X  X  X  X  X   X  X  , and  X  X  X  X  X   X   X ,  X   X  X ; is the minimum distance between  X   X  and  X   X  in document D.
 Metzler and Croft [8] proposed another method to capture the term dependency: a Markov random field (MRF) model for IR. A Markov random field is a grap hical model in which a graph G consists of query nodes  X   X  (each representing a term) and a document node D . The ranking function is defined based on the following joint distribution over the random variables in G : where  X   X   X   X   X ,...,  X  is a query,  X  X  X  X  X  the set of cliques in G ,  X  a normalization factor, and each  X  X ; X  X  X  a non-negative potential function over the clique configuration ( c ) parameterized by  X  . single query term, on ordered term clique and on unordered term clique. Each potential function is defined as a language modeling estimation smoothed by the collection, and the parameters  X   X ,  X   X ,  X  are weights associated to the models. Two specific dependency models are proposed  X  full dependency model (MRF-FD) which considers all dependen cies and sequential dependency model (MRF-SD) which only considers dependence between adjacent query terms. In practice, MRF-FD is difficult to implement because of its complexity, especially when the query becomes long. We notice that all the methods described above assign a fixed parameter to each component model. This means that all dependencies of a given type (e.g. adjacency or proximity) are assumed to have equal importance in the whole retrieval process. Although this makes the model easier to impl ement, the assumption is not reasonable. This fact has also been observed by Bendersky et al. [1]. In order to consider the variable impact of term dependencies, they extended the MRF-SD model so that the parameters become dependent on th e individual term or term pair: in which the functions  X   X   X  X  X  X  is a feature defined over unigrams or Documents are ranked according to the following equation:  X   X   X  |  X   X   X   X  X  X  X  X  X   X  X   X   X   X   X   X   X   X   X , X   X  The above model is called Wei ghted MRF-SD (WSD). This extension goes in the same direction as the method we propose in following aspects:  X  Term dependency is limited to two adjacent terms.  X  The ordered term bigram and unordered term biterm are In our model, we remove the above two limitations. In this paper, we propose a dependency model within the framework of discriminative m odels. This framework has the advantage of being able to incl ude some distant dependencies without having to consider all the dependencies as in MRF. A typical discriminative model is formulated as follows [4][11]: where  X   X   X  X , X  X  is a feature function with weights  X  normalization constant. In our mode l, in addition to unigrams, we consider the term dependencies be tween the following types term pair: (1) Ordered bigrams, (2) Unordered co-occurrence dependency within some distances. Several window sizes will be used: 2, 4, 8 and 16. Let us use  X   X  to denote term co-occurrences within the window size w. In particular,  X   X  considers unordered adjacent terms, or biterms [14]. The ranking function is extended from Equation (1) to the following one:  X   X   X  X  X  |  X , X   X   X  X  X  X   X   X  X   X  |  X   X   X   X   X  X   X   X  X , This model contains three classe s of features: unigram features  X   X  X   X   X  X , , bigram features  X   X   X  X   X   X   X  X  X   X  X , and co-occurrence features  X   X  associated with a function  X  X  X  X  X  denoting the importance of the feature for the query Q . This function allows us to take into account the dependencies between bigrams and co-occurring terms according to their strength and utility. This is fundamentally different from most previous models (except [1]) in which a fixed weight is assigned to the whole component model rather than to individual features. The discriminative feature functions we use are as follows: where  X  X   X   X .  X   X   X  denote a pair of co-occurring terms  X  document within a window of size w . For the ranking purpose, we will simply fix  X   X   X  X   X  |  X   X  at a constant 1, and try to vary the other  X  functions for bigrams and co-occurring terms. We will use a set of features to determine the importance of a bigram and co-occurring terms in a query (see Section 4). For the query models, we will simply use Maximum Likelihood (ML) estimation as follows, where  X   X  is an item of type R (a unigram, a bigram or a pair of co-occurring terms) and  X   X   X  its count in the query: For the document models on diffe rent types of item, we use Dirichlet smoothing as follows: where  X   X   X   X   X ;  X  is the number of times the item  X  document D (within a window for  X   X  );  X   X   X   X   X  |  X   X   X  the collection language model;  X   X  is a Dirichlet prior for the the expression of R , i.e. the total number of unigrams, bigrams or co-occurring terms within the corresponding window size. For instance, |  X  |  X  is the number of terms (unigrams) in the document, |  X  |  X  Putting all together, we have the following final model:  X   X   X  X  X  |  X , X   X   X   X  X  X  X  X  X   X  X   X   X  X  X   X   X   X  |  X   X   X  X  X  X   X   X   X   X   X ,  X  X  X   X |  X   X   X   X  X  X   X   X   X   X   X  X  X  |  X   X   X log  X   X   X   X   X   X  X  X   X  To give an example to illustrate the model, let us imagine a query of three words  X  abc  X . The first component of the model considers the unigrams a, b and c. The second component concerns the bigrams ab and bc . The third component considers the co-documents. We have the following free parameters to be estimated: (1) Dirichlet priors  X  for each component language model (2) Dependence strength  X  for each bigram and pair of co-occurring terms. Determining the Dirichlet Priors It is intuitive to see that a l onger document expression (e.g. with a require a large  X  . Therefore, the Dirichlet priors are set according the document length in the bigram and co-occurrence expressions. We set  X   X  ,  X   X  3000, 7000 and 15000 respectively. Learning the Importance of a Dependency following parameters to estimate:  X   X  X  X   X  X   X  X  X , X   X  parameters for  X   X  is  X   X   X  X  X  X   X  X  X  , X   X  to determine these parameters us ing a set of training data using SVM. The training data is obtained as follows: for each query  X  we try to determine the best parameters  X   X   X  so as to maximize the output E (MAP in our case):  X   X   X   X  X  X  X  X  X  X   X   X  , we use the coordinate-level ascent algorithm proposed in [9]. terms, and  X   X  is the optimal  X   X   X   X   X   X   X  (  X , X  X  X  X  found above. Then we use epsilon Support Vector Machine Regression (  X  -SVR ) [17] to train  X   X   X  X  X  X  based on a set of features. The features we use in this paper for a pair of terms include the pointwise mutual information in a large independent collection and in the current the binary value of term pair occurs in Termium 1 Wikipedia articles 2 , etc. In our experiments, we use the LIBSVM experiments, 10-fold cross validation is used. Our experiments are performed on the collections listed in Table 1. Table 1. Characteristics of do cument collections and queries Coll. #doc Size (GB) Query IDs #Qry WT10g 1,692,096 11.03 451-550 97 2.44 We performed the following common pre-processing on all documents: Some unimportant fi elds and tags are removed; Stopwords are removed using a 625-stopword list from Lemur toolkit; Words are stemmed by Porter stemmer. We compare our model with the following baseline models: unigram language model (Uni), MRF-SD, weighted MRF-SD (WSD), and PLM. For MRF-SD and PLM, we use a grid search to find the best MAP for each collection. The step for searching  X   X ,  X   X ,  X  for MRF-SD is 0.05. The search space of  X  X  X  X  in the PLM model is from 1.5 to 1.9 and  X  from 3 to 9. To compare with the WSD model, we simulate the implementation in [1] but without using features from Google n-grams corpus and Microsoft 2006 RFP query logs , which were used in [1]. In our model (DLM), the weights are assigned to individual term pairs. The weights are determined by cross validation: for each collection, 9/10 of the queries are used in turn as training data while the remaining 1/10 of the queries are used as test queries. In Table 2, we report the average effectiveness obtained in the cross validation. Compared to the other models, we can see that this model performs generally better. The only exception is on Disk4 data, compared to PLM. In a number of cases, the differences with the other models are statistically signif icant. This result shows that the model we propose in this paper can indeed lead to additional gains extensions we brought to this m odel, i.e. the consideration of more distant term dependencies and the weighting of individual incorporated into IR models. The improvements in some cases are not statistically significant. However, we have to notice that the weights we obtain from cross validation are far from optimal, wh ile for the other models we tune the parameters to their best. So, the above comparison gave A phrase dictionary built for French/English machine translation, which contains 853K phrases. http://download.wikimedia.org/enwiki/, on 2007-12-12 http://www.csie.ntu.e du.tw/~cjlin/libsvm/ http://www.lemurproject.org/ considerable advantages to the ot her models. In order to see the potential of a model with the a bove two extension, we try to determine the best weights for each individual term pair by a coordinate-level ascent search as explained in Section 4. The ideal optimal effectiveness is far be yond what we can obtain by cross validation. This shows that there is a large room for further improvements using our model. Terms in documents and queries ar e often dependent. A model that However, a model that consider s all the terms to be equally dependent also runs the danger to connect terms that are not strongly dependent and impose such a false dependency as a requirement in the retrieval proce ss. As a result, such a model may miss relevant documents in whic h the false dependency does not appear. If one treats all the dependencies of the same kind in a previous models, one will end up by assigning a moderate unique weight to the dependencies because of the above danger. The real problem is that term dependencies vary largely: a pair of terms such as  X  X lack Monday X  is strongly dependent, and the use of this dependency in the retrieval process is highly beneficial; while other separately. Therefore, each pair of terms should be treated in its own way according to the strength of the dependency and the usefulness of considering the pair of terms together. This is the goal of the approach proposed in this paper. Our model extends the existing dependency models on the two following aspects:  X  We assign weights to individual term pairs rather than to  X  We consider dependencies between more distant terms, while Our experimental results on TREC data showed that our model can of-the-art methods, given the optim al effectiveness we could expect to obtain with it. The difference between the result obtained by our implementation and the ideal case suggests several promising avenues to pursue in the future:  X  The set of features used to determine the weights of pairs of  X  Other methods for learning the weights could perform better;  X  Finally, we may need a larger amount of training data to [1] Bendersky, M., D. Metzler an d W. B. Croft. Learning [2] Croft, W.B., H.R. Turtle, and D. D. Lewis. The use of [3] Fagan, J. L. Automatic phrase indexing for document [4] Gao, J., H. Qi, X. Xia, J-Y. Nie. Linear disc riminant model [5] Gao, J., J.-Y. Nie, G. Wu, and G. Cao. Dependence language [6] Ken, E.M. Some aspects of proximity searching in text [7] Lv, Y. and C. Zhai. Positional language models for [8] Matzler, D. and W. B. Crof t. A Markov random field model [9] Metzler, D. and W. B. Croft. Linear feature-based models for [10] Nallapati, R. and J. Allan. Capturing term dependencies [11] Nallapati, R. Discriminative models for information retrieval. [12] Ribeiro-Neto, B.A. and R. M untz. A belief network model [13] Song, F. and W. Croft. A general language model for [14] Srikanth, M. and R. Srihari. Biterm language models for [15] Tao, T. and C. Zhai. An exploration of proximity measures in [16] Turtle, H. R. and W. B. Croft. Inference networks for [17] Vapnik, V.N. Statistical Learning Theory . Wiley, 1998. [18] Zhao, J. and Y. Yun. A Pr oximity language model for These are some of the aspects that we will investigate in the future. 
