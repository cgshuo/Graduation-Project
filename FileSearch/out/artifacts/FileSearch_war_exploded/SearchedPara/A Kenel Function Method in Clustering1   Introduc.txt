 Cluster analysis is one of main methods used in data mining. So far there have existed many cluster analysis approaches. For example, partitioning method [1][2], density-based [3][4], k-means [5], k-nearest neighborhood [4], neural networks [6], fuzzy clustering [7] etc. For each kind of clustering, the key is to define a specific metric to measure the similarity (or dissimilarity) among objects. So far various metrics have been adopted such as Euclidean distance, Manhattan distances, inner product, fuzzy membership function, etc. No matter what ki nd of measurement is used, in principle, there are basically two kinds: one for measuring the similarity between two objects set of data). It X  X  known that the possible regions partitioned by a clustering are limited. For example, K-means algorithm [5] can only partition the data into elliptical regions. So it X  X  hard to use these kinds of clustering to complex clustering. 
Kernel-based methods have wisely been used in machine learning [8][9]. So far they were used to supervised learning (classification) mainly [10]. In kernel-based separability of the input data by the non-linear transformation. For example, the SVM is one of the well-known supervised learni ng algorithms [10]. In the algorithm, by using the kernel function transformation the input data become linearly separable on the new space whereas the same data set is non-linearly separable on the original space. Therefore, the SVM algorithms have demonstrated more efficiently. Recently some researchers have explored a few kernel-based unsupervised learning algorithms, e.g., kernel-based K-means clustering [8][ 11]. So it X  X  needed to explore the basic principle underlain the algorithms such as whether the kernel function transformation can increase the separability of the input data in clustering and how to use the principle to construct new clustering methods. In this paper, we will discuss the problems. Definition 1: Given a space X, a set DX  X  and a criteria G, the clustering problem the criteria G. Set i C is called a cluster (class) of D and P is the clustering of D. Then we have the following propositions. Proposition 1: Given a metric space X. Assume that cluster C consists of n super-respectively. There exists a kernel function K and its corresponding map  X  . Space X is intersection of a super-sphere B and ( ) X  X  (see fig. 1,2). approximate. Construct a function 22 ( ) exp( ( ) / ), 1, 2,..., ii fx x x r i n = X  X  X  = . Let 12 ( ) ( ) ( ) ,..., ( ) n fx f x f x f x =+++ . spherical surface 1 () fx e  X  = can be approached by the boundary of set C that consists of n super-spheres. That is, C can represent set space Z. map. Then 22 exp( ( ) / ) ( ), ( ) i xy r x x  X  X   X  X  X  =&lt; &gt; . Letting the point () i  X  X   X  &lt;&gt;= is a super-plane in Z with  X  X   X  &lt;&gt; X  is a half space S of Z. Then ( ) C  X  falls into S. unit super-sphere and the half space of Z is a super-sphere neighborhood. Therefore, () C  X  falls into the intersection of the super-sphere neighborhood and ( ) X  X  . Proposition 2: X is a bounded Euclidean space and DX  X  . { } 12 , ,..., m PCC C = is a clustering of D. There exists a kernel fu nction K. X is mapped into a feature space Z intersection of ( ) X  X  and a super spherical surface with d as its radius in Z. cover { } (, ), ' i B Bx x C  X  = X  . According to the bounded covering theorem in compact spheres approximately. From proposition 1, we prove proposition 2. From the above propositions, we know that in clustering the separability of input data is also increased by using the kernel func tion transformation, since a simple sphere-like region can represent any cluster approximately in the new transformed space. Therefore, the principle can be used to improve the clustering algorithms. 
As we known, some simple clustering algorithms such as K-means [5] can only partition the data into elliptical regions. It X  X  difficult to use these kinds of algorithms to complex clustering problems. Kernel-based clustering algorithms can overcome the drawback, since a sphere-like region can represent any cluster in the transformed space in despite of the complex clustering problem. 
In neural networks, we presented a constructive learning algorithm based on the dimensional space. In the new space the data can be covered (partitioned) by a set of [12][13]. By using kernel-based clustering, the input data are transformed into a new high dimensional space. In the new space, we can always use a simple supper-sphere to describe a cluster approximately no matter how complex the original cluster is. In data mining, each cluster generally represents a rule among a data set, this means that we can use less and simpler rules to describe the same data by using kernel-based clustering. 
