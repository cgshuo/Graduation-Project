 Previous work on cluster-based document retrieval has used either static document clusters that are created offline, or query-specific (dynamic) document clusters that are created from top-retrieved documents. We present the potential merit of integrating these two types of clusters.
Utilizing information induced fr om clusters of similar doc-uments has long been suggested as a means to improving document-retrieval effectiveness [1].

Cluster-based document-retrieval models utilize mainly two types of clustering. The first is static (offline) cluster-ing performed prior to retrieval time [1, 3, 4]. Information induced from static clusters is used to rank the corpus in re-sponse to a query. Since the clustering is query-independent, the clusters that presumably pertain to a given query have to be identified at retrieval time. A potential merit of using static clustering, for example, is that relevant documents that exhibit low surface-level query-similarity can still be identified if they are associated with (belong to) clusters with documents that exhibit high query-similarity. Thus, vocabulary mismatch between a query and relevant docu-ments can be addressed.

Dynamic (query-specific) clusters are created from the documents most highly ranked by an initial search performed in response to a query [6, 4]. The clusters are often used to re-rank these initially highly-ranked documents so as to im-prove precision at the very top ranks [2]. In contrast to static clusters, dynamic clusters need to be computed for each query. Furthermore, they do not contain documents exhibiting very low surface-level query similarity. Neverthe-less, dynamic clusters are more  X  X uery-focused X  than static clusters by the virtue of the way they are created, and hence, are somewhat less likely to exhibit query drift [5].
Static and dynamic clusters were used separately in work on cluster-based document retrieval. We demonstrate the potential merit of integrating the two types of clusters.
Let q , d ,and D denote a query, a document, and a corpus, respectively.

Dynamic clusters have been used for the re-ranking task [6, 4, 2]. That is, re-ordering the documents in an initial list, D init ,thatwasretrievedinresponseto q by some search algorithm so as to improve precision at top ranks. Hence, we focus on this task, and present methods that rank only documents d in D init .

Our algorithms operate in the language modeling frame-work. We use p x ( y ) to denote the language-model-based similarity between texts x and y . 1 (A text can be a query, a document, or a cluster of documents; we describe the es-timate in Section 3.)
We assume that the corpus D is clustered prior to retrieval time into static clusters. We use S to denote the set of m s static clusters c that exhibit the highest query-similarity p ( q ). Hence, while static clusters are created in a query-independent fashion, we use those that could be considered as  X  X uery-related X . In addition, we cluster D init , the list to be re-ranked, into the set T of m t dynamic clusters.
The re-ranking methods that we present are based on the interpolation algorithm [3, 2]. The algorithm was shown to yield state-of-the-art performance when using static clusters to rank the entire corpus and when using dynamic clus-ters, the retrieval score of document d is:  X p d ( q )+(1  X  )
P ters serve as proxies for d for estimating query similarity. Re-ranking algorithms. A previously-proposed re-ranking algorithm [2], Interp ( T ), utilizes only query-specific clus-ters as proxies for d :  X p d ( q )+(1  X   X  ) other reference comparison that we consider is the Interp ( S ) method [3] that uses only static clusters as proxies for d :  X p d ( q )+(1  X   X  ) use both sets of clusters as proxies for d so as to leverage the merits of each (refer back to Section 1). The resul-tant algorithm, Interp(S  X  T) ,scores d by:  X p d ( q )+(1  X  )
P
Another approach that we propose here is based on uti-lizing inter-cluster similarities. Specifically, we let one type of clusters serve as a proxy for the other type for estimat-ing cluster-query and document-cluster similarities. For ex-ample, the Interp(T  X  S) algorithm uses dynamic clus-p x ( y ) can be thought of as a surrogate for the probability p ( y | x ) often used in work on language models for information retrieval; refer to [3] for more details. ters as proxies for d and static clusters as proxies for dy-namic clusters:  X p d ( q )+(1  X   X  ) Analogously, Interp(S  X  T) uses static clusters as prox-ies for d and dynamic clusters as proxies for static clusters:  X p d ( q )+(1  X   X  )
The TREC corpora specified in Table 1 were used for experiments. We applied tokenization, Porter stemming, and stopword removal (using the INQUERY list) via the Lemur toolkit (www.lemurproject.org), which was also used for language-model induction. Topic titles served as queries.
We use a previously-proposed language-model-based sim-ilarity estimate, which was shown to be effective for cluster-smoothed unigram language model (with smoothing param-eter  X  ) induced from text z . Then, p x ( y ) def = exp vergence. Unless otherwise specified, we set  X  = 1000.
Since the goal of re-ranking methods is to improve preci-sion at top ranks, we use the precision of the top 5 and 10 documents (p@5, p@10) for evaluation measures. Statistically-significant differences of performance are determined using the Wilcoxon two-tailed test at a 95% confidence level. Following previous work on re-ranking [2], we set the list D init upon which re-ranking is performed to the 50 doc-uments d in the corpus that yield the highest p d ( q ) (i.e., standard language-model-based retrieval), where  X  is set to optimize MAP; hence, the initial ranking of D init is of high quality. As a reference comparison we use an opti-mized baseline that ranks the corpus by p d ( q )with  X  set to optimize p@5. The value of  X  is chosen for each re-ranking method from { 0 , 0 . 1 ,..., 0 . 9 } to optimize p@5.
Each of the cluster-sets S and T is composed of m s = m t = 50 nearest-neighbors-based clusters of 10 documents that are created using the p x ( y ) similarity estimate [3, 2]. Studying the effect of the number of clusters, and the clus-ter size, on performance is left for future work. A cluster is represented by the concatenation of its constituent docu-ments. The order of concatenation has no effect since we use unigram language models that assume term independence.
We can see in Table 1 that both the algorithms that use either static ( S )ordynamic( T ) clusters, and our newly-proposed algorithms that integrate the two, are highly ef-fective in re-ranking. Indeed, the performance of the meth-ods transcends that of the initial ranking and the optimized baseline  X  often to a statistically significant degree  X  in most relevant comparisons (corpus  X  evaluation measure).
We can also see that our Interp( S  X  T )andInterp( S  X  T ) algorithms outperform Interp( T )andInterp( S )inmostrel-evant comparisons. (However, the differences are rarely sta-tistically significant.) Furthermore, Interp( S  X  T )istheonly algorithm among those in Table 1 that posts statistically-significant improvements over the initial ranking for all rel-evant comparisons. These findings attest to the merits of utilizing both static and dynamic clusters.
 Finally, we postulate that the performance of Interp( T  X  S ) is inferior to that of Interp( S  X  T )andInterp( S  X  T )be-cause Interp( T  X  S ) uses static query-independent clusters as proxies for dynamic query-specific clusters for estimating cluster-query similarities.
We demonstrated the potential merits of integrating static (offline created) and dynamic (query-specific) document clus-ters for ad hoc document retrieval. For future work we plan on devising additional integration methods.
 Acknowledgments We thank the reviewers for their com-ments, and Lillian Lee for discussions of ideas presented in this paper. The paper is based upon work supported in part by IBM X  X  and Google X  X  faculty research awards. Any opin-ions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily re-flect those of the sponsors.
