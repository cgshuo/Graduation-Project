 Crowdsourcing has emerged in recent years as a promising new avenue for leveraging today X  X  digitally-connected, diverse, distributed workforce. Generally speaking, crowdsourcing describes outsourcing of tasks to a large group of people instead of assigning such tasks to an in-house employee or contractor. Crowdsourcing platforms such as Amazon Mechanical Turk and CrowdFlower have gained partic ular attention as active online market places for reaching and tapping into this still largely under-utilized workforce. Crowdsourcing also offers intriguing new opportunities for accomplishing different kinds of tasks or achieving broader participation than previously possible, as well as completing standard tasks more accurately in less time and at lower cost. Unlocking the potential of crowdsourcing in practice, however, requires a tri-partite understanding of principles, platforms, and best practices. We will introduce the opportunities and challenges of crowdsourcing while discussing the three issues above. This will provide a basic foundation to begin crowdsourcing in the context of one X  X  own particular tasks. H.3.4 [ Information Storage and Retrieval ]: Systems and software  X  performance evaluation Measurements, performance, experimentation, human factors Crowdsourcing, human computation. To motivate and ground general di scussion of crowdsourcing, we will focus primarily upon applications to evaluating search accuracy (with other examples like blending automation with human computation for hybrid search). While search evaluation is an essential part of the development and maintenance of search engines and other information re trieval (IR) systems, current approaches for search evaluation face a variety of practical challenges. Many Web search engines reportedly use large editorial staffs to judge the rele vance of web pages for queries in an evaluation set. This is e xpensive and has obvious scalability issues. Academic researchers, without access to such editors, often rely instead on small groups of student volunteers. Because of the students X  limited time and availability, test sets are often smaller than desired, making it harder to detect statistically significant differences in performance by the experimental systems being tested. While beha vioral data, such as obtained automatically from search engine logs of user activity, is much cheaper than the editorial method, it requires access to a large stream of data, something not always available to a researcher testing an experimental system. These challenges provide an ideal setting for demonstrating both the potential and practical challenges for the crowdsourcing paradigm. The tutorial is designed for t hose with little to intermediate familiarity with crowdsourcing who want to learn about the capabilities and limitations of crowdsourcing techniques for IR. The tutorial will highlight opportunities and challenges of the crowdsourcing paradigm, emphasize design and execution of experiments, provide practical  X  X ow to X  knowledge on using existing platforms (Mechanical Turk and CrowdFlower), and present established best practices for achieving efficient, inexpensive, and accurate results with crowdsourcing. Recommended background includes basic familiarity with IR evaluation and experimentation.  X  Introduce crowdsourcing and human computation  X  Survey recent  X  X iller apps X  of crowdsourcing principles  X  Provide practical  X  X ow to X  guidance for effectively using  X  Summarize recent surveys of crowd worker demographics  X  Discuss a variety of incentive structures available to  X  Emphasize design and human-centric practices with  X  Describe Summarize key best practices for achieving  X  Review current opportunities, untapped potential, and open 
