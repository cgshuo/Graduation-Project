 1. Introduction
Many Natural Language Processing (NLP) systems perform with near-human precision for various tasks, such as part-of-ical semantic knowledge bases intended for inference purposes ( Harabagiu &amp; Moldovan, 1998 ).
A long-term and notoriously difficult research topic in linguistics, computational linguistics, two nouns. Previous research in computational linguistics has focused mostly on the semantic interpretation of noun X  X oun story ABOUT war.
 tween such cases based on various information sources such as context and world knowledge. (1) 015  X  X  X inding a h e 1 i piece h = e 1 i of h e 2 i cake h = e
In this paper we present a supervised knowledge-rich approach to identifying semantic relations between nominals. The results suggest the upper bound on what this representation can do.

The paper is organized as follows: Section 2 presents a summary of previous work on the automatic identification of imental setting. Section 6 shows the results obtained by our system on various datasets and presents some improvements, observations, and a comparison with other SemEval systems. Conclusions are offered in Section 7 . 2. Previous work ney, 2006; Vanderwende, 1994, inter alia), but mostly in limited syntactic contexts, such as noun compounds ( X  X  X  N X ) and lexico-syntactic patterns. For example, patterns like  X  X  X , such as N X  identify with high accuracy ple relations.
 Other researchers approached the task in the context of the taxonomic (conceptual) knowledge of the noun constituents.
For example, Finin (1980) and Vanderwende (1994) proposed to interpret noun compounds using rules involving the taxo-rules automatically relying on large hand-coded noun taxonomies. Rosario and Hearst (2001) and Rosario et al. (2002) fo-cused on the medical domain and explored the use of a large corpus and a large lexical hierarchy to interpret noun compounds.

Girju et al. (2005, 2006) and Moldovan and Badulescu (2005) propose an iterative semantic specialization procedure used in domain independent applications. Since in WordNet noun concepts are disambiguated (they are provided with syn-procedure on the WordNet noun hierarchies in order to generate the best semantic constraints for the semantic relation detection.

Besides these symbolic approaches to noun X  X oun interpretation, other researchers use corpus statistics to interpret noun compounds and collect information on the occurrence frequency of the noun constituents and use them in unsupervised pervised models applied to the noun compound interpretation task perform much better when the n -gram frequencies are method that computes the lexical similarity of unseen noun compounds with those found in training. the interpretation of noun X  X oun pairs. For example, Kim and Baldwin (2006) developed an automatic method for interpret-involving the constituent nouns and a set of seed verbs denoting the semantic relation (e.g., to own denotes similarity over the verb.

However, although both current symbolic and unsupervised approaches have brought considerable improvements to the tions between Nominals has introduced a benchmark dataset where the noun constituents can occur anywhere in the context of the NLP techniques needed for noun X  X oun semantic interpretation.

In the next section we present the task and describe the training and test data used in this evaluation. 3. Classification of Semantic Relations between Nominals
The SemEval 2007 task on semantic relations between nominals is to identify the underlying semantic relation between Whole relation is defined as follows ( Girju et al., 2007 ):
Definition 1. Part X  X hole ( X , Y ) is true for a sentence S that mentions entities X and Y iff: (b) according to common sense, the situation described in S entails that X is the part of Y .
Winston et al. (1987) performed psycholinguistic experiments to identify Part X  X hole instances based on the way in
For each relation, the instances were selected by applying wild-card search patterns on Google. The patterns were built manually, using the approach of Hearst (1992) and Nakov and Hearst (2006) . Examples of queries which potentially select
Part X  X hole instances are  X  X  is part of  X ,  X  X  has  X ,  X  X  contains  X .
Each SemEval-Task 4 organizer was responsible for a particular semantic relation for which they collected a corpus of tence, the nouns were identified and manually labeled with their corresponding WordNet 3.0 senses (given as sense keys).
The average inter-annotator agreement on relations (true/false) after the independent annotation step was 70.3%, and the average agreement on WordNet sense labels was 71.9%. In the process of arriving at a consensus between annotators, the definition of each relation was revised to cover explicitly cases where there had been disagreement. Table 1 shows all seven relations considered along with the positive/negative instance distribution and examples.
Moreover, each example was accompanied by the heuristic pattern (query) the relation organizer used to extract the sen-arguments were phrases rather than single words, while in testing there were 156 (14.21%). Our system used only the head noun of the entities identified by the SemEval Task 4 annotators.
 tiate automatically. Instances encoding these relations are called near-miss examples, as shown in (3). (2) 026  X  X  X e caught her h e 1 i arm h = e 1 i just above the h e (3)  X  X  X ot sure what brand of model it came from but the h e
The example in (3) is interpreted by inferring that the wings have been taken from a plane of which they used to be part 2007)). This goes way beyond sentential context into very complex inferences about our knowledge about the world. interpretation system decides whether the nouns are linked by the target semantic relation. Based on the information em-ployed, systems can be classified in four types of classes: (A) systems that use neither the given WordNet synsets nor the queries, (B) systems that use only WordNet senses, (C) systems that use only the queries, and (D) systems that use both WordNet senses and queries.
 Detailed information about the SemEval-Task 4 data and procedure can be found in Girju et al. (2007). and 6 . 4. Learning models
The system architecture is presented in Fig. 1 . Besides resources such as WordNet Fellbaum (1998) , NomLex-PLUS use of general tools such as a tokenizer, a part-of-speech tagger, and a semantic role labeler, ASSERT ( Pradhan, Ward, &amp; Martin, 2008 ).

In the next sections we present all these modules. Since SEMSCAT the 2008 ), and the features employed in the classification. In what follows, we refer to Semantic Scattering as our improved version SEMSCAT 2. 4.1. Semantic Scattering In this section we present the SEMSCAT 1 learning algorithm along with our recent improvements (which led to
The algorithm starts with the most general boundary corresponding to the entity noun hierarchy and then specializes it
Moldovan and Badulescu (2005) proved that Semantic Scattering performs consistently far better than other supervised We present below a detailed description of Semantic Scattering along with our improvements. We consider a set V  X  E [ R . The set E  X f E 1 ; E 2 g ranges over a set of entity types L
E we use R to denote the binary relation  X  E 1 ; E 2  X  or  X  E r 2 L R .
 s are happiness#2 and cycling#1 respectively. The set of values L set of relation labels L R is the set of seven SemEval relations.
 The task is defined as a classification problem with the prediction function F :  X  L Let T be the training set of examples or instances T  X  X  X  s examples  X  s E 1 s E 2  X  each accompanied by its corresponding semantic relation label r 2 L semantic relation to assign to a new, unseen example  X  s E L E L E ), one needs some kind of measure of the similarity (or the difference) between any two given members. 4.1.1. Input representation
The Semantic Scattering algorithm uses the WordNet IS  X  A hierarchy to classify new unseen noun X  X oun pairs.

The features used in this learning model are the WordNet semantic classes of the noun constituents. The semantic nyms. Thus, the training examples are represented as a 3-tuple called a data point : h s synsets (in the hypernymy chain of each noun entity synset) and r 2 L classified.
 the two concept nouns. For example, the noun X  X oun pair mouth  X  girl encoding a h mouth#2, girl#2, Part X  X hole i .

The main idea of the Semantic Scattering model is to find the best set of noun semantic classes that would separate the (a division in the WordNet noun hierarchy) that would best generalize over the training examples.
Definition 2. A semantic boundary is a set of synsets G k noun hierarchy, s E i 2 L E , one of the following is true: (a) s E i 2 G k ; (b) H  X  s E i  X \ G k  X   X  ; (c) 9 s E j 2 G k such that s E i 2 H  X  s E j  X  , where H  X  x  X  is the set of hypernyms of x 2 L E .
 be defined as on, above, or below the division. 4 In the next section we present an improved boundary detection algorithm, section, this improvement leads to new insights into the interpretation problem. 4.1.2. Boundary detection algorithm specialize it based on the training data until no more useful specializations can be made. Step 1. Create generalized boundary noun hierarchy. The training data examples (data points) are then mapped to the boundary.
Mapping a data point to a boundary consists of executing a breadth first search of the WordNet noun hierarchy from each s in the data point, traversing along hypernym relation edges until the nearest boundary member synset is found. Hence the mapping function is M : L E G ! L E , where M  X  s E i breadth-first search. Hence mapping a data point h s E 1 ; s the initial data point h sandwich#1, bag#1, Content X  X ontainer i maps to the boundary G dataset.
 Step 2. Calculate statistics tion C  X  s E 1 ; s E 2 ; r  X  X  N , where N is the number of data points h s probability function P is generated where
At this point the current boundary is pushed onto the boundary list G and the gathered statistics for this boundary are by specificity. Step 3. Identify the most ambiguous pair of entropy. In general, the entropy of two synsets h s E 1 And the weighted entropy is where N is the total number of times h s E 1 ; s E 2 i occurs in the mapped dataset, N with its most popular relation, and N min is the number of times h s
Weighting the entropy in this manner gives higher measurements to ambiguous noun pairs which occur a lot in the data-set and lower measurements to ambiguous noun pairs which occur infrequently in the dataset. We want to locate the most prevalent ambiguous pairs first because the more prevalent ambiguous pairs present the highest opportunity to subcatego-ary to have synsets which are as specific as possible.
 the boundary is below all our data points or none of the boundary elements are ambiguous anymore  X  the algorithm halts and the training phase is complete.
 Step 4. Specialize the boundary
The two noun synsets with the highest weighted entropy are the next candidates for boundary specialization. Boundary specialization is nothing more than replacing these noun synsets in the boundary with their hyponyms.
The first boundary G 1  X f entity # 1 g thus specializes to G tity#1 and physical _ entity#1 were the most ambiguous weighted pair  X  which is likely  X  they would be replaced by their hyponyms and so on.

Once the boundary has been specialized, the original dataset is mapped to the new boundary and the process iterates the algorithm will have generated the list of boundaries G , sorted by specificity, with their corresponding probability functions.
 Step 5. Predict the category When the system is asked to predict the category of two synsets h s boundary which has relevant statistics. Otherwise said, it maps them to the best boundary according to the corresponding count function C ; P r 2 L label is predicted by maximizing this boundary X  X  probability function:s
For example, if the system were asked to categorize h sandwich#1, bag#1 i , it might map the noun pair to h substance#7, container#1 i  X  assuming the relevant boundary had these members  X  and the system might predict, based on this bound-Container.

In the next section we run through a toy example showing the state of the system each step along the way in order to clarify the execution of SEMSCAT 2. 4.1.3. Algorithm walk-through example
In this example, we are dealing with the following toy dataset describing positive (1) and negative (0) examples of Con-tent X  X ontainer data points ( Table 2 ).
 rithm is omitted (although easily computed), though the result of the entropy calculation is displayed in column 4 which indicates the most ambiguous instance to be specialized next.
 training halts since there are no more ambiguous pairs (as column 2 shows).
 data point: h book#1, library#1,? i .
 Find best boundary (Step 4 of the algorithm) : Both synsets book#1 and library#1 are further down in the Wordnet process#1, substance#4, abstract entity#1, thing#8}.

Map test data point to boundary : Once the best boundary is found, the test data point is mapped onto it. Thus we map: h book#1, library#1,? i ? h physical object#1, physical object#1,? i .
 ing possible values: 0, and the system predicts the label 0.
 The learning model presented here is a marked improvement over the (2004) and Moldovan and Badulescu (2005) . In the next section we point out our contribution. 4.1.4. Improvements and observations
Our algorithm improves upon SEMSCAT 1 in a number of key ways which are presented in detail below. (1) One boundary vs. many boundaries
The most important difference between SEMSCAT 1 and SEMSCAT our algorithm keeps track of numerous boundaries, each more specialized than the previous. This has a few ramifications.
First, when predicting the category of two nouns which are both above the optimal boundary in Semantic Scattering, it is the nouns and results in classifying all noun pairs above the boundary to the same relation, whichever is most popular among the nouns higher up in the WordNet hierarchy. Attempting to map them down onto the boundary goes against sonable solution; this however leads to a performance decrease.
 Second, nouns which are much lower in the WordNet noun hierarchy than semantic categorical information when they are mapped to the boundary. The larger this gap is between the boundary and with a delicate balance between overspecializing its boundary, risking becoming too specialized to classify some unseen nouns, and underspecializing its boundary, risking becoming too general to be useful. The best end result one can hope for when using SEMSCAT1 is a boundary optimal in the average case.

Our improved algorithm instead keeps track of as many boundaries as the training data can provide. These boundaries span from the most general to the most specific possible given the training data. When new nouns are categorized, the boundary which categorizes them is chosen based on the input nouns X  locations in the hierarchy. By catering the boundary which is used to the nouns that need to be categorized, our algorithm minimizes the loss of information that occurs when below the best boundary, the reason is because the training dataset did not have enough data to create a more specific boundary, not because the algorithm chose to settle on a higher boundary. Hence the blame shifts from the algorithm to the training data, where it belongs. (2) Stability Another key difference between our improved algorithm and testing the boundary on the development set every iteration. When the performance begins to decrease, the training stops and the current boundary is considered to be optimal.

The problem with this approach is that since the development set is randomly selected every time the system is trained, selected by SEMSCAT 1 to be optimal tends to be unstable and thus, unreliable.

Our improved algorithm does not choose only one boundary, and thus overfitting the training data is not an issue. Be-cause of this, SEMSCAT 2 has no need to test the performance of each boundary along the way and thus does away with the development set. Given the same training data, it will produce the same set of trained boundaries every time. Our model in Section 6 . (3) Speed and space SEMSCAT 2 is a bit slower to train and test than SEMSCAT 1, and with proper book keeping and creative implementation, one can get around many of the efficiency issues and make training and testing just as tractable as it is with SEMSCAT 1.
 On the issue of time, while SEMSCAT 2 takes more time to train and test than they both train in polynomial time.
 training data points exist below the current boundary  X  the space requirements grow exponentially with a naive implemen-tation. However, there are ways to reduce the space burden. For example, in our implementation instead of storing each onyms because no hyponym of abstract_entity#1 will ever intersect the hypernym chain of a synset which is not an abstract_entity#1. 4.2. Support Vector Machines (SVM)
In general, Semantic Scattering captures successfully the conceptual meaning encoded by two nouns, but it disregards most of the context in which the nouns occur. Thus, for our SemEval system we used as the main learning model, Support
Vector Machines which works well with large feature sets derived from context. In particular, we worked with libSVM, an representing a nominal feature with n discrete values as n binary features.
We used the RBF kernel and built a binary classifier for each of the seven relations. In the next section we present the features employed in this model. 4.2.1. Feature space
Semantic relations between nominals can be encoded at different linguistic levels. Our previous experience with the task to all or a large number of relations and features specific to each relation.

Following this approach, we extend here over our previous work which focused mainly on noun compounds and other all or most relations, while the last set was manually determined from the SemEval definition provided for some of the relations.
 features, we list previous works where they proved useful. While features F1 X  X 4 were selected from our previous experi-ments, all the other features are entirely the contribution of this research.
 Feature set #1: core features into consideration only lexico-semantic information about the two target nouns.

Argument position (F1) indicates the position of the semantic arguments in the relation. This information is very valu-able, since some relations have a particular argument arrangement depending on the lexico-syntactic construction in which they occur. For example, most of the noun compounds encoding Stuff-Object (a subtype of Part X  X hole relation) list e 1 as the part and e 2 as the whole (e.g., h e 1 i silk h = e
However, there are also instances where the position is reversed. For example, in the instance the h e i car h = e 1 ih e 2 i door h = e 2 i ; e 1 is the whole and e (e.g., silk dress ).
 Semantic specialization (F2) is a binary feature representing the prediction of the sented in detail in Section 4.1 .

The nominalization features (F3, F4) indicate if the target noun, e tinguishherebetween agentialnouns , othernominalizations ,and neither .ThefeatureswereidentifiedbasedonWordNetandthe nominalizationdatabase,NomLex-PLUS 6 andwereintroducedtofiltersomeofthenegativeexamples.Forinstance,manyofthe nounshavingthesuffix X  X r(e.g., carowner )encodemostofthetimeaThemerelation(e.g., somebodywhoownsacar ).Moreover, other nominalization types proved to be an important feature in identifying Cause X  X ffect relations ( Girju, 2003 ). Spatio-temporal features (F5, F6) were also introduced for each target noun to identify some near-miss examples, such as word acting as a Theme should not indicate a period of time, as in h e fruits under the bowl (Location)  X  where the bowl ceases to be a container and becomes a covering. Feature set #2: context features
The features are detailed below.
The grammatical role features (F7, F8) determine if e 1 or e noun compounds and identifies some near-miss examples. For example, a restriction imposed by the definition of in case X or Y is an agential noun.

PP attachment (F9) is defined for NP PP constructions, where the prepositional phrase containing the noun e
NP in the sentence. For example, eat h e 1 i pizza h = e 1 after the sentence was syntactically parsed with Charniak X  X  parser.

Furthermore, we implemented and used two semantic role features which identify the semantic role of the phrase in a verb X  X rgument structure, phrase containing either e 1 (F10) or e filter out near-miss examples, especially for the Instrument X  X gency relation. For this, we used developed at the University of Colorado at Boulder. 8 two target nouns. For this we lemmatized the words in the input sentence with Porter X  X  stemmer. The sentence was also the weight, the more representative is the context sequence for that relation.
 Feature set #3: special features Table 6 and presented in detail below.
 factors. This list was augmented with a set of 10 preconditions such as foundation ( foundation%1:24:00:: and founda-tion%1:09:00:: ) and requirement%1:09:00:: since they would not be allowed as tools for the theme. argument identified as Instrument in the relation (e.g., e identified by the semantic role tool, ASSERT .

The syntactic attachment feature (F18) is a feature that indicates whether the argument identified as Instrument in the relation attaches to a verb or to a noun in the syntactically parsed sentence. 5. Experimental setting Since the size of the task training data per relation is small, we expanded them with new examples from various sources.
We added a new corpus of 3000 sentences of news articles from the TREC-9 QA (112), and Theme X  X ool (91). We also extracted 552 Product X  X roducer instances from eXtended WordNet their gloss definition). The Instrument X  X gency classifier was trained only on the task dataset. listed as containers in WordNet. Other restrictions involve prepositions attached to container nouns. Examples of such  X  where the bowl ceases to be a container and becomes a covering.

From the additional collections mentioned above we extracted both positive (examples representative for each relation oped and annotated for Cause X  X ffect with positive (Cause X  X ffect) and negative examples (no Cause X  X ffect). The Product X  was annotated based on a set of 35 semantic relations (for a multi-class semantic classification problem) from which we selected only those of interest to this task (as listed above).

Each instance in this text collection had the target nouns identified and annotated with WordNet senses. Since the anno-tations were done at different times and used different WordNet versions, senses were mapped to sense keys following the
SemEval Task 4 format. 6. Experimental results were employed in a Support Vector Machines model. Since for SemEval we used the prediction of also show the performance of our system when SEMSCAT 2 is used. In the second set of experiments (Section 6.2 ) we compare the performance of semantic relations are better suited for WordNet-based models than others, and that contextual data are important in the and that our learned WordNet-based representation is highly accurate so the performance results suggest the upper bound on what this representation can do. 6.1. The SemEval-2007 UIUC system
Table 7 shows the performance of our UIUC system for each semantic relation. These baselines determine the class of an line F-measure score (all true) and it always guesses  X  X  X rue X . Thus, Base-Acc maximizes accuracy, while Base-F maximizes recall.

The Average score of Precision, Recall, F-measure, and accuracy is macroaveraged over all seven relations. The top three for Cause X  X ffect Instrument X  X gency, Origin X  X ntity, and Content X  X ontainer (cf. Pearson X  X  Chi-square test: p -alue 6 0 : 01 ; a  X  0 : 05).

Table 8 shows the contribution of each feature to the overall performance (F-measure) for each semantic relation. The numbers show the difference in performance when a particular feature is removed. On average (column Avg. ), the results classes work much better for some semantic relations.

The Grammatical relation (F7, F8) and Psychological feature (F15, F16) were particularly important for Theme X  X ool, while ture was not helpful in most of the other cases due to parser errors.

Table 9 shows the UIUC system performance when trained only on the training dataset provided by the SemEval-Task 4 for which we did not provide external data and Content X  X ontainer for which we had only the external dictionary of con-tainer words (no additional contextual data). Another relation which was somewhat affected is Theme X  X ool which relies heavily on the noun features derived from WordNet.
 The difference between the two SEMSCAT models is very statistically significant in particular for Instrument X  X gency and Part X  X hole  X  p -v alue 6 0 : 005 ; a  X  0 : 05  X  .
 We also notice an increase in performance of 2.4% F-measure and 5.5% accuracy of the SemEval dataset ( Table 9 ).
 Table 10 shows the UIUC system performance when trained on all the data but with the prediction made by expected, we notice an increase in performance compared with the performance of the system when using
On average, the system increased by 5.5% (F-measure) and by 3.3% (Accuracy). 6.2. Improvements and observations
Most state-of-the-art semantic relation identification systems, including ours and other SemEval B-type systems, employ than the information derived from the context in which the nouns occur. Although our experimental results presented in Section 6.1 also support this claim, we performed further experiments comparing the performance of and that contextual data are important in the performance of a noun X  X oun semantic parser.
We present here four experiments. The first two experiments were performed only on the SemEval dataset and focus on the behavior of our improved model, SEMSCAT 2. Specifically, we show how does this model compare against top-ranked systems in the 2007 SemEval Task 4 competition. Furthermore, we distinguish between two types of data in-the learning curves for each relation. Finally, the last experiment shows the performance of that used in SemEval-2007. 6.2.1. Experiment I: SEMSCAT 1 vs. SEMSCAT 2 tested on the corresponding SemEval test set. SEMSCAT 2 outperforms  X  p -v alue  X  0 : 025 ; a  X  0 : 05  X  . For the other relations the difference is not significant. for training and because the UIUC system used in addition the other 17 features described in Section 4 . 6.2.2. Experiment II: regular vs. context-sensitive examples gether and 10-fold cross-validation is performed, yielding a prediction for each example.
Table 12 compares the performance of SEMSCAT 1 13 and SEMSCAT tion. The results show that SEMSCAT 2 has a marked improvement over tity. Moreover, the difference between SEMSCAT 2 and  X  p -performs SEMSCAT 1 by 6%.

Moreover, for each relation, we split the lumped training and test examples into regular and context-sensitive . Regular examples are those where the relation between the two given nouns can be determined out of context. Context-sensitive regular and (5) context-sensitive. (4) 177  X  X  X rude h e 1 i soybean h = e 1 ih e 2 i oil h = e (5) 153  X  X  X ot sure what brand of model it came from but the h e Each relation contains between 26 and 60 context-sensitive examples. Table 13 compares the performance in accuracy of the performance on positive and negative examples within each group. within each group is much lower than the one for the other relations, which explains the performance.
Separating regular examples also allows us to see which relations are best captured with the WordNet hierarchy. In par-
Part X  X hole, and Content X  X ontainer, while the poorest is Product X  X roducer. The difference between the two models is very statistically significant for Instrument X  X gency and Part X  X hole ( p -threshold a  X  0 : 05), and statistically significant for Theme X  X ool  X  p -6.3. Experiment III: learning curves
In this experiment, we determine the learning curve of each relation. Because the SemEval data contain only 140 exam-sented in Section 5 . Since the number of examples varies considerably from one relation to another, we group the relations into classes: Class I (approx. 135 examples per relation Theme X  X ool} and class II (approx. 1000 examples) has {Origin X  X ntity, Cause X  X ffect, Product X  X roducer, Part X  X hole}. Figs. 2 and 3 show the learning curves for each class. Each figure displays the results of both models are tested on SemEval test data and trained on all other data available.
 There are several observations to be made. First, at each level of training, margin, or performs similarly to it. Second, we note the stability of our improved Semantic Scattering model tends to increase or fluctuate around a relatively stable value when trained on achieve maximum performance. 6.4. Experiment IV: comparison with other corpora
The initial inter-annotator agreement on the SemEval data was relatively low, and most datasets developed in the com-noun pair and a given relation.
 with 22 semantic relations, four of which were used in the SemEval data. These are Cause X  X ffect, Product X  X roducer, Part X  Whole, and Origin X  X ntity.

The performance of SEMSCAT2 was performed on all 22 semantic relations on this collection. However, Table 14 shows only comparable with those obtained on the SemEval dataset: SEMSCAT2 6.5. Comparison with other systems
In this section we compare our system X  X  performance with that of the participating systems at SemEval 2007 Task 4. Table 15 shows the overall results. The UIUC tool ranked first among B-type systems and overall. Moreover, Table 16 shows that SEMSCAT2 places 5th with respect to the top-ranked systems in the SemEval competition.
This is despite the fact that it does not make use of sentence context, making a prediction using the information about the noun X  X oun pair only. Moreover, SEMSCAT2 outperforms the best system in category A ( ranks very close to the best systems in categories C and D ( ther WordNet nor the queries, and category C used only the queries, while category D used both WordNet and the queries. Table 16 also shows that the performance of the UIUC system when using while Base-Acc the baseline accuracy score (majority). When compared with Base-Acc, only the results obtained by UIUC (Cause X  X ffect and Content X  X ontainer) and FBK-IRST (Instrument X  X gency) are very statistically significant  X  p -nificant  X  p -v alue  X  0 : 025  X  .

From the 15 systems participating in SemEval Task 4, 3 employed only features which took into consideration the two Net-based and contextual features: CMU X  X T ( Tribble &amp; Fahlman, 2007 ), ( Butnariu &amp; Veale, 2007 ), UCD X  X N ( Nulty, 2007 ), and Scattering.
 Besides UIUC  X  X  system, other participants made use of external data for training: ated from applying pattern queries on the web ( UTH, UCB ), web counts ( 6.5.1. Comparison with type-D systems
Since type-D systems used both WordNet and the query information provided to the participants, we extended our fea-ture set with the queries provided for each example. Thus, we can measure the contribution of the queries to the overall performance and better compare our results with those obtained by type-D systems.

Thus, we created a new feature F19 (Query) whose values are the individual words part of the query string of each query tured by features F2 (semantic specialization) and F12, F13, F14 (inter-noun context sequence). 7. Discussion and conclusions
This research describes a state-of-the-art system that employs a supervised, knowledge-intensive approach to the auto-tem ranked first in the SemEval 2007 Task 4 evaluation series.
 The contributions of the paper are as follows: First, we have presented the system architecture and the learning models. open-domain noun X  X oun semantic parsing. Moreover, the system X  X  key module, model introduced in Moldovan et al. (2004) and Moldovan and Badulescu (2005) both in terms of performance accuracy and system stability. Moreover, our improved SEMSCAT module places 5th with respect to the top-ranked systems in the SemEval
Task 4 competition. We believe this is an important result, given that the model is based only on WordNet noun semantic sults suggest the upper bound on what this representation can do.
 sumed the argument identification solved. Another interesting experiment would be to test the performance of Semantic Scattering on EuroWordNet. However, we leave these tasks for future research.

Ultimately, we hope that this investigation will entice researchers toward new topics of semantic relation research. An-clusters of near-miss semantic relations.
 Acknowledgment This research has been supported in part by the University of Illinois at Urbana-Champaign. References
