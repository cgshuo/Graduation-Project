 1. Introduction
Text sentiment classification refers to the task of determining the sentiment polarity (e.g. positive or negative) of a given text document ( Liu, 2012 ). Recently, sentiment classification has received considerable attention in the natural language processing research community due to its many useful applications such as online product review classification ( Kang, Yoo, &amp; Han, 2012 ) and opinion summarization ( Ku, Liang, &amp; Chen, 2006 ).

Up until now, different methods have been used for sentiment classification. These methods can be categorised into two groups, namely; unsupervised and supervised. The unsupervised methods classify text documents based on the polarity of words and phrases contained in the text. If a text document contains more positive than negative terms, for example, it is  X  always used to determine the sentiment polarity of each term. In contrast, supervised methods train a sentiment classifier based on labelled data using some machine learning classification algorithms ( Pang, Lee, &amp; Vaithyanathan, 2002; Ye, Zhang, sentiment classifier.

Based on these two groups of methods, sentiment lexicons and annotated sentiment data can be seen as the most impor-tant resources for sentiment classification. However, since most recent research studies in sentiment classification have been presented in the English language, there are not enough labelled corpus and sentiment lexicons in other languages and time-consuming task. Therefore, the challenge is how to utilize labelled sentiment resources in one language (a resource-rich language such as English is always called the source language) for sentiment classification in another language (a resource-scarce language is called the target language). This subsequently leads to an interesting area of research called cross-lingual sentiment classification (CLSC). Most existing research works employ automatic machine translation engines to directly project information of labelled data from the source language into the target language ( Balahur &amp; Turchi, 2014; applied to the original test data for the classification task in the target language. Machine translation can be employed in the opposite direction by translating test documents from the target language into the source language ( Mart X n-Valdivia, is trained based on the original labelled data in the source language and then applied to the translated test data. However, the use of only translated data in the sentiment classification task results in two main problems. The first problem is the difference in term distribution between the original and the translated text documents due to the dissimilarity in cultures, writing styles and also linguistic expressions in the various languages. This subsequently leads to the creation of different feature distributions in the training and test data. The second problem relates to machine translation errors in the resource translation process. However, since machine translation quality is still far from satisfactory, there are some translation errors which occur in the resource projection process. To overcome the first problem, making use of unlabelled data from the target language can be helpful, because this type of data is always easy to obtain and has the same term distribution as the test data. Therefore, employing unlabelled data from the target language in the learning process is expected to result in a better classification in CLSC.

Active learning (AL) ( Wang, Kwong, &amp; Chen, 2012 ) and semi-supervised learning (SSL) ( Ortigosa-Hern X ndez et al., 2012 ) are two well-known techniques that make use of unlabelled data to improve classification performance. Both techniques are iterative processes. AL aims to reduce manual labelling efforts by finding the most informative examples for human labelling, while SSL tries to automatically label examples from unlabelled data in each cycle.

To reduce the effect of machine translation errors in the classification process, both directions of machine translation can be simultaneously employed. Therefore, we have training and test documents in both languages (original version of training documents and translated version of test documents in the source language and translated version of training documents and original version of test documents in the target language). If the translated version of a document has some translation error on one side, the original version of that document is used on the other side.

Given the two possible directions for data translation, we can consider sentiment data from two different views, namely; source language view and target language view. In this paper, we consider source language features and target language fea-tures as being two sufficient feature representations of labelled and unlabelled data. Accordingly, we propose a new model based on a combination of bi-view Active learning, Co-testing ( Muslea, Minton, &amp; Knoblock, 2006 ), and semi-supervised co-testing is a bi-view active learning process that aims to find the most informative unlabelled examples by considering the disagreement between two classifiers trained in each view. The intuitive theory behind co-testing is that if the classifiers trained in each view classify an unlabelled example differently, at least one classifier makes a mistake on its prediction.
Therefore, this unlabelled example can provide useful information for the classifier with an incorrect prediction. On the other hand, Co-training tries to select the most confidently-classified examples from unlabelled data in each view so as to add to the training data in the other view. These two techniques complement each other in order to reduce human labelling efforts.
From experimental results in co-testing, it can be seen that some of the selected contention examples cannot provide much help to the learner. The main reason for this issue is that some of these selected examples are outliers and therefore are not representatives. To avoid outlier selection in co-testing, we considered the density of selected examples in our pro-posed method and chose those contention examples that have maximum average density in the pool of unlabelled data.
The contributions of our work are as follows: (1) parallel combining of two bi-view approaches, co-training and co-testing, in order to incorporate unlabelled examples from the target language in the learning process of cross-lingual sentiment classification. This is achieved by selecting the most confident automatically-labelled examples, as well as a few of the most informative manually-labelled examples from unlabelled data. Specifically, the contribution degree was defined as the criteria of select informative instances. These select the contention examples which have a different predicted label between the source and target views of unlabelled documents. (2) When selecting the most informative unlabelled examples by co-testing, we propose to use density information of unlabelled examples in order to select not only the most informative examples, but also the most representative examples for manual labelling. Specifically, we select the contention examples which have a different predicted label in two views and have maximum average similarity with the other unla-belled examples in the unlabelled pool.

The proposed method was applied to book review datasets in three different languages and experiments showed that our method can effectively reduce the manual labelling effort while, at the same time, increasing the performance of cross-lin-gual sentiment classification in comparison with some baseline methods.

The reminder of this paper is organized as follows: The next section presents related works on sentiment classification and CLSC. Section 3 describes bi-view data creation. The proposed model is described in Section 4 , while an evaluation is given in Section 5 . Finally, Section 6 concludes this paper and outlines ideas for future research. 2. Related works 2.1. Sentiment classification
Sentiment classification can be performed on words, sentences or entire documents. Sentiment classification methods are categorised into two groups, namely; unsupervised and supervised. In unsupervised methods, the semantic orientation of phrases are calculated at the first stage and then the sentiment orientation of a document is predicted based on average semantic orientation of all phrases contained in it ( Harb et al., 2008; Turney, 2002 ). A research study by Turney (2002) pre-sented a simple unsupervised learning algorithm for classifying a review as being either recommended or not recommended.
He determined whether words are positive or negative, as well as the strength of the evaluation by computing the words X  point wise mutual information (PMI) for their co-occurrence with a positive seed word ( X  X  X xcellent X  X ) and a negative seed word ( X  X  X oor X  X ). He called this value the word X  X  semantic orientation. This method performed a scan through a review looking for phrases that match certain parts of speech patterns (adjectives and adverbs). It then computed the semantic orientation of those phrases and finally added up the semantic orientation of all of those phrases in order to compute the orientation of a review document. In contrast, supervised methods ( Pang et al., 2002; Ye et al., 2009 ) treat the sentiment classification as a conventional classification task and try to build a classifier based on a set of labelled documents (or sentences). In their study, Pang et al., 2002 compared the performance of three traditional supervised classification approaches (SVM, Na X ve Bayes and Maximum Entropy) and showed that these techniques outperformed human-generated baselines. A study by
Ye et al. (2009) incorporated the sentiment classification techniques into the domain of destination reviews. They used three supervised learning algorithms consisting of support vector machine, NB and the character based N-gram model in order to classify destination reviews. In addition, they used the frequency of words to represent a document instead of word pres-ence. They found that SVM outperforms the other two classifiers. 2.2. Cross-lingual sentiment classification
Cross-lingual sentiment classification has been extensively studied in recent years. These research studies are based on the use of annotated data in the source language (always English) to compensate for the lack of labelled data in the target language. Most approaches focus on resource adaptation from one language to another with few sentiment resources. For example, Mihalcea, Banea, and Wiebe (2007) generate subjectivity analysis resources into a new language from English sen-automatic machine translation engines were used to translate the English resources for subjectivity analysis. In a further study ( Banea et al., 2008 ), the authors showed that automatic machine translation is a viable alternative for the construction of resources for subjectivity analysis in a new language. In two different experiments, they first translated training data of subjectivity classification from the source language into the target language. They then utilized this translated data to train a experiment, machine translation was used to translate test data from the target language into the source language and a classifier was then trained based on training data in the source language. After the training phase, the translated test data was presented to the classifier for sentiment polarity prediction. Wan (2008) used unsupervised sentiment polarity classi-fication in Chinese product reviews. He translated Chinese reviews into English using a variety of machine translation engines and then performed sentiment analysis for both Chinese and English reviews using a lexicon-based technique.
Finally, he used ensemble methods by which to combine the analysis results. This method requires a sentiment lexicon in the target language and cannot be applied to other languages with no lexicon resource. Pan, Xue, Yu, and Wang (2011) designed a bi-view non-negative matrix tri-factorization (BNMTF) model to solve the problem of cross-lingual sentiment classification. They used the machine translation to achieve two representations of training and test data; one in the source language and the other in the target language. This model was then used to combine the information from two views.
Another approach is that of feature translation, which involves translating the features extracted from labelled documents into different languages. Subsequently, based on those translated features, a new model is trained for each language. This approach only needs a bilingual dictionary to translate the selected features. It can, however, suffer from the inaccuracies of dictionary translation, in that words may have different meanings in different contexts. Therefore, selecting the features to be translated can be an intricate process. In another work, Wan (2009, 2011) used the co-training algorithm to overcome the problem of CLSC. He first investigated basic methods for CLSC by using machine translation services. He then exploited a bilingual co-training approach by which to leverage annotated English resources to sentiment classification in Chinese reviews. In this work, firstly, machine translation services were used to translate English labelled documents (training doc-uments) into Chinese and similarly, Chinese unlabelled documents into English. The author used two different views (English and Chinese) in order to exploit the co-training approach into the classification problem. The co-training process usually selects high confidence examples to add to the training data. However, if the initial classifiers in each view are not good enough, there will be an increased probability of adding examples having incorrect labels to the training set. Therefore, the addition of noisy examples not only cannot increase the accuracy of the learning model, but will also gradually decrease the performance of each classifier.

In contrast to the results from this study ( Wan, 2009, 2011 ), which only utilized co-training to enrich a training set, we propose to combine active learning and co-training in order to enrich the initial training set. This will be achieved through selecting and adding incrementally as few manually-labelled examples as possible, along with some automatically-labelled examples from the unlabelled pool. To select unlabelled examples for manual labelling, we exploit the density information of unlabelled examples in order to select not only the most informative examples, but also the most representative ones. 3. Bi-view data creation
Document translation can be performed to project textual information from one language into another. In CLSC, two dif-ferent directions are possible for data projection; one from the source language into the target language and another from the target language into the source language. Given these two possible directions for document translation, we can look at sentiment data from two different views, namely; source view and target view. The source view consists of original labelled documents and translated versions of unlabelled documents. On the other hand, translated labelled documents and original unlabelled documents represent the target view. Considering these two views, we have labelled and unlabelled data occur-ring simultaneously in a bi-view framework which we refer to as  X  X  X i-view data X  X . Fig. 1 diagrammatically shows this bi-view data creation process. 4. The proposed model
As mentioned in the first section, because term distribution in the original and translated versions of text documents is different, the performance effectiveness of cross-lingual sentiment classifier is limited. To increase this performance, making use of unlabelled data from the target language can be helpful since these data have the same term distribution as test documents in the respective target language. However, manually labelling unlabelled data is a very difficult and time-consuming task.

In an attempt to reduce the labelling effort, we propose a new model based on bi-view data. This model attempts to enrich initial training data in cross-lingual sentiment classification through manual (AL) and automatic (Co-training) labelling of some unlabelled data from the target language. The framework of the proposed model is illustrated in Fig. 2 .

In the learning phase, after creation of bi-view data, two classifiers are trained based on the initial training data in each of the views and then applied to the unlabelled data in the corresponding view. From the newly classified unlabelled data, AL then selects a few of the most informative and representative examples based on the view disagreement and density analysis for human labelling. Simultaneously, co-training selects some of the more confident classified examples with corresponding predicted labels. These two groups of selected examples are added to the training data for the next learning cycle. At the next cycle, the model is retrained based on the augmented training data and this process is repeated until a termination condition responding views for the classification task. The final prediction for a given test example is computed based on the average of the prediction values of the two classifiers. AL and Co-training are described in detail in the following sections. 4.1. Active learning process
Active learning is a subcategory of machine learning ( Cohn, Atlas, &amp; Ladner, 1994 ). The main goal of active learning is to learn a classifier by labelling as few examples as possible when actively selecting the examples to be labelled in the learning 2012; Zhu, Wang, Yao, &amp; Tsou, 2008 ), active learning is a promising method by which to speed up data labelling while
Initially, the classifier C is trained based on labelled examples set L and applied to the unlabelled pool U . Following this, a set of the most informative examples is then selected from the unlabelled pool U using a query function Q and a true class label is assigned to each of them by an oracle O . After that, these new labelled examples are augmented to L and the classifier
C is retrained based on this upgraded training set. This sequential label requesting process continues for some predefined iteration or until a termination condition is satisfied. The algorithm is shown as follows: Algorithm 1: Active learning process Given
The query function is essential during the active learning process. The major difference between several previously-proposed active learning methods is due to the disparity in their query function. The simplest query function is that of uncertainty sampling ( Lewis &amp; Gale, 1994 ), in which unlabelled examples with maximum uncertainty are selected for manual labelling in each learning cycle. Maximum uncertainty means that the learner has less confidence in its classifi-cation of these unlabelled examples. Another sample selection strategy is committee-based sampling, where the active learner selects those unlabelled examples which have the largest disagreement among several committee classifiers.
Besides the query by committee (QBC) ( Freund, Seung, Shamir, &amp; Tishby, 1997 ) as the first of such type, co-testing exam-ines a committee of member classifiers from different viewpoints and selects those contention examples (i.e., unlabelled examples on which the views predict different labels) for manual labelling ( Muslea et al., 2006 ). The reasoning behind co-testing is that if the classifiers trained in each view classify an unlabelled example differently, at least one classifier makes a mistake in its classification. Therefore, this unlabelled example can provide useful information for the classifier with incorrect prediction. In our proposed model, we use the co-testing approach because it can be applied in the bi-view framework of our problem. 4.1.1. Density analysis by the query function cannot help the learner since they are outliers. This means that a good selected example for manual labelling in active learning should not only be the most informative example, but also the most representative one. There-fore, adding outlier examples to the training data cannot provide much help to the learner. To solve this problem, Tang et al. (2002) proposed a sampling method in which the most uncertain example is selected by a learner from each cluster and then weighted using a density measure. In another work Jingbo et al. (2010) proposed a density-based re-ranking technique to select the most informative and representative example from unlabelled data to solve the outlier problem in selective sam-pling. They introduced a density concept by which to determine whether an unlabelled example is an outlier or not. To deter-mine the density degree of an unlabelled example, they used a novel method called k -nearest neighbour based density ( k NN density). In this measure, the density degree of an unlabelled example is computed by evaluation of average similarity between this example and k most similar unlabelled examples in the unlabelled pool. Suppose S ( x ) = { s of k most similar unlabelled examples to the x . Therefore average similarity for x ( AS ( x )) can be computed based on the fol-lowing formula:
This density measure was then used in combination with an uncertainty measure to select the most representative and informative example from among the unlabelled pool for human labelling. Although the outlier problem has been considered with uncertainty sampling in previous studies, this problem may occur in query by committee (QBC) sampling, especially in co-testing and sentiment analysis. Therefore we employ the density degree introduced in a previous study ( Jingbo et al., 2010 ) to avoid selecting outlier examples in each cycle of co-testing. As mentioned in Section 3 , each unlabelled example has two different representations based on the features of both source and target languages. Therefore, the density of each unlabelled example can be computed in two different views, the source view and the target view respectively. Average similarity of an unlabelled example in the source (or the target) view shows the density value of that example based on the source (or the target) language features. Final density value of an unlabelled example is computed by averaging these two different density values.

We refer to this new active learning model as density-based active learning. In this model, after determining the contention examples, the algorithm chooses those contention examples that have maximum density value as being the most informative and representative unlabelled examples. They were then labelled by an oracle and added to training data.
We use cosine measure as the similarity function with which to compute the pair-wise similarity value between two examples. 4.2. The co-training algorithm
The co-training algorithm ( Blum &amp; Mitchell, 1998 ) is one of the most successful semi-supervised methods of handling unlabelled data. The standard co-training algorithm assumes that two different and sufficient sets of features or views exist that adequately describe the data. Two separate classifiers are trained over each of the views by incorporating an initial set of labelled data. The most confident classified examples are then recovered from unlabelled data as the new labelled data for each other. These newly-recovered examples are combined with previous labelled data to create a new training set for the next round. This iterative process continues for some predefined iteration. The final prediction for a given example is the combination of predictions of two classifiers. The pseudo-code of co-training algorithm for binary classification has been shown in Algorithm 2 as follows: Algorithm 2: Co-training algorithm Given:
Initial parameters: p: number of most confident positive examples selected by each view in each cycle
Two separate views, namely V 1 and V 2 are used during the learning process and the classifiers ( h retrained over each view for a predefined number of iterations. In each cycle, the classifiers select p positive and n negative most confident newly-labelled examples from the unlabelled pool to add them to the set of labelled training set ( L ). After that, the two classifiers are retrained based on this augmented set of labelled examples for the next cycle. 4.3. Density based Active Co-Training (DACT) model
After the creation of bi-view data, we then have training, test and unlabelled documents in two separate views. These two views are used in the co-training and active learning components of the proposed model in parallel. The co-training algo-rithm can be employed to label the most confident documents in each view and generate new training examples for another view. In each view, a classifier is trained based on training data and applied to the unlabelled examples directly. Since text documents with the same sentiment polarity in two languages may share some common terms and characteristics, these classifiers may be able to correctly predict the sentiment polarity of some unlabelled documents by using the classification information transferred from another language. These most confident classified examples are selected and added to the training set with corresponding predicted labels for the next step (automatic labelling). However, in some other documents, classifiers may be unable to distinguish the correct sentiment polarity, since the information for predicting the polarity of these documents cannot be detected. These misclassified documents are generally very informative toward the learning pro-cess. We exploit a bi-view active learning approach, Co-testing, in conjunction with co-training in order to find and manually label the most informative examples from unlabelled data. The pseudo-code of the proposed method has been shown in
Algorithm 3 as follows: Algorithm 3: Proposed algorithm Given:
Initial parameters: p: number of most confident positive examples selected by each view in each cycle
This algorithm starts with a set of labelled examples L from the source language as the initial training set and a set of unlabelled examples U from the target language. Each example (labelled and unlabelled) has two different views, specifi-cally: V 1 (source view) and V 2 (target view). Two separate classifiers, namely, h views of the training set. It means that classifier h 1 is trained based on the training data represented by the source language feature set ( V 1 view) while h 2 is trained based on the training data represented by the target language feature set ( V each newly classified example are separately computed in each classifier based on the distance of each example from the current decision boundary. The further each example lies from the decision boundary, the more confident it is in predicting the label of that example. Further, in each of the views, namely; p positive and n negative, the most confident examples are selected as auto-labelled examples to be added to the training data, along with the corresponding predicted label for the next iteration. Furthermore, a set of contention examples is created by selecting those unlabelled examples that have different predicted labels in the source and the target views respectively. The contention examples set ( C
These examples are considered as informative examples for active learning because they can provide useful information for at least one of the two classifiers that have incorrect predictions. The t examples that have maximum average similarity with the other examples in the unlabelled pool are selected from contention examples set for manual labelling. These two groups of newly labelled examples (both automatically labelled and manually labelled) are then added to the training data and removed from the unlabelled pool. The two classifiers are retrained based on the updated training set. This process is repeated for a predefined number of iterations. 5. Evaluation
In this section, we evaluate our proposed model via cross-lingual sentiment classification on three different languages in the book review domain and compare it with some baseline methods. The sensitivity of our proposed method to the number of nearest neighbour parameter ( k ) in density measure is also considered. 5.1. Datasets
We have selected book review documents from two different cross-lingual sentiment datasets. The first dataset was used by Prettenhofer and Stein (2011) , and contains Amazon product reviews for three different domains consisting of books, DVDs and music. These are in four different languages, specifically: English, French, German and Japanese. Each review document is labelled as being either positive or negative based on its sentiment polarity. We only selected book reviews from this dataset and in only three languages, namely: English, French and Japanese. The book review dataset in the English language contains 2000 (1000 positive and 1000 negative) documents considered as labelled examples. A total of 4000 review documents (2000 positive and 2000 negative) were selected from the French and Japanese datasets and considered as unlabelled examples. Another dataset used in this paper is the pan reviews dataset ( Pan et al., 2011 ). This collection con-sists of three review datasets in different domains (Movie, Book and Music) in both English and Chinese. We selected only book reviews in Chinese from this collection. This dataset also contains 4000 book review documents (2000 positive and 2000 negative) and is considered as unlabelled data in the Chinese language. By combining English review documents with documents in other languages, three different evaluation datasets for cross-lingual sentiment classification were subse-quently formed. We called them, namely: En X  X r (English X  X rench), En X  X h (English X  X hinese) and En X  X p (English X  X apanese) respectively. The English language is used as the source language and other languages are considered as the target languages.
In all datasets, all reviews in the source language are translated into target languages and similarly, all reviews in target languages are translated into the source language using the Google Translate engine ( http://translate.google.com/ ). Table 1 shows the properties of these three evaluation datasets.

In the pre-processing step, all English reviews are converted into lowercase. Special symbols and other unnecessary characters are eliminated from each review document. In the Japanese text document, we applied MeCab software to segment the reviews; while Chinese documents were segmented by the Stanford Chinese word segmenter. the feature extraction step, unigram and bi-gram patterns were extracted as sentimental patterns. To reduce computational complexity, especially in density estimation, we performed feature selection using the information gain (IG) technique. We selected 5000 high score unigrams and bi-grams as final features. Each document is represented by a feature vector, each entry of which contains a feature weight. We used term presence as feature weights since this method has been confirmed as the most effective feature-weighting method in sentiment classification ( Pang et al., 2002; Xia, Zong, &amp; Li, 2011 ). 5.2. Based lines methods
The following baseline methods are implemented in order to compare the effectiveness of the proposed model using the same number of manually-labelled examples.  X  Active Co-Training model (ACT): This model is similar to DACT but without consideration for the density measure of con-tention examples. In this model, after obtaining a contention example set based on classification results of two different views, t unlabelled examples are randomly selected from the contention examples set. This baseline model is used to evaluate the effectiveness of density analysis in the DACT model.  X  Uncertainty sampling active learning model (US-AL): In this model we used well-known uncertainty sampling as a query function in active learning. In this query function, examples that are closest to the decision boundary are considered as uncertain examples and chosen for manual labelling. The average of the uncertainty values in the source and target view is used as the final uncertainty degree of each unlabelled example. Following this, t unlabelled examples with maximum uncertainty are then selected for manual labelling in each learning cycle.  X  Co-testing: This model is based on the simple co-testing approach. In this model, t unlabelled examples from the contention examples set are selected for manual labelling in each learning cycle. This baseline model is then used to determine the effect of automatic labelling of unlabelled examples on the performance of the proposed model.  X  Co-training: This model is based on the method used by ( Wan, 2011 ). In order to compare this model with other active learning-based models, we randomly selected some manually-labelled examples from the unlabelled pool and added them to the initial training set before starting the learning process. The number of manually-labelled examples added to the initial training set is equal to the total number of examples selected by the query function of other active learning-based models.
  X  Random sampling model (RS): To evaluate the performance of the query function in proposed models, we implemented a passive learner process, whereby t examples are randomly selected from the unlabelled pool for manual labelling in each learning cycle. Since the random sampling model generates high variance results due to the random selection strategy, we averaged the performance of random sampling over 10 runs using the same experimental setting. 5.3. Experimental setup
To estimate average similarity for each unlabelled example in our proposed model, we used the Jingbo method ( Jingbo et al., 2010 ). At the beginning of the algorithm, we calculated the pair wise similarity value between any two unlabelled examples in the unlabelled pool in each view and stored these values in a matrix. During the learning process, in order to calculate the average similarity for each example in the unlabelled pool, other examples were sorted based on their similar-ity scores. Therefore, the average similarity for each example can be calculated efficiently by averaging the similarity scores of top-k examples in its sorted list.

In all experiments, we used the support vector machine classifier (SVM) ( Joachims, 1999 ) as the basic classifier in each view. SVM light ( http://svmlight.joachims.org/ ) is used as the SVM classifier in the experiments with all parameters being set to their default values. 5.3.1. Cross-validation on active learning and semi-supervised learning
To generate reliable results, we performed a 3-fold cross validation on active learning and semi-supervised learning pro-cesses. For this task, the unlabelled documents in the target language were randomly divided into three groups of equal size.
During each step of the cross validation, two groups of documents were treated as the unlabelled pool and the evaluation of the performance was based on the remaining group as an independent test set. The final results are averaged over three iter-ations. Fig. 3 shows the data splitting configuration in each fold of cross-validation. To calculate the average similarity of each of the unlabelled examples, only the pair-wise similarity of those examples that are included in the unlabelled pool were considered. 5.4. Performance measures Generally, the performance of sentiment classification is evaluated by using four indexes, namely: Accuracy, Precision,
Recall and F1-score. Accuracy is the portion of all true predicted instances against all predicted instances. An accuracy rate of 100% means that the predicted instances are exactly the same as the actual instances. Precision denotes the portion of true predicted instances against all predicted instances for each class. Recall represents the portion of true predicted instances against all actual instances for each class. F1 is a harmonic average of precision and recall. 5.5. Results and discussion
In this section, our proposed method is compared with five baseline methods. We set the parameter k in the k NN density measure to 10 for all experiments. In the next subsection, we will study this parameter further. We also used p = n = 5 for the co-training algorithm as in ( Wan, 2011 ) and selected 5 unlabelled examples ( t = 5) in each cycle of co-testing for the manual belled examples were selected for manual labelling, while 400 unlabelled examples were labelled automatically during the learning process. After each learning cycle, test data is presented into two learned classifiers and the final prediction for a given test example is computed based on the average of prediction values of these classifiers.

Table 2 shows the comparison results after the full learning process. As we can see, DACT and ACT models show better performance in comparison with co-training and co-testing after the full learning process. This supports the idea that combining the co-testing and co-training processes can actually be better than each individual approach. This is most likely due to the augmentation of the most confident automatic classified examples, along with manually-labelled examples, into training data during the learning process. Moreover, the DACT model outperforms ACT model in all datasets. This shows that using the density measure of unlabelled examples has a beneficial effect upon selecting the most representative examples for manual labelling. Also, when compared with uncertainty sampling (US-AL), co-testing shows slightly better accuracy in all datasets. This would indicate that co-testing is more effective than uncertainty sampling in the bi-view CLSC. In the uncer-tainty sampling (US-AL), the uncertain examples which are selected for manual labelling may be classified correctly by both classifiers and therefore cannot be helpful for improving the classification accuracy of classifiers. However, in the co-testing strategy, at least one classifier benefits from the selected examples in each learning step and the overall performance is sub-sequently improved step by step.

In comparison to the co-training method proposed by Wan (2011) , the co-training process usually selects high confidence examples to add to the training data. However, if the initial classifiers in each view are not good enough, there will be an increased probability of added examples providing incorrect labels to the training set. This ultimately leads to a decrease in the performance of classification. In our proposed method, the use of active learning results in an increase in the accuracy examples to the training set. In addition, the most confidently classified examples selected by co-training are not necessarily the most valuable examples for improving classification accuracy. Therefore, in our proposed method we combine co-training with active learning in order to select the most valuable examples for improving the classification accuracy along with high confident examples. The main limitation of our method in comparison with that of co-training is the need for human effort to label the selected examples in the active learning part.

As we can see in Table 2 , the classification accuracies vary for different languages. These differences can be said to have originated from two facts: first, sentiment classification shows diverse performance in different languages due to the dispar-ity in the structure of languages when expressing the sentimental data. Next, automatic machine translation systems per-form translation of varying quality in different languages. Therefore, the performances of CLSC using automatic machine translation can be seen to be varied in different languages.

In order to assess whether there are significant differences in terms of accuracy between the proposed model and baseline methods, we conducted a statistical test based on accuracy results obtained from 3-fold cross-validation. We used a paired t -test to evaluate whether differences between the two methods are statistically significant. Table 3 shows the numerical results of the statistical test. With the exception of those between DACT and ACT in the En X  X h and En X  X p datasets, all other comparisons showed statistically significant differences, for a significant level of a = 0.05.

Fig. 4 shows the learning curves of various methods on three evaluation datasets. In each of the iterations of cross-val-idation, all methods used the same initial training set (i.e. labelled documents from the source language), and hence have the same initial accuracy in the learning curve. Each fold of cross-validation generated a learning curve for the experiment of each model. The final learning curve was determined using the average accuracies of each point from generated curves. As shown in this figure, comparison of the proposed method (DACT) with the ACT model resulted in the classification accuracy of the proposed model improving very quickly in the first few learning cycles. This is due to the fact that the examples selected based on density measure for human labelling are more representative than examples selected randomly from the contention examples set. Accuracy of the learning model is far more important in the early steps rather than in the later steps, since the aim of active learning is to improve performance through the selection of a small number of unlabelled examples for manual labelling. For example, in the En X  X r dataset, in order to reach a level of 80% accuracy, DACT needs to label less than 20 examples from the unlabelled pool; while ACT, US-AL, Co-Testing, Co-Training and RS need about 50, 90, 80, 100 and more than 100 labelled examples respectively. Therefore, the proposed model, DACT, can reduce the labelling efforts while increasing the performance of CLSC.

Fig. 5 compares the accuracy of combined views (bi-view) and each of the individual views of the DACT model during the learning process in three datasets. This figure shows that the combination of the source view and the target view in CLSC outperforms both individual views in all datasets. This supports the idea that two views can complement each other so as to compensate for the negative effect of translation errors in resource translation. 5.5.1. Effect of different values of k on the performance of classification
We conducted a further experiment to estimate the appropriate value for parameter k used in the average density for-mula. We changed the value of k from 5 to 100 and experimented with the effect of different k on the accuracy of classifi-cation through the learning process.
Fig. 6 shows the results of this experiment on each dataset and the final results of DACT with different k are summarized in Table 4 . As shown in this table, k = 10 is appropriate by which to estimate the density of an unlabelled example in this method. We have used this value in all experiments.

The proposed model apparently depends on the accessibility of machine translation services to project data between the source and the target language. Although most of the machine translation services have the capability of translating text doc-uments from and into a large number of languages, they cannot be used to freely translate large amounts of data. This issue may limit the use of machine translation in CLSC. 6. Conclusion and future work
In this paper, we have proposed a new model based on bi-view classification by combining active learning and semi-supervised co-training approaches in order to reduce the human labelling effort in cross-lingual sentiment classification.
In this model, both labelled and unlabelled data are represented in the source and target languages using a machine trans-lation service to create two different views of data. Co-training and co-testing algorithms were then used to augment unla-belled data from the target language into the learning process. We also considered a density measure by which to avoid selecting outlier examples from unlabelled data so as to increase the representativeness of selected examples for manual labelling in the co-testing algorithm. We applied this method to cross-lingual sentiment classification datasets in three dif-ferent languages and compared the performance on the proposed model with some baseline methods. The experimental results show that the proposed model outperforms the baseline methods in almost all datasets. The experimental results further show that employing automatic labelling, along with active learning, can increase the speed of the learning process and therefore reduce the manual labelling workload. Experiments also show that considering the density of unlabelled data in the query function of active learning can be very efficient when selecting the most representative and informative exam-ples for manual labelling.

However, translated data cannot cover all the vocabulary idiosyncrasies used in the original data and hence many sen-timent-bearing words cannot be learnt from projected data. Moreover, using projected data increases the number of features and leads to a creation of sparseness in the data points in the classification task.

In future work, we plan to use training data from more than one language as the source language to cover more vocab-ularies of original target language data and employ multi-view learning in this case. Moreover, we will try to include syntax information so as to reduce the effect of translation errors.
 Acknowledgement
The authors would like to thank the reviewers for their valuable comments and suggestions to significantly improve the quality of this paper. This work is supported by the Ministry of Higher Education (MOHE) and Research Management Centre (RMC)attheUniversitiTeknologiMalaysia(UTM)underExploratoryResearchGrantScheme(VoteNo.R.J130000.7828.4L051).
 References
