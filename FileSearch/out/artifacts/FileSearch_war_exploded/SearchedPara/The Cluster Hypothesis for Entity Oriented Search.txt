 In this work we study the cluster hypothesis for entity ori-ented search (EOS). Specifically, we show that the hypothe-sis can hold to a substantial extent for several entity similar-ity measures. We also demonstrate the retrieval effectiveness merits of using clusters of similar entities for EOS.
The entity oriented search (EOS) task has attracted much research attention lately. The main goal is to rank entities in response to a query by their presumed relevance to the information need that the query expresses. For example, the goal in the TREC X  X  expert search task was to rank employees in the enterprise by their expertise in a topic [6]. The goal in INEX entity ranking track was to retrieve entities that pertain to a topic in the English Wikipedia [7, 8, 9]. The goal in TREC X  X  entity track was to rank Web entities with respect to a given entity by their relationships [2, 3, 4].
The EOS task is different than the standard ad-hoc doc-ument retrieval task as entities are somewhat more complex than (flat) documents. That is, entities are characterized by different properties such as name, type (e.g., place or person), and potentially, an associated document (e.g., a homepage or a Wikipedia page). Despite the fundamental difference between the two tasks, we set as a goal to study whether an important principle in ad-hoc document retrieval also holds for the EOS task; namely, the cluster hypothesis [22]. We present the first study of the cluster hypothesis for EOS, where the hypothesis is that  X  X losely associated entities tend to be relevant to the same requests X .
We use several inter-entity similarity measures to quan-tify the association between entities, which is a key point in the hypothesis. These measures are based on the entity type which is a highly important source of information [18, 14]. We then show that the cluster hypothesis, tested using Voorhees X  nearest neighbor test [23], can hold to a substan-tial extent for EOS for several of the similarity measures.
Motivated by the findings about the cluster hypothesis, we explore the merits of using clusters of similar entities for entity ranking. We show that ranking entity clusters by the percentage of relevant entities that they contain can be used to produce extremely effective entity ranking. We also demonstrate the effectiveness of using cluster ranking techniques that are based on estimating the percentage of relevant entities in the clusters for entity ranking.
Our main contributions are three fold: (i) showing that for several inter-entity similarity measures the cluster hypothe-sis holds for EOS to a substantial extent as determined by the nearest neighbor test; (ii) demonstrating the consider-able potential of using clusters of similar entities for EOS; and, (iii) showing that using simple cluster ranking meth-ods can help to improve retrieval performance with respect to that of an effective initial search.
Any entity in the INEX entity ranking track, which we use for our experiments, has a Wikipedia page. Hence, some previously proposed estimates for the entity-query similar-ity are based on Wikipedia X  X  category information [20, 1, 18, 14]. Similarly, we measure the similarity between two entities based on several measures of the similarity between the sets of categories of the two entity pages.

In Cao et al. X  X  work on expert search [5] the retrieval score assigned to an entity was smoothed with the retrieval score assigned to a cluster constructed from the entity. In con-trast, we explore a retrieval paradigm that ranks entity clus-ters and transforms the ranking to entity ranking. Work on document retrieval showed that cluster-based (score) smooth-ing and cluster ranking are complementary [16]. We leave the exploration of this finding for the EOS task for future work.

There are several tests of the cluster hypothesis for doc-ument retrieval [13, 10, 23, 19]. We use Voorhees X  nearest-neighbor test for the EOS task as it is directly connected with the nearest neighbor clusters we use for ranking.
The findings we present for the EOS task echo those re-ported for document retrieval. Namely, the extent to which the cluster hypothesis holds [23], and the (potential) merits of using cluster ranking [12, 21, 15, 17].
Our first goal is to explore the extent to which the cluster hypothesis holds for EOS. To this end, we use the nearest neighbor test [23]. Let L [ n ] q b e the list of n entities that are the highest ranked by an initial search performed in response to query q . For each relevant entity in L [ n ] record the percentage of relevant entities among its K near-est neighbors in L [ n ] q . The nearest neighbors are determined using one of the inter-entity similarity measures specified in Section 5.1. The test result is the average of the recorded percentages over all relevant entities in L [ n ] q , averaged over all test queries.

Some of the inter-entity similarity measures assign dis-crete values including 0. Hence, for some relevant entities there could be less than K neighbors as we do not consider neighbors with a 0 similarity value. In addition, a relevant entity might be assigned with more than K nearest neigh-bors due to ties in the similarity measure. That is, we keep collecting all entities having the same similarity value as that of the last one in the K neighbors list.
Our second goal is studying the potential merits of us-ing entity clusters to induce entity ranking. We re-rank the initial entity list L [ n ] q using a cluster-based paradigm which is very common in work on document retrieval [17]. Let Cl ( L [ n ] q ) be the set of clusters created from L [ n ] clustering method. The inter-entity similarity measures used for creating clusters are those used for testing the cluster hy-pothesis. (See Section 5.1 for further technical details.) The clusters in Cl ( L [ n ] q ) are ranked by the presumed percentage of relevant entities that they contain. Below we describe two cluster ranking methods. Then, each cluster is replaced with its constituent entities while omitting repeats. Within cluster entity ranking is based on the initial entity retrieval scores which were used to create the list L [ n ] q .
The MeanScore cluster ranking method scores cluster c by the mean retrieval score of its constituent entities:
P e  X  c S i nit ( e ; q ); S init ( e ; q ) is the initial retrieval score of entity e ; | c | is the number of entities in c .
When S init ( e ; q ) is a rank equivalent estimate to that of log( Pr ( q, e )) [18], the cluster score assigned by MeanScore is rank equivalent to the geometric mean of the joint query-entity probabilities X  estimates in the cluster. Using a geometric-mean-based representation for document clusters was shown to be highly effective for ranking document clusters [17].
The regularized mean score method, RegMeanScore in short, which is novel to this study, smoothes c  X  X  score: mean retrieval score of a cluster composed of c  X  X  entities and an additional  X  X seudo X  entity whose score is the mean score in the initial list. This method helps to address, among others, cluster-size bias issues.
We conducted experiments with the datasets of the INEX entity ranking track of 2007 [7], 2008 [8], and 2009 [9]. Table 1 provides a summary of the datasets. The tracks for 2007 and 2008 used the English Wikipedia dataset from 2006, while the 2009 track used the English Wikipedia from 2008. The set of test topics for 2007 is composed of 21 topics that w ere derived from the ad hoc 2007 assessments, and addi-tional 25 topics that were created by the participants specif-ically for the track. In 2008, 35 topics were created and used for testing. The topics used for testing in 2009 were 55 topics out of the 60 test topics used in 2007 and 2008.
We used Lucene (http://lucene.apache.org/core/) for ex-periments. The data was pre-processed using Lucene, in-cluding tokenization, stopword removal, and Porter stem-ming.
 larity measures that we use utilize Wikipedia categories. Specifically, the categories associated with the Wikipedia page of the entity, henceforth referred to as its category set, serve as the entity type.

The Tree similarity between two entities e 1 and e 2 is exp(  X   X d ( e 1 , e 2 )) where d ( e 1 , e 2 ) is the minimum distance over Wikipedia X  X  categories graph between a category in e category set and a category in e 2  X  X  category set;  X  is a decay constant determined as in [18]. The SharedCat measure is the cosine similarity between the binary vectors representing two entities. An entity vector is defined over the categories space. An entry in the vector is 1 if the corresponding cat-egory is associated with the entity and 0 otherwise. Thus, SharedCat measures the (normalized) number of categories shared by the two entities [20]. The CE measure is based on measuring the language-model-based similarity between the documents associated with the category sets of two en-tities [14]. More specifically, each category is represented in this case by the text that results from concatenating all Wikipedia pages associated with the category. The similar-ity between the texts x and y that represent two categories is p z (  X  ) is the Dirichlet-smoothed unigram language model in-duced from z with the smoothing parameter  X  (=1000). The CE similarity between two entities is defined as the maximal similarity, over all pairs of categories, one in the first entity X  X  category set and the other in the second entity X  X  category set, of the texts representing the categories. Finally, the ESA (Explicit Semantic Analysis) [11] similarity measure is the cosine between two vectors, each represents the category set of an entity. The vectors representing the category sets are defined over the entities space. The value of an entry in the vector is the number of the categories in the given category set that are associated with the corresponding entity. Using ESA to measure inter-entity similarity is novel to this study.
Three different initially retrieved entity lists, L [ n ] for both the cluster hypothesis test and cluster-based rank-ing. The lists are created in response to the query using highly effective entity retrieval methods [18]. The first list, L
Doc , is created by representing an entity with its Wikipedia document (page). The documents are ranked in response to the query using the standard language-model-based ap-proach with Dirichlet-smoothed unigram language models and the cross entropy similarity measure. The second list, Table 2: The cluster hypothesis test: the average p ercentage of relevant entities among the 5 nearest neighbors of a relevant entity.
 L
Doc ; T ype , is created by scoring entities with an interpola-tion of two scores. The first is that used to create the list L
Doc . The second is the similarity between the category set of the entity and the query target type (the set of cat-egories that are relevant to the query, as defined by INEX topics). The Tree estimate described above is used for mea-suring similarity between the two category sets. The third list, L Doc ; T ype ; Name , is created by scoring an entity with an interpolation of the score used to create L Doc ; T ype , and an estimate for the proximity-based association [18] between the query terms and the entity name (i.e., the title of its Wikipedia page) in the corpus. We employ the same train-test approach as in [18] to set the free-parameter values of the ranking methods used to create the initial lists. The number of entities in each initial list L [ n ] q is n = 50.
We use a simple nearest neighbor clustering method to cluster entities in the initial list L [ n ] q . Specifically, each en-tity in L [ n ] q and the K (= 5) entities in L [ n ] q that are the most similar to it, according to the inter-entity similarity measures described above, form a cluster. Using such small overlapping clusters was shown to be highly effective for cluster-based document retrieval [16, 15, 17]. We note that not all clusters necessarily contain K + 1 documents due to the reasons specified in Section 3. For consistency, we also use K = 5 in the cluster hypothesis test.
 Following the INEX guidelines, the evaluation metric for INEX 2007 is mean average precision (MAP) while that for INEX 2008 and 2009 is infAP. We also report the precision of the top 5 entities (p@5). Statistically significant differences of retrieval performance are determined using the two tailed paired t-test with a 95% confidence level.
Table 2 presents the results of the nearest neighbor cluster hypothesis test that was described in Section 3. The test is performed on the different initially retrieved entity lists us-ing the various inter-entity similarity measures. We see that the average percentage of relevant entities among the near-est neighbors of a relevant entity ranges between 30% and 53% across the various experimental settings. We also found out that, on average, the percentage of relevant entities in a list is often lower than 25% and can be as low as 10%. Thus, due to the relatively high percentage of relevant enti-ties among the nearest neighbors of relevant entities, we can conclude that the cluster hypothesis holds to a substantial extent, according to the nearest neighbor test, with various inter-entity similarity measures.

Table 2 also shows that for most of the data sets and similarity measures the test results for the L Doc ; T ype L ated using entity-query similarity measures that account for category information, while the similarity measure used to create L Doc does not use this information. The highest test results are obtained for the SharedCat similarity measure which, as noted above, measures the (normalized) number of shared categories between two entities.
Table 3 presents the results of employing cluster-based entity re-ranking, as described in Section 4, upon the three initial entity lists. The various inter-entity similarity mea-sures are used for creating the clusters.  X  X nitial X  refers to the initial ranking of a list.  X  X racle X  is the ranking of enti-ties that results from employing the cluster-based re-ranking paradigm described in Section 4; the clusters are ranked by the true percentage of relevant entities that they contain.
The high performance numbers for Oracle, which are sub-stantially and statistically significantly better than those for Initial, attest to the existence of clusters that contain a very high percentage of relevant entities. More generally, these numbers attest to the incredible potential of employing ef-fective cluster ranking methods to rank entities.

RegMeanScore, which outperforms MeanScore due to the regularization discussed in Section 4, is in quite a few cases more effective than Initial; specifically, using the Tree and SharedCat inter-entity similarity measures. While the im-provements for L Doc are often statistically significant, this rally, the more effective the initial ranking (Initial), the more challenging the re-ranking task. Yet, the very high Oracle tive cluster ranking methods can yield performance that is much better than that of the initial ranking. Finally, for in most cases attained by using RegMeanScore.
We showed that the cluster hypothesis can hold to a sub-stantial extent for the entity oriented search (EOS) task with several inter-entity similarity measures. We also demon-strated the potential merits of using a cluster-based retrieval paradigm for EOS that relies on ranking entity clusters. De-vising improved cluster ranking techniques is a future venue we intend to explore.
We thank the reviewers for their comments. Part of the work reported here was done while David Carmel was at IBM. This paper is based on work that has been supported in part by the Israel Science Foundation under grant no. 433/12 and by Google X  X  faculty research award. Any opin-ions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily re-flect those of the sponsors. Table 3: Retrieval performance. The best result in a column (excluding that of Oracle) per an initial list is boldfaced.  X  X  X  marks statistically significant differences with Initial.
