 Probabilistic mixture modeling [ 7 ] has been widely used for density estimation and clustering applications. The Expectation-Maximization (EM) algorithm [ 4 , 11 ] is one of the most used methods for this task for clear reasons  X  elegant formulation of an iterative procedure, ease of implementation, and guaranteed monotone convergence for the objective. On the other hand, the EM algorithm also has some acknowledged shortcomings. In particular, the E-step is linear in both the number of data points and the number of mixture components, and therefore computationally impractical for large-scale applications. Our work was motivated by a large-scale geo-spatial problem, demanding a mixture model of a customer base (a huge number of data points) for competing businesses (a large number mixture components), as the basis for site evaluation (where to locate a new store). Several approximation schemes for EM have been proposed to address the scalability problem, E-step that is truly sub-linear in sample size and also enjoys provable convergence for a well-defined objective function. More details are discussed in Section 5. Our work is inspired by the  X  X hunky EM X  algorithm in [ 17 , 16 ], a smart application of the variational EM framework [ 11 ], where a lower bound on the objective function increases at each iteration and convergence is guaranteed. An E-step in standard EM calculates expected sufficient statistics under mixture-component member-ship probabilities calculated for each individual data point given the most recent model estimate. The variational EM framework alters the E-step to use sufficient statistics calculated under a variational distribution instead. In chunky EM, the speedup is obtained by using a variational distribution with shared (variational) membership probabilities for blocks of data (in an exhaustive partition for the entire data into non-overlapping blocks of data). The chunky EM starts from a coarse partition of the data and gradually refines the partition until convergence.
 However, chunky EM does not scale well in the number of components, since all components share the same partition. The individual components are different  X  in order to obtain membership probabilities of appropriate quality, one component may need fine-grained blocks in one area of the data space, while another component is perfectly fine with coarse blocks in that area. Chunky EM expands the shared partition to match the needed granularity for the most demanding mixture component in any area of the data space, which might unnecessarily increase the computational cost. Here, we derive a principled variation, called component-specific EM (CS-EM) that allows component-specific partitions. We demonstrate a significant performance improvement over standard and chunky EM for experiments on synthetic and mentioned customer-business data. Variational EM. Given a set of i.i.d. data x , { x 1 ,  X  X  X  ,x N } , we are interested in estimating the parameters  X  = {  X  1: K , X  1: K } in the K -component mixture model with log-likelihood function For this task, we consider a variational generalization [ 11 ] of standard EM [ 4 ], which maximizes a lower bound of L (  X  ) through the introduction of a variational distribution q . We assume that the arbitrary discrete distribution over mixture components k = 1 ,...,K . We can lower bound L (  X  ) by multiplying each p ( x n |  X  k )  X  k in (1) with q n ( k ) q Kullback-Leibler (KL) divergence between q and p . The variational EM algorithm alternates the following two steps, i.e. coordinate ascent on F (  X ,q ) , until convergence.
 If q is not restricted in any form, the E-step produces q t +1 = Q n p (  X | x n , X  t ) , because the KL-divergence is the only term in (3) depending on q . The variational EM is in this case equivalent to the standard EM, and hence produces the maximum likelihood (ML) estimate. In the following, we consider certain ways of restricting q to attain speedup over standard EM, implying that the minimum KL-divergence between q n and p (  X | x n , X  ) is not necessarily zero. Still the variational EM defines a convergent algorithm, which instead optimizes a lower bound of the log-likelihood.
 Chunky EM. The chunky EM algorithm [ 17 , 16 ] falls into the framework of variational EM algorithms. In chunky EM, the variational distribution q = Q n q n is restricted according to a partition into exhaustive and mutually exclusive blocks of the data. For a given partition, if data points x i and x j are in the same block, then q i = q j . The intuition is that data points in the same block are somewhat similar and can be treated in the same way, which leads to computational savings in the E-step. If M is the number of blocks in a given partition, the E-step for chunky EM has cost O ( KM ) whereas in standard EM the cost is O ( KN ) . The speedup can be tremendous for M N . The speedup is gained by a trade-off between the tightness of the lower bound for the log-likelihood and the restrictiveness of constraints. Chunky EM starts from a coarse partition and iteratively refines it. This refinement process always produces a tighter bound, since restrictions on the variational distribution are gradually relaxed. The chunky EM algorithm stops when refining any block in a partition will not significantly increase the lower bound. In chunky EM, all mixture components share the same data partition. However, for a particular block of data, the variation in membership probabilities differs across components, resulting in varying differences from the equality constrained variational probabilities. Roughly, the variation in membership probabilities is greatest for components closer to a block of data, and, in particular, for components far away the membership probabilities are all so small that the variation is insignificant. This intuition suggests that we might gain a computational speedup, if we create component-specific data partitions, where a component pays more attention to nearby data (fine-grained blocks) than data far away (coarser blocks). Let M k be the number of data blocks in the partition for component k . The complexity for the E-step is then O ( P k M k ) , compared to O ( KM ) in chunky EM. Our conjecture is that we can lower bound the log-likelihood equally well with P k M k significantly smaller than KM , resulting in a much faster E-step. Since our model maintains different partitions for different mixture components, we call it the component-specific EM algorithm (CS-EM). Main Algorithm. Figure 2 (on p. 6) shows the main flow of CS-EM. Starting from a coarse partition for each component (see Section 4.1 for examples), CS-EM runs variational EM to convergence and then selectively refine the component-specific partitions. This process continues until further refinements will not significantly improve the lower bound. Sections 3.1-3.5 provide a detailed description of basic concepts in support of this brief outline for the main structure of the algorithm. 3.1 Marked Partition Trees It is convenient to organize the data into a pre-computed partition tree , where a node in the tree represents the union of the data represented by its children. Individual data points are not actually stored in each node, but rather, the sufficient statistics necessary for our estimation operations are pre-computed and stored here. (We discuss these statistics in Section 3.3.) Any hierarchical decomposition of data that ensures some degree of similarity between data in a block is suitable for constructing a partition tree. We exemplify our work by using KD-trees [ 9 ]. Creating a KD-tree and We will in the following consider tree-consistent partitions , where each data block in a partition corresponds to exactly one node for a cut (possibly across different levels) in the tree X  X ee Figure 1. Let us now define a marked partition tree (MPT), a simple encoding of all component-specific mixture component k . In Figure 1, for example, B 1 is the partition into data blocks associated with nodes { e,c,d } . In the shared data partition tree used to generate the component-specific partitions, we mark the corresponding nodes for the data blocks in each B k by the component identifier k . Each node v in the tree will in this way contain a (possibly empty) set of component marks, denoted by K v . The MPT is now the subtree obtained by pruning all unmarked nodes without marked descendants from the tree. Figure 1 shows an example of a MPT. This example is special in the sense that all nodes in the MPT are marked. In general, a MPT may have unmarked nodes at any location above the leaves. For example, in chunky EM, the component-specific partitions are the same for each mixture component. In this case, only the leaves in the MPT are marked, with each leaf marked by all mixture components. The following important property for a MPT holds since all component-specific partitions are constructed with respect to the same data partition tree.
 Property 1. Let T denote a MPT. The marked nodes on a path from leaf to root in T mark exactly one data block from each of the K component-specific data partitions.
 In the following, it becomes important to identify the data block in a component-specific partition, which embeds the block defined by a leaf. Let L denote the set of leaves in T , and let B L denote a partition with data blocks B l  X  B L according to these leaves. We let B k ( l ) denote the specific B k  X  X  k with the property that B l  X  B k . Property 1 ensures that B k ( l ) exists for all l,k . Example: In Figure 1, the path a  X  e  X  g in turn marks the components K a = { 3 , 4 } , K e = { 1 , 2 } , and K g = { 5 } and we see that each component is marked exactly once on this path, as stated in 3.2 The Variational Distribution Our variational distribution q assigns the same variational membership probability to mixture compo-nent k for all data points in a component-specific block B k  X  X  k . That is, which we denote as the component-specific block constraint . Unlike chunky EM, we do not assume that the data partition B k is the same across different mixture components. The extra flexibility complicates the estimation of q in the E-step. This is the central challenge of our algorithm. To further drive intuition behind the E-step complication, let us make the sum-to-one constraint for the to the above block constraint and using Property 1 can be reformulated as the |L| constraints Notice that since q B k can be associated with an internal node in T it may be the case that q B represent the same q B k across different constraints in (5). In fact, implying that the constraints in (5) are intertwined according to the nested structure given by T . The closer a data block B k is to the root of T the more constraints simultaneously involve the same q B k . Example: Consider the MPT in Figure 1. Here, q B density for component 5 is the same across all four sum-to-one constraints. Similarly, q B so the density is the same for component 1 in the two constraints associated with leaves a and b . 2 3.3 Efficient Variational E-step Accounting for the component-specific block constraint in (4), the lower bound, F (  X ,q ) , in Eq. (2) can be expressed as a sum of local parts, F (  X ,q B k ) , as follows where we have defined the block-specific geometric mean We integrate the sum-to-one constraints in (5) into the lower bound in (7) by using the standard principle of Lagrange duality (see, e.g., [1]). Accordingly, we construct the Lagrangian where  X  , {  X  1 ,..., X  L } are the Lagrange multipliers for the constraints in Eq. (5). Recall the relationship between q B k and q B Solving the dual optimization problem  X   X  = arg min  X  F (  X ,q (  X  ) , X  ) now leads to the primal solution given by q  X  B For chunky EM, the E-step is straightforward, because B k ( l ) = B l and therefore P l : B for all k = 1 ,...,K . Substituting (9) into the sum-to-one constraints in (5) reveals that each  X  l can be solved independently, leading to the following closed-form solution for q B where Z = P k  X  k exp( g B CS-EM does not enjoy a similar simple optimization, because of the intertwined constraints, as described in Section 3.2. Fortunately, we can still obtain a closed-form solution. Essentially, we use the nesting structure of the constraints to reduce Lagrange multipliers from the solution one at a time until only one is left, in which case the optimization is easily solved. We describe the basic approach here and defer the technical details (and pseudo-code) to the supplement.
 Consider a leaf node l  X  L and recall that K l denotes the components with B k ( l ) = B l in their partitions. The sum-to-one constraint in (5) that is associated with leaf l can therefore be written as Furthermore, for all k  X  X  l the q B Now, consider l  X  X  leaf-node sibling, l 0 . For example, in Figure 1, node l = a and l 0 = b . The two leaves share the same path from their parent to the root in T . Hence, using Property 1, it must be imply that q l = q l 0 . Using (11), it now follows that ment) shows how we more efficiently account for this parameter reduction and continue the process, now considering the parent node a new  X  X eaf X  node once all children have been processed. When  X  l can therefore be found analytically by solving the corresponding sum-to-one constraint in (5). Following, all optimal q  X  B It is therefore key to the computational efficiency of the CS-EM algorithm that g B k can be calculated from pre-computed statistics, which is in fact the case for the large class of exponential family distributions. These are the statistics that are stored in the nodes of the MPT.
 Example: Let p ( x |  X  k ) be an exponential family distribution A (  X  k ) is the normalizing constant. Then p ( x |  X  k ) = N d (  X  k ,  X  k ) , a Gaussian distribution, then 3.4 Efficient Variational M-step In the variational M-step the model parameters  X  = {  X  1: K , X  1: K } are updated by maximizing Eq. (7) w.r.t.  X  under the constraint P k  X  k = 1 . Hereby, the update is Thus, the M-step can be efficiently computed using the pre-computed sufficient statistics as well. Example: If p ( x |  X  k ) has the exponential family form in Eq. (12),  X  k is obtained by solving In particular, if p ( x |  X  k ) = N d (  X  k ,  X  k ) , then  X  3.5 Efficient Variational R-step Given the current component-specific data partitions, as marked in the MPT T , a refining step (R-step) selectively refines these partitions. Any refinement enlarges the family of variational distributions, and therefore always tightens the optimal lower bound for the log-likelihood. We define a refinement unit as the refinement of one data block in the current partition for one component in the model. The efficiency of CS-EM is affected by the number of refinement units performed at each R-step. With too few units we spend too much time on refining, and with too many units some of the refinements may be far from optimal and therefore unnecessarily slow down the algorithm. We have empirically found K refinement units at each R-step to be a good choice. This introduces K new free variational parameters, which is similar to a refinement step in chunky EM. However, chunky EM refines the same data block across all components, which is not the case in CS-EM. Ideally, an R-step should select the refinement units leading to optimal improvement for F . Good candidates can be found by performing a single E-step for each candidate and then select the units that improve F the most. This demands the evaluation of an E-step for each of the P k M k possible refinement units. Exact evaluation for this many full E-steps is prohibitively expensive, and we therefore instead approximate these refinement-guiding E-steps by a local computation scheme based on the intuition that refining a block for a specific component mostly affects components with similar local partition structures. The algorithm is described in Figure 3 with details as follows. Consider moving all component-marks for v  X  X  to its children ch ( v ) , where each child u  X  ch ( v ) value for each  X  q B refinement. In this case, the sum-to-one constraints for  X  q simplifies as not under u , and that q B constraints in (14) therefore reduces to the following | ch ( v ) | independent constraints Each  X  q B The improvement to F that is achieved by the refinement-guiding E-step for the refinement unit refining data block v for component k is denoted  X  F v,k , and can be computed as This improvement is computed for all possible refinement units and the K highest scoring units are then selected in the R-step. Notice that this selective refinement step will most likely not refine the same data block for all components and therefore creates component-specific partitions. With q 5( u ) held fixed, we will for each child u  X  X  a,b } optimize  X  q B In this section we provide a systematic evaluation of CS-EM, chunky EM, and standard EM on synthetic data, as well as a comparison between CS-EM and chunky EM on the business-customer data, mentioned in Section 1. (Standard EM is too slow to be included in the latter experiment.) 4.1 Experimental setup For the synthetic experiments, we generated random training and test data sets from Gaussian mixture models (GMMs) by varying one (in a single case two) of the following default settings: #data points N = 100 , 000 , #mixture components K = 40 , #dimensions d = 2 , and c -separation 2 c = 2 . The (proprietary) business-customer data was obtained through collaboration with PitneyBowes Inc. and Yellowpages.com LLC. For the experiments on this data, N = 6 . 5 million and d = 2 , corresponding to the latitude and longitude for potential customers in Washington state. The basic assumption is that potential customers act as rational consumers and frequent the somewhat closest business locations to purchase a good or service. The locations for competing stores of a particular type, in this way, correspond to fixed centers for components in a mixture model. (A less naive model with the penetration level for a good or service and the relative attractiveness for stores, is the object of related research, but is not important for the computational feasibility studied here.) The synthetic experiments are initialized as follows. After constructing KD-tree, the first tree-level containing at least K nodes ( d log 2 K e ) is used as the initial data partition for both chunky EM and all components in CS-EM. For all algorithms (including standard EM), we randomly chose K data blocks from the initial partition and initialized parameters for the individual mixture components accordingly. Mixture weights are initialized with a uniform distribution. The experiments on the business-customer data are initialized in the same way, except that the component centers are fixed and the initial data blocks that cover these centers are used for initializing the remaining parameters. For CS-EM we also considered an alternative initialization of data partitions, which better matches the rationale behind component-specific partitions. It starts from the CS-EM initialization and recursively, according to the KD-tree structure, merges two data blocks in a component-specific partition, if the merge has little effect on that component. 3 We name this variant as CS-EM  X  . 4.2 Results For the synthetic experiments, we compared the run-times for the competing algorithms to reach a parameter estimate of same quality (and therefore similar clustering performance not counting different local maxima), defined as follows. We recorded the log-likelihood for the test data at each iteration of the EM algorithm, and before each S-step in chunky EM and the CS-EM. We ran all algorithms to convergence at level 10  X  4 , and the test log-likelihood for the algorithm with lowest value was chosen as baseline. 4 We now recorded the run-time for each algorithm to reach this baseline, and computed the EM-speedup factors for chunky EM, CS-EM, and CS-EM  X  , each defined as the standard EM run-time divided by the run-time for the alternative algorithm. We repeated all experiments with five different parameter initializations and report the averaged results. Figure 4 shows the EM-speedups for the synthetic data. First of all, we see that both CS-EM and CS-EM  X  are significantly faster than chunky EM in all experiments. In general, the P k M k variational parameters needed for the CS-EM algorithms is far fewer than the KM parameters needed for chunky EM in order to reach an estimate of same quality. For example, for the default experimental setting, the ratio KM/ P k M k is 2 . 0 and 2 . 1 for, respectively, CS-EM and CS-EM  X  . We also see that there is no significant difference in speedup between CS-EM and CS-EM  X  . This observation can be explained by the fact that the resulting component-specific data partitions greatly refine the initial partitions, and any computational speedup due to the smarter initial partition in CS-EM  X  is therefore overwhelmed. Hence, a simple initial partition, as in CS-EM, is sufficient.
 Finally, similar to results already reported for chunky EM in [ 17 , 16 ], we see for all of chunky EM, CS-EM, and CS-EM  X  that the number of data points and the amount of c -separation have a positive effect on EM-speedup, while the number of dimensions and the number of components have a negative effect. However, the last plot in Figure 4 reveals an important difference between chunky EM and CS-EM: with a fixed ratio between number of data points and number of clusters, the EM-speedup declines a lot for chunky EM, as the number of clusters and data points increases. This observation is important for the business-customer data, where increasing the area of investigation (from city to county to state to country) has this characteristic for the data.
 In the second experiment on the business-customer data, standard EM is computationally too de-manding. For example, for the  X  X ail salon X  example in Figure 5, a single EM iteration takes about 5 hours. In contrast, CS-EM runs to convergence in 20 minutes. To compare run-times for chunky
Figure 4: EM-speedup factors on synthetic data. Figure 5: A comparison of run-time and final number EM and CS-EM, we therefore slightly modified the way we ensure that the two algorithm reach a parameter estimate of same quality. We use the lowest of the F values (on training data) obtained for the two algorithms at convergence as the baseline, and record the time for each algorithm to reach this baseline. Figure 5 shows the speedup (time ratio) and the reduction in number of variational parameters (parameter ratio) for CS-EM compared to chunky EM, as evaluated on exemplary types of businesses. Again, CS-EM is significantly faster than chunky EM and the speedup is achieved by a better targeting of variational distribution through the component-specific partitions. Related work. CS-EM combines the best from two major directions in the literature regarding speedup of EM for mixture modeling. The first direction is based on powerful heuristic ideas, but without provable convergence due to the lack of a well-defined objective function. The work in [ 10 ] is a prominent example, where KD-tree partitions were first used for speeding up EM. As also pointed out in [ 17 , 16 ], the method will likely X  X ut not provably X  X onverge for fine-grained partitions. In contrast, CS-EM is provable convergent X  X ven for arbitrary rough partitions, if extreme speedup is needed. The granularity of partitions in [ 10 ] is controlled by a user-specified threshold on the minimum and maximum membership probabilities that are reachable within the boundaries of a node in the KD-tree. In contrast, we have almost no tuning parameters. We instead let the data speak by itself by having the final convergence determine the granularity of partitions. Finally, [ 10 ]  X  X runes X  a component (sets the membership probability to zero) for data far away from the component. It relates to our component-specific partitions, but ours is more principled with convergence guarantees. The second direction of speedup approaches are based on the variational EM framework [ 11 ]. In [ 11 ], a  X  X parse X  EM was presented, which at some iterations, only updates part of the parameters and hence relates it to the pruning idea in [ 10 ]. [ 14 ] presents an  X  X ncremental X  and a  X  X azy X  EM, which gain speedup by performing E-steps on varying subsets of the data rather than the entire data. All three methods guarantee convergence. However, they need to periodically perform an E-step over the entire data, and, in contrast to CS-EM, their E-step is therefore not truly sub-linear in sample size, making them potentially unsuitable for large-scale applications. The chunky EM in [ 17 , 16 ] is the approach most similar to our CS-EM. Both are based on the variational EM framework and therefore guarantees convergence, but CS-EM is faster and scales better in the number of clusters. In addition, heuristic sub-sampling is common practice when faced with a large amount of data. One could argue that chunky EM is an intelligent sub-sampling method, where 1) instead of sampled data points it uses geometric averages for blocks of data in a given data partition, and 2) it automatically chooses the  X  X ampling size X  by a learning curve method, where F is used to measure the utility of increasing the granularity for the partition. Sub-sampling therefore has same computational complexity as chunky EM, and our results therefore suggest that we should expect CS-EM to be much faster than sub-sampling and scale better in the number of mixture components.
 Finally, we exemplified our work by using KD-trees as the tree-consistent partition structure for generating the component-specific partitions in CS-EM, which limited its effectiveness in high dimensions. However, any hierarchical partition structure can be used, and the work in [ 8 ] therefore suggest that changing to an anchor tree (a special kind of metric tree [ 15 ]) will also render CS-EM effective in high dimensions, under the assumption of lower intrinsic dimensionality for the data. Future Work. Future work will include parallelization of the algorithm and extensions to 1) non-probabilistic clustering methods, e.g., k-means clustering [ 6 , 13 , 5 ] and 2) general EM applications beyond mixture modeling. [1] S. Boyd and L. Vandenberghe. Convex Optimization . Cambridge University Press, 2004. [2] P. S. Bradley, U. M. Fayyad, and C. A. Reina. Scaling EM (expectation maximization) clustering [3] S. Dasgupta. Learning mixtures of Gaussians. In Proceedings of the 40th Annual Symposium [4] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum likelihood from incomplete data via [5] G. Hamerly. Making k-means even faster. In SIAM International Conference on Data Mining [6] T. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman, and A. Y. Wu. An [7] G. J. McLachlan and D. Peel. Finite Mixture Models . Wiley Interscience, New York, USA, [8] A. Moore. The anchors hierarchy: Using the triangle inequality to survive high-dimensional [9] A. W. Moore. A tutorial on kd-trees. Technical Report 209, University of Cambridge, 1991. [10] A. W. Moore. Very fast EM-based mixture model clustering using multiresolution kd-trees. In [11] R. Neal and G. E. Hinton. A view of the EM algorithm that justifies incremental, sparse, and [12] L. E. Ortiz and L. P. Kaelbling. Accelerating EM: An empirical study. In Proceedings of the [13] D. Pelleg and A. Moore. Accelerating exact k-means algorithms with geometric reasoning. In [14] B. Thiesson, C. Meek, and D. Heckerman. Accelerating EM for large databases. Machine [15] J. K. Uhlmann. Satisfying general proximity/similarity queries with metric trees. Information [16] J. J. Verbeek, J. R. Nunnink, and N. Vlassis. Accelerated EM-based clustering of large data sets. [17] J. J. Verbeek, N. Vlassis, and J. R. J. Nunnink. A variational EM algorithm for large-scale
