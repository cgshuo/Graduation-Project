 Comprehensive risk assessment lies in the core of enabling proactive healthcare delivery systems. In recent years, data-driven predictive modeling approaches have been increas-ingly recognized as promising techniques to help enhance healthcare quality and reduce cost. In this paper, we pro-pose a data-driven comprehensive risk prediction method, named Linkage , which can be used to jointly assess a set of associated risks in support of holistic care management. Our method can not only perform prediction but also discover the relationships among those risks. The advantages of the proposed model include: 1) It can leverage the relationship between risks and domains and achieve better risk prediction performance; 2) It provides a data-driven approach to un-derstand relationship between risks; 3) It leverages the infor-mation between risk prediction and risk association learning to regulate the improvement on both parts; 4) It provides flexibility to incorporate domain knowledge in learning risk associations. We validate the effectiveness of the proposed model on synthetic data and a real-world healthcare survey data set.
 H.4 [ Information Systems Applications ]: Miscellaneous; J.3 [ Life and Medical Sciences ]: Health, Medical infor-mation systems Algorithms Healthcare; Comprehensive risk prediction; Generalized lin-ear model; Generalized thresholding; Covariance Matrix
Healthcare is closely related to everyone X  X  daily life. Be-cause of the high complexity in healthcare industry, every c year a huge amount of money is wasted. In recent years re-searchers from different areas went into the healthcare world with the hope of helping to reduce the cost and improve the quality of care delivery. Among all those emerging trends, data driven technologies have captured a lot of attentions due to the availability of more and more healthcare data. Data-driven healthcare is at the center of the vision of learn-ing health systems and holds great promise for transforming the current healthcare status. Until now the research of data-driven healthcare is mainly in the clinical setting, i.e., by analyzing medical data from clinical institutes such as Electronic Health Records (EHR). However, for comprehen-sive care management, particularly among the older popula-tion, assessing risks in other domains such as daily tasks and social and behavioral activities are equally important, and can be critical for preventive care. Because of the complex-ity of patient health conditions, any specific health risk is usually associated with other related risks from these other domains. Those risks span medical, social, behavioral, men-tal, psychological aspects of the patients.

In this paper, we present the comprehensive risk prediction problem, wherein instead of just predicting a single risk, we will (1) jointly predict a set of associated risks; (2) discover the hidden risk associations. There are many challenges in this process include the followings: We propose a general approach called L inkage (LINKed tArgets reGrEssion), which models comprehensive risk pre-diction as a sparse optimization problem. Linkage builds a sparse linear predictor for every risk target, and assumes that the sparsity patterns on the coefficients of the linear predictors are similar for similar risks. Actually the sparsity pattern, i.e., the nonzero elements of the linear predictor co-efficients reflect the  X  X ctive X  features that really contribute to the predicted risk. Therefore our assumption is similar risks should have similar contributed features. We develop an efficient alternating optimization procedure to solve the problem and validate its effectiveness on both synthetic and real world data sets. It is worthwhile to highlight the fol-lowing aspects of the proposed method:
One thing that is worthy of mentioning here is that Link-age is closely related to Multi-Task Learning (MTL)[2, 8, 13, 12, 17, 22, 24], which is a learning paradigm aiming at learning a problem together with other related problems at the same time, under a shared representation. To the best of our knowledge, most existing multi-task learning methods assume the tasks are homogeneous, i.e., of the same type. MTL methods also do joint predictions and learn task as-sociations simultaneously, but they usually assume similar tasks will have similar coefficients for linear predictors, not similar sparse patterns [24, 25]. This is a much stronger assumption and may miss some task associations. There is also work on graphical Lasso which made similar assumption as Linkage [9], but the task association matrix is already given in their work. On the contrary, Linkage will learn the task association matrix from data.

The rest of this paper is organized as follows. Section 2 introduces the details of the proposed methodology. The empirical evaluations on both synthetic and real world data are introduced in Section 3, followed by the conclusions in Section 4.
Consider a problem of jointly predicting m risks for n observations (or samples, patients). Let y j  X  R n  X  1 be the vector of the j -th risk target, and Y = [ y 1 , . . . , y be the target matrix. Assume there are d features. Let x  X  R n  X  1 be the i -th feature vector, and X = [ x 1 , . . . , x R n  X  d be the feature matrix. In this section, we temporarily assume that both X and Y are completely observed. In EHR or healthcare related data sets, it is common that risk targets or features are incompletely observed. Different risks or features could be observed for different groups of samples, or they could partially share a group of samples. We will discuss an extension of our proposed framework in Section 2.3 to deal with incomplete observations.

For each risk target, we consider the following generalized linear model: where E ( ) denotes expectation, g ( ) is the link function, w j  X  R d  X  1 is the coefficient vector of target j , and Xw linear predictor. We collectively denote W = [ w 1 , . . . , w R d  X  m as the coefficient matrix. Each column of W contains the coefficients of one risk target, and each row contains the coefficients of one feature in the m targets.

The link function g ( ) describes the relationship between the mean of target response y j and features X . Depending on the type of target response y j , there are many commonly used link functions. In this paper, we consider two types of risks, which are continuous risks and binary risks. Their cor-responding choices of link function are discussed in Section 2.2.

One of the two major goals of our proposed framework is to explore the hidden association between risk targets. In this paper, we assume that the risk association is revealed in the structure of the coefficient matrix W . In the literature of multi-task learning, representations of target relatedness can be categorized into two types. Methods belong to the first type use the sparsity patterns of w j  X  X  to reflect target relatedness. Related targets are assumed to share the same group or similar groups of features [1, 7]. Methods in the second type use the covariance matrix of W to characterize risk association[24]. In this paper, we blend the two dif-ferent representations into a unified framework. Both the sparsity pattern and the covariance matrix of W are used to characterize risk associations. We follow Zhang and Yeung (2010) [24] and assume that the coefficient matrix W follows a Matrix Variate Normal (MVN) distribution [15], i.e. , In model (2), the first term 0 is a d -by-m matrix of zeros. It represents the location of W . The second term  X  is a d -by-d matrix. It represents the row-wise covariances of W . In this paper we set  X  =  X  2 I , where  X  2 is unknown, and is transformed into a tuning parameter in the objective func-tion (6). By setting  X  to be a diagonal matrix, we assume that rows of W are independent with each other. In other words, coefficients of different features in the same target are not correlated. This assumption can be relaxed without adding too much complexity to the model. The third block of parameter  X  is a m -by-m symmetric positive definite ma-trix.  X  represents the column-wise covariance of W . It is unknown and reflects risk association.

Estimating  X  when the dimension m is moderate or large is known to be a difficult problem [4]. In the field of health-care informatics, domain knowledge about risk association is often available or partially available. In order to utilize available domain knowledge, we impose a prior distribution on  X : where IW denotes the Inverse-Wishart distribution,  X  and  X  are two tuning parameters, and  X  0  X  R m  X  m is a known symmetric positive definite matrix.  X  0 c ontains all prior knowledge about risk association. When domain knowledge on risk association is available, the prior distribution can leverage the information and help improve the estimation of  X . When domain knowledge about risk association is not available, we set  X  0 to be  X I , where  X  is an arbitrary small value. In both cases,  X  0 is positive definite. Thus can help stabilize the proposed algorithm and enhance robustness. Combining models (1), (2) and (3), the full likelihood of W and  X  is expressed as follows: We use the Maximum Like Estimation (MLE) method to es-timate the coefficient matrix W and risk association matrix  X . As mentioned in the previous paragraph, the sparsity pattern of W also reflects risk association. To enforce spar-sity of W , we add an additional l 1 regularizer on W .
The structures of W and  X  are closely related. For in-stance, highly correlated risks may have similar groups of  X  X ctive X  features, and coefficients of two related risks may be similar. As far as we know, existing methods either im-pose regularizers on W [2, 8, 10, 13, 17, 18, 19, 20, 26] or on  X [22]. The connection between the two parts has not been fully utilized. In this paper, we propose a novel regularizer, called Linkage regularizer, to link the two components. The Linkage regularizer is given as follows: The notions || , kk 1 , and sign( ) denote the absolute value, the l 1 norm, and the sign function, respectively. Note that when  X  is known, the Linkage regularizer (5) reduces to the Graph-guided Fussed Lasso regularizer in Chen et al. (2012)[9]. In this paper, both W and  X  are unknown and needed to be estimated.
 The Linkage regularizer links the two components W and  X  and let them reciprocally leverage information from each other. To see the effect of Linkage on W , we re-write (5) as follows: pen (  X ,  X  , W ) ij = When  X  ij &gt; 0, risk targets i and j are positively correlated. The regularizer enforces the distance between w i and w j to be small. Consequently, if one element in w i is zero, its counterpart in w j is forced to be small. Together with the l regularizer, w i and w j tend to have similar sparsity patterns. When  X  ij &lt; 0, risk targets i and j are negatively correlated. The regularizer enforces the distance between w i and w j be large. When  X  ij = 0, targets i and j are not correlated. In this case no restriction is imposed on the distance between w i and w j .  X  controls the strength of the regularization.
The effect of (5) on  X  is less straight-forward. To make it clear, we temporarily treat w i and w j as scalars and con-sider them fixed. When w j and w j have the same sign, risks i and j are positively correlated, and consequently | w i  X  w j | X | w i + w j | . In this case a negative  X  ij gets larger penalty than a positive one, and  X  ij is  X  X ushed X  X owards the positive direction. In the reverse case where w i and w j opposite signs, risks i and j are negatively correlated, and we have | w i  X  w j | &gt; | w i + w j | . A positive  X  ij penalty than a negative  X  ij , and  X  ij is pushed towards the negative direction. Moreover, together with the l 1 regular-izer discussed before, Linkage regularizer is able to enforce similar sparsity pattern for associated targets.

Combining all aforementioned models and regularizers, the proposed model solves the following optimization prob-lem:
The first term l ( ) denotes the loss function, which is de-rived from the negative log-likelihood function of the gen-eralized linear model (1); tr and det denote the trace and determinant of a matrix; and  X  1 ,  X  2 ,  X  3 ,  X  1 , and  X  ing parameters.
The loss function term in (6) depends on the choice of link function in model (1), which further depends on the types of risks. In this paper, we consider two types of risks: the continuous risk and the binary risk.

When the support of y ij spans the whole real line, i.e. y ij  X  (  X  X  X  ,  X  ), risk j belongs to the continuous type. In this case y ij is assumed to follow a Gaussian distribution, and the corresponding link function is the identity function. The loss function can be written as follows: where x ( i ) denotes the i -th row of X , and kk 2 denote the l norm.

When y ij only have two possible outcomes, i.e. y ij  X  { X  1 , 1 } , risk j belongs to the binary type. In this case, y is assumed to follow a Bernoulli distribution. The corre-sponding loss function can be written as follows:
Let L be a n -by-m matrix where the ( i, j )-th element l is defined either as in (7) or in (8). Let 1 m denote a m -dimensional vector of all 1s. The loss function in (6) is de-fined as the summation of l ij across all i (observations) and all j (risks). The loss function can be expressed as follows:
When all risk targets belong to the continuous type, we refer to the model as a continuous model; when all risk tar-gets belong to the binary type, we refer to the model as a binary model; when both types of risk targets exist, the model is referred to as a mixed model. Incomplete observations are ubiquitous in healthcare data. Particularly when jointly predicting multiple risks, it is of-ten expensive, or impossible to obtain all information from all samples/patients. In this paper, we deal with incomplete observations in risk targets ( Y ) and features ( X ) using dif-ferent methods. Unobserved values in X are imputed in advance using off-the-shelf imputation methods, such as the K-nearest-neighbor method.
Missing values in Y a re excluded from the objective func-tion (6). Let S be a matrix of zeros and ones with the same dimension as Y . Particularly, S ij = 1 if Y ij is observed, and S ij = 0 otherwise. Note that Y is involved in the objective function only through the loss function term. We replace L in (6) with  X  L = S  X  L , where  X  denotes Hadamard prod-uct. In this way, missing values have no contribution to the objective function, thus are excluded from the analysis.
Another way to view S is that it works as a weighting matrix. In the above example, all unobserved values get extremely low weights (i.e. 0), and all observed values have equal weights (i.e. 1). It is possible that elements of S take values other than 0 and 1, and observed values could receive unequal weights. For example if a binary risk is highly unbalanced, a properly chosen S could mitigate the unbalanceness. How to chose a proper S is beyond the scope of this paper and we do not discuss the issue here. For simplicity of notation, we do not distinguish L and  X  L in the following discussion unless necessary.
Solving the optimization problem (6) is non-trivial due to the following reasons: 1) the term log det ( X ) is concave in  X , making the objective non-convex; 2) the Linkage regu-larizer involves the sign function and product terms of  X  and w j . In this paper, we propose an iterative algorithm to solve the problem. Within each iteration, the two blocks W and  X  are updated alternatively. In the following discussion, we refer to the iteration as the global iteration. In the t -th global iteration, we fix  X  =  X  ( t  X  1) and update W . The optimization problem in this step is given as follows: When  X  is fixed, the Linkage regularizer reduces to the Graph-guided Fussed Lasso regularizer. We follow Chen et al. (2012)[9] and use the Smoothing Proximal Gradient (SPG) method to solve the optimization problem (10).
Reformulate the Linkage regularizer as max k A k where A is an auxiliary matrix, kk  X  is the l  X  norm, and C is a m ( m  X  1)-by-m matrix with
Using the technique from Nesterov (2005)[21], a smooth approximation to the Linkage regularizer is constructed as follows: where &gt; 0 is arbitrary small number, A  X  ij = g h CW  X  and g h ( ) is the hard-thresholding function defined as fol-lows: It has been shown in Chen et al. (2012)[9] that the expres-sion (12) is convex and smooth in W . The gradient of (12) w.r.t. W is given by ( A  X  )  X  C . Replacing the Linkage reg-ularizer with the smooth approximation (12), we solve the following optimization problem:
Next, using the proximal method, the optimization prob-lem (13) is solved by iteratively solving the following prob-lem: where
To distinguish from the global iteration, we refer to this iteration as the inner SPG iteration. W ( k  X  1) is the solution obtained from the previous inner SPG iteration, and  X  is the step size. We use the ISTA with backtracking algorithm in Beck and Teboulle (2009)[3] to decide  X  .
 Problem (14) becomes the Graphical Lasso problem [11]. The problem can be solved by applying the soft-thresholding rule to each element of V . The solution is given as follows: To sum up, Algorithm 1 gives the steps to update W . Algorithm 1 S PG algorithm to update W Require:  X  =  X  ( t  X  1) from last global iteration, data set X , 2: for k = 0 , 1 , 2 , . . . until convergence of  X  ( k ) 3: Formulate C according to (11) and calculate A  X  5: Line search for step size  X  7: Update  X  ( k ) according to (15) 8: end for
I n this step, we fix W and update  X . The optimization problem in this step becomes the follows:
P roblem (16) is closely related to the optimization prob-lem in Bien and Tibshirani (2011)[5]. There are two differ-ences between the two optimization problems. First, Bien and Tibshirani (2011) imposes a l 1 regularizer on  X , whereas in this paper we use the Linkage regularizer. Second, Q in [5] (denoted as S ) denotes the sample covariance matrix. In (16), Q is a combination of the covariance matrix of W and the prior matrix  X  0 . The second difference provides a flexible way to blend data-driven approach and knowledge driven approach to discover risk association.

The first term in problem (16) is concave. We follow Bien and Tibshirani (2011)[5] and apply the Majorize-Minimization (MM) method[16] iteratively to deal with the non-convexity. To avoid confusion with later discussion, we refer to the iter-ation as the outer iteration. Let  X  ( k  X  1) denote the solution of  X  obtained from the last outer iteration. At the k -th outer iteration, the term log det ( X ) is majorized by its tangent at  X  objective function:
Within the k -th outer iteration, We use the proximal method and iteratively solve the the optimization problem (18). To distinguish from the outer iteration, we refer to the iteration as the inner iteration. , kk F denotes the Frobenius norm,  X  is the step size, and  X  ( i  X  1) denotes the solution from the last inner iteration. The following theorems gives the solution for optimization problem (18).

Theorem 1. Let a and b be two vectors of the same di-mension,  X  and x be two scalars. The univariate Link-age regularizer pen ( x ;  X , a, b ) =  X  | x |k a  X  sign ( x ) b k tinuous and convex in x . The solution x  X  for the problem min x 1 2 k x  X  u k 2 2 + p en ( x ;  X , a, b ) is given as follows:
Proof. Reformulate the function pen ( x ;  X , a, b ) as fol-lows:
Note that c 1 = k a  X  b k 1 and c 2 = k a + b k 1 are two non-negative constants. The function pen ( x ;  X , a, b ) resembles the absolute function with different slope on the negative and positive sides. It is easy to see that pen ( x ;  X , a, b ) is continuous and convex in x . An example of the function pen ( x ;  X , a, b ) is given in panel (a) of Figure 1, in which c = 2, c 2 = 0 . 5 and  X  = 0 . 5.

Let  X  X  x denote the sub-gradient of f x w.r.t. x . At the point x  X  , 0  X   X  X  x ( x  X  ). When x &gt; 0, we have  X  X  = x  X  u + c  X  = 0  X  x = u  X  c 1  X  &gt; 0  X  u &gt; c 1  X  ; When x &lt; 0, we have  X  X  x = x  X  u  X  c 2  X  = 0  X  x = u + c 2  X  &lt; 0  X  u &lt;  X  c and x = 0 otherwise. Therefore the solution can be written as follows: x  X  = g ( u ;  X , a, b ) = The solution can be simplified into (19). F igure 1: Plots of (a) the univariate Linkage regularizer, (b) the asymmetric soft thresholding function
Panel (b) of Figure 1 demonstrates an example of the function g ( u ;  X , a, b ). When u falls in the region between the two red dotted lines, a sparse solution is attained. The solution can be regarded as a soft-thresholding rule with different thresholds on the positive and negative parts. The underlying rationale is that the relative scales of c 1 and c reveal the information about whether two targets are more likely to be positively or negatively correlated. When c 1 c , two targets are more likely to be negatively correlated. The slope of the penalty function in the positive part is greater than that in the negative part, and the positive parts gets higher penalty. Similarly, when c 1 &lt; c 2 , two targets are more likely to be positively correlated. In this case, the negative part gets higher penalty. Due to the asymmetric property of the solution (20), we refer to it as the asymmetric soft-thresholding rule.

Theorem 1 gives the solution for univariate Linkage reg-ularizer. The solution for problem (18) is obtained by apply-ing the asymmetric soft-thresholding rule to all off-diagonal elements of U in (18). Algorithm 2 summarizes the steps to update  X . The following Proposition gives the properties of the solution.

Proposition 2. Let g ( x ;  X , a, b ) denote the asymmetric thresholding rule described in (20) . Problem (18) can be solved by applying g ( x ;  X , w i , w j ) element-wise to  X  i 6 = j . Under the regularity condition (5) in Bickel and Levina (2007) [4], the obtained solution is consistent with the true covariance matrix  X  .[23] A sketch of proof for Proposition 2 is given in Appendix B. To sum up, the steps to update  X  are described in Algorithm 2. In the global iteration, Algorithms 1 and 2 are applied alternatively until converge.

Note that the thresholding operator does not necessarily preserve positive definiteness even when applied only to off-diagonal elements[14]. To retain the positive definiteness of the solution, we bound the minimum eigenvalue of  X  by a small  X  &gt; 0. If the minimum eigenvalue of the obtained solution from (18) is greater than or equal to  X  , positive definiteness is preserved. If the minimum eigenvalue is less than  X  , we solve the following problem:
We use the alternating direction method of multipliers (ADMM) method[6] to solve (21). The algorithm is de-Algorithm 2 A symmetric thresholding rule to update  X  Require: W ( t  X  1 ) from last iteration, regularization param-2: for k = 0 , 1 , 2 , . . . until convergence do 4: for i = 0 , 1 , 2 , . . . until convergence do 5: Compute U according to (18) and line search for  X  6: Apply the asymmetric soft thresholding rule to off-7: end for 9: end for scribed in Appendix Algorithm 3. The choice of  X  c an be ar-bitrary, or can be calculated using the method in Appendix 2 of Bien and Tibshirani (2011)[5].
In this section, we evaluate the performance the Link-age method through synthetically generated data sets and real-world examples. We compare our method with the base-line model in which each target is modeled and predicted individually.
We conducted simulation studies in three scenarios. In the first scenario, all risk targets are continuous; in the second scenario, all targets are binary; and in the third scenario, the targets contain both continuous and binary types. We eval-uate the performance of the proposed framework through three aspects: (i) the performance of the predicted values; (ii) the performance of estimated W ; and (iii) the perfor-mance of estimated target covariance matrix  X .

Under each scenario, we generate 10 targets and 20 fea-tures. The covariance matrix  X  is set to be a block diagonal matrix with two blocks each containing 5 risks. W is gen-erated according to (2) with  X  = I . A continuous risk y j is generated from the Normal distribution N Xw j ,  X  2 = 1 . A binary risk y j is generated from the Bernoulli distribution with p = 1 / (1 + exp (  X  Xw j )). Features are generated from the normal distribution N (0 , 1). Ridge Regression and Lo-gistic regression with l 2 regularizer are used as the baseline models for continuous and binary targets, respectively. We use 10-fold cross validation to both Linkage and baseline model to select the tuning parameter(s). For each scenario, we repeat the experiment 20 times.

The relative change in the objective function (6) between two consecutive global iterations is used as the stopping cri-terion. We tested the convergence of the proposed algorithm under various settings. Based on our observation, the al-gorithm always converges and converges to the same point with different initial values. Figure 2 shows one example of the convergence of the algorithm. The x-axis denote the iteration numbers, and the y-axis denote the value of the objective function. In most cases, the algorithm converges within 5 global iterations.

Figure 2: Example of convergence of L inkage algorithm
For continuous targets, the average MSE of the predicted values across all targets, denoted as M SE p , is used to eval-uate model performance. For binary risks, the average Area Under Curve (AUC) is used as the evaluation criterion. The mean of M SE p and AUC across 20 repeated experiments under the three scenario are presented in Table 1. Their standard deviations are also reported (in parenthesis). In all three scenarios, Linkage consistently outperform the baseline model. The improvements in scenarios 1 and 2 are greater than the improvements in scenario 3. The reason is that in mixed models, selecting the step size  X  in the step of updating W is more challenging than continuous models or binary models. Therefore the improvement is mitigated.
Next we evaluate the performance of estimated W , de-noted as  X  W . The average MSE of  X  W , denoted as M SE w used as the evaluation criterion. Average M SE w across 20 repeated experiments in the three scenarios, together with their standard deviations, are demonstrated in Table 1. In all three scenarios, Linkage has better performance than the baseline model.
Next we evaluate the performance of estimated  X , denoted as  X 
 X . Similar as in the previous paragraph, the average MSE of  X 
 X  is used as the evaluation criterion. The average MSEs of  X 
 X  across 20 repeated experiments in the three scenarios and their standard deviations are summarized in the last two rows of Table 1. Linkage has lower M SE  X  than the baseline model in all three scenarios.

In the context of comprehensive risk prediction, risks usu-ally form groups. We conduct another experiment to demon-strate that the estimated  X  from Linkage is able to preserve group structures. We generated 20 targets and 50 features with 1000 observations. Targets are randomly assigned to continuous or binary type. The 20 targets are separated into 5 groups. The first four groups contain 3-6 targets. The last target set to be an  X  X utlier X  target and is not cor-related with any other targets. The heatmaps of  X   X  and the true  X  are showed in Figure 3, respectively. It is clear from Figure 3 that: (i) the Linkage can yield sparse estimation of  X , making it desirable when there exist group structures among risks; (ii) the block structure of  X  is successfully pre-served except that a few  X  X ff block X  elements are estimated as non-zero; (iii) the  X  X utlier X  target is separated from other groups of targets; (iv) the color within each block is lighter in  X   X  than that in  X . This phenomenon is expected since Linkage results in element-wise shrinkage estimate of  X .
In this section, we apply the Linkage framework to the easySHARE dataset, which is a simplified dataset from the Survey of Heath, Aging, and Retirement in Europe (SHARE). SHARE includes multidisciplinary and cross-national panel database on health, socio-economic status, and social and family networks of more than 85,000 individuals from 20 Eu-ropean countries aged 50 or over. The easySHARE dataset is a simplified dataset adapted for comparability with the US Health and Retirement Study (HRS). Four waves of inter-views were conducted during 2004-2005, 2006-2007, 2008-2009, and 2010-2011, respectively, and are referred to as WAVE1 to WAVE4 interviews. In this paper, we used the WAVE1 and WAVE2 easySHARE data.
 In total 20449 sample persons attended both WAVE1 and WAVE2 interviews. Multiple modules of interviews were conducted. The easySHARE data contains 105 variables from 13 interview modules. We extract risk targets from WAVE2 interview, and features from WAVE1 interview. Based on a literature review on comprehensive geriatric assess-ment, 15 variables are selected as risk targets. The 15 risks come from four interview modules:  X  X ocial Support &amp; Net-work X , X  X ental Health X , X  X unctional Limitation Indices X  X nd  X  X ognitive Function Indices X . In the following discussion, we refer to these four modules as the Social, Mental, Functional and Cognitive modules. Among the 15 risks, 11 are binary, and 4 are continuous. Therefore, we build a mixed type Linkage model in this section. The description of the risks, their interview modules, as well as their types, are listed in Table 2. Totally 75 features are constructed from 46 vari-ables in WAVE1 interview. The features cover a wide range of assessments, including demographics, household compo-sition, social support and network, physical health, mental h ealth, behavior risk, healthcare, occupation and income. Due to limited space, detailed description features are not listed in this paper.

Both targets and features have missing values due to non-response and questionnaire filtering. The average missing value rates in targets and features are 6 . 9% and 5 . 1%, re-spectively. Missing values in features are imputed in ad-vance using the K-nearest-neighbor method. The distance between sample i and sample j is defined as the averaged l distance of elements that are observed both in i and j . Let x ( i ) and x ( j ) denote the feature vectors of samples i and j . Let O i and O j be the sets of indices of observed ele-ments in x ( i ) and x ( j ) , and C i  X  j denote the cardinality of O  X  X  i . The distance between the two samples is calculated targets are handled using the method discussed in Section 2.3.
In Section 2, we discussed that prior knowledge on tar-get association could be incorporated through  X  0 In the easySHARE dataset, our presumption is that targets from the same interview module are related with each other. For example in the functional module, the Activities of Daily Living (ADL) index, the Instrumental Activities of Daily Living index (IADL), and the Mobility index all aim at as-sessing patients X  adequacies of performing basic daily func-tions. It is reasonable to assume that these three risk targets are correlated. On the other hand, relationship between tar-gets from different modules is unclear. Based on the above argument, the prior matrix  X  0 is set to be a block diago-nal matrix. Each block contains the risks in one interview module. The sample covariance matrix of y j  X  X  for j from the same module is used to fill the block.
In this section we evaluate the performance of the pre-dicted values and compare Linkage with the baseline model. Same as in the simulation experiments, Ridge Regression and Logistic Regression with l 2 regularizer are used as the baseline models. In Linkage , link functions for individual risks are chosen according to the types of the risks. We use 10-fold cross-validation to select the tuning parameters. Let X k train , Y k train be the training data in the k -th fold, and X test and Y k test the testing data in the k -th fold. Let  X  w be the estimated coefficient vector of the j -th target in the k -th fold. The predicted values in the the k -th fold are  X  y Predicted values from the 10 folds are combined together. For notational simplicity, we denote the predicted value of the i -th sample in the j -th risk as  X  y ij . The MSEs of pre-dicted values are calculated for all continuous targets and are showed in Table 3. The MSEs are decrease by 72-84%, in-dicating that Linkage has significantly better performance than the baseline model on easySHARE data.
 Table 3: Mean Squared Errors of continuous targets in easySHARE data
Two criteria are used to evaluate the prediction perfor-mances for binary targets. First, we evaluate the prediction accuracies from Linkage and the baseline model. The pre-diction accuracy of a target is defined to be the proportion of correctly predicted samples among all observed samples, i.e. and the overall prediction accuracy of a model is defined to be the average prediction accuracies across all binary tar-gets. The overall prediction accuracy from Linkage and the baseline models are 0 . 8192 and 0 . 7513, respectively. Link-age improves the baseline model by approximately 9%. Prediction accuracies of individual targets are depicted in panel (a) of Figure 4. Linkage outperforms the baseline model in 10 out of 11 targets. Furthermore, we plot the im-provement of prediction accuracies in Linkage versus the prediction accuracies from the baseline model. The scatter plot is showed in panel (b) of Figure 4. The plot demon-strates a negative relationship between the improvements and the prediction accuracies from the baseline model. Risks which have relatively low prediction accuracies from the baseline model (e.g. sleep, pessimism) achieve greater im-provements. On the other hand, risks that already achieve high prediction accuracies from the baseline model (e.g. ap-petite, orientation to date) only receive minor improvements or do not improve upon the baseline model.
Second, we generate the Receiver Operating Character-istic (ROC) curves from Linkage and the baseline model, and we calculated their corresponding AUCs. To evaluate the overall performance of Linkage , we combine all binary risks together and generate the overall ROC curves. The curves are showed in Figure 5. The overall AUCs from the baseline model and Linkage are 0 . 7578 and 0 . 8436, respec-tively. Linkage gets approximately 9 . 5% improvement from the baseline model. AUCs of individual risks are showed in Table 4. Although the magnitude of improvements in AUCs are less significant than that in prediction accuracies, AUCs from Linkage are higher than the baseline model in all 11 binary targets. The improvements in AUC also demonstrate a negative relationship with AUCs from the baseline model. Together with the results in previous paragraph, the obser-vations indicate that by leveraging the relationship between targets, the risk targets that have poor performance from baseline model can be significantly improved.
In this section we discuss the estimated risk association matrix  X  from Linkage . The estimated covariance matrix is denoted as  X   X . For fair comparison between risk targets, the covariance matrix is transformed to correlation matrix. Let  X  R denote the estimated correlation matrix, and  X  R ij correlation coefficient between risks i and j .  X  R ij ranges from  X  1 to 1. The sign of  X  R ij represents the direction of risk association, and the magnitude of  X  R ij represents the strength of the association. We perform a hierarchical clus-t ering on the 15 risks using 1  X   X  R as the distance measure. The heatmap of  X  R is showed in Figure 6. Both the rows and columns in Figure 6 are reordered from the hierarchical clustering. The row labels (on the right) are the risk names, and the column labels (at the bottom) gives the interview modules of the risks.

Figure 6 clearly indicates that targets form groups. Using half of the maximum distance as the cutoff, the 15 risks are separated into three groups. The first group (on the bottom left of Figure 6) consists of three targets from the Cognitive module. The second group consists of three targets from the Functional module and one target (appetite) from the Mental module. The two groups are negatively correlated. Note that the direction of the between group correlation is related with the how the the risk targets are coded. In the easySHARE dataset, higher functional values indicate more difficulties with functional activities, whereas positive cogni-tive values indicate better cognitive abilities. Therefore, the negative correlation actually indicates that that functional and cognitive abilities are positively correlated. The remain-ing eight targets form the third group. In general, targets in the third groups are not correlated with each other. How-ever, targets in the third group could be correlated with targets in the first two groups. For example, targets in the Mental module (sleep, depression, irritability and concen-tration) are correlated with appetite, which is also from the Mental module.
In this paper, we propose a framework called Linkage for comprehensive risk prediction. Two goals, predicting multi-ple risks and learning the relationship between risk targets are done simultaneously. The newly proposed Linkage reg-ularizer allows the coefficient matrix W and the target co-variance matrix  X  to reciprocally leverage information from each other. Therefore, achieve better performance. Link-age is convex in both W and  X  when the other part is fixed. We developed an alternating method to solve the framework and prove that under certain regularity condi-tion, the solution has good properties. We have conducted extensive evaluations on synthetic and real datasets. The results show that the proposed framework can improve the estimate of coefficient matrix and risk prediction, and at the same time has good performance in learning and under-standing the relationship between risk targets.
 [1] R. K. Ando and T. Zhang. A framework for learning [2] A. Argyriou, S. Cl  X emen  X con, and R. Zhang. Learning [3] A. Beck and M. Teboulle. A Fast Iterative [4] P. J. Bickel and E. Levina. Covariance regularization [5] J. Bien and R. J. Tibshirani. Sparse estimation of a [6] C. S. Boyd, N. Parikh, E. Chu, B. Peleato, [7] J. Chen, L. Tang, J. Liu, and J. Ye. A convex [8] J. Chen, J. Zhou, and J. Ye. Integrating low-rank and [9] X. Chen, Q. Lin, S. Kim, J. G. Carbonell, and E. P. [10] T. Evgeniou and M. Pontil. Regularized multi-task [11] J. Friedman, T. Hastie, and R. Tibshirani. Sparse [12] A. R. Goncalves, P. Das, S. Chatterjee, V. Sivakumar, [13] P. Gong, J. Ye, and C. Zhang. Robust multi-task [14] D. Guillot and B. Rajaratnam. Functions preserving [15] A. Gupta and D. Nagar. Matrix Variate Distributions . [16] D. R. Hunter and R. Li. Variable selection using mm [17] L. Jacob, F. Bach, and J.-P. Vert. Clustered multi-task [18] S. Kim and E. P. Xing. Tree-guided group lasso for [19] J. Liu, S. Ji, and J. Ye. Multi-task feature learning via [20] Y. Liu, A. Wu, D. Guo, K.-T. Yao, and [21] Y. Nesterov. Smooth minimization of non-smooth [22] P. Rai, A. Kumar, and H. D. III. Simultaneously [23] A. J. Rothman, E. Levina, and J. Zhu. Generalized [24] Y. Zhang and D. Yeung. A convex formulation for [25] Y. Zhang and D.-Y. Yeung. A regularization approach [26] J. Zhou, J. Chen, and J. Ye. Clustered multi-task Algorithm 3 A lternating Direction Method of Multiplier Require:  X  ,  X  , step size  X  . 1: Initialize  X  (0) =  X  ( k ) from Algorithm 2 and Z (0) = 0 2: If the minimum eigenvalue of  X  (0) &lt;  X  , proceed: 3: for i = 1 , 2 , . . . until converge do 5:  X  ( i ) = T D  X  T  X  , where D  X  = diag { max ( D jj ,  X  ) } 8: end for
Wi thout loss of generality, we assume that k w i  X  w j k is less than k w i + w j k 1 . The asymmetric thresholding rule satisfies the following conditions for all x  X  R : (i) | g ( x ;  X , w i , w j ) | X | x | (ii) | g ( x ;  X , w i , w j ) | = 0 for | x | X k w i  X  w j (iii) | g ( x ;  X , w i , w j )  X  x | X k w i + w j k 1
Therefore, the thresholding rule (18) belongs to the class of generalized thresholding rules in Rothman, Levina and Zhu (2008)[23]. Based on Theorem 1 in [23] and Theorem 1 in Bickel and Levina (2008)[4], the solution  X   X  is consistent with  X . For detailed proof, please refer to [23] and [4].
