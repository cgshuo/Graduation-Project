 1. Introduction This paper describes our research for INEX 2003 (the Initiative for the Evaluation of XML Retrieval). We participated with the TIJAH XML-IR retrieval system, a research prototype system is its layered design, following the basic system architecture of relational database management systems.

T raditional information retrieval systems represent a document as a  X  X ag-of-words X . In-documents. In the case of structured documents however, we think designing the retrieval system following  X  X he database approach X  is best to keep the more complex data represen-tation manageable.

The main characteristic of the database approach is a strong separation between con-ceptual, logical and physical levels, and the possibility of using different data models and 548 LIST ET AL. query languages at each of those levels (Tsichritzis and Klug 1978). In relational database systems, a significant benefit of this data abstraction (through the separation between the levels in database design) is to enable query optimization. A SQL query (a  X  X alculus ex-physical query plan. The physical algebra exploits techniques like hashing and sorting to improve efficiency (Graefe 1993).

Fo r XML-IR systems, following this separation in layers gives another, additional advan-of probabilistic techniques handling structural information is simplified, and kept orthog-onal to the rest of the system design. Section 3 details our approach, based on a region algebra extension for supporting ranked retrieval.

The TIJAH system has been used for participation in the Initiative for the Evaluation of XML Retrieval (INEX). INEX offers an XML document collection of approximately 500 MB, a set of topics, relevance assessments and a collection of evaluation measures. The document collection consists of IEEE scientific articles of various research fields from the years 1995 to 2002. In 2002 and 2003, the topics were created by the participating groups in accepted final topics. Relevance assessments were done on two dimensions: e xhaustivity and specificity . Exhaustivity describes the extent to which a retrieved component discusses focuses on the topic of request. Both dimensions have a four-graded scale.
 Three types of search tasks were defined for INEX 2003: Content-Only (CO), Strict Content-and Structure (SCAS) and Vague Content-and-Structure (VCAS). The CO task focused on retrieval in the XML collection without imposing any structural constraints. The SCAS and VCAS tasks were employed for queries that consider both structure and structural constraints could be relaxed.

Our paper is organized along the layers of the TIJAH system design. Section 2 describes the query language used at the conceptual level, identifies three patterns in the INEX topic for the about function. Section 3 introduces the score region algebra (SRA) and explains how the three query patterns are expressed at the logical level. Section 4 explains how the representation of the document collection using a  X  X umbering scheme X . We conclude with a discussion of experiments performed with our approach for the three INEX search tasks. 2. Conceptual level The INEX query language extends XPath with a special about function, ranking XML elements by their estimated relevance to a textual query. As such, the invocation of the about function can be regarded as the instantiation of the retrieval model.
 TIJAH: EMBRACING IR METHODS IN XML DATABASES 549 2.1. Retrieval model applying the language modeling approach to information retrieval to the INEX retrieval tasks, see e.g. (List and de Vries 2003, Hiemstra 2003). The exhaustivity dimension of relevance seems to be captured rather intuitively in the language modeling approach. It estimates relevance using a foreground model and a background model (both probability document component 1 D j , and P ( T i ) denoting the probability of the term T i in general in which n is the query length. The linear combination of the probabilities is also known as smoothing, and is done to avoid the sparse data problem (Hiemstra 2001). We want to av oid assigning a value of 0 to the entire product of probabilities, when a single term T i does not occur in the document component D j (the foreground probability value for term i will then be 0).
 based on specific collection statistics and are defined as maximum likelihood estimators. In the following equations, variable t ranges over the term domain, i.e., all terms in the the foreground probability, we use a well-known estimator based on the term frequency (measuring how many times a term T i occurs in a document component D j ): Fo r estimating the background probability, common estimators are collection frequencies (measuring in how many unique documents the term occurs). Document frequencies are actually document component frequencies in our case, measuring in how many unique document components a term occurs.
 The language model scores can be adjusted by a document component prior. The  X  X tandard X  components have a higher a priori probability of containing relevant information, simply due to their length: 550 LIST ET AL.

Note that this  X  X tandard prior X  rewards long document components, which might not periments presented in Section 5. The specificity dimension of relevance might be better We speculate here that the characteristics of the standard-log-normal distribution reflects and consequently, are likely to not be satisfying retrieval units. On the other hand, very long document components require much more effort from the user in locating the relevant pieces of information, which could be considered unsatisfying as well. The characteristics the likelihood that very short document components are relevant, while the long distribution tail reflects that we do not expect long document components to be very focused on the topic of request. The standard-log-normal length prior is defined as follows, where | D j | denotes the length of the document component D j .
 2.2. The CO task Retrieval Status Value of the document component D j ): We used the scoring functions in our three originally submitted experimental scenarios. Scoring function RSV 1 wa s used for two retrieval scenarios:  X  fixed retrieval, where the retrieval unit was restricted to article components;  X  Fo r additional experiments, we created three other scoring functions:
Finally, comparison of the evaluation results between our TIJAH system and the system TIJAH: EMBRACING IR METHODS IN XML DATABASES 551 difference. Since both systems are based on the same language modeling technique, we decided to also experiment with their approach, using evidence from the surrounding article component; the score of each component is smoothed with the score of its ancestor article node. We refer to the article-weighted versions of RSV 2 and RSV 3 as RSV 6 and RSV 7 respectively. 2.3. The SCAS and VCAS tasks Fo r both the SCAS and VCAS tasks, we introduced the concept of patterns . Since we lacked an automatic query processing mechanism at the time of evaluation, we processed the queries manually (but in a mechanic fashion). Inspection of the topic set highlights three repeating query patterns, shown in Table 1. Processing these INEX query patterns takes place in two steps:  X  classify the query into (a sequence of) three basic query patterns (shown in Table 1);  X  create a query plan to process the queries. The query patterns are visualized in Figure 1. The basic pattern for all XPath based queries is the single location step ,a s defined in 552 LIST ET AL. other words, a path leading to a certain location (or node) in the XML syntax tree. The XPath location steps may also apply (Boolean) predicate filters, e.g., selecting nodes with a particular value range for year ( yr element node in INEX collection).

P attern 1 is considered to be the elementary pattern for processing. The two other (more the nodes to be retrieved, ranked by an about e xpression over a node-set reached by a second location step ( axp ). For pattern 1, no distinction is made between SCAS and VCAS Section 3.2). 2.3.2. Pattern 2. P attern 2 distinguishes between the VCAS and the SCAS tasks, where we argue for the vague query scenario that absence of specified descendant steps does not render the requested (ancestor) target nodes irrelevant completely.

P attern 2 for VCAS. Fo r pattern 2 in the VCAS scenario, we assume that conjunctions In case node-sets axp1 and axp2 are equal, the pattern is rewritten to a pattern 1. If the node-sets axp1 and axp2 are not equal, it is possible these node-sets represent completely different parts of the (sub)tree below xp ,a s depicted in Figure 1.

Consider the following expression: If an article contains no abstract, but it does score on  X  X ML data X  in one or more of the scenario this might not be the case. Therefore, we decided to process these expression types results of the individual pattern 1 executions. Also, note that we use the terms and phrases of all separate about clauses together in each instance (experiments have shown that this gives better average performance on the INEX topics). So, the example above is split into the following two pattern 1 expressions: -//article[about(.//abs,  X  X nformation retrieval XML data X )] -//article[about(.//sec,  X  X nformation retrieval XML data X )] Both subpatterns are processed as pattern 1. The two resulting node-sets are combined for a final ranking (see Section 3.2).

P attern 2 for SCAS. If the (sub)tree starting at xp does not contain both paths axp1 and TIJAH: EMBRACING IR METHODS IN XML DATABASES 553 SCAS and VCAS scenarios for pattern 2 is the requirement that all of the descendant nodes present in axp1 and axp2 are contained in the context of the xp target nodes. 2.3.3. Pattern 3. Similarly to pattern 2, the two CAS scenarios are treated differently for pattern 3.

P attern 3 for VCAS. P attern 3 can be processed like pattern 2, except for the fact that into multiple instances of pattern 1: -xp[about(axp1,  X  X 1 | p1 t2 | p2 X )] -xp//xp2[about(axp2,  X  X 1 | p1 t2 | p2 X )] The pattern 1 execution already provides for aggregation of scores of a set of nodes of the same type, within a target element. The question remains however how to combine the scores of the nodes present in node-sets xp//axp1 and xp//xp2//axp2 . Like before, these node-sets can represent nodes in completely different parts of the (sub)tree.
Based on the observation that the user explicitly asks for the nodes present in the xp//xp2 predicate reduces node-set xp to those nodes for which a path axp1 e xists. For the vague scenario however, we argue that absence or presence of axp1 does not really influence target element relevance.

Summarizing, the first about predicate in the pattern mentioned at the start of this sub-section is dropped, rewriting the resulting pattern to a pattern 1 instance: This results in the following execution strategy for pattern 3 under the VCAS scenario: remove all about predicates from all location steps, except for the about predicate on the target element, and process it as a vague scenario for pattern 2.

P attern 3 for SCAS. The processing of pattern 3 for the SCAS scenario is stricter in the sense that we can not simply drop intermediate about predicates, as we did for the VCAS scenario. The general procedure consists of:  X  splitting up the pattern into separate location steps (similar to pattern 3 VCAS scenario);  X  structural correlation (using descendant or ancestor axis tests) of the resulting node-sets of each location step.
 The target elements for each sub pattern are ranked by their corresponding about predicate.
As an example, consider the following expression: We first split up the above expression into: 554 LIST ET AL. -//article[about(.//abstract,  X  X 1 | p1 t2 | p2 t3 | p3 X )] -//article//section[about(.//header,  X  X 1 | p1 t2 | p2 t3 | p3 X )] -//article//section//p[about(.,  X  X 1 | p1 t2 | p2 t3 | p3 X )]
All of the patterns above produce intermediate result node-sets that have to be structurally correlated to each other. Furthermore, the ranking results have to be combined (propagated) 3. Logical level P atterns recognised at the conceptual level are translated into physical query plans via an intermediate step, the logical level. We have based the logical query algebra and data model on a region algebra extended to support ranked retrieval at the logical level. The region algebra concept was introduced by Salminen and Tompa (1992) and Burkowski (1992) for searching in structured documents and supporting search and ranked retrieval in text dominated databases, respectively. It has been extended to support overlap between regions by Clarke et al. (1995), with operators for proximity search by Baeza-Yates and Navarro (1996), and with direct inclusion (parent-child relation) and  X  X oth included X  operators by Consens and Milo (1995). Jaakkola and Kilpelainen (1999) introduced nesting properties in region algebra, while Miller (2002) used the region algebra approach for processing of presented in Masuda (2003) and Masuda et al. (2003).

The basic idea behind the region algebra approaches is the representation of text docu-application of the idea of text extents to XML documents is straightforward. Regarding each XML document instance as a linearized string or a set of tokens (including the document that entirely linearized string. Figure 2 visualizes an example XML document (as a syntax tree) with the start point and end point numbering for the nodes or regions in the tree. As an example, the bdy -region corresponds to the (closed) interval [5 .. 24]. 3.1. A score region algebra and data model Since we focus on XML documents and want to support as much as we can from the W3C XML query languages and IR query languages, we decided to use the rich information set of the XML data model (Cowan and Tobin 2004) as a base for the definition of our score region algebra data model. Furthermore, given an expressive data model we would be able to distinguish between different information items in XML (see Cowan and Tobin 2004) and to further extend the algebra with new operators and concepts for ranked retrieval. As we enriched the region definition with a score concept, we named the algebra  X  X core region algebra X , or SRA in short.

The XML data model consists of element, text, comment, and processing instruction nodes, as well as attribute information and text node content. Although XML documents might be considered as graphs because of ID/IDREF typed attributes, we will simplify the TIJAH: EMBRACING IR METHODS IN XML DATABASES 555 XML structure and treat these entities as they are organized in a hierarchical (tree-like) structure. Regarding the complexity of the XML data model we could not directly apply any of the previous region algebra approaches for defining the logical data model.
In the specification of the SRA data model, we have to distinguish between the different node types in XML documents to provide a uniform platform for defining the region algebra operators. To be able to represent XML properly, we extended the definition of a region. The logical data model is based on re gion sets , where each region is defined as follows: Definition 1 . The SRA data model is defined on the domain R which represents a set attribute X  t , and region score attribute X  p .R e gion start and end attributes must satisfy ordering constraints ( e i  X  s i ).

The semantics of region start and region end attributes are the same as in other region algebra approaches: they denote the bounds of a region. The region name attributes are used to denote node names, content words, attribute names, attribute values, etc. To distinguish between different name  X  X oles X  in XML we used the region type information item. We used node, etc. Finally, the region score information item is used to specify the relevance score of a region with respect to a given query.

In SRA, the result of the application of operators cannot introduce new regions, that is, regions with different region bounds (i.e. s and e v alues) than originally specified in the database. Furthermore, the operators cannot change the name and type ( n and t )o fr e gions 556 LIST ET AL.
 identified in the database. However, algebra operators are allowed to change the value of a side-effect, which distinguishes it from other proposals (specifically, Burkowski (1992) and Masuda (2003)). 3.2. SRA operators Here we will define the score region algebra operators. SRA operators are based on the operators specified in the previous region algebra approaches, extended to support the SRA data model and enable ranked retrieval in XML documents.
 ( ), not containing ( ), contained in ( ), not contained in ( ), region set intersection corresponding non-capitals to denote regions in these region sets ( r i ), and corresponding two region sets as operands and produce a region set as result. We introduced a selection Leaving the selection criterion for one of the attributes unspecified corresponds to a wild-their name attribute.

New region algebra operators are defined to enable score computation and ranking regions based on the computed scores, as depicted in Table 3. Four complex scoring functions are introduced in the additional SRA operators: ( f , f , f , and f ), as well as two abstract operators (  X  and  X  ). The exact specification of these operators is given on the physical r contains or does not contain a term, or a set of terms present in R 2 .I n other words, they should define the retrieval model used.
 ing or contained regions, respectively. They specify whether the propagation is performed with or without normalization. Normalization can for example be based on the region size, TIJAH: EMBRACING IR METHODS IN XML DATABASES 557 or on the number of regions in a region set. The abstract operators  X  and  X  specify the combination of scores in AND and OR expressions respectively (e.g., taking the minimum or the product score for AND, or the maximum or the average score for OR).

Expressing query plans using the operators given in Tables 2 and 3 preserves data inde-enable the separation between the structural query processing and the underlying proba-bilistic model used for ranked retrieval: a design property termed content independence in De Vries (2001). The instantiation of these probabilistic operators is implementation depen-dent and does not influence the global system architecture. This gives us the opportunity to change or to modify the existing model while keeping the system framework, creating the opportunity to compare different probabilistic models with minimal implementation effort. 3.3. Pattern representation in SRA and selection operators). Optionally, the operator also processes predicate tests on node or attribute values specified in the XPath expression. The current SRA data model does not support the child axis step and it treats it as a descendant step. Notice however, that this simplified query model is consistent with the query language proposed in Trotman and O X  X eefe (2004), which forms the basis for the query language to be used at INEX 2004.
To be able to express predicate steps, extra selection criteria are introduced in the select operator, enabling numeric value comparison on SRA name concepts. These criteria are  X  ,  X } .F or example, using the selection operator, XPath expression xp [ .// yr  X  1999] can be expressed in the score region algebra as: where C represents the set of all regions in a database. 558 LIST ET AL.

T able 4 gives the probabilistic region algebra expressions corresponding to the INEX query patterns identified before. Pattern 1 distinguishes between term ( tm ) and phrase ex-P atterns 2 and 3 are rewritten into several interrelated instances of pattern 1. The choice between these options is made at the conceptual level. Similarly, tp2 denotes either tm 2 | pe 2o ra combination of tm 2 | pe 2 and tm 1 | pe 1. 4. Physical level The physical level of the TIJAH system relies on the MonetDB binary relational database k ernel (Boncz 2002). This Section details the implementation of the physical region algebra operators and the execution strategy for each of the patterns. 4.1. Physical data model XML text regions are encoded at the physical level using a tree encoding scheme, follow-ing (Grust 2002, Grust and van Keulen 2003). Regions are stored as five-tuples ( s , e  X  s i and e i represent the start and end positions of XML region i ;  X  n i is the (XML) tag of region i ;  X  t i is the type of region i ;  X  The set of all XML region tuples is named the node index N . Information about other types fragmentation of the region table. Yet, since these tables are not needed for the retrieval TIJAH: EMBRACING IR METHODS IN XML DATABASES 559 e xperiments, these are not further considered in paper. Terms present in the XML documents considered regions with a length of 1, but storing them separately reduces memory usage by not having to materialise the end positions for the word index. The physical layer has been extended with the physical text region operators shown in Table 5. Boolean predicate De Vries et al. (2003) and List and de Vries (2003). 4.2. The retrieval model ment the about function defined in Section 2. The context region in which we perform frequency counts is denoted by r , while the set R specifies the set of term regions. Since terms are considered regions themselves, a term selection over the word index W gives a region set as result, i.e., R =  X  n = term ( W ).
  X  represents the smoothing parameter for the inclusion of background statistics,  X  is the combination parameter for article weighting, and  X  is the mean value for the log-normal prior.

These auxiliary functions are implemented using two physical operators: the size operator of regions in a region set R .

Function tf ( r , R ) computes the term frequency for terms in R . The term frequency of region set R (i.e., a set of term positions) in context region r is computed as: Function cf ( R ) computes the collection frequency of R as follows: 560 LIST ET AL. where R oot represents the entire XML collection, i.e., the region that is not contained by any other region in the collection.

The document frequency function computes the document frequency for terms present in the region set R as follows 2 : log-normal length prior ( logn ).

Finally, function tf ( r , R ) computes the term frequency in a surrounding document: through the combination function g , which is defined in terms of these auxiliary functions and parameters:
We can now implement the retrieval models presented in Section 2.1 at the physical level, by instantiating combination function g with the right auxiliary functions. So, the instantiation of the combination function determines the actual retrieval model used. Note that other retrieval models may require the extension of the physical level with different auxiliary functions, e.g., to support other frequency measures.
 tainment operators and operators size and count . Thus, f ( r 1 , R 2 )i s defined using the Denoting the result of the application of the containment expression by Y = R 2  X  r 1 , the scoring function f ( r 1 , R 2 )i s defined as: where p 2 ( y )i s the score value of region y  X  Y . 4.3. Patterns for the SCAS and VCAS tasks 4.3.1. Pattern 1. Processing pattern 1 (see Table 1) consists of two basic steps: relating node-sets xp and axp to each other, and processing the about operator. Node sets xp and TIJAH: EMBRACING IR METHODS IN XML DATABASES 561 axp must have a ancestor X  X escendant structural relationship. Processing is the same for the VCAS and SCAS tasks.

The pattern is processed as follows:  X 
Determine the correct axp node-set for ranking. On the physical level, this is done by ex ecuting a containment join between the node sets xp and axp : axp  X  xp . The result of this containment join is cxp or the set of those nodes of axp which are contained within nodes in xp ;  X 
Perform the about operation on the nodes in cxp (the combination of p , p , and operators on the logical level) using a combination of functions explained in the previous subsection;  X  Return the ranking for the xp node-set, based on the rankings of the nodes present in cxp . for a single xp node (for example, multiple sections within an article). The experiments in Section 5 apply varying aggregation functions (maximum, average, and weighted normalized sum) to compute the final score for the xp node in question. This step is the physical equivalent of the logical operator. 4.3.2. Pattern 2.
 subpatterns are processed as pattern 1. The two resulting node-sets need to be combined for a final ranking. We have chosen two implementations for the combination function represented with the operator on the logical level: (1) taking the minimum of the (non-zero) scores, or (2) taking the product of the scores for corresponding regions.
Similarly, for the operator we used (1) the maximum of scores, or (2) the av erage of scores for corresponding regions.
 SCAS. Fo r the SCAS scenario, all of the descendant nodes present in axp1 and axp2 need to be present in the context of an xp node. In path-based terms: if the path xp does not contain both a path axp1 and a path axp2 , the path xp cannot be relevant. We filter out those xp paths, not containing both the axp1 and axp2 paths. This additional filtering step and the choice of the implementation of abstract operators, (minimum or product) and (maximum or average), define together the difference between strict and vague scenarios. 4.3.3. Pattern 3.
 VCAS. P attern 3 has been transformed at the conceptual level into an instance of pattern 2.
Therefore, it is processed like pattern 2, with the only difference that the target element lies deeper in the tree.
 SCAS. Fo r the strict scenario however, pattern 3 is divided into a number of subpatterns that have the form of pattern 1 or pattern 2. As explained in Section 2 these subpatterns produce intermediate result node-sets that have to be structurally correlated to each other sequence, or a bottom-up correlation sequence consisting of containment joins on these 562 LIST ET AL. result node-set. In the current implementation, the patterns are always processed top-down. Yet, the choice between a top-down or bottom-up sequence is really an optimization collection contains many paragraph elements, not contained within article elements, the system should decide to limit the amount of unnecessary executed about predicates by choosing a top-down approach.
 element by multiplication with the score of the target element. 5. Experiments This section describes the experiments performed using the TIJAH XML-IR system and (version 1.4), using the topic set of INEX 2003 (relevance assessments version 2.4). To find performed a number of runs for both the content-only (CO) and the content-and-structure (CAS) topics. Note that for the CAS topics we only include the experiments done with the strict v ariant (SCAS). Relevance judgements and measures for the vague v ariant (VCAS) are still under development, so we cannot report upon the effectiveness of our system for this task. 5.1. CO task experiments Fo r the CO task, we designed three original experimentation runs, using the two scoring baseline run of fixed  X  X lat-document X  retrieval, where the retrieval unit was limited to the article level. The second run regarded all subtrees in the collection as separate documents ( R comp ). The third run applies the log-normal distribution to model the specificity dimension ( R comp -logn ). Experiments for INEX 2002 showed that 2516 words was the average document function used in INEX 2002). Table 6 summarizes experimental results, where MAP stands TIJAH: EMBRACING IR METHODS IN XML DATABASES 563 for mean average precision . The official CO-runs use the keywords present in the keyword-element of each topic. Before executing each topic, query stop words were removed using the SMART stop word list, and all remaining keywords were stemmed with the Porter stemmer. Stop word removal (using the SMART stop word list) and stemming was also performed on the indexed collection terms, as well as the removal of terms shorter than two characters or longer than 25 characters.
 and keyword ( K ) component text. We then made combinations of the T , D and K key-w ord sets, and used the combinations in additional runs ( TD and TK ). Second, we create CO-runs where we replaced the log-normal element length prior ( logn runs) with a stan-dard element length prior ( ls runs). Finally, we experimented with the article weighting ( a w runs). From the average precision values in Table 7, the following observations are clear:  X  large elements should not be discounted (under the current metrics of evaluation; differ-ence between logn and ls runs);  X  combining element scores with their surrounding context scores appears to improve performance significantly ( a w runs);  X  results (comparing columns T and TD ).

We hypothesize that the difference in effectiveness observed between the logn and ls runs, base; refer to Kazai et al. (2004) for more details. Another explanation could be that the log-normal X  X  mean value of 2516 words as desired component size is not appropriate for the 2003 relevance assessments. 564 LIST ET AL. 5.2. SCAS task experiments The main goal of the experiments performed for the strict CAS retrieval task was to evaluate implementations of the complex functions ( f , f , f , and f ) and the abstract operators (  X  and  X  ).

Fo r each topic, all terms occurring in the separate about function calls present in the Then, the about function was executed with this term set on the set of all component in N . Processing concludes with resolving the structural constraints expressed in the query. For scoring the components, the scoring function RSV 1 wa s used with the  X  parameter set to 0.15.

One of the advantages of the pattern approach taken for the CAS queries is that we can easily modify the implementation of the algebra operators that define the combina-tion of different subqueries within a query. Therefore, we decided to study the following combination options:  X  the aggregation mechanism used to score a target element containing multiple ranked sub-elements in pattern 1 (operator );  X  the processing of AND and OR operators in pattern 2 (operators and );  X  the combination of the different sub-queries for pattern 3 (operator ).
 The first run, R SCAS orig , uses the minimum and the maximum to process the AND ( ) and OR ( ) operators respectively. The av erage mechanism is used to give a score to the target element when it has multiple scored descendants. The first about clause of Pattern 3 is more intuitive product and avg for AND and OR operators. The comparison of results obtained from these two runs is depicted in Table 8.

T able 9 summarizes the results obtained with various scenarios for handling multiple ranked descendants within the target element. To normalize the sum of multiple ranked target elements. It is interesting to note that, whatever the processing of the AND and OR operators, the run using  X  X ax X  outperforms the  X  X vg X . Apparently, high scores from the language model indicate relevance more reliably than intermediate scores. An alternative TIJAH: EMBRACING IR METHODS IN XML DATABASES 565 scored element over multiple less highly scored elements. Observe also that the weighted normalized sum is not working as well as we expected. Seemingly, the users do not agree in our way of penalizing document sizes. The users would prefer again documents that contain w ould not mind if the element containing the relevant information is very small as far as it is highly scored. On the other hand, we observed in additional runs (not in the table), (MAP 0.3069). Further research will include to map the modelling of this behaviour into our model.

Finally, to study the combination of the different sub-queries for Pattern 3, we did a new elements scores helps indeed to improve the performance of the system. 5.3. Lambda estimation The following experiments investigate the influence of smoothing parameter  X  on the effec-tiveness results. Note that this estimation has not been performed to validate or invalidate ah ypothesis; we just investigate a posteriori how sensitive the system results are for the right setting, given the collection, topicset and evaluation metric used in this study.
We performed two sets of runs. The first set consists of article retrieval with scoring functions RSV 1 , RSV 3 , RSV 4 , and RSV 5 . The second set of runs consists of component has been varied between the values 0 and 1, and the MAP values for each value of  X  are 566 LIST ET AL. computed. The value for  X  has been incremented by 0.05 in each step. Evaluation of the runs was done with the evaluation programs offered by the INEX initiative. The results of both sets of runs are shown in Figures 3 and 4.

The MAP values for article retrieval in Figure 3 show behavior comparable with previ-ously reported behavior on the Cranfield and TREC collections (Hiemstra 2001). Also, as in than the non-length prior modified ones (although the differences are not as large as those reported for the Cranfield and TREC collections). The performance seems rather stable TIJAH: EMBRACING IR METHODS IN XML DATABASES 567 across a large interval of values for the smoothing parameter. This phenomenon seems to be collection-independent, as the same was reported for the Cranfield and TREC collections.
The MAP values for component retrieval, shown in figure 4, exhibit more interesting component retrieval, the length-modified runs score much better than the non-length prior modified runs. As with article retrieval, we also see an interval where performance does not change radically.

We seek an explanation for the difference in performance between runs without length priors and those with a length prior in the fact that the language model without length prior will overestimate the relevance of (very) short document components that contain query terms, therefore outranking the larger (but usually preferable) components. Again, the influence of the artificially enlarged recall base could provide an additional reason. 5.4. Efficiency Besides measuring the effectiveness of our retrieval system, we also measured the efficiency of indexing and querying the collection. Table 11 shows the average topic execution times in that given run (with CO runs having 36 topics and the SCAS and VCAS runs having 30 topics). All measurements are wallclock timings, measured in seconds. The hardware used for the executions of the runs is an AMD Opteron machine, running at 1.4 GHz and having 2G Bo f main memory. The indexing time is divided into two separate parts:  X  the time needed for insertion of data T insert , measured at 176 seconds;  X  the time needed for post-processing T postprocess , measured at 191 seconds. Post-processing consists of determining collection frequencies, component text lengths (com-ponent lengths disregarding markup) and indexing of topics.
 568 LIST ET AL.
 Memory use of our system varied between 250 MB and 1 GB, where 1 GB was reached when materializing large components, or large component sets (large with regard to the number of components in the result set) for executing the language model. Moreover, memory use wa s increased by behavior of the database kernel used: the kernel loads tables completely memory use as a result of loading irrelevant data can be avoided by, for example, horizontal and ls runs (when compared to the comp run) can be explained by extra join-operations against parts of the index, needed for retrieving the component text lengths and calculation containment joins needed to resolve the specified structural constraints. Each of the SCAS runs discussed in the previous subsection took approximately 50 seconds (wallclocktime) to ex ecute.
 system indexes the full XPath (in string format) for each component in the collection. This full XPath indexing is redundant and can be replaced by a facility to resolve the component XPaths when presenting results to the user, or by a more compact index structure. Second, we are looking into possibilities for encoding other parts of the index into more compact structures, e.g., bitvectors. 5.5. Discussion The main goal of the experiments has been to test the  X  X roof of concept X  of our retrieval system. The main advantages of such a layered approach mentioned in Section 1 are data abstraction and content abstraction .W e consider our proof of concept partially successful. Defining and implementing new experimental scenarios, including the context weighted v ersion of the retrieval model, has proved straightforward.

However, after studying this paper, the skeptical reader of this paper might wonder about physical layers may seem rather one-on-one. And, the data models used on the logical and physical levels do not differ that much, so this mapping might as well be performed from conceptual to physical layer without intermediate step.

Fo r our INEX 2002 and 2003 experiments, this has been somewhat true; but, of course, homogeneous collection. The notion of a structured document also allows for other types of retrieval to be performed with the TIJAH system. We think that handling heterogeneous collections, even of different media types, would emphasize more the role of the logical layer in query processing. For example, the generally accepted structure of a video divides a video stream into scenes, which are composed of shots, and the shots themselves are composed of frames. Such video documents possess document structure (albeit simple), and, user annotations (MPEG-7) would introduce an even more meaningful structure for the set of videos.
 TIJAH: EMBRACING IR METHODS IN XML DATABASES 569 6. Conclusions and future work Our experience with the TIJAH system at the INEX evaluations can be considered a suc-cessful exercise in applying current and state of the art information retrieval technology to a database consisting of structured documents. We described our system architecture, aimed at simplifying the implementation of advanced retrieval models for the combination of structure and content in XML documents.
 We e xpect to take further advantage of the obtained flexibility in our future research. Fo r, the current approach to retrieval has only used a limited proportion of the wealth of structural information present in XML documents. Also, we aim to improve the efficiency of the system, both memory and CPU wise, by applying horizontal fragmentation and encoding of data into more compact structures. Longer term research plans include handling a wider v ariety of structured documents. More specifically, we will deploy the TIJAH system in our ongoing video retrieval research.
 Acknowledgments The authors are grateful to the Netherlands Organization for Scientific Research (NWO), for funding the research described in this paper (grant number 612.061.210). Notes References 570 LIST ET AL.

