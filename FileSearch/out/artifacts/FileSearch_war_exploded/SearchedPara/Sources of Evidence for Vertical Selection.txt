 Web search providers often include search services for domain-specific subcollections, called verticals , such as news, images, videos, job postings, company summaries, and artist pro-files. We address the problem of vertical selection , predict-ing relevant verticals (if any) for queries issued to the search engine X  X  main web search page. In contrast to prior query classification and resource selection tasks, vertical selection is associated with unique resources that can inform the clas-sification decision. We focus on three sources of evidence: (1) the query string, from which features are derived inde-pendent of external resources, (2) logs of queries previously issued directly to the vertical, and (3) corpora representa-tive of vertical content. We focus on 18 different verticals, which differ in terms of semantics, media type, size, and level of query traffic. We compare our method to prior work in federated search and retrieval effectiveness prediction. An in-depth error analysis reveals unique challenges across dif-ferent verticals and provides insight into vertical selection for future work.
 H.3.3 [ Information Search and Retrieval ]: Miscella-neous Algorithms  X  work done while at Yahoo! Labs Montr  X eal vertical selection, distributed information retrieval, resource selection, aggregated search, query classification
In recent years, major search engines have extended their services to include search on specialized subcollections or verticals focused on specific domains (e.g., news, travel, and local search) or media types (e.g., images and video). There are currently two ways through which a user can access verti-cal content. If the user suspects that relevant content exists in a vertical, she may issue the query directly to a vertical search engine. On the other hand, if the user is unaware of a relevant vertical or prefers a portal interface, she may issue the query directly to a portal search engine. To ad-dress this, search engines can include summaries of relevant vertical results in web results, as shown in Figure 1. In the research community, this is referred to as aggregated search and has been implemented by many major search engines [12].

Vertical selection is the task of selecting the relevant verti-cals, if any, in response to a user X  X  query. We focus on single vertical selection , defined as the task of predicting a single relevant vertical, if any. Figure 1 exemplifies a common ac-tion associated with single vertical selection X  X mbedding a short summary of the relevant vertical X  X  results above the first web result. We are conservative in predicting at most a single vertical, as some queries have multiple relevant ver-ticals. However, as we will see later, most queries in our evaluation set were assigned zero or one relevant vertical by human annotators.

Vertical selection is related to the task of resource selec-tion in federated search or distributed information retrieval . Resource selection is the task of deciding which collections to search given a user X  X  query [4]. Similar to resource se-lection, vertical selection can be informed by the content of each vertical. However, vertical selection has a few distin-guishing properties. First, verticals specialize on identifiable domains and types of media. This enables users to possi-bly express interest in vertical content explicitly, using key-Figure 1: A vertical selection system determines that words such as  X  X ews X  for the news vertical or  X  X ictures X  for the images vertical. Therefore, a potentially useful source of evidence for vertical selection is the query string itself, independent of any other resource. Second, some verticals have a search interface through which users directly search for vertical content. Because a vertical selection system and its target verticals are operated by a common entity (e.g., search engine company), we assume access to vertical query-logs. Third, users do not always seek vertical content, but may prefer the default web results instead. In contrast to re-source selection, where a resource is always selected in order to retrieve documents, in vertical selection we must decide when to not predict any vertical relevant.

We investigate a classification-based approach to vertical selection and exploit three feature types: (1) query string features, (2) corpus features, derived from vertical represen-tative corpora, and (3) query-log features, derived from ver-tical query-logs. Corpus and query-log features enrich the query representation beyond the query string and focus on two potentially complementary sources of evidence X  X orpus features relate to content production (i.e., content in the vertical) and query-log features relate to content demand (i.e., content sought by users). With respect to corpus fea-tures, we make use of and compare against prior work in resource selection for federated search (i.e., scoring a col-lection by its expected number of relevant documents) and retrieval effectiveness prediction (i.e., scoring a collection by the predicted quality of its retrieval). We evaluate corpus features on two types of collections: collections of vertical-sampled documents and surrogate collections representative of verticals constructed by sampling a non-vertical resource, the Wikipedia. 1 We evaluate several simple baselines, each focused on a single source of evidence and a supervised ap-proach that combines our three feature types. An error anal-ysis shows the contribution of each feature type and reveals unique challenges in vertical selection. http://www.wikipedia.org
If we consider verticals as external collections, we may view vertical selection analogous to resource selection in fed-erated search. Most prior approaches to resource selection derive evidence solely from the target collections either di-rectly or indirectly, using a sampling of documents as proxy for the collection [6, 17, 19, 20, 9]. Approaches such as CORI [6], CVV [20], and KL-divergence [19] treat collec-tions (or their sampled documents) as  X  X arge documents X  and adapt document scoring techniques to scoring collections. Because these techniques make no distinction between docu-ments, they do not model the number of relevant documents in a collection [16]. Approaches such as GlOSS (and its vari-ations) [9] as well as ReDDE [17] more explicitly model the distribution of relevant documents across resources. ReDDE issues the query to an index of documents sampled from the target collections and scores each collection proportional to the number of top-ranked documents originating from it, taking into account the difference between the size of the original collection and its sample size.

Some verticals are genre-specific. Therefore, some prior work in query-classification into topical categories is rele-vant to vertical selection [13, 14, 2, 1, 10]. Because queries are terse, many query-classification approaches augment the query with features beyond the query string, possibly de-rived from query-logs or corpora of documents associated with the target classes. Bietzel et al. use a large (unlabeled) query-log and a technique known as selectional preference  X  the query  X  X nterest rates X  belongs to target category finance because  X  X nterest X  and  X  X ates X  are distributionally similar to the term  X  X inance X  [1, 2]. Shen et al. [13] and other partici-pants of the KDD 2005 Cup [11] use corpus-based evidence. These techniques resemble ReDDE in that the query is is-sued to an index of documents associated with the target categories and the query X  X  membership to a category is pro-portional to the number of top-ranked documents associated with the category. In later work, Shen et al. derive a soft mapping from documents to target categories using term similarity [14]. The category representation is augmented with related terms using pseudo-relevance feedback.

There is some prior work on vertical selection. Li et al. focus on the shopping and jobs verticals [10]. They focus on query lexical features and use a query-click graph to propa-gate category labels to unlabeled queries. Our work differs from that of Li el al. in that we enrich the query represen-tation beyond query string features, focus on more verticals, and, by formulating the task as single vertical selection, we examine vertical contention resolution rather than evaluate on each vertical independently. Diaz investigates vertical se-lection with respect to the news vertical [8]. Diaz focuses on features derived from the news collection and from web and vertical query-logs and incorporates click-feedback into the model. We extend the work of Diaz by exploring more fea-tures, focusing on more verticals, and evaluating on human relevance judgements rather than clicks. Throughout the paper, we will use the following notation. We define single vertical selection as the following problem. Given query q , the objective is to predict a single relevant vertical,  X  v q  X  X  q , if one exists, and to predict the  X  X o relevant vertical X  class,  X  v q =  X  , otherwise. Formally, we want to maximize single vertical precision, P = 1 where I is the indicator function. The first term is the number of queries for which a relevant vertical was correctly predicted. The second term is the number of queries for which the  X  X o relevant vertical X  class was correctly predicted. We investigated the 18 verticals described in Table 1.
We investigated three sources of evidence for vertical selec-tion: the query string, vertical-representative corpora (not necessarily composed of vertical documents), and queries previously issued to the vertical.
Perhaps the lowest effort approach to vertical selection op-erates on the query string alone, disregarding hits on vertical collections or previous queries issued directly to the vertical. Query string features aim to capitalize on key phrases used in explicit requests for vertical content (e.g.  X  X nauguration pictures  X ) and a possible correlation between named entity types and a vertical (e.g., music vertical queries may men-tion a musician). We define two types of query string fea-tures: rule-based vertical triggers and geographic features.
Rule-based vertical triggers are based on a set of 45 classes aimed to characterize the query X  X  vertical intent (e.g., local phone , product , person , weather , movies , driving direction , music artist ). Some of these 45 triggers map conceptually one-to-one to a target vertical (e.g., movies  X  movies , autos  X  autos ). Others map many-to-one (e.g., { sports players , sports }  X  sports , { product review , product }  X  shopping ). Others do not map directly to a vertical, but may provide (positive or negative) evidence in a supervised classification framework (e.g., patent , events , weather ). Each trigger class is associated with hand-crafted rules using regular expres-sions and dictionary lookups. A query may be associated with multiple classes, each triggered if at least one rule in its inventory matches the query.
Geographic features were produced using a rule-based ge-ographic annotation tool that outputs a probability vector for a set of geographic entities possibly appearing in the query. We focused on the following geographic entities: air-port , colloquial (i.e., location information associated with a named entity, such as  X  X orth Shore Bank X ), continent , coun-try , county , estate , historical county , historical state , histor-ical town , island , land feature , point of interest (e.g, Eiffel Tower), sports team , suburb , supername (i.e., a region name, such as Middle East), town , and zip code . We used the prob-ability of each entity being present in the query as a separate feature. Geographic features are intended to inform classifi-cation into verticals whose queries often mention a location name, such as local , travel , and maps .
Query-log features use evidence from the queries previ-ously issued to the vertical, which reflect the topics in the vertical that are of interest to users. For each vertical, we compute the query likelihood given by a unigram language model constructed from the vertical X  X  query-log. Our query-log features (one per vertical) are defined by, Z = P V
We collected a year X  X  worth of vertical query-logs for the year preceding the gathering of our evaluation query set. In addition, to inform classification into the  X  X o relevant ver-tical X  class, we also collected Web query-logs. Since Web search sees much more traffic than vertical search, we col-lected only a month X  X  worth of Web query-logs. We used the CMU-Cambridge Language Modeling Toolkit 2 to build a unigram language model from each query-log. Each lan-guage model X  X  vocabulary was defined by its most frequent 20000 unigrams and we used Witten-Bell smoothing [18].
Query-log features were evaluated under two conditions: allowing and disallowing zero probabilities from out of vo-cabulary (OOV) terms. In the first condition, a single OOV query term results in a zero probability from the vertical. In the second condition, P (OOV |  X  V i ) was estimated pro-portional to the frequency of terms not in the top 20000 in vertical V i  X  X  query-log.

Some of our target verticals did not have query-logs pre-dating the collection of our evaluation query set. These in-cluded autos , maps , sports , and tv .
Corpus features are derived from document rankings ob-tained by issuing the query to different collections. Con-ducting a retrieval allows us to compare, for example, the number of retrieved documents from different verticals. In practice, issuing a query to all verticals can incur unneces-sary query load on the vertical retrieval system. Therefore, http://svr-www.eng.cam.ac.uk/ prc14/toolkit.html we construct smaller, representative corpora of vertical con-tent local to the vertical selector.
Before discussing our corpus-based features, we will de-scribe two methods for creating representative corpora: sam-pling from the vertical and using surrogate corpora.
Query-based sampling [5] is a technique for sampling doc-uments from collections assumed to provide only a query-in-documents-out interface. The most general query-based sampling approach iterates over the following steps. A single-term query is used to retrieve documents from the collection. Then, the collection X  X  content description is updated and a new single-term sampling query is selected from the up-dated content description. As documents are retrieved, the evolving resource description and, indirectly, new sampling queries are derived from retrieved documents. Shakouhi et al. show that using high-frequency query-log queries for sampling can produce more effective resource descriptions than queries derived from the sampled documents [15]. We follow a similar approach. While Shakouhi et al. use the same set of queries to sample from every collection, we use queries from vertical query-logs. Sampling with query-log queries has two effects. First, it decouples the sampling query from the sampled documents. Second, the sampled documents are biased towards those more likely to be seen by users. This is important when constructing small sam-ples of large corpora if a significant part of the corpus is not of interest to users.
 We used the following procedure to sample documents. First, we collected the top 100 documents returned by run-ning each of the 1000 most frequent non-stopword query-log unigrams as a query. Then, we uniformly sampled at most 25000 documents from the union of these documents. Because we used vertical query-logs to sample vertical docu-ments, verticals without query-logs preceding our evaluation queries also lacked a vertical-sampled collection. The  X  X o relevant vertical X  class does not have a vertical collection for sampling. We denote the set of documents sampled from
An alternative to sampling directly from the verticals is to sample from an external collection, if documents can be mapped conceptually to verticals. We sampled documents from Wikipedia, making use of Wikipedia categories to map documents to verticals using regular expressions. Each ar-ticle in Wikipedia belongs to one or more categories. For instance, a sample of documents representative of the au-tos vertical was gathered from articles assigned a Wikipedia category containing any of the terms  X  X utomobile X ,  X  X ar X , and  X  X ehicle X .

We do not claim that our mapping of Wikipedia docu-ments to verticals is optimal. The risk of associating doc-uments from an external collection to a vertical is misrep-resenting the vertical X  X  contents. However, sampling from Wikipedia may provide several advantages. First, Wikipedia is rich in text. Our corpus features, discussed next, are dependent on text richness. Documents typical of some verticals (used to represent the vertical in direct vertical sampling), such as images and video , tend to be text poor. Second, Wikipedia articles have a consistent format, which makes comparing rankings across collections easier. Third, Wikipedia articles are usually semantically coherent and on topic.
 For practical reasons, some verticals were not mapped to Wikipedia content. The directory vertical and the  X  X o rele-vant vertical X  class are too broad to be sensibly characterized by a set of Wikipedia categories while the maps vertical in-tersects semantically with local and travel . We denote the set of Wikipedia articles mapped to vertical V i by S wiki
Predicting retrieval effectiveness is the task of automat-ically assessing the quality of a retrieval without human relevance judgements. We applied an existing approach to predicting retrieval effectiveness, Clarity [7], to vertical se-lection. Our motivation is that a collection X  X  predicted re-trieval effectiveness with respect to a query may correlate with the collection X  X  relevance to the query. Clarity mea-sures the similarity between the language of the top ranked documents and the language of the collection, estimated us-ing the Kullback-Leibler divergence between the query and collection language model, where V is the vocabulary of collection C and P ( w |  X  q P ( w |  X  C ) are the query and collection language models, re-spectively. The query language model was estimated from the top 100 documents, R 100 , according to, where P ( q |  X  d ) is the query likelihood score of document d , and Z = P d  X  X  smaller as the top ranked documents approach a random sample from the collection (i.e., an ineffective retrieval).
We used two sets of Clarity features: one using collections of vertical-sampled documents and one using collections of Wikipedia-sampled documents. The final Clarity score for vertical V i is given by, where S  X  i denotes either S vertical i , the set of documents sam-pled from V i , or S wiki i , the set of Wikipedia documents mapped to V i and Z  X  = P V
As previously mentioned, in federated search, resource se-lection is the problem of deciding which collections to search given a query. We adapted an existing approach to re-source selection, ReDDE [17], to the task of vertical selec-tion. ReDDE scores a target collection based on its ex-pected number documents relevant to the query. It derives this expectation from a retrieval of a index that combines documents sampled from every target collection. Given this retrieval, ReDDE accumulates a collection X  X  score from its document scores, taking into account the difference between the size of the original collection and sampled set size. As with Clarity features, we generated two sets of ReDDE fea-tures: one using vertical-sampled documents and one using Wikipedia-sampled documents. ReDDE scores vertical V according to,
ReDDE  X  q ( V i ) = |V i | X where, The term |V i | is the number of documents in vertical V and S  X  i denotes either S vertical i , the documents sampled di-rectly from V i , or S wiki i , the Wikipedia documents mapped to V i . We normalize across vertical-and Wikipedia-sampled ReDDE features such that P V
ReDDE requires a hard assignment of documents to ver-ticals. When sampling from verticals, this assignment is trivial X  X  document represents the vertical from which it originates. When sampling from non-vertical collections (e.g., Wikipedia), this assignment is not trivial, and we risk mis-representing a vertical X  X  contents. We experimented with a novel approach similar to ReDDE. Soft.ReDDE computes a soft membership of a document to a vertical,  X  ( d, V i ), based the correlation between the document language model,  X  d , and vertical language model,  X  V i , estimated using the verti-cal X  X  query-log. We used the Bhattacharyya correlation [3], defined by, and normalize across verticals, Soft.ReDDE scores a vertical by the sum of documents scores, P ( q |  X  d ), weighted by the document X  X  similarity to the verti-cal, We normalize Soft.ReDDE features across verticals such that P
Compared to ReDDE, Soft.ReDDE has two potential ben-efits. First, every document in the ranking contributes, more or less, depending on its correlation, to a vertical X  X  score. Second, it is not necessary to manually map documents to verticals, so external collections can be used more freely. In our implementation, we used the full English Wikipedia. Clarity, ReDDE, and Soft.ReDDE features used the Indri IR toolkit. 3
Categorical features were derived from the topical cat-egories automatically assigned to the top 100 documents returned when issuing the query to a general Web index. Each document in the index is assigned, using a maximum entropy text classifier, at most three categories, resembling http://www.lemurproject.org/indri/ nodes from the Online Directory Project (ODP) ontology (e.g.,  X  X ecreation/sports/basketball X ). Categorical features were divided into two distinct sets: general (depth one) cate-gory features (e.g.,  X  X ecreation X ,  X  X cience X ,  X  X ealth X ) and spe-cific (depth two) category features, each which describes a subcategory of a general category (e.g.  X  X ecreation/travel X ,  X  X ecreation/sports X ,  X  X ealth/nutrition X ). Each category pre-diction on a document is associated with a confidence value. We set the value of category feature y i (of depth x ) to be the sum of confidence values over all occurrences of the category in the top 100 documents, CAT q ( y i ) = X where R 100 denotes the top 100 documents, Y d denotes the categories associated with document d , P ( y j | d ) is the con-fidence of predicted category y j on document d , and func-tion depth x ( y j ) returns the depth x ancestor of category y . For example, depth 1 ( X  X ecreation/sports X ) returns  X  X ecre-ation X . We focused on 14 general category features and 42 specific category features X  X he union of categories for the queries in our training set. In general, we expect the set of category features to depend on the queries the system is likely to encounter and the target verticals.
We evaluated 8 single-evidence baselines: The four combi-nations of Clarity and ReDDE with vertical-and Wikipedia-sampled collections, the query likelihood given the verti-cal X  X  query-log language model (allowing and disallowing zero probabilities), Soft.ReDDE, and an approach that al-ways predicts the  X  X o relevant vertical X  class. These vertical scoring functions were uniformly adapted for single vertical selection by normalizing across vertical scores and selecting the top vertical,  X  v , if its score exceeds a threshold,  X  , or else predicting  X  X o relevant vertical X .  X  v = where Z = P V  X  X o relevant vertical X  prediction. Parameter  X  was set using a 500 query validation set
For our multiple feature approach, we trained a multiclass classifier using all features. We trained 19 one-versus-all lo-gistic regression models (one for each of our 18 verticals and one for the  X  X o relevant vertical X  class) using the liblinear toolkit 4 . We complemented the  X  X o relevant vertical X  clas-sifier using the confidence of the 18 binary vertical classifiers using parameter  X  .

These classifiers were combined by predicting vertical  X  v according to,  X  v = http://www.csie.ntu.edu.tw/ cjlin/liblinear/ Table 2: Percentage of queries assigned each vertical. where P V i ( Y = 1 | q ) is the probability of a positive predic-tion from vertical V i  X  X  classifier. If the most confident verti-cal classifier predicts its vertical with confidence below  X  , we default to the  X  X o relevant vertical X  class. All features were scaled to zero minimum and unit maximum. Features associ-ated one-to-one with a vertical (Clarity, ReDDE, the query likelihood given the vertical X  X  query-log and Soft.ReDDE) were normalized across verticals before scaling. Supervised training/testing was done via 10-fold cross validation. Pa-rameter  X  was tuned for each training fold on the same 500 query validation set used for our single feature baselines.
Our evaluation data consisted of 25195 unique queries ob-tained from a commercial search engine X  X  query-log. Human editors were instructed to assign between zero and six rel-evant verticals per query based on their best guess of the user X  X  vertical intent. About 70% of queries were assigned ei-ther a single relevant vertical or no relevant vertical. About 26% of queries, mostly navigational (e.g.,  X  X yspace X ), were assigned  X  X o relevant vertical X  and 44% were assigned a sin-gle relevant vertical. Some queries assigned multiple rele-vant verticals were ambiguous in terms vertical intent (e.g., query  X  X airspray X  was assigned verticals movies , video , and shopping ). Table 2 shows the vertical distribution. We evaluated single vertical selection in terms of precision, P (see Equation 1), defined as the percentage of queries for which we either correctly predict a relevant vertical or cor-rectly predict  X  X o relevant vertical X . Because we make a sin-gle prediction when there are potentially multiple relevant verticals, a recall-flavored performance measure has undesir-able properties. For example, if two verticals are perfectly correlated in terms of the queries for which they are rele-vant, then a classifier that chooses the same vertical each time maximizes our objective (i.e, it selects a correct verti-cal each time) but recall would be perfect for one vertical and zero for the other. We also show % coverage (% cov), defined as the percentage of queries for which a vertical was predicted (correctly or incorrectly). Significance was tested using a 2-tailed paired t-test on queries. Results for single vertical selection are shown in Table 3.
The no.rel approach obtained P = 0 . 263 because 26 . 3% of queries had no true relevant vertical. Both Clarity us-ing vertical-and Wikipedia-sampled collections performed significantly worse than no.rel . Clarity scores for a given query may not be directly comparable across collections Table 3: Single Vertical Precision ( P ). Approaches are with different corpus statistics. In prior work, Clarity has been used to compare retrievals from different queries on the same collection, but not retrievals from the same query on different collections. Further experiments are needed to determine whether Clarity can be adapted for vertical selection. ReDDE using vertical-sampled documents out-performed ReDDE using Wikipedia-sampled documents, in spite of more verticals having a Wikipedia-sampled collec-tion than a vertical-sampled collection. We examined the types of classification errors each algorithm performed. Both approaches performed comparably with respect to the  X  X o relevant vertical X  class. However, redde.wiki more often predicted a wrong vertical. Precision on queries for which a vertical was predicted was 0 . 382 for redde.vertical and 0 . 284 for redde.wiki . Our mapping of Wikipedia categories to verticals may have misrepresented one or more vertical.
The query likelihood given the vertical X  X  query-log lan-guage model was the best single-evidence predictor. This method performed better when allowing than when disal-lowing zero probabilities. This may have been due to the non-uniformity of P (OOV) estimates across vertical lan-guage models. Each vertical X  X  P (OOV) estimate was based on the frequency of terms not in its top 20000, expected to be greater for verticals with a more open vocabulary. A vertical X  X  P (OOV) estimate affects the probability estimates of within vocabulary terms through discounting. Different P (OOV) estimates across verticals may have made the query likelihood given by different vertical language models less comparable.

Finally, our multiple-feature supervised approach ( LR ) out-performed all single-feature baselines by a large margin X  X  58% improvement over the best single-evidence predictor, q.log . Such a performance improvement may justify the cost of producing training data in the form of vertical rel-evance judgements on queries. Our supervised framework has several potential advantages. First, as our results show, it can integrate multiple sources of evidence. Second, by combining vertical-specific classifiers, single-evidence scores (e.g., ReDDE or Clarity) need not be directly comparable across verticals. For example, a classifier may learn to ignore a high ReDDE score if it is unreliable, perhaps due to poor sampling. Third, by sharing all features among vertical-specific base classifiers, a classifier may benefit from another vertical X  X  score if they are correlated.
In this section, we explore the contribution of different fea-Table 4: Feature Set Contribution. A  X  denotes a sig-ture sets on precision ( P ). A feature set is said to contribute significantly if the classifier X  X  performance drops significantly upon removing the feature set.
We analyze each feature set X  X  contribution to final predic-tion precision. We divide this analysis into two parts. First, we do a  X  X eave one feature type out X  analysis. Second, we do a  X  X eave one sampled corpus out X  analysis, where we com-pare the contribution of corpus features using vertical-vs Wikipedia-sampled documents.

Table 4 shows the change in precision associated with each feature type. Keep in mind that features were not evaluated in isolation. A non-significant performance drop in P does not necessarily mean the feature captures no useful evidence, as features may be correlated.

In terms of feature types, omitting q.log , triggers , and clarity.* features did not produce a significant drop in P . It is possible that q.log features, the best single-evidence predictor, did not contribute significantly because they are correlated with soft.redde features, which did contribute significantly. A positive trigger class was predicted only for 4367 (18%) queries, suffering from low coverage. Clarity scores for the same query may not be directly comparable across collections.

Corpus-based features contributed the most. The largest contribution came from cat.specific features. Interest-ingly, categorical features are not derived from resources as-sociated with a vertical (i.e., vertical documents or queries). The classifier learns to associate these features with a ver-tical from training data. The contribution of cat.specific features was significantly greater than that of cat.general features because cat.general categories were too coarse to discriminate between some verticals. For example, the gen-eral category recreation conflates recreation/sports , recre-ation/auto , and recreation/travel , which map conceptually to different verticals. The second and third most helpful fea-tures were soft.redde and redde.* features, respectively.
To evaluate the usefulness of evidence derived directly from the vertical, we omitted ReDDE and Clarity features using our vertical-sampled collections ( no.vertical.corpus ). Likewise, to evaluate the usefulness of evidence derived from surrogate corpora, we omitted ReDDE and Clarity features using our Wikipedia-sampled collections ( no.wiki.corpus ). Removing either set of features produced a significant drop in P . Vertical-and Wikipedia-sampled collections were sam-pled using different techniques and have a different collection size distribution. Thus, we cannot (and did not intend to) directly compare one against the other. This result, how-ever, shows that surrogate collections can provide evidence complementary to that derived directly from the vertical.
Table 5 shows precision per vertical/class using all our features, listed in descending order of precision. Column  X % true X  is the percentage of queries with the vertical as a true relevant vertical. Column  X % cov X  is the percentage of queries with the vertical as the predicted relevant vertical. Note that the  X % true X  column does not sum to one because queries may have more than one true relevant vertical. Col-umn  X % cov X  does sum to one because a single vertical/class was predicted per query. Although  X % cov X  can be expected to be less than  X % true X , ideally they should be comparable.
As previously noted, some verticals lacked query-logs (+) and/or a Wikipedia-sampled surrogate corpus (  X  ). Verticals autos , sports , and tv performed well in spite of lacking fea-tures derived from query-logs. Verticals video , news , and reference performed poorly in spite of having all resources. Therefore, the difference in performance across verticals can-not be attributed only to missing features.

The system performed best on verticals that focus on a co-herent topic with identifiable vocabulary (i.e., travel , health , games , music ). The vocabulary associated with these verti-cals may have been the least confusable with that of other verticals. Precision was higher for these verticals than ver-ticals shopping and reference and the  X  X o relevant vertical X  class, which had more positive examples for training.
The system performed worst on the verticals images , video , news , reference , maps , and directory . The maps vertical had the fewest positive instances for training, was feature-impoverished, and probably confusable with local and travel . Verticals images and video focus on a type of media rather than a specific genre. Queries related to reference and di-rectory characterize broad encyclopedic information needs. The news vertical tends to be highly dynamic and may re-quire features related to bursts in content demand, possibly derived from same-day vertical query-logs.

With respect to the  X  X o relevant vertical X  class, coverage was high and precision was below 50%. Although our evalu-ation metric weights all false positive errors equally, in some cases a  X  X o relevant vertical X  false positive may be less costly than a vertical false positive. A user may be more annoyed by seeing a non-relevant vertical display than by not seeing a relevant vertical display. Of our misclassifications on queries with at least one true relevant vertical ( |V q | &gt; 0), 57% of the time we incorrectly predicted  X  X o relevent vertical X  and 43% a non-relevant vertical.
In the context of resource selection for federated search, this work contributes several meaningful results. First, most prior work in resource selection has studied corpus-based evidence derived from the target collections. The use of collection-specific query-logs for resource selection has not been previously studied. This is in part because in an unco-operative environment, query-logs of searchable collections may be inaccessible. Our results show that in vertical se-lection, a type of cooperative federated search, query-logs are useful. Ranking verticals by the query likelihood given the vertical X  X  query-log language model was the best single-evidence predictor. In our supervised model, query-logs were used successfully to sample from vertical collections and to associate non-vertical documents (i.e., Wikipedia articles) with vertical collections. Second, some verticals (e.g., video ) are likely to be text-impoverished. We presented methods for successfully associating non-vertical, text-rich documents with verticals, which makes it possible to use existing tech-niques (e.g., ReDDE) for vertical selection. Finally, most prior work in resource selection has focused on unsuper-vised or weakly supervised collection ranking methods. Our classification-based approach to vertical selection allows us to combine features without manually associating them with a vertical. For example, our categorical and geographic fea-tures, which are not derived from a vertical resource, con-tribute significantly to prediction accuracy.

This work could be extended in several directions. Our corpus and query-log features are derived from external re-sources. Although the proposed approach requires train-ing data, it may not be necessary to retrain the model fre-quently, as long as the external resources used to compute these features reflect changes in the vertical X  X  relevance to a topic. Models that use only query string features may have to be retrained more frequently. Future work might empiri-cally evaluate the robustness of non-lexical features derived from external resources in a dynamic environment. Also, some verticals are bound to be resource-impoverished (e.g., lack query-logs or text-rich documents) and may require in-corporating user feedback into the selection model. We would like to thank Jean Fran  X cois Beaumont, Daniel Boies, Hugues Bouchard, Deborah Donato, Rosie Jones, Remi Kwan, Vanessa Murdock, Jian-Yun Nie, Jean Fran  X cois Paiement, and Alexandre Rochette for helpful discussions and feed-back. This work was supported in part by the NSF grant IIS-0841275 and a generous gift from Yahoo!. Any opin-ions, findings, conclusions, and recommendations expressed in this paper are the authors X  and do not necessarily reflect those of the sponsors.
