 straction level. Such a determinacy is called a Roll-Up De-
The problem of discovering functional dependencies (FDs) from relational databases has been studied [KMRS92]. Although FDs are the most important dependencies in database design, FDs are not prevalent in data mining. This may be attributed to the fact that FDs other than those known at design time, are fairly rare. We propose Roll-Up Dependencies (RUDs) with clear and interesting applications in data mining and OLAP. RUDs generalize FDs for attribute domains (called levels) that are organized in a finer-than order. 
Such finer-than relations are very common in data mining and OLAP; they have been called concept hierarchies [HCC93], or roll-up/drill-down lattices. For example, time can be measured in days (level DAY) or weeks (level WEEK). DAY is finer than WEEK, and there is a many-to-one mapping from DAY values to WEEK values capturing the natural containment of days in weeks. A price can be expressed in cents (level CENT) or integral Euros (level EURO). at different levels. For example, consider the relation scheme (D : DAY)(Price : CENT) that stores the daily price in cents of a particular product. We have one price per day, as expressed by the FD: 
In general, cent prices can change from day to day, and we may be interested in finding regularities in price fluctuations. We may find that within a week, all cent prices are equal if they are expressed to the nearest integral Euro, as stated by the RUD: 
The meaning is that if the D-values of two tuples belong to the same week then their Price-values round to the same integral Euro. It is worth noting that the dependency (1) above does not imply the dependency (2), nor vice versa. The dependency (2) does imply, for example, (D : DAY) + (Price : EURO). and applications of RUDs. Section 3 studies the problem of mining RUDs. We first define RUDs, and then we characterize the strength of a RUD by adapting the common notions of support and confidence. RUDMINE is the problem of finding the 
RUDs that satisfy specified support and confidence thresholds. We study the computational complexity of RUDMINE. Section 4 gives an algorithm for mining 
RUDs, and Section 5 reports on our experience with a real-life dataset. Section 6 discusses related work. 
Section 7 presents a conclusion. 
Consider the scheme: 
A tuple (1 : u)(D : v)(S : w)(Price : x) expresses that the price of item u on day v in store w amounted to x cents. Intuitively, one may think of this scheme as a data cube scheme, where I, D, and 5 X  are the dimensions, and Price is the measurement. A typical 
OLAP query then is  X  X ive for each item the highest sales price per area and year. X  a month and a store chain, as captured by the RUD: 
For example, the price of the item  X  X ego System 6115 X  in all stores of the Toysrus chain .invariably was 912 cents during January, 1997. Then a relation over scheme (3) will store a lot of redundant prices: whenever two tuples that agree on I roll up to the same month and chain, they must necessarily agree on Price. 
The data redundancy can be avoided by decomposing scheme (3) into the schemes: 
Scheme (4) is to store the price of an item for a particular month and chain. Scheme (5) is to store whether a particular item was sold in a particular store at a particular day, and can be dropped if such information is not needed. 2.2 Generalizing Data 
We may be faced with a large number of pricing infor-mation, giving such a profusion of detailed information that direct comparison is impossible. For OLAP and data mining, it is often desirable to reduce the mass of data to a few manageable  X  X epresentative X  figures from which meaningful comparisons can be made. In this section we show that RUDs are intimately related to data generalization and summarization. 2.2.1 Rolling Up Data 
On examination of a relation over scheme (4) we may observe that, although cent prices change within a year, the price fluctuations are small. In particular, we may find that within a year and a chain, an item is always sold at the same integral Euro price, as described by the RUD: For example, the round Euro price of the item  X  X eg0 
System 6115 X  in each store of the Toysrus chain invariably was 9 Euros during 1997. For applications in which Euro prices are sufficiently accurate, we can replace scheme (4) by the following one: 
The advantage is that relations over scheme (7) are, on average, twelve times smaller than relations over scheme (4) (as there are twelve months in a year). 2.2.2 Summarizing Data 
In the preceding example, we rolled up days to years, and stores to chains, and rounded Price-values to the nearest integral Euro. Another way of summarizing 
Price-values is by taking the arithmetic mean, or any other measure of central tendency (median, mode). 
However, if important decisions in real life are based on the summarized Price-values, then we have to be careful that the dispersion of the prices summarized is not too high, or else the arithmetic mean is of limited significance. We argue that RUDs in combination with confidence can be used as a measure of dispersion. As-sume that RUD (6) is highly but not fully satisfied. The confidence of this RUD is the conditional probability that the Price-values of two tuples round to the same integral Euro, given the two tuples concern the prices of the same item within a single year and a single chain. A high confidence then means that the dispersion of the prices of a particular item within a year and chain is small, and consequently, taking average prices is mean-ingful. In this way, RUDs give us an indication about the representativeness of central tendency measures. (like Price), but also for categorical data, as demon-strated by our real-life experiments (see Section 4). 
Consider a file system where files roll up to a cer-tain type (FTYPE), such as  X  X ocument, X   X  X rogram-source-code, X  or  X  X rogram-object-code. X  The question is whether one can reasonably classify directories ac-cording to the types of the files they contain. If files in the same directory are of the same type with a high con-fidence, then such classification is reasonable; otherwise it is not. 3.1 Definition of RUDs 
Domain names are called Newels. Every level I has associated with it a disjoint domain of values, denoted dam(Z). In all examples throughout this paper, levels appear in uppercase typewriter style. Examples used earlier include DAY, WEEK, CENT, and EURO. The family of all levels is a partially ordered set, as shown in 
Figure 1. The order is denoted 5, and corresponds to a finer-than relationship. Whenever 1 3 l X , there is a total function mapping each element v E dam(Z) to an element u X  E dom(Z X ), and we say that u rolls up to v X  in 1 X . Roll-up is transitive in the sense that whenever u rolls up to V X  in I X , and V X  rolls up to w X  in 1 X  then v must roll up to w X  in 2 X . Roll-up is reflexive in the sense that every element v E dam(Z) rolls up to itself in 1. Details can be found in [WN98]. h), . . . , (An : In)} where n &gt; 0, and A1,...,An are distinct attributes, and 11,. . . , I, are levels. For simplicity, curly braces and commas will be omitted in the denotation of a set. Let S denote the scheme (A1 : Zl)(Aa : 12). . . (A, : In) hereinafter. A tuple over S is a set (Al : vl)(Az : ~2). . . (A, : v,) where each vi E dom(li). A relation (instance) over S is a set of tuples over S. Figure 3 top shows a relation over (Path : PATH) (Fname : FNAME), with its intuitive meaning: a tuple (Path : x)(Fname : y) means that z is the location of file 1~. Paths are a concatenation of drive (C or D) and directory. 
A generalization scheme P of S is a set (Ai, : 4,) . . . (Ai,,, : li,) satisfying the following conditions: 2. whenever Aij = Ai, with j # k then lij $ li, and a generalization of S. Given the set of levels in Figure 1, there are more than three hundred generalizations of the scheme (I : ITEM)(D : DAY)(S : STORE)(Price : CENT). AREA and CHAIN are not comparable by 5, the last generalization can contain both (S : AREA) and (S : 
The only assumption we make about 5 is that it is a partial order. We do not assume that the order 5 is a lattice, nor do we partition the set of all levels into different hierarchies, as is done in other proposals. The order 5 on levels gives rise to a relation i on generalization schemes as follows. For generalizations P and Q of scheme S, we write P 9 Q iff for every (A : I) E Q, there exists some (A : I X ) E P such that 1 X  3 1. While 5 is only a partial order, the set of all generalization schemes of S, ordered by A, is a complete lattice (Theorem 1). We call this lattice the roZZ-up lattice over S. Figure 2 shows the roll-up lattice over possible to assign a number to the nodes of the roll-up lattice such that (i) the top is numbered 0, and (ii) if P is a child of Q in the roll-up lattice, then the number of P equals the number of Q plus one. (P is called a child of Q, denoted P E chiZdren( Q), if P is an immediate descendant of Q in the roll-up lattice. In terms of lattice theory, chiZdren( Q) are the nodes covered by Q.) The unique number that is in this way assigned to each node, is called the stratum of the node. This stratification of the roll-up lattice will be exploited by our RUD mining algorithm. Theorem 1 The set of all generalization schemes of S, ordered by 9, is a complete lattice. Furthermore, there exists a function f from the set of generalization schemes of S to N such that 2. if P E children(Q) then f(P) = f(Q) + 1. The proof of Theorem 1 can be found in [WN98, Ca199]. Our notion of roll-up lattice extends and generalizes several earlier proposals found in the literature. Our notion is more general than the one in [HRU96], because the same attribute can appear more than once in a lattice element. This extension is natural and useful in OLAP applications. Dimensionality reduction [GCB+97] is embedded naturally in our roll-up lattice. 
Finally, a roll-up dependency (RUD) over S is a statement of the form P -+ Q where P and Q are generalizations of S. A RUD gives rise to a convenient histogram representation of a relation instance, as is shown in Figure 3. Figure 3 bottom gives the histogram of the relation MYFILES and the RUD (Path : DRIVE) -+ (Fname : EXTENSION). The first row in the histogram indicates that there are 2 tuples t in MYFILES such that t(Path) rolls up to drive C and t(Fname) rolls up to extension dot. Let S be a scheme. Let t and t X  be tuples over S and let P be a generalization of S. We say that the tuples t and t X  are equivalent under P (or P-equivalent) iff for every member (A : I) of P, t(A) and t X (A) roll up to the same value in 1. To continue with the example relation MYFILES given in Figure 3 top, the first two tuples are equivalent under (Path : DRIVE)(Fname : EXTENSION), because their Path-values both roll up to drive C, and their Fname-values both roll up to extension dot. We say that a relation R satisfies a RUD P -+ Q iff for all tuples t, t X  of R, whenever t and t X  are P-equivalent 
Figure 3: A Relation Instance MYFILES over Scheme (Path : PATH)(Fname : FNAME) and the Histogram of (Path : DRIVE) -+ (Fname : EXTENSION) then they are Q-equivalent as well. It follows that the relation MYFILES falsifies the RUD (Path : DRIVE) + (Fname : EXTENSION), as the first and the third tuple are equivalent under (Path : DRIVE) but not under (Fname : EXTENSION). That is, it is not true that all files on the same drive have the same file extension. paragraph is classical in the sense that a RUD is either satisfied or falsified by a relation; there is no third possibility. In data mining, one is typically not only interested in rules that are fully satisfied, but also in rules that are highly satisfied. We now introduce a relaxed notion of satisfaction by adapting the classical notions of support and confidence [AIS93]. 
Q be a RUD over 5 X . The confidence of P + Q is the conditional probability that two tuples that are randomly selected from R without replacement, are Q-equivalent given they are already P-equivalent. 
The support of P + Q is the probability that two tuples that are randomly selected from R without replacement, are P-equivalent. Given a relation R, the support and the confidence of P --+ Q will be denoted sup(P + Q, R) and conf(P + Q, R) respectively. We give some differences between our notions of support and confidence and the notions with the same name used in association rule mining [AIS93]: l Our support and confidence refer to pairs of tuples, l In the definition of support, we consider only left-l The confidence of a RUD lies between 0 and 1. If Given a relation instance and a RUD, support and confidence can be computed from the histogram of the RUD. Precise mathematical formulas are easy to obtain and are given in [WN98]. We only include an example here. Recall that the support of (Path : DRIVE) -+ (Fname : EXTENSION) is the probability that two files reside on the same drive. For the relation MYFILES shown in Figure 3, the support of (Path : DRIVE) + (Fname : EXTENSION) is given by $$$ = &amp;. (Cr denotes the total number of combinaiions of choosing Ic objects from n objects.) The Ci gives the total number of pairs of files on drive C. Similarly, Cz corresponds to the pairs of files on drive D. Finally, C4+6 gives the total number of pairs of files. The cinfidence of (Path : DRIVE) -+ (Fname : EXTENSION) as discussed above,  X  X iv&amp; the total number of pairs of tuples that are equivalent under (Path : DRIVE). Among up to drive C and that roll up to the same file extension (dot and ps). Similarly, the next (C, X  + Cg + C, X ) in the numerator gives the total number of pairs of files that roll up to drive D and that roll, up to the same file extension. Here, Ci is considered to be zero. 
Apart from the notion of confidence discussed so far, we have also considered an alternative confidence measure for RUDs in [WN98]. The confidence measure discussed so far gives equal weight to all tuple pairs. Thus, larger groups may dominate smaller groups in the overall confidence. For the example given in Figure 3, there may be ten times more D-drive tuples than C-drive tuples. Under the current confidence measure, the RUD may become more a reflection of the situation on drive D than of a global one. For certain applications, it is desirable that each group be assigned equal weight, independent of its size. Thus, we also developed a measure that is based on the average confidence per group. See [WN98] for more details. Suppose we are given a very large relation over scheme (3). This relation gives such a profusion of detailed information that direct comparison of prices is impos-sible. The information has first to be simplified to a higher generalization level. In a first attempt, we may decide that prices in integral Euros are sufficiently ac-curate, so we roll up Price-values from CENT to EURO. We are still left with the problem of finding meaningful generalization levels for the attributes 1, D, and S. By meaningful, we mean that the dispersion of the Euro prices within each group of aggregated data must be reasonably small. That is, we are actually looking for a generalization P of (I : ITEM)(D : DAY) (5 X  : STORE) such that the confidence of P + (Price : EURO) is high. This problem is generalized and captured by RUDMINE. 
RUDMINE is the problem of finding all RUDs with a fixed right-hand generalization scheme that satisfy certain support and confidence thresholds. More for-mally, a RUDMINE problem is a quintet (S, Q, ts, tc, R) where S is a scheme, Q is a generalization scheme of S, support threshold ts and confidence threshold tc are rational numbers between 0 and 1, and R is a rela-tion over S. The answer to the RUDMINE problem (S, Q, ts, tc, R) is the set containing every generalization scheme P of S that satisfies the following conditions: (i) sup(P + Q, R) &gt; ts and conf(P + Q, R) 2 tc; and (ii) P has no attributes in common with Q. The proof of the following result can be found in [WN98]. Theorem 2 RUDMINE is NP-hard. An upper bound for the complexity of RUDMINE depends on the complexity of roll-up functions. For practical situations, RUDMINE is in NP, and hence is NP-complete. 
In the following section, we use the following nota-tional conventions. The support of a RUD P + Q is independent of Q. For convenience, we can write sup(P, R) instead of sup(P -+ Q, R) for any Q. Thus, we can talk about the support of a generalization scheme. Finally, we will use percentages to denote prob-abilities. Consider the RUDMINE problem (S, Q, ts, tc, R). Let Sl be the greatest subset of S that has no attributes in common with Q. Our algorithm is outlined in Figure 4 and follows the idea of levelwise search [MT97]: start from the top of the roll-up lattice over S X  X  (i.e., the most general node), and then generate and evaluate more and more nodes down the lattice, without ever evaluating those nodes that cannot be interesting given the information obtained in earlier iterations. 
More precisely, the roll-up lattice over S X  X  is traversed stratum-wise (cf. Theorem 1) from top to bottom to find the generalizations of Sl with sufficient support and confidence. The set C X  contains the candidate generalization schemes at stratum k. If an element P of C X  has sufficient support, then all children of P are inserted in C X +i . An  X  X priori trick X  is used for pruning: if the support of P -+ Q is below the threshold, and 
P X  q P, then we know a priori that P X  + Q must fail the support threshold. This is because the support decreases monotonically if we traverse the roll-up lattice from top to bottom. The set TooLow contains the nodes of the roll-up lattice whose support was effectively computed but failed the minimum threshold support; this corresponds to what is called the negative border in association rule mining. Nodes below the negative border must necessarily fail the threshold support. Note incidentally that no such monotonicity property holds for confidence, and hence confidence is not used for pruning.  X  X istogram inversion X  for computing the support and confidence of candidate generalization schemes. The inverted histogram of P + Q is the histogram of 
Q + P. Figure 5 shows the inverted histogram of the histogram in Figure 3. Both histograms contain the same figures but in a different order. For the RUDMINE problem (S, Q, ts, tc, R), the input database 
R is first partitioned according to the generalization scheme Q. This is shown in Figure 6 left for the relation MYFILES and the right-hand side (Fname : EXTENSION). Significantly, as Q is fixed for a given 
RUDMINE problem, this partitioning has to be done only once. From this partitioned input database, we can construct fragments of the inverted histogram of 
P + Q for any P. These fragments can then be merged to obtain all figures needed to compute the support and confidence of P -+ Q. Figure 6 shows the computation of the support and confidence of (Path : DRIVE) + (Fname : EXTENSION). The figure shows one merge operation; in practice, merge can be done two histograms at a time. In this example, the fixed right-hand side is a singleton, but in general, our algorithm allows multiple right-hand side attributes. application of the well-known Bayes X  theorem, which reads as follows in our framework: for any relation R. Equation (8) expresses the relation between P --+ Q and the  X  X nverted rule X  Q + P; note Figure 5: Inverted Histogram of (Path : DRIVE) + (Fname :EXTENSION) Figure 6: Technique Used for Computing Support and Confidence that Q and hence szlp(Q, R) are constant in RUDMINE. 
The advantages of the histogram inversion technique are as follows. The original non-inverted histograms can become very large as one goes down the roll-up lattice, and may not fit into main memory. In the histogram inversion approach, we only need to store smaller fragments, and we can compute fragments in main memory until it is filled. For generalization schemes near the top of the roll-up lattice, on the other hand, the size of the (inverted) histograms is small. In that case, we can compute several generalization schemes together in main memory, and the cost of reading the (partitioned) input relation is amortized over multiple generalization schemes. 
Significantly, our algorithm computes complete and exact results, in the sense that if a certain RUD is an answer of the RUDMINE problem under consideration, then that RUD will be found by the algorithm. Our experiments were performed on a 200 MHz Pen-tium PC with 64 MB of main memory. The database was built by gathering properties of files on a number of hard disks. Properties of files included Fname, Path, Time (of creation), and Size. The roll-up lattice con-tained up to 200 nodes. These data are fairly easy to obtain, roll-ups are natural, and our familiarity with the dataset facilitated interpreting the output of the algorithm. Scale-Up Figure 7 shows the execution time of the algorithm as we increase the number of tuples in the input database, for three different levels of threshold support (l%, 5%, and 10%). In the experiments, the fixed right-hand side was (Fname : EXTENSION). The graph shows that the algorithm scales quite linearly. Note that the execution time is independent of the threshold confidence, as the confidence is not used for pruning. 
Figure 8 shows the execution time as we increase the number of attributes that can appear at the left-hand side. More precisely, consider the RUDMINE problem has no attributes in common with Q, and let S,. = S\Sl. Sl contains all possible left-hand side attributes. In our experiment Sl contained seven attributes. For every non-empty subset 5 X ; of 5 X  X  we measured the time needed to solve the RUDMINE problem (Si U S,,Q, ts, tc,R). The graph shows that the execution time can vary considerably depending on which set of attributes is chosen. For example, attributes with a small number of distinct values are easier to handle. Pruning As stated by Theorem 2, RUDMINE is NP-hard. The source of exponentiality is that the size 
Figure 7: Execution Time in Function of the Number of Tuples 
Figure 8: Execution Time in Function of the Number of Attributes (50 000 Tuples, 5% Threshold Support) Figure 9: Execution Time in Function of the Threshold 
Support 
Figure 10: Number of Nodes of the Roll-Up Lattice that are Effectively Evaluated in Function of the Threshold 
Support of the roll-up lattice is exponential in the number of attributes. In the worst case, our algorithm has to evaluate every element of the roll-up lattice. Therefore, it is important to see how many nodes of the roll-up lattice have to be evaluated in practice. Figure 9 shows the execution time as we increase the threshold support, for two different database sizes. Increasing the threshold support from 0.1% to 0.7% results in a drastic decrease of the execution time. The reason is explained by Figure 10, which shows the number of nodes of the roll-up lattice that are effectively evaluated as we increase the threshold support. These are the nodes that are above or on the negative border. The diagram shows that the pruning strategy used is quite effective for this dataset. Significantly, the nodes that are pruned away are near the bottom of the roll-up lattice, below the negative border. These nodes correspond to 
RUDs with large histograms that would otherwise be expensive to evaluate. 
Interpretation of Output RUDs As expected, many RUDs had low support. Remind that our support only considers left-hand sides of RUDs. If the left-hand side is, for example, (Path : DIR), then the support of the RUD is the probability that two files belong to the same directory. If the number of directories is large, and files are evenly distributed over directories, then the support of the RUD is low. of a RUD. Here caution is in order when interpreting high confidences. For example, RUDs with (Path : 
DRIVE) at the right-hand side happened to have a high confidence independent of the left-hand side, just because the confidence of {} + (Path : DRIVE) is high. The confidence of {} -+ (Path : DRIVE) is the probability that two files reside on the same drive; if n files are distributed over two drives (C and D), then one can prove that this probability will be at least n-2 2n--2 M 50%. The confidence 0: a RUD P + Q has therefore to be compared with the confidence of {} + Q to verify its statistical significance. An example of a statistically significant RUD was (Path : DIR) + (Fname : EXTENSION), saying that files in the same directory tend to have the same file extension. This RUD had a confidence of more than 50%, while the 
RUD {} -+ (Fname:EXTENSION) had a confidence of less than 2%. 
RUDs generalize FDs for relational databases that support roll-up/drill-down. The discovery of FDs from relational databases has already been studied before the explosive growth of data mining research [KMRS92]. 
Roll-up plays an important role in data mining and related areas. A lattice framework for OLAP has been provided by Harinarayan et al. [HRU96]. As we pointed out in Section 3, our notion of roll-up lattice is more general than what has so far been proposed in the literature. 
We believe that the applicability of RUD mining is far beyond that of FD mining. An OLAP user can rely on discovered RUDs to decide which roll-up operations are beneficial-rather than performing roll-up in an ad hoc manner. The integration of OLAP and data mining is a promising research area, which has been called OLAP mining [Han97]. 
This work is also strongly related to OLAP in the following way. Many computational issues involved in the algorithm to solve the RUDMINE problem are similar to those in computing data cubes. As a concrete example, Harinarayan et al. [HRU96] study the selection of views to materialize for a data cube. The views typically group data by one or more dimensions, and then apply a distributive set function on each group so obtained. Rather than distributive, our notion of confidence is an example of a holistic set function [GCB+97]. Little work has addressed data cubes that compute a holistic set function. 
An interesting research topic is the use of RUDs in (multidimensional) database design. Our work was inspired by the work on temporal FDs (TFDs) and temporal normalization of Wang et al. [WBBJ97], and the TFDs proposed by ourselves [Wij99]. Loosely speaking, a TFD corresponds to a RUD where roll-up is only provided for one single dedicated timestamping attribute. 
Theorem 2 shows that RUDMINE is NP-hard in the number of attributes. Data mining problems that have exponential complexity in terms of the number of attributes are not rare. Another example is the mining of quantitative association rules [SASS, WM98]. Roll-up dependencies (RUDs) generalize FDs for do-mains (called levels) that are related by a partial order that captures roll-up semantics. From this partially or-dered set of levels we derive a complete ro&amp;up lattice. Our construct of roll-up lattice is a generalization of several earlier proposals. RUDs have a high application potential in database design, data mining, and OLAP. We addressed the problem RUDMINE: discover RUDs whose support and confidence exceed certain specified threshold values. We can show that the problem is NP-hard. An upper bound for the complexity depends on the complexity of roll-up functions. We implemented an algorithm for mining RUDs, based on a technique called  X  X istogram inversion. X  Experimental results show that the algorithm uses linear time in the number of tuples. 
An interesting and important research goal is to further generalize our roll-up framework, and to study the impact of such generalizations on RUDs. For example, instead of saying that two prices in cents roll up to the same integral Euro, we may wish to express that the distance between two cent prices is less than one Euro. [AIS93] [Cal991 [GCB+97] [Han971 [HCC93] [HRU96] [KMRS92] [MT971 [SA96] 
