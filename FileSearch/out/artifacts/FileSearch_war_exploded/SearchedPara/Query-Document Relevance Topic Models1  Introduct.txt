 Language model, which captures the statistical regularities of language gener-ation, (LM) has been successfully applied in information retrieval (IR) [13,22]. However, the LM-based IR approaches often suffer from the problem of the word usage variety. Using topic models to address the above issue has been an area of interesting and exciting research. Topic model refers to the language model that is commonly used for extracting and analyzing the semantic information in a collec-tion of documents. Probabilistic latent semantic analysis (PLSA) [7] and latent Dirichlet allocation (LDA) [2] are two well-known topic models for documents. In PLSA, a document model is a mixture of multinomials, where each mixture component corresponds to a topic. The parameters in the mixture of multino-mials, e.g., weights and multinomial parameters, can be easily estimated via the maximum likelihood principle. In LDA, weights and multinomial parameters are treated as random variables with the (conjugate) Dirichlet prior distributions. The maximum a posterior estimates for these variables are used for document models. Topic model and its variants have been applied to applications such as language modeling and language model adaptation [4,6,20], information retrieval [16,18,19], tag-based music retrieval [9,17], and social network analysis [10].
For IR applications, the state-of-the-art topic models can be somewhat de-ficient. The main issue here is that they often fail to exploit the valuable in-formation conveyed in the queries while focusing only on document contents. Chemudugunta et al. [3] propose a probabilistic topic model which assumes that words are generated either from a specific aspect distribution or a back-ground distribution. Wei and Croft [19] linearly combine the LDA model with document-specific word distributions to capture both general as well as specific information in documents. Another interesting topic modeling approach gives users the ability to provide feedback on the latent topic level and reformulate the original query [1,14]. In addition, Tao et al. [15] construct a method to ex-pand every document with its neighborhood information. As described in [12], query association is one of the most important forms of document context, which could improve the effectiveness of IR systems. In this paper, we aim to deal with this deficiency by integrating the concep t of relevance into the generation model from different topical aspects of the query rather than expanding a query from an initially retrieved set of documents [24]. That is, we design IR systems with emphasis on the degree of matchedness between the user X  X  information needs and the relevant documents.

In this paper, we propose a novel technique called relevance-dependent topic model (RDTM). The main contribution of this work is modeling the generation of a document and its relevant past queries with topics for information retrieval. Relevant past queries are incorporated to obtain a more accurate model for the information need. The model assumes that relevant information about the query may affect the mixture of the topics in the documents and the topic of each term in a document may be sampled from either using the normal document specific mixture weights in LDA or using query specific mixture weights. The parame-ter estimation of the proposed RDTM is implemented by the Gibbs sampling method [5].

The remainder of this paper is organized as follows. The background of this research work is surveyed in Section 2, with emphasis on the review of stochastic methods for information retrieval. Pro posed relevance-dependent topic models and the corresponding learning and inference algorithms based on Gibbs sam-pling are introduced and explained in details in Section 3. The experimental results are presented and discussed in Section 4. Lastly, summarization and the concluding remarks are given in Section 5. 2.1 LDA-Based Document Model In a topic model , the probability of a word in a document depends on the topic of the document. Without loss of generality, a word is denoted by w  X  { 1 , 2 ,...,V } ,where V is the number of distinct words/terms in a vocabulary. A document , represented by d = w 1 ,...,w n d , is a sequence of words. A col-lection of documents is denoted by D = { d 1 ,..., d D } . The number of topics is assumed to be K , so a topic is denoted by z  X  X  1 ,...,K } .A latent topic model is a topic model but the topics are not observed. Mathematically, a latent topic model is equivalent to a convex combination of a set of topic models. In this paper, the relevance-based topic model i s an extension of the latent Dirichlet allocation. Thus, we briefly review LDA as follows.
 Latent Dirichlet Allocation. In LDA [2], the weights and multinomial pa-rameters are random variables  X  and  X  ( d ) with conjugate Dirichlet priors. LDA can be represented by a graphical mod el (GM) as shown in Fig. 1 (a). The generation of D encoded in this graph is as follows.  X  Start  X  Sample from a Dirichlet prior with parameter  X  for the multinomial  X  z over  X  For each do cument d  X  X  1 ,...,D } 1  X  End For D { w 1 ,...,w  X  } = w , the joint probability is Marginalizing over  X ,  X  ,and z ,wehave P ( w |  X ,  X  )= Note that the posterior distribution for  X  ( d ) varies from document to document. Parameter Estimation of LDA via Gibbs Sampling. In LDA, the prior of  X  ( d ) and  X  , the model for the n th word w in a given document d can be approximated by a multinomial mixture model as follows multinomial parameter for words.

Recall that D is represented by w = { w 1 ,...,w  X  } . In principle, given samples ative frequencies. The key inferential problem is how to compute the posterior distribution P ( z | w ) , which is directly proportional to the joint distribution In practice, however, it is obvious that the denominator in (4) is an enormous discrete distribution with K  X  parameters, and sampling directly from P ( z | w ) is not feasible [5]. Alternative method s have been used to estimate the param-eters of topic models [2,5,11]. Therefore, we use the stochastic methods for the estimation problem.

In the Gibbs sampling method, z n is sequentially sampled using the so-called According to the graphical model depicted in Fig. 1 (a), we have n  X  n is the number of words in d n (the document that term n belongs to) n  X  n over k =1 ,...,K .

Prior parameters  X   X  X  and  X   X  X  are used to balance the prior knowledge and the simply given by The symbols in (6) have the same meaning as in (5) except that the current instance is not excluded.
 2.2 Topic Model with Background Distribution Topic models are unsupervised probabilistic models for the document collection and are generally used for extracting coarse-grained semantic information from the collection [2,7]. It assumes that words of a document are drawn from a set of topic distributions. Chemudugunta et al. [3] proposed SWB (special words with background) models for different aspects of a document. In SWB, special words are incorporated into a generative model. Each document is represented as a combination of three kinds of multinomial word distributions. Fig. 1 (b) shows the graphical model of SWB. A hidden switch variable y is used to control the generation of a word. y =0 means that the word is sampled from a mixture distribution  X  z over general topics z , y =1 means that the word is drawn from the document-specific multinomial distribution  X  with symmetric Dirichlet prioris parametrized by  X  1 ,and y =2 means that the word is a background word and sampled from the corpus-level multinomial distribution  X  with symmetric Dirichlet prioris parametrized by  X  2 .
 The conditional probability of a word w given a document d canbewrittenas: The model has been applied in informati on retrieval, and it has been showed that the model can match documents b oth at a general level and at a specific word level. 3.1 LDA with Model Expansion In the RDTM, we introduce a word-level switch variable x n for a topic z n in the graphical model of LDA. For each word position, the topic z is sampled from the distribution over topics associated with a latent variable x .Itisusedto determine whether to generate the word from a document specific distribution then x =1 , and the word is sampled from the general topic z specific to the query  X  ( d ) q .Otherwise,then x =0 , and the word is sampled from the general topic z specific to the document  X  ( d ) . In RDTM, observed variables include not only the words in a document but also the words in the set of queries that are relevant to the document.

The generation of  X  D =  X  w is stated as follows.  X  Start  X  Sample from a Dirichlet prior with parameter  X  for the multinomial  X  z for  X  SamplefromaBetapriorwithparameter  X  for the Bernoulli  X  ;  X  For each do cument d  X  X  1 ,...,D }  X  End Fig. 1 (c) depicts the graphical model expansion. Again, for each  X  d  X   X  D ,the observed variables consist of d and q ( d ) . Given hyperparameters  X ,  X  q , X  ,and  X  , the joint distribution of all observed and hidden variables can be factorized as follows
P (  X  d , z , x , X , X  q , X , X  |  X ,  X  q , X , X  )= P (  X  |  X  ) P (  X  |  X  )  X  Recall that in Section 2.1, the generation model for the word w in a given document d is approximated by where  X   X  and  X   X  are estimated by the Gibbs samples drawn from the posterior can be sampled from the following probabilities  X   X  x n can be sampled from the odds where  X   X  0 = n 3.2 RDTM for Information Retrieval When the corpus-level topic models are di rectly applied to the ad-hoc retrieval level topic distribution is too coarse [3,19]. Significant improvements can be achieved through a linear combination with the document model [3,18,19]. In the language-model approaches for information retrieval, the query likelihoods given the document models, P LM ( q |M d ) are used to rank the documents. By the bag-of-words assumption, the query likelihood can be expressed by [13] where M d is the language model estimated based on document d . The proba-bility P ( w |M d ) is defined as follows [23], query term w generated in the entire collection D (resp. d ). n d is the length of document d . Note that (14) is a Bayesian learning of the word probability with results in [19].

Compared to the standard query likelihood document model, RDTM offers a new and interesting framework to model documents. Motivated by the significant improvements obtained by Wei and Croft [19], we formulate our model as the linear combination of the original query likelihood document model and RDTM The RDTM model facilitates a new representation for a document based on topics. Given the posterior estimators (11), the query likelihood P RDTM ( q |M d ) can be calculated as follows: In this section we empirically evaluate RDTM in ad hoc information retrieval and compare it with other state-of-the-art models. 4.1 Data and Setting We perform experiments on two TREC tes ting collections: namely the Associ-ated Press Newswire (AP) 1988-90 on disk 1-3 with topics 51-150 as test queries, and the Wall Street Journal (WSJ) with topics 151-200 as test queries. Queries are taken from the  X  X itle X  field of TREC topics only (i.e., short queries). The remaining TREC topics are used as the his torical queries together with their corresponding relevant documents to learn the document models in the training phase. In other words, topics 151-300 are used as the historical queries for the AP task, while topics 51-150 and 201-300 are used as the historical queries for the WSJ task. The preprocessing steps include stemming and stop word removal.
Several parameters need to be determ ined in the experiments. We use sym-and  X  =0 . 5 , which are common settings in the literature. The number of topics K are set to 200. The interpolation parameter  X  is selected by cross validation, and it is finally set to 0.7.

The retrieval performance is evaluated in terms of the mean average precision (mAP) and 11-point recall/precision. To eva luate the significance of performance difference between two methods, we employ the Wilcoxon test [8] for the out-comes. All the statistically significant performance improvements with a 95% confidence according to the Wilcoxon test are marked by stars in the results. 4.2 Results We compare the effectiveness of our rele vance-dependent topic model (RDTM) with the query likelihood (QL) model [23], LDA-based document model (LBDM) [19] and special words with the background (SWB) model [3]. In addintion, we also adds the query terms into the relevant documents when training the LDA-based model. That is, we expand each document in the training set with the queries known to be relevant, and then learn the document language model based on the augmented text data. This method is referred to as RDTM0. For the query likelihood model, we use the Dirichlet mod eldescribedin(14).Retrievalresults on the WSJ collection are presented i n Table 1. We can see that both RDTM0 and RDTM achieves better results than QL, LBDM and SWB. This shows that incorporating query-doc ument relevance into the document model by using the relevant past queries is helpful to IR. From Table 1, it is obvious that both RDTM0 and RDTM significantly outperform QL. To evaluate the significance of improvements over LBDM and SWB, we employ the Wilcoxon test [8] with a 95% confidence. Statistically significant improvements of RDTM0 and RDTM over both LBDM (marked by *) and SWB (marked by **) are observed at many recall levels.
 Table 2 compares the results of QL, LBDM, and RDTM0 on two data sets. We can see that both LDA-based models (LBDM and RDTM0) improve over the query likelihood (QL) model. The mAP of RDTM0 is 0 . 2305 ,whichisbetter than those obtained by LBDM ( 0 . 2162 )andQL( 0 . 1939 ) on the AP collection. The relative improvement in mAP of RDTM0 over LBDM is 6 . 61% .Inthesame measure, the mAP of RDTM0 is 0 . 3489 , which is better than those obtained by LBDM ( 0 . 3347 )andQL( 0 . 3162 ) on the WSJ collection. In the table,  X * X  and  X ** X  mean that a significant improvement is achieved over QL and LBDM, respectively. In Table 3, we compare the retrieval results of RDTM with the LBDM and SWB on two data sets. Obviously, RDTM achieves improvements over both LBDM and SWB, and the improvements are significant. Considering that SWB has already obtained significant improv ements over LBDM, the significant per-formance improvements of RDTM over SWB are in fact very encouraging. The mAP of RDTM is 0 . 3536 , which is better than those obtained by SWB ( 0 . 342 ) and LBDM ( 0 . 3347 ), with a 3.39% and 5.65% improvement in mean average precision, respectively, on the WSJ coll ection. In the same measure, the rela-tive improvements of mAP of RDTM over SWB and LBDM are 1 . 85% ,and 7 . 12% , respectively, on the AP collection. In the table,  X * X  and  X ** X  mean that a significant improvement is achieved over LBDM and SWB, respectively.
Several comments can be made based on the results. First, IR performance can be improved by using topic models for document smoothing, as it is ob-served that RDTM, SWB, and LBDM achieve higher mAP than QL. Second, the document representation with known relevant queries works well, as both data expansion and model expansion lead to improvements over the baseline methods. This new representation could be applied to other retrieval, classifica-tion, and summarization tasks. In this paper, we investigate the relevance dependent generative model for text. The new methods for ad hoc information retrieval simultaneously model docu-ment contents and query information into the topic model based on latent Dirich-let allocation. One implementation is a data expansion approach that directly adds query terms into the related docume nts for the training of the LDA-based model (RDTM0), and the other is a model expansion approach that assumes relevant information about the query may affect the mixture of the topics in the documents (RDTM). Model expansion leads to a larger graph for which the parameter estimation is realized by the method of Gibbs sampling. Experimen-tal results on the TREC collection show that our proposed approach achieves significant improvements over the baseline methods using the query-likelihood (QL) model and the general LDA-based document model (LBDM and SWB).
In the future, it would be interesting to explore other ways of incorporating relevance into the topic-model framework for text. As in [21], we will try to explore the utility of different types of topic models for IR. In addition, we can test our approach on large corpora (such as the World Wide Web) or train our model in a semi-supervised manner. Alternatively, we can try to add more information to extend the existing model.
 Acknowledgments. This work was supported by Ministry of Economic Affairs, Taiwan, R.O.C. project under No. B3522P1200.

