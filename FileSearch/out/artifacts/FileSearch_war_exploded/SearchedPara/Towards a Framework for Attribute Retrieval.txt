 In this paper, we propose an attribute retrieval approach which extracts and ranks attributes from HTML tables. We distinguish between class attribute retrieval and instance attribute retrieval. On one hand, given an instance (e.g. University of Strathclyde) we retrieve from the Web its at-tributes (e.g. principal, location, number of students). On the other hand, given a class (e.g. universities) represented by a set of instances, we retrieve common attributes of its instances. Furthermore, we show we can reinforce instance attribute retrieval if similar instances are available.
Our approach uses HTML tables which are probably the largest source for attribute retrieval. Three recall oriented filters are applied over tables to check the following three properties: (i) is the table relational, (ii) has the table a header, and (iii) the conformity of its attributes and values. Candidate attributes are extracted from tables and ranked with a combination of relevance features. Our approach is shown to have a high recall and a reasonable precision. Moreover, it outperforms state of the art techniques. H.3.3 [ Information Systems ]: Information Search and Re-trieval Measurement, Experimentation, Algorithms information retrieval, attribute retrieval
Most information retrieval systems answer user queries with a list of documents, but there are many information needs that can be answered with one or more extracts of information coming from one or more documents. For in-stance, the query  X  X eatures of Mac Book X  is not asking for all pages speaking of Mac Books rather than for a list of features.

Inspired by the work in [6], we distinguish three types of information needs that can be answered differently:
When the query is specifically asking for an attribute, we can return its value right away. When the query is an instance, we can propose a summary of salient attributes (properties). When the query is a class of instances, the re-sult can be a comparative table of the class instances with their attributes. Figure 1 shows what these results might look like. A direct application can be Google Squared 1 , a commercial tool that produces similar results. We will call search based on attributes, instances and classes relational search .

Relational search is not the only application where at-tributes play a crucial role. They can also be used for query suggestion [4], faceted search [5] and aggregated search [15]. For instance, given the query  X  X ndonesia X  and using attributes of  X  X ndonesia X  we can produce suggestions such as  X  X ndonesia climate X ,  X  X ndonesia capital X  and  X  X ndonesia hotels X , while in a faceted search context we can navigate returned results on the query  X  X odern architecture X  by at-tributes such as  X  X ountry X ,  X  X rchitecture style X  and so on. Attributes can also be used to summarize content about in-stances/classes.

In this paper, we deal with attribute retrieval which falls in the above defined framework. More precisely, we deal with three attribute retrieval problems. First, we retrieve relevant attributes for one given instance (e.g.  X  X niversity of Strathclyde X ). Second, we retrieve attributes for a given class (e.g.  X  X niversities X ) represented as a set of instances. Third, we retrieve attributes for one instance when some other similar instances are given (from the same class). Attributes are extracted from HTML tables in the Web. There are some main reasons behind this choice. First, ta-bles are widespread across the Web. Second, many tables contain relational data (i.e. data in a relational form) [8]. Third, tables have already been used sucessfully for attribute extraction [10, 9]. http://www.google.com/squared Figure 1: Examples of relational search results
However, the task is not easy. Many of the HTML tables are used for layout design and navigation. In [8], authors estimate that only about 1% of the Web tables contain re-lational data. Furthermore, some relational tables have no headers (schema of attributes) and it is not easy to retrieve all relevant tables for a given instance.

We propose an attribute retrieval approach at Web scale which takes into account the above issues. For every given instance, we issue the instance as a query to a search en-gine. The retrieved documents are used to extract tables from. Successively, we apply 3 filters to candidate tables to check the following properties: (i) is the table relational, (ii) has the table a header, (iii) the conformity of its attributes and values. Then, depending on the considered problem, we rank candidate attributes with a combination of relevance features.
 Our approach integrates the work of Cafarella et al. [8]. Their work is at our knowledge the largest mining of Web tables for search purposes. They show we can filter out many tables that are not relational and that do not have a header. In addition to their work, we filter out many table columns (or rows) that do not contain attributes (name and values).
We also integrate our previous work [16, 17], which shows that we can rank attributes using a linear combination of rel-evance features such as table match, document relevance and external evidence from Web search, DBPedia and Wikipedia. In our previous work, we have focused on instance attribute retrieval, while in this paper we deal with instance and class attribute retrieval with an extended experimental setup. To the best of our knowledge, this is the first work that put to-gether attribute retrieval at instance and class level through the use Web tables. We also introduce a technique to rein-force attribute retrieval and we show some initial work on attribute value retrieval (which is not the main goal of this paper). The combination of these techniques contributes to build a framework for attribute retrieval at Web-scale.
The paper is structured as follows. The next section is about related work. In section 3, we describe our approach including filtering and ranking of attributes. Then we de-scribe the experimental setup (section 4) and results (section 5) following with conclusions.
Attributes can find different uses. They can be used for the summarization (representation) of content [29, 18], to as-sist search such as for query suggestion [4] and faceted search [5], or for question answering [13]. Attribute acquisition methods can be domain-independent or domain-dependent. Among domain-dependent approaches, we can mention ap-proaches that focus on products. In this domain, attributes have been used to improve product search and recommen-dation [18, 22], but also to enable data mining [27].
Attribute retrieval provides another granularity in Web search. This can interest communities that propose a more focused access to information or communities that envision aggregating pieces of information such as aggregated search [19, 15].

To acquire attributes from the Web, it is common to use decoration markup [30, 27, 11] and text [4, 24, 20]. HTML tags (for tables, lists and emphasis) have also been shown to help for attribute acquisition [30, 27]. Wong et al. [27] combine tags and textual features in a Conditional Random Fields model to learn attribute extraction rules, but they need a seed of relevant documents manually fed.

Another common technique to acquire attributes is through the use of lexico-syntactic rules. For example, Pasca et al. [1, 21] use rules such as  X  X  of I X  and  X  X  X  X  A X  to acquire at-tributes from query logs. Authors represent the class as a set of instances and multiple class instances are used to improve extraction. In [2], authors use more precise lexico-syntactic rules such as  X  X he A of I is X , but recall of these rules is lower. In [22], Popescu et al. use lexico-syntactic rules to extract product attributes from reviews.

At last, tables are known to be a mine for relational data and attributes. Cafarella et al. [8, 7] show we can iden-tify billions of relational tables in the Web. In [10], Chen et al. identify attributes using column (or row) similarities. Another common technique to extract attribute from tables is through wrapper induction [9, 11, 26]. Given a training set or a set of similar documents, wrapper induction learns extraction rules. Many wrappers extract at record level, but they do not distinguish between attribute name and at-tribute value. Furthermore, wrappers are precision oriented and they work well only for some sites.

To summarize, current attribute acquisition techniques can obtain a high precision. Although many of these tech-niques produce a considerable number of attributes, they cannot cover the needs that can be answered with the Web. Most of them are conceived to work offline and they cannot extract instance attributes whatever the instance.

Our work is inspired by the work in [1] and [7, 8]. It differs from [1], in that we do not use lexico-syntactic rules but Web tables to identify attributes. It differs from the work in [7, 8], in that we do not use tables for relation search. We make use of learnings in [7, 8], but we introduce another filter at line level and we introduce relevance features. The combination of filters and relevance ranking allows us to enable attribute retrieval which was not investigated in [7, 8].

We retrieve attributes for instances and classes. A clear distinction among class attributes and instance attributes can be found in [1]. They represent the class as a set of instances and they extract class attributes using class in-stances. We use the same setup to retrieve attributes for classes.

Class instances can also be acquired automatically. Here, we can mention the work done in named entity recognition [12, 23]. Class instances can be also acquired from hierar-chies such as the ones that derive from quality sources such as Wikipedia or DBPedia. Hearst [14] shows that we can ex-tract class instances using the pattern  X   X  such as  X  X  X   X  (e.g. cities such as London, Paris and Rome). However, class ac-quisition methods need to be improved to have high recall and reasonable precision at Web scale.We prefer separating the class acquisition issues from class attribute retrieval. In this paper, we thus consider that class instances are given.
We consider three different attribute retrieval problems detailed below.

Problem 1: We consider an information retrieval situa-tion where the query is an instance and the results is a list of attributes. Concretely, given an instance  X  , attribute retrieval should extract and rank attributes with re-spect to their relevance to  X  .

Retrieving attributes for whatever instance can be quite complex. There is no knowledge base with a list of attributes for the instance, but the Web with its size can help us re-trieve many of them. We need methods that can work for most instances and are domain-independent. In this paper, we aim retrieving attributes at Web scale using HTML ta-bles which are known to be a huge source for relational data.
Due to the complexity of the problem, we deal in this paper with attribute names only. To illustrate, given the instance  X  X rance X , we want to retrieve its attributes such as  X  X apital X ,  X  X resident X ,  X  X rea X ,  X  X opulation X ,  X  X DP X ,  X  X DP per capita X , etc. We do not focus on attribute values (e.g.  X  X aris X  is the value of the attribute  X  X apital X  for the instance  X  X rance X ). Although attribute names and values are usually met in the same line in tables (row or column), selecting the correct attribute value is not easy. We will discuss later shortly some of our experiments regarding attribute values, leaving for a future publication the entire study on attribute value retrieval.
 We extract attributes from the lines (rows or columns) of HTML tables from the Web. To do so, we use the following procedure. Initially, we issue the instance  X  as a query to a search engine and we retrieve tables from the retrieved documents. These tables are potentially relevant for the instance and are used to extract attributes from. However, the problem is far away from being solved. Tables in the Web are quite heterogeneous and many of the retrieved tables are partially or not relevant.

Before ranking attributes, we apply three filters on tables and attributes namely relational filter , header filter , and at-tribute line filter . The first two are the same as in [8]. They are recall-oriented classifiers that can filter out many tables that are not relational and that do not have headers. Still, after applying these filters there remain many tables which are not relational. As well there are many tables which are almost relational such as table 2 in figure 2. Instead of fil-tering out this kind of tables, we introduce another filter at column (or row) level which we call attribute line filter . Each line is checked for its conformity for being an attribute line i.e. an attribute name followed with attribute values of similar format and length.

After applying the three filters, we rank the remaining attribute with respect to their relevance (depending on the retrieval problem). The task is not easy. If we consider only tables that match the instance, we would lose many rele-vant tables (e.g. table 2 in figure 2 is relevant for  X  X rance X , but does not match it). To increase recall, we use all ta-bles from all retrieved documents. Extracted attributes are ranked with a relevance score  X  (  X , X  ) which is applied over an instance  X  and an attribute  X  .  X  (  X , X  ) is a simple linear com-bination of relevance features which was shown to work well to rank attributes in previous work [16]. It will be described in details in section 3.3.

Problem 2: We consider that the query is a set of in-stances  X  that represents a class. For example the class  X  X  X ountries X  X  can be represented by the set:  X  X  X rance X  X ,  X  X  X taly X  X ,  X  X  X K X  X , etc. The goal is to retrieve common relevant at-tributes for the class. The relevant attributes should not be specific to one instance only, but to all the class.
For every instance  X   X   X  , we repeat the same procedure as in problem 1 to retrieve candidate attributes. Then, we rank the set of all candidate attributes for all instances. The relevance score for the class (set of instances  X  ) is : In other terms, a relevant attribute for the class is likely to be present in many instances of the class. Though, we average the relevance score across all instances.

Problem 3: We reconsider retrieving attributes for an instance  X  , but we consider that a set of instances  X  the same class are given. The hypothesis is that having more instances can improve attribute retrieval due to the simple fact that similar instances share common attributes. However, even if most relevant attributes are shared among instances of the same class, many exceptions can occur. For example,  X  X ueen X  is relevant for the instance  X  X K X , but it is irrelevant for the instance  X  X SA X . To overcome this problem, when it comes to ranking, we privilege attributes retrieved for the instance  X  using only the instances in  X  + to reinforce ranking.

In practical terms, for every given instance (  X  , or  X  + we repeat the same procedure as in problem 1 to retrieve candidate attributes. The relevance score for the instance  X  is then computed as follows:
In the next sections, we will explain how our filters ( rela-tional , header and attribute line ) work and which are rele-vance features used to compute  X  (  X , X  ).
We build the relational filter and the header filter using the same features as done by Cafarella et al. in [8]. Features on the left of table 1 are used to learn a rule-based classifier for the relational filter and features on the right are used to learn a rule-based classifier for the header filter. Learning is done with a training set of human-classified tables described later in the experimental setup.

Features include the dimensions of the table (  X  1 ,  X  2 ), the fraction of lines with mostly nulls (empty cells) (  X  lines with non-string data (numbers, dates) (  X  4 ), statistics on the character length of cells and their variation (  X  5  X  ,  X  12 ,  X  13 ,  X  14 ) and conformity of the header cells (lower-case, punctuation, non-string data) (  X  8 - X  11 ).

The main difference with the work of Cafarella et al. is that they consider that relational tables are all oriented ver-tically, i.e. the table header (if present) is on top of the table and the attribute lines are vertical (the columns). For ex-ample, tables 1 and 3 in figure 2 are oriented vertically and table 2 is oriented horizontally. We extend their approach to work for both horizontally and vertically oriented tables. This is not difficult. We consider the origin table  X  and an-other table  X  which is obtained from  X  considering its rows as columns and its columns as rows.

If  X  passes both the relational and header filter, table columns are considered as candidate attribute lines to ex-tract attributes from. Similarly, if  X  passes both the rela-tional and header filter, the table rows are considered as candidate attribute lines. It can happen that both columns and rows are considered as candidate attribute lines. The latter are then passed to the attribute line filter.
Let  X  be the first cell of the line (row or column) and  X  be the rest of the cells of the line. We consider that a conform attribute line should contain an attribute name in the first cell and attribute values in the rest of the cells. Attribute values can correspond to one instance or multiple instances.
Typically, attribute names do not have much punctuation except of the colon and parenthesis. They rarely contain numbers. On the other hand, attribute values are usually in the same format (number, string, date) and their length is similar. Based on the above observations, we define the following features for the attribute line filter:
These features are then used to learn a rule-based classi-fier from a training set of human classified attribute lines de-scribed later in the experimental setup. Once candidate at-tributes are filtered, we rank the remaining set as explained in the following section.
It is not easy to tell whether an attribute is relevant for a given instance. There are many tables relevant to the in-stance where the instance is not even present in its cells. We propose combining different features to estimate a relevance score  X  (  X , X  ) for a candidate attribute  X  and an instance  X  . These features include a score on the match of the instance on the table, document relevance and external evidence from DBPedia, Wikipedia and Web search.  X  (  X , X  ) is a simple lin-ear combination of these features 2 . Relevance features are described below.

Table match : Tables from which attributes are ex-tracted should be relevant for the instance. A neces-sary condition but not sufficient for a table to be relevant is to be present in a relevant document for the instance. In some tables, the instance appears explicitly. Such ta-bles might be better candidates for extraction. In fact, we analyzed samples of Web tables and we observed a higher concentration of relevant attributes within tables that match the instance. The table match feature will measure at which extent the table matches the instance.
This combination has been shown to be the most effective way to combine features in previous experiments.
Let  X  be an attribute extracted from a table  X  for an instance  X  . The match of an instance within a table cell  X   X , X  is measured with the cosine distance among the terms of the instance and the terms of the table cell. Let  X  and  X  be the vector representation of the instance and the ta-ble cell content. We have  X  = (  X  1 , X  , X  2 , X  ,..., X   X , X  (  X  1 , X  , X  2 , X  ,..., X   X , X  ). Each dimension corresponds to a sep-arate term. If a term occurs in the instance, its value is 1 in  X  . If it occurs in the table cell content, its value is 1 in  X  . The match is computed with the cosine similarity among the vectors.
 The table match score is computed as the maximal match within table cells:
However an attribute name is unlikely to be present in the same line (row or column) with the instance name, while the relevant attribute value is likely to appear in the same line with the instance. The latter can be also observed in table (1) in figure 2. We will define the shadow area of a cell  X  as the set of cells in the same row and same column with  X  . There are some exceptions to this rule. We call headline cells, the ones that have are spanned (  X  X  X  X  X  X  X  X  X  X  &gt; 1) cover the entire table width such as the ones in table 2 in figure 2. Headline cells usually act as titles that introduce parts of the table. We consider that the headline cells are not part of the shadow of another cell (see figure 3). We define the match score of an attribute as the difference between the table match score and the shadow match score.  X  X  X  X  X  X  X  (  X , X , X  ) =  X  X  X  X  X  X  X  (  X , X  )  X   X  X  X  X  X  X  X  (  X , X  X  X  X  X  X  X  X  X  (  X  )) (4) where
Document relevance : If a document is relevant for the instance, the tables within the document are likely to be relevant for the instance. We should though take into account the document relevance. More precisely, let  X  X  X  X  X  X  X  X  X  X  X  be the number of retrieved results for an instance  X  and  X  X  X  X  X  X  be the rank of a document  X  within this list. We compute:
Search hits : Search hits is a feature that has already been used for attribute extraction [30, 22]. It corresponds to the number of search results returned by a Web search en-gine to a query  X  X ttribute of instance X  within double quotes (successive ordering of terms is obligatory, e.g.  X  X apital of France X ). As done in literature, we use the logarithm (base
The colspan is an HTML attribute that defines the number of columns a cell should span. 10) of the search hits count. To normalize, we used the following observation. Few attributes score higher than 6 i.e  X  X  X  X  10 (  X  X  X  X  X  X  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X  X  (  X   X  X  X   X  )) &gt; 6. All the attributes that score higher than 6 were given a score of 1, the other scores were normalized by 6. Doing so, we have all scores in the interval [0 , 1].

DBPedia feature : DBPedia represents a large ontology of information which partly relies on information extracted from Wikipedia [3]. Given an instance  X  and an attribute  X  , the DBPedia feature  X  X  X  X  X  X  X  X  X  X  (  X , X  ) is equal to 1 if  X  is found as an attribute of  X  in DBPedia.

Wikipedia feature : Although information in DBPedia is much more uniform, there exist many attributes in Wikipedia infobox tables which are not present in DBPedia. Infobox tables are available for many Wikipedia pages. They con-tain lists of attributes for the page. Given an instance  X  and a candidate attribute  X  , we set the Wikipedia feature  X  X  X  X  X  X  X  X  X  X  X  X  X  (  X , X  ) to 1 if  X  can be found in the infobox of a page for  X  in Wikipedia.

Other features: It is difficult to find features which help rank attributes that are domain-independent and apply at large-scale. We excluded from the relevance features the fre-quency feature . This feature is meant to measure how often an attribute appears within tables of relevant documents. Intuitively, we can think that a relevant attribute will repeat more frequently than non-attributes or irrelevant attributes. We observed that candidate attributes that repeat the most are the ones that are used in ads or forms. Thus, candidate attributes such as  X  X ogin X ,  X  X earch X ,  X  X revious X , etc. were the ones that were favored by this feature. To tackle this issue, we developed a stop-word list for attributes, but even with this list, previous experiments did not show the interest of the frequency feature. This may be due to the fact that our list is still incomplete and need to be built using the whole web. Another alternative is to divide frequency by the frequency of the candidate attribute in the entire col-lection of documents being considered (analogous to tf-idf ). In other terms, an attribute has to be more frequent in rel-evant documents than in a random sample of documents. Once again, preliminary experiments did not demonstrate the validity of the feature. We thus leave for future work a correct integration of this feature in the evaluation of  X  (  X , X  ).
Combination of features At last,  X  (  X , X  ) is evaluated according to the following formula: with  X  the table from which  X  was extracted and  X  the document containing  X  .
Data set: To evaluate our approaches, we built a dataset of classes and instances. First we chose a set of classes and then 10 instances per class. To choose instances and classes, we used sampling to avoid biases. 5 participants had to write down 10 classes each. Classes could be broad (e.g. Coun-tries ) or specific ( French speaking countries ). We sampled 20 classes out of the 50. This is a reasonable amount as in state of the art approaches [24, 30, 4, 28, 20], the number of assessed classes varies from 2-25 classes. Similarly for each selected class, we asked the 5 participants to write down 10 instances. Sampling and removing dupli-cates we obtained 10 instances per class.

This is the list of classes: rock bands, laptops, american universities, hotels, software, british army generals, chancel-lors of German, American films, IR papers, SLR cameras, 2000 novels, Nirvana songs, Nissan vehicles, programmable calculators, countries, drugs, companies, cities, painters, mo-bile phones . The entire dataset can be found in: http://www.irit.fr/  X  Arlind.Kopliku/FORCEdataset.txt .
General setup: For each instance of the dataset (200 in-stances), we retrieved top 50 search results using the Yahoo! BOSS API. These pages are used as a seed to extract tables and attributes. For each table, we apply the three filters.
For the problem 1, candidate attributes are ranked using  X  (  X , X  ). For the second problem, attributes are ranked using  X  (  X , X  ) For the third problem attributes are ranked using  X  (  X , X  ) reinforced with  X  (  X , X  + ) as described before.
Learning filters: The three filters correspond to rule-based classifiers. They were trained using the previously mentioned features using the WEKA package [25]. From the extracted tables, we randomly selected a sample of 3000 tables (with more than 1 row and more than 1 column) which were judged by 3 assessors. For each table assessors had to tell, if the table is relational. If  X  X es X  they had to tell if the table is oriented vertically or horizontally and whether the table has a header.

Similarly, we choose a sample of 3000 random attribute lines from our dataset of tables. They are as well assessed from our assessors. For each attribute line, the assessor has to tell if it is a conform attribute line i.e. it contains an attribute name and attribute values.

Similarly to [8], we cross-validated the trained classifiers by splitting the human-judged dataset into five parts. Each time four parts are used training and the fifth for testing. We trained five different classifiers, rotating the testing set each time. Performance numbers are averaged from the resulting five tests.

Ranking: Candidate attributes are ranked based on the relevance features and depending on the problem being treated. 5 human participants evaluated this task assessing each a disjoint set of attributes. To measure the performance of ranking we had to measure top attributes retrieved for the three problems. To assess the performance over the first and third problem, assessors judged the 30 top ranked at-tributes for each instance being the target. To assess the performance for the problem 2, top 30 attributes were as-sessed for all classes of our dataset.

Assessments were binary. An attribute is assigned 1 if it is relevant for the instance/class it was retrieved for It were assigned 0 otherwise. During evaluation, assessors could access the source page of the attribute or other sources (Web search, dictionary) to make their decision. Attributes were shown in alphabetical order to avoid any bias.
This section is about experimental results. First, we present the performance of the three filters. We then analyze the performance of attribute retrieval.
First, we analyzed all retrieved tables for all instances in our dataset. We found that only 16.9% of the tables had more than one row and more than one column. Within the 3000 tables that were assessed only 23% were considered relational. We can thus estimate a concentration of 3.9% re-lational tables within the entire set of tables in the retrieved Web pages. Now, we will analyze the effect of the filters. It is important to point out that our goal is not to compare with the results of Cafarella et al. [8], but to integrate their work and show its effect on attribute retrieval.

Relational filter: As we mentioned earlier, we learn sep-arately a classifier for relational tables that are oriented hor-izontally and another classifier for relational tables that are oriented vertically. Results are shown in table 2 aside with the results obtained by Cafarella et al. [8].

We tuned classification for high recall over positives. The performance of our classifiers is similar to Cafarella et al. The classifier of relational tables oriented horizontally re-tains 82% of the true positives and it filters out 94% of the true negatives. Similarly, the classifier of relational tables oriented vertically retains 81% of the true positives and it filters out 95% of the true negatives. After this filtering step, we will have about 45% of relational tables within our corpus out of an initial estimated concentration of about 3.9%.
Header filter: In table 3, we show results for the header filter. Results are better than those obtained by Cafarella et al. This can be explained with the fact that we use top search resuts which are presumably of better quality than random Web pages. In particular, we found that most of the horizontally oriented relational tables have headers. They usually have two to three columns and are easier to classify.
The header classifier for relational tables oriented hori-zontally retains 95% of the true positives with a precision
We recall that only attribute names were assessed. of 96%. Although the header classifier of relational tables oriented vertically is less performant, it retains 89% of the true positives with a precision of 76%. After this filtering step, about 87.5% of the relational tables will have headers.
Attribute line filter: As well as the other filters, the attribute line filter is tuned for recall over positives. Results are shown in table 4. This filter retains 95% of the correct attribute lines, while it filters out about 55% of the incor-rect attribute lines. It clearly helps in filtering out useless attribute lines at the cost of 5% of correct attribute lines. Table 4: Precision and recall for the attribute line filter in relational tables
In the next section, we will discuss on the effectiveness of the filters for the attribute retrieval task.
For the first problem considered (instance attribute re-trieval), attributes are ranked with  X  (  X , X  ). In figure 4, we show precision at rank averaged over all instances. We can say that results are promising. At rank 10, we have a preci-sion of about 83%. At rank 20 we have a precision of about 72%. This means that if we apply this ranking for query suggestion, we will have that about 8.3 correct suggestions within top 10 suggestions and about 14.4 correct suggestions among the top 20.
 Figure 4: Precision at rank for instance attribute retrieval
Table 5 allows us to analyze the impact of each filter for attribute retrieval. Symbol * after the results of the first row indicates statistical significance using a paired t-test at p &lt; 0.05 (all filters against no filters). We can see that the attribute line filter has a significant impact in the ranking as well as the relational and header filter. Combining all three filters provides the best performance. This is proba-bly because the filters remove useless tables and table lines, making it easier to rank attributes.

Attribute values: The above results concern only at-tribute names. We will introduce here some of our work on attribute values. Given a set of candidate attribute lines, we can rank attribute values in different ways. An easy way to rank is the following. First, we rank attribute lines with  X  (  X , X  ). Then, if the attribute line has two rows (or columns), we use the second cell as attribute value. If the instance name appears ortogonally with some cell (except Approach p@1 p@10 p@20 p@30 All filters 0.94* 0.83* 0.72* 0.61* Attribute line filter 0.90 0.82 0.70 0.61 Header and relat. filter 0.89 0.83 0.70 0.59 No filters 0.84 0.81 0.68 0.59 the first) in the attribute line, we select this cell as the at-tribute value for the attribute. Otherwise, we consider that the attribute value is the union of all cells (except the first). We call this attribute as multi-value. This simple method was shown to acquire correctly attribute values for 66% of relevant attributes in our dataset. These promising results will be completed by a complete analysis of attribute values retrieval in future work.
Figure 5 shows precision at rank for class attribute re-trieval (problem 2) when we represent the class as a set of 10 instances. We retrieve attributes for each of the 10 in-stances of the class and we rank with the function  X  (  X , X  ) defined in section 3 for the second problem. As expected, results are better than for instance attribute retrieval. We have precision of 0.95 at rank 10, a precision of 0.84 at rank 20 and a precision of 0.77 at rank 30. For the same problem, if we use 5 instances per class, we obtain precision 0.89 at rank 10, precision 0.76 at rank 20 and precision 0.66 at rank 30. In general, we noticed that class attribute retrieval per-formance improves with the increase of available instances. Figure 5: Precision at rank for class attribute re-trieval
In figure 6, we show how results look like in one Web application we built. Here, we rank attributes names and values with  X  (  X , X  ) and then we select attribute values as described in section 5.2. Then we add some photos using image search, that can be seen as some decoration in our application. The example in figure 6 concerns a query com-posed of three instances from the class  X  X obile phones X .
We also analyzed results class by class. Figure 7 shows precision at rank 30 by class. We can see that attribute retrieval can vary across classes. For some classes in our dataset, performance is lower such as for  X  X rugs X  and  X  X ro-grammable calculators X , while for others precision remains high such as for  X  X ritish army generals X ,  X  X hancelors X ,  X  X oun-tries X . The quality of attribute retrieval by class depends on the quality of attribute retrieval for every instance of the class. Results are relatively heterogeneous. Some instances are ambiguous. For some others, returned search results are slightly relevant. For some instances or class of instances, there is more tabular data than for other within search re-sults. Furthermore, for some classes there exist more rele-vant attributes than for others.
To reinforce instance attribute retrieval (problem 3), for each target instance we use the 9 other instances of the same class. Attributes are ranked combining both  X  (  X , X  ) and  X  (  X , X  + ) as described earlier. The intuition is that the other instances can provide additional evidence of relevance for attributes.

In figure 8, we can see the impact of reinforcement on instance attribute retrieval. The lower curve corresponds to instance attribute retrieval without reinforcement, while the upper curve corresponds to the reinforced attribute re-trieval. We can see that reinforcement improves significantly results 6 .

We can conclude that having multiple instances of the same class helps attribute retrieval for instance queries. We can explain this phenomena due to the fact that similar
The significance of the improvement was validated using a paired t-test with p &lt; 0.05 for P@10, P@20, P@30. instances have similar attributes. Promoting common at-tributes has a positive impact on retrieval.
To estimate in a reasonable time the recall of our method, we used 4 classes randomly selected from our dataset namely  X  X LR cameras X ,  X  X ountries X ,  X  X ompanies X ,  X  X issan vehicles X . For all instances of the class, our assessors evaluated all can-didate attributes (the ones that are not filtered out).
We found an average of 918 distinct candidate attributes per class (SLR cameras 689, countries 1253, companies 804, vehicles 925). Among them, there are on average 256 rele-vant attributes per class (cameras 303, countries 213, com-panies 160, vehicles 347). This is a considerable amount of relevant attributes and it shows the potential of our method. We can say that tables are a good source for attributes and that our filters keep a high concentration of relevant attributes.

It is difficult to estimate how many relevant attributes there are for a given class or instance as it is difficult to write down all of them. We used Wikipedia and DBPedia to estimate their coverage in terms of attributes and then compare it to our method. We used 10 instances for each of the 4 classes mentioned above which have Wikipedia pages and are present in DBPedia. We then extracted attributes which are either present in DBPedia or in infoboxes from Wikipedia. We found an average of 25 distinct relevant at-tributes per class (cameras 8, countries 38, companies 37, vehicles 18). Although Wikipedia and DBPedia are quality and huge sources of information, they have a much lower recall than our method.
We also compared our approach to the ones based on lexico-syntactic rules [4, 30, 24, 21] Lexico-syntactic rules are common for attribute extraction . We tested the lexico-syntactic extraction rules in our retrieval framework with our dataset. Concretely, we use the patterns  X   X  of  X   X  and  X   X   X  X   X   X  as done in [21, 24]. We collect candidate attributes for 10 instances of all classes.

To compare we used the class attribute retrieval approach (problem 2). We use top 50 search results for all instances of our dataset as extraction seed for both techniques. The at-tributes extracted by the lexico-syntactic extraction method are ranked with the same scoring (excluding table match score which does not apply to lexico-syntactic rules). Results are shown in table 6. Our method performs signifi-cantly better with 61% of of relevant attributes at rank 30 against 33% for the lexico-syntactic rules. We recall that symbol * after the results indicates statistical improvement using a paired t-test (p &lt; 0.05).
 Approach p@1 p@10 p@20 p@30 Our approach 0.94* 0.83* 0.72* 0.61* Lexico-syntactic rules 0.46 0.48 0.43 0.33 Table 6: Comparison with lexico-syntactic scores
We also compared both approach in term of recall, by considering only the 4 classes used to estimate recall. Lexico-syntactic rules have a lower recall, too. For each class, lexico-syntactic rules identify 55 candidate attributes on average. Among these there are about 24 relevant attributes per class (against 256 for our approach).

We can conclude that lexico-syntactic rules work well for certain applications such as queries, but they do not work well for long documents, especially in terms of precision.
In this paper, we propose 3 approaches for attribute re-trieval using HTML tables from the Web. Our approaches combine 3 filters and they rank attributes using relevance features. Results are promising both in terms of precision and recall.

We combine three filters. Two of them are already known in literature. They apply to HTML tables to filter out non relational tables and tables without header. In our experi-mental setup they prove their effectiveness. In addition, we propose a third filter which is specific to attributes. Sim-ilarly to the other two, this filter is shown to have a high recall over positives and to filter out a reasonable amount of useless data.

We treat 3 different problems for attribute retrieval. First, we retrieve attributes for a given instance. Then, we retrieve attributes at class level. Retrieval is shown to have a high recall and a reasonable precision for both problems. Then we propose a reinforced approach for instance attribute re-trieval. Using similar instances of the same class to reinforce attribute retrieval is shown to improve significantly results.
Our approaches have different advantages which were shown in this paper. First, they can be applied to whatever in-stances. They work at Web scale and they have a high recall. Furthermore, they outperform state of the art techniques for the same purpose.

We envision for future work the completition of the at-tribute retrieval framework with other recall-oriented or precision-oriented techniques. We believe that relations between classes, instances and attributes can be further explored for a bet-ter attribute retrieval. We will also explore more deeply the issue of attribute value retrieval. [1] E. Alfonseca, M. Pasca, and E. Robledo-Arnuncio. [2] A. Almuhareb and M. Poesio. Attribute-based and [3] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, [4] K. Bellare, P. P. Talukdar, G. Kumaran, O. Pereira, [5] O. Ben-Yitzhak, N. Golbandi, N. Har X  X l, R. Lempel, [6] M. J. Cafarella, M. Banko, and O. Etzioni. Relational [7] M. J. Cafarella, A. Halevy, D. Z. Wang, E. Wu, and [8] M. J. Cafarella, A. Y. Halevy, Y. Zhang, D. Z. Wang, [9] C.-H. Chang, M. Kayed, M. R. Girgis, and K. F.
 [10] H.-H. Chen, S.-C. Tsai, and J.-H. Tsai. Mining tables [11] V. Crescenzi, G. Mecca, and P. Merialdo. Roadrunner: [12] O. Etzioni, M. Cafarella, D. Downey, A.-M. Popescu, [13] M. Fleischman, E. Hovy, and A. Echihabi. Offline [14] M. A. Hearst. Automatic acquisition of hyponyms [15] A. Kopliku. Aggregated search: From information [16] A. Kopliku, K. Pinel-Sauvagnat, and M. Boughanem. [17] A. Kopliku, K. Pinel-Sauvagnat, and M. Boughanem. [18] Z. Nie, Y. Ma, S. Shi, J.-R. Wen, and W.-Y. Ma. Web [19] C. Paris, S. Wan, and P. Thomas. Focused and [20] M. Pasca and B. V. Durme. What you seek is what [21] M. Pasca and B. V. Durme. Weakly-supervised [22] A.-M. Popescu and O. Etzioni. Extracting product [23] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: a [24] K. Tokunaga and K. Torisawa. Automatic discovery of [25] I. H. Witten and E. Frank. Data mining: practical [26] T.-L. Wong and W. Lam. A probabilistic approach for [27] T.-L. Wong and W. Lam. An unsupervised method for [28] F. Wu, R. Hoffmann, and D. S. Weld. Information [29] M. Yoshida and K. Torisawa. A method to integrate [30] N. Yoshinaga and K. Torisawa. Open-domain
