 Retrieval effectiveness of temporal queries can be improved by tak-ing into account the time dimension. Existing temporal ranking models follow one of two main approaches: 1) a mixture model linearly combining textual similarity and temporal similarity, and 2) a probabilistic model generating a query from the textual and temporal part of document independently. In this paper, we pro-pose a novel time-aware ranking model based on learning-to-rank techniques. We employ two classes of features for learning a rank-ing model, entity-based and temporal features, which are derived from annotation data. Entity-based features are aimed at capturing the semantic similarity between a query and a document, whereas temporal features measure the temporal similarity . Through exten-sive experiments we show that our ranking model significantly im-proves the retrieval effectiveness over existing time-aware ranking models.

Searching in temporal collections such as news and web archives is not straightforward, because relevant documents are dependent on time. More precisely, documents are about events that hap-pened at a particular time period, and also accesses to the contents are time-sensitive, i.e., time is part of information needs as rep-resented by temporal queries (e.g., Illinois earthquake 1968 or Iraq 2001 ). As shown previously by analyzing real-world query logs, 1.5% of queries are explicitly provided with temporal crite-ria [19], i.e., containing temporal expressions, while about 7% of web queries have temporal intent implicitly provided [18].
As shown in previous work, taking the time dimension into ac-count in ranking can significantly improve the retrieval effective-ness of temporal queries [2, 7, 5, 14, 16, 18]. The existing time-aware ranking models follows one of two main approaches: 1) a mixture model linearly combining textual similarity and temporal similarity, and 2) a probabilistic model generating a query from the textual and the temporal parts of documents independently.
In this paper, we present a new approach for the above task: a time-aware ranking model based on learning-to-rank techniques . A fundamental issue in learning to rank, is the selection of features to be used in learning a ranking model. In this paper, we employ two classes of features that are derived from annotated documents: entity-based and temporal features. Thus, the main contributions of this paper are: 1) a new time-aware ranking model learned using two classes of features, 2) identification of appropriate features to be used, and 3) extensive experiments for evaluating the proposed time-aware ranking model and the features, using the New York Times Annotated Corpus in combination with temporal queries and relevance assessments from [2].

The organization of the rest of the paper is as follows. In Sec-tion 2, we give an overview of related work. In Section 3, we out-line the models for documents, annotated documents, and temporal queries, and present the ranking model. In Section 4, we propose two classes of features for learning a time-aware ranking model. In Section 5, we evaluate the proposed ranking model by comparing with existing time-aware ranking methods. Finally, in Section 6, we conclude the paper.
A number of ranking models exploiting temporal information have been proposed, including [2, 7, 16, 18]. In [16], Li and Croft incorporated time into language models, called time-based language models, by assigning a document prior using an exponen-tial decay function of a document creation date. They focused on recency queries, where the more recent documents obtain higher probabilities of relevance. In [7], Diaz and Jones also used docu-ment creation dates to measure the distribution of retrieved docu-ments and create the temporal profile of a query. They showed that the temporal profile together with the contents of retrieved docu-ments can improve average precision for the query by using a set of different features for discrimin ating between temporal profiles. Berberich et al. [2] integrated temporal expressions into query-likelihood language modeling, which considers uncertainty inher-ent to temporal expressions in a query and documents, i.e., tem-poral expressions can refer to the same time interval even if they are not exactly equal. Metzler et al. [18] considered implicit tem-poral information needs. They proposed mining query logs and analyze query frequencies over time in order to identify strongly time-related queries. Moreover, they presented a ranking concern-ing implicit temporal needs, and the experimental results showed the improvement of the retrieval effectiveness of temporal queries for web search.

In addition to the work above, there is also work that has fo-cused on recency ranking [4, 8, 9, 10]. That work is different to our work in term of a search scenario, since in our case a user can issue time as part of a query, so-called temporal criteria .There has also been work on analyzing queries over time, e.g., Kulkarni et al. [15] studied how users X  information needs change over time, and Shokouhi [21] employed a time series analysis method for de-tecting seasonal queries. For an entity-ranking task, Demartini et al. [6] analyzed news history (i.e., past related articles) for identi-fying relevant entities in current news articles.
In this section, we outline the m odels for documents, annotated documents and temporal queries, and then present a model for rank-ing documents.
Our document collection is composed of unstructured text doc-uments: C = { d 1 ,...,d n } . A document d is represented as a bag-of-words or an unordered list of terms: d i = { w 1 ,...,w where its publication date is denoted PubTime ( d i ) . Since the two classes of features are extracted from annotation data of documents, we present a model of annotated documents as follows.

For each document d i , its associated annotated document composed of 3 parts. First,  X  d i contains a set of named entities { e 1 ,...,e n } , where a named entity can be a person, location, or organization. The second part is a set of temporal expressions or event dates { t 1 ,...,t m } . Finally,  X  d i contains a set of sentences { s 1 ,...,s z } , where each sentence s y consists of tokens (terms), the part-of-speech and position information of each token.
A key aspect is that we distinguish between two temporal di-mensions associated with a document d i : 1) publication time (i.e., when a document was published), and 2) content time (i.e., what time a document refers to). The content time of a document (de-noted ContentTime ( d i ) ) or temporal expressions mentioned in d will be automatically extracted using a time and event recognition algorithm. The algorithm extracts temporal expressions mentioned in a document and normalizes them to dates so they can be an-chored on a timeline. As explained in [1], temporal expressions can be explicit, implicit or relative. Examples of explicit tempo-ral expressions are May 25, 2012 or June 17, 2011 that can be mapped directly to dates months, or years on the Gregorian cal-endar. An implicit temporal expression is an imprecise time point or interval, e.g., Independence Day 2011 that can be mapped to July 04, 2011 . Examples of relative temporal expressions are yesterday , last week or one month ago .
A temporal query q j is composed of two parts: keywords q and temporal expressions q time . Recall that a temporal expression can be explicitly provided as a part of temporal query, or implicitly provided. An example of the first type is Illinois earthquake 1968 where the user is interested in documents about Illinois earthquake in 1968 . Queries of the second type can be implicitly associated with particular time especially queries related to major real-world events, or seasonal queries[21]. Examples of a real-world event query and a seasonal query are Thailand tsunami associated with the year 2004 and U.S. presidential election , which can be as-sociated with the years 2000 , 2004 ,and 2008 .When q time given explicitly by the user, it has to be determined by the sys-tem [14]. In this paper, we assume that q time is explicitly provided.
A ranking model h ( d, q ) is obtained by training a set of labeled query/document pairs using a learning algorithm. A learned rank-ing model is essentially a weighted coefficient w i of a feature x An unseen document/query pair ( d ,q ) will be ranked according to a weighted sum of feature scores: where N is the number of features. Many existing learning al-gorithms have been proposed, and can be categorized into three approaches: pointwise, pairwise, and listwise approaches. For a more detailed description of each a pproach, please refer to [17]. In this work, we employ different learning-to-rank algorithms, such as, RankSVM [11], SVM MAP [22], and three stochastic gradient descent algorithms: SGD-SVM [23], PegasosSVM [20], and PA-Perceptron [3].
In this section, we present the two classes of features (temporal and entity-based) that are used for learning a time-aware ranking model.
Temporal features represent the temporal similarity between a query and a document. In this work, we employ five different meth-ods for measuring temporal similarity: L M TandL MT U [2], TS and TSU [14], and FuzzySet [12].

The time-aware ranking methods we study differ from each other in two main aspects: 1) whether or not time uncertainty is con-cerned, and 2) whether the publication time or the content time of a document is used in ranking. L M T ignores time uncertainty and it exploits the content time of d .L M T can be calculated as: where t d  X  ContentTime ( d ) , and the score will be equal to 1 iff a temporal expression t d is exactly equal to t q .L MT U concerns time uncertainty by assuming equal likelihood for any time interval t that t q can refer to, that is, t q = calculation of P ( t q | t d ) for L MT U is given as: where t d  X  ContentTime ( d ) . The detailed computation of | t | and | t d | is described in [2].

TS ignores time uncertainty. P ( t q | t d ) TS can be computed sim-d instead of the content time as computed for L M T. TSU exploits the publication time of d as done for TS, but it also takes time-uncertainty into account. P ( t q | t d ) TSU is defined using an exponen-tial decay function: where t d = PubTime ( d ) , DecayRate and  X  are constant, DecayRate &lt; 1 and  X &gt; 0 ,and  X  is a unit of time distance. The main idea is to give a score that decreases proportional to the time distance between t q and t d . The less time distance, the more temporally similar they are.

FuzzySet measures temporal similarity using a fuzzy member-ship function and it exploits the publication time of d for determin-ing temporal similarity. P ( t q | t d ) FuzzySet is given as: where t d = PubTime ( d ) . f 1 ( t d ) is a parameters a 1 ,a 4 ,n,m are determined empirically.
In addition to the temporal features presented above, we also use ten entity-based features aimed at measuring the similarity between a query and a document. The intuition is that a traditional term-matching method that use only statistics, e.g., TFIDF, ignores the semantic role of a query term. For example, consider the temporal query Iraq 2001 . A statistics-based model will rank a document having many occurrences of the terms Iraq or 2001 higher than a document with less frequency of the same terms without taking into account a semantic relationship between query terms, which can be determined by, e.g., a term distance in a sentence.
Entity-based features are computed for each entity e j in an an-notated document  X  d i , and the proposed features includes querySim , title , titleSim , senPos , senLen , cntSenSubj , cntEvent , cntEventSubj , timeDist ,and tagSim [13]. The first feature querySim is the term similarity score between q j and an entity e j in  X  d i . Here, we use Jaccard coefficient for measur ing term similarity. Feature title in-dicates whether e j is in the title of d i . Feature titleSim is the term similarity score between e j and the title. Feature senPos gives a normalized score of the position of the 1 st sentence where e curs in d i , while the feature senLen gives a normalized score of the length of the 1 st sentence of e j . Feature cntSenSubj is a nor-malized score of the number of sentences where e j is a subject. Feature cntEvent is a normalized score of the number of event sen-tences (or sentences annotated with temporal expressions) of e while the feature cntEventSubj a normalized score of the number of event sentences that e j is a subject. Feature timeDist is a nor-malized distance score of e j and a temporal expression within a sentence. Feature tagSim is the term similarity score between e and an entity tagged in d i . Note that the last feature is only appli-cable for a document collection provided with tags (e.g., the New York Times Annotated Corpus).

These features, except querySim, can be computed off-line be-cause they are query-independent. In order to represent d feature vector, we have to select a representative entity e choosing the e j that is the most similar to a query q j ,or e maximizes querySim . As a rule, a typical feature that is used for learning to rank is a retrieved score of a traditional ranking func-tion [17]. We also employ a retrieved score retScore ( q one of the features for learning a ranking model. This score must be normalized (to have a value between 0 and 1) by dividing by max d ments. The detailed computation of the entity-based features can be found in [13].
In this section, we evaluate different time-aware ranking models based on learning-to-rank algorithms. We first describe the exper-imental setting. Then, we show the experimental results as well as perform a feature analysis.
We used the New York Times Annotated Corpus (containing over 1.8 million news articles pub lished between January 1987 and June 2007) as a temporal document collection, and the 40 tempo-ral queries and crowdsourced relevance assessments from [2]. We employed a series of language processing tools for annotating doc-uments, including OpenNLP (for tokenization, sentence splitting and part-of-speech tagging, and shallow parsing), the SuperSense tagger (for named entity recogniti on) and TARSQI Toolkit (for an-notating documents with TimeML). The result of this is for each document: 1) entity information, e.g., all of persons, locations and organizations, 2) temporal expressions, e.g., all of event dates, and 3) sentence information, e.g., all sentences, entities and event dates occurs in each sentence, as well as position information. For tem-poral features, an exponential decay rate DecayRate =0 . 5  X  2 =0 . 5 are used. The fuzzySet parameters are n =2 , m =2 a = a 2  X  (0 . 25  X  ( a 3  X  a 2)) ,and a 4= a 3+(0 . 50  X  ( The smoothing parameter  X  1 is varied, and only the results of those performed best will be reported.

For learning a time-aware ranking model, we employed differ-ent learning-to-rank algorithms, w here default parameters of each learner were used. We performed five-fold cross validation by ran-domly partitioning 40 temporal queries into five folds (8 queries per fold): F1, F2, F3, F4, and F5. For each fold, four other folds (4*8=32 queries) are used for training a ranking model. In order to evaluate ranking models, the Apache Lucene search engine ver-sion 2.9.3 was employed. We have five competitive baselines. The first baseline is Lucene X  X  default similarity function (a variant of TFIDF). The four other baselines are proposed in [2]: L M L
T-EX, L MT U-IN, and L MT U-EX, where suffixes IN and EX re-fer to inclusive and exclusive mode respectively (whether query X  X  temporal expressions are also included as a part of query keywords q text or are excluded). The baseline TFIDF treats query X  X  temporal expressions as a part of q text , i.e., the inclusive mode. The retrieval effectiveness of time-aware ranki ng is measured by the precision at 1, 5 and 10 documents (P@1, P@5 and P@10 respectively), Mean Reciprocal Rank (MRR), and Mean Average Precision (MAP). The average performance over the five folds is used to measure the over-all performance of each ranking model. For all experiments, we measured statistical significance using a t-test with p &lt; 0.05. In the tables of results, bold face is used to indicate statistically significant difference from the respective baselines.
The ranking performance of the baselines and learned ranking models are displayed in Table 1. The results among the baselines are similar to those reported in [2]. In general, the exclusive mode performed better than the inclusive mode for both L M TandL and L MT U-EX gained the best performance over the other baselines.
Comparing different ranking models, RankSVM did not gain a significant improvement over the baselines, while PegasosSVM performed worse than the baselines and other learned ranking mod-els. SGD-SVM and PegasosSVM achieved the improvement over the baselines in all measurements. Finally, the listwise ranking SVM MAP performed better than the pairwise models, and also out-performed all the baselines significantly. Using P@1, SVM achieved the improvement over TFIDF and L MT U-EX up to 27.5% and 15% respectively. Using MAP, SVM MAP achieved the improve-ment over TFIDF and L MT U-EX up to 13.1% and 8.2% respectively.
In order to understand the importance of each feature, we per-formed feature analysis and the results are shown in Table 2. the average of each feature X  X  values. w i is a feature X  X  weight ob-tained from the learning method SVM MAP . The top-5 features with highest weights are querySim , TS , FuzzySet , retScore and senPos . Entity-based features, i.e., querySim , retScore and senPos , received high weights because they are well represented the importance of query terms within a document. It is interesting that TS and Fuzzy-Set gained higher weights than other temporal features, although they exploited publication time instead of the content time of a document, whereas TS did not consider time uncertainty. More-over, the results show that L M T and L MT U received negative weights indicating a negatively correlation with the retrieval effectiveness.
In order to observe the performance of individual features, we conducted 3 additional experiment s and measure the improvement in (%)MAP. First, we trained a ranking model with SVM MAP ing only retScore and selected one additional feature at each time to observe how the selected feature contributes to a ranking model. A baseline in this case is the model trained using retScore only with MAP of 0.483. The column add 1 shows the improvement in (%)MAP that each feature could produce on its own compared to the baseline. The top-5 features contributes in MAP for this anal-ysis are querySim , TS , FuzzySet , senLen ,and senPos , while adding cntEvent , timeDist ,or L MT U results in the decreased performance.
We then inspected how a single feature contributed to a rank-ing model when trained using retScore and another feature class . There are two baselines in this case: 1) the model trained only with retScore and all temporal features with MAP of 0.537, and 2) the model trained only with retScore and all entity-based features with MAP of 0.557. The column add 2 shows the improvement that each of entity-based features contributed to the first baseline model, and on the contrary, its shows the improvement that each of temporal features contributed to the second model. In summary, the top-2 best entity-based features are querySim and senLen , and the top-2 best temporal features are TS and FuzzySet .

Finally, we trained a ranking model using training data that con-sisted of all features except one at each time to see how a rank-ing model is dependent on that feature. The baseline is the model trained with all features, and its performance (MAP) is 0.617. The column remove shows the decrease of performance compared to the baseline, which is obtained by removing each feature. The top-5 features that made a significant drop in performance are querySim , senLen , TSU , titleSim ,and senPos .
In this paper, we have proposed a time-aware ranking approach based on learning-to-rank techniques for temporal queries. In order to learn the ranking model, we employed two classes of features de-rived from annotation data, namely , entity-based and temporal fea-tures. Through extensive experiments we have shown that the pro-posed learning-to-rank model significantly improves the retrieval effectiveness over existing time-aware ranking models.
 We would like to thank Klaus Berberich for providing the temporal queries and relevance judgments used in this paper.
