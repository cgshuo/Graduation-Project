 Discovering the author's interest over time from documents has important applications in recommendation systems, au-thorship identi cation and opinion extraction. In this pa-per, we propose an interest drift model (IDM), which mon-itors the evolution of author interests in time-stamped doc-uments. The model further uses the discovered author in-terest information to help nding better topics. Unlike tra-ditional topic models, our model is sensitive to the ordering of words, thus it extracts more information from the seman-tic meaning of the context. The experiment results show that the IDM model learns better topics than state-of-the-art topic models.
Author interests play a crucial role in a variety of nat-ural language processing (NLP) application. Motivated by the demand of understanding people's preferences and be-havioral pattern, a urry of researches devote to discovering author interests. The common approach is to build a gener-ative model that represents the author's interests as a latent variable (see Section 2). Most of these models (LDA-style models) make the bag-of-word assumption, which states that the meaning of the document is fully characterized by the unigram statistics of words. The bag-of-word assumption neglects the ordering of words, sentences and paragraphs, though these factors are important in de ning the seman-tics meaning of the context.

We propose a Interest Drift Model (IDM) for modeling the dynamic evolution of individual author's interest. The IDM learns the representation of words, sentences and document as vectors. It also learns the interests of authors as vectors. Since the interests of authors may change over time, each author's interests is represented by a sequence of vectors, F ei Xu is the corresponding author.
 where the speed of the interest drifting is controlled by the similarity of consecutive vectors. The IDM model also cap-tures the fact that co-authors usually share common inter-ests, by making their interest vectors positively correlated. The parameters of the model are learnt through a prediction task: given vectors associated with the context, the goal is to predict each word in the document. This scheme has been proven effective in discovering topics from documents [7, 6, 11].

Our model is conceptually related to the work of [11] that incorporates the ordering of words and the semantic mean-ing of sentences into topic modeling. However, [11] does not consider the authorship and temporal information of the text. In contrast, the goal of this paper is to address the challenges of modeling author interest evolution. Our model has three advantages over the traditional LDA model. First, given the document's authors, the document's topic is de-termined by the authors' interests, which may change over time. Second, our model provides a natural way to measure the similarity of interest, which helps controlling the speed of interest drift, as well as measuring the similarity between authors. Third, our model takes the ordering of words, sen-tences and paragraphs into account, so that the topics re ect the semantics of the context.
The Author-Topic model [8] is the rst generative model that simultaneously models the content of documents and the interests of authors. There are also some variants of Author-Topic models such as [5]. These models are devoted to discovering static latent topics and author's interests, without considering temporal information.

To characterize topics and their evolution over time, [2] propose a dynamic topic model (DTM) which jointly mod-els word co-occurrence and time. [9] propose a non-Markov continuous-time model (ToT). These models capture the evolution of topics, but they completely ignore the author-ship information.

There are recent works taking both timestamp and au-thorship information into account, including the Temporal-Author-Topic (TAT) model [4], the Author-topic over time (AToT) model [10]. Nevertheless, these models do not char-acterize the drift of the individual author's interests.
All of the aforementioned topic models employ the bag-of-words assumption, which is rarely true in practice. [11] p ropose a generative topic model that represent each topic as a cluster of multi-dimensional vectors. Nevertheless, this model doesn't use temporal information or models the in-terest of authors.
In this section, we describe the IDM model as a genera-tive model. Then we illustrate the inference algorithm for estimating the model parameters.
We assume that there are W different words in the vo-cabulary and there are D documents in corpus. In addition, these documents belong to T topics, where T is a hyper-parameter speci ed by the user. For a speci c document d , suppose that it has m authors a 1 ,..., a m . We use an interest vector vec( a i ; d ) 2 R p to represent the interests of author a when he/she writes document d . Two authors have sim-ilar interests if the distance between their interest vectors is small. Our goal is to learn these interest vectors from the content of the corpus. It is clear that the interests of co-authors should be correlated. The interests of the same author in writing different documents should consistent.
We now specify the generative model for vectors vec( a i ; d ) R p ( i = 1 ; : : : ; m ), conditioning on the interest vectors of all earlier documents. This de nes a valid model, since inter-est vectors can be generated sequentially from the earliest document to the latest document. Let t be the time that document d was written. Let d  X  i be the last document that author a i has written before she writes the current docu-ment, and let the timestamps for d  X  i be t  X  i . We have t all i = 1 ; ::::; m . We de ne a joint distribution on the vec-ate normal distribution on R mp taking the form N ( d ; d Firstly, we specify the mean d . Let where vec( a i ; d  X  i ) is the interest vector for author a she wrote document d  X  i . This de nition implies that the new interests of authors have connections to the history.
Second, we de ne the covariance matrix d . Note that d is an mp mp matrix, so that it can be partitioned into m 2 sub-matrices, each with dimension p p . Let ij 2 R p p be the sub-matrix of d on the i -th row and j -th column. If i = j , then the sub-matrix characterize the covariance of the author i 's interest. Let where ( x ) is an increasing function of x . It means that as time passing, the covariance matrix entries get bigger, indicating that the interest of author i is less likely to con-centrate on its mean { the interest vector when she wrote the earlier document d  X  . More concretely, we adapt a linear function ( x ) = + x with hyper-parameters &gt; 0 and &gt; 0. The values of and are chosen by cross-validation.
If i  X  = j , then the sub-matrix ij characterizes the corre-lation between the interest of the author i and the author j . Let where 2 [0 ; 1] is another hyper-parameter measuring the correlation of the interest of co-authors. if = 0, then vec( a 1 ; d ),...,vec( a m ; d ) are mutually independent, meaning that there is no correlation between co-authors. If = 1, direction for all co-authors, meaning that the interest drift is perfectly correlated. In summary, the density function of vector vec( a; d ) is de ned by p (vec( a; d ) = v ) / exp The vector d and the matrix d follow the de nition above.
The words, sentences and documents are represented by vectors so that their semantic representations are naturally connected with that of the author interests. For each word w 2f 1 ; : : : ; W g , there is a p -dimensional vector representa-tion vec( w ) 2R p . Each document with index d 2f 1 ; : : : ; D has a vector representation vec( d ) 2 R p . There are S sen-tences in the corpus, each sentence indexed by s 2f 1 ; : : : ; S The sentence with index s is associated with a vector repre-sentation vec( s ) 2R p .

We assume that all these vectors are generated from a multi-variate normal distribution conditioning on the topic. Given the a topic k , the conditional distribution of vec-tors are de ned by vec( w ) N ( word k ; word k ), vec( s ) represent the mean and the covariance matrix. We assume that the topic k is chosen with probability k such that  X  k =1 k = 1. Thus, the vectors are generated from a Gaus-sian mixture model with unknown mixture weights and un-known Gaussian parameters. We use to collectively rep-resent these parameters.
Given the vectors representing author interests, words, sentences and documents, we describe how the word tokens are generated. Let V be the collection of all latent vectors: For the i -th word in the sentence, we represent its word realization by w i , which is generated by of the vectors of the author interest, the document, the sen-tence and the previous n words. These linear transforma-tions are de ned by cient. These coefficients are unknown parameters of the model. They are shared across all word slots in the corpus. We use U to represent this collection of these coefficients.
Finally, we summarize the set of unknown parameters to be learned: the set V containing the vector of author inter-ests, words, sentences and documents, the set U containing the unknown linear transformation coefficients, and the set containing parameters that de nes the Gaussian mixture model. We will see that given arbitrary two of these sets, the remaining set of parameters can be solved by efficient algorithms.
Given the model parameters U , and the vectors V , we can infer the posterior probability distribution of topics. In particular, for a word w with vector representation vec( w ), the posterior distribution of its topic, namely q ( z ( w )), is easy to derive given the generative model. For any topic z 2 1 ; 2 ; : : : ; T , we have The topics of sentences, documents and author interest can be inferred in the same way. Similarly, we can use the same formula for inferring the author interest vectors' associated topics, i.e. The term q ( z ( a; d ) = z ) re ects the ratio of the author a 's interests in topic z when she writes document d . In the experiments, we demonstrate that such a scheme yields rea-sonable representation for the author's interests.
We estimate the model parameters and latent vectors , U and by maximizing the likelihood of the generative model. The parameter estimation consists of two stages. In Stage I, we maximize the likelihood of the model with respect to . Since characterizes a Gaussian mixture model, this proce-dure can be implemented by the Expectation Maximization (EM) algorithm. In Stage II, we maximize the model like-lihood with respect to U and , this procedure can be im-plemented by stochastic gradient descent. We alternatively execute Stage I and Stage II until the parameters converge.
In this stage, the latent vector of words, sentences and documents are xed. We estimate the parameters of the Gaussian mixture model = f k ; k ; k g . This is a clas-sical statistical estimation problem which can be solved by running the EM algorithm. The reader can refer to the book [1] for the implementation details.
When is known and xed, we estimate the model pa-rameters U and the latent vectors by maximizing the log-likelihood of the generative model. In particular, we iter-atively sample a location in the corpus, and consider the log-likelihood of the observed word at this location. Let the word realization at location i be represented by w . The log-likelihood of this location is equal to where log( p ( j )) is the log-prior of parameters . The (10). Using stochastic gradient descent, we update
T able 1: Comparison of test perplexity per word for the word vector vec( w i ) and the associated weight u Similarly, we update vec( s ), vec( d ), vec( a; d ) and u u author using the same gradient step. Once the gradient update is complete, we randomly sample another location to continue the update. The procedure terminates when there are sufficient number of updates performed, so that both U and converge to stationary values.
In this section, we test the IDM model on two publicly available datasets. We compare our model with state-of-the-art topic models with both quantitative and qualitative evaluations.
We use the NIPS paper data and the Enron emails data in our experiment. Data preprocessing is executed before training the models. We remove non-alphabet characters, numbers, pronoun, punctuation and stop words from the text. Then, stemming is applied to reduce the vocabulary size and settle the issue of data spareness. The detailed properties of the datasets are described as follow. NIPS papers (NIPS): This dataset is a collection of pa-pers from the NIPS conference between 1987 and 1999 1 . Af-ter data preprocessing, the data set contains 1,740 research papers with 2,037 authors. Each timestamp is determined by the year of the proceedings. Following the preprocessing in [8, 12], the 1,740 papers are further divided into a training set of 1,557 papers and a test set of 183 papers of which 102 are single-authored papers.
 Enron emails (Enron): This data was originally pub-lished by the FERC during its investigations 2 . We further clean the corpus by removing computer generated les from each user and remove \quoted original messages" in replies. Finally, there are 75,686 messages in our Enron data set. We further divide the data into a training set of 65,686 messages and a test set of 10,000 messages. Each author has at least one document in the training set.
In the experiments, the proposed IDM model is compared with several baseline methods, including Latent Dirichlet Allocation (LDA) model[3], Author-Topic (AT) model [8], Dynamic Topic model (DTM) [2], Gaussian Mixture Neural Topic Model (GMNTM) [11] and Author-Topic over Time (AToT) model [10]. We use the same settings for all hyper parameters as in their original papers.
In our model, the learning rate is set to 0 : 05 and grad-ually reduced to 0 : 0001. For each word, the previous six words in the same sentence are included in the context. For easy comparison with other models, the word vector size is set to p = 100. Increasing the word vector size further
A vailable at http://www.cs.nyu.edu/~roweis/data.html http://www.cs.cmu.edu/~enron/ improves the quality of the topics. To perform comparable experiments with restricted vocabulary, words outside of the vocabulary is replaced by a special token and doesn't count into the word perplexity calculation.
To evaluate the predictive power of the proposed model, we compare test-set perplexity of our model with that of the baselines on both NIPS and Enron data sets. Following the same evaluation as in [8], for the NIPS corpus we calculate the perplexity for the 102 single-authored test document. Whereas, for the Enron corpus, we calculate the perplexity for the 1000 held-out documents that are sampled from the test sets. After running the algorithm that estimates the vector representation of words, sentences, documents and author interests, the averaged test perplexity is de ned as where N test are the total number of words in the held-out test documents and p ( w d j a d ; D train ) is the probability as-signed to the words w d in the test document, conditioned on the known authors a d of the test document and training
Table 1 shows the results of the perplexity comparison on both data sets, with T = 100 topics. The IDM model is clearly better than the other models, as illustrated by its relatively low perplexity score on both data sets. This is because that the IDM model take the authorship informa-tion and the timestamps information into consideration. In addition, it also use the ordering of the words.
An interesting feature of our model is the capability of discovering those authors who have drifting interests. This interest-tracking feature is crucial in recommender systems and in assisting investigators nding criminal activities such as insider threats. Due to the limited space, in this experi-ment, we test our model on the NIPS dataset between 1995-1999. We take Neural Networks topic as an example. For the selected topic, we choose the top 5 most likely authors according to equation (12). The results are illustrated in Table 2. By examining the author's homepage, our results are consistent with the truth. For example, Bengio was in-terested in neural network over the whole period, while Se-jnowski shown great interest in neural network only in the early phase.
 We can also extract individual author's interests over time. In this paper, we report M. I. Jordan, who has published a sufficient number of paper on the NIPS conference. We show the top 3 most likely topics of the corresponding author (by equation (12)). The research interest evolution for the se-lected authors between 1995 and 1999 are reported in Table 3. As shown by Table 3, the most likely topics of the author are varying during the experimental period. For instance, Jordan's research interest is mainly focused on \expert net-works" and \dynamical model" in the early years, then it expanded to Reinforcement learning and Graphical models. This can be veri ed from his homepage.
In this paper, we have proposed a interest drift model (IDM) for topic modeling. The IDM model achieves sig-ni cantly lower perplexity compared to the state-of-the-art methods, indicating that our model nds high-quality top-ics. [1] C. M. Bishop. Pattern recognition and machine [2] D. M. Blei and J. D. Lafferty. Dynamic topic models. [3] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [4] A. Daud. Using time topic modeling for [5] N. Kawamae. Author interest topic model. In [6] Q. V. Le and T. Mikolov. Distributed representations [7] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and [8] M. Rosen-Zvi, T. Griffiths, M. Steyvers, and [9] X. Wang and A. McCallum. Topics over time: a [10] S. Xu, Q. Shi, X. Qiao, L. Zhu, H. Jung, S. Lee, and [11] M. Yang, T. Cui, and W. Tu. Ordering-sensitive and [12] M. Yang, D. Zhu, and K.-P. Chow. A topic model for
