 Traditionally, recommender systems are an approach to solving problems of in-formation overload. But in large organizations, they become one technology that assists to efficient locate people with n eeded expertise [4]. Such expertise rec-ommender systems are gaining increasing importance as organizations continue to look for better ways to exploit their internal knowledge capital and facilitate collaboration among their employees for increased productivity [11]. As infor-mation technology advances, expertise recommendation approaches have moved from employing a database housing the knowledge of each employee to auto-mated mining expertise representation s in heterogeneous collections [5],[10].
The core of expertise recommender systems is expert finding that searches appropriate experts for recommendation. Increasing interest in expert finding led to the launch of an expert search task as a part of the enterprise track at TREC in 2005 [3]. It formulated the problem of expert finding as a retrieval task to rank people who are candidate experts g iven a particular topical query. Most systems that participated in this task a pplied advanced information retrieval (IR) approaches, such as language modeling techniques. Typically, a two-stage language model proposed by Cao et al. [2] showed excellent performance in 2005, and then became popular at TREC 2006. In this model, each document is represented as both a bag of topical words and a bag of expert entities. So for each document, a language model of topical words which is called a relevance model is created, as well as a language model of expert entities called co-occurrence model. The probabilistic score of a candidate expert is then calculated from these two kinds of language models by assuming conditional independence between the candidate and the query. A general representation of this method is where e denotes a candidate expert, q a topical query, and d adocument.A similar model was introduced by Balog et al. [1] as their Model 2.

In this paper, we analyze the language modeling approach to expert find-ing, taking the two-stage model as an example. As the relevance model P ( d | q ) can be devised as a simple document ret rieval system using language modeling techniques, the smoothing methods applied in this model have been studied in [12]. In this paper, our focus is on the co-occurrence model P ( e | d ) which models expert entities. Our study reveals that the interpolation smoothing used in the co-occurrence model over-magnifies the affection of expert prior probability to give the model a tendency to prefer  X  X ommon X  experts. To avoid this short-coming, we present an approach that uses odds ratio, a relative value of the smoothed probability to the prior probability, to replace the smoothed probabil-ity itself. We also present a more general model by modeling expertise on a set of documents in which it occurs. 2.1 Language Model for Expert Finding and Its Smoothing The problem of identifying candidates who are experts for a given topic can be stated as  X  X hat is the probability of a candidate e being an expert given the topical query q  X . That is, we rank candidates according to the probability P ( e | q ). The candidates in the top are deemed the most probable expert for the given topic.

For expert ranking, language modeling techniques are applied to estimate the likelihood that one is an expert to a given topic. Such probability is computed based on the document collection which both the candidate and the topic are associated to, in a two-stage manner.
 where d denotes a document and P ( d | q ) is the same as a traditional document retrieval.

In the following discussion, we focus on P ( e | d, q ) which models the candi-dates. P ( e | d, q ), as the co-occurrence model, cap tures the association between a candidate and a document. For simplification, people usually assume the query and the candidate are conditionally independent. Then we get equation (1). Therefore, we can treat ea ch candidate identifier as a term and model it us-ing language modeling techniques to get the probability P ( e | d ). The maximum likelihood estimate of this probability is where tf e,d is the raw term frequency of candidate e in document d , | d | is the total frequency of candidates in d . Usually, the empirical estimator P ml ( e | d )is not accurate enough when dealing with sparse data like candidate identifiers. It needs smoothing.

Most smoothing methods make use of two distributions, a model used for  X  X een X  words that occur in the document, and a model for  X  X nseen X  words that do not [12]. We use P s ( e | d ) for  X  X een X  candidates and P u ( e | d ) for  X  X nseen X  candidates. The probability P ( e | q ) can be written in terms of these models as follows The probability of an unseen candidate is typically taken as being proportional to the collection language model of the candidate, i.e. P u ( e | d )=  X P ( e | C ). In interpolation smoothing, the probability of a seen candidate is represented as where  X  is constant parameter. In this case, we get
P ( e | q )= As the query is supposed to be created f rom the collection, we can just define It can be seen that the smoothing term is only proportional to P ( e | C ). It seems to be similar with the language model u sed to retrieve documents. However, they have different effects. In the language model of topical words, the task is ranking documents; whilst in the language model of expert entities, the task is ranking experts. So the smoothing in the former is aimed to accommodate common words to reduce their affection, but in the latter there is an inverse effect that gives more weight to  X  X ommon X  experts.

How comes such a conclusion? Let us assume that a candidate whose prior probability is small has a big maximum likelihood probability in a document while another candidate whose prior probability is large has a small maximum likelihood probability in that document. Apparently, the first person would be more likely an expert of the document than the second one. But the second person would possibly get a larger score if we use (7) for ranking. That smoothing strategy over-magnifi es the affection of P ( e | C ) and the results would be given a bad tendency to the candidates with a relatively lager prior probability. The influence of the smoothing is shown in Fig. 1. It makes the results decline. Balog et al. X  X  Model 2 which only applies maximum likelihood estimate gets the best performance.
 2.2 Using Odds Ratio To avoid the flaw that boosts up  X  X ommon X  experts, we try to reduce the affec-tion of the prior probability. Furthermore, it is better to make the prior proba-bility hold an inverse effect that accommodates  X  X ommon X  experts as it dose in document retrieval. So one of the most effective solutions is to apply a ratio of the seen probability and the collection background probability. That is what we called odds ratio in this paper, shown as Note that  X  is the const parameter which does not affect the results, so we use  X  here to show the same ranking. The underlying meaning of the odds ratio is how much a document votes to a candidate depends on how this document is relatively important to the candidate but not just their association. So it would act as the  X  X df X  in vector model to accommodate the experts who occur commonly in every documents.

From equation (8), we can see the score of a candidate expert is only associated with the documents in which it occurs. So it is better to just consider these documents and model the expert on them to get a more general model. Thus, the model is like Note that equation (9) beco mes (8) when the parameter  X  is 0. This is our general model that applies odds ratio to improve language modeling techniques for expert finding. For convenience, in the experiments next section, we will not explicitly give the results of (8) but mention them as included in the results of (9). We now present an experimental evaluation of the improved model for expert finding. First of all, we address the following research questions. We have re-ported the experimental results of the o riginal model, we then use its best set-ting (i.e., maximum likelihood estimate) for comparison. The question is how the improved model compares with the original model. We also state an interpo-lation smoothing model which models a candidate on the documents it occurs. We compare this model with the improved model to show the effectiveness of the odds ratio in a range of settings. We then describe the experiments on TREC collections.

W3C corpus comprises 331,037 documents crawled from the public W3C (*.w3c.org) sites in June 2004 [3]. This heterogeneous repository contains sev-eral types of web pages, such as emails, personal homepages, wiki pages and so on. All of such pages were processed in our experiments. The evaluation was on Ent05 collection, which included W3C corpus and queries EX1-EX50, and Ent06 collection, which included W3C corpus and queries EX51-EX105. Queries EX1-EX50 contain only a  X  X itle X  section; whilst queries EX51-EX105 contain  X  X itle X ,  X  X escription X  and  X  X arrative X  sections.

We implemented our approach on version 4.2 of the Lemur Toolkit [9] which was used for the relevance model. First of all, the named entities of the candidates were extracted. We employed a list of 1,092 candidates, including full name and email. It is a specialized named entity recognition task, for which Rule-based name matching mechanisms described in [7] were applied. In practice, 795 of 1,092 candidates were found in the corpus and they appeared 8,587,453 times. When building index, we ignored case, stemmed with Krovetz X  X  algorithm, and removed useless words by a list of 571 stop words. The same pre-processing was applied to the queries. We built another index for expert candidates, just treating each entity as a term. Language modeling approach was implemented on this index too.

We compared three expertise modeling approaches: maximum likelihood esti-mate (MLE), interpolation smoothing (IS) and odds ratio (OR). The OR model was formulized in (9). The MLE model would be stated as and the IS model is As all the models described above are general, they need to be detailed in the experiments. For convenience, we just applied the specific formula introduced by Cao et al. [2].
 Here Dirichlet prior were used in smoothing of parameter  X  : where  X  is average length of term frequency of candidates in the collection. In our experiments, it is 10,801. Note that we use a flexible  X  whichisassumedto fit the general models.

The performances were tested on using top n documents retrieved by the rele-vance model as we found the performances would change according to returning different subsets of documents. We tried different value of n and got the result curves in Fig. 2. It reveals that the performance of the OR model is significantly better than the other two. In average, the OR model exceeds the MLE by over 12.0 percentage points. And if only the specific model of (8) is considered, it would exceed the MLE by about 10.6 percentage points in average. Meanwhile, the OR model exceeds the IS model by about 16.9 percentage points. The more significant improvement was supposed to be due to the IS model uses the prior probability for interpolation smoothing.

So is the OR model better than the IS model in any settings? To answer it, we relaxed the specification of  X  in the two models and tested them by tuning  X  ,based on the condition of returning 200 documents as relevant. Such is the condition on which IS model almost gets its best results. Fig. 3 shows the results. It reveals that on all values of  X  the OR model outperforms the IS model. Something inter-esting in Fig. 3 is that the OR model outperforms the IS model more significantly on Ent05 than Ent06. When looking into it, we found that such difference may be caused by the different ways how the j udgements of these t wo collections cre-ated. In Ent05, the queries are the names of W3C groups and the judgements are independent of the documents. But in Ent06, the judgements are contributed by the participants according the information from the documents. As a result, the judgements for Ent06 would show more preference to the experts who occur more times in the corpus, i.e., the  X  X ommon X  ex perts. So there seems less improvement for Ent06 because the IS model gets m ore benefit from the judgements.
Comparing our best results with the official results of the TREC Enterprise track, we found that using OR model impr oves the performances from the fifth best to the third best in TREC 2005 and 2006. As it is much harder to improve a top 5 system, we believe that is an obvious improvement. Also consider that it is a general model for expert finding, so its performance can be further improved by applying some elaborate mechanisms. We analyzed the language modeling approach in expertise recommender sys-tems. Taking the two-stage expert finding system as an example, our study was focused on the language model created for expert entities. In such a model, the smoothing is different from that used in the language model for retrieving docu-ments. It wrongly uses expert prior probability which would be supposed to act as the  X  X df X  in vector model to accommo date  X  X ommon X  experts. As a result, people who have relatively larger prior probability become more probable to be an expert. To avoid such a bias, we suggested an odds ratio model that applies odds ratio instead of raw probability to build the association between experts and documents. Additionally, we modeled one X  X  expertise on a set of documents related to that person to get a general model. Our experiments on TREC collec-tions reveals notable improvement using the odds ratio model. Compared with other systems that participated in TREC, the odds ratio model is promising.
Though our study showed that the two-stage expert finding system boosts up  X  X ommon X  experts, we believe the problem also exists in other kinds of language modeling approaches to expert finding. We will further study several other expert recommender systems to prove this assumption.
 This work was supported by National Natural Science Foundation of China (Grant No.60475007, 60675001).

