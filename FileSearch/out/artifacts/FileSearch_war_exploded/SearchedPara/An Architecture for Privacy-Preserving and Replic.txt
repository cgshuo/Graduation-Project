 We demonstrate the infrastructure used in the TREC 2015 Total Recall track to facilitate controlled simulation of  X  X s-sessor in the loop X  high-recall retrieval experimentation. The implementation and corresponding design decisions are pre-sented for this platform. This includes the necessary consid-erations to ensure that experiments are privacy-preserving when using test collections that cannot be distributed. Fur-thermore, we describe the use of virtual machines as a means of system submission in order to to promote replicable exper-iments while also ensuring the security of system developers and data providers.
The Total Recall track at TREC 2015 [15, 14] sought to in-vestigate methods for achieving high-recall, with an assessor in the loop, through controlled simulation. The Total Re-call track offered an online evaluation platform that allowed participants to produce systems that could request docu-ment assessments in a document-at-a-time manner, which is similar to the Microblog track X  X   X  X valuation as a service X  methodology [11, 12]. This evaluation platform was primar-ily contained in a Web server (Section 3) that facilitated run creation, corpora distribution, the aforementioned doc-ument assessment process, and some basic online evalua-tion for training collections. Included in this platform was a baseline model implementation (the  X  X MI X , Section 4.1), distributed as a VirtualBox virtual machine (VM), which participants could freely modify as they saw fit.

In developing this platform we sought to mitigate issues that have arisen in previous information retrieval research, and, in particular, high-recall retrieval experimentation. The primary issues we sought to address are as follows:
We feel that we have successfully met these goals but we acknowledge that there are still improvements to be made. By controlling interaction through a simulated assessor (e.g., a system receives a binary judgement on a document), par-ticipant interaction does not rely on the ability of partici-pants to get information out of the assessor but rather on the documents themselves. The BMI provides a working system to participants that they can use as a baseline for their ex-periments. Accordingly, there is less burden on participants to have an in-depth understanding of the Web server and can focus on their particular algorithms.

Furthermore, the track offered two modes of participation: an At-Home mode where interaction occurred over the Web; and a Sandbox mode where interaction occurred locally on a single machine. Participation in the Sandbox mode re-quired participants to submit a working VM that would be ran, without Internet access, on test collections that were unknown to participants. Figure 1 provides a high-level con-ceptual depiction of how the live and sandbox modes differ.
We note that regardless of configuration, the data col-lections (the types and formats described in Section 2) are largely  X  X lug-and-play X , meaning that new data collections can, with little effort, be added to the platform. The result is that new experiments can be run on participant systems without necessitating participant action and without requir-ing participant effort  X  other than that required to run the system. Furthermore, the Sandbox mode of participation al-lows collections to be used that would otherwise be  X  X oo hot X  to distribute or would require onerous and time consuming anonymization that may not work.

In the remainder of this work, we describe the major com-ponents of the Total Recall track X  X  platform: the test col-lections and their format (Section 2); the Web server and its sandboxing (Section 3); the baseline system, participant systems, and their sandboxing (Section 4). We conclude with a discussion of limitations and future improvements to the platform (Section 5). Throughout these sections, we will provide the underlying reasoning as to why a particular approach was taken when another might have sufficed. Figure 1: A high-level look at how the various components interact in live and sandbox environments. Note that the dashed blue line denotes the Total Recall server and partic-ipant VMs running on the same machine.
For our purposes, we consider a test collection to be a set of documents along with a set of qrels, which are tu-ples mapping documents and topics (information needs) to relevance assessments. Data collections are transmitted to participant clients through the Total Recall service first by transmitting the set of documents and then by having sys-tems requests relevance assessments in a document-at-a-time manner. This simulates the interaction between participants and a gold standard assessor. that was present in previ-ous TREC Legal tracks [10, 7], TREC Filtering tracks [13], and the TREC Spam tracks [4, 3, 8], while also controlling that interaction to ensure that it is equitable amongst all participants. Furthermore, to ease system development we curated each test collection so that a single file contained a single document. This is contrary to many past TREC collections where a file might contain multiple documents and was performed to simplify document parsing. We addi-tionally translated each document into a plain text render-ing from whatever native format it was originally stored in (e.g., PST, WARC, etc) to ease the burden of system design as participants would not have to worry about processing multiple filetypes.

By taking this controlled approach to high-recall retrieval experimentation, we hope to mitigate any bias that might be perceived by spending  X  X oo much time X  with a topic author-ity (gold standard assessor) or the ability to ask the right questions. The focus is then on the algorithms used and less so on the human interaction between topic authorities and participants. This should not be construed to suggest that such interaction is bad but that we merely sought to eliminate the confound that different approaches to such in-teraction may have on system evaluation. Furthermore, this approach facilitates experimentation with more topics over more test collections since there is no requirement to provide access to a high-quality topic authority (that is willing to volunteer valuable time), which has previously limited how many topics and how much interaction could be performed [10, 7].

For the purposes of system development, we used three publicly available test collections: 20-Newsgroups 1 ; Reuters-21758 2 ; and, a variant of the Enron v2 e-mail col-http://qwone.com/  X jason/20Newsgroups/ http://www.daviddlewis.com/resources/testcollections/ lection 3 . The inclusion of the first two datasets is to facili-tate rapid development and testing of participant systems as they both contain approximately 20,000 documents. Neither was meant to be representative of a valid test collection but were merely there to help participant X  X  gain confidence that their system was working. Moreover, we offered samples of these first two corpora as a mechanism to ensure partici-pants understood and could correctly interact with the Web server, but without requiring the extensive processing time of a larger corpus. The Enron collection was our attempt to provide a representative collection, as it had previously been used in the TREC 2009 Legal track [10], that would more accurately indicate the effectiveness of participant systems. For all of these collections, we provided an online mecha-nism to provide recall, effort, precision, and F1 scores for checking system performance.

At the beginning of July, the At-Home phase of partic-ipation began and three additional corpora were released to participants (after signing the appropriate usage agree-ments). These collections were the official test collections and so participants did not receive any explicit system per-formance feedback as a means of preventing any potential meta-learning by systems. Although, we acknowledge that some participants may have had internal quality assurance processes that would have helped determine their own per-formance.

Not all corpora can be distributed by the service due to usage agreements or the risk of divulging sensitive data to the public, and so, generally, such restricted corpora will be distributed only in the form of Sandbox participation (dis-cussed in Section 3.1). This helps to ameliorate issues with imperfect anonymization and the inability to give all inter-ested parties access to the data. While no system that emits any information (e.g., even summary evaluation metrics) can ever be entirely secure, we believe that structuring the re-lease of collections in this way aids in preserving the privacy of all parties when private collections are used. For all test collections, we refer interested parties to the track overview [15] for in-depth descriptions.
The Total Recall server operates as a Web service that participant systems interact with over HTTP requests (i.e., a REST(ful) API). There are three main types of interactions between systems and the service: Secondary interactions can occur, including: starting and fi-nalizing a run; result generation for developmental test col-lections; error log checking; and, other track specific actions beyond the scope of this paper. reuters21578/ http://trec-legal.umiacs.umd.edu/corpora/trec/legal10/
Figure 2 provides the general workflow that a client system might be expected to perform. For brevity we omit the actual API that was used to interact with the service and instead direct interested parties to the API documentation Note that all requests and responses to and from the server are encoded in JSON format, which has become a standard format for passing data between a Web server and client software.

In terms of implementation the entirety of the Total Recall web server is written in Node.js 5 , which is a Javascript run-time that uses an event-driven model for developing sever-side Web applications (e.g., REST(ful) APIs). Node.js is designed to be efficient and lightweight and is extremely pop-ular in web development. While other architectures would have sufficed, Node.js was chosen due to its popularity and the opportunity to explore current state-of-the-art technolo-gies in Web development. Where necessary (e.g., in storing judgments and a log of participant requests), the server re-lies on an installation of MySQL for persistant storage. As will be discussed in Section 5, using a default installation of MySQL was perhaps a poor choice and resulted in a small bottleneck near the end of the experimental period but has been corrected for following iterations. All code for the To-tal Recall track is available for public download in a git repository 6 .
To facilitate the use of test collections that require com-plicated transmission protocols (e.g., transmitted by a third party, such as NIST) or are  X  X oo hot to handle X  (e.g., raw ver-sions of confidential email), we provided the Sandbox mode of participation. In this mode, participants submit their sys-tems as a VM (more details in Section 4) which we run in a restricted environment with access to the data.

Test collections do not necessarily have to be stored di-rectly on the sandbox machine; instead data providers will be able to distribute test collections via USB flashdrive or external hard drive. Once the test collection is loaded into the service (using some relatively simple shell scripts), the participant systems are can be ran with these collections much in the same way that they would in the live scenario. However, the participant X  X  VM is prohibited from accessing the Internet or Web in any way. This is done so that we can prevent any blatant data leakage from the sandbox environ-ment. This restriction is enforced by limiting access of the clients machines only to the service and (optionally) by air-walling the sandbox machine (i.e., never connecting it to the Internet once the test collections are present). Air-walling occurred for one of the Sandbox collections due to the nature of the of the collection.

By enforcing these restrictions on the sandbox machine, the data is protected from unintended transmission as well as unintended dissemination of personal or private data. Fur-thermore, we can limit the only output of the sandbox server to be only summary evaluation measures and statistics (dis-cussed in the track overview [15, 14]) once all participant systems have been ran. In doing so, the goal would be to prevent accidental distribution of private data that may be contained in qrels and document identifiers. quaid.uwaterloo.ca:33333/#/api
See https://nodejs.org/en/ for more information. http://repo.trec-total-recall.com Figure 2: The envisioned workflow of a Total Recall partic-ipant system.
The envisioned workflow for a client system is depicted in Figure 2, and is implemented by the the Baseline Model Implementation (Section 4.1). Participants can construct several different systems to interact with the Total Recall service. Clients for At-Home experiments can be developed in several different ways: a purely automatic program (or set of programs) that performs the Total Recall task; a cus-tomized virtual machine that runs the above; as a  X  X idecar X  (i.e., a directory containing additional scripts) supplied to the BMI X  X  Virtual Machine. A  X  X idecar X  submission can be run without any additional dependencies being added to the default BMI VM.

The Total Recall task is one that can be performed manu-ally or through some combination of manual and automatic approaches. Accordingly, we allowed participants to sub-mit a single manual run, which was to be performed before developing any automatic systems. Participants could sub-mit manual runs using either the API, used by automatic systems, or a Web-based interface that we provided. The Web-based interface was quite simple and was largely just a pretty interface for the API. Out of the 3 manual teams, 2 used the Web interface and 1 directly communicated with the Web server via the API.

For sandbox submission, participants are required to sub-mit systems that fall under the full VM or sidecar categories. This is done with the hope that systems would work out-of-the-box, which was mostly true for TREC 2015. Previous experience with running participant systems in a sandbox for the TREC Spam tracks [8, 3, 4] led us to believe that al-lowing the submission of arbitrary code would lead to wasted effort in getting code to run correctly. As coordinators would then have to debug and fix-up any code that was not compat-ible with the sandbox environment (e.g., installing libraries, having necessary compiler versions). By using a VM, we hoped to limit the necessity of such tasks and were successful in this by having only one system require major coordinator intervention (out of 11 different systems).

A final benefit to requiring submissions of VMs or sidecars is that it allows commercial vendors of high-recall retrieval software to submit their products without requiring source code submission (as was required in previous tracks that used sandboxing). Vendors can submit a fully compiled ver-sion of their software that runs on the VM and so do not expose their intellectual property to track coordinators, data providers, or any other parties that might come into contact with the software. Extra steps can also be taken, such as encrypting the VM and/or software so that outside parties cannot access the software at all (outside of the normal in-teraction between client and server).
Our primary baseline is an augmented version of the Con-tinuous Active Learning (CAL) method originally presented by Cormack and Grossman [5], which is called AutoTAR [6]. Unlike the original version of CAL, AutoTAR uses ex-ponentially increasing batches sizes, and unlike the reported results of the AutoTAR paper, we use a tuned version of sofia-ml 7 (based upon suggestions from the package X  X  au-thor).

This baseline is implemented as part of the virtual ma-chine with sidecar approach, where the AutoTAR algorithm is refactored as a sidecar for a simple Debian VM. This was done to provide a model implementation of a sidecar and a meaningful working implementation that participants would be free to augment in the course of experimentation. The BMI is implemented using a combination of C++ and bash scripts (along with associated command line tools). The canonical implementation is available for download under GPL 8 .

Cormack and Grossman have shown that AutoTar gen-erally outperforms a simple CAL implementation. Accord-ingly, our intent was to provide a reasonable baseline that could  X  X ast-track X  participants to a working system, with-out requiring them to worry about API programming. We hoped that the BMI would provide ample opportunity for participants to improve upon its results or be inspired by the technique and devise original algorithms of their own. Several of the participants made use of the BMI in their own submissions.
As with the development and use of any long running soft-ware, issues were likely to arise throughout the course of the track and did. Primarily, the biggest issue was a bottleneck in document assessment requests near the submission dead-line. This was primarily due to the vanilla installation of MySQL which became bogged down when there were many queries being ran. In hindsight, this issue is relatively easy to solve by configuring MySQL to handle queries more ef-fectively and by adding additional connections to the Web server so that multiple requests are not held up by a single connection to the database.

Surprisingly, the use of a REST(ful) API did not appear to cause participants too much trouble, in spite of our fears that it might during development. These troubles were likely ameliorated by providing tools like the BMI and the manual participation interface.

Another design decision to consider for future iterations is the use of full virtual machines. There are many mer-its to the use of virtual machines, primarily the fact that the entire system is self-contained and so security issues are more manageable due to more complete isolation from other software on the machine. This isolation comes with perfor-mance setbacks. For example, we could only have one par-https://code.google.com/archive/p/sofia-ml/ http://plg.uwaterloo.ca/  X gvcormac/trecvm/ ticipant system running at a time due to the high overhead of both participant systems and the cost of virtualization on our sandbox machines. An alternative may be to use a more lightweight system, like Docker, which makes use of software containers. In short, software containers package an application together with its dependencies for running on an arbitrary (Linux) server. The easiest way to envision software containers is somewhere between virtual machines and auto-building tools (e.g., make, Maven). Accordingly, there is less emulation overhead that occurs with containers but at the expense greater exposure to the underlying sys-tem. A more thorough analysis of the costs and benefits is on-going and the inclusion of Docker containers may occur for TREC 2016.
We have presented a brief overview of the Total Recall track X  X  online evaluation platform and the design decisions that went into its development. The use of this service is an attempt to develop a reusable system for evaluating infor-mation retrieval systems on both public and private data. In addition, we attempted to learn from issues that have arisen in previous TREC tracks dealing with high-recall re-trieval and private collections. Interested parties are encour-aged to read the track overview [15], visit the track website (trec-total-recall.org), join the mailing list 9 to follow the progress of the track, and contribute to the development of the platform.
Link available from track website.
