 Ola Amayri, Nizar Bouguila n 1. Introduction and analyzed have witnessed a growth in volume and change in its intrinsic structure and type ( Amayri and Bouguila, 2010 ; extract patterns of interest that support the decision of data management. Clustering, among other machine learning approaches, is an important data engineering technique that empowers the automatic discovery of similar object X  X  clusters and the consequent assignment of new unseen objects to appro-mixture models as a formal approach to unsupervised learning salient and compact representation. Thereby, to group these to generate input in a form usable by mixtures. The dominant
X !
Bayes decision rule. However, not all the D features in the membership. To cope with this problem, many feature selection approaches have been devoted to select the subset of relevant models and hold good generalization to unseen data ( Liu and Motoda, 2008 ).

Generally, there are many potential advantages of feature range of practical problems such as detection of spam emails which have been shown to degrade clustering performance. Thus, of feature space is an important step that generally improve et al., 2007 ; Zhang and Chau, 2009 ).

The majority of research works on feature selection, based on finite mixtures, have been blindly directed to develop generic models that do not take the nature of the data into account. data applied to images and videos segmentation. Feature selec-that we can infer this kind of data (i.e. L 2 -normalized) via of study in wide spectrum of areas ranging from biology, medi-work we shall consider movM in our statistical framework.
Our approach combines clustering and feature selection based the optimal number of clusters while assessing the features that permits model selection. In addition, we illustrate the potential of hybrid generative discriminative frameworks upon integrating feature selection, based on both movM and Support tion, to the best of our knowledge none of them has considered the case where these feature vectors are spherical so far. update models incrementally ( Bouguila and Ziou, 2006 ; Zhang et al., 2005 ; Beringer and Hullermeier, 2006 ; Amayri and object labels, finding optimal number of model clusters and finding relevant features at each time step. In Banerjee and Ghosh (2004) , an algorithm for online learning of von Mises
Fisher mixture (a.k.a Langevin mixture Amayri and Bouguila, clustering. The model proposed in Banerjee and Ghosh (2004) , however, updates cluster centroids upon the arrival of new documents while keeping the variance and mixing proportions
EM (RSEM) algorithm to update mixture model parameters based model (GMM). The RSEM was also used in Bouguila and Ziou to the problem of image databases summarization. Yet, previous works do not consider the problem of feature selection. In contrast to these previous works, we propose unsupervised approach that incrementally learns and adjusts the weights of extending RSEM to simultaneously consider feature relevancy while gradually updating given model. 1.1. Contribution
In this paper, we propose a simultaneous clustering and in which spherical data representations are involved. Further-more, our main contributions are: simultaneous feature selection and clustering in the case of to many realtime problems when the data appears as a steam of sequence with time. To this aim, in this paper, we extend this work to online settings extending RSEM to simultaneously
We tackle the problem of automatic determination of the number of components (i.e. model selection) of Von Mises a minimum message length objective that was minimized using EM in off-line scenario and RSEM for online scenario.
Due to the fact that hybrid generative discriminative frame-works have shown improved performance as compared to their generative or discriminative counterparts, we propose the combination of movM and SVM upon integrating feature selection. Indeed, we develop a hybrid framework that models image descriptors, in an unsupervised way, using movM from which Fisher kernel is generated for SVMs.

We present detailed comparison of the proposed Framework using Von Mises mixture model with the widely used Gaussian mixture model (GMM). We discuss the properties of proposed framework on abundant (hundreds of thousands), high-dimen-feature selection model. The merits of proposed approaches are future potential works in Section 6 . 2. Mixture density and feature selection Therein, each document (image) X ! i  X  X  X i 1 , ... , X iD a L -normalized D -dimensional feature vector, such that  X  X !  X 
X ! which the data are concentrated on the circumference of a unit gives the following: p  X  X ! 9 m ! , k !  X  X  Y D zero ( Mardia, 1972 ), y d  X  X  m ! d , k d  X  , m !  X  X  m !  X  X  m d 1 , m d 2  X  is the mean direction, k !  X  X  k 1 , ... , k and  X  Y ! a set of vectors is composed of examples that vary in their characteristics and represent dissimilar information and hence
M -components movM is given by p  X  X ! 9 parameters of the mixture model such that y jd  X  X  m ! jd , k parameters of the j th movM component for feature d , p j of mixing parameters that are positive and sum to one.
The clustering based on finite mixture models is explored by cally on the features that represent each document. Indeed, researchers have proven over the years the fallacy assumption that the more features representing the document the better vant) features that generally highly drop the performance. In order to overcome this problem, we adopt the approach proposed notion of feature saliency using the assumption that a given feature is irrelevant if it follows a common density p  X  Y
In particular, each d th feature is represented by movM of two is very high, then there is no significant difference with the p  X  X ! 9 Y  X  X  where Y  X f Y M , f r jd g , f l jd gg , l jd  X  X  m ! jd 9 l of vM from which the irrelevant feature is drawn. 3. Unsupervised learning of the model
In the following, we present our unsupervised learning approach for simultaneous clustering and feature selection. In model components and to estimate the different parameters. 3.1. Model selection and goodness of fit. Over the years, many approaches have been criterion to find the optimal number of mixture components by minimizing the following function ( Baxter and Oliver, 2000 ): is the expected Fisher information matrix which is generally approximated by complete-data Fisher information matrix in the the case of our unsupervised feature selection model.
In order to define MessLen for our model, in what follow, we assume the independence of the different groups of parameters, prior distribution over the parameters: h  X  Y  X  X  h  X  P !  X  with ( Law et al., 2004 ) h  X  P !  X  p
Moreover, we consider the following priors that we found efficient through our experiments for the concentration 4 mean parameters: h  X  m !
 X  X  1 2 p , h  X  k ! jd  X  X  2 p  X  1  X  k 2
F  X  Y  X  X  block diagonal F  X  P !  X  ,  X  matrix F  X  r jd  X  corresponding to r jd , F  X  y jd  X  and F  X  l and l 9
F  X  Y  X  9  X  9 F  X  P !  X  9
Following Boutemedjet et al. (2009) , we can approximate the determinant of the Fisher information matrix of P ! and r 9
F  X  P 9 F  X  y j  X  9  X  N 2 j where N j is the number observations affected to cluster j and is given by ( Dowe et al., 1995 )
F  X  y jd  X  X  where A  X  k jd  X  X  I 1  X  k jd  X  = I 0  X  k jd  X  and A 0  X  k
I 1  X  k jd  X  I 0  X  k jd  X  . Similarly, we can find F 1  X  l jd log MessLen  X  M  X  X  1 2  X  M  X  5 MD 1  X  log N  X  D 2  X   X   X   X  will use MML-Laplace to learn our model. 3.2. Parameter estimation goal, we adopt common EM approach which generates a sequence goal is to optimize the following objective function: S  X  Y , X  X  X  MessLen  X  M  X  X  x 1 to satisfy the constraints P M j  X  1 p j  X  1and r jd 1  X  r following by maximizing Eq. (13) (See Appendices A X  X  ): ^
Z  X  p  X  where  X  max  X  max ! ! and ! Raphson iterations to find k jd ,where k new follows: Algorithm 1. For each candidate value of M :
Step 0: Apply spherical K-means ( Dhillon and Modha, 2001 )to obtain the initial parameters for each component.

Step 1: Iterate the two following steps until convergence: 1. E-Step : Update ^ Z ij using Eq. (14) . 2. M-Step : Update p j , r jd , m ! jd , m ! jd 9 l , k jd (16), (19), (20) , respectively.
 using Eq. (12) .
 Select the optimal model M n such that M n  X  arg min MessLength  X  M  X  .
 on the EM estimation framework cost where both E-and M-steps have a complexity of O ( NMD ) which is the same complexity associated with standard EM-based learning approaches. 4. Online learning with feature selection
Formally, assume that at time t we have a dataset X  X f X ! of N documents which is represented by an M -component movM with parameters Y t N .Attime t  X  1 a new document X ! N  X  1 introduced and the model should be updated considering the new relevant features and optimal number of model components. To at time t  X  1 in the E-step, thus:
P  X  X ! and E  X  log P  X  X ! N  X  1 , Z ! N  X  1 , L ! N  X  1  X  X   X 
Z ! as Z
N  X  1 , j  X  1 if the new vector X
L ! indicates if feature d is relevant to cluster j ,where r jd
Using RSEM, in the M-step we update the model parameters with respect to Y t  X f F t  X f m t jd , k t jd , m t jd 9 l constraints 0 o p j r 1 and P M j  X  1 p j  X  1( Yao, 2000 ):
F  X  8 &gt; &gt; &lt; &gt; &gt; : where ^ Z positive numbers which decreases to zero or positive definite mixing proportion p j by introducing the Logit transform w  X  j  X  X  log p j = p M such that w M  X  0, where p  X  p  X  need to update r jd such that r jd A  X  0 , 1 .Let r jd 1  X  pose to use parametrization based on Logit transform h jd log r jd =  X  1 r jd  X  ,andweobtain h  X   X  where ^
L  X  8 &gt; &gt; &gt; &gt; &gt; &gt; &gt; &lt; &gt; &gt; &gt; &gt; &gt; &gt; &gt; : as follows: Algorithm 2. For each candidate M :
Step 0: (at iteration t ) Initialization: Y  X  t  X  M Compute the posterior probabilities using Eq. (23) .
Assign X ! N  X  1 to cluster which maximizes ^ Z N  X  1 , j using Eqs. (22), (34) and (27) for j A f M min , ... , M max using Eq. (12) .
  X  M  X  for M n A f M min , ... , M max g .
 running MML model concurrently for models f M min , ... , M tional complexity sake, when the number of components is large candidate model fitting with f Y M min , ... , Y M max g . 5. Experimental results proposed framework on high dimensional data extracted from irrelevant. 5.1. Datasets which contains emails from publicly well-known SpamArchive datasets along with many personal emails of the authors. In particular, it consists of 2021 ham images (HPer) and 3299 spam images (SPer) and a SpamArchive spam (SArc) set with 16 035 images. After preprocessing, we remove images smaller than 10 10 and those cannot be recognized with image processer we end up having 1770 ham and 3112 spam images and 8719 spams from SpamArchive.

For web categorization, we used Yahoo20 6 dataset which contains 2340 articles belonging to 20 categories: Business
Technology (60). 5.2. Evaluation criteria
For evaluation we used typical measures for spam filtering and framework on an Intel(R) Core(TM) 64 Processor PC with the
Windows XP Service Pack 3 operating system and a 4 GB main memory. While in the online, we reported the average time to assign new document (image). Moreover, we calculated accuracy, micro-averaged F 1 and macro-averaged F 1 as follows: F 1  X  micro-averaged  X  X  2 Precision Recall Precision  X  Recall F 1  X  macro-averaged  X  X  where
Precision  X  number of documents correctly predicted in class i
Recall  X  number of documents correctly predicted in class i higher classification quality. 5.3. Filtering using bag of visual words approach
Image-based spam email circumvents easily classic text based approaches consider, however, only the textual content of the texture, shape) which can be very helpful as clearly shown for instance in previous works about content-based image indexing advertise a website). Only few papers have considered the low spam or ham using SVM. This approach has some merits since the of important information about the image content as shown in Perronnin and Dance (2007) .
 feature selection in clustering performance and comparing movM to GMM both with feature selection (movMFS, GMMFS) and without feature selection (movM, GMM). An important step in words (BoVW) approach; thereby each image is represented by a each image, using difference-of-Gaussian (DoG) detector, which clustered using the K-Means algorithm providing 900 visual-words vocabulary. We have tested several vocabulary sizes and the best classification results were obtained with 900 visual is then represented by a 900-dimensional vector describing the frequencies of a set of visual words, provided from the con-structed visual vocabulary. Having these feature vectors, the Probabilistic Latent Semantic Analysis (pLSA) model is applied been shown to improve classification performance. obtained when the number of aspects is set to 40. After we prepared our dataset we randomly split dataset 10 times into training and testing sets, then we start spam filtering.
Fig. 2 shows the number of components found by our pro-posed algorithm when adopting movM and GMM. Evaluation optimal number of mixture components. According to these nents. Moreover, using movM shows a slight improvement over GMM in the majority of the scenarios.

In our next experiments, we investigate the performance of spam dataset. We first arranged images in chronological order. algorithm proposed in Section 3.2 . Later, we used the online algorithm (Section 4 ) each time we insert new image until the M choice of M min and M max is user defined and depends on the without compromising the model complexity.
 movM and GMM show a quite similar speed. However, GMM shows can only update the cluster statistics incrementally.
It is worth mentioning that in this paper we have considered have been used in the past. For instance, in our previous work framework to simultaneously consider feature relevancy while edge of application environment and expert impression where we vectoraswewillshowinournextexperimentswhichhasshownto user may refer to Bouguila (2011) and references therein. 5.4. Filtering using hybrid generative/discriminative learning
Many researchers have paid attention to the complementary and the performance of discriminative approaches, and hence many procedures have been proposed. One common approach 1998 ) which has been shown to provide an elegant way to build
Thus, we develop a hybrid framework that models image descrip-tors using movM from which Fisher kernel is generated for SVMs sequence X with respect to particular parameter. Through the model parameters: p j , k jd , m ! jd , r jd , m ! jd 9 l j  X  1 , ... , M , we obtain @ log p  X  X 9 Y  X  @ log p  X  X 9 Y  X  @ log p  X  X 9 Y  X  @ log p  X  X 9 Y  X  mixing parameters equals one and thus there are only M 1 free mixing parameters. The main goal of this experiment is to ing of movM and GMM with (HmovMFS, HGMMFS) and without feature selection (HmovM, HGMM). Details about the learning of used for SVMs classifier.

In this experiment, we replaced the visual words generation by fitting directly a given generative model (movM, GMM) to the between each of these mixture models were computed giving us kernel matrices to feed SVM classifier which represents our discriminative stage. A summary of the classification results obtained for the different classification tasks is shown in Table 3 . These results show that combining mixture models and
SVMs outperforms classification using pure generative models only. Note that the best results were obtained when hybrid framework was applied using feature selection. The performance of hybrid framework is rather promising, comparing to Maximum
Tree (.87 7 .020) in Dredze et al. (2007) . 5.5. Online web pages clustering
The revolution of the Internet has made Web a popular place in many cases has been accompanied by a large amount of noise that can seriously ruin automated information collection and mining on the Web such in Web pages clustering and information challenging problem as one would need to deal with high-dimensional vectors sometimes tens of thousands of features, application. The first one is to study the impact of feature selection on web pages clustering in off-line settings using
Algorithm 1 that we have proposed in Section 3.2 . The second one is to explore the influence of feature selection in online
We start by preparing our data by extracting the text of the news articles. Next, we applied stemming and removed words that occur less than 5 times and rare words while we kept stop (see Fig. 3 ) show that both movMFS and GMMFS with Feature selection outperform movM and GMM without feature selection. generally in stop words list, that has been shown to affect
Looking closely to macro-averaged F 1 we can clearly see that clustering was influenced by the fact that the categories are online settings in our next experiment we learnt web articles selection improves the clustering performance, we decided to remove stop words and rare words and apply stemming before experiments. We randomly select 1000 articles out of 2340 to insert 286 images based on movM and GMM is given in the insertion of the remaining 1340 articles in case of movM.
Fig. 4 shows the F1(micro-averaged) and F1(macro-averaged)
Note that both movM and GMM achieve best F1(microaveraged) and F1(macro-averaged) when we insert the whole set of docu-that using feature selection as apart of online learning has improved the quality of the clusters. Table 4 shows that the part when we insert the rest of articles. 6. Conclusion objective that was minimized using EM in off-line scenario and spherical data that we confront in many disciplines. Empirical experiments on spam image filtering and web clustering have proven that the proposed algorithm has yielded to good perfor-selection approach, the proposed algorithm has been shown to it defines the relevance of particular feature with respect to different mixture components. Therein, the redundancy between resolve such issue, we can consider Mutual Information (MI) betweenfeaturesinagivenmodel.Indeed,wefindtheapproxima-entropy) and a -degree entropy which have been discussed in paper, further enhancement can be done. A crucial factor, for processes.
 Acknowledgments
The completion of this research was made possible thanks to (NSERC). The helpful comments of anonymous referees are grate-fully acknowledged.
 Appendix A. Proof of Eq. (15) derivative of (13) w.r.t. p j to zero, we obtain @ S  X  Y , X  X  @ p @ S  X  Y , X  X  @ p
D p j x  X  34  X  Multiplying by p j ^
Z ij D p j x  X  0 p j  X  where ^
Z  X 
Computing the derivative w.r.t. x , we obtain 1 P M j  X  1 1 write it as p  X  Appendix B. Proof of Eq. (16) compute the derivative of Eq. (13) w.r.t. r jd 1 @ S  X  Y , X  X  @ r @ S  X  Y , X  X  @ r
M ^
Z @ S  X  Y , X  X  @ r @ S  X  Y , X  X  @ r ^
Z
Similarly, we compute the derivative of Eq. (13) w.r.t. n we obtain 1 r
Summing Eqs. (40) and (42) , we obtain  X   X  max
Thus, according to Eq. (40)  X  then, according to Eq. (42)  X  Appendix C. Proof of Eq. (17)
Compute the derivative of Eq. (13) w.r.t. m jd @ S  X  Y , X  X  @ m @ S  X  Y , X  X  @ m
We have @ @ m p  X  Y ! id 9 y jd  X  X  @ ! @ @ a p  X  Y ! id 9 y jd  X  X  @
Thus, !  X 
And !  X  Appendix D. Proof of Eq. (20)
Compute the derivative of Eq. (13) w.r.t. k jd given that k @ S  X  Y , X  X  @ S  X  Y , X  X 
We have @ @ k
Note that we can use Newton X  X aphson iterations to find k jd k new References
