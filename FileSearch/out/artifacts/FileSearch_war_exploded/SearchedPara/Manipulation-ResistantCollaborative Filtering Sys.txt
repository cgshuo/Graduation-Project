 A collaborative filtering system recommends to users prod-ucts that similar users like. Collaborative filtering systems influence purchase decisions, and hence have become tar-gets of manipulation by unscrupulous vendors. We provide theoretical and empirical results demonstrating that while common nearest neighbor algorithms, which are widely used in commercial systems, can be highly susceptible to manip-ulation, a class of collaborative filtering algorithms which we refer to as linear is relatively robust. These results pro-vide guidance for the design of future collaborative filtering systems.
 I.2.6 [ Computing Methodologies ]: Artificial Intelligence X  Learning Algorithms, Reliability
While the expanding universe of products available via In-ternet commerce provides consumers with valuable options, sifting through the numerous alternatives to identify desir-able choices can be challenging. Collaborative filtering (CF) systems aid this process by recommending to users products desired by similar individuals.

At the heart of a CF system is an algorithm that pre-dicts whether a given user will like various products based on his past behavior and that of other users. Nearest neigh-bor (NN) algorithms, for example, have enjoyed wide use in commercial CF systems, including those of Amazon, Netflix, and Youtube [3, 15, 30]. A prototypical NN algorithm stores each user X  X  history, which may include, for instance, his product ratings and purchase decisions. To predict whether a particular user will like a particular product, the algorithm identifies a number of other users with similar histories. A prediction is then generated based on how these so-called neighbors have responded to the product. This prediction could be, for example, a weighted average of past ratings supplied by neighbors.

Because purchase decisions are influenced by CF systems, they have become targets of manipulation by unscrupulous vendors. For instance, a vendor can create multiple online identities and use each to rate his own product highly and competitors X  products poorly. As an example, Amazon X  X  CF system was manipulated so that users who viewed a spiritual guide written by a well-known Christian evange-list were subsequently recommended a sex manual for gay men [27]. Although this incident may not have been driven by commercial motives, it highlights the vulnerability of CF systems. The research literature offers further empirical ev-idence that NN algorithms are susceptible to manipulation [6, 14, 18, 20, 22, 28, 31, 36].

In order to curb manipulation, one might consider authen-ticating each user by asking for, say, a credit card number to limit the number of fake identities. This may be effective in some situations. However, in web services that do not facilitate financial transactions, such as Youtube, requiring authentication would intrude privacy and drive users away. One might also consider using only customer purchase data, when they are available, as a basis for recommendations be-cause they are likely generated by honest users. Recommen-dation quality may be improved, however, if higher-volume data such as page views are also properly utilized.
In this paper, we seek to understand the extent to which manipulators can hurt the performance of CF systems and how CF algorithms should be designed to abate their in-fluence. We find that, while NN algorithms can be quite sensitive to manipulation, CF algorithms that carry out pre-dictions based on a particular class of probabilistic models are surprisingly robust. For reasons that we will explain in the paper, we will refer to algorithms of this kind as linear CF algorithms .

We find that as a user rates an increasing number of prod-ucts, the average accuracy of predictions made by a linear CF algorithm becomes insensitive to manipulated data. For instance, even if half of all ratings are provided by manipula-tors who try to promote half of the products, predictions for users with long histories will barely be distorted, on average. To provide some intuition for why our results should hold, we now offer an informal argument. A robust CF algorithm should learn from its mistakes. In particular, differences be-tween its predictions and actual ratings should help improve predictions on future ratings. A linear CF algorithm gener-ates predictions based on a probability distribution that is a convex combination of two distributions: one that it would learn given only data generated by honest users and one that it would learn given only manipulated data. As a user whose ratings we wish to predict provides more ratings, it becomes increasingly clear which of these two distributions better represents his preferences. As a result, the weight placed on manipulated data diminishes and distortion van-ishes.

The main theoretical result of this paper formalizes the above argument. In particular, we will define a notion of distortion induced by manipulators and establish an upper bound on distortion, which takes a particularly simple form: Here r is the fraction of data that is generated by manip-ulators and n is the number of products that have already been rated by a user whose future ratings we wish to pre-dict. The bound is very general. First, it applies to all linear CF algorithms. Second, it applies to all manipulation strategies even if manipulators coordinate their actions and produce data with knowledge of all data generated by hon-est users. The bound demonstrates that as the number of prior ratings n increases, distortion vanishes. It also identi-fies the number required to limit distortion to a certain level. This offers guidance for the design of a recommendation sys-tem: the system may, for example, assess and inform users about the confidence of each recommendation. The system may also require a new user to rate a set number of prod-ucts before making recommendations to him. To put this in perspective, consider the following numerical example. Sup-pose a CF system that accepts binary ratings predicts future ratings correctly 80% of the time in the absence of manip-ulation. If 10% of all ratings are provided by manipulators, according to our bound, the system can maintain a 75% rate of correct predictions by requiring each new user to rate at least 21 products before receiving recommendations.
We will also show that our distortion bound does not generally hold for NN algorithms. Intuitively, this is be-cause prediction errors do not always improve the selection of neighbors. In particular, as a user provides more ratings, manipulated data that contribute to inaccurate predictions of his future ratings may remain in the set of neighbors while data generated by honest users may be eliminated from it. As a result, distortion of predictions may not decrease. We will later provide an example to illustrate this.

In addition to theoretical results, this paper provides an empirical analysis using a publicly available set of movie rat-ings generated by users of Netflix X  X  recommendation system. We produce a distorted version of this data set by injecting manipulated ratings generated using a manipulation tech-nique studied in prior literature. We then compare results from application of an NN algorithm called the k nearest neighbor algorithm and a linear CF algorithm called the ker-nel density estimation algorithm. Results demonstrate that while performance of the NN algorithm is highly susceptible to manipulation, that of kernel density estimation algorithm is relatively robust. In particular, the latter experiences dis-tortion lower than the theoretical bound we provide, whereas the distortion for the former exceeds it by far.

One might also wonder whether manipulation robustness of a CF algorithm comes at the expense of its prediction accuracy. As an example, consider an algorithm that fixes predictions for all ratings to be a constant, without regard to the training data. This algorithm is uninfluenced by manip-ulation but is likely to yield poor predictions, and is therefore not useful. In our experiments, the accuracy demonstrated by both algorithms seems reasonable. This suggests that accuracy of a CF algorithm may be achieved alongside ro-bustness.

Our theoretical and empirical results together suggest that commercial recommendation systems using NN algorithms can be made more robust by adopting approaches that we describe. Note that we are not proposing that real-world systems should implement the specific algorithms we present in this paper. Rather, our analysis highlights properties of CF algorithms that lead to robustness and practitioners may benefit from taking these properties into consideration when designing CF systems.

This paper is organized as follows. In the next section, we discuss some related work. In Section 3, we formulate a simplified model that serves as a context for studying alter-native CF algorithms. We then establish results concerning the manipulation robustness of NN and linear CF algorithms in Section 4. In Section 5, we present our empirical study. We make some closing remarks in a final section. Due to space constraints, some results and discussions are abridged or omitted in this paper. For their details, we refer readers to a longer version of this paper [35].
Early research on CF systems focused on their perfor-mance in the absence of manipulation [5, 8, 12, 13, 24, 23, 32, 33]. Almost all work on manipulation robustness has been empirical. For example, [6, 14, 18, 20, 22, 28, 31, 36] present studies on product ratings made publicly available by Internet commerce sites. In each case, manipulated rat-ings were injected, and CF algorithms were tested on the altered data sets. The results point out that NN algorithms and their variants are susceptible to manipulation. This line of work identifies an effective manipulation scheme, which is to create multiple identities and with each identity, provide positive ratings on products to be promoted while rating other products in a manner indistinguishable from that of honest users. Although some algorithms are identified to be relatively robust to certain kinds of attacks, it remains un-clear which algorithms should be used in general and what their trade-offs are.

To the best of our knowledge, the only prior theoretical work on manipulation robustness of CF algorithms is re-ported in [28]. This work analyzed an NN algorithm that uses the majority rating among a set of neighbors as the pre-diction of a user X  X  rating in an asymptotic regime of many users, each of whom rates all products. Manipulators rate as honest users would except on one fixed product. A bound is established on the algorithm X  X  prediction error for this product X  X  rating as a function of the percentage of ratings provided by manipulators. In our work, we do not require users to rate all products and do not constrain manipula-tors to any particular strategies. Further, we study the performance distortion on average, rather than for a sin-gle product. Finally, a primary contribution of our work is in establishing manipulation robustness of linear CF algo-rithms, which turn out to be superior to NN algorithms in this dimension.
Several researchers have proposed alternative approaches to abating the influence of manipulators. In [29], a mecha-nism is proposed where users accumulate reputations while providing ratings that are later validated by observed prod-uct quality, and a user X  X  influence on ratings predictions is limited by his reputation. In this mechanism, a bound is established on the distortion induced by any finite number of manipulators. In [16, 26], researchers propose leveraging trust relationships among users to weight recommendations and fend off manipulation. [17, 21, 31, 34] suggest detecting manipulated ratings based on their patterns and discounting their impact. Our work complements this growing literature. First, additional sources of information can be integrated into the probabilistic framework that we introduce in this paper to further enhance manipulation robustness. Second, the analytical methods that we develop may be useful for studying the benefits of incorporating such information.
Distortion due to manipulation may also be viewed as a loss of utility in a sequential decision problem induced by errors in initial beliefs. Our analysis is based on ideas similar to those that have been used to study the latter topic, which is discussed in [10].

More broadly speaking, apart from collaborative filtering, there are other ways to aggregate users X  response to products in order to provide recommendations. Research has been performed on the manipulation robustness of these systems as well. To get a flavor of this line of work, see [4, 7, 9, 19].
We now formulate a simplified model that will serve as a context for assessing performance of alternative CF algo-rithms. We will first define the product ratings that we work with and then introduce measures of distortion induced by manipulators.
In our model, a user selects ratings from a set S . To sim-plify our discussion, we let S be a finite subset of [0 , 1]. For example, S could be { 0 , 1 } with 0 representing a negative rating and 1 representing a positive rating. Note that all the results in this paper can be easily generalized to accommo-date any finite set S . There are N products, and a user X  X  type is identified by a vector in S N . Each n th component of this vector reflects how the user would rate the n th product after inspecting it.

The CF system has access to ratings provided by M identi-ties who have rated products in the past. The data from each m th identity takes the form of a ratings vector w m  X  S N where S = S  X  X  ? } . Here, an element of S represents a prod-uct rating whereas a question mark indicates that a product has not been rated. We refer to W = ( w 1 ,...,w M )  X  S N  X  M as the training data . This data is used by a CF algorithm to predict future ratings.

Consider a user who is distinct from identities that gen-erated the training data and for whom we will generate rec-ommendations. We will refer to such a user as an active user . We will think of a CF algorithm as providing a prob-ability mass function (PMF) p n,x,W over S for each triplet ( n,x,W )  X  { 1 ,...,N } X  S N  X  S N  X  M . The PMF p n,x,W represents beliefs about how an active user who has so far provided ratings x would rate product n after inspecting it. Such an algorithm can be used to guide recommendations; for example, the CF system might recommend to the active user the product he is most likely to rate highly among those that he has not already rated.
To study the influence of manipulation, we consider a sit-uation where a fraction r of the identities are created by manipulators, while the remaining fraction 1  X  r correspond to distinct honest users. We denote the honest ratings vec-tors by y 1 ,...,y (1  X  r ) M  X  S N and the manipulated ratings vectors by z 1 ,...,z rM  X  S N . Let Y = ( y 1 ,...,y (1  X  r ) M Z = ( z 1 ,...,z rM ) so that the training data is W = ( Y,Z ).
To assess distortion of predictions made by a CF algo-rithm, we consider the following thought experiment. A hy-pothetical active user begins with a ratings vector x 0 , with each n th component set to x 0 n = ?, and inspects products in an order  X  = (  X  1 ,..., X  N )  X   X  N , where  X  N denotes the set of permutations of { 1 ,...,N } . After inspecting product  X  the user rates it by sampling from the PMF p  X  updated ratings vector x k is generated by incorporating this new rating in x k  X  1 . This stochastic process reflects how we would think honest users behave based on the CF algorithm and uncorrupted data set Y . We introduce the following measure of distortion, which we refer to as Kullback-Leibler (KL) distortion : n ( p, X ,Y,Z ) = where D denotes Kullback-Leibler divergence with the nat-ural log. That is, for any two PMFs p and q over support U , difference between PMFs is commonly used in information theory.

For each k , the PMF p  X  that would be made in the absence of manipulators, whereas p nipulation. Hence, D ` p  X  the extent to which the manipulated data Z influences the prediction. We take the expectation of this quantity, with x k  X  1 distributed as the CF algorithm would have predicted if the data set were not corrupted by manipulated data. KL distortion d KL n ( p, X ,Y,Z ) averages these terms over the first n inspected products.

Some algorithms such as NN algorithms generate predic-tions not in the form of PMFs, but as scalars that may be interpreted as the means of PMFs. For these algorithms, it may be more suitable to measure manipulation impact in terms of root-mean-squared (RMS) distortion : n ( p, X ,Y,Z ) = where  X  x  X  dictions of x  X  k by the algorithm based on ratings history x k  X  1 and data sets Y and ( Y,Z ), respectively. Note that if the algorithm generates PMFs as predictions,  X  x  X  and  X  x  X  respect to p  X  expectation in the definition of RMS distortion is taken with x k  X  1 distributed as the CF algorithm would have predicted based on Y . RMS distortion may offer a more transparent assessment than KL distortion because the former computes how much scalar predictions change in the same unit as the predictions themselves. RMS distortion is bounded by a function of KL distortion: This is established in Proposition 1 in [35].

To offer an intuitive interpretation for RMS distortion, we consider a setting where users provide binary ratings and the CF system offers binary predictions based on the PMFs that it generates. That is, we set S = { 0 , 1 } . Given training data Y , for a user with ratings history x system generates a prediction of  X  x  X  uct  X  k if p  X   X  x as the binary prediction based on ( Y,Z ). We define the fol-lowing binary prediction distortion : where each x  X  k is distributed according to p  X  x k  X  1 is distributed as the CF algorithm would have pre-dicted based on Y . This quantity captures the average de-crease in the probability of correct predictions, induced by manipulation. It turns out that binary prediction distortion is bounded by RMS distortion: Proved in Proposition 2 in [35], this result offers an inter-pretation of RMS distortion as an upper bound on the drop in the probability of correct predictions in a binary setting.
One might wonder how we arrive at our distortion mea-sures. For instance, regarding the motivating throught ex-periment, one might question why we sample an active user X  X  ratings from the distribution p  X  a posterior belief obtained by Bayesian update based on a prior belief and observed ratings x k  X  1 . We do so because posteriors are sometimes difficult to represent and Bayesian updates may be computationally prohibitive. The way we sample effectively uses the CF algorithm to encode the prior and approximate its posterior.

One might wonder why we choose our particular distortion measures over other candidates. For instance, one option is to consider the top n most desirable products based on predictions, and define as distortion some measure of their quality change due to the manipulated samples. One rea-son why we prefer KL and RMS distortions is that they are convex functions of predictions while this measure is not. As such, this measure is difficult to analyze. Further, as will be discussed in Section 5.4, in a recent competition of CF algorithms, Netflix uses RMS error to assess their pre-diction accuracies [25]. This suggests that commercial CF algorithms are typically designed to minimize convex mea-sures of error. Our choice of distortion measures is in line with this approach.

Another option one might consider is to measure the worst distortion over all products. While it would be attractive to keep this measure small for all manipulation schemes, that may not be achievable. Further, a large worst-case distortion does not necessarily imply that a CF algorithm is not useful. Consider, for instance, a case where many manipulators aim to distort ratings of many different products, and the out-put of the algorithm is such that the rating of one product is significantly distorted while the rest are not. We might still say that the algorithm is reasonably robust whereas a worst-case measure would indicate otherwise. That said, we note that since KL and RMS distortions characterize aver-age distortions, the robustness results of this paper do not provide guarantees on the distortion of individual products X  ratings.
In this section, we first introduce the notion of probabilis-tic CF algorithms . We then describe a class of such algo-rithms called linear CF algorithms and analyze its robust-ness to manipulation. Finally, we discuss nearest neighbor algorithms and their susceptibility to manipulation.
A probabilistic CF algorithm carries out predictions based on a probabilistic model of how the training data is gener-ated. We will model training data as being generated in the following way. First, user types w m  X  S N are sampled i.i.d. from some PMF. Then, w m  X  S N is sampled from a con-ditional PMF, conditioned on w m , which for each n assigns either w m n =? or w m n = w m n . Note that this model allows for dependence between the type of a user and the products he chooses to rate. This accommodates, for example, sys-tems in which users tend to inspect and rate only products that they care for. Given a PMF  X  over S N  X  S N , we de-note by  X  S and  X  S the marginal PMFs over S N and S N , respectively.

We will call a CF algorithm p probabilistic if for each W there exists a PMF  X   X  p,W over S N  X  S N such that for each n and x , p n,x,W is the marginal PMF of x n conditioned on x , with respect to the joint PMF  X   X  p,W . From here on, we will denote by  X   X  p,W PMF  X   X  p,W corresponding to a probabilistic CF algorithm p and training set W .
We say that a probabilistic CF algorithm p is linear if for This definition states that the PMF  X   X  p, ( W 1 ,W 2 ) CF algorithm p generates based on training data ( W 1 ,W 2 is a convex combination of two PMFs: namely, the PMF  X   X  that it generates based on W 2 .

We now examine the KL distortion that manipulators can induce on a linear CF algorithm. Consider training data W = ( Y,Z ) consisting of ratings vectors Y from honest users and Z from manipulators, with the latter making up a frac-tion r of the training data. The following theorem, which is the main theoretical contribution of this paper, establishes a bound on the resulting KL distortion.
Theorem 1. Fix the number of products N and let p be a linear CF algorithm. Then, for all M , r  X  X  0 , 1 /M,..., ( M  X  This result is proved in [35].

Note that the bound only depends on the number of ac-tive user ratings n and the fraction of data r generated by manipulators. Hence, it represents a worst case bound over all linear CF algorithms p , the number of products N , the quantity M and values ( Y,Z ) of the training data, and the order  X  in which the active user rates products. This means, for example, that it applies even if manipulators coordinate with each other and select ratings with knowledge of the specific CF algorithm p , the honest ratings Y , and the or-dering  X  . This also makes the bound relevant for realistic models of how a recommendation system might sequence products for a user; for example, each  X  k could be the prod-uct that the CF algorithm predicts as being most desirable among remaining ones after the user has inspected products  X 
Note that KL distortion vanishes as the number n of prod-ucts rated by the active user increases. To develop intuition for why this happens, we now offer an informal argument. tical to  X  p,Z will be zero. Otherwise, if  X  p,Y an active user inspects and rates products in the manner that we define, his ratings will tend to be distinguished as sampled from  X  p,Y influence of Z on predictions diminishes as n grows. The bound depends on r through the term ln(1 / (1  X  r )). This term captures the dependence of KL distortion on the fraction of data produced by manipulators. As one would expect, this term vanishes when r is set to zero.

As a corollary of Theorem 1 and the inequality in (1) we have the following bound on RMS distortion.

Corollary 1. Fix the number of products N and let p be a linear CF algorithm. Then, for all M , r  X  X  0 , 1 /M,..., ( M  X 
The bound can offer useful guidance. For example, it ensures that if an active user has rated 22 products and no more than 10% of the training data is manipulated, then the RMS distortion induced by manipulators is less than 0 . 05. In a setting where users provide binary ratings and the system generates binary predictions, according to our bound on binary prediction distortion in (2) in Section 3.2, the average probability of correct predictions decreases by at most 0 . 05. Hence, if a binary CF system predicts ratings correctly 80% of the time in the absence of manipulation, it can maintain this probability at 75% in the presence of manipulation if it requires active users to rate 21 products before receiving recommendations.
 We will introduce examples of linear CF algorithms in Section 5.3. Nearest neighbor algorithms, widely used in commercial CF systems [3, 15, 30], generally come in two classes. The first class predicts a user X  X  ratings based on those provided by similar users, referred to as neighbors. The second class makes predictions on a product based on ratings that the user has provided on similar products, which can also be viewed as neighbors. In this section, we study a simple NN algorithm of the first class and the extent to which its pre-dictions can be distorted by manipulators. We show that the bounds of the previous section do not apply to this NN algo-rithm, and unlike the case of linear CF algorithms, distortion does not generally diminish as the active user inspects and rates products. Though our analysis focuses on a particu-lar NN algorithm, the resulting insights apply more broadly and in particular, to NN algorithms of the second class as well.

We study the case of binary ratings. NN algorithms iden-tify and weight neighbors using a similarity measure. We will consider a similarity measure that increases by one for each pair of consistent ratings and decreases by one for each pair of inconsistent ratings: for any pair of ratings vectors x,y  X  S N .

We consider an NN algorithm that predicts the future rat-ing of product n for a user with ratings vector x by carrying out the following steps. First, the algorithm identifies the subset of the training data samples that offer ratings for product n . If this subset is empty, the NN algorithm op-timistically predicts a rating of 1. Otherwise, from among these ratings vectors, the ones most similar to x are identi-fied. We denote the resulting set of neighbors, which should be a singleton unless there is a tie, by N ( n,x,W ). Finally, an average of their ratings for product n forms the predic-tion: Our observations extend to other more complicated similar-ity metrics and neighbors selection methods. However, we focus on this particular case in order to keep our analysis clean.

We now consider a simple setting that facilitates analysis of RMS distortion in our NN algorithm. We are interested in how RMS distortion changes as the number of ratings n provided by an active user grows. Since n cannot exceed the number of products N , we will define an ensemble of models indexed by N . To facilitate our construction, we will only consider even N .

To keep things simple, we restrict attention to a situation where honest users agree on the ratings of all products. In particular, there is a single user type x odd which rates odd-indexed products 1 and even-indexed products 0. The user type PMF  X   X  S assigns all probability to this vector. Each honest ratings vector y m is generated by sampling a random set of odd numbers between 1 and N  X  1, then for each sample k , replacing components k and k + 1 of x odd with question marks. We assume that the honest ratings Y of training data is such that each set of odd numbers between 1 and N  X  1 is sampled exactly once. That is, each element of Y corresponds to an element of the set { (1 , 0) , (? , ?) } As such, there are 2 N/ 2 honest ratings vectors.

Recalling the setting that we use for assessing distortion, we now consider an active user who inspects products in the ordering  X  = (1 ,...,N ), rating each based on the predic-tion of the NN algorithm. It is easy to see that when there are no manipulators, the NN algorithm perfectly predicts  X  x products, his ratings history x k has x k j = x odd j for j  X  k and x j = ? for j &gt; k .

We assume that manipulators produce one half of the training data. For each honest ratings vector y m , manip-ulators produce a ratings vector z m which agrees with y m on all products rated by y m . However, question marks in y m are replaced by 1 for even indices and 0 for odd in-dices. That is, each z m corresponds to an element of the set { (1 , 0) , (0 , 1) } N/ 2 .

Suppose k is even. Given x k , the NN algorithm predicts what the active user X  X  rating will be for product k + 1. To do this, it identifies neighbors N ( k + 1 ,x k , ( Y,Z )), which includes the following subsets of the training data: Note that each of these sets is of cardinality 2 Vectors in Y 1 and Z 1 correctly rate product k + 1 as 1, whereas vectors in Z 2 incorrectly rate it as 0. As a con-sequence, the prediction for product k + 1 is  X  x k +1 ,x k , ( Y,Z ) 2 / 3 and the resulting squared error is
The preceding argument applies for all even k . For odd k , it is easy to show that the NN algorithm correctly predicts x k +1 = 0. It follows that the RMS distortion for even n is n ( p, X ,Y,Z ) =
The preceding example shows that the RMS distortion of an NN algorithm for r = 1 / 2 does not decrease as n grows. This happens because manipulated data are strate-gically generated to be sufficiently similar to honest data so that no matter how many ratings an active user provides, manipulated ratings vectors will make up a fixed fraction of the neighbors and consequently induce a significant amount of distortion.

In contrast, Corollary 1 establishes that linear CF algo-rithms exhibit a more graceful behavior, with RMS distor-tion vanishing as n increases. This is not to say it is im-possible to design an NN algorithm that exhibits a more desirable behavior when applied to our example. However, it is difficult to know for sure whether a given variation will behave gracefully in all relevant situations.
We now provide an intuitive explanation for why linear CF algorithms should be robust to manipulation relative to NN algorithms. First note that robustness depends on how a CF algorithm learns from its mistakes. In particular, a robust algorithm should notice as it observes differences between its predictions and an active user X  X  ratings that certain things learned from the data set are hurting rather than improving its predictions.

Recall that a linear CF algorithm p generates based on the training set ( Y,Z ) a PMF  X   X  p, ( Y,Z ) tion of  X   X  p,Y would generate based on Y and Z , respectively. As an ac-tive user rates more products, it will be increasingly clear by probabilistic inference that his ratings x are sampled from  X   X  crease the weight on  X   X  p,Y  X   X  makes future predictions more accurate.

In an NN algorithm, on the other hand, inaccurate pre-dictions do not generally improve further predictions. In particular, manipulated ratings vectors that contribute to inaccuracies may remain in the set of neighbors while honest ratings vectors may be eliminated from it. In the example in Section 4.3, for instance, manipulated data are generated so that no matter how long an active user X  X  ratings history is, each honest ratings vector selected as a neighbor has a manipulated counterpart that is as similar, and hence also selected as a neighbor. Consequently, as an active user pro-vides more ratings, the numbers of honest and manipulated neighbors both decrease and stay equal. As a result, inac-curate predictions do not decrease future distortion.
In this section, we present our empirical findings on the manipulation robustness of NN and linear CF algorithms. We first introduce the data set that we worked with and then describe the methods we used to evaluate robustness.
We obtained a set of movie ratings provided by users, made publicly available by Netflix X  X  recommendation sys-tem. Each rating is an integer between 1 and 5, which we and results in our paper apply directly. We randomly sam-pled from the data set 5000 users, who have provided 200000 ratings of 500 movies. We then randomly chose 4000 of these users and for the purpose of our experiments, treated them as honest users and their ratings as a training set Y . We used the ratings of the other 1000 users as a test set, which we refer to as X . We then generated three separate sets of 444, 1714, and 4000 manipulated ratings vectors, respec-tively. Each set, which we refer to as Z for simplicity of discussion, is generated to promote 50% of the movies by using a technique reported to be effective in the literature [6, 14, 18, 20, 22, 28, 31, 36]. Specifically, we randomly sampled 250 of the 500 movies in Y and let each manip-ulated ratings vector in each Z assign the highest ratings to these movies, and assign a random rating to each of the other movies, sampled from the movie X  X  empirical marginal PMF of ratings in Y . We then replaced a random subset of ratings in Z with question marks so that its fraction of ques-tion marks matches that in Y . Manipulated ratings vectors generated this way are meant to be similar to honest ratings vectors except on movies to promote.
To test the robustness of each CF algorithm p , we treated ratings in X as ratings that an active user would provide and let p predict them. Specifically, we fixed n and for each ratings vector x  X  X , identified n random products that it has assigned ratings to and randomly permuted them to x k  X  1 be a ratings vector that agrees with x on products  X  ,..., X  x k  X  1 and assigns question marks to the other prod-ucts. An algorithm p is then used to generate a scalar predic-tion  X  x  X  x and the honest data set Y . Similarly, a prediction based on a training set ( Y,Z ) corrupted by manipulated ratings is denoted by  X  x  X  x nipulation, for each n , we computed the following quantity, which we will refer to as empirical RMS distortion : Here,  X  X = { (  X  x 1 ,..., X  x n ) : x  X  X } . The empirical RMS dis-tortion measures changes of predictions for products rated ,Z ) that we defined earlier, with one difference: whereas n ( p, X ,Y,Z ) samples each x  X  k from the PMF p  X  k ,x k  X  1 that the algorithm generates based on Y ,  X  d RMS n ( p, X  uses elements of X as samples. We used empirical RMS distortion rather than RMS distortion to assess algorithms in our empirical study because computing RMS distortion would take too long, requiring a running time exponential in the number of products n rated by an active user. Further, if a CF algorithm generates a nearly correct distribution in the absence of manipulation, its empirical RMS distortion will be close to its RMS distortion.

Overall, for each algorithm p , we generated multiple sam-ples of X , Y , Z , and  X  X , and averaged their resultant  X 
X ,X,Y,Z ) across samples to obtain reliable estimates. To n  X  40. We tested two CF algorithms. One is an NN algo-rithm called k nearest neighbor. Since it is well-known, we do not present it here and refer the reader to its description in [35]. The other algorithm is a linear CF algorithm called kernel density estimation, which we present next.
Kernel density estimation (KDE) algorithms smooth the training data and use their resultant distribution to pre-dict future ratings. For an in-depth treatment of KDE al-gorithms, see [11]. In our context, we say that a proba-bilistic CF algorithm p is a KDE algorithm with kernels {K w : w  X  S N } if for any W  X  S N  X  M , where each K w is a PMF over S N parameterized by a ratings vector w . It turns out that any KDE algorithm is a linear CF algorithm and any linear CF algorithm is a KDE algorithm. This is established in Proposition 3 in [35].

In our experiments, we considered a KDE algorithm with kernels {K w } such that for each type x  X  S N , where for each s  X  S , k s is a PMF over S defined as follows. For s 6 =?, k s is the unique PMF that satisfies k s ( s ) /k s  X  S . That is, k s assigns the highest probability to s and exponentially lower probabilities to values different from s if s 6 =?, and assigns uniform probability to all values if s =?. It is easy to see that each K w thus defined is a PMF, and it assigns high probability to types similar to w and low probability to others. The constant  X  &gt; 0 tunes the shape of k s , which we set to be 0 . 15 in our experiments.
To predict the rating of product  X  n for a user with past ratings x n  X  1 , our KDE algorithm generates a PMF p  X  n which is the conditional PMF of x  X  n conditioned on x n  X  1 with respect to the joint PMF  X   X  p,W for each s  X  S . The corresponding scalar prediction is the expectation taken with respect to p  X  n ,x n  X  1 ,W :
Figure 1 shows the empirical RMS distortions for the al-gorithms that we tested, with fraction of manipulated data r = 0 . 5. Results for r = 0 . 1 and r = 0 . 3 yield similar in-sights and are presented in [35]. Our results suggest that in practice, KDE algorithm is significantly more robust than k NN. In particular, when a user X  X  ratings history is short, k NN incurs higher empirical RMS distortions than KDE. This difference arises because while k NN ignores question marks, KDE uses them and as a result, tempers its predic-tions. To gain some intuition, let us consider the following problem instance where ratings are binary: the set of honest ratings Y consists of K vectors whose entries are all 1s and as many vectors whose entries are all question marks. The set of manipulated ratings Z consists of K vectors whose entries are all 0s and as many vectors whose entries are all question marks. To predict the first rating x  X  1 of an active user, k NN would yield a prediction of 1 and 1 / 2 based on Y and ( Y,Z ), respectively, incurring an RMS distortion of 1 / 2. KDE would yield a prediction close to 3 / 4 based on Y and a prediction of 1 / 2 based on ( Y,Z ), incurring an RMS dis-tortion of 1 / 4, significantly less than that of k NN. Clearly, the presence of question marks smooths KDE X  X  predictions and keeps its distortion low.

In Figure 1, as more ratings are provided, distortions in-curred by both algorithms decrease. When a user X  X  ratings history is long, KDE incurs a distortion significantly lower than that of k NN. Note that distortion of KDE always stays below the bound in Corollary 1.  X  n ( p, X  X ,X,Y,Z ) Figure 1: Empirical RMS distortion as a function of n when r = 0 . 5 .

One might also wonder whether high robustness of CF algorithms stems from high prediction accuracy or comes at the expense of it. To understand this, we computed a measure of the empirical prediction errors of both CF algo-rithms, which turns out to be reasonably low when compared to, for instance, prediction errors of Netflix X  X  proprietary al-gorithm [25]. Presented in detail in [35], this result suggests that accuracy of a CF algorithm may be achieved alongside robustness.
Our analytical and empirical work suggests that linear CF algorithms can be more robust to manipulation than com-monly used nearest neighbor algorithms. Our results also suggest that it is possible to design algorithms that achieve accuracy alongside robustness. As such, recommendation systems of Internet commerce sites may improve their ro-bustness to manipulation by adopting the approaches that we describe. They may also use the bounds on distortion that we establish as a guide on how many ratings each user should provide to a recommendation system before its pre-dictions can be trusted.

The simple setting in our work serves as a context for the initial development of our idea, and can be extended in multiple ways. One direction is to study the robustness of collaborative filtering algorithms as measured by alternative metrics. One metric could be, for instance, a user X  X  utility loss due to manipulation. Another extension is to design algorithms that provide guarantees on both prediction ac-curacy and robustness.

The framework that we establish also facilitates studying the effectiveness of alternative techniques to abate influence by manipulators. For instance, given a scheme that incen-tivizes users to inspect and rate products, one could analyze how honest users and manipulators would behave, and then use our distortion metrics to assess the robustness of the scheme to manipulation.

It is also worth mentioning that many commercial recom-mendation systems build on multiple sources of information, not just collaborative filtering [1]. For example, as discussed in [2], recommendations should also be guided by features of the products being recommended. An added benefit of the approaches that we present is that they facilitate coherent fusion of multiple sources of information.
 The authors thank Christina Aperjis, Thomas Cover, Paul Cuff, Amir Dembo, Persi Diaconis, Vivek Farias, John Gill, Ramesh Johari, Yi-hao Kao, Yi Lu, Taesup Moon, Beomsoo Park, and Assaf Zeevi for helpful suggestions. This research was supported in part by the National Science Foundation through grant IIS-0428868.
