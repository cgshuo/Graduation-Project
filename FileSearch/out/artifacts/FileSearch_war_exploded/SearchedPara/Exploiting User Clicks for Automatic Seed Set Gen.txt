 Matching entities from different information sources is a very important problem in data analysis and data integration. It is, however, challenging due to the number and diversity of information sources involved, and the significant editorial efforts required to collect sufficient training data. In this pa-per, we present an approach that leverages user clicks during Web search to automatically generate training data for en-tity matching. The key insight of our approach is that Web pages clicked for a given query are likely to be about the same entity. We use random walk with restart to reduce data sparseness, rely on co-clustering to group queries and Web pages, and exploit page similarity to improve matching precision. Experimental results show that: (i) With 360K pages from 6 major travel websites, we obtain 84K match-ings (of 179K pages) that refer to the same entities, with an average precision of 0.826; (ii) The quality of matching obtained from a classifier trained on the resulted seed data is promising: the performance matches that of editorial data at small size and improves with size.
 I.5.3 [ Clustering ]: Algorithms; H.2.8 [ Database Appli-cations ]: Data mining Entity matching, user clicks, random walk, co-clustering
Entity matching is an important problem in data anal-ysis and data integration. We focus in this paper on the entity matching problem that aims to determine whether two (or more) references to an entity from different infor-mation sources describe the same real-world object. This problem is important since multiple organizations may de-scribe the same entity using various descriptions. Matching  X  Authors are ordered alphabetically.
 such references gives information providers the opportuni-ties to improve their services by (i) enriching the descrip-tion of an entity with more comprehensive information; and (ii) providing comparison of various descriptions of an en-tity, especially when inconsistent descriptions exist. For in-stance, travel websites like Yahoo! Travel 1 and TripAdvisor only provide very brief descriptions of tourist attractions, hotels, etc. Matching pages from such sites with Wikipedia pages that refer to the same entities may help these sites to automatically generate more comprehensive descriptions of entities. Moreover, when users look for tourist attractions in Paris,  X  X usee de l X  X rangerie X  receives very high rating compared to most of the well-known attractions like  X  X iffel Tower X  and  X  X usee du Louvre X  in TripAdvisor. Matching pages from Tripadvisor with pages from other travel web-sites that refer to the same entities may help users to realize that the high rating of  X  X usee de l X  X rangerie X  in TripAd-visor is mainly provided by art lovers rather than ordinary tourists. In fact, matching entities from different sources is also important for on-line comparison shopping sites like PriceGrabber 3 , NextTag 4 , etc. to gather and compare the prices, descriptions and reviews of the same products from different on-line shopping sites like Amazon, Walmart, etc. , which helps users to determine where to purchase a product.
However, the lack of unique entity identifiers across dif-ferent information sources, the large amount of information sources ( e.g. , travel, shopping websites, etc. ), and the larger amount of entities ( e.g. , tourist attractions, products, etc. ) make it very challenging to efficiently match the multiple references to the same entity. Existing entity matching ap-proaches mainly rely on machine learned classifiers ( e.g. , SVM [3], decision tree [21], etc. ) to determine whether a pair of entities match not. The performance of these su-pervised learning approaches highly depends on the quality and size of the available training data. Since labeling train-ing data usually requires editorial efforts, it is very expensive and inefficient to produce large-scale training data to ensure accurate entity matching.

In this paper, we propose an unsupervised approach for matching entities. More precisely, we propose an unsuper-vised approach for generating seed matches which can serve as training data for supervised approaches. There are al-ready unsupervised approaches for matching entities [20, 13], which usually determine whether two references are likely to refer to the same entity according to their similarity on a set of attributes. However, as information sources are usually maintained by different organizations, it is not al-ways obvious to access the entities and identify the corre-sponding attributes beforehand. Differently, we leverage the click behavior of Web search users to generate matching en-tities across websites. The basic intuition of the approach is the observation that users looking for an entity like  X  X iffel Tower X  usually click on pages of the same entity from differ-ent websites in response to search queries. Therefore, it is possible to link pages of the same entity across sites based on user click behaviors. This approach allows matching en-tities from different sources without knowing any specific attribute associated to them, and is thus more flexible. However, there are a few challenges with this approach. First, users may use different keywords for searching the same entity. Consequently, for robustness, we need to both correlate search queries and match pages using search queries. Second, clicks of search users are generally sparse as users often click on a very limited number of pages in re-sponse to their queries. This sparseness is both due to users only clicking on pages appearing relevant to their queries and to users not clicking on any page if the titles and snippets of the search results already provide sufficient information.
Contributions: Our main contribution in this work is an unsupervised approach for identifying matching of entities based on user click behaviors in search. More concretely:
The paper is organized as follows. Section 2 defines the problem. Section 3 presents our approach and analyzes its complexity. Section 4 reports its experimental performance. Section 5 surveys the related work and Section 6 concludes the work.
In this work, we focus on matching Web pages from dif-ferent websites that refer to the same entity by mining the Web query click logs. We use click data alone, without con-sidering page content or query content, to ensure scalability.
Data model. We model Web query click logs as a query-page bipartite click graph G =  X  X  , P , E X  . The queries in the logs constitute the partition Q = { q 1 ,q 2 ,...,q n clicked pages constitute the partition P = { p 1 ,p 2 ,...,p There is an edge e i,j  X  E between q i  X  Q and p j  X  P if query q i leads to at least one click on page p j . Each edge is associated with a weight w i,j , indicating the frequency that q leads to a click on p j . Figure 1 illustrates how the click graph is built based on a query click log through an example. Since page p 1 is clicked in response to query q 2 by user u at time t 1 and by user u 4 at time t 4 respectively, the weight of edge e 2 , 1 is set to 2. We use the term  X  X o-click X  in the paper to refer to the fact that query q i and q k both lead to clicks on page p j , i.e. , there exist both edges e i,j and e E . We say that query q i and q k co-click page p j .
Problem statement Given a bipartite click graph G =  X  X  , P , E X  , our objective is to determine, among all the pages in P , the subsets of pages P matching = { P 1 ,P 2 ,...,P where P z  X  P and each page p j  X  P z refers to the same entity o z . We call P z a matching of pages. Note that  X  not form matching with any page. To identify the matching of pages, the algorithm that mines the click graph should be We achieve these goals through a practical approach that exploits the click behavior of Web search users.
The underlying intuition of our approach is the observa-tion that users looking for entities such as  X  X iffel Tower X  usually click on pages from different websites in response to search queries of this entity. Although whether a clicked page refers to the searched entity depends on the query and its context, high level agreement among users ( i.e. , multiple users click on the same page to respond to the same query) can be a good indicator of the true page-entity association. Moreover, if two pages are frequently clicked to respond to the same queries, they are likely to refer to the same entity. Similarly, if two queries frequently lead to clicks on the same page, they are also likely to refer to the same entity.
Based on these observations, we rely on a spectral co-clustering approach [10] to cluster the queries and pages in the bipartite click graph. Basically, this approach recur-sively determines the queries that lead to clicks on the same pages, which in turn determine the pages that are clicked to respond to the same queries. The key challenges to ap-ply this approach in our context are (i) the sparseness of the click graph 5 as pages and queries referring to the same entity may not be sufficiently associated to each other due the the plausibly missing clicks; (ii) the necessity of deter-mining the expected number of clusters given a click graph as required by the co-clustering approach; and (iii) the am-biguity of the resulted clusters as some pages with similar titles (or snippets) but different content may be co-clicked when users were looking for their desired responses.
To reduce sparseness, before co-clustering the click graph, we smooth it using random walk with restart [19, 22, 23] to associate pages with more queries. A crucial aspect of smoothing is to adjust the weights of original edges with respect to additional edges. We explain this in Section 3.2.
To determine the expected number of clusters and refine the resulted clusters to form matchings of pages, we rely on an observation which we call  X  one page per entity per site  X  . That is  X  X or an entity-centric website, there is at most one page referring to a given entity X .

We use  X  X ntity-centric X  to refer to websites like travel and shopping sites that provide information in forms of descrip-tion and reviews on individual entities to facilitate content sharing and serving. Typically, there is only one page in such sites to describe an entity 6 . For example, for the at-traction  X  X iffel Tower X , there is only one (English) page in Wikipedia 7 that gathers information about its different as-pects, and one (English) page in TripAdvisor 8 that presents both travel information and user-generated reviews about it. In fact, as our objective is to match pages from differ-ent websites, even if some website occasionally has multiple pages for the same entity, it does impact the correctness of the matching we obtain.

We explain in Section 3.3 how we rely on the  X  X ne page per entity per site X  observation to determine the number of clus-ters and identify matching of pages from the resulted clus-ters. We refer to such matchings as ClusteredMatching .
It is still possible to have clusters that contain multiple pages from the same website, since ambiguous titles (or snip-pets) of some pages in the search results may lead users to co-click pages that do not refer to the same entity. We call such a cluster ambiguous and use AmbiguousCluster to denote it. We will see in Section 4.2, one third of clusters ob-tained through co-clustering are ambiguous. To improve the effectiveness of our approach, i.e. , identifying more match-ings in a click graph, we further refine AmbiguousCluster by exploiting similarity among pages. We refer to the match-ings of pages obtained in this way as AdvancedMatching and detail in Section 3.4 how they are identified.

Since co-clustering only applies for connected graphs, we first use depth-first search (DFS) [8] to identify all the con-nected components in the click graph. If any pages in a connected component are not from the same website, they naturally form a matching that refers to the same entity. We refer to this kind of matching as SimpleMatching . Queries in each connected component naturally provide an explana-tion of the entity referred to by the SimpleMatching . We then smooth, co-cluster, and refine each of the click graphs corresponding to the remaining connected components to identify ClusteredMatching and AdvancedMatching , which we explain in the following sections. Figure 2 sum-marizes our entity matching approach.
We smooth each connected click graph by performing ran-dom walk with restart on it to discover plausible missing clicks. Intuitively, there exists close semantic relation among neighbor vertices ( i.e. , queries and pages) in the click graph. For example, in Figure 3(a), q 1 and q 2 both lead to clicks on page p 1 , indicating that they are likely to look for the same entity. In the meantime, q 2 leads to clicks on page p 1 p , indicating that p 1 and p 2 are likely to refer to the same entity. Thus, it is likely that q 1 would also lead to a click on page p 2 . In other words, following the edges in the click graph, a random walk starting from q 1 has a probability of arriving at p 2 through path q 1  X  p 1  X  q 2  X  p 2 . If this probability is high enough, we smooth the click graph by adding the edge e 1 , 2 (dash line in Figure 3(b)).
Formally, given a connected click graph G c =  X  X  c , P c , E ( Q c  X  X  , P c  X  X  , E c  X  X  ), we have a n  X  m query-by-page matrix A where A ij is the weight w i,j of edge e i,j in E . The ( n + m )  X  ( n + m ) adjacency matrix of G c can be written as where the elements are ordered: the first n elements in each row (column) index the queries in Q c and the last m ele-ments in each row (column) index the pages in P c . v i de-notes a vertex in G c that is either query q i or page p i
We define the transition probability p ( v j | v i ) from vertex v to vertex v j by normalizing the clicks from v i to v j over all the clicks originated from v i . We obtain the transition matrix M of one step random walk on G c such that
To perform a random walk with restart on G c , at each step, instead of always following an edge in G c to walk to v with probability defined by M ij , the random walk starting at v has probability c (0  X  c  X  1) to go back to (restart from) v . Intuitively, higher probability for a random walk starting at v i to arrive at v j implies higher probability for them to refer to the same entity. We define the similarities between vertex v i and any other vertices in G c as the ( n + m )  X  1 steady-state probability vector ~s i that satisfies where ~r i is the ( n + m )  X  1 restart vector that corresponds to the position for the random walk starting at v i to restart. Thus, the value at the i th position of ~r i is 1 and all the other values are 0. Therefore, we have where I n + m is a ( n + m )  X  ( n + m ) identity matrix. The value of the j th element in ~s i corresponds to the steady-state probability of the random walk starting at v i to arrive at v and thus defines the similarity between these two vertices.
More generally, the similarities between any pair of ver-tices in G c , denoted as matrix S , can be obtained with The value of S ij corresponds the steady-state probability of the random walk starting at v i to arrive at v j and defines the similarity between v i and v j . In fact S = { ~s 1 ,~s 2
Given graph G c and its steady-state probability matrix S , the sub-matrix with values S ij , 1  X  i  X  n and n + 1  X  j  X  n + m , corresponds to the probability of the random walk starting at query q i to arrive at page p j in G c . To discover plausible missing edges originating from query q i , we iterate over all the pages p j in G c ( p j  X  P c ), i.e. , the values from the ( n + 1) th position to the ( n + m ) th position of ~s a pre-defined threshold  X  (0  X   X   X  1), if S ij &gt;  X  and there is no such an edge in G c , we add an edge between p and q j in the corresponding smoothed graph G s . Note that G s = ( Q c , P c , E s ) where Q c and P c are the same as in G
Once an edge is added in G s , we need to assign a weight to it. To motivate our design, we plot the distribution of weights in the original graph G in Figure 4 (Graph character-istics in Section 4.1). We observe that the weights of edges follow a power-law distribution. 59% edges have weight 1 and 99% edges have weight smaller than 10. Since high weight indicates high probability of referring to same entity, the additional edges should not have high weights compar-ing to the actual edges. Therefore, for each additional edge, we set its weight to 1. We also increment the weights of all the edges in G c that start from q i by 1 if at least one edge that starts from q i is added in G s . This ensures that an actual edge always has higher weight (and higher transition probability M ij ) than any additional edge that starts from the same query. Figure 3(b) depicts the smoothed graph af-ter adding edge e 1 , 2 , where e 1 , 2 has weight 1 and the actual edges starting from q 1 , i.e. , e 1 , 1 and e 1 , 4 , have weight 2.
Once the click graph is properly smoothed, we perform co-clustering on the smoothed graph to generate clusters of pages and queries that refer to the same entities. The matchings of pages are extracted from these clusters. Figure 4: Distribution of weights (log-log scale).
Co-clustering pages and queries aims to partition the smoothed click graph into k partitions such that the crossing edges among partitions have minimum weights. We perform singular value decomposition (SVD) on normalized matrix A n of A to obtain its n left singular vectors U = { ~u 1 ,~u ~u } and m right singular vectors V = { ~v 1 ,~v 2 ,...,~v m clusters are expected, we use l = d log 2 k e singular vectors ~u , ~u 3 , ..., ~u l +1 to form U k and ~v 2 , ~v 3 , ..., ~v We finally compute the l -dimensional data set Z k as and apply k -means algorithm to obtain the desired k clus-ters. We discuss the choice of k in the following as it is key to the quality of obtained clusters and matchings.

Ideally, the pages in a cluster should directly form a matching of pages that refer to the same entity. As the number of matchings that can be obtained in a click graph highly depends on the pages it contains, the value of k should be determined according to those pages. Clearly, if a graph only contains pages from one website, these pages do not form any matching of pages. Similarly, if a graph contains one page from Wikipedia and two pages from TripAdvisor, it makes no sense to obtain more than two clusters ( k  X  3) as a matching requires at least two pages. According to the  X  X ne page per entity per site X  observation, it is also unde-sirable to have one cluster ( k = 1) as once the three pages are clustered together, it requires additional refinement to keep only one page from TripAdvisor to form the matching with the Wikipedia page. Based on these observations, we use the following heuristic to determine the value of k . Suppose the smoothed click graph G s contain pages from X different websites, where each website site x contributes c pages to this graph ( i.e. , |P s | = P 1  X  x  X  X c x ). Since each website has at most one page referring to a given entity, the c pages from site x belong to c x different clusters. In order to have a matching contain pages from as many different websites as possible, we set the value of k to the maximum Algorithm 1 Co-clustering on smoothed click graph number of pages from the same website, i.e. ,
Algorithm 1 shows the pseudo-code of the co-clustering process on a smoothed graph. For each resulted cluster C in the set C of k clusters, if all the pages in C z are from dif-ferent websites, they form a ClusteredMatching of pages P z that refer to the same entity. The queries in C z natu-rally form an explanation of the entity that are referred to by this ClusteredMatching . There are also Ambiguous-Cluster that contain multiple pages from the same website. Additional refinement is necessary for them to build match-ings that only consist of pages from different websites.
Given an AmbiguousCluster , we selectively choose one page for each website that contributes multiple pages to the cluster to form a meaningful matching, i.e. , Advanced-Matching . We rely on the similarity among pages to select the pages to form AdvancedMatching . Without knowing the content of pages, an effective way to quantify their sim-ilarity is to exploit their semantic relations exposed by the edges in the click graph. Therefore, we use the steady-state probability matrix S that is computed to smooth the click graph (Section 3.2). This does not require additional com-putation. The sub-matrix with values S ij , n + 1  X  i  X  n + m and n + 1  X  j  X  n + m , corresponds to the probability of random walk starting at page p i to arrive at page p thus the similarity between p i and p j .

Algorithm 2 depicts the pseudo-code for refining an Am-biguousCluster . If AmbiguousCluster C a contains pages from Y websites and each site y contributes c y pages to C a , the pages from different site y with c y = 1 directly form part of a refined cluster C r,l . For site y with c page p i having the maximum similarity with the pages that are already in C r,l is added to C r,l , if this similarity is larger than a pre-defined threshold  X  (0  X   X   X  1). The similarity Similarity ( p i ,C r,l ) between page p i and the refined cluster C r,l is computed as where | C r,l | is the size of the refined cluster C r,l .
If there are multiple site y in C a having c y &gt; 1, besides this refined cluster, other pages from these websites can also form refined clusters. For instance, an AmbiguousCluster with pages of  X  X isneyland X  may contain several matchings Algorithm 2 Refining an ambiguous cluster of pages that refer to Disneyland in different locations of the world. Hence, we iterate over all the websites having multi-ple pages in C a , in ascending order of the number of pages they have ( i.e. , c y ), to obtain more refined clusters. If page p from site y has the maximum Similarity ( p i ,C r,l ) with all the pages in the refined cluster C r,l and Similarity ( p is larger than  X  , p i is added to C r,l .

The pages in each refined cluster containing multiple pages form an AdvancedMatching . We finally map queries to these clusters by selecting the queries in C a that associate at least one pair of pages in the refined clusters. Again, the queries in a refined cluster provide an explanation of the en-tity referred to by the corresponding AdvancedMatching .
As we have explained, the entire entity matching process consists of identifying the connected components in the orig-inal click graph G =  X  X  , P , E X  and smoothing, co-clustering, refining each connected click graph G c with n queries and m pages. The DFS algorithm identifies connected components in linear time, i.e. , O ( |Q| + |P| + |E| ). This can be reduced to logarithmic time with parallel algorithms [14].

The time for smoothing a graph is dominated by the in-version of transition matrix, which is in O (( n + m ) 3 ). If G consists of N connected components, the time for smoothing them does not surpass O ( N ( n + m ) 3 ). Yet, as we will see in Section 4.6, if we double the size of the original click graph G , the number of connected components it contains ( i.e. , N ) also doubles, while the size of the largest connected compo-nent ( i.e. , n + m ) is almost the same and is very small. This suggests that the computational cost of smoothing increases linearly with the size of G as N increases linearly with it.
Regarding to co-clustering, the choice of the desired num-ber of clusters k can be achieved in O ( m ), SVD can be achie-ved in O ( min { n 2 m,nm 2 } ) and k-means can be achieved in O (( n + m ) kI ) with the bounded number of iterations I . Again, for each smoothed graph, the values of n and m are very small compared to the size of G . Thus, the overcall cost of co-clustering increases linearly with the size of G .
The time for refining an ambiguous cluster depends on the number of websites Y in the cluster and the number of pages c y from each website. In the worst case where all the Y sites have the same number of pages, i.e. , c y = m/Y , this process requires c y  X  ( c y +1) 2  X  ( Y  X  1) computations of Simi -refining the ambiguous clusters is bounded by O ( Nm 2 /Y ).
As we will see in Section 4.2, only a small subset of con-nected components requires smoothing, co-clustering and refining to obtain ClusteredMatching and Advanced-Matching . More importantly, despite the cubic complexity to process each connected component, the overall cost only increases linearly with the size of the entire click graph, since the maximum size of the connect components remains small and stable in click graphs of different sizes. This is key to the scalability of our entity matching approach.
Dataset: In the experiments, we use a sample of recent 30-day query click logs of Yahoo! Web search engine. We build the click graph with pages from 6 websites, includ-ing  X  X ripadvisor.com X ,  X  X ravel.yahoo.com X ,  X  X ravelpod.com X ,  X  X irtualtourist.com X ,  X  X ikipedia.org X  and  X  X elp.com X . The first four sites are famous travel websites that assist users in gathering travel information and posting reviews, while  X  X elp.com X  provides a complete list of businesses through-out US and Canada with user-generated reviews. This graph contains 23 , 112 , 812 queries, 3 , 527 , 069 pages and 24 , 935 , 578 edges. Our approach takes advantage of shared clicks to perform clustering. If a page is not co-clicked with pages from other websites for any query, it cannot form any matching. Therefore, we filter out the queries that only lead to clicks on pages from the same websites, as well as the asso-ciated edges. We obtain a click graph with 191 , 645 queries, 360 , 082 pages and 599 , 743 edges. The numbers of pages and edges that related to each website are shown in Table 1.
Methodology: We first rely on DFS to identify all the connected components in the click graph. We identify Sim-pleMatching from the connected components that only consist of pages from different websites. We then apply each step presented in Section 3 to identify matchings in the connected components in which there are at least two pages from the same website. We compare the performance of our approach against an alternative that does not smooth the graph to highlight the benefits of smoothing. We also train a simple SVM model using the matchings generated with our approach to show their potential as seed data for supervised entity matching approaches.
We first present the statistics of the original, smoothed, clustered graphs, and the different types of matchings ob-tained. By performing DFS on the click graph, we obtain 85 , 282 connected components. Since 68 , 600 of them only consist of pages from different websites, the pages in each connected component form a SimpleMatching . We obtain 68 , 600 SimpleMatching .
For the remaining 16 , 682 connected components that con-sist of at least two pages from the same website, we first smooth their corresponding click graphs and then identify the matchings on the smoothed graphs.

We set c to 0.15 and 0.85. 0.15 is typically used in Web graphs to compute PageRank [17] and we use 0.85 as an op-posite case. For each c value, we set the smoothing threshold  X  to four different values. Table 2(a) and Table 2(b) de-pict the number of edges in the 16 , 682 smoothed graphs for c = 0 . 15 and c = 0 . 85. There are 206 , 031 edges in the origi-nal graph. Clearly, higher value of the smoothing threshold  X  leads to fewer added edges in the smoothed graphs.
We co-cluster the graphs to obtain ClusteredMatch-ing . Table 2(c) and Table 2(d) summarize the number of ClusteredMatching and AmbiguousCluster obtained on the smoothed graphs through co-clustering. If the graphs are not smoothed, we obtain 12 , 444 ClusteredMatch-ing and 7947 AmbiguousCluster . Smoothing the graphs slightly increases the number of ClusteredMatching and adding more edges results in more matchings. Smoothing also increases the number of AmbiguousCluster .
 We refine obtained AmbiguousCluster as described in Section 3.4 to exact AdvancedMatching . Figure 5 com-pares the number of AdvancedMatching identified in the original graph and different smoothed graphs with respect to different values of the page-page similarity threshold  X  . We observe that larger  X  leads to fewer AdvancedMatching .
We have measured the number of matchings ( i.e. , Cover-age ) that can be obtained using our approach, i.e. , 68 , 600 SimpleMatching , more than 12 , 000 ClusteredMatch-ing and up to 6600 AdvancedMatching . We further eval-uate the quality of these matchings with respect to different smoothing and refining parameters. We evaluate the accuracy of our approach using Precision , which is the number of correct matchings in which the pages refer to the same entities divided by the total number of obtained matchings. Higher value of preci-sion indicates higher accuracy of the matching approach.
To assess the quality of each obtained matching, we con-duct a user study to examine if all the pages it contains refer to the same entity. Different from the top-k results of a key-word search, whether two pages refer to the same entity is independent of the assessor X  X  own preference, the user study rarely introduces disagreement. Thus a matching is either correct or wrong as an entity can be uniquely identified by a combination of features. For example, for a matching of a hotel, we can compare the name, the address, the home-page of the hotel and the external booking websites ( e.g. ,  X  X ooking.com X ) from each website to verify if they refer to the same hotel. If the available features are not enough to make a judgment, the matching is considered as wrong.
Table 3 gives some examples of correct and wrong match-ings. We see that the queries of correct matchings usually indicate the name of the corresponding entity, such as  X  X y-att regency phoenix X  for the hotel entity, and  X  X atin quarter montreal X  for the tourist attraction entity. Wrong matchings are often due to the ambiguity in the entity names. In the example of wrong matching, the Wikipedia page presents the chain restaurant  X  X he hat X  in general while the other two pages refer to its branches in different locations. We manually inspect the quality of SimpleMatching , ClusteredMatching and AdvancedMatching , obtained either on original or smoothed graphs. In each experiment, we randomly sample 1% to 10% from each kind to ensure that at least 100 matchings are examined for that kind.
The precision of SimpleMatching is 0.843, indicating pages that are co-clicked by the same set of queries have high probability to refer to the same entities. This also confirms our motivation of mining page matchings in click graphs.
The precision of ClusteredMatching is 0.590 if the graphs are not smoothed. Figure 6 shows the precision and coverage of ClusteredMatching on smoothed graphs with different restart probability c and smoothing threshold  X  . We observe that with appropriate values of  X  , e.g. ,  X   X  0 . 1 for c = 0 . 15 and  X   X  0 . 001 for c = 0 . 85, the precision of
ClusteredMatching on smoothed graph is consistently better than on the original graph. Smoothing too much, i.e. , adding too many edges with small  X  , the precision may become worse than if the graph is not smoothed, such as  X  = 0 . 05 for c = 0 . 15. In contrast, if the graph is not smoothed enough with sufficient number of additional edges, the benefits of smoothing may not be significant as in the graph obtained with c = 0 . 85 and  X  = 0 . 0025.

Figure 7 compares the precision and coverage of Ad-vancedMatching for different parameters. We observe that refining AmbiguousCluster results in high quality AdvancedMatching and the precision can reach at 0 . 818. Yet, there is a trade-off between the number of Advanced-Matching and the corresponding precision. A larger value of  X  ensures high precision but fewer matchings are obtained. With  X  = 0 . 1 for c = 0 . 15 and  X  = 0 . 002 for c = 0 . 85, which provide the best precision of ClusteredMatching , the cor-responding precisions of AdvancedMatching are consis-tently better than the precisions obtained on original graph.
To demonstrate the effectiveness of the matchings as a seed set, we train a linear SVM model with bag-of-words features. For the baseline, we train the classifier with an editorially labeled set of 100 pages. From the matchings generated in by our approach (with the setting c = 0 . 15,  X  = 0 . 1 and  X  = 0 . 1), we choose seed sets of varying sizes: 100, 1000, 10000, and 50000 pages. The classifiers are tested on another set of 96 editorially labeled non-overlapping pages. Both editorially labeled sets are sampled from the correct matchings generated by our approach.

Table 4 summarizes the Precision (fraction of correct matchings in the classifier output). We observe that when the size of seed set is small, the performance of the classi-fier trained with automatically generated seed set performs the same as that trained with editorially labeled seed set. Moreover, by increasing the size of the seed set, even if the automatically generated seed matchings are not completely accurate, the performance of the classifier improves. Al-though we do not perform this experiment for large edito-rially labeled seed set given the associated difficulties (the main motivation of our approach), and we rely on simple ma-chine learning model, this result reveals that our approach is promising to generate high quality seed data in a scalable way for supervised entity matching approaches. Table 4: Matching precision wrt. different seed data Table 5: Running time of our approach (in seconds).
We measure the efficiency of our approach using its run-ning time for matching pages. All the experiments are con-ducted on a machine with Intel 2GHz CPU and 4G RAM, except the raw data is cleaned on a Hadoop cluster. We run each experiment for 3 times and report the average running time in Table 5. Basically, our approach takes 15 minutes to identify the potential matchings in the click graph. It is worth noticing that the value of smoothing parameter  X  has almost no impact on running time as the similarity be-tween every pair of query and page needs to be examined in the smoothing process regardless of the value of  X  , and although the time for identifying AdvancedMatching in-creases when the page-page similarity threshold  X  decreases, the actual running time does not surpass 15 seconds in our settings. Compared to matching the pages in the original graph without any smoothing, our approach only takes 154 seconds (16%) longer as no smoothing is performed and less information is examined in the following processes.
To convey the scalability of our approach, we conduct the same experiment with a smaller graph of 67,898 queries, 167,116 pages and 226,675 edges, derived from 10-day query logs of the same period. Interestingly, among the 41,443 connected components requiring co-clustering to identify matching of pages, the largest connected component only contains 203 queries and pages, i.e. , max { n + m } = 203. This value is almost the same as the maximum value in the larger graph derived from 30-day query logs, for which max { n + m } = 212. This reveals that performing our ap-proach in larger click graph does not increase the compu-tational cost cubicly even if the most expensive smoothing is in O (( n + m ) 3 ). Instead, the cost only increases linearly as there are twice more connected components to process in the larger graph with twice more vertices and edges. Table 6: Best-case performance (coverage and pre-cision) of the proposed matching technique (for c = 0 . 15 ,  X  = 0 . 1 ,  X  = 0 . 1 ) and comparison with baseline.
Our evaluation demonstrates that user clicks on Web search is a valuable source of information that can be lever-aged for effective entity matching. With appropriate param-eters, e.g. , c = 0 . 15,  X  = 0 . 1 and  X  = 0 . 1, we obtain up to 84 , 851 matchings on a click graph of 360 , 082 pages. Table 6 summarizes the three kinds of matchings with respect to their coverage (Cov.) and precision (Pre.). These match-ings consist of 179,563 pages, accounting for 49.9% pages in the click graph. This conveys the effectiveness of our entity matching approach. Importantly, using the proposed meth-ods ( i.e. , co-clustering and refining) increases the coverage of SimpleMatching by 23.7% at the cost of modest decrease (11%) in precision. Even if about 0 . 4% fewer matchings are obtained on the smoothed graph, the precisions of Clus-teredMatching and AdvancedMatching are 28% and 8% better than those on the original graph. The overall precision of matching on smoothed graph can reach 0.826, outperforming the precision on the original graph by 4%. This confirms the accuracy of our approach. In fact, our ap-proach is also efficient: 15 minutes are enough to obtain the potential matchings implied in the click graph and the time only increases linearly as the original graph grows. And, finally, we demonstrate the effectiveness of the seed set gen-erated as training data for entity matching.
The work presented in this paper straddles two areas of research: entity matching and user click analysis. Entity matching, also known as record linkage [2], object identifica-tion [21], de-duplication [3], etc. , has been extensively stud-ied in the literature [12, 16]. Numerous approaches, both supervised [3, 21] and unsupervised [13, 20], have been pro-posed for entity matching. The effectiveness of supervised approaches depends on size and quality of training data [16]. Providing training data typically involves editorial efforts to create and choose entity pairs to match, label matched en-tities, etc. Given the large amount of entities and their va-rieties in nature, it is usually difficult and time-consuming to determine a good set of training data. Unsupervised ap-proaches alleviate the necessity of training data, while the similarity functions they use rely on attributes to determine whether a pair of entities is a matching. Yet, information sources are usually maintained by different organizations. It is not always obvious to access the underlying databases of entities ( e.g. , XML or relational records), while extracting attributes from textual description [11] is not practical given the large amount of entities to match.

Therefore, we propose in this work to a novel approach that leverages user clicks on Web search to automatically generate large-scale training data for entity matching. User interactions with search engines has been used for several tasks such as query expansion and clustering [1, 5, 24], Web page clustering [6, 9], entity ranking [4, 15], etc. Specifically, it is revealed in [1] that search click graph captures semantic relations between queries while bicliques and session infor-mation in the click graph is used in [5] and [24] to cluster and suggest similar queries. In [6], search logs are used to find paths in the DOM trees that mark out important con-tent of pages to perform more accurate clustering of pages. In [9], pages relevant to a query are ranked based on user clicks through forward and backward random walks on the click graph. In [4], random walk is performed on both click graph and session graph to rank pages relevant to an en-tity. Differently, we use random walk with restarts to both smooth the graph and rank the pages.

In the area of entity matching, search logs are used to train a machine learned ranking model to predict the rel-evance of query-page pairs in [15]. An entity-aware click graph derived from search logs is used to match websites that fulfill similar user needs in [18]. It is worth noticing that these approaches using user clicks to cluster Web pages or rank entities are not very strict in the sense that related, but not necessarily the same, entities belong to a cluster. For example, while a cluster consisting of queries (or pages corresponding to)  X  X iffel Tower X  and  X  X otre Dame X  will be considered acceptable in all the above approaches, it is in-correct for entity matching. Hence entity matching is a very demanding clustering task. To the best of our knowledge, we are the first to leverage user click behaviors in search for entity matching.
We propose in this paper a practical approach that ex-ploits, for the first time, user clicks on Web search to gener-ate seed data that can be used to train models for large-scale entity matching. Experiments on real datasets show that shared clicks among queries and pages are good indicators for disambiguating and matching the pages referring to the same entities. Reducing the sparseness of Web search data through smoothing further improves the effectiveness and accuracy of matching. Use of the matched data as train-ing data in supervised approach to entity matching appears promising: the performance matches that of editorial data at small size and improves with size.

There is still room to improve the performance. For in-stance, parallel algorithms [14] might be used to identify connected components in click graphs. Once having the con-nected components, the process of identifying matchings in each component can be easily parallelized. Content-based features, such as keywords in queries and URLs, might be considered in addition to content-independent clicks as some incorrect matchings are due to ambiguity implied in the en-tities and could be clarified with more context information. This work was supported by the LEADS project (ICT-318809), funded by the European Community, and the Tor-res Quevedo Program from the Spanish Ministry of Science and Innovation, co-funded by the European Social Fund.
