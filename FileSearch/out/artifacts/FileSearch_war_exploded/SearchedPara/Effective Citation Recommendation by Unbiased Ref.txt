 tion draws more and more attentions recently. well.
 great favour. 
In this paper, we propose two RankSVM-based methods CR and CR+C which are costs less time. section 2.1 Problem Definition candidate papers d  X  D need to be recommended as high quality references. 2.2 RankSVM Model model for ranking training and prediction. 2.3 Feature Extraction definition to characterize these features. 2.4 Content-Based Features  X  Topic Diversity. Some papers proposed a universal method which could draw a 2.5 Structure-Based Features 3.1 CR Model Overview ferent (eg. rank(c 1 ) &lt; rank(c 3 ), rank(a 1 ) &lt; rank(a 4 )). priority between papers. papers with the model. 3.2 Model learning meet which are also the papers should be cited. Algorithm 1. Classifier Learning Input: Cited papers C , Uncited papers U , Loops I Output: Classifier 1. A = {} 2. repeat: 3. A = A  X  {d:d  X  U -A } 4. until: | A |=| C | 5. Classifier = TrainClassifier( C , A ) 6. for i=1,2,...,I do 7. A = {} 8. for d, d  X  C  X  U do 9. d_label = predict(classifier, d) 10. if (d_label==-1 &amp; d  X  U &amp; | A |&lt;| C |) 11. Update A : A = A  X  {d:d  X  U -A } 12. end for 13. Classifier = TrainClassifier( C , A ) 14. end for 15. Return: Learned Classifier sent the ratio between R 2 and R 1  X  R 0 , R 3 and R 1  X  R 0 separately. Algorithm 2. Ranking Model Learning Input: classifer , cited papers C , uncited papers U Output: ranking_model 1. P ={}, N ={}, R 2 ={}, R 3 ={}, ranks ={} 2. for d, d  X  T do 3. d_label = predict( classifier ,d) 4. if(d_label==1): 5. else: 6. end for 7. R 0 = P  X  C , R 1 = N  X  C 8. repeat : 9. R 2 = R 2  X  {d:d  X  P  X  U -R 2 } 10. until : | R 2 |=P 1 *(| R 0 |+| R 1 |) 11. repeat : 12. R 3 = R 3  X  {d:d  X  N  X  U -R 3 } 13. until : | R 3 |=P 2 *(| R 0 |+| R 1 |) 14. for i =1,2,3,4 do : 15. for d, d  X  R i do 16. rank(d)= 5-i 17. ranks = ranks  X  {rank(d)} 18. end for 19. end for 20. ranking_model = TrainRanking( R 0 , R 1 , R 2 , R 3 , ranks) 21. Return: ranking_model are ranked behind FP. We then propose another model called CR+C which do classi-papers. We describe CR+C prediction procedure in Algorithm 3. Algorithm 3. CR+C Recommendation Prediction Input: Classifier , ranking_model , test data T Output: Ranks 1. D ={}, Ranks ={} 2. for d, d  X  T do 3. d_label = predict( classifier , d) 4. if (d_label==1): 5. D = D  X  {d} 6. end for 7. for d, d  X  D do 8. rank(d) = predict(ranking_model, d) 9. Ranks = Ranks  X  {rank(d)} 10. end for 11. Return: Ranks prior researches. 4.1 Data Preparation shows some details about three data subsets. served in the training datasets: schemes [20]. 4.2 Experimental Settings 4.2.1 Compared Methods 4.2.2 Evaluation Metrics We employed common metrics Precision@M (P@M) and Recall@M (R@M) which MRR X   X  tion costs. 4.2.3 Performance Comparison Precision@10, 20, Recall@20, 50, MRR and ART. Table.2 summarizes the comparison 
Even CR and RankSV M biased reference priority RankSVM on all metrics . call@50, and 8.28% grow t For more comprehensive c o ent positions (5 to 100) to comparison results of 4 m positions. 
CR+C and CR get almo s tion, CR+C filters out so m references cannot be recall e gets 0.4979 which is slight l
Even CR+C suffer som e of ART. CR+C costs only that CR+C recalls 85% cit e to say, CR+C needs to do all like CR does. Classific a thus, doing classification b e In this paper, we propose a n are always used to solve c p rovement in Recall@50 a n variation of CR, performs a while, CR+C costs only 1 feasible. problem, such as music or commodity recommendation. Acknowledgments. This work is supported by the Program for New Century Excel-lent Talents of MOE China (Grant No. NCET-11-0213), the Natural Science Founda-of Graduate School of Nanjing University (Grant No. 2014CL03). 
