 Choosing the right product to consume or purchase is nowadays a challenging problem due to the growing number of products and variety of eCommerce services. While increasing the number of choices provides an opportunity for a consumer to acquire the products satisfying her special personal needs, it may at the same time overwhelm by providing too many choices [Anderson 2006]. Recommender Systems (RSs) tackle this problem providing personalized suggestions for digital content, products, or services, that better match the user X  X  needs and constraints than the mainstream products [Resnick and Varian 1997; Ricci et al. 2011b; Jannach et al. 2010]. In this article we focus on the collaborative filtering recommendation approach, and on techniques that are aimed at identifying what information about the user tastes should be elicited by the system to generate effective recommendations. A Collaborative Filtering (CF) recommender system uses ratings for items provided by a network of users to recommend items, which the target user has not yet considered but will likely enjoy [Koren and Bell 2011; Desrosiers and Karypis 2011]. A collabo-rative filtering system computes its recommendations by exploiting relationships and similarities between users. These relations are defined by observing the users X  access of the items managed by the system. For instance, consider the users actively using Amazon or Last.fm. They browse and search items to buy or to listen to. They read users comments and get recommendations computed by the system using the access logs, and the ratings of the users X  community.

The recommendations computed by a CF system are based on the network structure created by the users accessing the system. For instance, classical neighbor-based CF systems evaluate user-to-user or item-to-item similarities based on the corating struc-ture of the users or items. Two users are considered similar if they rate items in a correlated way. Analogously, two items are considered similar if the network of users have rated the items in a correlated way. The recommendations for an active user are then computed by suggesting the items that have a high average rating in the group of users similar to the active one (user-based CF). In item-based approaches, a user is recommended items that are similar to those she liked in the past, where similar means that users rated similarly the two items. Even more novel matrix factorization recommendation techniques [Koren and Bell 2011], which are considered in this arti-cle, are modeling users and items with a vector of abstract factors that are learned by mining the rating behavior of a network of users. In these factor models, in addition to the similarity of users with users and items with items, it is also possible to establish the similarity of users with items, since both of them are represented uniformly with a vector of factors.

Recommender systems are often integrated into eCommerce applications to suggest items to buy or consume, but RSs are now also frequently used in social networks, that is, applications primarily designed to support social interactions between their users. Moreover, in many popular social networks such as Facebook, Myspace, and Google plus, several applications have been introduced for eliciting user preferences and characteristics, and then to provide recommendations. For instance, in Facebook, the largest social network, there are applications where users can rate friends, movies, photos, or links. Examples of such applications are Rate My Friends (with more than 6,000 monthly active users), Rate my Photo, My Movie Rating, and LinkR. Some of these applications collect user preferences (ratings) only to create a user profile page. However, some of them use the data also to make recommendations to the users. For instance, using Rate My Friends, the user is requested to rate her friends. Then the application ranks her friends based on the ratings and presents the top scored users that she may be interested in connecting to. LinkR, another example of recommender systems integrated into a social network, recommends a number of links and allows the user to rate them. Moreover, Facebook itself does also collect ratings by offering a  X  X ike X  button in partner sites and exploits its usage to discover which friends, groups, apps, links, or games a particular user may like.

It is worth noting that all of the applications mentioned before must implement a rating elicitation strategy, that is, identify items to present to the user in order to collect her ratings. In this article we propose and evaluate some strategies for accomplishing this task. Hence, social networks can benefit from the techniques introduced here to generate better recommendations for establishing new social relationships and hence improving the core service of social networks. The CF rating prediction accuracy does depend on the characteristics of the prediction algorithm. Hence, in the last years several variants of CF algorithms have been pro-posed. Koren and Bell [2011] and Desrosiers and Karypis [2011] provide an up-to-date survey of memory-and model-based methods.

In addition to the rating prediction technique, the number, the distribution, and the quality of the ratings known by the system can influence system X  X  performance. In general, the more informative about the user preferences the available ratings are, the higher the recommendation accuracy. Therefore, it is important to keep acquiring from the users new and useful ratings, in order to maintain or improve the quality of the recommendations. This is especially true for the cold start stage, where a new user or a new item is added to the system [Schein et al. 2002; Liu et al. 2011; Zhou et al. 2011; Golbandi et al. 2011].

It is worth noting that RSs usually deal with huge catalogues, for example, Netflix, the popular American provider of on-demand Internet streaming media, manages al-most one million movies. Hence, if the recommender system wants to explicitly ask the user to rate some items, this set must be carefully chosen. First of all, the sys-tem should ask ratings for items that the user has experienced, otherwise no useful information can be acquired. This is not easy, especially when the user is new to the system and there is not much knowledge that can be leveraged to predict what items the user actually experienced in the past. But additionally, the system should exploit techniques to identify those items that if rated by the user would generate ratings data that do improve the precision of the future recommendations, not only for the target user but for all of the system X  X  users. Informative ratings can provide additional knowl-edge about preferences of the users as well as fixing the errors in the rating prediction model. In this work we focus on understanding the behavior of several ratings acquisition strategies, such as  X  X rovide your ratings for these top ten movies X . The goal of a rating acquisition strategy is to enlarge the set of available data in the optimal way for the whole system performance by eliciting the most useful ratings from each user. In practice, an RS user interface can be designed so that users browsing the existing items can rate them if they wish. But, new ratings can also be acquired by explicitly asking users. In fact, some RSs ask the users to rate the recommended items: mixing recommendations with users X  preference elicitation. We will show that this approach is feasible but it must be used with care, since relying on just one single strategy, such as asking the user opinion only for the items that the system believes the user likes, has a potentially dangerous impact on the system effectiveness. Hence a careful selection of the elicitation strategy is in order.

In this article we extend our previous work [Elahi et al. 2011a, 2011b] where we provided an initial evaluation of active learning rating elicitation strategies in collaborative filtering. In this article, in addition to the  X  X ure X  strategies, namely those implementing a single heuristic, we also consider  X  X artially randomized X  ones. Randomized strategies, in addition to asking (simulated) users to rate the items selected by a  X  X ure X  strategy, also ask to rate some randomly selected items. Random-ized strategies can diversify the item list presented to the user. But, more importantly, randomized strategies allow to cope with the nonmonotonically improving behavior of the system effectiveness that we observed during the simulation of certain  X  X ure X  strategies. In fact, we discovered (as hypothesized by Rashid et al. [2002]) that certain strategies, for instance, requesting to rate the items with the highest predicted ratings, may generate a system-wide bias, and inadvertently increase the system error. RSs can be evaluated online and offline [Herlocker et al. 2004; Shani and Gunawardana 2010; Cremonesi et al. 2010]. In the first case, one or more RSs are run and experiments on real users are performed. This requires building or accessing a (or some) fully developed RS, with a large user community, which is expensive and time consuming. Moreover, it is hard to test online several algorithms, such as those proposed here. Therefore, similarly to many previous experimental analyses, we per-formed offline experiments. We developed a program which simulates the real process of rating elicitation in a community of users (Movielens and Netflix), the consequent rating database growth starting from a relatively small one (cold start), and the system adaptation (retraining) to the new set of data. Moreover, in this article we evaluate the proposed strategies in two scenarios: when the simulated users are confined to rate only items that are presented to them by the active learning strategy or when they can voluntarily add ratings on their own.

In the experiments performed here we used a state-of-the-art matrix factorization rating prediction algorithm [Koren and Bell 2011; Timely Development 2008]. Hence our results can provide useful guidelines for managing real RSs that nowadays often rely on this technique. In factor models both users and items are assigned to factor vectors of the same size. Those vectors are obtained from the user ratings matrix with optimization techniques trying to approximate the original rating matrix. Each element of the factor vector assigned to an item reflects how well the item represents a particular latent aspect [Koren and Bell 2011]. For our experiments we employed a gradient descent optimization technique as proposed by Simon Funk [2006]. The main contribution of our research is the introduction and the empirical evaluation of a set of rating elicitation strategies for collaborative filtering with respect to their system-wide utility. Some of these strategies are new and some come from the liter-ature and the common practice. An important differentiating aspect of our study is measuring the effect of each strategy on several RSs evaluation measures and showing that the best strategy depends on the evaluation measure. Previous works focussed only on the rating prediction accuracy (Mean Absolute Error), and on the number of acquired ratings. We analyze those aspects, but in addition we consider the recom-mendation precision, and the goodness of the recommendations X  ranking, measured with Normalized Discounted Cumulative Gain (NDCG). These measures are crucial for determining the value of the recommendations [Shani and Gunawardana 2010].
Moreover, another major contribution of our work is the analysis of the performance of the elicitation strategies taking into account the size of the rating database. We show that different strategies can improve different aspects of the recommendation quality at different stages of the rating database development. We show that in some stages elicitation strategies may induce a bias on the system and ultimately result in a decrease of the recommendation accuracy.

In summary, this article provides a realistic, comprehensive evaluation of several applicable rating elicitation strategies, providing guidelines and conclusions that could help with their deployment in real RSs. Rating elicitation has been also tackled in a few previous works [McNee et al. 2003; Rashid et al. 2002, 2008, Carenini et al. 2003; Jin and Si 2004; Harpale and Yang 2008] that will be surveyed in Section 2. But these papers focused on a problem that is different from what we consider here. Namely, they measured the benefit of the rating elicited from one user, for example, in the sign-up stage, for improving the quality of the recommendations for that user. Conversely, we consider the impact of an elicitation strategy on the overall system behavior, for example, the prediction accuracy averaged on all the system X  X  users. In other words, we try to identify strategies that can elicit from a user ratings that will contribute to the improvement of the system performance for all of the users, and not just for the target user.

Previously conducted evaluations have assumed rather artificial conditions, that is, all the users and items have some ratings since the beginning of the evaluation process and the system only asks of the simulated user ratings that are present in the dataset. In other words, previous studies did not consider the new-item and the new-user problem. Moreover, only a few evaluations simulated users with limited knowledge about the items (e.g., Harpale and Yang [2008]). We generate initial conditions for the rating dataset based on the temporal evolution of the system, hence, in our experiments, new users and new items are present in a similar manner as in real settings. Moreover, the system does not know what items the simulated user has experienced, and may ask ratings for items that the user will not be able to provide. This better simulates a realistic scenario where not all rating requests can be satisfied by a user.
It is also important to note that previous analyses considered the situation where the active learning rating elicitation strategy was the only tool used to collect new ratings from the users. Hence, elicitation strategies were evaluated in isolation from ongoing system usage, where users can freely enter new ratings. We propose a more realistic evaluation setting, where in addition to the ratings acquired by the elicitation strategies, ratings are also added by users on a voluntary basis. Hence, for the valida-tion experiments, we have also utilized a simulation process in which active learning is combined with the natural acquisition of the users X  ratings.
 The rest of the article is structured as follows. In Section 2 we review related works. In Section 3 we introduce the rating elicitation strategies that we have analyzed. In Section 4 we present the first simulation procedure that we designed to more accurately evaluate the system X  X  recommendation performance (MAE, NDCG, and Precision). The results of our experiments are shown in Sections 5 and 6. Then in Section 7 we present the analysis of the active learning strategies when active learning is mixed with the natural acquisition of the user ratings. Finally in Section 8 we summarize the results of this research and outline directions for future work. Active learning in RS aims at actively acquiring user preference data to improve the output of the RS [Boutilier et al. 2003; Rubens et al. 2011]. Active learning for RS is a form of preference elicitation [Bonilla et al. 2010; Pu and Chen 2008; Chen and Pu 2012; Braziunas and Boutilier 2010; Guo and Sanner 2010; Birlutiu et al. 2012], but the current research on active learning for recommender systems has focussed on collaborative filtering, and in particular on the new-user problem. In this setting, it is assumed that a user has not rated any items, and the system is able to actively ask the user to rate some items in order to generate recommendations for the user. In this survey we will focus on AL in collaborative filtering.

In many previous works, which we will describe shortly, the evaluation of a rating elicitation strategy is performed by simulating the interaction with a new user while the system itself is not in a cold start stage, that is, it has already acquired many ratings from users.

Conversely, as we mentioned in the Introduction, in our work we simulate the appli-cation of several rating elicitation strategies in a more diverse set of scenarios; besides the typical settings in which the new user has not rated any items, while the system already possess many ratings provided by other users. We consider a more general scenario where the user repeatedly comes back to the system for receiving recom-mendations, that is, while the system has possibly elicited ratings from other users. Moreover, we simulate a scenario where the system has initially a small overall knowl-edge of the users X  preferences, that is, has a small set of ratings to train the prediction model. Then, step by step, as the users come to the system new ratings are elicited. Another important difference, compared to the state-of-the-art, is that we consider the impact of an elicitation strategy on the overall system behavior. This aims to measure how the ratings elicited from one user can contribute to the improvement of the system performance even when making recommendations for other users.

Finally, we have also investigated a more realistic evaluation scenario where active learning is combined with natural addition of the ratings, that is, some ratings are freely added by the users without being requested. This scenario has not been applied previously. The first research in AL for recommender systems were motivated by the need to implement more effective sign-up processes and used the classical neighbor-based ap-proaches to collaborative filtering [Desrosiers and Karypis 2011]. In Rashid et al. [2002] the authors focus explicitly on the sign-up process, that is, when a new user starts us-ing a collaborative filtering recommender system and must rate some items in order to provide to the system some initial information about her preferences. Rashid et al. [2002] considered six techniques for explicitly determining the items to ask a user to rate: entropy , where items with the largest rating entropy are preferred; random re-quest; popularity , which is measured as the number of ratings for an item, and hence the most frequently rated items are selected; log ( popularity )  X  entropy where items that are both popular and have diverse ratings are selected; and finally item-item personalized , where random items are proposed until the user rates one. Then, a rec-ommender is used to predict what items the user is likely to have seen based on the ratings already provided by the user. These predicted items are requested to the user to rate. Finally, the behavior of an item-to-item collaborative filtering system [Desrosiers and Karypis 2011] was evaluated with respect to MAE under an offline setting that simulated the sign-up process. The process was repeated multiple times and averaged for all the test users. In that scenario the log ( popularity )  X  entropy strategy was found to be the best. For this reason we have also evaluated log ( popularity )  X  entropy in our study. But, it is worth noting that their result could not be automatically extended to the scenario that we consider in this work, that is the evolution of the global sys-tem performance under the application of an active learning strategy applied to all the users. In fact, as we mentioned earlier, in our experiments we simulate the si-multaneous acquisition of ratings from all the users, by asking each user in turn to rate some items, and we repeat this process several times. This simulates the long-term usage of a recommender system where users utilize the system repeatedly to get new recommendations and ratings provided by a user are also used to generate better recommendations for other users (system performance). Subsequently, researchers understood that in order to generate more effective rating elicitation strategies the system should be conversational: it should better motivate the rating requests, focusing on the user preferences, and the user should be able to more freely enter her ratings, even without being explicitly requested.

In Carenini et al. [2003], a user-focussed approach is considered. They propose a set of techniques to intelligently select items to rate when the user is particularly motivated to provide such information. They present a conversational and collaborative interaction model that elicits ratings so that the benefit of doing that is clear to the user, thus increasing the motivation to provide a rating. Item-focused techniques that elicit ratings to improve the rating prediction for a specific item are also proposed. Popularity, entropy, and their combination are tested, as well as their item-focused modifications. The item-focused techniques are different from the classical ones in that popularity and entropy are not computed on the whole rating matrix, but only on the matrix of user X  X  neighbors that have rated an item for which the prediction accuracy is being improved. Results have shown that item-focused strategies are constantly better than unfocused ones.

McNee et al. [2003] address even a more general problem, aiming at understanding which, among the following methods, is the best solution for rating elicitation in the start-up phase: (a) allowing a user to enter the items and her ratings freely, (b) propos-ing to a user a list of items and asking her to rate them, or (c) combining the two approaches. They compare three interfaces for eliciting information from new users that implement the afore mentioned approaches. They performed an online experi-ment, which shows that the two pure approaches produced more accurate user models than the mixed model with respect to MAE. In another group of approaches AL is modeled as a Bayesian reasoning process. Harpale and Yang [2008] developed such an approach extending and criticizing a previous one introduced in Jin and Si [2004]. In fact, in Jin and Si [2004], as is rather common in most AL techniques and evaluation studies, the unrealistic assumption that a user can provide a rating for any presented item is made. Conversely, they propose a revised Bayesian item selection approach, which does not make such an assumption, and which introduces an estimate of the probability that a user has consumed an item in the past and is able to provide a rating. Their results show that the personalized Bayesian selection outperforms Bayesian selection and the random strategy with respect to MAE. Their simulation setting is similar to that used in Rashid et al. [2002], hence for the same reason their results are not directly comparable with ours. There are other important differences between their experiment and ours: their strategies elicit only one rating per request, while we assume that the system makes many rating requests at the same time; they compare the proposed approach only with the random strategy, while we study the performance of several strategies; they do not consider the new-user problem, since in their simulations all the users have at least three ratings at the beginning of the experiment, whereas in our experiments, there are users that have no ratings at all in the initial stage of the experiment; they use a different rating prediction algorithm (Bayesian versus matrix factorization). All these differences make the two sets of experiments and the conclusions hard to compare. Moreover, in their simulations they assume that the system has a larger number of known ratings than in our experiments.
 Many recent approaches to rating elicitation in RS identify the items to request to the user to rate as those providing the most useful knowledge for reducing the prediction error of the recommender system. Many of these approaches exploit decision trees to model the conditional selection of an item to be rated, with regards to the ratings provided by the user for the items presented previously.

In Rashid et al. [2008] the authors extend their former work [Rashid et al. 2002] using a rating elicitation approach based on the usage of decision trees. The proposed technique is called IGCN , and builds a tree where each node is labelled by a particular item to be asked for the user to rate. According to the user rating for the asked item a different branch is followed, and a new node that is labelled with another item to rate is determined. In order to build this decision tree, they first cluster the users in groups, by grouping users with similar profiles, and assigning each user to one of these clusters. The tree is incrementally extended by selecting for each node the item that provides the highest information gain for correctly classifying the user in the right cluster. Hence, the items whose ratings are more important to correctly classify the users in the right cluster are selected earlier in the tree. They also considered two alternative strategies. The first one is entropy 0 that differs from the more classical entropy strategy, which we mentioned before, because the missing value is considered as a possible rating (category 0). Then, the second one is called HELF , where items with the largest value of the harmonic mean of the entropy and popularity are selected. They have conducted offline and online simulations, and concluded that IGCN and entropy 0 perform the best with respect to MAE.

They evaluate the improvement of the rating prediction accuracy for the particu-lar user whose ratings are elicited, while, as we mentioned previously, we measure the overall system effectiveness of a rating elicitation strategy. Moreover, in their ex-periments they use a very different rating prediction algorithm, that is, a standard neighbor-based approach [Desrosiers and Karypis 2011], while we use matrix factor-ization [Koren and Bell 2011].

In a more recent work Golbandi et al. [2010] use three strategies for rating elicitation in collaborative filtering. For the first method, GreedyExtend , the items that minimize the Root Mean Square Error (RMSE) of the rating prediction (on the training set) are selected, that is, those items that have many ratings in the training set and with diverse values. Finally, for Co v erage method, the items with the largest coverage are selected. They defined coverage for an item as the total number of users who corated both the selected item and any other item. They evaluated the performance of these strategies and compared them with previously proposed ones ( popularity , entropy , entropy 0, HELF ,and random ). In their experiments, every strategy ranks and picks the top 200 items to be presented to new users. Then, considering the ratings of the users for these items as training set they predict ratings in a Netflix test set for every single user and compute RMSE. They show that GreedyExtend outperforms the other strategies. In fact, this strategy is quite effective, as it obtains the same error rate, after having acquired just 10 ratings, which the second best strategy ( Var ) achieves after 26 ratings. However, despite this remarkable achievement, GreedyExtend is static, that is, selects the items without considering the ratings previously entered by the user. Even here the authors focus on the new-user problem. In our work we do not make such assumption, and propose and evaluate strategies that can be used in all stages, and not only at the start-up stage.

Even more recently, in Golbandi et al. [2011] the same authors of the paper described before have developed an adaptive version of their approach. Here, the items selected for a user depend on the previous ratings she has provided. They also propose a tech-nique based on decision trees where at each node there is a test based on a particular item (movie). The node divides the users into three groups based on the rating of the user for that movie: lovers, who rated the movie highly; haters, who rated the movie as low; and unknowns, who did not rate the movie. In order to build the decision tree, at each node the movie whose rating knowledge produces the largest reduction of the RMSE is selected. The rating prediction is computed (approximated) as the weighted average of the ratings given by the users that belong to that node. They have evalu-ated their approach using the Netflix training dataset (100M ratings) to construct the trees, and evaluated the performance of the proposed strategy on the Netflix test set (2.8M ratings). The proposed strategy has shown a significant reduction of RMSE com-pared with GreedyExtend , Var ,and HELF strategies. They were able to achieve with only 6 ratings the same accuracy that is achieved by the next best strategy, namely GreedyExtend , after acquiring over 20 ratings. Moreover, that accuracy is obtained by Var and HELF strategies after acquiring more than 30 ratings.

It should be noted that their results are again rather difficult to compare with ours. They simulate a scenario where the system is trained and the decision tree is constructed from a large training dataset. So they assume a large initial knowledge of the system. Then, they focus on completely new users, that is, those without a single rating in the training set. In contrast, in our work, we assume that the system has a very limited global knowledge of the users. In our experiments this is simulated by giving to the system only 2% of the rating dataset. Moreover, we analyze the system dynamics as more users are repeatedly requested to enter their ratings. Finally we want to mention an interesting and related work that is not addressing the active learning process of rating elicitation but is studying the time-dependent evolution of a recommender system as new ratings are acquired. In Burke [2010] the author analyzes the temporal properties of a standard user-based collaborative filtering [Herlocker et al. 1999] and Influence Limiter [Resnick and Sami 2007], a collaborative filtering algorithm developed for counteracting profile injection attacks by considering the time at which a user has rated an item.

They evaluate the accuracy of these two prediction algorithms while the users are rating items and the database is growing. This is radically different from the typical evaluations that we mentioned earlier, where the rating dataset is decomposed into the training and testing sets without considering the timestamp of the ratings. In Burke [2010] it is argued that considering the time at which the ratings were added to the system gives a better picture of the real user experience during the interactions with the system in terms of recommendation accuracy. They conducted their analysis on Movielens large dataset (1M ratings), and discovered that while using Influence Limiter , MAE is not decreasing with the addition of more data indicating that the algo-rithm is not effective in terms of accuracy improvement. For the standard user-based collaborative filtering algorithm they observed the presence of two time segments: the start-up period, until day 70 with MAE dropping gradually, and the remaining period, where MAE was dropping much slower.

This analysis is complementary to our study. This work analyzes the performance of a recommendation algorithm while the users are adding their ratings in a natural manner, that is, without being explicitly requested to rate items selected by an active learning strategy. We have investigated the situation where in addition to this natural stream of ratings coming from the users, the system selectively chooses additional items and presents them to the users to get their ratings.
 A rating dataset R is a n  X  m matrix of real values (ratings) with possible null entries. The variable r ui , denotes the entry of the matrix in position ( u , i ), and contains the rating assigned by user u to item i . r ui could store a null value representing the fact that the system does not know the opinion of the user on that item. In the Movielens and Netflix datasets the rating values are integers between 1 and 5 (inclusive).
A rating elicitation strategy S is a function S ( u , N , K , U u ) = L which returns a list of items L ={ i 1 ,..., i M } , M  X  N , whose ratings should be elicited from the user u , where N is the maximum number of items that the strategy should return, K is the dataset of known ratings, that is, the ratings (of all the users) that have been already acquired by the RS. K is also an n  X  m matrix containing entries with real or null values. The not-null entries represent the knowledge of the system at a certain point of the RS evolution. Finally, U u is the set of items whose ratings have not yet been elicited from u , hence potentially interesting. The elicitation strategy enforces that L  X  U u and will not repeatedly ask a user to rate the same item; that is, after the items in L are shown to a user they are removed from U u .

Every elicitation strategy analyzes the dataset of known ratings K and scores the items in U u . If the strategy can score at least N different items, then the N items with the highest score are returned. Otherwise a smaller number of items M  X  N is returned. It is important to note that the user may have not experienced the items whose ratings are requested; in this case the system will not increase the number of known ratings. In practice, following a strategy may result in collecting a larger num-ber of ratings, while following another one may result in fewer but more informative ratings. These two properties (rating quantity and quality) play a fundamental role in rating elicitation. We considered two types of strategies: pure and partially randomized . The first ones implement a unique heuristic, whereas the second type of strategies hybridize a pure one by adding some random rating requests that are still unknown to the system. As we mentioned in the Introduction these strategies add some diversity to the system requests and, as we will show later, can cope with an observed problem of the pure strategies which may in some cases increase the system error.

The pure strategies that we have considered are as follows.  X  Popularity. For all the users the score for item i  X  U u is equal to the number of not-null ratings for i contained in K , that is, the number of known ratings for the item i . This strategy will rank the items according to the popularity score and then will select the top N items. Note that this strategy is not personalized, that is, the same
N items are proposed to be rated by any user. The rationale of this strategy is that more popular items are more likely to be known by the user, and hence it is more likely that a request for such a rating will increase the size of the rating database.  X  Log(popularity) * entropy. The score for the item i  X  U u is computed by multiplying the logarithm of the popularity of i with the entropy of the ratings for i in K .Also in this case, as for any strategy, the top N items according to the computed score are proposed to be rated by the user. This strategy tries to combine the effect of the popularity score, which is discussed previously, with the heuristics that favor items with more diverse ratings (larger entropy), which may provide more useful (discrim-inative) information about the user X  X  preferences [Carenini et al. 2003; Rashid et al. 2002].  X  Binary Prediction. The matrix K is transformed in a matrix B with the same number of rows and columns, by mapping null entries in K to 0, and not-null entries to 1.
Hence, the matrix B models only whether a user rated ( b ui = 1) or not ( b ui = 0) an item, regardless of its value [Koren 2008]. A factor model, similarly to what is done for standard rating prediction, is built using the matrix B as training data, to compute the predictions for the entries in B that are 0 [Koren and Bell 2011].
In this case predictions are numbers between 0 and 1. The larger is the predicted value for the entry b ui , the larger is the predicted probability that the user u has consumed an item i , and hence may be able to rate it. Finally, the score for the item i  X  U u corresponds to the predicted value for b ui . Hence, by selecting the top N items with the highest score this strategy tries to select the items that the user has most likely experienced, in order to maximize the likelihood that the user can provide the requested rating. In that sense it is similar to the popularity strategy, but it tries to make a better prediction of what items the user can rate by exploiting the knowledge of the items the user has rated in the past. Note that the better are the predictions b ui for the items in U u the larger is the number of ratings that this strategy can acquire.  X  Highest Predicted. A rating prediction  X  r ui , based on the ratings in K , is computed for all the items i  X  U u and the score for i is this predicted value  X  r ui . Then, the top N items according to this score are selected. The idea is that the items with the highest predicted rating are supposed to be the items that the user likes the most. Hence, it could also be more likely that the user has experienced these items. Moreover, her ratings could also reveal important information on what the user likes. We also note that this is the default strategy for RSs, that is, enabling the user to rate the recommendations.  X  Lowest Predicted. It uses an opposite heuristics compared to highest predicted: for all the items i  X  U u the prediction  X  r ui is computed, but then the score for i is Maxr  X   X  r ui , where Maxr is the maximum rating value (e.g., 5). This ensures that the items with the lowest predicted ratings  X  r ui will get the highest score and therefore will be selected for elicitation. Lowest predicted items are likely to reveal what the user dislikes, but are likely to elicit a few ratings, since users tend to not rate items that they do not like; reflected by the distributions of the ratings voluntarily provided by the users [Marlin et al. 2011].  X  Highest and Lowest Predicted. For all the items i  X  U u a prediction  X  r ui is computed.
The score for an item is | Maxr + Minr 2  X   X  r ui | , where Minr is the minimum rating value (e.g., 1). This score is simply the distance of the predicted rating of i from the midpoint of the rating scale. Hence, this strategy selects the items with extreme ratings, that is, the items that the user either hates or loves.  X  Random. The score for an item i  X  U u is a random integer from 1 to 5. Hence also the top N items in U u according to this score are simply randomly chosen. This is a baseline strategy, used for comparison.  X  Variance. The score for the item i  X  U u is equal to the variance of its ratings in the dataset K . Hence this strategy selects the items in U u that have been rated in a more diverse way by the users. This is representative of the strategies that try to collect more useful ratings, assuming that the opinions of the user on items with more diverse ratings are more useful for the generation of correct recommendations.  X  Voting. The score for the item i is the number of votes given by a committee of strategies including popularity, variance , entropy , highest-lowest predicted, binary prediction, and random . Each of these strategies produces its top 10 candidates for rating elicitation, and then the items appearing more often in these lists are selected.
This strategy depends on the selected voting strategies. We have also included ran-dom strategy so as to impose an exploratory behavior.
 Finally, we would like to note that we have also evaluated other strategies: entropy , and log ( pop )  X  v ariance . But, since their observed behaviors are very similar to some of the previously mentioned strategies, we did not include them. A pure strategy may not be able to return the requested number of items. For instance, there are cases where no rating predictions can be computed by the RS for the user u . This happens, for instance, when u is a new user and none of his ratings is known. Hence, in this situation the highest predicted strategy is not able to score any of the items. In this case the randomized version of the strategy can generate purely random items for the user to rate.

A partially randomized strategy modifies the list of items returned by a pure strategy introducing some random items. As we mentioned in the Introduction, the partially randomized strategies have been introduced to cope with some problems of the pure strategies (see Section 5). More precisely, the randomized version Ran of the strategy S with randomness p  X  [0 , 1] is a function Ran ( S ( u , N , K , U u ) , p ) returning a new list of items L computed as follows. (1) L = S ( u , N , K , U u ) is obtained. (2) If L is an empty list, that is, the strategy S for some reason could not generate the (3) If | L | &lt; N , L = L  X  X  i 1 ,..., i N  X  X  L | } , where i j is a random item in U u . (4) If | L |= N , L ={ l 1 ,..., l M , i M + 1 ,..., i N } , where l j is a random item in L , M = In order to study the effects of the considered elicitation strategies we set up the following simulation procedure. The goal is to simulate the influence of elicitation strategies on the evolution of an RS X  X  performance. To achieve this, we partition all the available (not-null) ratings in R into three different matrices with the same number of rows and columns as R .  X  K contains the ratings that are considered to be known by the system at a certain point in time.  X  X contains the ratings that are considered to be known by the users but not by the system. These ratings are incrementally elicited, that is, they are transferred into K if the system asks for them from the (simulated) users.  X  T contains a portion of the ratings that are known by the users but are withheld from X for evaluating the elicitation strategies, that is, to estimate the evaluation measures (defined later).

In Figure 1(b) we illustrate graphically how the partition of the available ratings in a dataset could look. As defined in the previous section, U u is the set of items whose ratings are not known to the system and may therefore be selected by the elicitation strategies. That means that k ui has a null value and the system has not yet asked u for it. In Figure 1(b) these are the items that for a certain user have ratings that are not marked with grey boxes. In this setting, a request to rate an item, which is identified by a strategy S , may end up with a new (not-null) rating k ui inserted in K , if the user has experienced the item i ,thatis,if x ui is not null, or in a no action, if x ui has a null value in the matrix X . The first case corresponds to the situation where the items are marked with a black box for user u in Figure 1(b). In any case, the system will remove the item i from U u as to avoid asking to rate the same item again.
 We will discuss later how the simulation is initialized, namely, how the matrices K , X ,and T are built from the full rating dataset R . In any case, these three matrices partition the full dataset R ;if r ui has a not-null value then either k ui or x ui or t ui is assigned that value, that is, only one of entries is not null.

The test of a strategy S proceeds in the following way. (1) The not-null ratings in R are partitioned into the three matrices K , X , T . (2) MAE, Precision, and NDCG are measured on T , training the rating prediction (3) For each user u : (4) MAE, Precision, and NDCG are measured on T , and the prediction model is re-(5) Repeat steps 3 and 4 (Iteration) for I times.

It is important to note here the peculiarity of this evaluation strategy that has been mentioned already in Section 2. Traditionally, the evaluation of active learning strategies has been user centered; that is, the usefulness of the elicited rating was judged based on the improvement in the user X  X  prediction error. This is illustrated in Figure 1(a). In this scenario the system is supposed to have a large number of ratings from several users, and focusing on a new user (the first one in Figure 1(a)) it first elicits ratings from this new user that are in X , and then system predictions for this user are evaluated on the test set T . Hence these traditional evaluations focussed on the new-user problem and measured how the ratings elicited from a new user may help the system to generate good recommendations for this particular user. We note that eliciting a rating from a user may improve not only the rating predictions for that user, but also the predictions for the other users, which is what we are evaluating in our experiments and it is graphically illustrated in Figure 1(b). To illustrate this point, let us consider an extreme example in which a new item is added to the system. The traditional user-centered AL strategy, when trying to identify the items that a particular user u should rate, may ignore obtaining his rating for that new item. In fact, this item has not been rated by any other user and therefore its ratings cannot contribute to improve the rating predictions for u . However, the rating of u for the new item would allow to bootstrap the predictions for the rest of the users in the system, and hence from the system X  X  perspective the elicited rating is indeed very informative.
The MovieLens [Miller et al. 2003] and Netflix rating databases were used for our experiments. Movielens consists of 100,000 ratings from 943 users on 1682 movies. From the full Netflix dataset, which contains 1,000,000 ratings, we extracted the first 100,000 ratings that were entered into the system. They come from 1491 users on 2380 items, so this sample of Netflix data is 2.24 times sparser than Movielens data.
We also performed some experiments with the larger versions of both Movielens and Netflix datasets (1,000,000 ratings) and obtained very similar results [Elahi et al. 2011a]. However, using the full set of Netflix data required much longer times to perform our experiments since we train and test a rating prediction model at each iteration: every time we add to K new ratings elicited from the simulated users. After having observed a very similar performance on some initial experiments we focussed on the smaller datasets to be able to run more experiments.

When deciding how to split the available data into the three matrices K , X ,and T an obvious choice is to follow the actual time evolution of the dataset, that is, to insert in K the first ratings acquired by the system, then to use a second temporal segment to populate X and finally use the remaining ratings for T . An approach that follows this idea is detailed in Section 7.

But, it is not sufficient to test the performance of the proposed strategies for a particular evolution of the rating dataset. Since we want to study the evolution of a rating dataset under the application of a new strategy we cannot test it only against the temporal distribution of the data that was generated by a particular (unknown) previously used elicitation strategy. Hence we first followed the approach also used in Harpale and Yang [2008] to randomly split the rating data, but unlike Harpale and Yang [2008] we generated several random splits of the ratings into K , X ,and T . This allows us to generate ratings configurations where there are users and items that initially have no ratings in the known dataset K . We believe that this approach provided us with a very realistic experimental setup, letting us to address both the new-user and the new-item problems [Ricci et al. 2011a].

Finally, for both datasets the experiments were conducted by partitioning (randomly) the 100,000 not-null ratings of R in the following manner: 2,000 ratings in K (i.e., very limited knowledge at the beginning), 68,000 ratings in X , and 30,000 ratings in T . Moreover, | L |= 10, which means that at each iteration the system asks from a user his opinion on at most 10 items. The number of iterations was set as I = 170 since after that stage almost all the ratings are acquired and the system performance is not changing anymore. Moreover, the number of factors in the SVD prediction model was set to 16, which enabled the system to obtain a very good prediction accuracy, not very different from configurations using hundreds of factors, as shown in Koren and Bell [2011]. Note that since the factor model is trained at each iteration and for each strategy learning the factor model is the major computational bottleneck of the conducted experiments for this reason we did not use a very large number of factors. Moreover, in these experiments we wanted to compare the system performance under the application of several strategies, hence, the key measure is the relative performance of the system rather than its absolute value. All the experiments were performed five times and results presented in the following section are obtained averaging these five repetitions.

We considered three evaluation measures: Mean Absolute Error (MAE), Precision, and Normalized Discounted Cumulative Gain (NDCG) [Herlocker et al. 2004; Shani and Gunawardana 2010; Manning 2008]. For computing precision we extracted, for each user, the top 10 recommended items (whose ratings also appear in T ) and consid-ered as relevant the items with true ratings equal to 4 or 5.

Discounted Cumulative Gain (DCG) is a measure originally used to evaluate effec-tiveness of information retrieval systems [J  X  arvelin and Kek  X  al  X  ainen 2002], and is also used for evaluating collaborative filtering RSs [Weimer et al. 2008; Liu and Yang 2008]. In RSs the relevance is measured by the rating value of the item in the predicted rec-ommendation list. Assume that the recommendations for u are sorted according to the predicted rating values, then DCG u is defined as where r i u is the true rating (as found in T ) for the item ranked in position i for user u , and N is the length of the recommendation list.
 Normalized discounted cumulative gain for user u is then calculated as where IDCG u stands for the maximum possible value of DCG u , that could be obtained if the recommended items are ordered by decreasing value of their true ratings. We measured also the overall average discounted cumulative gain NDCG by averaging NDCG u over the full population of users. In this section we present the results of a first set of experiments in which the pure strategies have been evaluated. We first illustrate how the system MAE is changing as the system is acquiring new ratings with the proposed strategies. Then we show how the NDCG and the system precision are affected by the considered rating acquisition strategies. The MAE computed on the test matrix T at the successive iterations of the application of the elicitation strategies (for all the users) is depicted in Figure 2. First of all, we can observe that the considered strategies have a similar behavior on both datasets (Netflix and MovieLens). Moreover, there are two clearly distinct groups of strategies. (1) Monotone error decreasing strategies are lowest-highest predicted, lowest pre-(2) Nonmonotone error decreasing strategies are binary predicted, highest predicted,
Strategies of the first group show an overall better performance (MAE) for the middle stage, but not for the beginning and the end stage. At the very beginning, that is, during the iterations 1 X 3 the best performing strategy is binary predicted. Then, during iterations 4 X 11 the voting strategy obtains the lowest MAE on the Movielens dataset. Then the random strategy becomes the best, and it is overtaken by the lowest-highest-predicted strategy only at iteration 46. The results on the Netflix dataset differ as follows. The binary predicted is the best strategy for a longer period, that is, from the beginning until iteration 7, and then voting outperforms this strategy untill iteration 46, where lowest-highest-predicted starts exhibiting the lowest error. At the iteration 80, the MAE stops changing for all of the prediction-based strategies. This occurs because the known set K at that point already reaches the largest possible size for those strategies, that is, all the ratings in X , which can be elicited by these strategies, have been transferred to K . Conversely, the MAE of the voting and random strategies keeps decreasing, until all of the ratings in X are moved to K . It is important to note that the prediction-based strategies (e.g., highest predicted) cannot elicit ratings for which the prediction cannot be made, for example, if a movie has no ratings in K .
The behavior of the nonmonotone strategies can be divided into three stages. First, they all decrease the MAE at the beginning (approximately iterations 1 X 5). Second, they slowly increase it, up to a point when MAE reaches a peek (approximately it-erations 6 X 35). Third, they slowly decrease MAE untill the end of the experiment (approximately iterations 36 X 80). This behavior occurs since the strategies in the sec-ond group have a strong selection bias with regards to the properties of the items which may negatively affect MAE. For instance, the highest predicted strategy at the initial iterations (from 1 X 5) elicits primarily items with high ratings, however, this behavior does not persist as could be seen from the rating distribution for the iterations 35 X 40 (Table I). As a result, in the beginning stages, this strategy adds to the known matrix ( K ) disproportionately more high ratings than low ones, and this ultimately biases the rating prediction towards overestimating the ratings.

Low rated movies are selected for elicitation by the highest predicted strategy in two cases: (1) when a low rated item is predicted to have a high rating, (2) when all the highest predicted ratings have been already elicited or marked as  X  X ot available X  (they are not present in X and removed from U u ). Looking into the data we discovered that at the iteration 36 the highest-predicted strategy has already elicited most of the highest ratings. Hence, the next ratings that are elicited are actually average or low ratings, which reduces the bias in K and also the prediction error. The random and lowest-highest predicted strategies do not introduce such a bias, and this results in a constant decrease of MAE. In addition to measuring the quality of the elicited ratings (as described in the previous section), it is also important to measure the number of elicited ratings. In fact, certain strategies can acquire more ratings by better estimating what items the user has actually experienced and is therefore able to rate. We simulate the limited knowledge of users by making available only the ratings in the matrix X . Conversely, while a strategy may not be able to acquire many ratings, those actually acquired can be very useful for improving recommendations.

Figure 3 shows the number of ratings in K that are known to the system, as the strategies elicit new ratings from the simulated users. It is worth nothing, even in this case, the strong similarity of the behavior of the elicitation strategies in both datasets. The only strategy that differs substantially in the two datasets is random. This is clearly caused by the larger number of users and items that are present in the Netflix data. In fact, while both datasets contain 100,000 ratings, the sparsity of the Movielens dataset is much higher: containing only 2.8% of the possible ratings (1491*2380) versus 6.3% of the possible ratings (943*1682) contained in the Movielens dataset. This larger sparsity makes it more difficult for a pure random strategy to select items that are known to the user. In general this is a major limitation of any random strategy, that is, a very slow rate of addition of new ratings. Hence for relatively small problems (with regards to the number of items and users) the random strategy may be applicable, but for larger problems it is rather impractical. In fact, observing Figure 3, one can see that in the Movielens simulations after 70 iterations, in which 70  X  10  X  943 = 660,100 ratings X  requests were made (iterations * number-of-rating-requests * users), the system has acquired on average only 28,000 new ratings (the system was initialized with 2,000 ratings, hence bringing the total number of ratings to 30,000). This means that only for one out of 23 random rating requests a user is able to provide a rating. In the Netflix dataset this ratio is even worse. It is interesting to note that even the popularity strategy has a poor performance in terms of number of elicited ratings; it elicited the first 28,000 ratings at a speed equal to one rating for each 6.7 rating requests. We also observe that according to our results, quite surprisingly, the higher sparsity of the Netflix sample has produced a substantially different impact only on the random strategy.
 It is also clear that certain strategies are not able to acquire all the ratings in X . For instance, lowest-highest-predicted, lowest-predicted, and highest-predicted stop acquiring new ratings once they have collected 50,000 ratings (Movielens). This is due to the fact that these strategies, in order to acquire from a user her ratings for items, need the recommender system to generate rating predictions for those items. This happens when the user X  X  ratings in the test set T have no corresponding ratings anywhere in the known dataset K , and hence matrix factorization cannot derive any rating predictions for them.

Figure 4 illustrates a related aspect, namely to what degree the acquired ratings are useful for the effectiveness of the system, that is, how the same number of ratings acquired by different strategies can reduce MAE. From Figure 4 it is clear that in the first stage of the process, that is, when a small number of ratings are present in the known matrix K , the random and lowest-predicted strategies collect ratings that are more effective in reducing MAE. Successively, the lowest-highest-predicted strategy acquires more useful ratings. This is an interesting result, showing that the items with the lowest-predicted ratings and random items are providing more useful information, even though these ratings are difficult to acquire. In this section we analyze the results of the experiments with regards to the NDCG metric. As discussed in Section 4, in order to compute NDCG for a particular user, first the ratings for the items in the recommendation list are predicted. Then, the Normalized Discounted Cumulative Gain (NDCG) is computed by dividing the DCG of the ranked list of the recommendations with the DCG obtained by the best ranking of the same items for that user. NDCG is computed on the top 10 recommendations for every user. Moreover, recommendations lists are created only with items that have ratings in the testing dataset. This is necessary in order to compute DCG. We note that sometimes the testing set contains less than 10 items for some users. In this case NDCG is computed on this smaller set.

Moreover, when computing NDCG, in some cases the rating prediction algorithm (matrix factorization) cannot generate rating predictions for all 10 items that are in test set of a user. This happens when the user X  X  ratings in the test set T have no corresponding ratings anywhere in the known dataset K , and hence matrix factoriza-tion cannot derive any rating predictions for them. It is important to notice that ideal recommendation lists for a user are rather stable during the experiments that use the same dataset. Therefore, if an algorithm is not able to generate predicted recommenda-tion lists of size 10, lists of the size which is available are used which results in smaller NDCG values.

Figure 5 depicts the NDCG curves for the pure strategies. Higher NDCG value corresponds to higher rated items being present in the predicted recommendation lists. Popularity is the best strategy at the beginning of the experiment. But at iteration 3, in the Movielens dataset, and 9 in the Netflix dataset, the voting strategy passes the popularity strategy and then remains the best one. In Movielens the random strategy overtakes the voting strategy at iteration 70, but this is not observed in the Netflix data. Excluding the voting and random strategies, popularity, log(popularity)*entropy, and variance are the best in both datasets. Lowest predicted is by far the worst, and this is quite surprising considering how effective it is in reducing MAE. By further analyzing the experiment data we discovered that the lowest-predicted strategy is not effective for NDCG since it is eliciting more ratings for the lowest ranked items which are useless for predicting the ranking of the top items. Another striking difference from the MAE experiments is that all the strategies improve NDCG monotonically. It is also important to note that here the random strategy is by far the best. This is again different from its behavior in MAE experiments.
 As we have already observed with regards to MAE and NDCG, for both Netflix and Movielens datasets very similar results were observed in the initial experiments. For this reason, in the rest of this article we use just the Movielens dataset. Precision, as it was described in Section 4, measures the proportion of items rated 4 and 5 that are found in the recommendation list. Figure 6 depicts the evolution of the system precision when the elicitation strategies are applied. Here, highest predicted is the best performing strategy for the largest part of the test. Starting from iteration 50 it is as good as the binary-predicted and the lowest-highest-predicted strategies. It is also interesting to note that all the strategies monotonically increase the precision. Moreover, the random strategy, differently from NDCG, does not perform so well, if compared with the highest-predicted strategy. This is again related to the fact that the random strategy increases substantially the coverage by introducing new users. But for new users the precision is significantly smaller as the system has not enough ratings to produce good predictions.

In summary from these experiments one can conclude that among the evaluated strategies there is no single best strategy, that dominates the others for all the eval-uation measures. The random and voting strategies are the best for NDCG, whereas for MAE low-high predicted performs quite good, and finally for Precision low-high predicted, highest predicted, and voting work well. Among the pure strategies only the random one is able to elicit ratings for items that have not been evaluated by the users already present in K . Partially randomized strate-gies address this problem by asking new users to rate random items (see Section 3). In this section we have used partially randomized strategies where p = 0 . 2, that is, at least 2 of the 10 items that are requested to be rated by the simulated users are chosen at random.

Figure 7 depicts the system MAE evolution during the experimental process. We note that here all the curves are monotone, that is, it is sufficient to add just a small portion of randomly selected ratings to the elicitation lists to reduce the bias of the pure, prediction-based, strategies.

It should be mentioned that we have not evaluated the partially randomized voting strategy because it already includes the random strategy as one of the voting strategies. The best performing partially randomized strategies, with respect to MAE, are, at the beginning of the process, the partially randomized binary-predicted, and subsequently the low-high-predicted (similarly to the pure strategies case).

Figure 8 shows the NDCG evolution under the effect of the partially randomized strategies. During iterations 1 X 6, the partially randomized popularity strategy obtains the best NDCG. During iterations 7 X 170, namely, for the largest part of the test, the best strategy is the partially randomized highest predicted. Again, as we observed for the pure strategy version, the worst is the lowest predicted. It is important to note that the strategies that show good performance at the beginning (partially-randomized-highest and binary-predicted strategies) are those aimed at finding items that a user may know and therefore is able to rate. Hence, these strategies are very effective in the early stage when there are many users with very few items in the known dataset K .
Figure 9 shows the precision of the partially randomized strategies. The partially randomized highest-predicted strategy shows again the best results during most of the test, as for NDCG. During the iterations 1 X 6 the best strategy with respect to precision is partially-randomized binary-predicted strategy, but then the classical approach of requesting the user to rate the items that the system considers the best recommenda-tions (highest predicted) is the winner. During iterations 111 X 170 partially randomized variance, popularity, log(popularity)*entropy, highest predicted and binary predicted have very similar precision values. Similarly to NDCG, the worst strategy is the lowest predicted, that is, eliciting ratings for the items that user dislikes does little to improve the recommender X  X  precision. Interestingly this is not the case if the goal is to improve MAE.
 For these experiments, we designed a procedure to simulate the evolution of an RS X  X  performance by mixing the usage of active learning strategies with the natural acqui-sition of ratings. We are interested in observing the temporal evolution of the quality of the recommendations generated by the system when, in addition to exploiting an active learning strategy for requesting the user to rate some items, the users were able to voluntarily add ratings without being explicitly requested to do so, just as happens in actual settings. To accomplish this goal, we have used the larger version of the Movielens dataset (1,000,000 ratings) for which we considered only the ratings of users that were active and rated movies for at least 8 weeks (2 month). Movielens consists of 377,302 ratings from 1,236 users on 3,574 movies. The ratings are timestamped with values ranging from 25/04/2000 to 28/02/2003. We measure the performance of the recommendation algorithm on a test set, as more and more ratings are added to the known set K , while the simulated time advances from 25/04/2000 to 28/02/2003. We combined this natural acquisition of the ratings with active learning as described next.

We split the available data into three matrices K , X ,and T , as we did previously, but now we also consider the timestamp of the ratings. Hence, we initially insert in K the ratings acquired by Movielens in the first week (3,705 ratings). Then we split randomly the remaining ratings to obtain 70% of the ratings in X (261,730) and 30% in T (111,867).

For these new experiments, we perform a simulated iteration every week. That is, each simulated day (starting from the second week) an active learning strategy requests for rating 40 items from each user, who already has some nonnull ratings in K ,thatis, those user X  X  ratings that are known by the system at that point in time. If these ratings are present in X , they are added to K . This procedure is repeated for 7 days (1 week). Then, all the ratings in the Movielens dataset that according to the timestamps were acquired in that week are also added to K . Finally, the system is trained using the ratings in K . To achieve a realistic setting for evaluating predictive performance of RS, we use only the items in T that users actually experienced during the following week (according to the timestamps). This procedure is repeated for I = 48 weeks (1 year).
In order to justify the large number of rating requests that the system makes each week, it is important to note that the simulated application of an active learning strategy, as we do in our experiments, is able to add many fewer ratings than what could be elicited in a real setting. In fact, the number of ratings that are supposed to be known by the users in the simulated process is limited by the number of ratings that have been actually acquired in the Movielens dataset. In Elahi et al. [2011a] it has been estimated that the number of items that are really known by the user is more than 4 times larger than what is typically observed in the simulations. Hence, a lot of our elicitation request would be unfulfilled, even though the user in actuality would have been able to rate the item. Therefore, instead of asking for 10 items as is typically done, we ask for 4 times as many items (40 items), to adjust for the discrepancy between the knowledge of the actual and simulated users.

In order to precisely describe the evaluation procedure, we use the following nota-tions, where n is the week index.  X  K n is the set of ratings known by the system at the end of the week n . These are the ratings that have been acquired up to week n . They are used to train the prediction model, compute the active learning rating elicitation strategies for week n + 1, and test the system X  X  performance using the ratings contained in the test set of the next week n + 1, T n + 1 .  X  T n + 1 is the set of ratings timestamped during the week n + 1 that are used as test set to measure the system performance after the ratings in the previous weeks have been added to K n .  X  AL n is the set of ratings, elicited by a particular elicitation strategy, and is added to known set ( K n )atweek n . We note that these are ratings that are present in X but not in T . This is required for assuring that the active learning strategies are not modifying the test set and that the system performance, under the application of the strategies, is consistently tested on the same set of ratings.
  X  X n This is the set of ratings in X timestamped in week n that are not in the test set T n . These ratings, together with the ratings in T n , are all of the ratings acquired in
Movielens during the week n , and therefore are considered to have been naturally provided by the (simulated) users without being asked by the system (natural acqui-sition). We note that it may happen that an elicitation strategy has already acquired some of these ratings, that is, the intersection of AL n and X n may be not empty. In this case, only those not yet actively acquired are added to K n .

The testing of an active learning strategy S now proceeds in the following way.  X  X ystem initialization: week 1 (1) The entire ratings are partitioned randomly into the two matrices X , T . (2) The not null ratings in X 1 and T 1 are added to K 1 : K 1 = X 1  X  T 1 . (3) U u , the unclear set of user u , is initialized to all the items i with a null value k ui (4) The rating prediction model is trained on K 1 , and MAE, Precision, and NDCG  X  X or all the weeks n starting from n = 2 (1) Initialize K n with all the ratings in K n  X  1 . (2) For each user u with at least 1 rating in K n  X  1 : (3) Add to K n the ratings timestamped in week n and those elicited by S : K n = (4) Train the factor model on K n . (5) Compute MAE, Precision, and NDCG on T n + 1 . Figure 10 shows the MAE time evolution for the different strategies. It should be noted that there is a huge fluctuation of MAE, from week to week. This is caused by the fact that for every week we train the system on the previous weeks data and we test the system performance on the next week X  X  ratings in the test set. Hence, the difficulty of making good predictions may differ from week to week. For this reason, in the figure we focus on a time range: weeks 1 X 17. In this figure the value at week n is obtained after the system has acquired the ratings for that week, and this is the result of evaluating the system X  X  performance on week n + 1 (see the description of the simulation procedure in the previous section). The natural acquisition curve shows the MAE of the system without using items acquired by the AL strategies, that is, the added ratings are only those that have been acquired during that week in the Movielens dataset.

The results show that in the second week the performance of the all strategies is very close. But starting from the third week popularity and log(popularity)*entropy both perform better than the others. These two strategies share similar characteristics and outperform all the other strategies on the whole rating elicitation process. Voting, variance, and random are the next best strategies in terms of MAE.

In order to better show the results of our experiments, in Figure 11 we plot three strategies that can be representative of other strategies. We have chosen log(popularity)*entropy since it is one of the state-of-the-art strategies, highest pre-dicted since it performs very similar to other prediction-based strategies, and voting which is a novel strategy.

Considering the MAE obtained by the natural acquisition of ratings as a baseline we can observe that the highest predicted does not perform very differently from the baseline. The main reason is that this strategy is not acquiring additional ratings besides those already collected by the natural process, that is, the user would rate these items on his own initiative. The other strategies, in addition to these ratings, are capable of eliciting more ratings, also those that the user will rate later on, namely in the successive weeks. We observe that here, differently from the previous experiments, all the strategies show a nonmonotone behavior. But, in this case, it is due to the fact that the test set, every week, is a subset of the ratings entered in Movielens during the following week. The predictive difficulty of this test set can therefore change from week to week, and hence influence the performance of the competing strategies.
In order to examine the results further, we have also plotted in Figure 12 the MAE of the strategies normalized with respect to the MAE of the baseline, that is, without this normalized behavior only for the three selected strategies. This figure more clearly shows the benefit of an active learning strategy in comparison with the natural process. Moreover, in Figure 13 the number of new users entering the system every week is also plotted, so as to understand the effect of new users entering the system on the system performance under the application of the considered strategies. The left y-axis in this figure shows the number of new users in the known set K n and the right y-axis shows the MAE normalized by the baseline. The gray solid line depicts the number of new users entering the system every week.

Comparing the strategies in Figure 13, we can distinguish two types of strategies: the first type corresponds to the highest-predicted strtategy whose normalized MAE is very close to the baseline. The second type includes log(popularity)*entropy and voting strategies that express larger variations of performance, and substantially differ from the baseline (excluding the week 10). The overall performance of these strategies is better than the performance of the first type. Moreover, observing the number of new users at each week we can see that the largest number of new users is entering at weeks 9, 10, and 14. For these weeks the normalized MAE shows the worst perfor-mances, with the largest value of MAE at week 10. Hence, the bad news is that in the presence of many new users none of the strategies is effective, and better solutions need to be developed.

Despite the fact that new users are detrimental to the accuracy of the prediction, in the long term, more users entering the system would result in a better recommender system. Thus, we have computed the correlation coefficients between MAE curves of the strategies and the total number of users in known set K n . Table II shows these correlations as well as the corresponding p-values. There is a clear negative correlation with the total number of users in the system. This means that the more users are entered to the system the lower the MAE becomes.

Another important aspect to consider is the number of ratings that are elicited by the considered strategies in addition to the natural acquisition of ratings. As discussed before, certain strategies can acquire more ratings by better estimating what items are likely to have been experienced by the user. Figure 14 illustrates the size of the known set K n as the strategies acquire more ratings from the simulated users. As shown in figure, although the number of ratings added naturally is by far larger than that of any strategy (more than 314,000 ratings in week 48), still the considered strategies can elicit many ratings. Popularity and log(popularity)*entropy are the strategies that add the most ratings; totaling more than 161,000 at the end of the experiment. On the other hand, voting is the strategy that elicits overall the smallest number of ratings. This can be due to the fact that sometimes most of the strategies vote for similar sets of items. Then the selected items would mostly overlap with naturally acquired ratings, which could result in fewer ratings being added to the known set. However, the remarkably good performance of voting may indicate that this strategy focuses more on informativeness of the items rather than on their ratability. In this work we have addressed the problem of selecting items to present to users for acquiring their ratings; this is also defined as the ratings elicitation problem. We have proposed and evaluated a set of ratings elicitation strategies. Some of them have been proposed in a previous work [Rashid et al. 2002] (popularity, log(popularity)*entropy, random, variance), and some, which we define as prediction-based strategies, are new: binary prediction, highest predicted, lowest predicted, and highest-lowest-predicted. Moreover, we have studied the behavior of other novel strategies and partially random-ized, which adds random ratings in the elicitation lists computed by the aforementioned strategies; voting, which requests to rate the items that are selected by the largest number of voting strategies. We have evaluated these strategies with regards to their system-wide effectiveness by implementing a simulation loop that models the day-by-day process of rating elicitation and rating database growth. We have taken into account the limited knowledge of the users, which means that the users may not be able to rate all the items that the system proposes to them to rate. During the sim-ulation we have measured several metrics at different phases of the rating database growth. The metrics include: MAE to measure the improvements in prediction accu-racy, Precision to measure the relevance of recommendations, Normalized Discounted Cumulative Gain (NDCG) to measure the quality of produced ranking, and coverage to measure the proportion of items over which the system can form predictions.
The evaluation (summarized in Table III) has shown that different strategies can improve different aspects of the recommendation quality and in different stages of the rating database development. Moreover, we have discovered that some pure strategies may incur the risk of increasing the system MAE if they keep adding only ratings with a certain value, for example, the highest ones, as for the highest-predicted strategy, an approach that is often adopted in real RSs. In addition, prediction-based strategies are not able to address either the problem of new users, nor of new items. Popularity and variance strategies are able to select items for new users, but can not select items that have no ratings.

Partially randomized strategies experience fewer problems because they elicit rat-ings for random items that had no ratings at all. In this case, the lowest-highest (highest) predicted is a good alternative if MAE (precision) is the targeted effective-ness measure. These strategies are easy to implement and as the experiments have shown can produce considerable benefits.

Moreover, our results have shown that mixing active learning strategies with natural acquisition of ratings influences the performance of the strategies. This is an impor-tant conclusion and no previous experiments have addressed and illustrated this issue. In this situation we show that the popularity and log(popularity)*entropy strategies outperform the other strategies. Our proposed voting strategy has shown good perfor-mance, that is, MAE but especially NDCG, with and without the natural acquisition.
This research identified a number of new problems that would definitely need to be studied further. First of all, it is important to note that the results presented in this work clearly depend, as in any experimental study, on the chosen simulation setup, which can only partially reflect the real evolution of a recommender system. In our work we assume that a randomly chosen set of ratings, among those that the user really gave to the system, represents the ratings known by the user, but yet unknown by the system. However, this set does not completely reflect all the user knowledge; it contains only the ratings acquired using the specific recommender system. For instance, Movielens used a combined random and popularity technique for rating elicitation. In reality, many more items are known by the user, but his ratings are not included in the dataset. This is a common problem of any offline evaluation of a recommender system, where the performance of the recommendation algorithm is estimated on a test set that is never coincident with the recommendations set. The recommendation set is composed of the items with the largest predicted ratings. But if such an item is not present in the test set, an offline evaluation will be never able to check if that prediction is correct.

Moreover, we have already observed that the performance of some strategies (e.g., random and voting) depends on the sparsity of the rating data. The Movielens data and the Netflix sample that we used still have a considerably low sparsity compared to other larger datasets. For example, if the data sparsity was higher, there would be only a very low probability for the random strategy to select an item that a user has consumed in the past and can provide a rating for. So the partially randomized strategies may perform worse in reality.

Furthermore, there remain many unexplored possibilities for sequentially applying several strategies that use different approaches depending on the state of system [Elahi 2011]. For instance, one may ask a user to rate popular items when the system does not know any user X  X  ratings yet, and use another strategy at a latter stage.
