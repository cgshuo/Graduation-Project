 concepts of ho&amp;rag and predictability. As is demonstrated laborating users incur the cost (in time and effort) of tering and nearest neighbor algorithms are frequently 1.1 Background 
Two early and well-established approaches to collab-orative filtering algorithms are those of LikeMinds [6] and Firefly [9]. It will be useful from the perspective of motivating our own work to describe these techniques briefly here. For more details see the papers themselves. that there are N collaborative filtering users and there are K items which can be rated. In general the number of items rated by user i will be much less than K. 
Define a O/l matrix M by setting M+ equal to 1 if user i has rated item j, and 0 otherwise. Define the set &amp; = (1 2 j 5 KIMi,j = l}, in other words the items rated by user i. Thus card(&amp;) is the row sum for the ith row, while the column sum for column j represents the number of users who have rated item j. 
For any matrix element Ikli,j = 1 we denote by Ti,j actual rating of item j by user i. We will assume that ratings are always integers on a scale from 1 to some fixed value V. For a so-called 7 point scale, which is commonly used, we have v = 7, and ri,j, if it is defined, will be an integer between 1 and 7. The value v = 13 is also commonly used, though there is some evidence to support the notion that a value of 7 is large enough from the perspective of a user X  X  ability to accurately differentiate ratings [9]. Odd values of v allow the user to express neutrality, while even values force the user off the fence one way or the other. item j for a specific user i (who hasn X  X  already rated that item) is computed as follows. Consider any other user k who has rated item j. LikeMinds calls user k a mentor for user i and item j, and we shall adopt this convention as well. Examine all the items 1 rated by both users i and h. LikeMinds defines a so-called closeness function C(]ri,r -rk,r]) based on the absolute value of the difference between two ratings for the item 1. Specifically, let us assume a 13 point rating scale, as is employed in [6]. Then the minimum distance between two ratings is 0 and the maximum distance is 12. LikeMinds employs the closeness function indicated in Figure 1. Now the closeness value total CVTi,k is defined as the sum over all items I rated in common of the closeness function values. Thus 
The so-called agreement scalar A.$,k is then computed as 
This can be viewed as the logarithmically scaled average value of the closeness function values. Now different mentors of user i and item j will have varying agreement scalars. (Presumably, though this is not explicitly noted in [6], only those mentors with agreement scalars greater than zero will figure into the actual rating process.) 
Two possible implementations of the algorithm are discussed. In one which we shall ignore hereafter, only the highest valued mentor k is used for predictive purposes. In the other, competing predictions from different mentors are weighted in proportion to the relative weights of their agreement scalars. The original rating Tk,j of item j by mentor k is then transformed into a rating prediction sk,j by  X  X caling X  it to fit the range of user i. We understand this to mean that if mini denotes the smallest rating given by user i and maxi denotes the largest such rating, then the rating is computed as 
So the final LikeMinds predicted rating .&amp;,j of item j for user i can be computed as larly, at least in spirit. Of course, the details are differ-ent. Specifically, they make use of a constrained Pear-son T coefficient to form a measure of closeness between two users. Consider again the rating of an item j by a user i, and let k be another user who has rated item j. 
Examining all the items 1 rated by both users i and j, and assuming a 13 point scale as above, the constrained 
Pearson r coefficient &amp;k is given by (In [9] the authors use a 7 point rather than 13 point scale, with the midpoint 4 instead of 7.) As noted there, the standard Pearson T coefficient, defined using the relevant means instead of the midpoints, is constrained to be between -1 and $1. The former implies perfect negative correlation between users i and I, while the latter implies perfect positive correlation. The authors use the constrained coefficient to mimic this while ensuring that its value increases only when both users have rated an item positively or both have rated an item negatively. They then consider all such mentors k: of user i and item j whose constrained Pearson r coefficient is greater than a predetermined threshold L, and form the weighted average of the ratings Tk,j, weighted being proportional to the coefficient. (In [9] the value L = 0.6 is used, though for a 7 point scale.) So the final Firefly predicted rating Ri,j of item j for user i can be computed as Note that the raw rating of user k is used in Firefly. There is no notion of scaling, as exists in LikeMinds. 1.2 Motivation These two collaborative filtering techniques are quite reasonable in design and have apparently been success-ful. We do believe, however, that they can be improved upon in various ways. Although details of the data structures used to implement the algorithms are not provided in [6] or [9], we certainly believe the techniques can and have been made sufficiently fast and scalable. Instead, our concerns pertain to the accuracy of the pre-dicted ratings given the inherently sparse data. Equiv-alently, we believe that these algorithms require more user ratings than absolutely necessary to make accurate predictions, thus phrasing our concerns in the dual con-text of a learning curve instead. Note that this ability to make accurate ratings predictions in the presence of limited data is by no means a trivial issue: Collabora-tive filtering algorithms are not deemed universally ac-ceptable precisely because users are not willing to invest much time or effort in rating the items. Unquestionably this issue is the Achilles heal of collaborative filtering. 
Specifically, the accuracy / data sparsity argument against the LikeMinds and Firefly algorithms is as follows. Consider again trying to find a rating of item j for user i. The ratings are calculated directly from mentors k of user i and item j. The number of users who have rated item j is small to begin with. The rating is based in both cases on some sort of closeness measure computed on the set &amp; fl Rk. But both sets &amp; and Rk are likely to be small. Each is probably not much more than some predetermined minimum threshold R, typically on the order of 10 or 20 items, which allows the user to participate in the collaborative filtering. (Such thresholds are universally employed in collaborative filtering systems -otherwise users might be tempted to reap the benefits without paying the costs.) The number of items is presumably very large. Thus the intersection Ri fl &amp; is likely to be extremely small in cardinality. Adding to this, only a fraction of the mentors will qualify for inclusion in the weighted average. So one may wind up basing the predictions on a very few mentors k, each of which has very little overlap in items rated with user i. Note, for example, that the agreement scalar defined for LikeMinds will be positive (provided the closeness value total is) when there are just 2 items in the intersection. If user k X  X  ratings of those two items happen to agree with user i X  X  ratings of those two items, the agreement scalar will equal 10 -not a terrible score in this context. The rating of item j may be based on such minimal commonality. 1.3 New Approach To counter this fundamental sparsity problem, we propose in this paper a collaborative filtering algorithm based on a new and different graph-theoretic approach. While we shall give the full details in a subsequent section, the basic idea is to form and maintain a directed graph whose nodes are the users and whose directed edges correspond to a new notion called predictability. This predictability notion is in one sense stronger than the various notions of closeness of LikeMinds and Firefly, and yet in another sense more generic (and thus more likely to occur). It is stronger in the sense that more common items are required to be rated in common by the source and target users. It is more generic in the sense that it accommodates not only pairs of users whose ratings are genuinely close, but also user pairs whose ratings are similar except that one user is more effusive with his or her ratings than the other user. It also accommodates user pairs who rate items more or less oppositely, as well as combinations of the above two scenarios. Whenever one user predicts another user, a linear transformation is constructed which translates ratings from one user to the other. The ultimate idea is that predicted rating of item j for user i can be computed as weighted averages computed via a few reasonably short directed paths joining multiple users. Each of these directed paths will connect user i at one end with another user k who has rated item j at the other end. No other users along the directed path will have rated item j. However, each directed arc in the graph will have the property that there is some real rating predictability from one user to the other. The overall rating for this path is computed by starting with the rating Tk,j and translating it via the composite of the various transformations corresponding opposite pairs will occur. Think of a politician whose introduce context into the rating process. The idea is see a recommendation for Handel X  X  Messiah than a jazz so-called creative links between item pairs which may who is examining Doyle X  X  Sherlock Holmes book may see a recommendation for the Seven Percent Solution Intelligent Recommendation Algorithm or IRA project 
We include there the related definitions of horting, and predictability, the formal rating algorithm, as well predetermined minimum number R of items. We first Query Qs: Show user i an ordered list of up to 
M (as yet unrated) items from a subset Z which are projected to be liked the most, subject to the constraint that each of them has a projected rating of at least some minimum value T. This subset can be chosen explicitly or implicitly in a variety of ways. 
For example, it may consist of a set of promotional items. Alternatively it can be defined as one or more categories in a hierarchical classification scheme, or category associated with the item or web page last our demonstration system. of at most some maximumvalue T. Surprisingly this for example, the popularity of videos such as the apply to this query as those for the most liked query like item i the most. This is effectively the inverse of the second query, and could be used by the e-commerce merchant to generate mailings (electronic It should be reasonably obvious that queries Q2 through 
Q4 can be readily answered based on repeated calls to the query Qr subroutine. 
The following additional queries can be answered based on the underlying data structures of the collab-orative filtering algorithm. All the queries below are aimed at forming chat groups and the like, and so must deal with anonymity issues. They could also each be made specific to a particular category. 
Query Qz: Show user i an ordered list of up to M other users (made suitably anonymous) with whom he or she has rated the most items in common. 
Query Qs: Show user i an ordered list of up to M other users with whom his or her ratings are most similar. l Query Qr: Show user i an ordered list of up to M All of the key queries described in this section are avail-able in our demonstration system, as is the personalized and optimized web advertisement engine. Our approach to generating good quality collaborative filtering predictions involves a pair of new concepts which we will call horting and predictability. We have already described the notion of predictability at a high level. This section contains the formal definitions and the mathematical details of the algorithm. 
We say that user ir horts user i2 provided either (1) card(&amp;, n &amp;,)/card(~,) &gt; F, where F 5 1 is some predetermined fraction, or else (2) card(&amp;, n&amp;J 2 G, where G is some predetermined threshold. Notice that horting is reflexive (by virtue of the first condition in the definition): User ir always horts user ii. But it is not symmetric: If user ir horts user i2 it does not follow that user is horts user il. (That is why we do not use the term cohorts. Horting is only symmetric for special cases, such as where the predetermined fraction 
F is 0 or 1.) Horting is also not transitive: If user il horts user iz and user is horts user is it does not follow that user ir horts user is. The intent here is that if user ii horts user is then there is enough ir X  X  perspective) to decide if user is predicts user ir in some fashion or not. Notice that the two conditions in the definition of horting can be replaced by a single condition co&amp;(&amp;, n a,) 1 min(Fcard(&amp;,), G), and this value is in turn greater than or equal the constant min(FR, G) -so we simply choose parameters such that this constant is sufficiently large. precise. For a given pair s E {-l,+l} and t E {t ratings Z X ,,t defined by Z X ,,t(r) = sr+t. (Here, t-1,0 t-l,1 = 2~ are chosen so that Z X -r,t keeps at least one value within (1 , . ..v} within that range. The value of will typically be close to the average of 2 and 2w, namely 21 + 1. Similarly, tr,o = 1 - X  X , tl,l = 2) -1 are chosen so that Tl,t keeps at least one value within (1, . ..v} within that range. The value oft will typically be close to the average of 1 -v and  X  X  -1, namely 0.) 
We say that user iz predicts user ir provided ir horts is (note the reversed order!), and provided there exist s E f-1, +lJ and t E 4-2~ + 1, . . . . 2w -1) such that where U is-some fixed threshold. The term D,,t = D(il,Ts,t(i2)) on the left represents the manhattan or 
L1 distance between the ratings of user ii and the situation that D,,t = 0 for s = 1 and t = 0, user iz behaves exactly like user il. If D,,t = 0 for s = 1 and rating unit) than user is. On the other hand, if D,,t = 0 
D s,t. (There are only 4v -2 possible pairs (s, t), so search.) The pair (s*, t*) will be called the predictor values. We will use the linear transformation Ts*,t* to i. Let Pi denote the set of users who predict user i. Thus Pi c Hi. and Qr can easily be answered via the directed graph. plexity explicitly here, but note that the time required to perform the shortest path search in step 3 is small, since the distances involved are not large and the di-rected graph is relatively sparse. (In the next section we will present data which supports these assertions.) 
As the number of users increases one can tighten the various parameters F, G and U, so that the system will be quite scalable. algorithm also contains techniques for presenting the user with those items which the system would most like to be rated. The idea is to thereby reduce the learning curve time for the collaborative filtering process. While we will not have space to go into detail here, fundamentally the approach is to partition the collection of items into a small hot set and a large cold set. (Typically the cardinality of the hot set is two orders of magnitude smaller than the cardinality of the cold set.) Items in the hot set are chosen based on their popularity, and some of these are presented to each user in order to increase commonality of rated items. 
Items in the large cold set are presented to the users in order to increase coverage of the remaining rated items. 
Partitions into more than two sets are also possible in our algorithm. We have not seen a discussion of this issue in other collaborative filtering papers, though we presume something similar is typically done. 
In this section we describe results of synthetic simu-lation experiments used to evaluate our collaborative filtering algorithm. We have done this as part of our demonstration system, and of course we are relying on these experiments until our full trial can be deployed. 
We state up front that this is not particularly fair to the other algorithms, because we are evaluating the ac-curacy of our collaborative filtering tool on data which happens to more or less fit our model. So the fact that our algorithm does best in these tests should certainly be taken with a grain of salt. However, we can proba-bly place somewhat more stock in the key derived data showing data sparsity and such. And this data is the motivating force behind our algorithm in the first place. employed in our simulations. We chose a v = 13 point rating scale. In all cases we used a total of 5,000 items, and these were partitioned into a hot set consisting of 50 commonly rated items and a cold set of 4,950 items which were infrequently rated. Again, the hot set items are hot in part because the user is  X  X ushed X  to rate these items. We assumed that each user would rate an average of 20 items, 10 from each of the hot and cold sets. Thus we adjusted the probabilities accordingly (phot = 0.2, pcord = 0.00202) and picked randomly for each user and item to determine if a rating was given. 
Next we describe the ratings themselves. The average rating m+ for each item i was chosen from a uniform distribution on the set (1, . . . . 13). Each user j was then randomly assigned an offset factor oj , chosen from a normal distribution about a mean of 0, to account for his or her level of  X  X ffusiveness X . (Positive means more effusive than average, negative means less so.) 
Finally, a small fraction of the users were chosen to be  X  X ontrarians X . In other words these users like items that other users dislike, and vice versa. Now, given that item i was determined to be rated by user j, a random rating 
R&amp;,j was chosen from a normal distribution with mean w, the offset factor oj was added, the overall rating was reversed if user j was a contrarian, and the final rating 
Ti,j was constrained to lie within 1 and 13. In formulas, this yields Ti,j = min(l3, m~~(l, RRi,j+oj)) for normal users, and Ti,j = min(l3, m~~(l, 14 -R&amp;,j -Oj)) for contrarians. first, consider Tables 1 through 3. These present, for each of the three collaborative filtering techniques discussed in this paper, results of experiments to show actual versus predicted ratings. The experiments were for 5,000 items and 10,000 users, but other similar experiments yielded comparable results. Each table is 13 by 13, with the rows indicating actual ratings and the columns indicating predicted ratings. Table 1 is for LikeMinds, Table 2 for Firefly, and 
Table 3 for our algorithm, here labeled IRA. The procedure was to choose a random pair of one item item and one user for which a rating had been given, mask that rating, perform the collaborative filtering algorithm, and finally compare the predicted and actual results. Only experiments in which the. algorithms were able to compute a rating were considered. The others were discarded in favor of a new random pair. (Table 4 h s ows the percentage of successful predictions.) Approximately 100 successful experiments were performed for each actual rating between 1 and 13, so that there were roughly 1300 experiments in all. Then the totals were normalized by dividing by the number of experiments for each actual rating. 
Turning them into percentages, each row sums to approximately 100 percent. Clearly it is  X  X oodness X  to be as close as possible to a diagonal matrix here. From that perspective, LikeMinds does quite well, with the diagonal values averaging over 61 percent. It is true, however, that LikeMinds does seem prone to making large errors on occasion, which we attribute to its inability to properly distinguish the reversal of some users relative to others. Firefly does less well, having a diagonal value average of approximately 31 percent. 
It is possible that varying some of the parameters associated with Firefly could improve this, but we were not able to achieve any better results. On the other hand, Firefly is probably less prone to having outliers, perhaps because scaling is not done. Clearly 
IRA performs best here. The average value along the diagonal is over 81 percent. and there are very few examples of truly bad performance. But again, we are assuming a model of data which precisely fits our technique in terms of effusiveness, contrarians and so on. So this level of performance is to be expected. 
The true test of our collaborative filtering algorithm will come in the user trial. 
Now consider Table 4, which shows certain key data about experiments under three different scenarios. In each case 1,000 random experiments were made, again via the technique of masking existing ratings for later comparison. In each case there were 5,000 items, as before. Scenario one had 1,000 users. Scenario two had 10,000 users, similar to the experiments of Tables 1 through 3. Scenario three had 100,000 users, which we believe is adequately large for any collaborative filtering test. 
Considering LikeMinds first, note that the number of successful runs of the algorithm went from 68 with 1,000 users, up to nearly universal success thereafter. 
The average number of mentors grows rapidly with the number of users, but only about three quarters of the mentors actually are puaZi,fying mentors. This holds, more or less, through the three scenarios. And those qualifying mentors have little intersection, which we believe is a key observation. Notice that on average only about 3 items are rated in common for the two relevant users. And the agreement scalar, as noted before, is not very high. We have also computed both the average error and the error for those items actually rated 11 and above. (As noted in [9], the latter is probably more important than the former in a recommendation system.) Both of these are relatively stable, though the error for well-liked items appears to drop with the number of users. Firefly lags somewhat behind LikeMinds in Table 4. 
The number of successful predictions grows somewhat more slowly, though eventually nearly every attempt yields success. The average number of mentors is, of course, identical to that of LikeMinds, but the average number of qualifying mentors is lower throughout. And again, the cardinality of the intersection is small, similar to that of LikeMinds. The value of ,0 is close to 1, but this is not as impressive because of the cardinality factor. The average errors shrink as the number of users grows, but the errors are fairly large. 
IRA does best of all, with a large percentage of successful predictions. The average number of directed paths grows with the number of users, but the path length does not. And here the intersection between adjacent nodes in the directed path is quite a bit higher. We should point out that we chose the minimum intersection cardinality G to be 5, 8 and 11, respectively, for the three different user size scenarios, the minimum number of ratings R to be 20, and the fraction F be 1 (causing the first constraint in the definition of horting to be irrelevant). The average error figures are best among all the algorithms considered. 
In all cases the ratings were computed with great speed. These algorithms all scale well. 
In this section we briefly list several additional features which are part of our collaborative filtering algorithm and are included in the current code. While we feel that these features are important, we do not have sufficient space to go into details in this paper. l The algorithm can prestore single or multiple rec-l The algorithm can keep track of recommendations l The algorithm can use a weighted keyword fre-larger context, our new collaborative filtering fits as one of the key engines of the Intelligent Recommendation Algorithm project under development at IBM Research. We will have more to say about the various components of IRA in the future. [31 [71 In this introductory paper we have described a new type of collaborative filtering algorithm, based on twin new notions of horting and predictability. The algorithm described performs quite well on artificial data, and will be tested in a real user trial in the near future. Put in a 
