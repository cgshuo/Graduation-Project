 Keyphrases play a key role in text indexing, summariza-tion, and categorization. However, most of the existing keyphrase extraction approaches require human-labeled training sets. In this paper, we propose an automatic key-phrase extraction algorithm using two novel feature weights, which can be used in both supervised and unsu-pervised tasks. This algorithm treats each document as a semantic network that holds both syntactic and statistical information. Structural dynamics of these networks can easily identify key nodes, which can be used to extract keyphrases unsupervisedly. Experiments demonstrate the proposed keyphrase extraction algorithm averagely im-proves 50% in effectiveness and 30% in efficiency in un-supervised tasks and performs comparatively with super-vised extractors. Moreover, by applying this algorithm to supervised task, we develop a more accurate classifier. In this classifier, we assemble several syntactic and statisti-cal features. Experiments show that the overall precision of supervised extraction can be up to 80%.
As a short list of topical phrases or words, keyphrases briefly describe the contents of a document. They are widely used in text indexing, summarization, and catego-rization. Faced to roughly 200,000 English digital books in our digital library project 1 , we are motivated to summa-rize documents and measure text similarity. But topical terms in a book X  X  metadata are too few to fulfill our task; instead, we plan to use extracted keyphrases. Therefore, keyphrase extraction is of interest. 
Most existing extraction algorithms [1, 18] are based on supervised learning for papers or web pages. When applied to digital books, they have several drawbacks. First, a book has longer text but less structural tags, mak-ing the extraction more complicated. Second, it is labori-ous to set up an appropriate training set with sufficient samples. Unsupervised learning is thus more preferable. Last but most important, most prevailing feature weights [15, 16] are not satisfactory for unsupervised keyphrase extraction. A feature weight (also called feature metric or term weighting scheme in text mining) measures the fea-ture value of a term in the source document, which helps determine whether the term is a keyphrase or not. Most prevailing feature weights solely rely on frequency-based [22] or specific structural information [17]. They are ei-ther too inaccurate or too hard to generalize. As a result, the accuracy of unsupervised learning using a single weight is unsatisfactory. Additionally, among these fea-ture weights, some are set-dependent [22] (set-dependent means that if a new document is added to the data set, the set-dependent feature weights of all the existing docu-ments needs rescoring). 
In this paper, we propose a keyphrase extraction method, using several novel set-independent feature weights, which can be used in both supervised and unsu-pervised tasks. This algorithm treats each document as a semantic network that holds syntactic relation in edges and frequency information in nodes. By analyzing the structural variables of these networks, we notice the exis-tence of Small-World Phenomenon (SWP) [11]  X  the net-works are clustered, comp act and connected. There are key nodes that play important roles in the structure of these networks, making them compact. We use several structural variables of SWP as feature weights to find out these key nodes. Given that the structure of the network represents the structure of the source document, and the edges represent syntactic relation, we select some key nodes as keyphrases. In the process of calculating these variables, no training set is required. Experiments demon-strate the proposed keyphrase extraction algorithm aver-agely improves 50% in effectiveness (with up to 1/4 rank and two times of weight) and 30% in efficiency in unsu-pervised tasks and performs comparatively with super-vised extractors. Moreover, we apply this algorithm to a supervised task, aiming at building a more accurate clas-sifier that can dynamically determine the number of the keyphrase. Experiments show that the overall accuracy of this supervised extraction method can be up to 80%. 
Section 2 surveys recent literatures on keyphrase ex-traction and semantic networks structure analysis. Section 3 outlines our knowledge organization system. Section 4 presents our unsupervised keyphrase extraction algorithm, and its application to build up a more accurate classifier for keyphrase extraction by assembling several features. Section 5 describes experiments on the effectiveness and efficiency of this keyphrase extraction method. Section 6 concludes this paper. 
In text mining, keyword acquisition falls into two ways: keyword assignment and keyword extraction. They differentiate in the source of the keywords: keyword as-signment acquires keywords from predefined corpus or thesaurus, and keyword extraction from the source docu-ment. Keyphrase extraction extends the acquisition from words to phrases. Phrase identification is a main issue during this extension. Some hypotheses are set up to se-lect phrases as candidate keyphrases, or all word se-quences are candidate keyphrases. 
Keyphrase extraction can be viewed as a supervised or unsupervised learning task. Though lots of literatures about keyphrases have been published, most of them cover supervised tasks. Viewing keyphrase extraction as a supervised learning task, researchers develop two meth-ods. Intuitively, it is a two-category classification task --a term in the document is a keyphrase or not. Based on some features, researchers build up classifiers to extract keyphrases. In a pioneer paper [1], Turney first treat it as a supervised learning task, by using frequency-based and part-of-speech information as features, along with deci-sion tree and an generic algorithm (called Genex) as clas-sifiers. The system is named Extractor. KEA [18] per-forms in a similar way with Native Bayes as the classifier. On the other hand, researchers in natural language proc-essing build up language models from a training set and select phrases whose feature weights are in accord with the distribution of keyphrases [24]. As an unsupervised learning task, it can be fulfilled by the means of ranking: set up some feature weights, rank the phrases under these metrics, and select phrases with top weights. PhraseR-ate[14] shares a similar goal with us to set up a feature weight to extract keyphrase without training, but it de-pends on HTML tags. 
Feature weights or feature metrics are essential in the preprocessing of data mining and pattern recognition, for extracting representative features of the objects. A num-ber of feature weights are developed in text mining. They can be divided into two categories: weights that can be used only in supervised learning and weights that can be used in both supervised and unsupervised learning. The first class of feature weights rely on priors or statistics acquired in the training process, such as Accuracy, F1-Measure, Information Gain, Mutual Information, Term Strength, Chi-Square, and Odds Ratio [15, 16]. 
The second class of features can be extracted without training. Most of them rely on structural rules or fre-quency-based information. Firstly, some weights set up rules on specific structure information [17], such as tags or positions, but they have limited capacity to generalize. Methods based on frequency information can be Term Frequency (TF), TFIDF [18], or more delicately Okapi X  X  BM25 [22]. Though they are widely used in information retrieval, there are several drawbacks. First, they are not accurate enough for unsupervised keyphrase extraction. This is the reason why KEA and Extractor do not extract keyphrases solely on TFIDF or TF. Second, ordinal in-formation and dependency between phrases are lost. Fi-nally, some of them need parameter tuning (as in BM25). To overcome these problems, some authors analyze the order or co-occurrence of words within a window in a fixed or dynamic size. Mihalcea and Tarau [23] use a graph to store the co-occurrence in a window of N words, view the relation as recommending, and rank the words in a PageRank-like manner. We store ordinal information in a similar way, but view a relation as a path for informa-tion flow. 
In a word, our main contributions are: (1) proposing a fast and effective phrase identification algorithm; (2) us-ing structural variables of Small-World Phenomenon (SWP) as feature weights, which make the result of unsu-pervised keyphrase extraction satisfactory. 
Currently, a body of literatures is on the study of struc-tural analysis on kinds of complex networks. Researchers have developed a system of theory on how to analyze the global structure of complex networks [11]. Network structure analysis is applied to kinds of semantic network structure analysis in natural language processing, includ-ing some for lexical pattern analysis [5], ontology of lan-guage, and language evolvement [6, 13], such as Associa-tive Network, WordNet. 
In network structure analysis, SWP is recognized as a key property of networks with a large number of nodes. Watts and Strogatz [8] present two important variables and the way to understand the collective dynamics of SWP. SWP provides several feature weights for structural analysis to find key nodes. 
As far as we know, we are the first to use collective dynamics to extract keyphrases in English documents. We define three variables to measure a term X  X  impact on the global structure of the semantic network in three aspects: connectedness, compactness, and the combination of con-nectedness and compactness, called Connectedness Cen-trality , Betweenness Centrality , and Relation Centrality respectively. Other authors use traffic [10], clustering coefficient [3], and betweenness for structure analysis. Among them, two most related works are: Zhu et al [4] use in a relatively small dataset without theoretical reasoning (we have a different definition of [21] use a similar approach with us (they call it informa-tion centrality), to analyze edges rather than nodes to find community structures. However, most of these literatures don X  X  mention many practical issues, such as unconnect-edness and computational complexity. 
Given a document, the performance of extraction de-pends on the richness of reserved information. A knowl-edge organization system (KOS) is needed to store infor-mation. Because of its power to hold different kinds of relationship between lexical units, we choose semantic networks as our KOS. Rather than simply as isolated points or linear sequence, we view the original text as a semantic network  X  a term (a word or a phrase) as a node and a relation between two terms as an edge. Frequency information is stored within nodes, and other ordinal or dependency information is stored in the structure of the network, which we will utilize in the extraction algorithm in the form of feature weights.

A phrase is a consecutive sequence of words without intervening punctuations, forming a grammatical con-stituent of a sentence. In the process of establishing nodes, we need to identify phrases among meaningless word sequence, and add the candidate phrases and the remain-quence, and add the candidate phrases and the remaining single words into the network. To identify phrases, an intuitive way is to combine the candidate set of all possi-ble word sequences with the word set, score them, and then select kephrases from them. Unluckily, this approach introduces compositional explosion. Hence, in KEA and Extractor, hypotheses are set up before scoring to filter meaningless word sequence. Such hypotheses are  X  X either stopwords (letters, conjunctions, articles, particles, prepo-sitions, pronouns, anomalous verbs, adjectives, and ad-verbs. We use a similar stopword list as in KEA) nor named entities are in between X  and  X  X  phrase has a maxi-mum length (usually three) X . But these hypotheses are so weak that most of the phrases in the candidate set are still semantically meaningless. Compositional explosion is still a haunted threat. Moreover, in semantic networks, structural variables are very sensitive to the changing of the structure. The failure of phrase identification leads to error values of these variable, and finally low precision of keyphrases extraction. 
After investigating the characteristics of topical terms in our eBook archive, we found a two-phase filtering al-gorithm more effective. On the first setp, three rules must be fulfilled; otherwise, the word sequence is filtered: 1. A candidate phrases can X  X  start or end up with stop-words. 2. In a candidate phrase, between two non-stopwords, there can be no stopwords except MidStopwords (proposition, nouns, and numbers in stopword list), and the number of MidStopwords should be less than four. Phrases as  X  X heat and rice X  are included, while they are not in Extractor. 3. The phrase frequency (PF) of a candidate phrase should be relatively high in the text. Second, we divide the remaining ones into groups. Candidate phrases are first divided into societies by the number of words they have, and then phrases of a society with the same word are further assigned to the same group. A candidate will be in n groups if it has n distinct non-stopwords. Every group has only one or none winner. A winning keyphrase candidate (we call it a giant phrase ) should have a top PPF (Percent of PF) and a PTF (Percent of TF) above a threshold in all n groups it belongs to. Only winners can remain in the candidate set of key-phrases. Take phrase i in group k for example ( phrase sists of word k ), where ) , ( quency of phrase i among all phrases in group k of phrase i versus the frequency of word k . Note that PF is the frequency of a phrase consist of more than one word, TF is the frequency of a single word. Since many word sequences have been filtered in the first phase, PPF in the remaining candidate set is usually much greater than PTF. 
Caropreso et al [20] report that duplicated information among uni-gram and bi-gram is detrimental to effective-ness. Therefore, we replace a single word with its giant phrase if it is available, on the assumption that the giant phrase is a proper substitute for its words in the context. 
As for the phrase length, Mladenic and Grobelnik [19] find that word sequences of length up to three improve the performance of feature weight. Extractor and KEA also support keyphrases consisting of less than four words. Observing 85% topical terms of eBooks have only key-phrases with less than two words (in fact most of the re-mainder 15% are made up by two keyphrases), we now support keyphrases with two words, and this algorithm is extensive to a longer length. 
Relationship stores the majority of information in a semantic network, which distinguish it from other types of KOSs. There are mainly three kinds of relationship between words, semantic relation, syntactic relation, and co-occurrence. 
Semantic relation can only be established when a dic-tionary or ontology as WordNet is available, and disam-biguation is needed. Syntactic relation builds networks on the grounds that words at a certain distance have syntactic or semantic relationship [5, 6]. It needs a supervised learning process for Part-of-speech (POS) tagging. Co-occurrence presumes that co-occurrence in some linguis-tic units (chapter, discourse, paragraph, and sentence) is syntactically or semantically indicative [4, 7]. Since co-occurrence is an approximate way to identify relationship, it is prone to introduce redundancy and false relation. Moreover, networks linked up by co-occurrence have the most edges among these three relations, leading to a high-est computational complexity. 
We aim to combine the last two kinds of relation, without a training set for POS and a relative low compu-tational complexity. Lyon et al [6] study that 70% of syn-tactic dependencies are between neighboring words, and 17% at a distance of 2. Furthermore, Cancho et al [5] con-clude that all syntactical relation within distance 2. Inter-estingly, we find that after filtering stopwords, nearly all syntactical relation at a distance of 2 is shortened to 1. Because of this, we consider neighboring relationship in the same sentence as the relationship between nodes. We carry out an experiment to validate the performance of neighboring relation (see Section 5.4) in several datasets. The result is that its performance is comparative to other relation with a larger co-occurrence window. Also, its computational and structural complexity is lowest. 
This neighboring relationship holds two properties: unweighted, and undirected. There are mainly two ways to calculate the weight of an edge. First, use POS tags to distinguish kinds of syntactic relation, and assign differ-ent weights to them. But training sets with syntactic labels are needed. Alternatively, given a training set, n-gram can calculate probabilistic distribution of the co-occurrence between words within a distance n, which can be natu-rally used as weights. One problem of weighted graph is that the computation of structural variables is more time-consuming. Therefore, currently we support unweighted graph. Since neighboring relation does not distinguish different kinds of syntactic relation, the graph is undi-rected. For example,  X  X at fish X  is a verb-object relation, and  X  X ish eat X  is a subject-verb relation, they are two kinds of syntactic relation, but in the neighboring relation, the only concern is the existence of syntactic relation, rather than what kind of syntactic relation they are. Therefore, they are the same in this graph. 
With semantic networks model and characteristics of candidate keyphrases defined, we now move to extract keyphrases by analyzing the structure of these networks without training. Granted that the structure of a network represents the structure of its source document and the edges represent syntactic relation, to extract a keyphrase important node in the structure of the network. 
Unsupervised keyphrase extraction algorithm takes two steps. First, it builds an accurate feature weight that can depict the importance of a node in the network with-out training. Second, by ranking phrases, it selects top n phrases as keyphrases. The n can be decided by the de-mand of the extraction. 
To kind key nodes in a semantic network, we must first decide what characteristic of network structure we will concentrate on. 
In traditional graph theory, connectedness is an impor-tant issue to study the structure of networks. Connected-ness measures how connected a graph is: whether a graph is connected or disconnected into components; whether a connected graph will turn into unconnected after remov-ing a node, and if it will, how unconnected it will be. Nodes that link different components are recognized as key nodes, named cut points . Alternatively, nodes with highest input and/or output degrees might be of interest. 
In complex network analysis, Small-World Phenome-non (SWP) is viewed as an important property of network structure. Though a network has a large amount of nodes, it still has small average minimum path length L , and high clustering coefficient C [8]. This is called SWP, also known as the Six Degree Separation Rule in Sociology, which is found to be a common phenomenon in complex networks. Characteristic Path Length (average shortest path length) [8] of node v , denoted as L(v) , demonstrates the average length of all shortest paths started from node v , and L is the average of L(v) among all starting nodes in the network. They are defined as: where ) , ( the network. Clustering coefficient [8] of node v , denoted as C(v) , depicts how fully connected node v and all its neighbors behave, and C of the network averages C(v) in the scope of all nodes. They are defined as bellow, where 
R is the number of pair-bonding links among i v and its l neighbors, 
Since there are thousands or ev en millions of words in one book, the structure of the corresponding network is rather complex with up to 60,000 nodes and 400,000 edges. Concluded from Table 1, SWP happens in our semantic networks. 
As mentioned above, two main concerns of networks with SWP are clusteredness and compactness, which are also two characteristics of SWP. Clusteredness (Newman [11] calls it Transitivity ) measures the cliquishness of a typical neighbor. Compactness measures the degrees of separation between every two nodes. It shares some simi-larity with radius of a network in the traditional graph theory, but with much more structural information. Note that C and L are not the only metrics for clusteredness and compactness respectively. 
Properties can be divided into two kinds: one is based on local information (clusteredness), and the other on global structure (connectedness and compactness). A node can learn its local properties with the knowledge of its neighboring structure, while global properties with the knowledge of the status of the entire network. For in-stance, variables for local properties are degree, TF, PF, clustering coefficient and the like. Since the extraction process is to find a keyphrase to summarize the whole text, we choose global properties such as connectedness and compactness to weight terms. 
To analyze connectedness and compactness mentioned above, there are several variables to symbolize the impor-tance of a node. These variables can be used as feature weights in unsupervised keyphrase extraction. Take syntactic relation as a kind of information flow. One node connects other nodes through this kind of flow. The cause of SWP lies in the existence of some in-formation hub, which keeps the network connected and compact. The possibility of the existence of certain topics increases when the network goes compacter. Therefore, if a keyphrase exists, it should be a hub that tightens the network. 
We use "betweenness centrality" and "relation central-ity", two global variables to capture the centrality of a term in the context and the role it plays in the compact-ness of the network. Keyphrases usually have high cen-tralities. All candidate feature weights are defined below. 
Connectedness Centrality H(v) . H(v) captures the im-portance of a node in the connectedness of the network. Two nodes are unconnected if there is no path connects them. H(v) summarizes pairs of nodes that become un-connected if node v is deleted from the graph. Betweenness Centrality B(v) [11] and Traffic T(v). These two variables capture the role a node plays in the compactness of the network. T(v) [10] sums the number of trajectories passing node v , identifying whether it is a hub, while B(v) sums the number of shortest paths that node v is in between. Since shortest paths contribute more to the compactness of networks than usual paths, we pre-fer B(v) to T(v). B(v) is defined as below, where and t , and ) ( v node v . 
Moreover, we want to set up a composite feature weight that measures the importance of a node in both connectedness and compactness. A way to fulfill this task is to use variables to measure the structural change of the network in a dynamic behavior. Watts and Strogatz [8] define a key node as a shortcut in a Growth Model (a net-work grows from a node to a graph): shortcuts are nodes that decrease L drastically when they are added into the network. Because we care the status quo more than the evolving process of the network, we view them in a Deca-dence Model: if these key nodes are removed, L soars, or even the network collapses. In consideration of uncon-nectedness of our semantic network, we define L as be-low (a variation from [11]): where E is called Efficiency [9], which measures how effi-ciently information flows over a network. The longer L is, the lower E becomes. E(v) and E are defined as: 
Note that self-distances are excluded in formula (4) to avoid infinity (while in [11] they are included). In essence, experiment results, this definition unexpectedly outper-forms the algebraic mean one of L . Moreover, the origi-solves it by taking the infinite distance as zero in har-monic average. Thus, we define the increment of L as v ' L  X  tion keeps a network connected, and this variable can capture the role that a term plays in both compactness and connectedness of the network, we define it the feature weight Relation Centrality S(v) of node v as 
S(v) and B(v) are intrinsically related. S(v) dynamically measures the contribution a node makes to the compact-ness and connectedness of the network, and B(v) statically counts how many routes B(v) shortens. Either of them can independently distinguish keyphrases without training. Therefore, in the rest of unsupervised keyphrase extrac-tion, we will concentrate on these two feature weights. 
Our preliminary experiments in the effectiveness of these variables prove three facts. First, global variables perform far better than local variables. Second, key-phrases are unessential to act importantly in connected-ness (connectedness centrality acts the worst among three centralities) Three, B(v) and S(v) act comparatively. Therefore, we provide two ways to extract keyphrases, one by ranking B(v) , the other by ranking S(v) . 
To implement the algorithm above, there are three main challenges. First, since n is very large, the computa-tional complexity of calculating L and B is quite high. In graph theory, the most famous and efficient algorithm to summarize lengths of all shortest paths is Floyd Algo-rithm, which takes O(n 3 ). It will be an intolerable experi-ence for one to extract metadata from a book when n comes to millions. Space complexity should be taken in consideration, too. Finally, unconnectedness is an easily ignored issue in complex network analysis, confining the use of L . 
Preprocessing. The algorithm starts in splitting context into word seuqences, identify candidate phrases. Before adding words into semantic network, we omit the words in stopword list, and substitute words with phrases that consist of them. Caldeira et al [7] have proved that this filtering treatment does not modify general behaviors of the network. Since most source books are in English, we use PorterStermmer [12] to map words to their stems.
Building the graph. We store nodes and edges in adja-cent list, to reduce computational complexity of space from O(n 2 ) to O(m+n). The possibility of memory over-flow plummets. We initialize each node as a connected component. When an edge is introduced between compo-nents, the smaller component is merged into the bigger one. Then to avoid unconnectedness, we carry out prun-ing. We observe that in our data, most networks have a main connected component with more than 95% nodes of it (as in Table 1). Strogatz [2] reports it as a common phenomenon. Steyvers and Tenenbaum [13] have the same discovery in WordNet and Roget X  X  thesaurus, up to even 99% of 20,000 and 29,000 nodes, and conclude this as a main property of SWP. Note that in statistics in Table 1, only 11.5% has a CC more than 1, and 96.9% among which have m/n less than 2 --they are approximately lin-ear graghs. Henceforth, most of the semantic networks can be pruned into a main branch. As a result, we prune branches with less than 5% nodes. 
Scoring. To overcome the time complexity, it is an in-tuitive way to delete some nodes before summarization of weights, but it is not a reasonable method since the dele-tion drastically changes the value of L and B . However, the semantic networks are sparse ( m&lt;&lt;n 2 ), unweighted, and undirected. Calculating L and B can be only a task of breath-first searching through an adjacent list. By using this method, we reduce the computational complexity of calculating L and B greatly. For B , only O(n+m) space and O(mn) time is required. For L , we store and reuse results of previous iteration, disregarding route informa-tion. The best time cost will be O( for calculating the distance of one path. Algorithm 1: Summarize L of semantic network 1: L[v][w]  X  0 2: for v  X  V do 3: visited[u]  X  0 , u  X  V ; 4: level[u]  X  0 , u  X  V ; 5: Q  X  empty queue ; enqueue v  X  Q ; 6: while Q not empty do 7: dequeue t  X  Q ; 8: if L[v][k]&lt;= level[t] k  X  V then break ; end if 9: foreach neighbor s of t and visited[s]=0 do 10: visited[s]  X  1 ; 11: level[s]  X  level[t]+1 ; 12: if s&lt;v then 13: for w  X  V and L[v][w]&gt;L[s][w]+level[s] do 14: L[v][w]  X  L[s][w]+level[s] ; 15: end for 16: else if s&gt;v and L[v][w]&gt;level[s] 17: enqueue w  X  Q ; 18: end if 19: end foreach 20: end while 21: end for
Unstemming. Unstemming is a process that adds the suffix back to stemmed keyphrases, making them under-standable. However, unstemming is an ill-posed problem (one-to-many mapping). Currently our unstemming algo-rithm is: search phrases with the stem, pick up the phrase with highest PF, and partially stem its suffixes (plural form of nouns and  X  X ng, -ed tense forms of verbs). 
Though the unsupervised keyphrase extraction above performs comparatively with exsisting supervised key-phrase extraction system (shown in Section 5), it still shares two drawbacks with KEA and Extractor. First, the precision is not high enough, usually below 50%. Second, the number of keyphrase is fixed. This might fulfill the tasks that the user intends to decide the number of re-turned keyphrases himself, but it fails when the number is unclear. Towards these ends, we apply our unsupervised keyphrase extraction algorithm to a supervised task, aim-ing at building a more accurate classifier that can dynami-cally determine the size of the returned keyphrase set. 
Viewing it as a supervised learning task, we treat it as a two-category classification task --a term in the docu-ment is a keyphrase (positive instance) or not (negative instance). To assign instances, we need a set of features to classify terms. A feature is a kind of property that is help-ful for determining the case (positive or negative) of an instance. Based on these features, we build up a classifier to extract keyphrases. Support Vector Machine (SVM) is a powerful method to build up accurate classifiers for data classification. It provides a model to predict target cate-gory or probability of instances. In fact, the model is an accurate hypersphere in the data space (a feature as a di-mension) to classify the instances. 
Feature selection. To assemble more information as, we choose both statistical and syntactic features from both local and global properties. For local properties, de-gree, PF or TF, and cluster coefficient C(v) (defined in formula (2)) are included. For global information, aver-aged shortest path length L(v) (defined in(4)), connected centrality H(v) , betweenness centrality B(v) (in (3)), rela-tion centrality S(v) (in (5)) are all included. Moreover, we normalize these features with the normalization factors defined in the table below. For example, H(v) is defined for pairs of nodes in the graph, and the maximum number of pairs is square of the number of nodes. Therefore, we use it as the normalization factor. Note that no normaliza-tion is used in unsupervised task, because a feature weight is used for ranking in only one source document, and these factors have no impact on this ranking. Variable Normalization factor Degree Number of nodes in the graph. 
In the experiment of comparison between extraction with original variables and extraction with normalized ones, among RBF, RBF with probability, and sigmoid kernels, the latter one has a higher accuracy in both nega-tive instances and the entire test set, but a lower in posi-tive instances, but in linear kernel, the result is reverse. 
Learning an SVM classifier. Given the features se-lected above, we view each instance a vector in the sam-ple space, where a dimension is a feature, valued by the corresponding feature weight. To facilitate the learning of a classifier, we use LIBSVM [25], an influential code library of SVM. The learning proceeds as below: select training set and testing set, preprocess the data, choose a class of classifier, tune the parameters, train the model, and test it. 
The traditional data selection approach is to select data from the set randomly, ignoring what category it is. How-ever, in keyphrase extraction, numbers of different cate-gories are highly unbalanced. Positive instances constitute a very small part in the sample space (the percentage is 0.2% to 2.4% in Turney X  X  dataset [1] and far below 0.1% in our eBook archive). In the traditional sampling ap-proach, positive instances have minute possibility to be chosen into the training set. To overcome this problem, we choose half of the instances in the training set from positive instances and another half from negative in-stances, and each instance is selected randomly from the instances of the same category. As for preprocessing, we use the L-1 norm scaling schemes as LIBSVM provides. Note that training set and testing set should be scaled in a similar way. We select C-SVC [25] as the type of SVM, linear kernel, RBF kernel, and sigmoid kernel as candi-date kernel. Grid search algorithm and five-fold cross validation is used in parameter tuning. 
We randomly select 1212 books from our archive as the source of training set, including 2379 positive in-stances and 106115 negative instances. Note that only topical terms excluding subfields are selected as positive instances. For example, if a LOC-assigned subject is  X  X rusts Industrial (United States) X , and  X  X nited States X  is a geographical subfield of topical term  X  X rust Industrial X , we only select  X  X rust Industrial X  as a positive instance. In data selection, we select 1000 positive instances and 1000 negative instances as the training set from this data set. The comparison experiment is detailed in Section 5.5. 
We carry out three experiments on the effectiveness and efficiency of our keyphrase extraction system, de-noted SW . For unsupervised tasks, it provides two results, one is from the ranking of relation centrality S(v) and the other from betweenness centrality B(v). Moreover, we detail two experiments for relationship establishment and the effectiveness of supervised keyphrase extraction men-tioned in Section 3 and Section 4. 
In this experiment, we choose two books from our ar-chive, use S(v) and B(v) individually as the feature weight, rank candidate keyphrases, and select top 7 of the re-turned list (we choose 7 because of the limitation of pages here). Moreover, we use Copernic Summarizer TM as the baseline to make comparison, since it is a leading key-phrase extractor without training (actually, it may be trained before it is published). The former influential keyphrase extractor system Extractor [1] has been inte-grated into it. 
Table 2 below is the result. The title of book B1 is  X  X rice responsiveness of world grain market X  , and book B2 is  X  X ore milk for more children X  . From the table, we can see that from semantic point of view, most topical terms are included. S(v) and B(v) slightly performs better in two aspects. First, fewer unrelated keyphrases are ex-tracted, such as  X  X int X  in B2. Second, candidate key-phrase identification promotes several useful keyphrases such as  X  X gricultural Economic s X  and  X  X heat and rice X  in B1. But two problems remain in all these systems. First, simple words can be easily extracted as in B2, but the phrase is hard to get. Second, synonym mapping is needed between  X  X rain X  and  X  X heat and rice X  in B1. 
In this experiment, we compare S(v) and B(v) with two prevailing features: TFIDF, widely used in document relevance analysis and keyphrase extraction, such as KEA; BM25, the most prevailing feature used in text indexing and ranking scheme in Information Retrieval. These two features are defined as respectively, where TF stands for term frequency of term t in document d , DF for the number of documents that consist of this term, | | d for the number of documents in the dataset, dl for the number of terms in the document, avgdl for the averaged dl in the dataset, and b and k two parameter need tuning. The first formula is the tradi-tional unnormailized TFIDF and the second one is de-fined as in Roberson X  X  paper [22]. Here, we select b and k to be 0.75 and 2 empirically as in most application in TREC . TFIDF/BM25 denotes their common performance. 
Data Generation. DUC is well-known to test the per-formance on automatic summarization. We choose all 1600 documents of DUC2005 as a test set. Most of these web pages are from FT and LATimes . They are labeled into 50 categories with topic terms by experts. Therefore, we use these topic terms as the target output and docu-ments as input to extract keyphrases. All the extraction algorithms use he same candidate keyphrase identification method and no TF filtering on words. 
Evaluation Measure. To evaluate the improvement, we retort to five measures: the ranking of the query word in the feature set, the normalized weight of the topical term, the size of this feature set, time duration, and miss rate of target keyphrases. Though precision and recall are widely used in pattern recognition, they can X  X  measure precisely the ranking and weight difference between feature metrics. To compare the weights between different metrics, we use the L-2 norm as the normalization factor for all four features, supposing ) f(v term document d , Results. Running on DELL workstation with Intel Xeon 3.0G CPU and 1.0G memory, TFIDF/BM25 fin-ishes in 96 X 53 X  X  and SW in 72 X 21 X  X , about 2/3 of the pre-vious one. SW filters nodes with only one degree in the returned set, since outskirts of the network won X  X  have a size of candidate phrase set of TFIDF/BM25, at a cost of an )% 3 . 4 (6.7  X  increase in miss rate. Other results are shown in the two figures bellow. We set up baselines in the graph to clarify the differences of performance. If all sample points are on the baseline, it means that the system of x-axis and that of y-axis have the same performance. Seen from the graphs below, in (a) and (c), all sample points are above the baseline, indicating that all ranks of SW are smaller than TFIDF and BM25, and in (b) and (d), most of the sample points are below the baseline, indicat-ing that most weights of SW are bigger than the other two. As a summarization of the samples, the result of linear regression functions shows that the performance of S(v) and B(v) are close, while they both outperforms TFIDF/BM25 with relatively higher ranks (smaller value, less than a half) and bigger weights (nearly 30% promo-tion from TFIDF and 80% from BM25). 
This experiment aims at the comparison between SW ( S(v) and B(v) ) and other extraction systems, no matter supervised or unsupervised. Since the source code or ser-vice of PhraseRate is unavailable, we choose two influen-tial systems, KEA and Summarizer TM mentioned in Sec-tion 2 as baselines. The keyphrases assigned by authors or librarians are also included as an optimal baseline, de-noted as Author . be datasets, web pages, journal papers, and some eBook from our archive. The first two kinds of datasets include FIPS, Aliweb, NASA , and Journals are all the same as in Turney X  X  paper [1], each consists of context files and keys assigned by author. And we train KEA with the same training set (55 documents of Journal ) as in [1]. The eBook dataset includes 101 English books, randomly cho-sen from all kinds of fields, with 4 . 4118 0 . 2528 1  X  words. The number of keyphrases that appears in the document is in Experiment 1. 90.26% of these keyphrases exist in the text. We regroup these five datasets into three for the length of the documents: Aliweb and NASA are grouped into Short , FIPS and Journal into Mid , and eBook re-mains. 
Evaluation Measure. To make the result more easily to compare with previous work of KEA and Extractor, we use the similar measures, the average precision within top n keyphrases in a dataset, denoted P@n, taking account of accuracy and a rough measure on ranking. If n is far big-ger than the number of assigned keyphrases, the precision will be very low that makes no significance. So we use n less than 15. 
Results. Without training, S(v) and B(v) still perform comparatively as supervised extractors. Moreover, in eBook and Mid , they slightly outperform Summarizer on most of the time, and they even perform best in the first five keyphrases. This result validates our assumption that dependency between neighboring phrases helps extract keyphrases. On the other hand, they perform relatively poor in Short . The reason is that the relation in the docu-ment is so sterile in short documents that most nodes with very low PF and degree make no differences in a seman-tic network, while in TFIDF may do (KEA uses normal-ized TFIDF as its only feature and Summarizer use nor-malized PF as a feature). Note that in Mid , KEA outper-forms other systems when the number of keyphrases is larger than 5. The reason might be that it is trained on this dataset, but its performance on the first 5 keyphrases is worst. Moreover, we can witness a big precision margin between state-of-art keyphrase extraction system and op-timal baseline Author. 
We carry out this experiment to test the performance of extraction with different kinds of definition of relation-ship in the semantic network. Relationships between terms in this comparison verify in the size of co-occurrence window: neighboring (size is 2, denoted Bi ), Tri (size is 3), Quad (size is 4), and co-occurrence ( Occ ) in the same sentence. We use the same dataset and evaluation measure as in Section 5.4. Terms are scored by S(v) and B(v) . 
Seen from the results of the experiment, these four kinds of relationship perform roughly the same. However, Bi has the lowest computational complexity, while the counting iterations of other relation are several times of Bi : Tri 2 times, Quad 3 times, Co-occurrence 4-10 times. 
We carry out this experiment to test the performance of extraction with normalized or unnormalized feature set, and with different kernels functions in SVM. The families of kernels we choose include: linear function, RBF func-tion, sigmoid function, and their functions with probabil-ity feedback (denoted L-Prob, R-Prob, and S-Prob). The test set is 10555 instance vectors (191 positive instances and 10364 positive ones) extracted from the eBook data-set mentioned before. Since the data is highly unbalanced, we test the accuracy in the set of positive instances (de-noted Pos), the set of negative instances (denoted Neg), and the entire test set. 
The result of this experiment is shown above. For most of the kernels, normalization brings a higher accuracy in both negative instances and the entire test set, but a lower in positive instances. For linear kernel, the result is re-verse. Linear, RBF, and R-Prob performs comparatively though linear has a better accuracy on Neg and All, and RBF/R-Prob has a better accuracy on Pos. Sigmoid has a very low accuracy on Pos (in fact, this is recall), but a very high precision (since Neg outnumbers Pos, accuracy of Neg is partially the same with precision). 
For efficiency, the training time depends on the sample size and the kernel function, and it has no significant rela-tion with the range of features. Sigmoid takes the least training time, followed by RBF and linear. Feedback of probability requires a different approach of learning [25], which drastically decrease efficiency.
Keyphrase extraction is a powerful tool for text sum-marization and similarity analysis. It is traditionally solved by supervised learning. Due to lack of fast, accu-rate feature weights that are also easy to generalize, exist-ing unsupervised learning approach seems to be unpracti-cal. We propose a keyphrase extraction algorithm, using two feature weights. Each of them can extracts key-phrases outperforms traditional feature weights both in effectiveness and efficiency. Moreover, we apply this algorithm to a supervised task, aiming at building a more accurate classifier that can dynamically determine the size of the returned keyphrase set. Experiments show that the overall accuracy of this supervised extraction method can be up to 80%.

This work is supported partially by the 863 Interna-tional Cooperation Project of Technology Ministry of China (No. 2003AA119010), and China-US Digital Academy Library (CADAL) Project (No. CADAL2004002). 
We are thankful to Mr. Peter D. Turney for generously sharing his datasets, Prof. Jinhu Lu for his insightful sug-gestions on the theory of Small-world Model, and several researchers of MSRA for their comments. We thank Yuanning Li and Fei Yang for helpful discussions. [1] Turney, P.D. Learning Algorithms for Keyphrase Ex-traction. Information Retrieval , 2, pp. 303-336 (2000). [2] Strogatz. S.H. Exploring Complex Networks. Nature 410, pp. 268 X 276 (2001). [3] Dorogovtscv, S.N. and Mcndes, J.F.F. Exactly solv-able analogy of small-world networks. Euro phys.Lett. 50, pp. 1 (2000). [4] Zhu, M., Cai, Z., and Cai, Q. Automatic Keywords Extraction Of Chinese Document Using Small World Structure. In Proc. of IEEE ICNLPKE X 03 , 2003. [5] i Cancho, R., Sole, R. The small world of human lan-guage. In Proc. R. Soc. London B , 2001. [6] Lyon, C., Nehaniv, C., and Dickerson, B. Entropy Indicators for Investigating Early Language Process. In AISB'05: Proceedings of EELC'05 , pp. 143 X 150. [7] Caldeira, S., Lobao, T., et al. The Network of Con-cepts in Written Texts. Eur. Phys. J. B 49, pp. 523 X 529 (2006). [8] Watts, D. and Strogatz, S., Collective dynamics of small-world networks, Nature 393, pp. 440 (1998). [9] Latora, V., Marchiori, M. Efficient Behavior of Small-World Networks. Phys. Rev. Lett., 87 (2001), art. No. 198701. [10] Sigman, M. and Cecchi, G. Global organization of the Wordnet lexicon. PNAS , USA, 99 (2002), pp. 1742 X  1747. [11] Newman, M. The structure and function of networks, Comput. Phys. Comm ., 147(2002), pp. 40 X 45. [12] Porter, M. The Porter Stemming Algorithm. (2005) ( http://www.tartarus.org/~martin/PorterStemmer ) [13] Steyvers, M. and Tenenbaum,J. The Large-Scale Structure of semantic networks: Statistical Analyses and a Model for Semantic Growth, Cognitive Science, 29(1, 2005), pp. 41-78 [14] Humphreys, J. PhraseRate: An HTML Keyphrase Extractor. Technical report, University of California, Riv-erside. June 2002. http://infomine.ucr.edu/ [15] Forman, G. Extensive empirical study of feature se-lection metrics for text classification. J. of Machine Learning Research , 3 (2003) pp. 1289-1305, MIT Press. [16] Yang, Y. and Pedersen, J.O. A Comparative Study on Feature Selection in Text Categorization. In Proc. of the ICML X 97 , pp. 412 X 420, 1997 [17] Giuffrida, G., Shek, E., and Yang, J. Knowledge-based metadata extraction from PostScript files. In Proc. of Fifth ACM Conference on Digital Libraries , 2000. [18] Witten, I.H. et al. KEA: practical automatic keyphrase extraction. In Proc. of Fourth ACM Confer-ence on Digital Libraries , 1999. [19] Mladenic, D. and Grobelnik, M. Word sequences as features in text. In Proc. of ERK X 98 , 1998. [20] Caropreso, M.F., et al. A Leaner-Independent Evaluation of the Usefulness of Statistical Phrase for Automated Text Categorization. Text Databases and Document Management: Theory and Practice , A.G.Chin, ed. Idea Group Publishing, Hershey, PA, pp.78 X 102, 2001. [21] Fortunato, S., et al. A Method to Find Community Structures Based on Information Centrality. Phy. Rev. E. (2004). [22] Robertson, S., et al. Simple BM25 extension to mul-tiple weighted fields. In Proc. of the CIKM X 04 , pp. 42 X 49, 2004. [23] Mihalcea, R. and Tarau P. TextRank: Bringing Order into Texts. I n Proc.of EMNLP 2004 , Barcelona, Spain, July 2004. [24] Tomokiyo, T. and Hurst M. A language model ap-proach to keyphrase extraction. In Proc.of the ACL Work-shop on Multiword Expressions , 2003. [25] Chang, C. and Lin, C. LIBSVM: a library for support vector machines, 2006. Software available at 
