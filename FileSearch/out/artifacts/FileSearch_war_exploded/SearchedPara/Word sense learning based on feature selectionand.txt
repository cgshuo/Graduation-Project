 Donghong Ji  X  Yanxiang He  X  Guozheng Xiao Abstract In this paper, we propose a word sense learning algorithm which is capable of unsupervised feature selection and cluster number identification. Feature selection for word sense learning is built on an entropy-based filter and formalized as a constraint optimization problem, the output of which is a set of important features. Cluster number identification is built on a Gaussian mixture model with a MDL-based criterion, and the optimal model order is inferred by minimizing the criterion. To evaluate closeness between the learned sense clusters with the ground-truth classes, we introduce a kind of weighted F-measure to model the effort needed to reconstruct the classes from the clusters. Experiments show that the algorithm can retrieve important features, roughly estimate the class numbers automatically and outperforms other algorithms in terms of the weighted F-measure. In addition, we also try to apply the algorithm to a specific task of adding new words into a Chinese thesaurus.
 Keywords Feature selection MDL Clustering Word senses Text processing 1 Introduction Word meaning has long been a concern in analytic philosophy and linguistics, and has become a key problem in computational lexicography and natural language processing in recent years (Manning and Schutze 1999 ). So far, there have been many applications of word meaning or word senses in information retrieval and machine translation (Krovetz and Croft 1993 ; Schutze and Pederson 1995 ; Sanderson 2000 ). However, such applications still heavily rely on manual lexical resources (e.g., common dictionaries or thesaurus, etc.) to provide word definitions or synonyms. One prominent problem in applications of such lexical resources lies in their incompleteness: some emerging or domain-specific senses or even new words themselves may be missing from the resources. So, solutions to automated learning of word senses become very important.

The problem of word sense learning is closely related with how word senses are represented. For different representation schemes, there would be different solutions. One representation strategy is to treat wo rd senses as synonym sets like synsets in Wordnet. Thus, to discover senses of a word will be to find its various synonym sets, with each set denoting one sense of the word. One method belonging to this strategy is concept discovery by committee-based method (Lin and Pantel 2002 ;PantelandLin 2002 ). For a word, the solution first finds its top k similar words based on some features specified by a syntactic parser, and then groups the k words into clusters (also called committees )using average-link clustering. Finally the wor disassignedtoeachoftheseclusters.
There are two difficulties with this committee-based method. The first concerns its feature designation. Here, a feature is a small structured context around the word, consisting of a content word and its syntactic relation with the target word. Thus the features may be very sparse, since a content word may involve various syntactic relations with the target word. Furthermore, the features rely on the output of a syntactic parser, and parsing is still a problem for some languages such as Chinese. The second difficulty with this method is that it needs to pre-define the number of committees manually, which, however, is generally not known in advance.
Another representation strategy is to take word senses as groups of word occurrences with semantically similar contexts. In this approach, word senses are determined by clustering word occurrences based on their contexts. One influential work under this strategy is context-group discrimination (CGD) (Schutze 1998 ), where the second-order co-occurrence information of an ambiguous word is used to create context vectors of word occurrences, and then the occurrences were clustered into coherent groups based on the similarity of these vectors.

Two observations can be made about the features and the clustering process in the CGD approach. One observation is that the method only considers individual feature weighting for feature selection and doesn X  X  evaluate the feature set as a whole. The other observation is that, similar to the committee-based approach, the method also needs to pre-specify the number of clusters. As the specified number changes, this method indeed can capture both coarse and fine sense distinctions. However, from a statistical point of view, there should be an optimal cluster number for any collection of word occurrences. What is more, being able to find the optimal number can be helpful in some applications, e.g., lexicography.

In this paper, we adopt the second representation scheme, and focus on feature selection and cluster number identification for word sense learning. Feature selection ensures that features are evaluated by their overall performance, not just by their individual behavior. With cluster number identification, the optimal number of word senses can be derived simultaneously with cluster members. In addition, to alleviate the problem of feature sparseness, we also use second-order statistics.
The remainder of the paper is organized as follows. In Sect. 2 , we give a general description of the problem. In Sect. 3 , we focus on feature selection. In Sect. 4 ,we specify the GMM clustering and MDL criterion. In Sect. 5 , we present experiments and evaluations. In Sect. 6 , we talk about a specific application of this method. In Sect. 7 , we discuss some related work. Finally in Sect. 8 , we give the conclusion and some future work. 2 Problem setting Given a set of word occurrences, the problem of word sense learning is to group the occurrences into some clusters according to their contexts. As a result, each cluster contains contextually similar occurrences and can be interpreted as one sense of the word.

Intuitively, for each sense of the target word, there would be some contextual words that are closely related with it in their meanings. Such words are features of the word sense. Although word senses might be obscure under the contexts, they could be disclosed under the features. So, in order to solve the problem, we could select these features from the contexts and use these features to cluster the occurrences.

One strategy for solving this problem is a wrapper-based method, which determines the features and clusters simultaneously (Dash and Liu 2000 ; Talavera 1999 ; Niu et al. 2004 ). However, since neither the class labels nor number of the senses is provided, it is difficult to estimate the accuracy of the clustering, especially when the clustering is conducted in different feature subspaces. Furthermore, wrapper-based methods are always computationally expensive, since they involve clustering in the process of feature selection.

We therefore adopt a two-phase strategy: a filter for feature selection followed by the clustering. The filter evaluates the candidate feature subsets based on their distributions or intrinsic properties in the feature space, and identifies the most important features. Then, these features are used to evaluate the similarity between the occurrences in the clustering algorithm.

For the filter, we adopt the entropy-based method (Dash and Liu 2000 ; Dash et al. 2002 ) with the assumption that if the occurrences have a good enough feature subset to disclose the underlying cluster structure, the entropy in terms of their point-wise distances should be lower; otherwise, the entropy should be higher. Thus, what we need to do is to find a feature subset to minimize the entropy. For the clustering procedure, we use a Gaussian mixture model (Figueiredo and Jain 2000 ), since it allows a formal and probabilistic modeling of the data for unsupervised clustering and by combining with MDL (Minimum Description Length), it can effectively infer the optimal number of the components or clusters (Figueiredo and Jain 2000 ; Law et al. 2002 ).
 3 Feature selection Suppose w is an ambiguous word, W ={ w 1 , w 2 , ..., w n } is the set of its n occurrences and C i is the context of w i . 1 Let C = C 1 [ C 2 [ C n .Then C is the set of all 1st order contextual words . For any word c [ C , given a large corpus, we can collect all of its contexts with a predefined window size. Suppose D c is the set of all the contextual words in this collection, and let D = [ c [ C D c , then, D is the set of all 2nd order contextual words . We can construct a C -by-D matrix T p q , where p =| C |, and q =| D |, and T ( i , j ) is the frequency of the j th word of D occurring in the contexts of the i th word of C . Intuitively, T p q contains the second order co-occurrence information.

For any word c [ C , let T c be the row in T p q corresponding with c , then for any occurrence w i , we can build its second order co-occurrence vector S i by summing T c for all c [ C i . Then, a W -by-D matrix S n q can be constructed to model the contexts of the n occurrences based on second order co-occurrence information.

Now, it appears that the featureselectionpro cedurecan beapplied tot his matrix toextract features. However, since the sense associated with a word occurrence is always determined there exist many more noisy words in the contexts than real features. So, simply summing the 2nd order co-occurrence vectors togeth er may result in a noise dominated matrix.
To deal with this problem, we extend the feature selection procedure backwards to the process of constructing the W -by-D matrix. That is to say, we select better feature words in the 1st order contexts so as to construct a better W -by-D matrix, enabling better feature subset selection.

Suppose X C , and | X | = 0, for any w i , we can construct a new second order co-built based on X . Let Y D , and S X, Y is the sub-matrix, the projection of S X  X  n q  X  in the feature sub-space Y , our problem can be formalized as finding X and Y to minimize the entropy of S X , Y : where n is the number of the word occurrences, and d i , j is the similarity between w i and w j in terms of the cosine distance between S i and S j . X is the set of the 1st order feature words, and Y is the set of the 2nd order feature words.

Since a 2nd order feature word tends to co-occur with some 1st order feature words, it is reasonable to assume that it also tends to occur with the ambiguous word under investigation, like the 1st order feature words. So we hold the following assumption.
Since the senses of the occurrences are determined by some feature words in its 1st order contexts, for each occurrence, there should be at least one feature word in its 1st order context. Formally, let cov ( W , X ) be the coverage rate of the feature set X with respect to W (i.e. the ratio of the number of the occurrences with at least one feature in their 1st order context against the number of the occurrences without any features in their 1st order contexts). We assume that:
This assumption also helps to avoid the bias toward fewer features based on 1, since if fewer features are selected, there may be some occurrences whose 1st order contexts contain no features. In such cases, their vectors in S are zero vectors and the entropy of the matrix will tend to be low.

Thus, feature selection here is formalized as a constraint optimization problem: to find X , which minimizes E ( S X , Y ) under cov ( W , X ) = 1. In this paper, we use v 2 as search heuristics. For the search algorithm, we use a sequential greedy forward floating search algorithm (Pudil et al. 1994 ). We set l =1, m = 1, where l is plus step, and m is take-away step. 4 Clustering After feature selection, we employ a Gaussian mixture model algorithm with a MDL criterion (Bouman et al. 1998 ; Law et al. 2002 ; Rissanen 1978 ) to estimate the optimal number of the word senses and the member occurrences of each sense. The MDL criterion is given by 4 and 5, where Y is the data points, y n is a data point in Y , K is the number of clusters, h is the parameter set, N is the number of the data points, M is the number of the dimensions, and L is given by 5.

In 4, the log likelihood in the first part measures the goodness of the model X  X  fit to the data points, while the second part gives a penalty to the complexity of the model. The estimator works by finding optimal parameters to minimize the code length for the data points.

For each K , an EM algorithm (Bouman et al. 1998 ; Law et al. 2002 ) can be used to seek the solution, which contains K clusters as well as their members. The initialization, E-Step and M-step are as follows.

Initialization :
E-Step : M-Step :
The algorithm will be terminated when the change of MDL( K , h ) is less than a threshold given in 15. By comparing the values of MDL among all K , we can get K * , which minimizes MDL( K , h ). A practical constraint for our problem here is that we let K &lt; 15, due to the fact that most words have less than 15 senses. 5 Experiments and evaluation 5.1 Datasets We have two datasets for evaluation of the algorithm. One is about the Chinese character  X  (/chao/) 2 , which was extracted from Sina News corpus (2001 X 2005) 3 , and the other is SenseVal 3 Chinese data. Tables 1 and 2 give the details of the two datasets respectively.
 With regard to the first dataset, one argument in Chinese linguistics is that single Chinese characters may not be meaningful. However, the contexts in which a Chinese character can occur, exactly like those for Chinese words, may be of much difference. If the contexts can be distinguished, it would be reasonable to assign different senses to a single Chinese character. In addition, there have been some dictionaries published listing senses for single Chinese characters.

The reason why we include such a dataset for single Chinese characters is that its meaning, if any, is largely dependent on the word containing the character. So, the other characters collocating with it within the word as well as the word itself should be good feature candidates (benchmark features) for its sense discrimination. Thus, we can evaluate the feature selection simply by checking whether such characters or words occur in the selected feature set.

Table 1 lists 4 senses of the Chinese character  X  (/chao/). For each sense, we collected 100 sentences containing the benchmark words from the Sina News corpus. Since the senses are mainly dependent on the words, Table 1 also lists such benchmark words and their counts in the collection. Also, Table 1 lists the collocating characters within the words as benchmark character features. For example,  X   X  (/chaoxian/, Korea) occurs in 20 sentences among 100 sentences with the first sense of  X  (/chao/),  X   X  (/chaoxian/, Korea) is a benchmark word feature and  X  (/xian/, Korea) is a benchmark character feature for this sense.
 The second dataset is for Chinese word sense disambiguation released by SenseVal 3. Although it was originally divided into training and testing parts, there was no need for training in our task, so we merged them for evaluation of our word sense learning algorithm. Table 2 lists the words and counts of the examples. For this dataset, the sense definitions come from Hownet 4 , a commonly-used Chinese thesaurus.
 In our experiment, a feature can be a Chinese character, a character bigram or a Chinese word. To ensure comparability, we set the 1st order context as the sentence containing the character for the first dataset or the word for the second dataset, and in both cases, the 2nd order co-occurrence matrix was built with a context window of 20 words around the feature. Unigram or bigram features were acquired from the 20-word context windows. 5.2 Feature selection evaluation For evaluation of feature selection for the first dataset, we compared the extracted character features with the benchmark character features, and found that all benchmark features were recalled except two ambiguous or functional characters,  X  (/zhong/, China) and  X  (/zhe/, * ing). For the character  X  (/zhong/, China), since it can act as a preposition (meaning  X  X nside X ), it may occur in any context, and indeed occurred in the example sentences with a high frequency. For the character  X  (/zhe/, * ing), being a function word, it can also be seen in any context of the occurrences. For both characters, high frequency reduces the distinctiveness of the features in the contexts, which is the reason why they were ruled out in the extracted feature set.

Besides the benchmark characters, the final feature set included some interesting character features. For example, related with the first sense of the character  X  (/ chao/), the algorithm also retrieved  X  (/he/, nuclear),  X  (/hui/, meeting),  X  (/tan/, talk), and  X  (/nan/, south), etc. In general, these features are all about an international hot topic, i.e., security of Korea Peninsula. From these features, we can learn that the first sense of the character  X  (/chao/) is related with  X  X orth/South Korea X ,  X  X uclear X ,  X  X eeting X ,  X  X alk X , etc.

Regarding word features, not surprisingly, we retrieved all those benchmark words in column 2 of Table 1 , except two words  X   X  (/chaozhe/, toward) and  X   X  (/zhongchao/, China &amp; Korea). Besides these words, we also acquired some other interesting word features. As an example, again related with the first sense of the character  X  (/chao/), the extracted word features included  X   X  (/huitan/, talk),  X   X  (/fenghui/, summit meeting),  X   X   X  (/hewuqi/, nuclear weapon),  X   X  (/yuanzi/, atom), etc. Intuitively, all these words are related with the issue of Korea Peninsula security.

We also noticed that some meaningful word features were not extracted due to their lower frequency or word segmentation errors. For example,  X   X  (/chaozhe/, toward),  X   X  (/zhongchao/, China &amp; Korea) and  X   X   X  (/wuhehua/, nuclear-free) should all be meaningful features, but the word segmentation module failed to merge the Chinese characters as a word as they were not included in the segmentation dictionary. Another example is  X   X  (/kate/, Carter), name of former U.S. president, which may also be a meaningful feature. However, its frequency (3) was too low in the example sentences, which made it difficult to be extracted out.
The overall aim of feature selection is to reveal the cluster structure behind the data. So from the resulted cluster structure, we can backwards evaluate the extracted features. Since we have known the true classes for both datasets, we check the change in intra-class or inter-class similarities arising from the feature selection. In general, a good feature set will increase intra-class similarities and decrease inter-class similarities simultaneously. Figure 1 shows the change of the two kinds of similarities with different features.

Figure 1 indicates that feature selection indeed helped to increase the similarity between examples within the classes and decrease the similarity between the classes. It can also be seen that by using words as features, we acquired the highest intra-class similarities and the lowest inter-class similarities. So, words-based features performed better than characters or character bigrams, while bigrams gave the worst performance. The reason may be that bigrams contain too many noisy features, and word segmentation helps to remove some of the noise.

For SenseVal dataset, we extracted word features from 1st and 2nd order contexts respectively, and also evaluated them on intra-class and inter-class similarities. Figure 2 shows the change of the similarities for two specific ambiguous words  X  (/bao/, bag, allocate, etc) and  X   X  (/fenzi/, molecule or member) as well as the average similarity over all the ambiguous words in the dataset.

From Fig. 2 , we have two findings. Firstly, feature selection helped to reveal the cluster structure from both the 1st order and the 2nd order contexts, since the inter-cluster similarities decreased and the intra-cluster similarities increased in all cases after feature selection. This confirms that feature selection is able to remove some noisy features. Secondly, 2nd order contexts can help make the cluster structure more distinct, since with the 2nd level contexts, we acquired lower intra-cluster similarities and higher inter-cluster similarities.

Consider the two specific ambiguous words,  X  (/bao/, bag, allocate, etc) and  X   X  (/fenzi/, molecule or member), we can see that the cluster structure for  X   X  (/fenzi/, molecule or member) is much clearer than  X  (/bao/, bag, allocate, etc). To see the reason, we checked the example sentences and found that there are 8 senses for the word  X  (/bao/, bag, allocate, etc), and some sense distinctions are very fine-grained. In contrast, there are 2 senses for the word  X   X  (/fenzi/, molecule or member), and their distinction is very clear: one is about molecules and the other is about membership. 5.3 Cluster number evaluation To evaluate cluster numbers, we can easily compare the number of ground-truth classes given in the dataset and the number of clusters produced by the algorithm. For both datasets, we have known the number of ground-truth classes.
 For the first dataset, Table 3 gives the clustering result with words as features. We can see that five clusters were derived for the actual four ground-truth classes. For each cluster, Table 3 lists the sense denoted by an English word as well as the counts of the examples included in the cluster. Since the sense can also be indicated by the benchmark word which contains the character, Table 3 also lists the words as well as their counts in the cluster. For example, the first cluster corresponds to the sense denoted by  X  X orea X , and it contains 105 example sentences. Among these examples, 20 contain the word  X   X  (/chaoxian/, Korea) and 11 contain  X   X  (/ zhongchao/, China &amp; Korea).
 Comparing the clusters with the ground-truth classes, we have several findings. Firstly, from Table 3 we can see that the examples with  X   X  (/chaozhe/, toward) were scattered into several clusters. To see the reason, we found that  X   X  (/chaozhe/, toward) was not included in the word segmentation dictionary, and it was segmented into two words during the word segmentation phase. As a result, it could not be extracted as a feature word like other benchmark words such as  X   X  (/chaoxian/, Korea). Furthermore,  X  (/zhe/, * ing), as a function word, was not retrieved as a feature word either due to its high frequency. Without these benchmark features, it was difficult to group the examples together.

Secondly, similarly with  X  (/zhe/, * ing),  X  (/zhong/, China) was not extracted as a feature word either, and  X   X  (/zhongchao/, China &amp; Korea), like  X   X  (/chaozhe/, toward), was segmented into two words,  X  (/zhong/, China) and  X  (/chao/, Korea). However, we found that the examples with  X   X  (/zhongchao/, China &amp; Korea) were largely retained in the first cluster. The reason is that such examples contain other word features, like  X   X  (/huitan/, talk),  X   X   X  (/hewuqi/, nuclear weapon), etc., which may be shared by most examples in the first cluster. So their contexts are still similar enough to keep them within the same cluster.

Thirdly, most examples with  X   X  (/yuanchao/, Yuan Dynasty) in the 2nd ground-truth class (Table 3 ) were separated to form another cluster (5th cluster in Table 3 ). To find the reason, we checked the examples with the words  X   X  (/qingchao/, Qing Dynasty),  X   X  (/mingchao/, Ming Dynasty), and  X   X  (/yuanchao/, Yuan Dynasty), and found that as far as these examples are concerned, the 2nd order context vectors for  X   X  (/qingchao/, Qing Dynasty) and  X   X  (/mingchao/, Ming Dynasty) are very similar, but neither sets are similar to most vectors of  X   X  (/yuanchao/, Yuan Dynasty).
For SenseVal data, Table 4 gives the comparison between the ground-truth class number and cluster number for each test word as well as their difference. It shows that the cluster numbers are equal with or close to the class numbers in all the cases. The two numbers are equal for 8 out of 20 words; for another 8 words, the class number is more than the cluster number by an average of 1.6(13/8); and for the remaining 4 words, the cluster number is more than the class number by an average of 1.2 (5/4).

Further analysis suggests some reasons for the inconsistency between class numbers and cluster numbers. First, some important features cannot be retrieved due to word segmentation errors. As an example, for the fourth sense of the first dataset, the important word feature  X   X  (/chaozhe/, toward) was not recalled. Second, some senses are more related to category features rather than word features. Again for the fourth sense of  X  (/chao/), even if  X   X  (/chaozhe/, toward) could be retrieved as a word feature, being a preposition, its sense should be closely related with the parts-of-speech of its objectives. However, we did not consider such category features in this work. Third, some senses have too little difference to be distinguished. For example, the first and third senses of  X   X  (/yanjiu/, study, discuss) share the same parts-of-speech and almost the same word features. Fourth, some senses have distinct parts-of-speech but share similar contextual word features. For example, consider the first and third senses of  X   X  (/tuchu/, salient, to make salient), which hold parts-of-speech as adjective and verb respectively, but share similar contextual word features. 5.4 Clustering evaluation It is known that unsupervised clustering is more difficult to evaluate than supervised classification, since the number of clusters may be different from that of ground-truth classes and there are no class labels available for the clusters. Thus, the commonly used evaluation criteria in supervised classification, F-measure, cannot be used to evaluate the quality of the clusters directly.

One possible evaluation strategy for unsupervised clustering is based on mutual information between the clusters and classes (Bradley et al. 1998 ; Vaithyanathan and Dom 1999 ), which gives a theoretical characterization of the difference between two distributions in clusters and classes respectively. However, in some applica-tions, one may be more interested in the edit distance between clusters and classes, which denotes the number of operations required to re-build the ground-truth classes from the produced clusters. In this paper, we adopt this operational view to evaluate the quality of the produced clusters.

To do that, we need to associate each ground-truth class with one cluster. One strategy is to build a one-to-one mapping between clusters and classes for the association by majority voting based on their common members (Lange et al. 2002 ; Sahami et al. 1998 ). However, from the viewpoint of re-constructing each class from some cluster, the association may not necessarily be a one-to-one mapping. We therefore additionally borrow the F-measure from supervised learning to relate each class with some cluster, since it considers both precision and recall and is more appropriate for capturing the practical difference between the clusters and the classes than majority voting.
 truth classes. For any i (1 i p ) and j (1 j q ), we can define the F-measure between H i and G j , F i , j , as usual with G j as the golden standard. For each G j ,we can select i to maximize F i , j , and the selected i is denoted as i ( j ). So, the weighted F-measure is given in 16.

Intuitively, F i , j denotes the operational difference between H i and G j , while the weighted F i , j denotes the average operational difference between the clusters and the classes.

Table 5 lists the weighted F-measure scores of our algorithm for the first dataset, two specific words in second dataset as well as the averaged score for the second dataset. For comparison, Table 6 also gives the scores achieved by the two other methods, stability-based and CGD.

For the CGD algorithm, since it adopts k -means clustering, it needs the number of clusters as input. For all cases, we input k as the ground-truth numbers. For the stability-based method (Niu et al. 2004 ), it conducts feature selection based on the performance of clustering with the features, so it is wrapper-based in nature. The key assumption behind the idea of stability is that if the features are good, the cluster structure estimated in the feature space should be stable across disjoint data subset (Lange et al. 2002 ). From Table 5 , we can see that the performance achieved by our entropy-based method is a little lower than that of context-group discrimination algorithm, which is of supervised nature, and outperforms stability-based method with a significant margin. 6 Application In this section, we seek to apply the word sense learning algorithm to a specific task. The task is to add some new words into a Chinese thesaurus, Tongyici Cilin (Mei et al. 1982 ; henceafter Cilin). This thesaurus contains about 70,000 Chinese words, and is normally used for people to select appropriate words in manual translation and writing, thus the words are organized in a hierarchical structure based on their meanings: 12 major classes, 94 medium classes, 1428 minor classes and 29765 synsets. Each category in the hierarchy is given an 8-character code. For example, Af040101 is the code for a synset, while Af04, Af and A are the codes for the minor, medium and major classes it belongs to respectively.
 As a public thesaurus for Chinese words, Cilin has found many applications in Chinese information processing since it its release. However, one prominent problem for its application is that it does not include many commonly used words. For example, about 15,000 words in Xiandai Hanyu Cidian (Modern Chinese Dictionary) do not occur in Cilin, let alone the many other new words occurring nowadays. Due to the huge human effort needed to enlarge it manually, it is very useful to find some way to add new words into the thesaurus automatically or semi-automatically.

One effort has been to find appropriate categories for new words based on their definitions (Ji et al. 1998 ), where the main idea is to compare definitions of new words with those of the words already in the synsets. The limitation of this method is that it requires some definition of new words, because many new words are not included in Chinese dictionaries and it is difficult to find their appropriate definitions.

For this task, word sense learning can play a role, since it can derive word senses with their respective typical contexts. Specifically, for a new word, we can use the word sense learning algorithm to group its contexts. Similarly, for a synset in the thesaurus, we can group the contexts of its member words, especially those of its unambiguous members. We can then determine appropriate synsets for the new word by comparing its contexts with those of the synsets.

The procedure of adding a new word includes the following three steps: (i) to learn the senses of the new word as well as their context vectors; (ii) to learn the context vectors of each synset in the thesaurus; (iii) to compare the context vectors of word senses and those of synsets, and for
For (ii), we selected unambiguous words in the synset and accumulated their contexts together as the contexts of the synset. For (iii), we used the cosine measure between two vectors.

Table 6 lists some examples. For the first two words,  X   X   X  (/chushengzheng/, birth certificate) and  X   X   X  (/chuzhongsheng/, junior student), they were inserted into one synset respectively. While for the third word,  X   X  (patient friend), the word sense learning algorithm produced two senses as well as their contexts. Consequently, this word was inserted into two synsets, one is about  X  X riend X , the other is about  X  X atient X .
 For evaluation, we selected 1,000 new words occurring in the Sina News corpus. Via word sense learning and the procedure of automatically adding new words, these new words were inserted into 1,323 synsets in thesaurus. This means that for each new word, there were 1.3 synsets in average selected by the algorithm. To check the performance, we selected 200 words randomly, and found the accuracy was 65.2% by subjective decision.

In order to check the performance of word insertion on a large scale, we selected 1,000 words which are already included in the thesaurus. The number of synsets which contain these words is 1,359. After applying the algorithms, we found these words were inserted into 1,286 synsets, with an accuracy of 72.1% and a recall of 68.2%. It is noted that word adding is purely by empirical contextual similarity whereas synonym judgments in the synsets are mainly based on subjective decisions. So, the result demonstrates that for 27.9% of the 1,286 synsets, contextual similarity between the test word and the synset excluding the test word is not confirmed subjectively. Conversely, for 31.8% of the 1,359 synsets, the subjective decisions regarding the membership of the test words in the synsets is not supported by their contextual similarity.

Further analysis indicates that the main reason for the errors lies in the inconsistency between subjective judgment in the thesaurus and empirical evidence from the corpus. For example,  X   X   X  (/yanjiusheng/, graduate) was originally together with  X   X   X  (/chuzhongsheng/, middle school student) and  X   X   X  (/daxuesheng/, undergraduate). However, it was inserted into the synset, Al030201, which contains  X   X  (/rencai/, talent). On the one hand,  X   X   X  (/yanjiusheng/, graduate) is still a kind of student, and it is reasonable to put it together with  X   X   X  (/chuzhongsheng/, middle school student) and  X   X   X  (/daxuesheng/, undergradu-ate) subjectively. On the other hand,  X   X   X  (/yanjiusheng/, graduate) can be considered to embody talent and it may co-occur with  X   X  (/rencai/, talent) frequently in the corpus, thus they may share similar contexts, and it is also reasonable to put them together. 7 Related work Besides Pantel and Lin ( 2002 ), Lin and Pantel ( 2002 ) and Schutze ( 1998 ), there are other related efforts on word sense learning, mainly on English words.

Fukumoto and Suzuki ( 1999 ) proposed a term weight learning algorithm for verb sense disambiguation, which could automatically extract nouns co-occurring with verbs and identify the number of senses of an ambiguous verb. The limitation of their method is the assumption that nouns co-occurring with verbs are disambig-uated in advance and that the number of senses of the target verb is no less than two.
Dorow and Widdow ( 2003 ) represented the target noun word, its neighbors and their relationships with a graph in which each node denoted a noun and two nodes had an edge between them if they co-occurred with more than a given number of times. Senses of the target word were then iteratively learned by clustering the local graph of similar words around the target word with a Markov clustering method. This method is of a hard clustering nature in the sense that a node in the graph was assigned to exactly one cluster. Later, Dorow et al. ( 2005 ) used the same clustering method, but tried to cluster the edges instead of the words in the same graph. As a result, a word could be assigned to multiple clusters, and in this sense, the method achieved a soft clustering effect. However, in both cases, the algorithm required a threshold as input, which controlled the number of senses.

Pedersen and Bruce ( 1997 ) described an experimental comparison of three clustering algorithms for word sense discrimination. Their feature sets included morphology of target words, parts of speech of contextual words, absence or presence of particular contextual words and collocation of frequent words. Occurrences of target word were then grouped into a pre-defined number of clusters. Similar with many other algorithms, their algorithm also required the cluster number to be provided.

Later, Purandare and Pedersen ( 2004 ) described a method for word sense discrimination by clustering contexts in vector and similarity spaces, in which both first and second order contexts were exploited and hierarchical, partitional and hybrid clustering methods were applied respectively. Kulkarni and Pedersen ( 2005 ) reported the work of SenseCluster, which was also based on unsupervised clustering. In their method, features included word unigrams, bigrams, co-occurrences, and target co-occurrences, while the clustering method used was Repeated Bisections. Compared with our work, although they used various features, their method focused on individual feature weighting instead of overall feature selection. In addition, the cluster number needs to be given in advance.

Pedersen and Kulkarni ( 2006 ) proposed a method to select the right number of word senses based on clustering criterion functions, which is actually the ratio of intra-cluster similarities against inter-cluster similarities. The objective of this method is to estimate the appropriate cluster numbers, which is similar with our work here. However, this measure has a bias toward larger cluster numbers, as was confirmed by their experiments. In comparison, the MDL measure used in our method does not have any bias. Notice that for a fixed number of clusters, the comparison between intra-cluster similarities and inter-cluster similarities is meaningful. This is why we used them in the evaluation of selected features. 8 Conclusion and future work In this paper, we propose a word sense learning algorithm which is capable of feature selection and cluster number identification. Feature selection for word sense learning is built on an entropy-based filter and formalized as a constraint optimization problem, the output of which is a set of important features to be used to determine word senses. Cluster number identification relies on a Gaussian mixture model with a MDL-based criterion, and the optimal model order is inferred by minimizing the criterion. The experiments show that the algorithm can retrieve important features and roughly estimate the cluster numbers automatically. Although the cluster numbers may sometimes be incorrect, the performance in terms of the weighted F-scores is comparable to that of other algorithms with ground-truth class numbers as input.

Effective search strategy is a key step for feature selection here, considering the high dimensional feature space and often very large data sets. The search heuristics we adopt is based on v 2 . However, some important features may be missed when the data points are unbalanced. Future work includes how to design more effective search heuristics to deal with unbalanced data, and how to design more effective search methods (e.g., evolutionary algorithms) to avoid local optimal. In addition, we will explore more syntactic features such as parts-of-speech or semantic features for word sense learning.

In our method, we treat single characters, bigrams or words in local contexts as features. It has been shown that the relatedness between different features or the global topic information may play important roles in word sense discrimination. Thus future work also includes how to combine local features, their relatedness and global topic information together. In particular, we will explore the usage of lexical relations in word sense learning.

Finally, there has been much work on unsupervised feature selection recently (Dy and Brodley 2004 ; Law et al. 2002 ; Mitra et al., 2002 ; Modha and Spangler 2003 ). However, these algorithms often work with smaller data sets. In future work, we will attempt to study which strategy is the most effective for the problem here with both large data sets and high dimensional space.
 References
