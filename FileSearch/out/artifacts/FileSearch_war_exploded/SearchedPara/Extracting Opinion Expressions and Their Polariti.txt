 Automatic systems for the analysis of opinions ex-pressed in text on the web have been studied exten-sively. Initially, this was formulated as a coarse-grained task  X  locating opinionated documents  X  and tackled using methods derived from standard re-trieval or categorization. However, in recent years there has been a shift towards a more detailed task: not only finding the text expressing the opinion, but also analysing it: who holds the opinion and to what is addressed; it is positive or negative ( polarity ); what its intensity is. This more complex formula-tion leads us deep into NLP territory; the methods employed here have been inspired by information extraction and semantic role labeling, combinatorial optimization and structured machine learning.
A crucial step in the automatic analysis of opinion is to mark up the opinion expressions : the pieces of text allowing us to infer that someone has a partic-ular feeling about some topic. Then, opinions can be assigned a polarity describing whether the feel-ing is positive, neutral or negative. These two tasks have generally been tackled in isolation. Breck et al. (2007) introduced a sequence model to extract opin-ions and we took this one step further by adding a reranker on top of the sequence labeler to take the global sentence structure into account in (Johansson and Moschitti, 2010b); later we also added holder extraction (Johansson and Moschitti, 2010a). For the task of classifiying the polarity of a given expres-sion, there has been fairly extensive work on suitable classification features (Wilson et al., 2009).
While the tasks of expression detection and polar-ity classification have mostly been studied in isola-tion, Choi and Cardie (2010) developed a sequence labeler that simultaneously extracted opinion ex-pressions and assigned polarities. This is so far the only published result on joint opinion segmenta-tion and polarity classification. However, their ex-periment lacked the obvious baseline: a standard pipeline consisting of an expression identifier fol-lowed by a polarity classifier.

In addition, while theirs is the first end-to-end sys-tem for expression extraction with polarities, it is still a sequence labeler, which, by construction, is restricted to use simple local features. In contrast, in (Johansson and Moschitti, 2010b), we showed that global structure matters: opinions interact to a large extent, and we can learn about their interactions on the opinion level by means of their interactions on the syntactic and semantic levels. It is intuitive that this should also be valid when polarities enter the picture  X  this was also noted by Choi and Cardie (2008). Evaluative adjectives referring to the same evaluee may cluster together in the same clause or be dominated by a verb of categorization; opinions with opposite polarities may be conjoined through a contrastive discourse connective such as but .
In this paper, we first implement two strong base-lines consisting of pipelines of opinion expression segmentation and polarity labeling and compare them to the joint opinion extractor and polarity clas-sifier by Choi and Cardie (2010). Secondly, we ex-tend the global structure approach and add features reflecting the polarity structure of the sentence. Our systems were superior by between 8 and 14 absolute F-measure points. Our system was developed using version 2.0 of the MPQA corpus (Wiebe et al., 2005). The central building block in the MPQA annotation is the opin-ion expression . Opinion expressions belong to two categories: Direct subjective expressions (DSEs) are explicit mentions of opinion whereas expressive subjective elements (ESEs) signal the attitude of the speaker by the choice of words. Opinions have two features: polarity and intensity , and most expres-sions are also associated with a holder , also called source . In this work, we only consider polarities, not intensities or holders. The polarity takes the val-for compatibility with Choi and Cardie (2010), we mapped B OTH to N EUTRAL . In order to test our hypothesis against strong base-lines, we developed two pipeline systems. The first part of each pipeline extracts opinion expressions, and this is followed by a multiclass classifier assign-ing a polarity to a given opinion expression, similar to that described by Wilson et al. (2009).

The first of the two baselines extracts opinion ex-pressions using a sequence labeler similar to that by Breck et al. (2007) and Choi et al. (2006). Sequence labeling techniques such as HMMs and CRFs are widely used for segmentation problems such as named entity recognition and noun chunk extraction. We trained a first-order labeler with the discrimi-native training method by Collins (2002) and used common features: words, POS, lemmas in a sliding window. In addition, we used subjectivity clues ex-tracted from the lexicon by Wilson et al. (2005).
For the second baseline, we added our opinion ex-pression reranker (Johansson and Moschitti, 2010b) on top of the expression sequence labeler.

Given an expression, we use a classifier to assign a polarity value: positive, neutral, or negative. We trained linear support vector machines to carry out this classification. The problem of polarity classi-fication has been studied in detail by Wilson et al. (2009), who used a set of carefully devised linguis-tic features. Our classifier is simpler and is based on fairly shallow features: words, POS, subjectivity clues, and bigrams inside and around the expression. We formulate the opinion extraction task as a struc-tured prediction problem  X  y = arg max y w  X   X ( x, y ) . where w is a weight vector and  X  a feature extractor representing a sentence x and a set y of polarity-labeled opinions. This is a high-level formulation  X  we still need an inference procedure for the arg max and a learner to estimate w on a training set. 4.1 Approximate Inference Since there is a combinatorial number of ways to segment a sentence and label the segments with po-larities, the tractability of the arg max operation will obviously depend on whether we can factorize the problem for a particular  X  .

Choi and Cardie (2010) used a Markov factor-ization and could thus apply standard sequence la-beling with a Viterbi arg max . However, in (Jo-hansson and Moschitti, 2010b), we showed that a large improvement can be achieved if relations be-tween possible expressions are considered; these re-lations can be syntactic or semantic in nature, for instance. This representation breaks the Markov as-sumption and the arg max becomes intractable. We instead used a reranking approximation: a Viterbi-based sequence tagger following Breck et al. (2007) generated a manageable hypothesis set of complete segmentations, from which the reranking classifier picked one hypothesis as its final output. Since the set is small, no particular structure assumption (such as Markovization) needs to be made, so the reranker can in principle use features of arbitrary complexity.
We now adapt that approach to the problem of joint opinion expression segmentation and polarity classification. In that case, we not only need hy-potheses generated by a sequence labeler, but also the polarity labelings output by a polarity classifier. The hypothesis generation thus proceeds as follows:  X  For a given sentence, let the base sequence la- X  for every sequence, apply the base polarity Thus, the hypothesis set size is at most k s  X  k p . We used a k s of 64 and a k p of 4 in all experiments.
To illustrate this process we give a hypothetical example, assuming k s = k p = 2 and the sentence The appeasement emboldened the terrorists . We first generate the opinion expression sequence candidates: and in the second step we add polarity values: 4.2 Features of the Joint Model The features used by the joint opinion segmenter and polarity classifier are based on pairs of opinions: ba-sic features extracted from each expression such as polarities and words, and relational features describ-ing their interaction. To extract relations we used the parser by Johansson and Nugues (2008) to annotate sentences with dependencies and shallow semantics in the PropBank (Palmer et al., 2005) and NomBank (Meyers et al., 2004) frameworks.

Figure 1 shows the sentence the appeasement em-boldened the terrorists , where appeasement and ter-rorists are opinions with negative polarity, with de-pendency syntax (above the text) and a predicate X  argument structure (below). The predicate em-boldened , an instance of the PropBank frame embolden.01 , has two semantic arguments: the Agent (A0) and the Theme (A1), realized syntacti-cally as a subject and a direct object, respectively.
The model used the following novel features that take the polarities of the expressions into account. The examples are given with respect to the two ex-pressions ( appeasement and terrorists ) in Figure 1.
Base polarity classifier score . Sum of the scores
Polarity pair . For every pair of opinions in the
Polarity pair and syntactic path . For a pair
Polarity pair and syntactic dominance . In addition
Polarity pair and word pair . The polarity pair
Polarity pair and types and syntactic path . From
Polarity pair and semantic relation . When two
Polarity pair and words along syntactic path . We
We also used the features we developed in (Jo-hansson and Moschitti, 2010b) to represent relations between expressions without taking polarity into ac-count. 4.3 Training the Model To train the model  X  find w  X  we applied max-margin estimation for structured outputs, a generalization of the well-known support vector machine from binary classification to prediction of structured objects. Formally, for a training set T = { X  x i , y i  X  X  , where the output space for the input x i is Y i , we state the learning problem as a quadratic program:
Since real-world data tends to be noisy, we may regularize to reduce overfitting and introduce a pa-rameter C as in regular SVMs (Taskar et al., 2004). The quadratic program is usually not solved directly since the number of constraints precludes a direct solution. Instead, an approximation is needed in practice; we used SVM struct (Tsochantaridis et al., 2005; Joachims et al., 2009), which finds a solu-tion by successively finding the most violated con-straints and adding them to a working set. The loss  X  was defined as 1 minus a weighted combi-nation of polarity-labeled and unlabeled intersection F-measure as described in Section 5. Opinion expression boundaries are hard to define rigorously (Wiebe et al., 2005), so evaluations of their quality typically use soft metrics. The MPQA annotators used the overlap metric: an expression is counted as correct if it overlaps with one in the gold standard. This has also been used to evaluate opinion extractors (Choi et al., 2006; Breck et al., 2007). However, this metric has a number of prob-lems: 1) it is possible to  X  X ool X  the metric by creat-ing expressions that cover the whole sentence; 2) it does not give higher credit to output that is  X  X lmost perfect X  rather than  X  X lmost incorrect X . Therefore, in (Johansson and Moschitti, 2010b), we measured the intersection between the system output and the gold standard: every compared segment is assigned a score between 0 and 1, as opposed to strict or over-lap scoring that only assigns 0 or 1. For compatibil-ity we present results in both metrics. 5.1 Evaluation of Segmentation with Polarity We first compared the two baselines to the new integrated segmentation/polarity system. Table 1 shows the performance according to the intersec-tion metric. Our first baseline consists of an expres-sion segmenter and a polarity classifier (ES+PC), while in the second baseline we also add the ex-pression reranker (ER) as we did in (Johansson and Moschitti, 2010b). The new reranker described in this paper is referred to as the expression/polarity reranker (EPR). We carried out the evaluation using the same partition of the MPQA dataset as in our previous work (Johansson and Moschitti, 2010b), with 541 documents in the training set and 150 in the test set.

The result shows that the reranking-based mod-els give us significant boosts in recall, following our previous results in (Johansson and Moschitti, 2010b), which also mainly improved the recall. The precision shows a slight drop but much lower than the recall improvement.

In addition, we see the benefit of the new reranker with polarity interaction features. The system using this reranker (ES+PC+EPR) outperforms the expres-sion reranker (ES+ER+PC). The performance dif-ferences are statistically significant according to a permutation test: precision p &lt; 0 . 02 , recall and F-measure p &lt; 0 . 005 . 5.2 Comparison with Previous Results Since the results by Choi and Cardie (2010) are the only ones that we are aware of, we carried out an evaluation in their setting. 1 Table 2 shows our fig-ures (for the two baselines and the new reranker) along with theirs, referred to as C &amp; C (2010). The table shows the scores for every polarity value. For compatibility with their evaluation, we used the overlap metric and carried out the evaluation us-ing a 10-fold cross-validation procedure on a 400-document subset of the MPQA corpus.

The C &amp; C system shows a large precision bias despite being optimized with respect to the recall-promoting overlap metric. In recall and F-measure, their system scores much lower than our simplest baseline, which is in turn clearly outper-formed by the stronger baseline and the polarity-based reranker. The precision is lower than for C &amp; C overall, but this is offset by recall boosts for all polarities that are much larger than the precision drops. The polarity-based reranker (ES+PC+EPR) soundly outperforms all other systems. We have studied the implementation of end-to-end systems for opinion expression extraction and po-larity labeling. We first showed that it was easy to improve over previous results simply by combining an opinion extractor and a polarity classifier; the im-provements were between 7.5 and 11 points in over-lap F-measure.

However, our most interesting result is that a joint model of expression extraction and polarity label-ing significantly improves over the sequential ap-proach. This model uses features describing the in-teraction of opinions through linguistic structures. This precludes exact inference, but we resorted to a reranker. The model was trained using approx-imate max-margin learning. The final system im-proved over the baseline by 4 points in intersection F-measure and 7 points in recall. The improvements over Choi and Cardie (2010) ranged between 10 and 15 in overlap F-measure and between 17 and 24 in recall.

This is not only of practical value but also con-firms our linguistic intuitions that surface phenom-ena such as syntax and semantic roles are used in encoding the rhetorical organization of the sentence, and that we can thus extract useful information from those structures. This would also suggest that we should leave the surface and instead process the dis-course structure , and this has indeed been proposed (Somasundaran et al., 2009). However, automatic discourse structure analysis is still in its infancy while syntactic and shallow semantic parsing are rel-atively mature.

Interesting future work should be devoted to ad-dress the use of structural kernels for the proposed reranker. This would allow to better exploit syn-tactic and shallow semantic structures, e.g. as in (Moschitti, 2008), also applying lexical similarity and syntactic kernels (Bloehdorn et al., 2006; Bloe-hdorn and Moschitti, 2007a; Bloehdorn and Mos-chitti, 2007b; Moschitti, 2009).
 The research described in this paper has received funding from the European Community X  X  Sev-enth Framework Programme (FP7/2007-2013) un-der grant 231126: LivingKnowledge  X  Facts, Opin-ions and Bias in Time, and under grant 247758: Trustworthy Eternal Systems via Evolving Software, Data and Knowledge (EternalS).
