 Vladimir Vovk vovk@cs.rhul.ac.uk Ilia Nouretdinov ilia@cs.rhul.ac.uk Alex Gammerman alex@cs.rhul.ac.uk The majority of theoretical results in machine learn-ing (such as PAC theory and statistical learning the-ory) depend on the exchangeability assumption, so it is surprising that so little work has been done on test-ing this assumption. Of course, testing is a traditional concern for statisticians, but the usual statistical tech-niques do not work for high-dimensional data sets such as the USPS data set (257 variables; see  X  4). This pa-per approaches the problem of testing exchangeability building on the theory of Transductive Confidence Ma-chine (TCM), first introduced in (Vovk et al., 1999) as a practically useful way of attaching confidence mea-sures to predictions. It was shown in (Vovk, 2002) that TCM is automatically well-calibrated under any ex-changeable distribution when used in the on-line mode. In this paper we strengthen that result showing (The-orem 1 on p. 3) that a modification of TCM, which we call  X  X andomised confidence transducer X , produces p-values that are independent and distributed accord-ing to the uniform distribution U in [0 , 1]. It turns out that Theorem 1 is a convenient tool for testing the hypothesis of exchangeability.
 We start, in  X  2, with the definition of exchangeability and stating formally the problem of testing exchange-ability on-line. TCM, in the form of  X  X onfidence trans-ducers X , is introduced in  X  3. In  X  4 we briefly describe the USPS data set and a particular confidence trans-ducer, the Nearest Neighbour transducer, which works reasonably well for predicting the digits in the USPS data set. In  X  5 we define a family of exchangeability martingales, which we call  X  X ower martingales X , and report experimental results for a simple mixture of NN power martingales (i.e., power martingales constructed from the Nearest Neighbour transducer). We found that the simple mixture, which is a non-negative ex-changeability martingale that starts from 1, ends with ity of this event under the null hypothesis of exchange-typical significance levels, such as 1% or 5%, used in statistics. In  X  6 we describe procedures for  X  X racking the best power martingale X ; one particular, very sim-ple, procedure performs considerably better than the best power martingale on the USPS data set, achieving In this section we set up our basic framework, mak-ing some important distinctions that have been glossed over so far: exchangeability vs. randomness, martin-gales vs. supermartingales, etc.
 In our learning protocol, Nature outputs elements z , z 2 , . . . , called examples , of a measurable space Z . (It is often the case that each example consists of two parts: an object and its label; we will not, however, need this additional structure in the theoretical con-siderations of this paper.) The hypothesis of random-ness is that each example z n , n = 1 , 2 , . . . , is out-put according to a probability distribution P in Z and the examples are independent of each other (so the sequence z 1 z 2 . . . is output by the power distribution P  X  ). The almost identical hypothesis of exchange-ability is that the examples z 1 z 2 . . . are output accord-ing to an exchangeable probability distributions Q in Z  X  , i.e., such that under Q the permuted examples z  X  (1) , . . . , z  X  ( n ) are distributed as the original exam-ples z 1 , . . . , z n , for any n and any permutation  X  of { 1 , . . . , n } . It is clear a priori that the exchangeabil-ity hypothesis is as weak as or weaker than the ran-domness hypothesis, since all power distributions are exchangeable.
 We are interested in testing the hypothesis of random-ness/exchangeability on-line : after observing each new example z n Learner is required to output a number M n reflecting the strength of evidence found against the hypothesis. The most natural way to do this is to use non-negative supermartingales starting from 1 (cf. Shafer &amp; Vovk, 2001). Suppose first that we want to test the simple hypothesis that z 1 , z 2 , . . . are gen-erated from a probability distribution Q in Z  X  . We say that a sequence of random variables M 0 , M 1 , . . . is a Q -supermartingale if, for all n = 0 , 1 , . . . , M n is a measurable function of z 1 , . . . , z n (in particular, M a constant) and If M 0 = 1 and inf n M n  X  0, M n can be regarded as the capital process of a player who starts from 1, never risks bankruptcy, at the beginning of each trial n places a fair (cf. (1)) bet on the z n to be chosen by Na-ture, and maybe sometimes throws money away (since (1) is an inequality). If such a supermartingale M ever takes a large value, our belief in Q is undermined; this intuition is formalized by Doob X  X  inequality, which im-plies where C is an arbitrary positive constant.
 When testing a composite hypothesis P (i.e., a family of probability distributions), we will use P -supermartingales , i.e., sequences of random variables M 0 , M 1 , . . . which are Q -supermartingales for all Q  X  P simultaneously. If P is the set of all power distributions P  X  , P ranging over the probability distributions in Z , P -supermartingales will be called randomness supermartingales . We will be even more interested in the wider family P consisting of all ex-changeable probability distributions Q in Z  X  ; in this case we will use the term exchangeability supermartin-gales for P -supermartingales.
 De Finetti X  X  theorem and the fact that Borel spaces are closed under countable products (see, e.g., Schervish, 1995, Theorem 1.49 and Lemma B.41) imply that each exchangeable distribution Q in Z  X  is a mixture of power distributions P  X  provided Z is Borel. This shows that the notions of non-negative randomness and exchangeability supermartingales coincide in the Borel case. But even without the assumption that Z is Borel, all randomness supermartingales are exchange-ability supermartingales.
 In this paper we will also need randomised exchange-ability martingales ; these are sequences of measurable functions M n ( z 1 ,  X  1 , . . . , z n ,  X  n ) (each example z tended by adding a random number  X  n  X  [0 , 1]) such that, for any exchangeable probability distribution Q in Z  X  , U being the uniform distribution in [0 , 1]. We refrain from giving the analogous definition of randomised randomness martingales; the discussion in the pre-vious paragraphs about the relation between ran-domness and exchangeability is also applicable in the randomised case (remember that Z  X  [0 , 1] is Borel when Z is). Doob X  X  inequality (2) is also true for non-negative randomised exchangeability martingales starting from 1.
 Remark Our definitions of martingale (3) and super-martingale (1) are from (Doob, 1953); a more modern approach (cf. Shiryaev, 1996; Shafer &amp; Vovk, 2001) would be to replace the condition  X  | M 1 , . . . , M n  X  in (3) and (1) by  X  |F n  X , where F n is the  X  -algebra generated by z 1 , . . . , z n in the case of (1) and z ,  X  1 , . . . , z n ,  X  n in the case of (3) (i.e., F n represents all information available by the end of trial n ). To see how restrictive conditions (3) and (1) are, notice that the notions of randomised exchangeability martingale and exchangeability supermartingale become trivial when this apparently small change is made: the latter will be non-increasing processes ( M 0  X  M 1  X  X  X  X  ) and the former will only gamble on the random numbers  X  ,  X  2 , . . . .
 Now we can state the goal of this paper. We will construct non-negative exchangeability supermartin-gales and randomised exchangeability martingales that, starting from 1, take large final values on data sets (concentrating on the USPS data set) deviating from exchangeability; as discussed earlier, this will also provide us with randomness supermartingales and randomised randomness martingales. Before this paper, it was not even clear that non-trivial super-martingales of this kind exist; we will see that they not only exist, but can attain huge final values starting from 1 and never risking bankruptcy. In this section we introduce the main tool for con-structing exchangeability martingales. A family of measurable functions { A n : n  X  N } , where A n : Z n  X  R n for all n , N is the set of all positive integers and R is the set of all real numbers (equipped with the Borel  X  -algebra) extended by adding  X  X  X  and  X  , is called an individual strangeness measure if, for any n  X  N , any permutation  X  of { 1 , . . . , n } , any ( z 1 , . . . , z and any (  X  1 , . . . ,  X  n )  X  R n , In other words, is an individual strangeness measure if every  X  i is de-termined by the multiset * z 1 , . . . , z n + and z i . (Some-time multisets are called  X  X ags X , whence our notation.) Individual strangeness measures will be our starting point when constructing exchangeability martingales. A randomised transducer is a function f of the type ( Z  X  [0 , 1])  X   X  [0 , 1]. It is called  X  X ransducer X  be-cause it can be regarded as mapping each input se-put sequence ( p 1 , p 2 , . . . ) of  X  X -values X  defined by p a randomised E/U-transducer if the output p-values p p 2 . . . are always distributed according to the uni-form distribution in [0 , 1]  X  , provided the input exam-ples z 1 z 2 . . . are generated by an exchangeable proba-bility distribution in Z  X  .
 We will construct randomised exchangeability mar-tingales from individual strangeness measures in two steps, first extracting randomised E/U transducers from the latter: given an individual strangeness mea-sure A , for each sequence ( z 1 ,  X  1 , . . . , z n ,  X  n [0 , 1])  X  define where  X  i , i = 1 , 2 , . . . , are computed from z i using A as per (5). Each randomised transducer f that can be obtained in this way will be called a randomised confidence transducer .
 Theorem 1 Each randomised confidence transducer is a randomised E/U transducer. A special case (labelled there as Theorem 2) of this theorem was used in (Vovk, 2002) as a tool for region prediction.
 In a similar way we can define (deterministic) confi-dence transducers f : given an individual strangeness measure A , for each sequence ( z 1 , . . . , z n )  X  Z  X  set where  X  i are computed as before. In general, a (deter-ministic) transducer is a function f of the type Z  X   X  [0 , 1]; as before, we associate with f a mapping from z We say that f is an E/U-supertransducer if p 1  X  p , p 2  X  p 2 , . . . for some random variables p 1 , p 2 , . . . distributed independently according to U , whatever the exchangeable distribution generating z 1 , z 2 , . . . is. The following implication of Theorem 1 is obvious: Corollary 1 Each deterministic confidence trans-ducer is an E/U supertransducer. The USPS data set (described in, e.g., Vapnik, 1998) consists of 7291 training examples and 2007 test ex-amples; we merged the training set and the test set, in this order, obtaining what we call the full USPS data set (or just USPS data set). Each example consists of an image (16  X  16 matrix of pixels) and its label (0 to 9). In region prediction (e.g., Vovk, 2002) it is usually beneficial to pre-process the images; no pre-processing is done in this paper.
 It is well-known that the USPS data set is heteroge-neous; in particular, the training and test sets seem to have different distributions. (See, e.g., Freund &amp; Schapire, 1996.) In the next two sections we will see the huge scale of this heterogeneity.
 It was shown in (Vovk, 2003) that a Nearest Neigh-bours TCM provides a universally optimal, in an asymptotic sense, on-line algorithm for predicting classifications under the assumption of exchangeabil-ity. On the empirical side, Figures 1 and 2 in (Vovk, 2002) show that a Nearest Neighbour TCM performs reasonably well on the USPS data set. Therefore, it is natural to expect that the Nearest Neighbour(s) idea will also perform well in the problem of testing ex-changeability. In our experiments we will only use the 1-Nearest Neighbour approach (although in principle almost any prediction algorithm can be adapted to testing exchangeability).
 We define the Nearest Neighbour (NN) individual strangeness measure as mapping (5) where in this formula, x i  X  R 256 is the image in a USPS (so that z i = ( x i , y i )), and d is the Euclidean distance; it is possible that  X  i =  X  (if the denominator in (7) is zero). Intuitively, an image is considered strange if it is in the middle of images labelled in a different way and is far from the images labelled in the same way. The corresponding confidence transducer (randomised or deterministic) will be called the NN transducer . In this and next sections we discuss the second step in transforming individual strangeness measures into (randomised) exchangeability (super)martingales: constructing the latter from (randomised) E/U (su-per)transducers. To this end we use the procedure suggested in (Vovk, 1993).
 Since where p n are the p-values output by a randomised con-fidence transducer, will be a non-negative randomised exchangeability martingale with initial value 1; this family of martingales, indexed by  X   X  [0 , 1], will be called the randomised power martingales .
 When applied to the NN transducer, the family of randomised power martingales ( randomised NN power martingales ) might at first not look very promising (Figure 1), but if we concentrate on a narrower range of  X  (Figure 2), it becomes clear that the final values for some  X  are very large.
 To eliminate the dependence on  X  , we may use the randomised exchangeability martingale which is called the simple mixture of M (  X  ) n . The simple mixture of randomised NN power martingales (which will also be referred to as the randomised NN SM ) tory is shown in Figure 3. This figure and Figures 5, 6, 8 below are affected by statistical variation (since the outcome depends on the random numbers  X  i actually generated), but the dependence is not too heavy. For example, in the case of the randomised NN SM the fi-nal values are: 2 . 18  X  10 10 (MATLAB pseudo-random numbers generator started from state 0), 1 . 32  X  10 10 (state 1), 1 . 60  X  10 10 (state 2),. . . ; in what follows we only give results for initial state 0.
 As clear from Figure 3, the difference between the training and test sets is not the only anomaly in the USPS data set: the rapid growth of the randomised NN SM starts already on the training set.
 If p n are output by the deterministic NN transducer, we call (8) the NN power supermartingales and we refer to (9) as the deterministic NN SM . As Figure 4 shows, the growth rate of the latter is slightly less than that of its randomised counterpart.
 The result for a randomly permuted USPS data set is shown in Figure 5. A low final value (about %) re-sults from NN SM X  X  futile attempts to gamble against a random sequence; to make possible spectacular gains against highly untypical sequences such as the original USPS data set, it has to underperform against random sequences. The simple mixture of the previous section has modest goals; the best it can do is to approximate the perfor-mance of the best power martingale. In this section we will see that it is possible to  X  X rack X  the best power martingale, so that the resulting performance consid-erably exceeds that of the best  X  X tatic X  martingale (8). We first generalise (8) as follows: for each  X  =  X   X  2 . . .  X  [0 , 1]  X  , we set For any probability distribution  X  in [0 , 1]  X  , define It is convenient to specify  X  in terms of the distribution of the coordinate random variables  X  n (but of course, since we integrate over  X  , this does not involve any extra randomisation; in particular, the mixture (11) is deterministic if p n are generated by a deterministic confidence transducer). One possible  X  is generated by the following Sleepy Jumper automaton. The states of Sleepy Jumper are elements of the Cartesian product { awake , asleep } X  [0 , 1]. Sleepy Jumper starts from the state (asleep , 1); when he is in a state ( s,  X  ), his tran-sition function prescribes that:  X  if s = asleep, he moves to the state (awake ,  X  )  X  if s = awake, he moves to the state ( s,  X  ), where The output of the Sleepy Jumper automaton start-ing from ( s 1 ,  X   X  1 ) = (passive , 1) and further moving  X  ,  X  2 , . . . , where The probability distribution  X  of  X  1 ,  X  2 , . . . generated in this way defines, by (11), a randomised exchangeabil-ity martingale (or exchangeability supermartingale), which we call the randomised Sleepy Jumper martin-gale (resp. Sleepy Jumper supermartingale ); if p n are produced by the NN transducer (randomised or deter-ministic, as appropriate), we refer to these processes as the randomised/deterministic NN SJ .
 Figures 6 and 7 show the performance of the ran-domised and deterministic NN SJ for parameters R = 0 . 01 and J = 0 . 001. When applied to the randomly permuted USPS data set, the randomised NN SJ X  X  per-formance is as shown in Figure 8. One way to improve the performance against a random data set is to de-crease the jumping rate: if J = 0 . 0001, we obtain a much better performance (Figure 9), even for a de-terministic NN SJ. It is easy to see the cause of the improvement: when J = 0 . 0001, the  X  -measure of su-permartingales (10) that make no jumps on the USPS mance on the original data set deteriorates (Figure 10) but not drastically.
 Remark The approach of this section is reminiscent of  X  X racking the best expert X  in the theory of pre-diction with expert advice. A general  X  X ggregating Algorithm X  (AA) for merging experts was introduced in (Vovk, 1990); in the context of this section, the experts are the power martingales and the mixing op-eration (9) plays the role of (and is a special case of) the AA. Herbster and Warmuth (1998) showed how to extend the AA to  X  X rack the best expert X , to try and outperform even the best static expert. Vovk (1999) noticed that Herbster and Warmuth X  X  algorithm is in fact a special case of the AA, when it is applied not to the original experts (in our case, (8)) but to  X  X uperex-perts X  (in our case, (10)).
 Of course, there are other ideas that can used when combining (10); e.g., it would be natural to allow  X  not only occasionally to jump randomly but also to allow it to drift slowly.
 We are grateful to the referees for their suggestions. This work was partially supported by EPSRC (grant GR/R46670/01), BBSRC (grant 111/BIO14428), EU (grant IST-1999-10226), and the Spanish Ministerio de Educacion, Cultura y Deporte (grant SAB2001-0057 to the School of Telecommunications, Universidad Po-litecnica de Madrid).
 Doob, J. L. (1953). Stochastic processes . New York: Wiley.
 Freund, Y., &amp; Schapire, R. E. (1996). Experiments with a new boosting algorithm. Machine Learning:
Proceedings of the Thirteenth International Confer-ence (pp. 148 X 156).
 Herbster, M., &amp; Warmuth, M. K. (1998). Tracking the best expert. Machine Learning , 32 , 151 X 178. Schervish, M. J. (1995). Theory of statistics . New York: Springer.
 Shafer, G., &amp; Vovk, V. (2001). Probability and finance: It X  X  only a game! New York: Wiley.
 Shiryaev, A. N. (1996). Probability . New York: Springer. Second edition.
 Vapnik, V. N. (1998). Statistical learning theory . New York: Wiley.
 Vovk, V. (1990). Aggregating strategies. Proceedings of the Third Annual Workshop on Computational Learning Theory (pp. 371 X 383). San Mateo, CA: Morgan Kaufmann.
 Vovk, V. (1993). A logic of probability, with appli-cation to the foundations of statistics (with discus-sion). Journal of the Royal Statistical Society B, 55 , 317 X 351.
 Vovk, V. (1999). Derandomizing stochastic prediction strategies. Machine Learning , 35 , 247 X 282.
 Vovk, V. (2002). On-line Confidence Machines are well-calibrated. Proceedings of the Forty Third An-nual Symposium on Foundations of Computer Sci-ence (pp. 187 X 196). IEEE Computer Society.
 Vovk, V. (2003). Universal well-calibrated algorithm for on-line classification. Proceedings of the Sixteenth Annual Conference on Learning Theory .
 Vovk, V., Gammerman, A., &amp; Saunders, C. (1999).
Machine-learning applications of algorithmic ran-domness. Proceedings of the Sixteenth International Conference on Machine Learning (pp. 444 X 453). San Francisco, CA: Morgan Kaufmann.
 Vovk, V., Nouretdinov, I., &amp; Gammerman, A. (2003).
Testing exchangeability on-line, On-line Com-pression Modelling project, http://vovk.net/kp , Working Paper #5.
 In this appendix we will give a short formal proof of Theorem 1; for further details, see the full version of this paper (Vovk et al., 2003). We will use the notation E F for the conditional expectation w.r. to a  X  -algebra F ; if necessary, the underlying probability distribution will be given as an upper index. Similarly, P F will stand for the conditional probability w.r. to F . Proof of Theorem 1 This proof is a generalization of the proof of Theorem 1 in (Vovk, 2002), with the same intuition and basic idea (described in detail in that paper). Define the  X  -algebra G n , n = 0 , 1 , 2 , . . . , as the collection of all measurable sets E  X  ( Z  X  [0 , 1])  X  which satisfy ( z for any permutation  X  of { 1 , . . . , n } and any sequences z z 2 . . .  X  Z  X  ,  X  1  X  2 . . .  X  [0 , 1]  X  ,  X   X  1 . . . In particular, G 0 (the most informative  X  -algebra) co-incides with the original  X  -algebra on ( Z  X  [0 , 1])  X  ; G 0  X  X  1  X  X  X  X  .
 Fix a randomised confidence transducer f ; it will usu-ally be left implicit in our notation. Let p n be the ran-P will refer to the probability distribution Q  X  U  X  (over examples z n and random numbers  X  n ) and E to the expectation w.r. to P . The proof will be based on the following lemma.
 Lemma 1 For any trial n and any  X   X  [0 , 1] , Proof This coincides with Lemma 1 in (Vovk, 2002) (p. 193), since err n = I { p numbers  X  n used by rTCM in (Vovk, 2002) are 1  X   X  n ), where I E means the indicator of a set E .
 The other basic result that we will need is the following lemma (whose simple proof is omitted).
 Lemma 2 For any trial n = 1 , 2 , . . . , p n is G n  X  1 -measurable.
 Fix temporarily positive integer N . First we prove that, for any n = 1 , . . . , N and any  X  1 , . . . ,  X  n The proof is by induction in n . For n = 1, (13) imme-diately follows from Lemma 1. For n &gt; 1 we obtain, making use of Lemmas 1 and 2, and the inductive as-sumption: almost surely.
 Equation (13) immediately implies Therefore, we have proved that the distribution of the random sequence p 1 p 2 . . .  X  [0 , 1]  X  coincides with U  X  on the  X  -algebra F N generated by the first N coor-dinate random variables p 1 , . . . , p N . It is well known (see, e.g., Shiryaev, 1996, Theorem II.3.3) that this implies that the distribution of p 1 p 2 . . . coincides with
