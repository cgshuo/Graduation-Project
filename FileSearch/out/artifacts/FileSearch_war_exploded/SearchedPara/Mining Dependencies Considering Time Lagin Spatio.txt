 ships that exist among variables of interest and quantify them. The problem of mining dependencies between variables in complex systems, such as economics, biological systems, traffic systems, climate change, etc., is important and fun-damental. Given these multiple variables, the goal is to use available variables to make precise prediction of future events and trends. In addition to this pri-mary goal, an important task is to identify dependencies between these variables wherein, data from one variable significantly help in marking predictions about another variable. For example, economists want to know whether burning natu-ral gas is a causal factor for the global warming, so they need to mine whether the global warming depends on burning nature gas.
 networks have been considered as a viable option for modeling dependencies in ever, most of them either do not consider the time lag, or only use a predefined value. In traffic temporal data, time-lagged relationships are crucial towards understanding the linkages and influence of the change between relative entrance ramps and exit ramps. These relationships are lagged in time because vehicles entrance ramps learning for exit ramps. Fig. 1 shows the Origin-Destination(OD) the entrance and exit ramps in the highway, respectively, and the values of the matrix represent the vehicle counts rushed from the entrances to the exits. The small vehicle counts. We say that there exits dependency between an entrance ramp and an exit ramp when the corresponding entry of OD matrix is bright. Owing to its dependencies to the traffic system, its understanding has the potential to aid forecasts of vehicle flow at exit ramps.
 One important model for mining causal dependencies considering time lag is Granger causality [ 9 ], which is a widely accepted notion of causality in econometrics. In par-ticular, it says that time series A causes B, if the current observations in A and B together, predict the future observations in B significantly more accurately, than the pre-dictions obtained by using just the current observations in B. Recently, there has been a surge of methods that combine this notion of causality with regression algorithms [ 10  X  12 ].
 However, they either do not emphasize the concept and important of time lag, or make all history data and current data together to predict, which leads to learning more irrelevant attributions. In this paper, we proposed a method to cope with mining dependencies of spatio-temporal traffic data. This method is based on the idea that combine with time L ag lasso (TLHL). TLHL decomposes the regression coefficients into a product between a feature-level component and a time-level component. Such a decomposition is very natural from the theory, namely, a specific regression the TLHL model places Gaussian and Cauchy distributions for the component coefficients as priors to control the model complexity. With the Gaussian like-lihood, we devise an efficient expectation maximization (EM) algorithm [ 13 ]to learn the model parameters. Moreover, we evaluate our model on both synthetic and real-world traffic data, and the conducted results show that the TLHL model is very effective in mining dependencies due to considering the time lag. review background and preliminaries. Section 3 presents the proposed method. Experimental studies are reported in Section 4. We conclude this paper and present future directions in Section 5. 2.1 Background Estimating the dependency structure in traffic networks plays an important role. Most of the existing works on dependency structure discovery do not consider the time lag. For example, Meinshausen etc. [ 14 ] learn the dependency structure by detect dependency relationships in traffic system by Gaussian graphical model. However, time lag is an important factor for discovering the traffic pattern, especially in spatio-temporal traffic data.
 ramp, thus, we need to consider this time when discovering the dependencies between entrance ramps and exit ramps. Traditional methods either use the same time stamp data of entrance ramps and exit ramps [ 8 ], or use neighbor graphical model has a parameter called the maximum lag. The maximum lag past to make accurate predictions of current and future events. However, all the past time stamp data in this method are treated uniformly. The disadvantage is that it does not consider time lag accurately, and always learns more irrelevant attributions. 2.2 Problem Definition Given d random variables X = { x 1 ,  X  X  X  ,x d } , and each variable x vations, x i =( x 1 i ,  X  X  X  ,x n i ) T .Let Y = { y i ,  X  X  X  We aim to learn the dependency structures between prediction variables and response variable.
 many vehicles will pass through some important exit ramps. We aim to encode the dependency structures between entrance ramps and exit ramps. The vehicles from relative entrance ramps of exit ramp are useful for predicting the vehicle ramps of exit ramp more accurately. To solve this, we make full use of time lag, which is a prior information in traffic data. Time lag is caused by travel time, vehicles might have different travel time, the mean travel time is not used as a lag to choose the relative past time data of entrance ramps for exit ramps more flexibly and accurately. In this section, we introduce the proposed TLHL model. TLHL model is based on the idea that combines the notion of causality with regression algorithm. The traditional regression coefficient is decomposed into products of two-level Specially, we propose a probabilistic framework for TLHL model, and devise an EM algorithm to infer the model parameters. 3.1 The TLHL Model First we propose a model for the component coefficients introduced previously. Most of the lasso-based algorithms solve the following optimization problem [ 10 , 14 , 19 , 20 ]: where  X  =(  X  1 ,  X  X  X  , X  i ) is the coefficient vector, L ( R (  X  ) is a regularizer that encodes different sparse pattern of  X  .
 In this paper, we propose a hierarchical model where each coefficient in  X  is we consider the two-level hierarchies, and  X  jl denotes the time-level coefficient of l th time lag with respect to the j th feature.
 In order to improve the efficiency of dependencies and time lag learning, we use some prior knowledge about time lag. For example, in traffic data, some travel time can be obtained from history data, or computed by road length and vehicle speed. However, different vehicles have different speeds, which leads to the fluctuation of time lag. Let K ( i )= exp (  X  ( i  X   X   X  j is mean time lag for vehicles from j th entrance ramp to the target exit ramp, and it is a known information. Thus, we have: low: where  X  =(  X  1 ,  X  X  X  , X  d )and  X  =(  X  11 ,  X  X  X  , X  1 L ,  X  X  X  3.2 A Probabilistic Framework for TLHL Model abilistic model. For a regression problem, we use normal distribution to define the likelihood for y i : where N (  X , s ) denotes a normal distribution with mean  X  and variance s we need to specify the prior over the parameter  X  j in  X  . Since  X  ponent coefficients corresponding to the feature-level and time-level are placed tribution: For feature-level coefficient, a Cauchy prior is placed: function defined as: where a and b represent the location and scale parameters respectively. The obtain sparse hyperparameter  X  jl , we place the Jeffreys prior [ 24 ]overthe  X  to learn the model parameters. 3.3 Parameter Inference are treated as hidden variables, and the model parameters are denoted by {  X  ,b, X ,  X  } where  X  = {  X  j } j  X  N d ,and  X  = {  X  j } j  X  N details in the EM algorithm.
 E-step: we construct the Q -function as where  X  t denotes the estimate of  X  in the t th iteration. It is easy to get ln p (
 X  | y ,  X  )  X  ln p ( y |  X  ,b, X ,  X  ,  X  )+ln p (  X  |  X  )+ln p ( Q (
 X  |  X  t )=  X  M-step: We maximize Q (  X  |  X  t ) to update the estimates of  X  . (1)For the estimation of  X  , we have to solve the following optimization prob-lem: By setting the derivatives of J with respect to  X  to zero, we only consider min By plugging the solution in Eq.( 9 ), we simplify Eq.( 8 )as second term of the objective function is non-convex. To solve this problem, we use the majorization-minimization (MM) algorithm [ 25 ] to solve this problem. obtained in the t th iteration in the MM algorithm as  X  t iteration, we only need to solve a weighted 1 minimization problem [ 26 , 27 ]: style solvers. where  X  l =(  X  1 l ,  X  X  X  , X  dl ),  X  x i l =(  X  1 x i 1 l  X  is where  X  X l =(  X  x l i ,  X  X  X  ,  X  x l n ),  X  Y =( X  y 1 , of them to zero and get For the estimation of  X  j , we also use a gradient method. The gradient can be calculated as In this section, we evaluate our proposed method on both synthetic dataset and real-world traffic dataset. The compared methods include: lasso without considering time lag [ 19 ], lasso with fixed time lag and Granger Lasso [ 28 ]. 4.1 Synthetic Data Setting: We first evaluate the effectiveness of our method on synthetic data. We simulate a regression problem with d = 20 features (prediction observations). The corresponding observations are generated from y = X X  +  X ,  X  is set as In order to simulate the corresponding observations generated from prediction observations of different time lag, we randomly generate mean time lag  X  each feature, only 5 out of coefficients of time-lag-level  X  values from [0 , 1]. Finally, we use exp (  X  ( l  X   X  j ) feature at the l th time. We set X  X  X  ( 0 , S P  X  P ) with s We use the mean squared error (MSE) ( 1 n percentage error (MAPE) ( 1 n the F1-score is computed as follows:
Pre = Results: We compare the methods when the sample size n is varying. Fig. 3 shows the values of MSE and MAPE. The methods with considering time lag out-perform traditional lasso algorithm. Specially, TLHL performs best. Due to the method of Lasso with fixed time lag only consider one value of lag, the lag fluctuating cannot be captured. Granger Lasso considers all the history prediction observations of lag equally, and it focuses on making best pre-diction, thus, many unrelated dependencies are learned, and might appear over fitting.
 Granger Lasso has a better prediction accu-racy on training samples than that on test samples. Fig. 2 gives the F1-score, which reflects the accuracy of dependency structures. We can see that the result of Granger Lasso is worse than TLHL, because the value of recall is small. Our method, TLHL, shows the best effect no matter on prediction accuracy or on dependencies learning. 4.2 Highway Traffic Data Description and Setting: We evaluate our methods on real-life traffic data. on ramps in a highway traffic network. Each observation is the vehicle count during 15 minutes interval.
 d = 236. The corresponding observations are collected at time interval 10:00-10:15 AM from 2010/8/1 to 2010/10/31 (n=92). All prediction observations are collected from 236 entrance ramps at 5:00-10:00 AM (L=20). The mean time lag of each entrance ramp is average travel time from this entrance ramp to exit ramp. The data in this experiment are all normalized.
 data, F1-score cannot be measured. Nevertheless, the dependencies detected is the most important for traffic prediction. We use MSE to evaluate the accuracy. Specifically, the dependency structure information can help domain experts to predict vehicle flow with relative prediction algorithm, thus, we combine local ramps, which is learned by dependency structure learning algorithm. The value of MAPE is the prediction results.
 Results: Fig. 4(a) shows the MSE of all methods. As can be seen, our pro-posed TLHL is consistently among the best. Time lag is the key feature of ramp and exit ramp, because some of vehicles from that entrance ramp do not arrive at the same time stamp. That is why the algorithm without considering The dependency structures obtained by dependency learning methods are essentially important for the analysis of traffic systems, such as vehicle flow prediction, anomaly detection. Domain experts can obtain the information of upper stations (entrance ramps) from the accurate dependencies, and predict flow prediction algorithm with dependency learning methods. We revise LWL prediction algorithm by monitoring the vehicle flow of entrance ramps. Due to our TLHL method can obtain the accurate dependency structure of entrance ramps and exit ramps, we can get precise information. The TLHL outperforms all other methods in terms of MAPE. In this paper, we propose a two-level hierarchies with time lag lasso to cope with dependency structure learning. We decompose the traditional regression time-level structure more accurately, we use the prior time lag information. We develop a probabilistic model to interpret how to construct the regularization form for model parameters. For experimental studies, we demonstrate the effec-The results show that our TLHL model can achieve significant improvements in both the datasets compared with other methods.
 learning to deal with time-varying observations. We can follow the evolvement of the dependencies and time lag in a network.

