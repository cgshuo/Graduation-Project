 How can one summarize a massive data set  X  X n the fly X , i.e., without even having seen it in its entirety? In this paper, we address the problem of extracting representative elements from a large stream of data. I.e., we would like to select a subset of say k data points from the stream that are most representative according to some objective function. Many natural notions of  X  X epresentativeness X  satisfy submodular-ity, an intuitive notion of diminishing returns. Thus, such problems can be reduced to maximizing a submodular set function subject to a cardinality constraint. Classical ap-proaches to submodular maximization require full access to the data set. We develop the first efficient streaming algo-rithm with constant factor 1 / 2  X   X  approximation guaran-tee to the optimum solution, requiring only a single pass through the data, and memory independent of data size. In our experiments, we extensively evaluate the effectiveness of our approach on several applications, including training large-scale kernel methods and exemplar-based clustering, on millions of data points. We observe that our streaming method, while achieving practically the same utility value, runs about 100 times faster than previous work.
 H.2.8 [ Database Management ]: Database applications X  Data mining Submodular functions; Streaming algorithms
The unprecedented growth in modern datasets  X  coming from different sources and modalities such as images, videos, sensor data, social networks, etc.  X  demands novel tech-niques that extract useful information from massive data, while still remaining computationally tractable. One com-pelling approach that has gained a lot of interest in recent years is data summarization : selecting representative subsets of manageable size out of large data sets. Applications range from exemplar-based clustering [8], to document [23, 7] and corpus summarization [33], to recommender systems [10, 9], just to name a few. A systematic way for data summariza-tion, used in all the aforementioned applications, is to turn the problem into selecting a subset of data elements opti-mizing a utility function that quantifies  X  X epresentativeness X  of the selected set. Often-times, these objective functions satisfy submodularity , an intuitive notion of diminishing re-turns (c.f., [27]), stating that selecting any given element earlier helps more than selecting it later. Thus, many prob-lems in data summarization require maximizing submodular set functions subject to cardinality constraints [14, 18], and big data means we have to solve this problem at scale.
Submodularity is a property of set functions with deep theoretical and practical consequences. The seminal result of Nemhauser et al. [27], that has been of great importance in data mining, is that a simple greedy algorithm produces so-lutions competitive with the optimal (intractable) solution. This greedy algorithm starts with the empty set, and iter-atively locates the element with maximal marginal benefit (increasing the utility the most over the elements picked so far). This greedy algorithm (and other standard algorithms for submodular optimization), however, unfortunately re-quires random access to the data. Hence, while it can easily be applied if the data fits in main memory, it is impractical for data residing on disk, or arriving over time at a fast pace.
In many domains, data volumes are increasing faster than the ability of individual computers to store them in main memory. In some cases, data may be produced so rapidly that it cannot even be stored. Thus, it becomes of crucial importance to process the data in a streaming fashion where at any point of time the algorithm has access only to a small fraction of data stored in primary memory. This approach not only avoids the need for vast amounts of random-access memory but also provides predictions in a timely manner based on the data seen so far, facilitating real-time analytics.
In this paper, we provide a simple streaming protocol, called Sieve-Streaming , for monotone submodular func-tion maximization, subject to the constraint that at most k points are selected. It requires only a single pass over the data, in arbitrary order, and provides a constant fac-tor 1 / 2  X   X  approximation to the optimum solution, for any  X  &gt; 0. At the same time, it only requires O (( k log k ) / X  ) memory (i.e., independent of the data set size), and pro-cesses data points with O ((log k ) / X  ) update time. To the best of our knowledge, it is the first streaming protocol that provides such strong theoretical guarantees if nothing but monotone submodularity is assumed. Our experimen-tal results demonstrate the effectiveness of our approach on several submodular maximization problems. We show that for problems such as exemplar-based clustering and ac-tive set selection in nonparametric learning, our approach leads to streaming solutions that provide competitive utility when compared with those obtained via classical methods, at a dramatically reduced fraction of the computational cost (about 1% in both the exemplar based clustering and active set selection applications).
Over the recent years, submodular optimization has been identified as a powerful tool for numerous data mining and machine learning applications including viral marketing [17], network monitoring [22], news article recommendation [10], nonparametric learning [14, 29], document and corpus sum-marization [23, 7, 33], network inference [30], and Determi-nantal Point Processes [13]. A problem of key importance in all these applications is to maximize a monotone submodu-lar function subject to a cardinality constraint (i.e., a bound on the number k of elements that can be selected). See [18] for a survey on submodular maximization.

Classical approaches for cardinality-constrained submod-ular optimization, such as the celebrated greedy algorithm of Nemhauser et al. [27], or its accelerated variants [24, 22, 3] require random access to the data. Once the size of the dataset increases beyond the memory capacity (typical in many modern datasets) or the data is arriving incrementally over time, neither the greedy algorithm, nor its accelerated versions can be used.

One possible approach to scale up submodular optimiza-tion is to distribute data to several machines, and seek par-allel computation methods. In particular, Mirzasoleiman et al. [25] in parallel to Kumar et al. [20] devised distributed al-gorithms for maximizing submodular functions, under some additional assumptions on the objective function: Lipschitz continuity [25] or bounded spread of the non-zero marginal gains [20]. Prior to [25] and [20], specific instances of dis-tributed submodular maximization, that often arise in large-scale graph mining problems, have been studied. In par-ticular, Chierichetti et al. [6] and later Blelloch et al. [5] addressed the MAX-COVER problem and provided a con-stant approximation to the centralized algorithm. Lattanzi et al. [21] addressed more general graph problems by intro-ducing the idea of filtering : reduce the size of the input in a distributed fashion so that the resulting, much smaller, problem instance can be solved on a single machine. Our streaming method Sieve-Streaming employs a similar fil-tering idea.

Another natural approach to scale up submodular opti-mization, explored in this paper, is to use streaming algo-rithms. In fact, in applications where data arrives at a pace that does not allow even storing it, this is the only viable op-tion. The first approach, Stream-Greedy , for submodular maximization on data streams is presented by [14]. How-ever, their approach makes strong assumptions about the way the data stream is generated, and unless their assump-tions are met, it is fairly easy to construct examples (and we demonstrate one in this paper) where the performance of their algorithm degrades quickly when compared to the optimum solution. Furthermore, the update time (compu-tational cost to process one data point) of their approach is  X ( k ), which is prohibitive for large k . We compare against their approach in this paper.

The work of [20] claims a multi-pass and a single-pass streaming algorithm. The claimed guarantees for the sin-gle pass algorithm depend on the maximum increase in the objective any element can offer (Thm. 27, [20]), while the multi-pass algorithm has a memory requirement depending on the data size n (Thm. 28, [20]). Our algorithm Sieve-Streaming lazily tracks the maximum valued element, en-abling a single pass streaming algorithm which does not de-pend on the maximum increase in the objective any element can offer. We cannot empirically compare against [20] as the details of both algorithms are omitted.

There is further related work on the submodular secretary problem [15, 4]. While also processing elements in a stream, these approaches are different in two important ways: (i) they work in the stronger model where they must either commit to or permanently discard newly arriving elements; (ii) they require random arrival of elements, and have a worse approximation ratio (  X  0 . 1 vs. 1 / 2  X  ). If elements arrive in arbitrary order, performance can degrade arbitrarily. Some approaches also require large (i.e., O ( n )) memory [15].
In this paper, we provide the first streaming algorithm for cardinality-constrained submodular maximization with 1) constant factor approximation guarantees, that 2) makes no assumptions on the data stream, 3) requires only a single pass, 4) only O ( k log k ) memory and 5) only O (log k ) update time, assuming 6) nothing but monotone submodularity.
We consider the problem of selecting subsets out of a large data set of size n , indexed by V (called ground set). Our goal is to maximize a non-negative set function f : 2 V  X  where, for S  X  V , f ( S ) quantifies the utility of set S , cap-turing, e.g., how well S represents V according to some ob-jective. We will discuss concrete instances of functions f in Section 4. A set function f is naturally associated with a discrete derivative , also called the marginal gain , where S  X  V and e  X  V , which quantifies the increase in utility obtained when adding e to set S . f is called mono-tone iff for all e and S it holds that 4 f ( e | S )  X  0. Further, f is submodular iff for all A  X  B  X  V and e  X  V \ B the following diminishing returns condition holds: That means, adding an element e in context of a set A helps at least as much as adding e in context of a superset B of A . Throughout this paper, we focus on such monotone submod-ular functions. For now, we adopt the common assumption that f is given in terms of a value oracle (a black box) that computes f ( S ) for any S  X  V . In Section 6, we will discuss the setting where f ( S ) itself depends on the entire data set V , and not just the selected subset S . Submodular functions contain a large class of functions that naturally arise in data mining and machine learning applications (c.f., [17, 22, 10, 14, 9, 23, 7, 33, 30, 18]).

The focus of this paper is on maximizing a monotone sub-modular function subject to a cardinality constraint, i.e., We will denote by S  X  the subset of size at most k that achieves the above maximization, i.e., the optimal solution, with value OPT = f ( S  X  ). Unfortunately, problem (3) is NP-hard, for many classes of submodular functions [11]. How-ever, a seminal result by Nemhauser et al. [27] shows that a simple greedy algorithm is highly effective. It starts with the empty set S 0 =  X  , and at each iteration i over the whole dataset, it chooses an element e  X  V maximizing (1), i.e., Let S g denote this greedy solution of size at most k . Nemhauser et al. prove that f ( S g )  X  (1  X  1 /e ) OPT , i.e., the greedy algo-rithm obtains a (1  X  1 /e )  X  0 . 63 approximation. For several classes of monotone submodular functions, it is known that (1  X  1 /e ) is the best approximation guarantee that one can hope for [26, 11, 19]. Moreover, the greedy algorithm can be accelerated using lazy evaluations [24, 22].

In many today X  X  data mining and machine learning appli-cations, running the standard greedy algorithm or its vari-ants [24, 22] is computationally prohibitive: either the data set does not fit in main memory on a single computer, pre-cluding random access, or the data itself arrives in a stream (e.g., activity logs, video streams), possibly in a pace that precludes storage. Hence, in such applications, we seek methods that can process quickly arriving data in a timely manner. Streaming algorithms with a limited memory avail-able to them (much less than the ground set) and limited processing time per item [12] are practical solutions in such scenarios. They access only a small fraction of data at any point in time and provide approximate solutions.

More formally, in context of streaming submodular max-imization, we assume that the ground set V = { e 1 ,...,e is ordered (in some arbitrary manner, w.l.o.g., the natural order 1 , 2 ,...,n ), and any streaming algorithm must process V in the given order. At each iteration t , the algorithm may maintain a memory M t  X  V of points, and must be ready to output a candidate feasible solution S t  X  M t of size at most | S |  X  k . Whenever a new point arrives from the stream, the algorithm may elect to remember it (i.e., insert it into its memory). However, if the memory exceeds a specified ca-pacity bound, it must discard elements before accepting new ones. The performance of a streaming algorithm is measured by four basic parameters:
Note that T can be bigger than n , if the algorithm makes multiple passes over the data.

The standard greedy algorithm (4) requires access to all elements of the ground set and hence cannot be directly ap-plied in the streaming setting. A naive way to implement it in a streaming fashion, when the data is static and does not increase over time, is to pass k times over the ground set and at each iteration select an element with the max-imum marginal gain. This naive approach will provide a (1  X  1 /e ) approximation at the price of passing many (i.e, k ) times over the data, using O ( k ) memory, O ( nk ) func-tion evaluations. Note that, if the data size increases over time (e.g., new elements are added to a log, video is be-ing recorded, etc.) we can no longer implement this naive approach. Moreover, the accelerated versions of the greedy algorithm do not provide any benefit in the streaming set-ting as the full ground set is not available for random access.
An alternative approach is to keep a memory of the best elements seen so far. For instance, we can start from the empty set S 0 =  X  . As long as no more than k elements e ,e 2 ,...,e t have arrived (or no more than k elements are read from the ground set), we keep all of them, i.e., S S t  X  1  X  X  e t } , for t  X  k . Then for each new data point e where t &gt; k , we check whether switching it with an element in S t  X  1 will increase the value of the utility function f . If so, we switch it with the one that maximizes the utility. Formally, if there exists an e  X  S t  X  1 such that f ( S t  X  1 { e } ) &gt; f ( S t  X  1 ), then we swap e and e t , setting S { e t }\{ e } . This greedy approach is the essence of Stream-Greedy of [14]. However, unless strong conditions are met, the performance of Stream-Greedy degrades arbitrarily with k (see Appendix).

Very recently, the existence of another streaming algo-rithm  X  Greedy-Scaling  X  was claimed in [20]. As nei-ther the algorithm nor its proof were described in the pa-per, we were unable to identify its running time in theory and its performance in practice. Based on their claim, if nothing but monotone submodularity is assumed, Greedy-Scaling has to pass over the dataset O (1 / X  ) times in or-der to provide a solution with  X / 2 approximation guarantee. Moreover, the required memory also increases with data as O ( kn  X  log n ). With a stronger assumption, namely that all (non-zero) marginals are bounded between 1 and  X , the exis-tence of a one-pass streaming algorithm with approximation guarantee 1 / 2  X  , and memory k/ log( n  X ) is claimed. Note that the above requirement on the bounded spread of the nonzero marginal gains is rather strong and does not hold in certain applications, such as the objective in (6) (here,  X  may increase exponentially in k ).

In this paper, we devise an algorithm X  Sieve-Streaming  X  that, for any  X  &gt; 0, within only one pass over the data stream, using only O ( k log( k ) / X  ) memory, running time of at most O ( n log( k ) / X  ) produces a 1 / 2  X   X  approximate solu-tion to (3). So, while the approximation guarantee is slightly worse compared to the classical greedy algorithm, a single pass suffices, and the running time is dramatically improved. Moreover,  X  serves as a tuning parameter for trading accu-racy and cost.
We now discuss two concrete applications, with their sub-modular objective functions f , where the size of the datasets or the nature of the problem often requires a streaming solu-tion. We report experimental results in Section 7. Note that update time per new element and approximation guarantee. For
Greedy-Scaling we report here the performance guarantees update time and compare with them in our experiments. For many more data mining applications have been identified to rely on submodular optimization (e.g., [17, 22, 10, 9, 23, 33, 30]), which can potentially benefit from our work. Providing a comprehensive survey is beyond the scope of this article.
We start with a classical data mining application. Sup-pose we wish to select a set of exemplars, that best represent a massive data set. One approach for finding such exemplars is solving the k -medoid problem [16], which aims to mini-mize the sum of pairwise dissimilarities between exemplars and elements of the dataset. More precisely, let us assume that for the data set V we are given a distance function d : V  X  V  X  R such that d (  X  ,  X  ) encodes dissimilarity between elements of the underlying set V . Then, the k -medoid loss function can be defined as follows: By introducing an auxiliary element e 0 (e.g., = 0 , the all zero vector) we can turn L into a monotone submodular function [14]: In words, f measures the decrease in the loss associated with the set S versus the loss associated with just the auxiliary element. It is easy to see that for suitable choice of e imizing f is equivalent to minimizing L . Hence, the stan-dard greedy algorithm provides a very good solution. But the problem becomes computationally challenging when we have a large data set and we wish to extract a small subset S of exemplars. Our streaming solution Sieve-Streaming addresses this challenge.

Note that in contrast to classical clustering algorithms (such as k -means), the submodularity-based approach is very general: It does not require any properties of the distance function d , except nonnegativity (i.e., d (  X  ,  X  )  X  0). In par-ticular, d is not necessarily assumed to be symmetric, nor obey the triangle inequality.
Besides extracting representative elements for sake of ex-plorative data analysis, data summarization is a powerful technique for speeding up learning algorithms.

As a concrete example, consider kernel machines [31] (such as kernelized SVMs/logistic regression, Gaussian processes, etc.), which are powerful non-parametric learning techniques. The key idea in these approaches is to reduce non-linear problems such as classification, regression, clustering etc. to linear problems  X  for which good algorithms are available  X  in a, typically high-dimensional, transformed space. Cru-cially, the data set V = { e 1 ,...,e n } is represented in this transformed space only implicitly via a kernel matrix Hereby K e i ,e j is the similarity of item i and j measured via a symmetric positive definite kernel function. For example, a commonly used kernel function in practice where elements of the ground set V are embedded in a Euclidean space is the squared exponential kernel Many different kernel functions are available for modeling various types of data beyond Euclidean vectors, such as se-quences, sets, graphs etc. Unfortunately, when scaling to large data sets, even representing the kernel matrix K requires space quadratic in n . Moreover, solving the learn-ing problems (such as kernelized SVMs, Gaussian processes, etc.) typically has cost  X ( n 2 ) (e.g., O ( n 3 ) for Gaussian pro-cess regression).

Thus, a common approach to scale kernel methods to large data sets is to perform active set selection (c.f., [28, 32]), i.e., select a small, representative subset S  X  V , and only work with the kernel matrix K S,S restricted to this subset. The key challenge is to select such a representative set S .
One prominent procedure that is often used in practice is the Informative Vector Machine (IVM) [32], which aims to select a set S maximizing the following objective function where  X  is a regularization parameter. Thus, sets maximiz-ing f ( S ) maximize the log-determinant I +  X   X  2  X  S,S therefore capture diversity of the selected elements S . It can be shown that f is monotone submodular [32]. Note that similar objective functions arise when performing MAP inference in Determinantal Point Processes, powerful prob-abilistic models for data summarization [13].

When the size of the ground set is small, standard greedy algorithms (akin to (4)) provide good solutions. Note that the objective function f only depends on the selected ele-ments (i.e., the cost of evaluating f does not depend on the size of V ). For massive data sets, however, classical greedy algorithms do not scale, and we must resort to streaming.
In Section 7, we will show how Sieve-Streaming can choose near-optimal subsets out of a data set of 45 million vectors (user visits from Yahoo! Front Page) by only ac-cessing a small portion of the dataset. Note that in many nonparametric learning applications, data naturally arrives over time. For instance, the Yahoo! Front Page is visited by thousands of people every hour. It is then advantageous, or sometimes the only way, to make predictions with kernel methods by choosing the active set on the fly.
We now present our main contribution, the Sieve-Streaming algorithm for streaming submodular maximization. Our ap-proach builds on three key ideas: 1) a way of simulating the (intractable) optimum algorithm via thresholding, 2) guess-ing the threshold based on the maximum singleton element, and 3) lazily running the algorithm for different thresholds when the maximum singleton element is updated. As our final algorithm is a careful mixture of these ideas, we show-case each of them by making certain assumptions and then removing each assumption to get the final algorithm.
The key reason why the classical greedy algorithm for sub-modular maximization works, is that at every iteration, an element is identified that reduces the  X  X ap X  to the optimal solution by a significant amount. More formally, it can be seen that, if S i is the set of the first i elements picked by the greedy algorithm (4), then the marginal value 4 f ( e i +1 of the next element e i +1 added is at least ( OPT  X  f ( S Thus, our main strategy for developing our streaming al-gorithm is identify elements with similarly high marginal value. The challenge in the streaming setting is that, when we receive the next element from the stream, we must im-mediately decide whether it has  X  X ufficient X  marginal value. This will require us to compare it to OPT in some way which gives the intuition that knowing OPT should help.

With the above intuition, we could try to pick the first element with marginal value OPT /k . This specific attempt does not work for instances that contain a single element with marginal value just above OPT /k towards the end of the stream, and where the rest of elements with marginal value just below OPT /k appear towards the beginning of the stream. Our algorithm would have then rejected these ele-ments with marginal value just below OPT /k and can never get their value. But we can immediately observe that if we had instead lowered our threshold from OPT /k to some  X  OPT /k , we could have still gotten these lower valued ele-ments while still making sure that we get the high valued elements when  X  is reasonably large. Below, we use  X  = 1 / 2.
Our algorithm will be based on the above intuition. Sup-pose we know OPT up to a constant factor  X  , i.e., we have a value v such that OPT  X  v  X   X   X  OPT for some 0  X   X   X  1. The algorithm starts with setting S =  X  and then, after observ-ing each element, it adds it to S if the marginal value is at least ( v/ 2  X  f ( S )) / ( k  X  X  S | ) and we are still below the cardi-nality constraint. Thus, it  X  X ieves X  out elements with large marginal value. The pseudocode is given in algorithm 1
Proposition 5.1. Assuming input v to algorithm 1 sat-isfies OPT  X  v  X   X  OPT , the algorithm satisfies the following properties Algorithm 1 SIEVE-STREAMING-KNOW-OPT-VAL Input: v such that OPT  X  v  X   X  OPT 1: S =  X  2: for i = 1 to n do 4: S := S  X  X  e i } 5: return S
Algorithm 1 requires that we know (a good approxima-tion) to the value of the optimal solution OPT . However, obtaining this value is a kind of chicken and egg problem where we have to estimate OPT to get the solution and use the solution to estimate OPT . The crucial insight is that, in order to get a very crude estimate on OPT , it is enough to know the maximum value of any singleton element m = max e  X  V f ( { e } ). From submodularity, we have that This estimate is not too useful yet; if we apply Proposi-tion 5.1 directly with v = km and  X  = 1 /k , we only obtain the guarantee that the solution will obtain a value of OPT / 2 k .
Fortunately, once we get this crude upper bound k  X  m on OPT , we can immediately refine it. In fact, consider the following set At least one of the thresholds v  X  O should be a pretty good estimate of OPT , i.e there should exist at least some v  X  O such that (1  X  ) OPT  X  v  X  OPT . That means, we could run Algorithm 1 once for each value v  X  O , requiring mul-tiple passes over the data. In fact, instead of using multiple passes, a single pass is enough: We simply run several copies of Algorithm 1 in parallel, producing one candidate solution for each threshold v  X  O . As final output, we return the best solution obtained.

More formally, the algorithm proceeds as follows. It as-sumes that the value m = max e  X  V f ( { e } ) is given at the beginning of the algorithm. The algorithm discretizes the range [ m,k  X  m ] to get the set O . Since the algorithm does not know which value among O is a good estimate for OPT , it simulates Algorithm 1 for each of these values v : Formally it starts with a set S v for each v  X  O and after observing each element, it adds to every S v for which it has a marginal value of at least ( v/ 2  X  f ( S v )) / ( k  X  X  S v | ) and S the cardinality constraint. Note that | O | = O ((log k ) / ), i.e., we only need to keep track of O ((log k ) / ) many sets S v of size at most k each, bounding the size of the mem-ory M =  X  v  X  O S v by O (( k log k ) / ). Moreover, the update time is O ((log k ) / ) per element. The pseudocode is given in Algorithm 2.

Proposition 5.2. Assuming input m to Algorithm 2 sat-isfies m = max e  X  V f ( { e } ) , the algorithm satisfies the follow-ing properties Algorithm 2 SIEVE-STREAMING-KNOW-MAX-VAL Input: m = max e  X  V f ( { e } ) 1: O = { (1 + ) i | i  X  Z ,m  X  (1 + ) i  X  k  X  m } 2: For each v  X  O,S v :=  X  3: for i = 1 to n do 4: for v  X  O do 6: S v := S v  X  X  e i } 7: return argmax v  X  O n f ( S v )
While Algorithm 2 successfully removed the unrealistic requirement of knowing OPT , obtaining the maximum value m of all singletons still requires one pass over the full data set, resulting in a two-pass algorithm.

It turns out that it is in fact possible to estimate m on the fly, within a single pass over the data. We will need two ideas to achieve this. The first natural idea is to maintain an auxiliary variable m which holds the current maximum singleton element after observing each element e i and lazily instantiate the thresholds v = (1 + ) i ,m  X  (1 + ) i  X  k  X  m . Unfortunately, this idea alone does not yet work: This is be-cause when we instantiate a threshold v we could potentially have already seen elements with marginal value v/ (2 k ) that we should have taken for the solution corresponding to S v
The second idea is to instead instantiate thresholds for an increased range v = (1 + ) i ,m  X  (1 + ) i  X  2  X  k  X  m . It can be seen that when a threshold v is instantiated from this expanded set O , every element with marginal value v/ (2 k ) to S v will appear on or after v is instantiated.

We now state the algorithm formally. It maintains an aux-iliary variable m that holds the current maximum singleton element after observing each element e i . Whenever m gets updated, the algorithm lazily instantiates the set O deletes all thresholds outside O i . Then it includes the ele-ment e i into every S v for v  X  O i if e i has the marginal value solution among S v . We call the resulting algorithm Sieve-Streaming , and present its pseudocode in Algorithm 3, as well as an illustration in Figure 1.
 Algorithm 3 Sieve-Streaming 1: O = { (1 + ) i | i  X  Z } 2: For each v  X  O,S v :=  X  (maintain the sets only for the 3: m := 0 4: for i = 1 to n do 5: m := max( m,f ( { e i } )) 6: O i = { (1 + ) i | m  X  (1 + ) i  X  2  X  k  X  m } 7: Delete all S v such that v /  X  O i . 8: for v  X  O i do 10: S v := S v  X  X  e i } 11: return argmax v  X  O
Theorem 5.3. Sieve-Streaming (Algorithm 3) satisfies the following properties Figure 1: Illustration of Sieve-Streaming . Data arrives in any order. The marginal gain of any new data point is computed with respect to all of the sieves. If it exceeds the specific threshold of any sieve that does not yet meet the cardinality constraint, the point will be added. Otherwise it will be discarded. Sieve-Streaming ensures that the num-ber of sieves is bounded. Moreover, it can provide statistics about the data accumulated at any time by returning the elements of the sieve with maximum utility.
Note that the total computation cost of Sieve-Streaming is O n log k . This is in contrast to the cost of O ( nk ) for the classical greedy algorithm. Thus, not only does Sieve-Streaming require only a single pass through the data, it also offers an accuracy X  X erformance tradeoff by providing the tuning parameter . We empirically evaluate this trade-off in our experiments in Section 7. Further note that when executing Sieve-Streaming , some sets S v do not  X  X ill up X  (i.e., meet the cardinality constraint). The empirical per-formance can be improved  X  without sacrificing any guaran-tees  X  by maintaining a reservoir (cf., Sec. 6) of O ( k log k ) random elements, and augment the non-full sets S v upon termination by greedily adding elements from this reservoir.
In the previous sections, we have effectively assumed a so called black-box model for the function evaluations: given any set S , our algorithm Sieve-Streaming can evaluate f ( S ) independently of the ground set V . I.e., the black-box implementing f only needs access to the selected elements S , but not the full data stream V . In several practical settings, however, this assumption is violated, meaning that the util-ity function f depends on the entire dataset. For instance, in order to evaluate (5) we need to know the loss function over the entire data set. Fortunately, many such functions (including (5)) share an important characteristic; they are additively decomposable [25] over the ground set V . That means, they can be written as where f e ( S ) is a non-negative submodular function. Decom-posability requires that there is a separate monotone sub-modular function associated with every data point e  X  V and the value of f ( S ) is nothing but the average of f e For the remaining of this section, we assume that the func-tions f e (  X  ) can all be evaluated without accessing the full ground set. We define the evaluation of the utility function f restricted to a subset W  X  V as follows: Hence f W ( S ) is the empirical average of f w.r.t. to set W . Note that as long as W is large enough and its elements are randomly chosen, the value of the empirical mean f will be a very good approximation of the true mean f ( S ).
Proposition 6.1. Assume that all of f e ( S ) are bounded and w.l.o.g. | f e ( S ) |  X  1 . Moreover, let W be uniformly sampled from V . Then by Hoeffding X  X  inequality we have There are at most | V | k sets of size at most k . Hence, in order to have the RHS  X   X  for any set S of size at most k we simply (using the union bound) need to ensure As long as we know how to sample uniformly at random from a data stream, we can ensure that for decomposable functions defined earlier, our estimate is close (within the error margin of  X  ) to the correct value. To sample randomly, we can use a reservoir sampling technique [35]. It creates a reservoir array of size | W | and populates it with the first | W | items of V . It then iterates through the remaining elements of the ground set until it is exhausted. At the i -th element where i &gt; | W | , the algorithm generates a random number j between 1 and i . If j is at most | W | , the j -th element of the reservoir array is replaced with the i th element of V . It can be shown that upon finishing the data stream, each item in V has equal probability of being chosen for the reservoir.
Now we can devise a two round streaming algorithm, which in the first round applies reservoir sampling to sample an evaluation set W uniformly at random. In the second round, it simply runs Sieve-Streaming and evaluates the utility function f only with respect to the reservoir W .
 Algorithm 4 Sieve-Streaming + Reservoir Sampling 1: Go through the data and find a reservoir W of size (8) 2: Run Sieve-Streaming by only evaluating f W (  X  )
Theorem 6.2. Suppose Sieve-Streaming uses a valida-probability 1  X   X  , the output of Alg 4 will be a set S of size at most k such that This result shows how Sieve-Streaming can be applied to decomposable submodular functions if we can afford enough memory to store a sufficiently large evaluation set. Note that the above approach naturally suggests a heuristic one-pass algorithm for decomposable functions: Take the first greedy algorithm on it to produce a candidate solution. Then process the remaining stream, updating the validation set via reservoir sampling, and applying Sieve-Streaming , us-ing the current validation set in order to approximately eval-uate f . We report our experimental results for the exemplar-based clustering application using an evaluation set W .
In this section, we address the following questions: To this end, we run Sieve-Streaming on the two data min-ing applications we described in Section 4: exemplar-based clustering and active set selection for nonparametric learn-ing. For both applications we report experiments on large datasets with millions of data points. Throughout this sec-tion we consider the following benchmarks: In all of our experiments, we stop the streaming algorithms if the utility function does not improve significantly (rela-tive improvement of at least  X  for some small value  X  &gt; 0). In all of our experiment, we chose  X  = 10  X  5 . This way, we can compare the performance of different algorithms in terms of computational efforts in a fair way. Throughout this section, we measure the computational cost in terms of the number of function evaluations used (more precisely, number of oracle queries). The advantage of this measure is that it is independent of the concrete implementation and platform. However, to demonstrate that the results remain almost identical, we also report the actual wall clock time for the exemplar-based clustering application. The random selection policy has the lowest computational cost among the streaming algorithms we consider in this paper. In fact, in terms of function evaluations its cost is one; at the end of the sampling process the selected set is evaluated once. To implement the random selection policy we can employ the reservoir sampling technique discussed earlier. On the other end of the spectrum, we have the standard greedy al-gorithm which makes k passes over the ground set, providing typically the best solution in terms of utility. Since it is com-putationally prohibitive we cannot run it for the large-scale datasets. However, we also report results on a smaller data set, where we compare against this algorithm.
For the active set selection objective described in Sec-tion 4.2, we chose a Gaussian kernel with h = 0 . 75 and  X  = 1. For the small-scale experiments, we used the Parkin-sons Telemonitoring dataset [34] consisting of 5,875 bio-medical voice measurements with 22 attributes from peo-ple with early-stage Parkinson X  X  disease. We normalized the vectors to zero mean and unit variance. Fig. 2a compares the performance of Sieve-Streaming to the benchmarks for a fixed active set size k = 20. The computational costs of all algorithms, as well as their acquired utilities, are nor-malized to those of the standard greedy. As we can see, Sieve-Streaming provides a solution close to that of the standard greedy algorithm with a much reduced computa-tional cost. For different values of k , Fig. 2d shows the per-formance of all the benchmarks. Again, Sieve-Streaming operates close to Stream-Greedy and (lazy) greedy.
 For our large-scale scale experiment, we used the Yahoo! Webscope data set consisting of 45,811,883 user visits from the Featured Tab of the Today Module on the Yahoo! Front Page [2]. For each visit, the user is associated with a feature vector of dimension six. Fig. 2c compares the performance of
Sieve-Streaming to the benchmarks for a fixed active set size k = 100. Since the size of the data set is large, we cannot run the standard (or lazy) greedy algorithm. As a consequence, computational costs and utilities are normal-ized to those of the Stream-Greedy benchmark. For such a large dataset, the benefit of using Sieve-Streaming is much more pronounced. As we see, Sieve-Streaming provides a solution close to that of Stream-Greedy while having several orders of magnitude lower computational cost. The performance of all algorithms (expect the standard and lazy greedy) for different values of k is shown in Fig. 2f. Our exemplar-based clustering experiment involves Sieve-Streaming applied to the clustering utility function (de-scribed in Section 4.1) with the squared Euclidean distance, namely, d ( x,x 0 ) = k x  X  x 0 k 2 . We run the streaming algo-rithms on the Census1990 dataset [1]. It consists of 2,458,285 data points with 68 attributes. We compare the performance of
Sieve-Streaming to the benchmarks as well as the clas-sical online k -means algorithm (where we snap each mean to the nearest exemplar in a second pass to obtain a subset S  X  V ). Again, the size of the dataset is too large to run the standard greedy algorithm. As discussed in Section 6, the clustering utility function depends on the whole data set. However, it is decomposable, thus an evaluation set W can be employed to estimate the utility of any set S based on the data seen so far. For our experiments, we used reservoir sampling with | W | = 1 / 10 | V | .

Fig 2b shows the performance of Sieve-Streaming com-pared to the benchmarks for a fixed active set size k = 5. The computational costs of all algorithms, as well as their acquired utilities, are normalized to those of Stream-Greedy . We did not add the performance of online k -means to this figure as online k -means is not based on submodular function maximization. Hence, it does not query the cluster-ing utility function. As we observe again, Sieve-Streaming provides a solution close to that of Stream-Greedy with substantially lower computational cost. For different values of k , Fig. 2e shows the performance of all the benchmarks. To compare the computational cost of online k -means with the benchmarks that utilize the clustering utility function, we measure the wall clock times of all the methods. This is reported in Table 2. We see again that the utility of Sieve-Streaming is comparable to Stream-Greedy and online k -means with much lower wall clock time. We have developed the first efficient streaming algorithm X  Sieve-Streaming  X  X or cardinality-constrained submodular maximization. Sieve-Streaming provides a constant fac-tor 1 / 2  X   X  approximation guarantee to the optimum solu-tion and requires only a single pass through the data and memory independent of the data size. In contrast to pre-vious work, which makes strong assumptions on the data stream V or on the utility function (e.g., bounded spread of marginal gains, or Lipschitz continuity), we assumed nothing but monotonicity and submodularity. We have also demon-strated the effectiveness of our approach through extensive large scale experiments. As shown in Section 7, Sieve-Streaming reaches the major fraction of the utility function with much (often several orders of magnitude) less computa-tional cost. This property of Sieve-Streaming makes it an appealing and sometimes the only viable method for solv-ing very large scale or streaming applications. Given the importance of submodular optimization to numerous data mining and machine learning applications, we believe our results provide an important step towards addressing such problems at scale.

Acknowledgments. This research was supported in part by NSF AF-0910940, SNF 200021-137971, DARPA MSEE FA8650-11-1-7156, ERC StG 307036, a Microsoft Faculty Fellowship and an ETH Fellowship. [1] Census1990, UCI machine learning repository, 2010. [2] Yahoo! academic relations. r6a, yahoo! front page [3] A. Badanidiyuru and J. Vondr  X ak. Fast algorithms for [4] M. Bateni, M. Hajiaghayi, and M. Zadimoghaddam. [5] G. E. Blelloch, R. Peng, and K. Tangwongsan.
 [6] F. Chierichetti, R. Kumar, and A. Tomkins.
 [7] A. Dasgupta, R. Kumar, and S. Ravi. Summarization [8] D. Dueck and B. J. Frey. Non-metric affinity [9] K. El-Arini and C. Guestrin. Beyond keyword search: [10] K. El-Arini, G. Veda, D. Shahaf, and C. Guestrin. [11] U. Feige. A threshold of ln n for approximating set [12] M. M. Gaber, A. Zaslavsky, and S. Krishnaswamy. [13] J. Gillenwater, A. Kulesza, and B. Taskar.
 [14] R. Gomes and A. Krause. Budgeted nonparametric [15] A. Gupta, A. Roth, G. Schoenebeck, and K. Talwar. [16] L. Kaufman and P. J. Rousseeuw. Finding groups in [17] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing [18] A. Krause and D. Golovin. Submodular function [19] A. Krause and C. Guestrin. Near-optimal nonmyopic [20] R. Kumar, B. Moseley, S. Vassilvitskii, and [21] S. Lattanzi, B. Moseley, S. Suri, and S. Vassilvitskii. [22] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, [23] H. Lin and J. Bilmes. A class of submodular functions [24] M. Minoux. Accelerated greedy algorithms for [25] B. Mirzasoleiman, A. Karbasi, R. Sarkar, and [26] G. L. Nemhauser and L. A. Wolsey. Best algorithms [27] G. L. Nemhauser, L. A. Wolsey, and M. L. Fisher. An [28] C. E. Rasmussen and C. K. I. Williams. Gaussian [29] C. Reed and Z. Ghahramini. Scaling the indian buffet [30] M. G. Rodriguez, J. Leskovec, and A. Krause.
 [31] B. Sch  X  olkopf and A. Smola. Learning with Kernels: [32] M. Seeger. Greedy forward selection in the informative online k -means Stream-Greedy 152  X  10 6 153  X  10 6 Table 2: Performance of Sieve-Streaming with respect to Stream-Greedy [33] R. Sipos, A. Swaminathan, P. Shivaswamy, and [34] A. Tsanas, M. A. Little, P. E. McSharry, and L. O. [35] J. Vitter. Random sampling with a reservoir. ACM
Proof of Proposition 5.1. We will first prove by in-duction that after adding | S | elements to S the solution will satisfy the following inequality The proof is by induction. The base case f (  X  )  X  0 is easy to see. Assume by induction that equation 9 holds for set S and we add element e . Then we know by the condition of the equation 9 into this equation and simplifying we get the
The rest of the proof is quite simple. There are two cases.  X  Case 1: At the end of the algorithm S has k elements.  X  Case 2: At the end of the algorithm | S | &lt; k . Then let
Proof of Proposition 5.2. The proof directly follows from Theorem 5.1 and the fact that there exists a v  X  O such that (1  X  ) OPT  X  v  X  OPT .

Proof of Theorem 5.3. First observe that when a thresh-old v is instantiated any element with marginal value at least v/ (2 k ) to S v appears on or after v is instantiated. This is because if such an element e i appeared before v was instanti-ated then v  X  O i and would have been instantiated when e appeared. Then the proof directly follows from Theorem 5.2 and the fact that Algorithm 3 is the same as Algorithm 2 with a lazy implementation.

Proof of Theorem 6.2. The proof closely follows the proof of the algorithm when we evaluate the submodular function exactly from Proposition 5.1, 5.2 and Theorem 5.3. From Proposition 6.1 we get that for any set S , | f W ( S )  X  f ( S ) | X  k . Hence, as we are taking k elements, the error for the solution after taking each element adds up and we get f ( S )  X  (1 / 2  X  ) OPT  X  k  X  k which is the desired result. We show that there exists a family of problem instances of streaming submodular maximization under a k cardinal-ity constraint, where it holds for the solution S produced by Stream-Greedy after one pass through the data, that f ( S ) / OPT  X  1 /k . For this claim, take the weighted coverage of a collection of sets: Fix a set X and a collection V of sub-sets of X . Then for a subcollection S  X  V , the monotone submodular function f is defined as Here, w (  X  ) is the weight function that assigns positive weights to any element from X . Now, let us assume that X is the set of natural numbers and elements arrive as follows: Let &lt;&lt; 1. We define the weights as Then it is clear that Stream-Greedy skips the non-singleton sets as they do not provide any benefit. In contrast, the opti-mum solution only consists of those sets. Now, after observ-ing O ( k 2 ) elements from the above data stream, the ratio between the solution provided by Stream-Greedy and the optimum algorithm decays as O (1 /k ).
