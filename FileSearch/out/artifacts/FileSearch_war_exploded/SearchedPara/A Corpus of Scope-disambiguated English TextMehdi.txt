 Since the early days of natural language unde r-standing (NLU), quantifier scope disambigu a tion has been an extremely hard task. Therefore, early NLU systems either devised some mech a nism for leaving the semantic representation u n derspecified (Woods 1978, Hobbs and Shieber 1987), or tried to assign scoping to se n tences based on heuristics ( VanLehn 1978, Moran 1 988, Alshawi 1992). There has been a lot of work since then on deve l-oping frameworks for scope -underspecified sema n-tic representations (A l shawi and Crouch 1992, Bos 1996, Copestake et al. , 2001 , Egg et al., 2001 ). The motivation of most recent forma l isms is to develop a constraint -based framework where you can i n-crementally add constraints to filter out u n wanted scopings. Ho w ever, almost all of these formalisms are based on hard constraints, which have to be satisfied in every reading of the se n tence. It seems that the story is different in pra c tice. Most of the constraints one can hope for ( imposed by di s-course , pragmatics, word know l edge , etc.) are soft constraints, that is they define a pre fe r ence over the possible readings of a sentence. As a r e sult, statistical methods seem to be well suited for scope disa m biguation.
 tensive work on statistical techniques in natural language processing, there has not been much work on scope disambiguation (see section 6 for a review). In addition, as discussed later, this work is very restricted. It considers se n tences with only two quantifiers, where the quant i fiers are picked from a predefined list. For example, it ignores d e-finites, bare singulars/plurals, and proper nouns , as well as negations and other scopal operato r s . cal scope disambiguation is the lack of a compr e he n sive scope -disambiguated corpus. In fact, there is not even a standard test set for evaluation pu r poses. The re a son behind this latter fact is simple. Scope disambiguation is very hard even for h u mans. In fact, our own early effort to annotate part of the Penn Treebank with full scope inform a tion soon proved to be too ambitious. many challenging phenomena in scope disa m-biguation, while keeping the scope disambiguation fairly intuitive. This helps us to build the fir st moderately sized corpus of natural language text with full scope information. By fully scoping a sentence, we mean to label the scope interaction between every two scopal elements in that se n-tence. We scope all scope -bearing NPs (quantified or not), neg ations, logical/modal operators, and other sentential adverbials. We also annotate pl u-rals with their distributive vs. colle c tive readings. In addition, we label sentences with corefe r ence relations because they affect the sc ope interaction b e tween NPs. The domain is the description of tasks about edi t-ing plain text files; in other words, a natural la n-guage interface for text editors such as Linux SED, AWK, or EMACS programs. Figure (1) gives some sentences from the co r pus. This domain has several properties that make it a great choice for a first effort to build a comprehensive scope -disambiguated corpus.
 shown in the examples, the domain ca r ries many quantified NPs. Als o, scopal operators such as n e-gation, and logical operators occur pretty often in the domain. Second, scope disambiguation is crit i-cal for deep understanding in this domain. Third, scoping is fairly i n tuitive, because a conscious knowledge of scoping is required in o r der to be able to accomplish the explained task. This is e x-actly the key property of this domain that makes building a comprehe n sive scope -disambiguated corpus feas i ble. 3.1 The core corpus The core part of the corpus has been gat h ered from three different resources, each making up roughly one third of the core corpus.
One liners : These are help documents found on the web for Linux command -line text editors such as SED and AWK, giving a description of a task plus one line of code perfor m ing the task .
Online tutorials : M any other online tutorials on using command -line editors and regular expre s-sions exist . Sentences were manually extracted from exa m ples a nd exercises in these tutorials.  X 
Computer science graduate students : These are the sentences provided by CS graduate students d e scribing some of the routine text editing tasks they often do. The sentences have been provided by both n a tive and non -native En glish speakers. 3.2 Expanding corpus with crowd sourcing The core corpus was used to get more sentences using crowd sourcing. We provided i n put/output (I/O) examples for each task in the core co r pus, and asked the workers on Mechanical Turk to pr o-vide the description of the task based on the I/O example(s) . Figure (2) shows an example of two I/O pair s given to the workers in order to get the description of a si n gle ta sk . The reason for using two I/O pairs (i n stead of only one) is that there is almost always a trivial descri p tion for a single I/O pair . Even with two I/O pair s , we sometimes get the description of a diffe r ent task, which happens to work for the both pairs . For example the original d e scription for the task given in figure (2) is: The following descriptions are provided by three workers based on the given input/output texts: (3) gives the description of a different task , but it works for the given I/O pairs . This is not a pro b lem for us, but actually a case that we would prefer to ha p pen, because this way, we not only get a variety of sentences defining the same task, but also obtain d e scriptions of new tasks. We can add these new tasks to the core co r pus, label them with new I/O pairs and hence expand the corpus in a bootstra p-ping fas h ion.
 ten quite noisy, therefore all sentences are r e-viewed manually and tagged with di f ferent categories (e.g. paraphrase of the original descri p-tion, wrong but coherent descri p tion, etc.). 3.3 Pre -processing the corpus The corpus is tokenized and parsed using the Sta n-ford PCFG parser (Klein and Manning 200 3). We guide the parser by giving su g gestions on part -of -speech (POS) tags based on the gold standard POS tags provided for some class es of words such as verbs . Shallow NP chunks and negations are aut o-matically extracted from the parse trees and i n-dexed. The resulting NP -chunked sentences are then reviewed manually , first to fix the chunking e r rors, hence providing gold standard chunks, and second, to add chunks for other scopal operators such as sentential adverbials since the above aut o-mated a p proach will not extract those. Figure (3) shows the examples in figure (1) after chunking. As shown in these examples, NP chunks are i n-dexed by numbers, negation by the letter  X  X  X  fo l-lowed by a number and all oth er scopal operators by the le t ter  X  X  X  followed by a number. The chunked sentences are given to the ann o tators for scope annotation. Given a pair of chunks i and j , three kinds of relation could hold between them. 
Outscoping constraints: represented as ( i&gt;j) , which means chunk i outscopes (i.e. has a wider scope over) chunk j . 
Coreference relations: represented as (i=j). This could be between a pronoun and its antecedent or between two nouns . 1
No scope interaction: If a pai r is left unscoped, it means that either there is no scope interaction b e tween the chunks, or switching the order of the chunks results in a logically equivalent fo r mula . The overall scoping is represented as a list of semicolon -separated co n straints. The annotators are allowed to cascade constraints to form a more concise represe nt a tion (see Figure 3). 4.1 Logical equivalence vs. intuitive scoping Our early experiments showed that a main source of inter -annotator disagreement are pair s of chunks for which, both orderings are logically equivalent (e.g. two existentials or two universals ) , but an a n-notator may label them with outscoping co n straints based on his/her in t u i tion. It turns out that the a n-notator s X  intuitions are not co n sistent in these cases. Even a single annotator does not remain consistent throughout the data in such cases . A l-though it does not make any difference in logic, this shows up as inter -annotator disagreement . In order to prevent this, annotators were asked to re c-ognize these cases and leave them u n scoped. 4.2 Plurals Plurals, in general, introduce a major source of complexity both in formal and computational s e-mantics ( Link 1997 ). From a scope  X  disambiguation point of view, the main issue with plurals come from the fact that they carry two po s-sible kinds of readings: collective vs. distributive . We trea t plurals as a set of i n dividuals and assume that the index of a plural NP refers to the set (co l-lective reading). However, we also assume that every plural potentially carries an implicit unive r-sal quantifier ranging over all el e ments in the set. We represent this implicit universal with id ( X  d  X  for distrib u tive) where i is the index of the plural NP. I t is important to notice that while most theoretical papers talk about the collectivity vs. distributi v ity distinction at the sentence level, for us the right treatment is to make this distinction at the co n-straint level. That is, a plural may have a collective reading in one co n straint but a distributive reading in another, as shown in e x ample 2 in figure (3). 4.3 Other challenges of scope annotation In spite of choosing a specific domain with fairly intuitive quantifier scop ing, the scope annotation has been a very challenging job. There are several m a jor sources of difficulty in scope annotation. First, there has not been much work on corpus -based study of quantifier scoping. Most work on quant i fier scoping focuses on sc oping phenomena , which may be interesting from theoretical perspe c-tive, but do not occur very often in practice . Ther e-fore many challenging practical phenomena remain unexplored. During annotation of the corpus , we e n countered a lot of these phenomena, which we have tried to generalize and find a re a sonable treatment for. Second, other sources of ambiguity are likely to show up as scope disagreement. F i-nally , very often the di s agreement in scoping does not result from the different interpr e tations of the sentence, but the different represent a tions of the same interpretation. In writing the annotation scheme, extreme care ha s been taken to prevent these spurious disagre e ments. Technical details of the annotation scheme are b e yond the scope of this paper. We leave those for a longer paper . The current corpus contains around 500 se n tences in the core level and 2000 s entences a c quired from crowd sourcing. The number of scopal terms per sentence is 3. 9 , out of which 95% are NPs and the rest are scopal operators. Table (1) shows the pe r-centage of different types of NP in the c o r pus . out of which a hundred sentences have been ann o-tated by three annotators in order to measure the inter -annotator agreement ( IAA ). Two of the ann o-tators are native English speakers and the third is a non -n ative speaker who is fluent in English . All three have some background in lingui s tics. 5.1 Inter -annotator agreement Although coreference relations were labeled in the corpus, we do not incorporate them in calculating IAA. This is because, annotating corefe rence rel a-tions is much easier than scope disambiguation, so incorporating them favors toward higher IAAs, which may be decei v ing. Furthermore previous work only considers scope relations and hence we do the same in order to have a fair comparison.
 We repr esent each scoping using a directed graph over the chunk indices. For every outscoping rel a-tion i&gt;j , node i is connected to node j by the d i-rected edge (i,j) . For example, figure (4a) represents the scoping in (5). Note that the directed graph must be a DAG (d i-rected acyclic graph), otherwise the scoping is not valid. In order to be able to measure the similarity o f two DAGs corresponding to two different sco p-ings of a single sentence , we borrow the notion of transitive closure from graph theory. The trans i tive cl o sure (TC) of a directed graph G=(V,E) is the graph G + =(V,E + ) , where E + is defined as fo l lows: Given the TC graph of a scoping, every pair (i,j) , where i precedes j in the sentence , has one of the following three labels: A pair is considered a match between two sco p-ings, if it has the same label in both. We define the metrics at two levels, co n straint level and sentence level . At constraint level, every pair of chunks in every sentence is considered one instance . At se n-tence level, every sentence is treated as an i n-stance. A sentence counts as a match if and only if every pair of chunks in the sentence has the same label in both scopings. Un like previous work (se c-tion 6) where there is a strong skew in label distr i-bution, in our corpus the labels are almost evenly distributed , each consisting around 33% of the i n-stances . We use Cohen X  X  kappa score for mu l tiple annotators ( Davies &amp; Fleiss 19 82 ) to measure IAA. T a ble (2) reports the kappa score.
 retical purposes, but an easier metric could be d e-fined which works fine for most practical pu r poses. For example, if the target language is first order logic with generalized quantifiers , the rel a tive scope of the chunks labeled NI does not affect the i nterpr e tation . 2 Therefore, we define a new version of observed agreement in which we co n sider a pair a match if it is labeled NI in one scoping or a s-signed the same l a bel in both scopings. Table (2) reports the IAA based on the latter sim ilarity measure , called  X  -EZ . To the best of our knowledge, there have been three major efforts on building a scope -disambiguated corpus for statistical scope disa m-biguation , among which Hig gins and Sadock (2003 ) is the mos t comprehensive . Their corpus co n sists of 8 9 0 sentences from the Wall Street journal section of the Penn Tre e bank. They pick sentences containing exactly two quantif i ers from a pre defined list. This list does not include def i nites, indefinites, or bare singulars/plurals. Every se n-tence is labeled with one of the three labels corr e spon d ing to the first quantifier having wide -scope, the second quantifier having wide scope, or no scope interaction b e tween the two . They achieve an I AA of 52% on this task . The majority of se n tences in their corpus (more than 60%) have been l a beled with no scope interaction. to provide scope -disambiguated data. They pick a set of se n tences from LSAT and GR E logic games, which again contain only two quantifiers from a limited list of quantifiers. Their corpus consists of 305 sentences. In around 70% of these se n tences, the first quantifier has wide scope. A major pro b-lem with this data is that the sentences are artif i-cially co n structed for the LSAT and GRE tests. study the usage of pra g matic knowledge in finding the intended scoping of a sentence . Their labeled data set consists of 46 sentences, e x tracted from Web1Tgram (from Google, Inc) and hence is open -domain . The corpus consists of short se n tences with two specific quantifiers: Every and A . All se n-tences share the same syntactic structure, an active voice English se n tence of the form (S (NP (V (NP | PP)))) . In fact, they try to isolate the e f fect of pragmatic knowledge on scope disambigu a tion . We have constructed a comprehensive scope  X  disambiguated co r pus of English text within the doma in of editing plain text files. The domain ca r-ries many scope interactions. Our work does not put any r e striction on the type or the number of scope -bearing el e ments in the sentence. We achieve the IAA of 75% on this task. Previous work focuses on annotati ng the rel a tive scope of two NPs per sentence, while i g noring the complex scope -bearing NPs such as definites and indef i-nites, and achieves the IAA of 52%. out of which 500 sentences have already been a n-notate d. Our goal is to expand the corpus up to twice in size . 20% of the corpus will be annotat e d and the rest will be left for the purpose of semi -supervised learning. Since world know l edge plays a major role in scope disambiguation, we believe that leveraging unlabeled domain specific data in order to extract lexi cal information is a promising a p proach for scope disambiguation. We hop e that availabi l ity of this corpus motivates more research on statistical scope di s ambiguation. This work was supported in part by grants from the National Scie nce Foundation (IIS -1012205) and The Office of Naval Research (N000141110417).

