 He  X  le ` ne Bonneau-Maynard  X  Matthieu Quignard  X  Alexandre Denis Abstract The aim of the French M EDIA project was to define a protocol for the evaluation of speech understanding modules for dialog systems. Accordingly, a corpus of 1,257 real spoken dialogs related to hotel reservation and tourist infor-mation was recorded, transcribed and semantically annotated, and a semantic attribute-value representation was defined in which each conceptual relationship was represented by the names of the attributes. Two semantic annotation levels are distinguished in this approach. At the first level, each utterance is considered separately and the annotation represents the meaning of the statement without taking into account the dialog context. The second level of annotation then corresponds to the interpretation of the meaning of the statement by taking into account the dialog context; in this way a semantic representation of the dialog context is defined. This paper discusses the data collection, the detailed definition of both annotation levels, and the annotation scheme. Then the paper comments on both evaluation campaigns which were carried out during the project and discusses some results.
 Keywords Dialog system Speech understanding Corpus Annotation Evaluation 1 Introduction The assessment of a dialog system is complex. This is partly due to the high integration factor and tight coupling between the various modules present in any spoken language dialog system (SLDS), for which today, no commonly accepted reference architecture exists. The other major difficulty stems from the dynamic nature of dialog. Hence most SLDS evaluations up to now have either tackled the system as a whole, or have measurements based on dialog-context-free information.
The European DISC project (Giachim et al. 1997 ) has collected a systematic list of bottom-up evaluation criteria, each corresponding to a partially ordered list of properties likely to be encountered in any SLDS. Although the DISC project results are quite extensive and are presented in an homogeneous way, they do not provide a direct answer to the problems posed by SLDS evaluation; their contribution lies more at the specification level. Moreover, although the approach and goals of the European EAGLES (King et al. 1996 ) project were different, one could make much the same remark about the results of the speech evaluation work group (Gibbon et al. 1997 ). The MADCOW (Multi Site Data Collection Working group) coordi-nation group set up in the USA by ARPA in the context of the ATIS (Air Travel Information Services) task to collect corpora, was the first to propose a common infrastructure for SLDS automatic evaluation (Hirschman 1992 ), this also addressed the problem of language understanding evaluation. The evaluation paradigm is based on system answer comparison-list of possible flights based on user constraints X  X o a pair of minimal and maximal reference answers. Unfortunately no direct diagnostic information can be produced, since understanding is estimated by gauging the distance from the system answer to the pair of reference answers. In ATIS, the protocol was only applied to context free sentences. It is relatively objective and generic because it relies on counts of explicit information and allows for a certain variation in answers.

PARADISE (Walker et al. 1998 ) can be seen as a sort of meta-paradigm that correlates objective and subjective measurements. Its grounding hypothesis states that the goal of any SLDS is to achieve user-satisfaction, which in turn can be predicted through task success and various interaction costs. With the help of the kappa coefficient, (Carletta 1996 ) proposes to represent the dialog success independently from the intrinsic task complexity, thus opening the way to generic task comparative evaluation. PARADISE has been used in the COMMUNICATOR project (Walker et al. 2001 , 2002 ), and has made it possible to evaluate SLDS performances with a series of domain independent global measures which can be automatically extracted from the log files of the dialogs.

The M EDIA project addresses only a part of the SLDS evaluation problem, using a paradigm for evaluating the context-sensitive understanding capability of any SLDS. The paradigm is based on test sets extracted from real corpora, and has three main advantages: it is generic, contextual and it offers diagnostic capabilities. Here genericity is envisaged in a context of information dialogs access. The diagnostic aspect is important in order to determine the different qualities of the systems under test. The contextual aspect of evaluation is a crucial point since dialog is dynamic by nature. The first step (Sect. 3 ) was dedicated to the definition and the collection of the M
EDIA corpus of French dialogs for the chosen task (tourist information). During the second step, the common semantic representation was defined. A dedicated annotation tool 1 was developed allowing the semantic annotation of the corpus. The literal annotation of the corpus is described in Sect. 4 . The definition of a semantic representation of the context is then given in Sect. 5 . Two evaluation campaigns were performed in the project using the proposed paradigm; Sects. 6.1 and 6.2 discuss them in detail. 2 The M EDIA project and consortium 2.1 Motivations In broad outline, SLDSs are composed of different modules for speech recognition, for natural language understanding, and for dialog management and generation. They usually include an explicit understanding model to represent the semantic level. The semantic interpretation can be decomposed into two steps. The first step consists of providing a semantic representation of an utterance (the literal semantic representation ) without taking into account the rest of the dialog (see the ATIS project). The literal representation is then reconsidered in a second step by taking into account the dialog context, thereby making it possible to solve inter-query references and providing the contextual semantic representation of the utterance.
Previous experiments with the PARADISE paradigm (Bonneau-Maynard et al. 2000 ) have shown that contextual understanding is strongly connected to user satisfaction and therefore to the overall quality of the dialog system. The aim of French Technolangue E VALDA -M EDIA project (referred to as M EDIA ) was to focus the quality evaluation on SDLS interpretation modules, for both literal and contextual understanding tasks.

The evaluation paradigm is based on the use of test suites from real-world corpora, a common semantic representation and common metrics. The evaluation environment relies on the assumption that, for database query systems, it is possible to construct a common semantic representation to which each system is capable of converting its own internal representation. The chosen semantic representation is generic. Most attributes are domain-independent so that the representation has been already used for other domains (Lefe  X  vre et al. 2002 ) or for other languages (Bonneau-Maynard et al. 2003 ) in the case of the IST-A MITIE  X  S project. Thanks to the precision of the semantic representation (which notably includes explicit represen-tation of references), selective evaluation on utterances including particular linguistic difficulties can be performed, as is described is Sect. 6.1.3 .

In a way, the M EDIA evaluation paradigm complements evaluation programs centered on performance evaluation with global measures. The global evaluations perform the comparison of systems on logs of dialogs, which is obviously of a great interest. However, specific recordings are needed to perform the evaluation of each system, which is known to be costly. On the other hand, the M EDIA paradigm performs the comparison of the systems on the same data and enables the evaluation on specific difficulties. New approaches can also be tested without recording new dialogs.

Finally, the objective of the M EDIA project is not only to give the scientific community the means to perform comparative evaluations of understanding modules, but also to offer the possibility to share corpora and define representations and generic common metrics. 2.2 The M EDIA consortium Participants from both academic (IRIT, LIA, LIMSI, LORIA, VALORIA, CLIPS) and industrial sites (France Telecom R&amp;D) took part in the project. The initiator of the project, the LIMSI Spoken Language processing group, was responsible for coordinating the scientific aspects of the project. To ensure impartiality, the campaign was coordinated and managed by ELDA who did not participate in the evaluation campaign. ELDA was also in charge of creating the corpus necessary for the project and responsible for creating or providing the software or tools necessary for the evaluation campaign itself. The company VECSYS provided the recording platform for the corpus (hardware and software including the  X  X izard of Oz X  system, see below). All partners were involved in the discussions concerning the choice of the task, the recording protocol of the corpus, and the common semantic representation. Only academic partners participated in the evaluation campaigns. This paradigm was used within two evaluation campaigns involving several sites carrying out the task of querying information from a database. 3 Data collection The dialogs are attempts to make hotel reservations using tourist information with data obtained from a web-based database. The corpus was recorded, the vocal tourist information server being simulated by a Wizard of Oz (WOZ) system (Devillers et al. 2003 ). In this way, each user believes she or he is talking to a machine whereas she or he is actually talking to a human being (a  X  X izard X ) who simulates the behavior of a tourist information server. This enabled a corpus of varied dialogs to be obtained, thanks in part to the flexible behavior of the wizard. The operator (wizard) used the graphical interface, developed by VECSYS, which assisted him in generating the responses communicated to the user. The generated replies were obtained by completing a sentence template with the information obtained by consulting a tourist information website taking into account the user X  X  request. The signal was recorded in digital format.

The callers referred to pre-defined tourist and hotel reservation scenarios (generated from a set of templates in such a way as to have a set of varied dialogs) and given to the callers by telephone. Several starting points were possible for the dialogs (for example, choice of town, itinerary, tourist event, festival, price, date, and so on). Eight scenario categories were defined, each with a different level of complexity. An example of a simple scenario is given in Table 1 . A complex-scenario could consist of reserving several hotels in several locations according to an itinerary.

In addition to the variety of scenarios given to the callers, a set of instructions for the wizard was defined in order to vary the type of dialogs. There are three categories of instructions. The first concerns speech recognition or comprehension errors. In this way, the wizard produces a response of having  X  X isunderstood X  the user request. The second involves explicit or implicit feedback to the user. The final type concerns the level of cooperation on the part of the wizard. At one end of the spectrum, the wizard returns all the information requested by the user. On the other end, he is not able to reply to any of the user X  X  requests. Between these two extremes, the wizard may provide partial information to the user, and here we may expect to observe misunderstandings, clarification requests, and so on, that are frequent in spoken dialogs. Most interesting phenomena (such as reference, negotiation, negation) were observed with complex scenarios and a non-cooperative wizard. 3.1 Corpus characteristics Main dialog characteristics are given in Table 2 . 1,257 dialogs were recorded, from 250 different speakers, where each caller carried out five different hotel reservation scenarios. The final corpus is on the order of 70 h of dialogs, which have been transcribed and semantically annotated by E LDA (Client utterances only were annotated). The total vocabulary size is 3,203 words including hotel and city names, with a mean number of words per utterance of around six for user requests. Although the wizards speak almost two times more (283 k words) than the users (155 k words), the lexicon size is much lower for the wizards (1,932) than for the users (2,715). This is due to the fact that the wizards pronounce sentences generated automatically while the users have no restrictions on their replies.
 4 Literal semantic representation and annotation scheme 4.1 Attribute/value representation In order to provide a diagnostic evaluation, the evaluation paradigm relies on a common generic semantic representation. The formalism was agreed upon by all project partners and chosen to enable a large corpus to be annotated with semantic tags. The selected common semantic representation, inspired by (Bonneau-Maynard et al. 2003 ), is based on an attribute-value structure in which conceptual relationships are implicitly represented by the name of the attributes. This formalism enables communicative acts as well as the semantic content of an utterance to be coded in a two level attribute-value representation.

Each turn of a dialog is segmented into one or more dialogic segments and each dialogic segment is segmented into one or more semantic segments with the assump-tion that a semantic segment corresponds to a single attribute. The communicative acts associated with each dialogic segment are derived from FIPA (FIPA 2002 ). Six dialog acts have been agreed to by all participants: Inform, Query, Accept (Confirm), Reject (Dis-confirm), Opening and Close, corresponding also roughly to the DAMSL 2 backward looking functions. This reduced list makes it possible to obtain a high level of inter-annotator agreement. However, since the project focussed on semantic evaluation, the partners involved in the campaigns were not expected to provide the dialogic segmentation and their corresponding communicative acts.
 An example of a literal semantic representation of a client utterance is given in Table 3 . An example of a whole dialog is given as an Appendix at the end of the paper. A semantic segment is represented by a triplet which contains the mode (affirmative  X  ?  X , negative  X - X , interrogative  X ? X  or optional  X  *  X ), the name of the attribute representing the meaning of the sequence of words and the value of the attribute. The order of the triplets in the semantic representation follows their order in the utterance. The values of the attributes are either numeric units, proper names or semantic classes merging lexical units, which are synonyms for the task. The modes are assigned per semantic segment basis. This allows disambiguating sentences like  X  X  X ot in Paris in Nancy X  X  which would be misleading for the dialog manager. This Attribute-Value Representation (AVR) allows for a simple annotation process.

The semantic representation relies on a hierarchy of attributes representing the task and domain ontology and defined in a semantic dictionary. The semantic dictionary was jointly developed by the M EDIA consortium. The basic attributes are divided into several classes. The database attributes correspond to the attributes of the database tables (e.g. DBObject or payment-amount ). The database attributes are classified in packages (e.g. time or payment ), which are domain-independent, and hotel which is domain-dependent. Each package is defined as a hierarchy of attributes (e.g. package payment involves a sub-attribute amount which in turn involves a sub-attribute int ). The modifiers attributes (e.g. comparative ) are linked to database attributes and used to modify the meaning of the relying database attribute (e.g. in Table 3 the comparative attribute, which value is lessthan is associated to the payment-amount attribute).
General attributes are also defined as command-task (cf segments number 1,26,47,61 in the Appendix ) which includes the different actions that can be performed on objects of the task (e.g. reservation , information ), or command-dial with values cancellation , correction , etc. One of the general attributes refLink is dedicated to reference annotation (cf segments number 24, 27, 37, 41, 44, 45 in the Appendix ). Three kinds of references are represented: co-references (as in  X  X  X n that hotel X  X  ), co-domain (as in  X  X  X nother hotel X  X  ), and element/set (as in  X  X  X he first hotel X  X  ). The general and modifier attributes are domain independent and were directly derived from other applications (Bonneau-Maynard et al. 2003 ) whereas most of the database attributes were derived from the database linked to the system.

Two types of connectors are also defined: connectAttr and connectProp which represent respectively the logical relations between attributes of a same object (with the default value and ), and relations for complex queries (with values explanation , consequence or opposition ). A connectAttr attribute indicates a semantic depen-dence between two attributes, as in the following example:
A connectProp attribute indicates a semantic dependence between two parts of a statement, each composed of several semantic segments (e.g. utterance C 2 , attribute 5 in the Appendix ). In the following utterance:  X  X  X lors a ` ce moment-la ` j X  aimerais re  X  server donc a ` au a ` l X  ho  X  tel du champ de mars euh mais par contre j X  aimerais conna X   X  tre le prix des chambres parce que mon budget serait infe  X  rieur de 150 fr(ancs) 150 euros pardon X  X  ( X  X  X hen I X  X  like to reserve then hmm at a at the Champs de Mars hotel hmm but on the contrary I X  X  like to known the price of the rooms because I can X  X  pay more than 150 fr(ancs) 150 euros sorry X  X ) the connectProp attribute has to be assigned to the semantic segment  X  X  X ais par contre X  X  ( X  X  X ut on the contrary X  X ) with the value opposition and to the segment  X  X  parce que  X  X  ( X  X  X ecause X  X ) with the value explanation .

Hierarchical semantic representation is powerful as it makes it possible to explicitly represent relationships between segments, possibly non-adjacent in the transcription of the statement. On the other hand, a flat representation facilitates manual annotation. A set of specifiers is defined to preserve the relationships which are combined with database or modifier attributes. Their combination with the database attributes specifies the exact relations between segments. The combination of the attributes and the specifiers together with connectors allows one to derive a hierarchical representation from the flat attribute/value represen-tation. In the example of Table 3 , the attribute name payment-amount-int-room results from the combination of a hierarchy of attributes from package payment-amount-int and the specifier room . Attribute comparative-payment is also derived from the combination of the comparative attribute and the payment specifier. The example of Table 3 can then be derived in the hierarchical representation given in Table 4 . 4.2 Corpus annotation Semantic annotation is done on dialog transcriptions. In order to decrease the annotation cost, the annotation tool described in (Bonneau-Maynard et al. 2003 ) was used. It helps for both the definition of the semantic representation and the annotation process. Semantic disambiguation may require listening to the signal. The Semantizer annotation tool 3 provides compatibility with Transcriber (Barras and Geoffrois 2001 ), which is becoming a standard for speech transcription. The formalization of the semantic dictionary and the assistance provided by the tool to the annotators increase the consistency of the annotations. For literal annotation, dialog turns are randomly presented to prevent the use of the dialog context. The attribute name is selected from the list generated from the semantic dictionary. Automatic completion of attribute names speeds up the process and is greatly appreciated by the annotators. An on-line verification is performed on the attribute value constraints. The tool ensures that the provided annotation respects the semantic representation defined in the semantic dictionary. Usually, the semantic annotation is keyword-based: the attributes are associated to the words which determine their value. In the chosen annotation scheme, a statement is segmented into semantic segments: the attributes are associated to sequences of words  X  X he segments X  X hich better disambiguate their semantic role.
Based on the semantic representation described above, the literal semantic annotation of the users utterances has been performed by two annotators. The semantic dictionary includes 83 basic attributes and 19 specifiers. The combination of the basic attributes and the specifiers, automatically generated by the annotation tool, results in a total of 1,121 attributes that are able to be used during the annotation process. The 83 basic attributes includes 73 database attributes, 4 modifiers, and 6 general attributes. The M EDIA consortium decided not to use semi-automatic techniques in order not to bias the evaluation process in favor of a participant system. The M EDIA corpus has been split by E LDA into randomly generated packages of 200 dialogs. The mean annotation time is about 5 times real time . In order to verify the quality of the annotations, periodic evaluations were performed, by computing the kappa statistic (Carletta 1996 ) for the mode and attribute identification. Therefore an alignment on per segment basis is performed by using the Media scoring tool (see Sect. 6.1.2 ) in order to deal with the cases where the annotators do not assign the same number of segments to the utterance. In the last inter-annotator experiment, the kappa is almost 0.9, which shows a good quality of annotation (usually, a kappa value is considered to be good for value greater than 0.8). The most common sources of disagreement across the annotators are due to connectors (14% of the errors, with a 0.7 agreement), the identification of the mode (14% of the errors, with a 0.97 agreement), and the reference links (12.5% of the errors, with a 0.8% agreement). Also 14% of the errors are due to specifiers.

The most frequent attributes are the yes/no response (17%), followed by reference attributes (6.9%) and command-task (6.8%). Those are task-independent. Task-dependent attributes (hotel, room...) represent only 14.1% of the observed attributes. The semantic dictionary ensures a good coverage of the task considering that only 0.1% segments are annotated with the unknown attribute.

Given that the objective of the project is to perform system evaluations, the client utterances have been divided into three corpora: the adaptation corpus which is necessary for the adaptation of the system to the domain and to the task, the development corpus which is used to test the evaluation procedure, and the test corpus itself. Table 5 gives their main characteristics.
 5 The contextual semantic annotation and annotation scheme The evaluation of understanding abilities that rely on context is a very difficult task because it depends on the contextual models of each system. We propose here a methodology for evaluating the final product of these abilities without considering the method actually used to build it. 5.1 Representation of the context First, we had to agree on what would be called the context during the evaluation. We studied four ways to represent the context. The first representation, called  X  X  ecological  X  X , contains only the preceding transcribed utterances (the dialog history). This representation is very close to real situations in which a system does not have any external information other than the utterance. However such evaluation would not distinguish errors that take place in the course of interpretation (like parsing or semantic building) from pure contextual understanding. The second representation, called abstract representations , only contains the literal and contextual representations of preceding utterances, but requires that all systems are able to take these representations as (unique) input. The third representation, called mixed-representation , contains both transcribed utterances and their literal and contextual representations. Finally, the context could have been encoded as a paraphrase , which is a small text that sums up the preceding dialog, more difficult to construct but usable by all systems. Each participant had to choose a preferred way to represent the context for the evaluation of their system, and we decided then to evaluate the systems according to the ecological and mixed representations. 5.2 Contextual representation and annotation scheme In the M EDIA framework, we define the contextual semantic representation as a product of the re-interpretation of the current utterance according to the previous dialog context . The process of re-interpretation of the context according to the current utterance has been excluded from the evaluation because it is too dependent on particular strategies and internal representations of each system. The contextual understanding abilities of the systems have been evaluated the same way as literal ones and focuses on two facets of understanding: the contextual meaning refinement which consists of modifying the semantic representation of an utterance according to the previous dialog history, and the reference resolution which consists of contextual annotation has to respect some practical constraints: first it should not introduce a new segmentation of the utterances with respect to the literal annotation. We avoid then the problem of comparing different segmentations. Second, it is necessary to have the same dictionary of features, considering that the contextual meaning of an utterance could be reformulated by literal semantic features. Third, for reference resolution, on the contrary to literal understanding, utterances of the system need also to be annotated. And fourth, reference annotation has to be done using descriptions instead of relationships (like coreference chains, see Sect. 5.2.2 ). 5.2.1 Contextual meaning refinement The refinement of the meaning of an utterance is only required if once we consider the context, this meaning differs from the literal interpretation. The contextual semantic specification consists of modifying the literal annotation using the same vocabulary: the set of concepts and their corresponding attributes cannot be altered. The following example (Table 6 ) shows how the meaning could be refined using the context (the revised meaning is in bold ). 5.2.2 Reference resolution In the MEDIA project, reference resolution was restricted to resolution of intra-linguistic anaphora and more precisely coreference , that is, when two referring expressions refer to the same individual (van Deemter and Kibble 2000 ). Most approaches evaluate the relationships between referring expressions (Popescu-Belis et al. 2004 ), and rely on annotation schemes focused on relations, like the MUC-6 and MUC-7 campaigns, based on coreferences (Chinchor et al. 1997 ; van Deemter and Kibble 2000 ), or the Reference Annotation Framework, RAF (Salmon-Alt et al. 2004 ) in which referring expressions are annotated by markables and relationships by referential links . These approaches are well designed for identifying the relationships but are less efficient to deal with particular types of references (like in  X  X  I take some  X  X , where  X  X  some  X  X  quantifies over a type of objects, here elliptical). In addition, they require to add a new level, completely different from the semantic level which entails developing new measures. We preferred to evaluate instead the semantic description of referents. First it allows us to deal with a larger scope of phenomena, and second, it does not require to develop new measures. However, globally evaluating the semantic description of referents is not very accurate because some semantic features are more important than others to identify objects (the city in a description of a room seems much more important than knowing if it has a bathroom). But as the systems were able to produce a semantic description, the evaluation of reference resolution is limited to this representation and to the description of referring expressions, with a taxonomy close to RAF (identity, codomain, or part-of).

The literal annotation of reference has been limited to the referring expression, as such, using a refLink feature, refined by the expression category. The different categories are very close to those used in RAF , (see Table 7 ), but without the part-of relation for which there was no agreement. On the contrary to RAF markables , only determinants of noun phrases are associated to a refLink feature because the rest of the noun phrase is already annotated by the literal semantic annotations. The value of the refLink feature equals the expected number of referents: singular ( X  X  X his hotel X  X ), plural ( X  X  X hose hotels X  X ) or undetermined when no information of number is given ( X  X  X here X  X  can refer to one or more hotels).

To keep the annotation cost low, while focusing on interesting phenomena, only referring expressions whose scope was beyond the utterance have been annotated. This excludes any referring expression whose antecedent is located in the same utterance, but also named entities or indefinite expressions. 4 Eventually, only entities of the task were annotated (hotel, room ... ). The Appendix shows room annotation (turn C 10 ), hotel annotation (turn C 12 ) and price annotation (turn C 22 ).
The contextual representation of a reference is based on the literal annotation of the referents. A reference is represented by a set of referents, each one described by a set of semantic features. We do so by adding a reference field to the refLink features; for instance,  X  X  X 1,t2; t3 X  X  would be the annotation of a referring expression that refers to an entity described by two features and another one described by only one feature. An example (turn C 10 from the dialog in Appendix ) is given below. The reference field of the feature 24 X  X he determiner  X  X  X es X  X  (the) in  X  X  X es chambres X  X  (the rooms) X  X ontains three referents described by preceding features: the city (13), the name of the hotel (14, 15, 18, 21), and the price (16, 17, 19, 20, 22, 23).
The main limit of this formalism is how the ambiguity phenomenon is approximated. Another level would have been expected to represent ambiguities of plural groups. Without this level, ambiguity is encoded as a plural group but with an additional specifier ( ambiguous ) on the refLink concept. For example, an ambiguous expression like the other hotel should be annotated as refLink-coDom-ambiguous .
We collectively designed annotation rules following three constraints: a low annotation cost, a large set of interesting phenomena taken into account, and a high inter-annotator agreement (see below). The most important rule is how to describe referents, and especially referents described by other referents. For instance, because  X  X  X oom X  X  is the reserved object, its description could gather all the features of the reservation, and as such would imply a high annotation cost. Several solutions have been studied: the maximal annotation constituted by all the features describing a referent (accurate but too costly), the discriminating annotation defined by the smallest description of the preceding context that can identify the referent without ambiguity (uninteresting to evaluate if there is no ambiguity), and a recency-based annotation composed of the descriptive features contained in the utterance containing the most recent antecedent (useless for pronouns or demonstratives).
Since none of these solutions is fully satisfactory, we made a compromise between the maximal and the discriminating annotation which relies on the type of entities: the named entities (or equivalent like a named hotel, date, price, city, etc.) are only described by a very small set of features which are discriminating by definition (the name or the value), whereas other entities (unnamed hotel, or room) are annotated with the largest set of features, including other referents X  features. Other annotation rules define the scope of a referent X  X  description which can contain all the semantic features present in preceding utterances. Finally we constrain the referent description to be normalized, that is, in a non-redundant, non-contradictory and fully specified semantic form.

During this second campaign, the corpus was again split into three subsets for adaptation, development (dry run) and the final test (Table 8 ).

The manual annotation of referring expressions has been controlled at three times, by measuring the inter-annotator agreement using the three levels evaluation measure presented Sect. 6.2.2 . The agreement, evaluated on 31 dialogs (taken from the 814 dialog training corpus), is very good with respect to the description of referring expressions (DRE, 95%) and referent identification (IREF, 95%). Still good, the full description of referents (DREF, 82%) is weaker than the former, showing the difficulty, even for human annotators, of providing the unique complete description of the referents.
 6 The M EDIA evaluation campaigns 6.1 Evaluation of literal understanding 6.1.1 Systems presentation Five systems participated in the evaluation. L IMSI -1 and L IA use corpus-based automatic training techniques, the L ORIA and V ALORIA systems rely on hand-crafted symbolic approaches, and the L IMSI -2 system is mixed.

The Spoken Language Understanding module developed at L IA (Raymond et al. 2006 ), starts with a translation process in which stochastic Language Models are implemented by Finite State Machines (FSM). The result of the translation process is a Structured n-Best list of interpretations. The last step in this interpretation process consists of a decision module, based on classifiers, choosing an hypothesis in this n -best list.

The L IMSI -1 system (Bonneau-Maynard et al. 2005 ) is founded on a corpus-based stochastic formulation. Two stages are composed: a first step of conceptual decoding produces the modality and attribute sequences associated with word segments, then a final step translates the word segments into the values expected by the representation. Basically, the understanding process consists of finding the best sequence of concepts given the sequence of words in the user statement under the maximum likelihood framework.

The L IMSI -2 system is based on previous work on automatic detection of dialog acts (Rosset et al. 2005 ) and consists of three modules: a symbolic approach is used for specific entities detection, utterance semantic segmentation is done using a 4-gram language model representation, and then automatic semantic annotation is performed by using a memory based learning approach.

The approach of the L ORIA system (Denis et al. 2008 ) is based on deep-parsing, and description logics. Derivation trees (even partial ones) are used to build a semantic graph by matching TAG elementary trees with logical predicates. The resulting conceptual graph is tested against an internal ontology to remove inconsistencies. Projection into the final representation is carried out by use of an external ontology, the one of M EDIA , and description logics.

The V ALORIA system, called L OGUS , implements a logical approach to the understanding of spoken French (Villaneau et al. 2004 ), according to the illocutionary logic of Vanderveken ( 1990 ). Concepts and conceptual structures are used in order to enable the logic formula to be convertible into a conceptual graph. 6.1.2 Evaluation protocol The scoring tool developed for the M EDIA project allows the alignment of two semantic representations and their comparison in terms of deletion, insertion, and substitution. It is able to handle alternative representations for each statement. The scoring is done on the whole triplet including [ mode , attribute name and attribute value ]. Different scoring methods have been performed on the system results. The Full scoring used the whole set of attributes, whereas in the Relax scoring, the specifiers are no longer considered. Another simplification consists of applying a projection on modes resulting in a mode distinction limited to affirmative and negative (two modes).

Each participant benefited from the same semantically annotated 11k utterance training corpus to enable the adaptation of its models to the task and the domain, as well as the semantic dictionary and the annotation manual. The 3,203 word lexicon of the M EDIA corpus and the list of 667 values for the open-value (as hotel or city names , as opposed to comparative attribute which values are given by the representation) attributes which appear in the corpus were also given to the participants. Following a dry-run on a 1k utterance set which enabled the definition of the test protocol, the literal evaluation campaign was performed on a test set of 3k utterances. As observed from the inter-annotation experiment some variability should be allowed in the semantic representation of a statement. In a post-result adjudication phase, the participants were asked to propose either modifications or alternatives for the test set annotation. At the end a consensus vote was carried out. Only 179 queries were associated to several alternative annotations, it means less than 6% of the whole test corpus, with approximately 2 alternatives per statement. 6.1.3 Results Table 9 gives the results obtained by the five participant systems in terms of understanding error rates (Bonneau-Maynard et al. 2006 ). First it can be observed that the corpus-based training systems (L IMSI -1, L IMSI -2 AND L IA ) obtain better results than the others. Concerning the performance of the symbolic systems, a significant part of the errors comes from a bad projection (or translation) into the expected annotation format, and not only from the understanding errors.

Given the number of attributes present in the test set (8 788), the 95% precision of the results is good ( p = 0.000 114). The understanding error rates are relatively high: 29% for the best system in Full scoring with four modes, and 19.6% for the best system in Relax scoring with two modes. This last result may be compared with the understanding error rate on the A RISE task (Lamel et al. 1999 ), with a similar evaluation protocol, which was around 10% on exact transcriptions (Lefe  X  vre et al. 2002 ). The gap in performance between the A RISE and M EDIA tasks may be explained by the number of attributes involved in the models which is much higher for the M EDIA task (83 attributes, 19 specifiers) than for the A RISE task (53 attributes, no specifiers).
The performance improvement between the results obtained with and without the specifiers ( Full vs. Relax ) is significant for all the systems. It is worth noting that no significant difference in performance is observed between systems using such a hierarchical representation internally to those obtained with systems implementing a tagging approach (the lowest relative increase in error rate (around 7%) is obtained by two systems (V ALORIA and L IMSI -1) representing both approaches).
 Using four modes instead of two is also a major difficulty for all the systems. This can be partially explained by the fact that the signal X  X hich was listened to by the human annotators X  X s often necessary to disambiguate between interrogative and affirmative mode.

The attributes on which errors are most frequently occuring are the reference link attribute ( refLink ). Obviously, the annotation of references represents the most difficult problem on which research teams may have to focus their efforts. This is also true for the connectors identification. Except for these two points, the nature of the errors is rather different among the systems. R OVER tests (Fiscus 1997 ) have been efficiently performed to exploit the nature of the errors made by the multiple systems: in an Oracle mode, the best combination of the five systems could reduce the error rate to 10%.
A meta-annotation of the test corpus has been performed in terms of linguistic difficulties, semi-automatically derived from the semantic annotation. Table 10 gives the systems X  error rate for the subsets of statements containing the most significant difficulties in the Full scoring mode with four modes. The first line gives the number of utterances in which each difficulty is observed in the test set and the corresponding 95% precision of the results ( p ). Complex requests correspond both to multiple requests or requests which are on the borderline of the M EDIA domain. Repetition is tagged when a concept is repeated in the utterance several times with the same value (as in  X  X  X he second the second week-end of March X  X  ), whereas Correction is used when the concept is repeated with different values (as in  X  X  X he second the third week-end of March X  X  ).

The understanding error rates become significantly greater for sentences including difficulties. The systems which have got the best results on the whole test set keep the best results for the difficulties. From a the relative point of view, L
IMSI -1 and L IMSI -2 systems are more resistant to errors for complex utterances (resp. 14 and 17% relative error increasing) than the other systems (around 30%). 6.2 Evaluation of contextual understanding 6.2.1 Systems presentation LORIA symbolic approach LORIA X  X  system focused on the processing of referring expressions, leaving apart the problem of meaning specification in dialog context. The reference solver developed in LORIA X  X  system (Denis et al. 2006 )is based on Reference Domains Theory (Salmon-Alt and Romary 2001 ). This theory assumes that referring expressions require the identification of a domain in which the expression isolates the referent. Although the theory was originally designed for multimodal reference processing, the MEDIA campaign was an opportunity to evaluate its relevance for anaphora resolution. In this framework, a reference domain consists of a support, a set of objects defined either in intension or in extension, and a set of differentiation criteria which discriminate their elements. Each designation activates the corresponding domain, in which the element is extracted and focalized, enhancing therefore the salience of this element for later designation. The alterity expression (e.g.  X  X he other hotel X ) looks for a domain having a focalized partition, in which the other part will be extracted.

The projection into the M EDIA formalism is carried out by collecting and merging along the dialog history the literal semantic representations of referents. In the mixed evaluation (see Sect. 5.1 ) we integrate the literal semantic representations at this step of the process: we do not use these information for solving the referring expressions.
LIA probabilistic approach As mentioned in the LIA system presentation concerning the literal understanding campaign, the contextual meaning refinement and reference resolution processes are carried out at a second stage, on the basis of the n -best concept chains produced at earlier stage.

Contextual meaning refinement is processed as a tagging task: specifiers are attributed by a probabilistic tagger, based on conditional random fields (CRF). CRF (Lafferty et al. 2001 ) have been successfully used for many tagging tasks and provide the ability to predict a tag from a sequence of observations happening in the past or in the future. This ability is very helpful for specifiers since the refinement of a given concept may be triggered by elements occurring before or after the concept in a broader context.

Once the tagging is over, the resolution of reference is done according to the following algorithm: all concepts in the closer dialog history (limited to the n previous utterances) which hold the same specifier as the object pointed by the referential link are associated with this link. Each object is described by a given number of features (for example, the town, the trademark, the name or the services associated with one hotel). The association algorithm will keep in the referential link all the concepts describing those features. More information on this approach is also given in (Denis et al. 2006 ). 6.2.2 Evaluation protocol The evaluation of reference resolution is carried out by comparing the semantic features describing each referent. Before describing a referent, this referent needs to be identified. This identification also requires that the system correctly identifies the referring expression. Since these tasks are based on potentially different abilities, we found it necessary to evaluate the process of reference resolution upon three levels, each giving rise to classical scores like recall, precision and f-measure:
DRE Ability to describe referring expressions, i.e. to provide the correct speci-
IREF Ability to identify the referents, i.e. to provide enough correct features for
DREF Ability to describe in extenso the referents. This evaluation only applies 6.2.3 Results and discussion Table 11 shows the results of the systems for both ecological and mixed evaluation conditions (see Sect 1 ). The confidence intervals are given with respect to a precision of 95%. In DRE, the LORIA gets a very average score in the ecological phase, which improves notably by having the correct literal description in the mixed phase. Concerning referent identification (IREF), the symbolic system has the same low recall score, in both conditions. This lack of improvement is explained by the fact that the additional information provided in the mixed protocol can only be integrated in the LORIA system after the referents have been identified, and would only help to better describe the referents (DREF). Finally, both systems equally improve their score in DREF comparing the mixed condition to the ecological one.
We scrutinized LORIA X  X  results according to its IREF errors. First, we noticed that only 57% of the errors come from the reference resolution algorithm while 43% come from upstream or downstream modules (literal projection, semantic form building, or syntactical or lexical analysis). The reference resolution errors have been classified into two groups: the phenomena that were not handled at all (35%) and the phenomena that were wrongly processed (65%). The first group contains complex cases like generic use of  X  X  X he room X  X , while the errors in the second group gather for instance wrong use of semantic constraints or erroneous domain management. This evaluation proves that the LORIA X  X  model is fine-grained but error-prone: one missed referent in the beginning of the dialog could lead to many other reference errors in the following utterances.

The results of LIA system in the mixed condition show which errors are only produced by the contextual meaning refinement and the reference resolution processes. Contextual meaning refinement and referential links are rather correctly tagged but these scores vary very much according to tag values (the specifiers). The low occurrence of some phenomena is problematic for probabilistic methods which requires a large number of examples to learn models. Reference identification (IREF) is performed with rather good precision, with respect to the very simple heuristics we designed for this task. Finer analyses show that the LIA system is quite good at resolving direct reference, which are the most common. Many errors concentrate on ambiguities and alterities. Finally, we note a limited drop of the score in the ecological condition with respect to the mixed one. Therefore this approach is rather robust. 7 Conclusion The paper has described in details the M EDIA annotation scheme for semantic annotation of spoken dialogs. Its main characteristics and advantages are that:  X  The representation is generic and provides compatibility with Transcriber.  X  It includes both literal and contextual annotation levels.  X  It enables a good level of precision (including explicit representation of  X  The reduced annotation time enables the annotation of large corpus.

The very good inter-annotator agreement validates the choice of the annotation formalism and the development of the corresponding annotation tool.

The M EDIA project provides a large dialog corpus to the community: more than 1,200 real dialogs with their corresponding semantic annotations. Because of the large size of the corpus, systems which require supervised learning have got enough data to train on.

Furthermore, the M EDIA consortium has designed a common framework for evaluating the understanding modules of dialog systems, including the possibility of evaluating the performances of understanding modules to take into account the local context. Specific evaluation tools have been developed, enabling cross-system comparison and detailed analyses such as literal understanding, contextual meaning refinement and reference resolution. The corpus also includes the speech signal, so that experiments from speech signal to speech understanding are possible. An evaluation package which includes the corpus along with protocols, scoring tools, and evaluation results is available and distributed by E LDA . 5
The documents (in particular annotation instruction manuals) and the tools (both annotation and evaluation tool) provided by the project enable the possibility to apply the methods to other domains. For example the European project L UNA -IST 33549 has used the semantic representation and the dedicated annotation tool for the annotation of their multi-lingual corpus of customer-operator dialogs. The M EDIA corpus has also been acquired by the UniversitT du Maine to perform studies on dialog systems.

The wide availability of those resources X  X orpus and evaluation tools X  X ill support the development of robust dialog studies. In pursuit of this goal, two PhD theses have been carried out within this project. Both propose to exploit the M EDIA corpus (dialogs and semantic annotations) for evaluating the ability for a system to overcome either difficulties (simulated user behaviors) (Allemandou 2007 )or reference solving errors by a grounding process (Denis et al. 2007 ).
 Appendix We give a full annotated dialog (#1037) from the MEDIA corpus, where W is the wizard, and C the client. Below each utterance the sequence of segments with their corresponding contextual annotation is given. The segment numbers (1 X 85) may be referred to, for referring expression annotation.
 References
