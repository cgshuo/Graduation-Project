 ORIGINAL PAPER Sreangsu Acharyya  X  Sumit Negi  X  L. Venkata Subramaniam  X  Shourya Roy Abstract Noise in textual data such as those introduced by multilinguality, misspellings, abbreviations, deletions, pho-netic spellings, non-standard transliteration, etc. pose consid-erable problems for text-mining. Such corruptions are very common in instant messenger and short message service data and they adversely affect off-the-shelf text mining methods. Most techniques address this problem by super-vised methods by making use of hand labeled corrections. But they require human generated labels and corrections that are very expensive and time consuming to obtain because of multilinguality and complexity of the corruptions. While we do not champion unsupervised methods over supervised when quality of results is the singular concern, we demon-strate that unsupervised methods can provide cost effective results without the need for expensive human intervention that is necessary to generate a parallel labeled corpora. A generative model based unsupervised technique is pre-sented that maps non-standard words to their correspond-ing conventional frequent form. A hidden Markov model (HMM) over a  X  X ubsequencized X  representation of words is used, where a word is represented as a bag of weighted sub-sequences. The approximate maximum likelihood inference algorithm used is such that the training phase involves clustering over vectors and not the customary and expensive dynamic programming (Baum X  X elch algorithm) over sequences that is necessary for HMMs. A principled trans-formation of maximum likelihood based  X  X entral clustering X  cost function of Baum X  X elch into a  X  X airwise similarity X  based clustering is proposed. This transformation makes it possible to apply  X  X ubsequence kernel X  based methods that model delete and insert corruptions well. The novelty of this approach lies in that the expensive (Baum X  X elch) iterations required for HMM, can be avoided through an approxima-tion of the loglikelihood function and by establishing a con-nection between the loglikelihood and a pairwise distance. Anecdotal evidence of efficacy is provided on public and proprietary data.
 Keywords Noisy text  X  Unsupervised learning  X  Clustering 1 Introduction and related work Millions of users of instant messaging (IM) services and short message service (SMS) generate electronic content in a dia-lect that does not adhere to conventional grammar, punctua-tion and spelling standards. This phenomena is a by-product of limited data entry options (9 keys over a phone) and due to the pressure of reducing communication latency (or cost) by keeping messages short yet intelligible. Words are inten-tionally compressed by non-standard spellings, abbreviations and phonetic transliteration are used .Ina multi-lingual envi-ronment this problem is even more formidable because of the additional ambiguity introduced by non-standard trans-literation of vernacular words using Latin characters and the associated language transitions within the sentence.
These corruptions reduce the word overlap similarity measure between two sentences. Off-the-shelf text mining techniques such as those geared for (i) frequent phrase extraction, (ii) clustering and (iii) classification do not per-form well under such a noisy environment, making it impor-tant that techniques for denoising are investigated. This domain of study is comparatively new, though considerable insight into probable approaches may be taken from the field of automatic spelling correctors [ 12 ] and approximate string matching [ 14 ]. Recent spelling correction approaches have used statistical language models and stochastic finite state automata as a generative model for spelling errors [ 3 ]. Statis-tical models of pronunciation have also been used to improve such models [ 19 ]. Another domain that has to deal with sim-ilar problem is that of spelling correction of web search que-ries [ 1 ]. Choudhury et al. [ 5 ] study the domain of correcting SMS messages that utilizes hand corrected training data con-sisting of original SMS and the corresponding expanded and corrected messages. From the hand generated parallel cor-pora of SMS and conventional language, a hidden Markov model (HMM) is learnt that mimics the SMS language gener-ation. Kothari et al. [ 11 ] present an unsupervised method for FAQ retrieval using SMS queries. They propose an efficient algorithm that handles noisy lexical and semantic variations and does not require an aligned corpus or explicit SMS nor-malization to handle the noise. Sproat et al. [ 18 ] investigated techniques for mapping non-standard words to standard but the focus of their technique was also on supervised learning. Their technique involved hand tagging of non-standard words with tags representing the type of corruption encountered and learning to tag unknown words by the inferred type of cor-ruption. Corrections were generated from the predicted tags. However they do discuss unsupervised methods in passing. Brown et al. X  X  [ 4 ] work on class based N -gram models look at average mutual information criteria to cluster similar words together. However they do not have clean way of optimizing their cost function and propose a hill climbing method. 2 Motivating a solution Before we describe our solution, we try to give an intuitive description of the approach that we follow and why we expect it to work. To correct corrupted text without labeled data is difficult, especially so when the corruptions are as severe as those seen in SMS and IM domains. In order to work in an unsupervised setting the technique has to exploit some property of the word that remains largely unchanged even after corruptions. The crucial starting point of the approach is to identify such an invariant. We motivate the search with a thought experiment: given a text document such that a frac-tion of occurrences of a common word say  X  X he X  are cor-rupted, how can one detect and correct such corruptions, i.e., how to find the regular word corresponding to the cor-rupted word.

If only few words are corrupted, a possible solution emerges after noticing the fact that the neighborhoods of the corrupted word remain largely untouched. Hence given a sufficiently long document, the distribution of the words near the corrupted variants will be similar to those near the corresponding un-corrupted word  X  X he X . We may therefore represent each word by the distribution of words appearing in its pre and post context windows and cluster the words using this representation. Of course, in our application, it is not a single word but rather almost every word that is cor-rupted to some extent sometimes. Such high noise to signal ratio means that even the contextual words will have noise in them and will not match exactly. However, a recursive defi-nition of similarity or, as we shall show, use of informative and corruption resistant features may be helpful in recover-ing sufficiently from such errors. In the next section, we take a more formal look at a model where clustering of contextual words can be used to solve the correspondence problem. 3 Problem formulation The problem of de-noising noisy word sequences lends itself naturally to a solution by HMMs. A HMM is a random pro-cess consisting of a Markov chain over a state space where the actual state occupied by the random process remains hidden from the observer, the observable quantity is a symbol emit-ted conditioned on the unobserved occupied state. For our application the state space is over the vocabulary of intended words W and at each such state the process emits an observ-able symbol (the typed and possibly corrupted word) condi-tioned on the occupied state.

We use the symbol X t to represent the random variable denoting the state at time t . The notation X [ T ] is used to donate a set of such random variables indexed by the mem-corresponding to time indices lying in the interval t 1  X  t t = 0 till t sisting of time equaling t 1 and above. Similarly we use Y to represent the observable random variable, the same con-ventions for indexing apply. The assignments to the random variables are denoted by their corresponding lower case let-ters. We use Z t to represent the paired variable ( X t , by W . In the HMM process that we consider, we allow the emission from a state w 1  X  W to be corrupted into any mem-ber s i  X  S 1 . This holds for all states i.e., P ( Y t  X  S w ) = 1 . 0. This is a simplification that rules out overlaps , i.e., no single s i is allowed to corrupt different states w tion space S = X  S j such that  X  i = j S i  X  S j = X  and size | J |=| W | . This simplification makes our hard clustering based solution possible. However the simplification can be easily removed by the use of soft clustering algorithms that assign a point to clusters with varying probabilities.
Formally the model is defined by the tuple { W , S , T (w w ),  X (w of dictionary words, T (w i | w j ) and  X (w i ) the transition and stationary probabilities on the state space, S is the  X  X ut of dic-tionary X  space that words are corrupted into, and Q ( s j the symbol emission probability from state w i .IntheHMM process that we consider, the states are visited in a Markov fashion, i.e., P ( X t + 1 | X [ ., t ] ) = P ( X t + 1 | w | X ability ( 1  X   X ) the process emits the observable Y t = X and with probability  X  it emits an out of dictionary symbol according to P ( Y t = s j | X t = w i ) = Q ( s j | w i ) Y can take values in Y = W  X  S . The parameter  X  controls the proportion of out of dictionary and in dictionary words emitted and can be easily estimated from a data set and a dictionary . To add a few words about notation, the count-ing function is represented by n (.), KL ( p || q ) represents the Kullback X  X eibler divergence defined as w  X  W p (w) log p (w) q (w) [ 6 ] and likelihood by L .
 the testing phase, the objective is to reconstruct the state is provided by the Viterbi algorithm [ 15 ]. The Viterbi algorithm operates on the observation sequence and the gramming. In the training phase, the objective is to learn the parameters of the model, this is provided for by an imple-mentation of the EM algorithm [ 7 ] specialized to the HMM setup, the Baum X  X elch method [ 2 ], which is also a dynamic programming method to maximize the loglikelihood of the marginalized data L y [ T ] with respect to the parameters { W , S , P (w and the Baum X  X elch methods are expensive because they require dynamic programming. We omit details of these two methods and refer the reader to the tutorial [ 15 ].
We seek a simpler model that allows a light-weight train-ing method, in particular those that lead to mixture mod-els on vectorial data rather than dynamic programming on sequences. One of our contributions is in the use of an algo-rithm to maximize an approximation to the loglikelihood of the observed HMM sequence. This replaces the expensive dynamic programming method by a clustering of symbol emission probability vectors . Each state will be represented by its previous and post symbol distributions and clustered into groups. Since a single symbol s can only be a corrup-tion of a single unknown state, one can consider the pre and post symbol frequencies/distributions to represent the cor-responding unknown state, which can then be grouped by clustering the distributions. Hence a single state corrupted into different symbols s i  X  S i may be identified. 3.1 Inference with missing values Note that the general problem we need to solve is to recon-ues is a well studied problem and the dominant paradigm used to solve it is by applying the EM algorithm [ 7 ] which is described next. EM is a general algorithm and need to be specialized to the situation.

Missing data samples can be modeled as samples drawn from a parametric joint distribution P ( X , Y |  X ) where the sample of X are hidden (or latent ). Hence one needs to estimate the maximum likelihood parameter  X  over the obser-vation, i.e., maximize the marginalized log-likeli-hood log x P ( x , y |  X ) or equivalently minimize KL Q ( distribution. Such an estimate can be computed using the Expectation Maximization (EM) algorithm [ 7 ].

EM was developed as a likelihood maximization tech-nique that works by maximizing successively improved tight lower-bounds of the likelihood function. EM is an iter-ative procedure that consists of two steps the E step and M step shown at the end of the paragraph . Given an empir-ical marginal distribution Q ( Y ) of a random variable Y that is drawn independently from a parametric joint distri-bution p  X  on the product space X  X  Y , and the set Q = { q : Q ( X , Y ) = Q ( Y ). P ( Y | X ) } of all distributions on the product space consistent with the empirical marginal distri-bution on Y , the marginal log-likelihood log x P ( x , y locally maximized by following the iterative steps initialized by  X  0 : 1. q t + 1 = Argmin q KL ( p  X  t || q ) q  X  Q 2.  X  t + 1 = Argmin  X  KL q t + 1 || p  X  ) Lemma 1 [ 7 ] Given a parametric joint distribution p  X  on the product space X  X  Y , the following inequality holds : The expression is an equality at  X  t + 1 =  X  t .
 What the lemma shows is that in the absence of samples of the hidden variable, the marginal likelihood can still be iteratively maximized by maximizing the expectation of the joint loglikelihood as a surrogate of the likelihood. This is so because the expected completed loglikelihood lower bounds the marginal loglikelihood tightly and the bound can be improved at the end of each iteration.
 efficiently we have to use the Baum X  X elch algorithm which is expensive . So in the interest of simplicity we will maxi-mize a upper bound of the expected completed log likelihood which in itself is a lower bound of the objective function: the marginal likelihood .
 hand is to reconstruct the mapping between S and W sig-nifying which members of W were corrupted into which s i . The estimation task is to find the posterior P (w i | s j parameter P ( s j | w i ). Both are found by maximizing the log-likelihood of the observed sequence. Note that the maximum likelihood updates are available in closed form when both X and Y t can be observed. However since X t is often hidden (1  X   X  fraction of the time) the random variable X to be marginalized out and the marginalized loglikelihood 3.2 Approximate EM First let us compute the joint likelihood L Z [ T ] of a data ten as a product of the pairwise joint probability of the con-secutive states divided by the probability of the single state of this expression can be easily verified.

Such a parameterization is possible because Markov chains are equivalent to a Markov Random Fields over an undirected chain graph [ 17 ] and hence its likelihood can be expressed as a product of potential functions defined over the cliques of the chain, in this case the edges and the vertices.
Consider a similar sliding window of products of the fol-lowing function over the subgraph defined by the states of the process at three consecutive instances of time t = i  X  t = i , t = i + 1, shown in Fig. 1 . The sliding product us the likelihood. Using the factorization above we obtain: P
In order to deal with hidden states we invoke the EM method for maximizing the likelihood. The maximization is obtained by iteratively maximizing the expected loglikeli-hood of the data sequence. Wherein the expectation is over the assignment of the hidden variables, conditioned on the entire data and the current parameters and can be computed from ( 1 ).

The EM algorithm allows one to handle the marginal log-likelihood by using the tight inequality L ( Ignoring the constant scaling by 1 2
The inequality ( a ) follows from ( 1 ), inequality ( b ) the concavity of log and the Jensen X  X  inequality and the inequality ( c ) follows by ignoring the terms P ( x i  X  1
Thus the factorized likelihood + is an approximation to the expected loglikelihood of the Note however that, unlike the expected completed loglikeli-hood, ( 3 ) ceases to be a bound of the loglikelihood function and is only an approximation. Expected completed likeli-hood is a lower bound to the marginal likelihood, ( 3 )isan upper bound of the expected completed likelihood. This may no longer bound the true likelihood but approximates it. We conjecture that its value lies in between the true likelihood and the expected loglikelihood and is hence a tighter bound to the loglikelihood. We intend to resolve the issue in future work. It is this approximation that allows us to do away with the dynamic programming step and work with vector clus-tering objective function.

The expression ( 3 ) is equivalent to the cost function for solving multinomial mixture model estimation [ 8 ] with the EM algorithm. Note that instead of being parameterized by the Markovian state transition probabilities P ( x i + 1 | now parameterized in terms of the previous observed sym-bol to state symbol probabilities P ( y i  X  1 | x i ) and subsequent observed symbol and state symbol probabilities P ( y i + 1 These parameters are evaluated by maximizing ( 3 ). To reduce ambiguity we introduce the following notation: P (w k s ) = P ( y w ) = P ( y index i will be understood from the context.

The reader should recollect that each symbol s j can cor-rupt only a single state, hence all the indices T ( s j ) ={ y w P T ( s j ) ={ i : y i = s j } and Baye X  X  rule. Corresponding to any symbol s j we can collect the post symbol frequencies n ( s j  X  s i ) and the previous symbol frequencies n ( s i  X  for different s i s.

Collecting such statistics ( 3 ) can be written as and the parameters estimated by maximizing it iteratively with respect to P (w k  X  s i ) and P ( s i  X  w k ) for differ-ent values of w k  X  W and s i  X  S . It can be shown that this is equivalent to EM maximization of this mixture model clustering: Argmin + The corresponding hard clustering problem where P by a K-means like algorithm shown in Fig. 2 . 3.3 Representation by subsequence kernels Given a word w i the members of S i that are allowed to cor-rupt it are usually  X  X lose X  to w i and to each other in some sense, usually in phonetic and orthographic space. This prior topological information has not been utilized in the formu-lation so far. It is possible to capture this phenomenon by using prior probability for every possible w i , s j pair. This will however incur a large number of parameters for the pri-ors.

To address this situation the representation of the words is changed. As is common in classification by kernel methods, the data space is mapped on to a feature vector space F .Let : W  X  S  X  F be such a mapping. It is desired that the if s has a high probability of being the corruption of w the two vectors (w) and ( s ) be  X  X ear X .

The choice of this feature space F and the mapping should be such that proximity between original word and its corruption pair be high, we explore a few choices. A word may be represented as a vector of all substring of a fixed length k that occur in the word. Such a representation will have some proximity preserving property because if some characters in the string are changed, other substrings remain unaffected. Thus the corrupted vector is expected to give a high similarity (low distance) measure with the original. Smaller the value of k , higher the degree of preservation of proximity and unfortunately higher the chances of generat-ing spurious matches. Increasing k also has the drawback of increasing the dimensionality exponentially.

In the domain that we are concerned with, a common mode of corruption is through deletion of characters and sub-stitution of a run of characters by replacement characters. SCEAM Substring vectors do not maintain sufficient proximity after such drastic corruptions. To give an example from the data-set that we tested the algorithm on,  X  X e-activate X  was often phonetically abbreviated to  X  X actvt X . The only common sub-string between the two is  X  X ct X . In order to be robust against deletions and substitutions, the feature space that we consider is fixed length subsequences. One can observe that the longest common subsequence between the two strings is  X  X actvt X . Following the motivation above we describe the word repre-sentation used. For a given word, its representation consists of a sparse vector of dimensionality k i = 1 i ! N + 2 i where N is the size of the alphabet, and k is the maximum width of the subsequences being considered Fig. 3 . The extra term in N + 2 comes because of we pad all words by a beginning of word and end of word special symbols. The components of this vector representation are the normalized weighted counts of appearance of each subsequence, where each sub-sequence has decaying exponential weights depending on the gap between the first and last character in the original string. The normalization is such that the L 1 norm is 1. The dot product between two such vectors forms a valid kernel [ 13 ] between strings and can be computed efficiently by dynamic programming (DP). We omit the exact steps of the DP, inter-ested readers may consult the reference [ 13 ].

This new representation is brought to bear only for the event of generating a corruption of a word and not word(state) transitions. We assume that the probability of emitting a cor-rupted word s j from state w j is Q ( s j | w i ) = 1  X   X  i.e., it has a Gaussian distribution in the feature vector rep-resentation. This has implications in the clustering problem that we have introduced. The KL divergence of the Gaussian distribution is a squared Euclidean distance in the feature vector representation. The squared Euclidean distance in the feature representation can be estimated efficiently without mapping to the feature space F as the dot product (w, s ) can be evaluated efficiently using dynamic programming. The clustering updates in the mapped space is shown in the Fig. 4 . One aspect needs to be clarified, the quantity Y  X  s described the feature space representation of words only. The feature space representation of a distribution over words is the expected feature space distribution of a word drawn from the distribution, or equivalently a weighted summation of the feature space representation of all the words that have non-zero probabilities with the weights being the word prob-abilities. 3.4 Transformation from central to pairwise clustering It is possible to minimize the cost function ( 5 )afterthe strings have been mapped to F by the function as indi-cated in Fig. 4 . However, such a procedure does not capi-talize on the property that the dot-product of two vectors in F space may be computed efficiently using dynamic pro-gramming in their data space. Such a method has another problem, the update equations require computing the clus-ter means  X  t + 1 (( c i  X  Y )),  X  t + 1 (( Y  X  c i )). If we are to work in the data space in order to utilize the dynamic pro-gramming methodology, the feature vector representation of these clusters have to be inverse mapped into words. Given a mean in subsequencized feature space we do not know how we can compute its image in the string data space that will be necessary to compute the distance of this mean from a word efficiently.

This problem can, however, be overcome by a transfor-mation from central clustering to pairwise clustering. We execute a transformation of the cost function from a central clustering that minimizes distance from cluster member to cluster center as indicated by the objective function ( 5 )toan equivalent pairwise clustering. This transformed clustering relies on the computation of pairwise distances and hence can be posed in terms of the dot products computed efficiently in the data space. For the said transformation we exploit the lemma below: Lemma 2 For a set of distributions f i and its mean  X ( f the sum of distances from distribution to its mean may be computed as a sum over pairwise distances The total central cost of clustering can hence be expressed as a sum of all within pair distances. Given the fact that the sum total of all pairwise distances is constant, the central clus-tering objective function is the difference between the total pairwise distances and the sum of all within pair distances. Hence, this can be approximated by the Min-Cut formulation where C is a cluster partitioning of the symbols.
 Argmin C + cos (( s The key advantages of this formulation is that the min-Cut solvers only require computation of pairwise distance (sim-ilarities) which can be computed by DP in data space. No explicit representation of the mean is required. Furthermore graph partitional methods have real relaxations that have global minima [ 16 ] and those global minima can be used to initialize the search for a good local minima in the non-relaxed space. 4 Experiments The algorithm was implemented to analyze a dataset that con-sisted of 150,000 SMS messages to a call center for a telecom service provider and also evaluated on a public SMS corpus. Because of lack of ground truth on misspelled or shortened words for the call center dataset, we are restricted to anec-dotal evidence only. Because of strong colloquial/ vernac-ular/multilingual content and inclusion of domain specific words, labeling expertise is not available. The messaging language consisted of a mix of English and several Indian regional languages primarily Hindi but including Punjabi, Gujarati and Bengali with language transitions happening within a sentence. The dataset contained several (about half of the vocabulary) non-English words that were transliterated into Latin characters but without any standardization. The scheme varied with the person sending the message. Since many of the messengers were non-native speakers of English, English words were phonetically transliterated, leading to very non-standard and inconsistent spellings. Examples of some typical messages selected from the corpus using the regular expression  X  X ceam|scheme X  is given in Fig. 5 .The counts of the frequent words was found to obey a power law, see Fig. 6 .

The objective of the analysis was to eventually facilitate text-mining tasks on the data set such as  X  Frequent complaint/issue mining  X  Clustering of incoming messages  X  Classification of incoming messages.
 All these tasks are adversely affected by the noise in the data as a result of which frequencies of phrases are under represented. SMS to SMS similarities are under estimated because the non-standard spellings leads to the phenomenon that words common to both messages are not registered as the same. Non-standard spellings also expand the vocabulary set so that the classifiers have to operate in an artificially mag-nified feature set. Mapping misspelled, transliterate, pho-netically spelled, abbreviated words to their frequent and conventional spelling thus forms a vital step to enable text mining on data sets such as this.

Preprocessing steps consisted of removing punctuation and non-alphanumeric characters. Replacing monetary amounts and telephone numbers by tags &lt; Rs &gt; and &lt; phone &gt; . Each sms was tokenized and the word dis-tributions P ( Y  X  s ) and P ( s  X  Y ) were computed. In order to account for a higher order Markov language model and to incorporate smoothing, these statistics were estimated using a kernel estimator with a fixed window and a exponentially decaying weight over the window. To compute P ( Y  X  s ) we considered words occurring in a window of length 4 before a given word s and similarly for P ( Y  X  s ) . In this window, the frequencies were estimated using the  X  X ag of words X  model. The optimal window size was tuned using a small hand labeled test set.

For a window size k and length of a message L computing the counts for P ( Y b | Y ) would require O ( Lk ) time. This can be sped up using dynamic programming noting the recursion Bucket ( k , p ) = Bucket ( k / 2 , p ) + Bucket ( k / 2 , where Bucket ( k , p ) is the content of the window of width k positioned at an offset p from the start of the message. This dynamic programming reduced the time requirement from linear to O ( L log k ). Each word in the corpus was rep-resented by the frequency of subsequences occurring in the front bucket and the back bucket, computed over the entire corpus. These frequency vectors were then clustered using min-cut algorithm using cosine similarity on the subsequence space as the similarity measure. The clustering tool used was Cluto [ 10 ].

In the absence of labels we report anecdotal evidence to show that the algorithm does manage to form canoni-cal clusters in an unsupervised setting. Some examples of the corruption of English and regional words detected are listed in Figs. 7 and 8 , respectively. We also used a graph partitional approach for separation of the clusters into dif-ferent languages, but omit details because of space limita-tions. Detected clusters not only consisted of phonetically confusable words but also words with common usage pat-terns,theseareshowninFig. 9 . One can observe that the word  X  X djective X  gets clustered with  X  X xecutive X  because the word  X  X djective X  is exclusively used in the data set as a phonetic analogue of  X  X xecutive X . Some clusters consisted of dates, names, and names of entrance and board examinations as shown in Fig. 10
We also validated our approach on a public SMS data-set. The NUS SMS Corpus [ 9 ] consists of over 10,000 SMS messages and represents the largest corpus of public-domain SMS messages for use in research. Some interesting clusters are shown in Fig. 11 .

In Fig. 12 , we present a comparison between the simpli-fied HMM approach of this paper and a naive cosine distance based clustering approach. For every token we created a left and right context vector. We then clustered the tokens based on nearness as measured by the cosine distance between their left and right context vectors. The naive approach produces clusters that are both noisy and incomplete. The example shown in the figure shows this deficiency in capturing the weeks of the day from the NUS SMS Corpus. 5 Conclusion and future work In this paper we proposed a simplified HMM that can be used for de-noising and canonicalization of text from noisy sources such as IM and SMS. The HMMs were factorized in a way that led to efficient algorithms for training that operated by clustering vectorial data and did not require expensive dynamic programming. We introduced a principled transfor-mation of a central clustering posed in terms of KL diver-gence into a pairwise clustering using cosine similarity. This transformation facilitated the use of subsequence kernels to represent words and deal with deletion and multiple character substitution noise. In spite of a totally unsupervised setup, the algorithm managed to collect together non-standard spellings of different words and meanings.

There are several avenues to take for future research. One aspect that we have not yet paid attention to is to select the right number of clusters from the data. The performance of the algorithm does depend on using an appropriate value. Another aspect to follow up is to use some minimal supervision to guide the formation of better clusters, such a semi-supervised approach is future work, as well as exploring alternative clustering strategies that use pairwise distance or similarity measures.

The equivalence to pairwise clustering makes approxi-mate methods of graph cuts especially those using spectral techniques available. Comparing the performance of near global versus the greedy methods will be interesting. But under the lack of labeled data validation or comparison of the clusters is difficult, so some form of human labeling is required for such further tests.
 References
