 We present our early explorations into developing a data mining based approach for enhancing the quality of text-books. We describe a diagnostic tool to algorithmically identify deficient sections in textbooks. We also discuss techniques for algorithmically augmenting textbook sections with links to selective content mined from the Web. Our evaluation, employing widely-used textbooks from India, in-dicates that developing technological approaches to help im-prove textbooks holds promise. Education is known to be a key determinant of economic growth and prosperity [49; 25]. While the issues in devis-ing a high-quality educational system are multi-faceted and complex, textbooks form one type of educational input most consistently associated with gains in student learning [46]. They are the primary conduits for delivering content knowl-edge to the students and the teachers base their lesson plans mainly on the material given in textbooks [21].
 Considerable research has gone into investigating what makes for good textbooks [24; 30; 48]. There has also been work on designing ideal textbooks [9; 32; 43]. While several factors determine the quality of a textbook, there is general agree-ment that the good textbooks should present concepts in a coherent manner and provide adequate coverage of impor-tant concepts.
 Unfortunately, many textbooks, particularly from emerging regions, suffer from two major problems: (1) the lack of clar-ity of language and incoherent presentation of concepts, and (2) inadequacy of information provided [1]. We quote from a critique of a grade IX Indian History textbook [36]:  X  X he whole (medieval) period has been presented as a dull and dry history of dynasties, cluttered with the names and mili-tary conquests of kings, followed by brief acknowledgements of  X  X ocial and cultural life X ,  X  X rt and architecture X ,  X  X evenue administration X , and so on. The entire Mughal period (1526-1707) is disposed of in six pages. X  In order to address the first problem, we present a diag-nostic tool for algorithmically identifying those sections of a textbook that are not well-written and hence can benefit from rewriting. The tool uses a probabilistic decision model, which is based on the notion of the dispersion of key con-cepts occurring in the section and the syntactic complexity of writing [5].
 To address the second problem, we draw upon the learn-ing research that shows that the linking of encyclopedic information to educational material can improve both the quality of the knowledge acquired and the time needed to obtain such knowledge [13]. It is also shown that the use of visual material enhances learning, not only by enabling retention of information but also by promoting comprehen-sion and transfer [11; 39]. We, therefore, present techniques for algorithmically augmenting textbook sections with links to selective articles and images mined from the Web. For this purpose, we identify key concept phrases occurring in a section, which are then used to find web articles represent-ing the central concepts presented in the section [6]. Using them, we also mine web images most relevant to a section, while respecting the constraint that the same image is not repeated in different sections of the same chapter [4]. We have applied the proposed techniques to high school text-books published by the Indian National Council of Educa-tional Research and Training (NCERT). The preliminary results are encouraging and indicate that developing tech-nological approaches to improving textbooks is a promising direction for research.
 The paper proceeds as follows. We first describe our method for determining key concepts and the relationships between them in  X  2. We then discuss our methodology for diagnosing deficient sections in  X  3. Techniques for augmenting sections with authoritative web articles and images are presented in  X  4 and  X  5 respectively. Illustrative results from the empirical evaluation of these techniques are presented in  X  6. Finally,  X  7 presents conclusions and directions for future work. We have derived this paper from papers published elsewhere [4; 5; 6]. Here, we focus on describing the main ideas and tech-niques and refer the reader to the original papers for in-depth descriptions. The basic building block underlying our approach is the identification of key concepts described in the book and in-ferring the relationships between them. We discuss it first. If a textbook includes a back-of-the-book index [38], it can be used for obtaining concept phrases. Unfortunately, not all books contain such indices; e.g. , in a study reported in [8], only 55% of the 113 books examined included them. Fortu-Algorithm 1 DetermineKeyConcepts Input: A section of text s ; Pattern R for detecting terminological noun phrases; Pruning parameters  X .
 Output: The set of key concept phrases for s . 1: Tag every sentence in s using a POS tagger. (  X  2.1.1 ) 2: Compute the set C of terminological noun phrases that 3: Prune phrases from C whose POS tagging is inconsistent 4: Prune common phrases from C based on the probability 5: Return C . nately, there is rich literature on algorithmically extracting key phrases from a document that can guide the task of extracting key concepts [7; 42; 50].
 After studying several textbooks, we devised the following approach (Algorithm 1). Concepts in our system correspond to terminological noun phrases . We first form a candidate set of concepts using linguistic patterns, with the help of a part-of-speech tagger. We used two of the linguistic patterns proposed in [31] that have been used widely in the NLP community. We supplemented this set by a third pattern based on our inspection of the key concepts we identified by studying books on different subjects. We then exploit com-plementary signals from a different source, namely, a lexical database, to correct errors made by the part-of-speech tag-ger. Next we eliminate both malformed phrases and very common phrases, based on the probabilities of occurrences of these phrases on the Web. The reason for eliminating common phrases is that they would be already well under-stood.
 Our implementation employs the Stanford POS Tagger [45] for part-of-speech tagging, WordNet [18] as the lexical database, and Microsoft Web N-gram Service [47] to aid pruning of malformed and common phrases. Our methodology, how-ever, is oblivious to the specific tools, though the perfor-mance of the system is dependent on them. We summarize our approach in Algorithm 1 and discuss each step in detail below. We tag every sentence in the given text using Stanford POS Tagger. We note that one could also use a shallow parser ( e.g. [2]) for this task. The tagger assigns a unique part-of-speech to each word in a sentence. It predicts the part-of-speech tag even for an unknown word (such as a proper noun) by exploiting the context of the word in a sentence. The corpus may contain poorly formed sentences, due to pdf parsing issues as well as the presence of text extracted from tables, mathematical equations, and other non-grammatical structures. For such sentences, the assigned part-of-speech tags may be incorrect. We next form a candidate set of concepts by determining the terminological noun phrases present in the text. The concepts of interest in our application typically consist of noun phrases containing adjectives, nouns, and sometimes prepositions. It is rare for concepts to contain other parts of speech such as verbs, adverbs, or conjunctions.
 We consider three patterns ( P 1 , P 2 , and P 3 ) for determin-ing terminological noun phrases. The first two of these are from [31] and the third is the one we added. We can express the three patterns using regular expressions as: where N refers to a noun, P a preposition, A an adjective, and C = A | N . The pattern P 1 corresponds to a sequence of zero or more adjectives or nouns, ending with a noun, while P 2 is a relaxation of P 1 that also permits two such pat-terns separated by a preposition. Examples of the former include  X  X umulative distribution function X ,  X  X iscal policy X , and  X  X lectromagnetic radiation X . Examples of the latter in-clude  X  X egrees of freedom X  and  X  X ingdom of Asoka X . P corresponds to a sequence of zero or more adjectives, fol-lowed by one or more nouns. This pattern is a restricted version of P 1 , where an adjective occurring between two nouns is not allowed. The motivation for this pattern comes from sentences such as the following:  X  X he experiment with Swadeshi gave Mahatma Gandhi important ideas about us-ing cloth as a symbolic weapon against British rule X . As a consequence of allowing arbitrary order of adjectives and nouns,  X  X ahatma Gandhi important ideas X  is detected as a terminological noun phrase by pattern P 1 . On the other hand, pattern P 3 would result in the better phrases,  X  X a-hatma Gandhi X  and  X  X mportant ideas X .
 Our candidate concepts comprise of maximal pattern matches. Thus, we will not have  X  X istribution function X  as a candi-date in the presence of  X  X umulative distribution function X . The intuition is that it is better to have more specific con-cepts than general concepts. A similar strategy was used in [33].
 It was found in the empirical study reported in [6] that the pattern P 1 outperforms P 2 . The pattern P 3 exhibited slightly better performance than P 1 in this study. The Stanford POS Tagger can make errors on poorly formed sentences. We experimented with using WordNet to detect these errors and correct them. WordNet is a large lexical database that groups words into sets of cognitive synonyms called synsets, each expressing a distinct concept. We use WordNet to determine possible parts of speech (noun, ad-jective, verb, adverb) for words in its knowledge base. How-ever, WordNet would fail to recognize words absent in its database. WordNet being a hand curated system should have better accuracy than an automated parsing tool, but lower coverage. We therefore use WordNet as a validation and error-correcting tool.
 We check whether the parts of speech tags assigned by the Stanford POS Tagger are consistent with those provided by WordNet. We say that disagreement occurs for a phrase if for some word w in the phrase, (a) WordNet recognizes w and returns one or more part-of-speech tags and (b) the part-of-speech tag assigned by the Stanford POS Tagger is not among the part-of-speech tags assigned by WordNet. For example, for the phrase  X  X teatite micro beads X , the Stan-ford POS Tagger assignment is &lt;Adjective&gt;&lt;Noun&gt;&lt;Noun&gt; In such cases, we change the POS Tagger assignment to the WordNet assignment, provided the latter still satisfies the linguistic pattern. In the above example, the assignment will be modified to &lt;Noun&gt;&lt;Adjective&gt;&lt;Noun&gt; . However, there may be cases where the WordNet assignment is not unique. For example, for the phrase  X  X ontrol mea-sures X , WordNet has a non-unique assignment: &lt;Noun|Verb&gt;&lt;Noun|Verb&gt; . Thus, the POS Tagger assign-ment &lt;Adjective&gt;&lt;Noun&gt; is in disagreement with the Word-Net assignment, but it cannot be uniquely corrected and hence we drop the phrase from the candidate set.
 The empirical evaluation demonstrated that a lexical database such as WordNet can be quite complementary to a generic part-of-speech tagger such as the Stanford POS Tagger, and we were able to successfully use WordNet for correcting er-rors made by the POS tagger [6]. The set of candidate phrases generated in the previous step is likely to contain a number of common knowledge phrases as well as some malformed or unimportant long phrases. For identifying such phrases, we obtain the probability of occurrence of the phrase on the Web using the Microsoft Web N-gram Service. We use this probability as a proxy for whether the phrase is part of common knowledge, since a common knowledge phrase is likely to have a significant presence on the Web. Similarly this probability can also indicate whether the phrase is malformed, as such phrases are less likely to occur on the Web. Thus, after obtaining the probability scores for each phrase, we compute the score distribution across phrases over the entire corpus, and prune based on this distribution to remove undesirable phrases. The Microsoft Web N-gram Service provides the probability of occurrence of a given phrase over three corpora: bod-ies of web pages, titles of pages, and anchor texts for web pages. Compared to title or body, we found that the an-chor provided a stronger signal, perhaps because the anchor text represents how other web authors succinctly describe the target page.
 Given the distribution D of N-gram log probability scores of candidate phrases, we compute certain parameterized statis-tical boundaries. Let Q 1 denote the first quartile, that is, Q satisfies Pr x  X  D ( x  X  Q 1 ) = 0 . 25. Similarly let Q third quartile, that is, Q 3 satisfies Pr x  X  D ( x  X  Q 3 The interquartile range IQR = Q 3  X  Q 1 is a measure of mid-spread of the distribution. Given non-negative param-eters t 1 and t 2 , we can define fences on both ends of the distribution: We prune phrases whose scores are not within the fences as the phrases with scores below the lower fence ( LF ( t 1 )) are likely to be malformed and those with scores above the upper fence ( UF ( t 2 )) are likely to be of common knowledge. As the distribution of scores is not symmetric around the mean, we may need to select different pruning parameters. Our empirical evaluation showed that our approach was quite effective in identifying and pruning concepts that were malformed or represented common knowledge concepts [6]. Algorithm 2 DetermineConceptGraph Input: The set of key concept phrases C for a given section s ; An authoritative structured external source of concepts that also contains relationships between them (e.g. Wikipedia).
 Output: The concept graph for s . 1: Determine the set V of nodes corresponding to concepts 2: Let W denote the set of all links in the external source. 3: Return G . Having determined the set of concepts, a straightforward ap-proach to derive relationships between concepts would be to manually label the concept pairs. However, labeling is a la-borious and subjective task. We instead consider an author-itative structured external source of concepts that also con-tains relationships between the concepts and use it to infer relationships between the textbook concepts (Algorithm 2). Our implementation maps textbook concepts to Wikipedia articles and treats a concept c 1 to be related to another concept c 2 if the Wikipedia article corresponding to c 1 a link to the Wikipedia article corresponding to c 2 . We only consider concept phrases that match the title of a Wikipedia article exactly. If any Wikipedia article is redirected to an-other article, we follow the redirect link till an article is found. We then consider the directed graph induced by these mapping articles and the Wikipedia links between them, thereby obtaining a concept graph that encapsulates the re-lationships between the concepts. Our decision model for identifying a poorly written sec-tion is based on the dispersion of key concepts mentioned in the section and the syntactic complexity of the writing. The model requires a tune set for learning its parameters. While human judgments may seem like an obvious way to obtain a tune set, it is difficult to assemble a sufficiently large group of qualified judges who can provide consistent ratings. Hence we generate the tune set automatically in a novel way. This procedure maps sampled text book sec-tions to the closest versions of Wikipedia articles having similar content and uses the maturity of those versions to assign need-for-exposition labels. The maturity of a version is computed by considering the revision history of the cor-responding Wikipedia article and convolving the changes in size with a smoothing filter. We first discuss the rationale for choosing these decision variables and formally define them, followed by a discussion of the model and the generation of the tune set. After going through several textbooks, we observed that a section that discussed concepts related to each other was more comprehensible than one that discussed many unre-(a) A section with very low dispersion (Grade IX Mathematics book) Algorithm 3 ComputeDispersion Input: A textbook section s .
 Output: Dispersion value for section s . 1: Compute the set of concepts C present in s . (  X  2.1) 2: Infer the concept graph E for the concepts in C . (  X  2.2) lated concepts. We formally capture this intuition by defin-ing a measure of dispersion over key concepts.
 Let V represent the set of key concepts in a section s . Let rel be a binary relation that determines whether a concept in V is related to another concept in V , that is, rel ( x,y ) is true if concept x is related to concept y and false otherwise. We define dispersion of a section as the fraction of ordered key concept pairs that are not related: dispersion ( s ) := We note that dispersion takes values between 0 and 1, with 0 corresponding to a section where all key concepts are mu-tually related and 1 corresponding to a section with mutu-ally unrelated key concepts.
 Algorithm 3 describes the computation of dispersion for a given section. We first identify concepts following the method discussed in  X  2.1, employing the pattern A  X  where A is an adjective and N a noun. We then obtain the concept graph of relationships as described in  X  2.2, with isolated nodes removed. We note that the dispersion as de-fined in Eq. 1 is the same as 1 minus the edge density of this resulting graph, which we compute in the last step of Algorithm 3.
 We illustrate our notion of dispersion through some exam-ples from the NCERT textbooks. Figure 1(a) and 1(b) show the concept graphs for two sections with small dispersion. The first section titled  X  X ypes of Quadrilaterals X  from the Grade IX Mathematics book has 19 directed edges over 6 nodes with dispersion 0 . 37 and the second section titled  X  X harged Particles in Matter X  from the Grade IX Science book has 29 directed edges over 8 nodes with dispersion 0 . 48. Indeed the concepts within each of these sections are quite related to each other, resulting in low dispersion values. Fig-ure 1(c) shows the concept graph for a section with large dispersion (with some isolated nodes also shown). This sec-tion titled  X  X ariety of Methods X  from Grade XII Sociology book has 9 edges over 13 non-isolated nodes, contributing to a dispersion value of 0 . 94. The section discusses rather unrelated concepts, leading to large dispersion. To measure syntactic complexity of writing, our first in-stinct was to use readability formulas [16]. Table 1 sum-marizes some of the popular ones and the variables they use. We observe that all formulas base their calculations on two classes of variables. First, they all use a sentence structure measure, generally sentence length, the underly-ing intuition being that longer sentences are harder to read and comprehend. The sentence length can be in terms of the number of letters or the number of words, though the empirical evidence from past studies overwhelmingly favors the number of words. The second measure they use cap-tures the difficulty of the vocabulary at word level in terms of word familiarity or word length. The Dale long list [14] is frequently used for computing word familiarity. We do not employ word familiarity because of potential vocabu-lary mismatch between textbooks written in local variants of English and the Dale list. The word length can be defined in terms of the number of syllables or the number of letters. Both the Coleman-Liau Index and the Automated Readabil-ity Index calculate word lengths as the number of letters. Their primary consideration, however, is data processing ef-ficiency and the effectiveness of this approach is suspicious [16]. Another approach is to compute word length in terms of the number of syllables, the intuition being that words with more syllables are more complex.
 We also note that different readability formulas combine the above two measures differently and the combinations are learned with respect to specific datasets (often McCall-Crabbs Standard Test Lessons in Reading [35]). As a result, these formulas are highly correlated, a fact we confirmed in our experiments. We find it unnatural to directly use the readability scores determined by these formulas as variables in the decision model.
 After considerable experimentation, we settled on the follow-ing two variables as measures for the syntactic complexity of writing: 1. Average sentence length : average number of words per 2. Average word length : average number of syllables per See [12; 17] for algorithms for computing the number of syl-lables per word. The number of syllables in a word can also be approximated by counting consonant-separated vowels. Each group of adjacent vowels counts as one syllable (for ex-ample,  X  X a X  in  X  X eal X  contributes one syllable, whereas  X  X ...a X  in  X  X egal X  contributes two syllables), but an  X  X  X  occurring at the end of a word does not contribute to syllable count. Each word has at least one syllable. We take a learning approach to arrive at the model for de-ciding whether a book section can benefit from rewriting. Our proposed model is probabilistic and its parameters are learned using an algorithmically generated tune set. The tune set consists of sections with different maturity, the in-tuition being that the more immature a section, the greater the need for its revision. Our goal is to learn a decision model that can provide a probabilistic score of whether a textbook section requires revision based on the values of decision variables for that section. We would also like such a decision model to auto-matically learn the relative importance between the decision variables. The binary logistic regression eminently lends it-self to this desiderata.
 Let z represent a section X  X  decision variables: a three di-mensional vector whose components are the average sen-tence length, average word length, and dispersion. Given z , the binary logistic regression predicts the probabilistic score that a section needs revision (i.e., label y = 1) through the logistic function: The parameter w is the weight vector of the function, with each component w j measuring the relative importance of the decision variable z j for predicting the label y .
 The weight vector w is learned from a tune set consisting of N textbook sections: { Z , y } = { ( z 1 ,y 1 ) ,..., ( z with ( z i ,y i ) representing the decision variable vector z the label y i for the i th textbook section. The optimal w is Algorithm 4 GenerateTuneSet Input: A collection of sections from a textbook corpus; A collection of versioned documents from an authoritative web resource such as Wikipedia; Threshold parameters  X  and  X  2 .
 Output: A tune set consisting of a subset of sections, each labeled either 1 (Revise) or 0 (Don X  X ). 1: for each section s do 2: Map section s to a set W ( s ) of most similar versioned 3: Compute immaturity score  X  m ( v ) for each versioned 4: Compute immaturity score m ( s ) for section s by ag-5: Label ( s ) := 1 if m ( s ) &gt;  X  1 ; 0 if m ( s ) &lt;  X  6: Output  X  s,Label ( s )  X  for sections s where Label ( s ) is ei-the one that maximizes the conditional log-likelihood of the labels in the tune set: Given the difficulty of obtaining manual judgments, we pro-pose using meta data associated with textbooks to obtain labels. One such meta data is the immaturity level of a sec-tion; an immature section hinders the positive learning ex-perience of a student, and therefore calls for revision. How-ever, immaturity computation requires access to rich data such as extent and timing of the revisions, which is typi-cally not available for textbooks. We, therefore, resort to an indirect device for estimating the maturity of a section. We note that authoritative information resources on the Web, such as Wikipedia, are created through collective ef-forts of multiple authors. The content gets repeatedly up-dated until writers expressing opinions on the subject come to a consensus. As new information becomes available, this cycle of revisions repeats. A key aspect of such web re-sources is that the revisions are recorded and maintained by the source. Hence, we map a textbook section to the most similar version of a similar article in a web resource and use the immaturity of that version as the proxy for the immaturity of the textbook section.
 The tune set generation is outlined in Algorithm 4. We sample a subset of textbook sections across all subjects and classes. For each section, we find a small set of closest match-ing versions in the web resource that are similar in con-tent. The matches are found using the technique described in  X  3.2.3 . We then compute the immaturity for these ver-sions using the technique given in  X  3.2.4 . The immaturity scores are then aggregated through a weighted combination (weights are the normalized similarity scores) to produce the maturity score for the textbook section. This score is then converted into a decision on what label should be assigned to this book section. Note that we need only a small amount of labeled data since the model has a very small number of parameters.
 We observe that the immaturity computation is reliable only at the extreme ends: very high values or very low values of scores. The parameters  X  1 and  X  2 allow us to achieve this goal. Their values are empirically determined, balancing the need for high precision with the need for having sufficient labeled data. In a document model where each document is treated as a bag (multi-set) of words, a well-known measure of similarity between documents A and B is the Jaccard index, defined as sim ( A,B ) := | A  X  B | / | A  X  B | . Here, we note that the terms and their associated weight, i.e., importance (e.g., tf-idf ) in the document gives raise to the multi-set representation of the document. Thus A = { &lt; x 1 ,w x 1 &gt;,&lt; x 2 ,w x ,w x n &gt; } . The large and often varying sizes of documents (i.e., cardinality of the sets) and further, terms with varying weights can make this similarity computation expensive. We use the well-known min-wise independent permutations [10] to get around these problems.
 We proceed as follows. Given a document A, we convert it to a set  X  A = { x  X  A | x  X  A V R ( x )  X  w x } using a con-sistent hash function R ( x ) that maps words in the docu-ment uniformly and randomly in the interval [0 , 1]. In other words, we include the significant terms in the document in the newly defined document set. Next, we compute the min-wise independent permutation of  X  A as MH (  X  A ) := arg min { R ( x ) | x  X   X  A } . Thus, MH (  X  A ) denotes the leftmost ele-ment of  X  A in the permutation. Now, for any two documents A and B , | A  X  B | / | A  X  B |  X  Pr[ MH (  X  A ) = MH ( Finally, we compute H min-hashes to yield the sketch of A , S ( A ) = { MH 1 (  X  A ) ,MH 2 (  X  A ) ,...,MH H (  X  for sim ( A,B ). Consider a web repository in which a new version of a doc-ument is created at the end of the day, ignoring multiple updates to the document within a day. Older versions of a document are saved when a new version is created. We ob-serve that paraphrasing, additions or deletions indicate the amount of revision. Thus, the relative change in the size of the document is an indicator of the maturity of a version (the smaller the change, the higher the maturity). The number of days for which a version remains the latest version is also an indicator of the maturity of the version (the longer the duration, the higher the maturity). Finally, people tend to consult nearby versions when creating a revision. Thus, ma-turity is a local phenomenon driven by local context. Armed with these observations, we proceed as follows.
 Assume days are numbered from 1 to the current day T . Algorithm 5 AugmentWithArticles Input: A textbook section s .
 Output: An ordered list of links to top k articles for embellishing section s . 1: Compute the set C of concepts present in s . (  X  2.1) 2: Infer the concept graph E for the concepts in C . (  X  2.2) 3: Compute k . (Eq. 2) 4: Compute the authority score of nodes in E . 5: Return the top k nodes in the decreasing order of node Consider a document whose initial version v 1 was created on day 1. Let L be a vector of length T whose i th component L i is equal to the size of the document (in number of words) on day i . Define a vector  X  ( L ) whose i th component is the relative change in document size between neighboring days i and i  X  1: For a particular version v created on day d , we define its immaturity  X  m ( v ) to be the value of convolution between  X  ( L ) and a smooth filter h on day d : where K is a parameter of the filter used in the convolution. The convolution with a smooth filter allows for modeling immaturity as a smooth continuous process, and the use of local neighborhood enables incorporating local context. We employ the frequently used Hann Filter that has K days spatial support with a smooth fall off in the chosen K sized neighborhood.
 We note that there have been efforts ( e.g. [26]) to assign quality index to Wikipedia articles taking into account edit history of the article such as the frequency and size of edits and the type and reputation of the authors. However, we are not aware of any work targeted at computing the matu-rity of an arbitrary version of a Wikipedia article, and the technique we presented could be of independent interest. Our goal is to embellish a textbook section with links to au-thoritative articles most relevant to the central concepts dis-cussed in the section. Our implementation uses Wikipedia as the source of supplementary material. In order to con-tain the cognitive burden on the reader, we add only up to k links.
 As described in Algorithm 5, we first identify key concept phrases present in the section. We use the algorithm from  X  2.1 for this purpose. We next form the concept graph for the section, inferring relationships between the concepts thus identified, using the algorithm from  X  2.2. We now com-pute the authority score ( e.g. page rank) of the nodes in the concept graph. We then sort the nodes in the decreasing or-der of their authority scores, select top k nodes, and augment the section with links to articles corresponding to them. The intuition is that the central concepts present in a section will be related to many concepts mentioned in the section. Fur-ther, given the progressive learning nature of the textbooks, it is worthwhile to exclude concepts that have already been used for augmentation earlier in the textbook.
 The number of articles, k , selected for embellishing a sec-tion can be determined using the distribution of the node authority scores. The node authority scores appear to fol-low Zipf X  X  ranked distribution, X r  X  r  X  1 / X  , where X r value of the r th ranked node authority score and  X  is the tail index parameter of the underlying Pareto distribution. The tail index can be estimated by regressing the log of order statistics on the log of the scores. For a desired coverage, c (say, 80%) and a limit k 0 (say, 3) on the maximum number of concepts to be shown, we obtain where n is the number of nodes in the induced graph. While this determination can be made empirically as well, we pro-pose fitting Zipf X  X  distribution to the node authority scores as it will help to characterize the distributions over differ-ent subjects and over different grade levels, with varying  X  values. Our goal is to find a small number of images that are most relevant to enhance the understanding of a particular section of the textbook, shunning repetition of an image in differ-ent sections of the same chapter. Our solution has three components: Image Mining . This component comprises of algorithms that mine the web for images relevant to a particular sec-tion and provide a ranked list of top k images along with their relevance scores. It is preferable to have algorithms that make use of orthogonal signals in their search for im-ages in order to have a broad selection of images to choose from. We provide two specific algorithms, namely Comity and Affinity , which satisfy these properties.
 Image Assignment . The image mining algorithms provide locally optimal solution in that they yield images that are best suited for the given section. Consequently, the same image might be selected for different sections of a chapter, giving rise to the need for chapter level optimal assignment of images.
 Given a set of candidate images and their relevance scores for every section of a chapter, the image assignment component assigns images to various sections in a way that maximizes the relevance score for the chapter while maintaining the constraints that no section has been assigned more than a certain maximum number of images and no image is used more than once in the chapter. We provide a polynomial time algorithm for implementing the optimizer.
 Image Ensembling . Since the relevance scores provided by different image mining algorithms will in general be incom-parable, the assignment of images to different sections of a chapter needs to be performed separately for each algo-rithm. The image ensembling component aggregates the ranked lists of image assignments to produce the final re-sult.
 Algorithm 6 Comity Input: A textbook section j ; Number of desired image results k ; Number of desired image search results per query t ; Number of desired concept phrases c .
 Output: A list of top k image results from web, along with relevance scores. 1: Obtain c concept phrases from section j . (  X  2.1) 2: Form queries consisting of two and three concepts 3: Obtain top t image search results for each of the queries 4: Aggregate over (potentially e ( c 2 + c 3 )) lists of images, 5: Return top k images along with their  X  ij values. While it is possible to use any rank aggregation algorithm, we wanted a voting scheme that considers all the elements of a ranked list and provides consensus ranking. The popular Borda X  X  method fits the bill [41]. Ensembling is done sequen-tially within a chapter, starting from the first section. Top images selected for a section are eliminated from the pool of available images for the remaining sections. The image assignment is then rerun, followed by an ensembling for the next section.
 We first present two algorithms for mining relevant images from the web, followed by the image assignment component consisting of an optimization problem, and finally the image ensembling component. Here we give particulars of the Comity and Affinity al-gorithms, the two algorithms used for obtaining the ranked list of top k images along with their relevance scores for a given section. Note that our system design admits various possible variants of these algorithms as well as additional image mining algorithms one could conceive.
 One might think that one could simply use the text string of a section to query a commercial image search engine and obtain the relevant images. However, the current search engines do not perform well with long queries [27]. Indeed, when we queried the search engines using even the first para-graph of a section, we got none or meaningless results. In one major stream of research on information retrieval with long queries, the focus is on selecting a subset of the query, while in another it is on weighting the terms of the query [51]. This body of research however is not designed to work for queries consisting of arbitrary textbook sections.
 Algorithm 6 ( Comity ) is based on using the key concepts present in a section to query the commercial image search engines. However, each concept phrase in isolation may not be representative of the section as a typical book section can discuss multiple concepts. Hence we form c 2 + c 3 image search queries by combining two and three concept phrases each, in order to provide more context about the section. A relevant image for the section is likely to occur among the top results for many such queries. Thus, by aggregating the image result lists over all the combination queries, we end up boosting the relevance scores of very relevant images for the section. We further increase the coverage by obtaining Algorithm 7 Affinity Input: A textbook section j ; Number of desired image results k ; Number of desired closest articles from an authoritative external source t 0 ; Number of desired concept phrases c .
 Output: A list of top k image results from the authorita-tive source, along with value scores. 1: Obtain c concept phrases from section j . (  X  2.1) 2: Obtain top t 0 closest articles from the authoritative ex-3: Extract the set of images present in these t 0 articles, as 4: For each image i , let n ij := Number of articles in which 5: Assign the relevance score  X  ij := n w 1 ij  X  d w 2 ij 6: Return top k images along with their  X  ij values. and merging results across e different search engines. We treat each search engine as a blackbox [27], that is, we have access to the ranking of results but do not have access to the internals of the search engine such as the score given to a document with respect to a query.
 Aggregation across multiple lists is performed as follows. Each of t images in a result list is assigned a position-discounted score equal to 1 / ( p +  X  ) where p denotes the po-sition and  X  is a smoothing parameter. For the same image occurring in multiple lists, the scores are added, weighted by a function f of the importance of the concept phrase present in the underlying query:  X  ij := P q f (Importance scores of concept phrases used in q )  X  (1 / ( p ( i,q,R ( q )) +  X  )). Here the summation is over e ( c 2 + c 3 ) queries issued and p ( i,q,R ( q )) denotes the position of image i in the result list R ( q ) for query q if i is present in R ( q ) and  X  otherwise. This choice is based on our empirical observation that an image occurring among the top results for multiple queries was more relevant to the section than an image that occurred among the top results for only one query.
 The intuition behind this algorithm is the observation that the images included in an authoritative article relevant to a topic are often illustrative of the key concepts underlying the topic. We therefore find authoritative articles whose contents have high textual similarity with a given section of the book. We then extract images contained in these articles and use their relevance scores to find top k images for the section.
 Algorithm 7 ( Affinity ) first obtains the key concept phrases present in a section as well as the closest authoritative arti-cles from the web. Thus the key topics discussed in the sec-tion are available in the form of the concept phrases while the search space for images is refined to the set of articles with high document similarity to the section. The relevance score for an image is computed by analyzing the overlap between the concept phrases and the cumulative metadata associated with the various copies of the image present in the narrowed set of articles. The metadata for an image com-prises of text adjacent to the image including caption and alternative text, filename of the image, anchor texts point-ing to the image, and queries that led to clicks on the image. The scoring has desirable properties such as: (a) an image occurring in multiple articles gets a higher score, (b) an im-age whose metadata contains multiple concept phrases gets a higher score, and (c) an image whose metadata contains words from many concepts gets a higher score. Given a set of candidate images relevant to the various sec-tions of a chapter and their relevance scores, the goal of the image assignment component is to allocate to each section the most relevant images, while respecting the constraints that each section is not augmented with too many images and that each image is used no more than once in a chapter. The rationale for these constraints is that an augmentation of a section with too many images will put undue cogni-tive burden on the reader while the repetition of an image across sections in the same chapter would be redundant for the reader.
 First, a few notations. Let I = { 1 , 2 ,...,n } denote the set of images and S = { 1 , 2 ,...,m } denote the set of sections in a chapter. Let  X  ij denote the (non-negative) relevance score of image i  X  I for section j  X  S (  X  ij = 0 if the image i is not present in the candidate set of images for section j ). Let K j denote the maximum number of images that can be associated with section j . K j could be either a fixed integer for all sections or a function of the length of the section j . This problem admits a natural greedy algorithm. Sort the  X  ij values in decreasing order and go through them. At each step, the greedy algorithm picks the highest  X  ij value such that an image can still be assigned to section j (that is, less than K j images have so far been assigned to j ) and then assigns image i to section j . This process ends when either all sections have been assigned the maximum number of images or there are no more images to be assigned. At a first glance, the greedy algorithm might seem optimal in terms of the sum of relevance scores of all assigned images. But the following counterexample shows that the optimal value can be substantially larger. Consider a chapter con-sisting of two sections and suppose that we want two images each for a section ( K 1 = K 2 = 2). Represent by ( i, X  ) that image i  X  X  relevance score is  X  . Let the top images and their relevance scores obtained by an image mining algorithm for various sections be as follows: s 1  X  X  ( i 1 , 1) , ( i 2 the greedy assignment would be s 1  X   X  i 1 ,i 2  X  ,s 2  X   X  i with a total score of 2 + . On the other hand, an optimal assignment is s 1  X   X  i 1 ,i 3  X  ,s 2  X   X  i 2 ,i 4  X  with a total score of 3  X  4 .
 We, therefore, instantiate the image assignment component as an optimization problem. We show that this optimization problem can be solved optimally in polynomial time and provide an efficient algorithm as part of the proof. The following is the statement of the optimization problem: Here, x ij is an indicator variable that takes value 1 if image i is selected for section j and 0 otherwise. Eq. 4 captures this binary constraint. Eq. 5 ensures that the number of images assigned to a section is at most K j . Eq. 6 enforces that each image is assigned to at most one section in a chapter. The optimization objective (Eq. 3) is the total relevance score for the chapter, defined as the sum over all sections of relevance scores of the images assigned to the section. Thus the goal of the optimization is to compute the binary variables x ij such that the total relevance score for the chapter is maximized.
Theorem 5.1. MaxRelevantImageAssignment can be solved optimally in polynomial time.

Proof. The proof follows by showing an efficient reduc-tion from MaxRelevantImageAssignment to the Maxi-mum Weighted Bipartite Matching problem [40], which admits an efficient polynomial time solution. Given an in-stance of MaxRelevantImageAssignment , form a com-plete weighted bipartite graph G = ( V,E ) as follows. As-sociate a node u i with each image i  X  I and associate K j nodes, v j 1 ,v j 2 ,...,v jK j , with each section j . Create an edge between every image node and every section node copy. Weight of the edge ( u i ,v jk ) is set to  X  k  X  { 1 , 2 ,...,K j } , that is, each of the K j edges joining an image i to the section j has the same weight, equal to the corresponding relevance score.
 We observe that any feasible solution to MaxRelevantIm-ageAssignment corresponds to selecting a matching in G . Given a satisfying assignment of x ij  X  X , we can obtain a match-ing in G by picking one of the K j edges corresponding to any x ij that is set to 1. Similarly, given any matching in G , there is a corresponding feasible solution. Further the objective of
MaxRelevantImageAssignment can be maximized by obtaining the maximum weight bipartite matching in G . As the Maximum Weighted Bipartite Matching problem can be solved optimally in O ( nm ( n + m )) time, it follows that MaxRelevantImageAssignment can also be solved optimally in O ( nm ( n + m )) time. We next describe our ensembling algorithm for combining the different image assignments. Since the relevance scores computed by the image mining algorithms will be incompa-rable in general, we combine the results after the MaxRel-evantImageAssignment optimization has been performed independently for each algorithm. We use only the order-ing returned by these algorithms and do rank aggregation without considering the magnitudes of the scores.
 We employ Borda X  X  method to merge l ranked lists corre-sponding to l different image mining algorithms. Borda X  X  Algorithm 8 Ensemble Input: Set of sections S = { 1 , 2 ,...,m } in a textbook chapter; Set of images I = { 1 , 2 ,...,n } ; Number of desired images K j for each section j  X  S ; Scores assigned by l different image mining algorithms for each image i  X  I ; Orderings produced after the optimization for these l algorithms.
 Output: A new assignment of images to sections. 1: Let I 0 := I and S 0 := S . For each of l image mining 2: for section j = 1 to m do 3: Merge l ranked lists (corresponding to l algorithms) 4: Remove the assigned images from consideration for 5: For each of l image mining algorithms, perform method tries to achieve a consensus ranking and satisfies certain desirable properties such as reversal symmetry [41]. It assigns a score corresponding to the positions in which an image appears within each ranked list of preferences, and the images are sorted by their total score.
 However, a consequence of performing rank aggregation for each section independently is that the same image may ap-pear more than once in a chapter. Consider a chapter con-sisting of two sections and suppose that we want two im-ages for every section. Assume that the optimal assignments (ranked lists) corresponding to the two image mining algo-rithms are as follows. Alg 1 (OPT): s 1  X  X  i 1 ,i 2  X  ,s 2 (that is, image i 1 has the highest relevance score and i the second highest score for section s 1 and similarly  X  i in that order are the top two images for section s Alg 2 (OPT): s 1  X   X  i 3 ,i 4  X  ,s 2  X   X  i 1 ,i 2  X  . Then the rank ag-gregation would give: s 1  X  X  i 1 ,i 3  X  , s 2  X  X  i 1 ,i 3 Algorithm 8 ( Ensemble ) avoids this problem by taking ad-vantage of the logical linear organization of sections within a chapter. It considers sections in a chapter sequentially from the first section to the last, ensembling at a section level, and then removing images selected for this section from the pool of available images for the remaining sections. Before mov-ing to a subsequent section, it reruns the image assignment optimization for the remaining sections over the remaining images. Thus images discarded due to merging for a section are taken into account for consideration in subsequent sec-tions as such images may be more relevant than any of the candidate images for a section.
 Consider a chapter consisting of three sections and suppose that we want two images for every section. Assume that the images and their relevance scores for different sections found by the two image mining algorithms are as follows. Alg 1 : s 1  X   X  ( i 1 , 1) , ( i 2 , 0 . 9)  X  , s 2  X   X  ( i  X  ( i 2 , 0 . 5) , ( i 3 , 0 . 4) , ( i 5 , 0 . 3)  X  , and Alg 2 s 2  X   X  ( i 7 , 0 . 6) , ( i 8 , 0 . 4)  X  , s 3  X   X  ( i 4 , 0 . 5) , ( i The optimal assignments would be: Alg 1 (OPT): s 1  X  X  i 1 s  X  X  i 7 ,i 8 } , s 3  X  X  i 3 ,i 5 } , and Alg 2 (OPT): s 1  X  X  i { i ,i 8 } , s 3  X  { i 1 ,i 6 } . The rank aggregation for the first section would give: s 1  X  X  i 1 ,i 3 } , thereby dropping i Alg 1 and i 4 from Alg 2 respectively. We note that i 2 is more relevant than current assignments for section s 3 under Alg and similarly, i 4 is more relevant than current assignments for section s 3 under Alg 2 . The benefit of rerunning the op-timization is that such dropped images can be assigned to later sections ( s 3 in our example). Ensemble would result in the final assignment: s 1  X  { i 1 ,i 3 } ,s 2  X  { i { i ,i 4 } , which is more desirable than an assignment that excludes assigned images from later sections but does not rerun optimization ( s 3  X  X  i 5 ,i 6 } ). We now present some illustrative results from the empirical evaluation of the proposed techniques [4; 5; 6]. The corpus used in the study consisted of high school textbooks pub-lished by the Indian National Council of Educational Re-search and Training. It included books from grades IX X  X II, covering four broad subject areas, namely, Sciences, Social Sciences, Commerce, and Mathematics. We selected this corpus as these book are used by millions of students every year and are freely available online. When applied to the corpus under study, our techniques were able to identify those sections of the books that could benefit from revision. The sections with high predicted scores for the need for revision often had a combination of large dispersion values (close to unity), lengthy sentences (up to six standard deviations to the right of the mean), and a large number of complex words.
 One such section was found in Grade XII Sociology book and titled  X  X ariety of Methods X . We can see from the con-cept graph shown in Fig. 1(c) that this section has a number of disparate concepts leading to large dispersion. In addi-tion, the section contains many long sentences, making the comprehension hard, e.g. :  X  X nterviews may be structured, that is, follow a pre-determined pattern of questions or un-structured, where only a set of topics is pre-decided, and the actual questions emerge as part of a conversation. X  Sections not needing update typically had low dispersion val-ues (up to eight standard deviations left of the mean). The concept graph for one such section from Grade XI Mathe-matics book is given in Fig. 1(a). They also had simpler sentence structure making it easier for the reader to grasp the material well. Fig. 2 reproduces the section titled  X  X mergence of Macroe-conomics X  from Grade XII Economics books. Our approach identified  X  X acroeconomics X , X  X nemployment rate X ,  X  X eynes X ,  X  X conomics X ,  X  X reat Depression X ,  X  X oods X , and  X  X emand X  as the key concepts occurring in the section and and proposed the link to the Wikipedia article titled  X  X reat Depression X  as the best link for augmenting the section, though there is a Wikipedia article titled  X  X acroeconomics X . If one reads the section carefully, one would notice that the large part of the section (starting from  X  X owever, the Great Depres-sion... X  to  X ...33 per cent X ) describes the calamitous effect of great depression. Interestingly, we found after careful ex-amination that the book had no other section where there was even a mention of great depression. We also examined the corresponding Wikipedia article and found that it con-tained information that a curious student would find very valuable.
 We also identified the following two images for augmenting this section: (a) the famous Dorothea Lange X  X  1936 paint-ing of migrant mother that depicts destitute pea pickers in California during great depression, and (b) image of John Maynard Keynes. Clearly, these images can further enhance understanding of this section. Fig. 3 shows the top five images proposed for three different sections from three different subjects. We can see that the images are quite relevant. We discuss the first example in more depth. This example shows the proposed augmenta-tions for the section on how organisms create exact copies of themselves, appearing in the eighth chapter titled  X  X ow do organisms reproduce X  in the grade X Science book. This section discusses three main points: (1) due to evolution, organisms are similar in their blueprint; (2) DNA replicates to pass on genetic material; and (3) DNA copying during re-production should be consistent so that the organism is well adjusted to its ecosystem. We observe that the proposed images convey related information. The image on Phylo-genetic trees captures the evolutionary relationships among biological species. The two images of DNA (chemical and physical structure) are illustrative of how the DNA can be easily replicated by breaking its double Helix structure. The section describes the consistency requirement of DNA copy-ing using bacteria as the example organism. The images of RecBCD pathway in E. coli bacterium are complementary as it plays crucial role of initiating recombinational repair of potentially lethal double strand breaks in DNA.
 We also conducted a user study employing the Amazon Me-chanical Turk platform [28]. Seven judges each, coming from a population of 56 judges, judged the results produced by our implementation for a random sample of 100 textbook sections. The results demonstrate the promise of the pro-posed system: the judges conservatively considered 87% of the images assigned to various sections to be helpful for un-derstanding the corresponding section and the performance was maintained across subjects. Given the centrality of education for economic growth and the role of textbooks in a high quality education system, we set out to devise technologies for enhancing textbooks. We presented a diagnostic tool for algorithmically identify-ing those sections of a book that are not well-written and hence should be candidates for revision. We also described techniques for algorithmically augmenting different sections of a book with links to selective articles and images mined from the Web. We carried out an empirical evaluation of the proposed techniques using a corpus of high school textbooks published by the Indian National Council of Educational Re-search and Training. The preliminary results are promising and indicate that significant benefits can accrue by bringing to bear data mining technologies for improving the quality of textbooks. However, much more remains to be done. A crucial element of our approach is the ability to iden-tify the key concepts present in a section of the textbook. Drawing upon the NLP literature, we defined concepts to be terminological noun phrases [31]. However, it is worthwhile investigating other definitions, e.g. using ideas from the For-mal Concept Analysis [20]. Similarly, additional techniques such as discourse analysis [34] can be applied for locating candidate concepts in the text.
 Education researchers concur that the good textbooks are organized in a systematically progressive fashion so that students acquire new knowledge and learn new concepts based on known items of information [32; 43]. Many text-books, however, suffer from the  X  X entioning X  problem that causes concepts to be encountered before they have been ad-equately explained [9]. The diagnostic tool we presented op-erates at section level, treating each section independently, and does not address the flow of writing across different sec-tions. A tool for diagnosing the comprehension burden due to non-sequential presentation of concepts would be valu-able. More generally, designing tools for quantifying the quality of books combining multiple dimensions such as hi-erarchical organization, sequentiality, coherence and read-ability and then providing actionable recommendations for improvement is a fruitful direction.
 Another promising direction is to examine what new issues arise if the ideas from this paper were to be extended for embellishing textbook material with other media types, e.g. video. There is also the related issue of selecting most appro-priate type of augmentation across media types and tailor-ing the augmentations to suit the knowledge and experience level of the reader.
 An obstacle we faced in our work was the lack of an estab-lished evaluation methodology for studying the performance of the proposed techniques. Ensuring objectivity and consis-tency across judges in the user studies are some challenges to be addressed in designing a direct measurement. Dis-counting externality and removing bias are some challenges to be addressed in designing an indirect measurement such as comparison of performance scores of students using good and bad versions of the same book.
 We presented techniques for proposing articles and images with which a section of a textbook can be augmented, but did not discuss specific mechanisms for integrating the aug-mentations into the textbook. Our techniques could be inte-grated into authoring tools for helping textbook authors de-cide what materials to use when writing or revising a book. They can also be used for creating supplementary material that is distributed with the paper version of the books. Fur-thermore, there are ongoing efforts aimed at creating plat-forms and inexpensive devices for distributing books in a digital form (see, for example, the use of interactive DVDs as an educational platform [19], inexpensive e-book read-ers [3], and mobile learning devices [29]). Our work fits quite naturally with these efforts, but details need to be worked out.
 Complementary approaches and related issues that merit se-rious future investigation include: (a) refining and enhanc-ing the results produced by our techniques using collabora-tion and crowdsourcing [1; 44], (b) implications for royalty sharing and intellectual property rights [15], and (c) inte-gration with other interventions for improving the learning outcomes [22; 37]. [1] Improving India X  X  Education System through Informa-[2] S. Abney. Parsing by chunks. Principle-based parsing , [3] A. Adams and J. van der Gaag. First step to literacy: [4] R. Agrawal, S. Gollapudi, A. Kannan, and K. Kentha-[5] R. Agrawal, S. Gollapudi, A. Kannan, and K. Kentha-[6] R. Agrawal, S. Gollapudi, K. Kenthapadi, N. Srivas-[7] J. Anderson and J. P  X erez-Carballo. The nature of in-[8] K. Bakewell. Research in indexing: more needed? In-[9] M. Chambliss and R. Calfee. Textbooks for Learning: [10] M. Charikar. Similarity estimation techniques from [11] J. Coiro, M. Knobel, C. Lankshear, and D. Leu, edi-[12] E. Coke and E. Rothkopf. Note on a simple algorithm [13] A. Csomai and R. Mihalcea. Linking educational mate-[14] E. Dale and J. Chall. A formula for predicting readabil-[15] L. Downes. The laws of disruption: Harnessing the new [16] W. DuBay. The principles of readability . Impact Infor-[17] I. Fang. By computer: Flesch X  X  reading ease score and [18] C. Fellbaum. WordNet: An electronic lexical database . [19] K. Gaikwad, G. Paruthi, and W. Thies. Interactive [20] B. Ganter, G. Stumme, and R. Wille. Formal concept [21] J. Gillies and J. Quijada. Opportunity to learn: A high [22] P. Glewwe, M. Kremer, and S. Moulin. Many children [23] S. Gollapudi and R. Panigrahy. Exploiting asymmetry [24] W. Gray and B. Leary. What makes a book readable . [25] E. A. Hanushek and L. Woessmann. The role of ed-[26] M. Hu, E. Lim, A. Sun, H. Lauw, and B. Vuong. Mea-[27] S. Huston and W. B. Croft. Evaluating verbose query [28] P. G. Ipeirotis. Analyzing the Amazon mechanical turk [29] A. Jawa, S. Datta, S. Nanda, V. Garg, V. Varma, [30] E. B. Johnsen. Textbooks in the Kaleidoscope: A Crit-[31] J. S. Justeson and S. M. Katz. Technical terminology: [32] D. Kieras and C. Dechert. Rules for comprehensible [33] B. Lent, R. Agrawal, and R. Srikant. Discovering trends [34] D. Marcu. Discourse trees are good indicators of impor-[35] W. McCall and L. Crabbs. Standard test lessons in [36] P. Menon. Mis-oriented textbooks. Frontline , August [37] J. Moulton. How do teachers use textbooks and other [38] N. Mulvany. Indexing books . University of Chicago [39] S. Panjwani, L. Micallef, K. Fenech, and K. Toyama. [40] C. Papadimitriou and K. Steiglitz. Combinatorial opti-[41] D. Saari. Decisions and elections: Explaining the unex-[42] S. Sarawagi. Information extraction. Foundations and [43] R. Seguin. The elaboration of school textbooks. Tech-[44] B. W. Speck, T. R. Johnson, C. P. Dice, and L. B. [45] K. Toutanova, D. Klein, C. D. Manning, and Y. Singer. [46] A. Verspoor and K. B. Wu. Textbooks and educational [47] K. Wang, C. Thrasher, E. Viegas, X. Li, and P. Hsu. [48] A. Woodward, D. L. Elliott, and C. Nagel. Textbooks [49] World-Bank. Knowledge for Development: World De-[50] S. E. Wright and G. Budin. Handbook of Terminology [51] X. Xue, S. Huston, and W. B. Croft. Improving verbose
