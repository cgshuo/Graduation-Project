
Text classification has matured as a research discipline over the last decade. Independently, business intelligence over structured databases has long been a source of insights for enterprises. In this work, we bring the two together for Customer Satisfaction(C-Sat) analysis in the services indus-try. We present ITACS , a solution combining text classifi-cation and business intelligence integrated with a novel in-teractive text labeling interface. ITACS has been deployed in multiple client accounts in contact centers. It can be extended to any services industry setting to analyze un-structured text data and derive operational and business in-sights. We highlight importance of interactivity in real-life text classification settings. We bring out some unique re-search challenges about label-sets, measuring accuracy, and interpretability that need serious attention in both academic and industrial research. We recount invaluable experiences and lessons learned as data mining researchers working to-ward seeing research technology deployed in the services in-dustry.
 H.4.0 [ Information Systems Applications ]: General; I.7.0 [ Document and Text Processing ]: General Design, Human Factors csat analysis, services, text classification, business intelli-gence
Unstructured text is emerging to be the single largest source of unprocessed data growing rapidly in today X  X  customer-centric enterprises. Gartner 1 reports indicate that over 80% data in enterprises is unstructured, noisy, and doubles every three months. Processing such huge amount of data and de-riving business insights from i t has become very important in expanding customer-centric programs like customer rela-tionship management (CRM) and customer experience man-agement (CEM). With the growing eminence of the services industry, such programs are critical to competitive growth, and sometimes even survival. CRM practices, contact cen-ters, and customer facing arms of companies are increasingly looking to text mining to help them understand and derive value and insights from text data.

The customer, the end consumer of products and services, is receiving increased attention. Analytics and business in-telligence (BI) applications revolving around the customer has led to emergence and increased attention on notions like customer service quality. Such customer focus is most ev-ident in contact centers, and this paper describes our re-search engagement with leading contact centers handling outsourced helpdesks of telecom, e-commerce, and finance companies. Our engagement concerned conceptualizing, de-veloping and deploying a unique text classification based so-lution which can be easily replicated across multiple client accounts, offers high accuracy, i ntegrated reporting, and in-teractivity with the system.
An ever-growing amount of unstructured text data is col-lected in the customer-centric services industry such as con-tact centers. Various  X  X oice of Customer X  (VoC) channels like e-mails, feedback surveys, text messages, service re-quests, agent logs, and conversation transcripts generate lots of unstructured data that needs to be stored and processed. The VoC process aims to listen to customers, understand what is being said, and proactively alter services to provide best in class service quality. It is suggested [19] that exis-tence of a gap between expected and perceived services hints at poor service quality leading to dissatisfied customers. Customer Satisfaction (C-Sat) analysis is one of the most popular techniques of analyzing VoC data.

Wikipedia defines C-Sat: Customer satisfaction, a busi-ness term, is a measure of how products and services sup-plied by a company meet or surpass customer expectation. It is seen as a key performance indicator within business and is part of the four perspectives of a Balanced Scorecard. C-Sat is very relevant to contact center business as it is unpardon-http://www.gartner.com Figure 1: C-Sat analysis setting in contact centers and industrial CRM practices able to ignore what a customer is directly saying about a company X  X  products and services. In a typical outsourcing or contact center scenario shown in Figure 1, customers con-tact the service or product provider through phone or email and usually get routed to 3 rd party contact centers. Agents, equipped with domain knowledge, online help, and a library of solutions, answer these queries. After this interaction is over, customer feedback is solicited over phone or web-based feedback forms are sent to get ratings on the service, as well as comments and suggestions. 10 X 20% of all customers are sent feedback requests and a fraction actually reply. For a large e-commerce client, this fraction amounted to about 40 X 50 thousand feedback comments a month.
 Typically Quality Analysts (QAs) examine a sample of C-Sat comments ( verbatims ) every few weeks, dig out the in-teractions between agents and customers (text or voice), and assign reason codes (reason for being dissatisfied) to cases. QAs understandably analyze cases of dissatisfied customers, determined by quantitative ratings, while largely ignoring satisfied customers. For illustration, some real comments from customers of a Telecom company are: In the above, the first one is categorized with reason code Canned Response , indicating that the interaction should be more personalized. The second complaint is about Delayed Response , the third is categorized Sales Pitch ,andthefourth is an Accent problem . Based on aggregate BI analysis, QAs aim to provide qualitative feedback to agents for operational improvements (personalize above response instead of copy-pasting replies) and sometimes for process improvements (like shift or queue management). Manual C-Sat analysis requires 60% of the time of 2 X 10 people per account for contact centers. Text classification can help automate this making it consistent and exhaustive.
Text classification is the task of learning models on pre-categorized document sets and applying the models for class assignments to new documents. It has matured as an aca-demic research field in the past decade with developments in many important areas. Discriminative learners like SVMs[9] and generative models like LDA[2] give state-of-the-art per-formance and understanding of document generation respec-tively. Other active research areas are ensemble learning[18], hierarchies [3], active learning [7], and exploiting unlabeled data [14]. While many challenges in real-world settings are usually assumed away in academia to isolate influence and drive research, we believe a lot can be learned from the unique challenges in fielding research applications in indus-trial settings.

In operational settings for example, simple statistical mod-els combined with carefully hand-tuned rule-based systems outperform sophisticated learners. The important require-ment of human review of results of classification is never tackled in academic research. Even measuring accuracy is non-trivial and often orthogonal to declaring operational success. The kinds of noise encountered is also unique, and sadly regulations don X  X  let interesting datasets reach academia. We visit these and other unique challenges in detail in Section 2. We describe our experiences in building and deploying ITACS , an automated system for C-Sat analysis. The largest component of contact center or CRM practice costs is labor; hence, technologies and processes to reduce costs or make on-board labor more efficient are crucial. ITACS is a first of its kind solution integrating text classification, BI, and inter-active document labeling for services industry deployments. We focus on contact centers because of relevance to their flood of noisy unstructured VoC text data and importance of C-Sat analysis for their prosperity and even survival.
Our unique contributions are: 1. A UIMA 2 based customizable text analytics engine 2. An integrated architecture comprising commercial (for 3. A freely available interactive text labeling tool that 4. We recount some valuable lessons learned as data (text) 5. We also highlight the new kinds of problems and data
We highlight some unique research challenges in text clas-sification as well as handling research engagements in Sec-tion 2. We describe the architecture of ITACS in Section 3 and present experiments in Section 4. Section 5 discusses our experiences and some lessons learned. We conclude in Section 6 outlining the impact of this project and cover fu-ture directions of work.
IBM Unstructured Information Management Architecture: http://www.research.ibm.com/UIMA/
In this section we outline some unique challenges we faced in the text classification domain that are not often encoun-tered in academic research or have to be viewed differently. We describe unique problems regarding label-sets and label-ing, measuring accuracy, viewing and interpreting results, and interacting with deployed text analytics systems.
The life-cycle of a text classification system begins with the definition of a good set of classes or label-set on which the system is based. Label-sets refer to the collection of classes to which customers comments are categorized depending on the root cause 3 . QAs usually provide a base label-set to build the system on.

However, it is known that there is seldom a correspon-dence between a human proposed label-set and a clustering of documents in some geometrical vector space (TFIDF) rep-resentation. QAs expect the label-set to be actionable i.e. la-bels should be convertible into exact business actions. To il-lustrate, lack of knowledge is a typical class where customers complain that agents do not have the technical depth re-quired to solve their problems; remedial action prescribed is imparting specific training to these agents. Similarly, agents with complaints on accent problems , need to be given appro-priate voice training. Text classification experts on the other hand expect classes to be well-defined and separable, not confusing with others. For example, the improper accent , wrong English usage ,and poor voice quality classes had to be merged under a accent problems class for a voice-based helpdesk account after discriminative analysis. Hence there is close interaction needed at the label set definition stage between machine learning experts and domain experts.
A pre-requisite to training a model for text classification is the availability of a large number of documents categorized manually in accordance with the label-set. The larger and more accurate this training set , machine learning wisdom suggests better the accuracy of the future classification will be. In academic research, the training data set is assumed to be sacrosanct, and there is limited investigation about its quality and consistency. On the other hand, we observed serious calibration issues in labeling of documents causes sig-nificant detrimental effect on system accuracy through the quality of the training data. Calibration here refers to the repeatability (consistency of a labeler over time) and repro-ducibility (consistency between labelers) of labeling. Though 30% inter-human disagreement in labeling has been observed[11, 6], we present some new interesting experiments about hu-man labeling consistency in Section 4.3.
One observation about real life text classification problems is a highly skewed class distribution. For example one of our clients had 75% of the complaints falling in two classes called Comprehension problems and Incomplete resolution .Sim-ply making near perfect classification predictions on these 2 classes will yield accuracy around 75% if classes contribute to overall accuracy in proportion to their population (micro-average accuracy). However since the label-set is carefully designed by domain experts and every class has relevance
We use labels, classes, categories, call drivers, root causes interchangably. to business, it is important for small classes not to get lost . Hence macro-average accuracy has to be considered where each class X  X  contribution is uniform. In the above case, if there are 10 classes and only 2 classes are perfectly accurate, total accuracy is 20%. Important low population classes can-not be ignored and we discuss how rule-based systems can drive macro-average accuracy in Section 3.1. Various ac-curacy measures are summarized in [16] and cost-sensitive classification has been well researched[5] but lessons in op-erational text classification settings are unique.
Another challenge is presented in measuring accuracy of systems. Ground truth is usually taken to be human labeled data and accuracy compares system predictions with this la-beling. However in operational services settings, we found that comparing with human labeled truth is not too mean-ingful. We encountered very low intra-human and inter-human consistency in repeated labeling of the same set of in-stances. Statistically sound repetition of experiments is un-feasibly expensive due to human labor cost involved. Hence, wrong conclusions are easily drawn about very low (or very high) system accuracy. We hence decided to measure human satisfaction with system predictions instead of any accuracy numbers. We show a few hundred comments and predicted classes to an expert and ask her for yes/no decisions on satis-faction. This is feasible and correct though it is likely to pos-itively (or negatively) bias the expert X  X  judgment since she sees the comment and the predicted label together. How-ever we believe this is only as bad or misleading as accuracy figures in light of the low labeling consistency numbers. We present interesting experiments around this in Section 4.3.
Our most important system development challenge was to output the results of the analysis (classification) in a manner understandable by various classes of business users. Reports at various granularity are expected, from detailed agent per-formance improvement clues for QAs and team leaders to birds eye views for senior management. Our solution in-cluded a full function BI product (IBM DB2 Alphablox) ca-pable of generating a comprehensive set of graphical reports which could be canned or built-up by experts. BI reporting enables improving agent performance by pointing out areas of improvement and provide operational insights for better customer experience management. We provide details in Section 3.1.
A feedback loop assumes great importance in real-world industrial settings; automated analytics solutions need to build trust in and after deployment. A text classification system becomes significantly more useful and trustworthy if there is a mechanism to inspect class assignments to docu-ments, as well as modify and correct them. Importance of trust and hence involvement of experts in text classification is unique to real world settings and absent in academic re-search. Interactivity and human review of text analysis sys-tem are not very well understood and limited research efforts [8] and [15] have addressed some bulk labeling and feature selection issues respectively. However these systems are re-search prototypes and we need to integrate interactivity and human review in all phases of a deployed text classification system.

We felt the need for an interactive document labeling sys-tem to close the human-machine feedback loop. We pro-posed and built a tool called IBM TICL (Tool for Inter-active text Classification and Labeling) that we present in Section3.2. Itisintegratedinthearchitectureofoursystem shown in Figure 2. This labeling package acts throughout the system lifecycle: (1) it can be used to generate training data and (2) it can be used to inspect and correct system label assignments after deployment. The core functionality in both cases remains that of a simple UI built on top of a learned model that predicts/assigns labels with some con-fidence to a set of unlabeled documents and presents them to the expert for validation. TICL also helps in updating statistical models based on user feedback.

Next, we describe the system architecture in detail and address the challenges mentioned above. After fitting all pieces of the system together, we present some experiments in Section 4.
In this section we describe the architecture of ITACS in detail. We describe next the building blocks of the back-end classifier and its integration with the BI component. We describe the interactive labeling module in Section 3.2. In Section 3.3 we relate to some of the challenges highlighted above and show how ITACS overcomes them.
ITACS is built of 3 independent but tightly coupled com-ponents; the classification engine, the database store, and the BI component. As shown in Figure 2 these are inte-grated with IBM TICL to address some of the issues from Section 2.1 and 2.4. We describe these components next.
The heart of the system is a UIMA (Unstructured Infor-mation Management Architecture) based back-end that is a combination of statistical (Bayesian[13] and SVM[9]) and rule-based classifiers for text. UIMA is an open, industrial-strength, scalable and extensible platform for creating, in-tegrating and deploying unstructured information manage-ment solutions. UIMA originated at IBM and is now open source 4 . UIMA applications work as pipelines of annotators for text processing. Typical applications ingest plain text, identify named entities (persons, places, organizations) and relations (works-for or located-at), and do various other text mining annotations. Our novelty was development of the su-http://incubator.apache.org/uima/ pervised classification engine as UIMA pipelines comprising loosely-coupled annotators. The engine has train and test capabilities and is executed as multiple pipelines shown in Figure 3.
 Figure 3: Integrated statistical and rule-based clas-sification engine
In the first pipeline, text do cuments are read and fea-tures extracted with pre-packaged and custom written read-ers that read documents from databases (our implementa-tion) or various file types. Documents are represented as Common Analysis System (CAS) objects and passed through Feature Extractors or annotators (FEs in Figure 3) such as tokenizers, stopword removers, n-gram extractors, named-entity annotators. Extracted features are added as annota-tions to the CAS objects. As shown, composite FEs can be arbitrary compositions of simple FEs in parallel or chained to each other. The first FEs on every parallel path work on the document text whereas subsequent FEs act as filters and work on extracted features.
 The second UIMA pipeline is used for feature selection. This is an optional corpus level operation to reduce data sparsity and try and increase separability among classes. We imple-mented standard feature selection methods like information gain and count/occurrence based methods. The list of se-lected features is stored on disk and the other features are removed from CAS objects.
 The third pipeline runs the actual classifier training algo-rithms and builds statistical models on stored CAS objects. We have implemented naive Bayes and have provided APIs that convert CAS objects to sparse vector representations thatcanbeusedinany3 rd -party classifier package. We have built hooks to popular Weka 5 and SVM packages. The trained model is stored on disk and is used in the application phase to classify new documents.

The single applier pipeline uses the stored selected fea-tures and models. Documents are again read using readers and passed through the same set of FEs to create CAS ob-jects. The test CAS objects are now passed through spe-cially tuned rule-based classifiers which are ordered collec-tions[4] of regular expressions. These help overcome the challenge of accuracy for small classes mentioned in Sec-tion 2.2. An example of a sparsely populated class with business importance in contact centers is the  X  X ales Pitch X  http://sourceforge.net/projects/weka/ class. Here customers complain about agents cross-selling or up-selling products/services and trying to meet time SLAs rather than solving problems. We could never garner enough training data to learn models for this class statistically. How-ever QAs could easily define high precision rules such as presence of keywords and phrases like sell me , promote , up-grade my . Expectedly, such rule based classifiers have high precision and low recall; they could classify only a fraction (about 10%) of the documents.

CAS objects which pass through the rule-set without get-ting classified are passed through the statistical models. They are updated with the assigned class labels. Finally, con-sumer components write CAS objects back to the databases or file system for future use. All configuration parame-ters like paths, feature selection method and size, annotator chains, and rule bases are managed through a set of plain XML-like text configuration files as per the UIMA architec-ture.
Post categorization, verbatims, label assignments and other relevant back-end interaction data is stored in a DB2 database. Such back-end data includes particulars of agents and teams interacting with the customer, date of interaction, target product or service, overall score assigned in feedback, agent training information, and other relevant enterprise informa-tion. BI tools are now used to produce interpretable graph-ical reports on the stored analyzed data. The database schema is a star or snowflake schema consisting of a fact table storing the actual comments and their categorization, and referencing a few dimensions. The idea is to store facts along different dimensions for cubing operations like slice and dice, rollup-down. To illustrate, the agent dimension has attributes like agent id , name , team lead id , team lead name with agent id as the primary key. This helps in analyzing agents across other dimensions like dates, labels, scores.
The third component in ITACS is a BI tool which is used for visualization of data stored in the above star schema. Our customized reporting tool, IBM DB2 Alphablox, pro-duces highly interactive graphs, charts, and reports showing correlations between various dimensions of analysis (agents, labels, scores, dates etc.) as shown in Figure 4. Using these reports users can see correlation between different data fields, drill-down and roll-up the cube views of data, and slice and dice to see different aspects of the analysis at vary-ing granularity. A typical report could compare label dis-tribution for under-performing agents of two teams over a few weeks, aggregated by satisfaction ratings. The agents can then be trained better depending on their assigned call-driver distribution to, say improve accent after undergoing training in voice-based helpdesks or be better at personal-ization while replying to emails. We believe a BI reporting goes a long way in providing interpretability of results in deployed text mining systems.
We outlined two sets of challenges relating to interactiv-ity in Section 2.1 and Section 2.4. The need was two fold; first, to assist domain experts in designing label-sets and building up training data sets for classification as in ITACS and second, to provide the trust factor required in manual review of the quality of label assignments at any stage in de-ployment. We developed an interactive document labeling interface called IBM Tool for Interactive text Classification and Labeling (TICL), freely available online 6 .TICLisanin-teractive interface to train, validate, correct, and refine the classification process continuously. It aims to enable end users to start building text classification systems without knowing statistical or rule-based text classification.
TICL attempts to bridge the gap between manual and automatic classification approaches combining the tunabil-ity of the former with the scalability of the latter. The TICL version available online can be used in stand-alone mode to create label-sets and training data using simple text config-uration files. An expert can generate training data following active learning[7] principles to overcome some of the chal-lenges mentioned in Section 2.1. A simple model built over a very small manually labeled training set can start off an expert in designing, tuning, and fixing a label-set prior to moving ahead with a deployment of ITACS .

When integrated with ITACS according to the architec-ture shown in Figure 2, TICL addresses the issue of provid-ing trust in deployed text classification scenarios by facilitat-ing human review. The application part of ITACS comprises validation and inspection of classification results where a human expert can intervene and interact with the system. The expert can validate and even correct label assignments of comments. From any drill down operation (double-click) in any Alphablox grid or graph report, TICL is invoked as shown in Figure 4. When inspecting aggregate statistics and reports over batches of new (or old) comments, the ex-pert can drill down and see the comments associated with any interesting portion of the graph seen. She can inspect comments and their labeling, even correct them and add re-labeled comments back for generating corrected reports.
This interaction can even be used to feed revised per-ceptions of meanings of labels back into the system over time; since corrected document labels can be treated as new training data, the classifier can be re-trained with new ev-idence (labeled documents/verbatims). This feedback has been found to be especially useful in the training of good classifiers for minority classes over time.

The user interface of TICL itself is very simple as shown and generates an HTML form listing sets of comments and their predicted labels. These can be accepted as they are or modified by experts and fed back, either just into the database for corrected reporting, or added as new training data to re-train classifiers. The interesting part of TICL is it X  X  flexibility since it is designed in a pluggable UIMA archi-tecture. We would like to note here that TICL is packaged on the same classification engine described in Section 3.1. It is bundled with no rule-bases to start with but these can be quickly included in the classification flow as described.
We have described the architecture of the ITACS system and its sub-components. We now take a step back and see how they fit together in the light of the challenges described in Section 2. A text analytics solution based on classification in the services industry setting encounters problems at the outset with labels and labeling. We described our develop-ment and deployment efforts around TICL to tackle some of these problems. The next operational challenge is typically presented around accuracy and we described human satis-faction metrics (for measurement) and rule-based systems http://www.alphaworks.ibm.com/tech/ticl (for driving minority classes). Another operational business requirement is that of interpretable reporting; we presented the Alphablox BI component for reporting for consumption at various levels of the people hierarchy. We described other usage modes of TICL which enabled human review of data at any point in the lifetime of the system. Our proposed system presents a first of its kind integrated C-Sat analysis tool comprising an end-to-end system based on text classification. The system can be adapted to other domains of unstructured data in service oriented and cus-tomer facing organizations with VoC data. It addresses all aspects of text classification systems starting from help-ing domain experts generate training data to human review of classification results via an interactive labeling interface. The proposed system offers a configurable combination of different classifiers with state-of-the-art performance. A BI reporting interface included in the system allows report gen-eration that uses the result of the analysis along with struc-tured data to draw valuable insights. We are not aware of similar existing solutions in the services industry setting.
In this section we describe some of the experiments we conducted while designing and developing the ITACS sys-tem. We first describe the noisy text classification industrial datasets and give real examples of unstructured documents (in contact centers). Next, we describe feature noise, refer-ring to our detailed study[1] and report some of the inter-esting results here. We then describe the problem of label noise arising out of human calibration issues, and describe experiments aimed at measuring this noise.
Table 1 summarizes the label-sets and documents made available to us over the span of a year. We would like to relate here to the stringent day to day operational pressures QAs work in. This is only evidence of how gathering real training data is laborious, slow, and tedious  X  a fact often discounted in the research community.

Our system was primarily designed for C-Sat analysis and our main data sources were C-Sat feedback forms. CCFb1, CCFb2, and CCFb3 are C-Sat analysis datasets compris-ing customer comments in feedback forms (examples in Sec-tion 1.1). They belong to contact center accounts of e-commerce, internet, and telecom companies respectively. Feed-back to contact centers tends to be short, crisp, and often contains abusive remarks from customers. Many times the verbatims are very short in length, ambiguous, and noisy. CCSum1 and CCSum2 are call summary datasets in a Telecom company X  X  contact center. After every call, the agent summarizes the call in very short sentences; this is often very noisy with lots of spelling mistakes and abbrevi-ations. The saving grace however from an informative fea-tures point of view is that abbreviations are usually com-mon across agents. This two-level hierarchical dataset has 6 classes at the top level like Billing , Credit , Broadband -this is CCSum1. The second level dataset has more fine grained 92 classes and both these have the same 26264 summaries. Business applications necessitate considering the first level label-set as a different dataset for dashboard reporting. An example agent summaries with private data masked is
CCMail is a email classification dataset of a financial in-stitution with over 600 categories. The process of handling email complaints in typical contact centers necessitates on the fly definitions of categories with obvious overlap and redundancy leading to a bad label-set from a classification perspective. We restricted our attention to only those 50 categories with over a 100 emails. An example email with masked private data is
We carried out a detailed study[1] on feature noise on all the real life noisy datasets (table 1) we got access to as a part of the ITACS project. We wanted to see how much feature noise is in operational text classification settings in contact center/CRM settings and how much time we should invest in feature engineering and feature selection. Figure 5: Text classification results on contact cen-ter datasets We report an important result of our study in Figure 5. For a host of real-life services industry datasets, we report text classification accuracies and discuss characteristics of these proprietary datasets described in Table 1. All accura-cies we report are averaged over 10 random 70 : 30 train-test splits. We achieved 60 . 1% accuracy with multinomial naive Bayes (NBM with different feature set sizes) for CCMail and 65 . 6% with SVMs. For three different C-Sat datasets, we got accuracies of 58 . 3%, 47 . 9%, 47 . 6% for NB and 59 . 1%, 53%, 47 . 8% for SVMs respectively. Similarly SVMs(88 . 3%, 82%4) outperformed NB(85 . 9%, 80 . 4%) for the agent sum-mary datasets CCSum1 and CCSum2 respectively. Text classification wisdom might find these accuracies low. How-ever, we believe, it is more due to the challenges in measuring accuracy in real life classification setting (Section 2.2) and less about the classification techniques. We will describe some experiments next to reaffirm this.

A unique kind of data we dealt with was speech transcript data. An abundance of voice calls is presently stored in con-tact center environments for regulatory requirements; but these are often left unprocessed. Automatic speech recogni-tion (ASR) transcripts inspite of high recognition word error rates[1] is very useful in bringing this data into textual form. Text classification is a simple first step towards understand-ing and processing this data, usually followed by complex natural language processing and information extraction sys-tems.

The challenge in this data is that only valid English words are output by the ASR system and recognition rates of nouns is very low. For example,  X  X he Heiwa Sogo Bank X  is a phrase in a Reuters-21578 article which is recognized by an ASR system as  X  X igh were so woman X . One of our contributions to the research community as part of our work with the services industry is a Reuters transcribed dataset. Available online 7 , this dataset contains 20 documents each spoken and transcribed from the top 10 populous Reuters classes; these classes are often used to report text classification results. We hope availability of this small dataset is a first step in the direction of research towards handling this new kind of noise in real-world text mining applications.
We pointed out difficulties with the manual document la-beling task in Section 2.1 and Section 2.4 stressed the need for interactivity between experts and a classification sys-tem throughout its lifecycle. We presented TICL as a first step toward alleviating some labeling issues faced by do-main experts who did not know classification or data min-ing. Clearly more needs to be done before classification sys-tems can automatically be built and adopted in real-world settings, but let us dwell upon some more labeling issues briefly.
 Label-sets. In the C-Sat analysis setting, when QAs started interacting with us to deploy a text classification solution, their existing label-set was a starting point around which we decided to collect labeled training data. This label-set, prac-tically, was a long list of classes intended to capture the sen-timent of customer comments. The label-set was designed purely out of the QAs X  experience and often contained long-unused, redundant, overlapping, and sometimes even irrel-evant (over time) classes. Our first task was to force them to inspect each label and the label-set as a whole for in-consistencies, obvious repetitions, and possible merges. We forced them to write 2 line summaries of each label and this often cut out unused, irrelevant, and repetitive labels. This streamlining effort was greatly appreciated and we forced a simple process around human calibration described next. Labeling. The next task was that of labeling documents and creating training data. One of the challenges here was deciding for or against a multi-labeled system. It was easy for QAs to label confusing instances as different classes at different points in time owing to different contexts and hu-man factors. Allowing multi-labeled instances seemed the natural answer to this but we immediately saw pitfalls in terms of understanding results and BI reporting driven out of the proposed star-schema data store. It is not clear whether a comment pointing to, say, accent problems and rude be-http://kdd.ics.uci.edu/databases/reuters_ transcribed/reuters_transcribed.html haviour of agents should be counted twice during reporting or stored only once in the database with a multi-valued la-bel column. In the former case, overall dashboard reporting views go haywire and in the latter, there is a challenge work-ing with existing BI products over such special multi-valued columns. In the light of this our clients almost always chose a uni-labeled system deployment though ITACS (and TICL) are implemented as multi-labeled systems with a label-set size of 1. Next, we present interesting results to bring out the problem of label noise arising out of inconsistency be-tween human labelers.
 Label noise. In a calibration exercise with a Telecom client X  X  contact center (not in Table 1), QAs had defined 31 classes for C-Sat analysis. Two experts independently labeled 200 comments with these classes. These experts repeated this exercise after a week (thinking they were labeling different comments). A statistical ANOVA Gauge Reproducibility and Repeatability (R&amp;R) 8 test was performed on these la-bellings. It turned out that repeatability (of an expert with herself) was only 65% and reproducibility (inter-expert con-sistency) stood at merely 53%. With such low calibration, the training data was deemed to be of poor quality, and building a text classifier turned out to be demoralizing. Ac-curacy turned out to be unacceptably low, matching one human labeling 55 X 60% of the times. Inter-human disagree-ment has been studied before[6] though in the context of NLP tasks like word sense disambiguation but we believe our experience in text classification settings is unique and educating.

We handled this issue, as mentioned earlier, by measuring human satisfaction on predicted labels. For another Tele-com client account, we built a combination of naive Bayes and regex rule-based classifiers as described in Section 3.1. The classification engine was tuned with rule-sets for specific important low population classes. We carried out another consistency check similar to the R&amp;R study above. In this account we had only one QA who managed all C-Sat analy-sis and human labeling. We trained our classification engine on about 5000 labeled comments. We had 1400 comments labeled by this expert as unseen test data. After more than a month, we asked the same expert to label the first 700 (out of 1400) comments again not revealing to her that these had already been labeled. In a uni-labeled setting, we found a consistency of only 74%. For the other 700 comments we let our tuned classification engine make predictions and we showed the user the comments and predictions together. The expert was asked to assign yes/no satisfaction ratings to these predictions, and she accepted over 85% of these. On inspecting the remaining comments with the expert, we concluded that the predictions made were appropriate (and acceptable for aggregate analysis) though not the best, and a better label existed in the label-set. This brings out again the multi-labeled nature of real-world data, and pegs ac-ceptability of our system at about 85%. We had similar acceptance figures with other client accounts.

The root problem in this domain is not feature noise, dis-cussed before, as much as label noise. Multi-labeling clearly contributes to very low consistency rates, but there is a larger problem of bad label-set design and lack of a consis-tent labeling process. Such an observation is known to some http://en.wikipedia.org/wiki/ANOVA_Gage_R&amp;R extent to text classification practitioners and about 30% dis-agreement amongst expert human labelers is known[11]. In designing real-life systems, label noise emerges as a very im-portant kind of noise to consider. We restrict further discus-sion on label noise here; it remains an important open avenue for future work. We would like to note here that domain spe-cific efforts to improve operational text classification systems have been successful in dealing with feature as well as label noise to some extent[12]. This, however, requires significant care and cost not always possible.
In this section, we share our experience of designing, build-ing, and deploying a text mining based solution for the ser-vices industry, specifically our contact center clients. We have highlighted several unique challenges in Section 2 that we faced in taking text classification technology out into the field. Here we highlight a few other experiences and lessons learned in our client engagement; we hope these will help future research technology deployment efforts especially in the services industry context.
Data (Text) mining researchers have started seeing ser-vices organizations like contact centers, as goldmines of new types of data and problems. We too came across gigabytes of unstructured text data in call logs, transcripts, emails, feedback (all VoC channels) left unprocessed and unana-lyzed. While structured data analysis, typically OLAP ap-plications, helps process data and generate deep insights, text analytics is not very common. Text is often stored and left unprocessed either because of a lack of  X  X rust X  of unstructured content or because of lack of in-house exper-tise required for text analytics. Most data is sensitive data about customers, hence it is never made public and doesn X  X  reach the general research community. However the unique research opportunities in dealing with services organizations and their data deserve to be bought out.

In our research engagements in ITACS like projects, we were able to access this goldmine. For text classification in particular, C-Sat analysis data is very different from bench-mark datasets like Reuters-21578 and 20-newsgroups. We encountered noisy, poorly formatted, wrongly labeled, multi-lingual datasets and blind application of state of the art techniques produced disastrous results. We had to get into issues of data cleansing and noise handling apart from get-ting down to all the issues related to labels highlighted in Section 4.3. We have been able to work extensively [10, 17, 1] in the area of Noisy Text Analytics primarily because of ITACS and other CRM and contact center projects.
Contact centers resemble manufacturing assembly lines; once processes are fixed, jobs and shifts are routine. They strive for operational efficiency to scale to large operations and cost efficiency to keep/move ahead of intensely growing competition while maintaining quality of service. These cost pressures narrow down forward looking windows of oppor-tunity of moving up the value chain.  X  X hort term X  benefits from tool deployments to make daily operations efficient are perceivedtobemorevaluablethan potential  X  X onger term X  benefits that may or may not accrue from research engage-ments.
Modeling and automating C-Sat analysis proved to be more than designing a text classification system as the con-cept and meaning of C-Sat varies across business scenar-ios. We had to be involved in understanding the domain, typical problems, common issues between customers and agents, separation of ownership of issues (between prod-uct/service seller and contact center), action-ability of is-sues etc. We interacted closely with QAs understanding op-erations, customer-agent interactions, and jointly designed experiments (of Section 4) to get our insights validated.
When explaining text classification and benefits of C-Sat analysis automation, business users were very excited at pos-sibilities of (1) hierarchical label-sets -for a wide spectrum of analysis, and (2) multi-labeled documents -they understood difficulty in assigning just one label to a customer comment. However, these were rarely implemented in client accounts because of challenges in defining label-sets and gathering training data. The most impacted business challenge was interpretable BI reporting; it was not clear how to handle the multi-labeled setting in reporting and this needs further investigation in implementation and tracking benefits. Busi-ness needs forced simple implementation of a flat single label setup.

A non-technical challenge that stumped us was relating 85 X 90% accuracy (and satisfaction) levels to dollar bene-fits and improved C-Sat ratings by customers. Impacting business outcomes is not directly in the hands of techni-cal solutions. A system can only point out that an agent needs accent training, but actually imparting it to the agent and improving her customer handling requires some level of manual and process intervention. Transition management in such a project was more complex than that of software installation or upgrade. We also clearly learned that train-ing programs were imperative to users of the system; if not used properly technical tools can never impact business, and success can not be declared by just technically achieving 85 X  90% text classification accuracy.
ITACS has been deployed for C-Sat analysis in e-commerce and telecom client accounts of large contact centers. QAs are using ITACS for analyzing operational data and identify-ing customer pain points, problematic products and agents X  shortcomings. Accuracy has been consistently measured around 85 X 95%. ITACS has also had other kinds of im-pact where systematic label-set design was greatly appreci-ated by business users and QAs and forced a process around label-set design. Quality operations also benefited greatly by forcing calibration exercises and our client is trying to establish a certification program around manual analysis for QAs. ITACS can be used not only for verbatim analysis but for analyzing any kind of textual data generated in contact centers and more generally customer facing services depart-ments of organizations. We released IBM TICL as a first of its kind freely available interactive classification and la-beling tool. We believe systems designed incorporating such technologies are necessary to provide intervention and inter-action between systems and experts in an effective manner. We hope simple components like TICL drive awareness and adoption of smart classification systems in real-world set-tings.

Among future directions, we are looking at automating the training process for ITACS for a new domain, at least partially. The vision is to develop a generic C-Sat mecha-nism, with standard class labels and a standard set of text features which can be tuned for a new deployment with a few button clicks. It will drastically reduce the time and cost of introducing automatic C-Sat analysis for an inter-ested business. On the text classification side we plan to investigate label noise more systematically.
