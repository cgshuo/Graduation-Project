 1. Introduction
Rich Site Syndication (RSS) is an XML format for publishing concise information updates. It is mainly ings. Both researchers and commercial companies have noticed that blogs and RSS feeds have the potential to be used for public-opinion gathering or marketing purposes, and hence there has been a drive to develop researchers have used word/noun/noun phrase time series analysis methods with simple frequency statistics stage of this process is to compare these methods to assess which are the most suitable for this new data source.

In this paper, a  X  X erm X  is a noun or noun phrase and a  X  X eature X  is a term that is judged to be significant within a collection of documents. Feature selection methods such as v mation Gain ( I ), have been commonly used in different application domains. One example is automatic text densing documents by removing redundant words, in order to speed document classification without reducing classification quality. Feature selection that is too  X  X ggressive X , in terms of removing too many words, will result in poor document classifications. A very different example is Topic Detection and Tracking (TDT) event/topic, and tracking the previously identified event/topic with regard to new incoming stories. This may be achieved by identifying and clustering collections of related terms, but other methods are also used, such as Information Extraction (e.g., Luo, 2004 ). In comparison to document classification, TDT implicitly uments rather than to capture the essence of individual documents. A relevant application of TDT is the auto-over a given time period.

Despite previous research into term selection methods, there is no clear indication of the superiority of any particular method for all types of data: each has its own strengths and weaknesses. Yang and Pedersen (1997) , supported by Sebastiani X  X  (2002) automatic text categorisation review article, suggest that I and v performance in supporting document classification, with both being significantly better than MI . Of these, v value is less than 5 ( Dunning, 1993 ). There is no strong evidence, however, to suggest which method is the
Swan and Allan (2000) have adopted v 2 for this purpose. It is not clear that methods which work best for doc-eliminate terms based upon a high degree of association with remaining terms, which is not a consideration for
TDT ( Swan &amp; Allan, 2000 ). Moreover, each method uses a probabilistic function based upon assumptions about the distribution of the data, such as independence, which are violated in practice to varying degrees ( Cooper, 1995 ). Hence experiments are required with each new type of data source and for each new type of task to assess the strengths and weaknesses of the leading methods in practice.

In this paper the Mutual Information ( MI ), v 2 and Information Gain ( I ) feature selection methods are evaluated for an evolving RSS feed corpus in order to decide which is the most suitable for identifying features that are significant across a number of documents within the collection (the TDT type of task). In
Term Strength (TS) and Document Frequency (DF) Thresholding are significantly different from the other three methods, as these two methods only consider the document space, rather than individual parameters, such as category or date, and require a training corpus. TS, in particular, requires computationally expensive evolving data set.

In addition, the assumption that very large values indicate highly significant terms is investigated; and the extent to which the three methods agree with each other about the significance of a term on a given date.
This is particularly relevant for TDT. An evaluation method that is inspired by the way in which conference papers are reviewed is also proposed and used. A paper may be accepted for publication if two or three referees put a good recommendation. For the evaluation of the feature selection methods used, the extent to determined.

This paper is organised as follows. Section 2 reviews the existing work in the area of automatic text Section 4 describes the feature selection method evaluation procedure and presents the evaluation results.
Section 5 presents the conclusions. 2. Related work This section describes research relevant to feature selection in the areas of automatic text classification,
Topic Detection and Tracking and overview timeline generation. It also reviews current blog text analysis. 2.1. Automatic text classification
A number of researchers have proposed various techniques to carry out an automatic classification task, work Based Classification ( Chen, Hsu, Orwig, Hoopes, &amp; Nunamaker, 1994; Ng et al., 1997; Yang, 1994 ); K-Nearest Neighbour Classification ( Yang, 1999; Kwon &amp; Lee, 2000 ); and Support Vector Machine Based
Classification ( Dumais &amp; Chen, 2000 ). There are two main issues for automatic classification. 1. Selecting appropriate features for each category and document. Some researchers use only one particular feature selection method and intuitively set a threshold to exclude terms. 2. Assigning documents to appropriate categories. This task can be hard for a large number of categories and a deep hierarchical classification scheme.

The evaluation strategy that is commonly used to evaluate feature selection methods is to use an application as a means to measure the effectiveness of each method. Yang and Pedersen (1997) have used an automatic cost and requires human resources to pre-categorise a collection of documents. In contrast, Swan and Allan (2000) use a clustering algorithm to evaluate the effectiveness of the v implementation cost and requires human resources to evaluate the quality of the clusters that are automati-cally generated by the clustering algorithm. In addition, the parameters and algorithm used in the application tion methods, but also the performance of the application used. 2.2. Topic detection and tracking
In the context of Topic Detection and Tracking, an event is defined as something that happens at a partic-used to describe the natural units of text in which the information arrives, such as a single newswire report.
In this context, a set of events can be grouped into a number of topics. Each topic becomes the abstraction scripts, TDT encompasses the following issues: 1. Story segmentation : Segmenting a stream of text into its constituent topically cohesive stories ( Allan,
Carbonell, Doddington, Yamron, &amp; Yang, 1998 ). ( Allan, Papka, et al., 1998 ).
 1998 ). &amp; Walls, 1999 ).

Event detection task consists of two parts. 1. Given a set of stories: (b) Select the most significant terms within each story. Yang et al., 1998 ).

The event tracking task is only concerned with incoming new stories, and consists of three parts. 1. Given a new story, (b) Select the most significant terms within the new story. 2. Compute the similarity between the term vector representation of the new story and all the term vectors of the existing stories. 3. Assign the most appropriate event that the new story discusses, if any. Otherwise, a new event is detected. quite similar to the event detection and tracking tasks. The main difference is that the former focuses on the identification of topics across stories, whilst the latter focuses on the identification of events. 2.3. The generation of overview timelines
Given a set of stories, Swan and Allan (1999, 2000) and Swan and Jensen (2000) have built a v difference in sequential process between the two approaches, in respect to a topic detection task.
Given a collection of stories, both approaches can be used to identify a number significant terms which can period. Tables 2 and 3 highlight the strengths and weaknesses of each approach. 2.4. Blog text analysis
Existing blog text analyses focuses on the extraction of useful information from a set of blogs. The texts found within the blogs are analysed for the following purposes:
Determining the most significant keywords and proper names within a certain time period. Glance et al. (2004) focus on developing automatic trend discovery across blogs. Their system can detect key persons, phrases and paragraphs on a daily basis.
 to predict the spreading of a topic across blogs.

Organising a set of terms so that concept-based topic identification can be carried out. Given a collection of texts, Avesani, Cova, Hayes, and Massa (2005) set out to develop a system which can generate a topic-centric view so that bloggers can find related blog entries with respect to a specific topic.
The above initiatives all exploit textual data to find and track some useful information, such as a topic and the name of a person/an organisation. They are closely related to TDT, but also different in important methods tend to be based upon simple term frequency time series and, like overview timeline generation, focus on identifying individual significant terms (i.e. aggressive feature selection). 3. Data collection and processing
In recent years, bloggers, the people who create and edit weblogs (blogs), have used weblogs as a means for publication and a communication medium ( Glance et al., 2004 ). A blog may have a corresponding RSS feed which contains all the blog postings in RSS format. RSS is an XML-based metadata syndication format ( Hammersley, 2005 ). In this context, the term, syndication, refers to the use of an RSS feed as a medium for publishing and sharing information. Despite the fact that RSS feeds have grown more slowly than blogs, per-haps due to a number of RSS specification changes, RSS technology has been adopted by a number of news-paper agencies as a syndication technology at a brisk pace ( Gill, 2005 ). In RSS terminology, an RSS element only contains one sub-element, a  X  X hannel X . A channel element contains information about the RSS channel is used here as the basic unit of analysis, analogous to the TDT  X  X tory X . The RSS analysis experimental pro-cedure is described below.

Mozhdeh: RSS feed collector and data monitoring system . Mozhdeh ( Thelwall, Prabowo, &amp; Fairclough, 2006 ) was used for collecting data, and has been implemented and maintained by the second author. As a starting point, the system automatically collected 19 587 RSS feeds from Google, and blogstreet ( http:// www.blogstreet.com/ ). The system monitored this set of feeds hourly (daily for infrequently updated feeds) and stored each new incoming RSS item found. For each new RSS item, the system recorded the parent feed.
Note that since the findings from this experiment were based upon a single evolving RSS feed corpus they ing and the uses to which it is put are also evolving beyond news feeds and blogs. Hence it is not possible to static, classified RSS test corpus against which to benchmark the different methods.

Text extractor . For each item, the Text Extractor extracted the title, description and publication date and then converted the publication date into its associated GMT time. A date converter program was implemented that can converted different types of timestamp formats and different timezones into GMT times, by analysing the timestamp patterns. The total number of timezones used was 63 ( http://www.lns.cornell.edu/public/
COMP/krb5/krb5-admin/kadmin-Time-Zones.html , accessed at 23 December 2004). The total number of timestamp patterns used was 36. The title and description texts were then stored in a text file. POS tagger ( Brill, 1994 ) and NP Chunker ( Ramshaw &amp; Marcus, 1995 ). For each text file, a POS tagger and
NP Chunker were used to tag and chunk the texts to determine the nouns, noun phrases and proper nouns found in the text files.
 each one as a two column database table. 1. TermItem. Used to determine to which item a term belongs; 2. ItemRSSFeed. Used to determine to which RSS feed an item belongs; 3. ItemDate. Used to determine when an item was posted in GMT time (i.e., publication date).
Feature selector . The Feature Selector was implemented with each of v tion Gain. The Feature Selector assigned a value to each term from the RSS feed data on each day during which the term occurred. 4. The evaluation of the three feature selection methods
This section describes how v 2 , Mutual Information ( MI ) and Information Gain ( I ) values were computed and 4.1. Computing v 2 , MI and I values 4.1.1. 2  X  2 contingency tables
Prior to calculating v 2 and MI ,a2  X  2 contingency table was required. Given a term, term tion date, date j ,a2  X  2 contingency table was constructed, where a is the number of items which contain term i and are posted on date b is the number of items which do not contain term i and are posted on date c is the number of items which contain term i and are not posted on date d is the number of items which do not contain term i and are not posted on date
For each term, a 2  X  2 contingency table is constructed to determine the observed frequencies, O . A second set of 2  X  2 contingency tables was also constructed for the expected frequencies, E . 4.1.2. Computing v 2 values
Given two sets of 2  X  2 contingency tables, described in Section 4.1.1 , the v respect to a publication date, date j was computed as follows: applied to each v 2 calculation as the degree of freedom is 1. The v approximate the v 2 value, such as described in Yang and Pedersen (1997) and Swan and Allan (2000) . The larger a v 2 value, the stronger the evidence to reject the null hypothesis, which means that term are dependent on each other, i.e. the proportion of items containing term or lower than elsewhere in the corpus. 4.1.3. Computing mutual information (MI) values
Given a set of 2  X  2 contingency tables containing the observed frequencies, O , described in Section 4.1.1 , the MI value of a term, term i with respect to a publication date date er the association strength between term i and date j , where MI ( term joint probability, P ( term i , date j ) must be greater than the product of the probability of P ( term Two examples of the use of this method for measuring the strength of two terms association can be found in
Conrad and Utt (1994) and Church and Hanks (1989) . 4.1.4. Computing information gain (I) values
The information gain, I ( D ; T ) is computed based on an entropy value of the date D and the conditional entropy of the date D given the term, T .
 D  X f d ; d g is a date variable which represents two events: the presence and absence of the Date, D . T  X f t ; t g is a term variable which represents two events: the presence and absence of the Term, T . I ( D ; T ) is then defined as follows: The conditional entropy of D given T is computed as follows:
In information theory ( Shannon &amp; Weaver, 1963 ), I ( D ; T ) is used to measure the average reduction in uncertainty about D that results from learning the value of T . This is the average amount of information that about D .

In this paper, the notion of uncertainty was applied to determine the degree of closeness between D and T of the publication date was taken into account, and H ( D j T ) was used to quantify the degree of uncertainty that T was a good indicator for D .
 Three examples of the use of this method can be found in Yang and Pedersen (1997), MacKay (2003) and Moore (2003) . Confusingly, Information Gain, I is sometimes called Mutual Information. In this paper, without the use of entropy, as explained in Section 4.1.3 . 4.2. Evaluating v 2 , MI and I
As discussed in the introduction, the evaluation method used was inspired by the way a journal paper is reviewed. In this context, the extent to which the three methods agreed with each other on determining the degree of the significance of a term on a given date was investigated. 4.2.1. Evaluation method and procedure
The evaluation procedure comprised four steps. 1. Three sets of significant terms were selected, one set for each feature selection method. contains the terms that all three feature selection methods agree are significant. The second set, the 2 X 3-votes set , contains the terms that at least two feature selection methods agree are significant. sets are used as the criteria against which the performance of each feature selection method could be assessed. 4. The performance of each method was then measured in terms of (b) disagreement . The method selects a term as significant, but a criterion set does not include it. 4.2.2. Limitations
The evaluation method does not employ human judgement to determine the effectiveness of the three meth-ods as ranking functions. This leads to the following limitations. term  X  X greement X  is used instead of  X  X recision X . 2. The fact that a criterion set disagrees with a method does not necessarily mean that the method incorrectly used instead of  X  X rror X . 3. The term,  X  X iss rate X , can only be interpreted from the criterion set perspective. From a human judgement point of view, it does not necessarily mean that the methods miss genuinely significant terms. 4.2.3. Evaluation results
The total number of unique terms identified was 1736715, and the total number of term X  X ate pairs was 2 912 581, each having a v 2 , MI and I value. Each method had different lower (LV) and upper values (UV); v 1.89E 10  X  715895.8, MI : 7.85  X  19.65, I : 7.9E 17  X  0.01. To ensure a fair evaluation, the term X  X ate pairs were sorted by their v 2 , MI and I values, and the same proportion of term X  X ate pairs were selected for each levels of reduction (from 61.56% until 99% reduction). By doing this, the proportion of agreements, disagree-ple, Table 4 (1st row) means for 61.56% reduction, the percentage of term X  X ate pairs selected was 38.44% (= 38.44/100 * 2912581 = 1119596), and the associated threshold values were 6.64 for v 4.83E 6 for I .
 proportion of agreements between the three feature selection methods and the 3-votes set gradually decreases after a 71% reduction, and sharply decreases after a 97% reduction. At 89% reduction, the proportion of agreements (48%) is less than the proportion of disagreements (51.55%). This experimental evidence suggests that few terms which are assigned extremely high values may actually be less significant than some other terms assigned lower values, as the three methods show a strong disagreement over judging term significance. From
Figs. 2 X 4 : v 2 has the largest proportion of agreements, and the smallest proportion of disagreement with the 2 X 3-votes set.
The proportion of agreements sharply decreases after a certain level of reduction (96% for v 89% for I );
The proportion of disagreements and miss rates overlap each other frequently; v 2 has an average miss rate of 0.32% with respect to the 39 reduction levels. I 16.43%, and MI 20.40%.
Despite the use of 2 X 3-votes sets, which are less stringent than 3-votes sets, the evidence also suggests that the three methods show strong disagreement for a high level of reduction. In addition, the miss rates indicate that MI and I are less effective than v 2 for the aggressive exclusion of terms, as MI and I seem to be more aggressive than v 2 . This is due to the following reasons. The 2 X 3-votes sets are dominated by v supported by either MI or I . Hence, v 2 can have the largest proportion of agreements and the smallest pro-portion of disagreements and miss rates.

In addition, Fig. 5 shows the proportion of agreement between two methods only, i.e. between v and MI , MI and I , which means that the 2 X 3-votes and 3-votes set were not used for comparison. Hence, we can see which pairs agree/disagree. Fig. 5 shows that v 2
MI also show some levels of agreement. In contrast, MI and I show a strong sense of disagreement from 72% reduction onwards. Moreover, despite some small fluctuations, the proportion of agreements for both pairs v  X  I and MI  X  I gradually decrease. The proportion of agreements between v fluctuation from 65.01% to 83.03% for the reduction levels between 91% and 96%. This fluctuation indicates that for a high level of reduction, v 2 and MI tend to agree with each other.
 important. The first 10000 RSS items were selected from the large collection, and the experimental procedure was used to generate 64346 term X  X ate pairs. Diagrams corresponding to Figs. 1 X 5 with respect to the 64 346 a slight increase in the proportion of agreements for the reduction levels 98 X 99%. There is also a significant difference between Figs. 5 and 10 . Fig. 5 shows that v 2
Fig. 10 shows that v 2 and MI agree with each other the most. Both figures, however, show that MI and I exhi-bit relatively strong disagreement. 4.2.4. Discussion
As stated in Section 4.2.3 , the proportion of agreements for I sharply decreases earlier than v
I takes the presence and absence of terms into account. This results in a higher degree of term reduction than be excluded because of the very high values that less genuinely significant terms may have.
With respect to the average proportion of miss rates for 39 reduction levels, MI has the largest proportion of miss rates (20.40%). Fig. 3 shows that for reduction levels between 75% and 90%, MI tends to exclude a large proportion of terms judged significant (an average 26.20% miss rate), whilst v (an average 12.95% miss rate) do not. This finding suggests that MI is different from the other two methods means that MI is less effective for aggressive term elimination than the other two methods.
As Figs. 5 and 10 show, MI and I disagree relatively strongly. This is due to the fact that I considers the amount of uncertainty in the presence and absence of a term, t date, d j , as stated in Eqs. (4) and (5) . In contrast, MI only considers the joint probability, P ( t ability that the term, t i is present, P ( t i ), and the probability that the date, d date, and (2) an illustration of how MI behaves differently from v threshold values of each method are v 2 = 6.64, I = 4.83E 6 and MI = 3.93. Table 5 shows that MI starts to judge the term  X  X sian nations X  to be significant on 12/10/2004, but, v tsunami. The three methods agree that the term is significant between 26/12/2004 and 28/12/2004. ing to the evaluation method used, v 2 is the best of the three methods. In addition, v
MI and I , with MI and I showing significant disagreement. Aside from the evaluation method used, it is pos-
This suggests that the v 2 method would be unreliable for an overwhelming majority of the data, which does were judged to be highly significant.
The basic assumption of using any kind of feature selection method is that insignificant terms can reliably be excluded by increasing threshold values. The experimental evidence, however, suggests otherwise. The three selection methods showed strong disagreement at a high level of reduction. I and MI disagree with each other the most, as they identify different types of unusual/significant behavior, whilst v sual/significant behavior, but the key question is whether either or both of these types of unusual behavior represent genuinely significant real-world events. Table 6 shows an example of five terms which were assigned extremely large v 2 values, along with their associated I and MI values. Excerpts of the RSS items posted on relevant dates are shown below. out a lot of work. [03/06/2004]: You can justify the slightly increased price when you think of the extra work and expense involved with just getting certified as organic. [06/09/2004]: Building a pond was incredibly easy. Getting the hole dug was the hardest part .
The corpus used contained two significant world-events, i.e. the US election (3/11/2004) and the tsunami (26/12/2004). The v 2 , I and MI values of  X  X S election X  and  X  X sunami X  are used for comparison. For the term,  X  X S election X , the maximum value of v 2 , MI and I are 6066.67, 10.95 and 5.31E 05. For the term,  X  X sunami X  date. It is, however, highly unlikely that they are much more significant than the two terms representing two or I (i.e. found in the 2 X 3-votes set). 5. Conclusions
The three feature selection methods, v 2 , Mutual Information ( MI ) and Information Gain ( I ), were evalu-
The experimental evidence suggests that the three methods will have a significant degree of disagreement for some terms assigned extremely large values.

Finally, the voting method used to compare the three methods provided a great deal of illumination on the differences between the methods and was able to suggest a best method. Ultimately, however, human classi-fications might be needed to decide which method was the best at identifying significant terms at different since the evaluation method showed that there is no convergence between methods for high levels of reduction, it follows that caution would always be needed in interpreting results, whichever method is used. It may be that heuristics such as upper thresholds or the exclusion of low frequency terms might be required to provide improved results but a full human classification exercise would be needed to evaluate such methods. Acknowledgements The work was supported by a European Union grant for activity code NEST-2003-Path-1. It is part of the
CREEN project (Critical Events in Evolving Networks, contract 012684). We thank the reviewers for their helpful comments.
 References
