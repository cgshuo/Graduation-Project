 Transformation of both the response variable and the predictors is commonly used in fitting regression models. However, these transformation methods do not always provide the maximum linear correlation between the response variable and the predictors, especially when th ere are non-linear relationships between predictors and the respons e such as the medical data set used in this study. A spline based transformation method is proposed that is second order smooth, continuous, and minimizes the mean squared e rror between the response and each predictor. Since the computation time for generating this spline is O(n), the processing tim e is reasonable with massive data sets. In contrast to c ubic smoothing splines, the resulting transformation equations also disp lay a high level of efficiency for scoring. Data used for pred icting health outcomes contains an abundance of non-linear relations hips between predictors and the outcomes requiring an algorithm for modeling them accurately. Thus, a transformation that fits an adaptive cubic spline to each of a set of variab les is proposed. These curves are used as a set of transformation functions on the predictors. A case study of how the transformed variables can be fed into a simple linear regression model to predict risk outcomes is presented. The results show significant improvement over the performance of the original variables in both linear and non-linear models. I.2.6 [Artificial Intelligence]: Learning spline, variable transformation, linear model, data mining, prediction, adaptive, risk, outcomes Transformation methods (Draper and Smith, 1981) are commonly used in regression analysis. Most of these transformation methods are very simple because it is not only of between predictors and the response. Although there are transformation methods such as those proposed by Breiman and Friedman (1985) that can maximize the correlation between variables, this method is not suitable when there are piecewise relationships. A transformation that can maximize the linearity between predictors and the re sponse is proposed. The proposed method works well even if there are piecewise relationships. In data mining context, this method is very useful because the focus is on maximizing the predictive value (Hastie et al., 2001) while limited in time spent on understanding this relationship between variables. Many different non-linear m odeling techniques claim to describe relationships in non-linear variables. However, due to the large number of non-linear patte rns to investigate, even if sufficient computational power is available, the probability of finding false patterns is increased. Rather, analysis and transformation of one variable at a time to maximize its linear relationship to the dependent variable maintains low complexity of the solution, if done carefully. While this algorithm does not identify interactive relationships in the data, it does not take away from the ability to model inte ractions either. The gain in using a piecewise algorithm versus combinations of functions on the entire range of values is th at the complexity of the model can be kept low, while effectivel y adapting to different behavior at different ranges within a predictor. While many spline algorithms (Boucheta and Djeddi, 1994; Lindstorm, 1999; Prvan, 2000; L uo and Wahba, 1997; and Jupp, 1978) exist, a smooth series of third order polynomials was chosen. Because of the strange distributions of real-world data, we will show that polynomial curves cannot always fit the relationship between variables accurately. Cubic smoothing splines may be a good fit, but it is often the case that one part of the curve works well with one smoothing constant, and the other side of the curve works well with a different smoothing constant. The data is an 115,000 member subset of a repository of various (MEDai). The sample is constrained to members enrolled for the full period being evaluated and the variables available to be used as predictors are based on ETGs (Episode Treatment Groups). The data set is one used to project future annual cost based on previous charges within 21 ETG classes of drugs being taken by those members. The classes of drugs include Arthritis, Asthma, Bronchitis, Breast Neopl asm, CHF (Congestive Heart Failure), CNS (Central Nervous System), CV medications, Degenerative Diseases, Dermatology, Diabetes, Fractures, GI, GU, Hypertension, Metabolic, Pneumonia, Psychosis, Renal Failure, Skin Inflammation, Tonsillitis and Trauma. The relationship between these drug costs and future healthcare cost generally is extremely non-linear, and has a high number of outliers. These relationships will be modeled individually using the adaptive spline, transformed, and finally fed into a multivariate model for evaluation. While multi-dimensional splines are theoretically obtainable, it has been found that these over-fit very quickly as the number of variables increases (Fan and Gijb els, 2000). In order to create a generalizable model, we chos e to smooth the data in one dimension at a time, and feed the transformed variables into a global linear model. Transformation of the response variables are not considered (Draper and Smith, 1981) because of the large residuals that those transformations create in m odeling these kinds of data sets data sets where the response variable under-went a transformation yielded R-Squared values less than zero because the bias is so great. The proposed choice of splines is one that is flexible enough to adapt to practically any non-linear relationship, but one that is smooth and has a minimal number of degrees of freedom to prevent over-fitting. Given a set of knots (x-coordinates specified only), we construct the set of cubic curves beginning and ending at the knots that collectively minimize the Mean Squared Error. To keep the degrees of freedom at a minimum, we require 1 st and 2 nd order smoothness at the knots. No boundary conditions are imposed at the end-points, leaving the number of degrees of freedom to be N+3, where N is the number of segments used fo r constructing the curve. Given the set of x-coordinates construct the N cubic equations with end-points at these x-coordinates minimizing the sum of the squares of the errors, while meeting the following 3N-3 smoothing conditions (matching end-points, first, a nd second derivatives at nodes): For i = 1 to N-1, Define the N equations, I = 1 to N: Define the Error of the aggregate curve: There are 4N coefficients that need to be solved for, but the 3N-3 smoothing conditions leave only N+3 degrees of freedom on which the optimization will be performed. If the following unknowns can be solved for (1) the remaining 3N-3 coefficients can be written using the following algorithm: For i = 2 to N: (2) Now set the N+3 partial derivatives of the Error Function equal to zero. Also, we may refer to the variables in (1) as Setting the partial derivatives to zero yields the following N+3 equations. For each i from 1 to N+3 (3) The partial derivatives of the N individual cubic pieces can be denoted by The notation is changed to accommodate the fact that the 4N coefficients are not known, but are linear functions of the N+3 [A][v]=[b]. Multiplying out the terms within the summations so that both sides of the equation are functions of x, we get: [A]= [b]= Any matrix solver will yield the N+3 unknown values of [v], and the remaining 3N-3 coefficients can be calculated by substituting the N+3 values into equation (2). The 4N coefficients have now been calculated. For the result presented in this pa per, an automated procedure is used, recursively finding the best location for an additional knot. A significance test is performed comparing the change in accuracy before and after the proposed addition of each knot. If iterations are stopped. This proce dure finds that it is best to concentrate most of the knots where the curve is most non-linear clearly the left side of the curve. On hypertension drug cost, knots were placed at x={0,2,5, 15,30,50,100}. The knot at x=0 would normally be unnecessary because it is at the end-point, but this particular data set ha d 5-10 missing values per variable (out of over 300,000), and they were assigned a value of  X 1. Having a knot there prevented those few data points from having undue influence the rest of the curve. The curves appear visually to follow the non-linear trend of the relationship between the independe nt variables and the response smoothness restrictions prevent the third order segments of the spline from over-fitting to some of the bobbles in the relationship that appear random. The 95% confidence intervals relationship of lowest complexity that truly follows the trend of the data. Figure 3: Modeling hypertension drugs versus next year dollar cost. Spline is shown in red. Regression line is shown in blue. Similarly, knots were placed at x={0,2,5,10,20,60,200} for arthritis drug cost. Figure 4: Modeling arthritis drugs versus next year dollar cost. Spline is shown in red. Reg ression line is shown in blue. Attempts were be made to fit these relationships with polynomial curves. The resulting cu rves are shown in figures 5 and 6. Figure 5: Modeling hypertension drugs versus next year dollar cost. Similar to figure 3 plus 2 nd and 3 polynomial curves shown in blue. On hypertension drugs, the 3 rd order polynomial fits the data better than the linear regression. Ho wever, it is apparent that the inherent relationship of the variab le is not that of a polynomial. Of course a polynomial with a high enough degree will fit the data perfectly, but over-fitting becomes a concern with this. Figure 6: Modeling arthritis drugs versus next year dollar cost. Similar to figure 4 plus 2 nd and 3 rd order polynomial curves shown in blue. On arthritis drug cost, the polynomial is even less of a decent fit for this non-linear relationship. The curves tend to accommodate the few outliers even more. More specifically, 2,952 members (2.6%) of the 115,000 take arthritis drugs. Out of those 2,952, apparent from the graph that th e polynomial focuses on less than 10% of the data and is completely inaccurate on the 90% of arthritis members with drug costs less than $500. The spline is much more flexible, and is give n more knots at the lower costs so that all the members can have their risk assessed accurately. Splines are generally good only for small dimensional problems as their complexity grows geometrically for each additional dimension. To avoid over-fitting in this high dimensional problem, we keep the complexity as low as possible by transforming each of the variables individually to its one-dimensional spline curve. The end goal is that the new variables will perform well in a linear regression. Therefore, if follows intuitively that the best transformation would likely create a new variable with a linear relationship to the dependent variable. Using the splines for hypertension drug cost and arthritis drug cost, we create new variables called  X  X ypertension Spline Estimate X  and  X  X rthritis Spline Estimate. X  Graphing the transformed variable against the dependent variable, it is clear that the new relationship is now linear, as the grouped means match up very well with the regression line. The correlation of the original hypertension drug cost variable to the dependent variable is 0.1603. The correlation of the new variable to the dependent variable is 0.1758. While this improvement is only 10%, a more substantial improvement can be observed in the arthritis drug cost transformation, as that variable is much more non-linear to begin with. Figure 7: Transformed hyperten sion drug variable versus next year dollar cost. The new variable is linear with respect to the next year cost. Figure 8: Transformed arthriti s drug variable versus next year dollar cost. The new variable is linear with respect to the next year cost. The spline fit is not perfect here , and could have benefited from more knot placement on the low end. Nevertheless, the fit is still  X  X ood X  as the regression line passes through or is close to all the grouped means. The correlation of the original arthritis drug cost variable to the dependent variable is only 0.0847. The correlation of the new spline estimate variable to the dependent variable 0.1142, an increase of 35%. The transformation is applied to all 21 independent variables, and the 7 variables most improved in correlation are recorded in the table 1. ETG Original Renal Failure .1870 .2547 36% Skin Inflammation .0449 .0590 31% The average percent increase in correlation of all 21 variables is 22%. The range of improvement is from 5% to 40% as the most non-linear variables are going to be altered more drastically in the transformation than will the variables with a fairly linear relationship to begin with. It is known that the improvement of correlation of individual variables does not necessarily improve the accuracy of the aggregate model, so it is necessary to combine the transformed variables into a model and evaluate the change in accuracy. Five models were calculated and evaluated for comparison: The linear regression on the original variables is important to include, because it serves as the baseline. It allows us to see the results of a straight linear model with no transformations. The second and third models ar e polynomial regression models, which give an idea of how a simple non-linear model performs in comparison with the spline transformation model. It also gives an idea of how quickly over-fitting can occur, even with a data set this large. The reason for including the fourth model is that it is common within the medical industry to us e binary versions of ETGs for predictive models. For example, diabetes charge greater than zero indicates that the member has diabetes, and would be assigned a value of 1. A model w ith the binary variables gives an idea of what is the industry standard that we are comparing to. In the evaluation of these 5 m odels, we use two statistics: Table 4: Differential between training and validation results 2nd Degree -10% 3rd Degree -20% Table 5: Improvement made by using the spline variables over each of the other techniques (based on validation results).
 Percent Improved R-Squared Sensitivity 2nd Degree 25% 5% 3rd Degree 28% 5% The tables are based on a 67-33 split of the data set of 115,018 members. 76,679 members were placed in the training set and 38,339 members were placed in the validation set so that the training and validation results could be compared. On the validation set, there was a small improvement from the linear model to the 2 nd degree polynomial, but the R-Squared value started to decline when the 3 rd degree polynomial was applied. It should also be noted that percent decline between the training and validation R-Squared increased dramatically for the 3 degree polynomial, indicating that the complexity of the model was over-fitting on even this large data set. The accuracy on the linear regression dropped 8% from .1463 to .1341, a notable amount, but likely not e nough to characterize the model dropped 20% from the training set to the validation set. The 2 degree polynomial showed a 10% decline in R-Squared as it was applied on the validation sa mple, but yielded the best overall R-Squared value of the polynomial models. The model using the spline transf ormed variables performed the best in every area. Although a simple linear regression was used as the final model, the variables had under-gone a low-complexity transformation that customized them to work well in a linear model. We acknowledge A.I. Insight, Inc. and MEDai, Inc. for the use of their predictive modeling technology, MITCH (Multiple Intelligent Tasking Computer Heur istics), as well as the use of their repository of medical data. [1] Boucheta, R., Djeddi, M. (1994). Smoothing spline: local [2] Breiman, L. and Friedman, J. (1985),  X  X stimating Optimal [3] Draper, N. R. and Smith, H. (1981). Applied Regression [4] Fan, J. and Gijbels, I. (2000). Local polynomial fitting , [5] Hastie, T., Tibshirani, R. and Friedman, J. (2001). The [6] Jupp, D. (1978),  X  X pproximation to data by splines with [7] Lindstrom, M. (1999),  X  X ena lized estimation of free-knot [8] Luo, Z., and Wahba, G. (1997),  X  X ybrid adaptive splines X , [9] Prvan, T. (2000),  X  X east squares splines with variable knots 
