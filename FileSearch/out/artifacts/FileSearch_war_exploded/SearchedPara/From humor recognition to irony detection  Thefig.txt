 1. Introduction
Figurative language is one of the most arduous topics facing Natural Language Processing (NLP). Unlike literal language, the sarcastic expressions to express a negative attitude [24,3] ).

Such devices entail cognitive capabilities to abstract and meta-represent meanings beyond and so on.

In this framework, this paper aims at showing how two specific domains of figurative language such as polarity and emotional scenarios. We are especially focused on discussing how underlying knowledge, which relies on tual level.
 The paper is organized as follows. In Section 2 we underline the theoretical issues which underlies humor and irony. In draw some final remarks and address the future work. 2. Theoretical issues 2.1. Humor view, Ruch [37] has analyzed the relationship between personality and humor appreciation, providing interesting observations studies have tried to explain humor by means of semantic and pragmatic patterns. Attardo [1,2] tries to explain verbal humor background in order to categorize humor as an entire phenomenon. 2.2. Irony is focused on verbal irony. We think that there are some features given in situations where humor and irony are implied that are worth analyzing.
 considering any pragmatic rules. often considered as fundamental for ironic expressions.
 are any elements in common that may allow us to represent the basic features of both phenomena. 3. Figurative language processing
Now, considering the approaches related to automatic humor processing, we can divide them in two areas: generation and rec-erate a funny effect. The research described in [6] has shown the importance of these forms of patterns, especially based on ments that underlie humor: 1  X  What do you use to talk to an elephant? An elly-phone".
 phonologically and semantically to the word which gives its right meaning: elephant. This form of funny question answering ed how incongruity and opposite concepts are important elements for producing funny senses. By means of combining words, which in social terms represent opposite referents, authors have automatically produced new funny senses for acronyms such as MIT (Massachusetts Institute of Technology): 2  X  Mythical Institute of Theology".
 With respect to humor recognition, most investigation is focused on the analysis of particular funny structures: one-liners. in 3: 3  X  Infants don't enjoy infancy like adults do adultery".
 elements have provided evidence for characterizing humor in terms of formal components (which may automatically be recog-nized). For instance, Mihalcea and Strapparava [29] applied machine-learning techniques to identify humorous patterns in spaces which are triggers of humor: human centric vocabulary (example 4), negative orientation (example 5), and professional communities (example 6): 4  X 
Of all the things I lost, I miss my mind the most". 5  X 
Money can't buy your friends, but you do get a better class of enemy". 6  X  It was so cold last winter that I saw a lawyer with his hands in his own pockets".

Moreover, the work of Sj X bergh and Araki [40] is focused on finding patterns in syntactic and semantic layers. According to their results, devices such as similarity, style or idiomatic expressions, are sources in which humor tends to appear. When considering bigger structures such as news articles or blogs, the research of Mihalcea and Pulman [28] has evidenced how of semantic ambiguity and keyness to characterize funny blogs. Furthermore, the evaluation described in [34] has supported the relevance of some outstanding features, such as affective and emotional content, to automatically retrieve funny web comments.
 sources to investigate deeper features (see [29] ).

On the other hand, the computational approaches which deal with more abstract uses of figurative language tend to be more ing, trends discovery, or electronic commerce. For instance, regarding automatic irony processing, the research described by analyzed a large quantity of humorous similes of the form the fact of detecting emoticons, onomatopoeic expressions, punctuation and quotation marks. Based on this simple approach, cally distinguish irony from non-irony in figurative comparisons. They noted how the presence of markers like produce a rule-based categorization between ironic and non-ironic texts.

In fine-grained approaches, Burfoot and Baldwin [9] have tried to determine whether or not newswire articles can automat-
Based on a semi-supervised approach, they proposed surface features such as content words (words regarding information results, precision and recall scores are significantly positive.

Although such approaches have proved that both humor and ir ony can be handled in terms of computational means, it is necessary to improve the mechanisms to represent their chara cteristics, especially, to create a feature model capable of symbolizing as possible, both linguistic and social knowledge in order to describe deeper and more general properties. For 4. Feature model favorable and unfavorable ironic contexts based on profiled polarity, unexpectedness and emotional scenarios. 4.1. Preliminaries usually entails knowledge beyond the word or the sentence. Let us consider example 7 below to clarify this point. 7  X  Jesus saves, and at today's prices, that's a miracle!"
Unlike examples 1 and 3, in which humor was given by phonological ambiguity (elly-phone vs. telephone), in this example humor is given by exploiting semantic and pragmatic ambiguity. The funny effect relies on different possible interpretations that are based on semantic meanings and cultural information. Such facts, according to cognitive grammar arguments
Langacker [25] , turn the figure of the sentence; i.e.  X  Jesus saves sentence. Those changes generate an ambiguous meaning and, consequently, a funny result. In other words, this example or loss. The second one shifts such meaning, from the logical sense related to a religious interpretation, to a ground sense concerning economy. This sense is promoted as figure and then the meaning of the entire sentence becomes funny. Such type of strategies, according to Mihalcea and Strapparava [29] , leads surprise and create the humorous effect.
On the other hand, since irony cuts through different aspects of language (from pronunciation to lexical choice, syntactic ples to illustrate the difficulty of this task: 10  X  I thank God that you are unique!."
According to people's perception, which is profiled by employing specific user-generated tags, these three examples could be aggressiveness, surprise, desire, and why not, zest and pleasure. Finally, we cannot obviate their funny effect.
Based on these preliminary arguments, the set of features we evaluate are: i. ambiguity, concerning with three layers: structural, morphosyntactic and semantic; ii. polarity, concerning with words that denote either positive or negative semantic orientation; iii. unexpectedness, concerning with contextual imbalances among the semantic meanings of the words; iv. emotional scenarios, concerning with psychological contexts regarding natural language concepts. 4.2. Structural ambiguity this property, we aim at investigating how much valuable information could be extracted by measuring, in terms of language tural ambiguity. Such structural ambiguity can be represented by the dispersion in the number of combinations among the tential funny situations and should thus appear quite often in humorous texts. 4.3. Morphosyntactic ambiguity mine the complexity of humorous and non-humorous texts. 4.4. Semantic ambiguity
Ambiguity is closely related to the different meanings that a word, phrase or sentence may produce. As we already mentioned, triggers and, consequently, we aim at studying how valuable information can be obtained from these layers. We defined a mea-
The measure is based on the hypernym distance between synsets, calculated with respect to WordNet. Our hypothesis relies on logical meaning is broken and humor is produced. 4.5. Polarity
One of the most common properties, both in humor and irony, relies on conveying the opposite meaning by profiling positive 76,400 entries (30,458 positive and 45,942 negative ones). 4.6. Unexpectedness Irony often relies on situational phenomena related to incongruity, non expected situations, senseless, absurd, and so on.
Lucariello [27] suggests the term unexpectedness to represent when facing natural language processing tasks. Therefore, our underlying hypothesis is: the lesser semantic relatedness, the greater contextual imbalance (funny/ironic texts); the greater semantic relatedness, the lesser contextual imbalance (non funny/ironic texts). 4.7. Emotional scenarios rative expressions rely on these contents to produce their effect (cf. example 8) . ing information regarding contents beyond grammar, and beyond positive or negative polarity. In others words, this feature word), and pleasantness (degree of pleasure produced by the words). 5. Evaluation
Several experiments were performed in order to evaluate the capabilities of automatically discriminating both humorous and ironic texts. The following schema summarizes the evaluation phase: i. feature representativeness. Phase focused on representing our evaluation corpus by means of the features previously discussed. ii. feature relevance. Phase focused on assessing the features by means of a classification task. 5.1. Evaluation corpus personal judgments, we decided to collect examples a-priori considered either funny or ironic. Thus, we centered on user-for one set, was determined in order to retrieve the remaining four: they should be labeled with a hashtag ; i.e. a user-generated tag provided by the users themselves when posting their texts to focus their contributions on particular subjects. requirements.

By considering this approach, apart from avoiding personal judgments, we obtained two adjacent benefits: i) neither a manual extend the scope of this research to others types of texts which contain figurative language. 5.2. Feature representativeness were trained with trigrams, employing interpolation and Kneser-Ney discount as smoothing methods. The perplexity for each data set was determined by comparing its language model against Google language model. Subsequently, every text was repre-the size of the data set (i.e. 10,000), and finally multiplying this result for the length of each text.
With respect to the sentence complexity, we employed Formula 1, introduced in [5] , to estimate how complex the syntactic structure is: where v l and n l are the number of verbal and nominal links respectively; and cl is the number of clauses for text t order to reduce ambiguity due to POS tags, we enable a POS disambiguation module before running the syntactic parser. The last experiment concerning ambiguity was addressed to represent semantic dispersion among the words of a text. Thus,
Formula 2 was employed: the length of the hypernym path between synsets ( s i , s among the senses of a word. For instance, the noun killer has four synsets. data set, summing the semantic dispersion of all the words in a text, and dividing by its length.
The following experiment was focused on determining polarity in the texts. First, all texts were stemmed and stopwords were spective, the underlying polarity: where s p is the set of positive words; s n is the set of negative words; and | d | is the length of d
Contextual imbalance was determined by measuring the semantic similarity among the words. As conducted on the previous experiment, texts were stemmed and stopwords removed. The semantic similarity was estimated by applying the Resnik mea-ited in a 3-word window. A backoff method to assign the most frequent sense was enabled as well. representable (score=3), and tends to be pleasant (score=2.75). 5.3. Feature relevance the set of features, each one of the 50,000 documents was converted in a frequency-weighted term vector ii. features regarding polarity (positive and negative), unexpectedness (contextual imbalance), and emotional scenarios (ac-iv. features regarding polarity, unexpectedness, emotional scenarios, and ambiguity, considering the set irony vs . the sets
Results in terms of accuracy, precision, recall, and F-measure, are detailed in Tables 1 concerning irony (fourth classifier).

Implications are discussed in the following section. 6. Discussion guage (humor and irony) are satisfactory.
 ( Table 1 ), humor achieves ratios which always exceed 70% of accuracy, whereas irony hardly achieves ratios higher than 60% criminating humor vs . irony, and vice versa, results are (usually) better than when classifying the remaining sets. lying patterns in both figurative devices that are well-represented by these features.
 humor and politics sets, the most informative features were perplexity, pleasantness, sentence complexity, and semantic perplexity, and contextual imbalance.

This is clearer when analyzing the learning curves achieved in each classification. In Figs. 1 the ones regarding the third and fourth classifier, respectively.
 formance to be satisfactory.

Finally, we would like to stress some remarks regarding every feature. given two different distributional schemes, the structures that have a broader range of combinations are the ones concerning humorous and ironic discourses.
 2. Morphosyntactic ambiguity seems to be another important feature to represent figurative language. By means of measuring syntactic complexity, we could note that both funny and ironic texts are less complex than texts in the remaining sets. This mantic and pragmatic layers. tions, it is more likely to generate hollows of ambiguity which contribute to produce more complex meanings both in funny and ironic texts.
They suggest the relevance of negative information to generate humor. In addition, concerning irony, these results makes question our assumption about the use of positive information to express an underlying negative meaning. nificantly differ among them is more likely to be used in figurative language, than a text whose words project senses that ironic effects. through which, people easily produce favorable contexts to express figurative language. 7. Conclusions and further work
In this paper we have presented an approach to the representation of tow important figurative devices in short online texts: mal linguistic elements. An evaluation corpus of 50,000 texts automatically retrieved from Twitter was used to evaluate the features by verifying their performance with other types of data sets, and considering other types of figurative devices. Acknowledgments
This work has been done in the framework of the VLC/CAMPUS Microcluster on Multimodal Interaction in Intelligent Systems 7 Marie Curie People Framework, and by MICINN as part of the Text-Enterprise 2.0 project (TIN2009-13391-C04-03) within the Plan I+D+I. The National Council for Science and Technology (CONACyT -Mexico) has funded the research work of Antonio Reyes.

References
