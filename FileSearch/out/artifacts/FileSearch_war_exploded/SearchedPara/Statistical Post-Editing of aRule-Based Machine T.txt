 Current machine translation systems are far from perfect. To achieve high-quality output, the raw translations they generate often need to be corrected, or post-edited by human translators. One way of in-creasing the productivity of the whole process is the development of automatic post-editing ( APE ) sys-tems (Dugast et al., 2007; Simard et al., 2007).
Many of these works propose a combination of rule-based machine translation ( RBMT ) and statisti-cal machine translation ( SMT ) systems, in order to take advantage of the particular capabilities of each system (Chen and Chen, 1997).

A possible combination is to automatically post-edit the output of a RBMT system employing a SMT system. In this work, we will apply this technique into two different corpora: Parliament and Proto-cols . In addition, we will propose a new human eva-luation measure that will deal with the impact of the automatic post-editing.

This paper is structured as follows: after a brief introduction of the RBMT , SMT , and APE systems in Section 2, Section 3 details the carried out experi-mentation, discussing its results. Finally, some con-clusions and future work are presented in Section 4. Three different systems are compared in this work, namely the RBMT , SMT , and APE systems.
 Rule-based machine translation. RBMT was the first approach to machine translation, and thus, a relatively mature area in this field. RBMT sys-tems are basically constituted by two components: the rules, that account for the syntactic knowledge, and the lexicon, which deals with the morphologi-cal, syntactic, and semantic information. Both rules and lexicons are grounded on linguistic knowledge and generated by expert linguists. As a result, the build process is expensive and the system is difficult to maintain (Bennett and Slocum, 1985). Further-more, RBMT systems fail to adapt to new domains. Although they usually provide a mechanism to cre-ate new rules and extend and adapt the lexicon, changes are usually very costly and the results, fre-quently, do not pay off (Isabelle et al., 2007). Statistical machine translation. In SMT , transla-tions are generated on the basis of statistical models, which are derived from the analysis of bilingual text corpora. The translation problem can be statistically formulated as in (Brown et al., 1993). In practice, several models are often combined into a log-linear fashion. Each model can represent an important fea-ture for the translation, such as phrase-based , lan-guage , or lexical models (Koehn et al., 2003). Automatic post-editing. An APE system can be viewed as a translation process between the output from a previous MT system, and the target language. In our case, an APE system based on statistical mod-els will be trained to correct the translation errors made by a RBMT system. As a result, both RBMT and SMT technologies will be combined in order to increase the overall translation quality. We present some experiments carried out using the introduced APE system, and comparing its perfor-mance with that of the RBMT and SMT systems. In the experimentation, two different English-to-Spanish corpora have been chosen, Parliament and Protocols , both of them provided by a professional translation agency.
 Corpora. The Parliament corpus consists of a se-ries of documents from proceedings of parliamen-tary sessions, provided by a client of the transla-tion agency involved in this work. Most of the sen-tences are transcriptions of parliamentary speeches, and thus, with the peculiarities of the oral language. Despite of the multi-topic nature of the speeches, differences in training and test perplexities indicate that the topics in test are well represented in the training set (corpus statistics in Table 1).
On the other hand, the Protocols corpus is a collection of medical protocols. This is a more difficult task, as its statistics reflect in Table 1. There are many factors that explain this complexity, such as the different companies involved in training and test sets, out-of-domain test data (see perplexity and Training Test out-of-vocabulary words), non-native authors, etc. Evaluation. In order to assess the proposed sys-tems, a series of measures have been considered. In first place, some state-of-the-art automatic metrics have been chosen to give a first idea of the quality of the translations. These translations have been also evaluated by professional translators to assess the in-crease of productivity when using each system. Automatic evaluation. The automatic assessment of the translation quality has been carried out us-ing the BiLingual Evaluation Understudy (BLEU) (Papineni et al., 2002), and the Translation Error Rate (TER) (Snover et al., 2006). The latter takes into account the number of edits required to con-vert the system output into the reference. Hence, this measure roughly estimates the post-edition process. Human evaluation. A new human evaluation measure has been proposed to roughly estimate the productivity increase when using each of the systems in a real scenario, grounded on previous works for human evaluation of qualitative fac-tors (Callison-Burch et al., 2007). One of the de-sired qualities for this measure was that it should pose little effort to the human evaluator. Thus, a binary measure was chosen, the suitability , where the translations are identified as suitable or not sui-table. A given translation is considered to be suitable if it can be manually post-edited with effort savings, i.e., the evaluator thinks that a manual post-editing will increase his productivity. On the contrary, if the evaluator prefers to ignore the proposed translation and start it over, the sentence is deemed not suitable. Significance tests. Significance of the results has been assessed by the paired bootstrap resampling method, described in (Koehn, 2004). It estimates how confidently the conclusion that a system outper-forms another one can be drawn from a test result. Experimental setup. Rule-based translation was performed by means of a commercial RBMT system. On the other hand, statistical training and translation in both SMT and APE systems were carried out using the Moses toolkit (Koehn et al., 2007). It should be noted that APE system was trained taking the RBMT output as source, instead of the original text. In this way, it is able to post-edit the RBMT translations.
Finally, the texts employed for the human eva-luation were composed by 350 sentences randomly drawn from each one of the two test corpora des-cribed in this paper. Two professional translators carried out the human evaluation. 3.1 Results and discussion Experimentation results in terms of automatic and human evaluation are shown in this section.
 Automatic evaluation. Table 2 presents Parlia-ment and Protocols corpora translation results in terms of automatic metrics. Note that, as there is a single reference, this results are somehow pes-simistic.

In the case of the Parliament corpus, SMT system outperforms the rest of the systems. APE results are slightly worse than SMT , but far better than RBMT .
However, when moving to the Protocols corpus, a more difficult task (as seen in perplexity in Table 1), the results show quite the contrary. SMT and APE systems show how they are more sensitive to out-of-domain documents. Nevertheless, the RBMT sys-tem seems to be more robust under such conditions. Despite of the degradation of the statistical models, APE manages to achieve much better results than the other two systems. It is able to conserve the robust-ness of RBMT , while its statistical counterpart deals with the particularities of the corpus.
 Human evaluation. Table 3 shows the percentage of translations deemed suitable by the human eva-luators. Two professional evaluators analysed the suitability of the output of each system
In the Parliament case, APE performance is found much more suitable than the rest of the systems. In fact, this difference between APE and the rest is sta-tistically significant at a 99% level of confidence. In addition, significance tests show that, on average, APE improves RBMT on 59 . 5% of translations.
Regarding to the Protocols corpus, it must be noted that a first review of the translations pointed out that the SMT system performed quite poorly. Hence, SMT was not considered for the human eva-luation on this corpus.

Figures show that APE complements and im-proves RBMT , although differences between them are tighter than in the Parliament corpus. However, significance tests still prove that these improvements are statistically significant ( 68% of confidence), and that the average improvement is 6 . 5% .

It is interesting to note how automatic measures and human evaluation seem not to be quite corre-lated. In terms of automatic measures, the best sys-tem to translate the Parliament test is the SMT . This improvement has been checked by carrying out sig-nificance tests, resulting statistically significant with a 99% of confidence. However, in the human eva-luation, SMT is worse than APE (this difference is also significant at 99% ). On the other hand, when working with the Protocols corpus, automatic me-trics indicate that APE improves the rest (significant improvement at 99% ). Nevertheless, human evalua-tors seem to think that the difference between APE and RBMT is not so significant, only with a confi-dence of 68% . Previous works confirm this apparent discrepancy between automatic and human evalua-tions (Callison-Burch et al., 2007).
 Translator X  X  commentaries. As a subproduct of the human evaluation, the evaluators gave some personal impressions regarding each system perfor-mance. They concluded that, when working with the Parliament corpus, there was a net improvement in the overall performance when using APE . Changes between RBMT and APE were minor but useful. Thus, APE did not pose a system degradation with respect to the RBMT . Furthermore, a rough estima-tion indicated that over 10% of the sentences were perfectly translated, i.e. the translation was human-like. In addition, some frequent collocations were found to be correctly post-edited by the APE system, which was felt very effort saving.

With respect to the Protocols corpus, as expected, results were found not so satisfactory. However, human translators find themselves these documents complex.

Finally, in both cases, APE is able to make the translation more similar to the reference by fix-ing some words without altering the grammatical structure of the sentence. Finally, translators would find very useful a system that automatically decided when to automatically post-edit the RBMT outputs. We have presented an automatic post-editing sys-tem that can be added at the core of the professional translation workflow. Furthermore, we have tested it with two corpora of different complexity.

For the Parliament corpus, we have shown that the APE system complements and improves the RBMT system in terms of suitability in a real transla-tion scenario (average improvement 59 . 5% ). Results for the Protocols corpus, although less conclusive, are promising as well (average improvement 6 . 5% ). Moreover, 67% of Protocols translations, and 94% of Parliament translations were considered to be sui-table.

Finally, a procedure for effortless human eva-luation has been established. A future improve-ment for this would be to integrate the process in the core of the translator X  X  workflow, so that on-the-fly evaluation can be made. In addition, several pos-sible sources of errors have been identified which will help develop future system enhancements. For example, as stated in the translator X  X  commentaries, the automatic selection of the most suitable transla-tion among the systems is a desirable feature.
