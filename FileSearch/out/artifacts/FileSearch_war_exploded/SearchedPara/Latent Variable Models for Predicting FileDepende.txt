
Diane J. Hu 1 , Laurens van der Maaten 1 , 2 , Youngmin Cho 1 , Lawrence K. Saul 1 , Sorin Lerner 1 As software systems grow in size and complexity, they become more difficult to develop and main-tain. Nowadays, it is not uncommon for a code base to contain source files in multiple programming languages, text documents with meta information, XML documents for web interfaces, and even platform-dependent versions of the same application. This complexity creates many challenges be-cause no single developer can be an expert in all things.
 One such challenge arises whenever a developer wishes to update one or more files in the code base. Often, seemingly localized changes will require many parts of the code base to be updated. Unfortunately, these dependencies can be difficult to detect. Let S denote a set of starter files that the developer wishes to modify, and let R denote the set of relevant files that require updating after modifying S . In a large system, where the developer cannot possibly be familiar with the entire code base, automated tools that can recommend files in R given starter files in S are extremely useful. A number of automated tools now make recommendations of this sort by mining the development history of the code base [1, 2]. Work in this area has been facilitated by code versioning systems, such as CVS or Subversion, which record the development histories of large software projects. In changes have been submitted to the code base within a short time interval. Statistical analyses of past transactions can reveal which files depend on each other and need to be modified together. In this paper, we explore the use of latent variable models (LVMs) for modeling the development history of large code bases. We consider a number of different models, including Bernoulli mixture models, exponential family PCA, restricted Boltzmann machines, and fully Bayesian approaches. In these models, the problem of recommending relevant files can be viewed as a problem in binary matrix completion. We present experimental results on the development histories of three large open-source systems: Mozilla Firefox, Eclipse Subversive, and Gimp. In all of these applications, we find that LVMs outperform the current leading method for mining development histories. Two broad classes of methods are used for identifying file dependencies in large code bases; one analyzes the semantic content of the code base while the other analyzes its development history. 2.1 Impact analysis The field of impact analysis [3] draws on tools from software engineering in order to identify the consequences of code modifications. Most approaches in this tradition attempt to identify program dependencies by inspecting and/or running the program itself. Such dependence-based techniques include transitive traversal of the call graph as well as static [4, 5, 6] and dynamic [7, 8] slicing techniques. These methods can identify many dependencies; however, they have trouble on cer-tain difficult cases such as cross-language dependencies (e.g., between a data configuration file and the code that uses it) and cross-program dependencies (e.g., between the front and back ends of a compiler). These difficulties have led researchers to explore the methods we consider next. 2.2 Mining of development histories Data-driven methods identify file dependencies in large software projects by analyzing their devel-opment histories. Two of the most widely recognized works in this area are by Ying et al. [1] and Zimmerman et al. [2]. Both groups use frequent itemset mining (FIM) [9], a general heuristic for identifying frequent patterns in large databases. The patterns extracted from development histories are just those sets of files that have been jointly modified at some point in the past; the frequent patterns are the patterns that have occurred at least  X  times. The parameter  X  is called the minimum support threshold . In practice, it is tuned to yield the best possible balance of precision and recall. Given a database and a minimum support threshold, the resulting set of frequent patterns is uniquely specified. Much work has been devoted to making FIM as fast and efficient as possible. Ying et al. [1] uses a FIM algorithm called FP-growth, which extracts frequent patterns by using a tree-like data structure that is cleverly designed to prune the number of possible patterns to be searched. FP-growth is used to find all frequent patterns that contain the set of starter files; the joint sets of these frequent patterns are then returned as recommendations. As a baseline in our experiments we use a variant of FP-growth called FP-Max [10] which outputs only maximal sets for added efficiency. Zimmerman et al. [2] uses the popular Apriori algorithm [11] (which uses FIM to solve a subtask) to form association rules from the development history. These rules are of the form x 1  X  x 2 , where x 1 and x 2 are disjoint sets; they indicate that  X  X f x 1 is observed, then based on experience, x 2 should also be observed. X  After identifying all rules in which starter files appear on the left hand side, their tool recommends all files that appear on the right hand side. They also work with content on a finer granularity, recommending not only relevant files, but also relevant code blocks within files. Both Ying et al. [1] and Zimmerman et al. [2] evaluate the data-driven approach by its f-measure , as measured against  X  X round-truth X  recommendations. For Ying et al. [1], these ground-truth recom-mendations are the files committed for a completed modification task, as recorded in that project X  X  Bugzilla. For Zimmerman et al. [2], the ground-truth recommendations are the files checked-in together at some point in the past, as revealed by the development history.
 Other researchers have also used the development history to detect file dependencies, but in markedly different ways. Shirabad et al. [12] formulate the problem as one of binary classifica-tion; they label pairs of source files as relevant or non-relevant based on their joint modification histories. Robillard [13] analyzes the topology of structural dependencies between files at the code-block level. Kagdi et al [14] improve on the accuracy of existing file recommendation methods by considering asymmetric file dependencies; this information is also used to return a partial or-dering over recommended files. Finally, Sherriff et al. [15] identify clusters of dependent files by performing singular value decomposition on the development history. We examine four latent variable models of file dependence in software systems. All these models represent the development history as an N  X  D large binary matrix, where non-zero elements in the same row indicate files that were checked-in together or jointly modified at some point in time. To detect dependent files, we infer the values of missing elements in this matrix from the values of known elements. The inferences are made from the probability distributions defined by each model. We use the following notation for all models: 3.1 Bernoulli mixture model The simplest model that we explore is a Bernoulli mixture model (BMM). Figure 1(a) shows the BMM X  X  graphical model in plate notation. In training, the observed variables are the D bi-bel z  X  X  1 , 2 ,...,k } that can be viewed as assigning each transaction vector to one of k clusters. The joint distribution of the BMM is given by: As implied by the graph in Fig. 1(a), we model the different elements of x as conditionally inde-pendent given the label z . Here, the parameter  X  z = p ( z |  X  ) denotes the prior probability of the latent variable z , while the parameter  X  iz = p ( x i = 1 | z,  X  ) denotes the conditional mean of the observed variable x i . We use the EM algorithm to estimate parameters that maximize the likelihood p ( D|  X  ,  X  ) = Q n p ( x n |  X  ,  X  ) of the transactions in the development history. When a software developer wishes to modify a set of starter files, she can query a trained BMM to identify a set of relevant files. Let s = { x i 1 ,...,x i s } denote the elements of the transaction vector indicating the files in the starter set S . Let r denote the D  X  s remaining elements of the files are relevant by computing the posterior probability p ( r | s = 1 ,  X  ,  X  ) . Using Bayes rule and conditional independence, this posterior probability is given (up to a constant factor) by: The most likely set of relevant files, according to the model, is given by the completed transaction r  X  that maximizes the right hand side of eq. (2). Unfortunately, while we can efficiently compute the maximize eq. (2) over all 2 D  X  s possible ways to complete the transaction. As an approximation, we Then we recommend all files whose posterior probabilities p ( x i = 1 | s = 1 ) exceed some threshold; we optimize the threshold on a held-out set of training examples. 3.2 Bayesian Bernoulli mixture model We also explore a Bayesian treatment of the BMM. In a Bayesian Bernoulli mixture (BBM), instead of learning point estimates of the parameters {  X  ,  X  } , we introduce a prior distribution p (  X  ,  X  ) and make predictions by averaging over the posterior distribution p (  X  ,  X  |D ) . The generative model for the BBM is shown graphically in Figure 1(b). Figure 1: Graphical model of the Bernoulli mixture model (BMM), the Bayesian Bernoulli mixture (BBM), the restricted Boltzmann machine (RBM), and logistic PCA.
 In our BBMs, the mixture weight parameters are drawn from a Dirichlet prior 1 : where k indicates (as before) the number of mixture components and  X  is a hyperparameter of the Dirichlet prior, the so-called concentration parameter 2 . Likewise, the parameters of the k Bernoulli distributions are drawn from Beta priors: where  X  j is a D -dimensional vector, and  X  and  X  are hyperparameters of the Beta prior. As exact inference in BBMs is intractable, we resort to collapsed Gibbs sampling and make pre-dictions by averaging over samples from the posterior. In particular, we integrate out the Bernoulli parameters  X  and the cluster distribution parameters  X  , and we sample the cluster assignment vari-ables z . For Gibbs sampling, we must compute the conditional probability p ( z n = j | z  X  n , D ) that the n th transaction is assigned to cluster j , given the training data D and all other cluster assign-ments z  X  n . This probability is given by: where N  X  nj counts the number of transactions assigned to cluster j (excluding the n th transaction) and N  X  nij counts the number of times that the i th file belongs to one of these N  X  nj transactions. After each full Gibbs sweep, we obtain a sample z ( t ) (and corresponding counts N ( t ) j of the number of points assigned to cluster j ), which can be used to infer the Bernoulli parameters  X  ( t ) j . We use T of these samples to estimate the probability that a file x i needs to be changed given files in the starter set S . In particular, averaging predictions over the T Gibbs samples, we estimate: p ( x i = 1 | s = 1 )  X  3.3 Restricted Boltzmann Machines A restricted Boltzmann machine (RBM) is a Markov random field (MRF) whose nodes are (typi-cally) binary random variables [17]. The graphical model of an RBM is a fully connected bipartite graph with D observed variables x i in one layer and k latent variables y j in the other; see Fig. 1(c). Due to the bipartite structure, the latent variables are conditionally independent given the observed variables (and vice versa). For the RBMs in this paper, we model the joint distribution as: where W stores the weight matrix between layers, b and c store (respectively) the biases on ob-served and hidden nodes, and Z is a normalization factor that depends on the model X  X  parameters. The product form of RBMs can model much sharper distributions over the observed variables than mixture models [17], making them an interesting alternative to consider for our application. RBMs are trained by maximum likelihood estimation. Exact inference in RBMs is intractable due to the exponential sum in the normalization factor Z . However, the conditional distributions required for Gibbs sampling have a particularly simple form: where  X  ( z ) = [1 + e  X  z ]  X  1 is the sigmoid function. The obtained Gibbs samples can be used to approximate the gradient of the likelihood function with respect to the model parameters; see [17, 18] for further discussion of sampling strategies 3 .
 To determine whether a file f i is relevant given starter files in S , we can either (i) clamp the ob-served variables representing starter files and perform Gibbs sampling on the rest, or (ii) compute the posterior over the remaining files using a fast, factorized approximation [19]. In preliminary experiments, we found the latter to work best. Hence, we recommend files by computing then thresholding these probabilities on some value determined on held-out examples. 3.4 Logistic PCA Logistic PCA is a method for dimensionality reduction of binary data; see Fig. 1(d) for its graph-ical model. Logistic PCA belongs to a family of algorithms known as exponential family PCA; these algorithms generalize PCA to data modeled by non-Gaussian distributions of the exponential family [20, 21, 22]. To use logistic PCA, we stack the N transaction vectors x n  X  X  0 , 1 } D of the development history into a N  X  D binary matrix X . Then, modeling each element of this matrix as a Bernoulli random variable, we attempt to find a low-rank factorization of the N  X  D real-valued matrix  X  whose elements are the log-odds parameters of these random variables.
 The low-rank factorization in logistic PCA is computed by maximizing the log-likelihood of the observed data X . In terms of the log-odds matrix  X  , this log-likelihood is given by: We obtain a low dimensional representation of the data by factoring the log-odds matrix  X   X &lt; N  X  D as the product of two smaller matrices U  X &lt; N  X  L and V  X &lt; L  X  D . Specifically, we have: Note that the reduced rank L D plays a role analogous to the number of clusters k in BMMs. After obtaining a low-rank factorization of the log-odds matrix  X  = UV , we can use it to rec-ommend relevant files from starter files S = { f i 1 ,f i 2 ,...,f i s } . To recommend relevant files, we compute the vector u that optimizes the regularized log-loss: Table 1: Datasets statistics, showing the time period from which transactions were extracted, and the number of transactions and unique files in the training and test sets (for a single starter file). where in the first term, v ` denotes the ` th column of the matrix V , and in the second term,  X  is a regularization parameter. The vector u obtained in this way is the low dimensional representation of the transaction with starter files in S . To determine whether file f i is relevant, we compute the probability p ( x i = 1 | u , V ) =  X  ( u  X  v i ) and recommend the file if this probability exceeds some threshold. (We tune the threshold on held-out transactions from the development history). We evaluated our models on three datasets 4 constructed from check-in records of Mozilla Firefox, Eclipse Subversive, and Gimp. These open-source projects use software configuration management (SCM) tools which provide logs that allow us to extract binary vectors indicating which files were changed during a transaction. Our experimental setup and results are described below. 4.1 Experimental setup We preprocess the raw data obtained from SCM X  X  check-in records in two steps. First, follow-ing Ying et al [1], we eliminate all transactions consisting of more than 100 files (as these usually do not correspond to meaningful changes). Second, we simulate the minimum support threshold (see Section 2.2) by removing all files in the code base that occur very infrequently. This pruning allows us to make a fair comparison with latent variable models (LVMs).
 After pre-processing, the dataset is chronologically ordered; the first two-thirds is used as training data, and the last one-third as testing data. For each transaction in the test set, we formed a  X  X uery X  and  X  X abel X  set by randomly picking a set of changed files as starter files. The remaining files that were changed in the transaction form the label set, which is the set of files our models must predict. Following [1], we only include transactions for which the label set is non-empty in the train data. Table 1 shows the number of transactions for training and test set, as well as the total number of unique files that appear in these transactions.
 We trained the LVMs as follows. The Bernoulli mixture models (BMMs) were trained by 100 or fewer iterations of the EM algorithm. For the Bayesian mixtures (BBMs), we ran 30 separate Markov chains and made predictions after 30 full Gibbs sweeps 5 . The RBMs were trained for 300 iterations of contrastive divergence (CD), starting with CD-1 and gradually increasing the number of Gibbs sweeps to CD-9 [17]. The parameters U and V of logistic PCA were learned using an alternating least squares procedure [21] that converges to a local maximum of the log-likelihood. We initialized the matrices U and V from an SVD of the matrix X .
 The parameters of the LVMs (i.e., number of hidden components in the BMM and RBM, as well as the number of dimensions and the regularization parameter  X  in logistic PCA) were selected based on the performance on a small held-out validation set. The hyperparameters of the Bayesian Bernoulli mixtures were set based on prior knowledge from the domain: the Beta-prior parameters  X  and  X  were set to 0 . 005 and 0 . 95 , respectively, to reflect our prior knowledge that most files are not changed in a transaction. The concentration parameter  X  was set to 50 to reflect our prior knowledge that file dependencies typically form a large number of small clusters. Table 2: Performance of FIM and LVMs on three datasets for queries with 1 or 3 starter files. Each shaded column presents the f -measure, and each white column presents the correct prediction ratio. 4.2 Results Our experiments evaluated the performance of each LVM, as well as a highly efficient implemen-tation of FIM called FP-Max [10]. Several experiments were run on different values of starter files (abbreviated  X  X tart X ) and minimum support thresholds (abbreviated  X  X upport X ). Table 2 shows the comparison of each model in terms of the f -measure (the harmonic mean of the precision and re-call) and the  X  X orrect prediction ratio, X  or CPR (the fraction of files we predict correctly, assuming that the number of files to be predicted is given). The latter measure reflects how well our models identify relevant files for a particular starter file, without the added complication of thresholding. Experiments that achieve the highest result for each of the two measures are boldfaced. From our results, we see that most LVMs outperform the popular FIM approach. In particular, the BBMs outperform all other approaches on two of the three datasets, with a high of CPR = 79% in Eclipse Subversive. This means that an average of 79% of all dependent files are detected as relevant by the BBM. We also observe that f -measure generally decreases with the addition of starter files  X  since the average size of transactions is relatively small (around four files for Firefox), adding starter files must make predictions less obvious in the case that the total number of relevant files is not given to us. Increasing support, on the other hand, seems to effectively remove noise caused by infrequent files. Finally, we see that recommendations are most accurate on Eclipse Subversive, the smallest dataset. We believe this is because a smaller test set does not require a model to predict as far into the future as a larger one. Thus, our results suggest that an online learning algorithm may further increase accuracy. The use of LVMs has significant advantages over traditional approaches to impact analysis (see Section 2), namely its ability to find dependent files written in different languages. To show this, we present the three clusters with the highest weights, as discovered by a BMM in the Firefox data, in Table 3. The table reveals that the clusters correspond to interpretable structure in the code that span multiple data formats and languages. The first cluster deals with the JIT compiler for JavaScript, while the second and third deal with the CSS style sheet manager and web browser properties. The dependencies in the last two clusters would have been missed by conventional impact analysis. Table 3: Three of the clusters from Firefox, identified by the BMM. We show the clusters with the largest mixing proportion. Within each cluster, the 10 files with highest membership probabil-ities are shown; note how these files span multiple data formats and program languages, revealing dependencies that would escape the notice of traditional methods.
 LVMs also have important advantages over FIM. Given a set S of starter files, FIM simply looks at co-occurrence data; it recommends a set of files R for which the number of transactions that contain both R and S is frequent . By contrast, LVMs can exploit higher-order information by discovering the underlying structure of the data. Our results suggest that the ability to leverage such structure leads to better predictions. Admittedly, in terms of computation, LVMs have a larger one-time training cost than the FIM, as we must first train the model or generate and store the Gibbs samples. However, for a single query, the time required to compute recommendations is comparable to that of the FP-Max algorithm we used for FIM.
 The results from the previous section also revealed significant differences between the LVMs we considered. In the majority of our experiments, mixture models (with many mixture components) appear to outperform RBMs and logistic PCA. This result suggests that our dataset consists of a large number of transactions with a number of small, highly interrelated files. Modeling such data with a product of experts such as an RBM is difficult as each individual expert has the ability to  X  X eto X  a prediction. We tried to resolve this problem by using a sparsity prior on the states of the hidden units y to make the RBMs behave more like a mixture model [23], but in preliminary experiments, we did not find this to improve the performance. Another interesting observation is that the Bayesian treatment of the Bernoulli mixture model generally leads to better predictions than a maximum likelihood approach, as it is less susceptible to overfitting. This advantage is particularly useful in file dependency prediction which requires models with a large number of mixture components to appropriately model data that consists of many small, distinct clusters while having few training instances (i.e., transactions). In this paper, we have described a new application of binary matrix completion for predicting file dependencies in software projects. For this application, we investigated the performance of four different LVMs and compared our results to that of the widely used of FIM. Our results indicate that LVMs can significantly outperform FIM by exploiting latent, higher-order structure in the data. Admittedly, our present study is still limited in scope, and it is very likely that our results can be further improved. For instance, results from the Netflix competition have shown that blending the predictions from various models often leads to better performance [24]. The raw transactions also contain additional information that could be harvested to make more accurate predictions. Such information includes the identity of users who committed transactions to the code base, as well as the text of actual changes to the source code. It remains a grand challenge to incorporate all the available information from development histories into a probabilistic model for predicting which files need to be modified. In future work, we aim to explore discriminative methods for parameter estimation, as well as online algorithms for tracking non-stationary trends in the code base. Acknowledgments LvdM acknowledges support by the Netherlands Organisation for Scientific Research (grant no. 680.50.0908) and by EU-FP7 NoE on Social Signal Processing (SSPNet).
