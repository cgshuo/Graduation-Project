 Search systems have for some time provided users with the ability to request documents similar to a given document. Interfaces provide this feature via a link or button for each document in the search results. We call this feature find-similar or similarity browsing . We examined find-similar as a search tool, like relevance feedback, for improving retrieval performance. Our investigation focused on find-similar X  X  document-to-document similarity, the reexamination of doc-uments during a search, and the user X  X  browsing pattern. Find-similar with a query-biased similarity, avoiding the re-examination of documents, and a breadth-like browsing pat-tern achieved a 23% increase in the arithmetic mean average precision and a 66% increase in the geometric mean aver-age precision over our baseline retrieval. This performance matched that of a more traditionally styled iterative rele-vance feedback technique.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Algorithms, Experimentation Find-Similar, Similarity Browsing, Relevance Feedback
Relevancefeedback is nota widely adopted feature of pop-ular search services even though it is known to be a powerful tool for improving retrieval performance. The feedback-like feature that has been adopted by some services is a feature we term find-similar or similarity browsing . Find-similar allows a user to request documents that are similar to a particular document. For example, the Excite web search Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. engine once provided this feature by adding a link to each result that read  X  X ore Like This: Click here for a list of documents like this one X  X 31]. Today, Google X  X  web search engine provides a find-similar link for each item in the search results. The U.S. National Library of Medicine X  X  PubMed search system offers find-similar as a link to  X  X elated Arti-cles X  for each search result [26].

In our experiments, the starting point for the use of find-similar is always a results list produced by a query. Find-similar can be applied to any document listed in the re-sults list. Like the above examples, typical instantiations of find-similar show a button or link next to each document in the results list. Clicking on find-similar produces a new re-sults list of documents similar to the selected document. To browse acollection of documentsbysimilarity, auser can use find-similar to jump from list to list of similar documents.
This paper focuses on find-similar X  X  use as a search tool rather than as a browsing interface. To use find-similar as a search tool, a user will apply find-similar to a relevant docu-ment to find more relevant documents, and so forth. While feedback-like, the version of find-similar that we study has no notion of relevance. Each use of find-similar on a docu-ment is separate from uses on other documents. While the potential exists for some form of implicit relevance feedback, we examine find-similar merely as a tool that returns similar documents.

In contrast to find-similar, the traditional research inter-face to support relevance feedback involves showing the user the top 5 or 10 results for the query and asking the user to judge the relevance of these results. The user X  X  feedback is then used to update the query and a new ranked list is produced. Typically the already judged documents are not shown in the new results list. Repeated use of this rele-vance feedback iterates through the collection of documents, and we call this iterative relevance feedback . In both batch and user experiments, iterative relevance feedback has been shown to be an effective means for improving search results [27, 11, 8, 15].

It appears that some users attempt to use relevance feed-back systems designed for judgments on multiple documents in amannerresembling find-similar. Croft reports thatusers will often use a single document, which may be unrelated to the query, for relevance feedback and effectively be  X  X rows-ing using feedback X  [7]. Hancock-Beaulieu et al. studied 58 user sessions that used interactive query expansion (IQE) via a relevance feedback interface [10]. Of the 58 sessions, 17 used only a single document for feedback. Algorithms de-signed for multiple-document feedback may not always work well for single document feedback. Only 3 of the 17 sessions were successful at finding additional relevant material.
We ran all of our experiments in a batch style without user involvement. We agree with White et al. [40] that simulation studies can find better ways to implement the algorithms behind interface features before investing in user studies. Simulation studies don X  X  replace user studies.

We usedtwo browsing patterns toevaluate various aspects of find-similar: a greedy depth-first pattern and a breadth-like pattern. At best, these browsing patterns are crude models of user behavior, but our primary use of the patterns is to demonstrate find-similar X  X  possible effect on retrieval.
Using these browsing patterns, we examined find-similar X  X  potential as a search tool to improve document retrieval as compared to iterative relevance feedback. An important as-pect of our investigation focused on find-similar X  X  document-to-document similarity. We also looked at the cost of hav-ing to reexamine documents while using find-similar. The browsing patterns themselves give insight into how much a user X  X  browsing pattern can affect performance.
Wilbur and Coffee studied several aspects of find-similar [41]. They found that on average, a single relevant docu-ment used as a query does not perform as well as the origi-nal query, but that relevant documents similar to the query will do better than the query. They also used a set of brows-ing patterns and found that a method they called paral lel neighborhood searching performed betterthan theother pat-terns. This method attempts to search the find-similar lists of all discovered relevant documents tothe same depth. This browsing pattern is likely too complex for a user to follow. They suggested that a system could hide the complexity by showing the user one document at a time to judge, but such a system no longer supports similarity browsing or tradi-tional lists of results.

Spink et al. have analyzed samples of Excite X  X  query logs and reported that between 5 and 9.7 percent of the queries came from the use of the  X  X ore like this X  find-similar feature [31, 32]. There is little evidence that users repeatedly used the find-similar feature to browse by feedback. Most web users look at very few results [32]. Thus, it is not surprising that find-similar found limited use by web users who are likely precision oriented.
 Othersystemsofferingsimilarity browsing includethoseof Thompson and Croft [34] and Campbell [4]. Both systems draw a display that allows the user to browse to similar documents. One of the functions of such a display is as a map to prevent users from becoming lost in their browsing. Sucha display may beof use tointerfaces incorporating find-similar, but find-similar does not require such a display.
One possible reason for search systems X  adoption of find-similar is its appearance as an easily understood and simple to use form of relevance feedback. Of the large body of relevance feedback research [28], Aalbersberg X  X  incremental feedback is an illustrative example of simplifying relevance feedback [1]. With incremental feedback, the user is shown one result at a time. To see more results, the user must judge the relevance of the presented item. In batch experi-ments, Aalbersberg found that incremental feedback worked better than Rocchio, Ide Regular, and Ide Dec-Hi. For these other approaches, Aalbersberg used an iteration size of 15 documents. While incremental feedback builds a model of relevant documents one document at a time, each use of find-similar involves a single document without any accu-mulation of documents or model of relevance.

In many systems, users can browse documents via hyper-links. If a collection lacks hyperlinks, they can be automati-cally generated [2]. Find-similar effectively adds a hyperlink from a document to those most similar to it. For hypertext systems like the web, researchers have created programs to assist the user with finding relevant pages via browsing [20, 24]. In contrast to these approaches, find-similar does not observe the user to determine what the user considers rele-vant, and find-similar does not offer any assistance in choos-ing where to browse.

Another set of research has focused on helping the user better process ranked retrieval results. This work is related to but different from relevance feedback and find-similar, both of which are applied to the entire collection of docu-ments and not restricted to the set of top ranked results. For example, Leuski [19] created a software agent to guide users in their exploration of the top results. Other approaches involve presenting the results grouped by an online cluster-ing of the results or by predetermined categories [12, 9, 13, 5]. These approaches are different from find-similar in that while the user gets to see documents grouped by similarity, the user does not get to request more documents similar to adocument.
We first describe in section 2.1 how we retrieved docu-ments for find-similar, the baseline, and an implementation of iterative relevance feedback. We then explain in section 2.2 how we created a query model for a document to which a user has applied find-similar. In sections 2.3 and 2.4 we discuss our hypothetical user interfaces and two browsing patterns used for evaluation of find-similar. We finish by describing the test collection and the evaluation methodol-ogy in sections 2.5 and 2.6.
We used both the language modeling approach to infor-mation retrieval [25] and its combination with the inference network approach [21] as implemented in the Lemur [18] and Indri [33] retrieval systems.

Language modeling represents documents and queries as probabilistic models. We used multinomials as our proba-bilistic models of text. For a given piece of text T ,wewrite the probability of the word w given the model M T of the text as P ( w | M T ).

The maximum likelihood estimated (MLE) model of text estimates the probability of a word as the count of that word divided by the total number of words in the text. As such, the probability of a word w given a text T is: P ( w | M T T ( w ) / | T | ,where T ( w ) is the count of word w in the text and | T | = Forfind-similar, we rankeddocumentsusingtheKullback-Leibler divergence of the query model M Q with the docu-ment model M D : where 0log0 = 0, and the query model is a model of the document to which find-similar is being applied. We detail #weight( 0.8 #combine( international organized crime ) 0.1 #combine( 0.1 #combine( Figure 1: TREC topic 301,  X  X nternational organized crime, X  converted to an Indri query by Metzler and Croft X  X  dependence models. This query gives a weight of 0.8 to the unigram model of the topic. The ordered phrases, #1 , have a weight of 0.1 as well as the unordered windows, #uwN .Notshownhereisthe unigram relevance model that provides a pseudo-relevance feedback component when combined with the dependence model query for our baseline run. the two ways we constructed query models for find-similar in section 2.2.

To avoid zero probabilities and better estimate the doc-ument models, we calculated the document models using where P ( w | C ) is the MLE model of the collection, and the Dirichlet prior smoothing parameter.

The inference network approach by Metzler and Croft [21] takes the probability estimates from language modeling and uses them as part of the Bayesian inference network model of Turtle and Croft [37]. The inference network provides a formal method for combination of evidence, and is easily accessed by users via a structured query language.
For our baseline, we used Metzler et al. X  X  method [23] that combines Metzler and Croft X  X  [22] dependence models with Lavrenko and Croft X  X  [17] relevance models. This method can be seen as using a precision enhancing retrieval method, dependence models, with a pseudo-relevance feedback tech-nique, relevance models. Unlike Metzler et al., we used only the existing collection for query expansion with relevance models and did not use any external collections for expan-sion.

The dependence model uses the Indri query language to combine three types of evidence. The first is the standard bag-of-words unigram model as used by language model-ing. The second type captures the sequential ordering of the terms in the query. The third uses the close proxim-ity of query terms as evidence. Figure 1 shows the Indri query produced by Metzler and Croft X  X  dependence models for TREC topic 301,  X  X nternational organized crime. X 
To perform the baseline retrieval, first the dependence model Q of the query is run. Then a relevance model is cre-ated from the top k ranked documents. The relevance model M R is calculated as: P ( w | M R )= where P ( D i | Q )= P ( Q | D i ) / is the Indri belief that document model D i is relevant to the query Q . Finally, the dependence model and the relevance model are combined to create the final baseline query using Indri X  X  #weight operator.
 Parameter Value Dirichlet smoothing for unigram terms, m 1500
Dirichlet smoothing for ordered and unordered windows, m Weight of unigram model in dependence model 0.8
Weight of ordered windows model in dependence model
Weight of unordered windows model in depen-dence model
Number of pseudo feedback documents for rele-vance model
Weight of dependence model when mixed with pseudo relevance model Max. terms in pseudo feedback relevance model 25 Max. terms in find-similar document models 50 Max. terms in iterative feedback relevance model 50
Weight of initial query when mixed with iterative feedback relevance model
The baseline is also used as the initial retrieval for both find-similar and iterative relevance feedback.

Our implementation of iterative relevance feedback is akin to that used by Rocchio [27]. We mix in a model of the relevant documents with the original baseline query model using Indri X  X  #weight operator. We tried weights of 0.0, 0.3, 0.5, and 0.7 for the original query and found 0.3 to work best. The model of relevant documents is calculated as: P ( w | M R )= 1 k of documents the user has judged to be relevant. An alter-native is for us to replace the pseudo feedback component of the baseline query model with the real relevance model as provided by the user X  X  judgments, but we have not yet investigated this variant.

We used the same parameter settings that Metzler et al. derived from training on the TREC 2004 Robust track data and that they used for the 2005 Robust track [23]. The 2004 Robust track includes the same 150 topics we used for evaluation (topics 301-450) in its 250 topics. Table 1 shows the retrieval parameters X  settings for all runs. We used the same smoothing parameters for all experiments.
An obvious way to implement find-similar for documents is to treat the document as a very long query. A problem with thisapproach is that each document will often be about several topics of which only one is the user X  X  search topic. A document may well be about  X  X rganized crime X  but it may also be about the prosecution of criminals. Not all sto-ries about criminal prosecution are about organized crime. Rather than finding documents that are similar to all the topics mentioned in a story, we think a user will want to find documents that are similar with respect to the current search topic.

Weexamined two typesof similarity for find-similar: regu-lar and query-biased . Regularsimilarity treatsthedocument as a query to find other similar documents. Query-biased similarity aims to find similar documents given the context of the user X  X  search and avoid extraneous topics. For both regular and query-biased similarity, we construct a unigram model of the find-similar document that is then used as a query to find similar documents (see equation 1). Regular similarity uses the maximum likelihood estimated (MLE) model of the document as the query. For query-biased sim-ilarity, we create a MLE model of the document text that consists of all words within a certain distance W of all query terms in the document. For our experiments, we set W to 5. Thus the 5 preceding words, the query term, and the 5 words following a query term are used. Should a docu-ment not contain any query terms, the whole document is used. Forboth typesof similarity, we truncatethedocument model to the 50 most probable terms.

Ournotionofquery-biasedsimilarity is more akintoquery-biased summaries [36, 29] than to query-biased clustering [9, 13] or query sensitive similarity [35]. The nature of query-biased summaries is to extract the sentences or text surrounding query terms in a document and use this ex-tracted text as a summary of the document. In contrast to query-biased summaries, both Eguchi [9] and Iwayama [13] increase the weight of query terms in the documents before clustering. Tombros X  query sensitive similarity modifies the cosine similarity measure to place more weight on the query terms [35]. Preliminary experiments where we linearly com-bined the query model with the document model as a form of query-biasing showed poorer performance. We hypothe-size that this poorer performance was the result of a lack of diversity in the find-similar lists.
We ran all of our experiments in a batch style without user involvement. Assumptions about the interface affect the batch evaluation of retrieval features. In particular, we only consider browsing patterns that could be reasonably executed by a user with our hypothetical user interface. We nextdescribeourhypotheticaluserinterfacesfor find-similar and iterative relevance feedback.

The find-similar interface we envision is similar to the web-based PubMed search system [26]. Our hypothetical interface has  X  X ack button X  support like a web browser. If a user has performed find-similar on a document, the user can decide to stop examining the documents presented as sim-ilar to that document and hit the back button. The back button returns the user to the previous list at the position in the list where the user had applied find-similar.
Results are presented in rank order with query-biased summaries for both the initial query and the find-similar lists. Sanderson [29] demonstrated that users are able to judge the relevance of documents from query-biased sum-maries with 75% of the accuracy of full documents. Thus, we assume users will examine most documents by reading the already visible summaries. Reading a summary requires no manipulation of the interface and therefore provides no feedback to the system that the document has been exam-ined. When a user applies find-similar to a document, the user will be presented with a new page listing the similar documents. The find-similar lists will contain some docu-ments thatthe userhas already examined on previous pages. The user will have to reexamine documents unless there is a visual marker to designate already examined documents.
In our evaluation, we compared two conceptual variations of our imagined find-similar interface. In one variation non-relevant documents arereexamined and in theother theyare not. Both variations prevent the reexamination of relevant documents.

Thehypotheticaliterativerelevancefeedbackinterfacedis-plays the top N documents of the ranked results. The user judges each of the displayed documents and then submits the feedback to retrieve the next N documents. In our ex-periments, we set the iteration size N to 10. The previously displayed documents are not shown again for the current topic. This process repeats until 1000 documents have been examined. This interface does not provide for use of a back button like find-similar. The system only allows forward iteration.
Kl  X  ockner et al. [14] used an eye tracker to observe how people processed search results. They used a Google results list containing 25 results for a query. The subjects X  task was to find the relevant documents in the list. Subjects could click on a result tosee the result X  X  web page. Ofthe subjects, 65% followed a depth-first strategy. These users examined the documents in order, from highest to lowest rank, and did not look ahead. Another 15% used a breadth-first strategy by looking at the entire results list before opening a single web page. The remaining 20% used a partial breadth-first strategy by sometimes looking ahead at a few results before opening a web page.

Given the user behavior observed by Kl  X  ockner et al., we used two browsing patterns to evaluate find-similar. The greedy pattern represents the depth-first behavior, and the breadth-like pattern aims to capture the breadth-first search behaviors. Neither pattern is a true depth-first or breadth-first search pattern. A true depth-first pattern does not reflect that a user is likely to stop examining a results list if no relevant documents are found. A true breadth-first pat-tern is not feasible for a user to implement. While inspired by the user behavior observed by Kl  X  ockner et al., these pat-terns are at best crude models of user behavior. Users could execute these patterns, but we have little knowledge of how users actually search with find-similar. Instead, these pat-terns give us insight into the potential of find-similar and the degree to which find-similar X  X  performance can be af-fected by different browsing patterns. Both patterns use the baseline as the initial retrieval.

The greedy browser examines documents in the order that they appear in a results list. As section 2.3 explained, the browser will only examine a relevant document once. When a relevant document is examined, the greedy browser per-forms a find-similar operation on this document. The greedy browser ceases to examine documents in a results list after examining 5 contiguous non-relevant documents. When the browser stops examining a list, the browser hits the  X  X ack button X  and returns to the previous list and continues ex-amining documents in that list. If the browser is examining the initially retrieved list of documents, the only stopping criterion is that the browser stops when 1000 documents have been examined.

The breadth-like browser also examines documents in the order that they appear in a results list. What differs from the greedy pattern is that the breadth-like browser only be-gins to browse via find-similar when the results list X  X  quality becomes too poor. As the breadth-like browser examines relevant documents, it places these documents in a first-in first-out queue local to the current list. When the precision at
N ,where N is the rank of the current document, drops below 0 . 5 or when 5 contiguous non-relevant documents are encountered, the browser applies find-similar to the first rel-evant document in the queue. When the browser returns to the current list, it applies find-similar to the next document in the queue until the queue is empty. The browser never uses find-similar on a relevant document more than once. Thus documents in the queue will be ignored if the browser has already performed find-similar on them. There is not any notion that the breadth-like browser knows which rele-vant documents are the best for find-similar. The breadth-like browser merely delays exploration until the current list seems to have gone  X  X old. X  The browser stops examining a results list in the same manner and with the same crite-ria, i.e. 5 contiguous non-relevant documents, as the greedy browser.

Early experiments with a greedy browsing pattern influ-enced our design of the breadth-like browser. We saw that the greedy browser could degrade the performance of an al-ready good retrieval. Thus, the breadth-like browser uses list quality as its criteria for delaying use of find-similar. While the breadth-like browsing pattern could be seen as a  X  X orrected X  greedy pattern, we feel that it does capture the goal of a breadth-first user, that is, to look ahead before acting.
The topics used for the experiments consisted of TREC topics 301-450, which are the ad-hoc topics for TREC 6, 7, and 8. TREC topics consist of a short title, a sentence length description, and a paragraph sized narrative. The titles best approximate a short keyword query, and we used them as our queries.
 We used TREC volumes 4 and 5 minus the Congressional Record for our collection. This 1.85 GB, heterogeneous collection contains 528,155 documents from the Financial Times Limited, the Federal Register, the Foreign Broadcast Information Service, and the Los Angeles Times.

We used the Lemur toolkit [18] for all of our experiments including its Indri subsystem [33]. In particular, we gener-ated theresults for thefind-similar runsusing a Lemurindex of the collection with stop words removed at index time. For the baseline and iterative relevance feedback runs we used an Indri index with stop word removal at query time. We stemmed all words with the Krovetz stemmer [16]. We used an in-house stopword list of 418 noise words.
We constructed our runs X  results lists for evaluation in the same manner as Aalbersberg [1]. The results lists that we evaluated represent the order in which the simulated user examines the documents. For the baseline retrieval, the documents are examined in rank order. For find-similar, the browsing patterns of section 2.4 determine the order in which documentsare examined. For iterative relevance feed-back, documents are examined in the same manner they are judged  X  one iteration of 10 documents at a time.
All relevance judgments are made using the  X  X rue X  rel-evance judgments per NIST. We treat a reexamined non-relevant document the same as any other non-relevant docu-mentfoundatthatposition intheresults. Alloftheretrieval techniques we studied do not reexamine relevant documents.
We report metrics using both the arithmetic mean and the geometric mean. The TREC Robust track has estab-lished the geometric mean as a useful tool for analyzing performance [38]. As opposed to the usual arithmetic mean, the geometric mean emphasizes the lower performing topics. The arithmetic mean can hide large changes in performance on poorly performing topics with small changes in the bet-ter performing topics. As with the 2005 TREC Robust track [39], for computing the geometric mean, we set values less than 0.00001 to 0.00001 to avoid zeros. We used version 8 of trec eval [3] to compute per topic metrics. We measured statistical significance with a two-sided, paired, randomiza-tion test with 100 , 000 samples (see page 168 [6]). Unless otherwise stated, significance is at the p&lt; 0 . 05 level.
Table 2 shows the arithmetic mean, non-interpolated, av-erage precision (AMAP) and the geometric mean (GMAP) across the 150 topics of TREC 6, 7, and 8, for the base-line, find-similar, and iterative relevance feedback runs. The find-similar runs vary based on whether or not non-relevant documents were reexamined (section 2.3), whether a greedy or breadth-like browsing pattern was used (section 2.4), and whether the similarity was regular or query-biased (section 2.2).

In general, find-similar and iterative relevance feedback are better able to improve on a poor initial retrieval than on a good initial retrieval. To highlight this behavior, Table 2 also reports results for the 150 topics divided into three sets of 50 topics. The topics are ordered by their perfor-mance on the baseline and then divided into three sets (like quartiles except into thirds instead of quarters). These sets are roughly equivalent to poor, fair, and good retrieval per-formance with baseline AMAPs of 0 . 036, 0 . 202, and 0 . respectively. With the topics divided up in this manner, the geometric mean adds little insight and we report only the arithmetic mean of each topic set.

The average precision results are based on theTRECstan-dard of 1000 results. To understand the performance when a user examines fewer documents, Table 3 shows the precision at 20 and 100 documents. Feedback techniques can increase recall as well as precision. Table 4 shows the recall at 1000 documents.
The best find-similar run avoids reexamining non-relevant documents, follows a breadth-like browsing pattern, and uses query-biased similarity. Table 2 shows that this run matches the performance of our implementation of iterative relevance feedback and achieves a 23% improvement in the arithmetic mean average precision (AMAP) and a 66% im-provementinthe geometric meanaverage precision (GMAP) over the baseline. Iterative relevance feedback achieves a 69% improvement in GMAP, but this is not a statistically significant difference compared to the best find-similar run.
The use of a high quality baseline retrieval is required to avoid overstating the performance gains possible with a retrieval technique. We used the method developed by Metzler et al. [23] for our baseline (see section 2.1). This method had the best title run as measured by mean average precision and had the second best geometric mean average precision for both title and description runs submitted to TREC X  X  2005 Robust track [39]. We achieved larger relative performance improvements during initial experiments with a weaker baseline. . 169 0 . 160 0 . 193 0 . 197 0 . 216 0 . 220 . 101 0 . 108 0 . 119 0 . 114 0 . 134 0 . 129 . 509 0 . 346 0 . 461 0.544 0.560 0 . 580 . 137 0.129 0 . 152 0 . 145 0 . 163 0 . 162 are different from the baseline at a statistically significant level (p paired, randomization test with 100,000 samples. . 746 0 . 806 0 . 809 0 . 808 0 . 811 0 . 823 . 700 0 . 763 0 . 765 0 . 764 0 . 767 0 . 779 different from the baseline at a statistically significant level (p randomization test with 100,000 samples.
We also tested iterative relevance feedback with an it-eration size of 1, which is Aalbersberg X  X  incremental feed-back [1]. An iteration size of 1 performed as well as an iteration size of 10, with the larger iteration size yielding a negligibly larger AMAP (0.322 vs. 0.321).

All the find-similar runs that avoid reexamination of non-relevant documents perform better than the corresponding runs that do reexamine non-relevant documents. An inter-face that supports find-similar may need to provide a mech-anism to help the user avoid reexamination of non-relevant documents. If a user has to keep track of judgments, it would seem that find-similar and traditional multiple item relevance feedback should be able to co-exist in the same retrieval system.

While both the greedy and breadth-like browsing patterns show significant improvements in GMAP over the baseline, following a breadth-like browsing pattern is superior to the greedy browsing pattern. Table 2 shows that the greedy browsing pattern in particular has difficulty with the bet-ter performing topics. As section 2.4 noted, the work by Kl  X  ockner et al. [14] motivated the two browsing patterns we used, but the performance of the greedy pattern influenced our design of the breadth-like browser. A user that follows a greedy browsing pattern will be harmed by the find-similar feature onbetter performing topics. The breadth-likebrows-ing pattern avoids using find-similar while the retrieval qual-ity of a list is high. We leave for future work the question of whether find-similar can be used to improve an already high quality retrieval.

Query-biased similarity shows consistently better perfor-mance than regular similarity. The query-biased similarity helps the greedy browsing pattern perform over 20% bet-ter than with regular similarity as measured by AMAP and GMAP on all 150 topics. Query-biased similarity also helps the breadth-like browser but to a lesser degree.
Given a search topic, a perfect document-to-document similarity method for find-similar makes the topic X  X  relevant documents most similar to each other. We can character-ize this notion of relevant documents being more similar to each other by measuring the distance from all relevant doc-uments to all other relevant documents. For each topic, we constructed a directed graph as follows. Each relevant doc-ument is a node in the graph. There is an edge from each node to each other node. The weight of an out edge is the rank of the target document in the ranked list produced by applying find-similar to the source document. Given this graph, we compute the all pairs shortest paths (APSP) to obtain the shortest distance from every node to every other node. This distanceis thenumberof documentsthatneed to be examined to navigate from one relevant document to an-other using find-similar. We used the Boost Graph Library X  X  implementation of the Floyd-Warshall APSP algorithm [30]. The distribution of distances is highly skewed. The distance to some documents is so large that they are  X  X ut of reach X  via find-similar. A single topic can greatly skew the average, too. Therefore we use the median rather than the mean to handle these skewed distributions. The median of the me-dian distances is 70.8 for regular and 33.0 for query-biased similarity. It appears that query-biased similarity creates a tighter grouping of relevant documents than does regular similarity. This result, that query-biased similarity better clusters relevant documents, echoes the results of Tombros X  work on query sensitive similarity [35].

The find-similar and feedback runs show a much greater improvement in GMAP than in AMAP. Table 2 highlights this difference and shows that the majority of the improve-ment comes from improving the poorer performing topics. Forthepoorest performing topics, thebaseline has anAMAP of 0.036, and on average, 1 document in 28 is relevant. On these same topics, find-similar raises this ratio to 1 in 7 with an AMAP of 0.134. Besides having a large relative per-formance improvement for poorly performing topics, find-similar can provide performance gains that should be no-ticeable by the end user.

Being able to improve precision early in a ranked list may influence user adoption of a retrieval tool such as find-similar. Table 3 shows that find-similar can achieve im-provements over the baseline in precision at 20 and 100 doc-uments. While not shown, the best find-similar run also ob-tained a statistically significant 7% increase in P@10 (arith-metic mean) over the baseline.

For find-similar X  X  best run, its P@100 arithmetic mean improvement of 22% is comparable to its AMAP improve-ment of 23%. For this same run, the P@100 geometric mean improvement of 34% is nearly half that of the 66% improve-ment in GMAP. A fair amount of the GMAP performance may come from improving very poorly performing topics with feedback on low ranking relevant documents. For some poor performing topics, if users are unwilling to dig deep into the ranked results, they may be unable to use feedback to help their search.

Table 4 shows that all of the find-similar runs increase recall at 1000 documents and the best performance is com-parable to iterative relevance feedback. Retrieval techniques that cluster or reorder the top N results cannot increase the recall at N [13, 19]. Interestingly, the different similarity and browsing types do not significantly impact recall at 1000 documents.
We found that find-similar, as a feedback-like search tool, has the potential to improve document retrieval. The best performance improvement attained by find-similar matched that of an implementation of iterative relevance feedback. Find-similar achieved a 23% improvement in the arithmetic mean average precision and a 66% improvement in the geo-metric mean average precision. The geometric mean empha-sizes the poorer performing topics.

We found differences in performance for find-similar along the dimensions of document-to-document similarity, reex-amination of documents, and the browsing pattern. First, we discovered that a query-biased similarity performs signif-icantly better than using a document alone as a query for find-similar. We demonstrated the greater clustering power of query-biased similarity using an all pairs shortest path analysis that we believe is novel. Secondly, interfaces sup-porting find-similar as a search tool will likely need to help the user avoid reexamining already examined documents. Finally, auser X  X browsingpatterncan substantiallyaffectthe performance of find-similar. Between two simulated brows-ingpatterns, wefoundthatabreadth-firstlikepatternworks better than a greedy, depth-first like pattern. Both patterns show significant improvement in the geometric mean average precision over a strong baseline retrieval.

Given the potential of find-similar, future work should include user studies to determine if users can obtain simi-lar improvements. While one could create more elaborate browsing patterns, our preference would be to implement a find-similar interface and study users. Future work should also examine in greater detail the many ways of comput-ing document-to-document similarity. Analyzing the ability of similarity methods to cluster relevant documents could continue to be done with batch style experiments.
ThankstoDonMetzlerforprovidinghisdependencemodel code and guidanceon theuseof dependencemodels. Thanks to Trevor Strohman, Hema Raghavan, and the anonymous reviewers for their helpful feedback.

This work was supported in part by the Center for In-telligent Information Retrieval and in part by the Defense Advanced Research Projects Agency (DARPA) under con-tract number HR0011-06-C-0023. Any opinions, findings and conclusions or recommendations expressed in this ma-terial are those of the authors and do not necessarily reflect those of the sponsor.
