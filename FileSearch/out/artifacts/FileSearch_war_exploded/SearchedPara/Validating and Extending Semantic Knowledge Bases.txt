 Large-scale knowledge bases are an essential component of many approaches in Natural Lan-guage Processing (NLP). Semantic knowledge bases such as WordNet (Fellbaum, 1998), YAGO (Suchanek et al., 2007), and BabelNet (Navigli and Ponzetto, 2010) provide ontological struc-ture that enables a wide range of tasks, such as measuring semantic relatedness (Budanitsky and Hirst, 2006) and similarity (Pilehvar et al., 2013), paraphrasing (Kauchak and Barzilay, 2006), and word sense disambiguation (Navigli and Ponzetto, 2012; Moro et al., 2014). Furthermore, such knowledge bases are essential for building unsu-pervised algorithms when training data is sparse or unavailable. However, constructing and updat-ing semantic knowledge bases is often limited by the significant time and human resources required.
Recent approaches have attempted to build or extend these knowledge bases automatically. For example, Snow et al. (2006) and Navigli (2005) extend WordNet using distributional or structural features to identify novel semantic connections between concepts. The recent advent of large semi-structured resources has enabled the creation of new semantic knowledge bases (Medelyan et al., 2009; Hovy et al., 2013) through automati-cally merging WordNet and Wikipedia (Suchanek et al., 2007; Navigli and Ponzetto, 2010; Nie-mann and Gurevych, 2011). While these auto-matic approaches offer the scale needed for open-domain applications, the automatic processes of-ten introduce errors, which can prove detrimental to downstream applications. To overcome issues from fully-automatic construction methods, sev-eral works have proposed validating or extending knowledge bases using crowdsourcing (Biemann and Nygaard, 2010; Eom et al., 2012; Sarasua et al., 2012). However, these methods, too, are lim-ited by the resources required for acquiring large numbers of responses.

In this paper, we propose validating and extend-ing semantic knowledge bases using video games with a purpose . Here, the annotation tasks are transformed into elements of a video game where players accomplish their jobs by virtue of playing the game, rather than by performing a more tradi-tional annotation task. While prior efforts in NLP have incorporated games for performing annota-tion and validation (Siorpaes and Hepp, 2008b; Herda  X  gdelen and Baroni, 2012; Poesio et al., 2013), these games have largely been text-based, adding game-like features such as high-scores on top of an existing annotation task. In contrast, we introduce two video games with graphical 2D gameplay that is similar to what game players are familiar with. The fun nature of the games pro-vides an intrinsic motivation for players to keep playing, which can increase the quality of their work and lower the cost per annotation.

Our work provides the following three contribu-tions. First, we demonstrate effective video game-based methods for both validating and extending semantic networks, using two games that operate on complementary sources of information: seman-tic relations and sense-image mappings. In con-trast to previous work, the annotation quality is determined in a fully automatic way. Second, we demonstrate that converting games with a purpose into more traditional video games creates an in-creased player incentive such that players annotate for free, thereby significantly lowering annotation costs below that of crowdsourcing. Third, for both games, we show that games produce better quality annotations than crowdsourcing. Multiple works have proposed linguistic annotation-based games with a purpose for tasks such as anaphora resolution (Hladk  X  a et al., 2009; Poesio et al., 2013), paraphrasing (Chklovski and Gil, 2005), term associations (Artignan et al., 2009; Lafourcade and Joubert, 2010), query expansion (Simko et al., 2011), and word sense disambiguation (Chklovski and Mi-halcea, 2002; Seemakurty et al., 2010; Venhuizen et al., 2013). Notably, all of these linguistic games focus on users interacting with text, in contrast to other highly successful games with a purpose in other domains, such as Foldit (Cooper et al., 2010), in which players fold protein sequences, and the ESP game (von Ahn and Dabbish, 2004), where players label images with words.

Most similar to our work are games that create or validate common sense knowledge. Two games with a purpose have incorporated video game-like mechanics for annotation. First, Herda  X  gdelen and Baroni (2012) validate automatically acquired common sense relations using a slot machine game where players must identify valid relations and arguments from randomly aligned data within a time limit. Although the validation is embedded in a game-like setting, players are limited to one action (pulling the lever) unlike our games, which feature a variety of actions and rich gameplay ex-perience to keep players interested longer. Sec-ond, Kuo et al. (2009) describe a pet-raising game where players must answer common sense ques-tions in order to obtain pet food. While their game is among the most video game-like, the annotation task is a chore the player must perform in order to return to the game, rather than an integrated, fun part of the game X  X  objectives, which potentially decreases motivation for answering correctly.
Several works have proposed adapting existing word-based board game designs to create or val-idate common sense knowledge. von Ahn et al. (2006) generate common sense facts by using a list facts about a computer-selected lemma and a second player must guess the original lemma hav-ing seen only the facts. Similarly, Vickrey et al. (2008) gather free associations to a target word players cannot enter a small set of banned words. Vickrey et al. (2008) also present two games simi-a category and then must list things in that cate-gory. The two variants differ in the constraints im-posed on the players, such as beginning all items with a specific letter. For all three games, two players play the same game under time limits and then are rewarded if their answers match.

Last, three two-player games have focused on validating and extending knowledge bases. Rzeniewicz and Szyma  X  nski (2013) extend Word-Net with common-sense knowledge using a 20 Questions-like game. In a rapid-play style game, OntoPronto attempts to classify Wikipedia pages as either categories or individuals (Siorpaes and Hepp, 2008a). SpotTheLink uses a similar rapid question format to have players align the DBpedia and PROTON ontologies by agreeing on the dis-tinctions between classes (Thaler et al., 2011).
Unlike dynamic gaming elements common in our video games, the above games are all focused on interacting with textual items. Another major limitation is their need for always having two play-ers, which requires them to sustain enough inter-est to always maintain an active pool of players. While the computer can potentially act as a second player, such a simulated player is often limited to using preexisting knowledge or responses, which makes it difficult to validate new types of entities or create novel answers. In contrast, we drop this requirement thanks to a new strategy for assign-ing confidence scores to the annotations based on negative associations. To create video games, our development process focused on a common design philosophy and a common data set. 3.1 Design Objectives Three design objectives were used to develop the video games. First, the annotation task should be a central and natural action with familiar video game mechanics. That is, the annotation should be supplied by common actions such as collecting items, puzzles, or destroying objects, rather than through extrinsic tasks that players must complete in order to return to the game. This design has the benefits of (1) growing the annotator pool with video games players, and (2) potentially increas-ing annotator enjoyment.

Second, the game should be playable by a single player, with reinforcement for correct game play that gold standard examples may come from both true positive and true negative items.

Third, the game design should be sufficiently general to annotate a variety of linguistic phenom-ena, such that only the game data need be changed to accomplish a different annotation task. While some complex linguistic annotation tasks such as preposition attachment may be difficult to inte-grate directly into gameplay, many simpler but still necessary annotation tasks such as word and im-age associations can be easily modeled with tradi-tional video game mechanics. 3.2 Annotation Setup Tasks We focused on two annotation tasks: (1) validating associations between two concepts, and (2) validating associations between a concept and an image. For each task we developed a video game with a purpose that integrates the task within the game, as illustrated in Sections 4 and 5. Knowledge base As the reference knowledge 2010), a large-scale multilingual semantic ontol-ogy created by automatically merging WordNet with other collaboratively-constructed resources such as Wikipedia and OmegaWiki. BabelNet data offers two necessary features for generat-ing the games X  datasets. First, by connecting WordNet synsets to Wikipedia pages, most synsets are associated with a set of pictures; while often noisy, these pictures sometimes illustrate the tar-get concept and are an ideal case for validation. Second, BabelNet contains the semantic relations from both WordNet and hyperlinks in Wikipedia; these relations are again an ideal case of valida-tion, as not all hyperlinks connect semantically-related pages in Wikipedia. Last, we stress that while our games use BabelNet data, they could easily validate or extend other knowledge bases such as YAGO (Suchanek et al., 2007) as well. Data We created a common set of concepts, C , used in both games, containing sixty synsets se-lected from all BabelNet synsets with at least fifty associated images. Using the same set of synsets, separate datasets were created for the two valida-tion tasks. In each dataset, a concept c  X  C is associated with two sets: a set V c containing items to validate, and a set N c with examples of true neg-ative items (i.e., items where the relation to c does not hold). We use the notation V and N when re-ferring to the to-validate and true negative sets for all concepts in a dataset, respectively.

For the concept-concept dataset, V c is the union of V B c , which contains the lemmas of all synsets incident to c in BabelNet, and V n c , which con-tains novel lemmas derived from statistical asso-ciations. Specifically, novel lemmas were selected by computing the  X  2 statistic for co-occurrences between the lemmas of c and all other part of speech-tagged lemmas in Wikipedia. The 30 lem-mas with the highest  X  2 are included in V c . To enable concept-to-concept annotations, we disam-biguate novel lemmas using a simple heuristic based on link co-occurrence count (Navigli and Ponzetto, 2012). Each set V c contains 77.6 lem-mas on average.
 For the concept-image data, V c is the union of c , which contains all images associated with c in BabelNet, and V n c , which contains web-gathered images using a lemma of c as the query. Web-gathered images were retrieved using Yahoo! Boss image search and the first result set (35 images) was added to V c . Each set V c contains 77.0 images on average.

For both datasets, each negative set N c is con-lated in BabelNet to all other concepts in C . By constructing N c directly from the knowledge base, play actions may be validated based on recogni-tion of true negatives, removing the heavy burden for ever manually creating a gold standard test set. Annotation Aggregation In each game, an item is annotated when players make a binary choice as to whether the item X  X  relation is true (e.g., whether an image is related to a concept). To produce a final annotation, a rating of p  X  n is computed, where p and n denote the number of times players have marked the item X  X  relation as true or false, re-spectively. Items with a positive rating after aggre-gating are marked as true examples of the relation and false otherwise. The first game, Infection , validates the concept-concept relation dataset.
 Design Infection is designed as a top-down shooter game in the style of Commando. Infection features the classic game premise that a virus has partially infected humanity, turning people into zombies. The player X  X  responsibility is to stop zombies from reaching the city and rescue humans that are fleeing to the city. Both zombies and hu-mans appear at the top of the screen, advance to the bottom and, upon reaching it, enter the city.
In the game, some humans are infected, but have not yet become zombies; these infected hu-mans must be stopped before reaching the city. Because infected and uninfected humans look identical, the player uses a passphrase call-and-response mechanism to distinguish between the two. Each level features a randomly-chosen passphrase that the player X  X  character shouts. Un-infected humans are expected to respond with a word or phrase related to the passphrase; in con-trast, infected humans have become confused due to the infection and will say something completely unrelated in an attempt to sneak past. When an in-fected human reaches the city, the city X  X  total in-fection level increases; should the infection level increase beyond a certain threshold, the player fails the stage and must replay it to advance the game. Furthermore, if any time after ten humans have been seen, the player has killed more than 80% of the uninfected humans, the player X  X  gun is taken by the survivors and she loses the stage.
Figure 1a shows instructions for the passphrase  X  X edicine. X  In the corresponding gameplay, shown in the close up of Figure 1b, a hu-man shouts a valid response,  X  X adiology X  for the level X  X  passphrase, while the nearby infected hu-man shouts an incorrect response  X  X ongitude. X 
Gameplay is divided into eight stages, each with increasing difficulty. Each stage has a goal of saving a specific number of uninfected humans. Infection incorporates common game mechanics, such as unlockable weapons, power-ups that re-store health, and achievements. Scoring is based on both the number of zombies killed and the per-centage of uninfected humans saved, motivating players to kill infected humans in order to increase their score. Importantly, Infection also includes a leaderboard where players compete for top posi-tions based on their total scores.
 Annotation Each human is assigned a response selected uniformly from V or N . Humans with responses from N are treated as infected. Players annotate by selecting which humans are infected: Allowing a human with a response from V to enter the city is treated as a positive annotation; killing that human is treated as a negative annotation.
The design of Infection enables annotating mul-tiple types of conceptual relations such as syn-onymy or antonymy by changing only the descrip-tion of the passphrase and how uninfected humans are expected to respond.
 Quality Enforcement Mechanisms Infection in-cludes two game mechanics to limit adversarial players from creating many low quality annota-tions. Specifically, the game prevents players from both (1) allowing all humans to live, via the city infection level and (2) killing all humans, via survivors taking the player X  X  gun; these actions would both generate many false positives and false negatives, respectively. These mechanics ensure the game naturally produces better quality anno-tations; in contrast, common crowdsourcing plat-forms do not support analogous mechanics for en-forcing this type of correctness at annotation time. The second game, The Knowledge Towers (TKT), validates the concept-image dataset.
 Design TKT is designed as a single-player role playing game (RPG) where the player explores a series of towers to unlock long-forgotten knowl-edge. At the start of each tower, a target con-cept is shown, e.g., the tower of  X  X ango, X  along with a description of the concept (Figure 2a). The player must then recover the knowledge of the tar-get concept by acquiring pictures of it. Pictures are obtained through defeating monsters and opening treasure chests, such as those shown in Figure 2c. However, players must distinguish pictures of the tower X  X  concept from unrelated pictures. When an image is picked up, the player may keep or discard it, as shown in Figure 2b. A player X  X  inventory is limited to eight pictures to encourage them to se-lect the most relevant pictures only.

Once the player has collected enough pictures, the door to the boss room is unlocked and the player may enter to defeat the boss and complete the tower. Pictures may also be deposited in spe-cial reward chests that grant experience bonuses if the deposited pictures are from V . Gathering un-related pictures has adverse effects on the player. If the player finishes the level with a majority of unrelated pictures, the player X  X  journey is unsuc-cessful and she must replay the tower.

TKT includes RPG game elements commonly found in game series such as Diablo and the Leg-end of Zelda: players begin with a specific charac-ter class that has class-specific skills, such as War-rior or Thief, but will unlock the ability to play as other classes by successfully completing the tow-ers. Last, TKT includes a leaderboard where play-ers can compete for positions; a player X  X  score is based on increasing her character X  X  abilities and her accuracy at discarding images from N .
 Annotation Players annotate by deciding which images to keep in their inventory. Images receive positive rating annotations from: (1) depositing the image in a reward chest, and (2) ending the level with the image still in the inventory. Con-versely, images receive a negative rating when a player (1) views the image but intentionally avoids picking it up or (2) drops the image from her in-ventory.

TKT is designed to assist in the validation and extension of automatically-created image libraries that link to semantic concepts, such as ImageNet (Deng et al., 2009) and that of Torralba et al. (2008). However, its general design allows for other types of annotations, such as image labeling, by changing the tower X  X  instructions and pictures. Quality Enforcement Mechanisms Similar to Infection, TKT includes analogous mechanisms for limiting adversarial player annotations. Play-ers who collect no images are prevented from en-tering the boss room, limiting their ability to gen-erate false negative annotations. Similarly, players who collect all images are likely to have half of their images from N and therefore fail the tower X  X  quality-check after defeating the boss. Two experiments were performed with Infection and TKT: (1) an evaluation of players X  ability to play accurately and to validate semantic relations and image associations and (2) a comprehensive cost comparison. Each experiment compared (a) free and financially-incentivized versions of each game, (b) crowdsourcing, and (c) a non-video game with a purpose. 6.1 Experimental Setup Gold Standard Data To compare the quality of annotation from games and crowdsourcing, a gold standard annotation was produced for a 10% sam-ple of each dataset (cf. Section 3.2). Two annota-tors independently rated the items and, in cases of disagreement, a third expert annotator adjudicated. Unlike in the game setting, annotators were free to consult additional resources such as Wikipedia.
To measure inter-annotator agreement (IAA) on the gold standard annotations, we calculated Krip-pendorff X  X   X  (Krippendorff, 2004; Artstein and Poesio, 2008);  X  ranges between [-1,1] where 1 indicates complete agreement, -1 indicates sys-tematic disagreement, and values near 0 indicate agreement at chance levels. Gold standard an-notators had high agreement, 0.774, for concept-concept relations. However, image-concept agree-ment was only moderate, 0.549. A further analy-sis revealed differences in the annotators X  thresh-olds for determining association, with one anno-tator permitting more abstract relations. However, the adjudication process resolved these disputes, resulting in substantial agreement by all annota-tors on the final gold annotations.
 Incentives At the start of each game, players were shown brief descriptions of the game and a de-scription of a contest where the top-ranked players would win either (1) monetary prizes in the form of gift cards, or (2) a mention and thanks in this paper. We refer to these as the paid and free ver-sions of the game, respectively. In the paid setting, the five top-ranking players were offered gift cards valued at 25, 15, 15, 10, and 10 USD, starting from first place (a total of 75 USD per game). To in-crease competition among players and to perform a fairer time comparison with crowdsourcing, the contest period was limited to two weeks. 6.2 Comparison Methods To compare with the video games, items were annotated using two additional methods: crowd-sourcing and a non-video game with a purpose. Crowdsourcing Setup Crowdsourcing was per-formed using the CrowdFlower platform. Anno-tation tasks were designed to closely match each game X  X  annotation process. A task begins with a description of a target synset and its textual def-inition; following, ten annotation questions are shown. Separate tasks were used for validat-ing concept-concept and concept-image relations. Each tasks X  questions were shown as a binary choice of whether the item is related to the task X  X  concept. Workers were paid 0.05 USD per task. Each question was answered by three workers.
Following common practices for guarding against adversarial workers (Mason and Suri, 2012), the tasks for concept c include quality check questions using items from N c . Workers who rate too many relations from N c as valid are removed by CrowdFlower and prevented from par-ticipating further. One of the ten questions in a task used an item from N c , resulting in a task mix-ture of 90% annotation questions and 10% quality-check questions. However, we note that both of our video games use data that is 50% annotation, 50% quality-check. While the crowdsourcing task could be adjusted to use an increased number of quality-check options, such a design is uncommon and artificially inflates the cost of the crowdsourc-ing comparison beyond what would be expected. Therefore, although the crowdsourcing and game-based annotation tasks differ slightly, we chose to use the common setup in order to create a fair cost-comparison between the two.
 Non-video Game with a Purpose To measure the impact of the video game itself on the anno-tation process, we developed a non-video game with a purpose, referred to as SuchGame . Players perform a single action in SuchGame: after be-ing shown a concept c and its textual definition, a player answers whether an item is related to the concept. Items are drawn equally from V c and N c , with players scoring a point each time they select that an item from N is not related. A round of gameplay contains ten questions. After the round ends, players see their score for that round and the current leaderboard. Two versions of SuchGame were released, one for each dataset. SuchGame was promoted with same free recognition incen-tive as Infection and TKT. 6.3 Game Release Both video games were released to multiple on-line forums, social media sites, and Facebook groups. SuchGame was released to separate Face-book groups promoting free webgames and groups for indie games. For each release, we estimated an upper-bound of the audience sizes using avail-able statistics such as Facebook group sites, web-site analytics, and view counts. The free and paid versions had sizes of 21,546 and 14,842 people, respectively; SuchGame had an upper bound of 569,131 people. Notices promoting the game were separated so that audiences saw promotions for one of either the paid or free incentive version. Games were also released in such a way as to pre-serve the anonymity of the study, which limited our ability to advertise to public venues where the anonymity might be compromised. 7.1 Gameplay Analysis In this section we analyze the games in terms of participation and player X  X  ability to correctly play. Players completed over 1388 games during the Crowdflower 290 13854 -0.478 59.5 93.7 66.2 $0.008 Infection free 89 3150 71.0 0.445 67.8 68.4 68.1 $0.000 Infection paid 163 3355 65.9 0.330 69.1 54.8 61.1 $0.022 Crowdflower 1097 13764 -0.167 16.9 96.4 59.6 $0.008 study period. The paid and free versions of TKT had similar numbers of players, while the paid ver-sion of Infection attracted nearly twice the play-ers compared to the free version, shown in Ta-ble 1, Column 1. However, both versions created approximately the same number of annotations, shown in Column 2. Surprisingly, SuchGame re-ceived little attention, with only a few players completing a full round of game play. We believe this emphasizes the strength of video game-based annotation; adding incentives and game-like fea-tures to an annotation task will not necessarily in-crease its appeal. Given SuchGame X  X  minimal in-terest, we omit it from further analysis.

Second, the type of incentive did not change the percentage of items from N that players correctly reject, shown for all players as N -accuracy in Ta-ble 1 Column 3 and per-player in Figure 3. How-ever, players were much more accurate at reject-ing items from N in TKT than in Infection. We attribute this difference to the nature of the items and the format of the games. The images used by TKT provide concrete examples of a concept, which can be easily compared with the game X  X  cur-rent concept; in addition, TKT allows players to inspect items as long as a player prefers. In con-trast, concept-concept associations require more background knowledge to determine if a relation exists; furthermore, Infection gives players limited time to decide (due to board length) and also con-tains cognitive distractors (zombies). Neverthe-less, player accuracy remains high for both games (Table 1, Col. 3) indicating the games represent a viable medium for making annotation decisions.
Last, the distribution of player annotation fre-quencies (Figure 3) suggests that the leaderboard and incentives motivated players. Especially in the paid condition, a clear group appears in the top five positions, which were advertised as receiving prizes. The close proximity of players in the paid positions is a result of continued competition as players jostled for higher-paying prizes. 7.2 Annotation Quality This section assesses the annotation quality of both games and of CrowdFlower in terms of (1) the IAA of the participants, measured using Krip-pendorff X  X   X  , and (2) the percentage agreement of the resulting annotations with the gold standard. Players in both free and paid games had similar IAA, though the free version is consistently higher workers have a higher IAA than game players; however, this increased agreement is due to ad-versarial workers consistently selecting the same, incorrect answer. In contrast, both video games contain mechanisms for limiting such behavior.
The strength of both crowdsourcing and games with a purpose comes from aggregating multiple annotations of a single item; i.e., while IAA may were rated as valid in the aggregated CrowdFlower annotations. be low, the majority annotation of an item may be correct. Therefore, in Table 1, we calculate the percentage agreement of the aggregated annota-tions with the gold standard annotations for ap-proving valid relations (true positives; Col. 5), re-jecting invalid relations (true negatives; Col. 6), and for both combined (Col. 7). On average, both video games in all settings produce more accurate annotations than crowdsourcing. Indeed, despite having lower IAA for images, the free version of TKT provides an absolute 16.3% improvement in gold standard agreement over crowdsourcing.
Examining the difference in annotation quality for true positives and negatives, we see a strong bias with crowdsourcing towards rejecting all items. This bias leads to annotations with few false positives, but as Column 5 shows, crowdflower workers consistently performed much worse than game players at identifying valid relations, pro-ducing many false negative annotations. Indeed, for concept-concept relations, workers identified only 16.9% of the valid relations.

In contrast to crowdsourcing, both games were effective at identifying valid relations. Table 2 shows examples of the most frequently cho-sen items from V for the free versions of both games. For both games, players were equally likely to select novel items, suggesting the games can serve a useful purpose of adding these miss-ing relations in automatically constructed knowl-edge bases. Highlighting one example, the five most selected concept-concept relations for chord were all novel; BabelNet included many relations to highly-specific concepts (e.g.,  X  X ircle of fifths X ) but did not include relations to more commonly-associated concepts, like note and harmony . 7.3 Cost Analysis This section provides a cost-comparison between the video games and crowdsourcing. The free versions of both games proved highly success-ful, yielding high-quality annotations at no direct cost. Both free and paid conditions produced sim-ilar volumes of annotations, suggesting that play-ers do not need financial incentives provided that the games are fun to play. It could be argued that the recognition incentive was motivating players in the free condition and thus some incentive was required. However, player behavior indicates oth-erwise: After the contest period ended, no players in the free setting registered for being acknowl-edged by name, which strongly suggests the in-centive was not contributing to their motivation for playing. Furthermore, a minority of players con-tinued to play even after the contest period ended, suggesting that enjoyment was a driving factor. Last, while crowdsourcing has seen different qual-ity and volume from workers in paid and unpaid settings (Rogstadius et al., 2011), in contrast, our games produced approximately-equivalent results from players in both settings.

Crowdsourcing was slightly more cost-effective than both games in the paid condition, as shown in Table 1, Column 8. However, three additional factors need to be considered. First, both games intentionally uniformly sample between V and N larger number of annotations for items in N than are produced by crowdsourcing. When annota-tions on items in N are included for both games and crowdsourcing, the costs per annotation drop to comparable levels: $0.007 for CrowdFlower tasks, $0.008 for TKT, and $0.011 for Infection.
Second, for both annotation tasks, crowdsourc-ing produced lower quality annotations, especially for valid relations. Based on agreement with the gold standard (Table 1, Col. 5), the estimated cost for crowdsourcing a correct true positive annota-tion increases to $0.014 for a concept-image and a $0.048 for concepts-concept annotation. In con-trast, the cost when using video games increases only to $0.033 for concept-image and $0.031 for concept-concept. These cost increases suggest that crowdsourcing is not always cheaper with re-spect to quality.

Third, we note that both video games in the paid setting incur a fixed cost (for the prizes) and there-fore additional games played can only further de-crease the cost per annotation. Indeed, the present study divided the audience pool into two separate groups which effectively halved the potential num-ber of annotations per game. Assuming combining the audiences would produce the same number of annotations, both our games X  costs per annotation drop to $0.012.

Last, video games can potentially come with indirect costs due to software development and maintenance. Indeed, Poesio et al. (2013) report spending 60,000  X  in developing their Phrase De-tectives game with a purpose over a two-year pe-riod. In contrast, both games here were developed as a part of student projects using open source soft-ware and assets and thus incurred no cost; fur-thermore, games were created in a few months, rather than years. Given that few online games attain significant sustained interest, we argue that our lightweight model is preferable for producing video games with a purpose. While using students is not always possible, the development process is fast enough to sufficiently reduce costs below those reported for Phrase Detectives. Two video games have been presented for vali-dating and extending knowledge bases. The first game, Infection, validates concept-concept rela-tions, and the second, The Knowledge Towers, validates image-concept relations. In experiments involving online players, we demonstrate three contributions. First, games were released in two conditions whereby players either saw financial incentives for playing or a personal satisfaction incentive where they were thanked by us. We demonstrated that both conditions produced nearly identical numbers of annotations and, moreover, that players were disinterested in the satisfaction incentive, suggesting they played out of interest in the game itself. Furthermore, we demonstrated the effectiveness of a novel design for games with a purpose which does not require two players for validation and instead reinforces behavior only using true negative items that required no man-ual annotation. Second, in a comparison with crowdsourcing, we demonstrate that video game-based annotations consistently generated higher-quality annotations. Last, we demonstrate that video game-based annotation can be more cost-effective than crowdsourcing or annotation tasks with game-like features: The significant number of annotations generated by the satisfaction incen-tive condition shows that a fun game can generate high-quality annotations at virtually no cost. All annotated resources, demos of the games, and a live version of the top-ranking items for each con-
In the future we will apply our video games to the validation of more data, such as the new Wikipedia bitaxonomy (Flati et al., 2014).

We thank Francesco Cecconi for his support with the websites and the many video game play-ers without whose enjoyment this work would not be possible.
