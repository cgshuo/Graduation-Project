 Given a complete sentence, semantic dependency parsing aims at determining all the word pairs related to each other semantically and assigning specific pre-defined semantic relations. The results of semantic dependency parsing can be highly beneficial for question answering ( who did what to whom when and where ). tended from traditional tree-structured representation of Chinese sentences. This graph structure can capture richer latent semantics. The goal of SemEval-2016 Task 9: Chinese Semantic Dependency Parsing is to identify such semantic struc-tures from a corpus of Chinese sentences. 1 The task provides two distinguished corpora in the NEWS domain and the TEXTBOOKS domain. An example of semantic dependency graph is presented in Fig. 1. We can see that  X  (he) has two head words, respectively  X   X  (leave) and  X  (go) with the Agent (Agt) rela-tion in the graph representation. More detailed information about the corpora will be reported in Section 4.
 NLP because of its speed and accurate performance. This kind of parser con-struct dependency trees by using a sequence of transition actions over input sen-tences. They are mostly used to produce dependency trees, rather than graphs, or more accurately DAG in Chinese Semantic Dependency Parsing . this work. Our first approach has two stages. First, we adopt a set of linguistic-motivated rules to transform graphs to trees, based on which a traditional non-projective transition-based parser is trained. Then, we use a classifier to recover the extra arcs from a candidate arc set generated also by rules and add them to the parser X  X  output to form dependency graphs. A similar approach was studied in [23], which applied pseudo-projective transformations [20] to transform non-projective dependencies to projective ones, and then use a dependency graph parser to parse projective graphs. This approach typically requires inconvenient pre-process and post-process.
 DAG directly for Chinese sentences. This parser is a variant of the list-based al-gorithm of [4], which was designed to parse dependency trees. In order to handle dependency graphs, we change the preconditions of the transitions to allow some words to have multiple head words. We have also tried to simplify the transition action set to investigate the relation between the parser performance and the number of transition actions. Extensive experiments show that both of our ap-proaches obtain significantly better results compared with the top participated systems in the SemEval-2016 Task 9: Chinese Semantic Dependency Parsing . Since most existing current dependency parsers only deal with dependency trees, it is a challenge to produce graphs with transition-based parser. To begin with, we propose a hybrid system which combines a traditional dependency tree pars-er and a binary classifier for complementing extra arcs. Then we propose a transition-based dependency parser that produce dependency graphs directly. 2.1 Tree-Based Method In our first approach, we separate the task into two steps. In the first step, we use a traditional transition-based dependency parser to parse preprocessed semantic dependency trees. Then an SVM classifier is used to identify extra arcs that will be added to the parser results from candidate arc set created by rules. Pre-process Transforming DAGs to trees is typically a process of removing extra arcs of words which have multiple heads. We designed certain rules to remove such extra arcs in given semantic dependency graphs so that we can get semantic dependency re-generate candidate arc set from trees produced by the tree parser. mantic dependency graphs in training data. We present them in the Appendix. Our rules can cover 95.5% graph situations in the NEWS corpus and 95.0% in the TEXTBOOKS corpus. The uncovered ones are basically all irregular situations, under which we hold the closest arc of the multi-head words.
 verb as well as the subject of the following verb, which introduces multiple heads to it. Fig. 2 shows an instance of this situation. For this kind of graph, we remove the dependency arcs between the noun and the following verb (  X   X  (introduce) Agt  X  X  X  X   X   X   X  (Mr.Zhang)).
 arcs from parsing results. For every verb pair with an arc between them, we find all noun children of the head verb  X   X  (invite) X , and add arcs from the child verb  X   X   X  (introduce) X  to these nouns to the candidate arc set (the two dashed arcs). According to the gold-standard graph, the label of the red arc is Agt , while the label of the blue one is NULL, meaning it should not be added to the final semantic dependency graph.
 Dependency Parser The semantic dependency trees transformed from DAGs are not necessarily projective. So we adopt the Swap -based algorithm [19], which can parse non-projective trees with a time complexity that is quadratic in the worst case but still linear in the best case. the classifier, which solved several problems caused by traditional sparse indica-tor features and yielded good results. Following their footsteps, we train a neural network classifier for use in a greedy, transition-based dependency parser. Their feature templates are used as the basic features in our experiment.
 SVM Classifier for Post-process The preprocessing rules mentioned above are used reversely to generate the training samples of SVM classifier from preprocessed semantic dependency trees. Labels of these samples include all possible relations of extra arcs and one NULL label indicating that the corresponding candidate arc should not be added to the tree. The features primarily include words, POS tags and distances. The specific feature templates are presented in [8]. 2.2 Dependency Graph Parser The above method is a compromise to the tree parser, which still requires careful pre-process and post-process. Moreover, for new dependency relation sets, the preprocessing rules need to be redesigned. We then present a transition-based dependency graph parser, which parses dependency graphs directly. In the Swap -based algorithm [19], we have to precompute the projective order which indicates when the Swap transition appears while generating oracle transition sequences. However, the coocurrence of non-projectivity and multiple heads makes it hard to compute the projective order . Here, we base our graph parser on the list-based arc-eager algorithm that was originally introduced to parse non-projective trees [4]. By simply modifying the preconditions of transitions, we make it capable of parsing dependency graphs. The specific preconditions are introduced in the next section.
 stack holding processed words,  X  is a stack holding words popped out of  X  that will be pushed back in the future, and  X  is a buffer holding unprocessed words. A is a set of labeled dependency arcs. We use index i to represent word w i , and the index 0 represents the root of the graph w 0 . It X  X  important to notice that in this task all the roots w 0 of DAGs are required to have only one child. The initial parsing, arcs will only be generated between top element of  X  , w i , and the first element of  X  , w j . The transition is generated by consulting the gold-standard trees during training and a neural network classifier during decoding. Transition System 1 corresponding preconditions are described in Table 2. Left l - X  and Right l - X  add an arc with label l from w j to w i , and vice versa. These transitions are performed only when one of w i and w j is the head of the other. Otherwise, No- X  will be performed.  X  -Shift is performed when no dependency exists between w j and any word in  X  other than w i , which pushes all words in  X  and w j into  X  .  X  -Reduce is performed only when w i has head and is not the head or child of any word in  X  , which pops w i out of  X  .  X  -Pass is performed when neither  X  -Shift nor  X  -Reduce can be performed, which moves w i to the front of  X  . ( i l  X  X  X  j ) is used to denote an arc from w i to w j with label l . ( i  X  j ) and ( i  X   X  j ) indicate that w i is a head and an ancestor of w j respectively.
 [4] X  X  algorithm. For Left l - X  and Right l - X  , it X  X  necessary to make sure no word has multiple heads in their tree parser, which is not required in our graph parser. For  X  -Reduce , we have to confirm that all of w i  X  X  heads and children are found before removing it from  X  . While they only need to check w i  X  X  children, since w i already has and can only have one head in tree structure.
 arc-eager algorithm is presented in Table 3. In state 4, w 1 is moved from  X  to  X  because it has another head w 5 in  X  , and the arc between them will be generated in the future (state 9). In state 8, the transition is Right-Pass rather than Right-Shift because w 5 still has a child w 1 in  X  , w 3 is moved to  X  so that the arc between w 5 and w 1 can be generated (state 9). In states 3, 7 and 9, w 2 , w 4 and w 1 are popped out of  X  respectively because all of their children and heads have been generated. Transition System 2 based arc-eager algorithm [5] which has less transitions which is referred to as simplified list-based algorithm henceforth.
 Pass , Left l -Reduce , Right l -Pass , No-Shift and No-Pass in system 1 re-spectively. While Right l -Shift and No-Reduce are discarded. In labeled pars-ing task like semantic dependency parsing, the number of transitions that gen-erate arcs ( Left l - X  , Right l - X  ) equals to the number of label classes. Thus the number of transitions is reduced by a quarter for our task.
 same semantic dependency graph in Fig. 1 is presented in Table 5. Since Right-Shift is discarded, a Right-Arc and a Shift is used to replace it in the sim-plified algorithm, resulting in an extra transition for each Right-Shift (states 5 and 6, states 7 and 8). Since No-Reduce is discarded, the word that should be popped out of  X  stays, which generate an extra Pass every time it is moved to  X  (state 9). Generally speaking, we reduce the number of transitions at the price of longer transition sequences.
 Feature Templates Our feature templates are generally adapted from [3]. Considering the difference between dependency graphs and dependency trees, we add additional features regarding the heads of words on top of  X  and in the front positions of  X  . The complete feature templates are presented in Table 6.
 valency , distance and cluster . The numbers of left and right modifiers to a given head are identified as left valency and right valency respectively [30]. We extend the definition to graphs and use left head valency and right head valency to denote the numbers of left and right heads to a given modifier respectively. And the original valencies are then referred to as left child valency and right child valency . Brown Clustering [1] is a form of hierarchical clustering of words based on the contexts in which they occur and have proved beneficial for neural parsing [11]. We use pre-trained Brown clusters for each word involved in the baseline features. All of these new features are represented as embeddings and then pass through the neural network. 3.1 Datasets The SemEval-2016 Task 9 provides two distinguished corpora in the domain of NEWS and TEXTBOOKS. Detailed statics are presented in Table 7. The non-local dependencies [25] in the table refers to the dependency arcs which make dependency trees collapsed. and the complete sentence. Labeled and unlabeled precision and recall with re-spect to predicted dependencies are used as evaluation measures. Since non-local dependencies are extremely difficult to discover, they are evaluated separately. For sentence level, labeled and unlabeled exact matches are used to measure sentence parsing accuracy. These metrics are abbreviated as: 3.2 Results The following hyper-parameters are used in all of our neural models: basic em-bedding size (for words, POS tags and dependency labels) d = 50, hidden layer size h = 400, dropout rate p d = 0 . 5, regularization parameter  X  = 10  X  8 , initial learning rate of Adagrad  X  = 0 . 01.
 portion of the Chinese Gigawords. We use 20-dimensional vector for cluster, and 10 for valency and distance. The number of Brown clusters are set to 256. The numbers of training samples for the SVM classifier in NEWS corpus and TEXTBOOKS corpus are 1,869,819 (6,748 positive) and 248,716 (3,542 positive) respectively. We use the liblinear toolkit [10] for training linear-kernel SVM classifiers. We conduct experiments with baseline features alone and with all features (baseline features, valency, distance and cluster) in both approaches. Table 8 and Table 9 show the results of our systems and other participating systems. The detailed description of other participating systems are presented in [2]. 3.3 Discussions The labeled F1 scores of our three systems with baseline features are higher than other participating systems. The first approach (Tree+SVM) has the best performance in LF and UF but worst in NLF. However, its NUF is higher than others. It is mainly due to the low precision of the SVM classifier, resulting from the small amount of the positive training samples. Some of the labels only occur very few times, making it difficult to predict the dependency labels of the extra arcs precisely. However, in the second approach (dependency graph parser), with lower NUF, we present much higher NLF scores, showing the high precision of dependency label prediction. This is because the labeling part is trained with all samples rather than non-local samples alone in dependency graph parser. tant to notice that the NUF and NLF of the first approach do not benefit from richer features which improve the non-local scores of the second approach a lot. This is probably because richer features only help improve the performance of tree parsing which do not provide non-local information. This is an advantage of our dependency graph parser, since we can improve the performance of entire structure prediction and non-local dependency prediction at the same time by using richer features. than simplified list-based algorithm in most situations. This is because we reduce the number of transitions at the price of lengthening transition sequences in the simplified list-based algorithm . Longer transition sequences make it harder to discover non-local informations (NUF and NLF). However, smaller transition set provides higher precision for the entire structure prediction (UF and LF). in LF and UF, it requires extra human knowledge to design preprocessing rules. So for new dependency relation sets, we have to redesign the rules manually in the first approach. Also its NLF is limited by the tree parser X  X  performance. However, our dependency graph parser in the second approach can parse dependency graphs completely automatically and does not require extra human knowledge. Transition-based dependency parsing [28, 20, 29, 19, 13] makes structural predic-tions with a deterministic shift-reduce process. Most of these parsers are designed to parse dependency trees. [23] presented a transition-based approach to DAG parsing on the work of [20] on pseudo-projective transformations. Since their parser can only parse projective dependency graphs, their approach requires pre-process and post-process. [17] presented a DAG parser by extending the maxi-mum spanning tree dependency framework of their early work in 2005, which is a graph-based dependency parser. Other parsing approaches that produce depen-dency graphs [6, 18, 22] are generally based on linguistically-motivated lexicalized grammar formalisms, such as HPSG, CCG and LFG.
 Treebank parser, applying neural networks to dependency parsing and represent-ing the states with dense embedding vectors has been more and more popular [26, 24, 3, 31, 27]. Recently, [9] proposed a technique for learning representations of parsing states in transition-based dependency parser with stack LSTM and yielded state-of-the-art performance.
 information in the sentence based on dependency grammar [21]. [14] were the first to use dependency grammar in semantic analysis. Then [7] proposed Stanford typed dependencies representations. [15] [16] were the first to work on Chinese semantic dependency, and have manually annotated a corpus in the scale of one million words. HIT semantic dependency is established by Research Center for Social Computing and Information Retrieval in Harbin Institute of Technology in 2011. [8] refined the HIT dependency scheme with stronger linguistic theories, yielding a dependency scheme with more clear hierarchy. We present two transition-based approaches for DAG parsing, and studied two kinds of transition set for the latter one. All of our systems yield significantly better LF than other participating systems in SemEval-2016 Task 9. We further provide extensive analysis and show the advantages and disadvantages of both approaches.
 can be further improved by using richer features. Therefore, our approach is expected to benefit from the recently proposed LSTM-based architectures [9]. We leave it to our future work.
 Preprocessing rules used in our first approach:
