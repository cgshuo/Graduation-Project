 Labeled data for natural language processing tasks such as part-of-speech tagging is often in short sup-ply. Semi-supervised learning algorithms are de-signed to learn from a mixture of labeled and un-labeled data. Many different semi-supervised algo-rithms have been applied to natural language pro-cessing tasks, but the simplest algorithm, namely self-training, is the one that has attracted most atten-tion, together with expectation maximization (Ab-ney, 2008). The idea behind self-training is simply to let a model trained on the labeled data label the unlabeled data points and then to retrain the model on the mixture of the original labeled data and the newly labeled data.

The nearest neighbor algorithm (Cover and Hart, 1967) is a memory-based or so-called lazy learn-ing algorithm. It is one of the most extensively used nonparametric classification algorithms, sim-ple to implement yet powerful, owing to its theo-retical properties guaranteeing that for all distribu-tions, its probability of error is bound by twice the Bayes probability of error (Cover and Hart, 1967). Memory-based learning has been applied to a wide range of natural language processing tasks including part-of-speech tagging (Daelemans et al., 1996), de-pendency parsing (Nivre, 2003) and word sense dis-ambiguation (K  X ubler and Zhekova, 2009). Memory-based learning algorithms are said to be lazy be-cause no model is learned from the labeled data points. The labeled data points are the model. Con-sequently, classification time is proportional to the number of labeled data points. This is of course im-practical. Many algorithms have been proposed to make memory-based learning more efficient. The intuition behind many of them is that the set of la-beled data points can be reduced or condensed, since many labeled data points are more or less redundant. The algorithms try to extract a subset of the overall training set that correctly classifies all the discarded data points through the nearest neighbor rule. Intu-itively, the model finds good representatives of clus-ters in the data or discards the data points that are far from the decision boundaries. Such algorithms are called training set condensation algorithms.
The need for training set condensation is partic-ularly important in semi-supervised learning where we rely on a mixture of labeled and unlabeled data points. While the number of labeled data points is typically limited, the number of unlabeled data points is typically high. In this paper, we intro-duce a new semi-supervised learning algorithm that combines self-training and condensation to produce small subsets of labeled and unlabeled data points that are highly relevant for determining good deci-sion boundaries. The nearest neighbor (NN) algorithm (Cover and Hart, 1967) is conceptually simple, yet very pow-erful. Given a set of labeled data points T , label any new data point (feature vector) is the data point in T most similar to Similarity is usually measured in terms of Euclidean distance. The generalization of the nearest neighbor algorithm, k nearest neighbor, finds the k most simi-lar data points T that: with E ( , ) Euclidean distance and || || = 1 if the argument is true (else 0). In other words, the k most similar points take a weighted vote on the class of
Naive implementations of the algorithm store all the labeled data points and compare each of them to the data point that is to be classified. Several strate-gies have been proposed to make nearest neighbor classification more efficient (Angiulli, 2005). In particular, training set condensation techniques have been much studied.

The condensed nearest neighbor (CNN) algorithm was first introduced in Hart (1968). Finding a sub-set of the labeled data points may lead to faster and more accurate classification, but finding the best subset is an intractable problem (Wilfong, 1992). CNN can be seen as a simple technique for approxi-mating such a subset of labeled data points.
The CNN algorithm is defined in Figure 1 with T the set of labeled data points and T ( t ) is label pre-dicted for t by a nearest neighbor classifier  X  X rained X  on T .

Essentially we discard all labeled data points whose label we can already predict with the cur-rent subset of labeled data points. Note that we have simplified the CNN algorithm a bit compared to Hart (1968), as suggested, for example, in Alpay-din (1997), iterating only once over data rather than waiting for convergence. This will give us a smaller set of labeled data points, and therefore classifica-tion requires less space and time. Note that while the NN rule is stable, and cannot be improved by
T = {h x 1 , y 1 i , . . . , h x n , y n i} , C =  X  for h end for return C
T = {h x 1 , y 1 i , . . . , h x n , y n i} , C =  X  for h end for return C techniques such as bagging (Breiman, 1996), CNN is unstable (Alpaydin, 1997).

We also introduce a weakened version of the al-gorithm which not only includes misclassified data points in the classifier C , but also correctly classi-fied data points which were labeled with relatively low confidence. So C includes all data points that were misclassified and those whose correct label was predicted with low confidence. The weakened condensed nearest neighbor (WCNN) algorithm is sketched in Figure 2.

C inspects k nearest neighbors when labeling new data points, where k is estimated by cross-validation. CNN was first generalized to k -NN in Gates (1972).

Two related condensation techniques, namely re-moving typical elements and removing elements by class prediction strength, were argued not to be useful for most problems in natural language pro-cessing in Daelemans et al. (1999), but our experi-ments showed that CNN often perform about as well as NN, and our semi-supervised CNN algorithm leads to substantial improvements. The condensa-tion techniques are also very different: While re-moving typical elements and removing elements by class prediction strength are methods for removing data points close to decision boundaries, CNN ide-ally only removes elements close to decision bound-aries when the classifier has no use of them.
Intuitively, with relatively simple problems, e.g. mixtures of Gaussians, CNN and WCNN try to find the best possible representatives for each clus-ter in the distribution of data, i.e. finding the points closest to the center of each cluster. Ideally, CNN returns one point for each cluster, namely the cen-ter of each cluster. However, a sample of labeled data may not include data points that are near the center of a cluster. Consequently, CNN sometimes needs several points to stabilize the representation of a cluster; e.g. the two positives in Figure 3.
When a large number of unlabeled data points that are labeled according to nearest neighbors pop-ulates the clusters, chances increase that we find data points near the centers of our clusters, e.g. the  X  X ood representative X  in Figure 3. Of course the centers of our clusters may move, but the positive results ob-tained experimentally below suggest that it is more likely that labeling unlabeled data by nearest neigh-bors will enable us to do better training set conden-sation.

This is exactly what semi-supervised condensed nearest neighbor (SCNN) does. We first run a WCNN C and obtain a condensed set of labeled data points. To this set of labeled data points we add a large number of unlabeled data points labeled by a NN classifier T on the original data set. We use a simple selection criterion and include all data points 1: T = {h x 1 , y 1 i , . . . , h x n , y n i} , C =  X  , C 2: U = {h x  X  1 i , . . . , h x  X  m i} # unlabeled data 3: for h x i , y i i  X  T do 5: C = C  X  {h x i , y i i} 6: end if 7: end for 8: for h x  X  i i  X  U do 9: if P T ( h x  X  i , T ( x  X  i ) i| w i ) &gt; 0 . 90 then 10: C = C  X  {h x  X  i , T ( x  X  i ) i} 11: end if 12: end for 13: for h x i , y i i  X  C do 14: if C  X  ( x i ) 6 = y i then 16: end if 17: end for 18: return C  X  that are labeled with confidence greater than 90%. We then obtain a new WCNN C  X  from the new data set which is a mixture of labeled and unlabeled data points. See Figure 4 for details. Our part-of-speech tagging data set is the standard data set from Wall Street Journal included in Penn-III (Marcus et al., 1993). We use the standard splits and construct our data set in the following way, fol-lowing S X gaard (2010): Each word in the data w is associated with a feature vector where x 1 and Marquez, 2004) trained on Sect. 0 X 18, and x 2 is a prediction on w speech tagger (a cluster label), in our case Unsu-pos (Biemann, 2006) trained on the British National nearest neighbor classifier on Sect. 19 of the devel-opment data and unlabeled data from the Brown cor-pus and apply it to Sect. 22 X 24. The labeled data points are thus of the form (one data point or word per line): where the first column is the class labels or the gold tags, the second column the predicted tags and the third column is the  X  X ags X  provided by the unsu-pervised tagger. Words marked by  X * X  are out-of-vocabulary words, i.e. words that did not occur in the British National Corpus. The unsupervised tag-ger is used to cluster tokens in a meaningful way. Intuitively, we try to learn part-of-speech tagging by learning when to rely on SVMTool.
 The best reported results in the literature on Wall Street Journal Sect. 22 X 24 are 97.40% in Suzuki et al. (2009) and 97.44% in Spoustova et al. (2009); both systems use semi-supervised learning tech-niques. Our semi-supervised condensed nearest neighbor classifier achieves an accuracy of 97.50%. Equally importantly it condensates the available data points, from Sect. 19 and the Brown corpus, that is more than 1.2M data points, to only 2249 data points, making the classifier very fast. CNN alone is a lot worse than the input tagger, with an accuracy of 95.79%. Our approach is also significantly better than S X gaard (2010) who apply tri-training (Li and Zhou, 2005) to the output of SVMTool and Unsu-pos.
In our second experiment, where we vary the amount of unlabeled data points, we only train our ensemble on the first 5000 words in Sect. 19 and evaluate on the first 5000 words in Sect. 22 X 24. The derived learning curve for the semi-supervised learner is depicted in Figure 5. The immediate drop in the red scatter plot illustrates the condensation ef-fect of semi-supervised learning: when we begin to add unlabeled data, accuracy increases by more than 1.5% and the data set becomes more condensed. Semi-supervised learning means that we populate clusters in the data, making it easier to identify rep-resentative data points. Since we can easier identify representative data points, training set condensation becomes more effective. The implementation used in the experiments builds on Orange 2.0b for Mac OS X (Python and C++). In particular, we made use of the implementations of Euclidean distance and random sampling in their package. Our code is available at: We have introduced a new learning algorithm that simultaneously condensates labeled data and learns from a mixture of labeled and unlabeled data. We have compared the algorithm to condensed nearest neighbor (Hart, 1968; Alpaydin, 1997) and showed that the algorithm leads to more condensed models, and that it performs significantly better than con-densed nearest neighbor. For part-of-speech tag-ging, the error reduction over condensed nearest neighbor is more than 40%, and our model is 40% smaller than the one induced by condensed nearest neighbor. While we have provided no theory for semi-supervised condensed nearest neighbor, we be-lieve that these results demonstrate the potential of the proposed method.
