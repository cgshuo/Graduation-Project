 Continuous queries enable alerts, predictions, and early warn-ing in various domains such as health care, business process monitoring, financial applications, and environment protec-tion. Currently, the consistency of the result cannot be as-sessed by the application, since only the query processor has enough internal information to determine when the output has reached a consistent state. To our knowledge, this is the first paper that addresses the problem of consistency un-der the assumptions and constraints of a continuous query model. In addition to defining an appropriate consistency notion, we propose techniques for guaranteeing consistency. We implemented the proposed techniques in our existing stream engine, and we report on the characteristics of the observed performance. As we show, these methods are prac-tical as they impose only a small overhead on the system. H.2 [ Information Systems ]: Database Management Algorithms
Monitoring systems are increasingly automated, as the vast amounts of information are beyond the capacity of man-ual processing. One example of large scale monitoring is the tracking of goods in time and space, enabled by the use of RFID tags and readers. Similarly, in the enterprise context, activities are run by processes usually monitored for perfor-mance and quality with key performance indicators. Events that are emitted by states of business processes can give an indication of slow-down in process performance, and lead to better understanding of root causes in case of errors and exceptions.

Monitoring systems often use the model of continuous queries to implement alert queries, whose output is sent to a dashboard or to an alert manager program. The task of the Stream Processing Engine (SPE) is to run and manage continuous queries. Inside the engine there are delays and scheduling policies that affect the order of the output tu-ples. The problem is that there is no way for an end user to distinguish between output sequences that are consistent and outputs inconsistent with the input. Although there has been extensive research on ways to speed up the pro-cessing of streaming data, little attention has been payed to the semantics and guarantees of the generated results.
Preceding the SPE itself, a Feed Cleanser prepares the input for the processing engine core by attaching system timestamps, ordering the events according to certain time se-mantics, etc. Since this is a separate, application-dependent issue, we assume that the Feed Cleanser processed the input correctly according to application semantics, and therefore the input into the SPE satisfies these requirements. Pre-vious proposals defined consistency and correctness for this Feed Cleanser, but not for the processing of the SPE itself. They addressed issues such as ordering events based on a system time, source time, or both [5]. Other approaches were proposed to correct the data captured by events [16, 10]. Yet another class of related work analyzes the effects of approximations and guarantees in the presence of load shedding [18].

By contrast to previous work in the area, our goal is to guarantee the consistency of results. The SPE can still ex-pose partial results to the monitoring applications / dash-board, but it will mark and, if needed, enforce consistent outputs. The guarantees are tied to the input sequence into the system: we ensure an anomaly-free output, and that the results can be reproduced and retraced. The methods we propose vary in the degree of control the system has over consistent output states. At one end of the spectrum the output can be simply observed for consistency (given certain labeling of the events). At the other end, the system controls and adapts the frequency of synchronization steps to meet query correctness requirements, while satisfying throughput constraints.

We will point out anomalies in stream processing, and define various levels of consistency of the query output. A similar problem was observed in the maintenance of multi-ple dependent views in relational databases [21]. However, the solution proposed there is not applicable to continuous queries. While speed and throughput are essential to stream processing, view maintenance can easily absorb the overhead of compensation queries and additional computation. Fur-thermore, stream ope rators do not produce atomic results such as in view maintenance. We explicitly recognize and identify the need for atomicity in ways that are appropriate to stream processing
To the best of our knowledge, this is the first attempt to address the problem of recognizing and eliminating process-ing anomalies in the context of continuous query processing. This is a problem complementary to that of replication, ap-proximation, or feed cleansing, and it is essential in guar-anteeing the correctness of query results. We implemented the techniques proposed in this paper in our event stream processing engine, and studied the various trade-offs that arise.

The presentation is organized as follows. In Section 3 we introduce the output anomaly problem, and propose levels of consistency appropriate for the specific requirements of continuous queries. In Section 4 we discuss various alter-natives for eliminating output anomalies. After exploring the range of possible solutions, we first describe the heed-ing, wait-and-see approach. By contrast, the second solution takes advantage of periodic synchronization, and it can also be used to maintain recoverable states in case of failures. Finally, in Section 6 we analyze the performance character-istics of the proposed solutions, and we conclude with some of related research directions that we are exploring.
Let us consider a continuous query composed of a number of operators, that processes one or more inbound streams of events. Operators can be Select, Project, Window Joins or Aggregates that output results incrementally. Since we are assuming a tuple format for these events, we will refer inter-changeably to them as  X  X uples X  and  X  X vents X . These events can be sensor measurements, RFID readings, process noti-fications etc. The final output of a query is generated by a single Output operator. Another particular type of opera-tor introduced in our implementation of a Stream Process-ing Engine is Emit, which receives events from one inbound stream, formats them if necessary, and sends the events to the relevant query operators. In general, operators consume events and send events from one to another using queues. A scheduler coordinates when and for how long operators run. We say that an operator O 1 that sends its output to an operator O 2 is a parent of O 2 (and conversely O 2 is the child of O 1 ). An operator O 1 is an ancestor of operator O if there is a path of parent-child links from O 1 to O 2 .
Another assumption is that all queues/ channels in the system are FIFO and reliable. That is, messages arrive in the order they are sent and no messages are lost. Our model assumes one channel between each pair of connected oper-ators, and an inbound channel (or inbound input) for each stream of events entering the system. The k -th input chan-nel of an operator O j is denoted by In k ( O j ), while the k -th output channel is Out k ( Oj ). The history H ( e, I ) of a channel is the ordered set of events read from that chan-nel up to and including event e . All events that are read from the channel later than e are not part of the history H ( e, I ) of the channel. Since history states are actually sets of events, a history H ( e ,I )isa prefix of a history H ( e, I ) if H ( e ,I )  X  H ( e, I ). When all events in a history H ( e, I ) have timestamps smaller than t and no later event can have a timestamp smaller than t ,wedenotethisas H ( e, I )  X  t . Note that timestamps have semantics particular to the ap-plication (such as originator time, system time, etc).
The state C ( e, I ) of a channel is the set of events in the channel that include e ,wherealleventsotherthan e entered the channel before e . For clarity, we assume that an operator marks the top event in one of its input queues as  X  X n use X  while processing it, and removes it from the input queue only after the entire result of processing this event is placed in the output queue.

Consider an operator O j that processes an input event a . The event a is read from one of the input queues, the operator processes the event based on its current state, and the output tuples/events b, c are placed in the output queue. We say that b and c are actively derived from a ,orthatevent a actively derives b and c . By contrast, events are passively derived from events that are in the state of the operator and contribute to composing this output. For example a streaming window join will join the incoming event a with a window of events from other stream, and as a result the output tuple b may be composed of attributes from a as well as attributes from an event d from the join window. Then b is actively-derived from a and passively-derived from d . Note that the notions of active and passive derivations are with respect to a given execution schedule . Let a streaming join operator process input from two streams. As an event is received from one of the inputs, it is joined with the window of events maintained over the other input stream, the resulting events are placed in the output chan-nel, and the event is inserted in the corresponding window. As events are read by an external application from the out-put channel of the operator, there is no available indication of the completeness of a result. A partial result can be mis-leading if used for other computations or displayed. Example. The example of Figure 1 shows the state and out-put produced by processing events from two streams. This is a very simple query with one join operator, where the input is inbound (coming from external sources), and the output is sent incrementally to an application. For simplic-ity we do not show the Output operator. In the state of Figure 1(a) tuples a , b ,and k have already been read from the left and right input stream, and processed. The output is then tuples ak and bk actively derived from k .Inthelast step (Figure 1(b)), an event tuple l from the second input is processed and as a result two new events are placed into the output channel: al and bl , actively derived from l .Consider now the output right before tuple bl is computed. At this point the user has an inconsistent view of the results. The problem is that the output reflects the existence of b in the inbound input (from output tuple bk ), but only partially. The output represents a state of the input in which b was inserted and then deleted before the operator processed event l , a sequence of actions that is incorrect since it does not correspond to the input data .

Partial results are processed correctly internally, but should either be buffered and not sent to the user, or include explicit delimiters. As operators are chained some bookkeeping is necessary to ensure correct results.
 Example Consider a query that is composed of operators O , O 2 and O 3 according to the flow shown in Figure 2. Emitter operators receive inbound input streams from two inbound channels, and forward them to operators O 1 and O . The output of O 1 and O 2 is processed by O 3 .Figure 2(a) shows a snapshot of the system at a time when the events in the left inbound channel are a and b , while the right inbound channel has events k, l, m .Thetwoevents from Emitter1 are processed first by the operator O 1 , while operator O 2 processes the first two events from Emitter2. Due to different scheduling and computational requirements, the two operators O 1 and O 2 do not process the events from the common channel at the same time. At some point in time, as shown in Figure 2(b), event l is still in the input channel of O 1 , while the tuple l  X  l actively-derived from l through operator O 2 is in the queue of operator O 3 .Next, O 3 processes l  X  l . The problem is that now the state of the output of O 3 reflects only partially the effect of processing event l . The output of O 3 incorporates effects of l , while its parent O 1 has not seen l yet. Since this is an internal anomaly, the receiver of the output has no indication that the result is incomplete, and incorrect to use.

The above examples illustrate the need for the output to be update-complete . The following definition helps guarantee that events actively-derived from an input event are either all included in the output or none of them is included. Let an output state of an operator be C ( e 0 ,Out ( O j )). Then we define update-completeness to ensure that there cannot be an event e that is produced by the operator O j later than e 0 , that is actively-derived from the same input event as an event e in C ( e 0 ,Out ( O j )). An important property of update-completeness that we should point out is that it is based on active-derivation, which is specific to the processing sequence followed by the system. For the same input tuples, a different scheduling decision leads to different decisions on update-completeness.

Definition 1. : Update-completeness. An output state C ( e 0 ,Out ( O j ) of an operator O j is update-complete iff:  X  e ,e  X  Out ( O j ) ,e  X  e 0  X  e e 0 =  X   X  ( e ) =  X  ( e ) .
Update-completeness allows for outputs that include ef-fects of some input events, and exclude the effects of other interleaving input events. In other words, it allows for out-put to be the result of selectively processing input events, making  X  X oles X  in the input. If all events in the output are update-complete, then the corresponding inbound events that actively derived this output must have been fully pro-cessed. This does not guarantee that there is no other in-bound event whose processing is delayed. Therefore, if an output is update-complete, it still may not correspond to a prefix of the inbound input. A stronger definition, input-prefix update-completeness imposes restrictions on consistent output to be update-complete and also represent the execu-tion over entire input prefixes. Let the output state of an operator O j be C ( e 0 ,Out ( O j )), and I 1 ,  X  X  X  I i , inbound inputs. Let current histories of the inbound events be H ( e i ,I i ),1  X  i  X  p . For given events e i  X  H ( e i  X  p ,wedenoteas input-prefix H [ H ( e 1 ,I 1 ),  X  X  X  H ( e H ( e p ,I p )] the set formed by the corresponding prefixes of the current history of each inbound stream. Input-prefix update-completeness ensures that there is a prefix of inputs such that the output state contains all and only those events that are actively derived from events in that prefix. Definition 2. : Input-prefix update-completeness. An output state C ( e 0 ,Out ( O j )) of an operator O j is input-prefix update-complete iff  X  e i  X  H ( e i ,I i ) , 1  X  i such that: A
In theory, a related notion of consistency based on passively-derived events would give stronger guarantees. In practice however, such a notion is not relevant for stream processing because it undermines the  X  X ontinuous X  aspect of continuous queries. For an infinite input stream the query may never return a consistent output.

The previous definition limits the guarantee to a given execution sequence. In general, one may ask if there exists an execution sequence over the same input such that the output is consistent.

Example Let us revisit the example in Figure 1, but as-sume that event k does not exist. If the operator makes dif-ferent scheduling decisions, it can first receive events from the second input and then from the first. When tuple l is processed the result is empty. The subsequent processing of events on the first input will then each lead to one output tuple. The result is update-complete after the first output tuple al , and it is update complete again after the second tuple bl . This is different than the decision based on the previous processing order: if tuples a and b are processed first, then following the processing of tuple l the two output event form an atomic unit. It is not possible to infer when the output reached a correct state simply by analyzing the events in the output .

Next, we define a broader notion of correctness based on input-prefix consistency , that does not depend on the inter-nal decisions of the system on the processing sequence.
Let input prefixes be H [ H ( e 1 ,I 1 ),  X  X  X  H ( e i ,I i Also, let Q [ H [  X  X  X  ]] be the result tuples after running the computation of the (deterministic) query Q over all and only the events in prefix H [  X  X  X  ]]. An output is input-prefix con-sistent if it corresponds to a prefix of the inbound inputs: all results of processing the events in these prefix sets are included in the output, and there is no event in the out-put that is not derived only from events in the prefix sets. An important property of input-prefix consistency is that it guarantees the reproducibility of results as any execution schedule over the same prefix will produce the same results (modulo order). For the rest of the paper we will refer to input-prefix consistency simply as consistency .

Definition 3. : Input-prefix consistency. The out-put C ( e 0 ,Out ( O j )) of an operator O j is input-prefix con-sistent iff  X  i , 1  X  i  X  p ,  X  e i  X  H ( e i ,I i ) , such that : Q (
H [ H ( e 1 ,I 1 ) ,  X  X  X  H ( e
An input prefix is a set of prefixes of all inbound channels, not necessarily synchronized. If a system needs to guarantee an output where the input prefixes are synchronized, then the following is a more appropriate definition.

Definition 4. : Synchronized Input-prefix consis-tency. The output C ( e 0 ,Out ( O j )) of an operator O j chronized input-prefix consistent iff there exists a time t such that: Q (
H [ H ( e 1 ,I 1 ) ,  X  X  X  H ( e where  X  i , 1  X  i  X  p , e i is the last event in the history of I such that e i  X  t .

In the rest of the paper we will focus on consistency no-tions that do not impose synchronization of inputs. The modifications needed for solutions to guarantee synchronized input-prefix consistency are straight forward.

Let us review the relationship between update-completeness, input-prefix update-completeness, and input-prefix consis-tency.

If an output is input-prefix consistent , then it requires that 1) there exists a processing order that would recognize the output as update-complete , and 2)that there are no  X  X oles X  processing the input. The difference between input-prefix consistency and input-prefix update-completeness is that the latter imposes update-completeness for a given execution of the event processing, while the former only requires that the output corresponds to some possible execution. Input-prefix consistency considers the processing of the SPE as a black box, and it is based only on the inbound inputs and SPE output. By contrast, (input-prefix) update-completeness is tied to the processing order of the events. There relation-ships are shown in Figure 3.

The challenge in ensuring that an operator X  X  output is in a consistent state is that there is no global knowledge about channel and operator states. Without access to in-formation about the state of ancestor operators, an external application cannot differenti ate between a consistent and an inconsistent output state.

The consistency problem that we observed is similar to that of updating a set of dependent materialized views in relational databases [21]. Our definitions cover different levels that are practical in the stream processing setting. Input prefix consistency is similar to the level required by data warehouse applications. The complexity of a multi-view maintenance algorithm arises from the need to cor-rectly propagate updates from base sources to views and recreate synchronicity in an asynchronous system. For ex-ample, updates are tagged with their lineage, so states can be recreated locally to include out-of-order updates. Unlike the algorithm proposed for view maintenance, stream op-erators cannot query other operators or perform additional compensation steps. A strea m operator needs information about the synchronization and dependencies from all over the system. In this case it is more efficient to be able to recognize consistent states, or impose periodically a low-overhead synchronization step. For this purpose we have to distinguish throughout the operator chain between active and passive events, and keep track of queue states. Next, we discuss different methods that can be used to guarantee a consistent output.
In this section we analyze a range of techniques that can be used to ensure the consistency of the output when process-ing continuous queries. If the semantics of the application require that consistency is supported as part of the quality of service agreement, the system can buffer the output and send it out only when a consistent state is detected. Al-ternatively, the operator can send output continuously, but annotate the consistent states so that buffering can be done at the application layer.

Naturally, there is a trade-off between the frequency of consistent output states and performance. Consistent out-put states are easily enforced if each event is processed en-tirely by the system and the result is sent to the output before the next event can be taken from the inbound input. We refer to this method as draining the system. The advan-tage of this approach is that it exposes the largest number of consistent states. More granular states would not sat-isfy the update-completeness condition of consistency. The drawback of this approach is that it cannot take advantage of pipelining and in general either global or local optimiza-tions, essential in ensuring high throughput. In this case the scheduler is restricted to bottom-up operator order, and operators do not have any autonomy in processing input events.

Coordination methods for the processing of events can be grouped as follows (in order of decreasing freedom in operator scheduling): 1. annotate and observe consistent output states .With-2. force periodic synchronization . This approach has the 3. force coordination at the most granular level possible .
These methods enforce input-prefix update-completeness because they recognize consistent output according to some processing order (in other words, the methods do not recog-nize if the output will be consistent according to any process-ing order). The states that are input-prefix update-complete are therefore also input prefix consistent. We discuss two algorithms in detail: the heeding method and the synchro-nization token approach , with tradeoffs outlined in Table 4.
The heeding method achieves consistency guarantees with-out impacting operator scheduling. This solution returns no false positives, but it may miss some of the states that are in fact consistent. The main idea is that events are labeled when they enter the system from inbound streams and then get re-labeled when output by operators. When an event is received in the final output buffer of the query, the system verifies if the output reached a consistent state based on the labels of events in the output buffer and labels of events still being processed in the system. If the state is consistent, then the output can be sent further to the consuming ap-plication. Otherwise the event will be kept in the output buffer until consistency is guaranteed at a later time. Alter-natively, partial results can be sent to the application, but output will be labeled complete/incomplete .

Each label is an n -ary vector of numbers, where n is the number of inbound inputs. When an inbound event is re-ceived by an Emit operator from an inbound stream I k ( k is the stream identifier, known by all Emits ), it is assigned the label (0 ,...,e id ,..., 0) that contains an event id e id k -th position and 0 everywhere else. Since there is no more than one Emit operator per inbound stream, event id sare unique within each inbound input stream, and in increasing order over time.

An operator that processes an input event e calculates component-wise the maximum between the label of e and all passively deriving events that participate in the processing of e and are stored in the state of the operator. In addition, the label meta-data differentiates between active and passive labels .The k -th label in the output event is set to active if the k -th label in the actively-deriving input event was active. In case of emit operators for stream I k ,the k -th label (the only one that is non-zero) is set to active.

Labels and active/passive markers allow us to track active and passive lineage which is important to decide whether any events that could cause additional outputs are still in the system (and would render the output therefore incon-sistent). Tracking active derivation is necessary to ensure that, if an event e in the output is actively derived from an event e in the inbound input, then the system waits for all events actively derived from e (Definition 2). This is done by comparing active labels in the output with active labels still transient (being processed) in the system. To illustrate the need to also keep track of passive lineage, consider the following example.
 Example Events from two streams A and B are processed in the order of arrival, as shown in Figure 4. Assume that tuples are joined such that the complete output that can be generated from these tuples is { a 1 b 1 , a 2 b 1 , a 1 b scheduling inside the stream engine, the first output tuple, a b 1 , is actively derived from b 1 (underlined in the example), and the second output tuple, a 2 b 2 , is actively derived from a . The last tuple a 1 b 2 , actively derived from b 2 , is delayed. If we only keep track of active derivation lineage, we may incorrectly assume that the current output is consistent. By also tracking passive derivation, we are able to detect that the passively deriving event b 2 is still in the system and may actively derive more events later.

As shown by the above example, it is possible to have an event e in the output passively derived from an event e in the inbound stream, while some events actively derived from e are still transient in the system. Then the output reflects the existence of e , but only partially. To avoid such a scenario, we need to also compare passive labels in the output with active labels in the system.

Given the labeling scheme just discussed, how can we de-tect at the query output level whether a state is consistent, or in other words, whether there are more events in the sys-tem that may cause actively derived output events in the future? Let us assume we keep a buffer E of events to be output to the consuming application. We want to delay the sending out of these events until input-prefix update-consistency is reached. The basic idea is now to check for each event e in E whether there are any events in the system (i.e., in any queue) that have the same or a smaller label than e for any of the input channels and that are marked active for that channel. If no such event is found in any queue, then there are no more events that generated e that may generate additional other events in the future. Therefore, it is safe to output e to the consuming application.
The detailed steps of the heeding algorithm are outlined below. The labeling of events is performed by each operator whenever a new event is generated. It essentially assigns the maximum of labels of participating events (component-wise) to the output event, as discussed earlier. It also prop-agates the active marker from the actively-deriving event to the output event. Detection of consistency points is per-formed globally for each output channel. The algorithm first collects the labels from all pending events in E and then checks which of these labels have any  X  X ncestors X  (i.e., events with smaller or equal active labels in the same input channel) in the system. This test is described by predicate hasActiveAncestor . If none of the collected labels have such ancestors, the events in E constitute a consistent state and can be output.
 We note that in the case of aggregation operations, the Max calculation in the Labeling algorithm calculates the maximum between the labels of the new incoming event (the actively deriving event) and all buffered events in the oper-ator (the passively deriving events) that participate in the aggregation operation. In other words, events that do not fit the aggregation window will not participate in the maximum calculation.

There are several ways to imp rove the performance of this algorithm in a practical implementation. First, instead of in-specting all queues for the evaluation of hasActiveAncestor , it is possible to maintain a multiset M that stores the ac-tive component of labels with their original input channel I of all events in all queues. When an event enters a chan-nel between two operators, the corresponding active label is added to M . Whenaneventisremovedfromachannel,one occurrence of its active label is removed from M .Since M is a multiset, a label will only be completely removed from M if there are no more events in any queue that have that label marked as active (note that a label/inbound stream pair may occur multiple times in the system). In order to further boost performance, M canbemaintainedasanarray of multisets, one for each inbound stream.

A second optimization opportunity is the frequency with which the Detection operation is performed. This check can be performed every time a new output event is generated. It is however possible that some of the pending events in E form a consistent state earlier than that, for example, if some active event is later removed in some operator due to a filter condition. In this case, the now consistent events would only be output once a new output event arrives. In order to output consistent events earlier, Detection can be executed whenever a new output event arrives and when an active event is removed from the system. Naturally, there is a trade-off between the overhead of the Detection check and the frequency of output consistent events.

Proposition 1. The Heeding method correctly recognizes input-prefix update-complete output states. Proof. (omitted for lack of space.)
The drawback in practice is that the system has no control on the frequency of output states guaranteed to be consis-tent. We expect that consistency will also be included as part of the quality of service features of stream engines, in which case control over output is needed. A way to provide this control is to enforce coordination steps, as explained next.
By contrast to the wait-and-see approach of the heeding method, the next method actively enforces consistent states. In a system that supports quality of service guarantees on latency, draining the system is not a practical alternative. Instead, a more adequate solution is to simulate draining the system periodically, without the actual overhead of the draining procedure. Draining lets the system stop the input and execute the processing of all the events still left at the operators and in the queues/ channels. Under this require-ment, we chose to use synchronization tokens, generated by the emitters and injected into the stream of events. The concept of tokens has been effectively used in various areas of distributed processing. Here we use tokens to divide the flow of events through the system into sections with clear boundaries, and obtain a similar effect as draining the sys-tem. The advantage is that we are able to provide consistent output at controlled intervals with a very low impact on the system.

The tokenization process is the following. Emitters gen-erate and insert a token periodically in the input stream, at every  X T increment of time. A non-emitter operator pro-cesses events as in a token-less system, until it receives a token from one of its input queues. The operator will block the reading from that queue, until one token is received on each input. When all tokens are received, one output token is generated and sent on all the output queues of the opera-tor. Whenever a token is received in the output of the last operator, the output state is deemed consistent.

Since tokens are sent at the same time interval from emit-ters, the same number of tokens is inserted on each stream. An operator that integrates two or more streams will also integrate the tokens: there will be one output token for each set that includes the first incoming token from each stream. Since for each token in the input of the operator there will be exactly one token in the output, we can easily see that the cumulative numbers of tokens on any two paths from an operator to its emitters differ by at most a number equal to the number of operators in that path.

Note that punctuation/ heartbeats can be used in the same way as tokens. One condition is that the system is able to control the periodicity of the punctuation; this is not the case when punctuation is sent by the event genera-tor, and is tied to the local semantics. The necessary control layer will be very similar to that of tokens, and therefore we do not discuss it separately.

Proposition 2. The token-based synchronization technique correctly identifies input-prefix update-complete output states. Proof. (omitted for lack of space.)
Although to avoid slowing down streaming inputs it is desirable to synchronize the inter-token periods, the cor-rectness of the output does not depend on it. One way to synchronize the tokens is to extend the query model with ad-ditional emitSynch operators. These new operators can be added for each cluster of emitters that need to be synchro-nized. For high throughput however, the use of emitSynch operators should be avoided as they can become bottlenecks in the system.

Synchronization tokens introduce an overhead in latency, the time it takes for events to be processed, measured start-ing from emitters and ending when their effects reach the top operator. This overhead is the largest when the fre-quency of tokens is the greatest; in an extreme case tokens are inserted after each tuple, in which case the overhead due to processing the tokens becomes a significant factor of the total latency. However, if tokens are less frequent, even one in a couple of hundred of events, the additional latency in-troduced by tokens is imperceptible (see Section 6 for more details). At the same time, the more frequent the tokens, the shorter is the wait for the next consistent state, which directly affects the application or user that is using the re-sults of the query. One way to set the token frequency is to start with an initial inter-token period, and modify it at runtime to reach the best compromise between the two re-quirements. If the latency is affected too much and emitters start creating increasing backlogs of events, then the period should be increased until the throughput is large enough for the system to be able to keep up with the incoming rate of events. This period should be kept as short as possible under the limitation that it should not introduce backlogs, and frequently provide consistent output.
Examples of data stream management systems are Au-rora [2], STREAM [13], TelegraphCQ [6], and Gigascope [8]. The first two are designed to operate in a single physical node, and focus on minimizing the end-to-end data pro-cessing latency. Subsequent work extends Aurora to dis-tributed systems [7, 1]. The extended framework, Borealis, includes a load distribution algorithm that aims at minimiz-ing the average end-to-end latencies [20]. It also employs checkpointing for recovery purposes, not linked with the ac-tual processing flow of operators. TelegraphCQ is based on the Eddy [4] and SteM [15] mechanisms, enabling shared and adaptive processing of multiple queries. STREAM also discusses the problem of time management for data stream systems [17]. All of these systems focus on optimizing per-formance measures such as latency, and enforcing correct results in the presence of out-of-order events. Our work, in contrast, focuses on providing consistency of the generated output, for query processing structures that include directed acyclic graphs.

A significant amount of research has investigated various issues of streaming punctuations. Tucker et al. [19] intro-duce the concept of punctuations as a method to reduce memory consumption and operator response time. For ex-ample, if the punctuation is  X  X ll following events have date greater than 01/01/2005 X  then a stream join operator may be able to discard stored events with date less than or equal to 01/01/2005 if the join is on date. Similarly, the operator can return the results of joins with date less than or equal to 01/01/2005 knowing that all following events will not add more join results for these dates. Since punctuations break the infinite stream into finite substreams, Tucker et al. also showed that traditional relational operators can be  X  X eused X  for processing the substreams. Li et al. [12] discuss ways to efficiently check for purgeability of stream join states and space unboundedness of join operators. They introduce a punctuation graph which describes the relationship between join predicates and punctuation schemes. This graph is used to determine whether a state in a stream join operator can be safely purged. The second application of the punctuation graph is to check whether stream join operators require un-bounded storage and should therefore be marked  X  X nsafe X . A closely related idea are stream heartbeats, first proposed by Motwani et al. [14] and discussed in the context of Gi-gascope by Johnson et al. [11]. Heartbeats are extra events inserted into the input streams at periodic intervals based on a global clock. Motwani et al. [14] use heartbeats to prevent out-of-orderness in input streams. This idea is extended by Srivastava and Widom [17] to handle skew between streams and stream latency. Similarly, Johnson et al. [11] use heart-beats to prevent multi-stream operators from being blocked while waiting for events. This is achieved by augmenting simple heartbeat events with punctuations. In summary, punctuations and heartbeats were introduced (1) to facili-tate operator reuse by breaking infinite streams into finite substreams and (2) to improve memory usage and response time and to deal with event loss or out-of-orderness. The purpose of our synchronization markers, on the other hand, is to allow for consistency guarantees to the user.
Closely related to this area is the area of consistency guar-antees during updates in data warehouses. Data warehouses integrate and summarize information from one or several base sources in materialized views. User queries are directed to the materialized views for fast response time, while up-dates are sent to the base tables and eventually propagated to the materialized views to avoid staleness of the data. The maintenance module refreshes the warehouse view by either re-deriving the whole view, or by incrementally computing and integrating the source updates with the current view [3, 21]. The choice of a maintenance approach directly af-fects the speed and the freshness of answers to user queries. Some methods log a history of updates or auxiliary infor-mation on certain intermediate results [9]. This can lead to significant storage overhead that cannot be supported by a stream engine.
We implemented the consistency control methods in our existing WhiteWater stream engine and we evaluated their performance on various continuous queries using synthetic stream data.

The stream engine supports typical stream operators (eg. emit, select, project, window joins, window aggregate). The operators are inter-connected by queues and their operation is coordinated by a global scheduler. The scheduler runs one operator at a time, in a round-robin fashion, for a dura-tion proportional to the size of the input queues. Operators choose which input to access and when to switch between in-puts. The window join operator implements the symmetric hash join algorithm. Although the system handles aggrega-tions and filters in addition to joins, we focus our evaluation on join because of their complexity. We first examine the effectiveness of each method in isolation and then we per-form a comparative study of their efficiency (overhead on the engine).
First, we wanted to study the effectiveness of the heeding methodforatypicalquery. Moreprecisely,wewereinter-ested to see how often do consistent states occur in practice, in the absence of any consistency enforcement mechanism. For this experiment, we ran a three join query over two streams of synthetic data. The first two joins are over the two input streams (on different join columns) and the third join processes their outputs. The keys were uniformly dis-tributed in the interval 1 to 5000.

In order to study the frequency of consistent states, we measured the distance between consistent states, expressed as the number of output tuples generated in between two such states. We summarized the results from this experi-ment in a histogram, shown in Figure 5(a). Here, the hori-zontal axis represents bins for the distance (eg. up to 10 tu-ples, between 10 and 20, etc.), and the vertical axis captures the number of occurrences. The vast majority of consistent states occur close together (a few tuples apart). However, there are instances where consecutive consistent states are separated by hundreds of tuples. We observe here a Zipf-like distribution: the number of occurrences decreases with the distance between consistent states according to a power-law.
We ran the same experiment for several queries and data distributions and we observed similar results, only with dif-ferent scaling factors, which depend on the input rates and join selectivities.

The significance of this type of behavior is the following: for the applications which can tolerate unexpected delays between consistent states, the heeding method is sufficient. Since the user is made aware whether the output is con-sistent at any given time, this is an adequate method for these types of applications. However, in general, the impor-tant limitation of this method is its inability to control the frequency of consistent states.
Next, we implemented the token-based synchronization technique and studied its behavior on the same query as in Section 6.1. In the first three experiments the join keys are uniformly distributed in the interval 1 to 5000 and in the fourth one, the keys on the two streams are synchronized.
In order to understand the impact on performance of in-jecting tokens into the input streams, we varied the to-ken frequency from the extreme 1 token between every two events to 1 token every 10,000 events (input rates are uni-form). In Figure 5(b) we show the effects of this variation on the maximum and average latency. Latency is measured only for tokens as the elapsed time between a token enters the system and the corresponding token is output. As we can observe, the latency is highest when the tokens are most fre-quent. This is due to the token processing overhead and the increased sizes of the queues. In fact, in this extreme setting, the queue sizes are doubled. As the tokens are sent at larger and larger intervals, their relative impact on the processing decreases asymptotically, down to a minimum (equivalent to the non-token case).

The third curve in Figure 5(b) represents the output to-ken frequency (inter-token delay) , which naturally follows the input token frequency. A good solution for consistency would tend to push the token frequency down so that consis-tent states are more frequent. However, this solution would seriously impact the latency. Clearly, minimizing the la-tency and output token frequency are two conflicting goals, and the choice of prioritizing one or the other depends on the application.

One way of compromising between these two conflicting optimization goals is to consider the cumulative latency. Cu-mulative latency is the maximum elapsed time between the moment an event enters the system and the first output consistency token that was generated after that. This can be computed by adding the latency and the output inter-token delay. The resulting curve is U shaped with a global minimum around 250 events between tokens. This suggests an  X  X ptimum X  token frequency of 1 in 250 tuples for this query. In practice, however, a different token frequency may be chosen, based on the application requirements. The main advantage of this method is that it gives the system administrator complete control over the frequency of con-sistent output states. It should be noted that the output token frequency represents an upper bound on the inter-consistent state distance, as consistent states may occur more frequently than output tokens (this can be monitored using the heeding method, if needed).
Afterstudyingeachmethodinisolation,wewantedto compare the overhead incurred by each method on the en-gine.
For this experiment, we measured the output throughput and latency for four different queries Q1-4 (the query num-berrepresentsthenumberofjoins),andforfourvariants of the WhiteWater engine: WW (baseline, no consistency checking), WWM (heeding method with multiset), WWN (naive heeding method), and WWT (token-based method with a token frequency set at 1 token for 100 input tuples). The naive heeding method uses the same labeling as the multiset one, but performs the detection of the consistent state by scanning all the inter-operator queues to confirm whether a given event label is still present in any queue.
The measured output throughput for each query and each engine are summarized in Figure 6(a). As we can observe, the impact on the system was negligible for both the multiset-based heeding method (WWM) and the token-based method (WWT), as the throughput is roughly the same as for the baseline engine (WW). Each number represents an aver-age over 5 separate runs of the same configuration. The differences are in the range of the variability between dif-ferent runs, which explain why WWM and WWT seem to outperform WW in some cases. The only variant that ex-hibited a significant performance degradation was the naive heeding method (WWN): the deterioration (relative to the base) is proportional with the number of queues that had to be scanned (3 for Q1, 5 or Q2, 7 for Q3 and 9 for Q4), as expected. Please note that the comparisons only make sense for the same query, not between queries, as the output throughput is dependent on the overall query selectivity.
The average tuple latencies for each query and each engine are given in Figure 6(b). As in the case of the throughput measurements, the overhead on the engine is high only for the naive heeding method (up to six times higher latency than baseline). However, we do observe a slight latency degradation for the token-based method (eg. 4% for query Q3 and 10% for query Q4), whereas the multiset-based heed-ing method had no significant penalty. This is explained by the increased queue sizes due to the tokens, which in turn increases the time tuples spend waiting in queues.
The increasing latency as queries get more complex is due to the increasing height of the query plan, and thus of the number of queues an event has to travel through from Emit to the Output operator.
We measured the maximum memory used for the same 4 queries and the methods: WW (baseline), WWM (heeding with multiset), WWN (naive heeding), and WWT (token) (Figure 6(c)). WWM uses more memory that WW mostly because each tuple has additional columns to store labels. Although WWN X  X  memory footprint seems to be smaller, in reality this is because it produces less results and therefore processes a smaller number of tuples. To analyze this effect, we show in Figure 6(d) the memory footprint relative to throughput. There we can clearly see that WWN is the least efficient: it spends a significant part of the time scanning the queues and therefore produces less results, although it uses more memory for each tuple. The overhead of WWT is negligible, and almost the same as that of WW.

To conclude, the multiset-based heeding method has no noticeable effect on the system X  X  performance and the token-based method only a slight adverse effect on the latency and no effect on the throughput. However, the token-based method provides better control over the frequency of con-sistent states and gives flexibility in managing the trade-off between performance and consistency.
In this paper we pointed out the problem of consistency for continuous queries with processing structures that are directed acyclic graphs. We defined several levels of con-sistency that pertain specifically to stream processors, and studied the challenges of supporting this consistency. To en-sure consistency, we proposed two very different approaches. The wait-and-see heeding solution uses meta-data associated with events to infer if a query output state is consistent. For applications where control over the frequency of consistent states is important, we proposed an alternative solution that introduces tokens in the streams to be used as synchroniza-tion points. We implemented variants of the heeding method and the token-based method in out stream engine, and the overhead in all cases was indeed minimal. The benefit of the token-based approach is not only that it ensures con-sistency, but it also easily enables the persistence of correct snapshots for recovery. We are currently exploring the im-plications of the token-based synchronization in the context of distributed continuous query processing, and also its ap-plication in fault-tolerance.

In relational databases, the importance of ensuring correct query results has been recognized a while ago. By contrast, intricacies in stream processing are still not well understood. This paper uncovers one milestone in reaching a better un-derstanding of the field. [1] D.J.Abadi,Y.Ahmad,M.Balazinska,U.Cetintemel, [2] D. J. Abadi, D. Carney, U. Cetintemel, M. Cherniack, [3] D.Agrawal,A.E.Abbadi,A.K.Singh,andT.Yurek.
 [4] R. Avnur and J. M. Hellerstein. Eddies: Continuously [5] R. S. Barga, J. Goldstein, M. H. Ali, and M. Hong. [6] S. Chandrasekaran, O. Cooper, A. Deshpande, M. J. [7] M. Cherniack, H. Balakrishnan, M. Balazinska, [8] C. D. Cranor, T. Johnson, O. Spatscheck, and [9] Y. Cui and J. Widom. Practical lineage tracing in [10] S. R. Jeffery, G. Alonso, M. J. Franklin, W. Hong, and [11] T. Johnson, S. Muthukrishnan, V. Shkapenyuk, and [12] H.-G. Li, S. Chen, J. Tatemura, D. Agrawal, K. S. [13] R. Motwani, J. Widom, A. Arasu, B. Babcock, [14] R. Motwani, J. Widom, A. Arasu, B. Babcock, [15] V. Raman, A. Deshpande, and J. M. Hellerstein. [16] J. Rao, S. Doraiswamy, H. Thakkar, and L. S. Colby. [17] U. Srivastava and J. Widom. Flexible time [18] N. Tatbul, U.  X  Cetintemel,S.B.Zdonik,M.Cherniack, [19] P. A. Tucker, D. Maier, T. Sheard, and L. Fegaras. [20] Y. Xing, S. Zdonik, and J.-H. Hwang. Dynamic load [21] Y. Zhuge, H. Garcia-Molina, J. Hammer, and
