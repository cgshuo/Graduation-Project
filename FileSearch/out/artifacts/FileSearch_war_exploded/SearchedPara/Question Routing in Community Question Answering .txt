  X  This paper investigates a ground-breaking incorporation of question category to Question Routing (QR) in Commu-nity Question Answering (CQA) services. The incorpora-tion of question category was designed to estimate answerer expertise for routing questions to potential answerers. Two category-sensitive Language Models (LMs) were developed with large-scale real world data sets being experimented. Results demonstrated that higher accuracies of routing ques-tions with lower computational costs were achieved, relative to traditional Query Likelihood LM (QLLM), state-of-the-art Cluster-Based LM (CBLM) and the mixture of Latent Dirichlet Allocation and QLLM (LDALM).
 H.3.3 [ Information Storage and Retrieval ]: Informa-tion Search and Retrieval X  information filtering, selection process Algorithms, Experimentation, Performance question routing, community question answering, question category, category-sensitive language model
Since the inception of forums for asking and answering questions, Community Question Answering (CQA) services have been providing users with web platforms to obtain use-ful information, for example, the development of Yahoo! An- X  Irwin King is currently on leave from CUHK to be with AT&amp;T Labs.
 Figure 1: An example of question category in CQA services (captured from Yahoo! Answers on January 20, 2011) swers 1 and Quora 2 . In recent years, the efficiency of CQA services, however, is challenged by a sharp increase of ques-tions raised in the communities. Such increasing amount of questions have thus influenced access of answerers to their appropriate questions, with the process of question answer-ing being hindered in CQA services [2]. To facilitate an-swerer access to proper questions, an approach of Question Routing (QR) has been initiated and developed in CQA ser-vices [2, 4, 6, 3, 7].

The concept of QR refers to routing newly posted ques-tions to potential answerers; the appropriateness of poten-tial answerers (expertise estimation, hereafter) is estimated based on archives of their previously answered questions. Volumes of studies have been conducted regarding exper-tise estimation, including Query Likelihood Language Model (QLLM) [5], Cluster-Based Language Model (CBLM) [7], mixture of Latent Dirichlet Allocation (LDA) and QLLM [4]. However, as for an answerer , a complete set of questions the answerer has answered is utilized in the models, although certain amount of answered questions might be irrelevant to questions to be routed. To solve this problem, question category will be utilized to sifted out irrelevant questions in profile of an answerer for expertise estimation. In CQA ser-vices, askers have to choose a category for the question they asked.AsshowninFig.1,eachquestionisclassifiedintoa particular category. The categories of new questions would allow much latitude in screening irrelevant questions of an answerer to enhance the efficiency of expertise estimation. To date, few attempts have been made regarding category http://answers.yahoo.com http://www.quora.com information in studies of QR. This study was thus designed to fill the gap.

The paper is organized as follows. Related work is first reviewed in Section 2. Categor y-sensitive LMs are developed in Section 3. Experimental setup as well as results are then reported in Section 4 and 5. In the end, a conclusion is drawn in Section 6.
Expertise estimation, as mentioned, has been of paramount importance to assess potential of answerers for solving ques-tions to be routed [2, 4, 6, 3, 7]. In studies of expertise estimation, two families of models have been widely em-ployed: Language Models [3, 7] and Topic Models [2, 6]. Cao et al. [1] leveraged question category to enhance ques-tion retrieval in CQA and the experimental results ensured this approach X  X  effectiveness on various of retrieval models. To our knowledge, no previous work, however, estimates an-swerer expertise using question category for QR. the basic category-sensitive LM ( BCS-LM ) is defined as follows: and of answering questions in c j for u i ,and q u ij represents the question texts of all previously answered questions in c j u .

It is noted that BCS-LM is based on the same-leaf-category assumption, with potential answerers under sim-ilar leaf categories being omitted. As shown in Fig. 2, CQA portals like Yahoo! Answers set refined category hierarchy. Under one main category, there exist similar leaf categories. For example, the leaf categories of  X  Programming &amp; Design  X  and  X  Software  X  in Fig. 2. Answerers with expertise in  X  Pro-gramming &amp; Design  X  may also be an expert on questions asked in X  Software  X . To supply such omissions, we have come up with a transferred category-sensitive QLLM ( TCS-LM ) as follows: where  X  adjusts the weight between the original leaf category and other similar leaf categories, the lower  X  , more weights are given to similar categories. Tran ( c j ) denotes the set of categories which are transferable from (similar to) c j and T ( c k  X  c j ) represents the probability of transferring from category c k to c j .

We define where  X  is a threshold between 0 and 1.

We use an answerer-based approach to estimate the trans-ferring probability between two categories, which assumes that if there are many same answerers posting answers in two categories, these two categories should be similar with each other. To be specific, we construct a category-answerer matrix E from resolved questions, each row of E repre-sents one (leaf) category and each column represents one answerer. In addition, the value of e ji denotes the number of answers u i provided in category c j .Let e j and e k denote two row vectors of c j and c k , we apply the cosine similarity to estimate the transferring probability ( T ans (  X  )) between two categories:
The data comprised over 400, 000 resolved questions (June to October 2010) from Computers &amp; Internet and Enter-tainment &amp; Music categories of Yahoo! Answers through provided API 3 . The two categories included 20 and 25 leaf categories respectively 4 . Table 1 reports the statistics of datasets. As for all selected questions, the information re-garding affiliated category , texts and answerer IDs was avail-able. Those questions were further classified into Set A (Test data, questions posted after 6 May: 382,695 ques-tions, 1,335,892 answers and 243,167 answerers) and Set B (Archive data, remaining questions: 50,377 questions, 174,639 answers and 49,466 answerers). In addition, an-swerers in Set A were used as ground truth.
 Table 1: Description of the Yahoo! Answers data set (after stop words removing and stemming) Number of questions 433,072 Number of answers 1,510,531 Average number of answers for one question 3.49 Maximum number of answers for one question 50 Mean first reply duration (in minutes) 197.32
Average question length in words 43.87 (both subject and content) Average answer length in words 30.08 Number of askers 240,277 Number of answerers 270,043 Number of both askers and answerers 68,551 Number of askers only 171,726
Number of answerers only 201,492 Cluster-based language model (CBLM) [7] and mixture of LDA and QLLM (LDALM) [4] were selected to be compared http://developer.yahoo.com/answers/
The leaf category Polls &amp; Surveys was excluded since this leaf category was used to elicit public opinion. The dataset was thus composed of 44 leaf categories. with category-sensitive LMs for expertise estimation based on the following two considerations: 1. In CBLM [7], similar questions under same topic are 2. Experimental results in [4] showed that utilizing latent
In addition, the original QLLM was included in com-parisons as the baseline method. We used the tool Gibb-sLDA++ 5 to estimate the posterior probabilities of LDA (say,  X  of each answerer and  X  of each topic). The default setting was adopted and the number of latent topics was set as 100 empirically. We adopted Precision at K , Mean Average Precision and Mean Reciprocal Rank as evaluation metrics for the ranking lists generated by various LMs in expertise estimation. Fur-thermore, we employed the mean QR time (MQRT) which calculates the average time spent on routing one question (including expertise estimation and ranking) as the metric of time efficiency for all methods.

We set  X  =3 . 5 for TCS-LM in the experiments empir-ically as this setting yields the best performance. As it is time-consuming to test all que stions in Set A (Test set), we sampled 440 questions randomly from Set A (10 questions from each leaf category) as testing data. All algorithms ran in a PC with two 2.4GHz CPUs and 3G main memory. Table 2 reports Prec @ K for all algorithms with different Ks from 1 to 100, and Table 3 presents the MRR and MAP of all methods. Table 4 gives the time-efficiency of each method in QR based on MQRT. http://gibbslda.sourceforge.net/ From Table 2 we observe that, for various of K s, both BCS-LM and TCS-LM outperform QLLM significantly on Prec @ K . For instance, when routing questions to the top 1 answerers, on average QLLM gives less than 8 successful routings per 100; BCS-LM and TCS-LM make more than 11 and 12 successful routings, which improve QLLM by 40.13% and 54.34% respectively. For other K s, category sensitive LMs also perform better than QLLM.

The MRR of BCS-LM and TCS-LM increase that of QLLM by 29.66% and 34.59%. From the definition of MRR, each new question will be answered by at least one answerer in the top 5 answerers using BCS-LM or TCS-LM. However, with QLLM on average we have to route the question to almost top 7 answerers to get an answer.

As to MAP, BCS-LM and TCS-LM improve QLLM by 33.08% and 37.29% respectively and it shows that category sensitive LMs give more accurate rankings on the whole.
To sum up, the above results have assured the effectiveness of utilizing category information in expertise estimation.
Now let X  X  turn to the time costs of QLLM and category-sensitive LMs. Table 4 gives the average time of routing a question for each model. We find that BCS-LM saves 47.16% of time while TCS-LM costs 13.80% less time than QLLM, which demonstrates that category-sensitive LMs are more time-efficient than QLLM in expertise estimation and thus make QR faster. The lower costs of BCS-LM lie in only relevant profiles are utilized in expertise estimation, which reduces the computational cost. TCS-LM spends more time than BCS-LM because of employing profiles in relevant cat-egories for expertise estimation. Although TCS-LM is more time-consuming than BCS-LM, it is possible to reduce the time through parallel computing since the expertise estima-tion with different categories X  profiles is independent with each other.
Looking at Table 2, we find that similar categories improve accuracies of expertise estimation when K is small. In par-ticular, the Prec @1 of TCS-LM is 10.14% higher than those of BCS-LM. In addition, the Prec @10 of TCS-LM is 2.04% more accurate than that of BCS-LM. Although when K be-comes large (say, high than 40), TCS-LM improves fewer or even a little worse than BCS-LM, the former one is still a better choice as a QR system has to route a question to min-imum number of potential answerers in practice. The MRR and MAP of TCS-LM are also better than those of BCS-LM from Table 3. TCS-LM utilizes similar categories X  profiles  X  54.34%) 0.0989 (  X  24.40%) 0.0000  X  41.05%) 0.1950 (  X  17.54%) 0.0000  X  29.36%) 0.2455 (  X  17.41%) 0.0000  X  27.73%) 0.3102 (  X  14.68%) 0.0000  X  16.13%) 0.3710 (  X  9.57%) 0.0091  X  11.00%) 0.4392 (  X  6.19%) 0.0273  X  7.10%) 0.4649 (  X  3.84%) 0.0545  X  3.85%) 0.4867 (  X  2.96%) 0.0727  X  4.18%) 0.4979 (  X  1.43%) 0.0795 Table 3: MRR and MAP of various models (best results in bold) BCS-QLLM 0.1893 (  X  29.66%) 0.1424 (  X  33.08%) TCS-QLLM 0.1965 (  X  34.59%) 0.1469 (  X  37.29%) Table 4: Various methods X  MQRT in QR (in sec-onds)
QLLM BCS-QLLM TCS-QLLM LDALM CBLM 10.4271 5.5098 8.9884 16.7689 4.2488 and assign weights to these profiles according to the degree of similarities. Therefore, they give more precise expertise estimation and thus improve QR X  X  performance. Across these four methods, CBLM performs the worst. The probable reason is that a great amount of answerers only answered in one cluster (leaf category), as such their contributions to this cluster are 1. Under this circumstance, these answerers X  expertise is actually measured by those clusters X  X  X xpertise X , which will cause many answerers to own the same expertise and thus make the ranking meaningless. LDALM increases Prec @ K of QLLM, which shows the im-pact of utilizing latent topics, but explicit question category provides more help than latent topics as category-sensitive LMs outperform LDALM at various K s. MRR and MAP of these four methods report the similar results and detail will not be provided here.

When turning to MQRT, we find that CBLM works the best, followed by BCS-LM and TCS-LM, while LDALM costsmuchmoretimeininfer ence. CBLM estimates an-swerer expertise through combining answerer X  X  contribution to each cluster (which is pre-computed) and the probability of generating the routed question from each cluster (which is efficient to calculate), thus it makes the fastest estimation. However, the estimation made by CBLM is most inaccurate, as stated above. On the whole, category-sensitive LMs are time-efficient among the four methods.

In summary, category-sensitive LMs give more accurate expertise estimation than CBLM and LDALM and at the same time keep high time-efficiency.
This paper reported here is an investigation of apply-ing question category to QR in CQA services. The ques-tion category was adopted to the development of category-sensitive LMs for estimating answerer expertise. Experi-ments on large-scale real world data revealed that category-sensitive LMs obtained more accuracies of expertise esti-mation, relative to QLLM and state-of-the-art algorithms including CBLM and LDALM. Results of experiments have proven that higher accuracies with lower costs are achieved due to the inclusion of question category in routing ques-tions, which have therefore provided empirical evidence to validate the incorporation of question category in QR for CQA services. In future work, effects of question category on the content quality of answers and questions in CQA services can be further detected.
 This work is supported by two grants from the Research Grants Council of the Hong Kong SAR, China (Project No. CUHK 413210 and Project No. CUHK 415410) and a grant supported by a research funding from Google Focused Grant Project  X  X obile 2014 X .
