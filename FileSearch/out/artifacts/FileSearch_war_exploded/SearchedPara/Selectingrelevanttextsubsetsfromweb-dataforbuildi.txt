 One of the main challenges in the rapid deplo yment of NLP applications is the lack of in-domain data required for training statistical models. Language models, especially n-gram based, are key compo-nents of most NLP applications, such as speech recognition and machine translation, where the y serv e as priors in the decoding process. To estimate a n-gram language model we require examples of in-domain transcribed utterances, which in absence of readily available rele vant corpora have to be col-lected manually . This poses severe constraints in terms of both system turnaround time and cost. This led to a gro wing interest in using the World Wide Web (WWW) as a corpus for NLP (Lapata, 2005; Resnik and Smith, 2003). The web can serv e as a good resource for automatically gathering data for building task-specic language models. Web-pages of interest can be identied by generating query terms either manually or automatically from an initial set of in-domain sentences by measures such as TFIDF or Relati ve Entrop y (R.E). These webpages can then be con verted to a text corpus (which we will refer to as web-data ) by appropri-ate preprocessing. Ho we ver text gathered from the web will rarely t the demands or the nature of the domain of interest completely . Ev en with the best queries and web cra wling schemes, both the style and content of the web-data will usually dif fer sig-nicantly from the specic needs. For example, a speech recognition system requires con versational style text whereas most of the data on the web is literary .

The mismatch between in-domain data and web-data can be seen as a semi-supervised learning prob-lem. We can model the web-data as a mix of sen-tences from two classes: in-domain ( I ) and noise ( N ) (or out-of-domain). The labels I and N are la-tent and unkno wn for the sentences in web-data but we usually have a small number of examples of in-domain examples I . Selecting the right labels for the unlabeled set is important for beneting from it. Recent research on semi-supervised learning sho ws that in man y cases (Nigam et al., 2000; Zhu, 2005) poor preprocessing of unlabeled data might actually lower the performance of classiers. We found sim-ilar results in our language modeling experiments where the presence of a lar ge set of noisy N ex-amples in training actually lowers the performance slightly in both perple xity and WER terms. Recent literature on building language models from text ac-quired from the web addresses this issue partly by using various rank-and-select schemes for identify-ing the set I (Ostendorf et al., 2005; Sethy , 2005; Sarikaya, 2005). Ho we ver we belie ve that simi-lar to the question of balance (Zhu, 2005) in semi-supervised learning for classication, we need to ad-dress the question of distrib utional similarity while selecting the appropriate utterances for building a language model from noisy data. The subset of sen-tences from web-data which are selected to build the adaptation language should have a distrib ution sim-ilar to the in-domain data model.

To address the issue of distrib utional similarity we present an incremental algorithm which compares the distrib ution of the selected set and the in-domain examples by using a relati ve entrop y (R.E) criterion. We will revie w in section 2 some of the ranking schemes which pro vide baselines for performance comparison and in section 3 we describe the pro-posed algorithm. Experimental results are pro vided in section 4, before we conclude with a summary of this work and directions for the future. The central idea behind text cleanup schemes in re-cent literature, on using web-data for language mod-eling, is to use a scoring function that measures the similarity of each observ ed sentence in the web-data to the in-domain set and assigns an appropri-ate score. The subsequent step is to set a threshold in terms of either the minimum score or the num-ber of top scoring sentences. The threshold can usu-ally be x ed using a heldout set. Ostendorf (2005) use perple xity from an in-domain n-gram language model as a scoring function. More recently , a mod-ied version of the BLEU metric which measures sentence similarity in machine translation has been proposed by Sarikaya (2005) as a scoring function. Instead of explicit ranking and thresholding it is also possible to design a classier in a learning from pos-itive and unlabeled examples frame work (LPU) (Liu et al., 2003). In this system, a subset of the unla-beled set is selected as the negati ve or the noise set N . A two class classier is then trained using the in-domain set and the negati ve set. The classier is then used to label the sentences in the web-data. The classier can then be iterati vely rened by us-ing a better and lar ger subset of the I/N sentences selected in each iteration.

Rank ordering schemes do not address the issue of distrib utional similarity and select man y sentences which already have a high probability in the in-domain text. Adapting models on such data has the tendenc y to skew the distrib ution even further to-wards the center . For example, in our doctor -patient interaction task short sentences containing the word `okay' such as `okay',`yes okay', `okay okay' were very frequent in the in-domain data. Perple xity or other similarity measures give a high score to all such examples in the web-data boosting the prob-ability of these words even further while other perti-nent sentences unseen in the in-domain data such as `Can you stand up please?' are rank ed low and get rejected. To address the issue of distrib utional similarity we developed an incremental greedy selection scheme based on relati ve entrop y which selects a sentence if adding it to the already selected set of sentences reduces the relati ve entrop y with respect to the in-domain data distrib ution.

Let us denote the language model built from in-domain data as P and let P init be a language model for initialization purposes which we estimate by bagging samples from the same in-domain data. To describe our algorithm we will emplo y the paradigm of unigram probabilities though the method general-izes to higher n-grams also.

Let W ( i ) be a initial set of counts for the words i in the vocab ulary V initialized using P init . We de-note the count of word i in the j th sentence s web-data with m ber of words in the sentence and N = P the total number of words already selected. The rel-ative entrop y of the maximum lik elihood estimate of the language model of the selected sentences to the initial model P is given by
If we select the sentence s
H ( j ) =  X  X
Direct computation of R.E using the abo ve ex-pressions for every sentence in the web-data will have a very high computational cost since O ( V ) computations per sentence in the web-data are re-quired. Ho we ver given the fact that m we can split the summation H ( j ) into
Intuiti vely , the term T 1 measures the decrease in probability mass because of adding n more to the corpus and the term T 2 measures the in-domain distrib ution P weighted impro vement in probability for words with non-zero m
For the R.E to decrease with selection of sentence s we require T 1 &lt; T 2 . To mak e the selection more rened we can impose a condition T 1 + thr ( j ) &lt; T 2 where thr ( j ) is a function of j . A good choice for thr ( j ) based on empirical study is a function that declines at the same rate as the ratio ln ( N + n j ) n /N  X  1 /kj where k is the average number of words for every sentence.

The proposed algorithm is sequential and greedy in nature and can benet from randomization of the order in which it scans the corpus. We generate per -mutes of the corpus by scanning through the corpus and randomly sw apping sentences. Ne xt we do se-quential selection on each permutation and mer ge the selected sets.

The choice of using maximum lik elihood estima-tion for estimating the intermediate language mod-els for W ( j ) is moti vated by the simplication in the entrop y calculation which reduces the order from O ( V ) to O ( k ) . Ho we ver, maximum lik elihood esti-mation of language models is poor when compared to smoothing based estimation. To balance the com-putation cost and estimation accurac y, we modify the counts W ( j ) using Kneser -Ne y smoothing pe-riodically after x ed number of sentences. Our experiments were conducted on medical do-main data collected for building the English ASR of our English-Persian Speech to Speech translation project (Geor giou et al., 2003). We have 50K in-domain sentences for this task available. We down-loaded around 60GB data from the web using au-tomatically generated queries which after ltering and normalization amount to 150M words. The test set for perple xity evaluations consists of 5000 sen-tences(35K words) and the heldout set had 2000 sentences (12K words). The test set for word er-ror rate evaluation consisted of 520 utterances. A generic con versational speech language model was built from the WSJ, Fisher and SWB corpora in-terpolated with the CMU LM. All language models built from web-data and in-domain data were inter -polated with this language model with the interpola-tion weight determined on the heldout set.

We rst compare our proposed algorithm against baselines based on perple xity(PPL), BLEU and LPU classication in terms of test set perple xity . As the comparison sho ws the proposed algorithm outper -forms the rank-and-select schemes with just 1/10th of data. Table 1 sho ws the test set perple xity with dif ferent amounts of initial in-domain data. Table 2 sho ws the number of sentences selected for the best perple xity on the heldout set by the abo ve schemes. The average relati ve perple xity reduction is around 6%. In addition to the PPL and WER impro vements we were able to achei ve a factor of 5 reduction in the number of estimated language model parameters (bigram+trigram) and a 30% reduction in the vocab-No Web 60 49.6 42.2 39.7 AllW eb 57.1 48.1 41.8 38.2 PPL 56.1 48.1 41.8 38.2 BLEU 56.3 48.2 42.0 38.3 LPU 56.3 48.2 42.0 38.3 Proposed 54.8 46.8 40.7 38.1 Table 1: Perple xity of testdata with the web adapted model for dif ferent number of initial sentences. ulary size. No Web refers to the language model built from just in-domain data with no web-data. All-Web refers to the case where the entire web-data was used.

The WER results in Table 3 sho w that adding data from the web without proper ltering can actually harm the performance of the speech recognition sys-tem when the initial in-domain data size increases. This can be attrib uted to the lar ge increase in vo-cab ulary size which increases the acoustic decoder perple xity . The average reduction in WER using the proposed scheme is close to 3% relati ve. It is inter -esting to note that for our data selection scheme the perple xity impro vments correlate surprisingly well with WER impro vments. A plausible explanation is that the perple xity impro vments are accompanied by a signicant reduction in the number of language model parameters. In this paper we have presented a computationally efcient scheme for selecting a subset of data from an unclean generic corpus such as data acquired from the web . Our results indicate that with this scheme, we can identify small subsets of sentences (about 1/10th of the original corpus), with which we can build language models which are substantially smaller in size and yet have better performance in PPL 93 92 91 91 BLEU 91 90 89 89 LPU 90 88 87 87 Proposed 12 11 11 12 Table 2: Percentage of web-data selected for dif fer -ent number of initial sentences. No Web 19.8 18.9 18.3 17.9 AllW eb 19.5 19.1 18.7 17.9 PPL 19.2 18.8 18.5 17.9 BLEU 19.3 18.8 18.5 17.9 LPU 19.2 18.8 18.5 17.8 Proposed 18.3 18.2 18.2 17.3 Table 3: Word Error Rate (WER) with web adapted models for dif ferent number of initial sentences. both perple xity and WER terms compared to models built using the entire corpus. Although our focus in the paper was on web-data, we belie ve the proposed method can be used for adaptation of topic specic models from lar ge generic corpora.

We are currently exploring ways to use multiple bagged in-domain language models for the selection process. Instead of sequential scan of the corpus, we are exploring the use of rank-and-select methods to give a better search sequence.

