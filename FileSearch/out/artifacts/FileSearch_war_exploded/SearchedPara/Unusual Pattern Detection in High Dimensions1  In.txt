 Data mining is the process of discovering meaningful nontrivial patterns in large data sets. In this field, clustering analys is plays an important role. The cluster-ing algorithms divide the similar observations into groups in order to extract the common patterns of the data. In order to learn the general patterns, small clusters and nondominant patterns are discarded or simply undetected . Despite their relatively small size, these cluster s may be invaluable because their nondom-inant patterns may reveal important knowledge. Network Intrusion, malicious computer activity and fraudulent transaction detection are the typical applica-tions for this kind of problem [1]. Recentl y, outlier detectio n has emerged as an approach for discovering nondominant patterns by measuring the deviation of the outliers from the norm [2,3,4].The top outliers will be the most interesting ones. However, outlier detection has two major drawbacks. First, the false alarm rate is high. Even for a good outlier detection algorithm that can discover all true outliers in terms of the deviation from the norm, most outliers except for extreme ones are unimportant.This is the nature of an outlier detection approach because the outliers are defined based on the distance between the outliers and the norm. Noise is also classified as an outlier for its deviation from the norm. In real life, domain experts are often required to investigate and analyze the outliers manually in order to understand their meaning. If the algorithm returns many outliers, which is likely in large and heterogeneous data sets, this approach becomes difficult when the interesting ou tliers do not always appear in the top outliers. Another drawback is that each point can be considered an outlier by it-self in high dimensions. The volume of the data space grows exponentially when the dimensionality increases [5]. In other words, the data is very sparse and the density of the data set approaces zero. As a result, except for extreme outliers, using an outlier detection method to discover novel patterns is difficult in high dimensional and heterogeneous data.

To overcome the limitatio ns associated with outlie r detection, we propose to use the number of similar surrounding observations that deviate from others as a metric to measure the level of interestin gness instead of the degree of deviation metric. From this perspective, all non extreme outliers are equal even though their rankings are different because mos tobservationsinhighdimensionsare outliers by themselves. However, when some outliers start to form a small cluster, they are not simply noise and do not appear as outliers accidentially. They indicate interesting unusual behaviors in the data set. One may ask if we can apply several clustering algorithms on the top outliers to cluster them in order to discover their pattern. As shown in the experiments, it is not the case. The interesting observations can be the outliers with low rankings and they are often removed from the list of top outliers. As a result, the clustering algorithms can not detect those imp ortant clusters.

In this paper, we introduce an algorithm that can discover small clusters in high dimensional and heterogenous datasets. We have shown that our algorithm can effectively discover these clusters. In addition, our algorithm has discov-ered novel patterns based on our propose d metric of interestingness for unusual observations The closest work to our approach is that of density-based clustering algorithms. Among the well-known algorithms,i.e. DBSCAN, Chamelon, CURE, shared nearest neighbor (SNN), SNN [6] shows the best performance because it can find clusters of different sizes, shapes and densities. This algorithm is based on the concept of core points and the number of strong links of the core points. A point is a core point if the number of st rong links exceeds a given threshold. A core point will be the representative of the cluster. Any point that has the strength of the link with a core point ex ceeding a certain threshold will be in the same cluster as the core point. Finally, a point that has a neighbor in a cluster and the strength of the link between them is greater than a threshold will be put into the same cluster as its neighbor. The algorithm works very well for a two dimensional data set. The algorithm can find small sized clusters but it is sensitive to the deviation of a point from its neighbors. In high dimensions, the clusters are broken into many tiny clusters. In contrast, our algorithm separates a point from a cluster only if it deviates largely from the other points, which makes the algorithm more suitable for unsual pattern detection.

Another similar work is outlier detectio n. Recently, Breunig et al [2] intro-duced the local based outlier factor (LOF) which can detect local outliers. The main idea is that if a point is inside a tight cluster, its LOF value should be close to one and if it is outside the cluster or non-tight area, LOF should be greater than one. A point with LOF greater than one is an outlier. In high di-mensionality, the clusters are no longer tight as assumed [5] and LOF becomes unbounded. Virtually, most observations are local outliers according to LOF. Our approach is based on a variation of k-nearest neighbors (KNN) and the concept of dual-neighbor to cluster the data set. In clustering, KNN is used to cluster the data set by constructing a list of k nearest neighbors for each point in the data set. The distance from a point to its k th -nearest neighbor is considered as its neighborhood distance. A point and its neighbors are considered to be similar to each other. The definition of similarity can be misleading since the close points may not be actually close to each other as illustrated in figure 1(a). Point q belongs to a dense region while point p is in a less dense region. With k = 5, s is in the list of KNNs of p and s is considered to be similar to p. However, as shown in the figure, s is not similar to p because the distance between q and its nearest neighbors is less than that between q and p. Those two dissimilar points will be in the same cluster.

To solve the problem, Jarvis and Patrick introduced the concept of shared nearest neighbor [7]. The strength of the similarity between two points is mea-sured by the number of nearest neighbors shared between them. Two points belong to the same cluster if the strengt h of the link exceeds a certain threshold. The clustering algorithm can produce excellent results. However, it is non-trivial to select an approriate value of k and to justify the results of SNN in high dimen-sions. Ertoz et al improved the SNN by introducing the topic threshold. A point with the number of strong links exceeding t he topic threshold will represent its neighbors. Their clustering algorithm is based on the number of strong links and the link strength of the point in the data set. In high dimensions, the points in the small clusters can not have the number of strong links sufficient enough to form a cluster. The points in this cluster will be broken into smaller clusters even though they may be only slightly different from other points. Another problem is that the parameter k is the same for all points in data set. As illustrated in figure 1(a), the result will be inconsistent with a global parameter k. Figure 1(a) illustrates a simplified case when k is small. In the figure, the distance from p to its 4 th -nearest neighbor is twice the distance from q to its 4 th -nearest neighbor even though the distance from p and q to their 2 th nearest neighbors are the same. The volumes of k-distances of p and q will be different signficantly with a small increase in k.
In this paper, we propose the use of adaptive nearest neighbors (ANN) to define the neighborhood distance. The approach has three parameters to fine tune the adaptive neighborhood distan ce. From our perspective, the concept of neighhood distance of a point, say p, is a relative concept since it can not be de-fined without surrounding points. As illustrated in figure 1(a). the neighborhood distance of p is greater than that of q because the first two nearest neighbors of p are farther than those of q.

With this observation, the first few nearest neighbors are used to define the initial neighborhood distance . Those neighbors are called the initial neighbors or i-neighbors in short. The distance from p to its i-neighbors is called i-distance . The i-distance defines the minimum neighborhood distance of p regardless of k. When p is in a dense cluster, the i-distance tends to be smaller.

The next parameter,  X  , is used to control the local variation of the neighbor-hood distance around p. In figure 1(b), r, s and t are i-neighbors of p whereas q is the 4 th nearest neighbor of p. First, we project r, s and t on the line passing two points p and q. ph is chosen for it is the longest projected line segment of r,s and t on pq. If the ratio between the line segment pq and ph is less than  X  , then q is included in the adaptive nearest neighbor list of p, denoted by ANN(p). This process is repeated until there is a point w in the k-nearest neighbor list of p whose ratio is greater than  X  . Point w and all the nearest neighbors of p farther than w are excluded from the adaptive nearest neighbor list of p. Point w is called the boundary point .  X  controls the local maximum variation of the nearest neighbors. The idea of  X  is that the neighbor should be excluded from the list of nearest neighbors when it is signifcantly different from the others in the list and  X  measures the level of differences.

The last parameter is to adjust the granularity level . For the small data set as in figure 1(a), it makes sense to cluster it into two distinct clusters. But in a larger data set, the two clusters should be merged into one if the distinction between them is small compared with others. The boundary points can be used for controlling the granularity. We use the parameter z for this. The procedure for constructing the lists of ANNs is modified as follows. Instead of stopping the construction of ANN list for p when a boundary point is reached, we continue to put it into the ANN list of p. The process is stopped when z equals the number of times we reach the boundary points. The algorithm achieves the finest granularity level when z = 1. The detailed procedure for constructing the ANN list is described in algorithm 1.. In algorithm 1., s is the number of i-neighbors and z is the granularity tuning parameter. Also, k is the maximum number of nearest neighbors that are computed for a point p.

With adaptive nearest neighbors, we can define the neighborhood distance of a point independent of k with different lev els of granularity. This neighborhood distance is called the adaptive neighbor distance, denoted by a  X  distance .Ac-cording to the discussion above, we can say that any point within the adaptive neighborhood of a point p is truly a natural neighbor of p. Also, we observe that the similarlity must be a mutual relation. In other words, if two points are considered naturally close to each other, they should be in the list of ANNs of each other. We formally define the closeness as follows: Definition 1. Given any two points p and q in dataset D, p and q have a dual-neighbor relationship, denoted by dual ( p, q )  X  true ,ifandonlyif p  X  AN N ( q ) and q  X  AN N ( p ) In the definition, two points are considered neighbors to each other when they have a dual-neighbor relationship. With this definition, we can address the prob-lem of KNN as illustrated in 1(a) when p and q have no dual-neighbor relation-ship where z = 1. In a coarser granularity level, i.e. z = 2, p and q become neighbors to each other. Another useful concept is the indirect dual-neighbor relationship.
 Definition 2. Given any two points p and q in dataset D, p and q have an indirect dual-neighbor relationship, denoted by indual ( p, q )  X  true ,ifandonly if As discussed above, we are interested in discovering unusual patterns. With the definition of the indirect dual-neighbor, we can formally define the concepts of usual and unusual patterns as follows: Definition 3. An observation is usual if it belongs to a large group of obser-vation where each observation has at least one observation in the group that is similar to it. The group that contains those usual observations is called a usual pattern.
 Definition 4. An observation is said to have an unusual pattern if it belongs to a small group of observations where each observation has at least one observation in the group that is similar to it and has no other observation outside of the group similar to it. The group that contains those unusual observations is called an unusual pattern.
 In this paper, ANN and the dual-neighbor are used to define the similarity between two points. The indirect dual-neighbor shows the indirect similarity between two observations belonging to the same pattern. With the definitions of unusual pattern, the clustering criteria of our approach is stated as follows:
Given two points p, q in dataset D, p and q belong to the same cluster C if and only if indual ( p, q )  X  true . This definition implies the chain affect and it can produce very large clusters. This, however, is acceptable because the ob servations in large c lusters are usual. As mentioned above, we are interested in discovering unusual patterns. To be unsual, the observations should deviate from other usual patterns. Therefore, the chain affect will have no impact on the results for discovering unusual patterns.
The parameters i -neighbors,  X  and z play an important role in defining the level of similarity. In a uniformly distributed region, the choice of the number of i-neighbors has less affect since all points should belong to the same cluster. The concept of i-neighbor is useful in non-uni formly distributed regions. In this case, the number of i-neighbors should be small, which is usually less than 10. The parameter  X  is used to control the local variance of the neighborhood distance according to the i-neighbors. The parameter  X  defines the upperbound of the acceptable deviation between the n eighbors. The last parameter is z which is used for adjusting the level of granularity. When z =1,wecanseeallnatural clusters in terms of ANN. When z is increased, the nearb y clusters are merged together. In practice, the n umber of i-neighbors and  X  are less important than z since they can be easily selected withou t affecting the results. Intuitively, we can set  X  =1 . 2and z  X  [2 , 4].
 Algorithm 1. Adpative Nearest Neighbors
Algorithm 2. shows the linear time pro cessing steps to cluster the data set after the lists of adaptive nearest neigh bors have been computed according to algorithm 1.. For every unclustered po int, we randomly select a point to form a new cluster where the selected point is t he representative of the cluster. Then, we expand the cluster by including all the dual-neighbors and the indirect dual-neighbors of the point into the cluster. To facilitate the algorithm, we create a stack S to store the dual-neighbors. As shown in steps 11-12, an unclustered point p is removed from the data set. Since p does not belong to any cluster, a new cluster C is created for p before pushing p onto stack S. In steps 13-16, a point q is popped from S and q is added to cluster C. Besides, all dual-neighbors of q are pushed onto the stack. Those steps are repeated until S is empty, which means the inner while loop is stopped when indirect dual-neighbors of the points in cluster C are included in the cluster.
 Algorithm 2. Outcast Pseudocode In this section, we present e xperimental results using the SAM X  X  Club data set [8]. The data set contains the sales transaction data for 18 Sam X  X  club stores between the dates of January 1 and January 31, 2000. From the sales transactions, we create a new data set of 34,250 tuples with 31 attributes. Each tuple represents a sale item and the attributes represent the total sales quantities for each individual item between the dates of January 1. The total sale varies from 0 to 16,788. The purpose of this experiment is to apply the well-known local outlier detection method LOF and the density-based clustering algorithm SNN on the data set in order to detect any unusual sales patterns. We first ran LOF on the data set to determine the top local outliers. We then ran KMEAN and SNN on the top 5% outliers to produce a summary of the outliers. We also ran SNN on the whole data set with different values of k in the attempt to discover unusual patterns by studying the small clusters returne d by SNN. We then compared the results with those from our alogirthm. 4.1 LOF, KMEAN and SNN Figure 2(a) shows the average sales for each day in January for all items in the dataset. According to the figure, the sal e follows the same pattern every week. Sales gradually decrease from the start of the week toward the middle of the week and then slightly increase toward the end of the week before achieving its peak on Saturday. The sales quickly drops on Sunday. This pattern repeats every week in January. The figure illustrates that most customers tend to go shopping on Saturdays.

For the first test, we computed the LOF values for all items. The LOF values vary greatly from 0.012 to 2681.28. There are 13990 items with LOF greater than 2 and 8495 items with LOF greater than 10. According to the LOF algorithm [2], most of items in the dataset are outliers. This confirms our remark that most data points become outliers in high dimensions due to the space sparsity. The values of the top 10 outliers and their sale information are shown in table 1(a) and figure 2(b). The strongest outlier is item 1 whose pattern deviates from the norm since its sales increase slightly on Saturdays and tends to fall toward the end of the month. For the next 9 outliers ranked by the LOF approach, the sale pattern resembles that in figure 2(a).

We take the top 5% of the items ranked by LOF to form a new dataset with the size of 1712 and then apply several clustering algorithms on the new data set. The purpose is to group the top outliers together in order to learn the common patterns of these outliers in an attempt to explain their significance. In this experiment, we use KMEAN and SNN to cluster the dataset.

Figure 3(a) shows the average sales amount and its standard deviation for items in the clusters clustered by KME AN when k = 20. According to figure 3(a), KMEAN clusters the outliers into groups with different ranges of sale volume (less than 500, 500, 1000 and 1500) and the average size of the clusters is 85.6. The sale patterns for those clusters are the same as the common pattern of the whole data set. Similar results are obtained when we ran KMEAN with different values of k.
Figure 3(b) shows the results of SNN when k = 20. There are 32 clusters with the average size of 53.5. Clusters 1 and 17 are two main clusters with the size of 791 and 100 respectively. The average sale of cluster 1 ranges from 32.3 to 89.3 and its standard deviation ranges from 167 to 419.4. The sales volume of the items in the cluster are quite different even though they belong to the same cluster. As illustrated, the sales pattern of the clusters resembles the common sales pattern of the whole data set.

In the next experiment, we ran SNN on the whole data set, varying k from 20 to 200. Table 1(b) shows the list of clusters with the size greater than 30 for each k. In table 1(b),  X  is the average sales for each cluster and  X  is the average standard deviation of the sales for the dates in January. We found that most items form a cluster by themselves a nd that there are at most two clusters with the size greater than 30 for each k. Also, the fourth and fifth columns of table 1(b) show that  X  is twice  X  . It means that the sale quantity varies greatly for the items in the same clusters as shown in figure 4(a). Consequently, We have found no interesting patterns in this experiment. 4.2 Outcast Table 2(a) shows the size of the interesting clusters found by our algorithm with the granularity level 2. There is one major cluster with the size of 6203 and 16 small clusters with their size ranging from 40 to 209. Among them, cluster 1 and 110 (figure 4(b)) have sale patterns that resemble the common pattern. We found that the top 14 outliers recognized by LOF belong to cluster 1 and that 51 out of 58 items in cluster 1 are in the top 100 outliers.

Cluster 241 with the size of 49 is the most interesting pattern found by our algorithm. Figure 5(a) shows the sales pattern of the items in the cluster. Even though the average sale volumes of the items vary from 80.74 to 389.35, they fol-low the same pattern which is reversed from the common sale pattern (fig.2(a)). The sale achieves the peak at the beginning of the week instead of on Satur-day and then slightly decr eases toward the weekend before reaching its lowest on Sunday. It is interesting to find that all the sales in the second week of the items in this cluster jump sharply on Friday instead of Saturday as the common pattern and then the sale drops quickly on the Saturday and Sunday. The sales on this day is almost double the sales on the peaks of the other weeks. When we further investigate the items in the clusters, we found that all of those items are cigarettes. Table 2(b) shows the top highest and lowest LOF values for items in cluster 241. Even though the items have interesting sales patterns, their LOF ranking is very low. The item with highest rank in the cluster is item 3175 and its rank is 10974 th . The ranking varies greatly from 15111 th to 10974 th despite the fact that the sale patterns are very similar for those items.

Two other interesting patterns occur in clusters 93 and 652 as shown in figure 5(b). Even though cluster 93 res embles the weekly common sales pattern in the way that the sale is highest on Saturday as compared with the other days in the same week, the overall sales in ev ery week tends to decrease toward the end of the month. In contrast, items in cluster 652 almost have no sale for the first three weeks. In the last week, the sales increase rapidly toward the end of the month and achieve their peak on the last Saturday of the month. Figure 6(a) shows the sale pattern for clusters 363 and 663. Those two clusters are similar to clusters 93 and 652 except that the sales for those clusters are four times less than that of clusters 93 and 652.

Figure 6(b) shows the sale patterns for clusters 60, 463, 444 and 331. Cluster 444 contains 209 items and those items have almost no sales except for a few sales on the last Saturday. The other clusters are less interesting than the ones mentioned above due to their small sale volume.

In summary, we ran experiments with different combinations of the outlier detection and clustering algorithms. With LOF, most items in the data set were classified as outliers. When examing the top 10 outliers. We found that the sales pattern of the top outlier is slightly different from the common weekly sales pattern. The top 10 outliers have high sale volumes and their sales pattern follow the weekly pattern. We then clustered the dataset with KMEAN and SNN. Those clustering algorithms divide the top 5% of the outliers into groups of different sale volumes but no interesting patterns are found. It is the same when we ran SNN on the whole data set. However, when we tested the data set with our algorithm, we discover six unusual patterns. Among them, the sale pattern of cluster 1 does not differ from the weekly sales pattern. We found that 89% of the items in the cluster are in the top 100 outliers ranked by LOF. Cluster 241 is the most interesting since we found that cigarrete sales follow the Friday sales pattern rather than the Saturday pattern. The other four clusters do not follow the common sales pattern. The experiment confirms that interesting patterns may not be discovered by simply clustering the top outliers. Clustering and outlier detection are two different approaches that can be used to learn general patterns and novel events. However, both of these approaches can not detect unusual patterns that appear in small clusters, which may be interesting. For most clustering algorit hms, small size clusters are sacrified in order to discover large size clusters. In contrast, the outlier detection approach simply focuses on single outliers rather than groups of outliers. Top outliers are the most interesting events. In our experiments, we have shown that top outliers are not always interesting since they may simply be noise in high dimensions, all data points may be considered outliers due to the sparsity of the data.
In this paper, we present an alternative approach for knowledge learning by introducing the concept of an unusual pattern, based on the size of the small clusters and their deviation from the common patterns. We have developed an algorithm to detect those unusual patterns. The parameters of the algorithm are used to adjust the granularity level of the output. Our experiments on a real world data set show that our algorithm can discover interesting unusual patterns which are undetected by two well-known oultier detection and clustering techniques , namely LOF and SNN, and their combination.
 This research was funded in part by a grant from the Vietnam Education Foun-dation (VEF). The opinions, findings, and conclusions stated herein are those of the authors and do not necessarily reflect those of VEF.

