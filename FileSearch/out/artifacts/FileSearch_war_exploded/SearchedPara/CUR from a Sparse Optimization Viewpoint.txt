 Department of Statistics CUR decompositions are a recently-popular class of randomi zed algorithms that approximate a data rank decomposition is also shared by sparse PCA (SPCA) metho ds, which are optimization-based procedures that have been of interest recently in statistic s and machine learning. Although CUR and SPCA methods start with similar motivation s, they proceed very differently. For example, most CUR methods have been randomized, and they tak e a purely algorithmic approach. By contrast, most SPCA methods start with a combinatorial op timization problem, and they then SPCA approaches are related. It is the purpose of this paper t o understand CUR decompositions from a sparse optimization viewpoint, thereby elucidating the connection between CUR decompo-sitions and the SPCA class of sparse optimization methods.
 To do so, we begin by putting forth a combinatorial optimizat ion problem (see (6) below) which the SPCA viewpoint; and second, CUR is implicitly optimizin g a regression-type objective. These non-randomized optimization-based version of CUR (see Pro blem 1: GL -R EG in Section 3) that is based on a convex relaxation of the CUR combinatorial optimi zation problem; (b) second, we show that, in contrast to the original PCA-based motivation for C UR, CUR X  X  implicit objective cannot we propose an SPCA approach (see Problem 2: GL -SPCA in Section 5) that achieves the sparsity structure of CUR within the PCA framework. We also provide a b rief empirical evaluation of our two proposed objectives. While our proposed GL -R EG and GL -SPCA methods are promising in our goal is to use them to help clarify the connection between CUR and SPCA methods. We conclude this introduction with some remarks on notation . Given a matrix A , we use A A Finally, we let L In this section, we provide a brief background on CUR and SPCA methods, with a particular em-an input matrix X , Principal Component Analysis (PCA) seeks the k -dimensional hyperplane with the lowest reconstruction error. That is, it computes a p  X  k orthogonal matrix W that minimizes Writing the SVD of X as U X V T , the minimizer of (1) is given by V of columns of X , which is equivalent to V 2.1 CUR matrix decompositions CUR decompositions were proposed by Drineas and Mahoney [12 , 4] to provide a low-rank approx-imation to a data matrix X by using only a small number of actual columns and/or rows of X . Fast variants [17] have also been considered. Observing that the best rank-k approximation to the SVD randomness as computational resources to obtain strong qua lity-of-approximation guarantees. Theorem 1 (Relative-error CUR [12]) . Given an arbitrary matrix X  X  R n  X  p and an integer k , with probability at least 1  X   X  , where X The algorithm referred to by Theorem 1 is very simple: 1) Compute the normalized statistical leverage scores , defined below in (3). 3) Return the n  X  c matrix X I consisting of these selected columns.
 be the top-k right singular vectors of X . Then the normalized statistical leverage scores are for all i = 1 , . . . , p , where V the data matrix [8, 12]. 2.2 Regularized sparse PCA methods problem which explicitly encourages sparsity in V based on a Lasso constraint [18]. d X  X spremont et al. [2] take a similar approach, but instead formulate the probl em as an SDP. begin by formulating PCA as the solution to a regression-typ e problem.
 be p  X  k matrices. Then, for any  X  &gt; 0 , let Then, the minimizing matrices A  X  and V  X  s = 1 or  X  1 .
 That is, up to signs, A  X  consists of the top-k right singular vectors of X , and V  X  an
L 1 penalty on W : where || W || solution V  X  takes a purely algorithmic approach to the problem of approx imating a matrix in terms of a small From Theorem 1, it is clear that CUR seeks a subset I of size c for which min following combinatorial optimization problem: X . Notice that relaxing |I| = c to |I|  X  c does not affect the optimum. This optimization problem is analogous to all-subsets multivariate regression [7], w hich is known to be NP-hard. However, by using ideas from the optimization literature we can approximate this combinatorial the constraint in (7) is the same as finding some subset I with |I| X  c such that B corresponding to CUR. First notice that (7) uses an L However, we can approximate the L convex heuristic proposed by Yuan et al. [20] that encourages prespecified groups of parameters to be simultaneously sparse. Thus, the combinatorial probl em in (6) can be approximated by the following convex (and thus tractable) problem: Problem 1 ( Group lasso regression: GL -R EG ). Given an arbitrary matrix X  X  R n  X  p , let B  X  R p  X  p and t &gt; 0 . The GL -R EG problem is to solve where t is chosen to get c nonzero rows in B  X  . Since the rows of B are grouped together in the penalty P p for sparse inverse covariance estimation [6, 15].) whether CUR could be seen as an SPCA-type method. So far, we ha ve established CUR X  X  con-nection to regression by showing that CUR can be thought of as an approximation algorithm for the PCA, and we show that CUR cannot be directly cast as an SPCA met hod.
 On the other hand, PCA-type methods find a set of directions W that minimize Here, unlike in (1), we do not assume that W is orthogonal, since the minimizer produced from SPCA methods is often not required to be orthogonal (recall S ection 2.2).
 cases by taking B = I We have seen in Section 3 that CUR effectively requires B to be row-sparse; in the standard PCA setting, W is taken to be rank k (with k &lt; p ), in which case (10) is minimized by V the optimal value ERR ( V are imposed, consider the 2-dimensional toy example in Figu re 1. In this example, we compare B which is a simple linear regression, represented by the blac k thick line and minimizing the sum direction, which minimizes ERR ( W ) among all rank-one matrices W . Here, ERR ( W ) is the sum optimizing a PCA-type objective such as (10). Next, we will m ake this intuition more precise. The first step to showing that CUR is an SPCA method would be to p roduce a matrix V CUR for approximation. However, this equality implies L CUR could be regarded as implicitly performing sparse PCA in the sense that (a) V CUR is sparse; and (b) by Theorem 1 (with high probability), ERR ( V CUR )  X  (1 +  X  ) ERR ( V of such a V CUR would cast CUR directly as a randomized approximation algor ithm for SPCA. How-exist a matrix V CUR for which ERR ( V CUR ) = || X  X  X I X I + X || theorem is that CUR cannot be directly viewed as an SPCA-type method.
 Theorem 3. Let I  X  X  1 , . . . , p } be an index set and suppose W  X  R p  X  p satisfies W unless L imposed. Left: regression with row-sparsity constraint (b lack) compared with PCA with low rank the dotted lines.
 Proof. The last inequality is strict unless X I X I + X I c = 0 . Although CUR cannot be directly cast as an SPCA-type method, in this section we propose a sparse PCA approach (which we call the group lasso SPCA or GL -SPCA) that accomplishes something tivated by the following two observations about CUR. First, following from the definition of the leverage scores (3), CUR chooses columns of X based on the norm of their corresponding rows of V Second, as we have noted in Section 4, if CUR could be expresse d as a PCA method, its principal those columns of X .
 Recall that Zou et al. [21] obtain a sparse V  X  by including in (5) an additional L the optimization problem (4). Since the L it encourages only unstructured sparsity. To achieve the CU R-type row sparsity, we propose the following modification of (4): Problem 2 ( Group lasso SPCA: GL -SPCA). Given an arbitrary matrix X  X  R n  X  p and an integer k , let A and W be p  X  k matrices, and let  X ,  X  ( A  X  , V  X  ) = argmin A , W || X  X  XWA T || 2 F +  X  || W || 2 F +  X  1 Thus, the lasso penalty  X   X  be either dense or entirely zero.
 Importantly, the GL -SPCA problem is not convex in W and A together; it is, however, convex in then Algorithm 1 can be used to solve the GL -R EG problem discussed in Section 3. Algorithm 1: Iterative algorithm for solving the GL -SPCA (and GL -R EG ) problems. Input : Data matrix X and initial estimates for A and W Output : Final estimates for A and W repeat 1 Compute SVD of X T XW as UDV T and then A  X  UV T ; 2 Compute b i = P until convergence ; the application of CUR to DNA SNP analysis [14]. The unstruct ured V  X  , by contrast, would not noise columns. On the other hand, requiring such structured sparsity is more restrictive and may not be desirable. For example, in microarray analysis in whi ch we have measured p genes on n p  X  c genes does not allow a different sparse subset of genes to be a ctive in each factor. We finish this section by pointing out that while most SPCA met hods only enforce unstructured explored [9]. Our GL -SPCA problem falls within the broad framework of this idea. thetic and real data. In particular, we compare the randomiz ed CUR algorithm of Mahoney and Drineas [12, 4] to our GL -R EG (of Problem 1), and we compare the SPCA algorithm proposed by Zou et al. [21] to our GL -SPCA (of Problem 2). We have also compared against the SPCA 6.1 Simulations We first consider synthetic examples of the form X = b X + E , where b X is the underlying signal signal b X has one of the following forms:
Case I) b X = [ 0 Case II) b X = UV T where U and V each consist of k &lt; p orthogonal columns. In addition to Case III) b X = UV T where U and V each consist of k &lt; p orthogonal columns. Here V is use the regression-type reconstruction error ERR comparison of SPCA and GL -SPCA, we use the PCA-type error ERR ( V ) = || b X  X  XVV + || Table 1 presents the simulation results from the three cases . All comparisons use n = 100 and are averaged over 5 trials.
 zeros (in parentheses).
 We notice in Table 1 that the two regression-type methods CUR and GL -R EG have very similar two PCA-type methods perform similarly as well. Again, the g roup lasso method seems to work were measuring row-sparsity, methods like SPCA would perfo rm poorly since they do not encourage entire rows to be zero. 6.2 Microarray example honey and Drineas [12] apply CUR to this dataset of n = 31 tissue samples and p = 5520 genes. As with the simulation results, we use two sets of comparison s: we compare CUR with GL -R EG , and we compare SPCA with GL -SPCA. Since we do not observe the underlying truth b X , we take shows ERR SPCA seen in the figure. The algorithm alternates between minimizing with respect t o A and B until convergence. which case the optimization problem becomes min problem was considered by Zou et al. [21], who showed that the solution is obtained by computing Algorithm 1.
 Solving for B given A : If A is fixed, then (11) becomes an unconstrained convex optimiza tion problem in B . The subgradient equations (using that A T A = I SPCA with SPCA (specifically, Zou et al. [21]). where the subgradient vectors s define b can be written as The following claim explains Step 3 in Algorithm 1.
 Claim 1. B Proof. First, if B Since || s direction, recall that B By Claim 1, || A T X T X ( i )  X  b B In this paper, we have elucidated several connections betwe en two recently-popular matrix decom-decompositions. In doing so, we have suggested two optimiza tion problems, GL -R EG and GL -the other hand, CUR methods operate by using randomness and a pproximation as computational re-for applications where the size scale of the data is expected to increase.
 Acknowledgments We would like to thank Art Owen and Robert Tibshirani for enco uragement and helpful suggestions. Jacob Bien was supported by the Urbanek Family Stanford Grad uate Fellowship, and Ya Xu was supported by the Melvin and Joan Lane Stanford Graduate Fell owship. In addition, support from the NSF and AFOSR is gratefully acknowledged. [3] P. Drineas, R. Kannan, and M.W. Mahoney. Fast Monte Carlo algorithms for matrices III: [4] P. Drineas, M.W. Mahoney, and S. Muthukrishnan. Relativ e-error CUR matrix decompositions. [5] S.A. Goreinov and E.E. Tyrtyshnikov. The maximum-volum e concept in approximation by [8] D.C. Hoaglin and R.E. Welsch. The hat matrix in regressio n and ANOVA. The American [11] S. Kumar, M. Mohri, and A. Talwalkar. Ensemble Nystr  X om method. In Annual Advances in [12] M.W. Mahoney and P. Drineas. CUR matrix decompositions for improved data analysis. Proc. [13] T. Nielsen, R.B. West, S.C. Linn, O. Alter, M.A. Knowlin g, J. O X  X onnell, S. Zhu, M. Fero, [14] P. Paschou, E. Ziv, E.G. Burchard, S. Choudhry, W. Rodri guez-Cintron, M.W. Mahoney, and [20] M. Yuan and Y. Lin. Model selection and estimation in reg ression with grouped variables.
