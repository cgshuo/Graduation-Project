 Proactive learning is a generalization of active learning de-signed to relax unrealistic assumptions and thereby reach practical applications. Active learning seeks to select th e most informative unlabeled instances and ask an omniscient oracle for their labels, so as to retrain the learning algori thm maximizing accuracy. However, the oracle is assumed to be infallible (never wrong), indefatigable (always answers) , in-dividual (only one oracle), and insensitive to costs (alway s free or always charges the same). Proactive learning relaxe s all four of these assumptions, relying on a decision-theore tic approach to jointly select the optimal oracle and instance, by casting the problem as a utility optimization problem subject to a budget constraint. Results on multi-oracle op-timization over several data sets demonstrate the superior ity of our approach over the single-imperfect-oracle baseline s in most cases.
 I.2.6 [ Artificial Intelligence ]: Learning X  concept learning, knowledge acquisition ; H.0 [ General ]: [Classification, cost-sensitive active learning] Algorithms, Experimentation Cost-sensitive active learning, decision-theory, multip le ora-cles
In most machine learning domains, unlabeled data is avail-able in abundance, but obtaining class labels or ranking pre f-erences requires extensive human effort, sometimes from ex-perts with very limited availability. For instance, it is ea sy to crawl the web, but much more costly to pay human an-notators to examine carefully the web documents in order to assign topics for cataloging or relevance-based judgmen ts in a document retrieval scenario. It is also simple to col-lect images, but much harder to obtain linguistic content labels. For tasks such as classifying galaxies in the Sloan Sky Catalog, scarce expertise is required. Thus, it is cruci al to design methods that will considerably reduce the labelin g effort without sacrificing a significant loss of generalizati on accuracy.

The active learning paradigm addresses this challenge. In active learning, a few labeled instances are typically pro-vided together with a large set of unlabeled instances. The objective is first to select optimal instance(s) for an exter -nal oracle to label, and then re-run the learning method to minimize prediction error, i.e. to improve performance. The active learning task attempts to optimize learning by selecting the most informative instances to be labeled, whe re informativeness is typically defined as maximal expected im -provement in classification accuracy. Several studies [12, 10, 8, 4, 3] show that active learning greatly helps reduce the labeling effort in various domains. However, active learn-ing relies on unrealistic assumptions, largely swept under the proverbial carpet thus far. For instance, active learn-ing assumes there is a unique omniscient oracle. In real life , it is possible and more general to have multiple sources of information with differing reliabilities or areas of expert ise. Active learning also assumes that the single oracle is perfe ct, always providing a correct answer when requested. In real-ity, though, an  X  X racle X  (if we generalize the term to mean any source of expert information) may be incorrect (fallibl e) with a probability that should be a function of the difficulty of the question. Moreover, an oracle may be reluctant  X  it may refuse to answer if it is too uncertain or too busy. Fi-nally, active learning presumes the oracle is either free or charges uniform cost in label elicitation. Such an assump-tion is naive since cost is likely to be regulated by difficulty (amount of work required to formulate an answer) or other factors. In this paper, we propose proactive learning as a new approach to address these issues. Proactive learning enables active learning to reach practical applications.
We frame the proactive learning challenge as inherently a decision-theoretic problem, and focus on three scenarios . These scenarios are designed to explore different oracle typ es in a multi-oracle setting, i.e. oracles reluctant to give an -swers, oracles that charge non-uniform cost, and fallible o r-acles that might provide wrong answers. We assume that each of these properties can be defined as a function of the query difficulty, i.e. the level of difficulty to classify the sampled instance. Each scenario analyzes a single property ; i.e. reluctance, non-uniform cost and fallibility. In mult i-oracle proactive sampling, it is crucial to select the optim al data instance(s) to be queried as well as the optimal oracle. We achieved promising results on benchmark classification datasets by transforming the problem into expected utility maximization. We further assume a pre-defined and fixed budget; hence, the task becomes a constraint optimization problem. The results demonstrate the effectiveness of joint sampling of the optimal oracle-example pair as compared to sampling with respect to a single oracle.

The remainder of the paper is organized as follows: Sec-tion 2 reviews related work in active learning, and decision theory and some recent work in cost-sensitive active learn-ing. Section 3 describes in detail three scenarios we analyz e and presents the proposed solution to multi-oracle proacti ve learning in classification. Section 4 discusses the experim en-tal results. Finally, we give our conclusions in Section 5.
Proactive learning is a brand new machine learning area, although there is related work in cost-sensitive active lea rn-ing. We review some recent work in this direction and give a broad overview of decision-theory since it provides the mat h-ematical tools necessary to develop algorithms for proacti ve learning.

Statistical decision theory is an appealing framework for proactive learning since it offers a systematic way to repre-sent cost-benefit tradeoffs, in decision-making under uncer -tainty. Uncertainty usually refers to a state of nature, whi ch is typically unknown, but controls the sampling distributi on of the observed data. A decision is defined in terms of a policy  X  : X n  X  A that takes an action based on a set of observations ( x 1 , x 2 , ..., x n ), x i  X  X . Given a non-negative loss function L :  X   X  [0 , +  X  ), each action can be associated with an expected loss (risk): where  X  encodes the parameter that governs the distribution p (  X  ) that generates the observed data ( x 1 , ..., x n tion to the risk, each action  X  is associated with a reward (benefit), denoted by a utility function U (  X  ) whose expecta-tion is taken over all possible outcomes resulting from tak-ing the action. We can denote the cost-benefit tradeoffs of different policies with a utility-risk pair ( U (  X  ) , R (  X  )). The goal of decision-making is, then, to take the best action tha t maximizes the expected utility while minimizing the risk.
We can formulate proactive learning in the above decision-theoretic framework. The decision rule or policy correspon ds to deciding whether or not to elicitate data from an oracle, i.e. querying a set of instances to obtain their labels from a human expert in a classification setting. Taking the elic-itation action introduces a certain expected reward due to the effect of the additional data on improving the learn-ing model, e.g. [10]. However, the elicitation effort incurs a cost, possibly depending on the difficulty of the task. Proac-tive learning, built with tools provided by decision theory , systematically addresses this utility-cost tradeoff for in cre-mentally optimal data selection (Globally optimal selecti on in the general case is NP-hard).

Although active learning has received great attention amon g researchers, incorporating cost-benefit tradeoffs into act ive learning is a rather new line of investigation. Traditional ac-tive learning assumes access to unlabeled data and acquires the labels of the most informative instances at zero or uni-form cost. However, the right way is to take into account the labeling costs to design economically reliable learning me th-ods. Saar-Tsechansky and Provost [11] took a step towards that direction by proposing an active learning framework as a decision-making task. Consider the decision of initi-ating a business action such as offering a costly incentive for contract renewal. Active learning targeting improved accuracy, or in other words reduced loss, may not be suit-able for cost-effective decision making. Thus, they propose a goal-oriented strategy that selects only the examples wher e a small change in model estimation can affect decision-making [11]. Specifically, each unlabeled example is assigned a sco re that reflects the expected effect the example has on decision-making if labeled and added to the training set.

Dimitrakakis and Savu-Krohn [2] address labeling costs explicitly, where the learning goal is defined as a minimiza-tion problem over a function of the expected model perfor-mance and the total cost of labeling. This problem repre-sents a weighted combination of the generalization error of the model incurred after obtaining additional training exa m-ples and the total cost associated with acquiring their labe ls. But, the main focus of [2] is to develop an optimal stopping criterion for sampling based on the comparison between the expected performance gain and the cost of acquiring more labels. However, the proposed stopping strategy requires the use of independent and identically distributed example s, which makes it problematic to couple with active learning.
Melville et al [6] address the cost-sensitivity in active le arn-ing in the context of feature-value acquisition. In some ma-chine learning tasks, the training data has missing feature values which are often quite expensive to obtain. The goal of active feature-value acquisition is to incrementally sele ct fea-ture values that are most cost-effective for improving learn -ing performance. Melville et al propose a selection approac h based on the expected utility of acquiring the value of a fea-ture. The utility of an acquisition is defined in terms of the improvement in model accuracy per unit cost [6]. Since the true values for the model accuracy (accuracy on the unseen test data) is unknown, it is estimated by the training set accuracy. If the feature costs are assumed to be equal, this strategy is similar to the loss reduction principles presen ted earlier in several research studies [10, 8, 3]. Note that the y still assume a single perfect oracle.

Although cost-benefit tradeoffs have started to appear in active learning, there has not yet been any notion of multi-ple oracles, with different costs, different reliabilities, differ-ent probabilities of answering, and possibly different expe r-tise. The general proactive learning problem requires join t maximization of the expected improvement of the learner over oracle and instance choice. In this paper, we propose a decision-theoretic approach where the most cost-effectiv e (highest utility) oracle-instance pair is selected for dat a elic-itation.
In this section, we present a proactive learning method for classification. We focus on three scenarios embodying the notion of multiple oracles with differing properties and costs. Let us begin by explaining  X  X cenario 1 X . In this sce-nario, we assume there exist one reliable oracle and one re-luctant oracle. The reliable oracle gives an answer every time it is invoked with a query, and the answer is always correct. The reluctant oracle, on the other hand, does not always provide an answer, but when it answers it does so correctly. The probability of getting an answer from the re-luctant oracle depends on the difficulty of the classification task. Not surprisingly, they charge different fees: the reli able oracle is more expensive than the reluctant one. We experi-mented with various cost combinations to simulate different real-world situations, with results in the next section.
Rather than fixing the number of instances to sample, as in standard active learning, proactive learning fixes a maxi -mum budget envelope since instances and oracles may have variable costs. Now, let us formulize the problem step by step as a joint optimization of which instance(s) to sample and which oracle to use to purchase their labels. The objec-tive is to maximize the information gain under a pre-defined budget: where B is the budget, S is the set of instances to be sam-pled, and E [ V ( S )] is the expected value of information of the sampled data to the learning algorithm. V ( S ) is a value function that can be replaced with any active selection cri-terion. For instance, it could be the estimated uncertainty of the current learning function at S , or a density weighted uncertainty score, or the estimated error on the unlabeled data if S is labeled and added to the training set. In our experiments, we adopted the density weighted uncertainty score proposed in [4], which significantly outperforms othe r strong baselines.

The above equation can be rewritten by incorporating the budget constraint into the objective function: where the subscript k  X  K denotes the chosen oracle from the set of oracles, K , and  X  is the parameter controlling the relative importance of maximizing the information and min-imizing the cost. For simplicity, we assumed  X  = 1 in this paper. C k and t k indicate the cost of the chosen oracle and the number of times it is invoked, respectively. UL is the set of unlabeled examples, | S | is the total size of the sampled set 1 . Although this formulation is appealing, there is a ma-jor drawback. It is at best difficult to optimize directly due to the fact that the maximization is over the entire set of potential sampling sequences, an exponentially large num-ber. However, the learning function is updated with each additional example, which affects which examples will be sampled in the future, though we can only calculate this ef-fect after we know which examples are chosen and labeled. Thus, we cannot decide all the points to be sampled at once. A tractable alternative is a greedy approximation that will perform the optimal strategy at each round where only a sin-gle example or a small batch of examples is sampled. Now, let us see below how the greedy approach works:
The extension of this formulation to more than two oracles is straightforward.
 E [ V ( x )] is the expected value of information of the example x with respect to corresponding oracle k . We can extend the above expectation by incorporating the probability of receiving an answer and obtain the following 2 :
Our goal in this scenario is to attain the maximum gain under the budget constraint. If both oracles were reliable, then the most cost-effective solution would be to use the cheapest oracle for every query. However, the cheapest or-acle may not respond to every request, especially when the query is difficult. We define a utility score, U ( x, k ), which is a function of the oracle k and the data point x : When the utility is defined as above, it is often necessary to normalize the scores and the costs into the same range. In order to avoid the normalization, we re-define the utility of an example given the oracle as the information value of that example at unit cost:
Unfortunately, there do not exist real-world datasets that have ground truth information on the reliability (in this ca se, P ( ans | x, k )) of the labeling source (e.g. oracle, annotator). Therefore, we simulate the reliability as follows. We assum e the amount of labeled training data available to an oracle de -termines its knowledge (expertise). For instance, the reli able (perfect) oracle resembles a system that has been trained on the entire dataset so it has perfect knowledge on each and every data point. Unlike the perfect oracle, a reluctant ora -cle has access only to a small portion of the data; therefore, it is not knowledgeable for every point. Whenever it en-counters an ambiguous data point to classify, it becomes reluctant to provide an answer. We train a classifier on a small random subset of the entire data to obtain a posterior class distribution P ( y | x ). For its simplicity and probabilis-tic nature, we adopted logistic regression in our experimen ts to calculate the class posterior. The class posterior is the n used for measuring uncertainty, min y  X  X  P ( y | x ), where Y is the set of target labels. We assume that the chance of obtaining an answer from the reluctant oracle is low when the uncertainty is high and vice versa. We explain how we design the reluctance in Section 4.1 in more detail.
In order to calculate the utility as shown in Equation 3, we need to know the answer probability of the reluctant oracle. However, it is unrealistic to be given each oracle X  X  knowl-edge level and response characteristics apriori, so we esti -mate these properties in a discovery phase. First, we cluste r the unlabeled data using kmeans clustering [5]. The number of clusters depends on the pre-defined budget available for this phase and the cost of the reluctant oracle. Second, for each cluster, we inquire the label of the data point closest to the centroid. The number of successful inquiries (i.e. th e number of data points that we obtain the labels of) varies depending on the reluctance of the oracle 3 . We hypothesize that if the oracle does not provide the label of a data point
The expectation is equal to the actual value of information for the reliable oracle since P ( ans | x, reliable ) = 1  X  x .
We experimented with varying reluctance levels for a thor-ough investigation. then it is unlikely to provide the labels for the nearby point s since we assume that similar points share similar posterior class probabilities. Therefore, it is reasonable to estima te the answer probability of the reluctant oracle by inquiring the labels of the cluster centroids.

For each cluster, if we obtain the label of the centroid, then we increase the answer probability of the points in this cluster. Similarly, we decrease the answer probability of t he points in the clusters whose centroids we did not obtain the labels of. This can be regarded as a belief propagation step. If we receive the label of a centroid, then we propagate our belief in receiving a label to similar points and vice versa. Initially, we assume the answer probability for each unla-beled point is 0 . 5, which indicates a random guess. Then, we adopt the following update to estimate the answer prob-ability of each point so that it changes as a function of the proximity of the point to the cluster centroid and oracle re-sponsiveness:
P ( ans | x, reluctant ) = where Z is a normalization constant. x c t is the centroid of cator function which is equal to 1 when we receive the label y c for the centroid x c , and  X  1 otherwise. In Algorithm 1, g denotes the number of centroids for which we receive the label. k x c  X  x k is the Euclidean distance between the cluster centroid x c and the point x , and max d := max x  X  c ,x is the maximum distance between any cluster centroid and data point.

We substitute the estimated answer probability into the sampling of the oracle-example pair can now be performed as shown in Algorithm 1. The algorithm works in rounds till the budget is exhausted. Each round corresponds to a single label acquisition attempt where sampling persists until obtaining a label. One important point to note here is that we need to restrain from spending too much on a single attempt by adaptively penalizing the reluctant orac le every time it refuses to answer. At any given round, if the algorithm chooses the reluctant oracle and does not receive an answer, the utility of remaining examples with respect to this oracle decreases by the amount spent thus far at this round: where C round is the amount spent thus far in the given round. This penalization only applies to the reluctant ora-cle since the reliable oracle always provides the label. Alg o-rithm 1 selects the maximum utility examples. This frame-work leads to an incrementally optimal solution in the sense that the most useful data is sampled at the minimum cost.
In real-world, there might also be fallible oracles which answer each query, but the credibility of the answer is ques-tionable. We simulate this setting in  X  X cenario 2 X , where we assume two oracles; one reliable and one unreliable or-acle. The reliable oracle is the perfect oracle that always provides the correct answer to any query. The unreliable or-acle in this scenario is fallible that it may provide the wron g Algorithm 1 Proactive Learning: Scenario 1
Input: a classifier f , labeled data L , unlabeled data UL , entire budget B , clustering budget B C &lt; B , two oracles, each with a cost C k , k  X  K = { reliable, reluctant } Output: f -Cluster UL into p = B C /C reluctant clusters -Let x c t be the data point closest to its cluster centroid,  X  t = 1 , ..., p -Query the label y c t for each cluster centroid x c t -Identify { x c 1 , ..., x c g } for which we obtain the labels -Estimate  X  P ( ans | x, reluctant ) via Equation 4 -Update L = L  X  X  x c t , y c t } g t =1 , UL = UL \{ x c -cost spent so far C T = B C while C T &lt; B do end while label for a given example. Specifically, if an example ap-proaches the decision boundary, the probability of correct classification approaches 0 . 5 (random guess). The proba-bility of acquiring a correct label, P ( correct | x, fallible ) is modeled the same way as in  X  X cenario 1 X . The solution we propose is similar to the method introduced for  X  X ce-nario 1 X , with slight variations. For instance, the learnin g method receives a random label for the queried example x with probability 1  X  P ( correct | x, fallible ). Moreover, we use the clustering step exploiting the fallible oracle to es -timate the correctness probability P ( correct | x, fallible ). Similar to the previous scenario, we inquire the labels of th e cluster centroids. Unlike the reluctant oracle, the fallib le oracle provides the label together with its confidence. The confidence is its posterior class probability for the provid ed label, P ( y | x ). If the class posterior is within an uncertainty range, then we decide not to use the provided label since it is likely to be noisy (See Section 4.1 for details). We de-crease the correctness probability for the points in the clu ster whose centroid has a class posterior in the uncertainty rang e. We increase the correctness probability for the points in th e clusters with highly confident centroids; i.e.  X  P ( correct | where  X  h ( x c t , y c t ) =  X  1 if min y P ( y | x c t tainty range, and 1 otherwise. In Algorithm 2, h denotes the number of high confident centroids.

Thus far, we have only considered the settings where a uni-form fee is charged for every query by an oracle, although each oracle may charge differently. Fraud detection in bank-ing transactions is a good example for this setting. The Algorithm 2 Proactive Learning: Scenario 2
Input: a classifier f , labeled data L , unlabeled data UL , entire budget B , clustering budget B C &lt; B , two oracles, each with a cost C k , k  X  K = { reliable, fallible } Output: f -Cluster UL into p = B C /C fallible clusters -Let x c t be the data point closest to its cluster centroid,  X  t = 1 , ..., p -Query the label y c t for each cluster centroid x c t -Identify { x c 1 , ..., x c h } for which the fallible oracle has high confidence -Estimate  X  P ( correct | x, fallible ) -Update L = L  X  X  x c t , y c t } h t =1 , UL = UL \{ x c -cost spent so far C T = B C while C T &lt; B do end while customer records are saved in the bank database so it takes the same amount of time and effort, hence the same cost, to look up any entry in the database. On the contrary, it is pos-sible that the costs are distributed non-uniformly over the set of instances. For instance in text categorization, it mi ght be relatively easy for an annotator to categorize a web page; hence the cost is modest. On the other hand, assigning a book into a category incurs a considerable reading time and therefore cost. Another example of a non-uniform cost sce-nario is medical diagnosis. Some diseases such as herpes are easy to diagnose. Such diagnoses are not costly since there i s usually a major definitive symptom, i.e. outbreak of blister s on the skin. On the other hand, diagnosing hepatitis can be very costly since it may require blood and urine tests, CT scans, or even a liver biopsy. In  X  X cenario 3 X , we explore the problem of deciding which instances to query for the labels when label acquisition cost varies with the instance. We as-sume two oracles one of which has a uniform and fixed cost for each query whereas the other charges according to the task difficulty. We further assume that these oracles always provide an answer and both are perfectly reliable in their answers.

In order to simulate the variable-cost (non-uniform) ora-cle, we model the cost of each example x as a function of the posterior class distribution P ( y | x ). We use the class posterior calculated similarly in the previous scenarios. The non-uniform cost C non  X  unif ( x ) per instance is then defined as follows: The cost increases as the instance approaches the decision boundary and vice versa. In other words, the oracle charges based on how valuable the instance is to the learner. This may not exactly be the case in the real world, but this sets up a more challenging decision in terms of the utility-cost trade-off. The utility score in this scenario is calculated a s the difference between the information value and the cost instead of the information value per unit cost 4 . This is to avoid infinitely large utility scores as a result of the divis ion by small -cost. Thus, the revised utility score per oracle is given as follows: where C unif is the fixed cost of the uniform-cost oracle. The pseudocode of the algorithm is given in Algorithm 3. There Algorithm 3 Proactive Learning: Scenario 3
Input: a classifier f , labeled data L , unlabeled data UL , entire budget B , two oracles, each with a cost C k , k  X  K = { unif, non  X  unif }
Output: f cost spent so far C T = 0 while C T &lt; B do end while is no clustering phase in Algorithm 3 since we assume we know the cost of every instance, which is realistic for many real-world applications.
In this section, we first describe the problem setup, and then present the empirical results on various benchmark datasets.
In order to simulate the reliability of the labeling source (oracle), we assume that a perfectly reliable oracle resemb les by a classifier trained on the entire data. An unreliable ora-cle, then, resembles a classifier trained on only a small subs et of the entire data. We randomly sampled a small subset from each dataset and trained a logistic regression classifier on this sample to output a posterior class distribution. Then, we identified the instances whose class posterior falls into the uncertainty range, i.e. min y P ( y | x )  X  [0 . 45 , 0 . 5]. This range is used to filter the instances that the reluctant oracl e does not answer or the fallible oracle outputs a random la-bel. One can argue that the same effect can be achieved by randomly picking such instances. However, our simulation forces a trade-off between the reliability and the informati on value of an instance since uncertain instances are generall y informative for active learners. In order to cover a wider spectrum, we varied the percentage of instances that fall into the uncertainty range [ . 45 , . 5]. The second column in Table 1 shows the different percentages used in our exper-iments. The cost of the unreliable oracle is inversely pro-portional to its reliability. We choose higher cost ratios f or
In general, if the cost and information value are not as-sessed in the same units, then they are normalized into the same range. Table 1: Oracle Properties and Costs. B C is the clustering budget, B is the entire budget. Uncertain % is the percentage of the uncertain data points.
 Cost Ratio is the ratio of the cost of the reliable oracle to the cost of the unreliable one.
 the fallibility scenario since receiving a noisy label shou ld be penalized more than receiving no label at all. The tradeoff between cost and unreliability is crucial to have an incen-tive to choose between oracles rather than exploiting a sing le one. See Table 1 for details.

The other case we need to simulate is the uniform and non-uniform cost oracles. The cost of each instance for the variable-cost oracle is defined as a function of the class pos te-rior obtained on the randomly chosen subset. This indicates a positive relationship between the difficulty of classifyin g an instance with its cost, which is realistic for many real-wor ld situations. The cost of labeling each instance is known to the learning algorithm. Thus, we do not need any clustering phase in Scenario 3. We choose the cost of the uniform-cost oracle within the range of instance costs for the variable-cost oracle. Hence, the costs will be comparable in the same range. We varied the fixed cost such that there is always an incentive to choose between oracles instead of fully exploi t-ing a single one.

We compared our method against sampling with randomly chosen oracles and sampling with a single oracle. Each base-line uses the clustering step for a fair comparative analysi s. However, only our method estimates the oracle unreliabilit y to help sampling the optimal oracle-example pair.

All the results reported in this paper are averaged over 10 runs. At each run, we start with one randomly chosen labeled example from each class. The rest of the data is con-sidered unlabeled. The learner selects one example at each iteration to be labeled, and the learning function is tested on the remaining unlabeled set once the label is obtained. The learner pays the cost of each queried example regardless of whether a label is obtained. To show the effectiveness of each method, the learning curves display the classification error versus the data elicitation cost. The budget is fixed at 300 in Scenario 1 and 2, and at 20 in Scenario 3. A small budget is enough for the latter since the cost of individual instances can be very small depending on the posterior prob-ability. We have observed that 20 is more than enough to reach a desirable accuracy in this scenario. The clustering budget, on the other hand, varies according to the unreli-ability, but is the same for each baseline under the same scenario (See Table 1). The number of clusters, though, is determined by dividing the clustering budget by the cost of the oracle used during this phase. For the initial clusterin g phase, the unreliable oracle is used in our method and in the unreliable-oracle baselines. Thus, they obtain the sam e labeled data during this step, which results in the same er-ror rate. The random oracle baseline uses a fixed number of Table 2: Overview of Datasets. +/-is the posi-tive/negative ratio. Dim is the dimensionality.
 clusters, but for each cluster centroid it randomly chooses the oracle to invoke and continues until the clustering sub-budget exhausts.
We followed the density-sensitive sampling method pro-posed by [4] to evaluate the value of information of the un-labeled instances in our experiments. The method of [4] relies on conditional entropy maximization weighted by a density measure. The proposed scoring function captures not only the information content of an instance (measured by the uncertainty, i.e. min y P ( y | x ), but also the prox-imity weighted information content of its neighbors. The original method adopts a density-sensitive distance funct ion and performs sampling in pairs of instances. However, we use Euclidean distance and sample a single point at a time in our experiments:
U ( x i ) = log min [4] argues that using Euclidean distance to measure the pair -wise proximity gives comparable results and faster compu-tation.
We study the performance of the proposed methods on various real-world benchmark datasets. The face detection dataset [9] has a total number of 393360 images, which we used a random subsample of size 2500 as in [4]. UCI-Letter is another image dataset for recognizing English capital le t-ters where we labeled the letter  X  X  X  as the positive class and the letter  X  X  X  as the negative class. This is one of the most ambiguous pairs in the data. The Spambase and the Adult datasets are also popular datasets available from the UCI Machine Learning Repository [7]. The Spambase data contains 4601 instances and 57 condition attributes. It is used to classify emails as spam and non-spam. Most of the attributes indicate whether a certain word or charac-ter appears frequently in emails. For the Adult dataset, we adopted the smaller version constructed for the IJCNN 2007 Workshop on Agnostic Learning [1]. This version has 48 fea-tures and 4147 instances in total. The task of Adult data is to discover high revenue people from the census bureau. A summary of datasets is provided in Table 2.
We conducted a thorough analysis to examine the perfor-mance of our method under various conditions. Due to the lack of existing work on cost-sensitive active learning wit h multiple oracles, we compared our method against active indicated above each plot. sampling with randomly chosen oracles and active sampling with a single oracle. We denote our method of jointly opti-mizing oracle and instance selection Joint , the random sam-pling of oracles Random . Reliable , Reluctant , and Fallible refer to the corresponding single oracle baseline.
We next clarify why the maximum cost of data elicitation, shown in Figures 1-8, differ in various tasks and scenarios. The results are averaged over 10 runs for each experiment. At each run, the total number of iterations to spend the entire budget may differ depending on how the budget is allocated between oracles. In order to take the average of the results, we rely on the minimum number of iterations attained over 10 runs for each experiment. This ensures that all runs equally contribute to the average. This also results in different maximum elicitation costs smaller than the budget for different experiments. Nevertheless, the Joint strategy outperforms the others even after spending only a small amount in most cases.

Figure 1 shows the results for the reluctance scenario on the Spambase dataset. Each plot indicates a different cost ratio. Our method outperforms the others on every case while the performance gap increases with the cost ratio. This is largely because the oracle differences leave more roo m for improvement via oracle selection in the latter case. Whe n the unreliability gets higher, the reluctant oracle tends t o spend almost the entire budget on a single label acquisition attempt. This leads to acquiring only a small amount of labeled data; hence, its poor performance. As a result, we do not report the reluctant oracle baseline except in its bes t case, the 1 : 3 cost ratio.

Figures 2 and 3 show the comparison between our method and the other baselines for Scenario 1 on the Adult and VY-Letter datasets, respectively. For the Adult dataset, our Joint method outperforms the others when the cost ratio is 1 : 3 while it tracks the best performer for the other cost ratios. Generally, Joint tracks the best performer when the best performer is a clear winner for the entire operating range. This pattern is also evident in Figure 3 for the cost ratio 1 : 3. For the other cost ratios, Joint significantly outperforms the other baselines on the VY-Letter dataset. Figure 4 compares the performances for Scenario 2 on the VY-Letter dataset. The Fallible oracle in this scenario per -forms poorly when the relative cost ratio is high. As shown in Table 1, the cost ratio increases with the number of unre-liable instances. In other words, a higher cost ratio indica tes a more unreliable oracle. Thus, the Fallible oracle may in-crease the classification error with more labeled data since the labels are increasingly likely to be noisy. This pattern is especially evident in Figure 4 for the cost ratio 1 : 7. On the other hand, Joint strategy is quite effective for reducing the error in this scenario, indicating that it is capable of r e-ducing the risk of introducing noisy data through strategic selection between oracles.

We present the rest of the results in Table 3. Due to space constraints, we selected a representative cost ratio for each dataset. The values in bold correspond to the winning methods. Joint wins frequently (i.e. 10 out of 16) and is a close runner-up on the other cases.

Figures 5, 7 and 8 present the evaluation results when the cost varies non-uniformly across the set of instances. We ex -perimented with different assignments of the fixed cost, each of which is a function of the average instance cost, denoted avg , for the non-uniform cost oracle. We present two rep-resentative assignments for each dataset: Cost1:= avg/ 1 . 5 and Cost2:= avg/ 2. The remaining cost values are not in-cluded since they are similar to those reported here. On the Face and the Spambase datasets, Joint is the best performer throughout the full operating range. Moreover, Joint pre-dominantly outperforms the others on the VY-letter dataset . The performance difference between Joint and each baseline is also statistically significant based on a paired two-side d t-test ( p &lt; 0 . 01). For the Adult dataset, both cost cases performed equivalently; hence, there was no opportunity fo r  X  X oint X  to optimize further.

In order to investigate if the initial clustering phase help s all the baselines, we re-ran each baseline excluding the clu s-tering step. In this case, there is no separate clustering budget; hence, the entire budget is spent in rounds for data elicitation. Figure 6 compares each baseline with the clus-tering restriction on the Spambase dataset for Scenario 1. Every baseline significantly benefits from clustering, with the biggest boost in improvement occurring for the Reluc-tant oracle. Hence, both the baselines and the  X  X oint X  strat -egy benefit from the diversity-based sampling via clusterin g in their initial steps. Without pre-clustering, the Reluct ant oracle is prone to spend too much on a single elicitation at-indicated above each plot. indicated above each plot.
 corresponding error rate. The best result on each row is give n in bold. indicated above each plot.
 Figure 5: Comparison of different algorithms under non-uniform cost structures (Scenario 3) on Face. a) (Top panel) Fixed-Cost oracle has Cost1 b) (Bottom panel) Fixed-Cost oracle has Cost2. tempt due to unsuccessful labeling requests. It can, howeve r, maximize the chance of receiving a label through diversity sampling during the clustering step instead of getting stuc k in one round for a single label.
In this paper, we proposed proactive learning to over-come the unrealistic assumptions of active learning. We introduced three scenarios that analyze the effect of mul-Figure 7: Comparison of different algorithms under non-uniform cost structures (Scenario 3) on Spam-base. a) (Top panel) Fixed-Cost oracle has Cost1 b) (Bottom Panel) Fixed-Cost oracle has Cost2. tiple imperfect oracles with differing properties and costs on selective sampling. The proposed methods formulated in a decision-theoretic framework rely on expected utility max i-mization across oracle-example pairs. The empirical resul ts demonstrate the effectiveness of this approach against ran-dom oracle selection and exploitation of a single oracle, ev en the best one. This paper takes a step towards filling in a gap between active learning and real-world tasks to make active learning reach practical applications. As future wo rk, baseline is given in the title. The cost ratio is 1:3. Figure 8: Comparison of different algorithms un-der non-uniform cost structures (Scenario 3) on VY-Letter. a) (Top panel) Fixed-Cost oracle has Cost1 b) (Bottom panel) Fixed-Cost oracle has Cost2. we will investigate relocating resources in scenarios with no apriori information about oracle properties to optimize th e cost-benefit tradeoff.
This material is based in part upon work supported by the Defense Advanced Research Projects Agency (DARPA) under Contract No. NBCHD030010. [1] Agnostic learning vs. prior knowledge challenge and [2] C. Dimitrakakis and C. Savu-Krohn. Cost-minimising [3] P. Donmez and J. G. Carbonell. Optimizing estimated [4] P. Donmez and J. G. Carbonell. Paired sampling in [5] J. Hartigan and M. Wong. A k-means clustering [6] P. Melville, M. Saar-Tsechansky, F. Provost, and [7] D. Newman, S. Hettich, C. Blake, and C. Merz. UCI [8] H. Nguyen and A. Smeulders. Active learning with [9] T. Pham, M. Worring, and A. Smeulders. Face [10] N. Roy and A. McCallum. Toward optimal active [11] M. Saar-Tsechansky and F. Provost. Decision-centric [12] S. Tong and D. Koller. Support vector machine active [13] G. M. Weiss and Y. Tian. Maximizing classifier utility
