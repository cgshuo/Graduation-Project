 The two major problems with traditional association rule mining are the high cost of generating association rules and the large number of excess rules that are generated. Traditional association rule mining algorithms, such as Apriori [1], use the support confidence framework, which requires a user defined support threshold. In light of this, there has been much research into developing techniques [2,3,4] to find a meaningful support threshold. Setting a hard support thres hold is no longer sufficient. If the support threshold is set too high, we produce rules which are of common knowledge and we may prune rare itemsets which have a high confidence and have valuable information. However setting the minimum support thres hold too low would produce many trivial rules. It also does not guarantee a strong association. Strongly associated rules have more predictive power, and are more useful.

We propose to use the minimum absolute support (MinAbsSup) function[5] that gen-erates a minimum absolute support value fo r each candidate itemset. This replaces the original minimum support threshold in Apriori. When the support of an itemset is be-low its minimum absolute support value we assume that it is occurring due to random coincidence and it is pruned. This method proposed is statistically more meaningful compared to other methods used to arbitrarily choose a minimum support threshold. MinAbsSup function was proposed to eliminate the occurrences of itemsets that occur due to some random process. These itemsets are occurring together by coincidence are not strongly associated or statistically significant. We call this the coincidental itemset problem . These are important because itemset s that have high support and high confi-dence may be appearing so frequently in a t ransaction database that they cannot help but appear together, while itemsets that have a low support but high confidence may be occurring due to chance and could be consider ed as  X  X oise. X  In these rare cases a high confidence value may not be sufficient to determine a valid rule. When dealing with rare cases we are dealing with low support, thus even a small variation in the support of an itemset would dramatically effect its confidence value. The following is a formal statement of association rule mining for transaction databases. Let I = { i 1 ,...,i m } be the universe of items. A set X  X  I of items is called an item-set. A transaction t =( tid ,X ) is a tuple where tid is a unique transaction ID and X is an itemset. A transaction database D is a set of transactions. The support of an itemset X in D , denoted by supp ( X ) , is the proportion of transactions in D that contain X .The rule X  X  Y holds in the transaction set D with confidence where conf ( X  X  Y ) is the proportion of transaction that contain X also containing Y . There has been a lot of research into developing efficient algorithms for mining itemsets with a variable mini-mum support threshold [2,3,6,4]. These algorithms are exhaustive in their generation of rules, and so spend time looking for rules with high support and high confidence. If the varied minimum support value is set close to zero, they will take a similar amount of time to that taken by Apriori to generate low-support rules in amongst the high-support rules. These methods generate all rules that have high confidence and high support. To include rare items, the minsup threshold mus t be lower, which consequently generates an enormous set of rules consisting of both frequent and infrequent items. A uniform minimum support threshold is not effective for datasets with a skewed distribution be-cause they tend to generate many trivial patterns or miss potential low-support patterns. Hence another approach is to use association rule mining without support threshold, but it usually introduces another constraint such as similarity or confidence pruning. How-ever none of these researches have directly considered the coincidental itemset problem. There has been some research that is releva nt to the coincidental itemset problem. In order to improve the support-confidence framework, some have proposed using an ad-ditional measure [7,8,9,10]. Brin et al. (1997) proposed a pruning method based on the chi-square model. They use the chi-square test to prune out the insignificant rules by using it to test whether the antecedent and th e consequent of the rule are statistically associated. A rule is significant if and only if i t is statistically associated. However the chi-square test is only an approximation of the true level of association and does not work well with rare itemsets, which are genuinely associated and have low support and high confidence. There are two types of candidate itemsets: non-coincidental itemsets and coinciden-tal itemsets. Non-coincidental itemsets are generated by some non-random process. Whereas, there are two different circumsta nces in which a coincidental itemset may occur. Items within an itemset may be appearing so frequently that they cannot help but appear together and in turn generate a non-coincidental rule. For example, in an ob-stetrician medical dataset, we may generate a rule { pregnant = yes } X  X  female } with support of 0.95 and confidence 1.00. This particular rule would not be interesting as it is of common knowledge. Itemsets with low support but high confidence may seem interesting, but some of these rules may in fact be occurring due to chance, and should be considered as noise . For example, in a general medical dataset we may generate a rule { meningtitis } X  X  leg fracture } with support of 0.01 and confidence 1.00. A real dataset will contain noise, which usually occurs at levels of low support. In Apriori, setting a high minimum support threshold w ould cut the noise out but also prune out interesting rare rules. Inherently we want to be able to detect these expensive rare rules with low support. We are interested in finding a method to filter out noise from inter-esting items, and detecting non-coincidental occurrences of itemsets. However up until now there has not been a method that allows us to distinguish itemsets that are occurring by coincidence. This is called the coincidental itemset problem .
 We are interested in finding rules without having to set an ad-hoc support threshold. Here we introduce the use of the minimu m absolute support function which replaces the user-defined minimum support threshold in the original Apriori algorithm. This is used to prune out the rules for itemsets that are likely occurring together due to chance. Previously, the minimum absolute support (MinAbsSup) function was introduced by Koh et al. (2006) to differentiate noise and valid itemsets in rare rule mining. Here we extend the usage of the function to the area of pruning coincidental frequent itemsets. 3.1 Apriori with the MinAbsSup Function In this section we look at how MinAbsSup is used in Apriori as shown in the algorithm below. Only candidate itemsets whose support is above their calculated MinAbsSup value will be extended. As the MinAbsSup va lue is generated on-the-fly for each candi-date itemset, we no longer need to use a user-defined minimum support threshold. Note that only rules with support larger than their MinAbsSup value will be kept, the rest are pruned just as if their support was less than the user-defined minimum support in standard Apriori.
 Apriori with MinAbsSup pruning algorithm Input: Transaction database D , universe of items I , Output: Non-coincidental frequent itemsets N  X  X  D | Idx  X  invert ( D, I ) k  X  1 L k  X  X { i }| i  X  dom Idx , count ( { i } , Idx )  X  1 } while ( L k =  X  ) end while MinAbsSup check, MinAbsSup check ( c, L, N, Idx , X  ) Input: Itemset c , Level L , Size of dataset N , Inverted Output: True or False i  X  arg min { j  X  count ( j, Idx ) | j  X  c } a  X  count ( { i } , Idx ) b  X  count ( c \{ i } , Idx ) return (  X  x  X  c | c \{ x } X  L ) AND count ( c, Idx ) &gt; MinAbsSup ( N, a, b,  X  ) We compared the performance of the MinAbsSup function and the standard Apriori al-gorithm on eleven different datasets from the UCI Machine Learning Repository [11]. Table 1 displays the results from our implementation of Apriori with MinAbsSup, and database named in the left-most colum n. In the experiments, minconf is set to 0 . 90 . For the Apriori algorithm, this involves setting minimum support to include itemsets that occur more than once. To give an indication of the amount of work Apriori is doing to find low-support rules, we set a time constraint of ten thousand seconds to process each dataset.

When Apriori with MinAbsSup is compared against Apriori, the reduction in the number of rules (with all possible consequent lengths) generated is drastic. The reduc-tion ranges from a factor of 15 to 60809, dependi ng on the particular dataset. By setting the arbitrary threshold too low we may be flooded with many trivial rules. We would need wade through the rules to find those that may be of some interest. However set-ting the support too high we may miss out useful rules. To take the Lenses dataset as an example, normal Apriori finds 83 rules. The list below shows a subset of the rules found using normal Apriori with its confidence and lift value. We concentrate on this particular subset because they contain simila r a consequent. The rest of the rules in the subset were not found as the itemsets could not be differentiated from noise.
From this particular grouping Apriori with MinAbsSup finds { astigmatic = 1 } X  { tear production rate = 3 } . Note that our algorithm did not find the rest of the rules. Note that all these rules have the same lift and confidence value. All of the rules have the same consequent { tear production rate = 3 } . From the set below, we did not find trivial rules. Trivial rules are rules whose a ntecedents cover exac tly the same records as one of their parent rules. From the list the rules found the rule { astigmatic = 1 } X  X  tear production rate = 3 } is considered as the parent rule.

Note that the set of rules generated by normal Apriori in this section should not be considered as the most compact set of rules . In order to obtain a compact set of rules, we require some form of post-pruning method to eliminate trivial and redundant rules. Some plausible pruning techniques have been previously researched[12,9]. However, for experimental purposes we are evaluating the performance of our algorithm without other pruning techniques. The results here show that MinAbsSup reduces the need for post-pruning, and this in turn reduces time and space requirements. Setting a suitable minimum support threshold has been investigated by many researchers. In this paper, we introduced the MinAbsSup function whi ch replaces the fixed mini-mum support threshold of standard Apriori. This calculates a custom minimum support for each itemset based on the itemset X  X  probability of chance collision, preventing co-incidental rules from being generated. Here we show that MinAbsSup efficiently finds rules which are non-coincidental without using arbitrary support thresholds. In this pa-per we are only concerned about setting a suita ble threshold. To produce non-trivial and non-redundant rules we still would benefit fro m some form of post p runing technique.
