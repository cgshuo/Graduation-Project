 Sentiment extraction by modern sentiment analy-sis (SA) systems is usually based on searching the input text for sentiment -bearing words and expressions, either general (language -wide) or domain -specific. In most common SA approach-es, each such expression carries a polarity value ("positive" or "negative") which is possibly weighted. The sum of all polarity values from all expressions foun d in a text becomes the senti-ment score for the whole text.

P eople are, however, usually interested in sen-timents regarding some entity or situation, and not in sentiments of a particular document. A natural way to make the SA more focused is to explicitl y bind each sentiment expression to a specific entity, or to a small set of entities from among all entities mentioned in the document. The choice of which entity to bind a sentiment expression to, can be made according to the proximity (physical, syntacti cal, and/or semantic) and/or salience of the entities.

In this paper, we argue that all of these meth-ods can be useful in different contexts, and so the best single algorithm should use all available proximity information, of all kinds, together with addit ional context information  X  position in the document, section, or paragraph; proximity of other entities; lexical contents; etc. One of the most important context information is the type of relation between the target entity and the docu-ment  X  whether the e ntity is the main topic of the document, or one of several main topics, or men-tioned in passing, etc.

Another layer that we'd like to add concerns the interaction of different entity types during SA . In a typical situation, there is only one entity type wh ich is the target for SA . In such cases, clearly distinguishing between the relevancy of target and non -target entities types is not essen-tial. For example, when the general topic is a COMPANY, and there is a sentiment expression referring to a PERSON or a PRODUCT, this sentiment expression is still relevant to the com-pany and can be regarded as such. In other situa-tions, SA users m ay be specifically interested in an interaction between entities of different types. For example, in a medical forum setting, i t may be interesting to know the users' sentiments re-garding a given DRUG in the context of a given DISEASE. We will show that such situations are modeled well enough using intersections of re-gions of relevance of the participating entity types, with the r elevance region for each type calculated separately.

W e purposefully exclude possible interactions between entities of the same type, because they behave in a different way. The precise analysis of such interactions is a different topic from rele-vance dete ction, and so it is mostly ignored in this paper. The task of SA has drawn the attention of many researchers worldwide (Connor et al., 2010; Liu, 2012; Loughran and Mcdonald, 2010; Pang and Lee, 2004; Turney, 2002) . While mo st SA re-search is focused on discovering and classifying the expressions, some are also concerned with the targets of the expressions and explicitly iden-tify the syntactic targets of sentiment expressions (Pang and Lee, 2004) . 
Other related works belong to the Passage Re-trieval field, since the relevance detection prob-lem can be construed as a specific form of pas-sage retrieval problem (Liu and Croft, 2002; Tiedemann and Mur, 2008) . Different approach-es were suggested for passage retrieval (Buscaldi et al., 2010; Comas et al., 2012; Hearst, 1997; Lafferty et al., 2001; Lin et al., 2012; Liu and Croft, 2002; Lloret et al., 2012; O X  X onnor et al., 2013; Otterbacher et al., 2009; Salton et al ., 1993; Wachsmuth, 2013) , some are more sophis-ticated than others. The closest approach to ours is the one of Scheible and Sch X tze ( 2013) , b ut i n contrast to them , we strive to discover sentiments ' relevance for all entities (of a given type) mentioned in the document, not necessarily topical. An instance of the sentiment relevance detection problem for a single entity consist s of a text doc-ument, a sentiment expression within the docu-ment, and a target entity. The task is a binary decision: 'relevant' vs. 'irrelevant'. To solve this task, we can use any information that can be found by analyzing the document. Thus, we can assu me that we know the parse trees of all sen-tences and the locations of all references of all entities in the document, including co -references.
In addition, we make use of an extra piece of information for each target entity  X  its "status within the documen t", or "document type with respect to the entity". We dis tinguish between several types which are intuitively clearly differ-ent:  X  'Target'  X  the entity is the main topic of the document;  X  'Accidental'  X  the entity is not the main topic of the document, and i s mentioned in passing;  X  'RelationTarget'  X  the main topic of the doc-ument is a relation between the entity and some other entities of the same type;  X  'ListTarget'  X  the entity is one of a few equal-ly important topics, dealt with sequentially.
In the dataset s we use for experiments, each entity is manually annotated with its status with-in the document, which allows us to directly ob-serve the influence of this data on the accuracy of relevance discernment. We also show that this data can be automatically extra cted using super-vised classification.

Since this paper is primarily a study of senti-ment relevance, the actual sentiment expressions are not always labeled in our datasets. Instead, relevance ranges are annotated for each entity, in the style of passage re trieval problems, with the expectation that sentiment expre s sions relevant to an ent i ty only appear in the parts of the doc u-ment that are l a beled as "relevant", and converse-ly, that all expressions appearing in parts labeled "irrelevant" are irrelevant. Th is way of annotat-ing allows the comparing of different rele vance detec tion strategies independently of the main sentiment extraction tool.

All of the algorithms discussed in this paper use the same document processing methods, thus allowing us to compare the algorithms them-selves independent of the quality and specifics of the underlying NLP.

T he multiple -entity relevance problem is dis-tinguished from the single -entity relevance prob-lem by the requirement for the sentiment expres-sion to be relevant to seve ral entities of different types. The problem is close to Relation Extrac-tion in this sense. The examples we are interested in are in the medical domain and deal with three main entity types: PERSON, DRUG, and DISEASE, where PERSON is restricted to known ph ysicians. While each of the entity types can be the target of a sentiment expression, the more interesting questions in this domain involve multiple entities , s pecifically, DRUG + DISEASE ("how effective is this drug for this disease?"), and PERSON + DRUG + DISEASE ("what does this physician say about using this drug to cur e this disease?").

We solve the multiple -entity relevance prob-lem by intersecting the relevance ranges of dif-ferent -type entities, thus reducing the problem to the single -entity relevance detection. As such, the experiments regarding the multiple -entity relevance need only check the accuracy of this reduction. In the medical domain, at least, this accuracy appears to be adequate. Each algorithm receives, as input , the text of the document, with labeled reference of the target entity and other entities of the same type. The labeled references also include a ll coreferential reference s , extracted automatically by an NLP system. The input text also includes labeled can-didat e sentiment expressions, either manually labeled or automatically extracted by a rele-vance -ignoring SA system 1 . The task of the algo-rithms is to label each candidate expression as relevant or irrelevant to the target entity. The algorithms are evaluated ac cording to the accura-cy (recall, precision, and F 1 ) of this labeling of individual sentiment expressions. 
This method produces a reasonably well -understandable quality measure (the percentage of expressions that the algorithms get right or wrong), and als o allows us to compare algo-rithms focused on individual expressions and algorithms working on text ranges. The algo-rithms we evaluate are as follows:  X 
Baseline -Every expression is declared rele-vant. This is the standard mode of operation of document -level SA tools, although it is usually only applied to the 'Target' entities  X  the main topic(s) of the document.  X 
P hysical -proximity -based -A text -range fo-cused algorithm, which labels pieces of text as relevant or irrelevant according to their place-ment relat ive to the reference s of the target en-tity and other entities of the same type, as well as some other contextual clues, such as para-graph boundaries. Generally, the mentioning of an entity starts its relevance range (and stops the relevance range of the pr eviously men-tioned entity). For the first entity reference in a paragraph, the range also extends backward to the beginning of the sentence. T here are three flavors of the algorithm, spe cifically adapted for different document -types -with -res pect -to -the -tar get -entity :  X 
Syntactic -proximity -based -An expression -focused algorithm, which labels expressio ns as relevant or irrelevant according to their dis-tance to various entity references in the de-pendency parse graph. There are two flavors of the algorithm: direct and reverse. The former considers an expression relevant only if it is closest to the target entity from among all enti-ties of the same type, and the distance is suffi-ciently close. The latter considers an expres-sion irrelevant only if it has the above -described relation to some non -target entity of the same type. The rationale for the two flavor s is the distinction between 'Targeted' and 'Acci-dental' document types regarding the target en-tity. For the 'Accidental' entities, a sentiment expression is assumed to be relevant only if it is explicitly connected to the entity. F or 'Tar-geted' entities, an expression is irrelevant only if it is explicitly connected to some other entity of the same type.  X 
Classification -based -This algorithm consid-ers each candidate senti ment expression as an instance of a binary classification problem, to be solved using supervised classification. For evaluating this algorithm, some part of the test corpus is used for training, and the other for testing, with N -fold cross -validation. The fea-tures for classification may use any infor-mation present in the input. ence s of target and non -target entities, appear-ances of paragraph and document boundaries, length of syntactic connections to target and non -target entities, when available, and explicit entity status within document s , when ava ila ble . 
The (binary) classification features are built from sequences of up to 5 occurrences of the above -described pieces, with the pieces ap-pearing before and after the sentiment expres-sion tracked separately. For classification , we use a linear classifi er with Large Margin train-ing (regularized perceptron, as discussed in 
Scheible and Sch X tze, (2013)).  X 
Sequence -classification -based -The algo-rithm uses exactly the same features as the di-rect classification -based above, but instead of considering each exp ression separately, it con-siders them as a sequence, one per document. 
So, instead of a Large Margin binary classifier, a probabilistic sequence classifier is used (CRF, as discussed in Lafferty et al. (20 0 1)). For the experiments, we use two m anually -PANIEs are used as tar get entities and in the medical corpus, DISEASEs , DRUGs and PER-SONs are the entity types that are used as target entities. For the purp ose of the experiments, we are interested only in single -entity sentiments about DRUGs, and multiple -entity sentiments about DRUGs + DISEASEs, and DRUGs + DISEASEs + PERSONs.

The evaluation metrics in all of the experi-ments are precision, recall, and F 1 . F or the clas-sification -based algorithms, unless stated other-wise, we use 10 -fold cross -validation. 5.1 Experiment: Importance of relevance In the first experiment , we demonstrate the im-portance of using relevance when calculating the consolidated sentiment sco re of an entity within a set of documents. For each entity, we set the 'correct' consolidated sentiment score to the av-erage of polarities of all sentiments in a corpus which are labeled as relevant to the entity. Then , we compare the correct value to the two scores calculated without considering relevance:  X  'Baseline' -the average of polarities of all sen-timents in all documents where the entity is mentioned, and  X  'TargetedOnly' -the average of polarities of all sentiments in the documents where the enti-ty is labeled as target (main topic of the docu-ment). This case models the typical state of a relevance -agnostic SA system.

For this evaluation , we only compare the sign of the final sentiment scores, without considering their magnitudes ( unless it is close to zero, in which it is considered 'neutral') . The errors at this level indicate definite SA errors  X  miscalculating entity's sentiment into its opposite.
 The results of the evaluation are as follows: The 'Baseline' scores show a large difference from the correct scores, with 33% and 38% of entities having wrong final polarity in the finan-cial (COMPANY) and medical (DRUG) do-mains, respectively. The 'TargetedOnly' scores are somewhat closer to correct, with 12% and 28% of entities with incorrect final polar ities. However, the 'TargetedOnly' method naturally suffers from a very low recall, with only 19% and 38% of entities covered in the financial and medical domains, respectively. 5.2 Experiment: Influence of entity status In this experiment , we compare the perf ormance of various algorithms while either providing or withholding the information about the document -type -with -respect -to -the -target -entity.
The performance of the physical proximity al-gorithms on the financial corpus is shown at the top left hand side o f Table 1. The set of all in-stances of relevance detection problems in the corpus (an instance consists of a sentiment ex-pression within a text, together with a target enti-ty) is divided into three subsets, according to the status of the target entity with in the document. As expected, the three flavors of the physical proximity algorithm perform much better on the corpus subsets they are adapted to. At the bottom left hand side of Table 1 , we similarly show the performance of the two flavors of the syntax -p roximity -based algorithm on the medical do-main (DRUG entities). Same as above, there is a large difference in the performance of the two flavors of the algorithm on different subsets of the problem set. Finally, at the top of Table 2 , we compare the perfor mance of the two classifica-tion -based algorithms on the two (whole) prob-lem sets, while either keeping or withholding the entity status information from the classifier. The difference in results is less pronounced here, but is still noticeable. The reason for the smaller dif-ference, we hypothesize, is the ability of the clas-sifiers to partially infer the entity status from the various context clues that are used as classifica-tion features (see the experiment 5.3). 5.3 Experiment: Automatic identification of In this experiment , we confirm that it is possible to identify the entity status within document s using supervised classification.
The results of direct evaluation show that the accuracies of the Medical and Financial corpora (using 10 -fold X -validation) are 87.8% and 82.2% respectively, and the ac curacy when using the Medical corpus for training the Financial corpus for testing and vice versa, are 78.2% and 86.1% , respectively. 
T he results of relevance detection using the automatically extracted entity status values are shown at the right hand si de of Table 1 and in the middle of Table 2 , which utilize the same da-tasets and algorithms as at the left hand side of Table 1 and at the top of Table 2 . As can be seen from the tables, the drop in performance is small, demonstrating the success of classi fication -based extraction of entity status information. 5.4 Experiment: Cross -domain applicability In this experiment , we test how well the classifi-ers trained on data from one domain work on input from a different domain.

T he classification results using dif ferent types of training data are shown in Table 3 .

Table 3 . Performance of classification -based algorithms using different training data (F 1 ).
The table confirms general independence of the classification performance on the domain. Comparing the 2 -fold and 10 -fold cross -validation results (the differ ence is equivalent to doubling the amount of training data), shows that the amount of training data is sufficient. 5.5 Experiment: Overall performance of In this experiment , we simply compare the over-all accuracy of various algorithms for relevance discernment, operating at their best parameters. The results are shown at the bottom of Table 2 . Overall, classification -based algorithms perform better than the deterministic ones, with sequence -classification performing significantly better than direct c lassification. Syntactic proximity -based is precise, but has relatively low recall, reducing its overall performance. Physical proximity -based is simplest, and produce reasonably high overall results, although worse than the best -performing classification -based methods. The results are mostly intuitively underst ood and confirm the expectations. We confirmed that relevance detection is essential for producing correct consolidated SA results. We found that the entity status within the document is o ne of the important clues for solving the relevance detection problem, and showed that this infor-mation can be effectively automatically extracted using supervised classification. We also com-pared several algorithms for relevance detection, with the result s that classification -based algo-rithms generally outperform simpler ones based on the same clues, although a very simple prox-imity -based algorithm performs reasonably well if allowed to use the entity status information. Acknowledgments This work is sup ported by the Israel Ministry of Science and Technology Center of Knowledge in Machine Learning and Artificial Intelligence and the Israel Ministry of Defense.

