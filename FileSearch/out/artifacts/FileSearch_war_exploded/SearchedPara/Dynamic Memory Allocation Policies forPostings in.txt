 We explore a real-time Twitter search application where tweets are arriving at a rate of several thousands per sec-ond. Real-time search demands that they be indexed and searchable immediately, which leads to a number of imple-mentation challenges. In this paper, we focus on one as-pect: dynamic postings allocation policies for index struc-tures that are completely held in main memory. The core issue can be characterized as a  X  X oldilocks Problem X . Be-cause memory remains today a scare resource, an allocation policy that is too aggressive leads to inefficient utilization, while a policy that is too conservative is slow and leads to fragmented postings lists. We present a dynamic postings al-location policy that allocates memory in increasingly-larger  X  X lices X  from a small number of large, fixed pools of memory. With an analytical model and experiments, we explore dif-ferent settings that balance time (query evaluation speed) and space (memory utilization).
 Categories and Subject Descriptors : H.3.3 [Information Storage and Retrieval]: Information Search and Retrieval Keywords: memory allocation; inverted indexing
The rise of social media and other forms of user-generated content challenges the traditional notion of search as operat-ing on either static document collections or document collec-tions that evolve slowly enough where periodically running a batch indexer (e.g., every hour) suffices. We focus on real-time search in the context of Twitter, where users demand to know what X  X  happening right now , especially in response to breaking news stories and other shared events. For this, they often turn to real-time search.

The context of this study is Twitter X  X  Earlybird retrieval engine [3], which serves over two billion queries a day with an average query latency of 50 ms. Usually, tweets are search-able within 10 seconds after creation (most of the latency is from the processing pipeline X  X weet indexing itself takes less than a millisecond). The service as a whole is of course a complex, distributed system with many components. In this paper, we focus on one aspect X  X ynamic memory allocation policies for postings allocation.

A key feature of Earlybird is that it incrementally in-dexes tweets as they are posted and makes them immedi-ately searchable. The indexing process naturally requires al-locating space for postings in a dynamic manner X  X e adopt a zero-copy approach that yields non-contiguous postings lists. The fundamental challenge boils down to a  X  X oldilocks Problem X , since memory today remains a scarce resource. A policy that is too aggressive in allocating memory for post-ings leads to inefficient utilization, because much of the al-located space will be empty. On the other hand, a policy that is too conservative slows the system, since memory al-location is a relatively costly operation and postings lists will become fragmented. Ideally, we X  X  like to strike a bal-ance between the two extreme and find a  X  X weet spot X  that balances speed with utilization.

We present a dynamic postings allocation policy that al-locates increasingly-larger  X  X lices X  from a small number of memory pools. The production system, previously described in Busch et al. [3], deploys a particular instantiation of a general framework, which we articulate for the first time here. Until now, we have not thoroughly explored alterna-tive parameter settings in a rigorous and controlled manner. Thus, the contribution of this paper is a detailed study of the design space for dynamic postings allocation in the con-text of our basic framework: we present an analytical model for estimating time and space costs, which is subsequently validated by experiments on real data.
We begin by relating real-time search to  X  X raditional X  (e.g., web) search. They are similar in the need for low-latency, high-throughput query evaluation, which is usually achieved using in-memory indexes. However, there are important dif-ferences as well:  X  Immediate data availability. In real-time search, docu-ments arrive rapidly, and users expect content to be search-able within seconds. This means that the indexer must achieve both low latency and high throughput. This re-quirement departs from common assumptions that index-ing can be considered a batch operation. Although web crawlers achieve high throughput, it is generally not ex-pected that crawled content be indexed immediately X  an indexing delay of minutes to hours may be accept-able. This allows efficient indexing with batch processing frameworks such as MapReduce [8]. In contrast, real-time search demands that documents be searchable in seconds .  X  Shared mutable state. A real-time search engine must han-dle shared mutable state in a multi-threaded execution environment with concurrent indexing and retrieval op-erations. In contrast, concurrency-related challenges are easier to handle in web search: for example, it is possible to atomically  X  X wap out X  an old index with an updated new index without service disruption. Such a design would be impractical in real-time search.  X  Dominance of the temporal signal. The nature of real-time search means that temporal signals are important for rel-evance ranking. This contrasts with web search, where document timestamps have a relatively minor role in de-termining relevance (news search being the obvious excep-tion). This holds implications for how postings should be organized in index structures.
Twitter X  X  production real-time search service is a complex distributed system spanning many machines, the details of which are beyond the scope of this paper. In this study, we specifically focus on Earlybird, which is the core retrieval engine. For the purposes of this paper, Earlybird receives boolean queries and returns tweets that satisfy the query, sorted in reverse chronological order. No relevance scoring is performed, which is, functionally speaking, handled by another component. Incoming tweets are hash partitioned across a number of replicated Earlybird instances, so that each individual instance serves a fraction of all tweets.
To understand our contributions, it is necessary to first provide some technical background. Here, we summarize material presented in a previous paper [3].
Earlybird is built on top of the open-source Lucene search engine 1 and adapted to meet the demands of real-time search discussed in Section 2. The system is written completely in Java, primarily for three reasons: to take advantage of the existing Lucene Java codebase, to fit into Twitter X  X  JVM-centric development environment, and to take advantage of the easy-to-understand memory model for concurrency of-fered by Java and the JVM.

As with nearly all modern retrieval engines, Earlybird maintains an inverted index: postings are maintained in for-ward chronological order (most recent last) but are traversed backwards (most recent first); this is accomplished by main-taining a pointer to the current end of each postings list (more details in the next section).

Earlybird supports a full boolean query language consist-ing of conjunctions (ANDs), disjunctions (ORs), negations (NOTs), and phrase queries. Results are returned in re-verse chronological order, i.e., most recent first. Boolean query evaluation is relatively straightforward: Lucene pro-vides an abstraction for postings lists and for traversing postings X  X arlybird provides an implementation for its cus-tom indexes, and thus is able to reuse existing Lucene query evaluation code.

A particularly noteworthy aspect of Earlybird is the man-ner in which it handles shared mutable state (concurrent in-dex reads and writes) using lightweight memory barriers. As this is not germane to the subject of this paper, we refer the reader elsewhere [3] for details. However, it is worth men-tioning that the general strategy for handling concurrency is to limit the scope of data structures that hold shared muta-ble state. This is accomplished as follows: each instance of Earlybird manages multiple index segments, and each seg-ment holds a relatively small number of tweets (2 23  X  8 . 4 million tweets). Ingested tweets first fill up a segment before proceeding to the next one. Therefore, at any given time, there is at most one index segment actively being modified, whereas the remaining segments are read-only. Once an in-dex segment ceases to accept new tweets, it is transformed from a write-friendly structure into an optimized and com-pressed read-only structure.

Because of this design, our paper is only concerned with the active index segment within an Earlybird instance: only for that index do we need to allocate memory for postings dynamically. This is described in more detail next.
As mentioned, the dominance of the temporal signal is one distinguishing characteristic of real-time search compared to web search. The implication of this is that it would be desir-able to traverse postings in reverse temporal order for query evaluation. Although this is not an absolute requirement, such a traversal order is the most convenient.

Following this reasoning further, it appears that existing approaches to index structure organization are not appropri-ate. The information retrieval literature discusses two types of indexes: document sorted and frequency/impact sorted. The latter seems unsuited for real-time search. What about document-sorted indexes? If we assign document ids to new tweets in ascending order, there are two obvious possibilities when indexing new documents:
First, we could append new postings to the ends of post-ings lists. However, this would require us to read postings backwards to achieve a reverse chronological traversal order. Unfortunately, this is not directly compatible with modern index compression techniques. Typically, document ids are converted into document gaps, or differences between con-secutive document ids. These gaps are then compressed with integer coding techniques such as  X  codes, Rice codes, or PFor [19, 20]. It would be tricky for gap-based compression to support backwards traversal. Prefix-free codes (  X  and Rice codes) are meant to be decoded in the forward direc-tion. More recent techniques such as PForDelta are block-based, in that they code relatively large blocks of integers (e.g., 128 document ids) at a time. Reconciling this with the desire to have low-latency indexing would require additional complexity, although none of these issues are technically in-surmountable.

Alternatively, we could prepend new postings to the be-ginnings of postings lists. This would allow us to read post-ings in the forward direction and preserve a reverse chrono-logical traversal order. However, this presents memory man-agement challenges, i.e., how would space for new postings be allocated? We are unaware of any work that has ex-plored this strategy. Note that the na  X   X ve implementation using linked lists would be hopelessly inefficient: linked list traversal is slow due to the lack of reference locality and pre-dictable memory access patterns. Furthermore, linked lists have rather large memory footprints due to object overhead and the need to store  X  X ext X  pointers.
Based on the above analysis, it does not appear that real-time search capabilities can be efficiently realized with ob-vious extensions or adaptations of existing techniques.
Earlybird implements the following solution: each posting is simply a 32-bit integer X 24 bits are devoted to storing the document id and 8 bits for the term position. Since tweets are limited to 140 characters, 8 bits are sufficient to hold term positions. Therefore, a list of postings is simply an integer array, and indexing new documents involves in-serting elements into a pre-allocated array. Postings traver-sal in reverse chronological order corresponds to iterating through the array backwards. This organization also al-lows every array position to be a possible entry point for postings traversal to evaluate queries. In addition, it al-lows for binary search (to find a particular document id), and doesn X  X  require any additional skip-pointers [13] to en-able faster traversal through the postings lists. Finally, this organization is cache friendly, since array traversal involves linear memory scans and this predictable access pattern pro-vides prefetch cues to the hardware.

In essence, the design punts on the problem of postings compression X  X ut we feel that this is a reasonable design choice given its simplicity and the above advantages. Fur-thermore, since the active index segment holds relatively few tweets, a particular segment doesn X  X  spend much time in the uncompressed state. Once an index segment stops accept-ing new tweets, it is converted into an optimized read-only structure: we apply a variant of PForDelta after reversing the order of the postings.

Having provided adequate background, we finally arrive at the heart of this paper: the allocation of space for post-ings lists. Obviously, this process needs to be dynamic, since postings list growth is only bounded by the size of the collec-tion itself. There are a few challenges to overcome: postings lists vary significantly in size, since term and document fre-quencies are Zipfian (roughly). As a result, it is tricky to choose the correct amount of memory to allocate for each term X  X  postings (i.e., size of the integer array). Selecting a value that is too large leads to inefficient memory utiliza-tion, because most of the allocated space for storing post-ings will be empty. On the other hand, selecting a value that is too small leads to waste: time, obviously, for mem-ory allocation (which is a relatively costly operation), but also space because non-contiguous postings require pointers to chain them together (in the limit, allocating one posting at a time is the same as a linked list). Furthermore, during postings traversal, blocks that are too small result in subop-timal memory access patterns (e.g., due to cache misses, lack of memory prefetching, etc.). This is exactly the  X  X oldilocks Problem X  described in the introduction.

Our approach to address these issues is to create four sep-arate  X  X ools X  for holding postings. Conceptually, each pool can be treated as an unbounded integer array. In practice, pools are large integer arrays allocated in 2 15 element blocks; that is, if a pool fills up, another block is allocated, grow-ing the pool. In each pool, we allocate  X  X lices X , which hold individual postings belonging to a term. In each pool, the slice sizes are fixed: they are 2 1 , 2 4 , 2 7 , and 2 11 (see Figure 1). For convenience, we refer to these as pools 1 through 4, respectively. When a term is first encountered, a 2 1 integer slice is allocated in the first pool, which is suf-ficient to hold postings for the first two term occurrences. When the first slice runs out of space, a slice of 2 4 integers is Figure 1: Organization of the active index segment where tweets are ingested. Increasingly larger slices are allocated in the pools to hold postings. Except for slices in pool 1 (the bottom pool), the first 32 bits are used for storing the pointer that links the slices together. Pool 4 (the top pool) can hold multiple slices for a term. The green rectangles illustrate the the  X  X urrent X  postings list that is being written into. allocated in pool 2 to hold the next 2 4  X  1 term occurrences (32 bits are reserved for the  X  X revious X  pointer, discussed be-low). After running out of space, a slice is allocated in pool 3 to store the next 2 7  X  1 term occurrences and finally 2 term occurrences in pool 4. Additional space is allocated in pool 4 in 2 11 integer blocks as needed.

One advantage of this strategy is that no array copies are required as postings lists grow in length X  X hich means that there is no garbage to collect. However, the tradeoff is that postings are non-contiguous and we need a mechanism to link the slices together. Addressing slice positions is accom-plished using 32-bit pointers: 2 bits are used to address the pool, 19 X 29 bits are used to address the slice index, and 1 X  11 bits are used to address the offset within the slice. This creates a symmetry in that postings and addressing point-ers both fit in a standard 32-bit integer. The dictionary maintains pointers to the current  X  X ail X  of the postings list using this addressing scheme (thereby marking where the next posting should be inserted and where query evaluation should begin). Pointers in the same format are used to  X  X ink X  the slices in different pools together and, possibly, multiple slices in pool 4. In all but the first pool, the first 32 bits of each slice are used to store this  X  X revious X  pointer.
It is evident that Earlybird represents a specific instantia-tion of a general solution to the problem of dynamically allo-cating postings for real-time search: from a small number of large memory pools, we allocate increasingly larger slices for postings as more term occurrences are encountered. Within this general framework, a particular instantiation can be de-scribed by Z =  X  z 0 ,z 1 ,...,z P  X  1  X  , the slice size settings (as powers of two), where P is the number of pools. For ex-ample, in the production deployment, Z =  X  1 , 4 , 7 , 11  X  . For best utilization of bits in addressing pointers, it is helpful to restrict | P | to a power of two also.

Note that this framework provides a general solution to real-time indexing (not only tweets): we simply assume that slices hold spaces for postings and pointers to previous slices. In the case of tweets, both postings and pointers are 32-bit integers, but nothing in our model precludes other encod-ings. Thus, for the remainder of this paper, we measure postings in terms of  X  X emory slots X . For simplicity, we as-sume that pointers also fit in a memory slot, but if this isn X  X  the case, a small constant factor adjustment will suffice.
How efficient is the current production deployment, com-pared to alternative configurations? Prior to this study, we have not attempted to answer this question in a rigorous, controlled fashion. In this paper, we tackle the question as follows: First, we define a cost model in terms of speed and memory usage, the two characteristics we seek to balance. Second, we develop an analytical model that allows us to assess the time and space costs of a particular configura-tion. Finally, for promising configurations identified by our analytical model, we follow up with experiments.
Since our analytical model makes use of real data to es-timate parameters, we begin by describing our datasets. For tweets, we used the Tweets2011 corpus created for the TREC 2011 microblog track [14], 2 which is comprised of approximately 16 million tweets over a period of two weeks (24th January 2011 until 8th February, inclusive) which cov-ers both the time period of the Egyptian revolution and the US Superbowl. Different types of tweets are present, includ-ing replies and retweets. The corpus represents a sample of the entire tweet stream, but since tweets are hash par-titioned across multiple Earlybird instances in production, experiments on these tweets are a reasonably accurate fac-simile of studying an individual Earlybird instance. Three different sets of queries were used in our evaluation. First, we took the TREC 2005 terabyte track  X  X fficiency X  queries 3 (50,000 queries total). Second, we sampled 100,000 queries randomly from the AOL query log [15], which con-tains around 10 million queries in total. Our sample pre-serves the original query length distribution. Finally, we used queries from the TREC 2011 microblog track. How-ever, since there were only 50 queries (which is insufficient for efficiency experiments), we augmented the queries by first generating the power set of all query terms and then used the  X  X elated queries X  API of a commercial web search engine to harvest query variants. In this way, we were able to construct a set of approximately 3100 queries.

Our choice of these three datasets represents an attempt to balance several factors. Although we have access to Twit-ter query logs, experiments on them would have several drawbacks: First, due to their proprietary nature, our results would not be replicable. Second, many Twitter queries are associated with trends, which are not particularly interest-ing from an efficiency point of view. Furthermore, we X  X  like to study the types of information needs that real-time search could solve, not what the service is doing right now. Thus, triangulating with three query sets paints a more complete picture: the AOL queries represent general web queries; the TREC efficiency queries are representative of ad hoc queries, closer to the  X  X orso X  of the query distribution (mostly infor-mational, as opposed to navigational); finally, the TREC mi-croblog queries represent a conception of information needs specific to such collections.
Given a collection of documents C and a set of queries Q , we define a cost function for memory usage. The total mem-ory  X  X asted X  is equal to the memory allocated for postings minus the size of the postings list (i.e., number of postings), summed across all terms t in the collection: Since postings size is constant for a given collection, we can simply define the memory cost as follows:
Similarly, let us define the time cost as the time it would take to read all postings (end to end) for all query terms in each query of Q : Note that this cost function does not actually take into ac-count time spent in query evaluation (e.g., intersection of postings lists for conjunctive query processing). We decided to factor out those costs for two reasons: First, to support a simpler model (since a large number of postings traversal techniques are available, each with different optimizations and tradeoffs). Second, even if we wished to, it is unclear how we could analytically model postings intersection time, which is a function of term occurrences in real-world data.
The advantage of our model is that instantiating it with parameters is fairly easy. If we assume that term frequen-cies in a collection follow a Zipfian distribution (a standard assumption in information retrieval), we can analytically es-timate the memory cost for various Z configurations. Simi-larly, if the postings length distribution of query terms is known, we can analytically model the time cost as well. With models of the two costs, we can find configurations that strike a desired memory/speed balance. The remain-der of this section explains how we accomplish this.
Given that the frequency of a term t in a collection is f , and the pool configuration is Z =  X  z 0 ,z 1 ,...,z P  X  1 can calculate the exact number of memory slots required to hold the postings list of term t . Let us define a step function M that maps a frequency to the number of memory slots required by configuration Z . First, we recursively define a set of thresholds  X  i  X  X  on the frequencies as follows: 4 For each term frequency interval { f r  X  N |  X  i  X  1 &lt; f the value of the step function M can be computed as follows: This function computes the amount of memory (i.e., number of slots) that needs to be allocated to store pointers along with the actual postings. Given function M , we can rewrite Equation (1) as: where f r ( t ) is the frequency of term t , and | V | is the size of the vocabulary. Making a standard simplifying assump-tion, if we rank the terms in the collection with respect to their frequencies, the resulting pairs of  X  r,  X  f r  X  (where
Note that the maximum frequency of a term is bounded and therefore the set of  X  i  X  X  is a finite set. normalized) form a Zipfian distribution, with the following probability mass function (PMF): where H  X , X  is the  X  th generalized harmonic number, and  X  is a parameter. From Equation (4), one can estimate a term frequency given the rank of term r ( t ) and the total number of terms in the collection N as: f r ( t ) = N  X  p ( r ( t )). Thus, we can rewrite Equation (3) as follows: where r is the rank (with respect to frequency) of a term in the collection. Equation (5) gives an analytical model for estimating the memory cost of indexing a particular collec-tion, given N (total number of terms) and the characteristic Zipfian parameter  X  .

Furthermore, we can speed up the computation of Equa-tion (5) by exploiting the fact that the PMF of a Zipfian distribution is a one-to-one function. In this way, based on the definition of the step function M , we have: Therefore, we can rewrite Equation (5) as follows by substi-tuting the above in the definition of M : C
To summarize, given a characteristic Zipfian parameter  X  , the total number of terms N , and a configuration Z , we can compute the memory cost of indexing a particular collection in closed form.
We now turn to our analytical model of time cost, that of the sum of reading postings lists corresponding to all query terms. Let us assume that the cost of reading postings for a configuration Z is equal to the sum of two components: (1) the cost of a sequential scan of equivalent postings lists stored as contiguous arrays and (2) the cost of following pointers that link together non-contiguous slices between different pools. The first component is the same for all con-figurations (give a collection) so we can ignore as a constant. The number of pointers for a term t with frequency f r can be computed easily given a particular configuration Z , so we can redefine our cost function as follows: where C p is the cost of following a pointer and | Pointers(  X  ) | is the number of pointers needed in a particular postings list given a configuration Z . The number of pointers can be easily estimated given the step function M defined in Section 5.1. Thus, assuming we have an estimate of the distribution of | Pointers(  X  ) | (from a query log), we are able to analytically compute a time cost.
 What about C p , the cost of following a particular pointer? Where exactly does this cost come from? Although all our index structures are held in main memory, latencies can still vary by orders of magnitude due to the design of cache hi-erarchies in modern processor architectures. Reading con-tiguous blocks of postings (in a slice) is a very fast operation since (1) neighboring postings are likely to be on the same cache line, and (2) predictable memory access when strid-ing postings means that pre-fetching is likely to occur. On the other hand, when posting traversal reaches the end of a slice, the algorithm needs to follow the pointer to the next slice and begin reading there X  X ost of the time, this will re-sult in a cache miss, which will trigger a significantly slower reference to main memory. Therefore, the cost C p is domi-nated by the cost of a cache miss. However, since we model C p as a constant, it is not necessary to estimate its actual value X  X herefore, our analytical time costs are modeled in abstract units of C p .

To summarize, we can analytically estimate the time cost if we are given the postings length distribution of query terms and the cost of a cache miss using Equation (6). We stress that this model is overly simplistic and does not ac-count for time spent intersecting postings. Nevertheless, this simplification is acceptable since we use the analytical model only to guide our experiments on real data, and in our em-pirical results we do measure end-to-end query latency.
Given a set of configurations Z = { Z 0 ,Z 1 ,...,Z m } , we can estimate the memory cost C M as well as the simplified time cost C T for any configuration Z  X  X  . However, to complete our model we need to know the total number of terms N , size of the vocabulary | V | , and parameter  X  . To determine these values, we divided the Tweets2011 collection into two equally-sized partitions and used the first half for parameter estimation; the second half is used in our actual experiments (described later). We determined  X  to be 1 . 0, and | V | and N to 11  X  10 6 and 76  X  10 6 respectively.

As explained in Section 5.2, in order to estimate the time cost we need the distribution of length of postings for a set of query terms: this is shown for all three query sets in Fig-ure 2. This figure shows that the overall distribution is simi-lar among all query sets. In particular, the distribution from the AOL and terabyte queries are nearly identical. Data from the microblog queries give rise to a similarly shaped distribution, although with less emphasis at the extremes (both very common and very rare terms).

Given all these parameters, as well as the set of configu-rations Z , we can estimate the time cost and the memory cost for each configuration. On a scatter plot of the time versus memory cost, each configuration Z  X  X  would repre-Time cost Time cost Figure 3: Scatter plots of analytical time cost C T sent a point: points closer to the origin would be considered  X  X etter X  configurations (faster, less memory).

Our strategy for exploring the configuration space was to first use our analytical model to quickly determine the trade-offs associated with a large set of configurations, and then from those select a subset on which to run actual experi-ments. We considered slice sizes between 0 and 12 (inclu-sive) and pool sizes between 4 and 8 (inclusive). Another experiment specifically focused on four-pool configurations (as in the production system). Within these ranges, we com-puted the memory and time cost for all possible configura-tions. Since a scatter plot of all configurations would not be readable, we grouped the configurations into equally-sized buckets in terms of memory cost, and from each bucket, we picked the configuration that has the smallest time cost. Figure 3 shows the scatter plot constructed in this manner, using the AOL queries for the time cost estimates (results using other queries look nearly identical, and are not shown for space considerations). The right plot shows only four-pool configurations; the left plot shows all pool sizes between 4 and 8 (inclusive).

Based on these figures, we selected a set of candidate con-figurations that appear to present good time/cost tradeoffs. As our analytical model demonstrates, after a certain point the memory costs increase while the time costs level off, thereby making most of the configurations uninteresting. The more preferable configurations are those that appear near the origin in the plots in Figure 3. The configurations selected for experimental analysis are noted. There is one additional issue we consider. Given that Earlybird maintains several index segments in memory (one  X  X ctive X , the rest read-only), it has easy access to histori-cal term statistics from preceding index segments. It stands to reason that we can take advantage of this information. Although it seems obvious that such statistics would help, there are countervailing considerations as well. We have found that there is a great deal of  X  X hurn X  in tweet con-tent [11]; for example, approximately 7% of the top 10,000 terms (ordered by frequency) from one day are no longer in the top 10,000 on the next day. This makes sense since dis-cussions on Twitter evolve quickly in response to breaking news events and idiosyncratic internet memes. Therefore, using term statistics may not actually help: a term that ap-peared frequently in the previous index segment may be re-lated to a news story that is no longer  X  X ot X , and as a result we might over-allocate memory and waste space.

To empirically determine how these factors play out on real data, we experimented with different policies for allo-cating the first slice (i.e., instead of always starting from the first pool, choose a pool with a larger slice size). We refer to this as the Starting Pool (SP) policy:  X  SP( z 0 ) : This is the default policy that does not take advantage of any term frequency history. Every allocation starts from the first memory pool (i.e., z 0 ).  X  SP( d H ( t ) e ) : This policy starts indexing a term t from the memory pool with the smallest slice size that is larger than the given historical frequency H ( t ), i.e., from the previous index segment. That is, start from pool p if  X  SP( b H ( t ) c ) : According to this policy, indexing starts from the memory pool with the largest slice size that is smaller than the given historical frequency of a term H ( t ). That is, start from pool p if 2 z p  X  H ( t ) &lt; 2 z p +1 or pool  X  SP(  X ( H ( t ) ,z P  X  1 ) ) : Based on this policy, if the frequency of a term H ( t ) is greater than or equal to the slice size starts from the last pool. Otherwise, indexing starts from the default pool, z 0 . Function  X ( H ( t ) ,z P  X  1
H ( t )  X  2 z P  X  1 and z 0 otherwise. This basically divides postings into  X  X ong X  and  X  X hort X , with the last slice size as the break point. Table 1: Memory cost ( C  X  M ), per-query postings traversal time C In all of the above policies, when we encounter an out-of-vocabulary term while indexing, we default to starting from the first memory pool (i.e., z 0 ).
To isolate the effects that we are after, our experiments were not conducted on the production codebase, but rather a separate Java implementation. This allowed us to separate unrelated issues such as management of multiple segments and query brokering from the core problem of memory al-location. Experiments were performed on a server running Red Hat Linux, with dual Intel Xeon  X  X estmere X  quad-core processors (E5620 2.4GHz) and 128GB RAM. All experi-ments were run on a single thread.

Our metrics were as follows: Evaluation of memory us-age was quantified in terms of memory slots allocated once all tweets have been indexed (denoted C  X  M ). Similarly, time costs were measured with different queries after all the tweets have been indexed. This is a simplification, since in the pro-duction system query evaluation is interleaved with index-ing. However, in production, concurrency is managed by an elaborate set of memory barriers, which is not germane to the current study. For our first time metric, we computed the per-query average time to read postings for all query terms in their entirety, i.e., These costs are measured in milliseconds, unlike in our an-alytical model. In addition, we measured the per-query av-erage time to retrieve k = 100 results in conjunctive query processing mode, i.e., the most recent 100 hits that contain all query terms (we denote this R 100 ). We used a simple lin-ear merge algorithm for postings intersection. Note that al-though more effective algorithms are available [6], it remains unclear whether they are suitable for our indexes. Those techniques implicitly assume contiguous postings lists, since they use variants of binary search to seek through postings. We felt that to isolate the effects of different query evalua-tion algorithms, linear merge was a reasonable choice.
To evaluate different policies for taking advantage of term history, we divided the Tweets2011 corpus roughly in half (chronologically). All experiments were run on the second half, using statistics from the first half (where appropriate). Note that, somewhat coincidentally, half of the Tweets2011 corpus corresponds roughly to the size of the index segments deployed in production, adding realism to our results.
Table 1 reports experimental results evaluating different pool configurations, showing memory cost ( C  X  M ), per-query postings traversal time C  X  T , and per-query top k document retrieval time ( R k ). In all cases, time is measured in mil-liseconds, and results are averaged across 3 trials, reported with 95% confidence intervals. We show results using the AOL, TREC terabyte (TB) and microblog (MB) queries in separate columns. The first row of the table shows the pro-duction configuration; the second  X  X lock X  shows select con-figurations with the number of pools between 4 and 8 (in-clusive); the third  X  X lock X  restricts consideration to 4 pool configurations (as in production). In all cases we did not take term history into account, i.e., postings allocation be-gan in the first pool, which corresponds to SP( z 0 ).
When considering the 4 pool configurations, analytical modeling suggests that the production configuration Z g bal-ances memory and time quite well (see Figure 3, right). This is indeed confirmed by our experimental results. Although during the original implementation of Earlybird no rigorous evaluations along these lines were conducted, the develop-ers nevertheless honed in on a good point in the solution space. For example, Z 0 4 and Z 0 5 yield smaller footprints, and perhaps suggest faster query evaluation, but the results are inconclusive: no significant difference in C  X  T ; significantly better for two sets of queries on R 100 but significantly worse for the third. Based on our results, it doesn X  X  appear possi-ble to consistently speed up query evaluation, regardless of configuration. On the other hand, it is possible to dramat-ically decrease memory consumption by sacrificing speed, e.g., Z 0 0 (as predicted by our analytical model).
Turning to configurations involving between 4 and 8 pools, we see opportunities to improve over the current production configuration. Configuration Z 2 , for example, yields a sub-stantially smaller memory footprint, while not slowing down query evaluation. However, the cost is more complex code to manage 4 versus 8 pools (of course, not modeled in our study). Nevertheless, these experiments point to possible future improvements in the production codebase.

Note that in this discussion, we have avoided use of the term  X  X ptimal X , since that assumes a single objective met-ric for combining time and space costs in a sensical man-ner. Judgments on the relative merits of memory footprint and speed must be made with respect to an organization X  X  resources, machine specifications, etc. Therefore, through-out this paper, we have presented all results in terms of a memory/speed tradeoff. Any additional attempts to sim-plify would be not justified by real-world constraints.
Overall, we find that the predictions made by our analyt-ical model ( C M and C T ) match the empirical results quite reasonably ( C  X  M and C  X  T ): not in terms of actual physical quantities, of course, but in terms of capturing the tradeoff between memory and speed. As we proceed from Z 0 to Z 5 , and from Z 0 0 to Z 0 7 , memory consumption increases while time trends downward. However, the overall time differences are not as large as Figure 3 would suggest (i.e., the vertical axes in the scatter plots are exaggerated). We note that time estimates produced by our analytical model are in terms of abstract C p units (cost of referencing non-contiguous post-ings), not actual time. This congruence between analytical and experimental results justifies the assumptions made in our model, and validates the use of analytical estimates to quickly explore the large configuration space (which is too large to experimentally explore). On the other hand, the match between our analytical time cost C T and top 100 re-trieval time R 100 is not as good X  X o be expected, since top k retrieval involves postings intersection, which is difficult to model analytically. This points to the limitations of our analysis and the need to perform experiments on real data.
In our second set of experiments, we investigated the im-pact of Starting Pool policies. As previously described, we divided the Tweets2011 corpus in half, gathered term statistics from the first half, and performed experiments on the second half. Experiments focused on particularly interesting pool configurations from the previous results: Z duction configuration, Z g  X  1 , 4 , 7 , 11  X  .

When taking advantage of historical term statistics, there are many issues at play. First, we would expect faster query evaluation since the postings lists are more likely to be con-tiguous. This suggests less time overall when traversing all postings ( C  X  M ), although the impact on R 100 is unknown since top 100 retrieval is unlikely to require traversal of all postings. In terms of space, there are two considerations: starting at larger slices might save memory due to fewer pointers; on the other hand, if past statistics are not en-tirely predictive, memory will be wasted. How these factors balance out is an empirical question.

Table 2 shows results for various settings on our three sets of queries. Time is measured across 3 trials with 95% confidence intervals and the table is organized in a similar manner as Table 1. Note that SP( z 0 ) is equivalent to using no term statistics, and is exactly the same as in Table 1 (row duplicated for convenience).

Results show that in all cases different SP policies waste space (i.e, results in a larger memory footprint), without a clear convincing gain in speed. For example, the most aggressive policy SP( d H ( t ) e ) is the most wasteful (8 X 16% more memory). Despite the intuitive appeal of using histor-ical term statistics, there does not seem to be a benefit, at least for the policies we studied.
The problem of incremental indexing, of course, is not new. However, the literature generally explores different points in the design space: others typically assume that the inverted lists are too large to fit in memory and therefore the index must reside on disk. Most algorithm operate by buffering documents and performing in-memory inver-sion (e.g., [9]), up to the capacity of a memory buffer. After the buffer is exhausted, inverted lists are flushed to disk; af-ter repeated cycles of this process, we now face the challenge of how to integrate the in-memory portion of the index with one or more index segments that have been written to disk. There are three general strategies. The simplest is to rebuild the on-disk index in its entirely whenever the in-memory buffer is exhausted. This strategy is useful as a baseline, but highly inefficient in practice. The second option is to mod-ify postings in-place on disk whenever possible [7, 17, 2], for example, by  X  X agerly X  allocating empty space at the end of existing inverted lists for additional postings. However, no  X  X re-allocation X  heuristic can perfectly predict postings that have yet to be encountered, so inevitably there is either not enough space or space is wasted. For the in-place strategy, if insufficient free space is available, to keep the postings contiguous, the indexer must relocate the entire inverted list elsewhere, requiring expensive disk seeks for copying the data. The third strategy avoids expensive random accesses by merging in-memory and portions of on-disk inverted lists whenever the memory buffer fills up [4, 10]: index merging takes advantage of the good bandwidth of disk reads and writes. In particular, Lester et al. [10] advocate a geometric partitioning and hierarchical merging strategy that limits the number of outstanding partitions, similar to [4]. Mar-garitis and Anastasiadis [12] present an interesting alterna-tive: when the in-memory buffer reaches capacity, instead of flushing the entire in-memory index, they choose to flush only a portion of the term space (a contiguous range of terms based on lexicographic sort order), performing a merge with the corresponding on-disk portions of the inverted lists.
Other than the obvious difference of in-memory vs. on-disk storage of the index, there is another more subtle point that distinguishes the Earlybird design. The approaches above generally try to keep postings lists contiguous since disk seeks are expensive. There is, however, substantial cost in maintaining contiguity in terms of disk operations that are needed at index time. In contrast, since Earlybird in-dex structures are in main memory, we found it acceptable for postings to be discontiguous (cf. [1]). While it is true that traversing non-contiguous postings in memory results in cache misses, this approach allow us to implement zero-copy indexing X  X nce postings are written, we never need to move them around. In a managed memory environment such as the JVM, this leads to far less pressure on the garbage collector, since buffer copying yields garbage objects.
More recently, we have begun to explore different points in the design space of in-memory incremental indexers [1]. Un-like in Earlybird, our new work examines incremental post-ings compression in a single monolithic index segment. Another interesting point in the design space is Google X  X  Percolator [16], which is built on top of Bigtable [5] X  X  dis-tributed, multi-dimensional sparse sorted map based log-structured merge trees. Percolator supports incremental data processing through observers , similar to database trig-gers, which provide cross-row transactions. This architec-ture represents a very different design from our system, which makes a fair comparison difficult: Percolator was designed to encompass the entire webpage ingestion pipeline, handling not only indexing but other aspects of document processing as well X  X hereas Earlybird is highly specialized for building in-memory inverted indexes.

Finally, a few notes about our strategy for allocating post-ings slices from fixed-size pools: there are similarities we can point to in previous work, such as the work of Brown et al. [2]. Tracing the lineage of various storage allocation mechanisms further back in time, we would arrive at a rich literature on general-purposes memory allocation for heap-based languages (e.g., malloc in C). According to the tax-onomy of Wilson et al. [18], Earlybird X  X  allocation strategy would be an example of segregated free lists, an approach that dates back to the 1960s. Of course, since we X  X e allocat-ing memory for the very specific purpose of storing postings, we can accomplish the task much more efficiently since there are much tighter constraints. Nevertheless, it would be fair to think of our work as a highly-specialized variant of general purpose memory allocators for heap-based languages.
Although the problem of online indexing is not new, we explore a part of the design space that makes fundamentally different assumptions compared to most previous work: we consider index structures that are completely in memory and applications that have much tighter index latency require-ments. There are many challenges for such applications, and we examined in depth one particular issue X  X ynamic postings allocation X  X ithin a general framework for incre-mental indexing defined by a production system. Beyond sharing specific results, we hope to achieve the broader goal of bringing real-time search problems to the attention of the research community and to spur more work in this area. This work has been supported in part by NSF under awards IIS-0916043, IIS-1144034, and IIS-1218043. Any opinions, findings, or conclusions are the authors X  and do not neces-sarily reflect those of the sponsor. The first author X  X  deep-est gratitude goes to Katherine, for her invaluable encour-agement and wholehearted support. The second author is grateful to Esther and Kiri for their loving support and ded-icates this work to Joshua and Jacob.
