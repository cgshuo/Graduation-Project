 Conformal Prediction (CP) [1] is a framework for producing reliable confidence measures associated with the predictions of an underlying classification or regres-sion model. Given a confidence level  X   X  (0 , 1), a conformal predictor outputs prediction regions that, in the long run, contain the true target value with a probability of at least 1  X   X  . Unlike Bayesian models, CP does not rely on any knowledge of the apriori distribution of the problem space; and, compared to the PAC learning framework, CP is much more resiliant to noise in the data.
Clearly, the motivation for using CP is the fact that the resulting predici-ton regions are guaranteed to be valid. With this in mind, it is vital to fully understand what validity means in a CP context. Existing literature (e.g. [1]) provides a thorough explanation of how the validity concept relates to conformal classification, but leaves something to be desired regarding conformal regression.
In this paper, we identify an inherent but non-obvious weakness associated with the most common type of inductive conformal regressor  X  conformal regres-sors where the nonconformity score is based on the absolute error of a predictive regression model (Abs. Error CP Regre ssion, or AECPR). Specifically we show that when the underlying model has a skewed error distribution, AECPR pro-duces unbalanced prediction intervals  X  prediction intervals with no guarantee regarding the distribution of errors above and below the prediction interval  X  and argue that this limits the expressiveness of AECPR models. We suggest to instead produce two one-tailed conformal predictors, one for the low end of the prediction interval and one for the high end. The only modification needed is to use a nonconformity score based on the signed error of the underlying model. Once we have these two conformal predictors, they can either be used to output valid one-tailed predictions, or combined to create two-tailed prediction intervals that exhibit a stronger guarantee of validity than standard AECPR prediction intervals. In addition, we show that the suggested approach is more robust, i.e., less sensitive to outliers. In particular when  X  is small, AECPR may be seriously affected by outliers, resulting in very cons ervative (large) prediction intervals. CP was originally introduced in [2], and further developed in [3], as a trans-ductive approach for associating classification predictions from Support Vector Machine models with a measure of confidence. Vovk, Gammerman &amp; Shafer pro-vide a comprehensive guide on conformal classification in [1], and Shafer &amp; Vovk provide an abridged tutorial in [4]. Since its introduction, CP has been frequently applied to predictive modeling and used in combination with several different classification and regression algorithms, including Ridge Regression [5] k-Nearest Neighbors [6], Artificial Neural Networks [7, 8] and Evolutionary Algorithms [9].
In [10] and [5], Papadopoulos proposes a modified version of CP based on inductive inference called Inductive Conformal Prediction (ICP). In ICP, only one predictive model is generated, thus avoiding the relative computational in-efficiency of (transductive ) conformal predictors.

Conformal predictors have been applied to a number of problems where con-fidence in the predictions is of concern, including prediction of space weather parameters [8], estimation of software project effort [11], early diagnostics of ovarian and breast cancers [12], diagnosis of acute abdominal pain [13] and as-sessment of stroke risk [14]. 2.1 Conformal Prediction seen input pattern x j , the general idea behind CP is to consider each possible target value  X  y and determine the likelihood of observing ( x j ,  X  y )in Z .
To measure the likelihood of observing ( x j ,  X  y )in Z , a conformal predictor first assigns a nonconformity score  X   X  y i to each instance in the extended set  X  Z = Z  X  X  ( x instance ( x i ,y i )  X   X  Z compared to the rest of the set, and is, in a predictive modeling scenario, often based on the pred ictions from a model generated using a traditional machine learning algorithm, referred to as the underlying model of the conformal predictor. The underlying model is trained using  X  Z as training data, and the nonconformity score for an instance ( x i ,y i )  X   X  Z is defined as the level of disagreement (according to some e rror measure) between the prediction of the underlying model  X  y i and the true label y i .
The nonconformity score  X   X  y j is compared to the nonconformity scores of all other instances in  X  Z to determine how unusual ( x j ,  X  y ) is according to the non-conformity measure used. Specifically, we calculate the p -value of  X  y using
A key property of conforma l prediction is that if p (  X  y ) is below some threshold at most 5% likely that  X  y isthetruelabelfor x j .These p -values are calculated for each tentative label  X  y , and the conformal predictor outputs a prediction region true label of x j with probability 1  X   X  . Given that we already know the probability of any prediction region containing the true output for a test instance x j ,the goal in CP is not to maximize this probability, but rather to minimize the size of the prediction regions. In essence, CP performs a form of hypothesis testing. with Z ,andforevery  X  y we are able to reject, we reduce the size of the prediction region, thus increasing the efficiency of the conformal predictor.

Since ( x j ,  X  y ) is included in the training data for the underlying model, the model needs to be retrained for each tentative label  X  y ; as such, this form of CP suffers from a rather poor computational complexity. ICP, as described in the next subsection, solves this problem by dividing the data set into two disjunct subsets: a proper training set and a calibration set. 2.2 Inductive Conformal Prediction An ICP needs to be trained only once, using the following scheme: 2. Train the underlying model h Z using Z as training data. 3. For each calibration instance ( x i ,y i )  X  Z :
For a novel (test) instance we simply supply the input pattern x j to the under-lying model and calculate  X   X  y j using our nonconformity function. The p -value of each tentative label  X  y is then calculated by comparing  X   X  y j to the nonconformity scores of the calibration set:  X  y is the true output of x j ,and  X  y is thus excluded from the prediction region. 2.3 Inductive Conformal Regression In regression it is not possible to consider every possible output value  X  y ,sowe cannot explicity calculate the p -value for each and every  X  y . Instead a conformal regressor must effectively work in r everse. First, the size of the (1  X   X  )-percentile nonconformity score,  X  s (  X  ) , is determined; second, the nonconformity function is used to calculate the magnitude of error that would result in x j being given a nonconformity score at most  X  s (  X  ) ; i.e., the conformal regressor determines the largest error that would be commited by the underlying model when predicting y j with probability 1 nonconformity function, typically using the absolute error, see e.g., [5 X 8]:
Then, given a significance level  X  and a set of calibration scores S , the goal score  X  and, due to the definition of (3), also the largest absolute error  X  with probability 1  X   X  . To do this, we simply sort the calibration scores in a descending order, and define the prediction interval as of nonconformity scores. Since the underlying model X  X  error is at most  X  s (  X  ) with probability 1  X   X  , the resulting interval covers the true target y j with probability 1  X   X  . Note that when using (3) and (4) the conformal regressor will, for any specific significance level  X  , always produce prediction intervals of the same size for every x j ; i.e., it does not consider the difficulty of a certain instance x j . Papadopoulos et al. [5] suggest that prediction intervals can be normalized using some estimation of the difficulty of each i nstance, e.g., by using a separate model for estimating the error of the underlying predictor. In this paper, we will not consider normalized nonconformity scores, but leave them for future work. AECPR will, as described in the Background, always produce  X  X ymmetrical X  prediction intervals where the underlying model X  X  prediction is the center of the interval, and the distance from the interval X  X  center to either boundary is equal to  X  s (  X  ) , i.e., the absolute error from the calibration set associated with the significance level 1  X   X  . If the errors of the underlying model are symmetri-cally distributed  X  i.e., the underlying model is equally likely to underestimate and overestimate the true output  X  AECPR will always yield optimal inter-val boundaries in the sense that neither boundary is overly optimistic nor overly pessimistic in relation to the error distribution of the model (as estimated on the calibration set). However, when the error distribution of the underlying model is skewed, it is possible for one of the boundaries to become overly optimistic, while the other becomes overly pessimistic , simply because the errors committed in one direction will influence the nonconformity scores, and consuequently the pre-diction intervals, in both directions. Figure 1 shows an example of a skewed error distribution, where the AECPR nonconformity scores of the underlying model (a neural network) fail to capture the model X  X  tendency to produce smaller negative errors (overestimation) and larger positive errors (underestimation).
AECPR is proven valid [5], and thus there is no need to suspect that the mismatch between absolute error nonconformity scores and skewed error distri-butions would lead to invalid prediction intervals; however, it is necessary to be very specific about what validity means in the context of AECPR. Given that AECPR operates on the magnitude of the underlying model X  X  error, the validity applies to the magnitude of errors and nothing else; i.e., AECPR guarantees that the absolute error of the underlying model is no larger than  X  s (  X  ) with probability 1  X   X  . However, without considering the underlying model X  X  tendency to commit positive or negative errors, AECPR cannot provide information regarding how  X  is distributed above and below the prediction interval.

Without this information, it is not possible for a user of AECPR to distinctly assess the validity of the prediction boundaries. To illustrate, consider a 95%-confidence prediction interval on the form (  X  1 , 1). If asked to assess the likelihood of the true value y j being greater than 1, one is easily tempted to assume a probability of 2.5%, since intuitively, the probability of y j being greater than the interval X  X  upper boundary should be about the same as the probability of y j being less than the interval X  X  lower boundary. The true answer however, is that AECPR can only guarantee that there is at most a 5% probability of y j being less than  X  1, and at most a 5% probability of y j being greater than 1, since we have no information of the probabilities of the true value being higher or lower than the prediction interval X  X  upper and lower boundaries respectively. In the following subsections, we expand on this argument, and propose a straightforward method for producing prediction intervals that possess a stronger guarantee of validity for the individual boundaries than for the full interval, while maintaining efficiency. 3.1 Validity of Interval Boundaries If  X 
Y =(  X  Y low ,  X  Y high ) is a valid prediction region at  X  = d , the one-tailed predic-as they both cover at least the same error probability mass covered by  X  Y .How-ever, without any knowledge of how the error probability d is distributed above and below  X  Y it is not possible to assume that one specific one-tailed prediction is valid at any  X &lt;d . Hence, we can, in fact, only be confident in the one-tailed (  X  X  X  ,  X  Y
On the other hand, if (  X  X  X  ,  X  Y high )and(  X  Y low , +  X  ) are both known to be valid at  X  = d 2 , we know by definition that either of these one-tailed predictions will be incorrect with probability at most d 2 . Thus, if the two one-tailed predictions are combined into a prediction interval  X  Y c =(  X  Y low ,  X  Y high ), we can guarantee that  X  Y c will be wrong in a specific direction with probability at most d 2 ,and in total with probability at most d . Now, we are able to express not only a confidence 1  X  d for the interval, but also a greater confidence 1  X  d 2 for the when two one-tailed predictions are combined into a two-tailed interval, the probability of the resulting interval being wrong is the sum of the probabilities of the boundaries being wrong.

Using AECPR, prediction intervals with boundaries guaranteed at  X  2 can be we X  X e not only  X  X ncreasing X  the guarantee of validity for the individual boundaries in  X  terval is unnecessarily large! We would much rather output a combined interval the predicted boundaries  X  Y low and  X  Y high . We note the following: if the underlying model X  X  predictions tend to underestimate the true output values, we are only re-quired to adjust the upper boundary of the prediction intervals for them to remain valid; similarly, if the underlying model X  X  predictions tend to overestimate, we are only required to adjust the lower boundary. Hence when predicting  X  Y low we only need to consider the negative errors made by the underlying predictor; and, when predicting  X  Y high , we only need to consider the positive errors made by the under-lying predictor. That is, we can construct the interval boundaries by considering only about half of the errors commited by the underlying model, thus reducing the expected size of each boundary while maintaining validity in the one-tailed pre-dictions. This follows directly from the semantics of validity we are applying to the lower and upper boundary predictions  X  in both cases, we are only interested in guaranteeing validity in a single direction, i.e., for the upper boundary, we want to and for the lower boundary we want to guarantee that the probability of y j being boundary of y j , we can define the nonconformity measure as  X  i = y i  X   X  y i , i.e., a nonconformity function that returns larger nonconformity scores with larger pos-itive errors, and define the prediction interval as (  X  X  X  ,  X  y j +  X  s (  X  guarantee that the true value y j  X   X  y j +  X  s (  X  we can define the nonconformity score such that it increases with larger negative errors, i.e.,  X  i = X  y i  X  y i , output the prediction interval ( X  y j  X   X  s (  X  guarantee that y j  X   X  y j  X   X  s (  X 
From this point, we are able to do one of two things: we can output either (  X  X  X  ,  X  Y combine the two one-tailed predictions, and guarantee the boundaries at 1  X   X  2 , and the interval at 1  X   X  . 3.2 Signed Error CPR To approximate the (potentially skewed) error distribution of the underlying model, we propose a nonconformity measure based on the signed error of the model; i.e., we define the nonconformity score as
Just as in AECPR, we sort  X  in a descending order  X  note though, that while the sorted  X  in AECPR contains the absolute errors from largest to smallest, the sorted  X  in SECPR ranges from maximum positive error to maximum negative error. The prediction int erval for a novel instance x j is then formulated as where high (  X  2 )= performs two simultaneous conformal predictions  X  one for each boundary of the interval. The boundaries are predicted with confidence 1  X   X  2 and, when combined, form a prediction interval with confidence 1  X   X  . 3.3 Evaluation The two methods (AECPR and SECPR) were evaluated on 33 publicly available data sets from the UCI [15] and Delve [16] repositories. Before experimentation all output values were scaled to [0 , 1], only to enhance interpretability in effi-ciency comparisons  X  with the outputs scaled to [0 , 1], the size of a prediction interval expresses the fraction of possible outputs covered by the interval. For each data set, 100 random sub-sampling tests were performed; in each iteration, a randomized subsample (20%) of the data set was used as the test set, and re-maining instances were used for training and calibration. The calibration set size was defined as a function of the training set size: | Z | = 100
Standard multilayer perceptron neural networks with were used as underlying models for the conformal predictors, where k is the number of input features of each data set. In each iteration, both AECPR and SECPR predictions were calculated from the same model and calibration instances. Given any  X   X  (0 , 1), an ICR should produce valid prediction intervals  X  in reality however, predictions with low confidence are rarely of interest. Thus, we choose to show the empirical validity and evaluate the efficiency of AECPR and SECPR at three commonly used confidence levels: 99%, 95% and 90%. 4.1 Validity of Intervals As illustrated in Table 1, both methods produce prediction intervals that cover the true targets of the test set at or very close to the predefined significance levels; thus, in terms of interval coverage both methods are, as expected, valid. 4.2 Validity of Boundaries Here, we take a closer look at the coverage of the lower and upper boundaries of the intervals produced by AECPR and SECPR. Specifically, we expect AECPR and SECPR to have a clear difference in coverage of their one-tailed predictions (  X 
Y 1
Table 2 reveals that AECPR X  X  interval boundaries show only a small deviance from 1  X   X  2 -validity on average (across all data sets). However, more often than not, one of the boundaries is overly optimistic and the other overly pessimistic to compensate, and the reason the two interval boundaries appear valid on average is the simple fact that they are alternatingly optimistic and pessimistic. We also note that, in the worst cases (e.g. the treasury data set), one of the boundaries is valid only at 1  X   X  . In contrast, SECPR interval boundaries show coverage at or very near 1  X   X  2 -validity in both the average and worst cases.

To further illustrate, we let the D  X  p be the average coverage of the boundary (low or high) that has the highest average coverage for data set D (the most pessimistic boundary), and we let D  X  o be the average coverage of the boundary with the lowest average coverage for D (the most optimistic boundary). E.g., for abalone, AECPR has D 99 o =0 . 991 and D 99 p =0 . 999; and SECPR has D 99 o = each  X  , and for both AECPR and SECPR (Table 3).

In Table 3 we can clearly see that, as we argued in the Method, AECPR intervals show a tendency towards having an overly pessimistic boundary and an overly optimistic boundary, in such a way that the error probability  X  above and below the interval is unevenly distributed. Furthermore, we are not given any information regarding the distribution of  X  , and thus as noted in the Method and supported by the empirical evidence, AECPR can only guarantee the validity of its interval boundaries at 1  X   X  .

SECPR intervals can guarantee the interval boundaries at 1  X   X  2 ; so, this state-ment is supported by the empirical evidence. We also note that SECPR tends to produce balanced intervals  X  i.e., intervals where  X  is evenly distributed above and below the prediction intervals. Mor e importantly, even in the cases where the intervals are not perfectly balanced, we have already defined the boundaries to be valid at 1  X   X  2 . 4.3 Efficiency We can choose to compare the interval sizes of AECPR and SECPR (Table 4) based on two different criteria: either we compare intervals that share the same guarantee of validity for the full interval; or, we compare intervals that share the same guarantee of validity for the individual boundaries. That is, we either compare the 99% SECPR intervals to the 99% AECPR intervals, and so on, and remember that SECPR provides a stronger guarantee of validity for the boundaries than does AECPR; or, we compare the 90% SECPR intervals to the 95% AECPR intervals, and so on, and remember that the two methods in this case provide the same guarantee for the one-tailed prediction boundaries.
First, we consider predictions that share the same guarantee of validity for the full interval. Here, we note that on average SECPR produces tighter intervals than AECPR at the 99% confidence level, due to SECPR taking into account the sign of the underlying model X  X  error. If the underlying model commits large errors in only one direction, only one of the interval boundaries predicted by SECPR is affected, while both boundaries are affected in AECPR. Thus, for data sets where the underlying model commits outlier errors  X  atypical errors that are of a much larger magnitude than typical errors  X  in only one direction, SECPR will produce tighter intervals than AECPR. It must be noted that while this applies to all significance levels, most of the outlier errors will, for lower signficance levels, be excluded from the prediction intervals anyway. Consuequently, at the 95% confidence level we observe a similar pattern, but the effect is much less pronounced. At 90%, the effect is all but gone, and SECPR is instead slightly less effective than AECPR on almost all data sets. A Wilcoxon signed-ranks test at  X  =0 . 05 shows that, in terms of efficiency, SECPR is significantly better than AECPR for 99%-confidence predictions, while AECPR is significantly better than SECPR for 90%-confidence predictions. Thus, at 90% confidence or lower, we can expect that the strengthened gua rantee of validity provided by SECPR is accompanied with a small but si gnificant decrease in efficiency.

Second, we consider one-tailed predictions, specifically at the 95%-confidence level (i.e., AECPR at 95%, SECPR at 90%); here, we can clearly see that SECPR produces tighter intervals than AECPR f or all data sets. Hen ce, simply by ensur-ing that the boundaries are affected only by the relevant errors of the underlying model, we can significantly increase the efficiency of one-tailed predictions. In this paper, we have shown that conformal regressors based on the absolute error of the underlying model produce unbalanced prediction intervals  X  inter-vals with no guarantee for the distribution of error above and below the intervals  X  and that this unbalance leads to prediction intervals with weak guarantees of validity for the individual interval boundaries. To address this issue, we have proposed a straightforward approach for producing prediction intervals based on the signed error of the underlying model (SECPR) that can provide a stronger guarantee of validity for the interval boundaries. Also, we have shown that SECPR is less sensitive to outlier errors than AECPR, resulting in more effi-cient prediction intervals at the highest confidence levels. Finally, we show that, when expected to provide the same guarantees of boundary validity as AECPR, SECPR produces much more efficient predi ction intervals than corresponding AECPR models.

