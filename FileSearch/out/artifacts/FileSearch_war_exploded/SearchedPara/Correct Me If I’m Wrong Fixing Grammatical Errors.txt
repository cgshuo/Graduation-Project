 The detection and correction of grammatical errors still rep-resent very hard problems for modern error-correction sys-tems. As an example, the top-performing systems at the preposition correction challenge CoNLL-2013 only achieved a F1 score of 17%. In this paper, we propose and extensively evaluate a series of approaches for correcting prepositions, analyzing a large body of high-quality textual content to capture language usage. Leveraging n-gram statistics, as-sociation measures, and machine learning techniques, our system is able to learn which words or phrases govern the usage of a specific preposition. Our approach makes heavy use of n-gram statistics generated from very large textual corpora. In particular, one of our key features is the use of n-gram association measures (e.g., Pointwise Mutual Infor-mation) between words and prepositions to generate better aggregated preposition rankings for the individual n-grams. We evaluate the effectiveness of our approach using cross-validation with different feature combinations and on two test collections created from a set of English language exams and StackExchange forums. We also compare against state-of-the-art supervised methods. Experimental results from the CoNLL-2013 test collection show that our approach to preposition correction achieves  X  30% in F1 score which re-sults in 13% absolute improvement over the best performing approach at that challenge.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Text analysis ; I.7.m [ Document and Text Process-ing ]: [Miscellaneous] Algorithms, Experimentation preposition correction; n-gram statistics; supervised learn-ing; pointwise mutual information Grammatically correct textual content is highly valuable: On one hand, grammatically correct textual content is easier to understand by humans. On the other hand, automated systems (such as machine translation or speech recognition systems) can produce better results by integrating grammar correction techniques.

Compared to editorially curated content  X  such as news articles or e-commerce product pages  X  online User-Generated Content (UGC) is much more likely to contain grammatical errors. This is due to many factors, including the short time dedicated to content writing and proof-reading, but also to the fact that online authors may not be native speakers of the language they use to produce new content.

In this paper, we address the task of automatically correct-ing grammatical errors in textual content, focusing in par-ticular on English language and on the problem of preposi-tion correction. Prepositional errors have received relatively little attention by the research community, despite their im-portance. According to Leacock et al. [13], prepositional errors are the second most common error made by English learners and account for about 13% of all errors. Possible applications of our approach include correcting grammatical errors in a type-as-you-go fashion for Web applications, or curating textual content for further automated processing.
In this paper, we propose a set of approaches based on an n-gram decomposition of the input sentences. Specifically, our techniques indicate which preposition should most likely be used in a given sentence based on statistical evidence of words X  associativity extracted from a large collection of En-glish books. Our approach generates a ranked list of prepo-sitions, which are ordered by their likelihood of being correct for the given query sentence.

In order to evaluate our proposed approach and to ex-perimentally compare with state-of-the-art techniques, we rely both on standard evaluation collections as well as on a newly created dataset. Standard collections for this task are usually built from English language exams, which con-tain manually labeled errors made by non-native English speakers. The new dataset that we create uses data from the Stack Exchange forum websites 1 . This second dataset is more challenging than commonly used collections since the language used on online social platforms is typically more informal than that of English exams or curated content.
In summary, the main contributions of this paper are:
The rest of this paper is structured as follows. We discuss related work in the area of preposition correction below in Section 2. Section 3 introduces the task we address, while Section 4 gives an overview of our system and describes how we leverage a large corpus of n-grams to detect and correct grammatical errors. In Section 5, we define the different features used by our approach and how they are combined by means of machine learning. We experimentally evaluate the different approaches we propose on several collections in Section 6, before concluding in Section 7.
Grammatical error correction is a popular task in the NLP community, where identifying and correcting wrong prepo-sitions is studied as well. A recent initiative [5] introduces the task of preposition and determiner error detection and correction. The task is split into 3 parts X  X etection, recog-nition, and correction. The detection task is about deter-mining if something is wrong in a text; The recognition task is about identifying the error and its type; The correction task, finally, is about proposing a correction that matches the gold standard. The approach we adopt in this work cov-ers the three tasks as, given a sentence, it replaces some of the prepositions whenever it determines that the preposi-tions are wrong. We can thus evaluate our entire pipeline on the final output as compared to the gold standard cor-rections and compare our system against state-of-the-art ap-proaches. In this initiative, the training collection consists of the Cambridge Learner Corpus (CLC)  X  X irst Certificate of English X  (FCE) examinations of 2000 and 2001 created by Cambridge University. The two top-performing systems [4] and [12] respectively achieved 42%/17% and 61%/5% in terms of Precision/Recall. Both systems adopted multi-class classification approaches to choose the correct preposition from a predefined confusion set. In their work, the authors applied a sophisticated set of features, including lexical fea-tures, POS tags, head verbs and nouns, Web n-gram counts and word dependencies. The main difference between these two systems stems from using different preposition candi-date sets. The system with lower recall and higher precision values used a set of only 11 most common prepositions, while http://stackexchange.com/ the other system used a set of 36 prepositions. However, the test collection used in this challenge was not made publicly available, thus it is not possible for our research to compare with these results directly.

The more recent CoNLL-2013 Shared Task on grammati-cal error correction [16] organized by the National University of Singapore developed a new test collection from scratch us-ing essays written by university students. The collection was made publicly available for further use and studies. In this work we compare our approach against the participating sys-tems from this shared task. The top-performing systems in the preposition correction task used variations of statistical machine translation approaches and achieved a maximum F1 score of 17.5%, which is significantly less then the F1 score we achieve using the solution proposed in this paper. Other systems used either machine learning or language modeling approaches [11][19], including the ones based on statistical n-gram counts from large corpora, the most common one being the Google Web-1T corpus 2 .

Another approach to grammatical error correction is to re-duce the correction task to a language disambiguation task, following the assumption that the context of a preposition can completely determine the preposition itself. This allows to use high-quality texts as both training and test collections by simply omitting existing prepositions and choosing the potentially correct preposition based on its context solely. In this context, Bergsma et al. [1] applied unsupervised techniques for preposition selection by using the log-counts of the n-gram frequencies appearing in their context. In this case, the context consisted of a sliding window of n-grams around the preposition, where n was ranging from two to five. Additionally, the authors used supervised techniques with a very large training set 3 (100k+ samples) to learn linear combinations of the n-gram counts used in the unsu-pervised approach, observing minor effectiveness increase as compared to their unsupervised approach. Unfortunately, such approaches do not perform well on real-world error correction tasks since the number of incorrect prepositions is often much lower than the number of correct ones (e.g., around 5% of errors) in English learner collections. Thus, rather than simply omitting it, leveraging statistics about the existing preposition in a sentence becomes an impor-tant piece of evidence. In our work, we consider the original preposition and the probability of a writer confusing it with others.

Additional work in [6] shows that leveraging big-data n-gram statistics from the Web yields better performance com-pared to traditional linguistic features. In [8], Heilman et al. extend the approach discussed above [1] by complementing the n-gram log-count method with rule-based and supervised techniques. N-gram count statistics were also successfully applied to various Information Retrieval problems such as named entity recognition [18], query spelling correction and query segmentation [9]. Some of the most recent approaches to grammatical error correction integrate large collections of n-gram counts together with supervised [20] or unsupervised [10] techniques. As compared to such approaches, we also leverage a large n-gram corpus in our work but in addition focus on features like skip n-grams and n-gram distance. https://catalog.ldc.upenn.edu/LDC2006T13
They used the New York Times (NYT) section of the Giga-word http://catalog.ldc.upenn.edu/LDC2003T05 corpus.
We focus in this paper on correcting preposition usage at the sentence level. A recent approach proposed in [3] is able to analyze complete sentences and correct multiple er-rors that have interdependencies. Compared to this piece of work, the problem we address in this paper is more fo-cused as it aims at correcting the usage of prepositions . Our focused approach allows us to obtain some significant effec-tiveness improvement (i.e., 13% absolute improvement) over state-of-the-art approaches for preposition correction.
As compared to previous work from the NLP field, our work tackles a more challenging issue, which is applying grammar correction to Web content rather than simply proposing error correction for academic texts produced by ESL writers. In this section, we define the problem we are tackling. Furthermore, we introduce the datasets we use in our evalu-ation and present the metrics we use to compare to previous systems.
In the following, we define the task we address in this work. Given a sentence in English that contains a preposi-tion, our system generates a ranked list of prepositions that could be used in place of the original one. In case the top ranked preposition selected by our system matches the orig-inal preposition, our system performs no correction. In the other case, the system suggests a list of ranked prepositions as alternatives to the preposition used in original sentence by the author.

Formally, given a sentence s = t 1 ..p j ..t m consisting of a list of tokens t i and a preposition p j , and given a candidate preposition set P = { p 1 , .., p m } , the task of preposition se-lection consists in generating a list of prepositions from P ranked by their likelihood of being correct in order to po-tentially replace p j . If the top ranked preposition is equal to p , then the sentence is considered correct by the approach. Otherwise, the the sentence is considered as incorrect and the top-1 preposition is selected instead. In this work, we use the set of the 49 most frequent English prepositions as our candidate preposition set P 4 .
As mentioned above in Section 2, there exist different training and test collections for grammatical error correc-tion. Since we are addressing errors that people make when producing textual content, the most preferable option would be to reuse an existing collection of English learners X  texts. The survey in [13] gives a comprehensive overview of avail-able datasets of this type. Unfortunately, most of the exam correction datasets are proprietary and not publicly avail-able.
 However, as mentioned in Section 2, the CoNLL-2013 Shared Task on grammatical error correction published their about, above, absent, across, after, against, along, along-side, amid, among, amongst, around, at, before, behind, below, beneath, beside, besides, between, beyond, but, by, despite, during, except, for, from, in, inside, into, of, off, on, onto, opposite, outside, over, since, than, through, to, toward, towards, under, underneath, until, upon, with. test dataset consisting of 50 essays written by 25 non-native English students from the National University of Singapore. Thus, we are using CoNLL-2013 dataset as our primary test collection for results comparison. As a training collection, we decided to use the standard Cambridge First Certificate in English (FCE) examinations from 2000-2001, which was also permitted by the rules of the CoNLL-2013 Shared Task.
Most of the available datasets for this task were built using collections that were generated in an exam context, where learners know in advance that they need to do their best in terms of grammatical correctness. On the contrary, text written by non-native speakers on the Web usually contains many more grammatical errors because of the informal en-vironment. Thus, in our work we want to additionally focus on correcting the actual errors made by people online. The Stack Exchange Q&amp;A website network represents one exam-ple of UGC web sites, where users ask field-specific questions and others answer them.

With respect to our task, the Stack Exchange (SE) net-work possesses two very valuable properties: The SE public dataset contains the edit history of every post (question or answer) from the SE network together with the comments accompanying the edits.

To create an evaluation collection of grammatical errors with corresponding corrections, we processed this dataset and extracted all edits that contained the string  X  X rammar X  in their comment field. For each extracted edit, we compared the original and the edited versions of the post and then only extracted those edits that modified one of the prepositions from our candidate preposition set. 7 The exact version of the SE data dump we used is from March 2013.

The statistics for the collections we are using in this work are summarized in Table 1. As compared to test collections based on academic text, where each document is manually checked by an expert, we do not have the full ground truth here as some of the errors might have been overlooked by the Web community. Therefore, we only extract sentences that have at least one error correction. Thus, the relative error percentage for the SE collection (38.2) is not directly comparable with the ones we observe in the academic col-lections.

As we mentioned earlier, we use the CoNLL-2013 dataset as a primary evaluation collection to compare the proposed approaches to the state-of-the-art, while we use the SE col-lection to determine if our proposed techniques are applica-ble on Web content as well.
We evaluate our approaches in terms of Precision, Recall and F1 score, which are defined for grammatical error https://www.quantcast.com/stackoverflow.com/geo/ countries https://archive.org/details/stackexchange The collection is available online at: https://github.com/ XI-lab/preposition-data-cikm2014 . the last one has been constructed based on StackExchange edit history. Figure 2: Tokenization example, PREP is a preposi-tion placeholder. correction as follows: Precision = valid suggested corrections total suggested corrections Recall = valid suggested corrections total valid corrections
F1 = 2  X  P recision  X  Recall P recision + Recall
The overall system for preposition correction works as fol-lows (see Figure 1). Starting from a textual document, the system extracts sentences and tokenizes them. The gen-erated list of tokens is used to produce n-grams, compute n-gram distances (Section 4.2), and to compute association measures based on external n-gram statistics (Section 4.3). These evidences are then used to produce features that al-low a supervised classification component to select the most appropriate preposition in the context of a sentence (Sec-tion 4.5). Finally, the possibly corrected sentences are ag-gregated back into the document. Within this pipeline, in Section 5 we focus on how we use n-gram information to generate features for preposition ranking and selection.
Given an input sentence, the preprocessing pipeline of our system consists of tokenizing the sentence and generating n-grams that contain pairs of (n-1)-grams and the preposition. To better understand the pipeline, we make use of the exam-ple tokenization in Figure 2 and Table 2. Here, the sentence is tokenized and we first consider all contiguous (n-1)-grams excluding the preposition itself. Second, we take these (n-1)-grams and generate all possible n-grams by adding the preposition, respecting the relative position of the preposi-tion to the (n-1)-gram 8 .
If multiple prepositions occur in the same sentence, we form n-grams for each preposition independently.
 Table 2: Example n-gram types and distances for the tokenization example on Figure 2 where j = 5 .

Next, we define the n-gram distance of an n-gram contain-ing a preposition based on the underlying (n-1)-gram used to generate it. In detail, given a tokenized sentence { t 1 with a preposition in position j , we define the distance of an (n-1)-gram [ i, i + n  X  2] as where C = min ( | i  X  j | , | i + n  X  2  X  j | ).

Based on the definition of n-gram distance, we classify n-grams containing a preposition into three major classes based on their n-gram distance:
Note that in our work we consider punctuation marks as n-gram tokens; While for some applications punctuation needs to be filtered out, in our setting preposition usage can of-ten be governed by their position with respect to a certain punctuation symbol.
As discussed in Section 2, most of the previous approaches tackled the preposition correction task by means of prepo-sition selection via multi-class classification among prepo-sition confusion sets. Contrary to this, we propose an ap-proach to rank prepositions in a sentence according to some confidence score. This confidence score in turn incorpo-rates information from the individual n-gram rankings of the prepositions.

The motivation behind our approach is that the usage of a certain preposition is often governed by a particular word the suggested preposition for every sentence.
 Table 3: Sample PMI scores for the tokenization example.
 or a phrase in a sentence. While previous approaches con-sidered only a short window of other n-grams in a certain vicinity near the preposition, we are instead considering all possible n-grams in a sentence. In order to limit the com-plexity and avoid considering a large number of n-grams, we start from the preposition position and build n-grams by limiting the n-gram distance (see Section 4.2) we consider. We experimentally compare different n-gram distances in Section 6.3.

In detail, for all n-grams composing a sentence, our sys-tem generates a ranking of prepositions according to some n-gram association measure. The n-gram association mea-sure is used to compute a score that is proportional to the probability of an n-gram appearing together with a given preposition. In the literature, a number of association mea-sures between words were proposed (see [14], Chapter 5), each one having its own advantages and disadvantages. In this work, we experimented with three association measures: Point-wise Mutual Information (PMI) [2], a variant of the Mutual Information and the Student X  X  t-test[14], and found that PMI yields the best results in our context.

Briefly, PMI measures the gap between the probability of two variables being the same given their joint distribution and their individual distributions. The statistical frequen-cies we use to compute PMI-based rankings are taken from the Google N-gram corpus [15]. This corpus represents a collection of statistical n-gram data obtained from English books, and given its large size it helps to overcome PMI X  X  inconsistent ranking on sparse data. Sample PMI scores for our running tokenization example are presented in Table 3.
Determiners represent the most commonly used part of speech in English. We found that in both our training and test collections, around 30% of the prepositions are used in proximity to determiners such as  X  X  X  and  X  X he X . Generally speaking, determiners do not influence the choice of a partic-ular preposition in a phrase. Figure 3 shows the distribution of correct prepositions ranks for two POS tags: determin-ers and nouns. We can confirm that the rank distribution for determiners is somewhat random, while for nouns there exists a clear bias towards higher ranks.

Given these observations, we generate so-called skip n-grams during the n-gram extraction process, where we strip determiners from the n-grams, whenever present. To better understand this process, let us consider the short phrase  X  X ne of the most X . Without skip n-grams, we receive the following trigrams during extraction:  X  X ne of the X ,  X  X f the most X . When we apply determiner skips, we produce an additional trigram:  X  X ne of most X .

Since the Google N-gram corpus does not contain skip n-grams directly, we produce them ourselves to get the cor-rect counts for all n-grams containing any of the possible determiners 9 .
Given the generated ranking of prepositions for each indi-vidual n-gram, our objective is to select the right preposition in a sentence. To achieve this goal, we apply a supervised learning method by means of two-class classification.
For every preposition occurrence in the text, we gener-ate a number of potential replacements equal to the size of our candidate preposition set. Each potential replacement receives its own feature values that correspond to a par-ticular preposition from the candidate set. These feature values incorporate the individual n-gram rankings we intro-duced earlier, and the classifier makes a binary decision on whether or not a particular preposition is correct for a given sentence. Each decision is given with an accompanying con-fidence score. 10 In the following section, we discuss the set of features we use in our supervised classification step.
We use the following list of determiners: a, an, the, my, our, your, their.
Generally speaking, more than one preposition can be clas-sified as correct using this classification approach. In such are based on the FCE collection.
As outlined in Section 4.3, we use PMI scores to obtain a ranking of prepositions for every n-gram. A possible way to select prepositions is to use the numerical rank of a prepo-sition directly as a measure of association.

The PMI score has the following property: A positive PMI score signals the presence of some association, while a nega-tive score indicates disassociation. Thus, we can directly use the PMI score of a preposition as a feature for a supervised approach.

Based on these observations, we propose the following set of features:
For each of the three features listed above, it is possible to calculate the various scores of different logical groups of n-grams; In this work, we group by n-gram size (unigram, bigram, etc.) and by n-gram distances. Thus, in total we get 3  X  n + 3  X  k number of features, where n is the number of different n-gram sizes and k is the number of different n-gram distances.
The n-grams that are central to a preposition (dis-tance=0) stand aside other n-grams, since they contain both left and right contexts for a given preposition. Figure 4 shows how often a top-ranked preposition for an n-gram is correct with respect to the n-gram distance.
We observe that central n-grams represent the largest chunk in the distribution. Therefore, it is important to in-corporate their preposition rankings as separate features. c ases, we select the most likely preposition according to the classifier X  X  confidence score.
 F igure 4: Distribution of correct preposition counts on top of PMI ranks with respect to n-gram dis-tance. The statistics are based on the FCE collec-tion.
 Following the features designed in Section 5.1, we add the PMI score and the rank of a given preposition from a central n-gram as features. Since there might be no central n-gram for some sentences, we also add a binary indicator feature to indicate whether or not such an n-gram is present. In case both skip and non-skip central n-grams coexist, we use the skip n-grams.
The selection of the correct preposition can also be made based on the currently observed preposition. Non-native En-glish speakers tend to commonly substitute a correct prepo-sition using the same, incorrect preposition, depending on the learner X  X  skills and background. In this case, the ob-served (but wrong) preposition directly correlates to a cor-rect one, and we use this as a potentially highly discrimina-tive feature.

Since we are generating potential replacements for every possible preposition, we can directly leverage the probability of the observed preposition as learned from the probability matrix of the training collection (Table 4), which indicates the probability of a given preposition being used instead of another preposition.

Another valuable information in the matrix is that some correct (and observed) preposition pairs have very low or zero probability: This represents valuable evidence indicat-ing that certain substitutions are very unlikely to happen and should not be considered by our correction approach.
Thus, for each preposition from our candidate set, we take its probability from the confusion matrix given an original preposition and use it as a feature (See, for instance, Ta-ble 4).
Part-of-Speech (POS) tags have often been considered as an important discriminative feature for many NLP tasks. Many previous contributions on grammatical error correc-tion used POS tag information from the context words sur-rounding the preposition and proved it to be important. In this work, we also take the top-5 most frequent POS tags that either immediately follow or precede a preposition and use them as binary features in our classifier (see Figure 5). The n-grams whose POS tags do no match the top-5 tags are assigned to the category  X  X THER X , which is also used as a binary feature.
In addition to the features described above, we also add binary features that represent the preposition itself. That is, we generate a sparse vector of size equal to our candidate preposition set with only one value equal to one, denoting the currently observed preposition.
As outlined in Section 4.5, given a sentence with a prepo-sition, our goal is to rank every preposition from the candi-date set according to their likelihood of being the correct one based on the context (n-grams) surrounding the preposition. In this paper, we consider n-grams with n  X  X  2 , 3 } including the preposition itself. For skip n-grams (see Section 4.4), we also consider n = 4, including prepositions and determiners.
The classification itself was performed using a random for-est classifier [7] using an implementation from the scikit-learn package [17]. This type of classifier allows to automat-ically rank features by their importance score without the need to evaluate different feature combinations manually.
As we can see from Table 1, preposition errors account for just 5% of all preposition occurrences in both training and CoNLL-2013 test collections. Furthermore, the features based on the confusion matrix highly correlate the observed prepositions with the correct ones. Thus, we need to balance the training collection with a similar amount of negative and positive examples.

By experimentally comparing different balancing methods on cross-validated training collections, we found that our classifier performs best when under-sampling non-errors so that the amount of real error samples is equal to the amount of non-errors. Both oversampling and under-sampling were performed using uniform random sampling 11 .
We also tried to sample while keeping the proportion of prepositions equal to the ones in the original collection, but did not observe any improvement over random sampling. Table 5: Features ranked by their importance scores computed over the training collection.

Feature name Importance score conf matrix score (Sec. 5.3) 0.28 top prep count 2gram (Sec. 5.1) 0.13 avg rank dist0 (Sec. 5.1) 0.06 central ngram rank (Sec. 5.2) 0.06 avg rank dist1 (Sec. 5.1) 0.05
The rest of this section is structured as follows: First, we discuss our approach to hyper-parameter optimization to prevent over-fitting the training data and show which fea-tures play the most important role for the classifier. We then evaluate the effects of restricting the distance of n-grams on the classifier X  X  performance. Next, we analyze the impact of restricting the size of the n-grams as well as the impact of skip n-grams. Finally, we perform an evaluation of our clas-sifier on the standard CoNLL-2013 collection by comparing our approach against the top-performing classifier from the CoNLL-2013 Shared Task, as well as on the SE test collec-tion we built.
In any classifier based on decision trees, it is possible to op-timize at least two parameters: the depth of the tree, and the minimum number of samples in a leaf. By restricting both of parameters to a certain range, we can effectively prevent the classifier from over-fitting the training data. Figure 6 shows the F1 values resulting from different combinations of depths and minimum samples using 10-fold cross-validation on the training collection. We observe that the best results are achieved with deeper trees and a small number of mini-mum samples in a leaf. Given these results, we fix the range of possible depth values between 5 and 50, and the range of minimum number of samples between 1 and 100. The results presented in Sections 6.2, 6.3 and 6.4 report maximum scores by taking all possible combinations of the hyper-parameters into account. Each score itself represents an average score obtained through 10-fold cross-validation.
Table 5 shows the top-5 features ranked by the importance scores assigned by the classifier. As expected, we observe that the feature based on the confusion matrix score is se-lected as most significant by the classifier. This is explained by the fact that in  X  95% of the cases there is no need to change the original preposition. The second most important feature is the total count of the preposition appearing on the first place of the n-gram rankings based on PMI. The next most relevant features include the preposition rank of the central n-gram and the average ranks grouped by n-gram distance.
According to Section 5.2, the number of n-grams with a correct preposition on top of the ranking decreases with the increase of the absolute distance to a preposition. By considering the n-grams distances from a preposition, we can evaluate if restricting the distance affects the results replaced with the preposition in the row.
 Count (right) preposition. and which combinations of distances yield the best results. Another benefit of restricting the distance is efficiency: the less n-grams we need to consider, the faster we can compute PMI scores.

To find the optimal set of distances, we compare the per-formance of our classifier by restricting the distances to cer-tain ranges. The results of this comparison are shown in Ta-ble 6. The best performing approach restricts the distance in the (  X  2 , 2) range, but it is not significantly different from the (  X  1 , 1) range. Therefore, we decided to use n-grams in the (  X  1 , 1) distance range for all further experiments, which drastically reduces the number of n-grams to consider, thus making our approach much more efficient.
In the following, we compare the effectiveness of our sys-tem by restricting the set of permitted n-gram sizes. The results of this experiment are presented in Table 7. We can confirm that the introduction of skip n-grams (i.e., results using n = 4) contributes to better final classification results. We also observe that the unrestricted set of n-gram sizes yields the best score.
 Table 6: Effectiveness values for different n-gram distance restrictions. The symbol  X  indicates a sta-tistically significant difference (t-test p &lt; 0 . 01 ) as compared to the best performing approach (bold).

Distance Restriction Precision Recall F1 score (0) 0.3077  X  0.3908  X  0.3442  X  (  X  1 , 1) 0.3231 0.4166 0.3637 (  X  2 , 2) 0.3214 0.4222 0.3648 (  X  5 , 5) 0.3223 0.4028 0.3577
No restriction 0.3214 0.3924  X  0.3532
Finally, we evaluate our classifier on two different test col-lections. At first, we evaluate our approach using the test collection of the CoNLL-2013 Shared Task. For this experi-ment, we trained the classifier on the complete training col-lection and set the other parameters to the ones that yield the best scores according to the previous experiments run on the Cambridge FCE collection. Table 8 shows the results of Table 7: Effectiveness values for different combi-nations of n-gram sizes. The symbol  X  indicates a statistically significant difference (t-test p &lt; 0 . 01 ) as compared to the best performing approach (bold).
 this evaluation. We observe that our classifier clearly out-performs the best approach from the CoNLL-2013 Shared Task, with a 13% absolute and 76% relative improvement. In the second part of the evaluation, we consider the Stack Exchange test collection (see Section 3.2). For this collec-tion, we evaluate the effectiveness of our classifier in two different setups. In the first experiment, we simply take the trained classifier used for the CoNLL-2013 collection, and apply it on the SE collection. Preposition errors are randomly under-sampled so that they constitute 5% of the training set. The experiment is repeated 10 times and we report the mean value in Table 8. In the second experi-ment, we perform 10-fold cross-validation on the SE collec-tion, where the test part is sampled similarly as for the first experiment.

We can see that when evaluating our approach by cross-validation on a collection built on top of user-generated con-tent, we obtain effectiveness scores (i.e., 28% F1) compara-ble to the ones obtained on the CoNLL-2013 collection in-dicating the robustness of the proposed approach. While training our classifier on the CoNLL-2013 collection and ap-plying it on the SE collection yields a drop in performance, our approach still obtains reasonable results indicating its portability to different domains.
Identifying and correcting grammatical errors in textual content is important for many applications. In this paper, we focused on the correction of preposition errors in English text. We proposed a supervised approach that uses a set of features designed around the notion of n-gram rankings. Our system uses a large collection of English books as evi-dence of correct preposition usage and generates a ranking of candidate prepositions for replacement in the sentence. The decision of which preposition to use is then made at the sentence-level, through a binary-classification step. We eval-uated our techniques over a standard evaluation collection as well as over a newly created collection of UGC. We con-firmed with extensive experiments that n-gram-based classi-fication and preposition ranking outperforms more complex multi-class classification methods for the task of preposition error correction.

As future work, we would like to address the issue of in-complete n-gram counts. For example, when a certain n-gram represents a very specific concept or a new entity , we might not find enough evidence to understand its correct usage in our n-gram corpus. However, it could be possible to use entity type information (e.g., Actor) to find similar entities and use their counts instead. Finally, as our prepo-sition classification approach allows to select multiple valid prepositions for a given sentence, we plan to broaden our approach and evaluation methodology to determine when a sentence indeed permits multiple valid preposition choices. This work was supported by the Swiss National Science Foundation under grant numbers PP00P2 128459 and 200021 143649. [1] S. Bergsma, D. Lin, and R. Goebel. Web-scale n-gram [2] K. W. Church and P. Hanks. Word association norms, [3] D. Dahlmeier and H. T. Ng. A beam-search decoder [4] D. Dahlmeier, H. T. Ng, and E. J. F. Ng. Nus at the [5] R. Dale, I. Anisimoff, and G. Narroway. Hoo 2012: A [6] A. Elghafari, D. Meurers, and H. Wunsch. Exploring [7] P. Geurts, D. Ernst, and L. Wehenkel. Extremely [8] M. Heilman, A. Cahill, and J. Tetreault. Precision [9] J. Huang, J. Gao, J. Miao, X. Li, K. Wang, F. Behr, [10] A. Islam and D. Inkpen. An unsupervised approach to [11] T.-h. Kao, Y.-w. Chang, H.-w. Chiu, T.-H. Yen, [12] E. Kochmar, O. Andersen, and T. Briscoe. Hoo 2012 [13] C. Leacock, M. Chodorow, M. Gamon, and [14] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [15] J.-B. Michel, Y. K. Shen, A. P. Aiden, A. Veres, M. K. [16] H. T. Ng, S. M. Wu, Y. Wu, C. Hadiwinoto, and [17] F. Pedregosa, G. Varoquaux, A. Gramfort, V. Michel, [18] R. Prokofyev, G. Demartini, and P. Cudr  X e-Mauroux. [19] A. Rozovskaya, K.-W. Chang, M. Sammons, and [20] Y. Xiang, B. Yuan, Y. Zhang, X. Wang, W. Zheng,
