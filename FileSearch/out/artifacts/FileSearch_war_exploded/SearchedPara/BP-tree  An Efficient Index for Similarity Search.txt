 Similarity search in high-dimensional metric spaces is a key operation in many applications, such as multimedia data-bases, image retrieval, object recognition, and others. The high dimensionality of the data requires special index struc-tures to facilitate the search. Most of existing indexes are constructed by partitioning the data set using distance-based criteria. However, those methods either produce disjoint partitions, but ignore the distribution properties of the data; or produce non-disjoint groups, which greatly affect the search performance. In this paper, we study the performance of a new index structure, called Ball-and-Plane tree (BP-tree), which overcomes the above disadvantages. BP-tree is con-structed by recursively dividing the data set into compact clusters. Distinctive from other techniques, it integrates the advantages of both disjoint and non-disjoint paradigms in order to achieve a structure of tight and low overlapping clusters, yielding significantly improved performance. Re-sults obtained from an extensive experimental evaluation with real-world data sets show that BP-tree consistently out-performs state-of-the-art solutions.
 H.2.2 [ Database Management ]: Physical Design X  access methods ; H.3.1 [ Information Storage and Retrieval ]: Content Analysis and Indexing X  indexing methods ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  search process Algorithms, Performance clustering methods, database indexing, metric access meth-ods, metric spaces, similarity search  X 
The authors thank the financial support of Brazilian agen-cies FAPESP, CNPq, and CAPES.

Similarity search in metric spaces is a subject of interest for many research communities. For over two decades, sig-nificant research efforts have been spent trying to improve the performance in processing similarity queries.

Most of existing indexes employed to accelerate data re-trieval are constructed by partitioning a set of objects using distance-based criteria [6, 12]. In general, those techniques follow two basic paradigms. One type of methods produces disjoint partitions, but ignores the distribution properties of the data [2, 5, 11, 15, 17]. The other type of methods pro-duces non-disjoint groups, which greatly affect the search performance [3, 7, 14, 16].

In this paper, we study the performance of a new in-dex structure, called Ball-and-Plane tree (BP-tree), which is constructed by dividing the data set into compact clus-ters. It combines the advantages of both disjoint and non-disjoint paradigms in order to achieve a structure of tight and low overlapping clusters, yielding significantly improved performance. Results from a rigorous empirical comparison on real-world data sets show that our approach consistently outperforms the state-of-the-art solutions.

The remainder of this paper is organized as follows. Sec-tion 2 describes related work. Section 3 presents BP-tree and shows how to apply it to similarity search. Section 4 reports the results of our experiments and compares our approach with others methods. Finally, we offer our conclusions and directions for future work in Section 5.
The problem of supporting similarity queries in metric spaces has long been pursued in the database area. A survey of metric access methods can be found in [6].

Previous works have focused on the static case. For in-stance, the structures proposed by Burkhard and Keller [4], Uhlmann [15], and also the VP-tree [17], GNAT [3], MVP-tree [2], and List of Clusters [5] do not support further inser-tions, deletions, and updates after the creation of the index structure. Overcoming this inconvenience, dynamic struc-tures, including M-tree [7], Slim-tree [14], SAT [11], and DBM-tree [16], have been developed.

In general, all those approaches follow two basic para-digms: disjoint or non-disjoint. The former partitions the data set into disjoint clusters, but ignores the distribution properties of the data [2, 5, 11, 15, 17]. The latter produces non-disjoint groups, which greatly affect the search perfor-mance [3, 7, 14, 16].

Different from all of the previous techniques, BP-tree does not divide the data set into disjoint or non-disjoint groups. Instead, it is an index structure that combines the advan-tages of both those strategies.
Overall, BP-tree is an unbalanced tree index generated by the hierarchical partitioning of the data set. Like other metric trees, the objects of the data set are stored into fixed size disk pages. Each page holds a predefined maximum number of objects K . Table 1 summarizes the symbols used in this paper.

Symbols Definitions
BP-tree has two kinds of nodes: leaf nodes and index nodes. Each index node corresponds to a single disk-page and contains a partitioning information. In contrast, each leaf node consists of a list of disk pages and, hence, may have an unlimited capacity. The objects of the data set are stored in both index and leaf nodes.

The structure of a leaf node is where oid ( o i ) is the identifier of the object o i and d ( o is the distance from the object o i to the representative o of this leaf node. The structure of an index node is where o i keeps the representative of the subtree pointed to by ptr ( T ( o i )) and r ( o i ) is the covering radius of the bound-ing region defined by o i . The distance between o i and the representative of this node o rep is kept in d ( o i ,o rep distance between the o i and the reference object o ref that established the bounding region with the minimum covering radius r ( o ref )iskeptin d ( o i ,o ref ). The pointer ptr ( T ( o points to the root node of the subtree rooted by o i . The tree construction is performed in a top-down fashion. In order to clarify this approach, look at Figure 1. At the beginning, the set of objects O = { o 1 ,o 2 ,...,o N } is consid-ered to be part of a single partition. Objects in this set are first divided into k  X  K disjoint subpartitions c 1 ,c 2 ,...,c Information about all those subpartitions form the index node of the first level of the tree. For each partition c subset O c i is created by taking the objects of c i . To build subsequent levels of the tree, this process is repeated for all of the new subset of objects at each level, creating the hier-archy of index nodes. The process stops when the number of objects in a subset is less than or equals to K or the number of partitions spanned by a subset is less than 2. Then, the objects in the subset are written to a leaf node on disk.
Algorithm 1 formalizes the above procedure. It starts by checking the cardinality of the set of objects O (line 2). If it can fit into a disk page, the function Create-leafnode is used to create a leaf node (line 3). Otherwise, we call the function Split in order to divide the set into k  X  K parti-tions with a minimum occupation equals to M (line 5). The function Split can use any partitional clustering method, such as k-medoids [1]. The partitional algorithm is respon-sible for finding the representatives and the reference ob-jects of each level. Next, we check if the set can be divided (line 7). If so, we call the function Initialize-indexnode which creates an index node using the information about all those partitions (line 10). Thereafter, for each partition, the function Create-subset is used to create a subset of objects (line 12). This function is responsible for taking objects of a given partition. After that, we repeat this process for all of the new subset of objects (line 14). Finally, the function Update-indexnode is used to update the information in each entry of the index node (line 15).
 Algorithm 1 BP-tree construction. 1: function BP-tree ( d, K, M, N, O ) 2: if N  X  K then 3: return Create-leafnode ( N, O ); 4: else 5: C  X  Split ( d, K, M, N, O ); 6: k  X  cardinality ( C ); 7: if k&lt; 2 then 8: return Create-leafnode ( N, O ); 9: else 10: Parent  X  Initialize-indexnode ( k, C ); 11: for c  X  C do 12: O c  X  Create-subset ( c, k, C, N, O ); 13: N c  X  cardinality ( O c ); 14: Child  X  BP-tree ( d, K, M, N c ,O c ); 15: Update-indexnode ( c , Parent , Child ); 16: end for 17: return Parent 18: end if 19: end if 20: end function
BP-tree can answer the two commonest types of similarity queries: range query R ( o q ,r ( o q )), which retrieves all objects found within distance r ( o q ) of a query object o q ; and near-est neighbor query kNN ( o q ), which retrieves the k nearest neighbors of the query object o q .

The range search algorithm starts at the root node and traverses the tree in a depth-first manner, visiting all non-discardable subtrees. During the search, all the stored dis-tances are used to prune subtrees.

If the current node is an index node, we consider all non-empty entries as follows. First, if | d ( o q ,o rep )  X  d ( o r ( o i ) &gt;r ( o q ) holds, the subtree pointed to by ptr ( T ( o pruned. Otherwise, we compute the distance d ( o q ,o i ) and, if result. Afterwards, we prune all subtrees via the criterion ity and avoid to access subtrees by checking if | d ( o q on the fact that the expression | d ( o q ,o i )  X  d ( o r ( o ref ) forms the lower bound on the distances between the query object o q and any object in the subtree pointed to by ptr ( T ( o i )). All non-pruned subtrees ptr ( T ( o i distances d ( o q ,o i ) are stored in a list. Thereafter, we find the entry whose representative o i is closest to the query ob-ject o q (i.e., d min  X  argmin d ( o q ,o i )) and, then, we remove from the list all entries such that ( d ( o q ,o i )  X  d min Finally, the algorithm proceeds by recursively searching the remaining subtrees in the list.

Leaf nodes are similarly processed. Each entry is exam-ined using the pruning condition | d ( o q ,o rep )  X  d ( o r ( o q ). If it holds, the entry can be safely ignored. Other-wise, the distance d ( o q ,o i ) is evaluated and the object o reported if d ( o q ,o i )  X  r ( o q ).

The algorithm for k nearest neighbors queries is performed by simulating a dynamic range search with the query range r ( o q ) being the distance between the query object o q the current k -th nearest neighbor. At the beginning, r ( o is set to infinity and, during the search process, it is updated (decreased) when a new nearest neighbor is found (  X  if we still have less than k candidates).

For this purpose, we have a priority queue of nodes, the most promising first. Initially, we insert the root node in the priority queue. Iteratively, we extract the most promising node, process it, and insert all non-pruned subtrees in the priority queue. This is repeated until the priority queue becomes empty.

The distance d ( o q ,o i ) is used to measure how promising is a subtree ptr ( T ( o i )) and, hence, the subtrees inserted in the priority queue are visited in increasing order of prox-imity to the query object o q .Since r ( o q ) is reduced along the search, a subtree may seem useful at the moment it is inserted in the priority queue, and becomes useless later when it is extracted from the priority queue to be pro-cessed. So, we store the lower bounds d ( o q ,o i | d ( o q ,o i )  X  d ( o i ,o ref ) | X  r ( o ref ), and ( d ( o gether with the subtree ptr ( T ( o i )) and its distance d ( o As we extract an entry from the priority queue, we check whether those lower bounds exceed r ( o q ), in which case the extracted node is discarded.
In this section, we compare the performance of our tech-nique with previous work in processing similarity queries. We implemented BP-tree from scratch in C++ under Linux. In our implementation, we use the k-medoids to partition data. The experiments were performed on a machine equipped with a processor Intel Xeon QuadCore X3320 2.5 GHz and 8 GBytes of DDR2-memory. The machine run Gentoo Linux (2.6.31 kernel) and the ext3 file system.
 We compared BP-tree against MVP-tree, SAT, List of Clusters, M-tree, Slim-tree, and DBM-tree, which are the most popular approaches for similarity search in metric spaces. For our experimental evaluation, we adopted the implemen-tation of MVP-tree, SAT, and List of Clusters from the Met-ric Space Library 1 and the implementation of M-tree, Slim-tree, and DBM-tree from the GBDI Arboretum Library 2 .In order to guarantee a fair comparison, all the compared meth-ods were configured using their best recommended setup. The page capacity used to build all the compared indexes was set to 64 objects.

BP-tree was tested using two sets of images described in the literature and extensively used by computer vision and http://www.sisap.org/Metric_Space_Library.html http://www.gbdi.icmc.usp.br/arboretum/ image processing communities. The first set contains 30,607 images from the Caltech-256 Object Category Dataset 3 [9]. We converted each image to a 64-dimensional feature vector by computing a Color Histogram [13]. The color histograms were extracted as follows: the RGB space is divided into 64 subspaces (colors), using four ranges in each color channel. The value for each dimension of a feature vector is the den-sity of each color in the entire image. The other data set is a collection of 72,000 images from the Amsterdam Library of Object Images (ALOI) 4 [8]. In this case, each image is characterized by a 256-dimensional feature vector which rep-resents a Color Correlogram [10]. Each color correlogram is a table indexed by color pairs, where the k -th entry for a pair &lt;i,j&gt; specifies the probability of finding a pixel of color j at a distance k from a pixel of color i in the image. The distance function used to compare the feature vectors of both the data sets is the Manhattan ( L 1 )distance.
For each collection, we randomly selected about five per-cent of the total number of objects to be used as queries. In order to ensure statistically sound results, five replications were performed for each corpora. Thereafter, we performed k -NN queries and varied the number k of nearest neighbors requested for the query from 2 to 20. For each data set, the 99% confidence interval on the mean was computed based on the mean and standard deviation of each replication.
Figure 2 shows the performance measurements attained for all the compared methods on both the datasets. The first column (Figures 2(a) and (d)) shows the average number of distance calculations. The results are plotted in log scale to minimize the large difference resulting from queries with small and large k , which makes the comparison easier.
Clearly, BP-tree consistently outperforms all the com-pared indexes in the number of distance calculations, with a high statistical significance (confidence higher than 0.99). BP-tree saves, on average, 50% of distance calculations for all the data sets when compared to its best competitor.
In order to evaluate the behaviour of our approach with respect to the number of disk accesses to answer a query, we compared BP-tree with M-tree, Slim-tree, and DBM-tree as they are the only ones that consider I/O costs in their analysis. For this purpose, BP-tree was implemented into the GBDI Arboretum Library, with the same code op-timization, to obtain a fair comparison. The second column (Figures 2(b) and (e)) presents the average number of disk accesses required for each of the above techniques.
It can be seen from those plots that BP-tree requires a few more disk accesses than M-tree and Slim-tree for the Caltech-256 dataset (Figure 2(b)). On the other hand, BP-tree shows better performance for the ALOI dataset (Fig-ure 2(e)). One of the reasons is that the BP-tree is an un-balanced tree. Notice that a similar behaviour is performed by DBM-tree, which is also an unbalanced tree.

Nevertheless, the search cost for similarity search in high-dimensional spaces is determined by the pruning rate of the search space, not by the height of the tree, as stated in [5]. It can be seen by comparing the query time of the above in-dexes as it reflects the total complexity of the algorithms be-sides the number of distance calculations and the number of disk accesses. We present the average time (in milliseconds) required for those indexes in the third column (Figures 2(c) and (f)). Clearly, BP-tree is significantly faster than the compared indexes for performing similarity queries. http://www.vision.caltech.edu/Image_Datasets/ Caltech256/ http://staff.science.uva.nl/~aloi/
In this paper, we have presented BP-tree, a new approach for performing similarity search in high-dimensional met-ric spaces. In BP-tree, the data set is divided into com-pact clusters by respecting their distribution. It combines the advantages of both disjoint and non-disjoint strategies in order to achieve a structure of tight and low overlapping clusters, yielding significantly improved performance on sim-ilarity queries especially for high-dimensional spaces.
Our experiments conducted over real-world data sets have shown that BP-tree consistently outperforms the state-of-the-art indexes for similarity search in metric spaces. BP-tree is, on average, up to 50% faster than traditional ap-proaches for performing similarity queries.

Future work includes the investigation of pros and cons of using existing clustering algorithms for indexing high-dimensional data. We also plan to extend BP-tree to per-form regional repartitioning for supporting insertions and deletions after the initial creation of the index structure. [1] C.M.Bishop. Pattern Recognition and Machine [2] T. Bozkaya and Z. M.  X  Ozsoyoglu. Indexing large [3] S. Brin. Near neighbor search in large metric spaces. [4] W. A. Burkhard and R. M. Keller. Some approaches [5] E. Ch  X  avez and G. Navarro. A compact space [6] E. Ch  X  avez, G. Navarro, R. A. Baeza-Yates, and J. L. [7] P. Ciaccia, M. Patella, and P. Zezula. M-tree: An [8] J.-M. Geusebroek, G. J. Burghouts, and A. W. M. [9] G. Griffin, A. Holub, and P. Perona. Caltech-256 [10] J. Huang, R. Kumar, M. Mitra, W.-J. Zhu, and [11] G. Navarro. Searching in metric spaces by spatial [12] A. Rocha, J. Almeida, M. A. Nascimento, R. Torres, [13] M. J. Swain and B. H. Ballard. Color indexing. IJCV , [14] C. Traina Jr., A. J. M. Traina, C. Faloutsos, and [15] J. K. Uhlmann. Satisfying general [16] M.R.Vieira,C.TrainaJr.,F.J.T.Chino,and [17] P. N. Yianilos. Data structures and algorithms for
