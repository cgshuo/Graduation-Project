 Learning from imbalanced datasets has got more an d more emphases in recent years. A dataset is imbalanced if its class distributions are skewed. The class imbalance problem is of crucial importance since it is encountered by a large number of real world applications, such as fraud detection [1], the detection of oil spills in satellite radar images [2], and text classification [3]. In these scenarios, we are usually more interested in the minority class instead of the majority class. The traditional data mining algorithms have a poor performance due to the fact that they give equal attention to the minority class and the majority class.
One way for solving the imbalance learning problem is to develop  X  X mbalanced data oriented algorithms X  that can perform well on the imbalanced datasets. For example, Wu et al. proposed class boundary alignment algorithm which modi-fies the class boundary by changing the kernel function of SVMs [4]. Ensemble methods were used to improve performance on imbalance datasets [5]. In 2010, Liu et al. proposed the Class Confidence Proportion Decision Tree (CCPDT) [6]. Furthermore, there are other effect ive methods such as cost-based learning [7] and one class learning [8].

Another important way to improve the results of learning from imbalanced data is to modify the class distributions in the training data by over-sampling the minority class or under-sampling the majority class [9]. The simplest sam-pling methods are Random Over-Sampling (ROS) and Random Under-Sampling (RUS). The former increases the number of the minority class instances by du-plicating the instances of the minority, while the latter randomly removes some instances of the majority class. Sampling with replacement has been shown to be ineffective for improving the recognition of minority class significantly. [9][10]. Chawla et al. interpret this phenomenon i n terms of decision regions in feature space and proposed the Synthetic Minority Over-Sampling Technique (SMOTE) [11]. There are also many other synthetic over-sampling techniques, such as Borderline-SMOTE [12] and ADASYN [13]. To summarize, under-sampling meth-ods can reduce useful information of the datasets; over-sampling methods may make the decision regions of the learner smaller and more specific, thus may cause the learner to over-fit.

In this paper, we analyze the performance of over-sampling techniques from the perspective of the large margin principle and find that the over-sampling methods are inherently risky from this perspective. Aiming to reduce this risk, we propose a new synthetic over-sampling method, called Margin-guided Syn-thetic Over-Sampling (MSYN). Our work is largely inspired by the previous works in feature selection using the large margin principle [14] [15] and prob-lems of over-sampling for imbalance learning [16]. The empirical study revealed the effectiveness of our proposed method.

The rest of this paper is organized as fo llows. Section 2 reviews the related works. Section 3 presents the margin-ba sed analysis for over-sampling. Then in Section 4 we propose the new synthetic over-sampling algorithm. In Section 5, we test the performance of the algorithms on various machine learning benchmarks datasets. Finally, the conclusion and future work are given in Section 6. We use A to denote a dataset of n instances A = { a 1 , ..., a n } ,where a i is a real-valued vector of dimension m .Let A P  X  A denote the minority class instances, A
Over-sampling techniqu es augment the minority class to balance between the numbers of the majority and minority class instances. The simplest over-sampling method is ROS. However, it may make the decision regions of the ma-jority smaller and more specific, and thus can cause the learner to over-fit [16].
Chawla et al. over-sampled the minority class with their SMOTE method, which generates new synth etic instances along the line between the minority in-stances and their selected nearest neighbors [11]. Specifically, for the subset A P , they consider the k -nearest neighbors for each instances a i  X  A p . For some spec-ified integer number k ,the k -nearest neighbors are define as the k elements of A
P , whose Euclidian distance to the element a i under consideration is the small-est. To create a synthetic instance, one of the k -nearest neighbors is randomly selected and then multiplied by the corresponding feature vector difference with a random number between [0 , 1]. Take a two-dimensional problem for example: where a i  X  A P is the minority instance under consideration, a nn is one of the k -nearest neighbors from the minority class, and  X   X  [0 , 1]. This leads to generating a random instance along the line segment between two specific instances and thus effectively forces the decision regi on of the minority class to become more general [11]. The advantage of SMOTE is that it makes the decision regions larger and less specific [16].

Borderline-SMOTE focuses the instan ces on the borderline of each class and the ones nearby. The consideration behind it is: the instances on the borderline (or nearby) are more likely to be miscla ssified than the ones far from the border-line, and thus more important for classific ation. Therefore, Borderline-SMOTE only generates synthetic instances for t hose minority instances closer to the border while SMOTE generates synthetic i nstances for each minority instance. ADASYN uses a density distribution as a criterion to automatically decide the number of synthetic instances that need to be generated for each minority in-stance. The density distribution is a measurement of the distribution of the weights for different minority class instances according to their level of difficulty in learning. The consideration is similar to the idea of AdaBoost [17]: one should pay more attention to the difficult instances. In summary, either Borderline-SMOTE or ADASYN improves the performance of over-sampling techniques by paying more attention on some specific instances. They, however, did not touch the essential problem of the over-sampling techniques which causes over-fitting.
Different from the previous work, we resort to margins to analyze the problem of over-sampling, since margins offer a theoretic tool to analyze the generalization ability. Margins play an indispensable role in machine learning research. Roughly speaking, margins measure the level of confidence a classifier has with respect to its decision. There are two natural ways of defining the margin with respect to a classifier [14]. One approach is to define the margin as the distance between an instance and the decision boundary induced by the classification rule. Support Vector Machines are based on this definition of margin, which we refer to as sample margin. An alternative definition of the margin can be the Hypothesis Margin; in this definition the margin is the distance that the classifier can travel without changing the way it labels any of the sample points [14]. For prototype-based problems (e. g. the nearest neighbor classifer), the classifier is defined by a set of training points (prototypes) and the decision boundary is the Voronoi tessellation [18]. The sample margin in this case is the distance between the instance and the Voronoi tessellation. Therefore it measures the sensitivity to small changes of the instance position. The hypothesis margin R for this case is the maximal distance such that the following condition holds: if we draw a sphere with radius R around each prototype, any change of the location of prototypes inside their sphere will not change the assigned labels. Therefore, the hypothesis margin measures the stability to small changes in the prototypes locations. See Figure 1 for illustration.

Throughout this paper we will focus on the margins for the Nearest Neighbor rule (NN). For this special case, it is proved the following results [14]: 1. The hypothesis-margin lower bounds the sample-margin 2. It is easy to compute the hypothesis-margin of an instance x with respect to a set of instances A by the following formula: where nearesthit A ( x )and nearestmiss A ( x ) denote the nearest instance to x in dataset A with the same and different label, respectively.

In the case of the NN, we can know that the hypothesis margin is easy to calculate and that a set of prototypes with large hypothesis margin then it has large sample margin as well [14].
 Now we consider the over-sampling problem using the large margin principle. When adding a new minority class instance x , we consider the difference of the overall margins for the minority class: where A \ a denotes the dataset excluding a from the dataset A ,and A \ a  X  X  x } denotes the union of A \ a and { x } .
 follows that  X  P ( x )  X  0. We call  X  P ( x ) the margin gain for the minority class. Further, the difference of the overall margins for majority class is: || that  X  N ( x )  X  0. We call  X   X  N ( x ) the margin loss for the majority class.
In summary, it is shown that the over-sampling methods are inherently risky from the perspective of the large margin principle. The over-sampling methods, such as SMOTE, will enlarge the nearest-neighbor based margins for the minority class while may decrease the nearest ne ighbor based margins for the majority class. Hence, over-sampling will not only bias towards the minority class but also may be detrimental to the majority class. We cannot eliminate these effects when adopting over-sampling for imbala nce learning completely, but we can seek methods to optimize the two parts.

In the simplest way, one can maximize the margins for the minority class and ignore the margins loss for the majority class, i.e., the following formula: Alternatively, one may also minimize the margins loss for the majority class, which is One intuitive method is to seek a good ba lance between maximizing the margins gain for the minority class and minimizing the margins loss for the majority class. This can be conducted by minimizing Eq. (6): where  X  is a positive constant to ensure that the denominator of Eq. (6) to be non-zero. In this section we apply the above analysis to the over-sampling techniques. Without loss of generality, our algorithm is designed on the basis of SMOTE. The general idea behind it, however, c an also be applied to any other over-sampling technique
Based on the analysis in the previous s ection, Eq. (6) is employed to decide whether a new synthetic instance is good enough to be added into the training dataset. Our new Margin-guided Synthetic Over-Sampling algorithm, MSYN for short, is given in Algorithm 1. The major focus of MSYN is to use margin-based guideline to select the synthetic instances. P ressure  X  N , a natural number, is a parameter for controlling the selection pressure. In order to get ( m N  X  m P )new synthetic instances, we first create ( m N  X  m P )  X  P ressure new instances, then we only select top best ( m N  X  m P ) new instances accordin g to the values of Eq. (6) and discard the rest instances. This sel ection process implicitly decides whether an original minority instance is used to create a synthetic instances as well as how many synthetic instances will be generated, which is different from SMOTE since SMOTE generates the same number of synthetic instances for each original minority instances. Moreover, it is easy to see that computational complexity of MSYN is O ( n 2 ), which is mainly decided by calculating the distance matrix. The Weka X  X  C4.5 implementation [19] is employed in our experiments. We com-pare our proposed MSYN with SMOTE [11], ADASYN [13], Borderline-SMOTE [12] and ROS. All experiments were carried out using 10 runs of 10-fold cross-validation. For MSYN, the parameter P ressure is set to 10 and the  X  can be any random positive real number; for other methods, the parameters are set as recommended in the corresponding paper.

To evaluate the performance of our approach, experiments on both artificial and real datasets have been performed. The former is used to show the behavior of the MSYN on known data distributions while the latter is used to verify the utility of our method when dealing with real-world problems. 5.1 Synthetic Datasets This part of our experiments focuses on synthetic data to analyze the character-istics of the proposed MSYN. We used the dataset Concentric from the ELENA project [20]. The Concentric dataset is a two-dimensional uniform concentric circular distributions problem with two classes. The instances of minority class uniformly distribute within a circle of radius 0.3 centered on (0.5, 0.5). The points of majority class are uniformly distribute within a ring centered on (0.5, 0.5) with internal and external radius respectively to 0.3 and 0.5.

In order to investigate the problem of over-fitting to noise, we modify the dataset by randomly flipping the labels of 1% instances, as shown in Figure 2.
In order to show the performance of the various synthetic over-sampling tech-niques, we sketch them in Figure 3. The ne w synthetic instances created by each over-sampling method, the original majority instances and the corresponding C4.5 decision boundary are drawn. From Figure 3, we can see that MSYN shows good performance in the presence of noise while SMOTE and ADASYN suffer greatly from over-fitting the noise. MSYN generates no noise instances. This can be attributed to the fact that the margin-based Eq. (6) contains the information of the neighboring instances, and this information helps to decrease the influence of noise. Both SMOTE and ADASYN generate a large number of noise instances and their decision boundary is greatly influenced. Borderline-SMOTE generates a small number of noise instances and its decision boundary is slightly influenced. Furthermore, Borderline-SMOTE pays li ttle attention to interior instances and creates only a few of synthetic instances. 5.2 Real World Problems We test the algorithms on ten datasets from the UCI Machine Learning Repos-itory [21]. Information about these datasets is summarized in Table 1, where num is the size of the dataset, attr is the number of features, min% is the ratio of the number of minority class number to NUM.

Instead of using the overall classificat ion accuracy, we uadopt metrics related to Receiver Operating Characteristics ( ROC) curve [22] to evaluate the compared algorithms, because traditional overall classification accuracy may not be able to provide a comprehensive assessment of the observed learning algorithms in case of class imbalanced datasets [3]. Specifically, we use the AUC [22] and F-Measure [23] to evaluate the performance. We apply the Wilcoxon signed rank test with a 95% confidence level on each dataset to see whether the difference between the compared algorithms is statistically significant.

Table 2 and Table 3 show the AUC and F-Measure for the datasets, respec-tively. The results of Table 2 reveal that MSYN wins against SMOTE on nine out of ten datasets, beats ADASYN on seven out of ten datasets, outperforms ROS on nine out of ten datasets, and wins against Borderline-SMOTE on six out of ten datasets. The results of Table 3 show that MSYN wins against SMOTE on seven out of ten datasets, beats ADASYN on six out of ten datasets, beats ROS on six out of ten datasets, and wins against Borderline-SMOTE on six out of ten datasets. The comparisons reveal that MSYN outperforms the other methods in terms of both AUC and F-measure. This paper gives an analysis of over-sample techniques from the viewpoint of the large margin principle. It is shown that over-sampling techniques will not only bias towards the minority class but may also bring detrimental effects to the clas-sification of the majority class. This inherent dilemma of over-sampling cannot be entirely eliminated, but only reduced. We propose a new synthetic over-sampling method to strike a balance between the t wo contradictory objectives. We eval-uate our new method on a wide variety of imbalanced datasets using different performance measures and compare it to the established over-sampling methods. The results support our analysis and indicate that the proposed method, MSYN, is indeed superior.

As a new sampling method, MSYN can be further extended along several di-rections. First of all, we investigate the performance of MSYN using C4.5. Based on the nearest neighbor margin, MSYN has a bias for the 1-NN. Some strategies, however, can be adopted to approximate the hypothesis margin for the other clas-sification rules. For example, we can use the confidence of the classifiers X  output to approximate the hypothesis margin. T hus we expect MSYN can be extended to work well with other learning algorithms, such as k-NN, RIPPER [28]. But solid empirical study is required to justify this expectation. Besides, ensemble learning algorithms can improve the accuracy and robustness of the learning procedure [25]. It is thus worthy of integrating MSYN with ensemble learning algorithms. Such an investigation can be conducted following the methodology employed in the work of SMOTEBoost [5], DataBoost-IM [26], BalanceCascade [27], etc.

Secondly, MSYN can be generalized to multiple-class imbalance learning as well. For each minority class i , a straightforward idea is to extend Eq. (6) to: where  X  i ( x ) denotes the margin gain of minority class i by adding a new minority instance x ( x belongs to class i ), and  X   X  i,j ( x ) denotes the margin loss for class j by adding a new minority instance x ( x belongs to class i ). Then we create the synthetic instances for each minoirty class to make the number of them being equal to the number of the majority class, which has the maximum number of instances. However, this idea is by no means the only one. Extending a technique from binary to multi-class problems is usually non-trivial, and more in-depth investigation is necessary to seek the best strategy.

