 The text processing and speech processing re-search communities have similar problems and goals, but the technical approaches in these two communities develop largely independently. In this paper we compare dimensionality reduction techniques on multinomial language data from the text and speech communities. We consider a multinomial formulation of the i-vector model (hereafter  X  X i-vector X  model) from the speech community (Soufifar et al., 2011), the sparse ad-ditive generative (SAGE) (Eisenstein et al., 2011) and latent Dirichlet allocation (LDA) (Blei et al., 2003b) topic models from the text community, and latent semantic analysis (LSA) (Deerwester et al., 1990). Both the mi-vector model and the SAGE topic model represent a multinomial pa-rameter vector as the softmax of a sum of vectors, one of which is a background vector representing overall word usage in the corpus, and so we might expect mi-vectors and SAGE to produce similar results on real-world data. We evaluate these two recent models and two more conventional mod-els, LDA and LSA (a term describing a class of methods based on the singular value decomposi-tion, or SVD, which is used broadly in both re-search communities). We assess the similarity of mi-vectors and SAGE and expose the strengths and weaknesses of all four learned representations by evaluating them on the supervised task of topic identification (topic ID), depicted in Figure 1. We also evaluate the representations on the unsuper-vised, less easily-measurable task of topic discov-ery. As a proxy for controlled human annotations, we quantify topic discovery performance by dis-tributional similarity to gold-standard topics.
We use the bag-of-words multinomial represen-tation of text data, i.e., each document is repre-sented by a vector of counts over the word vo-cabulary. For speech data, we use a modern au-tomatic speech recognition (ASR) system to pro-duce frame-wise triphone state cluster posteriors and we take the sum of these posteriors across all frames in a document to obtain a document-level vector of triphone state cluster soft counts. Modern topic ID systems for speech use ASR out-put instead of a lower-resource representation like these soft counts to improve performance (Hazen et al., 2007). ASR word counts are high-resource and can be viewed as a noisy version of word counts from text. We wish to assess the relative performance of our learned representations, not the quality of the data pre-processing scheme, and we desire to strengthen our results by evaluating performance on two distinct views of a corpus. Hence we break from convention and use triphone state cluster soft counts as speech data.

While previous work has juxtaposed the mi-vector model against LDA (Chen et al., 2014; Morchid et al., 2014), the current study is the first to provide cross-community evaluations of mi-vectors and a contemporaneous model from the text community on both text and speech data. This study is also novel in its direct application of the mi-vector model to topic ID and topic dis-covery, two separate tasks with different motiva-tions and preferring different types of models, and in its use of low-resource triphone state cluster soft counts as speech data for topic ID. The low-resource setting reflects constraints often faced in real-world applications, and we report topic ID performance under limited supervision to better il-luminate the practical strengths and weaknesses of the learned representations. Finally, we believe that the centralized comparison herein of several prominent learned representations on two comple-mentary tasks on both text and speech will provide a useful point of reference for future research. Previous work has compared and composed the mi-vector model with older dimensionality reduc-tion techniques, including LDA. Chen et al. (2014) compared a mi-vector language model against LDA and other models on the task of spoken doc-ument retrieval, and found the mi-vector model to significantly outperform the other models on words, but not on subwords (syllable pairs), de-rived from ASR. The syllable pairs are similar in granularity to the triphone state clusters used as multinomial speech data in the current work.
Morchid et al. (2014) improved conversation theme identification by employing LDA and a Gaussian i-vector model in a pipeline. They learn LDA models of varying dimensions (numbers of topics) on ASR output and use them to gener-ate a suite of feature vectors. The feature vector for each document-dimension pair is created by marginalizing over topics according to the docu-ment X  X  inferred topic proportions. A Gaussian i-vector model is then learned on those feature vec-tors; the i-vectors are normalized and used to iden-tify document themes via the Bayes decision rule.
Note that we have fundamentally different ap-proaches, goals and methodology from that of Morchid et al. (2014). First, in an effort to provide a scientific comparison of independently created models, we use multinomial i-vectors, whereas Morchid et al., focusing on a particular task set-ting, used traditional Gaussian i-vectors. Simi-larly, while we treat multiple types of topic mod-els as goals in their own rights, directly compar-ing SAGE and LDA, Morchid et al. use LDA as a pre-processing step to Gaussian i-vectors. Sec-ond, we use triphone state cluster soft counts in-stead of ASR word counts, hence our representa-tion of speech data is significantly lower-resource. Third, we also evaluate performance on text data, and where Morchid et al. limit their vocabulary (from ASR) to 166 task-specific words, we use all 26,606 words present in our training data. Our data is drawn from Part 1 of the Fisher English speech corpus (Cieri et al., 2004c), which contains audio recordings (Cieri et al., 2004a) and man-ual transcriptions (Cieri et al., 2004b) of telephone conversations. Specifically, we use the topic ID training and evaluation test subsets defined in prior work (Hazen et al., 2007). In each conversation in these subsets of the data, two study participants are prompted to speak on one of a predefined set of forty topics. There are 1374 training conver-sations and 686 test conversations. We represent each conversation by two documents, one for each side (speaker), resulting in a training set of 2748 documents and a test set of 1372 documents. The deep neural network (DNN) used to infer the tri-phone state cluster posteriors forming the basis of our speech data was trained on Parts 1 and 2 of the Fisher English speech corpus (Cieri et al., 2004a; Cieri et al., 2005); see the supplement for further details about our dataset and ASR system.

To quantify the sparsity of the raw text (word count) and speech (triphone state cluster soft count) representations, we consider the represen-tation density (number of non-zero entries) on our training set. The text representation is sparse, with median density 292 and maximum 500 (out of 26,606 dimensions); the speech representation is dense, with median density 7586 and maximum 7591 (out of 7591 dimensions).

To assess approximate sparsity, we plot his-tograms of the entropy of the normalized multino-mial views of our training set in Figure 2. The me-dian entropy for speech is less than two bits away from the uniform entropy, so the speech data is neither sparse nor approximately sparse.
 Finally, we note that topic occurrence in the Fisher English training set is unbalanced, with quartiles (including minimum and maximum) of 6, 18.75, 29.5, 50.25, and 87. We consider four main dimensionality reduction models: the mi-vector model from the speech community, the SAGE and LDA topic models from the text community, and LSA. The learned representations we consider explain which words appear in a document d via a latent, lower-dimensional representation  X  ( d ) . All representa-tions operate under a bag-of-words assumption. To compare mi-vectors, topic models and LSA, we find it useful to formulate each learned representa-tion as operating on different  X  X reas X  or  X  X ontexts X  a of a document; such a formulation does not negate the fundamental bag-of-words assumption. The four models represent the words that appear in an area a  X  X ither the entire document or each token X  X ia multinomial-style parameters  X  ( a ) . 1,2 Each model consists of K components (e.g., a K -dimensional affine subspace), and shared param-eters H k,v prescribe the amount of weight each component k places on each vocabulary word v . The models construct  X  ( a ) by combining H and  X  ( d ) ; often empirical word statistics m are also used to stabilize the representations. 4.1 LSA LSA (Deerwester et al., 1990) factorizes a term-document matrix by truncated SVD, learning the projection of the data onto a linear subspace of fixed rank such that the approximation error of the reconstructed term-document matrix (as measured by the Frobenius norm) is minimized. In the basic version of LSA, SVD is applied to the raw term counts, giving the low-dimensional representation where  X  ( d ) is the vector of observed multinomial counts in document d , H is the matrix of left sin-gular vectors of the term-document count matrix, and  X  ( d ) is the inferred representation of  X  ( d ) . In practice, LSA is often applied instead to the term-document matrix weighted by term frequency X  inverse document frequency (tf-idf) in order to normalize terms by importance. We can also ap-ply further pre-processing steps, such as term-wise centering by subtracting the column-wise mean m of the data, in which case LSA finds an affine sub-space that approximates the data. 4.2 Mi-vector Model The original acoustic i-vector model represents continuous, high-dimensional ASR system state (namely, Gaussian mixture model supervectors) in an affine subspace (Dehak et al., 2011). Prior work has found this dense, low-dimensional representa-tion to be effective for a number of tasks, including language recognition (Mart  X   X nez et al., 2011) and speaker recognition (Dehak et al., 2011; Garcia-Romero and Espy-Wilson, 2011).

Recently the i-vector model was augmented for multinomial observations (Soufifar et al., 2011) and applied accordingly to language recogni-tion (Soufifar et al., 2011; McCree and Garcia-Romero, 2015), speaker recognition (Kockmann et al., 2010), and spoken document retrieval (Chen et al., 2014). In this version of the i-vector model the observations are draws from a multinomial and the (unnormalized) natural parameters of that dis-tribution are represented in an affine subspace: We call this multinomial version of the i-vector model the mi-vector model. The latent variable  X  ( d ) is the multinomial i-vector , or mi-vector . H is an unconstrained linear transformation. The bias term m is computed as the log of the l 1 -normalized background word count vector. The Gaussian prior on the mi-vector  X  ( d ) is effec-tively an l 2 regularizer; mi-vectors are neither non-negative nor sparse in general.

Unlike many Bayesian topic models, word oc-currences in the mi-vector model are i.i.d. draws from a document-level multinomial  X  ( d ) ; as in LSA, each latent component contributes equally to each word within a given document. Specifically, in the mi-vector model, the natural parameter vec-tor of the multinomial for all words in a given doc-ument is determined by an additive offset from a background parameter vector. 4.3 Bayesian Discrete Topic Models Bayesian topic models explain word occurrences via K latent components H k (topics) each drawn from some prior distribution G . Unlike mi-vectors and LSA, multinomial topic models are admixture models: each token n is drawn from a particular distribution H k . Latent token assignment vari-ables z ( d ) n , taking integral values between 1 and K , dictate the token X  X  topic choice. A document d controls how often each topic is chosen via the K -dimension multinomial distribution  X  ( d ) . In the parametric settings we consider, Dirichlet priors are often placed on  X  ( d ) , allowing experimenta-tion with the topic representation H . 3 A mapping Q ( H k ) , possibly the identity, ensures  X  ( d,n ) are probability vectors. A general formulation is then The hyperparameters  X  and  X  dictate the infor-mativeness of the priors over H k and  X  ( d ) : often (empirically optimized) symmetric hyperparame-ters are employed, resulting in a form of Laplace smoothing during topic estimation. In the current work, we follow this strategy, noting that there have been concerted efforts to encode domain or expert knowledge via the hyperparameters (Gorm-ley et al., 2012; Paul and Dredze, 2015).
 SAGE Topic Model The Sparse Additive Gen-erative (SAGE) model (Eisenstein et al., 2011) is a generative Bayesian modeling framework in which  X  ( d,n ) are formed by summing a back-ground vector and one or more sparse vectors generated from appropriate priors. The additive components can reflect the contributions of doc-uments, aspects, topics, or other factors chosen by the modeler. A basic SAGE topic model sets  X  from some sparsity-inducing distribution G , e.g., the Laplace distribution. As m is a shared back-ground frequency vector, H k is the learned resid-ual frequency vector of topic k .

Replacing the topic assignment in SAGE by its conditional expectation gives  X   X  This modification of the SAGE topic model is the same as the mi-vector model but with differ-ent regularization on the representation vector  X  ( d ) and l 1 regularization on the basis vectors H k . This  X  X arginal SAGE X  model could be useful in fu-ture work: the marginalization may mitigate the problem of topic-switching, yielding a more iden-tifiable (but perhaps less interpretable) model and lending to downstream tasks such as topic ID. LDA Latent Dirichlet Allocation (LDA) (Blei et al., 2003b) is a generative Bayesian topic model similar to SAGE, but in which each topic is drawn from a Dirichlet prior G rather than a sparsity-inducing distribution. LDA does not explicitly ac-count for the background distribution; to account for this, it is common practice to threshold the vo-cabulary a priori to remove very common and very rare words (though in our experiments, we do not do this). Therefore,  X  ( d,n ) is exactly H H k  X  Dirichlet (  X  ) . We compare these four models of learned repre-sentations empirically on two distinct tasks, topic ID and topic discovery. The essential imple-mentation details of the models are as follows; further details are provided in the supplement. We learn the mi-vector model in a maximum a posteriori framework as in McCree and Garcia-Romero (2015). Our own C++ implementation of SAGE , available online, 4 uses approximate mean-field variational inference, as in Eisenstein et al. (2011). We learn the LDA model using Gibbs sampling, implemented in MALLET (McCallum, 2002). 5 We perform LSA using centered tf-idf X  weighted word counts and centered l 2 -normalized triphone state cluster soft counts. We implement tf-idf by scaling the raw term count by the log in-verse document frequency. We apply l 2 normal-ization rather than tf-idf weighting to the speech data because it is dense and tf-idf is thus inappro-priate. On both text and speech, mean-centering is performed after the respective normalization, as this pre-processing recipe performed best of all the
For each of the four models, the low-dimensional real vector  X  ( d ) represents a given document d in our experiments. We also con-sider two high-dimensional baseline representa-tions: raw (soft) counts on both the text and speech data, and, only on the text data, tf-idf  X  weighted word counts. These tf-idf weights con-stitute a high-dimensional learned representation. 5.1 Topic ID In our first topic ID experiment we evaluate topic ID error on raw multinomial views of the data. To our knowledge, we are the first to adopt a multi-nomial view of triphone state clusters and apply it to topic ID. In subsequent experiments we explore the interaction of representation dimension with each model and dataset, and evaluate relative per-formance when the classifier is only given a frac-tion of the available data for training. This latter configuration is the most interesting, as it reflects the cost of obtaining supervised data in practice.
Given feature vectors for some representation of the documents in a corpus, topic ID is performed in a one-versus-all framework. We use logistic re-gression as the per-class binary classifier, imple-mented using LIBLINEAR (Fan et al., 2008). Re-sults were similar when logistic regression was re-placed by support vector machines. All document representations are length-normalized (divided by their l 2 norm) before they are input to the classi-fier. Performance is measured by topic ID error, the error of multi-class prediction where the class predicted for each document is that of the per-class classifier that gave it the highest weight. Baseline performance on the test set (where the baseline classifier chooses the most prevalent topic in the training set for all test examples) is 96.2% error. Note that this error rate differs from the uniform-at-random classification error rate of 97.5% be-cause of the uneven distribution of topics. Document Construction Prior work (Hazen et al., 2007; Wintrode and Khudanpur, 2014) treated whole conversations as documents in addition to separating each conversation into its two sides. We perform a small topic ID experiment in this configuration to probe the impact of this design choice. Ten-fold cross-validation (CV) is used to tune the logistic regression regularizers. On the test set, the classifier achieves topic ID error of 12.4% and 15.6% for whole-conversation and individual-side text data, respectively, and 20.1% and 29.5% for whole-conversation and individual-side speech data, respectively. These results cor-respond roughly to results listed in Table 3 of Hazen et al. (2007), specifically, the topic ID er-ror of 8.2% and 12.4% for whole-conversation and individual-side transcriptions, respectively, and 22.9% and 35.3% for whole-conversation and individual-side triphones derived from ASR lat-tices, respectively (Hazen et al., 2007). However, we use logistic regression without feature selec-tion instead of Na  X   X ve Bayes with feature selection, and we apply our classifier to triphone state cluster soft counts inferred by a DNN instead of triphone counts from ASR lattices. We believe that the dis-crepancies in performance with respect to prior work are due to these differences in experimen-tal configuration. Our results and those of prior work show that using whole-conversation docu-ments instead of individual-side documents make the topic ID task easier. As a result, we expect that differences in performance between the different learned representations will be more clearly pro-nounced on individual conversation sides and we restrict the rest of our study to that setting. Dimensionality Study We perform topic ID on learned representations at dimensions K  X  { 10 , 50 , 100 , 200 , 300 , 600 } on individual conver-sation sides, using ten-fold cross-validation to tune the logistic regression regularizers. Figure 3 gives topic ID error results on the test set, vary-ing K . (Selected values are listed in Table 1.) In both datasets, as the dimension K increases, topic ID error decreases, approaching (approxi-mately) the raw baseline. On text, tf-idf performs slightly better than the raw representation. LSA is marginally the best-performing lower-dimensional learned representation; LDA and mi-vectors per-form well at some representation sizes, depending on the data source, but their performance is less consistent. SAGE performs poorly overall. Limited Data Study The raw text and speech representations (multinomial observations) are very high-dimensional, and the classifier is likely to overfit to specific components (words or tri-phone state clusters) in these representations. To measure this effect and attempt to separate the predictive power of logistic regression from the quality of the learned representations in our anal-ysis, we experiment with reducing the number of labeled training examples the classifier can use; we still learn representations on the full (unla-beled) training set. This experiment represents the limited-supervision setting in which supervised data is costly to obtain but unlabeled data abounds.
We run this experiment twice, using ` = 2 and ` = 6 labeled examples per topic, for a total of 80 and 240 classifier training examples, respec-tively. Ten-fold cross-validation is used to fit the regularizer; per-class loss coefficients are set ac-cording to the class prior in the original training set in order to counteract the artificial balancing of the classes in the limited-supervision dataset. We report cross-validation estimates of the topic ID error on the training set for K = 10 (Figure 4), K = 100 (Figure 5), and K = 600 (Figure 6). For K = 100 and K = 600 , LSA dominates in the limited-supervision setting. Mi-vectors perform as well as or better than other low-dimensional learned representations at K = 10 , and exhibit mixed performance for larger K . SAGE performs ter than SAGE, but not as well as mi-vectors. Fi-nally, tf-idf X  X eighted word counts perform very well on text, often achieving the best performance of all representations, even under limited supervi-sion (but at the same dimension as the raw data). 5.2 Topic Discovery To quantitatively assess representations X  potential for topic discovery we compute their V-measure against the gold-standard labels. V-measure is an unsupervised measure of similarity between two partitions (Rosenberg and Hirschberg, 2007) and is equivalent to the mutual information normalized by the sum of the entropy (Becker, 2011).

For all representations, we compute V-measure between a partition induced by that representation and the gold-standard topic labels on the test set. A partition is induced on a representation by assign-ing each document d to the cluster indexed by the coordinate of  X  ( d ) with highest value (the argmax). Results of this analysis are displayed in Figure 7. (Selected values are listed in Table 2.) On the text data, SAGE dominates the lower-dimensional rep-resentations, LSA is next best overall, and LDA and mi-vectors exhibit relatively low performance; the high-dimensional tf-idf weights are surpassed by SAGE for K &gt; 10 but beat other representa-tions by a significant margin. On speech, SAGE is best overall, mi-vectors exhibit similar but gener-ally lower performance, LDA performs worse, and LSA is worst.

We also measure the topic discovery potential of the mi-vector and SAGE representations more directly. First, we provide a manual inspection of the learned topics: in Table 3 we show the top-five word lists for five random topics from the 600-dimensional mi-vector and SAGE models (respec-tively) learned on the text data. In both models, the top five words in a topic are selected accord-ing to the five largest positive values in the cor-responding vector H k . Qualitatively, the SAGE topics are considerably more interpretable than the mi-vector topics: the SAGE topics represent is-sues of censorship, foreign relations, coffee fran-chises, welfare, and professional basketball, while the mi-vector topics are less succinctly characteri-zable and more polluted by uninformative words. We complement this qualitative analysis with Mimno et al. (2011) X  X  intrinsic coherence mea-sure, a standard quantitative method. This scor-ing function, which correlates well with human quality judgments, averages estimates of the con-ditional log-likelihoods of each topic X  X  M highest-weighted words across all topics. Using K = 600 models on text as before and picking M = 20 , we compute mi-vector coherence as  X  453 . 34 and SAGE coherence (averaged over three runs) as  X  407 . 52 , indicating that SAGE is more amenable to topic discovery and human interaction. We have theoretically and empirically compared several content-bearing representations of text and speech from prior work. We have measured the relative performance of these representations on topic ID, an easy-to-evaluate task familiar to both text and speech research communities. We have also assessed the representations in their ability to discover the topics inherent in a corpus, a task that is more prominent in the text community and more difficult to evaluate. On our subset of the Fisher English data, these tasks appear to have compet-ing objectives: the best representations in one task are not necessarily the best in the other. In partic-ular, while SAGE yields the worst performance as a feature learner for topic ID, it is demonstrably superior to other low-dimensional learned repre-sentations in topic discovery. We have evaluated performance in topic discovery by distributional similarity to gold-standard topics as a proxy for human-annotated judgments of topic quality, and briefly compared the interpretability of mi-vectors and SAGE; future work could pursue expert or crowd-sourced human evaluations.

In the full-supervision setting of topic ID, the lower-dimensional learned representations con-verge in performance to the raw representation as the dimension K increases. However, if only a couple of labeled examples per class are available, reflecting the expense of obtaining labels in prac-tice, then learned representations generally outper-form the raw representation, which is more prone to overfitting. It is surprising that tf-idf performs so well in the limited supervision setting; it is learned from the data, but it should be prone to overfitting due to its high dimensionality. It is also surprising that SAGE performance on text de-grades significantly at high dimensions; we sus-pect this is due to topic switching, but further in-vestigation is warranted. Overall, though, for topic ID on word counts or triphone state cluster soft counts, if labeled data is scarce, we benefit from training on unsupervised learned representations.
In the V-measure experiment, the documents were partitioned according to the heaviest coordi-nate in their representations. This choice of exper-imental protocol is a nuisance variable in our re-sults; other partition constructions may yield dif-ferent conclusions. In particular, the heaviest-coordinate partition may favor topic models, whose representations are probability vectors, and disfavor mi-vectors and LSA, whose representa-tions may have positive and negative coordinates encoding general linear combinations.

Within each task, the ranking of the represen-tations (by performance) is generally consistent between the text and speech data; however, mi-vectors often outperform LDA on the speech data, while LDA often outperforms mi-vectors on the text data. This may be evidence that the two com-munities have already independently identified ap-propriate dimensionality reduction techniques for their respective data sources. However, our re-sults support that the speech community can bene-fit from broader use of sparsity-inducing graphical models such as SAGE in tasks like spoken topic discovery and recommendation, in which human-interpretable representations are desired. The text community may similarly benefit from parsimo-nious models such as LSA or mi-vectors in down-stream tasks; underparametrized mi-vectors per-form particularly well on text, and future work may benefit from investigating this setting.
Word counts and triphone state cluster soft counts provide only one view of text and speech (respectively), and other input representations may yield different conclusions. The particular LSA approach we used for text, based on tf-idf weight-ing, is not as appropriate for our speech data, which is dense. Future work could evaluate other implementations of LSA or use a higher-level view of speech, such as triphone state cluster n -grams, that more naturally exhibits sparsity and lends to tf-idf weighting. In particular, weight-ing by a likelihood ratio test statistic and apply-ing a log transform has generated better perfor-mance in several other tasks (Lapesa and Evert, 2014). Future work could also test our conclusions on higher-resource views of speech, such as ASR word counts, or lower-resource views such as mel-frequency cepstral coefficients (MFCCs).

We have provided a brief cross-community evaluation of learned representations on multi-nomial text and speech data. Some prior work has evaluated related learned representations on text data alone, surveying parameters and tasks at greater breadth (Lapesa and Evert, 2014; Levy et al., 2015). A similarly comprehensive evalua-tion spanning the text and speech research com-munities would demand great effort but provide a large and versatile resource. In complement, a de-tailed, case-by-case analysis of errors made by the models in our study could illuminate future model-ing efforts by exposing exactly how and why each model errs or excels in each task. Topic ID and topic discovery are competing ob-jectives in our setting: we found that the best-performing representations per task were the same whether considering text-or speech-based com-munications. By evaluating learned representa-tions from both the text and speech communities on a common set of data and tasks, we have pro-vided a framework for better understanding the topic ID and topic discovery objectives, among others. More generally, we hope to encourage cross-community collaboration to accelerate con-vergence toward comprehensive models of lan-guage.
 Acknowledgments We would like to thank the three anonymous re-viewers for their feedback. A National Science Foundation Graduate Research Fellowship, under Grant No. DGE-1232825, supported the second author. We would like to thank the Johns Hopkins HLTCOE for providing support. Any opinions ex-pressed in this work are those of the authors.
