 Community Question Answering (CQA) has recently become a popular type of web pairs from the large scale CQA archives. Examples of such CQA services include Yahoo! Answers 1 and Baidu Zhidao 2 , etc. 
To effectively share the knowledge in the large scale CQA archives, it is essential pairs that are semantically equivalent or relevant to the queried questions. 
As a specific application of the traditional Information Retrieval, Question Re-questions are organized into a hierarchy of cat egories. That is to say, each question in the CQA archives has a category label. The questions in the same category or subca-tegory usually relate to the same topic. For example, the questions in the subcategory  X  X ravel.Asia Pacific.China X  mainly relate to travel to or in the country of China. This performance. To exemplify how a categorization of questions can be exploited, con-sider the following queried question (  X  ):  X  X an you recommend sightseeing opportuni-ties for senior citizens in China? X  The user is interested in sightseeing specifically in China, not in other countries. Hence, the historical question (  X  )  X  X an you recommend vant to the queried question although the two questions are syntactically very similar, making it likely that existing question retrieval methods will rank question  X  highly among the returned ranking list. If we can establish a connection between  X  and the could be promoted, thus perhaps improving the question retrieval performance. 
Several work [3, 2] has been done to inco rporate the category information into ex-isting retrieval models. These work shows that exploiting the category information for each question properly can improve the retrieval performance effectively. However, [3]. 
Language modeling has become a very promising direction for information retriev-al because of its empirical good performance. In this paper, we try to integrate catego-term may be more important in some category than other categories. For example, but not in the category  X  X ravel.Asia Pacific.Korea X . We model the individual term X  X  ponding term emission parameter of the multinomial question language model. Thus, we attain an internal formula that effectively boosts the score contribution from terms when the term is more salient in the category of the historical question. This incorpo-category information at the word level [2] or at the document level [3] when we con-duct experiments on a large scale real world CQA dataset from Yahoo! Answers. methods on how to exploit the category information to enhance the question retrieval performance. Section 3 details our proposed category-integrated language model. Section 4 gives a method of measuring a term X  X  saliency in a specific category. Sec-tion 5 describes the experimental study on a large scale real world CQA dataset from section 6. lexical gap problem between the queried question and the historical questions. Beside method to find similar questions. Ming et al. [8] explore domain-specific term weight [3] propose different strategies to exploiting the category information to enhance the question retrieval performance in the whole collection with the whole categories. Cai et al. [1] propose to learn the latent topics from the historical questions to alleviate the lexical gap problem, while Ji et al. [5] propos e to learn the latent topics aligned across the historical question-answer pairs to solve the lexical gap problem. Moreover, Cai et al. [1] also incorporate the category informa tion of the historical questions into learn-ing the latent topics for further improvements. 
Because our work in this paper mainly focuses on investigating a new method to category information to enhance the question retrieval performance. 2.1 Leaf Category Smoothing Enhancement As the first attempt to exploit the category information, Cao et al. [2] propose a two-level smoothing model to enhance the performance of language model based question retrieval. A category language model is computed and then smoothed with the whole category language model. The experimental results showed that it can improve the performance of the language model significantly. Following [2], given a queried function of the two-level smoothing model can be written as:  X   X   X  |  X   X   X  X  X   X   X 1 X   X   X   X  X  X   X   X  |  X   X   X  X  X  X   X   X 1 X   X   X   X  X  X   X  X  X  X  X  X   X   X   X   X  X  X  X   X  X  X   X   X  |  X  X  X  X   X   X  ranking function will reduce to the unigram language model with Jelinek-Mercer (JM)  X  spectively. 
From Equation (1), we can see that this approach combines the smoothed category language model with the original question language model linearly only in an external way at the word level . However, in our work we will integrate the category informa-seems to be more promising. 2.2 Category Enhancement Realizing that the leaf category smoothing enhancement method is tightly coupled with the language model and is not applicable to other question retrieval models, Cao which is to compute the ranking score of a historical question  X  based on an interpo-lation of two relevance scores: a global relevance score between the queried question  X  queried question  X  and the historical question  X  . In this case, various retrieval mod-Then they normalize them into the same range before linear combining. Following [3], the final ranking function can be written as: to control the linear interpolation, and  X  X  X  X  is the normalization function for the local and global relevance scores. 
From Equation (2), we can see that this approach combines the two relevance models. However, in our work we will focus on the language modeling framework to internal way at the word level . 
Overall, leaf category smoothing enhancement and category enhancement methods by the category-specific term saliency into the unigram language modeling approach in a more systematic and internal way at the word level which is more effective than linear combination in an external way at the word level or at the document level. We proceed to present our method in the following sections. 3.1 The Unigram Language Model mate a model for each historical question, and then rank the questions by the likelih-ood of the queried question according to the estimated models. same number of parameters (i.e. word emission probabilities) as the number of words likelihood would be  X   X   X  |  X   X   X  X  X  X  X  X   X   X |  X   X  where  X  X , X  X  X  is the count of word  X  in the queried question  X  . simplest way is the maximum likelihood estimation:  X   X  X  X   X  X  X  X   X   X  X  estimation, which commonly interpolate the estimated document language model with the collection language model [11]. 3.2 Integration with Category Information Now, we discuss how to integrate the category information of each historical question into the unigram language model. some category than other categories. That is to say, a term of a historical question in For example,  X  X irus X  in the category  X  X omputers &amp; Internet.Security X  is more salient than the term in the category  X  X omputers &amp; Internet.Software X . 
By empirical Bayesian analysis, we could express our knowledge or belief on the distribution where the parameters come from is usually exploited to express the prior belief. The natural conjugate of multinomial distribution is Dirichlet distribution. 
Specifically, supposing that  X  X  X  is the category-specific term saliency computing on  X   X  with hyper-parameters  X  X  X  X  X   X   X ,  X   X ,..., | X  X   X  , given by  X  X  X   X   X  X  X  X  X |  X   X  X  X  X   X   X  X   X   X   X   X   X  X  where  X   X   X  X   X  X  X  X   X  X  X   X   X   X   X  X  X  X  X  X  X ,  X  ,  X   X  X  X  X  is the parameter, and  X   X   X  not depend on the parameter  X   X  . 
Then, with the Dirichlet prior, the posterior distribution of  X   X  is given by  X  X  X   X   X  X  X , X  X  of  X   X  reflects the updated beliefs about  X   X  . 
Given the posterior distribution, the estimation of the word emission probability can be noted as In the above estimated question model, we can see that the category information of a way, the category-specific term saliency could be seen as transformed to word count information which is the primary element that unigram language model has the ability to model. From another point of view, we could consider that the  X  X ag-of-words X  representation of the question  X  is transformed into a pseudo  X  X ag-of-words X  repre-sentation  X   X  X  X  X  given the question X  X  category-specific term saliency computing mod-to |  X  |  X   X   X   X  X  X  X   X  X  X   X   X   X   X  X  X  X  X  X  X ,  X  | X  X  tion  X  given a queried question  X  is changed to ranking  X   X  X  X  X  . On such ground, any  X  X ag-of-words X  based language model, e.g. the query likelihood model or the KL tion of the historical question in an internal way. 
Next, we further smooth the category-integrated language model by a collection language model  X  X  X  X  X | X  X  X  to account for unseen words as in [11]. Specifically, the used for smoothing. Thus, we can get the smoothed category-integrated estimation:  X  X  X   X   X  X   X   X   X  X  X  X   X  X  Note that, the final word count for the smoothed category-integrated language model  X   X  count  X  X   X   X   X  |  X  X  X  X   X  . 
If we set  X   X  X  X  X   X 0 , the ranking function will ignore the category information, thus will reduce to the unigram language model with Dirichlet smoothing [11]. 
Finally, the smoothed category-integrated ranking function in the query likelihood language modeling framework can be written as:  X   X   X  |  X   X   X  X  X  X  X  X   X   X |  X   X   X  X  X  X   X  A key notion in our proposed category-integrated language model is a term X  X  catego-ry-specific term saliency  X  X  X   X   X  X  X  X  X  X  X , X   X  in category  X  X  X  X  X  X  X  containing ques-tion  X  , which represents a term X  X  importance or saliency in the category that the ques-tion belongs to. Term distribution in a specifi c category is biased as compared to that whole collection. 4.1 Divergence-based Feature reveals the significance of terms globally in its category. We employ Jensen-Shannon defined as the mean of the relative entropy of each distribution to the mean distribu-thus we examine the point-wise function for each individual term as follows:  X   X  X  X   X  X  X  X || X , X  X  and  X   X   X  X  X  X  denote their corresponding probability distribution obtained by maximum likelihood estimation.  X   X  X  X   X  X  X | X , X  X  means the JS divergence of term  X  between the specific category and general whole collection. The higher  X   X  X  X   X  X  X | X , X  X  is, the more important or salient term  X  is. 4.2 Estimating Term Saliency from Divergence-based Feature needed to transform the JS divergence featur e into the term saliency score. The trans-formation function plays a very important role in setting the scale of proportional ratio between the JS divergence feature score of different terms. The following exponential form is used and tested, of which  X  is the JS divergence feature score,  X  X  X  X  is the parameter to control the scale of the transformed score and  X  is a smoothing parame-ter which is set as 1e-8 in our experiments. Then, the category-specific term saliency computing model can be written as: 5.1 Experimental Setup Question Retrieval Methods. To evaluate the performance of the proposed category-integrated language model, we use the following four types of baseline methods:  X 
LM: Unigram Language Model without considering the category information, we will give the results of unigram language model using JM smoothing and Dirichlet smoothing together, denoted as LM JM and LM Dir respectively 3 .  X 
LM@OptC: This method performs retrieval only in the queried question X  X  category specified by users of Yahoo! Answers using LM, where JM smoothing is used.  X 
LM@LS: Leaf Category Smoothing Enhancement method. (in section 2.1)  X 
LM+LM: Category Enhancement method that the global relevance and local relev-ance are computed using LM, where we also use JM smoothing. (in Section 2.2)  X  CLM: This is our proposed Category-integrated Language Model. Dataset. We use an open dataset 4 which is used in the pioneer work [3, 2] of exploit-world CQA dataset from Yahoo! Answers. The open dataset contains 252 queried experiments, we perform preprocess as follows: all the questions are lowercased and 1262 categories at the leaf level. Each question belongs to a unique leaf category. Metrics. We evaluate the performance of all the ranking methods using the following metrics: Mean Average Precision (MAP) and Precision@n (P@n). MAP rewards results. P@n reports the fraction of the top-n questions retrieved that are relevant. We also perform a significance test using a paired t -test with a significant level of 0.05. 5.2 Parameter Setting There are several parameters to be set in our experiments. Following the literature [3, using JM smoothing and  X 0.2  X , X 0.2  X  in Equation (1) for getting LM@LS. In LM+LM, we set  X 0.1 X  in Equation (2) [3]. when  X 5 X  the performance on MAP metric reaches high and is almost stable; when  X 11 X 3 X  the performance on P@10 metric is relative high, however when  X 11  X  , the performance on P@10 drops quickly. Thus, the best Dirichlet (around 500~2500) on the retrieval tasks on TREC data [11]. The main reason may be that the historical questions in question retrieval tasks are very short, which is not the case on the traditional retrieval tasks on TREC data. Specifically, we will fix  X 10 X  in all the following experiments. parameters by maximizing the empirical performance on the collections through ex-haustive searches in the following parameter space showed in Table 1. 
Figure 2 reports the parameter sensitivity of CLM on the test collection. From Fig-drops quickly when  X   X  X  X  X   X 10 . The best performance is achieved when  X   X  X  X  X  is set to about 7. Moreover, when  X 2.3  X  X  X  X  the best value of  X   X  X  X  X  is relative insensitive to performance of CLM reported in the next subsection is:  X   X  X  X  X   X 2.7 X  X  X  X , X 7 . 5.3 Performance Comparison Table 2 presents the comparison of different methods for question retrieval. Row 1 and Row 2 are the Unigram Language Model without considering the category informa-Row 4 and Row 5 are the Leaf Category Smoothing Enhancement and Category En-hancement methods, which are the proposed methods in the pioneer work [3, 2]. Row 6 is our proposed method. There are some clear trends in the results of Table 2: 1. Unigram language model using Dirichlet Smoothing significantly outperforms that using JM smoothing in the setting of our experiments (Row 1 vs. Row 2). 2. The method retrieving in the optimal category that specified to the queried question by users of Yahoo! Answers does not improve the methods without considering the category information (Row 3 vs. Row1, 2). The results show that the queried question X  X  category information does not help the retrieval performance when used directly and this is consistent with the former work [3, 2]. This maybe because re-levant questions are also contained in the other categories but not only in the cate-gory of the queried question. 3. When using the methods proposed in [3, 2], which incorporate the category infor-mation into the retrieval model externally at the word level (LM@LS) or at the document level (LM+LM), the performance can significantly outperform the uni-gram language models that do not consider the category information (Row 4, 5 vs. our experiments of LM+LM does not outperforms LM@LS, which is not the case in the results of [3]. We think that there are mainly two possible reasons: First, the preprocessing of the dataset maybe different. Second, LM+LM combines two rele-
LM@LS at the word level. 4. Our proposed CLM significantly outperforms LM@LS and LM+LM (Row 6 vs. 
Row 4, 5). We conduct a significant test (a paired t -test with a significant level of 0.05) on the improvements of our approach over LM@LS and LM+LM. The re-evaluation metrics. This demonstrates that our proposed CLM which integrate the category information into the unigram language model at the word level internally is more effective than the methods which incorporate the category information level. In this paper, we propose a novel category-integrated language model (CLM) to inte-grate the category information into the unigram language model internally at the word level for improving the performance of question retrieval in CQA archives, which views category-specific term saliency as Dirichlet hyper-parameters to weight the parameters of the multinomial language model. This integration method has solid mathematical foundation, and experiments conducted on a large scale real world CQA dataset from Yahoo! Answers demonstrate that our proposed CLM can significantly outperforms the previous work that incorporats the category information into the uni-gram language model externally at the word level or at the document level. 
Some interesting future work should be continued. First, it is of relevance to apply and evaluate other question retrieval methods, e.g., translation-based methods, in the proposed framework. Second, we believe it is interesting to include answers into our proposed framework. Finally, the hierarchical category structures may perhaps be exploited to further improve the performance of the CLM model. Acknowledgements. This work is supported by the National Science Foundation of China under Grant No. 61070111 and the Strategic Priority Research Program of the Chinese Academy of Sciences under Grant No. XDA06030200. 
