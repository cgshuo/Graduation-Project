 In a number of application areas, distributed database systems can be used to provide persistent storage of data while providing ef-ficient access for both local and remote data. With an increasing number of sites (computers) involved in a query, the probability of failure at query time increases. Recovery has previously only focused on database updates while query failures have been han-dled by complete restart of the query. This technique is not always applicable in the context of large queries and queries with dead-lines. In this paper we present an approach for partial restart of queries that incurs minimal extra network traffic during query re-covery. Based on results from experiments on an implementation of the partial restart technique in a distributed database system, we demonstrate its applicability and significant reduction of query cost in the presence of failures.
 H.2.4 [ Database Management ]: Systems X  Query processing Algorithms, Reliability, Performance Distributed querying, fault-tolerance, query restart
In a number of application areas, distributed database systems can be used to provide a combination of persistent data storage and efficient access to both local and remote data. Traditionally, both updates and queries have been characterized by short transactions accessing only small amounts of data. This has changed with the emergence of application areas such as Grid databases, distributed data warehouses and peer-to-peer databases. In these areas, time-consuming queries involving very large amounts of data can be ex-pected. With an increasing number of sites (computers) involved in a query, the probability of failure increases. The probability also increases with longer duration of queries and/or higher churn rate (unavailable sites).

Previously, failure of queries has been handled by complete query restart. While this is an appropriate technique for small and medium-sized queries, it can be expensive for very large queries, and in some application areas there can also be deadlines on results so that complete restart should be avoided. An alternative to complete restart is a technique supporting partial restart . In this case, queries can be resumed on new sites after failures, utilizing partial results already produced before the failure. These results can be both re-sults generated at non-failing sites as well as results from failing sites that have already been communicated to non-failing sites.
In this paper we present an approach to partial restart of queries in distributed database systems (PROQID) which is based on de-terministic tuple delivery and caching of partial query results. The proposed approach has been implemented in a distributed database system, and the applicability of the approach is demonstrated from the experimental results which shows that query cost in the pres-ence of failures can be significantly reduced. The main contri-bution of our work is a technique for partial restart that: 1) Re-duces query execution time compared to complete restart, 2) In-curs only marginal extra network traffic during recovery from query failure, 3) employs decentralized failure detection, 4) supports non-blocking operators and 5) handles recovery from multi-site failures.
While we in this paper focus on using our technique for reduc-ing query cost in the context of failure during execution of large queries, we also note that the technique can be applied to solve a related problem: distributed query suspend and resume, where on-going low-priority queries can be suspended when higher-priority queries arrive. In this case our approach can be used to efficiently store the current state of the query with subsequent restart from the current state.

The organization of the rest of this paper is as follows. In Sec-tion 2 we give an overview of related work. In Section 3 we outline the assumed system model and query execution. In Section 4 we discuss how failures can be detected, which sites can fail and differ-ent times of failure. In Section 5 we describe how to support partial restart. In Section 6 we describe how to handle partial restart in the context of updates during the query processing. In Section 7 we evaluate the usefulness of our approach. Finally, in Section 8, we conclude the paper and outline issues for further work.
Previously in database systems, only fault tolerance with respect to updates has been considered. This has been solved through the concepts of atomic transactions, usually supported by logging of operations which can be used in the recovery process. Recently, in particular in the context of internet applications, persistent ap-plication and application recovery has received some attention [2, 18].

Much of the previous work on distributed database systems is obviously relevant. For a survey of state of the art in this area we refer to [13]. Recent work in this area includes query processors for Internet data sources, for example ObjectGlobe [4], and query execution in Grid databases [9].

A related work that directly addresses the issues regarding query restart, is the work of Smith and Watson on fault-tolerance in dis-tributed query processing [17]. Their work is based on GSA-DQP [1], a distributed query processing system for the Grid. Smith and Wat-son describe how they have extended this system to support fault-tolerance during query execution. Each site executing a query oper-ator, stores tuples produced in a recovery log before sending them towards the query initiator. When a failure is detected by the global fault detector, a new site is selected to replace the failed site. Re-covery logs from upstream sites are then replayed so that the new site can restart the failed operation. This assumes that the opera-tions produce tuples in deterministic order, however, how this can be achieved is not described. In contrast to our approach, the new site will produce duplicate tuples in cases where the failed oper-ation was partially completed. These are afterwards discarded at downstream sites by using a system of checkpoint marker identi-fiers. Our approach improves on the approach of Smith and Watson by avoiding duplication of tuples, employing decentralized failure-detection, supporting pipelined hash-join, and handling multi-site failures.

A similar approach is presented by Hwang et. al. [10, 11]. Their focus is on recovery from failures in a distributed stream process-ing setting. Three alternatives are presented, two which focus on having standby sites for all sites participating in the query. The third, upstream backup , is related to our tuple cache. In contrast to our work, their focus is on fast recovery at the expense of increased network load during execution and failure handling.

While our approach relies on the DBMS to make use of repli-cation in order to handle failures, an alternative is returning partial results [3]. The authors present an approach where the partial re-sult can be resubmitted by the application at a later time in order to hopefully access previously unavailable data sources. The pre-sented solution does not handle failures during query processing, a data source is assumed to either be available or unavailable for the entire query execution.

Related to query restart because of failures, is suspend and re-sume , typically done because of the arrival of a higher-priority query. In [5] the problem is discussed in context of a central-ized system, where variants of lightweight checkpointing are used to support resume. A similar centralized approach is described in [6]. Here restart cost is minimized by saving a bounded num-ber of tuples produced by filter operators (intermediate or final re-sults). These tuples are chosen so that they maximize the number of source tuples that can be skipped during restart of scan operations. In contrast to our work, these approaches deal with restarts caused by planned suspend rather than site failures. Also, they don X  X  look at restart of complete queries where our methods can also handle restart of (unfinished) subqueries.

Several approaches to restart loading after failure during loading of data warehouses have been presented, for example [14]. In con-trast to our work, the same source sites are restarted after failure, only failing sources sites are considered, and which source tuples can be skipped have to be computed after failure.

Using stored results are also done in a number of other con-texts, for example in semantic caching [8, 15], use of materialized views in queries [7], query caching [16], and techniques particu-larly useful for large and long-running queries, for example reopti-mization [12].

Traditionally, also various checkpoint-restart techniques have been employed to avoid complete restart of operations. However, these techniques have been geared towards update/load operations, and assuming the checkpointing information is stored locally, the query will be delayed until the failed site is online again.
In this section we outline the system model that provides the context for the rest of the paper and basic assumptions regarding query execution. We also introduce symbols to be used throughout the paper, summarized in Table 1. The system consists of a number of sites , S i . Each site has a DBMS, and a site can access local data and take part in the exe-cution of distributed queries, i.e., the DBMSes together constitute a distributed database system. The distribution aspects can be sup-ported directly by the local DBMS or can be provided through mid-dleware.

The degree of autonomy is orthogonal to the techniques pre-sented in this paper, i.e., the sites can be completely autonymous or managed by a single entity. The only requirement is that they have a common protocol for execution of queries and metadata manage-ment.

Our approach assumes that data can be represented in the rela-tional data model, i.e., tuples t i being part of a table T . A table can be stored in its entirety on one site, or it can be horizontally frag-mented over a number of sites. Fragment i of table T is denoted T . Vertical fragmentation can be considered as redesign of tables and require no special treatment in our context.

In order to improve both performance as well as availability, fragments can be replicated, i.e., a part of a table can be stored in more than one site. Replica j of fragment T i is denoted T
We assume queries are written in some language that can be transformed into relational algebra operators, for example SQL. During query planning the different algebra nodes n i are assigned to sites. This requires catalog lookups in order to transform logical table accesses into physical localization programs (for example a set of accesses to horizontal table fragments). We assume that sites can be assigned more than one algebra node so that one site can be assigned a subquery. As all sites have the capability to execute operators, sites containing table fragments used in the query are typically also assigned query operations on these fragments during planning. This tends to reduce network traffic as tuples can be pro-cessed locally. Detailed algorithms for assigning nodes to sites are beyond the scope of this paper. Example algebra with site assign-ment is shown in Figure 1. Symbol Description S f Failed site, assigned the algebra node n f .

S s Site(s) with the source algebra node(s) n s (i.e., pro-S r Site replacing S f . Will restart n f .

S t Site with the algebra node receiving the results from
By supporting partial restart of queries, one site is able to pick up and continue an operation after the site that originally executed the operation has failed. We now give a brief overview on how to per-form query restart before we delve into the details in the following sections.
 For now we assume that each site only has a single algebra node. This node, as described in Section 4, can be either a local table fragment access or an operator node. This restriction is lifted in Section 5.5 where we look at handling multiple failures.
With local table fragment access, restarting is independent of other sites. After S s detects a failure of S f , it must locate a suit-able S r using catalog lookup and transmit the table fragment access operation to it so that the operation can be restarted.

In the case of operator failure, the failed node had one or more operands located on source site(s) S s . As part of the restart, it is necessary to inform these of the new S r so that operand tuples can be sent there. The amount of tuples that must be sent from S pends on the failed operation and will be discussed in Section 5.2.
In the worst case, all tuples that S s ever sent to S f must be resent to S r . If no measures were taken to prevent it, S s would then be required to completely restart its/their operation(s). If these oper-ations were not local table fragment accesses, restarting S require resending of its operand tuples and so on. To prevent this cascading effect to lead to a complete restart of the algebraic sub-tree to node n f , each site can use a tuple cache . This cache stores tuples produced by the algebra node furthest downstream on each site. In this way tuples can be resent after site replacement with-out recomputation and without involving sites further upstream. Caching will of course require storage at each site, but depending on operator this cost can be marginal compared to network cost of a complete restart of the algebraic subtree.

Assuming a failed operation was partially completed, restarting it can produce duplicates. Our approach is to prevent these dupli-cates from being sent rather than later removing them downstream. Details are given in Section 5.2.

In this section we will assume that operations are mostly read-only, with updates only done in batch at particular times (i.e., simi-lar to assumption doen in previous work as e.g. [6]). This assump-tion fits well with our intended application areas. In Section 6 we will describe how partial restart can be supported also in the context of updates.

In the rest of this section, we present in more detail the approach for partial restarts of queries.
After S t detects a failure in S f , it must find a replacement site S (denoted f indReplacementSite () in the following algorithms). How this is done depends on whether one or more of the algebra nodes at S f were table/fragment access operations or not.
After S r has been selected, sites downstream must be notified of the selection so that they can update their local copy of the algebra tree. This is necessary to ensure that handling of subsequent fail-ures are done using an algebra tree with correct node assignment.
Site with no table/fragment access: If n f is an operator, any site can potentially be selected as replacement (we assume equal capabilities of all sites). Because S t knows its algebraic subtree(s) and assigned sites, it has enough information to select an S limits network traffic (i.e., the same site as one of S s
Site with table/fragment access: If n f is a local table fragment access, the replacement site must be selected among other sites with replicas of the same fragment. If no live replicas exist, the detector site notifies the initiator site. Application dependent, the initiator can chose to abort or continue with incomplete data.
This section describes in detail how different algebra nodes can be restarted. We first discuss the simple case of table access op-erations. We then turn to operators, which can be categorized into two categories: stateless and stateful operators. In the case of state-less operators, each tuple is processed completely independent of other tuples. Examples of such operators are select and project. In the case of stateful operators, a result tuple is dependent on several operand tuples. Examples of such operators are join and aggrega-tion.
Restarting local table fragment access can be performed inde-pendent of other sites. After S s detects a failure of S f cate a suitable S r using catalog lookup and transmit the table frag-ment access operation to it so that the operation can be restarted. To avoid sending duplicates, S s notifies S r of the number of tuples it received from S f before the failure. By assuming deterministic or-der of sent tuples (for example by acessing the table in the  X  X atural order X , either based on row identifier or primary key), this number is enough to ensure that S r can restart exactly where S f
In the case of stateless operators like select and project, each operand tuple can be processed independently, and the tuple is not needed after it has been processed as long as the results are not lost. Therefore, a full resend of tuples from source nodes is not needed when partially restarting such operators.

For example, if t i was the last operand tuple processed by a project node before it failed, the replacement project node must start processing on operand tuple t i +1 . To know how many operand tuples an operation has consumed, each produced tuple packet is tagged with the last operand tuple(s) used to produce the result tu-ples in the packet. This can be more than one number if the operand was a fragmented table (one operand tuple number per fragment). This algorithm is shown in Algorithm 2 and explained below. Algorithm 2 Restart of select or project.
 At site S t , after detecting failure of S f :
S r  X  f indReplacementSite () t ns  X  tupleN umbers ( lastP acket ) Send ( N f ,S r ) {Algebra subtree} Send ( t ns ,S r ) At site S r , after receiving N f and tupleN um : n f  X  root ( N f ) n s  X  children ( n f ) {May be more than one node} S s  X  getAssignedSites ( n s ) Send ( S r ,S s ) { S s may be more than one site}
Send ( t ns ,S s )
After S f fails, S t selects S r and transmits the relevant algebra subtree (with n f as root). In order to do a partial restart, S resume select/project where S f failed. To do this, tuple packets produced by n f are tagged by the last source tuple number(s) tn from n s used to produce the packet. During recovery, tn s mitted from S t to S r and then to S s . This makes it possible for S s to resend tuples from tn s +1 to S r . In order for this to work, n f must produce tuples in a deterministic order (cf. Section 5.3 on how this can be achieved).

When n f is restarted at S r it must continue the tuple numbering where S f left off. I.e., the first packet received at S t have a number higher than t ns . This is necessary to ensure that subsequent failures of n f is restarted from the correct point.
For stateful operators like join and aggregation, each result tuple is dependent on more than one operand tuple. For example, when joining two operands A and B, each tuple from A is matched with each tuple from B. This means that as long as not all tuples from A has been received, all tuples from B are needed. Thus when restarting join, all tuples from B must be resent if the join crashed before all tuples from A had been processed (and vice versa).
Aggregation has similar properties: no results can be produced before the operand has been completely received (at least if we as-sume no grouping and unsorted tuples). Also, all result tuples are dependent on all operand tuples. So regardless of when aggrega-tion fails (before/during transfer of the result), all operands must be resent to the replacement node.

What can be done when restarting operators such as join and ag-gregation, is to prevent S r from sending tuples already sent by S before it failed. In order to do this, S t sends S r the number of tu-ples it has received, tn f . During processing at S r , these tuples are discarded. This is possible if tuples are produced in a determinis-tic order  X  so that the tn f first tuples from S r are the exact same tn f tuples S f produced first. The algorithms for restart of join and aggregation are presented formally in Algorithm 3 and 4.
We assume a pipelined hash-join algorithm in order to have the join node produce results as early as possible. This allows opera-tors further downstream to start processing as soon as possible and therefore lets more operators execute in parallel.
 Algorithm 3 Restart of join.
 At site S t , after detecting failure of S f :
S r  X  f indReplacementSite () tn f  X  numberOf T uplesReceived ( n f ) Send ( N f ,S r ) {Algebra subtree} Send ( tn f ,S r ) At site S r , after receiving N f and tn f : n f  X  root ( N f ) n s  X  child ( n f ) S s 1  X  getAssignedSite ( n s ) S s 2  X  getAssignedSite ( n s ) Send ( S r ,S s 1 ) Send ( S r ,S s 2 )
For joins, it is possible to optimize the algorithm if the failed join node had processed all tuples from one of the sources. Then the processed tuples from the other source do not have to be resent. This is because one can be certain that these tuples have already been joined with all tuples from the first source.
 Algorithm 4 Restart of aggregation.
 At site S t , after detecting failure of S f :
S r  X  f indReplacementSite () tn f  X  numberOf T uplesReceived ( n f ) Send ( N f ,S r ) {Algebra subtree} Send ( tn f ,S r ) At site S r , after receiving N f and tn f : n f  X  root ( N f ) n s  X  child ( n f ) S s  X  getAssignedSite ( n s )
Send ( S r ,S s )
For the algorithms described above, deterministic order of tuples produced by operations is vital. It allows us to use a single tuple number to describe the restart point for the transmissions from the replacement site. Note that deterministic order does not imply that the tuples have to be sorted.

For table fragment accesses, deterministic order can be ensured by, for example, sorting tuples on primary key or by having a lo-cal DBMS with deterministic table access operations (which is the general case). For operator nodes, we require that all sites use the same algorithms and that these give deterministic results if they process operand tuples in a deterministic order.

Two issues have to be taken care of in order to achieve deter-ministic order of operand tuples. First, a single operand can have multiple sources (for example due to table fragmentation). Second, an operator node can have two operands (e.g., join). Both these issues are handled by processing tuple packets from the various source sites in an alternating manner  X  pausing if a packet from the next source is not yet received. The first packet is taken from the source site with the lowest ID. These IDs are assigned during query planning. Assuming sources can supply tuples with equal rates, al-ternating between sources rather than processing unordered, should only incur a minor overhead.
After a replacement site S r has been selected and sent the alge-bra node n f , it must notify any sites S s with source nodes about its existence and request sending of operand tuples. As explained above, the extent tuples have to be resent depends on which opera-tion n f is. Regardless, S s may be asked to send tuples produced by algebra nodes long since completed. If means are not taken to pre-vent it, this can cause a cascading restart of the entire subtree as S will have to retrieve its source operand tuples in order to produce the tuples that are to be sent to S r .

In order to prevent this cascading restart, each site can use a tu-ple cache where produced tuples are stored until the query is com-pleted and there is no risk of restart. This cache is optional as there is always an option to restart the subtree completely, but as will be shown in Section 7, savings can be large  X  especially for opera-tions that produce far fewer tuples that they consume (aggregation is a good example). It is therefore possible to have tuple caching of results from some operations and not for others  X  this can for example be decided during query planning.

Only tuples produced by the most downstream algebra node as-signed to a specific site, need to be cached. A tuple cache between two nodes assigned to the same site would in any case be lost when the site fails and would thus be of no use during query restart. Fur-ther, tuples from table fragment access operations should not be cached as it can be assumed that they can equally well be retrieved  X 
C is the extra network traffic needed to handle the failure as NC/N where N is the number of tuples sent by all sites during execution without failure, while NC is the number of tuples sent by all sites during execution with failure.

With complete restart of the query, all data generated before the failure is removed from the participating sites and discarded. Therefore the percentage of extra tuples with complete restart is simply the amount of tuples transmitted before failure. In our ex-ample, each select produced only 16000 tuples so due to overhead with detecting failure of S 2 (message timeout and alive check) the other sites had sent all their tuples when S 2 was crashed. With larger amounts of data, this overhead would have been reduced and made it possible to stop other sites before they had sent all their tu-ples. We have therefore plotted two cases of complete restart: The measured values and an estimated best case. The best case assumes that all all sites S 0 ...S 4 have equal rate of tuple transmission and that all stop as soon as S 2 fail.

With partial restart, S 7 can continue right where S 2 stopped as described in Section 5.2. Nodes S 0 , S 1 , S 3 and S 4 were unaffected by the failure. Therefore no extra tuples were transmitted for partial restarts compared with no failure.
As argued in Section 5.2, stateful operators such as join are fun-damentally different from select . This test case is a simple join designed to highlight these differences: SELECT * FROM nation JOIN customer
The resulting algebra tree is illustrated in Figure 5 (left). We see that nation has two fragments while customer has five. Site 1 was selected for the join operator during planning to minimize network traffic as it has a fragment of both involved tables. The results from the execution of this query are shown in Figure 5 (right).
Since join is a stateful operator, all source operands must be re-transmitted to the replacement site. The only way partial restart is able to improve on complete restart in this case, is to prevent the new join node from transmitting tuples already sent from the failed site. These tuples make up the difference between the two lines in the graph.
 In this query, the crashed operator has operands on other sites. Tuples that are received from these sites but not processed before crash, cause the graph lines to deviate slightly from a straight line.
After investigating the properties of stateless and stateful of op-erators separately, we made a more complex test case containing a combination of operators: SELECT * FROM supplier JOIN ( SELECT ps_suppkey, COUNT( * ) FROM part JOIN partsupp WHERE ps_availqty &lt; 1000 AND p_size &lt; 21 GROUP BY ps_suppkey).
 This query was transformed into the algebra tree illustrated in Figure 6. After select each fragment of part and partsupp was reduced to about 16000 tuples. Aggregation produced 9624 tuples which was joined with two fragments of supplier , each 5000 tu-ples. The final result was 9624 tuples as well.

With this query it made sense to vary which site we crashed. S S and S 4 all have stateless operators and no external operands and can therefore be treated the same. We first evaluated a crash of S As S 4 has a similar role to S 2 in Query 1, it is not surprising that we got similar results. Partial restart can be done without any extra tuples transferred.

If S 1 is the failing site, all operands to its join must be resent. Un-fortunately, as aggregation does not send anything until it is com-pleted, this is an example where the partial restart in effect provides negligible benefits. Beyond avoiding duplicates from the aggrega-tion node, the only saving in our example is that tuples sent from the second supplier fragment on S 3 will not have to be resent.
The last site to evaluate failure for is S 2 . Execution of algebra nodes on this site has two separate phases. The first phase is local
