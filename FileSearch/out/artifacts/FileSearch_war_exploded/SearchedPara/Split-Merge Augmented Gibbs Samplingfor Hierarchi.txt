 The hierarchical Dirichlet process (HD P) [1] is an important tool for Bayesian nonparametric topic modelling, particular ly when mixed-membership exists, such as in a document collection. Each document is modelled as a group of words, generated from the underlying latent topic. It is an extension of Latent Dirichlet Allocation (LDA) [2], allowing unbounded latent dimensionality, with capacity to automatically infer the number of topics in a document set. The HDP is a hierarchial version of the Drichilet pro cess (DP) clustering model, where a cor-pus of documents are assumed to be generated from a set of top-level topics with independent mixing distribution. In contrast to the DP mixture model for which the metaphor is a Chinese Restu rant Process (CRP), a HDP can be ex-pressed using a metaphor of Chinese Restaurant Franchise (CRF), where a set of dishes is shared across a collection of f ranchise restaurants, each having tables generated using a CRP from the customers arriving at that franchise.
As with Bayesian nonparametric models, exact posterior inference is not tractable. MCMC or variational approximation are used for approximate poste-rior inference. In this paper, we focus on MCMC sampling, wherein posterior is computed from the empirical distribution of samples from a Markov chain, whose stationary distribution is the posterior of interest. [1] propose two MCMC sam-pling schemes, one based on the CRF and the other on the auxiliary variable method. In many cases the use of auxiliary variable sampling method is pre-ferred to keep the sampling simple and easily extendable to elaborate models such as iHMM [1]. The basic MCMC sampler for the HDP is an incremental Gibbs sampler -the topic is sampled for a single observation, one at a time, conditioned on preceding observations a nd model hyperparameters. Since, only one state change takes place at a time, mixing may be slow, requiring many Gibbs iterations for the MCMC chain to converge to its stationary distribution. Whereas CRF based sampling is staightforward in formulation, the implemen-tation is tedious, requiring tracking of individual table assignments for each restaurant, and then tracking the dish preference for each table. The auxiliary variable split-merge sampler is based on directly sampling the topic assignment (dish) of the words (customers) in the documents and thus straightforward in implementation.

Split-merge MCMC samplers have been pr oposed for Bayesian nonparametric models, such as for DPM to accelerate mix ing [3]. In a split-m erge setting, a group of observations are moved together in the state space based on whether splitting or merging of topics are accepte d based on a Metropolis-Hastings ratio. In practice, each sampling run consists of a Gibbs sampling followed by a split-merge proposal evaluation. Since, the state change may occur for a group of points at each iteration, the MCMC chain can quickly traverse the state-space and potentially converge faster than if only the Gibbs sampler is used.
Motivated by this, we propose a split-m erge procedure for the HDP to accel-erate the mixing of the MCMC chain for the auxiliary variable sampling scheme called the Split-Merge Augmented Gibbs sampler . Assuming each word (cus-tomer) in the document corpus has been assigned to a topic(dish) at the higher level, we evaluate a split-merge proposal on the customer-dish relationship i.e. we either propose to split all the customers in all the franchise restaurants who share the same dish into two different d ishes or propose merging the set of all customers sharing two different dishes. In contrast to the CRF based split-merge sampling scheme [4], we do not worry about the lower-level customer-table as-signments and thus the proposed split-m erge scheme is effective at both levels of the HDP.

We evaluate and analyze the proposed algorithm on synthetic data and two benchmark document corpus, -NIPS abstracts and 20 News Group data. In synthetic experiments, we generate topics with low separability and show that the incremental Gibbs sampler is unable to recover all the correct topics; however, our split-merge augmented Gibbs sample r is able to recover all topics correctly. For the document corpus, we evaluate the performance of Gibbs vs our sampler based on the perplexity of held-out data and show that our proposed method is able to produce lower perplexity in similar time.

The layout is as follows: Related background on HDP and inference tech-niques is described in the section 2; in t he section 3, we detail the split-merge procedure after briefly reviewing the Gibbs sampling procedure based on the auxiliary variable scheme. Experimental results are discussed in section 4 and finally, section 5 concludes our discussion. Dirichlet Proess (DP) mixture model for clustering with theoretically unbounded mixture component has been first studied in [5] with [6] giving a stick-breaking construction for the DP prior. Hierarch ical Dirichlet Process (HDP) extends the DP in two level where the bottom level DP uses the top-level DP as the base measure was first proposed in [1]. This is a mixed-membership model where a group is sampled from a mixture of topics and has been used extensively for document analysis [7], multi-population haplotype phasing [8], image/object retrieval [9] etc.

Split-merge sampling for DP mixture model was first proposed in [3] and splitting of a single cluster by running a Restricted Gibbs sampler on the subset of points belonging to that topic is described. Whilst a merge proposal is easy to generate, generating a split proposal takes some work as a random split will most likely to be a bad proposal and th ey would be rejected. Hence, the need for the Restricted Gibbs sampler. Using the same framework [10] proposed a slightly different split-merge algorithm by having a simpler split routine using a sequential allocation scheme. In contrast to running a Gibbs sampler to generate a split proposal they proposed a single run sequential allocation scheme to gen-erate the split, thus reducing the overhead cost. Split-merge sampler for HDP based on the Chinese Restaurant Franchise sampling scheme has been proposed in [4]. This perform splitting or merging only at the top level assignments using the similar procedure for the DP with additional factors coming from the bottom level when computing the prior clustering probability. 3.1 Hierarchical Dirichlet Process The hierarchical Dirichlet process is a distribution over a set of random prob-ability measure over (  X , B ) . It is a hierarchical version of the DP, where a set of group level random probability measures ( G j ) J j =1 are defined for each group which shares a global random probability measure G 0 at the higher level. The global measure G 0 is a draw from a DP with a base measure H and a concen-tration parameter  X  . The group specific random measures G j are subsequently drawn from a DP with G 0 as its base measure, with j denoting the group. Since G j are drawn from the almost surely discrete distribution of G 0 , it ensures that the top level atoms are shared across the groups. In the topic model context each document is a group of words and the atoms (topics) are the distribution over words. The stick-breaking representation of G 0 can be expressed as, where  X  k  X  H independently and (  X  k )  X  k =1 admitting stick-breaking construction can be expressed as, whereitcanbeshownthat[1]  X  j  X  DP (  X  0 , X  ) . The stick-breaking representa-tion for HDP is given below, 3.2 Posterior Inference with Auxiliary Variable With the stick breaking representation of 5, the state space consists of ( z , X , X , X  ) . Since z and  X  forms a conjugate pair,  X  can be integrated out giving the condi-tional probability of z given  X  as.
 From 6 the prior probability for z ji given z  X  ji and  X  can be expressed as, where  X  =[  X  1  X  2 ... X  u ] such that  X  u =  X  k = K +1  X  k . Adding the likelihood term we can have the sampling formula of z ji as, where  X  u is sampled from its prior H .Ifanewtopic( K +1 ) is created then we set  X  K +1 = b X  u ,where b  X  Beta (1 , X  ) .Tosample  X  we use the auxiliary variable method as outlined in [Teh]. We first sample the auxiliary variable m from, where s ( n jk ,m ) are the unsigned Stirling numbers of the first kind. Subsequently,  X  is sampled from, Eq 8910 completes the Gibbs sampling formula for HDP inference. For elabora-tion please refer to [1,7]. 3.3 Split-Merge procedure The split-merge proposal is a form of Metropolis-Hasting algorithm where the algorithm draws a new candidate state C  X  from a distribution with density  X  ( C ) according to a proposal density q ( C  X  /C ) and then evaluation of the proposal based on the Metropolis-Hasting ratio of The proposal C  X  is accepted with the probability a ( C  X  ,C ) . If it is accepted the state changes to C  X  or it remains at C .ForHDPmixturemodeltheabove formula takes the form of  X  as, where c k is the k th topic. Given this assignement scheme, the probability of a topic set { c k } K k =1 can be expressed as, denotes the rising factorial and can be computed as the ratio of two gamma functions. For a split proposal a particular topic k is splitted in k 1 and k 2 and the new configuaration is denoted as C split . After we generate new latent as-signements z split corresponding to C split ,weresample  X  using 9 and 10 to obtain  X  split . The configuration probability of P ( C split / X  split ) can now be computed as, proposal when topics k 1 and k 2 are merged into a single topic k and  X  merge k is sampled with the new z merge ,thenwehave, proposed method we will use the conditional configuration probability ratio in place of P ( C  X  ) P ( C ) as our target distribution is  X  ( C |  X  ).
The likelihood term L ( C/ x ) is computed over all the words of all the docu-ments and is given as, where H ji,c ji is the posterior distribution of  X  based on the prior G 0 and all the observations x j ,i such that j &lt;j and i &lt;i . The above integral is analytically tractable if G 0 is conjugate prior. We can express the above likelihood equation as a product over topics such that, Expressing this way now we can compute the ratio of likelihoods between a split proposal C split and the existing configuration C as, Similarly, for merge proposal the ratio of likelihood is, To evaluate the proposal density q ( C  X  | C ) we need to create an algorithm for cre-ating C  X  from the existing configuaration C . Here we use sequential assignment method similar to [10] for that. Let us assume that we are generating a split proposal for the topic k into two topics k 1 and k 2 . We need to divide the words a random word from a random document as the seed for the topic k 1 and sim-such that ( jr 1 ,ir 1 ) =( jr 2 ,ir 2 ) . The rest of the words from the set S can be assigned by sampling from, where, a simple allocation of  X  split k  X  / 2 . The proposal probability q ( C split | C ) is computed as a product of the above probabilities based on the actual assignment. The reverse proposal probability q ( C | C split )=1 since the set of two sets of words can only be combined in a single way. We propose merge proposal as combining the two topics k 1 and k 2 into a single topic k .Inthiscase q ( C merge | C )=1 , however, to compute the reverse proposal probability q ( C | C merge ) we need to create a dummy split proposal and compute q ( C | C merge )= q ( C dummysplit | C merge ) following the previously described split procedure.

Our split-merge procedure runs after each Gibbs iteration and at each run of aplit-merge procedure we either sel ect to perform a split or merge. Till now we have not discussed whether a split or a merge proposal is to be evaluated. The simplest way to determine that by way of sampling two random words from the document corpus and then depending on whether they belong to the same topic or not we evaluate a split or me rge proposal respectively. Whilst this scheme works fine it is understood that with the increasing number of topics we may encounter more merge proposal being evaluated than split proposals. To circumvent that we propose sampling from a binary random variable with equal probability of selecting a merge or split proposal at each run. When a split proposal has to be created we first se lect a topic at random and then proceed with splitting that topic, similarly when a merge proposal has to be created we select two topics at random and th en proceed with the merging. From our experience this provides faster co nvergence than the naive method. We evaluate our proposed split-merge algorithm for HDP topic models for both synthetic and real world data. In all experiments, we run the normal conditional Gibbs sampler and the proposed split-merge augmented Gibbs sampler for the HDP model, with identical initialization of state space and variables. The nor-mal Gibbs sampler visits each document and all words within it sequentially, assigning each to one to an existing topic or creating a new one based on the predictive likelihood of the word. The split-merge augmented Gibbs sampler runs a Gibbs iteration followed by the split-merge procedure. A split or a merge is proposed based on user-defined selection probability (a simple scheme is to have equal probability of acceptance). Depending on whether a split or merge has been selected, we pick two words randomly from a single topic or from two different topics for split and merge resp ectively. We then propose the split or the merge and accept them based on its acceptance probability. 4.1 Synthetic Data We use synthetic data to demonstrate the performance of our proposed split-merge augmented sampler in comparison to the simple conditional Gibbs sam-pler. We generate 10 topics from a vocabul ary size of 10. The topics are created such that the first topic uses all the words with equal probability, and the rest use lesser number of words, with the last topic using only a single word, as shown in the Fig 2a. Fig 2b shows the extracted four groups. The topic mixture for each group has been generated as a random simplex.

Both the Gibbs sampler and the split-merge augmented Gibbs samplers are run for 1000 iterations and the posterior for the cluster number is shown in the Fig 3b and Fig 3a respectively. Whilst the naive conditional sampler fails to recover exact topics even after 1000 iterations, the split-merge augmented Gibbs Sampler is able to find the correct number of topics within the first 25 Algorithm 1. Split-merge augmented Gibbs sampler for HDP iterations. This is a significant speed up. The reason the naive Gibbs sampler fails to separate the topic is because they are not easily separable, however, our algorithm is able to split topics that are hard to separate. Fig 3a shows the split-merge acceptance ratio after each iteration. As expected the ratio falls with increasing number of samples, once all 1 0 topics have been recovered correctly. The confusion matrix for the topics as recovered by the two sampling algorithms is shown in Fig 4. Since the first few topics have a higher overlap, they are hard to separate.Thus it is nor surprising that the naive Gibbs sampling fails to separate them, however, our algorithm, with its capability to explore state-space in an efficient way, is able to separate the topics. 4.2 Document Corpus We used NIPS abstract data and 20 News Group data to study the convergence of our proposed method. NIPS0-12 data is a collection of abstracts published in the NIPS conference from the year 1988-1999. We select 1392 abstracts consisting of 263K words. The Dirichlet prior is set at Dir (0 . 5) . Both the Gibbs sampler and our sampler was initialized with the same initial topic distribution. We used random 80% of the data for topic modelling and the rest 20% data for perplexity computation. We run them for the same time and plot the the perplexity at each iteration in Fig 5a.
The 20 News Group data contains 16242 documents with vocabulary size of 100. The Dirichlet parameter is set at 0.05. Similar to above setting, we learn our model with a random set of 80% of documents and the remaining 20% are used for perplexity computation. Both the Gibbs and our algorithm are run with the same initialization. Perplexity at each iteration is reported in Fig 5b. Superior perplexity is observed, although the algorithms ran for the same time. In this paper we proposed a novel split-merge algorithm for HDP based on the direct conditional assignement of wor ds-to-topics. The incremental Gibbs sampler can often be slow to mix and may often fail to provide a good posterior estimate in a limited time. The split-merge sampler with its ability to make a bigger move across the state-space mixes faster and often lead to very good posterior estimates. We experimente donbothsyntheticandrealworlddata and demonstrate the convergence speedup of the proposed combined Gibbs and split-merge sampler over the plain Gibbs sampling method.

