 Most up-to-date well-behaved t opic-based summari zation systems are built upon the extractive framework. They score the sentences based on the associated features by manually assigning or experimentally tuning the weights of the features. In this paper, we discuss how to develop learni ng strategies in order to obtain the optimal feature weights automatically, which can be used for assigning a sound score to a sentence characterized with a set of features. The two fundamental issues are about training data and learning models. To save the co stly manual annotation time and effort, we construct the training data by labeling the sentence with a  X  X rue X  score calculated according to human summaries. The Support Vector Regression (SVR) m odel is then used to learn how to relate the  X  X rue X  score of the sentence to its features. Once the relations have been mathematically modeled, SVR is able to predict the  X  X stimated X  score for any given sentence. The evaluations by ROUGE-2 criterion on DUC 2006 and DUC 2005 document sets demonstrate the competitiveness and the adaptability of the pr oposed approaches. I.7.5 [ Document and Text Processing ]: Document Capture  X  document analysis.
 Algorithms, Performance, De sign, Experimentation Document Summarization, Support Vector Regression Currently, most successful summarization systems are implemented by extracting the most salient sentences from the given documents into the summary. For these systems, sentence scoring that determines which sentences are more important than others is a key process fo r ranking and selecting sentences. Typically, the scoring methods calculate the combinational effects of various features which are designed to characterize the different aspects of the sentences and/or their relevance to the topics. Certainly, the selection of the appropriate features highly influences system performance. That is why many high performance systems have devoted much effort to exploring effective features. However, the combination of the features should also be an equally importa nt issue. Yet so far not much attention has been paid to it. Normally, the features are simply combined by a linear function in which the weights are assigned manually or tuned experimentally. There are several shortcomings with these methods. (1) The pe rformance of manually assigned weights is not predictable. (2) When the feature set becomes large, the complexity of experime ntally tuning weights grows exponentially. Our objective in this paper is to explore how the optimal weights can be obtaine d automatically by developing learning strategies. This involve s two fundamental issues which must be addressed by any learni ng based approach, i.e. learning models and training data. In the past, machine learning approaches have been successfully applied in many natural langua ge processing, information extraction and question answering ta sks and so on. However, they have not been well acknowledged in extractive summarization. The bottleneck is the lack of appropriate and sufficient training data which is necessary for trai ning models. While a few learning based systems attempt to solve this problem by utilizing the existing key sentence sets or the document classification information existing on the WWW, most systems are still in the stage of using human assigned or tuned weights. In this paper, we apply a machine learning approach to topic-based summarization by regard ing sentence scoring as a regression problem. The regressi on function is learned from the Support Vector Regression (SVR) m odel, which is the regression type of Support Vector Machin e (SVM) and is capable of building state-of-art optimum approximation functions. It provides a way of combining the features automatically and effectively. To save the costly manual annotation time and effort, we construct training data automatically from the document sets where the reference summaries ge nerated by human have been provided. To make use of human summaries, we develop N -gram methods to approximately meas ure the  X  X rue X  sentence scores. The SVR models then learn how to relate the  X  X rue X  scores of the sentences to the corresponding feat ures. Once the relations have been mathematically modeled, they are capable of predicting the  X  X stimated X  score for any given sentence. In experiment the performance of the N -gram methods is compared to show how to find the best method for tr aining data construction. The remainder of the paper is organized as follows. Section 2 briefly introduces the related work . Section 3 details the training data construction methods and SVR learning models. Section 4 then presents experiments and ev aluations as well as discussions. Section 5 finally concludes the paper. The most straightforward way to apply machine learning approaches in text summarization is to regard the sentence extraction task as a binary classification problem. Kupiec et al [11] developed a trainable summarizati on system which adopts various features and uses a Bayesian classifier to learn tuning the feature weights according to a set of documents with the corresponding extracts, and the system performed better than any other system using only a single feature. Fr om a set of documents where sentences have been annotated manually, Hirao et al [18] trained a SVM model to learn how to extract the important sentences from the given documents. The mode l outperformed other learning based models such as those using decision-tree or boosting methods in the Japanese Text Summarization Challenge (TSC). Later, Zhou and Hovy [5] introduced a HMM-based model to estimate the extract desirability of an English sentence and trained the parameters on data coll ected from YAHOO. The resultant system was comparable to the best system in DUC 2001. Following the same idea, Zhao, Wu and Huang [13] applied the Conditional Maximum Entropy m odel in DUC 2005 topic-based summarization task. Unfortunate ly, only moderate performance was achieved. Since classificati on based models require training corpus with sufficient size and high quality, they are largely are not widely acknowledged. Recently, statistical analysis has been concerned with examining human generated summaries. Seve ral sentence scoring methods based on human summaries and simp le statistic measures have yielded powerful summarization sy stems. For example, Nenkova and Vanderwende [1] examined the impact of word frequency on summarization by extensively st udying the relations of term frequency and human practice in su mmarization. It is remarkable that the system simply based on frequency features could perform unexpectedly well in MSE 2005. Conroy and Schlesinger [10] also defined  X  X racle X  score which was calculated from the probability distribution of Uni-grams in human summaries. By extracting sentences depending on the  X  X racle X  scores, they built a system which performed even be tter than some human generated summaries in DUC 2006. We believe that the idea of comparing sentences with human summaries should yield a good scoring method for ranking and selecting se ntences. It should be able to give sentences the credible scores and provide a good mean to automatically generate training da ta for regression models, which are more similar to the sentence scoring task than classification models. The task of DUC topic-based summarization requires creating from a set of relevant docum ents (most set contains 25 documents) a brief, well-organized and fluent summary to the information seeking need which is indicated in the topic description. Here is an example of the topic description. 
For each topic, four human summarizers are asked to provide a 250-word summary of the topic from the 25 related documents for automatic evaluation. NIST assessors developed a to tal of 50 DUC topics both in DUC2005 data set and DUC2006 data se t. The topic structures of the two data sets are almost the same except that the source and number of documents are somewhat different. Each DUC2005 topic includes 25-50 related documents selected from the Los Angeles Times and Financial Times of London , while each DUC2006 topic is composed of exactly 25 documents from Associated Press , New York Times and Xinhua Newswire. Sentences are scored according to th e features. Therefore, features play an important role in our work. We design a set of features containing three topic dependent features and four topic independent features to characterize the sentences for topic-based summarization task. The design criterion we follow is: we try to capture the important informati on that a sentence conveys in a more direct and explicit way. The feature values are calculated as follows. (1) Word Matching Feature (2) Semantics Matching Feature (3) Name Entity Matching Feature (4) Document Centroid Feature (5) Named Entity Number Feature (6) Stop Word Penalty Feature (7) Sentence Position Feature To construct training data, we propose several N -gram methods to assign  X  X rue X  sentence scores with reference to human summaries. The main hypothesis behind the us e of them is: if the human summaries are the excellent sum-ups of the documents which contain abundant information, th e sentences in the documents which are more similar to thos e in human summaries should be more likely to be the good sum-ups as well, and thus they should stand a good chance to be assigned higher scores by the scoring methods. set } , , { 1 m H H H L = ( H i is the human summary generated by score ) | ( H s score . Basically, the scoring methods compute the N -gram (more specific, Uni-gram or Bi-gram) probabilities of s to be recognized as a summary sentence given human summaries. The probability of an N -gram t under a single human summary 
H can be calculated by where ) ( t freq is the frequency of t and | | summaries, we propose two strategies, namely Maximum and Average strategies. Maximum emphasizes on the diversity of different summaries. It selects the largest probability under single human summaries, In contrast, Average treats all human summaries as a whole and computes probability average, probabilities of all the N -grams it contains, indicated by s t Considering the fact that all human summaries are almost of the same length, the equation of computing probability of N -gram t under a single human summary H i can be simplified to Finally, we have two alternativ e sentence scoring methods based on N -gram frequencies, i.e. and In addition to frequency, binary N -gram appearance judgment can also be applied in sentence score calculation. Based on N -gram appearance, the probability of an N -gram t under single human summary is Accordingly, sentence scoring methods are revised as and Though the methods introduced above are quite simple, they are considerably effective in picking up good summary sentences. You will see in Experiment 1 described in Section 4.2, the N -gram-based methods that rely on human summaries can produce the extractive summaries that are even better than the human summaries. However, human summa ries are normally unavailable when document sentences are scored for inclusion in a machine generated summary. Therefore, the N -gram methods cannot be applied to the practical summarization task directly. One has to seek for alternative ways to appr oximate sentence scoring process. Our solution is to build a regr ession based appr oximation function select to characterize any given sentence s . The output from it will be the  X  X stimated X  score ) (  X  s re o sc of s . Though the N -gram methods cannot be applied directly, they can be used to construct the training data which is then us ed to learn the core component of our solution, i.e. the regression function. Under feature-based summariza tion framework, normally the scoring function needs to combine the impacts of the features. A common way is to use the linear combination of the features by tuning the weights of the features manually or experimentally. A problem of such method is that when the number of the features gets larger, the complexity of assigning weights grows exponentially. Some attempts have been made on using classification models to tune th e weights automatically to avoid the exponential complexity. However, scoring function is a continuous real-value function wh ile classification models are usually adopted to solve discrete problems, thus it is imprecise to use classification models on the sentence scoring task. In this section, we explore regression model for sentence scoring such that credible and controllable solutions can be expected. Models are trained from the document sets D where the human summaries H are given. Each s in D associates with a score sentence X  X  score and its feature set together. Thus the task of predicting the score of the sentence s in another document sets D ' given its F ( s ) is just a regression problem, to generate the on } | )) ( ), | ( {( D s s F H s score  X  . The linear v -SVR model [15] is used to learn the regression function. It chooses the optimum function 0 0 0 ) ( b x w x f +  X  = minimizing the structure risk function the factors. Comparing to traditional regression models, SVR is more generative and robust by introducing a normalization factor 2 || || Once the regression function 0 f is learned, we define Normally, a summary is limited in length to a maximum number of words. With this considerati on, the score should be normalized by the sentence length. The score function is then refined as where | | s is the number of the words in s . Under the above sentence scoring approach, if the terms of two sentences are very similar, th e sentences may probably have approximate feature values, therefore, they may also probably have approximate scores. Thus the extracted summary may include high score sentences which are very similar, this will cause redundant information in summary. To solve this problem, a maximum marginal relevance (MMR) approach is applied during the sentence selection process. First all the sentences are ordered by score from highest to lowest, and then the summary sentences are selected iteratively, each time the current candidate sentence is compared to the sentences already in the summary. If the sentence is not too similar to any sentence already in the summary (the similarity value of the two sentences is lower than a given threshold), the sentence is then selected to the summary. The iteration is repeated until the length of the summary reach the upper limit. In this paper, we use the cosine metric as the similarity function, and the threshold is set to 0.6. We set up our preliminary e xperiments on DUC 2006 and then further test on DUC 2005 document sets. All documents are pre-processed by removing stop wo rds and conducting stemming. Named entities including person and organization names, locations and time are automatically tagged by GATE According to the task definition system generated summaries are strictly limited to 250 English words in length. After sentence scoring, we select the highest sc ored sentences from the original documents into the summary until the word (actually the sentence) limitation is reached. Considering the focus of this study, no post-processing such as sentence compression or information fusion is carried out. We will present the re sults of the eight combinations with consideration of N -grams (Uni-gram or Bi-gram), probability calculations (frequency or appear ance) and scoring strategies (Maximum or Average). In DUC2005 and DUC2006, summaries are evaluated by several manual or automatic evaluation metrics [8][9]. In this paper, we use two of the DUC evaluation criteria, ROUGE-2 and ROUGE-SU4, to compare our systems built upon the proposed scoring and learning methods with human summarizers and several top performing DUC systems. ROUGE (Recall Oriented Understudy summarization evalua tion method based upon n -gram comparison. It is publicly available from http://gate.ac.uk/. In DUC, NIST assessors devel oped several model summaries manually for each DUC topic. Using model human summaries as the golden standard, ROUGE eval uates summaries submitted by comparing them with the model summaries. For example, ROUGE-2 evaluates a system su mmary by matching its bigrams against the model summaries: where S is a summary to be evaluated, ) ,..., 1 ( h j H model human summaries, i t is a bigram in S. ) | ( j i the number of tim es the bigram i t occurred in the j -th model human summary j H and ) |S,H Count(t j i is the number of times i t occurred both in summary S and j H . ROUGE-SU4 is very similar to R OUGE-2, the difference is that it matches unigrams and skip-bigrams of a summary against model summaries instead of bigrams. A skip-bigram is a pair of words in their sentence order, allowing for gaps within a limited size. Though ROUGE is just based on simple n -gram matching framework, it has been working well in DUC. In DUC2005, ROUGE-2 had a Spearman correlation of 0.95 and a Pearson correlation of 0.97 compared with human evaluation. of N -gram methods in assigning the  X  X rue X  sentence score by using the score directly for selecting sentences. Table 1 shows the average ROUGE-2 value and the 95% confidential interval (CI) over 50 document sets for the systems based upon the N -gram methods (in grey shade) and the best summar y created by human. Based on this evaluation, th e systems developed with N -gram methods even achieve much better performance than human summarizer, showing that the score assigned by N -gram methods is reliable. Most important, it de monstrates that the documents themselves contain the senten ces that are good enough to produce a summary even better than human summaries when evaluated by ROUGE-2, which shows the potential of extractive summarization. The N -gram methods are designed pa rticularly for constructing training data for SVR model l earning. Now, we conduct the second set of experiments to ex amine how well the training data constructed can be used for learning regression functions. In this set of experiments, 50 document sets are divided into training data composed of 5 sets and test da ta composed of 45 sets. Cross-validation is applied to leverage th e results. In this paper, we use LIBSVM [2] to implement the v -SVR model and set the parameters of LIBSVM as defau lt values. Table 2 presents the results of the SVR-based systems (in grey shade) together with the 8 top performing systems at DUC 2006 (labeled as 24, ... 2), one worst human summary (labeled as A), and a baseline system which uses the linear function to combine the features. The weights of the features in the baseline system are set manually. The average ROUGE-1, ROUGE-2 and ROUGE-SU4 value and corresponding 95% confidential intervals of all systems are all proposed. The SVR-based systems perform comparably to the top performed systems at DUC 2006. All of them outperform the baseline system. These results clearly show the advantage benefited from applying SVR in combining features and the rationality of generating training data using the N -gram methods based on human summaries. Note that this time  X  X i+Appr+Max X  falls off significantly but  X  X  ni+Freq+Max X  climbs up. To further examine the capability of the SVR-based score approximation methods, we extend E xperiment 2. The models are trained again on 5 DUC 2006 document sets, while the evaluation runs against the 50 document sets of DUC 2005. The results are presented in Table 3. H represents the result when evaluating the worst human summaries The ROUGE-2 values are reported. officially at DUC 2005 [8], only the average scores are listed. It is quite encouraging that th e best SVR-based system performs better than all the systems at DUC 2005. Even for the worst SVR-based system, it can be ranked in the third place. The results verify the reliability and the adaptability of the SVR-based learning approaches to a certain extent, though a large scale of experiments are still needed to further confirm this conclusion. Table 4: Results of manually assigned weights in DUC 2005 Human Assigned Weight 1 0.0631 (0.0594, 0.0668) Human Assigned Weight 2 0.0572 (0.0537, 0.0606) Human Assigned Weight 3 0.0645 (0.0606, 0.0683) Human Assigned Weight 4 0.0657 (0.0623, 0.0691) Human Assigned Weight 5 0.0620 (0.0586, 0.0651) 
Average of 5 systems 0.0625 The weights of the baseline system in Experiment 3 are assigned manually. One may argue that these weights may happen to produce the worst ROUGE-2 scores. To clarify this concern, we conduct Experiment 4 with five di fferent sets of weights assigned by human, and present the results in Table 4. Human assigned weights 1 represents the one used as the baseline system in Experiment 3, which is already above the average in this experiment. As we know, how the feature set is formed directly influences the performance of SVR-based systems. In this set of experiments, we fix on  X  X ni+ Freq+ Max X , which is the best scoring method in Experiment 3. The seven features introduced in Section 3.2 are selected and combined gradually to form nine different feature sets for learning functions. The evaluation is again carried out on DUC 2005 document sets. Table 5 illustrates how the performance is improved when more features are included. When more appropriate features are involved, the SVR models are capable of tuning the weights of the incremented feature sets such that the optimal combinations c ould always be achieved. Thus a more accurate score approximation function can be obtained. The result proves that SVR is effective in searching for optimal weights. Comparing Uni-gram and Bi-gram methods, we find that the Bi-grams spread sparser than Uni-grams in the document sets. The number of sentences which receives a zero score in Bi-gram-based methods (which means the Bi-grams of these sentences never appear in human summaries) is much larger than the number in Uni-gram-based met hods. It is about 75% vs. 20%. Thus the data sparsity problem is more serious in Bi-gram methods than in Uni-gram methods . It unavoidably influences the performance of machine learning approaches. Comparing frequency-based and appearance-based methods, we can see that frequency-based me thods perform better even though not very significantly. There may be two reasons to explain it. First, frequency-based methods maintain more information of the original documents. Second, most features designed are also based upon frequency calculation. This paper proposes the methods for constructing training data based on human summaries and tr aining sentence scoring models Summaries extracted based on thes e methods can perform as well as human-generated abstracts when evaluated by ROUGE-2 on DUC 2005 and 2006 document sets. More important, compared to the other systems in DUC competitions, our SVR-based system can achieve very good performances with very simple work on sentence compression and redundant removal. The work described in this pape r was partially supported by Hong Kong RGC Project (No. PolyU5211/05E) and partially supported by china NSFC Project (No. 60603093) and IBM-PKU Joint Research Project. [1] Ani Nenkova and Lucy Vanderwende. The Impact of [2] Chih-Chung Chang and Chih-Jen Lin. LIBSVM: A Library [3] Chin-Yew Lin and Eduard Hovy. Manual and Automatic [4] Christiane Fellbaum, editer. WordNet: An Electronic Lexical [5] Deepak Ravichandran, Eduard Hovy. Learning Surface Text [6] Dragomir R. Radev, Jahna Otterbacher, Hong Qi, Daniel [7] Hamish Cunningham, Diana Maynard, Kalina Bontcheva, [8] Hoa Trang Dang. Overview of DUC 2005. Document [9] Hoa Trang Dang. Overview of DUC 2006 . Document [10] John M.Conroy, Judith D.Schles inger, Dianne P.O X  X eary. [11] Julian M. Kupiec, Jan Pedersen, and Francine Chen. A [12] Liang Zhou and Eduard Hovy. A Web-trained Extraction [13] Lin Zhao, Lide Wu, Xuanjing Huang. Fudan University at [14] Satanjeev Banerjee, Ted Pedersen. An Adapted Lesk [15] Sch X lkopf, B., Bartlett, P. L., Smola, A., &amp; Williamson, [16] Seeger Fisher, Brian Roark. Query-Focused Summarization [17] Steve R. Gunn. Support Vector Machines for Classification [18] Tsutomu Hirao, Hideki Isozaki. Extracting Important [19] Vladimir Vapnik. Statistical Learning Theory . John Wiley 
