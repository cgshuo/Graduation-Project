 Kam-Fai Wong  X  Yunqing Xia Abstract Real-time communication platforms such as ICQ, MSN and online chat rooms are getting more popular than ever on the Internet. There are, however, real risks where criminals and terrorists can perpetrate illegal and criminal abuses. This highlights the security significance of accurate detection and translation of the chat language to its stand language counterpart. The language used on these platforms differs significantly from the standard language. This language, referred to as chat language, is comparatively informal, anomalous and dynamic. Such features render conventional language resources such as dictionaries, and processing tools such as parsers ineffective. In this paper, we present the NIL corpus, a chat language text collection annotated to facilitate training and testing of chat language processing algorithms. We analyse the NIL corpus to study the linguistic characteristics and contextual behaviour of a chat language. First we observe that majority of the chat terms, i.e. informal words in a chat text, is formed by phonetic mapping. We then propose the eXtended Source Channel Model (XSCM) for the normalization of the chat language, which is a process to convert messages expressed in a chat language to its standard language counterpart. Experimental results indicate that the perfor-mance of XSCM in terms of chat term recognition and normalization accuracy is superior to its Source Channel Model (SCM) counterparts, and is also more con-sistent over time.
 Keywords Chinese chat language  X  Phonetic mapping  X  Chat language Modelling  X  Chat term normalization  X  Natural language processing 1 Introduction The Internet supports online chatting via different real-time communication platforms such as ICQ, MSN, online chat rooms, BBS, email, blogs, etc. The language used on these platforms, referred to as a chat language, is gaining global popularity, especially within the Chinese communities worldwide. Investigation reveals that chat language texts appear frequently in chat logs of online education (Heard-White et al. 2004 ), customer relationship management (CRM) (Gianforte 2003 ), and so on. Online chatting now become has now become an important communication medium making daily life easier. But, at the same time, many Internet chat rooms and BBS systems are constantly abused by perpetrators of terrorism, pornography and crime (McCullagh 2004 ). In general, whether it is to provide better services in online education or CRM, or to monitor and control the Web against terrorism, there is an urgent social and business need to understand the communications over the Internet. This highlights the significance of our project, the normalization of the chat language in the Chinese domain. Normalization implies translation of an expression in the chat language to its standard language counterpart.

We observe that, compared with the standard language, a chat language is anomalous and dynamic in nature (see Sect. 2). Words specific to the chat language, referred to as chat terms, are different from the standard language in two obvious ways: either they fall outside the standard language X  X  vocabulary; or they represent different meanings from their standard language counterparts. We find that on average one chat term is found in every 2.74 sentences in a BBS text collection. Chat terms are not covered by conventional linguistic tools like dictionaries and thesauruses rendering existing natural language processing techniques ineffective. The dynamic nature of the chat language is attributed to the regular changes of chat terms and its grammatical structure. Since the chat language is an informal language (i.e. a dialect), users are free to make up their own words and phrase structures, which are mostly short; as such, the users could make better use of the communication bandwidth. Also, influenced by the contemporary social environ-ment such as catchy phrases on television, chat terms and phrases are often creative and fashionable. For example, many popular chat terms used a year ago have been discarded and many new chat terms are created in the present year.

The anomalous nature of Chinese chat language was investigated in an earlier work (Xia et al. 2005 ). Pattern matching and support vector machines (SVM) methods were proposed to recognize ambiguous chat terms from BBS chat text. Experiments show that chat term recognition rate measured in f-1 reached 87.1%. It was, however, found that quality of both methods dropped significantly when the training set became older. The dynamic nature of Chinese chat language was investigated in another study (Xia and Wong 2006 ). An error-driven approach was proposed to detect chat terms in Chinese chat texts. It made use of linguistic information embedded in both a standard Chinese corpus and the Network Informal Language (NIL) 1.0 corpus (Xia et al. 2006a ). The standard Chinese corpus provided negative text samples and NIL 1.0 corpus positive ones. The approach worked out the confidence and entropy values of the input BBS text and used a threshold value to identify the embedded chat terms. Compare with existing methods, the proposed method performed consistently over a time-varying test set. However, the issue of chat term normalization was not addressed in that study (Xia et al. 2006a ). Conventionally, a check in the dictionary is the simplest approach to handle term translation. But this is inapplicable to chat term normalization due to the serious Out-Of-Vocabulary (OOV) problem caused by the regularly changing nature of the chat language. The targets of this project are (1) to compile a sizeable chat language corpus; (2) to analyse the corpus to study the characteristics of the chat language; and (3) to design a practical chat term normalization algorithm based on the analysis.

The rest of this paper is organized as follows. In Sect. 2, a study of the linguistic characteristics and contextual behaviour of the chat language is presented. We present a character mapping-based source channel model method for chat term normalization in Sect. 3 and show its limitations and ineffective points. In Sect. 4, we introduce the concept of phonetic mapping and outline the technical details of the phonetic mapping model. Section 5 presents an extended source channel model, which incorporates the phonetic mapping models, for chat term normalization. Experimental results and error analysis are presented in Sect. 6. Finally, Sect. 7 draws preliminary conclusions. 2 Chat language feature analysis 2.1 A Chinese chat language corpus The NIL corpus is a collection of Chinese chat language sentences compiled to facilitate training and testing of the chat language knowledge engineering tools and processing methods. The NIL corpus is constructed by manual annotation.
Sources of on-line chat language texts were not easy to find. Obtaining online chat logs maintained by ICQ, MSN and online chat rooms was complicated. This was mainly due to data privacy restriction. We therefore resolved to sources that are publicly accessible such as BBS discussion postings. We found that BBS text within  X   X   X   X  (meaning: free chat zone; Chinese pinyin: da4 zui3 qu1 ) X  discussion zone in YESKY BBS system ( http://www.bbs.yesky.com/bbs/ ) closely resembled the characteristics of Chinese chat language and contained a vast amount of chat terms. Thus, BBS postings in that zone were finally used as the text source to produce the NIL corpus.

An early version of the corpus, i.e. NIL 1.0, covered chat language text created between December 2004 and February 2005. In the current version, NIL 2.0, we included chat language text created from March 2005 to February 2006. 30,392 chat sentences selected from 120,429 BBS postings were annotated in the NIL 2.0 corpus. 1 Within it, 861 chat terms occur 43,963 times.
 We begin our corpus analysis by investigating the linguistic characteristics of the Chinese chat language, including anomalous types, ambiguous status, morpholog-ical forms and phonetic behaviour. Problems caused by these linguistic character-istics are also studied. This is followed by a study on the contextual behaviour of the chat language. The goal is to understand its dynamism and the associated problems, which in turn will provide useful insights for us to design the ultimate chat term normalization algorithm. 2.2 Linguistic characteristics 2.2.1 Anomalous types A chat language is linguistically anomalous to its standard language. There are mainly two types of anomaly. Firstly, some chat terms do not exist in conventional dictionaries. They are referred to as anomalous entries. For example,  X   X   X  (here; jie4 li3 ) X  is not a standard word in any contemporary Chinese dictionary while it is often used to replace  X   X   X  (here; zhe4 li3 ) X  in Chinese chat language. Secondly, while some chat terms can be found in conventional dictionaries as standard entries, their meanings are unknown to the dictionaries, which are referred to as anomalous senses. For example, in chat text  X   X  (even; ou3 ) X  is often used to replace  X   X  (me, wo3 ) X . But  X   X  (even; ou3 ) X  merely means  X  X ven X  in a conventional dictionary. This substitution takes place as the two words sound similar in Chinese. Distribution of chat terms in the two anomalous types is presented in Table 1 .

Table 1 reveals that 46.82% chat terms are anomalous entries to conventional dictionary and the remaining 53.18% chat terms uses anomalous senses. To handle chat terms in both cases, a chat language dictionary was suggested to collect all chat terms and their senses. This approach is, however, impractical as unknown chat terms are created and relinquished too frequently for any conventional dictionary update mechanism. Thus, we conclude that knowledge-based methods are ineffective for chat language processing.
 2.2.2 Ambiguous usages In chat text, chat terms and standard words are inter-mixed. Thus, recognition of chat terms is not straightforward, in particular the case in anomalous sense (see Table 1 ). Further, like standard language terms, many chat terms have multiple senses and the actual meaning is context dependent. Table 2 shows the statistics of sense distribution in the NIL corpus.

Table 2 shows that 66.99% occurrences of the 861 chat terms are ambiguously used (i.e. more than one sense). Disambiguation complexity increases with the number of sense of a word. Words with two, three and four meanings are most significant. They occupy 51.40%, 6.95% and 5.20%, respectively. In the ambiguous chat terms,  X   X  (even; ou3 ) X  occurs most (i.e. 8,735 times) which is used to replace  X   X  (I; wo3 ) X ;  X  X J X  appears second most (i.e. 5,405 times) which represents  X   X  X  (older sister; jie3 jie3 ) X . But surprisingly, 325 chat terms appear only once in the NIL corpus. This implies a serious data sparseness problem for statistical chat language processing (see also Sect. 2.3).

We grouped the ambiguous chat terms into the two aforementioned anomalous types in Table 3 . Chat terms in the anomalous sense group are highly ambiguous as they present at least two senses, i.e. one in the chat language and the other in the standard language contexts. This group also reflects that chat terms and standard language words are well mixed in the NIL corpus and recognizing chat terms from the mixed text is an essential step in the normalization process. Chat terms in the anomalous entry group represent the chat language specific words. 29.38% of them are ambiguous (i.e., representing more than one sense). This accounts for a significant portion and should not be ignored in any language processing tools. 2.2.3 Morphological forms Chat terms are composed in various forms. The majority of chat terms is formed by a string of Chinese characters. Many others are composed by strings of letters, numbers and a mixture of Chinese and alphanumeric characters. Table 4 presents the chat term composition distribution in the NIL corpus.
 In conventional Chinese text, very few letters are used; but this is not the case in Chinese chat text. This is due mainly to the fact that letters can help reduce the burden of inputting Chinese characters to the computer. In the NIL corpus, 35.29% of the chat terms (i.e. 197 unique entries) consist of letters. It is noteworthy that in this category, 173 chat terms use Chinese pinyin abbreviations or initials, e.g.  X  X J X  representing  X   X  X   X  and the rest uses English pronunciation or initials, e.g. ASAP.
It is common to find numbers in the Chinese standard language. But in the chat language, numeric characters do not always represent numbers. For example,  X 7 (seven; qi1 ) X  is used in the chat sentence  X   X   X  7  X   X  (I like to eat beef; wo3 ai4 qi1 niu3 rou4 ) X  to represent  X   X  (eat; chi1 ) X .

The mixed form makes the chat language most different from the standard language. Chat terms in this type combine Chinese characters, letters and/or numeric characters. The mixed form would present problems to conventional morphological analysis tools. Conventional Chinese word segmentation tools separate numbers and letters from Chinese characters. For example,  X 8  X  (not bad; ba1 cuo4 ) X  would be split to  X 8 (eight; ba1 ) X  and  X   X  (wrong; cuo4 ) X  by ICTCLAS (Zhang et al 2003 ), a popular word segmentation tool. But in fact it should be treated as one  X  X ord X  in the chat language representing  X   X   X  (not bad; bu3 cuo4 ) X .
There are 0.22% chat terms with other forms. They are mainly emotions, which make uses of combination of punctuations, numbers and letters to represent different emotions, e.g.  X :-) X  represents a happy face and  X :-( X  X  sad face. 2.2.4 Phonetic behaviour Our observation on phonetic behaviour of chat terms indicates that most chat terms are created using phonetic mappings instead of character mappings. In other words, most chat terms and their standard language counterparts are similar in phonetic transcription. For example, ignoring their tones, chat term  X   X  (drop; di1 ) X  and  X   X  (ground; di4 ) X  share the same Chinese pinyin, i.e. di . In addition, formation of many Chinese chat terms is based on Chinese dialects rather than standard Chinese, i.e. Mandarin. For example, the chat term  X   X  (powder; fen3 ) X  and  X   X  (very; hen3 ) X  are phonetically equal in a southern Chinese dialect. Table 5 presents distribution of chat terms in terms of phonetic behaviour.
 Table 5 shows that 97.28% chat terms are formed based on phonetic mapping. This observation provides very important clues to chat language modelling and normalization. Intuitively, one would consider using a character mapping method to translate chat terms to their counterparts. However, this method would seriously suffer from data sparseness problem because a large chat language corpus is not available. What is even worse is that chat terms are created and relinquished quickly rendering available character mappings invalid to model them.

In contrast, the phonetic mapping method for chat term normalization is more flexible. Firstly, phonetic mappings can be produced beforehand using standard language corpus. This can ensure completeness of the phonetic mapping model. Secondly, the mapping space between chat terms and standard language words is significantly reduced by the phonetic mapping method. Chinese characters are first grouped and then mapped between each other via similar pinyin. Thus, phonetic mapping is actually based on group-to-group mapping. Our text corpus comprises of 5,095 simplified Chinese characters but only 735 pinyin units leading to a significant reduction in the mapping space, i.e. around 7 times. Ambiguity arising from the phonetic grouping is inevitable, yet pinyin similarity and character frequency are found effective parameters for disambiguation in this research. 2.3 Contextual behaviour of chat terms We define contextual behaviour of chat terms as the rates chat terms are created and relinquished. This reflects the dynamism of the chat language. We show in this section that the chat language is dynamic and that leads to serious sparse data problems. 2.3.1 Creation and relinquish rates The chat language is dynamic. New terms are created and old terms are relinquished regularly. We define the creation rate as percentage that new chat terms are created and the relinquish rate the percentage that old chat terms are relinquished in 2 months. Suppose we have two chat term sets, i.e. TS 1 and TS 2 , in 2 month periods T 1 and T 2 respectively where T 2 is later than T 1 and hence, TS 2 is newer than TS 1 . The creation and relinquish rates are defined as follows: We group chat terms in batches of 2 months in the period from December 2004 to February 2006. Then their creation and relinquish rates are calculated using Eqs. 1 and 2 and presented in Tables 6 and 7 , respectively.

Tables 6 and 7 reveal that, within 12 months (i.e. from Dec-04 to Dec-05), 17.28% chat terms are created and 17.82% chat terms relinquished. Such rates are different from the standard language, which changes slightly in more than 5 years.
Some event-related chat terms are convincing evidences for the above observation. It is observed that some chat terms were frequently used for only a short period of time. This happens especially in popular events. For example, the chat term  X   X   X  (corn; yu4 mi3 ) X  appeared during the hottest Chinese TV show  X   X   X   X   X  (Super Girl Voice; chao1 ji2 nv3 sheng1 ) X  in 2006; and it replaced  X   X   X   X  (fans of Yuchun, the Super Girl champion; yu3 mi2 ). Today, this chat term has become obsolete as that TV show was over. In general, since such terms are formed based on phonetic clues, they have been catered for in our research work. 2.3.2 Sparse data This is a classical problem in statistical NLP approaches. It occurs when training data in the specific language domain are insufficient. Now we study the chat term distribution in terms of their occurrences (see Fig. 1 ).

We observe that in the NIL corpus, 540 out of the 861 unique chat terms occur less than five times. This would lead to serious sparse data problem in statistical learning. The problem is made even worse by the dynamic nature of the chat language. As shown in Tables 6 and 7 , 17.28% chat terms are created and 17.82% relinquished in the first year. This would lead to further data sparseness since chat language models trained on chat text collected last year would be outdated and hence, ineffective for processing chat texts this year. 3 Normalization with source channel model In this paper we propose a new chat language modelling technique to address the problems caused by the anomalous and dynamic nature of the chat language. The goal of this research is to design an effective method to recognize chat terms in random chat language text and translate them to their standard language counterparts, i.e. normalization. In this section, the baseline method implemented using the classical source channel model (SCM) is outlined. We study the deficiency of SCM for chat term normalization and propose the extended source channel model (XSCM) in Sect. 5. We also use performance of SCM as the reference in evaluating the performance of XSCM in Sect. 6. 3.1 The source channel model The source channel model (SCM) is a widely used statistical approach in speech recognition and machine translation (Brown et al. 1990 ). Since chat term normalization is very similar to these applications, SCM is deemed most appropriate characters, SCM aims to find the most probable translation character string C={ c i } j= 1,2,..., n where c i  X  X  are the output characters, viz: language model, i.e. p ( C ). p ( T | C ) is actually a character mapping model produced with a chat language corpus. The two models can be estimated with the maximum likelihood method using the character trigram in the NIL corpus. 3.2 The problems Two problems are worth noting in applying SCM in chat term normalization. First, data sparseness problem is serious because size of the timely chat language corpus is too small to provide sufficient character mappings. The NIL corpus contains only 30,292 chat sentences created in 15 months. This is insufficient to train the chat term translation observation model. Second, training effectiveness is poor again due to the dynamic nature of the chat language. Trained on static chat text pieces, the SCM approach would perform poorly in processing future chat text.

Updating the NIL corpus with recent chat text constantly is an ineffective solution to the above problems. It is desirable to find some linguistic information underlying chat terms to help address the data sparseness and dynamic problems. Observations outlined in Sect. 2.2.4 provide evidence that phonetic mappings exist between most chat terms and their standard language counterparts. Thus, we apply such mappings in resolving the two problems. 4 Phonetic mapping models 4.1 Phonetic mapping Phonetic mapping connects two characters via phonetic transcription, i.e. Chinese pinyin in our case. For example, the bracket is phonetic mapping probability between the two characters. Some categorized examples of phonetic mappings are given in Appendix 1.

Technically, phonetic mappings can be constructed between any two characters within any Chinese corpus. In the chat language, as any Chinese characters can be used in a chat term, phonetic mappings are used to translate the chat term to its standard language counterpart. Different from character mappings which are merely extracted from the chat language corpus, phonetic mappings can be obtained from a standard Chinese corpus; and the chat language corpus is then used to refine the phonetic mapping probabilities. 4.2 Phonetic assumption and justifications To make use of phonetic mappings as the fundamental knowledge in chat term normalization, the following phonetic assumption is made.

Phonetic assumption : In the Chinese chat language, chat terms are mainly created using phonetic mappings and the phonetic mappings are stable over time.
To ensure that the assumption holds in our method, two questions must be answered. First, how many percentage of chat terms are created via phonetic mappings? Second, why are phonetic mappings stable and character mappings not in the chat language? The first question has already been answered in phonetic behaviour analysis on the chat language in Sect. 2.2.4. We would like to focus on the second question.

Analysis on creation/relinquish rates in Sect. 2.3 shows that chat terms evolve dynamically. The analysis, however, examined character rather than phonetic behaviour. We conducted another analysis on fifteen chat term sets, i.e. one for each month of the NIL corpus from December 04 to February 06 (see Sect. 2.1) to investigate how phonetic mappings of these chat terms behave over time. We created fifteen chat language phonetic mapping sets, one for each of the fifteen chat term sets and a standard phonetic mapping set using Chinese Gigaword Second Edition (Graf et al. 2005 ). We compared each of the fifteen phonetic mapping sets against the standard set and observed that the standard set consistently covered more than 97% phonetic mappings in each month. Compare with the creation/relinquish rates in Sect. 2.3, we are convinced that the phonetic mappings constructed with the standard Chinese language are more stable over time. 4.3 Formalism Phonetic mapping is modelled by a five-element tuple, i.e. which comprises of an input chat term character t ; the output standard language the mapping probability Pr pm ( t | c ) in which t is mapped to c via the phonetic mapping:
Since phonetic mappings concern mappings between any Chinese character pairs via pinyin and the characters are not necessarily related to chat terms, they could be obtained from a standard language corpus. This results in two advantages: (1) the impact of sparse data problem is reduced as the standard language corpus can provide broader coverage (see Sect. 4.2); and (2) the phonetic mapping model is as stable as the standard language. As such, in chat term normalization, when the phonetic mapping models are used to represent mappings between chat term characters and their standard language counterparts, the dynamic problem can be addressed effectively. In contrast, SCM adopts the character mapping model (see Sect. 3.1). The model connects two Chinese characters directly. It is modelled by a three-element tuple, i.e. which comprises the input chat term character t , the corresponding output standard language character c and the character mapping probability Pr cm ( t | c ) that t is mapped to c via this character mapping. As they must be constructed from the chat language training set, which is significantly smaller than the standard language corpus, it is very likely that the character mapping model suffers more from both the data sparseness and dynamic problems. 4.4 Parameter estimation To construct the phonetic mapping models, we first extract all Chinese characters from a standard Chinese language corpus and use them to form the character mapping models. We then generate phonetic transcriptions of the Chinese characters and calculate the phonetic mapping probability for each character mapping. We exclude those character mappings holding zero probability. Finally, character mappings are converted to phonetic mappings by phonetic transcription and the phonetic mapping probability of each conversion is incorporated. Specifically, this is how the phonetic mapping probabilities are estimated.
The phonetic mapping probability is calculated by combining phonetic similarity and character frequency in the standard language as terms of phonetic transcription. fr slc ( x ) is a function that returns frequency of between characters x 1 and x 2 .
 Phonetic similarity between two Chinese characters is calculated based on Chinese pinyin as and final ( y ) return initial ( shengmu ) and final ( yunmu ) of Chinese pinyin y , which  X  zh  X  is the pinyin initial and  X  e  X  the pinyin final. In cases where either an initial or a final is empty, we use similarity of the existing parts, e.g. we calculate phonetic similarity of  X   X  (scatter; sa3 ) X  and  X   X  (ah, an exclamation; a4 ) X  as follows.

An algorithm to calculate the similarity of initial pairs and final pairs is proposed in (Li et al. 2003 ) based on letter matching. The problem of this algorithm is that it always assigns zero similarity to those pairs containing no common letter. For However, in fact, pronunciations of the two initials are very close to each other in spoken Chinese. For this reason, non-zero similarity values should be assigned to All similarity values have been validated by several native Chinese speakers. Thus the aforementioned algorithm is extended to output a pre-defined similarity value before letter matching. For example, pinyin similarity between  X  chi  X  and  X  qi  X  X s calculated as follows.

At this point, Pr pm  X  c j c  X  is only estimated with standard language corpus. We further propose to tune Pr pm  X  c j c  X  using character frequencies in the NIL corpus. Pr where fr NIL ( x ) returns frequency of character x in the NIL corpus. As some character might not appear in the NIL corpus, we choose to assign a smoothing frequency to each of those zero-frequency characters based on its frequency in the standard language corpus, i.e. Equation 6 is then rewritten as otherwise. 5 The extended source channel model To handle the problems encountered in the method based on source channel models, we propose to extend the source channel model by inserting a phonetic mapping model M ={ m i } i =1,2,..., n into Eq. 3, in which chat term character t i is mapped to channel model (XSCM) is formulated as follows.
Three components are involved in XSCM, i.e. chat term normalization obser-vation model p ( T|M,C ), phonetic mapping model p ( M|C ) and language model p ( C ). The chat term normalization model : We assume that phonetic mappings between Chinese chat terms and their standard language counterparts are independent of each other. Thus chat term normalization probability can be derived as follows. The p ( t i | m i ,c i ) X  X  are estimated using maximum likelihood estimation method with Chinese character trigram model on the NIL corpus.

The phonetic mapping model : We assume that the phonetic mapping model depends merely on the current observation and calculate the phonetic mapping probability as follows. corpus.
The language model : The language model p ( C ) can be estimated using maximum likelihood estimation method with Chinese character trigram model on the standard Chinese language corpus. In our implementation, the Katz Backoff smoothing technique (Katz 1987 ) is used to handle the sparse data problem and Viterbi algorithm (Manning and Schu  X  tze 1999 ) is employed to search for the optimal solution in XSCM. 6 Evaluation 6.1 Data sets 6.1.1 Training sets Two types of training data are used in our experiments. We use the Chinese Gigaword Second Edition (CNGIGA) as the standard Chinese language corpus to construct phonetic mapping models because of its excellent coverage of the standard Simplified Chinese. We use the NIL 2.0 corpus as the chat language corpus (see Sect. 2.1).

To evaluate our method on time-varying training data, five chat language corpora, i.e. CT#1 X  X T#5, are created with NIL corpus (see Table 8 ). To evaluate our method on size-varying training data, the five time-varying training sets are in turn used to produce five size-varying training sets, i.e. CS#1 X  X S#5 (see Table 9 ). The size-varying training sets are created by accumulating the time-varying training sets from recent set (i.e., CT#5) to remote set (i.e., CT#1). This treatment accords to the way that people expand training set for dynamic language in real applications. Basically, a recent text is more similar to a contemporary text than a remote text, thus it is more useful in expanding the training set for a dynamic language. 6.1.2 Test sets We extracted 1,000 chat language sentences posted each month from January 2006 to June 2006 and compiled six time-varying test sets, T#1 X  X #6, in which timestamp of T#1 was the earliest and that of T#6 the newest. Notice that the NIL corpus covered chat sentences in January and February 2006. Since it overlapped with T#1 and T#2, these two test sets were regarded as closed test sets and the others open ones. For evaluation purpose, standard answers were produced manually from the six test sets. 6.2 Evaluation criteria We evaluated two tasks in our experiments, i.e. recognition and normalization. In where x denotes number of correctly recognized chat terms, y number of incorrectly recognized chat terms and z number of unrecognized chat terms.

For normalization, we used accuracy ( a ), which was commonly accepted by machine translation researchers as a standard evaluation criterion. The normal-ization accuracy is defined as percentage of correctly normalized chat terms in all chat terms in the test set. Every output of the normalization methods was compared to the standard answer to produce the corresponding normalization accuracy. 6.3 Experiment I: Time-varying chat language corpora The objective of this experiment is to prove two claims. First, the chat language is dynamic. Second, XSCM is more effective to handle the dynamic problem. In this experiment we used five time-varying chat language corpora, i.e. CT#1 X  CT#5, as the training chat language corpus, respectively and used six time-varying test sets, i.e. T#1 X  X #6, for testing. In each test, SCM and XSCM were trained on one time-varying chat language corpus and tested on the six time-varying test sets. Recognition f-1 measure ( f ) and normalization accuracy ( a ) are presented in Table 10 .

Sets of f-1measure and accuracy curves are showed in Fig. 2 . Figure 3 shows performance gap between SCM and XSCM. These curves reveal three tendencies in the experimental results.
 i. Performance of both methods dropped on the same test sets when they were ii. Performance of both methods dropped on the time-varying test sets under the iii Performance gap between SCM and XSCM became bigger when the test set It should be pointed out that performance of XSCM dropped as the time-varying test sets became newer. This might be considered as counter-proof to our claim that XSCM could achieve high quality chat term normalization consistently. We found that this was due to insufficient training data. The NIL corpus was the only training corpus in this experiment and its coverage of phonetic mapping was limited. Thus, XSCM was in a sense under-trained leading to the performance drop. For this reason, we introduced the standard language corpus, i.e. CNGIGA, in XSCM training and re-ran the experiments. Recognition f-1 measure ( f ) and normalization accuracy ( a ) in the revised experimental are shown in Table 11 .
Compared with Tables 10 , 11 presents the same values for SCM but much better values for XSCM. Two conclusions are drawn. i. CNGIGA improved the performance of XSCM. ii. CNGIGA facilitated XSCM to perform consistently well over all test sets. However, the curves presented in Fig. 6 show that performance gain on both recognition f-1 measure ( f ) and normalization accuracy ( a ) saturates at CS#4. Accuracy gain on CS#5 over CS#4 is very little, i.e. around 0.001. We can thus conclude that size of corpus CS#4, i.e. 24,265, would be enough for XSCM to produce satisfactory performance and increasing training size beyond that number would not yield any noticeable performance gain. 6.4 Experiment II: size-varying chat language corpora Although satisfactory performance was achieved in Experiment I, it was still uncertain whether the performance of XSCM could be further improved by increasing the size of the chat language corpus, i.e. the training corpus.
In this experiment, XSCM was trained on each of the five size-varying chat language corpora, i.e. CS#1 X  X S#5 and CNGIGA and tested on the six test sets T#1 X  X #6. Recognition f-1 measure ( f ) and normalization accuracy ( a ) are presented in Table 12 .

Table 12 shows training size influences the performance of XSCM. XSCM performs best on the largest training chat language corpus, i.e. CS#5, and worst on the smallest, i.e. CS#1. Figure 5 reveals that XSCM favours bigger chat language corpus for training. Thus extending the chat language corpus should be one choice to improve the quality of chat language term normalization.

However, the curves presented in Fig. 6 show that the performance gain on both recognition f-1 measure ( f ) and normalization accuracy ( a ) saturates at CS#4. Accuracy gain on CS#5 over CS#4 is very little, i.e. around 0.001. We can thus conclude that the size of corpus CS#4, i.e. 24,265, would be enough for XSCM to produce satisfactory performance and increasing training size beyond that number would not yield any noticeable performance gain. 6.5 Error analysis Table 12 shows that XSCM achieved 0.88 f-1 measure and 0.88 normalization accuracy in Experiment II. The 0.12 error rate is mainly caused by three types of errors.
 Err. 1: Ambiguous chat terms.
 Example-1:  X   X   X   X  (He was scared away; ta1 ha1 pao3 le1 ) Output:  X   X   X   X  (He ran away; ta1 a4 pao3 le1 ) Answer:  X   X   X   X  (He was scared away; ta1 xia4 pao3 le1 )
In Example-1,  X   X  (ha, an exclamation; ha1 ) X  is a chat term representing  X   X  (scare; xia4 ) X . This error occurs because  X   X   X  can be used to express an exclamation depicting laughing in standard Chinese language. We find  X   X   X  can represent seven senses in the chat language. It is difficult for XSCM to disambiguate such a chat term with multiple senses. For example,  X   X   X  can be normalized to  X   X  (ah, an exclamation; a1 ) X  (i.e., the output of XSCM in this case) because it proceeds a pronoun, i.e.  X   X  (he; ta1 ) X . However, the chat term can be used to represent  X   X   X  when a verb  X   X  (run way; pao3 ) X  follows. Unfortunately,  X   X   X  was found more possible by XSCM to be the normalization counterpart for chat term  X   X   X . In Experiment II, 197 errors of this type were caused by ambiguity. Err. 2: Unrecognized chat terms.
 Example-2:  X   X   X   X   X   X  (I was born in 1982; wo3 suo3 ba1 er4 nian3 sheng1 ) Output:  X   X   X   X   X   X  ( The sentence doesn X  X  make sense in standard Chinese ) Answer:  X   X   X   X   X   X  (I was born in 1982; wo3 shi4 ba1 er4 nian3 sheng1 )
In Example-2, the chat term  X   X  (rope, suo3 ) X  representing  X   X  (was, shi4 ) X  is not recognized by XSCM. This is because phonetic similarity between  X   X   X  and  X   X   X  is too low (i.e., 2.1e-9) to be significant in the phonetic mappings. Eight errors of this type occurred in Experiment II. It is thus revealed that chat terms holding very low phonetic similarity might be mistakenly ignored by XSCM.

In Example-3, XSCM cannot recognize  X   X   X  (vermicelli; fen3 si1 ). This is because the chat term is created using English-Chinese transliteration (Gao et al. 2004 ) instead of phonetic mapping between two Chinese terms. Although transliteration could be considered as another type of phonetic mapping, i.e. cross-lingual phonetic mapping, it is not catered for in our approach. Err. 3: Chat terms created in manners other than phonetic mapping.
 Example-3:  X  X  X   X   X   X  (They are fans; ta1 men2 shi4 fen3 si1 ) Output:  X  X  X   X   X   X  (They are vermicelli; ta1 men2 shi4 fen3 si1 ) Answer:  X   X   X   X   X  (They are fans; ta1 men2 shi4 ai4 hao4 zhe3 )
It is shown in Table 5 that around 2.72% chat terms did not contain any phonetic clue. They include English-Chinese transliteration (Gao et al. 2004 ) (e.g.,  X   X   X   X  represents the English word  X  X ans X ), multiple phonetic mappings (e.g.,  X   X  (watch; biao3 ) X , a short form phonetic representation of  X   X   X  (do not; bu3 yao4 ) X ), emoticons (e.g.,  X :-) X  represents  X  X appy X ) and some idiosyncratic/personal usages (e.g.,  X 9 X  represents monkey). 56 errors of this type occurred in Experiment II. In practice, we used a dictionary to handle these exceptions before we applied the phonetic mapping method. 7 Conclusions We presented a Chinese chat language corpus, namely NIL corpus 2.0, which is the first text collection of this kind. NIL corpus is useful to research in chat language processing. Analysis of the NIL corpus reveals that the chat language is dynamic in nature and anomalous to the standard language rendering conventional NLP resources and tools ineffective. We also observed that most chat terms are similar to some forms of phonetic transcription of their standard language counterparts.
In addition, we offered an introduction to the normalization of chat terms, the process to translate a chat term to its standard language counterpart. Source channel model (SCM) is examined for this purpose, which is found ineffective as its translation model is based on character mapping. We extended SCM by incorporating the phonetic mapping model resulting in the XSCM method. XSCM trained with NIL corpus outperforms SCM under the same training condition in both chat term recognition and chat term normalization. Meanwhile, we demonstrated that by further training the XSCM with a standard Chinese language corpus (i.e. CNGIGA), its performance becomes more stable. However, there are around 12% errors in the existing implementation. They are mainly due to special chat term types. Contextual and semantic analysis techniques can be used to overcome them.

At present, we only focused on chat term normalization. However, full-fledged chat language normalization also involves sentences. Preliminary review shows that, compared with standard language sentences, chat sentences are shorter in length, often ungrammatical, anomalous in word order, and often with ellipsis. These are characteristics of human dialogue and it often involves multiple parties. These will form the major core of our continuous research in the  X  X hinese Chat Language Normalization X .

Furthermore, it is worth noting that today many people are concerned with the impact of network terms on human languages (Cheng 2004 , http://www.tech.163. com/special/w/wlyy.html ). But the contextual behaviour of chat terms has never been studied systematically. In this paper, our research findings on the contextual behaviour of chat terms are presented in details. This helps linguists understand how chat terms are created and relinquished. This in turn will provide foundation for social linguistic research in Network Informal Languages (NIL). Observation on life cycles of new words for each language is an important research issue. Metcalf has been tracking English new words as they arise for 60 years (Metcalf 2002 ). Enlightened by his work, we plan to track life cycles of Chinese chat terms in the future to see how they gradually evolve to become standard words.
 Appendix 1: Some categorized examples of phonetic mappings 1. Chinese to Chinese phonetic mappings 2. Letter to Chinese phonetic mappings 3. Number to Chinese phonetic mappings References
