 In this paper, we study an inherent problem of mining Frequent Itemsets (FIs) : the number of FIs mined is often too large. The large number of FIs not only affects the min-ing performance, but also severely thwarts the application of FI mining. In the literature, Closed FIs (CFIs) and Max-imal FIs (MFIs) are proposed as concise representations of FIs . However, the number of CFIs is still too large in many cases, while MFIs lose information about the frequency of the FIs. To address this problem, we relax the restrictive definition of CFIs and propose the  X  -Tolerance CFIs ( TCFIs) . Mining  X  -TCFIs recursively removes all subsets of a  X  -TCFI that fall within a frequency distance bounded by  X  . We propose two algorithms, CFI2TCFI and MineTCFI , to mine  X  -TCFIs. CFI2TCFI achieves very high accuracy on the estimated frequency of the recovered FIs but is less effi-cient when the number of CFIs is large, since it is based on CFI mining. MineTCFI is significantly faster and consumes less memory than the algorithms of the state-of-the-art con -cise representations of FIs, while the accuracy of MineTCFI is only slightly lower than that of CFI2TCFI.
Frequent Itemset ( FI ) mining [1, 2] is fundamental to many important data mining tasks such as associations [1], correlations [6], sequences [3], episodes [13], emerging p at-terns [8], indexing [17] and caching [18], etc. Over the last decade, a huge amount of research has been conducted on improving the efficiency of mining FIs and many fast algo-rithms [9] have been proposed. However, the mining oper-ation can easily return an explosive number of FIs, which rectly affects the mining efficiency.

To address this problem, Maximal Frequent Itemsets proposed as concise representations of FIs . MFIs are also size n has (2 n  X  1) non-empty subset FIs, mining MFIs ef-fectively addresses the problem of too many FIs. However, resented by the FIs, but also require their occurrence fre-quency in the database for further analysis. For example, we need the frequency of the FIs to compute the support and confidence of association rules. MFIs, however, lose the frequency information of most FIs.

On the contrary, the set of CFIs is a lossless representa-tion of FIs. CFIs are FIs that have no proper superset with the same frequency. Thus, we can retrieve the frequency of the non-closed FIs from their closed supersets. However, CFI covers its subset only if the CFI appears in every trans-action that its subset appears in. This is unusual when the database is large, especially for a sparse dataset.
In this paper, we investigate the relationship between the frequency of an itemset and its superset and propose a re-laxation on the rigid definition of CFIs. We motivate our approach by the following example. Example 1 Figure 1 shows 15 FIs (nodes) obtained from a retail dataset, where abcd is an abbreviation for the itemset { a , b , c , d } and the number following  X : X  is the frequency of abcd .

Although we have only 1 MFI, i.e., abcd , the best esti-mation for the frequency of the 14 proper subsets of abcd is that they have frequency at least 100, which is the fre-quency of abcd . However, we are certainly interested in the knowledge that the FIs b , d and bd have a frequency CFIs preserve the frequency information but all the 15 FIs are CFIs, even though the frequency of many FIs only differ slightly from that of their supersets.

We investigate the relationship between the frequency of the FIs. In Figure 1, the number on each edge is computed as perset that has the greatest frequency. For CFIs, if we want to remove X from the mining result,  X  has to be equal to 0 which is a restrictive condition in most cases. However, if we relax this equality condition to allow a small tolerance, say  X   X  0 . 04 , we can immediately prune 11 FIs and retain only abcd , bcd , bd and b (i.e., the bold nodes in Figure 1). The frequency of the pruned FIs can be accurately es-timated as the average frequency of the pruned FIs that are of the same size and covered by the same superset. For ex-ample, ab , ac and ad are of the same size and covered by the same superset abcd ; thus, their frequency is estimated
We find that a majority of the FIs mined from most of the well-known real datasets [9], as well as from the preva-lently used synthetic datasets [12], exhibit the above char -acteristic in their frequency. Therefore, we propose to all ow tolerance, bounded by a threshold  X  , in the condition for the closure of CFIs, and define a new concise representation of FIs called the  X  -Tolerance CFIs (  X  -TCFIs ). The notion of as illustrated in the above example.

We propose two algorithms to mine  X  -TCFIs. Our algo-rithm, CFI2TCFI , is based on the fact that the set of CFIs is a lossless representation of FIs. CFI2TCFI first obtains the CFIs and then generates the  X  -TCFIs by checking the condition of  X  -tolerance on the CFIs. However, CFI2TCFI becomes inefficient when the number of CFIs is large.
We study the closure of the  X  -TCFIs and propose an-other algorithm, MineTCFI , which makes use of the  X  tolerance in the closure to perform greater pruning on the mining space. Since the pruning condition is a relaxation on the pruning condition of mining CFIs, MineTCFI is al-ways more efficient than CFI2TCFI. The effectiveness of the pruning can also be inferred from Example 1 as the ma-jority of the itemsets can be pruned when the closure defin-ition of CFIs is relaxed.
 We compare our algorithms with FPclose [10], NDI [7], MinEx [5] and RPlocal [16], which are the state-of-the-art algorithms for mining the four respective concise represen -tations of FIs. Our experimental results on real datasets [9 ] show that the number of  X  -TCFIs is many times (up to or-ders of magnitude) smaller than the number of itemsets ob-tained by the other algorithms. We also measure the error rate of the estimated frequency of the FIs that are recovered from the  X  -TCFIs. In all cases, the error rate of CFI2TCFI is significantly lower than  X  while that of MineTCFI is also considerably lower than  X  . Most importantly, MineTCFI memory consumption of MineTCFI is also small and in most cases smaller than that of the other algorithms.
Another important finding of mining  X  -TCFIs is when  X  increases, the error rate only increases at a much slower rate. Thus, we can further reduce the number of  X  -TCFIs by using a larger  X  , while still attaining high accuracy. Organization. Section 2 gives the preliminaries. Then, Section 3 defines the notion of  X  -TCFIs and Section 4 presents the algorithms CFI2TCFI and MineTCFI. Section 5 reports the experimental results. Section 6 discusses re-lated work and Section 7 concludes the paper.
Let I = { x itemset. We say that a transaction Y supports an itemset X if
Y  X  X . For brevity, an itemset { x k written as x
Let D be a database of transactions. The frequency of an itemset X , denoted as freq ( X ) , is the number of transac-tions in D that support X . X is called a Frequent Itemset specified minimum support threshold . X is called a Maxi-mal Frequent Itemset ( MFI ) if X is an FI and there exists no FI Y such that Y  X  X . X is called a Closed Frequent Itemset ( CFI ) if X is an FI and there exists no FI Y such that Y  X  X and freq ( Y ) = freq ( X ) . Then, we discuss how we estimate the frequency of the FIs that are recovered from the  X  -TCFIs. Finally, we give an analysis on the error bound of the estimated frequency of the recovered FIs. 3.1 The Notion of  X  -TCFIs Definition 1 (  X  -Tolerance Closed Frequent Itemset) An itemset X is a  X  -tolerance closed frequent itemset (  X  -TCFI) if and only if X is an FI and there exists no FI Y such that where  X  ( 0  X   X   X  1 ) is a user-specified frequency toler-ance factor .
 We can define CFIs and MFIs by our  X  -TCFIs as follows. Lemma 1 An itemset X is a CFI if and only if X is a 0 -TCFI.
 Lemma 2 An itemset X is an MFI if and only if X is a 1 -TCFI.
 form the upper bound and the lower bound of the set of all  X  -TCFIs, respectively.
 Example 2 Referring to the 15 FIs in Figure 1. Let abcd } . For example, b is a 0 . 04 -TCFI since b does not have a proper superset that has frequency greater than ((1  X  0 . 04)  X  139) = 133 . The FI a is not a 0 . 04 -TCFI sively covered by their superset that has 1 more item and then finally covered by the 0 . 04 -TCFI abcd .

The set of 0 . 07 -TCFIs is { bd , abcd } , while the set of TCFIs, i.e., MFIs, is { abcd } . However, the set of 0 -TCFIs, i.e., CFIs, is all the 15 FIs. 2 In the rest of the paper, we use F to denote the set of all FIs and T to denote the set of all  X  -TCFIs, for a given  X 
Given T , we can recover F (when demanded by appli-cations). The frequency of an FI X  X  F can be estimated from the frequency of its supersets in T . We discuss the frequency estimation in this subsection.
 It is possible that for an FI X , there are more than one FI
Y , where Y  X  X , | Y | = | X | + 1 and freq ( Y )  X  ((1  X  has the greatest frequency can best estimate the frequency of
X . Thus, we define this superset as the closest superset of X as follows.
 Definition 2 (Closest Superset) Given an itemset X , let Y = Y : Y  X  X, | Y | = | X | + 1 , and freq ( Y ) = MAX { freq ( Y  X  ) : Y  X   X  X, | Y  X  | = | X | + 1 } . Y is the closest superset of X if Y  X  Y and Y is lexicographically ordered before all other itemsets in Y .

Given an itemset X , we can follow a path of closest su-persets and finally reach one closest superset, which is a TCFI. We define this  X  -TCFI superset as the closest  X  -TCFI superset of X as follows.
 Definition 3 (Closest  X  -TCFI Superset) Given n itemsets, X 1 , . . . , X n | X X X Example 3 Referring to Figure 1, the closest superset of a is ac , that of ac is acd and that of acd is abcd . For the two supersets of ab that have the same frequency, we choose abc as the closest superset of ab since abc is or-dered before abd . When  X  = 0 . 04 , abcd is the closest  X  -TCFI superset of all its subsets that contain the item a , while bcd is the closest  X  -TCFI superset of bc , cd and c . 2
To estimate the frequency of the FIs with the same clos-est  X  -TCFI superset Y , we group the FIs according to their size and define the frequency extension of Y as follows. Definition 4 (Frequency Extension) Given a  X  -TCFI Y , let X i = { X : | X | = | Y | X  i set of X } , where 1  X  i  X  m and m = MAX { i : X ( ext ( Y, 1) , . . . , ext ( Y, m )) , where ext ( Y, i ) m , is defined as
The size of the frequency extension of Y , denoted as | ext ( Y ) | , is defined as | ext ( Y ) | = m .
The frequency extension of Y is essentially a list of aver-aged frequency ratio grouped by the size of the FIs. With the frequency extension of Y , we can estimate the frequency of each X  X  X frequency estimation by Example 4.
 Example 4 Referring to Example 3, let Y = abcd , then X 1 = { { a ext ( abcd , 3) = 111 100 / 1 = 1 . 11 .

Thus, the frequency of abc , abd and acd are estimated as ( freq ( abcd ) ext ( abcd , 1)) = 103 , the frequency of ab , ac and ad are estimated as ( freq ( abcd ) ext ( abcd , 2)) = 107 , while the frequency of a is estimated as ( freq ( abcd ext ( abcd , 3)) = 111 . 2
We now analyze the error bound of the frequency esti-mation. We first give Lemmas 3 and 4, which we use to define the error bound.
 Lemma 3  X  X  X  ( F  X  T ) ,  X  Y  X  T such that Y  X  X and freq ( Y )  X  ((1  X   X  ) | Y | X  X  X | freq ( X )) .
 Lemma 4 For any  X  -TCFI Y , 1  X  ext ( Y, i )  X  1 Lemma 5 (Error Bound of Estimated Frequency) Given an FI X and X  X  X  closest  X  -TCFI superset Y , where | Y |  X  | X | = i . Let freq ( X ) be the exact frequency of X and g of
X . Then, where  X  = (1  X   X  ) i .
 4, we have 0  X  freq ( Y )  X  g freq ( X )  X  freq ( Y ) 3, we have 0  X  freq ( Y )  X  freq ( X )  X  freq ( Y ) (
Lemma 5 gives the theoretical error bound of the fre-quency of an FI estimated from the frequency of its closest  X  -TCFI superset. However, according to Definition 4, each quency ratio of the FIs in X the relative difference in the frequency of any two FIs in is bounded by  X  . Thus, in practice, the estimated frequency is highly accurate and the error bound is much smaller than the theoretical bound defined in Lemma 5, which is also verified by our extensive experiments.
In this section, we first present an algorithm that com-putes  X  -TCFIs from the set of CFIs. Then, we propose a more efficient algorithm that employs pruning based on the closure of the  X  -TCFIs.
Mining CFIs is in general much more efficient than min-ing FIs. Since the set of CFIs is a lossless representation of FIs, we devise an algorithm which takes advantage of the efficiency of mining CFIs. The algorithm first generates the CFIs and then computes the  X  -TCFIs from the CFIs. Algorithm 1 CFI2TCFI
Our algorithm, CFI2TCFI , is shown in Algorithm 1. We first generate all CFIs and partition them according to the size of the CFIs. Let C from i = 1 , we find the closest CFI superset of each CFI (Line 5). Here, the closest CFI superset of X is defined as X  X  X  CFI superset that has the smallest size and the greatest frequency among all other CFI supersets of X . If X  X  X  clos-est CFI superset is not found, then X is a  X  -TCFI and we include X in T (Line 10). If X has a closest CFI superset Y clude X in T (Line 10). Otherwise, we update ext ( Y ) with
CFI2TCFI computes the exact set of all  X  -TCFIs and as we show in Section 5, the estimated frequency of the FIs re-covered from the  X  -TCFIs obtained by CFI2TCFI is highly accurate in all cases. However, the search for the closest CFI superset of each CFI is costly when the number of CFIs is large. Thus, we propose a more scalable algorithm whose efficiency is not affected by the number of CFIs. MineTCFI , for mining  X  -TCFIs. We first describe the data structures used in MineTCFI in Sections 4.2.1 and 4.2.2. Then, we discuss an effective pruning in Section 4.2.3 and present the main algorithm in Section 4.2.4. 4.2.1 FP-Tree and FP-Growth The pattern-growth method , FP-growth , by Han et al. [11] is one of the most efficient methods for mining FIs, CFIs and MFIs [10]. We adopt the pattern-growth procedure as the skeleton of our algorithm MineTCFI.

FP-growth mines FIs using an extended prefix-tree struc-ture called the FP-tree . As an example, Figure 2 shows the FP-tree, T erates the FIs in Figure 1.
FP-growth mines the set of FIs as follows. Given an FP-tree T the original database. For each item x in T growth follows the list of pointers to extract all paths from the root to the node representing x in T the conditional pattern base of Y = X  X  { x } , denoted as
B Y , from which FP-growth constructs a local FP-tree, called the conditional FP-tree , denoted as T frequent items in B re-orders the frequent items in each path in B quent items are discarded) and inserts the new path into T Figure 3 shows the conditional FP-tree, T structed from the FP-tree T
The above procedure is applied recursively until the con-ditional FP-tree consists of only a single path, P which FP-growth generates the itemsets represented by all sub-paths of P . 4.2.2 The  X  -TCFI Tree A crucial operation in MineTCFI is the search for the su-persets of an itemset in the set of  X  -TCFIs already discov-ered. Performing a subset testing by comparing the itemset with every existing  X  -TCFI is clearly inefficient. In mining CFIs, the subset testing can be efficiently processed by an FP-tree-like structure [10]. We thus develop a similar stru c-
To avoid testing all existing  X  -TCFIs with X , a condi-tional  X  -TCFI tree , C ditional FP-tree T procedure calls. Each C TCFIs that are supersets of X . Thus, this local C smaller than a global  X  -TCFI tree that contains all  X  -TCFIs. Each node v in C and  X  -TCFI-link , where the item label indicates which item v represents, the level is the level of v in C at Level 0), and the  X  -TCFI-link is a pointer to the  X  -TCFI represented by the root-to-v path. Since each  X  -TCFI has a frequency extension, we keep the  X  -TCFIs in an array so that the frequency extension will not be duplicated in each of the conditional  X  -TCFI trees.
 Like T C
X . header items in T C
X . header A have item label x and level l .
 Example 5 If  X  = 0 . 027 , we obtain seven  X  -TCFIs after processing the item a . Figure 4 shows the global  X  -TCFI tree, C shows the conditional  X  -TCFI tree, C  X  -TCFIs that are supersets of c . C the FP-trees T that a is not in C This is because all  X  -TCFIs containing a have already been generated and hence there is no need to include a in C
In C v )  X  means that A c has three entries: A c [1] has a pointer to v at Level 1, A A [3] has a pointer to v 3 at Level 3. 2 Update and Construction of  X  -TCFI Tree. To insert a  X  -TCFI Z = X  X  Y into C the order of the items in C inserted into C as a path in C link, link , of each node on the path as follows. Assume link new node is created for an item in Y , then its  X  -TCFI-link points to Z .
 To construct a conditional  X  -TCFI tree, C x in C C
Y . header we access each node v in C tract the root-to-v path, P . After discarding the nodes on that do not correspond to an item in C the remaining nodes on P according to C insert the path into C we insert Z into C Example 6 To insert the  X  -TCFI cbd into C we first sort bd as db according to C 5(a). Then, we share the path h v link of v TCFI tree after the insertion of cbd is shown in Figure 5(b), where C 4.2.3 Closure-Based Pruning The efficiency of CFI mining is mainly due to the pruning based on the closure of CFIs. We make use of the tolerance in the closure of the  X  -TCFIs to achieve greater pruning in MineTCFI.
 The pruning is described as follows. Given an FI X and X  X  X  conditional FP-tree T is an item in T generated from Y  X  X  conditional FP-tree T is covered if there exists a  X  -TCFI Z such that Z  X  Y and we generate Y , if Y is already covered, then we prune all FIs in F ( T The above pruning can be directly applied to mine 0-TCFIs (i.e., CFIs), since the FIs in F ( T covered by some 0-TCFIs that are found before we generate Y . However, when  X  &gt; 0 , a minority of FIs in F ( T not be covered by any existing  X  -TCFI due to the frequency tolerance in the closure. Some of this minority of FIs may later become  X  -TCFIs. However, only a very small number of these FIs will become  X  -TCFIs. Missing these  X  -TCFIs will only slightly degrade the accuracy of the estimated fre -quency of the recovered FIs, while we can still recover all FIs from their other  X  -TCFI supersets. But to improve the accuracy of the estimated frequency, we apply an additional checking to prevent pruning these potential  X  -TCFIs, as de-scribed by the following heuristic.
 Heuristic 1 Let H be the set of frequent items in Y  X  X  condi-tional pattern base, B
Heuristic 1 is based on the proximity of frequency of the itemsets found in most datasets: if Y is covered and Y generated from T FIs in-between Y and Y  X  are also covered.

However, at the time when we generate Y , the frequency of if there exists a  X  -TCFI, Z  X  , which is a superset of Y quency). Thus, we obtain the following heuristic. Heuristic 2 If Y is covered and there exists a  X  -TCFI, Z
Heuristic 2 implies that we only need to check the subset-superset condition without knowing the frequency of Y  X  . To further increase the probability that other FIs in F ( T also covered, we can add one more level of checking that | ext ( Z  X  ) |  X  ( | Z  X  |  X  | Y |  X  1) , which means that already covered subsets of size from ( | Y | +1) to ( | Z Let U and V be any two such subsets covered by Z  X  , where | V | = | U | + 1 , then the difference between the frequency of
U and that of V is bounded by  X  . Since the FIs in F ( T Y also share the same superset Z  X  , this proximity of frequency in F ( T Y ) are also covered. Thus, we obtain Heuristic 3. We first define that Y  X  is conditionally covered by a  X  -Heuristic 3 If Y is covered and there exists a  X  -TCFI, Z all FIs in F ( T Example 7 Based on T find abdc is a 0.07-TCFI after processing a . Then, when we process c in T { d , b } in B cd . Since c is covered by abdc , ( c  X  { d , b } )  X  abdc and | ext ( abdc ) | = 3 &gt; ( | abdc |  X  | c |  X  1) = 2 sure that the frequency of cb , cbd and cd can be estimated with ext ( abdc ) . Thus, we can prune cb , cbd and cd .
Note that if  X  =0 . 04 , then abdc does not cover c . Hence, we will continue from c and find the  X  -TCFI cbd . 2 Coverage Testing. We now discuss how Heuristic 3 can be efficiently processed using the  X  -TCFI tree.
 Given an FI X and X  X  X   X  -TCFI tree C of
Y in C X as follows. We access A x in C X . header and follow the pointers in A to visit the nodes that have item label x and are at Level of
C X . For each node v visited, let v  X  X   X  -TCFI-link point to
Z , we check if Y is covered by Z by testing freq ( Z )  X 
If Y is covered by Z , then Heuristic 3 requires us to check if Y  X  = Y  X  H is conditionally covered, where H is the set of frequent items in B sort the items in H as their order in C sorted H be H = x x
We first process A the nodes at Level k in C pointer in A a superset of H . The checking starts from v  X  X  parent up to the root and we compare both the item label and the level of each node along the path. When we compare x i  X  k  X  1 ) with a node u , if u  X  X  level is smaller than stop the comparison and move on to process the next node pointer in A we finish A Since C that are supersets of X , the number of comparisons is usu-ally small. In addition, those  X  -TCFIs that are accessed via pointers in A paths from the root to those nodes have less nodes than the number of items in H and hence cannot be supersets of H . In the same way, the level of a node also helps terminate many of the subset testings earlier.

When a root-to-v path is found to be a superset of H , let v is conditionally covered by Z  X  , Heuristic 3 is then applied and all FIs in F ( T
In MineTCFI, if Y is covered by Z and Y  X  is condition- X  -TCFI superset of Y in order to update the frequency ex-tension of Z . To do this, we need to check whether the size of
Z is the smallest among all  X  -TCFIs that are supersets of
Y . But this does not mean that we need to process all  X  -TCFIs that are supersets of Y . We do not process any of the  X  -TCFIs that are accessed via the pointers in A &gt; | Z | X  X  X | , because the pointers in A x [ j ] link to of size at least ( | X | + j ) &gt; | Z | . In most cases,  X  -TCFI superset is found via a pointer in A do we go through many entries of A 4.2.4 Algorithm MineTCFI We now present our algorithm, MineTCFI , as shown in Algorithm 2 . After constructing the global FP-tree T MineTCFI invokes the recursive pattern-growth procedure GenTCFI , which is shown in Procedure 1 .

In Procedure 1, the processing of IsCovered (Lines 4 and 14), IsCondCovered (Line 15) and the search for the closest  X  -TCFI superset (Lines 5 and 16) are discussed in Coverage Testing in Section 4.2.3. Procedure 1 can be divided into two parts: when the input conditional FP-tree, T of only one single path (Lines 1-9), and when T than one path (Lines 10-23).
 When T generates all itemsets which satisfy locally the condition of a  X  -TCFI. Then, for each local  X  -TCFI Y , GenTCFI checks if
Y is covered. If Y is not covered, then Y is a  X  -TCFI and we add it to T (Line 8). GenTCFI also inserts Y into all the conditional  X  -TCFI trees which are constructed along the path of the previous recursive calls of GenTCFI (Line 9), so  X  -TCFI trees correctly. If Y is covered, GenTCFI finds Y  X  X  closest  X  -TCFI superset Z from C quency extension with the frequency of Y (Lines 5-6). When T processes each item x in T GenTCFI constructs the conditional pattern base B Y = X  X  { x } . Let H be the set of frequent items in B Y If Y is covered and ( Y  X  H ) is conditionally covered, by Heuristic 3, GenTCFI prunes all supersets of Y that are to Algorithm 2 MineTCFI Procedure 1 GenTCFI ( T be generated from T constructs Y  X  X  conditional FP-tree T TCFI tree C is then called to process on T
We now evaluate our approach of mining  X  -TCFIs. We run all experiments on a PC with an Intel P4 3.2GHz CPU and 2GB RAM, running Linux 64-bit.
 Datasets. We use the real datasets from the popular FIMI Dataset Repository [9]. We choose three datasets with the following representative characteristics. For a wide rang e of values of  X  :  X  pumsb * : the number of CFIs is orders of magnitude  X  accidents : the number of CFIs is almost the same  X  mushroom : the number of CFIs is orders of magni-Algorithms for Comparison. We compare our algorithms CFI2TCFI and MineTCFI with the following algorithms:  X  FPclose [10]: the winner of FIMI 2003 [9] and one of  X  NDI [7]: the algorithm (the faster DFS approach) for  X  MinEx [5]: the algorithm for mining the set of frequent  X  RPlocal [16]: the faster algorithm (than RPglobal )
We first study the performance of the different algo-rithms by varying the minimum support threshold  X  . We fix  X  =0 . 05 for both pumsb * and accidents . For mushroom , since the difference between the number of CFIs and that of MFIs is much smaller than the other two datasets, we set a larger  X  = 0 . 2 to obtain a greater reduc-tion for the algorithms with the parameter  X  .

We use the same  X  for CFI2TCFI, MineTCFI and RPlo-cal. However, the  X  defined in MinEx is an absolute value. Thus, in each case we compare with MinEx, we find a  X  for MinEx such that the error rate of MinEx approximately matches that of MineTCFI. 5.1.1 Number of Itemsets and Error Rate We compare the size of each of the concise representations of FIs. For simplicity, we use Num ( alg ) to denote the num-ber of itemsets obtained by the algorithm alg .

Figures 6 to 8 report the number of itemsets returned by each algorithm. In most cases, Num ( CFI2TCFI ) and Num ( MineTCFI ) are about an order of magnitude smaller than Num ( FPclose ) and Num ( NDI ) , many times smaller tained by both MineTCFI and CFI2TCFI is very close to the number of MFIs.

Table 1 shows the error rate of the estimated frequency of the FIs recovered from the  X  -TCFIs. We can see the er-ror rate of CFI2TCFI is much lower than  X  in all cases. The cially that for mushroom is only 1/10 of  X  . The error rate of MineTCFI is higher than CFI2TCFI because MineTCFI of a  X  -TCFI in its frequency extension, as some of the sub-sets are pruned. The error rate of MinEx is the same as that of MineTCFI. NDIs and CFIs are lossless representations of FIs, while the error rate of RPlocal is bounded by  X  . 5.1.2 Running Time and Memory Consumption Figure 9 reports the running time and memory consumption of the algorithms. We truncate the time and memory that are orders of magnitude larger than the largest points presente d in the respective figures, since most of the time and memory usage are small and will be squeezed into a single line if we use a logarithmic scale.
 nificantly faster than all other algorithms. The running tim e of RPlocal is the closest to that of MineTCFI but still about 3 times longer on average. CFI2TCFI is also fast in most of the cases, except when the number of CFIs is large.
The memory consumption of the algorithms is small in most cases, except that CFI2TCFI and FPclose use more memory when the number of CFIs is large. For mushroom as shown in Figure 9 (c2), MineTCFI consumes more mem-ory than other algorithms but the difference is only 2MB. However, in most of the other cases, MineTCFI has the low-est memory consumption among all algorithms, as shown in Figures 9 (a2) and (b2). 5.2 Effect of Different Values of  X 
We now study the effect of different values of  X  on min-ing  X  -TCFIs. We test on the two larger datasets pumsb * and accidents . We fix  X  at 0.3 and vary  X  from 0.001 (a sufficiently low error rate in our opinion) to 0.2 (a  X  which the set of  X  -TCFIs is almost the set of MFIs).
Figures 10 (a) and (b) show the number of  X  -TCFIs ob-tained by CFI2TCFI and MineTCFI, as well as the number of CFIs and MFIs as references. The number of  X  -TCFIs is about 4 to 5 times smaller than that of CFIs at  X  = 0 . 001 and already becomes over an order of magnitude smaller at  X  = 0 . 01 . The number of  X  -TCFIs is within 2 times of that of MFIs at  X  = 0 . 05 and is almost the same as that of MFIs at  X  = 0 . 2 . Figure 11 shows the error rate of CFI2TCFI and MineTCFI for pumsb * and accidents . At  X  = 0 . 001 , the error rate of CFI2TCFI and MineTCFI is significantly (up to 20 times) smaller than  X  , except that of MineTCFI for pumsb * which is approximately 0.001. The error rate increases only slightly for large values of  X  . For pumsb at 0 . 05  X   X   X  0 . 2 and accidents at 0 . 1  X   X   X  0 . 2 , the error rate increases only within the range of 0.01.
This result shows that the actual error rate does not grow with the theoretical error bound given in Lemma 5, but re-mains to be small when  X  becomes large. This is an impor-tant finding since for many applications the user is allowed to specify a large  X  , while we can still achieve high accu-racy, which is not largely affected by  X  , and obtain a very concise set of  X  -TCFIs. The small error rate also demon-strates the need for the frequency extension of a  X  -TCFI in maintaining high accuracy of the estimated frequency.
In addition to MFIs [4] and CFIs [14] we have discussed in Section 1, we are aware of the work by Xin et al. [16] a cluster if  X  Y (called a representative pattern ), such that  X  X  X  S , X  X  Y and (1  X  freq ( Y ) this definition is non-recursive, while the definition of our  X  -TCFIs removes the redundant subsets recursively. Thus, our approach is able to achieve better compression as evi-denced by experimental results. We note that the number of  X  -TCFIs can be significantly reduced when a relaxed mini-mum support threshold is used as in [16]. However, in our experiments, we do not relax the minimum support thresh-other algorithms under comparison.

Boulicaut et al. [5] define an itemset X as a  X  -free-set if  X  X  X   X  X ,  X  Y  X  X  X  such that ( freq ( Y )  X  freq ( X  X  frequency of an FI X is estimated from its subsets; thus, an extra set of border itemsets is required in order to de-termine whether X is frequent. Calders and Goethals [7] utilize the inclusion-exclusion principle to deduce the lo wer bound and the upper bound for the frequency of an itemset and define an itemset as non-derivable if the lower bound and the upper bound are not equal. The set of NDIs is a some cases. Pei et al. [15] propose two types of condensed FI bases to approximate the frequency of itemsets with a user-defined error bound k . The frequency of an FI can be derived from either its subsets or supersets in the FI base.
We propose  X  -TCFIs as a concise and flexible represen-tune  X  to enjoy the benefits of both MFIs and CFIs: we can prune a great amount of redundant patterns from the min-ing result as do MFIs, while we can retain the frequency information of the recovered FIs as do CFIs. Experimental close to the number of MFIs and much smaller than all other existing concise representations of FIs [10, 7, 5, 16]. The results also show that the actual error rate of the estimated frequency of the recovered FIs is much lower than the theo-retical error bound. In particular, our algorithm CFI2TCFI attains an error rate significantly lower than  X  in all cases. CFI2TCFI is also shown to be very efficient in most cases except when the number of CFIs is large. Our second algo-rithm MineTCFI attains an accuracy slightly lower than that of CFI2TCFI; however, MineTCFI is significantly faster than all other algorithms [10, 7, 5, 16] in all cases and also consumes less memory in most cases.
 Acknowledgement. This work is partially supported by RGC CERG under grant number HKUST6185/03E. The viding us FPclose, Dr. Bart Goethals for providing us NDI, Prof. Christophe Rigotti for providing us MinEx, and Prof. Jiawei Han and Mr. Dong Xin for providing us RPlocal.
