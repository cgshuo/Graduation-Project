 filter and manage the information easily and quickly on the Web is growing accordingly. Named Entity Recognition (NER) is such a technology which is useful in digital libraries, question answering, etc. 
The NER task was first introduced as Message Understanding Conference (MUC) subtask in 1995 (MUC-6) [1]. Named Entities were defined as entity names (personal expressions and time expressions) and number expressions. Compared with the entity names recognition, the recognition of temporal expressions and number expressions is simpler. However the recognition of entity names is harder because of their opening, expansibility and randomness. 
There have been a lot of researches on English NER and great successes achieved as follows. 1. There is no space to mark the word boundary and no standard definition of words segmentation are interactional.[2] especially for abbreviations and aliases. 3. Unlike English, Chinese lacks the obvious features such as capitalization that plays an important role in indicating named entities. This paper presents an approach that combines two statistical models: HMM and MEM. The hybrid-statistical model integrates NER and Part-of-Speech tagging into a and number expressions are also checked. 
The experiments on open-test data achieve preferable results. Here lists the /79.96% (personal names), 81.57%/86.64% (location names) and 71.23%/78.42% expressions is 88.37%/91.69%. experiment results and analysis are given in Section 5. Finally, in Section 6 we make conclusions. methods in NER: rule-based and statistic-based. 
Traditionally, the approaches to NER have been rule-based. This method begins which the first two systems are for English and the last one is for Japanese. However, the rule-based method is neither robust nor portable. It is bottlenecked by knowledge acquisition. So people turned to the statistic-based method, which depends less on the is better. There are some statistical models used frequently in NER such as HMM [6], [10] and conditional Markov model [11]. rule-based method are more accurate. However, Chinese syntax is very complex and because of the complexity of natural languages, the rule-based system becomes tremendous so as to inevitable conflict among rules. So its achievement is limited. On method. Consequently, current research on NER has focused on hybrid method--combining the statistical model with linguistic knowledge. 
Up to now, much research work has been done on NER, especially on English. For Portuguese are respectively 97%, 93%, 94%, while the F1 of Chinese NER is 85%. This paper implements NER by integrating two statistical models (HMM and MEM) and applying linguistic knowledge in them. MEM is invoked when integrated into a unified framework. Our method is characterized as follows: 1. NER and Part-of-Speech tagging are integrated into a unified framework. of the current word with its context of adjacent words. 3. The Part-of-Speech information of the context is utilized by MEM. statistical models, applying some knowledge in rules while recognizing. The model invoke MEM include surnames, suffixes of location names and organization names. When the word sequence matches a rule of specific entity names, MEM is invoked. The second part is HMM. MEM is regarded as a sub-model of HMM. Viterbi word sequence (sentence). It can make full use of the constraints in the sentence to select the best tagging path. The output of HMM is a sentence with Part-of-Speech respectively in HMM and MEM can effectively constrict the search space and speedup the recognizing process. 3.1 MEM [7] contextual information, and has strong ability of knowledge representation. MEM has proved to be a good method on Text Classification, Data Preparation, Part-of-Speech tagging and so on. 
The MEM estimates probabilities with the principle of making as few assumptions about unknown constraints as possible. All known constraints are derived from training data expressing some relationship between features and outcomes. In NER, the features are information of the context, while the outcomes entropy, which agrees with the maximum-likelihood distribution. The particular parameters, can be found in reference [7]. Table 2 are designated directly. features observed in the training data at least K times, where K is the given threshold (in our experiment, it is set to 10). Although this method does not guarantee to obtain the best selection, it turned out to perform well in practice. 
All of the features in Table 2 make obviously positive effect on NER and then just need to be counted in the training data. 3.2 HMM HMM has been widely used in many fields, especially in speech recognition, Part-of-Speech tagging and also NER. In our method we use the Viterbi algorithm mainly. Part-of-Speech sequences for the word sequence. In the frame of HMM, NER has been a part of the Part-of-Speech tagging. 
In our method, Viterbi algorithm consists of two parts, building the searching space ntfirst, ntmid and ntlast , which are respectively corresponding to three types of entity names: personal names, location names and organization names. NE-states are added Viterbi algorithm can work. So in our method, the ambiguity of entities is dealt as that of tagging. 3.3 Linguistic Knowledge been introduced above in section 3.1. algorithm is a sentence with a simple segmentation. The algorithm used for segmentation is Forward Maximum Length Matching. In Viterbi algorithm, when with the context as a candidate entity, or teat them as separate common words. The ambiguity will be kept until Part-of-Speech tagging. When this kind of ambiguity is words to mark all the possible positions in the candidate entity. And the observation probability of the candidate entity is calculated by MEM. The above process would traversing the states space. statistical models and Named Entity Filtering. 4.1 Integration of Two Statistical Models Speech sequence; at last we will combine the NE-states into Named Entity tags from the final Part-of-Speech sequence and update the global cache. Firstly, we define some symbols in Table 4. the last three ones correspond to nt . The definitions of are shown in Table 5 and 6. 
The following is the description of our algorithm which is framed in Viterbi algorithm, invoking MEM to calculate the observation probabilities of candidate entity names. In the algorithm, the recognized temporal expressions and number done in the filtering procedure. likely sequence). We take the sentence  X   X  as an example. 1) Viterbi algorithm 1.1) making the state spaces: For each word O i : 
Firstly, T(i) is initialized by the Part-of-Speech dictionary, global cache and lists of named entity. For example, when processing the third word  X   X , we will receive its Part-of-Speech information (auxiliary) from the Part-of-Speech dictionary. word  X   X  X s an example. invoked. Then the state space of  X   X  will consist of nrfirst , and the state space of  X   X  X ill consist of nrlast . rule of organization name. So do  X  X  i-2 O i-1 O i  X  and  X  X  i-3 O i-2 O i-1 O i  X . algorithm can run smoothly. 1.2) traditional Viterbi algorithm: expressions and number expressions. 2) The Use of MEM 
For many entity names are unknown words, MEM is invoked to calculate the observation probability of this kind of words. 2.1) Feature Matching: matching the features in MEM with information of the context 2.2) Calculating the observation probability. 
The final identification result is as follows: 4.2 Named Entity Filtering respectively. This ignorance caused much misrecognition. So in Named Entity results show that it acquires a great increase in the precision. 
Filtering is used within and after Viterbi algorithm. The filtering of location names and organization names has been carried on within Viterbi algorithm. For the suffix. The threshold value of each kind of entity is acquired on empirical observation from corpus. 1) Filtering within Viterbi algorithm 
Only the word sequence with the probability larger than the corresponding its NE-states will be added into the state spaces of the word sequence, otherwise NE-states are not added. Table 7 introduces the probability calculating method of a word sequence as a location name. The processing on organization names is similar. 2) Filtering after Viterbi algorithm grouping temporal expressions and number expressions. 
Because some words are marked as entities just for ensuring that Viterbi algorithm segmented in the form of  X  X ne character + one character X . If the sequence consists of three single words, it is segmented in the form of  X  X ne character + two characters X  or Therefore they will be segmented again as . 
In Viterbi algorithm we have recognized some temporal expressions and number expressions which are in known entities lists. All number strings in the algorithm are recognized as part of temporal expressions and number expressions. So in this second string is  X 12 X  and the following of it is  X   X (mean month)  X  then a temporal expression is recognized as  X 12  X (means December). 5.1 Training Corpus and Testing Corpus contains 28603 sentences (about 716815 Chinese words), including 11138 personal names, 14229 location names, 6745 organization names, 13287 temporal expressions and 39612 number expressions. The testing data contains 6987 sentences (about organization names, 3433 temporal expressions and 9226 number expressions. 5.2 Resource of Knowledge The knowledge is acquired from various linguistic resources, shown in Table 9. 5.3 Experimental Results ones after  X / X  are the results with filtering. 5.4 Analysis of the Experimental Results The results without filtration show that the recalls are better than the precisions. This is because the knowledge used in NER is on the set level and all elements in one set So we add filtering during and after the recognizing procedure. The great increase in precision can be seen in the result with filtration, especially for personal names, which is fifteen percents. We also find that the results of organization are worse than those organization, the other is that our method has not looked further enough to recognize  X   X . In addition, the improvement on time and number expressions. examples of these entities are  X   X , the indicative words of which are  X   X . A method of avoiding this phenomenon is using semantic knowledge as much as possible. 
In our method the global cache can ensure the coherence of the same named are as follows:  X   X . So we should consider the context information and the tradeoff of space-time seriously. 
Form the above results and analyses, we will focus on some work as follows in our future research. 1. Introducing more features into MEM. 2. Threshold value selection with more scientific measure. 3. Improving temporal and number expressions recognition in more detail. 4. Finding more trigger points of recognition. statistical model integrating knowledge into two statistical models, in which NER and Part-of-Speech have been combined into a unified framework. The results are preferable. However we should realize that there is still much work to do to improve our method, such as feature selection and finding more trigger points. China (60403050) and the National High Technology Research and Development Program.

