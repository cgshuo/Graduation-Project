 In this paper, we consider the task of estimating query effectiveness, i.e., assessment of the retrieval system performance in absence of user relevance judgments. In our approach we model the score associated with each document in the result set as a Gaussian random variable. The mean and the variance of each document score can then be used to estimate the probability that a document will be ranked above another one and thus calculate the expected rank of the document in the ranked list . We propose to measure the effectiveness of the system performance by comparing the predicted and actual ranks of the retrieved documents. In our experiments we consider two retrieval models and five document scoring methods and evaluate their impact on the proposed estimation measures. Our experiments with standardized data sets that include document relevance judgments and the task of predicting the relative query effectiveness show that the expected rank metric is robust to variations in document scoring and retrieval algorithms . H.3.4 [Systems and Software] : Performance Evaluation Measurement, Reliability, Experimentation Information retrieval (IR) systems help users find information by providing a ranked list of documents in response to a user query . In order to improve system performance, it is important to observe user assessment of the relevance of retrieved documents for a variety of queries. However, obtaining user judgments is time consuming and costly . Therefore we need methods t o assess the quality of retrieval results for a given query when associated relevance judgments are not available. We refer to this problem as query effectiveness estimation . Recently, Diaz [2] considered the problem of estimating query effectiveness by observing the similarity between two vectors  X  and  X  : the vector  X  defined as a vector of scores associated with the top N documents retriev ed for query q and vector  X  that represent the regularized scores computed as  X  =  X  .  X  , where W is N u N matrix with W ij corresponding to the similarity score between document d i and document d j in the result set. The rows are normalized so that  X   X  X  X  X  =1,  X  = 1 and diagonal elements W ii =0 . Thus, the regularized score  X   X  is independent of the actual score s i . The estimated performance of the query q is then given by the Pearson correlation between  X  and  X  :  X   X  ,  X  = In constructing the retrieval result set, an IR system may assign a score to each document, indicative of its relevance to the query and then rank the documents based on their scores. Alternatively, it can directly produce a ranking without explicitly associating scores with the documents at any point. In general, the methods we devise for query effectiveness estimation need to be invariant to monotonic transformations of the scores. When the scoring function is not explicit we hope for a normalization method that best work s for our purposes . We investigated two normalization approaches: 1) N(0, 1) normalization Adjust the scores to be of 0-mean a nd 2) [0, 1] normalization: Adjust the scores so that the least value When retrieved results comprise only a ranking, without scores, we consider three options for simulating document scores from their ranking. Given N ranked documents we define the score of a document at rank r by: 1) (N;r)-scoring : Score (r) = (N -r)/N 2) AP -scoring : Score(r) = 1 2  X  1 + 1 3) The C-scoring : Score(r) = 2(  X  +1  X  X  X  ) Each of the se five methods produces a corresponding document score vector s . Let W denote a square matrix containing pair-wise similarities with W ii = 0. This similarity matrix can be interpreted as a representation of the weighted nearest neighbor graph comprising N nodes. Inferring the value on a node from its neighbors is a form of semi-supervised learning. We can thus use results from this field to determine both the mean and the variance [6] of the values at unlabelled nodes. If the score s i predicted value  X   X  can be used as a substitute where When W contains document-document similarities, this score is exactly the same as the regularized score used by Diaz [2] . Within any such inference process there is some inherent uncertainty. Thus, rather than using a deterministic score  X  represent the score of a document d i as a Gaussian variable with the mean given by equation (1) and variance by  X  have  X   X   X  ;  X   X  ,  X   X  , which indicates that  X  Gaussian distribution with the mean  X   X  and the variance  X  We use the score distributions S i to estimate the uncertainty in the returned ranked ordering. The similarity matrix W is computed from the cosine dot product of document vectors obtained using only term -frequency information ,  X   X  X  X  =  X   X  X  X   X   X  X  X   X  is the number of occurrences of term k in document i. We decided not to use tf-idf sc oring because calculating idf require s statistics from the entire docum ent collection whereas tf scores can be calculated from individual documents . When a document d associated with a deterministic score s j , the rank of this document is given by the number of other documents d i with s we consider the score as a random variable, the rank of a document itself be comes a random variable. Following the method described in [4] , we calculate the expected rank of a document as follows. Let S ij denote the probability that the document d i will be ranked above document d j  X   X   X  &gt;  X   X  =  X  (  X   X   X  X  X   X  &gt; 0) . For Gaussian variables S have The expected rank of a document j is then given by  X   X  =1,  X  X  X  X  . Each S ij reflects the probability that the document d will win in a pair -wise contest with the document d scores variables. The fraction of pair -wise contests for which the document d j wins over other documents is the expected rank  X  We use t he Pearson correlation  X   X  ,  X  between the real ranks (the effectivenes s. We perform ed experiments on standardized data sets that include user relevance judgments and thus enable evaluation of our performance estimation measure  X   X  ,  X  . Here we report detail s of the experiments involving the estimation of relative query effectiveness . For a given set of queries and a given retrieval engine , we attempt to rank the queries in the order that corresponds to the ranking based on average precision, cal culated from user relevance judgment s. To test the robustness of our query ranking predictions across retrieval models, we use standard implementations ( [7] with default parameters) of two well -known retrieval models: BM25 and Language Model . We experiment with TREC Disks 4 &amp; 5 data without the CR sub -collection and a set of 200 topics (301 -450 and 601 -650) . From the topics we generated two set of queries in order to investigate the effect of the query length : the short queries comprising the topic title only and the long queries , using the description field of the topic s. For each query and a given retrieval algorithm we generate a result set and compute the corresponding average precision statistics. This represents the actual performanc e of the retrieval system. We then attempt to predict the performance for each individual query without using relevance judgments. This is the predicted perfo rmance . We measure the success of our predictions by calculating the Kendall -W between two ranked lists : queries ranked by the actual performance and by the predicted performance. For each of the five scoring methods we calculate the document score in rel ation to a given query. We consider a result set of size 100 for each query, i.e., the similarity matrix is 100 u 100. The effect of result set size will be explored in future work.
 As a baseline effectiveness estimation measure we use the spatial autocorrel ation method, indicated by  X   X  ,  X  and compare with  X   X  ,  X  . Note that the reported quantities for  X   X  ,  X  are not exactly the same as described in [2] because we do not use idf statistics in similarity computation.
 The results in Table 1 show that the estimations using expected rank metrics con sistently out -perform the baseline. In particular, in the case of short queries, the rank correlation measure  X   X  ,  X  performs best for both retrieval algorithms . When examining the effect of the 5 scoring methods, we find that  X   X  ,  X  is sensitive to this choice, often preferring the AP -scoring method. In contrast, our three metrics exhibit stable performance across all five alternatives. W e also note that AP -scoring is most preferable for estimation purposes. We suspect this is due to the fact that we are trying to estimate average precision and the AP -scoring is based on implicit weighting that average precision gives at dif ferent ranks. [1] Cronen -Townsend, S., Zhou, Y. and Croft, W. B. Predicting query [2] Diaz, F. Performance prediction using spatial autocorrelation. [3] Montague, M. and Aslam, J. A. Relevance score normalization for [7] The Lemur Toolkit for Language Modeling and Information 
