 Sentiment classification is the task to predict the sentiment polarity of a given review document or a comment sentence. In recent year s, sentiment classification interests more and more researchers in natural language processing and data mining fields be-cause of rapid development of techniques and growing population of sentiment-rich resources.

But most annotated sentiment corpora are in English, and annotating the sentiment corpora in other language is difficult, time-consuming or expensive. In order to solve the lack of annotated data problem, cross-lingual sentiment classification aims to use annotated sentiment corpora in one language (e.g. English) as training data, to predict the sentiment polarity of the data in another language (e.g. Chinese). Some pilot studies projected the data in target language into source language, and then treated the problem as sentiment classification in single language. Although various projection techniques (dictionary-based, mach ine-translation-based, etc.) h ave been applied, the performance is far from satisfactory.

In this paper, we target at finding an effective approach for the cross-lingual senti-ment classification problem. To achieve our goals, we propose a bi-view non-negative matrix tri-factorization (BNMTF) model. Our BNMTF model attacks the problem from the following directions:  X  Combine the information from both the source language view and the target lan- X  Incorporate both the lexical knowledge and the document label knowledge by ex- X  Add the information from the test data by transductive learning setting. The rest of the paper is organized as follows. We review some related work in Section 2. Then we give the formal definition and settin g of the cross-lingual sentiment classifica-tion problem we address in this paper in Section 3. The BNMTF model is proposed in Section 4, and we also give some brief analysis on our solution. We conduct a series of experiments to evaluate the effectiveness of our proposed model in Section 5. Finally, we conclude our work in Section 6. In this section, we review the prior researches mostly related to our work. Most of the related work comes from two research areas, cross-lingual information access and sentiment classification.

Cross-lingual information access (CLIA) i s concerned with technologies and appli-cations that enable people to freely access inf ormation that is expressed in any language. A lot of previous work emphasizes on cross-lingual text classification and clustering. [2] studied English-Spanish cross-language classification problem in the poly-lingual and cross-lingual cases. [11] classified Czech documents using the classifier built on English training data by a probabilistic English-Czech dictionary. [9] developed a novel method known as the information bottleneck technique for Chinese web pages classi-fication using the English web pages as training data. [17] proposed a framework to cross-lingual query classification which extended the cross-lingual classification prob-lem into short text case. [18] incorporated document similarity propagation method with the spectral clustering algorithm to cluster the news in different languages.
In this paper, we focus on cross-lingual text classification and we also employ the machine translate system to help us project the data from one language into another language.

Sentiment classification aims to predict the sentiment polarity of text data. The ap-proaches to solve the problem can be generally categorized into lexicon-based and corpus-based. Lexicon-based approaches measure the sentiment of the text based on sentiment lexicons. [14] proposed the semantic oriented method to predict the senti-ment orientation of a review while [6] built three models to assign sentence sentiment. Corpus-based approaches classify the sen timent of a given sentence or document by the classifier built using labeled sentiment d ata. Since the work of [13], various corpus-based methods have proposed.

An important issue in sentiment classification is the domain dependency problem, so a lot of efforts on domain adaptation have been made. [3] proposed the structural cor-respondence learning (SCL) algorithm while [12] proposed the spectral feature align-ment (SFA) algorithm for the cross-domain sentiment classification problem. And we can also treat the cross-lingual adaptation as a special case of domain adaptation if we consider different languages as different domains. The cross-lingual sentiment classifi-cation problem has also been studied by some previous work. [10] used cross-lingual projection through dictionaries or parallel corpora to judge the document subjectivity, while [1] and [15] employed the machine translation systems to bridge the data in dif-ferent languages. [16] proposed to use the co-training approach to address the problem.
In this paper, we try to combine the lexicon-based and corpus-based approaches like previous work on non-negative matrix tri-factorization [8]. In fact, the NMTF model can be treated as the basic component of our models. We refine the model to satisfy the cross-lingual condition in the problem we address in this paper. And Like [16], we also view the classification problem in two independent views, the source language view and the target language view. But we have two main difference between the standard co-training approach. One is that our model incorporate the lexical knowledge, and the other is that we combine the two views in the matrix factorization process so that we only need to train the BNMTF classifier once while a series of classifiers need to be trained if the standard co-training approach is applied. present some definitions.
 Definition 1. (Vocabulary) V s denotes the vocabulary in source language, while V t denotes the vocabulary in target language. Furthermore, | V s | = m s and | V t | = m t . In this paper, we use English as source language and Chinese as target language. Definition 2. (Sentiment Lexicon) W + s = { w + s 1 ,w + s 2 , ... } denotes the positive lexi-source language. Similarly, we can define W + t and W  X  t .
 Sentiment lexicons are very useful and important to the lexicon-based approaches for sentiment classification.
 Definition 3. (Document Set) D s = { d s 1 ,d s 2 , ..., d s n source language. We have d s i  X  V  X  s . Similarly, we can define D t .
 Definition 4. (Label) Y s = { y s 1 ,y s 2 , ..., y s n set in source language. y s i =+1 if the overall sentiment expressed in d s i is positive, while y s i =  X  1 if the overall sentiment expressed in d s i is negative. Similarly, we can define Y t . Please notice that in the problem we address in this paper, Y s is given, but the Y t is unknown and needs us to predict.

As sentiment lexicons to the lexicon-base d approaches, labeled documents are very useful and important to the corpora-based approaches for sentiment classification. Definition 5. (Translator) T src  X  tar : V  X  s  X  V  X  t denotes a translator from the source T In practice, translators can be dictionaries , machine translation systems, or other projec-tions from one language to anot her language. Through the translators, each document is able to have two views (one in source language and the other in target language) despite its original language.

Based on the definitions described above, now we can define the problem we try to address in this paper as follows: W In order to solve this problem, we try to achieve the following subgoals:  X  Combine the information from both the source language view and the target lan- X  Incorporate both the lexical knowledge (lexicon-based approach) and the document  X  Furthermore, adding information from the test unlabeled data to the model can help In this section, we describe our basic idea of our bi-view non-negative matrix factor-ization (BNMTF) model, and then give the ma thematical formulation. We also analyze our solution briefly in this section. 4.1 Basic Idea Firstly, let X  X  consider the problem in the source language side. We build a term-document matrix X s using V s , D s and D s t . Here machine translation s ervice is employed so that all the documents(including training and test) have representations in both source lan-guage and target language. And in this pape r, we use both the training documents and the test documents together to build the term-document matrix in order to add knowl-edge from the unlabeled data to the model, which means the models we proposed here is transductive (i.e., we use test data without label in training phrase).
And then similar to [8], we can set up a non-negative matrix tri-factorization (NMTF) problem as equation (1).
 We normalize these matrices so that we can explain the equation using the concepts in probabilistic latent semantic indexing (PLSI) model [5]. X s , F s , G s and S s can be treat as the joint distribution between words and documents, the word class-conditional prob-ability, the document class-conditional p robability and the class probability distribution.

According to [8], the lexical knowledge and training document label knowledge can be incorporated. Here we incorporate them by the following ways:  X  (Lexical Knowledge) We set ( F 0 s ) i 1 = p ( p  X  1 )and ( F 0 s ) i 2 =1  X  p if the  X  (Training Document Label Knowledge) We set ( G 0 s ) j 1 = q ( q  X  1 )and ( G 0 s ) j 2 = For the test documents, it is better if good prior can be given. In [8], the authors use the K-means clustering r esults for initialization. Here we proposed an alternative way. We use the training data to build a traditional classifier (e.g. maximum entropy classifier, support vector machine, etc.), and then using the classification results on the test data as prior knowledge. In our model, we also put this prior knowledge into G 0 s .
Now we have given the details of our model in source language view, while in the target language view everything is almost the same. Similar to equation (1), we can set up another non-negative matrix tri-factorization problem as equation (2) using the data in target language view. The corresponding matrices can be set in the same way as in source language view.
After we set up the two NMTF problems, we see them as two views of our model and connect them in order to combine the information from the two views. Equation (3) is a simple constraint which gives a useful connection: The sentiment of a document is the same (at least close to each other) in the two views.
 The schema of our basic idea is shown in Figure 1. We build two term-document matri-ces, one according to representation of all th e documents(both training and test) in the source language and the other in the target la nguage. The transductive learning setting ensure the information from test data included. And then we factorize the two matri-ces into document and word space, the lexical knowledge and training document label knowledge is incorporated in the factorizatio n process. The factorization process is also under a useful constraint so that we can combine the information from two views. 4.2 Mathematical Formulation and Brief Analysis In this subsection, we propose our BNMTF model in mathematical formulation and a iterative solution to the BNMTF with some brief analysis.
 If we put equation (1) and (2) together with a bi-view coefficient  X  and let G s = G t = G , we get equation (4). It is a good mathematical representation of our basic idea in last subsection.  X   X  [0 , 1] is the bi-view coefficient which indi cate how confident we are to each view.
To solve equation (4), we design a iterative procedure which is list as follows 1 .  X  r = s/t  X  Iteration:  X  Initialization: Then, we briefly analyze the correctness 2 , convergence 3 and computational complexity of our updating rules.

For proving the correctness, we need just calculate the gradient of the objective func-tion, and then test the KKT condition like [4].
 For proving the convergence, we need just use auxiliary function approach like [7]. And empirically, we need only a few iterati ons before practical convergence (see in Subsection 5.4).
 Last but not the least, let us consider about computational complexity. The same as NMTF model, our BNMTF model scales linearly with the dimensions and density of the term-document matrices. For one iteration, it takes O ( k 2 ( m s + m t + n s + n t )+ kz ) time, while k =2 and z ( m s + m t )( n s + n t ) is the number of non-zero entries in the two term-document matrices here. In this section, we describe our experiments to show the effectiveness of our proposed bi-view non-negative matrix factorization (BNMTF) model for cross-lingual sentiment classification. 5.1 Datasets We use three review datasets in different domains (MOVIE/BOOK/MUSIC) in the ex-periments. On the English side, we use movie reivews from the IMDB archives used in [13], book and music reviews from Amazon.com used in [3], while on the Chinese side, we collect movie, book and music reviews from a popular Chinese social web, douban 4 . We treat the English reviews as training data, and use the Chinese reviews for test. Furthermore, to evaluate our algorith m, we label the Chinese reviews according to the users X  ratings.
 For all the datasets, all the reviews are translated into another language by Google Translate Service 5 . The data in English view is tokenized and lowercased by some perl scripts 6 , while the data in Chinese view is segmented by the Stanford Chinese Word Segmenter 7 . At last stopwords are removed.
 The summary of the review datasets is shown in Table 1.

And we also use four sentiment lexicons from HowNet Knowledge Database 8 .To-tally, there are 3,730 positive words and 3,116 negative words in Chinese, with 3,594 positive words and 3,563 negative words in English. 5.2 Baselines In the experiments, we use the following baseline methods to compare with our BNMTF model described in Subsection 4.2. For the first three baselines, only training data is used in training phrase, while for others, all the data is used (except the labels of test data).  X  MaxEnt(CHN): It applies maximum entropy classifier 9 with only Chinese terms.  X  MaxEnt(ENG): It applies maximum entropy classifier with only English terms.  X  MaxEnt(CE): It averages the results of MaxEnt(CHN) and MaxEnt(ENG).  X  CoTrain: It is like the co-training approach for cross-lingual sentiment classification  X  NMTF(CHN): It solves equation(2). The prediction of MaxEnt(CHN) on test data  X  NMTF(ENG): It solves equation(1). The prediction of MaxEnt(ENG) on test data  X  NMTF(CE): It averages the results of NMTF(CHN) and NMTF(ENG).
 We also calculate the 10-fold for every dataset by 10-fold cross validation on the test data using maximum entropy classifier. It is an approximation of the upper bound of classification results.

A quick word about parameter setting. According to [8], we set  X  src =  X  tar =0 . 5 and  X  src =  X  tar =1 .Andweset  X  =0 . 5 and I = 100 (the number of iterations for non-negative matrix tri-factorization) for the overall comparison. 5.3 Overall Comparison Results We use accuracy to evaluate the classification result. Figure 2 shows the overall comparison results. From the figure, we can observe that our BNMTF model outperforms other baseline methods significantly.

The first three baseline methods do not use the unlabeled test data in training phrase, so they can not perform as good as other methods which are benefited by the informa-tion from the unlabeled test data. Among al l the baselines, the accuracy of CoTrain is almost the closest to that of our BNMTF m odel because these two methods combine the information both Chinese view and English view in the whole process, where others are trained in only one view or combine the two views in the last time. We can also see that generally NMTF models are better than corresponding MaxEnt models because of the sentiment lexical knowledge. It indicat es that both sentiment lexicons and labeled training documents is useful and important to sentiment classification task. It is also the main reason why our BNMTF outperform the strongest baseline, CoTrain. We also note that on MUSIC, our BNMTF model even outperform the uppperbound. It is reasonable because our BNMTF model contains the sentim ent lexical knowledge which is not used when we calculate the 10-fold. 5.4 Influence of Parameters We test two main parameters of our BNMTF model, the number of iterations ( I )and the bi-view coefficient (  X  ).

Figure 3(a) shows the influences of number of iterations to the classification perfor-mance. In this experiment, we fix  X  =0 . 5 and change the value of I from 25 to 150 with step length 25 . We can see that after 100 iterations , the classification accuracy be-come robust. On MOVIE and MUSIC, the accuracy increases after 100 iterations but the speed is slower. On BOOK, the accuracy d ecreases after 100 iterations because the noise of the data, but the results are still acceptable. The experiment results show that our algorithms converge fast in practice, which implies the scalability of our approach.
Figure 3(b) shows the influences of bi-view coefficient to the classification perfor-mance. In this experiment, we fix I = 100 and change the value of  X  from 0 . 1 to 0 . 9 with step length 0 . 1 . Generally speaking, the classification accuracy is higher when  X  is closer to 0 . 5 . The experiment results indicate that both Chinese view and English view are beneficial to classification accu racy. And our BNMTF model, which combines the information from the two views, is able to gain the benefits from both views. Al-though the Chinese view on BOOK is a little poor so that we get the best results on BOOK when  X  =0 . 3 not  X  =0 . 5 , we can also see the benefit of the Chinese view. Furthermore, this situation implies that we can estimate the parameter via a validation set instead of setting it manually.
 In this paper, we propose a bi-view non-negative matrix tri-factorization (BNMTF) model for the cross-lingual sentiment classification problem. Our model have three main advantages, (1) combining the info rmation from two views by a useful and im-portant constraint, (2) incorporating the lexical knowledge and training document label knowledge by the extended NMTF and (3) adding information from test documents by the transductive learning setting. Our experiments on cross-lingual sentiment classifica-tion in three different domains demonstrate the effectiveness of our proposed approach.
In the future, we are planning to extend our BNMTF model from transductive learn-ing setting into inductive learning setting. To do this, we need just use other unlabeled data 10 instead of the test data in training phrase, and then predict the test data in the non-negative matrix tri-factorization framework. In additional, currently we estimate all the parameters manually, while it seems more reasonable if we can estimate parameters via a validation set. Finally, we are also planning to extend our proposed BNMTF models to solve more general cross-lingual classification and other cross-lingual information access problems.
 This work is supported by National Natural Science Foundation of China(No. 60873211). We also thank the anonymous reviewers for their elaborate and helpful comments.
