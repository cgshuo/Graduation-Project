 Machine transliteration plays an important role in machine translation, cross-language information retrieval and named entity recognition. Term transliteration in machine transliteration addresses the problem of converting terms in one language into their phonetic equivalents in the target language via spoken form using character or phonetic units. Many papers work on transliterating foreign terms into Chinese [1][5][6][7][8]. [1] and [5] are two typical works on this issue using phoneme-based and character-based approaches, respectively. 
Gao [1] used a direct transliteration model for transliterating out-of-vocabulary foreign names. A statistical learning algorithm was used to generate basic pronunciation units from 1, 500 cognate pairs. These units consisting of chunks of phonemes do not always compose of meaningful combinations of consonants or transliteration. Li [5] proposed an n-gram direct orthographical mapping for converting basic pronunciation units in English characters into Chinese counterparts. This model uses character-based syllables as units and does not involve any letter-to-phoneme system. The problem of this model is it requires a very large training corpus to mitigate the data sparseness problem. For example,  X  X new X  and  X  X ew X  may be pronounced the same. They have the same pronunciation units in phoneme level, but have ones in character level. This will also result in losses of finding mapping units in term transliteration. 
To overcome the data sparseness problem, a huge training corpus may be used to achieve this goal. Such a huge corpus may not always available. An approach using phonemes is proposed in this paper to alleviate this problem. Firstly, using phoneme information can reduce the combinations of different terms with the same phonemes. Secondly, combining both syllables and sub-syllables can compose of almost all the syllable-based pronunciation units. In transliteration, when the syllable-based units are not available, the sub-syllable-based units can be used to generate candidates of syllables. Generally, using syllables can achieve a higher precision in mapping basic  X  X nformation Retrieval X  in a document implies  X  X nformation X  and  X  X etrieval X  in that document; however,  X  X nformation X  and  X  X etrieval X  in a document does not imply  X  X nformation retrieval X  in that document. 
Character and word error rates are always used to evaluate their performances [1][5][7]. Evaluating the transliteration performances using these criteria does not reflect the real usages of transliterations. For example, if the standard transliteration to the source term  X  X mith X  is  X   X  (shi-mi-si).  X   X  (shi-mi-si) may be a better transliteration than  X   X  (shi-mi-si) even if the character error rate of  X   X  is higher than that of  X   X . In addition to that, a foreign term may be transliterated into Chinese in various forms. For example, the  X  X ush X  of  X  X he President Bush X  may be rendered into  X  (bu-xi) X ,  X  (bu-shu) X  and  X  (bu-she) X  in Taiwan, Hong-Kong and Mainland China, respectively [1]. Taking these regional transliteration variants into account will be helpful to boost the performances of term transliteration and cross-language information retrieval. Therefore, an evaluation method taking the real cases of transliterations into consideration using Web corpora is proposed to measure transliteration performances. 
The remainder of the paper is organized as follows. Section 2 describes the proposed approach. Experimental results are presented in Section 3. Conclusions are drawn in Section 4. In transliteration, terms need to be decomposed into basic pronunciation units. Then, basic units are aligned and converted cross-linguistically. Finally, a transliteration model using contextual information is proposed to incorporate the variations in pronunciations or characters. From the extraction results reported in [4], some of the isolated pronunciation units, such as /r/ an d /l/, may be elided in transliteration. Therefore, contextual information is used to boost the performance of transliteration. 
A simple text-based syllabification algorithm for English terms is available in the literature [8]. This algorithm is applied to letters to convert letters into phonemes on the fly using heuristic mapping rules. EM algorithm has been used widely in alignment and obtaining basic pronunciation units [1][5]; however, not all the chunks of phonemes consist of basic linguistic components. A letter-to-sound system using machine-learning techniques can reduce complicated combinations of characters to a limited set of phonemes; therefore, each foreign term can be converted into phonemes using such a system in this paper. The phoneme-based syllabification approach used here is very similar to the classic one described in [2]. Traditionally, an English syllable is composed of an initial consonant cluster followed by a vowel and with the option of a final consonant cluster. However, in order to converting English syllables into Chinese syllables, all consonants in the final consonant cluster are then segmented into isolated consonants. Such a syllable may be viewed as the basic pronunciation unit in transliteration. Combining syllable and sub-syllable information can alleviate data sparseness and hence achieve a better performance. 
From an analysis on our training corpus shows that 70% of cognate pairs are with alignment the basic pronunciation units of English and Chinese terms can be generated by adopting equal syllable numbers. Then the obtained statistical information can then be used to align other cognate pairs with unequal syllable numbers. Correspondences between basic pronunciation units of English and Chinese terms are shown in Table 1. One point worthy of noting is that some isolated syllables may or may not be elided from training corpus. For example, the /er/ in  X  X arlo X  is elided when the term is rendered into Chinese; on the other hand, the /er/ is  X  X arson X  has been reserved. Another point also observed is that two the same syllables may be transliterated into different targets. For example,  X  X a X  X  in  X  X arlo X  and  X  X arson X  have been mapped to /ka/ and /ke/, depending on neigh boring syllables, respectively. 
One of the most important factors that affect term transliteration is pronunciation variation. Pronunciation variation is a phenomenon of pronunciation ambiguity. Some phonemes in source language terms may be pronounced swiftly, quietly or strongly in many different situations according to speakers X  speaking habits. Elision is one of the important problems. For example, elision is quite common in English speech. /t/ and /d/ are often elided before consonants or when they are parts of a sequence of two or three consonants. Another type of isolated pronunciation units, such as /l/ of  X  X oulder X , may or may not be transliterated into Chinese depending on the translators. A generative framework using finite state-machine has been proposed for English-Japanese term transliteration [3]. This learning algorithm decomposed the transliteration process into many subsystems. Each subsystem can perform a transformation independently. Following this framework, suppose that the possible transliterated-token pair is denoted by  X  =( X ,  X  ) to indicate that there is a transliteration equivalent,  X  , in target language with the largest probability can be selected for each token  X  in the source language. Each  X  can be determined by means of equation (1). where i s H and t H are phonemes converted from terms in source language and target language, respectively. If an existing source language letter-to-sound system is used and in order to simplify the explanation, let the target language phoneme-to-text conversion be deterministic, then the cross-linguistic phoneme-to-phoneme conversion is the main focus in this algorithm. Taking the elision of isolated unit into considera-contain of many different combinations in which isolated syllables may or may not be silent in transliteration. There are k syllables and U sub-sets of syllables in i s H in total and the items in each sub-set are sorted in descending order in indexes. Each sub-set of the source-language syllables is a basic unit used to transliterate into a term in the target language. The syllable-to-syllable probability then can be estimated by a set of context-dependent syllables trained from the transliteration lexicon directly. The main focus of equation (1) can be expressed by means of equation (2): index of w in i s H and  X  is a very small constant used to avoid any zero probability. A null syllable is attached for the cases of those syllables at the beginning and end of the pronunciation units. The fast growing Internet is one of the largest distributed databases in the world. Though the World Wide Web is not systematically organized, much invaluable information can be obtained from this huge text corpus. A large quantity of transliteration terms have been devised personally and used on the Web. For example,  X  X axton X  can be transliterated into  X  (pai-ke-si-dun) X ,  X  (pai-ke-si-dun) X ,  X  (pa-shi-dun) X ,  X  (pai-si-dun) X  and  X  (pei-si-dun) X . Most of these terms have not been registered to be transliterated terms of the source term in the dictionary. This information can be used to evaluate the transliteration performances. 
If a cognate pair can be found on the Internet with sufficiently large instances meaning that the transliteration can be regarded as correct. Two formal definitions are given to evaluate the transliteration performances. The first one defines what a possible cognate pair is when incorporating pronunciation variation. Those cognate pairs verified according to the first definition are then validated by the second procedure. The second one defines the automatic validation proposed in this paper. Definition 1. A cognate pair generated by a transliteration process is qualified if |t|/|s| &gt; m, where m is a dynamically determined threshold, |s| is the number of the syllabified transliterated terms. Definition 2. A transliteration term, t, can be validated to be a correct cognate of the source language term, s, if the co-occurrences, f(s, t), of this cognate pair exceed some specified threshold in a sentence on text snippets obtained from search engines. 
According to the above criteria, four different approaches using phoneme-based direct transliteration with language model (PDTM-LM), syllable-based direct transliteration model with language model (SDTM-LM), direct orthography-based model (DOM) and the proposed approach, context-based model (CM), are compared. The training data composed of about 30,000 cognate pairs are used in the experiment. Many terms in source language are not originating from English in this corpus. The test data set is composed of one hundred entries selecting the least popular fifty girls X  names and fifty boys X  names from the USA top-1000 popular baby names 1 . Names in this data set are not frequently used as that of top-ten ones in the list. They may have chances to be found on the Web. If a name is used too frequently, it has a chance to be collected and registered in a lexicon. Methods 1-best 3-best 5-best 10-best 1-best 3-best 5-best 10-best PDTM-LM 1 1 2 3 5 17 21 43 SDTM-LM 6 12 13 16 10 25 40 65 
The results of the top-one, top-three, top-five and top-ten transliterations generated by transliteration systems are validated and shown in Table 2 with instances at least three times and. For those cognate pairs co-occur only once in a sentence cannot be viewed as qualified because the instances are too small within top-100 snippets in a query. From the experimental results show the context-model (CM) achieved a better result than others on the average. 
Several points can be observed from the experimental results. DOM achieves a relatively good performance with only slightly worse than that of syllable-based approaches. Examining the basic units required in the transliteration, it discloses that most of the character-based basic units can be found; however some complex cases such combinations of one consonant cluster and one vowel cluster haves higher possibilities of being able to lose. These cases impact the transliteration performance. The performance achieved by phoneme-based direct transliteration model is worse than those syllable-based models. The reason is that converting a syllable cross-linguistically is easier and precisely than mapping the combinations generated from a group of sub-syllables, although, syllable-based approaches have problems in obtaining sufficiently large syllables without huge training data. The main reason that the proposed approach achieved a better performance than the syllable-based direct transliteration model is because sub-syllable information can be used when any syllable account. A syllable-based model utilizing contextual information and combining different levels of phoneme information for term transliteration is proposed in this paper. Experiments on term transliteration using phoneme-ba sed and character-based direct transliteration approaches and the proposed approach were also conducted. Using an objective evaluation method, which in turn utilizing Web corpora can reflect the term usages of experimental results showed that taking contextual information into account is helpful to term transliteration and validating the generated results using Web corpora can provide more concrete evidences to transliteration evaluation. 
