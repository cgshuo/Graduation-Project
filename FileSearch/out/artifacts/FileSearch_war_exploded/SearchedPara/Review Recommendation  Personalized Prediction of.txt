 The problem of identifying high quality and helpful reviews automatically has attracted many attention recently. Cur-rent methods assume that the helpfulness of a review is in-dependent from the readers of that review. However, we argue that the quality of a review may not be the same for different users. In this paper, we employ latent factor models to address this problem. We evaluate the proposed models using a real life database from Epinions.com. The experiments demonstrate that the latent factor models out-perform the state-of-the-art approaches and confirms that the helpfulness of a review is indeed not the same for all users.
 H.2.8.d [ Information Technology and System ]: Database Application X  Data Miming ; G.3 [ Mathematics of Com-puting ]: Probability and Statistics X  Statistical Computing, Multivariate Statistics Algorithms, Design, Experimentation Personalized Review Quality Prediction, Review Recommen-dation, Matrix Factorization, Tensor Factorization
Other people X  X  opinions have always played an important role in decision-making process. As online commerce activ-ity continues to grow, the role of online reviews is expected to become increasingly important [7, 8]. However, due to the fact that most online reviews are written by non-professional authors, they tend to be of very different quality. The grow-ing importance of online reviews in the purchasing decision making process of users has also led to the creation of an ever growing number of spam reviews, which try to mislead online users. Given the huge number of online reviews avail-able and their greatly varying quality, it becomes more and more difficult to find high-quality, helpful reviews.
Most previous works [3, 4, 11] attempt to predict the qual-ity of reviews by extracting textual features (e.g. total num-ber of tokens, ratio of nouns, adjectives, etc.) and learning a function of these features for predicting review quality. There are also some recent works [5, 10] utilizing social fea-tures (e.g. number of past reviews by reviewer, PageRank of the reviewer, etc.) in addition to the textual features to assess the quality of reviews.

To the best of our knowledge, all of the existing methods assume that the helpfulness of a review is the same for all users and determine the quality of a review in general (i.e. in a user unspecific way). However, this assumption may not be true in all cases. We argue that the quality of the same review may not be the same for different users. For example, a professional and an amateur photographer may rate the helpfulness of a review very differently because of their different evaluation criteria. We believe differences in the users X  criteria may result in different evaluations of a review.

In this paper, we introduce the new problem of personal-ized review quality prediction for recommendation of help-ful reviews. To address this problem, we employ two latent factor models based on the assumption that the observed re-view ratings depend on latent features of the reviews, review-ers, raters, and products. We first present a model based on probabilistic Matrix Factorization (MF) (Section 2.1). The underlying assumption of this model is that both reviews and raters (review readers) can be modeled by a set of la-tent features. The MF model personalizes review quality prediction by decomposing the rater  X  review matrix and learning latent factors for each rater and each review. We also employ the concept of Tensor Factorization (TF) (Sec-tion 2.2). The TF based model assumes that the latent fea-ture vector of each review is generated in a probabilistic way by aggregating the latent feature vector of its reviewer and the latent feature vector of the corresponding product (since each review is written by a reviewer about a product). That is, the rater  X  review two-dimensional matrix is converted into a three-dimensional tensor rater  X  reviewer  X  product .
We empirically verify that the TF model improves MF, which does not take into account the product and reviewer factors. In addition, our experiments demonstrate that la-tent factor models outperform the state-of-the-art non-per-sonalized approaches using textual and social features. Thus, o ur experiments confirm that the helpfulness of a review is indeed not the same for all users, and there are some latent factors that affect a user X  X  evaluation of review quality.
In this section, we present two factor models for estimating review quality: A probabilistic matrix factorization (MF) model, and a tensor factorization (TF) model. Note that, while MF and TF have already been used in the literature for item recommendation [1, 2], these methods have not yet been explored for review recommendation.

In the following models, p , v , u , and r denote the latent variables for product, reviewer, rater, and review, respec-tively.
In this section we employ MF to factorize the rater  X  review matrix for predicting review quality. The underlying assumption of this model is that both raters and reviews can be modeled by a set of latent features, which together determine the rating of a review in a probabilistic way. Suppose we have a set of N u raters and a set of N r reviews. The quality ratings expressed by raters on reviews are given in a matrix q = [ q ij ] N u N r . In this matrix q ij represents the quality rating of rater u i for review r j . q ij can be any real number, but often ratings are integers in the range [1 ; 5]. In this paper, without loss of generality, we map the ratings to the interval [0 ; 1] by normalizing the ratings. Let u matrices, with column vector u i representing K -dimensional rater-specific latent feature vectors of rater u i and column vector r j representing K -dimensional review-specific latent feature vectors of review r j . The goal of MF is to learn these latent variables and exploit them for recommendation. The conditional probability of the observed ratings in our MF model is defined as: where N ( x | ; 2 ) is the probability density function of the Gaussian distribution with mean and variance 2 , and I q ij is the indicator function that is equal to 1 if rater u i review r j ( q ij is observed) and equal to 0 otherwise. The function g ( x ) is the logistic function g ( x ) = 1 = (1+ exp ( which bounds the range of u T i r j within [0 ; 1]. We also place a zero-mean spherical Gaussian prior on rater and review feature vectors:
Through a Bayesian inference, the log of the posterior distribution over rater and review features can be obtained as follows: ln P ( u; r | q; 2 q ; 2 u ; 2 r ) = where C is a constant that does not depend on the param-eters. Maximizing the log-posterior over latent rater and review variables with hyper-parameters kept fixed is equiv-alent to minimizing the following sum-of-squared-errors loss function with regularization terms: Frobenius norm. A local minimum of this objective function can be found by performing stochastic gradient descent.
In this section, we employ TF to personalize review qual-ity prediction. The underlying assumption of our model is that since each review is written by a reviewer about a product, the latent feature vector of a review can be gener-ated in a probabilistic way by aggregating the latent feature vector of its reviewer and the latent feature vector of the corresponding product. That is, the rater  X  review two-dimensional matrix used as input in MF is converted into three-dimensional tensor rater  X  reviewer  X  product . Note that, in contrast to most of the TF-based recommendation system approaches which add a third dimension to the tra-ditional user-item matrix, we propose to replace the review dimension in MF with reviewer  X  product in TF.

Suppose we have a set of N u raters, a set of N v review-ers, and a set of N p products. The quality ratings ex-pressed by raters on reviews written by reviewers about different products are given in a three-dimensional tensor q = [ q ijk ] N u N v N p . In this tensor q ijk represents the qual-ity rating of rater u i for the review written by reviewer v about product p k . The three-dimensional tensor is factor-ized into three matrices and one central tensor. Let u  X  viewer, and product feature matrices, with column vector u representing K -dimensional rater-specific latent feature vec-tor of rater u i , column vector v j representing K -dimensional reviewer-specific latent feature vector of reviewer v j , and col-umn vector p k representing K -dimensional product-specific latent feature vector of product p k . Let also s  X  R K K K be the central tensor where K is the dimension of the model latent factors. The goal of tensor factorization is to learn these latent variables and exploit them for rating predic-tion. Given the feature matrices and the central tensor, the prediction can be made as follows: where h ijk is the predicted value for q ijk . To simplify Equation 6, we use the tensor-matrix product operator  X  a to multiply a matrix on dimension a with a tensor [2], e.g. t = s  X  u u i is a K  X  K matrix where t yz = So, the prediction is made by multiplying the three feature matrices and the central tensor:
Similar to MF, we minimize the sum-of-squared-errors between the observed rating q ijk and the predicted rating g ( h ijk ) [2]. We also add regularization terms to avoid over-fitting. So, the objective function for the minimization prob-lem is as follows: where I q ijk is the indicator function that is equal to 1 if q is observed (i.e. rater u i rates the review written by reviewer v about product p k ) and equal to 0 otherwise. The function g ( x ) is the logistic function defined in Section 2.1 which bound the range of h ijk within [0 ; 1]. Again, to minimize the objective function, we perform stochastic gradient descent.
In this section, we describe the real-life dataset we used for our experiments and present the results of our experimental evaluation and comparison with state-of-the-art methods.
To evaluate the proposed models, we performed exper-iments on a dataset from the popular reviewing website Epinions.com. Epinions.com is a consumers opinion web-site where users can write reviews about products (such as digital cameras, laptops, books, movies, software, etc.) and assign them numeric ratings in the range from 1 to 5. It also allows users to express their feedback about the help-fulness of a review using numeric quality ratings. We used the version of the Epinions X  dataset 1 published by the au-thors of [6]. This dataset was given directly by Epinions staff to the authors and, different from other publicly avail-able Epinions datasets, contains ratings of reviews assigned by different users. This dataset does not have the text of the reviews, but it provides the review identifiers. We en-hanced this dataset by downloading the review text from the Epinions website (using the available identifiers). This h ttp://www.trustlet.org/wiki/Extended Ep inions d ataset dataset is very large and covers a very wide range of more than 200,000 products from different categories which can be considered as a collection of several review datasets. T able 1 presents some statistics of our Epinions dataset. We can see that about 50% of the reviews have been rated by at least one rater. It is also shown that on average each rated review has been rated by 18 different raters which indicates the richness of this dataset.
As pointed out already, all of the current works have for-mulated the problem of determining review quality as a clas-sification or regression problem using a set of observed fea-tures, i.e. textual and/or social features. Classification-based approaches mainly employ SVM for classifying re-views [10, 4]. Our proposed models, on the other hand, pre-dict real-value ratings which cannot be compared with the classification results. For a fair comparison, we use the re-gression version of SVM, Support Vector Regression (SVR), which is also used in some regression-based approaches [3, 11]. Since other regression-based approaches mainly apply Linear Regression (LR) [5, 11] to predict the quality rating of reviews, we use LR as another comparison partner.
Each of the current works has defined a set of features to predict the quality of reviews. Most of them only use textual features [3, 4, 11] , while some of them combined both tex-tual and social features [5, 10, 9]. Our proposed models do not use observed features, however our comparison partners need them to be learned. We use textual and social fea-tures defined in the most recent work [5] to implement our comparison partners. Textual features include total number of tokens and sentences, ratio of positive and negative sen-timent words, ratio of verbs, adverbs, etc. Social features include number of past reviews by the reviewer, in-degree and out-degree of the reviewer, PageRank score of the re-viewer, etc. For the sake of clarity, we list all comparison partners as follows:
LR-Text Features : Linear Regression on textual fea-tures.

LR-Social Features : Linear Regression on social fea-tures.

LR-All Features : Linear Regression on textual and so-cial features.

SVR-Text Features : Support Vector Regression on tex-tual features.

SVR-Social Features : Support Vector Regression on social features.

SVR-All Features : Support Vector Regression on tex-tual and social features.

Matrix Factorization : Matrix factorization model de-scribed in Section 2.1.

Tensor Factorization : Tensor Factorization model de-scribed in Section 2.2. Figure 1: Sensitivity analysis for RMSE of MF and TF models for different values of K
In our proposed models, the dimension of the latent fac-tors, K , need to be specified as input. We perform sensitiv-ity analysis to find the optimum value for K . Figure 1 shows the RMSE of MF and TF models for different values of K . MF performs best at K = 5 and TF reaches its minimum RMSE at K = 4. It should be noted that in our experiments we set all regularization parameters (  X  X ) to 0.1.
In this section, we report the results of our experimental evaluation for the comparison partners listed in Section 3.2. For evaluation, we performed 10-fold cross validation on the Epinions dataset. In each fold we used 90% of the data as the training set and the remaining 10% as the test set. The evaluation metric is Root Mean Squared Error (RMSE), the standard metric for evaluation of recommender systems, which is defined as: where q is the actual quality rating of a review for a spe-cific user in the test set, h is the predicted value for q , and |
D test | is the number of ratings in the test set.

Analyzing the RMSE results (Table 2) confirms the find-ing from the literature [5] that using the combination of textual and social features can improve the accuracy of pre-diction over the text-only and social-only methods. The MF model, which is learned based on rater and review la-tent factors, outperforms all of the existing non-personalized methods, showing the superior ability of latent factor models in personalized review quality prediction. As expected, the TF model outperforms the MF model, since TF decomposes the review latent factor into reviewer and product latent fac-tors, providing a richer model of the effects underlying the observed data.
 Table 2: RMSE of the proposed models and com-parison partners
In this paper, we introduce the problem of personalized review quality prediction. To address this problem, we have employed two probabilistic factorization models, MF and TF. Experiments on a real-life dataset from Epinions.com demonstrate that the latent factor models outperform state-of-the-art comparison partners using observed features (LR and SVR). We also observed that TF performs better than MF by replacing the review latent factor with the reviewer and product latent factors. Finally, our experiments confirm that the helpfulness of a review is indeed not the same for all users and personalized predictions are more accurate. [1] M. Jamali and M. Ester. A matrix factorization [2] A. Karatzoglou, X. Amatriain, L. Baltrunas, and [3] S.-M. Kim, P. Pantel, T. Chklovski, and [4] J. Liu, Y. Cao, C.-Y. Lin, Y. Huang, and M. Zhou. [5] Y. Lu, P. Tsaparas, A. Ntoulas, and L. Polanyi. [6] P. Massa and P. Avesani. Controversial users demand [7] S. Moghaddam and M. Ester. Ilda: Interdependent lda [8] S. Moghaddam and M. Ester. Opinion digger: An [9] M. P. O X  X ahony and B. Smyth. Learning to [10] Y.-D. Tseng and C. C. Chen. Using an information [11] Z. Zhang and B. Varadarajan. Utility scoring of
