 Grammatical error correction (GEC) has been rec-ognized as an interesting as well as commercially attractive problem in natural language process-ing (NLP), in particular for learners of English as a foreign or second language (EFL/ESL).

Despite the growing interest, research has been hindered by the lack of a large annotated corpus of learner text that is available for research purposes. As a result, the standard approach to GEC has been to train an off-the-shelf classifier to re-predict words in non-learner text. Learning GEC models directly from annotated learner corpora is not well explored, as are methods that combine learner and non-learner text. Furthermore, the evaluation of GEC has been problematic. Previous work has either evaluated on artificial test instances as a substitute for real learner errors or on proprietary data that is not available to other researchers. As a consequence, existing meth-ods have not been compared on the same test set, leaving it unclear where the current state of the art really is.
 In this work, we aim to overcome both problems. First, we present a novel approach to GEC based on Alternating Structure Optimization (ASO) (Ando and Zhang, 2005). Our approach is able to train models on annotated learner corpora while still tak-ing advantage of large non-learner corpora. Sec-ond, we introduce the NUS Corpus of Learner En-glish (NUCLE) , a fully annotated one million words corpus of learner English available for research pur-poses. We conduct an extensive evaluation for ar-ticle and preposition errors using six different fea-ture sets proposed in previous work. We com-pare our proposed ASO method with two baselines trained on non-learner text and learner text, respec-tively. To the best of our knowledge, this is the first extensive comparison of different feature sets on real learner text which is another contribution of our work. Our experiments show that our pro-posed ASO algorithm significantly improves over both baselines. It also outperforms two commercial grammar checking software packages in a manual evaluation.

The remainder of this paper is organized as fol-lows. The next section reviews related work. Sec-tion 3 describes the tasks. Section 4 formulates GEC as a classification problem. Section 5 extends this to the ASO algorithm. The experiments are presented in Section 6 and the results in Section 7. Section 8 contains a more detailed analysis of the results. Sec-tion 9 concludes the paper. In this section, we give a brief overview on related work on article and preposition errors. For a more comprehensive survey, see (Leacock et al., 2010).
The seminal work on grammatical error correc-tion was done by Knight and Chander (1994) on arti-cle errors. Subsequent work has focused on design-ing better features and testing different classifiers, including memory-based learning (Minnen et al., 2000), decision tree learning (Nagata et al., 2006; Gamon et al., 2008), and logistic regression (Lee, 2004; Han et al., 2006; De Felice, 2008). Work on preposition errors has used a similar classifica-tion approach and mainly differs in terms of the fea-tures employed (Chodorow et al., 2007; Gamon et al., 2008; Lee and Knutsson, 2008; Tetreault and Chodorow, 2008; Tetreault et al., 2010; De Felice, 2008). All of the above works only use non-learner text for training.

Recent work has shown that training on anno-tated learner text can give better performance (Han et al., 2010) and that the observed word used by the writer is an important feature (Rozovskaya and Roth, 2010b). However, training data has either been small (Izumi et al., 2003), only partly anno-tated (Han et al., 2010), or artificially created (Ro-zovskaya and Roth, 2010b; Rozovskaya and Roth, 2010a).

Almost no work has investigated ways to combine learner and non-learner text for training. The only exception is Gamon (2010), who combined features from the output of logistic-regression classifiers and language models trained on non-learner text in a meta-classifier trained on learner text. In this work, we show a more direct way to combine learner and non-learner text in a single model.

Finally, researchers have investigated GEC in connection with web-based models in NLP (Lapata and Keller, 2005; Bergsma et al., 2009; Yi et al., 2008). These methods do not use classifiers, but rely on simple n-gram counts or page hits from the Web. In this work, we focus on article and preposition er-rors, as they are among the most frequent types of errors made by EFL learners. 3.1 Selection vs. Correction Task There is an important difference between training on annotated learner text and training on non-learner text, namely whether the observed word can be used as a feature or not. When training on non-learner text, the observed word cannot be used as a feature. The word choice of the writer is  X  X lanked out X  from the text and serves as the correct class. A classifier is trained to re-predict the word given the surround-ing context. The confusion set of possible classes is usually pre-defined. This selection task formula-tion is convenient as training examples can be cre-ated  X  X or free X  from any text that is assumed to be free of grammatical errors. We define the more re-alistic correction task as follows: given a particular word and its context, propose an appropriate correc-tion. The proposed correction can be identical to the observed word, i.e., no correction is necessary. The main difference is that the word choice of the writer can be encoded as part of the features. 3.2 Article Errors For article errors, the classes are the three articles a , the , and the zero-article . This covers article inser-tion, deletion, and substitution errors. During train-ing, each noun phrase (NP) in the training data is one training example. When training on learner text, the correct class is the article provided by the human annotator. When training on non-learner text, the correct class is the observed article. The context is encoded via a set of feature functions. During test-ing, each NP in the test set is one test example. The correct class is the article provided by the human an-notator when testing on learner text or the observed article when testing on non-learner text. 3.3 Preposition Errors The approach to preposition errors is similar to ar-ticles but typically focuses on preposition substitu-tion errors. In our work, the classes are 36 frequent English prepositions ( about, along, among, around, as, at, beside, besides, between, by, down, during, except, for, from, in, inside, into, of, off, on, onto, outside, over, through, to, toward, towards, under, underneath, until, up, upon, with, within, without ), which we adopt from previous work. Every prepo-sitional phrase (PP) that is governed by one of the 36 prepositions is one training or test example. We ignore PPs governed by other prepositions. In this section, we formulate GEC as a classification problem and describe the feature sets for each task. 4.1 Linear Classifiers We use classifiers to approximate the unknown rela-tion between articles or prepositions and their con-texts in learner text, and their valid corrections. The articles or prepositions and their contexts are repre-sented as feature vectors X  X  X . The corrections are the classes Y  X  X  .

In this work, we employ binary linear classifiers of the form u T X where u is a weight vector. The outcome is considered +1 if the score is positive and  X  1 otherwise. A popular method for finding u is empirical risk minimization with least square regu-larization . Given a training set { X i ,Y i } i =1 ,...,n aim to find the weight vector that minimizes the em-pirical loss on the training data  X u = arg min where L is a loss function. We use a modification of Huber X  X  robust loss function. We fix the regulariza-tion parameter  X  to 10  X  4 . A multi-class classifica-tion problem with m classes can be cast as m binary classification problems in a one-vs-rest arrangement. The prediction of the classifier is the class with the highest score  X  Y = arg max Y  X  X  ( u T Y X ) . In earlier experiments, this linear classifier gave comparable or superior performance compared to a logistic re-gression classifier. 4.2 Features We re-implement six feature extraction methods from previous work, three for articles and three for prepositions. The methods require different lin-guistic pre-processing: chunking, CCG parsing, and constituency parsing. 4.2.1 Article Errors  X  DeFelice The system in (De Felice, 2008) for  X  Han The system in (Han et al., 2006) relies on  X  Lee The system in (Lee, 2004) uses a con-4.2.2 Preposition Errors  X  DeFelice The system in (De Felice, 2008) for  X  TetreaultChunk The system in (Tetreault and  X  TetreaultParse The system in (Tetreault et al., For each of the above feature sets, we add the ob-served article or preposition as an additional feature when training on learner text. This section describes the ASO algorithm and shows how it can be used for grammatical error correction. 5.1 The ASO algorithm Alternating Structure Optimization (Ando and Zhang, 2005) is a multi-task learning algorithm that takes advantage of the common structure of multiple related problems. Let us assume that we have m bi-nary classification problems. Each classifier u i is a weight vector of dimension p . Let  X  be an orthonor-mal h  X  p matrix that captures the common struc-ture of the m weight vectors. We assume that each weight vector can be decomposed into two parts: one part that models the particular i -th classification problem and one part that models the common struc-ture The parameters [ { w i , v i } ,  X ] can be learned by joint empirical risk minimization , i.e., by minimizing the joint empirical loss of the m problems on the train-ing data The key observation in ASO is that the problems used to find  X  do not have to be same as the target problems that we ultimately want to solve. Instead, we can automatically create auxiliary problems for the sole purpose of learning a better  X  .

Let us assume that we have k target problems and m auxiliary problems. We can obtain an approxi-mate solution to Equation 3 by performing the fol-lowing algorithm (Ando and Zhang, 2005): 5.2 ASO for Grammatical Error Correction The key observation in our work is that the selection task on non-learner text is a highly informative aux-iliary problem for the correction task on learner text. For example, a classifier that can predict the pres-ence or absence of the preposition on can be help-ful for correcting wrong uses of on in learner text, e.g., if the classifier X  X  confidence for on is low but the writer used the preposition on , the writer might have made a mistake. As the auxiliary problems can be created automatically, we can leverage the power of very large corpora of non-learner text.

Let us assume a grammatical error correction task with m classes. For each class, we define a bi-nary auxiliary problem. The feature space of the auxiliary problems is a restriction of the original feature space X to all features except the observed word: X\{ X obs } . The weight vectors of the aux-iliary problems form the matrix U in Step 2 of the ASO algorithm from which we obtain  X  through SVD. Given  X  , we learn the vectors w j and v j , j = 1 ,...,k from the annotated learner text using the complete feature space X .

This can be seen as an instance of transfer learn-ing (Pan and Yang, 2010), as the auxiliary problems are trained on data from a different domain (non-learner text) and have a slightly different feature space ( X\{ X obs } ). We note that our method is gen-eral and can be applied to any classification problem in GEC. 6.1 Data Sets The main corpus in our experiments is the NUS Cor-pus of Learner English (NUCLE) . The corpus con-sists of about 1,400 essays written by EFL/ESL uni-versity students on a wide range of topics, like en-vironmental pollution or healthcare. It contains over one million words which are completely annotated with error tags and corrections. All annotations have been performed by professional English instructors. We use about 80% of the essays for training, 10% for development, and 10% for testing. We ensure that no sentences from the same essay appear in both the training and the test or development data. NUCLE is available to the community for research purposes.
On average, only 1.8% of the articles and 1.3% of the prepositions in NUCLE contain an error. This figure is considerably lower compared to other learner corpora (Leacock et al., 2010, Ch. 3) and shows that our writers have a relatively high profi-ciency of English. We argue that this makes the task considerably more difficult. Furthermore, to keep the task as realistic as possible, we do not filter the test data in any way.
 In addition to NUCLE, we use a subset of the New York Times section of the Gigaword corpus 1 and the Wall Street Journal section of the Penn Tree-bank (Marcus et al., 1993) for some experiments. We pre-process all corpora using the following tools: We use NLTK 2 for sentence splitting, OpenNLP 3 for POS tagging, YamCha (Kudo and Matsumoto, 2003) for chunking, the C&amp;C tools (Clark and Cur-ran, 2007) for CCG parsing and named entity recog-nition, and the Stanford parser (Klein and Manning, 2003a; Klein and Manning, 2003b) for constituency and dependency parsing. 6.2 Evaluation Metrics For experiments on non-learner text, we report ac-curacy, which is defined as the number of correct predictions divided by the total number of test in-stances. For experiments on learner text, we report F -measure where precision is the number of suggested correc-tions that agree with the human annotator divided by the total number of proposed corrections by the system, and recall is the number of suggested cor-rections that agree with the human annotator divided by the total number of errors annotated by the human annotator. 6.3 Selection Task Experiments on WSJ Test The first set of experiments investigates predicting articles and prepositions in non-learner text. This primarily serves as a reference point for the correc-tion task described in the next section. We train classifiers as described in Section 4 on the Giga-word corpus. We train with up to 10 million train-ing instances, which corresponds to about 37 million words of text for articles and 112 million words of text for prepositions. The test instances are extracted from section 23 of the WSJ and no text from the WSJ is included in the training data. The observed article or preposition choice of the writer is the class we want to predict. Therefore, the article or prepo-sition cannot be part of the input features. Our pro-posed ASO method is not included in these experi-ments, as it uses the observed article or preposition as a feature which is only applicable when testing on learner text. 6.4 Correction Task Experiments on NUCLE The second set of experiments investigates the pri-mary goal of this work: to automatically correct grammatical errors in learner text. The test instances are extracted from NUCLE. In contrast to the previ-ous selection task, the observed word choice of the writer can be different from the correct class and the observed word is available during testing. We inves-tigate two different baselines and our ASO method.
The first baseline is a classifier trained on the Gi-gaword corpus in the same way as described in the selection task experiment. We use a simple thresh-olding strategy to make use of the observed word during testing. The system only flags an error if the difference between the classifier X  X  confidence for its first choice and the confidence for the observed word is higher than a threshold t . The threshold parame-ter t is tuned on the NUCLE development data for each feature set. In our experiments, the value for t is between 0.7 and 1.2.
 The second baseline is a classifier trained on NU-CLE. The classifier is trained in the same way as the Gigaword model, except that the observed word choice of the writer is included as a feature. The cor-rect class during training is the correction provided by the human annotator. As the observed word is part of the features, this model does not need an ex-tra thresholding step. Indeed, we found that thresh-olding is harmful in this case. During training, the instances that do not contain an error greatly out-number the instances that do contain an error. To re-duce this imbalance, we keep all instances that con-tain an error and retain a random sample of q percent of the instances that do not contain an error. The undersample parameter q is tuned on the NUCLE development data for each data set. In our experi-ments, the value for q is between 20% and 40%. Our ASO method is trained in the following way. We create binary auxiliary problems for articles or prepositions, i.e., there are 3 auxiliary problems for articles and 36 auxiliary problems for prepositions. We train the classifiers for the auxiliary problems on the complete 10 million instances from Gigaword in the same ways as in the selection task experiment. The weight vectors of the auxiliary problems form the matrix U . We perform SVD to get U = V 1 DV T 2 . We keep all columns of V 1 to form  X  . The target problems are again binary classification problems for each article or preposition, but this time trained on NUCLE. The observed word choice of the writer is included as a feature for the target problems. We again undersample the instances that do not contain an error and tune the parameter q on the NUCLE de-velopment data. The value for q is between 20% and 40%. No thresholding is applied.

We also experimented with a classifier that is trained on the concatenated data from NUCLE and Gigaword. This model always performed worse than the better of the individual baselines. The reason is that the two data sets have different feature spaces which prevents simple concatenation of the training data. We therefore omit these results from the paper. The learning curves of the selection task experi-ments on WSJ test data are shown in Figure 1. The three curves in each plot correspond to different fea-ture sets. Accuracy improves quickly in the be-ginning but improvements get smaller as the size of the training data increases. The best results are 87.56% for articles (Han) and 68.25% for prepo-sitions (TetreaultParse). The best accuracy for ar-ticles is comparable to the best reported results of 87.70% (Lee, 2004) on this data set.

The learning curves of the correction task ex-periments on NUCLE test data are shown in Fig-ure 2 and 3. Each sub-plot shows the curves of three models as described in the last section: ASO trained on NUCLE and Gigaword, the baseline clas-sifier trained on NUCLE, and the baseline classifier trained on Gigaword. For ASO, the x-axis shows the number of target problem training instances. The first observation is that high accuracy for the selec-tion task on non-learner text does not automatically entail high F 1 -measure on learner text. We also note that feature sets with similar performance on non-learner text can show very different performance on Figure 1: Accuracy for the selection task on WSJ test data. learner text. The second observation is that train-ing on annotated learner text can significantly im-prove performance. In three experiments (articles DeFelice, Han, prepositions DeFelice), the NUCLE model outperforms the Gigaword model trained on 10 million instances. Finally, the ASO models show the best results. In the experiments where the NU-CLE models already perform better than the Giga-word baseline, ASO gives comparable or slightly better results (articles DeFelice, Han, Lee, preposi-tions DeFelice). In those experiments where neither baseline shows good performance (TetreaultChunk, TetreaultParse), ASO results in a large improvement over either baseline. The best results are 19.29% F 1 -measure for articles (Han) and 11.15% F 1 -measure for prepositions (TetreaultParse) achieved by the ASO model. baselines for a particular feature set. two baselines for a particular feature set. In this section, we analyze the results in more detail and show examples from our test set for illustration.
Table 1 shows precision, recall, and F 1 -measure for the best models in our experiments. ASO achieves a higher F 1 -measure than either baseline. We use the sign-test with bootstrap re-sampling for statistical significance testing. The sign-test is a non-parametric test that makes fewer assumptions than parametric tests like the t-test. The improvements in F -measure of ASO over either baseline are statis-tically significant ( p &lt; 0 . 001 ) for both articles and prepositions.

The difficulty in GEC is that in many cases, more than one word choice can be correct. Even with a threshold, the Gigaword baseline model suggests too many corrections, because the model cannot make use of the observed word as a feature. This results in low precision. For example, the model replaces as Table 1: Best results for the correction task on NU-CLE test data. Improvements for ASO over either baseline are statistically significant ( p &lt; 0 . 001 ) for both tasks. with by in the sentence  X  X his group should be cate-gorized as the vulnerable group  X  , which is wrong.
In contrast, the NUCLE model learns a bias to-wards the observed word and therefore achieves higher precision. However, the training data is smaller and therefore recall is low as the model has not seen enough examples during training. This is especially true for prepositions which can occur in a large variety of contexts. For example, the preposi-tion in should be on in the sentence  X ... psychology had an impact in the way we process and manage technology X  . The phrase  X  X mpact on the way X  does not appear in the NUCLE training data and the NU-CLE baseline fails to detect the error.

The ASO model is able to take advantage of both the annotated learner text and the large non-learner text, thus achieving overall high F 1 -measure. The phrase  X  X mpact on the way X  , for example, appears many times in the Gigaword training data. With the common structure learned from the auxiliary prob-lems, the ASO model successfully finds and corrects this mistake. 8.1 Manual Evaluation We carried out a manual evaluation of the best ASO models and compared their output with two com-mercial grammar checking software packages which we call System A and System B . We randomly sam-pled 1000 test instances for articles and 2000 test instances for prepositions and manually categorized each test instance into one of the following cate-gories: (1) Correct means that both human and sys-tem flag an error and suggest the same correction. If the system X  X  correction differs from the human but is equally acceptable, it is considered (2) Both Ok . If the system identifies an error but fails to cor-rect it, we consider it (3) Both Wrong , as both the writer and the system are wrong. (4) Other Error means that the system X  X  correction does not result in a grammatical sentence because of another gram-matical error that is outside the scope of article or preposition errors, e.g., a noun number error as in  X  X ll the dog X  . If the system corrupts a previously correct sentence it is a (5) False Flag . If the hu-man flags an error but the system does not, it is a (6) Miss . (7) No Flag means that neither the human annotator nor the system flags an error. We calculate precision by dividing the count of category (1) by the sum of counts of categories (1), (3), and (5), and re-call by dividing the count of category (1) by the sum of counts of categories (1), (3), and (6). The results are shown in Table 2. Our ASO method outperforms both commercial software packages. Our evalua-Table 2: Manual evaluation and comparison with commercial grammar checking software. tion shows that even commercial software packages achieve low F 1 -measure for article and preposition errors, which confirms the difficulty of these tasks. We have presented a novel approach to grammati-cal error correction based on Alternating Structure Optimization. We have introduced the NUS Corpus of Learner English (NUCLE), a fully annotated cor-pus of learner text. Our experiments for article and preposition errors show the advantage of our ASO approach over two baseline methods. Our ASO ap-proach also outperforms two commercial grammar checking software packages in a manual evaluation. This research was done for CSIDM Project No. CSIDM-200804 partially funded by a grant from the National Research Foundation (NRF) adminis-tered by the Media Development Authority (MDA) of Singapore.
