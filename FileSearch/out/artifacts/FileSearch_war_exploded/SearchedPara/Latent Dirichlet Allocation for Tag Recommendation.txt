 Tagging systems have become major infrastructures on the Web. They allow users to create tags that annotate and cat-egorize content and share them with other users, very helpful in particular for searching multimedia content. However, as tagging is not constrained by a controlled vocabulary and annotation guidelines, tags tend to be noisy and sparse. Es-pecially new resources annotated by only a few users have often rather idiosyncratic tags that do not reflect a common perspective useful for search. In this paper we introduce an approach based on Latent Dirichlet Allocation (LDA) for recommending tags of resources in order to improve search. Resources annotated by many users and thus equipped with a fairly stable and complete tag set are used to elicit la-tent topics to which new resources with only a few tags are mapped. Based on this, other tags belonging to a topic can be recommended for the new resource. Our evaluation shows that the approach achieves significantly better preci-sion and recall than the use of association rules, suggested in previous work, and also recommends more specific tags. Moreover, extending resources with these recommended tags significantly improves search for new resources.
 E.1 [ Data ]: Data Structures X  Graphs and networks ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Clustering, Information filtering ; I.2.7 [ Ar-tificial Intelligence ]: Natural Language Processing X  Lan-guage models Algorithms, Experimentation, Measurement social bookmarking system, delicious, tag recommendation, tag search
Tagging systems [23] like Flickr 1 , Last.fm 2 or Delicious have become major infrastructures on the Web. These sys-tems allow users to create and manage tags to annotate and categorize content. In social tagging systems like Delicious the user can not only annotate his own content but also content of others. The service offered by these systems is twofold: They allow users to publish content and to search for content. Thus tagging also serves two purposes for the user: 1. Tags help to organize and manage own content, and 2. Find relevant content shared by other users.
 Tag recommendation can focus on one of the two aspects. Personalized tag recommendation helps individual users to annotate their content in order to manage and retrieve their own resources. Collective tag recommendation aims at mak-ing resources more visible to other users by recommending tags that facilitate browsing and search.

However, since tags are not restricted to a certain vocabu-lary, users can pick any tags they like to describe resources. Thus, these tags can be inconsistent and idiosyncratic, both due to users X  personal terminology as well as due to the dif-ferent purposes tags fulfill [15]. This reduces the usefulness of tags in particular for resources annotated by only a few users (aka cold start problem in tagging), whereas for pop-ular resources collaborative tagging typically saturates at some point, i.e., the rate of new descriptive tags quickly de-creases with the number of users annotating a resource [18].
The goal of the approach presented in this paper is to over-come the cold start problem for tagging new resources. To this end, we use Latent Dirichlet Allocation (LDA) to elicit latent topics from resources with a fairly stable and complete tag set to recommend topics for new resources with only a few tags. Based on this, other tags belonging to the recom-mended topics can be recommended. Compared to an ap-proach using association rules, suggested previously for tag recommendation, our approach achieves significantly better precision and recall. Moreover, the recommended tags are more specific for a particular resource, and thus more useful for searching and recommending resources to other users [9]. The remainder of this paper is organized as follows. In Section 2, we define the problem of tag recommendation more formally, and introduce the two approaches based on http://www.flickr.com http://www.lastfm.com http://delicious.com association rules and LDA. In Section 3 we present our eval-uation results. In Section 4 we discuss related work, and in Section 5 we summarize and outline possible future research directions.
To evaluate our approach using LDA for tag recommenda-tion we compare our approach to association rules  X  a state-of-the-art method for tag recommendation proposed e.g. by Heymann et. al. [18]. After a formal problem description we introduce the two approaches in this Section.
Given a set of resources R , tags T , and users U , the ternary relation X  X  R  X  T  X  U represents the user specific assignment of tags to resources. A bookmark b ( r i ,u j source r i  X  R and a user u j  X  U comprises all tags assigned tag recommendation is to suggest new tags for a resource r with only a few bookmarks based on tag assignments to other resources collected in Y =  X  r 6 = r i  X  r,t X  X  R  X  T .
Association rules have been investigated in [18] for tag recommendation. They have the form T 1  X  T 2 , where T 1 and T 2 are tagsets. The three key measures for asso-ciation rules are support, confidence, and interest. Sup-port is the (relative) number of resources that contain all tags of T 1 and T 2 , i.e., an estimate of the joint probability P ( T 1 ,T 2 ). Confidence is an estimate of the conditional prob-ability P ( T 2 | T 1 ), i.e., how likely is T 2 given T 1 called lift) is defined as the ratio between the common sup-port for T 1 and T 2 , and the individual support of T 1 and T ( often together than expected, if they were statistically in-dependent. There exist efficient algorithms to exhaustively mine association rules with some minimum support from large datasets (e.g. [1]).

The basic idea of using association rules for tag recom-mendation is simple: If many resources with tags T 1 (high support) are typically also annotated with tags T 2 (high con-fidence), then a new resource with tags T 1 may also be mean-ingfully annotated with tags T 2 . More formally, given the tagset from (a few) bookmarks T = S b ( r,u i ) for a resource r by users u i , and an association rule T 1  X  T 2 , all tags in T 2 are recommended, if T 1  X  T .

Table 1 gives a selection of association rules with high confidence mined from our dataset. As also observed in [18] these rules cover all sorts of terminological relationships in-cluding spelling variants and synonyms (humour  X  humor; tools, utilities, utility  X  tool), loose notions of hypernyms (tutorial, resources  X  reference), and closely related terms (software, mac, apple  X  osx).

While the mined association rules are very intuitive, they typically recommend rather generic, frequent tags, such as  X  X oftware X  or  X  X eb X . This is a direct consequence of requiring some minimum support for T 1 and T 2 . Such generic tags are not necessarily useful for finding specific resources. Indeed, for personalized tag recommendation Xu et al. [31] explicitly projection  X  and selection  X  operate on multisets without removing duplicate tuples Table 1: Selection of tag association rules with con-fidence  X  0.9 penalize tag co-occurrences, when they have been annotated by different users.
The general idea of Latent Dirichlet Allocation (LDA) is based on the hypothesis that a person writing a document has certain topics in mind. To write about a topic then means to pick a word with a certain probability from the pool of words of that topic. A whole document can then be represented as a mixture of different topics. When the author of a document is one person, these topics reflect the person X  X  view of a document and her particular vocabulary. In the context of tagging systems where multiple users are annotating resources, the resulting topics reflect a collabora-tive shared view of the document and the tags of the topics reflect a common vocabulary to describe the document.
More generally, LDA helps to explain the similarity of data by grouping features of this data into unobserved sets. A mixture of these sets then constitutes the observable data. The method was first introduced by Blei et al. [10] and ap-plied to solve various tasks including topic identification [16], entity resolution [7], and Web spam classification [8].
The modeling process of LDA can be described as finding a mixture of topics for each resource, i.e., P ( z | d ), with each topic described by terms following another probability distribution, i.e., P ( t | z ). This can be formalized as where P ( t i | d ) is the probability of the i th term for a given document d and z i is the latent topic. P ( t i | z i = j ) is the probability of t i within topic j . P ( z i = j | d ) is the proba-bility of picking a term from topic j in the document. The number of latent topics Z has to be defined in advance and allows to adjust the degree of specialization of the latent top-ics. LDA estimates the topic X  X erm distribution P ( t | z ) and the document X  X opic distribution P ( z | d ) from an unlabeled corpus of documents using Dirichlet priors for the distribu-tions and a fixed number of topics. Gibbs sampling [16] is one possible approach to this end: It iterates multiple times over each term t i in document d i , and samples a new topic j for the term based on the probability P ( z i = j | t i based on Equation 2, until the LDA model parameters con-verge.
 P ( z i = j | t i ,d i ,z  X  i )  X  C
T Z maintains a count of all topic X  X erm assignments, C DZ counts the document X  X opic assignments, z  X  i represents all topic X  X erm and document X  X opic assignments except the cur-rent assignment z i for term t i , and  X  and  X  are the (sym-metric) hyperparameters for the Dirichlet priors, serving as smoothing parameters for the counts. Based on the counts the posterior probabilities in Equation 1 can be estimated as follows:
LDA assigns to each document latent topics together with a probability value that each topic contributes to the overall document. For tagging systems the documents are resources r  X  R , and each resource is described by tags t  X  T assigned by users u  X  U . Instead of documents composed of terms, we have resources composed of tags. To build an LDA model we need resources and associated tags previously assigned by users. For each resource r we need some bookmarks b ( r,u assigned by users u i ,i  X  { 1 ...n } . Then we can represent each resource in the system not with its actual tags but with the tags from topics discovered by LDA.

For a new resource r new where we only have a small num-ber of bookmarks ( i  X  { 1 ... 5 } ), i.e., only one to five users annotated this resource, we can expand the latent topic rep-resentation of this resource with the top tags of each latent topic. To accomodate the fact of some tags being added by multiple users whereas others are only added by one or two users we can use the probabilities that LDA assigns. As formalized in Equation 1 this is a two level process. Proba-bilities are assigned not only to the latent topics for a single resource but also to each tag within a latent topic to indi-cate the probability of this tag being part of that particular topic. We represent each resource r i as the probabilities P ( z | r i ) for each latent topic z j  X  Z . Every topic z sented as the probabilities P ( t | z j ) for each tag t n combining these two probabilities for each tag for r new , we get a probability value for each tag that can be interpreted similarly as the relative tag frequency of a resource. Setting a threshold allows to adjust the number of recommended tags and emphasis can be shifted from recall to precision.
Imagine a resource with the following tags:  X  X hoto X ,  X  X ho-tography X , and  X  X owto X . Table 2 shows the top terms for two topics related with the assigned tags. It is interesting to compare these two topics with the corresponding association rules in Table 1. Whereas the association rules indicate only fairly simple term expansions, the latent topics comprise an arguably broader notion of (digital) photography and the Table 2: Top terms composing the latent topics  X  X hotography X  and  X  X owto X  various aspects of tutorial material. Given these topics we can easily extend the current tag set or recommend new tags to users by looking at the latent topics. In our example, we can recommend  X  X hotos X ,  X  X mages X ,  X  X hotoshop X ,  X  X utorial X ,  X  X eference X , and  X  X ips X  if we set the threshold for the accu-mulated probabilities to 0 . 045 . LDA would assume that our resource in question belongs to 66% to the  X  X hoto X -topic and to 33% to the  X  X owto X -topic. Multiplying these probabilities with the individual tag probabilities of the latent topics re-sults in a ranked list of relevant tags for our resource.
To compare the two algorithms we evaluated both on a common dataset.
As a dataset for our evaluations we use a crawl from De-licious provided by Hotho et. al. [19]. The dataset consists of  X  75,000 users,  X  500,000 tags and  X  3,200,000 resources connected via  X  17,000,000 tag assignments of users.
The overlap between tags, resources and users is very sparse. To get a dense subset of the data we computed p -cores [4] for different levels. For p = 100 we get enough bookmarks for each resource to split the data into meaning-ful training and test sets (90%:10%). The test sets differ in the number of bookmarks each resource has assigned to simulate new resources that only have one to five user an-notations. For this we removed all tags not belonging to the first n bookmarks, n  X  X  1 ... 5 } .

Our final dataset consists of  X  10,000 resources,  X  10,000 users, and  X  3600 tags occuring in  X  3,200,000 tag assign-ments. We have five test sets containing 10% of the data. The 100-core ensures that each tag, each resource and each user appears at least 100 times in the tag assignments.
On this set, the only preprocessing of the tag assignments performed was the decapitalization of the tags. No stem-ming or other simplifications were applied. More sophis-ticated preprocessing might improve the results but would complicate the evaluation of the algorithms.
In this Section we report results for our LDA-based al-gorithm and compare these with the numbers we get using association rules for the same task on the same dataset. Table 3: Results for tag recommendation using as-sociation rules with different minimum confidences and 5 known bookmarks Table 4: Results for tag recommendation using as-sociation rules with minimum confidence 0.9 for 1 X 5 known bookmarks For mining association rules, we have used RapidMiner [24]. For the 9000 resources in the training set we get almost 550 K association rules with a minimum support of 0 . 05 and a minimum confidence of 0 . 1, many of which are of course par-tially redundant. Table 3 gives the results for 5 bookmarks, at different confidence levels (Conf). Precision (Prec), re-call (Rec), f-measure (FM) are measured at macro level, i.e., they are averaged over the individual measures for each resource. The maximum precision (Prec) of 0 . 648 for con-fidence  X  0 . 9 is lower than the 0 . 873 reported in [18], who operated on a bigger dataset (about 60 K resources, split into 50 K training and 10 K testing). Maximum f-measure is reached with association rules above the fairly low con-fidence of 0 . 3. The last two columns give the average and median TFIDF for correctly recommended tags. Both val-ues lie in the same range as the corresponding values for the actual tags in the testset (0 . 054 and 0 . 018), which indi-cates that association rules tend to recommend rather gen-eral tags. In an attempt to recommend more specific tags, we have also experimented with a smaller support of 0 . 01. This however only increases recall at the cost of precision; the average and median specificity of recommended tags re-mains in the same range. For a smaller number of available bookmarks, precision goes up and recall goes down, and the f-measure slightly decreases. Average and median TFIDF remain essentially constant (see Table 4). The tag recommendation algorithm is implemented in Java. We used LingPipe [2], to perform the Latent Dirichlet Allo-cation with Gibbs sampling. The LDA algorithm takes three input parameters: the number of terms to represent a latent topic, the number of latent topics to represent a document, and the overall number of latent topics to be identified in the given corpus. After some experiments with varying the first two parameters we fixed them at a value of 100. As described in Section 2.3 we can set a threshold for the
Thresh Prec Rec FM Avg Median TFIDF TFIDF Table 5: Results for tag recommendation using LDA with 100 topics with different thresholds to recom-mend a tag for 5 known bookmarks Table 6: Results for tag recommendation using LDA with 100 topics and threshold 0.01 for 1 X 5 known bookmarks probabilities up to which we recommended tags. Table 5 shows precision, recall, f-measure (FM), as well as average TFIDF and median TFIDF of the  X  X orrectly X  recommended tags. Not surprisingly, precision decreases when lowering the threshold whereas recall increases. We get a maximum f-measure at 0 . 001 of 0 . 401
Table 6 gives detailed results for different numbers of known bookmarks using a threshold of 0.01 to recommend with high precision. Knowing more bookmarks in advance for a resource does not increase precision (2 bookmarks  X  0.717; 5 bookmarks  X  0.717) but increases recall significantly. The average TFIDF gives the expected value for the specificity of a tag whereas the median gives the typical specificity. Be-cause the TFIDF values show a power law distribution, the average is of course larger than the median. Both values are significantly higher for tags recommended by LDA than by association rules, but also higher than the average and median TFIDF of the actual tags present in our tag set. As can be seen in Table 4 and Table 6 the TFIDF values are two to four times higher. Recommending resource specific tags with high TFIDF is particularily useful for search as pointed out in [9], fairly infrequent tags are usually used for topical and type annotations.

The results for varying the number of latent topics are shown in Table 7. The f-measure is shown for 50, 100, 250, and 500 latent topics. The number of bookmarks (#BM) in-dicates the number of users that have annotated a resource in the test set. The threshold for our recommendation was set to 0 . 001. As can be seen in the table, performance de-creases with the LDA topic size for the 1 BM case. This effect is reversed when adding more bookmarks. A small number of topics typically leads to fairly general topics that are mixtures of more specific subtopics. Such general topics have a higher chance to be evoked by one of the few tags in one bookmark, leading to a higher recall. With more bookmarks, there are more tags, and it is more beneficial to separate the general topics into more specific topics. 100 LDA topics give the best average results. www.connotea.org Table 7: F-measure for different sized test set and different number of LDA topics (threshold 0.001)
Table 8 shows the actual tag distribution for a randomly selected resource ( http://www.connotea.org ), the top tags recommended by LDA with aggregated probabilities, and (all) the tags recommended by association rules based on five known bookmarks. The tags available in the known bookmarks (first column) 5 and the correctly recommended tags (forth and sixth column) are marked in bold. As the actual tags indicate, Connotea is a tagging site focusing on scientists and scientific resources. The tags recommended by LDA come from five latent topics, comprising social sys-tems, tagging, science, business, and language. These tags characterize Connotea quite well, and accordingly among the nine most likely recommended tags, there is only one rather general tag (business) that is not among the actual tags. In contrast, the tags recommended by association rules hardly characterize the site, but are rather non descriptive and general.
To evaluate the effectiveness of our recommended tags for tag search we compared three result lists: The first is based on the testset with only 1  X  5 bookmarks per resource, the
The first five bookmarks contain three more tags with rather low TF: webware, management, and social software. second uses the tags recommended by our algorithm. These two lists are compared with the list based on all original tags assigned to the test set. For the ranking of the results in each list, we implemented a simple baseline algorithm based on single keyword search. The resources are weighted according to the TFIDF score of the query tag. E.g. a search for the keyword  X  X eb X  gives a list with resources annotated with the tag  X  X eb X . The list is ranked according to tag frequency, i.e., how high is the number of  X  X eb X -tags compared to the overall number of tags assigned to a resource.
 The testset without recommended tags is also ranked by TFIDF, whereas the testset with recommended tags is ranked by the probability assigned by LDA. To compare the three ranked lists, we need to first decide which of the baseline results are considered relevant. We report scores for taking the top 10 and the top 20 resources as relevant results. A well known measure for comparing rankings in information retrieval is Mean Average precision (MAP) [22], computed as follows: with R jk the set of ranked results from the top of the list down to item k in the list, where the set of relevant items is { i 1 ...i m j } . If no relevant document is retrieved, precision is taken to be 0.

Table 9 shows the MAP values based on the number of known bookmarks. When considering the top 10 TFIDF ranked results as relevant, extension of the resources with our recommended tags increases MAP by more than 300% for one known bookmark. When considering the top 20 re-sults of the baseline algorithm as relevant, the MAP score for the LDA probabilities weighted ranked list increases by more then 400% in the one bookmark case. Table 9: Mean Average Precision (MAP) for tag search with and without extention of recommended tags
Tag recommendation has received considerable interest in recent years. Most work has focused on personalized tag recommendation, suggesting tags to the user bookmarking a new resource: This is often done using collaborative filter-ing, taking into account similarities between users, resources, and tags. [25] introduces an approach to recommend tags for weblogs, based on similar weblogs tagged by the same user. Chirita et al. [11] realize this idea for the personal desktop, recommending tags for web resources by retrieving and ranking tags from similar documents on the desktop. [31] aims at recommending a few descriptive tags to users by rewarding co-occuring tags that have been assigned by the same user, penalizing co-occuring tags that have been assigned by different users, and boosting tags with high de-scriptiveness (TFIDF). As pointed out in Section 2.2, penal-izing co-occuring tags assigned by different users in an effort to recommend personalized tags is in contrast to using tag association rules to recommend general tags to improve re-call for search. Sigurbj  X  ornsson and van Zwol [28] also look at co-occurence of tags to recommend tags based on a user de-fined set of tags. The co-occuring tags are then ranked and promoted based on e.g. descriptiveness. Jaeschke et al. [20] compare two variants of collaborative filtering and Folkrank, a graph based algorithm for personalized tag recommenda-tion. For collaborative filtering, once the similarity between users on tags, and once the similarity between users on re-sources is used for recommendation. Folkrank uses random walk techniques on the user-resource-tag (URT) graph based on the idea that popular users, resources, and tags can re-inforce each other. These algorithms take co-occurrence of tags into account only indirectly, via the URT graph. Syme-onidis et al. [30] employ dimensionality reduction to per-sonalized tag recommendation. Whereas [20] operate on the URT graph directly, [30] use generalized techniques of SVD (Singular Value Decomposition) for n-dimensional tensors. The 3 dimensional tensor corresponding to the URT graph is unfolded into 3 matrices, which are reduced by means of SVD individually, and combined again to arrive at a more dense URT tensor approximating the original graph. Tag recommendation then suggests tags to users, if their weight is above some threshold. An interactive approach is pre-sented in [14]. After the user enters a tag for a new resource, the algorithm recommends tags based on co-occurence of tags for resources which the user or others used together in the past. After each tag the user assigns or selects, the set is narrowed down to make the tags more specific. In [27], Shepitsen et al. propose a resource recommendation system based on hierarchical clustering of the tag space. The rec-ommended resources are identified using user profiles and tag clusters to personalize the recommendation results. Us-ing LDA topic models to recommend resources rather than tags is subject for future work.

An approach to collective tag recommendation based on association rule mined from the resource tag matrix has been introduced in [18]. As discussed in Section 3.2, this ap-proach recommends rather general tags with low TFIDF, and achieves smaller recall and precision than the approach based on LDA introduced in this paper. When content of resources is available, tag recommendation can also be ap-proached as a classification problem, predicting tags from content. A recent approach in this direction is presented in [29]. They cluster the document-term-tag matrix after an approximate dimensionality reduction, and obtain a ranked membership of tags to clusters. Tags for new resources are recommended by classifying the resources into clusters, and ranking the cluster tags accordingly.

Tags have been proven to be very useful for search: in case of image search where content based features are very difficult to extract [12], in case of enterprise search where not enough link information is available [13], or in case of web search to optimize results [3]. A large scale evaluation of Delicious regarding search is presented in [17]. They found that 50% of the pages annotated by a particular tag contain the tag within the page X  X  content. Bischoff et al. [9] provide an indepth analysis of a number of tagging systems with respect to to their usefulness for search. They observe that descriptive tags such as topic or type tags are much more frequent than personal tags such as  X  X o read X , especially in the mid and low tag frequency range, and that these tags are indeed used in search. Berendt and Hanser [6] argue that tags can be considered content and not just metadata which makes them valuable in a content based document retrieval scenario as well.

Recently a number of papers deal with imroving search in tagging systems. Krestel and Chen [21] propose a method to measure the quality of tags with respect to the annotated resource to identify high quality tags that describe a re-source better than others. Hotho et al. [19] propose exploit-ing co-ocurrence of users, resources, and tags for searching and ranking within tagging systems.  X  X olkRank X  is using a graph model to represent the folksonomy and can be used to rank classical keyword search results. In [5], Begelman et al. present a tag clustering algorithm to improve search. The setting is similar to ours: Related tags are identified that can be used for extending existing resource annotations, query expansion or result clustering. The clustering is based on simple co-occurence counts. Unfortunately, the paper does not contain a sound evaluation of the results. Schenkel et al. [26] propose to improve search in tagging systems by ex-panding a user query with semantically similar tags and rank the result additionally based on a social component, which means that tagging information of friends of a user in the network is taken into account when a user submits a query.
In this paper we have investigated the use of Latent Dirich-let Allocation for collective tag recommendation. Compared to association rules, LDA achieves better accuracy, and in particular recommends more specific tags, which are more useful for search. In general, our LDA-based approach is able to elicit a shared topical structure from the collabo-rative tagging effort of multiple users, whereas association rules are more focused on simple terminology expansion. However, both approaches succeed to some degree in over-coming the idiosyncracies of individual tagging practices.
For future work we are interested to see whether it is ben-eficial to combine association rules and LDA. As we showed in Section 3.2 the tags that are recommended by both algo-rithms differ significantly from each other. Our hypothesis is that accuracy can be improved by combining the more gen-eral tags recommended by association rules with the more specific tags recommended by LDA. Along similar lines, we also plan to investigate combining language models derived from the actual tags annotated to a resource with the latent topic models.

The main contribution of latent topic models is to reduce sparsity of the tag space. This gives rise to several interest-ing lines of research we will investigate: Mapping resources to their latent topics may result in more robust resource recommendation. Eliciting latent topics from the tagging practices of individual users and combining them with the latent topics for resources is a promising direction for per-sonalized tag recommendation. Finally, we will experiment with using the probability of tags derived from topic mod-els for visualizing tag recommendations in the form of tag clouds.

Regarding data sets, we also want to experiment with datasets from different domains, to check whether photo, video, or music tagging sites show different system behavior influencing our algorithms.
This work was supported in part by the EU project IST 45035 -Platform for searcH of Audiovisual Resources across Online Spaces (PHAROS). [1] R. Agrawal, T. Imielinski, and S. A. Mining [2] Alias-i. Lingpipe 3.7.0. [3] S. Bao, G.-R. Xue, X. Wu, Y. Yu, B. Fei, and Z. Su. [4] V. Batagelj and M. Zaversnik. Generalized cores. [5] G. Begelman, P. Keller, and F. Smadja. Automated [6] B. Berendt and C. Hanser. Tags are not metadata, but [7] I. Bhattacharya and L. Getoor. A latent dirichlet [8] I. B  X  X r  X o, D. Sikl  X osi, J. Szab  X o, and A. A. Bencz  X ur. [9] K. Bischoff, C. S. Firan, W. Nejdl, and R. Paiu. Can [10] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [11] P. A. Chirita, S. Costache, W. Nejdl, and [12] R. Datta, W. Ge, J. Li, and J. Wang. Toward bridging [13] P. A. Dmitriev, N. Eiron, M. Fontoura, and E. J. [14] N. Garg and I. Weber. Personalized, interactive tag [15] S. Golder and B. A. Huberman. Usage patterns of [16] T. L. Griffiths and M. Steyvers. Finding scientific [17] P. Heymann, G. Koutrika, and H. Garcia-Molina. Can [18] P. Heymann, D. Ramage, and H. Garcia-Molina. [19] A. Hotho, R. J  X  aschke, C. Schmitz, and G. Stumme. [20] R. J  X  aschke, L. B. Marinho, A. Hotho, [21] R. Krestel and L. Chen. The art of tagging: [22] C. D. Manning, P. Raghavan, and H. Sch  X  utze. [23] C. Marlow, M. Naaman, D. Boyd, and M. Davis. [24] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and [25] G. Mishne. Autotag: a collaborative approach to [26] R. Schenkel, T. Crecelius, M. Kacimi, S. Michel, [27] A. Shepitsen, J. Gemmell, B. Mobasher, and R. D. [28] B. Sigurbj  X  ornsson and R. van Zwol. Flickr tag [29] Y. Song, Z. Zhuang, H. Li, Q. Zhao, J. Li, W.-C. Lee, [30] P. Symeonidis, A. Nanopoulos, and Y. Manolopoulos. [31] Z. Xu, Y. Fu, J. Mao, and D. Su. Towards the
