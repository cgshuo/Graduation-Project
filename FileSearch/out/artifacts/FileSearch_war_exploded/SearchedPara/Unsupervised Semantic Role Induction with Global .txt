 Unsupervised semantic role induction has gained significant interest recently (Lang and Lapata, 2011b) due to limited amounts of annotated corpora. A Semantic Role Labeling (SRL) system should provide consistent argument labels across different syntactic realizations of the same verb (Palmer et al., 2005), as in ( a. ) [ Mark ] A 0 drove [ the car ] A 1 ( b. ) [ The car ] A 1 was driven by [ Mark ] A 0
This simple example also shows that while certain local syntactic and semantic features could provide clues to the semantic role label of a constituent, non-local features such as predicate voice could provide information about the expected semantic role se-quence. Sentence a is in active voice with sequence voice with sequence ( A 1 , P REDICAT E , A 0 ) . Addi-tional global preferences, such as arguments A 0 and A 1 rarely repeat in a frame (as seen in the corpus), could also be useful in addition to local features.
Supervised SRL systems have mostly used local classifiers that assign a role to each constituent inde-pendently of others, and only modeled limited cor-relations among roles in a sequence (Toutanova et al., 2008). The correlations have been modeled via role sets (Gildea and Jurafsky, 2002), role repeti-tion constraints (Punyakanok et al., 2004), language model over roles (Thompson et al., 2003; Pradhan et al., 2005), and global role sequence (Toutanova et al., 2008). Unsupervised SRL systems have ex-plored even fewer correlations. Lang and Lapata (2011a; 2011b) use the relative position (left/right) of the argument w.r.t. the predicate. Grenager and Manning (2006) use an ordering of the linking of se-mantic roles and syntactic relations. However, as the space of possible linkings is large, language-specific knowledge is used to constrain this space.

Similar to Toutanova et al. (2008), we propose to use global role ordering preferences but in a gener-ative model in contrast to their discriminative one. Further, unlike Grenager and Manning (2006), we do not explicitly generate the linking of semantic roles and syntactic relations, thus keeping the pa-rameter space tractable. The main contribution of this work is an unsupervised model that uses global role ordering and repetition preferences without as-suming any language-specific constraints.

Following Gildea and Jurafsky (2002), previous work has typically broken the SRL task into (i) argu-ment identification, and (ii) argument classification (M`arquez et al., 2008). The latter is our focus in this work. Given the dependency parse tree of a sentence with correctly identified arguments, the aim is to as-sign a semantic role label to each argument. Algorithm 1 Generative process We assume the roles to be predicate-specific. We begin by introducing a few terms: Primary Role (PR) For every predicate, we assume the existence of K primary roles (PRs) denoted by , P 2 , ..., P K . These roles are not allowed to re-peat in a frame and serve as  X  X nchor points X  in the global role ordering. Intuitively, the model attempts to choose PRs such that they occur with high fre-quency, do not repeat, and their ordering influences the positioning of other roles. Note that a PR may correspond to either a core role or a modifier role. For ease of explication, we create 3 additional PRs: ST ART denoting the start of the role sequence, END denoting its end, and P RED denoting the predicate. Secondary Role (SR) The roles that are not PRs are called secondary roles (SRs). Given N roles in total, there are ( N  X  K ) SRs, denoted by S Unlike PRs, SRs are not constrained to occur only once in a frame and do not participate in the global role ordering.
 Interval An interval is a sequence of SRs bounded by PRs, for instance ( P Ordering An ordering is the sequence of PRs ob-served in a frame. For example, if the complete role Figure 1: Proposed model. Shaded and unshaded nodes represent visible and hidden variables resp. ordering is defined as ( ST ART , P Features We have explored 1 frame level (global) feature (i) voice : active/passive, and 3 argument level (local) features (i) deprel : dependency relation of an argument to its head in the dependency parse tree, (ii) head : head word of the argument, and (iii) pos-head : Part-of-Speech tag of head .

Algorithm 1 describes the generative story of our model and Figure 1 illustrates it graphically. Given a predicate and its voice, an ordering is selected from a multinomial. This ordering gives us the sequence of PRs ( P R utive PRs, P R to an interval I 0 or more SRs ( SR Generate an indicator variable: CONT INUE/ST OP from a binomial distribution. If CONT INUE , gen-erate a SR from the multinomial corresponding to the interval. Generate another indicator variable and continue the process till a ST OP has been generated. In addition to the interval, the indicator variable also depends on whether we are generating the first SR ( adj = 0 ) or a subsequent one ( adj = 1 ). For each role, primary as well as secondary, we now generate the corresponding constituent by generating each of its features independently ( F
Given a frame instance with predicate p and voice vc , Figure 2 gives (i) Eq. 1: the joint distribution of the ordering o , role sequence r , and constituent sequence f , and (ii) Eq. 2: the marginal distribution of an instance. The likelihood of the whole corpus is the product of marginals of individual instances.
P ( f i | r i , p ) where P ( r ( I ) , f ( I ) | I, p ) = Y
P ( f | p, vc ) =  X  o  X  { Figure 2: r denote the SR sequence and feature sequence in interval I . f i,t denotes the value of feature t at position i .
This particular choice of model is inspired from different sources. Firstly, making the role order-ing dependent only on PRs aligns with the obser-vation by Pradhan et al. (2005) and Toutanova et al. (2008) that including the ordering information of only core roles helped improve the SRL perfor-mance as opposed to the complete role sequence. Although our assumption here is softer in that we assume the existence of some roles which define the ordering which may or may not correspond to core roles. Secondly, generating the SRs indepen-dently of each other given the interval is based on the intuition that knowing the core roles informs us about the expected non-core roles that occur be-tween them. This intuition is supported by the statis-tics in the annotated data, where we found that if we consider the core roles as PRs, then most of the in-tervals tend to have only a few types of SRs and a given SR tends to occur only in a few types of in-tervals. The concept of intervals is also related to the linguistic theory of topological fields (Diderich-sen, 1966; Drach, 1937). This simplifying assump-tion that given the PRs at the interval boundary, the SRs in that interval are independent of the other roles in the sequence, keeps the parameter space lim-ited, which helps unsupervised learning. Thirdly, not allowing some or all roles to repeat has been employed as a useful constraint in previous work (Punyakanok et al., 2004; Lang and Lapata, 2011b), which we use here for PRs. Lastly, conditioning the
ST OP/CONT INUE ) indicator variable on the adja-cency value ( adj ) is inspired from the DMV model (Klein and Manning, 2004) for unsupervised depen-dency parsing. We found in the annotated corpus that if we map core roles to PRs, then most of the time the intervals do not generate any SRs at all. So, the probability to ST OP should be very high when generating the first SR.

We use an EM procedure to train the model. In the E-step, we calculate the expected counts of all the hidden variables in our model using the Inside-Outside algorithm (Baker, 1979). In the M-step, we add the counts corresponding to the Bayesian priors to the expected counts and use the resulting counts to calculate the MAP estimate of the parameters. Following the experimental settings of Lang and La-pata (2011b), we use the CoNLL 2008 shared task dataset (Surdeanu et al., 2008), only consider ver-bal predicates, and run unsupervised training on the standard training set. The evaluation measures are also the same: (i) Purity (PU) that measures how well an induced cluster corresponds to a single gold role, (ii) Collocation (CO) that measures how well a gold role corresponds to a single induced cluster, and (iii) F1 which is the harmonic mean of PU and CO. Final scores are computed by weighting each predicate by the number of its argument instances. We chose a uniform Dirichlet prior with concentra-tion parameter as 0.1 for all the model parameters 50 training iterations were used. 3.1 Results Since the dataset has 21 semantic roles in total, we fix the total number of roles in our model to be 21. Further, we set the number of PRs to 2 (excluding ST ART , END and P RED ), and SRs to 21-2=19. Table 1: Evaluation. d refers to deprel , h refers to head and p-h refers to pos-head .

Table 1 gives the results using different feature combinations. Line 0 reports the performance of Lang and Lapata (2011b) X  X  baseline, which has been shown difficult to outperform. This baseline maps 20 most frequent deprel to a role each, and the rest are mapped to the 21 st role. By just using deprel as a feature, the proposed model outperforms the base-line by 0.6 points in terms of F1 score. In this con-figuration, the only addition over the baseline is the ordering model. Adding head as a feature leads to sparsity, which results in a substantial decrease in collocation (lines 1b and 1d). However, just adding pos-head (line 1c) does not cause this problem and gives the best F1 score. To address sparsity, we in-duced a distributed hidden representation for each word via a neural network, capturing the semantic similarity between words. Preliminary experiments improved the F1 score when using this word repre-sentation as a feature instead of the word directly.
Lang and Lapata (2011b) give the results of three methods on this task. In terms of F1 score, the La-tent Logistic and Graph Partitioning methods result in slight reduction in performance over the baseline, while the Split-Merge method results in an improve-ment of 0.6 points. Table 1, line 1c achieves an im-provement of 1.1 points over the baseline. 3.2 Further Evaluation Table 2 shows the variation in performance w.r.t. uration (Table 1, line 1c). On one extreme, when there are 0 PRs, there are only two possible in-means that the only context information a SR has is whether it is to the left or right of the predicate. Table 2: Performance variation with the number of PRs (excluding ST ART , END and P RED ) With only this additional ordering information, the performance is the same as the baseline. Adding just 1 PR leads to a big increase in both purity and col-location. Increasing the number of PRs beyond 1 leads to a gradual increase in purity and decline in collocation, with the best F1 score at 2 PRs. This behavior could be explained by the fact that increas-ing the number of PRs also increases the number of intervals, which makes the probability distributions more sparse. In the extreme case, where all the roles are PRs and there are no SRs, the model would just learn the complete sequence of roles, which would make the parameter space too large to be tractable.
For calculating purity, each induced cluster (or role) is mapped to a particular gold role that has the maximum instances in the cluster. Analyzing the output of our model (line 1c in Table 1), we found that about 98% of the PRs and 40% of the SRs got mapped to the gold core roles ( A 0 , A 1 , etc.). This suggests that the model is indeed following the intu-ition that (i) the ordering of core roles is important information for SRL systems, and (ii) the intervals bounded by core roles provide good context infor-mation for classification of other roles. We propose a unified generative model for unsu-pervised semantic role induction that incorporates global role correlations as well as local feature infor-mation. The results indicate that a small number of ordered primary roles (PRs) is a good representation of global ordering constraints for SRL. This repre-sentation keeps the parameter space small enough for unsupervised learning.
 This work was funded by the Swiss NSF grant 200021 125137 and EC FP7 grant PARLANCE.
