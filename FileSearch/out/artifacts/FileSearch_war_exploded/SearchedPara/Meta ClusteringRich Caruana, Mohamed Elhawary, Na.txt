
Clustering is ill-defined. Unlike supervised learning where labels lead to crisp performance criteria such as ac-curacy and squared error, clustering quality depends on how the clusters will be used. Devising clustering crite-ria that capture what users need is difficult. Most clus-tering algorithms search for optimal clusterings based on a pre-specified clustering criterion. Our approach differs. We search for many alternate clusterings of the data, and then allow users to select the clustering(s) that best fit their needs. Meta clustering first finds a variety of clusterings and then clusters this diverse set of clusterings so that users must only examine a small number of qualitatively different clusterings. We present methods for automatically generat-ing a diverse set of alternate clusterings, as well as meth-ods for grouping clusterings into meta clusters. We evaluate meta clustering on four test problems and two case studies. Surprisingly, clusterings that would be of most interest to users often are not very compact clusterings.
Clustering performance is difficult to evaluate [30]. In supervised learning, model performance is assessed by comparing model predictions to targets. In clustering we do not have targets and usually do not know a priori what groupings of the data are best. This hinders discerning when one clustering is better than another, or when one cluster-ing algorithm outperforms another. To make matters worse, clustering often is applied early during data exploration, be-fore users understand the data well enough to define suitable clustering criteria. This creates a chicken-or-the-egg prob-lem where knowing how to define a good clustering crite-rion requires understanding the data, but clustering is one of the principal tools used to help understand the data.
This fundamental differences between supervised and unsupervised learning have profound consequences. In particular, while it makes sense to talk about the  X  X est X  model(s) in supervised learning (e.g. the most accurate model(s)), often it does not make sense to talk about the  X  X est X  clustering. Consider a database containing infor-mation about people X  X  age, gender, education, job history, spending patterns, debts, medical history, etc. Clustering could be applied to the database to find groups of similar people. A user who wants to find groups of consumers who will buy a car probably wants different clusters than a medical researcher looking for groups with high risk of heart disease. In exploring the same data, different users want different clusterings. No  X  X orrect X  clustering exists. Moreover, theoretical work suggests that it is not possible to achieve all of the properties one might desire of cluster-ing in a single clustering of the data [21].

Most clustering methodologies focus on finding optimal or near-optimal clusterings, according to specific clustering criteria. However, this approach often is misguided. When users cannot specify appropriate clustering criteria in ad-vance, effort should be devoted to helping users find appro-priate clustering criteria. In practice, users commonly be-gin by clustering their data and examining the results. They then make educated guesses about how to change the dis-tance metrics or algorithm in order to yield a more useful clustering. Such a search is tedious and may miss interest-ing partitionings of the data.

In this paper we introduce meta clustering , a new ap-proach to the problem of clustering. Meta clustering aims at creating a new mode of interaction between users, the clustering system, and the data. Rather than finding one optimal clustering of the data, meta clustering finds many alternate good clusterings of the data and allows the user to select which of these clusterings is most useful, explor-ing the space of reasonable clusterings. To prevent the user from having to evaluate too many clusterings, the many base-level clusterings are organized into a meta clustering, a clustering of clusterings that groups similar base-level clus-terings together. This meta clustering makes it easier for users to evaluate the clusterings and efficiently navigate to the clustering(s) useful for their purposes.

Meta clustering consists of three steps. First, a large number of potentially useful clusterings is generated. Then a distance metric over clusterings measures the similarity between pairs of clusterings. Finally, the clusterings are themselves clustered at the meta level using the computed pairwise similarities. The clustering at the meta level al-lows the user to select a few representative yet qualitatively different clusterings for examination. If one of these clus-terings is appropriate for the task at hand, the user may then examine other nearby clusterings in the meta level space.
An analogy may be helpful. Photoshop, the photo edit-ing software, has a tool called  X  X ariations X  that presents to the user different renditions of the picture that have differ-ent color balances, brightnesses, contrasts, and color satu-rations. Instead of having to know exactly what tool to use to modify the picture (which requires substantial expertise), the user only has to be able to select the variation that looks best. The selected variation then becomes the new center, and variations of it are presented to the user. This process allows users to quickly zero in on the desired image ren-dition. The goal in meta clustering is to provide a similar  X  X ariations X  tool for clustering so that users do not have to know how to modify distance metrics and clustering algo-rithms to find useful clusterings of their data. Instead, meta clustering presents users with an organized set of clustering variations; users can select and then refine the variation(s) that are best suited to their purposes.

The paper proceeds as follows. Section 2 defines meta clustering. Section 2.1 describes how to generate diverse high-quality clusterings. Section 2.2 describes how to mea-sure similarity between clusterings to cluster clusterings at the meta level. Section 3 describes four data sets used for evaluation, and Section 4 presents the empirical results. Section 5 presents a protein clustering case study. Section 6 presents a phoneme clustering case study. Section 7 covers the related work. Section 8 is a discussion and summary.
The approach to meta clustering presented in this pa-per is a sampling-based approach that searches for distance metrics that yield the clusterings most useful to the user. Algorithmic (i.e. non-stochastic) approaches to meta clus-tering are possible and currently are being developed.
Here we break meta clustering into three steps: 1. Generate many good, yet qualitatively different, base-2. Measure the similarity between the base-level cluster-3. Organize the base-level clusterings at a meta level (ei-
These steps are described in the rest of this section.
The key insight behind meta clustering is that in many applications, data may be clustered into a variety of alter-nate groupings, each of which may be beneficial for a dif-ferent purpose. To be useful, the alternate clusterings can-not be random partitions of the data, but must reflect gen-uine structure within the data . We follow two approaches to generate a diverse set of quality clusterings. In the first, we note that k-means generates many different reasonable clus-terings (all but the  X  X est X  of which are typically discarded) because different random initializations of k-means often get stuck in different local minima. In the second approach, we apply random weights to feature vectors before cluster-ing the data with k-means to emphasize different aspects of the data. These approaches for finding diverse clusterings are described in the remainder of Section 2.1. 2.1.1 Diverse Clusterings from K-Means Minima K-means is an iterative refinement algorithm that attempts to minimize a squared error criterion [11]. Each cluster is initialized by setting its mean to a random point in the data set. Each step of the iterative refinement performs two tasks. First, the data points are classified as being a member of the cluster with the nearest cluster mean. Second, the cluster means are updated to be the actual mean of the data points in each cluster. This is repeated until no points change mem-bership or for some maximum number of iterations. When no points change membership, k-means is at a local mini-mum in the search space: there is no longer a move that can reduce the squared error. The output of k-means is typically highly dependent on the initialization of the cluster means: the search space has many local minima [3, 5]. In practice, k-means is run many times with many different initializa-tions, and the clustering with the smallest sum-of-squared distances between cluster means and cluster members is re-turned as the final result.

In meta clustering, however, we are interested in generat-ing a wide variety of reasonable clusterings. The local min-ima of k-means provide a set of easily-attainable cluster-ings, each of which is reasonable since no point can change membership to improve the clustering. K-means can be run many times with many different random initializations, and each local minimum can be recorded. As we shall see in Section 4.2, the space of k-means local minima is small compared to the space of reasonable clusterings, so we use an additional means of generating diverse clusterings: ran-dom feature weighting. 2.1.2 Diverse Clusterings from Feature Weighting Consider data in vector format. Each item is described by a vector of features, and each dimension in the vector is
Figure 1. Zipf Distribution. Each row visual-izes a Zipf distribution with a different shape parameter,  X  . Each row has 50 bars repre-senting 50 random samples from the distri-bution, with the height of the bar proportional to the value of the sample. a feature that will be used when calculating the similar-ity of points for clustering. By weighting features before distances are calculated (i.e. multiplying feature values by particular scalars), we can control the importance of each feature to clustering [34]. Clustering many times with dif-ferent random feature weights allows us to find qualitatively different clusterings for the data using the same clustering algorithm.

Feature weighting requires a distribution to generate the random weights. We consider both uniform and power law distributions. Empirically, uniformly distributed weights often do not explore the weight space thoroughly. Consider the case where only a few features contain useful informa-tion, while the others are noise. It is unlikely that a uni-form distribution would generate values that weight the few important variables highly while assigning low weights to the majority of the variables, while weights generated from a power law distribution can weight only a few variables highly.

We will use a Zipf power law distribution because there is empirical evidence that feature importance is Zipf-distributed in a number of real-world problems [8, 15]. A Zipf distribution describes a range of integer values from 1 to some maximum value K . The frequency of each integer is proportional to 1 i  X  where i is the integer value and  X  is a shape parameter. Thus, for  X  = 0 , the Zipf distribution be-comes a uniform distribution from 1 to K . As  X  increases, the distribution becomes more biased toward smaller num-bers, with only the occasional value approaching K . See Figure 1. Random values from a Zipf distribution can be generated in the manner of [7].
 Algorithm 1 : Generate a diverse set of clusterings
Input : X = { x 1 , x 2 , ..., x n } for x i  X  R d , k is the
Output : A set of m alternate clusterings of the data begin end
Algorithm 1 is the procedure that generates different clusterings. First the Zipf shape parameter,  X  , is drawn uni-formly from the interval [0 ,  X  max ] . Here we use  X  max 1 . 5 . (This allows us to sample the space of random weight-ings, from a uniform distribution (  X  = 0 ) to a severe dis-tribution (  X  = 1 . 5 ) that gives significant weight to just a few variables.) Then a weight vector w  X  R d is generated according to the Zipf distribution with that  X  . Next the fea-tures in the original data set are weighted with the weight vector w . Finally, k-means is used to cluster the feature re-weighted data set. 2.1.3 The Problem With Correlated Features Random feature weights may fail to create diverse cluster-ings in the presence of correlated features: weights given to one feature can be compensated by weights given to other correlated features.

The problem with correlated features can be avoided by applying Principal Component Analysis [9] to the data prior to weighting. Sometimes PCA yields a more interesting set of distance functions by compressing important aspects of the problem into a small set of components. Other times, however, PCA hides important structure. Because of this, we apply random feature weightings both before and after rotating the vector space with PCA. See [6] for a longer version of this paper that includes a thourough description of the use of PCA in meta clustering. 2.1.4 Dealing with Non-vector Data Feature weighting only works for data in feature-vector for-mat. Data often is available only as pairwise similarities. This problem can be solved using MultiDimensional Scal-ing (MDS) [11]. MDS transforms pairwise distances to a feature-vector format to which random weights can then be applied. We implement MDS following [11]. In order to avoid local minima and speed the search, we utilize a vari-able step size and randomize the locations of the points if progress becomes slow or halts.
The methods in the preceding section generate a large, diverse set of candidate clusterings. Usually it is infeasi-ble for a user to examine thousands of clusterings to find a few that are most useful for the application at hand. To avoid overwhelming the user, meta clustering groups sim-ilar clusterings together by clustering the clusterings at a meta level. To do this, we need a similarity measure be-tween clusterings. 2.2.1 Measuring the Similarity Between Clusterings Several measures of clustering similarity have been pro-posed in the literature [18, 19, 20]. Here we use a measure of clustering similarity related to the Rand index [29]: de-fine I ij as 1 if points i and j are in the same cluster in one clustering, but in different clusters in the other clustering, and I ij is 0 otherwise. The dissimilarity of two clustering models is defined as: where N is the total number of data points. This measure is a metric as well. In the remainder of this paper we refer to it as Cluster Difference. 2.2.2 Agglomerative Clustering at the Meta Level Once the distances between all pairs of clusterings are found using the Cluster Difference metric, the clusterings are themselves clustered at the meta level. This meta clustering can be performed using any clustering algorithm that works with pairwise similarity data. We use agglomerative cluster-ing at the meta level because it works with similarity data, because it does not require the user to prespecify the number of clusters, and because the resulting hierarchy makes nav-igating the space of clusterings easier. ([14] presents one of the few studies to examine the tradeoffs between clustering complexity, efficiency, and interpretability.)
An alternate approach for presenting different cluster-ings to users is to find a low-dimensional projection of the pairwise clustering distances. For example, MDS can be used to find principal components of the meta clustering space from the clustering similarity data. The clusterings can then be presented to the user in a low dimensional plot where similar clusterings are positioned close to each other. One problem we have found with this approach is that often the meta level clustering space is not low in dimension, so any low-dimensional projection of the data has significant distortion. Although easier to visualize, this makes the 2-D projections less useful than the meta clustering tree.
We evaluate meta clustering on six data sets. For each data set we also classify points using labels external to the clustering. We will call these test classifications the auxil-iary labels. The labels are intended as an objective proxy for what users might consider to be good clusterings for their particular application. In practice, users may have only a vague idea of the desired clustering (and thus may not be able to provide the constraints necessary for semi-supervised clustering [10, 33]). Or users may have no idea what to expect from the clustering. The auxiliary labels are meant to represent one clustering users might find useful. In no way are they intended to represent an exclusive ground truth for the clustering. If such a classification existed, su-pervised learning would be more appropriate. Instead, the auxiliary labels are intended to represent a clustering that is good for one application, while acknowledging many appli-cations with other good clusterings exist.

The Australia Coastal data is a subset of the data avail-able from the Biogeoinformatics of Hexacorals environ-mental database [28]. The data contain measurements from the Australia coastline at every half-degree of longitude and latitude. The features describe environmental properties of each grid cell such as temperature, salinity, rainfall, and soil moisture. Each variable was scaled to have a mean of zero and a mean absolute deviation of one. The auxiliary la-bels are based on the  X  X errestrial Ecoregions of the World X  available from [16].

The Bergmark data was collected from 25 focused web crawls, each with different keywords. The variables are counts in a bags-of-words model for the pages. The aux-iliary labels are the 25 web crawls that generated the data. The Covertype data is from the UCI Machine Learning Repository [27]. It contains cartographic variables sampled at 30  X  30 meter grid cells in wilderness areas. Data was scaled to a mean of zero and a standard deviation of one. The true forest cover type for each cell is used as the auxil-iary labels.

The letters data is a subset of the isolet spoken letter data set from the UCI Machine Learning Repository [27]. We took random utterances of the letters A, B, C, D, F, H, and J in these proportions: A: 109 B: 56 C: 126 D: 59 F: 49 H: 62 J: 53. Each utterance is described by spectral co-efficients, contour features, sonarant features, pre-sonarant features, and post-sonarant features. The spoken letters are the auxiliary labels.

The Protein data is the pairwise similarities between 639 proteins. It was created by crystallographers developing techniques to learn relationships between protein sequence and structure. This data is used as a case study in Section 5. The Phoneme data is from the UCI Machine Learning Repository [27]. It records the 11 phonemes of 15 speakers. This data is used as a case study in Section 6.

See Table 1 for a summary of the data sets.
In this section we present empirical results on four test problems used to develop meta clustering. First, in Sec-tion 4.1 we show the effect of Zipf random weighting of feature vectors. In Section 4.2 we compare k-means with weighted feature vectors to the local minima found by k-means on unweighted data. Then in Section 4.4 we show the results of agglomerative hierarchical clustering at the meta level. The results demonstrate the importance of generating a diverse set of clusterings when the clustering objective is not well defined prior to clustering.

We examine two clustering performance metrics. The first is compactness. Compactness is defined as: where k is the number of clusters; N i is the number of points in the i th cluster; d jk is the distance between points j and k , and N = P k i =1 N i . Compactness measures the av-erage pairwise distances between points in the same cluster. Regardless of the feature weighting used in clustering, com-pactness is always measured relative to the original data set. In many traditional clustering algorithms such as k-means and agglomerative clustering, the optimization crite-rion is closely related to this measure of compactness.
The second clustering performance metric is accuracy, which is measured relative to the auxiliary labels for each data set. Again, the auxiliary labels are only a proxy for what a specific user might want in a particular application. They do not represent a single  X  X rue X  classification of the data, as different users may desire different clusterings for alternate applications. Accuracy is defined as: where C i is the set of points in the i th cluster; L i is the labels for all points in the i th cluster, and max ( C i | L number of points with the plurality label in the i th cluster (if label l appeared in cluster i more often than any other label, then max ( C i | L i ) is the number of points in C the label l ).

Determining the number of clusters in a data set is chal-lenging [26]. Indeed, the  X  X orrect X  number of clusters de-pends on how the clustering will be used. For simplicity, we make the unrealistic assumption that the desired number of clusters is predefined. It is easy to extend meta clus-tering to explore the number of clusters as well: in addi-tion to k-means local minima and various variable weight-ing, clusterings can be generated with different numbers of clusters. Our similarity metric can accommodate cluster-ings with different numbers of clusters, so the meta-level clustering will still group similar clusterings together, even if they have different numbers of clusters. Using a fixed k helps to demonstrate how effective the methods we present are at generating diverse clusterings.
In Figure 2 each point in the plots represents an en-tire clustering of the data. The scatter plots in each row show clusterings generated by weighting features with Zipf-distributed random weights with different shape parameters  X  for the four test data sets. We test Zipf distributions with  X  = 0.00, 0.25, 0.50, 0.75, 1.00, 1.25, and 1 . 50 .
Note that for different  X  values, feature weightings ex-plore different regions of the clustering space. In all data sets that we test, as the  X  value increases, feature weighting explores a region of lower compactness. We also observe an interesting phenomenon that some of the most accurate clusterings are generated when applying feature weighting with higher  X  values. In particular, high  X  in the Covertype data set reveals a cloud of more accurate (and much less compact) clusterings which do not show up at low  X  val-ues. When  X  = 0 . 00 (1st row) the Zipf distribution yields a uniform distribution. It is clear that a uniform distribution alone is insufficient to explore the clustering space.
The bottom row of Figure 3 shows histograms of the compactness of the clusterings. The most compact clus-terings are on the left of the histograms. The arrow in each figure marks the most accurate clusterings. In Australia and Covertype, the most accurate clusterings are not in the top type, and Letter data sets. Each point is a full clustering. than the most accurate one. 50% of the most compact clusterings. In Bergmark and Letter, the most compact clusterings occur 20-30% down in the distribution. Standard clustering techniques would never find these most accurate clusterings. Looking at the leftmost points in each plot, it is clear that the most com-pact clustering is significantly less accurate than the most accurate clustering. Since there exist reasonable cluster-ings which are more accurate than the most compact clus-tering, we conclude that the usual method of searching for the most compact clustering can be counterproductive. It is important to explore the space of reasonable clusterings more thoroughly.
Figure 4 shows scatter plots of clusterings generated using k-means, comparing weighting features with Zipf distributed random weights versus without using feature weighting for the four test problems. The x-axis is the com-pactness of the clusterings (as defined in Section 4). The y-axis is the accuracy of the clusterings using the auxiliary labels. The top row shows clusterings generated by random Zipf weighting. The bottom row shows the clusterings gen-erated using iterated k-means where each point is the result of a different random initialization.

For Australia, Bergmark, and Letter, weighting features yields more diverse clusterings  X  the figures extend further to the lower right, while retaining points in the upper left. For Covertype, not applying feature weighting fails to dis-cover the cloud of more accurate (yet not compact) clus-terings. For Australia, different random initialization of k-means without feature weighting finds more clusterings in the upper left corner (both accurate and compact).
For a thorough comparison, we also apply other cluster-ing approaches to the four data sets: hierarchical agglom-erative clustering (HAC) [22], EM-based mixture model clustering [23], and two types of spectral clustering [24, 1] (See Figure 4). HAC is implemented using three differ-ent linkage criteria: single (min-link), complete (max-link), and centroid (average-link) [22]. The EM-based mixture model clustering estimates Gaussian mixture models using the EM algorithm. The result for the EM-based mixture model clustering is the highest accuracy from multiple runs with different initialization. For both the spectral clustering methods, we also report the highest accuracy from multiple runs. Spectral clustering requires an input similarity matrix S , where S = exp (  X  X | x i  X  x j || / X  2 ) . To obtain a variety of results, we used different values of the shape parameter, tained from other methods fall within the range of cluster-ings generated from k-means with random feature weight-ing. Across the four data set the spectral method proposed by [1] has the best performance, though it (and the other alternative clustering methods) are always outperformed by at least one clustering found with the metaclustering tech-nique using kmeans for the base-level clusterings. We are currently investigating random feature weighting with other clustering techniques (in addition to k-means) to create di-verse base-level clusterings.

Figure 5. Meta level clustering trees for Aus-tralia (left) and Letter (right). Color (yellow to blue) indicates to accuracy (low to high). (The meta clustering dendrograms are much easier to interpret if viewed in color.)
In this section we cluster the clusterings at the meta level using agglomerative clustering with the  X  X lexible strategy X  criterion described in [22] and show that clusterings near each other at the meta level are similar, and clusterings far from each other at the meta level are qualitatively different.
Figure 5 shows the meta level clustering trees for two data sets, Australia and Letter. Each node in the clustering tree is colored by accuracy. Yellow indicates low accuracy. Blue indicates high accuracy. Note that clusterings of sim-ilar accuracy tend to be grouped together at the meta level even though meta clustering has no information about ac-curacy. The results suggest that groups of clusterings at the meta level reflect aspects of the base-level clusterings that might be important to users.
 Figure 6 shows four representative clusterings of the Australia data set selected from the meta clustering tree. The clusterings on the left were selected as two very similar (but not identical) clusterings from one region of the tree, while the two on the right were selected to be distinct from each other and from the two on the left. Each represents a different, reasonable way of clustering the data set. Once again we see that clusterings close to (far from) each other at the meta level are similar to (different from) each other.
Figure 7 shows compactness (Equation 1) vs. the number of clusters for hierarchical agglomerative clustering at the meta level for the four data sets. These plots can be used to detect structure at the meta level; jumps in the curves indi-cate merges between groups of dissimilar clusterings. The Australia, Covertype, and Letter data sets exhibit signifi-cant structure at the meta level. However, in the Bergmark data set, little structure at the meta level is observed. (Meta clustering can be useful even in the absence of significant structure at the meta level since clustering at the meta level will still group similar clusterings together, making explor-ing the space of clusterings easier.) clustering methods marked with special symbols.

In this section we apply meta clustering to a real protein clustering problem. The protein data consists of pairwise distances between 639 proteins. The distance between each pair of proteins is computed by aligning 3-D structures of each protein and computing the mean distance in angstroms between corresponding atoms in the two structures.
This data was created by crystallographers developing automated techniques to learn relationships between pro-tein sequence and structure. For their work they need to find groups of proteins containing as many proteins with similar structure as possible. To achieve this goal, and also to better understand the data, the developers employed a number of techniques including clustering, MDS, univariate, and mul-tivariate statistical analysis. In the course of their work they generated and tested a large variety of hand-tuned cluster-ing criteria before finding satisfactory clusterings. After a month of effort, the two largest homogeneous groups of proteins (less than 1.25 angstrom mean distance between aligned atoms) discovered contained 28 and 30 proteins.
We apply meta clustering to the same data to try to au-tomatically find sets of proteins similar to those the experts found manually. For this data we only have pairwise dis-tances between proteins, not a feature-space, so we can-not apply random Zipf weighting directly to the original at-tributes. Instead, we use the MDS method described earlier in Section 2.1.4 to convert the pairwise distance matrix to a vector space, and then apply random Zipf weighting.
We apply the meta clustering method described in Sec-tion 2.1.2 using random Zipf weighting with Zipf shapes se-lected uniformly on the range (0.0,1.5). For each weighting we run iterated k-means to find clusterings (local minima). We repeat this process 5000 times, yielding 5000 different clusterings of the protein data. The left plot in Figure 8 shows the number of points in the largest cluster satisfying the 1.25 angstrom constraint, plotted as a function of clus-tering compactness. Note that although the majority of clus-terings found by meta clustering do not have large clusters with 30 or more points, meta clustering has found a number of clusterings that contain clusters with more than 30 points. The largest homogeneous cluster found by meta clustering contains 48 proteins, 60% more than the experts were able to find using manually guided methods. In a few hours meta clustering finds better clusterings than could be found man-ually with a month of work. The compactness histogram in the right of the Figure 8 shows that the  X  X ptimal X  clustering had mediocre overall compactness, falling near the middle of the distribution of clustering compactnesses.

An examination of compactness (see Section 4.4) as a function of the number of clusters for agglomerative clus-tering at the meta level shows large jumps, suggesting struc-ture at the meta level, i.e., meta clustering has found qual-itatively different ways of clustering the protein data that cluster together at the meta level. If users needed cluster-ings that satisfied different criteria, it is likely that groups of alternate clusterings have already been found by meta clustering that would perform well according to these other criteria.

The clustering goal used in this case study (clusterings smaller than 1.25 angstroms containing many points) was not known in advance when crystallographers began work-ing with this data. This criterion emerged only after exam-ining the results of many clusterings. Optimizing directly to this criterion is not straightforward. Meta clustering au-tomatically finds a diverse set of clusterings, a few of which have the desired property, without knowing the criterion in advance and without optimizing directly to that criterion.
In this section we apply meta clustering to a phoneme clustering problem. The data set contains 15 different speakers saying 11 different phonemes 6 times each (for a total of 990 data points). For this data set, we consider users interested in identifying either speakers or phonemes and evaluate the clusterings based on both of these criteria.
Meta clustering successfully finds clusterings that are ac-curate for each criterion. Figure 9 shows the scatter plots of clusterings (top two rows) and the meta level cluster-ing dendrograms (bottom row) colored with respect to the two accuracy measures (identifying speakers and recogniz-ing phonemes). In the scatter plots of compactness and ac-curacy (top row), there is a small cloud of clusterings with high accuracy and medium compactness. If the task were to identify speakers, the most accurate clustering occurs at 38 % in the compactness distribution, i.e. the most accurate clustering for this criterion is not one of the more compact clusterings. For the task of identifying phonemes, the most accurate clustering does not even occur in the top half of the most compact clusterings and falls at 53% in the compact-ness distribution.

In the scatter plot of the two accuracy measures (second row), the task of identifying speakers and the task of identi-fying phonemes are essentially uncorrelated. The most ac-curate clustering for identifying speakers is generated from applying Zipf weighting with the shape parameter set at 0 . 25 . The most accurate clustering of identifying phonemes is generated from applying Zipf weighting with the shape parameter set at 1 . 25 . This confirms the need to sample a variety of Zipf weighting parameters.

For comparison, the scatter plots in Figure 9 show the consensus clustering found using the cluster aggregation method proposed in [25] (marked with a  X + X  in the fig-ures). This is the clustering that represents the consensus of all found clusterings. As expected, the consensus clus-tering is very compact (because less compact clusterings of-ten disagree with each other, but the most compact cluster-ings often agree thus forming a strong consensus). Note, however, that the consensus clustering is not as compact as the most compact clusterings found by meta clustering, and also not very accurate on either the speaker identification or phoneme recognition tasks.

Again we use agglomerative clustering at the meta level to group similar clusterings. Figure 9 shows two copies of the same meta level clustering dendrogram (bottom row) colored by accuracy on the speaker identification and phoneme recognition tasks. Asterisks under the dendro-grams indicate groups of clusterings that have significantly different accuracy for the two different tasks. Users may examine different clusterings by clicking on a clustering in the dendrogram, allowing users to zero-in on regions that appear promising.

If a user selects a cluster of clusterings in the meta level dendrogram (as opposed to a single base-level clustering), they can examine either the most central clustering in this branch of the dendrogram, or can examine a consensus clus-
Figure 9. Accuracy vs. compactness scatter plots for the two accuracy measures (identi-fying speakers vs. recognizing phonemes) (1st row), scatter plot of the two accuracy measures (2nd row), meta level clustering dendrograms shaded by accuracy in the two measures. (The dendrorgams are much eas-ier to interpret if viewed in color. See [6]) tering formed from the clusterings in the branch. The large dots in the scatter plots represent consensus clusterings that a user might select. 1 [2] presents a very different algorithm for finding alter-nate clusterings of the data. In this approach a probability matrix that defines the likelihood of jumping from one point to another is used to generate a random walk. The tran-sition probability is defined as a function of the Euclidean distance between each pair of points. The random walk al-lows particles to transition between instances according to the transition probability. Instead of clustering the data di-rectly, distributions of the locations of the particles are clus-tered. Gaps in the eigen values indicate potentially good partitionings. At any random walk step with a local maxi-mum eigen gap, the partition that maximizes this gap is re-ported. One of the ways in which this approach differs from meta clustering is that it uses a fixed method for measuring the distance between instances (euclidean distance). Also, the method only generates one clustering for each K. (All of the clusterings found with meta clustering in this paper are for a single fixed K.)
A number of ensemble clustering methods improve per-formance by generating multiple clusterings. We mention only a few here. The cluster ensemble problem is formu-lated as a graph partitioning problem in [31] where the goal is to encode the clusterings into a graph and then partition it into K parts with the objective of minimizing the sum of the edges connecting those parts. [12] proposes a different cluster ensemble method where both clusters and instances are modeled as vertices in a bi-partite graph. Edges connect instances with clusters with a weight of zero or one depending on whether the in-stance does or does not belong to the cluster, thus capturing the similarity between instances and the similarity between clusters when producing the final clustering.

Another cluster ensemble method was proposed by [17] where the objective is to find a final clustering that has minimum disagreement between it and all of the cluster-ings. In this framework, the clustering aggregation problem is mapped to the correlation clustering problem where we have objects and distances between every pair of them and the goal is to produce a partition that minimizes the sum of the distances inside each partition and maximizes the sum of the distances across different partitions.

Instead of partitioning, [25] used agglomerative cluster-ing to produce the final clustering after generating a similar-ity matrix from many base-level clusterings. Cluster aggre-gation was formulated as a maximum likelihood estimation problem in [32] where the ensemble is modeled as a new set of features that describe the instances and the final cluster-ing is produced by applying K-means while solving an EM problem. Linear programming was used in [4] to find the relation between the clusters in the different clusterings and the clusters of the final clustering. Simulated annealing and local search was used in [13] to find the final clustering.
The main difference between these ensemble methods and meta clustering is that most ensemble methods combine the clusterings they find into one final clustering because their goal is to find a better, single, very compact cluster-ing. Because the most compact clusterings are not neces-sarily the most useful clusterings, meta clustering does not combine different clusterings into one clustering. Instead, it groups different clusterings into meta clusters to allow users to select the clustering that is most useful for them.
Searching for the single best clustering may be inappro-priate: the clustering that is  X  X est X  depends on how the clus-ters will be used and the data may need to be clustered in different ways for different uses. When clustering is used as a tool to help users understand the data, an appropriate clustering criterion cannot be defined in advance.
The standard approach is for users to try to express in the distance metric a notion of similarity appropriate to the task at hand. This is awkward. Having to define the clus-tering distance metric, and then refine it when the clusters found are not what was wanted, is akin to having to mod-ify word processing software when the formatting it gen-erates is not what was expected. Few of the many poten-tial users of clustering are adept at defining distance metrics and at understanding the (often subtle) implications a dis-tance metric has on clustering. Even clustering researchers have difficulty modifying distance metrics to achieve bet-ter clusterings when the first metric they try does not work adequately.

In this paper we used auxiliary labels not available to clustering to measure clustering accuracy. We use accu-racy as a proxy for users who have unspecified goals and intended uses of the clusterings. This allows an objective evaluation of meta clustering. Experiments with four test problems show that meta clustering is able to automatically find superior clusterings. Contrary to common wisdom, in these experiments we find only modest correlation between clustering compactness and clustering accuracy. The most accurate clusterings sometimes are not even in the most compact 50% of the clusterings. This reinforces our belief that searching for a single, optimal clustering is inappropri-ate when correct clustering criteria cannot be specified in advance. Instead, it is more productive to focus clustering on finding a large number of good, qualitatively different clusterings and allow users (or some form of post process-ing) to select the clusterings that appear to be best.
Experiments with a phoneme clustering problem showed that clusterings that are good for one criterion can be very suboptimal for another criterion. Different users may need different clusterings. Meta clustering automatically found good (different) clusterings for each criterion. Experiments with a protein clustering problem provide a case study where meta clustering was able to improve clustering qual-ity 60% above the best that could be achieved by human ex-perts working with this data. Meta clustering achieved this improvement fully automatically in less than a day of com-putation. We believe the results demonstrate that meta clus-tering can make clustering more useful to non-specialists and will reduce the effort required to find excellent cluster-ings that are appropriate for the task at hand.

We thank John Rosenberg and Paul Hodor for the Pro-tein Data. Donna Bergmark provided the Bergmark data. Pedro Artigas, Anna Goldenberg, and Anton Likhodedov helped with early experiments in meta clustering as part of a class project at CMU. Anton Likhodedov suggested ap-plying PCA before random weighting to bias search toward weightings of natural dimensions. Casey Smith was sup-ported by an NSF Fellowship. This work was supported by NSF CAREER Grant # 0347318.

