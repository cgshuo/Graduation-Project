 We consider the problem of open-domain question answering (Open QA) over massive knowledge bases (KBs). Existing approaches use either manually curated KBs like Freebase or KBs automatically extracted from unstructured text. In this paper, we present oqa , the first approach to leverage both curated and extracted KBs.

A key technical challenge is designing systems that are robust to the high variability in both natural language ques-tions and massive KBs. oqa achieves robustness by decom-posing the full Open QA problem into smaller sub-problems including question paraphrasing and query reformulation. oqa solves these sub-problems by mining millions of rules from an unlabeled question corpus and across multiple KBs. oqa then learns to integrate these rules by performing dis-criminative training on question-answer pairs using a latent-variable structured perceptron algorithm. We evaluate oqa on three benchmark question sets and demonstrate that it achieves up to twice the precision and recall of a state-of-the-art Open QA system.
Open-domain question answering (Open QA) is a long-standing problem that has been studied for decades [12, 13]. Open QA systems need broad knowledge to achieve high coverage. Early systems took an information retrieval ap-proach, where question answering is reduced to returning passages of text containing an answer as a substring [24]. Recent advances in constructing large-scale knowledge bases (KBs) [21, 2, 5] have enabled new systems that return an exact answer from a KB [4, 28, 23, 25, 10, 15, 3]. Some such systems have used curated KBs like Freebase, 1 which are high-precision but incomplete. Other systems have used ex-tracted KBs like Open Information Extraction, 2 which have higher coverage but generally lower precision. In this paper,
Work completed while at the University of Washington. http://freebase.com http://openie.cs.washington.edu Figure 1: OQA automatically mines millions of op-erators (left) from unlabeled data, then learns to compose them to answer questions (right) using ev-idence from multiple knowledge bases. we present oqa , the first Open QA system to leverage both curated and extracted KBs.

A key challenge in Open QA is to be robust to the high variability found in natural language and the many ways of expressing knowledge in large-scale KBs. oqa achieves this robustness by decomposing the full QA problem into smaller sub-problems that are easier to solve. Figure 1 shows an example of how oqa maps the question  X  X ow can you tell if you have the flu? X  to the answer  X  X hills X  over four steps. The first step rewrites the input question to  X  X hat are signs of the flu? X  using a paraphrase operator mined from a large corpus of questions. The second step uses a hand-written template to parse the paraphrased question to the KB query  X ?x : (?x, signs of, the flu). X  These two steps are synergistic; paraphrase operators effectively reduce the variance of the input questions, allowing oqa to use a small set of high-precision parsing rules while maintaining recall. The third step uses a query-rewrite operator to reformulate the query as  X ?x : (the flu, symptoms, ?x). X  Query-rewrite operators are automatically mined from the KB, and allow the vocabulary mismatch between question words and KB symbols to be solved independent of parsing. Finally, the fourth step executes the rewritten query against the KB, returning the final answer.

The operators and KB are noisy, so it is possible to con-struct many different sequences of operations (called deriva-tions), very few of which will produce a correct answer. oqa learns from a small amount of question-answer data to find the best derivations. Because the derivations are unobserved in the training data, we use a latent-variable structured per-ceptron algorithm [31, 17, 22]. oqa uses a small set of gen-eral features that allow it to generalize from a limited num-ber of training examples. Experiments on three benchmark question sets show that oqa outperforms the state-of-the-art Open QA system Paralex [10], achieving twice the preci-sion and recall.

In summary, we make the following contributions: In Section 2, we describe related work in more detail be-fore moving on to the description of oqa (Sections 3 X 8) and experiments (Section 9).
Early work in Open QA used search engines as a source of background knowledge and relied on hand-written templates to map questions to search engine queries [16, 1]. In con-trast, oqa utilizes a set of KBs, which enable it to combine knowledge extracted from Web text with curated knowledge. The KB abstraction also allows oqa to join multiple pieces of evidence to arrive at an answer, a technique that is not possible using just a search engine.

A major research thread in QA has been scaling up se-mantic parsing systems from small, single-domain KBs [30, 31, 26, 18, 7] to larger, multi-domain KBs like YAGO2 [28], DBpedia [23], and Freebase [4, 3, 15]. Curated KBs like Freebase are attractive for QA because they allow systems to reason over high-precision knowledge and return accurate answers. However, these systems have limited recall due to the inherent incompleteness of curated KBs. This phe-nomenon can be understood as a power-generality tradeoff: QA systems can rely on the accuracy and conciseness of a curated KB, but incomplete knowledge limits their general-ity. https://github.com/afader/oqa
The Paralex system [10] was the first Open QA system to operate over a noisy, extracted KB. The biggest difference between Paralex and oqa is how they decompose the QA problem. Paralex uses self-labeled data to learn templates that directly map questions to queries X  X ssentially perform-ing paraphrasing, parsing, and query-rewriting in one step. oqa treats these as separate problems, which allows it to combine high-recall data mining techniques (for paraphras-ing and query rewriting) with high-precision, hand-written rules (for parsing). oqa  X  X  feature representation also differs from previous work. Previous systems use a large number of lexicalized features X  those involving specific lexemes or KB symbols. oqa uses unlexicalized features that operate on the level of function words, part-of-speech tags, and corpus statistics. We found that the an unlexicalized feature representation generalizes better to questions involving relationships that were never seen during training. In this section, we define the QA task and give a high-level outline of oqa and our experiments.
 Task and Metrics: We focus on the task of factoid QA, where the system takes a natural language question like  X  X ow can you tell if you have the flu? X  as input and re-turns a short string answer like  X  X hills X  from a KB, or  X  X o answer. X  We use precision (fraction of answered questions that are correct) and recall (fraction of questions that are correctly answered) as our primary evaluation metrics. Knowledge Base: oqa uses a simple KB abstraction where ground facts are represented as string triples (argument1, relation, argument2). We use triples from curated and ex-tracted knowledge sources (Section 4.1) and provide a light-weight query language to access the KB (Section 4.2). Operators and Scoring Function: oqa models QA as a process where answers are derived from questions using op-erators Ops (Section 5.1). A sequence of operators linking a question to an answer is called a derivation. oqa putes the confidence of an answer derivation d using a linear scoring function score( d | f , w ), which is parameterized by a feature function f and feature weights w (Section 5.2). Inference: In practice, the space of derivations defined by Ops is too large to enumerate. oqa uses heuristic search over partial derivations, guided by score( d | f , w ), to generate high-scoring candidate answers for an input question (Sec-tion 6).
 Learning: oqa learns the weights w from a small set of question-answer pairs. Because annotated answer deriva-tions are difficult to obtain, we use a latent-variable struc-tured perceptron algorithm that treats answer derivations as unobserved variables in the training data (Section 7). Operators and Features: oqa uses four types of opera-tors: a small set of parsing operators (Section 8.1), a large set of paraphrase operators mined from a question corpus (Section 8.2), a large set of query-rewrite rules mined from the KB (Section 8.3), and an execution operator that inter-faces with the KB (Section 8.4). Each operator is paired with a small set of features used to compute f . Using a small feature set results in better generalization from lim-ited training data. System Evaluation: We evaluate oqa using three ques-tion sets. We compare oqa to the Open QA system Paralex and the Freebase QA system Sempre . We then test the con-tributions of each knowledge source and system component via ablation.
This section describes where oqa  X  X  knowledge comes from and the query language it uses to access the knowledge.
Table 1 summarizes the knowledge sources in oqa . oqa uses one curated KB (Freebase) and three extracted KBs (Open IE, Probase, and NELL).
 Freebase is an open-domain, collaboratively edited KB. Freebase has relatively comprehensive coverage of certain domains like film or geography, but does not contain in-formal assertions like  X  X hicken is high in protein. X  Freebase maintains canonical string representations of its entities and relations, which we use to coerce facts into string triples. Open IE [2, 9] is a family of techniques used to extract binary relationships from billions of web pages containing unstructured text. Open IE has the unique property that its relations are unnormalized natural language, which results in over two orders of magnitude more relation phrases than Freebase. The Open IE assertions are noisy and lack the comprehensive domain coverage found in Freebase. How-ever, Open IE contains many of the informal assertions that are not found in curated KBs. For example, Open IE pro-duces assertions like (pepper, provides a source of, vitamins a and c). Open IE triples are annotated with metadata in-cluding extractor confidence and corpus frequency, and some triple arguments are linked to Freebase entities [20]. Probase [27] is an extracted KB containing  X  X s-a X  relations, e.g. , (paris, is-a, beautiful city) or (physicist, is-a, scientist). Probase triples are annotated with statistical metadata that measure the confidence of each extraction.
 NELL [5] is an extracted KB that contains approximately 300 relation phrases. NELL generally has high precision, but low recall.
 The union of these KBs forms a single resource contain-ing a billion noisy, redundant, and inconsistent assertions. While there is a vast body of literature exploring the prob-lem of data-integration [8], these techniques require a target schema, which does not exist for our knowledge sources. In-stead of making an offline commitment to a single schema, at runtime oqa hypothesizes many interpretations for each question. These hypotheses are encoded using the query language described in the next section.
The query language used in oqa provides a lightweight interface between natural language and KB assertions. In We discard Freebase facts that are not binary relations. Figure 2: Top: An example question and query used by OQA. Middle: The query semantics expressed as SQL. Bottom: The results when executed against a knowledge base (answers highlighted). contrast to the semantic parsing literature [30], a oqa query is not intended to represent the complete, formal semantic interpretation of a question. Instead, the query language is used to separate the parsing problem (identifying predicate-argument structure) from the vocabulary-matching problem (matching natural language symbols to KB symbols) [12, 15]. This factorization is at the core of the oqa approach, which uses different operators to solve each problem. oqa  X  X  query language is capable of representing conjunc-tive queries [6]. Because our KB is unnormalized and con-tains only strings, oqa uses keyword matching and string similarity as primitive operations. Figure 2 shows how the question  X  X hat fruits are a source of vitamin C? X  can be represented as the query ?x : (?x, is-a, fruit) (?x, source of, vitamin c). This particular query represents one possible mapping of the question to a predicate-argument structure. The middle box of Figure 2 shows how the semantics of the query can be interpreted as a SQL expression over a single table triples with string columns arg1 , rel , and arg2 . A oqa query consists of a projection variable ( e.g. , ?x) and a list of conjuncts. Each conjunct contains a mix of string lit-erals ( e.g. , fruit) and variables. String literals correspond to keyword-matching constraints on the table columns, while variables correspond to string-similarity join constraints.
Having keyword matching and string similarity incorpo-rated into the query semantics leads to another useful fac-torization. The query language provides a general, high-recall solution to the problem of minor surface-form varia-tions ( e.g. , joining  X  X tar-fruit X  with  X  X tarfruit X  or matching  X  X ource of X  with  X  X rovides a source of X  in Figure 2). can then increase precision by computing question-or KB-specific features as soft constraints on the output. For ex-ample, it uses a feature that checks whether two join keys are linked to the same Freebase entity, if this information is available. This lets oqa maintain a simple data model (entity-linking is not required) while allowing for domain knowledge to be modeled via features. oqa factors question answering into a set of smaller, re-lated problems including paraphrasing, parsing, and query reformulation. The solutions to each of these sub-problems can then be applied in sequence to give a complete mapping from question to answer. Figure 3 shows example deriva-tions for the question  X  X ow can you tell if you have the flu? X  Our approach consists of two parts: (1) derivation operators, which define the space of possible answers for a given ques-tion, and (2) a scoring function, which returns a real-valued confidence for a derivation.
More formally, we model question answering as the pro-cess of mapping a question q to an answer a by apply-ing operators from some set Ops . Each operator o  X  Ops takes a state object s  X  States as input and returns a set o ( s )  X  States of successor states as output. State objects en-code intermediate values that are used during the question-answering procedure. In Figure 3, the intermediate question  X  X hat are signs of the flu? X  is a state object that is related to the query  X ?x : (?x, sign of, flu) X  via a parsing operator. We use three types of states: question states, query states, and answer states.

Operations can be chained together into a derivation. A single derivation step encodes the process of applying an operator o to some state s and picking a successor state s  X  o ( s ) from the output. A derivation d = ( o , s , k ) consists of a sequence of k operators o = ( o 1 , . . . , o k ) and a sequence of k + 1 states s = ( s 0 , s 1 , . . . , s k ) satisfying s all 1  X  i  X  k . 5
An answer a is derivable from a question q under the op-erator set Ops if there exists some derivation ( o , s , k ) such that s 0 = q and s k = a . We use the notation Derivs ( q, Ops ) to represent the space of all possible derivations from the question q under the operations Ops ending at answer a .
In our implementation of oqa , the operator set Ops con-tains millions of operators, combining both hand-written op-erators and operators learned from data. These operators are noisy: incorrect answers can be derived from most ques-tions. Thus, estimating the confidence of a derivation is necessary for returning answers with high precision.
To compute the confidence of a derivation, oqa uses a scoring function. The scoring function computes a real value for a given derivation, where large, positive scores are as-signed to high-confidence derivations.

We make two assumptions about the form of the scoring function. First, we assume that the score is a linear function over features computed from a derivation. This will allow us to use familiar algorithms to learn the function from data. Second, we assume that the feature function decomposes over derivation steps. This allows us to use the scoring func-tion to score partial derivations, which is useful for searching over Derivs ( q, Ops ) (discussed further in Section 6).
Under these assumptions, the score of a derivation d = ( o , s , k ) can be written as where f is an n -dimensional feature function that maps a
In oqa , 2  X  k  X  4: parsing and execution steps are re-quired to derive an answer, and we limit derivations to have at most one paraphrase step and one query-rewrite step. Figure 3: OQA compute operator-specific features to discriminate between correct derivations (left) and incorrect derivations (right). derivation step into R n and w is an n -dimensional weight vector. Because s 0 (the input question) is a constant in all derivations, we pass it as an argument to the feature function. This allows features to compute properties relating derivation steps to the input question.

Figure 3 shows an example of how the scoring function can discriminate between a correct answer (left) and an incor-rect answer (right). The derivation on the left rewrites the original query using a high-confidence rule and uses high-confidence evidence returned from the KB. The derivation on the right uses a low-confidence rewrite rule and low-confidence evidence X  X nd is thus assigned a lower score.
In our implementation of oqa , we learn w from a small set of question-answer pairs (Section 7) and define f to compute operator-specific features (Section 8).
We focus on the task of finding a single answer with the highest confidence under the scoring function, which amounts to solving the following equation for an input question q : The underlying knowledge base and set of operators are both large enough that exhaustively enumerating Derivs ( q, Ops ) is not feasible. Instead, oqa uses beam search informed by the scoring function to explore Derivs ( q, Ops ). We refer to the beam search routine as DeriveAnswers .

The algorithm takes a question q as input and returns a set of derivations D  X  Derivs ( q, Ops ). The output set D is con-structed by iteratively growing partial derivations starting at the initial question state s 0 = q . The algorithm maintains a beam of partial derivations, each scored by the function score( d | f , w ). At every iteration, a partial derivation is se-lected to be extended. Extending a derivation d = ( o , s , k ) amounts to computing successors to the state s k and ap-pending the successors to construct new derivations. This process is repeated until there are no partial derivations left to extend or until a time limit is reached.

In practice, the scoring function score( d | f , w ) generally as-signs higher scores to short derivations, e.g. derivations that do not use a query-rewrite operator. We found that this bias will flood the beam with high-scoring partial deriva-tions occurring early in the search, and later options will not be considered. To avoid this problem, we maintain sep-arate beams for each state-type in the search, similar to the decoding algorithms used in statistical machine translation [14]. oqa uses beam search both at runtime and during learning, which we describe in the next section.
A key challenge in learning score( d | f , w ) is that obtaining labeled answer derivations requires expert annotators and is extremely time consuming. Following recent work in se-mantic parsing [7, 3, 15], we use question-answer pairs as indirect supervision and treat answer derivations as unob-served variables in the training data. Question-answer pairs like q =  X  X ow can you tell if you have the flu? X , A = {  X  X hills X ,  X  X ever X ,  X  X ches X  } are significantly easier to obtain and do not require expert annotators.

We use the latent-variable structured perceptron algo-rithm [31, 17, 22] to learn w from example question-answer pairs. Figure 4 shows the pseudocode for the LearnWeights algorithm.

The algorithm takes as input a set of N pairs ( q i , A i i = 1 , . . . , N , where A i is a set containing string answers to q i . For each training example, the algorithm calls riveAnswers to generate a candidate set of answer deriva-tions D . The algorithm then chooses a derivation  X  has the highest score according to the current weights and it is in A i ), the algorithm proceeds to the next example. If  X  a is incorrect, then the learner picks the highest scoring derivation d  X  such that answer( d  X  ) is in A i . The algorithm then performs an additive update w = w + f ( d  X  )  X  f (  X  there are no derivations with a correct answer in D , then the learner immediately proceeds to the next example with-out performing an update. Finally, the algorithm returns the average value of w over all iterations, which improves generalization [11].
In this section, we describe the operators that oqa uses to derive answers. The operators factor the end-to-end QA problem into smaller subproblems: Parsing operators (Section 8.1) are responsible for inter-facing between natural language questions and the KB query language described in Section 4.2. oqa uses a small number of high-precision templates to map questions to queries. Paraphrase operators (Section 8.2) are responsible for rewording the input question into the domain of a parsing operator. In an offline process, oqa mines 5 million lex-icalized paraphrase-templates from an unlabeled corpus of open-domain questions.
 Query-rewrite operators (Section 8.3) are responsible for interfacing between the vocabulary used in the input ques-tion and the internal vocabulary used by the KBs. oqa im-plements its query-rewrite operators by mining a set of 75 million relation-entailment pairs from the knowledge bases described in Section 4.1.
 The execution operator (Section 8.4) is responsible for fetching and combining evidence from the KB, given a query. For each operator, oqa computes general, domain indepen-dent features that are used in the scoring function. These features are unlexicalized in the sense that they do not com-pute any values associated with content words in either the question or the KB.

In the following subsections, we describe each type of op-erator in detail and describe the operator-specific features used by the scoring function.
To map questions to queries, we use the set of 10 hand-written operators shown in Table 2. Each operator consists of a question pattern and a query pattern.

A question pattern is expressed as a regular expression over part-of-speech (POS) tags and function words. To de-tect noun phrases (NPs), we use a POS pattern that matches a sequence of nouns, determiners, and adjectives. We use the ReVerb pattern [9] to detect relation phrases. Each ques-tion pattern uses named capture-groups to select substrings from the input question.

A query pattern consists of a query (defined in Section 4.2) containing pointers to capture groups from the ques-tion pattern. When a question pattern matches a question, the captured strings are substituted into the query pattern to generate a complete query. For example, the question capture groups.
 Table 3: Example paraphrase operators that ex-tracted from a corpus of unlabeled questions. pattern in the first row of Table 2 matches  X  X ho invented papyrus? X  and captures the substrings { rel  X  invented, arg  X  papyrus } . These are substituted into the query pattern (?x, rel, arg) to produce the output (?x, invented, papyrus).
Features: Because some question patterns may be more reliable than others, we include an indicator feature for each question pattern. We also include indicator features for the POS sequence of the capture groups and the POS tags to the left and right of each capture group.
The parsing operators in Table 2 have high precision but low recall. To increase recall, we use paraphrase operators to map questions onto the domain of the parsing operators. Each paraphrase operator is implemented as a pair of para-phrase templates like the examples in Table 3.

Each paraphrase template consists of a natural language string with a slot that captures some argument. For exam-ple, the first source template in Table 3 matches the question  X  X ow does nicotine affect your body? X  This question can then be paraphrased by substituting the argument  X  X icotine X  into the target template, yielding the new question  X  X hat body system does nicotine affect? X 
We follow the work of Paralex and automatically mine these source/target template pairs from the WikiAnswers 6 paraphrase corpus [10]. The WikiAnswers paraphrase cor-pus consists of 23 million question-clusters that WikiAn-swers users have grouped as synonymous. Each question cluster contains an average of 25 questions. In general, the clusters have low precision due to mistakes or users group-ing related, but non-synonymous questions ( e.g. ,  X  X ow to say shut up in french? X  is grouped with  X  X s it nice to say shut up? X ).

We extracted 200,000 templates that occurred in at least 10 question clusters. For each pair of templates ( t, t 0 fine the co-occurrence count c ( t, t 0 ) to be the number of clus-ters where t and t 0 both occur with the same argument. For http://wiki.answers.com Table 4: Example query-rewrite operators mined from the knowledge bases described in Section 4.1. example, if a cluster contains the questions  X  X hy do we use computers ? X  and  X  X hat did computers replace? X  we would increment the count c (Why do we use ? , What did replace?) by 1. For each template pair ( t, t 0 ) such that c ( t, t we define paraphrase operators t  X  t 0 and t 0  X  t . This generates a set of 5 million paraphrase operators. During inference, all possible paraphrases of a question q are com-puted by considering all substrings of q (up to 5 tokens) as the argument.

Features: The paraphrase operators are automatically extracted from a noisy corpus, and are not always reliable. We compute statistical and syntactic features to estimate the confidence of using the operator t  X  t 0 to paraphrase a question q to a new question q 0 using argument a . The statistical features include the pointwise mutual information (PMI) between t and t 0 in the WikiAnswers corpus and a language model score of q 0 . The syntactic features include the POS sequence of the matched argument a , and the POS tags to the left and right of a in q .
To handle the mismatch between natural language vocab-ulary and the KB vocabulary, we mine query rewrite rules. We focus on handling the mismatch between relation words in the question and relation words in the KB. 7 Table 4 lists example query rewrite rules. Each rule encodes a transla-tion from one relation phrase to another, with a possible re-ordering of the arguments. For example, the first row in Table 4 is an operator that allows the relation phrase  X  X hildren X  to be rewritten as  X  X as born to  X  1 , X  where the superscript denotes inverted argument ordering.

We follow previous work on mining equivalent relations [19, 29] and count the number of shared argument-pairs be-tween two relation phrases. For example, the tuples (her-mann einstein, children, albert einstein) and (albert einstein, was born to, hermann einstein) both appear in the KB, so  X  X hildren X  and  X  X as born to  X  1  X  share the argument pair
Rewriting arguments is future work. (hermman einstein, albert einstein). We construct a query rewrite operator for each pair of relation phrases ( r, r share at least 10 argument pairs. This results in a set of 74 million ( r, r 0 ) pairs that we include as operators.
Features: As with the paraphrase templates (Section 8.2), we compute the PMI for each pair of relation phrases as a feature.
The execution operator takes a query as input and returns a set of tuples, as shown in Figure 2. We store the KB (arg1, relation, arg2) triples in an inverted index 8 that allows for efficient keyword search over the three triple fields. We implemented a simple query optimizer that performs joins over triples by making multiple queries to the KB. Due to the size of the KB, we limit each keyword search over the triples to return the top 100 hits. The output of the execution operator is an answer state, containing a string answer and a joined tuple of evidence.

Features: We use features to estimate the reliability of the KB output. The features examine properties of the query, the returned tuple evidence, and the answer.
We measure the keyword similarity between two strings by lemmatizing them, removing stopwords, and computing the cosine similarity. We then include the keyword similar-ity between the query and the input question, the keyword similarity between the query and the returned evidence, and an indicator feature for whether the query involves a join.
The evidence features compute KB-specific properties. Ex-tracted triples have confidence scores, which are included as features. We compute the join-key string similarity mea-sured using the Levenshtein distance. We also include indi-cator features for the source of each triple ( e.g. , whether the triple is from Open IE or Freebase).

The answer features compute conjunctions of properties of the input question and the answer. We compute whether the question begins with some common prefixes ( e.g. , Who, What, When, How many, etc. ). For the answer, we com-pute word-shape features ( e.g. ,  X  X ansas X  has the word shape  X  X aaa X  and  X  X ecember 1941 X  has the word shape  X  X aaa 1111 X ). This allows the system to learn that features like question starts with When  X  answer has shape 1111 are in-dicative of a correct answer.
We are interested in answering three questions: (1) How does oqa compare to the state-of-the-art systems Paralex and Sempre ? (2) How do the different knowledge sources affect performance? (3) How do the different system com-ponents affect performance?
We investigate these questions by comparing performance on three question sets. Given a question q , each system re-turns an answer a with confidence c  X  R or  X  X o answer. X  We then measure the precision (correct answers/answers re-turned), recall (correct answers/questions), and F1 score (harmonic mean of precision and recall). We also compute precision-recall curves that show how precision is traded for recall as the minimum confidence to return an answer is var-ied. We describe the three question sets in Section 9.1 and the system settings in Section 9.2. https://lucene.apache.org/solr/ Table 5: The three question sets used in our exper-iments.
In our experiments, we use three question sets: WebQues-tions, TREC, and WikiAnswers. Figure 5 shows statistics and example questions from each set.
 WebQuestions was introduced by the authors of the Sem-pre system [3]. The questions were generated from Google Autocomplete using a seed set of Freebase entities. Amazon Mechanical Turk users then provided answers in the form of Freebase concepts. Questions that could not be answered using Freebase were filtered out. Out of the three test sets, WebQuestions has the unique property that the questions are known a priori to be answerable using Freebase. TREC was introduced for the purpose of evaluating infor-mation retrieval QA systems [24]. We re-purpose the TREC questions to test our KB-based Open QA systems. While the TREC questions were designed to be answerable using a small collection of test documents, they are not guaranteed to be answerable using any of the KBs described in Section 4.1.
 WikiAnswers is a set of questions that were randomly sampled from a crawl of WikiAnswers. The WikiAnswers question set is completely disjoint from the corpus used to extract the paraphrasing operators described in Section 8.2. WikiAnswers questions are very challenging and ambiguous, and are not necessarily answerable by any KB.
 WebQuestions and TREC both have gold-standard answer sets for each question, and WikiAnswers questions often have no answers available. However, due to the open-domain nature of our experiments, the gold-standard answer sets are incomplete. If a system X  X  top answer was not already included in the provided gold-standard sets, we manually tagged the answers as correct or incorrect. OQA: We examine two different training settings for oqa . In the first setting, we trained oqa on each question set independently, resulting three different sets of weights. In the second setting, we trained oqa on the union of the We-bQuestions, TREC, and WikiAnswers training sets, result-ing in one set of weights.

For inference, we used a beam capacity of 1,000 and a search time limit of 20 seconds. For learning, we initialized Figure 5: Training OQA on questions from all ques-tion sets leads to greater precision and recall than training on domain questions only. Figure 6: OQA has higher precision and recall than the Open QA system Paralex. 10 of the feature weights to be +1/-1 based on whether the features are indicative of good derivations ( e.g. , PMI scores) or bad derivations ( e.g. , verbs as paraphrase-template argu-ments). We set the number of perceptron iterations (be-tween 1 and 5) using a fraction of held-out training data. For the first perceptron iteration, we interactively trained the system by providing the set D  X  in Figure 4.
 Paralex: The authors of Paralex provide a learned model. 9 We used Paralex to parse questions to queries, and then execute them against the same KB as oqa . Paralex pro-vides a score for each query. For each answer that the query returns, we use the score from the query as a measure of confidence.
 Sempre: The authors of Sempre also make it available for download. 10 Sempre comes with a model trained on the WebQuestions question set. We attempted to train Sempre with questions from TREC and WikiAnswers, but found that the WebQuestions model had higher performance on held-out development questions, so we use the WebQues-tions model in our experiments.
Figure 5 shows the precision-recall curves comparing oqa under the two different training settings. Training the scor-ing function on questions from all three question sets re-sulted in higher precision and recall on TREC and WikiAn-swers, but had no effect on WebQuestions performance. This is likely due to the fact that oqa is unable to derive correct answers for many of the questions in TREC and WikiAn-swers, so the effective number of training examples is smaller than WebQuestions.
 Figure 6 shows the precision-recall curves of oqa and Paralex on the test questions. oqa achieves both higher precision and recall than Paralex across all three question http://knowitall.cs.washington.edu/paralex/ https://github.com/percyliang/sempre Figure 7: Sempre has higher precision and recall on WebQuestions, which are known to be answerable in Freebase. However, OQA outperforms Sempre on TREC and WikiAnswers, which were not developed for any particular KB. sets. oqa  X  X  scoring function was able to avoid many of the errors made by Paralex . For example, Paralex made a systematic error confusing  X  X here X  and  X  X hen X  questions, e.g. , it was unable to tell that  X 1985 X  is an unlikely answer to a question that begins with  X  X here. X  In contrast, oqa was able to compute features of the full derivation (including the answer), which allowed it to learn not to make this type of error.
 Figure 7 shows the precision-recall curves of oqa and Sempre on the test questions. In this case, Sempre has higher precision and recall than oqa on WebQuestions. Sempre performs better on WebQuestions through its use of lexicalized features, e.g. , there is a single feature indicating that the string  X  X ee in X  corresponds to the Freebase relation  X  X ourist attraction. X  These features allow Sempre to better fit the distribution of relations and entities in WebQuestions. In contrast, oqa uses only unlexicalized features like POS tags and corpus statistics like PMI, which limit oqa  X  X  ability to fit the WebQuestions training data.
 However, oqa performs significantly better on TREC and WikiAnswers for two reasons. First, oqa is uses both ex-tracted and curated knowledge sources, so it is more likely to have an answer in its KB. Second, Sempre requires sig-nificant lexical overlap in its training and testing set, which is not satisfied in the TREC and WikiAnswers questions.
Figure 8 shows the effects of removing different compo-nents from oqa . The weight-learning algorithm significantly improves performance over the default weights defined in the experimental setup.

The paraphrase operators improve performance on We-bQuestions and WikiAnswers, but not on TREC. We found that many TREC questions were covered by the parser op-erators in Table 2, so the paraphrase operators did not add much. In contrast, the WebQuestions and WikiAnswers questions exhibit much more lexical and syntactic variation, so the paraphrase operators were more useful.

The query-rewrite operators led to only a slight improve-ment on the TREC question set, and had at best no effect on WebQuestions and WikiAnswers. We examined the output and found some high-confidence examples where the query-rewrite operators helped, e.g. , the question  X  X hen did the Big Dig begin? X  was answered by rewriting  X (big dig, begin in, ?x) X  to  X (big dig, broke ground in, ?x). X  However, most derivations that used a query-rewrite operator were assigned low confidence, and had limited effect on recall.

Figure 9 shows the effects of removing a knowledge source from oqa on system performance. Removing Open IE from the KB lowers the F1 score across all test sets. Freebase Figure 8: The relative contributions of each system component depend on the distribution of test ques-tions. (Error bars represent one standard devia-tion from the mean, computed over 10,000 bootstrap samples of test data.) Figure 9: OQA performs best using multiple knowl-edge sources, in particular Open IE, Freebase, and Probase. helps the most on the WebQuestions set (which was de-signed specifically for Freebase), but is less useful for TREC and WikiAnswers. Probase is most useful for WikiAnswers, which contains many  X  X hat is. . .  X  questions that can be answered using Probase X  X  is-a relations. NELL is largely a subset of the other KBs, so it had no effect on oqa  X  X  perfor-mance.
The experimental results in the previous section exem-plify the power-generality tradeoff discussed in the introduc-tion: oqa uses a small number of general, unlexicalized fea-tures, which provided better generalization. However, this limits oqa  X  X  ability to take full advantage of the training data. For example, oqa was unable to answer questions like  X  X hat time zone is in South Africa have? X  despite see-ing several nearly-identical questions in the WebQuestions training data. A challenge for the future is to engineer to take advantage of lexical cues when they are available. Extending oqa to induce higher-precision operators during discriminative training may be one way.

One problem that has gone unaddressed by all of the dis-cussed QA systems is modeling whether a given question is answerable with the given KB and operators. For exam-ple, oqa currently has no way to answer truth-false ques-tions like  X  X re dogs mammals? X  Yet oqa systematically chains low-confidence operators together to derive incorrect answers for them, which hurts precision. A measure of an-swerability would be useful in scenarios where high-precision is required.

Table 6 shows examples from the test data where oqa derives correct answers. The first example shows a query rewrite operator that modifies the relation  X  X arry X  to  X  X as wife. X  The second example shows a paraphrase operator that maps  X  X hat are made of? X  to  X  X hat material are made of? X  In this case, the paraphrase operator introduces a type constraint that does not appear in the input question, Table 6: Examples from the test data where OQA derives a correct answer.
 Table 7: Example derivations from the test data where OQA derives an incorrect answer. which is beneficial for selecting the correct answer. The third example highlights the benefit of extracted knowledge, which contains obscure assertions like  X (changing light bulb, is-a, small building maintenance job). X 
Table 7 shows examples where oqa derives incorrect an-swers. The first example shows that the paraphrase opera-tors can be too general, in this case overgeneralizing  X  X nimal X  to  X  X ymbol. X  This combines with  X  X alifornia Water Service X  incorrectly matching  X  X alifornia, X  resulting in the incorrect answer  X  X WT. X  The second example shows that better fea-tures are needed to prevent errors like matching an active voice ( X  X dmire X ) with the passive voice ( X  X as admired by X ).
We introduced oqa , a novel Open QA system that is the first to leverage both curated and extracted knowledge. We described inference and learning algorithms that oqa uses to derive high-confidence answers. Our experiments demon-strate that oqa generalizes well to unseen questions and makes significant improvements over a state-of-the-art Open QA baseline. The data and code for this work is available at https://github.com/afader/oqa .
 This research was supported in part by ONR grant N00014-11-1-0294, DARPA contract FA8750-13-2-0019, and ARO grant W911NF-13-1-0246, and was carried out at the Uni-versity of Washington X  X  Turing Center. [1] M. Banko, E. Brill, S. Dumais, and J. Lin. AskMSR: [2] M. Banko, M. J. Cafarella, S. Soderland, [3] J. Berant, A. Chou, R. Frostig, and P. Liang.
 [4] Q. Cai and A. Yates. Large-scale Semantic Parsing via [5] A. Carlson, J. Betteridge, B. Kisiel, B. Settles, E. H. [6] A. K. Chandra and P. M. Merlin. Optimal [7] J. Clarke, D. Goldwasser, M.-W. Chang, and D. Roth. [8] A. Doan, A. Y. Halevy, and Z. G. Ives. Principles of [9] A. Fader, S. Soderland, and O. Etzioni. Identifying [10] A. Fader, L. Zettlemoyer, and O. Etzioni.
 [11] Y. Freund and R. E. Schapire. Large margin [12] B. J. Grosz, D. E. Appelt, P. A. Martin, and F. C. N. [13] B. Katz. Annotating the World Wide Web using [14] P. Koehn. Pharaoh: A beam search decoder for [15] T. Kwiatkowski, E. Choi, Y. Artzi, and [16] C. Kwok, O. Etzioni, and D. S. Weld. Scaling question [17] P. Liang, A. Bouchard-C X ot  X e, D. Klein, and B. Taskar. [18] P. Liang, M. Jordan, and D. Klein. Learning [19] D. Lin and P. Pantel. DIRT  X  Discovery of inference [20] T. Lin, Mausam, and O. Etzioni. Entity linking at [21] M. Mintz, S. Bills, R. Snow, and D. Jurafsky. Distant [22] X. Sun, T. Matsuzaki, D. Okanohara, and J. Tsujii. [23] C. Unger, L. B  X  uhmann, J. Lehmann, A.-C. N. Ngomo, [24] E. M. Voorhees and D. M. Tice. Building a question [25] S. Walter, C. Unger, P. Cimiano, and D. B  X  ar. [26] Y. W. Wong and R. J. Mooney. Learning synchronous [27] W. Wu, H. Li, H. Wang, and K. Q. Zhu. Probase: a [28] M. Yahya, K. Berberich, S. Elbassuoni, M. Ramanath, [29] A. Yates and O. Etzioni. Unsupervised methods for [30] J. M. Zelle and R. J. Mooney. Learning to Parse [31] L. S. Zettlemoyer and M. Collins. Learning to Map
