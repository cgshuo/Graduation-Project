 JUN-MING XU, XIAOJIN ZHU, and TIMOTHY T. ROGERS, University of Wisconsin-Madison Since Fechner invented the field of psychophysics [Fechner 1860], psychologists have been interested in the way our mind perceives similarities between objects. Compu-tationally, each object (or stimulus) is usually represented by a feature vector x  X  R d . This feature representation induces a convenient objective distance between two stim-uli x i , x j through the q norm. Often q is taken to be 2 for the Euclidean distance, or 1 for the Manhattan distance.
However, it is less clear how such objective distances relate to the subjective dis-tances , or the perceived similarities, among stimuli. For instance, many empirical studies of human learning employ Gabor patches, shown in Figure 1. Such stimuli are thought to be especially useful because they appear to closely match the kinds of infor-mation extracted by neurons in early visua l processing streams in mammalian cortex. The feature vector of a Gabor patch consists of the angle of the grating, its frequency, and its amplitude (i.e., degree of contrast). The problem is, it is not clear how these features relate to the subjective distances. Human perception is subject to a variety of nonlinear and configural processes that can make any such relationship extremely opaque. For instance, people appear to treat horizontal and vertical gratings as qual-itatively distinct from oblique angles. Dimensions that are independent in principle, like the amplitude and frequency of a Gabor grating, are interdependent in perception, with some amplitudes being harder to see at higher frequencies. Even perception of individual dimensions for such stimuli can be highly nonlinear. And if these issues pose challenges even for simple stimuli like Gabor patches, one can imagine the chal-lenges posed by real-world stimuli, like faces or objects. Yet computational theories of human learning depend critically upon having subjective distances existing among the stimuli being learned. Essentially, all such theories invoke this subjective psychologi-cal distance to explain how learning about an individual item will generalize to other items.

To address this issue, cognitive scientists have typically taken one of two ap-proaches. The first is to make the simplifying assumption that the subjective distances are proportional to the objective distances, at least for restricted ranges for some kinds of stimuli. For instance, in studies with Gabor patches, the theorist might take care to employ stimuli with oblique orientations that are not too close to vertical or horizontal; and might further assume that, within this range, the true difference in orientation for a given pair of items is proportional to the perceived dissimilarity of the items. Such assumptions may seem intuitively reasonable but are difficult to formally jus-tify; and moreover, it seems limiting to simply avoid problematic stimuli like vertical and horizontal gratings.

The second approach uses multidimensional scaling (MDS) to find an embedding that relates to the subjective distances among some fixed set of stimuli [Shepard 1958]. There are a variety of different methods for collecting information about subjective similarity, and MDS has been applied to all of these. One of them, the confusion ma-trix, is widely accepted as yielding the most robust results. In the standard procedure, participants learn n classes, each containing a single unique item. Subsequently, they are tested by classifying each of the n items multiple times. From this data, a confu-sion matrix is constructed describing the number of times that item x i was misclas-sified as item x j . MDS can then be applied to this matrix to embed the n items into a low dimensional space, which is taken to be the psychological space in which the subjective distances are defined. This approach has the advantage that it can be ap-plied to any arbitrary set of stimuli and, furthermore, has yielded very robust results. Shepard[1987], for instance, famously showed that the generalization of a learned cat-egory falls off exponentially when distances are measured in this way. Nosofsky has capitalized on this method to develop a computational theory of identification and cate-gorization that captures human performance with a high degree of accuracy [Nosofsky 1992, 2011]. People have used other kinds of procedure to estimate subjective similar-ity including similarity judgments and same/different judgments, but these have well known problems [Nosofsky 1985].

This MDS approach still has two important drawbacks, however. The first is that the learning and testing phase of the procedure is very labor-intensive, so that it is only possible to measure distances among a limited set of items. For example, much of Nosofsky X  X  work has involved just n = 16 individual items. The second problem is that MDS only yields an embedding on training stimuli X  X t does not offer a way to map an unseen stimulus from its feature space to the psychological space.

This article aims to address these problems by developing a novel method to trans-late objective distances to subjective distances, and vice versa. Our method is a general metric-learning procedure that elicits perceived similarities in humans and learns a continuous, nonlinear mapping from the feature space to the psychological space. This mapping (and its inverse mapping) can be applied to novel stimuli. In order to study subjective distances, one needs to measure some form of the perceived similarities among stimuli. As previously noted, past work in cognitive psychology has made extensive use of confusion matrices generated within an identity-recognition paradigm to measure such similarities. As also noted, however, such work is extremely labor-intensive and allows the psychologist to measure similarities amongst only a small number of items. For these reasons, we have instead adapted a method fre-quently used to measure perceptual discriminability in psychophysics, specifically, the two-alternative forced-choice ma tch-to-sample (2AFC) procedure.

In each trial of the 2AFC procedure, participants encounter three stimuli. The first stimulus (X) is displayed by its own on the screen. It disappears after a short while, and two stimuli A and B appear simultaneously on the screen. One of A or B is iden-tical to X, while the other is different from X. The relative position of A and B is randomized in each trial.

The participant must judge whether the first stimulus X matched A or B. This procedure is sensitive to the perceptual similarity between A and B: as they grow more similar, performance drops to chan ce. This procedure does not require any category or identity learning and so can be applied to a much larger set of stimuli. Like identity-learning, the method produ ces a confusion matrix in which each cell indicates, for some pair of A and B, how frequently the participant chose the wrong match for X. More frequent incorrect choices indicate more perceptual confusability (and hence similarity) between A and B. We now formally define our learning problem. Let X  X  R d be an objective space ,in which items x  X  X are represented as d -dimensional feature vectors. We assume that there is a corresponding subjective space  X  ( X )  X  R p where potentially p = d .The function  X  : R d  X  R p is an unknown nonlinear mapping from the objective space to the subjective space. We cannot directly observe  X  (). Nonetheless, its effects can be felt by the following means. For any two items x i , x j  X  X ,wemayobservea comparison function f : X  X  X  X  R . We overload f here to declare that f depends only on the subjective distance ||  X  ( x i )  X   X  ( x j ) || between the two items. Notice that f is a stochastic function, because it is intended to model a random outcome of a psychology experiment. The outcome may be affected by many factors, such as the subjects and experimental conditions. Given a set of items x 1 ,..., x n  X  X and a limited number of comparisons using f ,wewantto estimate the mapping  X  .

This task is more general than embedding, whose goal is to find the images of the training items:  X  ( x 1 ) ,..., X  ( x n ). However, embedding usually does not aim to learn the mapping function  X  , and therefore has difficulty generalizing to the image of an unseen test item x  X  . This task is also more general than traditional metric learning, metric learning does not aim at producing an embedding or mapping. To fully define the problem, we need to instantiate the stochastic comparison function should be zero. In this case, in the 2AFC experiment, a human participant would have chance probability ( 1 2 ) of picking out the target item. As the two items become gradually distinct, their subjective distance grows. The probability of the participant picking out the wrong item should decrease toward zero. This suggests that we can model the probability of participant error on the pair x i , x j in 2AFC with a monotonic function. The function  X  : R +  X  [0 , 0 . 5] is closely related to the choice model in psychology. Following [Nosofsky 1985], we consider two function forms, see Figure 2. One is an exponential decay function another is a Gaussian decay function Of course, we do not observe p ij but rather the binary outcomes of each 2AFC trial. Our stochastic comparison function f models this as independent Bernoulli random variables.
We are now ready to describe the 2AFC experiment formally. First of all, we aggre-gate all participants together and treat their behavioral data collectively. Altogether, shown n ij times to the participants. The number n ij could be zero for some pairs X  X n this case we do not have 2AFC behavioral data for those pairs. Each time ( x i , x j )was shown, a participant makes a choice. The binary outcome of the stochastic compari-completion of the experiments, let a ij be the number of times that the participants were incorrect on the pair ( x i , x j ), and b ij be the number of times that they were correct, We further define the set of pairs on which we have 2AFC data as Given a set of items represented as feature vectors in the objective space x 1 ,..., x n  X  R dimensionality of subjective space p , our goal is to learn the mapping  X  : R d  X  R p which relates the two spaces.
 We introduce Yada ( Yet Another Dimension-reduction Algorithm ) for this purpose. We start with a simple linear mapping in the form where A is a p  X  d matrix to be learned. Then the subjective distance between two items x i and x j is Applying the choice model, we get the probability that a participant chooses the incor-rect item in 2AFC experiments on that pair, As different subjects X  choices can be treated as independent Bernoulli trials, the log-likelihood function is For stability, we add the squared Frobenius norm on A as a regularizer, and arrive at the linear Yada optimization problem, where  X &gt; 0 is a regularization parameter.

The expressive power of this linear mapping is very limited. It is likely that the mapping  X  is nonlinear in human cognition. Therefore, we enhance our model by the kernel trick. Note that the objective function is the sum of a loss function plus a monotonic function of Frobenius norm of A . By the representer theorem [Argyriou et al. 2009; Sch  X  olkopf et al. 2001], the solution A to the optimization problem of Equation (16) admits the following form.
 where W is a p  X  n matrix, and X =( x 1 ... x n ) is the n  X  d input matrix. The subjective distance between item x i and x j can be written as Let K be an n  X  n kernel Gram matrix on x 1 ,..., x n ,whoseentriesare K ij = x i , x j ,  X  is the feature mapping induced by kernel K . With the notation of K , the perceptual distance can be written as Thus, the log-likelihood is now a function of W , The regularizer can be represented in terms of W ,too.
 Therefore, the kernelized Yada optimization problem is It is easy to see that this problem is non-convex, because rotating W does not change the objective. In practice, though, we found that simple gradient methods are sufficient to solve Equation (22) up to local optimum satisfactorily, and multiple local optima are not a serious issue if we are careful about initialization, as discussed in Section 3.3.
The gradient of (22) is  X  If we use the exponential choice model  X  e of Equation (5), If we use the Gaussian choice model  X  G of Equation (6), It is well known in the machine learning community that informative initialization can alleviate the multiple local optima issue. Therefore, we propose the following ini-tialization procedure. When the dimensions of the objective space and the subjective spacearethesame( d = p ), we initialize W with W 0 ,where W 0 generates an embedding closest to the objective space, that is, This initialization has a closed-form solution of If the dimension of the objective space is greater than that of the subjective space ( d &gt; p ), we run principal component analysis (PCA) on X to project it down to a p -dimensional subspace and then apply the initialization process of Equation (26). If the objective space has lower dimensionality than the subjective space ( d &lt; p ), we augment x with p  X  d extra dimensions and set these dimensions with random values sampled from N (0 , 1), and then apply Equation (26).

Besides informative initialization, we also experimented with multiple random starting points. We observed that, in most cases, there was no significant difference on the result between the two initialization methods. However, a multiple random starting points method took much more computation resources than informative ini-tialization, because it requires running an optimization procedure multiple times. After we solve the Yada problem in Equation (22), we can perform the forward map-ping  X  : R d  X  R p for any x (not limited to the training items) from objective space to subjective space. where k x =( K ( x 1 , x ) ,..., K ( x n , x )) .
 In some circumstances, a backward mapping  X   X  1 : R p  X  R d is also of interest. Psychologists may be interested in generating a stimulus x that maps to a specific point in the subjective space. For example, they may wish to design a set of stimuli that lie on a regular grid in the subjective space to experiment with. These stimuli would have the desirable property that the subjective distance between neighboring items are normalized.
 Since  X  may map multiple points in R d tothesamepointinthesubjectivespace R p , the backward mapping  X   X  1 may be one-to-many. This backward mapping can be formulated as an optimization problem. Given y  X  R p inthesubjectivespace,wewant to y . The optimization problem depends on the kernel K and may not be convex for some kernels. However, we can still use gradient descent to find a local minimum. To choose for x in (30).

The complete Yada algorithm is given in Algorithm 1. In order to evaluate the effectiveness of Yada in learning the mapping  X  , we conducted experiments on synthetic and real datasets. We compared Yada X  X  performance to other existing algorithms, including nonmetric multidimensional scaling and three other metric-learning algorithms. In the following, we first introduce the datasets we ex-perimented on. We then describe our experim ent procedure and eva luation criteria. We found that compared to other algorithms, Yada recovers the mapping  X  better in both evaluation criteria considered.
 4.1.1. Synthetic Datasets. One advantage of synthetic datasets is that the true map-ping  X  is available and we know the true pairwise distances in the subjective space. Therefore, we can compare how well differe nt algorithms recove r the pairwise dis-tances. We created three 1D datasets and three 2D datasets with different character-in the objective space. For 1D datasets, we chose 11 items in the objective space on a see Figure 3(a). For 2D datasets, we chose 36 items on a regular grid between  X  1and1 over both axis with stepsize 0.4, see Figure 4(a). For each objective dataset, we created three different subjective datasets, as discussed next.
 (1) 1D-RBF . In all our experiments, we let K be the RBF kernel. Therefore, we created (2) 1D-cos . The mapping is (3) 1D-exp . The mapping is an exponential function of x ,seeFigure3(d). (4) 2D-RBF . We constructed a 2D dataset within the function family our algorithm (5) 2D-tanh . This 2D dataset has a mapping that is dense near the boundaries while (6) 2D-poly . This 2D dataset has a polynomial function for the true mapping, see then used the Gaussian decay choice model  X  G of Equation (6) to compute the error probabilities p ij .Wesimulated n ij = 50 subjects participated in each 2AFC experiment, which is a reasonable number in real experiments. Because different subjects X  choices are modeled by independent Bernoulli trails, we sampled a ij  X  Binomial(50 , p ij )as the number of subjects choosing the wrong items in the 2AFC experiment for pair ( x , x Figure 5(a) shows the confusion matrix [ a ij ] generated by the preceding procedure for the 1D-exp dataset. We label and order the items as they appear in Figure 3(d). distances are small. On the other hand, for items on the right (e.g., j and k ), most a
In reality, the number of ( x i , x j ) pairs a human participant can work on is limited, too. We simulated this by controlling the size of E for synthetic datasets. For a 1D dataset, as there are only 11 stimuli and therefore 55 distinct pairs, it is realistic to expect participants to examine all pairs. Therefore, for all 1D datasets, we let E = { ( i , j ) | 1  X  i &lt; j  X  n } . However, there are 630 distinct pairs in each 2D dataset. j  X  x 4.1.2. Real Datasets. We conducted three real 2AFC experiments on different types of stimuli. The participants are undergraduate students from the University of Wisconsin-Madison, participating for partial course credit. (1) Lines . The stimuli consist of line segments with different lengths, see Figure 6. (2) Blobs . The stimuli are novel computer-generated shapes parametrized by a single (3) Screws . Each stimulus consists of a circle and a line inside and can be de-It is impossible to directly measure how close the learned mapping  X   X  is to the ground truth mapping  X  on real datasets. Therefore, we de fine two criteria to evaluate differ-ent algorithms.
 (1) Normalized log-likelihood . If the learned mapping  X   X  is close to the ground truth, (2) Recovery error E of pairwise distances in the subjective spaces. One expects the We used ten-fold cross-validation to evaluate Yada and baseline algorithms (see Section 4.4). Since some baseline algorithms cannot handle out-of-sample items, all stimuli in the test set must also appear in the training set. Therefore, we did not the folds formed E trn and the remaining one, E tst . We trained each algorithm on E trn performance), respectively, when applic able. We report the average results over ten folds. We also plot the embedding produced by different algorithms.

Many algorithms have parameters to be se t. We used five-fold cross-validation on the first training set E trn to tune these parameters. We selected the parameters with the highest normalized log-likelihood on the tuning set and applied them to all subse-quent splits for that dataset. For Yada, there are two parameters: the regularization parameter  X  and the RBF kernel bandwidth. We searched  X  and the bandwidth jointly over the grid { 10  X  7 , 10  X  6 ,..., 10 2 } X { 2  X  2 , 2  X  1 . 5 ,..., 2 3 } .
We implemented Yada in Matlab. We solved the optimization problems by calling fminunc in Matlab with the default setting, except that the maximum number of itera-tions was increased from 400 to 700. We supplied the gradient of the objective function to speed up fminunc .

We conducted experiments with two different choice models  X  e from Equation (5) and  X  G from Equation (6). Some algorithms do not take choice models into account, therefore their outcomes with different choice models are the same. However, the choice model may affect the log-likelihood. Since we tune parameters based on log-likelihood, the optimal parameters depend on the choice model. For different choice models, we carried out the previous experiment procedure separately. We compared our method with several embedding and metric-learning algorithms [Borg and Groenen 2005; Globerson and Roweis 2007; Schultz and Joachims 2004; Xing et al. 2003]. Due to the lack of an appropriate way of assigning the pairwise distances to the pairs with a ij = 0, we did not include metric MDS as a comparison method. We briefly discuss these baseline algorithms below. 4.4.1. Nonmetric MDS (NMMDS). Nonmetric MDS takes the ranking of subjective dis-tances as input. To generate the ranking, we sorted all pairs by a ij / n ij in descending der of subjective distances. The pair with the smallest a ij / n ij value was set as rank 1, the second smallest one as rank 2, and so on. We tested NMMDS with the Matlab implementation mdscale .
 NMMDS also requires initial coordinates in the subjective space R p . The default in Matlab of initializing NMMDS with metric MDS does not apply for the reason stated earlier. Instead, we initialized NMMDS in the same way as we initialize Yada, as discussed in Section 3.3.

The output of NMMDS is scale invariant, because its goal is to find an embedding which preserves the ranking of distances. If NMMDS produces  X  ( x 1 ) ,..., X  ( x n ), then However, the scaling factor  X  affects the pairwise distances and, in turn, the normal-ized log-likelihood (Equation (41)) and recovery error (Equation (43)). To give NMMDS maximum advantage, we optimized  X  by maximizing the training set log-likelihood (Equation (41)). 4.4.2. Schultz and Joachims (S&amp;J). One may compare the subjective distances in the Boolean judgment results as input to learn a metric [Schultz and Joachims 2004]. In order to turn our 2AFC experiment outcomes into this form, we transformed them as a / n versa. Comparisons with such simple ratios turn out to be effective, and we report considered the comparisons achieving significant level 0.05. The results were very similar, and we do not report them here.

As a metric-learning algorithm, S&amp;J pro duces a distance metric but not an embed-ding in R p or a mapping  X  to that space. We followed the authors in Schultz and Joachims [2004] and applied MDS on the learned distances to produce an embedding. Because S&amp;J does not involve a choice model, to make the comparison fair, we also maximized normalized log-likelihood (Equation (41)) by finding an optimal scaling fac-tor  X  with the same procedure as in NMMDS. As in Schultz and Joachims [2004], we ran S&amp;J with SVM-light and set the parameter C at default value 1. We used the RBF kernel and the same bandwidth parameter tuning procedure, as in Yada. 4.4.3. Xing et al. (XNJR). One may make a Boolean judgment on whether two items x , x learns a metric. To convert our 2AFC experiment outcomes into such judgments, we used the following procedure. Intuitively, if most participants failed to distinguish a pair of items, these two items are similar to each other. Therefore, we threshold a / n This procedure depends critically on the threshold. In our experiments, we selected the threshold from { 0 . 05 , 0 . 1 ,..., 0 . 5 } with cross validation.

In Xing et al. [2003], the algorithm has not been kernelized and will not handle our synthetic datasets. To make a fair comparison, we followed the idea of kernel PAC, compute the Gram matrix in advance and take the columns as input feature vectors to the XNJR algorithm. We also optimized the scaling factor  X  , as we did for NMMDS, to improve its performance. We used the same RBF kernel and tuned the threshold and kernel parameter jointly, as we did for Yada. 4.4.4. Globerson and Roweis (G&amp;R). G&amp;R [Globerson and Roweis 2007] is another em-bedding algorithm that works on the same similarity judgment data as XNJR. We converted our 2AFC data in the same way. There are both parametric and nonpara-metric embedding methods in Globerson and Roweis [2007]. Since our goal is to learn the mapping, we compared with the parametric version KPSDE. There are three pa-rameters to be set in the G&amp;R algorithm and, similar to XNJR, a threshold is needed to transform the data. We tuned them by cross validation. First, we fixed the weight of the trace  X  and the weight of penalty on constraint violations  X  to 1 and tuned the threshold and kernel bandwidth with five-fold cross validation. Then, we fixed the fold cross validation. We also used the same scaling method as we did for NMMDS to maximize its performance. The experiment results in this section demonstrate the advantage of Yada over the baseline algorithms in learning an objective-space-to-subjective-space mapping from 2AFC data. We present two kinds of results: Tables I, II, and III show the test set ( E tst ) performance of all algorithms on synthetic and real datasets, while Figures 9, 10, and 11 visualize the learned mappings of these algorithms. In all tables, the best algorithm for each dataset (i.e., each row) is marked with boldface.

Table I reports the normalized log-likelihood (  X   X  ) on the test set E tst .Yadaachieves the highest test set log-likelihood on all synthetic and real datasets (with ties). The baselines NMMDS, S&amp;J, XNJR, and G&amp;R are consistently worse than Yada, most no-ticeably on the synthetic datasets. We speculate that the inferior performance of these algorithms is caused by their inability to exploit all information from the 2AFC exper-iments. For example, S&amp;J only considers the Boolean similarity judgments among triples. XNJR and G&amp;R lose even more information by only considering pairwise Boolean judgments.

Table II reports the other evaluation criterion, recovery error E (  X   X  ), on the test sets of the synthetic datasets. The table does not include the real datasets because for them, the ground truth is unknown. Again, Yada achieves lower recovery error than all other methods, often very significantly. This is to be expected, given that the way the synthetic datasets were generated matches the model assumption behind Yada. Nonetheless, it serves as validation of the Yada algorithm. It is worth pointing out that all algorithms have difficulty with the synthetic dataset 1D-exp: the recovery error is very large. This can be explained by recalling that in Figure 3(d) the right-most items are far from all other items. In an 2AFC experiment, based on the Gaussian choice model  X  G in Equation (6), these items are easily distinguished from any other items. However, this poses a problem for any algorithm using the confusion counts a , b algorithms do not have enough information on precisely how far apart they should be placed.

Table III shows that Yada is not very sensitive to the assumption of the choice model  X  . This table is the same as Table I, except that we used the exponential choice model  X  e instead of the Gaussian choice model malized log-likelihood (  X   X  ). Recall that the synthetic datasets were generated using the Gaussian choice model. Therefore, this represents a mismatch between data and learning models. Fortunately, the results in Table III suggest that the performance of Yada and baseline algorithms on synthetic datasets is not very sensitive to the choice model. This provides some assurance on applying Yada to real datasets, even though we do not know the true choice model in humans, and it is likely that neither  X  e nor  X  G is the correct one.

Figures 9 and 10 visualize the learned nonlinear mapping on the six synthetic datasets. Each plot shows the position of the training points in the subjective space  X   X  ( x 1 ) ,...,  X   X  ( x n ) as produced by Yada and the baseline algorithms. Because the map-ping  X   X  is shift and rotation invariant for 1D results, we centered them i  X   X  ( x i )=0 and also flipped the direction when appropriate (which is equivalent to rotating the mapping by 180 degrees) to make comparison easier. We did not shift or rotate the 2D results. All training items are coded by the same colors as in Figures 3 and 4, so the reader can track their positions.

On all the synthetic datasets, Yada succeeds in qualitatively recovering the struc-ture of the mapping. Most of them are close to the ground truth. On the particularly difficult 1D-exp dataset, Yada shrinks the range of items and does not place the far-right items far enough apart. However, all algorithms have the same issues due to the lack of information. We can still observe the increasing gap in Yada X  X  result, which is not obvious for the baseline algorithms. The baseline algorithms in general produce more distorted mapping s. For example, NMMDS fails to recover the L-shape on 2D-RBF; S&amp;J, XNJR, and G&amp;R do not recover the shuffled mapping on 1D-cos.

We are also interested in the inverse mapping  X   X  1 ( y ). Figure 11 shows some inverse mapping results produced by Yada on synthetic datasets 2D-tanh and 2D-poly. The first column shows the forward-then-backward mapping on the training  X  x = x i , i =1 ... 36 and, thus, a regular grid like that in Figure 4(a). But this is not al-ways the case in Figure 11. We attribute it to the fact that  X  may be (nearly) many-to-one, and the inverse mapping can have difficulty finding a unique pre-image. Indeed, we are able to verify that  X  ( X  x i )  X   X  ( x i )  X  0 , i =1 ... 36.

As mentioned earlier, an application of  X   X  1 ( y ) is for psychologists to design a set of perceptually normalized stimuli. In other words, one starts with stimuli y 1 ... y m on a regular grid in the subjective space. Examples of this are shown in the second column of Figure 11. One must compute their inverse mappings x  X  =  X   X  1 ( y )inorder to actually create the stimuli. For instance, for the Screws, x  X  1 specifies the angle and x  X  2 the diameter, while y (being in the subjective space) does not directly specify such physical properties. The third column of Figure 11 shows that, as expected, the inverse mapping reverses the nonlinear effects shown on the first row of Figure 10. These stimuli are indeed perceptually equidistant when mapped back to the subjective space. Multidimensional scaling (MDS) aims at embedding a set of items into a Euclidean space such that pairwise distances are preserved. There are many varieties of the classical MDS [Borg and Groenen 2005]. Since it is not always possible to find an embedding in R p to reproduce all pairwise distances, metric MDS will approximate the given distances as closely as possible. Unfortunately, pairwise distances are not always available in real-word applications. For example, in the 2AFC experiments such, distances are not directly available.

Instead of preserving the actual pairwise distances, nonmetric MDS (NMMDS) seeks an embedding to preserve their rankings. There are two major differences be-tween our Yada model and NMMDS. First, NMMDS only embeds training items. When there are new items in the objective space, we have to rerun nonmetric MDS with all available items and pairwise distances to obtain the embedding for the new items. In contrast, Yada produces a mapping function  X   X  which can be used to embed new items. Second, NMMDS only preserves the ranking but not the exact distances. Yada uses the choice model to convert 2AFC count data into distances.

Metric learning is another line of related work. Many metric-learning algorithms aim at learning a Mahalanobis Distances ( x i  X  x j ) M ( x i  X  x j ) induced by a positive semidefinite matrix M 0 [Mahalanobis 1936]. Once M is learned, it is possible to define a linear mapping with M 1 / 2 x ,orarank q approximation to it. There are many variants of metric-learning algorithms that take in different input. One kind of metric-learning algorithm is purely unsupervised. There is no additional information other than the feature vectors. In this case, algorithms have been proposed to learn a mapping which preserves local distances and local structures, such as structure-preserving embedding [Shaw and Jebara 2009] and maximum-variance unfolding [Weinberger et al. 2004]. The information to be preserved are inferred from the original feature spaces without user input. Another kind of method takes advantages of side information, other than class labels, to learn a better metric [De Bie et al. 2003; Song et al. 2008]. For example, users may want to put some pairs closer and some pairs far away. We discuss this kind of algorithms in the following, based on the type of side information.

One type of input is Boolean similarity/dissimilar judgments on a set of pairs, E .A distance metric respecting such relationships, which assigns small distances to similar pairs and large distances to dissimilar ones, is preferred. Xing et al. [2003] formulated this problem by minimizing the sum of distances between similar pairs, with the con-straint that distances between dissimilar pairs are greater than a constant. Later, Bilenko et al. [2004] proposed a more general semi-supervised learning framework to exploit this side information in unsupervised learning. With a similar input, Glober-son and Roweis [2007] proposed both nonparametric and parametric embedding al-gorithms to visualize the binary pairwise similarity measurements. They formulated it as a semidefinite programming problem. Davis et al. [2007] took an information-theoretic approach by introducing a new regularizer and constraints.

Another type of input is the knowledge that some items are in the same class, although their actual class labels are unknown. Bar-Hillel et al. [2005] proposed Rel-evant Component Analysis to learn metrics-from such equivalence constraints. Tsang et al. [2005] kernelized this algorithm. Under the supervised setting, Goldberger et al. [2005] and Weinberger and Saul [2009] learned a Mahalanobis distance to max-imize the performance of nearest neighbor classifications. With the same intuition, Globerson and Roweis [2006] proposed learning a metric to collapse classes, which maps all items in the same class to a single point and pushes other items infinitely far away.

Yet another type of input is relative distance judgments. For example, a relative judgment over a triple has the form that item x j is more similar to x i than x k is to x [Schultz and Joachims 2004]. Such comparison information is used as constraints which the learned distance metric should obey. As another example, McFee and Lanck-riet proposed a parametric embedding algorithm with multiple kernels on quadruples, with the form that the distance between x i and x j is smaller than the one between x k and x l [McFee and Lanckriet 2009]. Tamuz et al. [2011] introduced a learning algorithm which adaptively chooses triplet-based relative-similarity queries to learn similarity with as few queries as possible, since judgments are usually expensive to obtain.

From this perspective, Yada also produces a Mahalanobis distance. However, one major difference between Yada and the other algorithms is that Yada incorporates a natural rank-q constraint. In addition, Yada is designed to take the 2AFC outcomes as input. The other algorithms would have to undergo lossy preprocessing in order to work with the 2AFC outcomes. Furthermore, Yada provides a simple method to compute the backward mapping  X   X  1 . In this article, we presented Yada X  X  novel algorithm for finding a mapping from an objective to a subjective similarity space, or vice versa which is a central problem in psychology. We formulated Yada as a general metric-learning problem by maximiz-ing a regularized log-likelihood on 2AFC experiment outcomes. Yada not only learns the relation between objective distances and subjective distances but also produces forward and backward mappings between the two spaces. These mappings provide convenient tools to help psychologists understand human cognitive processes like learning and categorization, and to design behavioral experiments assessing these abilities. We conducted experiments on synthetic and real datasets to compare the performance of Yada with several baseline algorithms. The results show that Yada can best utilize the information from 2AFC experiments and recover the underlying mapping  X  .
 One future direction to enhance our algorithm is to generate one-to-one mappings. For some types of stimuli, we expect the mappings to be one-to-one. Currently, the mapping generated by Yada may be many-to-one. Furthermore, we are also interested in mappings with monotonic properties. For example, if length is one of the features in the objective space, we may want the mapping to be monotonic in length. This extra constraint may help us avoid overfitting an d produce more interpretable results.
Some prior work attempted to infer a metric back from MDS solutions. One example is the ISOMAP algorithm [Tenenbaum et al. 2000], where a neural network was trained to perform the mapping from the input space to the embedded space once coordinates in the embedded space had been identified. The ISOMAP approach is ca-pable of generating subjective distances for novel items situated in an objective space. In future work, it would be interesting to compare the behavior of ISOMAP to our approach.

Since human participants can only make a limited number of judgments, care-fully choosing informative pairs to experiment with is very important. Currently, our method passively accepts 2AFC experiment results designed by psychologists as input. If it can adaptively choose informative pairs as in Tamuz et al. [2011], the algorithm may achieve better performance with the same number of pairs.

