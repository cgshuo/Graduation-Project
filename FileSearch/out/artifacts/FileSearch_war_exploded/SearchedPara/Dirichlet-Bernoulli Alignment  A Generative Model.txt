 represented in a universal space. What makes the problem mor e complicated and challenging is Traditional classification algorithms are typically incap able of handling such complications. Even for corpora consisting of relatively homogenous data, treating the tasks as M 3 C might still be advantageous since it enables us to explore the inner stru ctures and the ambiguity of the data focused than the whole document), base the classification on the whole document would incur too both the exact location and class label of each object in the i mage, which, though not completely Recently, the Latent Dirichlet Allocation (LDA, [4]) model has been established for automatic ex-as a mixture over topics, LDA allows each document to be assoc iated with multiple topics with different proportions, and thus provides a promising way to capture the heterogeneity/ambiguity in explicit topic to gain discriminative power from the data. T hrough likelihood maximization, DBA problem, i.e., named entity disambiguation for web search q ueries. The experiments confirm the usefulness of the proposed DBA model.
 of our model. Section 4 introduces the detailed DBA model. In Section 5, we establish algorithms for inference and parameter estimation for DBA. And in Secti on 6, we apply the DBA model to text classification and query disambiguation tasks. Finally, Se ction 7 presents concluding remarks. associated with multiple classes. Although both MLC and MIC have drawn increasing attentions in i.e, the number of sub-tasks can be huge, making the training data extremely sparse for each sub-al proposed a discriminative framework [6] based on convex s urrogate loss minimization for clas-sifying ambiguously labeled images; and Xu et al establishe d a hybrid generative/discriminative approach (i.e., a heuristically regularized LDA classifier ) [12] to mining named entity from web search click-through data. In this paper, we present a generative approach for M 3 C. Our proposed DBA model can be viewed as a supervised version o f topic models. A widely used topic model for categorical data is the LDA model [4]. By mode ling a pattern as a random mixture over latent topics and a topic as a Multinomial distribution over features in a dictionary, LDA is effective in discovering implicit topics from a corpus. The supervised LDA (sLDA) model [2], by Linear Models. However, both LDA and sLDA are in essence dime nsionality reduction techniques, pattern class instance sentation of the DBA model with multinomial bag-of-feature instance model. the set of class labels, and F = { f label multi-instance corpus D consists of a set of input patterns { X corresponding labels { Y of instances x make the following assumptions.
 belongs to a single class.
 These assumptions are equivalent to assuming a tree structu re for the corpus (Figure 1(a)). In this section, we present Dirichlet-Bernoulli Alignment (DBA), a probabilistic generative model pattern X in a corpus D is assumed to be generated by the following process: We assume the total number of predefined classes, C , is known and fixed. In DBA, a = [ a 1 , . . . , a C ]  X  with a c &gt; 0 , c = 1 , . . . , C tion Dir( a ), which is defined in the ( C -1)-simplex:  X  a binary C -vector with the 1-of-C code: z y and y ing to the labels of the instances in it, which is intuitively reasonable. As a result, y generated from a Bernoulli distribution, i.e., p ( y is the average of z  X   X  Dir(  X  1  X  z 1 , . . . ,  X  C  X  z C ). In this paper, we use a logistic model: videos on a web page. Without loss of generality, we follow th e convention of topic models to assume that each instance x is a bag of discrete features { f [ b 1 , . . . , frequency of f The graphical model for DBA is depicted in Figure 1(b). We can see that DBA has a diagram very pattern as well as the instances within it. probabilities. We use variational methods to approximate t hose distributions. 5.1 Variational Approximations tion of the latent variables: where  X  and  X  =[  X  where KL ( q ( x ) || p ( x )) = R distributions p and q , and L ( ) is the variational lower bound for the log-likelihood: tional expectation of the log likelihood for instance obser vations is: the labels given the topic assignments: We bound the second term above by using the lower bound for log istic function [9]: where  X  = [  X  term is omitted since the lower bound is exact when  X  updating the variational parameters according to the follo wing formulas: where  X ( ) is the digamma function. Note that instead of only one featur e contributing to  X  tends to make DBA more robust to data sparsity. Also, DBA make s use of the supervision infor-mation with a term P C longs: { max  X  the classes to which the pattern does not belong: { min  X  the name Dirichlet-Bernoulli Alignment . 5.2 Parameter Estimation The maximum likelihood parameter estimation of DBA relies o n the variational approximation pro-cedure. Given a corpus D = { ( X
Data Set #Train #Test D C | Y | Text 1200 679 500 10 1.4 721 (38.4%) 8.2 1 36
Query 300 100 2000 101 1.4 99 (24.8%) 65 3 731 Figure 2: Accuracies(%) of DBA, MNB, MIMLSVM, and MIMLBoost for text classification. The two-layer optimization in Eq.(10) involves two groups o f parameters corresponding to the DBA groups leads to a Variational Expectation Maximization (VE M) algorithm similar to the one used in And the M-step in turn maximizes the objective in Eq.(6) w.r. t. the model parameters. These two steps are repeated alternatively until convergence. 5.3 Inference tern classification, addresses prediction of labels for a ne w pattern X : p ( y exp(  X  c  X   X  c ) / (1 + exp(  X  c  X   X  c )) , where  X   X  c = 1 for each instances within a pattern: p ( z p ( z mc = 1 | X, y ) =  X  mc . search queries. Table 1 shows the information of the data set s used in our experiments. 6.1 Text Classification This experiment is conducted on the ModApte split of the Reuters-21578 text collection, which contains 10788 documents belonging to the most popular 10 cl asses. We use the top 500 words with the highest document frequency as features, and represent e ach document as a pattern with each of of 1879 documents, among which 721 documents (about 38.4%) h ave multiple labels. The average number of labels per document is 1.4  X  0.6 and the average number of instances (paragraphs) per documents for training and the rest for testing.
 For comparison, we also test two state-of-the-art M 3 C algorithms, the MIMLSVM and MIMLBoost whole documents as the baseline. For a fair comparison, line ar kernel is used in both MIMLSVM We use the Hamming-Accuracy [13] to evaluate the results, fo r DBA and MNB, the label is esti-on 5-fold cross validation. Each experiment is repeated for 5 random runs and the average results M 3 C algorithms outperform the MNB baseline; (2) the performan ce of DBA is at least comparable with MIMLBoost and MIMLSVM. For most classes and overall , DBA performs the best, whereas for some classes, MIMLBoost and MIMLSVM perform even slight ly worse than MNB. A possi-ble reason might be: if the documents are very short, splitti ng them might introduce severe data sparseness and in turn harms the performance. We also observ e that DBA is much more efficient than MIMLBoost and MIMLSVM. For training, DBA takes 42 mins o n average, in contrast to 557 minutes (MIMLSVM) and 806 minutes (MIMLBoost). 6.2 Named Entity Disambiguation this section, we employ DBA to disambiguate the named entiti es in web search queries. This is a very challenging problem because queries are usually very s hort (2 to 3 words on average), noisy precise information need of the user and in turn improve sear ch by responding with the truly most relevant documents. For example, when a user inputs  X  When are the casting calls for Harry Potter in USA?  X , the system should be able to identify that the ambiguous na med entity  X  Harry Potter  X  (i.e., it can be a movie , a book or a game ) really refers to a movie in this specific query. We treat the ambiguity of e as a hidden class z over e and make use of the query log as a data source for mining the relationship among e , w and z . In particular, the query log can be viewed as a multi-class, multi-label and multi-instance corpus { ( X tern X corresponds to a named-entity e and is characterized by a set of instances { x corresponding to all the contexts { w contains all the ambiguities of e .
 Our data was based on a snapshot of answers.yahoo.com crawled in early 2008, containing 216563 queries from 101 classes. We manually collect 400 named entities and label th em according are used as training data and the other 100 are used for testin g. We compare our DBA based method TFIDF ) as word attributes, and SVM classifier using TF ( SVM-TF ) or TFIDF ( SVM-TF-IDF ). For Table 2 demonstrates the Accuracy@ N ( N = 1 , 2 , 3 ) as well as micro-averaged and macro-average cross-validation. From the table, we observe that DBA achie ves significantly better performance than all the other methods. In particular, for Accuracy@1 sc ores, DBA can achieve a gain of about 30% relative to two MNB methods, and about 10% relative to two SVM methods; for macro-average F-measures, DBA can achieve a gain of about 50% over MNB methods, and about 25% over SVM achieve a gain of more than 50% relative to MNB and SVM baselines. named-entity disambiguation tasks.
 experiment, we found that substantial improvement could be achieved by simply enforcing label moniousness X  in a principled way. Another meaningful inves tigation would be to explicitly capture than Dirichlet.
 Hongyuan Zha is supported by NSF #DMS-0736328 and grant from Microsoft. Bao-Gang Hu is supported by NSFC #60275025 and the MOST of China grant #2007 DFC10740.
 [1] Andrews S. and Hofmann T. (2003) Multiple Instance Learn ing via Disjunctive Programming [2] Blei D. and McAuliffe J. (2007) Supervised topic models. In Advances in Neural Information [4] Blei D., Ng A. and Jordan M. (2003) Latent Dirichlet Alloc ation. Journal of Machine Learning [5] Boutell M. R., Luo J., Shen X. and Brown C. M. (2004) Learni ng Multi-Label Scene Classifi-[6] Cour T., Sapp B., Jordan C. and Taskar B. (2009) Learning f rom Ambiguously Labeled Images, [8] Ghamrawi N. and McCallum A. (2005) Collective Multi-Lab el Classification, In ACM Interna-[9] Jaakkola, T. and Jordan M. I. (2000). Bayesian parameter estimation via variational methods. [10] Ueda N. and Saito K. (2002) Parametric Mixture Models Fo r Multi-Labeled Text. In Advances [11] Viola P., Platt J. and Zhang C. (2006). Multiple Instanc e Boosting For Object Detection. In [12] Xu G., Yang S.-H. and Li H. (2009) Named Entity Mining fro m Click-Through Data Using [13] Zhou Z.-H. and Zhang M.-L. (2006) Multi-Instance Multi -Label Learning with Application to
