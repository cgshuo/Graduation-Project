 available relational data, which can be as complex as citation graphs, the World Wide Web, or rela-tional databases. Relational Markov Networks (RMNs) are excellent tools to capture the statistical dependency among entities in a relational dataset, as has been shown in many tasks such as col-lective classification [22] and information extraction [18][2]. Unlike Bayesian networks, RMNs avoid the difficulty of defining a coherent generative model, thereby allowing tremendous flexibility in representing complex patterns [21]. For example, Markov Logic Networks [10] can be auto-matically instantiated as a RMN, given just a set of predicates representing attributes and relations among entities. The algorithm can be applied to tasks in different domains without any change. Relational Bayesian networks [22], in contrary, would require expert knowledge to design proper model structures and parameterizations whenever the schema of the domain under consideration is changed. However, this flexibility of RMN comes at a high price in training very complex models. For example, work by Kok and Domingos [10][11][12] has shown that a prominent problem of re-lational undirected models is how to handle the exponentially many features, each of which is an conjunction of several neighboring variables (or  X  X round atoms X  in terms of first order logic). Much computation is spent on proposing and evaluating candidate features.
 The main goal of this paper is to show that instead of learning a very expressive relational model, which can be extremely expensive, an alternative approach that explores Hidden Variable Detection (HVD) to compensate a family of restricted relational models (e.g., treeRMNs) can yield a very efficient yet competent relational learning framework. First, to achieve efficient inference, we intro-duce a restricted class of RMNs called relation tree-based RMNs (treeRMNs), which only considers unary (single variable assignment) and pairwise (conjunction of two variable assignments) features. Since the Markov blanket of a variable is concisely defined by a relation tree on the schema, we can easily control the complexities of treeRMN models. Second, to compensate for the restricted expressive power of treeRMNs, we further introduce a hidden variable induction algorithm called Contrastive Variable Induction (CVI), which can effectively detect latent variables capturing long range dependencies. It has been shown in relational Bayesian networks [24] that hidden variables can help propagating information across network structures, thus reducing the burden of extensive structural learning. In this work, we explore the usefulness of hidden variables in learning RMNs. Our experiments on four real datasets show that the proposed relational learning framework can achieve similar prediction quality to the state-of-the-art RMN models, but is significantly more ef-ficient in training. Furthermore, the induced hidden variables are semantically meaningful and are crucial to improving training speed of treeRMN.
 In the remainder of this paper, we first briefly review related work and training undirected graphical models with mean field contrastive divergence. Then we present the treeRMN model and the CVI algorithm for variable induction. Finally, we present experimental results and conclude this paper. There has been a series of work by Kok and Domingos [10][11][12] developing Markov Logic Networks (MLNs) and showing their flexibility in different applications. The treeRMN model we introduced in this work is intended to be a simpler model than MLNs, which can be trained more efficiently, yet still be able to capture complex dependencies. Most of the existing RMN models construct Markov networks by applying templates to entity relation graphs [21][8]. The treeRMN model that we are going to introduce uses a type of template called a relation tree, which is very general and applicable to a wide range of applications. This relation tree template resembles the path-based feature generation approach for relational classifiers developed by Huang et al. [7]. Recently, much work has been done to induce hidden variables for generative Bayesian networks [5][4][16][9][20][14]. However, previous studies [6][19] have pointed out that the generality of Bayesian Networks is limited by their need for prior knowledge on the ordering of nodes. On the other hand, very little progress has been made in the direction of non-parametric hidden variable models based on discriminative Markov networks (MNs). One recent attempt is the Multiple Re-lational Clustering (MRC) [11] algorithm, which performs top-down clustering of predicates and symbols. However, it is computationally expensive because of its need for parameter estimation when evaluating candidate structures. The CVI algorithm introduced in this work is most similar to the  X  X deal parent X  algorithm [16] for Gaussian Bayesian networks. The  X  X deal parent X  evaluates can-didate hidden variables based on the estimated gain of log-likelihood they can bring to the Bayesian network. Similarly, the CVI algorithm evaluates candidate hidden variables based on the estimated gain of an regularized RMN log-likelihood, thus avoids the costly step of parameter estimation. Before describing our model, let X  X  briefly review undirected graphical models (a.k.a, Markov net-works). Since our goal is to develop an efficient RMN model, we use the simple but very efficient mean field contrastive divergence [23] method. Our empirical results show that even the simplest naive mean field can yield very promising results. Extension to using more accurate (but also more expensive) inference methods, such as loopy BP [15] or structured mean fields can be done similarly. Here we consider the general case that Markov networks have observed variables O , labeled vari-ables Y , and hidden variables H . Let X = ( Y , H ) be the joint of hidden and labeled variables. The is a vector of feature functions f k ;  X  is a vector of weights; Z (  X  ) = assume that the range of each variable is discrete and finite. Many commonly used graphical mod-els have tied parameters, which allow a small number of parameters to govern a large number of features. For example, in a linear chain CRF, each parameter is associated with a feature template : e.g.  X  X he current node having label y t = 1 and the immediate next neighbor having label y t +1 = 1  X . After applying each template to all the nodes in a graph, we get a graphical model with a large number of features (i.e., instantiations of feature templates). In general, a model X  X  order of Markov dependence is determined by the maximal number of neighboring steps considered by any one of its feature templates. In the context of relational learning, the templates can be defined similarly, except having richer representations X  X ith multiple types of entities and neighboring relations. Given a set of training samples D = { ( y m , o m ) } M m =1 , the parameter estimation of MN can be formulated as maximizing the following regularized log-likelihood where  X  and  X  are non-negative regularization constants for the  X  1 and  X  2 -norm respectively. Be-cause of its singularity at the origin, the  X  1 -norm can yield a sparse estimate, which is a desired property for hidden variable discovery, as we shall see. The differentiable  X  2 -norm is useful when there are strongly correlated features. The composite  X  1 / X  2 -norm is known as ElasticNet [27], which has been shown to have nice properties. The log-likelihood for a single sample is and its gradient is  X   X  l (  X  ) =  X  f  X  p For simple (e.g. tree-structured) MNs, message passing algorithms can be used to infer the marginal probabilities as required in the gradients exactly. For general MNs, however, we need approxi-mate strategies like variational or Monte Carlo methods. Here we use simple mean field variational method [23]. By analogy with statistical physics, the free energy of any distribution q is defined as Therefore, F ( p ) =  X  log Z (  X  ) , F ( p y ) =  X  log y free. Then F ( q 0 )  X  F ( q t )  X  F ( q  X  )  X  F ( p ) . As in [23], we set t = 1 , and use L orthant-wise L-BFGS [1] to estimate the parameters  X  . In the following, we formally define the treeRMN model with relation tree templates, which is very general and applicable to a wide range of applications.
 A schema S (Figure 1 left) is a pair ( T , R ) . T = { T i } is a set of entity types which include  X  P erson, Class  X  ). Each entity type is associated with a set of attributes A ( T ) = { T.A i } : e.g., A ( P erson ) = { P erson.gender } . R = { R } is a set of binary relations. We use dom ( R ) to denote the domain type of R and range ( R ) to denote its range. For each argument of a composite entity type, we define two relations, one with outward direction (e.g. P P 1 means from a Person-Person the inverse of a relation. We further introduce a T win relation, which connects a composite entity type to itself. Its semantics will be clear later. In principle, we can define other types of relations An entity relation graph G = I E ( S ) (Figure 1 right), is the instantiation of schema S on a set of basic entities E = { e i } . We define the instantiation of a basic entity type T as I E ( T ) = { e : I the composite entity made of the same basic entities but in reversed order. Therefore, we add the Figure 1: (Left) is a schema, where round and rectangular boxes represent basic and composite entity types respectively. (Right) is a corresponding entity relation graph with three basic entities: p1, p2, c1. For clarity we only show one direction of the relations and omit their labels. Given a schema, we can conveniently express how one entity can reach another entity by the con-cept of a relation path. A relation path P is a sequence of relations R 1 . . . R  X  for which the do-mains and ranges of adjacent relations are compatible X  X .e., range ( R i ) = dom ( R i +1 ) . We define phasize the types associated with each step in a path, we will write the path P = R 1 . . . R  X  as T so on. Note that, because some of the relations reflect one-to-one mappings, there are groups of  X 
P erson, Class  X  P C 1  X  X  X  X  X  X  P erson . To avoid creating these uninteresting paths, we add a constraint to outward composite relations (e.g. P P 1 , P C 1 ) that they cannot be immediately preceded by their inverse. We also constrain that the T win relation should not be combined with any other relations. Now, the Markov blanket of an entity e  X  T can be concisely defined by the set of all relation paths with domain T and of length  X   X  (as shown in Figure 2). We call this set the relation tree of type T , and denote it as T ree ( T,  X  ) = { P } . We define a unary template as T.A i = a , where A i is an attribute of type T , and a  X  range ( A i ) . This template can be applied to any entity e of type T in the entity relation graph. We define a pairwise template as T.A i = a e relation path P . For example, the following template can be applied to any person-person pair, and it fires whenever co-author=1 for this person pair, and the first person (identified as pp PP 1  X  X  X  X  X  X  p ) also have advise=1 with another person. Here we use p as a shorthand for the type P erson , and pp a shorthand for  X  P erson, P erson  X  . In our current implementation, we systematically enumerate all possible unary and pairwise templates. Given the above concepts, we define a treeRMN model M = ( G, f ,  X  ) as the tuple of an entity rela-tion graph G , a set of feature functions f , and their weights  X  . Each feature function f k counts the number of times the k -th template fires in G . Generally, the complexity of inference is exponential in the depth of the relation trees, because both the number of templates and their sizes of Markov blankets grow exponentially w.r.t. the depth  X  . TreeRMN provides us a very convenient way to con-trol the complexity by the single parameter  X  . Since treeRMN only considers pairwise and unary features, it is less expressive than Markov Logic Networks [10], which can define higher order features by conjunction of predicates; and treeRMN is also less expressive than relational Bayesian networks [9][20][14], which have factor functions with three arguments. However, the limited ex-pressive power of treeRMN can be effectively compensated for by detecting hidden variables, which is another key component of our relational learning approach, as explained in the next section. Algorithm 1 Contrastive Variable Induction initialize a treeRMN M = ( G, f ,  X  ) while true do end while return M As we have explained in the previous section, in order to compensate for the limited expressive power of a shallow treeRMN and capture long-range dependencies in complex relational data, we propose to introduce hidden variables. These variables are detected effectively with the Contrastive Variable Induction (CVI) algorithm as explained below.
 The basic procedure (Algorithm 1) starts with a treeRMN model on observed variables, which can be manually designed or automatically learned [13]; then it iteratively introduces new HVs to the model and estimate its parameters. The key to making this simple procedure highly efficient is a fast algorithm to evaluate and select good candidate HVs. We give closed-form expressions of the likelihood gain and the weights of newly added features under contrastive divergence approximation [23] (other type of inference can be done similarly). Therefore, the CVI process can be very efficient, only adding small overhead to the training of a regular treeRMN.
 Consider introducing a new HV H to the entity type T . In order for H to influence the model, it needs to be connected to the existing model. This is done by defining additional feature templates: butions of the hidden variable H on all entities of type T , f H is a set of pairwise feature templates that connect H to the existing model, and  X  H is a vector of feature weights. Here we assume that any feature f  X  f H is in the pairwise form f H =1  X  A = a , where a is the assignment to one of the existing variables A in the relation tree of type T . Ideally, we would like to identify the candidate HV, which gives the maximal gain in the regularized objective function L CD 1 (  X  ) . For easy evaluation of H , we set its mean field variational parameters  X  H to either 0 or 1 on the entities of type T . This yields a lower bound to the gain of L CD 1 (  X  ) . Therefore, a candidate HV second order Taylor expansion, we can show that for a particular feature f  X  f H the maximal gain is achieved at  X  f  X  augmented by the distribution of H parameterized by the index set I . q 0 and q 1 are the wake and sleep distributions estimated by 1-step mean-field CD. The estimations in Eq. (5) and (6) are simple, yet have nice intuitive explanations about the effects of the  X  1 and  X  2 regularizer as used in Eq. (1): a large  X  2 -norm (i.e. large  X  ) smoothly shrinks both the (estimated) likelihood gain and the feature weights; while the non-differentiable  X  1 -norm not only shrink the estimated gain and feature weights, but also drive features to have zero gains, therefore, can automatically select the features. If we assume that the gains of individual features are independent, then the estimated gain for H is However, finding the index set I that maximizes  X  I is still non-trivial X  X n NP-hard combinatory optimization problem, which is often tackled by top-down or bottom-up procedures in the clustering literature. Algorithm 2 uses a simple bottom up clustering algorithm to build a hierarchy of clusters. It starts with each sample as an individual cluster, and then repeatedly merges the two clusters that lead to the best increment of gain. The merging is stopped if the best increment  X  0 . After clustering, we introduce a single categorical variable that treats each cluster with positive gain as a category, and the remaining useless clusters are merged into a separate category. Introducing this categorical variable is equivalent to introducing a set of binary variables X  X ne for each cluster with positive gain. From the above derivation, we can see that the essential part of the CVI algorithm is to compute the expectations and variances of RMN features, both of which can be done by any inference procedures, including the mean field as we have used. Therefore, in principle, the CVI algorithm can be extended to use other inference methods like belief propagation or exact inference. Remark 1 after the induction step, the introduced HVs are treated as observations: i.e. their vari-free variables. This can potentially correct the errors made by the greedy clustering procedure. The cardinalities of HVs may be adapted by operators like deleting, merging, or splitting of categories. Remark 2 currently, we only induce HVs to basic entity types. Extension to composite types can show interesting tenary relations such as  X  X bnormality can be PartOf Animals X . However, this requires clustering over a much larger number of entities, which cannot be done by our simple implementation of bottom up clustering. In this section, we present both qualitative and quantitative results of treeRMN model. We demon-strate that CVI can discover semantically meaningful hidden variables, which can significantly im-prove the speed and quality of treeRMN models. 6.1 Datasets Basic Composite Table 1 shows the statistics of the four datasets used in our ex-periments. These datasets are commonly used by previous work in relational learning [9][11][20][14]. The Animal dataset con-tains a set of animals and their attributes. It consists exclusively of unary predicates of the form A ( a ) where A is an attribute and a is an animal (e.g., Swims(Dolphin)). This is a simple proposi-tional dataset with no relational structure, but is useful as a base case for comparison. The Nation dataset contains attributes of nations and relations among them. The binary predicates are of the form R ( n 1 , n 2 ) , where n 1 , n 2 are nations and R is a relation between them (e.g., ExportsTo , GivesEconomicAidTo ). The unary predicates are of the form A ( n ) , where n is a nation and A is a attribute (e.g., Communist ( China ) ). The UML dataset is a biomedical ontology called Unified Medical Lan-concepts and R is a relation between them (e.g., Treats(Antibiotic,Disease) ). The Kinship dataset contains kinship relationships among members of the Alyawarra tribe from Central Australia. Pred-animal data, the number of composite entities is the square of the number of basic entities. 6.2 Characterization of treeRMN and CVI In this section, we analyze the properties of the discovered hidden variables and demonstrate the behavior of the CVI algorithm. For the simple non-relational Animal data, if we start with a full model with all pairwise features, CVI will decide not to introduce any hidden variables. If we run CVI starting from a model with only unary features, however, CVI decides to introduce one hidden variable H 0 with 8 categories. Table 2 shows the associated entities and features for the first four categories. We can see that they nicely identify marine mammals, predators, rodents, and primates. Table 2: The associated entities and features (sorted by decreasing magnitude of feature weights) for the first four categories of the induced hidden variable a.H 0 on the Animal data. The features are in the form a.H 0 = C i Table 3: The associated entities and features (sorted by decreasing magnitude of feature weights) for the first three categories of the induced hidden variable c.H 0 on the UML data. The features are in the form c.H 0 = C i For the three relational datasets, we use UML as an example. The induction process of Nation and Kinship datasets are similar, and we omit their details due to space limitation. For the UML task, CVI induces two multinomial hidden variables H 0 and H 1 . As we can see from Figure 3, the inclusion of each hidden variable sig-nificantly improves the conditional log likelihood of the model.
 The first hidden variable C.H 0 has 43 categories, and Table 3 shows the top three of them. We can see that these categories represent the hidden concepts Abnormalities , Animals and Plants respectively. Abnormalities can be caused or treated by other con-cepts, and it can also be a part of other concepts. Plants can be the location of some other concepts; and some other concepts can be part of or the property of Animals . These grouping of concepts are similar to those reported by Kok and Domingos [11]. 6.3 Overall Performance Now we present quantitative evaluation of the treeRMN model, and compare it with other relational learning methods including MLN structure learning (MLS) [10], Infinite Relational Models (IRM) [9] and Multiple Relational Clustering (MRC) [11]. Following the methodology of [11], we situate our experiment in prediction tasks. We perform 10 fold cross validation by randomly splitting all the variables into 10 sets. At each run, we treat one fold as hidden during training, and then evaluate the prediction of these variables conditioned on the observed variables during testing. The overall performance is measured by training time, average Conditional Log-Likelihood (CLL), and Area Under the precision-recall Curve (AUC) [11]. All implementation is done with Java 6.0. Table 4 compares the overall performance of treeRMN (RMN), treeRMN with hidden variable dis-covery (RMN CV I ), and other relational models (MSL, IRM and MRC) as reported in [11]. We use subscripts (0, 1, 2) to indicate the order of Markov dependency (depth of relation trees), and dim  X  for the number of parameters. First, we can see that, without HVs, the treeRMNs with higher Markov orders generally perform better in terms of CLL and AUC. However, due to the complex-ity of high-order treeRMNs, this comes with large increases in training time. In some cases (e.g., Kinship data), a high order treeRMN can perform worse than a low order treeRMN probably due to the difficulty of inference with a large number of features. Second, training a treeRMN with CVI Table 4: Overall performance. Bold identifies the best performance, and  X  marks the standard deviations. Experiments are conducted with Intel Xeon 2.33GHz CPU (E5410).  X  These results were started with a treeRMN that only has unary features.  X  The CLL of kinship data is not comparable to previous approaches, because we treat each of its labels as one variable with 26 categories instead of 26 binary variables.  X  The results of existing methods were run on different machines (Intel Xeon 2.8GHz CPU), and their 10-fold data splits are independent to those used for the RMN models. They were allowed to run up to 10-24 hours, and here we assumes that these methods cannot achieve similar accuracy when the amount of training time is significantly reduced. is only 2  X  4 times slower than training a treeRMN of the same order of Markov dependency. On all three relational datasets, treeRMNs with CVI can significantly improve CLL and AUC. For the simple Animal dataset, the improvement is less significant because there is no long range depen-dency to be captured in this data. Although the CVI models have similar number features as the second order treeRMNs, their inferences are much faster due to their much smaller Markov blan-kets. Finally, on all datasets, the treeRMNs with CVI can achieve similar prediction quality as the existing methods (i.e., MSL, IRM and MRC), but is about two orders of magnitude more efficient in training. Specifically, it achieves significant improvements on the Animal and Nation data, but moderately worse results on the UML and Kinship data. Since both UML and Kinship data have no attributes in basic entity types, composite entities become more important to model. Therefore, we suspect that the MRC model achieves better performance because it can perform clustering on two-argument predicates which corresponds to composite entities. We have presented a novel approach for efficient relational learning, which consists of a restricted class of Relational Markov Networks (RMN) called relation tree-based RMN (treeRMN) and an efficient hidden variable induction algorithm called Contrastive Variable Induction (CVI). By using simple treeRMNs, we achieve computational efficiency, and CVI can effectively detect hidden vari-ables, which compensates for the limited expressive power of treeRMNs. Experiments on four real datasets show that the proposed relational learning approach can achieve state-of-the-art prediction accuracy and is much faster than existing relational Markov network models.
 We can improve the presented approach in several aspects. First, to further speedup the treeRMN model we can apply efficient Markov network feature selection methods [17][26] instead of sys-tematically enumerating all possible feature templates. Second, as we have explained at the end of section 5, we X  X  like to apply HVD on composite entity types. Third, we X  X  also like to treat the introduced hidden variables as free variables and to make their cardinalities adaptive. Finally, we would like to explore high order features which involves more than two variable assignments. Acknowledgements.
 We gratefully acknowledge the support of NSF grant IIS-0811562 and NIH grant R01GM081293. [1] Galen Andrew and Jianfeng Gao. Scalable training of  X  1 -regularized log-linear models. In [2] Razvan C. Bunescu and Raymond J. Mooney. Collective information extraction with relational [3] Miguel A. Carreira-Perpinan and Geoffrey E. Hinton. On contrastive divergence learning. In [4] Gal Elidan and Nir Friedman. The information bottleneck em algorithm. In UAI , 2003. [5] Gal Elidan, Noam Lotner, Nir Friedman, and Daphne Koller. Discovering hidden variables: A [6] Nir Friedman, Lise Getoor, Daphne Koller, and Avi Pfeffer. Learning probabilistic relational [7] Yi Huang, Volker Tresp, and Stefan Hagen Weber. Predictive modeling using features derived [8] Ariel Jaimovich, Ofer Meshi, and Nir Friedman. Template-based inference in symmetric rela-[9] Charles Kemp, Joshua B. Tenenbaum, Thomas L. Griffiths, Takeshi Yamada, and Naonori [10] Stanley Kok and Pedro Domingos. Learning the structure of Markov logic networks. In ICML , [11] Stanley Kok and Pedro Domingos. Statistical predicate invention. In ICML , 2007. [12] Stanley Kok and Pedro Domingos. Learning Markov logic networks using structural motifs. [13] Su-In Lee, Varun Ganapathi, and Daphne Koller. Efficient structure learning of Markov net-[14] Kurt T. Miller, Thomas L. Griffiths, and Michael I. Jordan. Nonparametric latent feature mod-[15] Kevin P. Murphy, Yair Weiss, and Michael I. Jordan. Loopy belief propagation for approximate [16] Iftach Nachman, Gal Elidan, and Nir Friedman.  X  X deal parent X  structure learning for continu-[17] Simon Perkins, Kevin Lacker, and James Theiler. Grafting: Fast, incremental feature selection [18] Hoifung Poon and Pedro Domingos. Joint inference in information extraction. In AAAI , 2007. [19] Karen Sachs, Omar Perez, Dana Peer, Douglas A. Lauffenburger, and Garry P. Nolan. Causal [20] Ilya Sutskever, Ruslan Salakhutdinov, and Josh Tenenbaum. Modelling relational data using [21] Benjamin Taskar, Pieter Abbeel, and Daphne Koller. Discriminative probabilistic models for [22] Benjamin Taskar, Eran Segal, and Daphne Koller. Probabilistic classification and clustering in [23] Max Welling and Geoffrey E. Hinton. A new learning algorithm for mean field Boltzmann [24] Zhao Xu, Volker Tresp, Kai Yu, and Hans-Peter Kriegel. Infinite hidden relational models. In [25] Alan Yuille. The convergence of contrastive divergence. In NIPS , 2004. [26] Jun Zhu, Ni Lao, and Eric P. Xing. Grafting-light: Fast, incremental feature selection and [27] Hui Zou and Trevor Hastie. Regularization and variable selection via the elastic net. In Journal
