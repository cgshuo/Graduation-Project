  X  Documents in many corpora, such as digital libraries and webpages, contain both content and link information. To explicitly consider the document relations represented by links, in this paper we propose a citation-topic (CT) model which assumes a probabilistic generative process for corpora. In the CT model a given document is modeled as a mixture of a set of topic distributions, each of which is borrowed (cited) from a document that is related to the given docu-ment. Moreover, the CT model contains a random process for selecting the related documents according to the struc-ture of the generative model determined by links and there-fore, the transitivity of the relations among documents is captured. We apply the CT model on the document clus-tering task and the experimental comparisons against sev-eral state-of-the-art approaches demonstrate very promising performances.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  Retrieval models Algorithms, Experimentation Topic model, document clustering
One of the most fundamental problems in the informa-tion retrieval field is to characterize the content of docu-ments. By capturing the essential characteristics in docu-ments, one gives documents a new representation, which is often more parsimonious and less noise-sensitive. Among the existing methods that extract essential characteristics from documents, topic model plays a central role. Topic models extract a set of latent topics from a corpus and as a consequence represent documents in a new latent semantic space. One of the well-known topic models is the Proba-bilistic Latent Semantic Indexing (PLSI) model proposed by Hofmann [4]. In PLSI each document is modeled as a prob-abilistic mixture of a set of topics. Going beyond PLSI, Blei et al. [1] presented the Latent Dirichlet Allocation (LDA) model by incorporating a prior for the topic distributions of documents. In these probabilistic topic models, one as-sumption underpinning the generative process is that the documents are independent. However, this assumption does not always hold true in practice, because documents in a cor-pus are usually related to each other in certain ways. Very often, one can explicitly observe such relations in a corpus, e.g., through the citations and co-authors of a paper. In such a case, these observations should be incorporated into topic models in order to derive more accurate latent topics that better reflect the relations among the documents.
In this paper, we propose a generative model, called the citation-topic (CT) model, for modeling linked documents that explicitly considers the relations among documents. In our model, the content of each document is a mixture of two sources: (1) the topics of the given document and (2) the topics of the documents that are related to (e.g., cited by) the given document. This perspective actually reflects the process of writing a scientific article: the authors probably first learn knowledge from the literature and then combine their own creative ideas with the learned knowledge to form the content of the paper. Furthermore, to capture the indi-rect relations among documents, our model contains a gener-ative process to select related documents where the related documents are not necessarily directly linked to the given document. We apply the CT model to the document clus-tering task and the experimental comparisons against sev-eral state-of-the-art approaches demonstrate very promising performances.
Suppose that the corpus consists of N documents { d j } N in which M distinct words { w i } M i =1 occur. Each document d mighthaveasetofcitations C d , and so the documents are linked together by these citations.

The CT model assumes the following generative process for each word w in the document d in the corpus. 1. Choose a related document c from p ( c | d,  X  ), a multi-2. Choose a topic z from the topic distribution of the 3. Choose a word w which follows the multinomial distri-As a result, one obtains the observed pair ( d, w ), while the latent random variables c, z are discarded. To obtain a doc-ument d , one repeats this process | d | times, where | d length of the document d . The corpus is obtained once ev-ery document in the corpus is generated by this process, as shown in Fig. 1. In this generative model, the dimension-ality K of the topic variable z isassumedknownandthe document relations are parameterized by an N  X  N matrix  X  where  X  lj = p ( c = l | d = j ), which is computed from the citation information of the corpus.

Following the maximum likelihood principle, one estimates the parameters by maximizing the log-likelihood function where n ( w i ,d j ) denotes the number of the times w i occurs in d j . According to the above generative process, the log-likelihood function can be rewritten as the following equa-tion L = The expectation-maximization (EM) algorithm can be ap-plied to estimate the parameters.

The document relation matrix  X  is computed from the citation information of the corpus. Suppose that the docu-ment d j has a set of citations Q d j .Amatrix S is constructed to denote the direct relationships among the documents in this way: S lj =1 / | Q d j | for d l  X  Q d j and 0 otherwise, where |
Q d j | denotes the size of the set Q d j . A simple method to ob-tain  X  is to set  X  = S . However, this strategy only captures direct relations among the documents and overlooks indirect relationships. To capture this transitive property, we choose a related document by a random walk on the directed graph represented by S . The probability that the random walk stops at the current node (and therefore chooses the current document as the related document) is specified by a param-eter  X  . According to the properties of random walk,  X  can be obtained by  X  =(1  X   X  )( I  X   X  S )  X  1 .
In this section, we investigate the document clustering task on a standard dataset Cora with the citation informa-tion available. Cora [5] contains the papers published in the conferences and journals of the different research areas in computer science, such as artificial intelligence, information retrieval, and hardware. A unique label has been assigned to each paper to indicate the research area it belongs to. These labels serve as the ground truth in our performance studies. In the Cora dataset, there are 9998 documents where 3609 distinct words occur.

By representing documents in terms of latent topic space, topic models can assign each document to the most proba-ble latent topic according to the topic distributions of the documents. For the evaluation purpose, we compare the CT model with the following representative clustering methods. 1. Traditional K-means. 2. Spectral Clustering with Normalized Cuts (Ncut) [6]. 3. Nonnegative Matrix Factorization (NMF) [7]. 4. Probabilistic Latent Semantic Indexing (PLSI) [4]. 5. Latent Dirichlet Allocation (LDA) [1]. 6. PHITS [2]. 7. PLSI+PHITS, which corresponds to  X  =0 . 5in[3]. We adopt the evaluation strategy in [7] for the clustering performance. The test data used for evaluating the cluster-ing methods are constructed by mixing the documents from multiple clusters randomly selected from the corpus. The evaluations are conducted for different number of clusters K . At each run of the test, the documents from a selected number K of clusters are mixed, and the mixed document set, along with the cluster number K , is provided to the clus-tering methods. For each given cluster number K ,20test runs are conducted on different randomly chosen clusters, and the final performance scores are obtained by averaging the scores over the 20 test runs.
 The parameter  X  is simply fixed at 0.99 for the CT model. The accuracy comparisons with various numbers of clusters are reported in Fig. 2, which shows that CT has the best per-formance in terms of accuracy and the relationships among the documents do offer help in the document clustering. Figure 2: Accuracy comparisons (the higher, the better).
A novel probabilistic generative citation-topic (CT) model is presented in this paper. The model incorporates the re-lationships among documents and models each document as a distribution over a set of topics, which is a mixture of the distributions associated with the related documents. The experimental comparisons against state-of-the-art ap-proaches demonstrate very promising performances. This work is supported in part by an internship at NEC Laboratories America, Inc. and the NSF (IIS-0535162, IIS-0812114).
