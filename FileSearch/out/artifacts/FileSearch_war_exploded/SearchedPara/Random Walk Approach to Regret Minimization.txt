 distributions over convex bodies .
 crucial for linear optimization with limited feedback.
 number of advantages over the other random walks.
 minimization problems.
 O (  X  the number of steps necessary per round would tend to infinity as T tends to infinity.  X  supported on K . Define the following sequence of functions Regularized Leader and Mirror Descent methods [23, 5], [7, Therem 11.1]. Specializing to the case ! ( x )  X  [0 , 1] over K , distribution p U and the prior  X  0 is bounded, the result yields O (  X  x of optimization intriguing.
 is the same as time spent inverting a Hessian matrix, which is O ( n 3 ) or less. q stant. Therefore, regret of Algorithm 1 is within O ( particular, by choosing  X  appropriately, for an absolute constant C $ , bounded losses  X  3.1 Bounding Mixing Time h , . . . , h k  X  R n and x  X  int ( K ) , for k  X  1 , we recursively define of D 2 F ( x )[ v, v ] , and to a pair of vectors v, w , the inner product  X  v, w  X   X  ( x, y ) = inf  X  on R n such that
G x ( y )  X  exp K that possesses a density whose logarithm is concave. Then, Algorithm 1 One Step Random Walk ( X t , s t ) Input : current point X t  X  K and scaled cumulative cost s t . Output : next point X t +1  X  K Toss a fair coin. If Heads , set X t +1 := X t .
 Else , close-by points, their transition distributions cannot be far apart. Lemma 4. If x, y  X  K and  X  ( x, y )  X  r Theorem 3 and Lemma 4 together give a lower bound on conductance of the Markov Chain. the reduction of distance between distributions.
 the k th step. Let M := sup every bounded f , let -f -2 ,  X  denote x to walk ( k =1 in Theorem 6) is close to the desired distribution  X  t . 3.2 Tracking the distributions Let {  X  i }  X  - X  - X  for a measurable function f : K  X  R , -f -i = i +1 are comparable: The mixing results of Lemma 5 together with Theorem 6 imply Corollary 7. For any i , is close to  X  i for all i (Theorem 9).
 Lemma 8. For any i , it holds that Proof. and f  X  ( x ) = min(0 ,f ( x )) . By the triangle inequality, Now, using (4) and (5), Thus, (6) is bounded as 1 1 1 1 Next, a bound on (7) follows simply by the norm comparison inequality (5): The statement follows by rearranging the terms.
 Theorem 9. If Consequently, for all i " Proof. By Corollary 7 and Lemma 8, we see that Since  X   X  = o ( 1 x d s ( x )=0 . For x  X  K and a vector v , | v | x is defined to be sup measure on M , the isoperimetric constant is defined as  X  ( d Nesterov and Todd [22, Lemma 3.1], whenever -x  X  y -x &lt; 1 . From (9) lim y  X  x  X  ( x,y ) Hence, using Theorem 10, the Hilbert metric and the Riemannian metric satisfy on K that possesses a density whose logarithm is concave, then Proof of Lemma 5 . Let S 1 be a measurable subset of K such that  X  ( S 1 )  X  1 its complement. Let S $ reversibility of the chain, which is easily checked, If x  X  S $ 1 and y  X  S $ 2 then, Lemma 4 implies that if  X  ( x, y )  X  r Therefore Theorem 3 implies that First suppose  X  ( S $ and we are done. Otherwise, without loss of generality, suppose  X  ( S $ and we are done.
 Proof of Lemma 1 . We have that D Rearranging, canceling the telescoping terms, and using the fact that Z 0 =1 Let U be a random variable with a probability distribution p U . Then Combining, Now, from Eq. (11), the KL divergence can be also written as D Lemma A.1 in [7]. This proves the second part of the lemma.
