  X  Search result examination is an important part of search-ing. High page load latency for landing pages (clicked re-sults) can reduce the efficiency of the search process. Proac-tively prefetching landing pages in advance of clickthrough can save searchers valuable time. However, prefetching con-sumes resources that are wasted unless the prefetched results are requested by searchers. Balancing the costs in prefetch-ing particular results against the benefits in reduced latency to searchers represents the search result prefetching chal-lenge. We present methods that leverage searchers X  cursor movements on search result pages in real time to dynami-cally estimate the result that searchers will request next. We demonstrate through large-scale log analysis that our ap-proach significantly outperforms three strong baselines that prefetch results based on (i) the search engine result rank-ing, (ii) past clicks from all searchers for the query, or (iii) past clicks from the current searcher for the query. Our promising findings have implications for the design of search support that makes the search process more efficient.
Search result selection is a core part of the Web search ex-perience. Following the generation of a search engine result page (SERP) comprised of candidate results, searchers must select results of interest and wait for them to load. While the latency in SERP generation has been well studied and shown to impact measures of the search experience (e.g., higher SERP generation latency leads to higher dissatisfac-tion and reduced SERP engagement) [2, 38], the relationship between latency and the loading of landing pages is less well understood. Despite a lack of research on this topic in the Web search community, general research on latencies in in-teractions with computer systems [39] and interactions with Web pages in particular [32] suggests that it has a significant impact on the overall experience. Methods to proactively fetch the content of particular search results before they are  X 
Author order alphabetical.  X  Work conducted while at Microsoft.
 Figure 1: Example of cursor-based prefetching. The model estimate of whether the searcher will select one of the top three results over time (at 50ms, 100ms, 150ms) is shown alongside each link (as Score ). These estimates change as a function of the searcher X  X  cursor movements. The cursor trail and the trajectory towards the second result are high-lighted in the figure with solid and dashed lines respectively. When the model score for a result reaches a certain threshold (0.8 for Search Result 2), that result is prefetched. selected could benefit searchers, while balancing the costs involved in downloading content that is never viewed; we define this as the search result prefetching challenge .
To address this challenge, the prefetching system needs to predict which result the user will select next. In previ-ous research, such predictions are usually made via static estimates learned from historic usage data [13, 14, 22, 26, 45]. For example, search engines prefetch the top result for queries where there is little variation in intent (primarily navigational queries [1]). However, these methods are only applied for small sets of queries where the dominant intent is clearly defined and observable via prior click patterns.
In contrast, we propose to address this challenge by dy-namically updating the estimates of likelihood of each result being selected during interaction with a SERP; when the system is confident that a link will be selected, the page is prefetched (Figure 1). Our method leverages the mouse cur-sor movements on the SERP (which can now be collected in a scalable manner [8, 20]), contextualized by the query, the results, and the searcher X  X  historic activity to make real-time predictions . If we can correctly prefetch early enough, we can save people significant time X  X his is especially important for those accessing the Web on low-bandwidth connections, who may need to be more selective about the results that they view. Indeed, on average, our methods save searchers 650ms per query for the 65% of queries where we correctly prefetch the clicked result.

We make the following contributions with our research:  X  Introduce the search result prefetching challenge and es-timate the scope of its potential impact on searchers and their search experience (in terms of the fraction of query volume and average time savings per query of our method);  X  Develop machine-learned models that leverage a rich array of features to prefetch search results pre-click;  X  Experiment with large-scale logs and demonstrate gains over strong baselines, including variations for different query types. We also identify the important feature classes in the learned models via ablation experiments, and;  X  Present implications of result prefetching for search sys-tem design and for society (e.g., enabling more rapid in-formation access for those on slower Internet connections).
The remainder of this paper is structured as follows. Sec-tion 2 describes related work in areas such as prefetching, the impact of response latency on user engagement, and mouse cursor monitoring. Section 3 motivates our research by demonstrating the potential impact of our prefetching system. Section 4 presents a formal description of the search result prefetching challenge. Section 5 describes the pre-fetching approach, including the data used, the features gen-erated, and the models that result. In Section 6, we describe our experimental setup, including datasets and evaluation. In Section 7, we present the results of our experiments, demonstrating the effectiveness of our method compared to baselines. Section 8 discusses our findings, their limitations given the data and the problem setting, and their impli-cations for the design of search systems. We conclude in Section 9 and present opportunities for future work.
The following areas are relevant: (a) latency effects in human-computer interaction and SERP generation, (b) meth-ods to reduce latency during search, and (c) using cursor movements collected in laboratory settings and at scale.
Since the early days of human computer interaction, re-searchers have studied the influence of system response time on the success, speed, and satisfaction of interactions [31, Chapter 5]. These studies have found that rapid responses (i.e., less than a second) are preferred and can increase user productivity [31]. Shneiderman [39] reviewed the literature on computer response time and recommended that comput-ers should respond immediately, in part based on limitations in human short-term memory [27]. Web page download time is affected by factors such as browser performance, Internet connection speed, local network traffic, load on the remote host, and the structure and format of requested content. Download speeds are an important aspect of the user ex-perience [32]. Studies have examined the interplay between page load time and the user experience, including tolerable wait times [30] and their psychological impact [36].
Intensive research and engineering efforts have been de-voted to achieving low latency in large, complex computing systems such as search engines [10]. Modern Web search engines deliver results rapidly because fast results are per-ceived as being of higher quality and searchers engage more with them. For example, Google conducted online experi-ments where they intentionally injected server-side delays, ranging from 100 to 400 milliseconds, into the search results to observe changes to people X  X  behavior. They found that increasing the load time of the SERP by as little as 100 milliseconds decreased the number of searches per person. These differences increased over time and persisted even af-ter the experiment ended [38]. In similar experiments, Bing added server delays ranging from 50 to 2000 milliseconds. They observed decreases in queries and clicks, and an in-crease in time to click, with larger effects with more delay [38]. Arapakis et al. employed user studies and large-scale log analysis, and showed that response latencies of 500ms were noticeable by searchers [2]. Recognizing the impor-tance of speed to searchers, Google added site speed (i.e., how quickly a Website responds to requests) as a relevance signal in search ranking [40]. Recent work on slow search discusses the cost-benefit tradeoffs in retrieving search re-sults quickly, and what could be accomplished given more time [41]. Beyond just making search faster (or slower), there are other reasons why page load latency matters, e.g., to address network bandwidth constraints [14].
To address latency in query responses, researchers have designed caches to rapidly serve results [5], including ways to leverage historic search behavior [13, 22, 26]. These methods limit the set of documents searched in response to queries, incurring increased infrastructure costs. Search engines al-ready try to reduce time to click by promoting popular re-sults [1]. Since repeat visits to the same result is common [43], search engines have promoted results that are likely to be selected; reducing the time for searchers to locate these results on SERPs. The Bing search engine already uses new browser capabilities to prefetch results that are highly likely to be selected [35]. This can enhance the search experience by reducing latency beyond SERP creation.

Moving beyond search engine support for latency reduc-tion, Padbanabham and Mogul [33] use N-hop Markov mod-els based on surfing patterns for improving pre-fetching strate-gies for Web caches. Fan et al. [14] leverage a user X  X  his-toric surfing activity and a Markov predictor tree to pre-dict future page accesses. Yang et al. [45] mined frequent sequences from Website access logs and employed them to derive association rules that could be used in prefetching de-cisions. More recently, White et al. [44] used recent search interactions and other contextual signals (e.g., incoming hy-perlinks) to predict searchers X  future topical interests given a Web page. Research on continual computation [18] proposed decision-theoretic methods for the ideal use of idle time for computational problem solving. These methods have been applied for selective (utility-directed) content prefetching, including the partial prefetching of specific Web page ele-ments, while balancing associated costs and benefits.
To be successful, all of these methods rely on popularity data and/or the sequence of visited Web pages. This may be reasonable for popular queries/pages or active users, but less applicable for other scenarios with less data (e.g., tail queries). The methods that leverage browsing data relies on being able to track sequences of Web page visits, which can require significant infrastructure, especially at scale across many Websites. It is also not clear the extent to which such an approach applies in search scenarios, where the results returned for a query are dynamically generated and the set of prefetching candidates may change over time [6].
We are focused on dynamically prefetching search result content once a query has been issued. Retrieving the top result for all queries or all results for all queries incurs a high cost in terms of resource consumption (primarily net-work bandwidth but also battery power on mobile devices). We hypothesize that by monitoring searcher attention in real-time on SERPs we can perform better prefetching than these crude methods. Lee et al. [25] performed prefetching of video materials via models learned from eye-gaze and cur-sor movements. They did so in a laboratory setting with ac-cess to gaze tracking technology. Recent research has shown that there is a strong association between gaze and cursor position [16, 20], especially around the time of a click [19]. This can be used to predict attention and intentions from cursor movement data [16, 19] in laboratory environments. Beyond controlled settings, recent work has shown that such methods can be deployed at scale online [8, 20]. This facil-itates a better understanding of search behavior [8], and enables predictions of SERP examination activity [11], im-proved relevance estimation [19], and ranking [23] based on common patterns in cursor signals. From this data we learn models to predict which results will be clicked in real time , and evaluate our models in a natural setting.

Human-computer interaction (HCI) researchers have pre-dicted aspects of cursor movement termination, focusing on endpoint prediction (i.e., predicting the terminal location of the cursor) and target prediction (i.e., deciding between multiple targets). Within HCI, these have traditionally been used in applications to expedite the acquisition of targets.
Simple versions of target prediction leverages distance from the mouse cursor [24] or consider angles between the move-ment vector and vectors for the target positions [29]. More sophisticated methods build probabilistic models based on previous clicks [46], or using kinematic template matching [34]. Although there are similarities between this work and ours, experiments in this area are generally conducted in carefully-controlled, artificial environments. On SERPs, there are many results to choose from, there are many aspects of the page competing for searcher attention in addition to the results (e.g., advertisements, related searches), and there are preconceived biases that affect where people click in the re-sult list, irrespective of content (e.g., positional biases [21]). All of these factors make the task of real-time click pre-diction on SERPs quite challenging, especially if searchers X  clicks disagree with the query X  X  aggregate click distribution. Our research extends previous work in a number of ways. First, we focus on prefetching in dynamic environments where the list of targets can change over time, even for the same query. Most prefetching methods rely on static predictions and transition probabilities learned from historic data. Sec-ond, many of the studies of end-point prediction focus on carefully controlled studies in laboratory settings. In con-trast, we operate in a non-controlled environment with cur-sor movements with multiple targets, potential distractions, and biases that can affect behaviors irrespective of the rel-Figure 2: Cumulative distribution function of landing page load times for result clicks on SERPs. evance of the results retrieved by the search engine. Third, by leveraging cursor movements and other features our ap-proach can better adapt to the current search situation and less common informational queries. Many current prefetch-ing methods within Websites and search engines focus only on providing this support for popular pages. Finally, since we propose a model that is learned offline from many searchers X  behavior, there is no need for searchers to calibrate the model to accommodate their search activity before the first use (as is the case in other models, e.g., [4]).
Before proceeding, it is important to quantify the poten-tial gains from prefetching in a Web search setting. If all landing pages were to load instantly then there would be no benefit from prefetching. Obviously this is not the case given the computational differences in the machines serv-ing and accessing online content, and the network transport pipeline. To better understand the potential of prefetching in Web search settings we analyzed six months of search-click logs from millions of consenting users of Internet Explorer in 2013. From these data, we obtained a distribution of page load times for hyperlink clicks on SERPs from a popular Web search engine. The mean and median landing page load times (PLTs) were 1282ms and 672ms respectively. The em-pirical cumulative distribution of PLTs from 200ms to five seconds or more is shown in Figure 2. Given that searchers start to notice PLT delays at 500ms [2] (or even earlier [3]), the figure suggests that, if we could make an accurate pre-diction about which results to prefetch, we could noticeably improve the search experience for at least 64% of SERP clicks (as highlighted in red in Figure 2). These findings serve as a strong motivation for developing the prefetching model introduced and evaluated in this paper.
Let U be a set of algorithmic results. Each result is asso-ciated with a specific SERP region or area of interest (AOI), comprising the region spanning the result title, snippet, and URL. Given that the SERP was presented to the searcher at time 0 and the searcher clicks on u  X   X  X  at time T, we would like to fetch u  X  at some point before T. Decision-making is online: starting at time 0, the system observes a sequence of interactions (cursor movements in our case) which might inform its decision-making. Once a decision to fetch has been made, the system may not fetch another page until the searcher clicks on u  X  . This budget represents a conservative estimate of the cost, in terms of time and bandwidth, of prefetching the result, and considers all clicks equally, e.g., for simplicity the size of the page is not considered in the evaluation, even though prefetching a large/media-rich doc-ument costs more than prefetching a smaller resource.
We regard the search result prefetching challenge as a ranking problem. We model probability of clicking on a result as a function of static features of the result (e.g., po-sition) as well as dynamic features of the result (e.g., proxim-ity of the cursor to the result AOI). Our model computes this relevance score throughout the searcher X  X  interaction with the SERP. If the score exceeds a threshold  X  , then the land-ing page content is fetched. In this section, we will describe the features and model that we developed for this challenge.
Our features can be divided into static and dynamic based on whether they are the same across all cursor movements for a query impression (static) or change as the searcher moves the cursor (dynamic). Each group can be further di-vided into global and local . Global features of the SERP are the same over all AOIs (e.g., SERP includes an advertise-ment). Local features refer to properties unique to each re-sult AOI (e.g., Euclidean distance between cursor and AOI). Features are computed every time the cursor positon is sam-pled (i.e., after 250ms have passed or the cursor has moved at least eight pixels) for each of the result AOIs, since the goal is to predict the result AOI that will be selected.
Previous research has shown that modeling differences with the normative behavior for each searcher can help bet-ter estimate document relevance [17]. As such, we include the normalized version for each feature for each searcher us-ing the deviations from average for the  X  searcher , feature  X  pair. User deviation is defined as the difference between the feature value at the current cursor position and the the av-erage feature value for that searcher computed over all their historic actions. User deviation variants are included for Dynamic Global and Dynamic Local features (Table 1).
Query Features comprise query frequency and query click entropy (as defined in [12]). Searchers may interact differ-ently for different query classes, e.g., navigational queries vs. informational queries, as shown previously [9].

SERP Features capture the presence of non-target ele-ments such as advertisements and related searches, which may influence the click behavior on the target organic search results. Binary features for SERP has advertisements and SERP has related searches are included.
Global Cursor Features capture the dynamics of cursor movements up to the cursor sample in question. Features include velocity, acceleration, jerk (i.e., rate of acceleration change) and changes from previous cursor sample. These also include features that capture where the cursor was lo-cated on the SERP, such as its horizontal and vertical co-ordinates, the maximum vertical coordinate reached by the Table 1: Features used in prefetching model. Coordinates are relative to upper-left corner of the SERP. Distances, co-ordinates, and areas measured in pixels. Features with max-ima or totals (denoted *) are computed since SERP load. cursor and maximum AOI rank that the cursor is observed passing over, and the number of non-hyperlink clicks the im-pression has received up to the cursor sample. The rationale for these features is to capture the different stages of the cur-sor movements. A directed, rapid movement may mean that the searcher has found content of potential value; slow, undi-rected movements may suggest that they are still searching. Determining the maximum vertical position of the cursor offers insight into the number of search results considered. Finally, to explicitly model reading behavior using cursor as an aid [37] (which could be an indicator of interest in a land-ing page), a binary feature captures whether two consecutive left-right cursor movements are observed.
AOI Features characterize rhe target AOIs (in our case, each search result). For each AOI, specific features include the rank position of the AOI, horizontal and vertical co-ordinates, width, height, area of the AOI, and the type of the AOI (e.g., whether AOI is an aggregated result). The rationale here is that behavior is highly influenced by the presentation of the AOIs, for example, higher ranked AOIs may be more likely to receive clicks while bigger AOIs and AOIs with cards (e.g., brand logos) may be more likely to attract searcher attention and result in clicks [11].
Local Cursor Features capture the interaction between cursor movements and each AOI, in particular, to capture if the cursor is moving towards the AOI for a potential click. Features include the overall, vertical, horizontal distances, between the AOI and the cursor, the angle between the mov-ing direction of the cursor and the AOI, and, in turn, the proximity between the two (2 = moving away from AOI, 1 = moving toward AOI, 0 = same distance from AOI), as well as whether the AOI is on target per the cursor trajectory (i.e., draw a least squares line through last five cursor move-ments and return true/false on whether it intersects each AOI). We also compute the dwell time the cursor hover on the AOI and the title of the AOI, respectively, as they may be strong indicators of a potential click on the AOI.
View Features capture whether the AOI is visible in the viewport. Invisible AOIs cannot be selected by searchers.
We train a regression model to predict which result will be clicked. The training procedure builds an ensemble of decision trees based on gradient boosting [7]. This technique has been shown to provide state-of-the-art performance for various applications. In the context of learning to rank, we treat each cursor sample as a query and each u as a document. The objective of the model is to predict u  X  . At test time, for each cursor sample, we featurize and score each u  X  U . If the score of one or more results is above  X  , we fetch u with the highest score.
We now define the methods employed in training and eval-uating our prefetching models, starting with the log data.
To record searcher interactions with the Bing SERP at scale without the need to install any browser plugins, we used an efficient and scalable approach [8]. JavaScript-based logging functions were embedded into the HTML source code of the SERP. To obtain a detailed understanding of user interactions with the SERP, we recorded information on mouse cursor movements, clicks, scrolling, text selection events, focus gain and loss events of the browser window, as well as bounding boxes of several AOIs on the SERP and the browser X  X  viewport size. We optimize our implementation and run large-scale live experiments to ensure no significant delay in PLT for SERPs with this logging enabled.

The JavaScript function for logging mouse cursor posi-tions checked the cursor X  X  horizontal and vertical coordi-nates relative to the top-left corner of the SERP every 250 milliseconds. Whenever the cursor moved more than eight pixels away from its previously logged position, its new co-ordinates were sent to the remote Web server. Eight pixels correspond to approximately the height of half a line of text on the SERP. We used this approach rather than recording every cursor movement since we wanted to minimize the data gathered and transmitted so as to not adversely affect the user experience with delays associated with log data capture and data uploads to the remote server. Since cursor tracking was relative to the document, we captured cursor alignment to SERP content regardless of how the user reached that position (e.g., by scrolling or keyboard).
 Mouse clicks were recorded using the JavaScript onMouse-Down event handling method. The backend server received log entries with location coordinates for every mouse click, including clicks that occurred on a hyperlink as well as those that occurred elsewhere on the SERP (even on white space containing no content). To identify clicks on hyperlinks and differentiate them from clicks on inactive page elements, we logged unique hyperlink identifiers embedded in the SERP.
The width and height of the browser viewport in pixels at SERP load time were also logged. This told us which AOIs were visible. Browser window resizing during SERP interaction was not accounted for. We also recorded the current scroll position, i.e., the vertical coordinate of the uppermost visible pixel of the SERP in the browser view-port. This coordinate was checked three times per second and was recorded whenever it had changed by more than 40 pixels compared to the last logged scrolling position. This corresponds to approximately the height of two lines of text. Simply logging the text of what was displayed on the SERP is insufficient for reconstructing its layout since SERPs vary per query (depending on whether vertical results are shown, etc.), font sizes, and other browser preferences.
To reconstruct the exact SERP layout as it was rendered in the user X  X  browser, we recorded the positions and sizes of AOIs. We use the method from [8] to identify and record the exact position of AOIs on SERP loading. The specific AOIs recorded were: (i) top and bottom search boxes, (ii) left rail and its contained related searches, search history, and query refinement areas, (iii) mainline results area and its contained result entries, including advertisements and an-swers, and (iv) right rail. Some of these AOIs are visualized in Figure 3. For each AOI bounding box, we determined and logged the coordinates of its upper left corner as well as its width and height in pixels. Using this information, we map cursor positions and clicks to AOIs. Although we record the position all AOIs shown in Figure 3, we focus only on predicting clicks on one of the ten result AOIs.
The final, processed dataset consists of batches of instances Figure 3: Segmentation of a search engine results page (SERP) by areas of interest (AOI). Each of the 10 search result captions (title, snippet, URL) is regarded as its own AOI. These 10 result AOIs are our click prediction targets. of the form:  X   X ,t,u, X ,y  X  where  X  is a unique identifier for this impression, encoding information about the user and query, t denotes the timestamp (relative to 0) of the mea-surement, u is a unique identifier for the AOI,  X  represents all of the feature values computed as discussed in Section 5.1, and y is a boolean variable indicating whether u was clicked during the session  X  . We focus on impressions with exactly one detected click 1 and at least five detected cursor positions (73.8% of all query impressions). Running exper-iments with a smaller number of minimum cursor positions resulted in no performance differences relative to our base-lines. This is because when cursor data is missing ( &lt; 10% of all query impressions), our model effectively fall back to the strongest baseline of original search result ranking (as it is part of our model), and our gains over the baseline would just be slightly diluted by including this small fraction of traffic but findings and conclusions would remain the same. We recorded the SERP interactions from the Microsoft Bing Web search engine. Log data were gathered from searchers in the control groups (i.e., with no experimental treatments on frontend or backend) of multiple experiments on the search engine in the U.S. English geographic locale, run between May 2011 and June 2012, during external ex-periments on small fractions of user traffic. For the duration of the experiments, all queries from searchers in the control groups (which is a representative sample of the overall search traffic) are recorded along with their cursor movements.
We selected a random set of 100K users (and their 1M queries) from our dataset. We split the set by searcher and time with 80% for training and 20% for testing. That is, all tuples belonging to the same searcher X  X nd therefore
The comparison on impressions without a click is not very relevant since they do not impact the user experience. impression X  X ere in the same split. The prediction targets are set to 4 for clicked hyperlinks and 0 for hyperlinks that were not clicked. Our model outputs a score for each hy-perlink at each cursor movement, stopping and prefetching a link when its score exceeds a threshold  X  . Instead of op-timizing for a fixed  X  , we present precision-recall curves to demonstrate performance at different operating points.
We focus evaluation on prefetching algorithmic search re-sults. As discussed in Section 3, the page load time can vary dramatically. We therefore want to test our algorithm un-der various regimes. For a given score  X  , we evaluate our algorithm when given at least ` milliseconds to fetch a page before the click. So, for example, if ` is 500 ms and the user took 2000 ms to investigate the page before clicking, then we can observe the user X  X  behavior for 1500 ms before losing our chance to get any benefit from prefetching. We com-pute precision and recall for `  X  X  500 , 5000 } to demonstrate regimes of normal and severely-limited bandwidth. For a given ` , the true positive ( TP ) is defined as the prefetch-ing decision made for the link that was actually clicked at least ` time later. A negative ( TN ) occurs when the system accurately predicts that no clicks on any links occurred dur-ing the impression. The prefetching on an unclicked target link is considered false positive ( FP ) while prefetching made with lead time shorter than ` is considered as late positive ( LP ). A false negative ( FN ) is an impression where the model did not select any link, even though the user eventu-ally clicked one. We evaluate the performance of our models (and the baselines described in the next subsection) using precision and recall. With the above definitions, the preci-sion is then defined as TP/ ( TP + FP ) while recall is defined as TP/ ( TP + LP + FN ). Notice that a random prefetching system will achieve precision of 1 |U| .
We employed three strong baselines: (i) the original search engine ranking, (ii) historic clicks from all searchers for the query, and (iii) historic clicks from the current searcher for the query. These baselines, in particular the latter two, are similar to prefetching methods proposed in prior research, in which, prefetching is determined by static estimates of likelihood of future content access from historic usage data [1, 13, 22, 26]. The baselines are defined as:
Search engine ranking: This baseline always prefetches the top-ranked result for the query as returned by the Bing search engine at the time the logs were collected. Note that this baseline is expected to be very strong since commercial search engines rank results by leveraging a variety of sources of evidence, including content and historic usage data, and the top-ranked search result often receives most clicks.
Historic clicks (all users) (denoted p ( click all )): Selects the most popular clicked URL for the query. Multi-year click logs from a separate data source (same search engine, but not the cursor-tracking flights) were used to compute the probability of selecting a particular URL given the cur-rent query. The separate dataset was much larger than the set of data collected during the online cursor tracking ex-periments. This enabled broader query coverage and more reliable click predictions. The result that is most likely to be clicked based on this historic data was prefetched if it appeared in the current result ranking. To improve cover-age, all URLs were normalized to remove trailing slashes, lowercase, and collapse https and http protocols.

Historic clicks (current user) (denoted p ( click user )): Ap-plies the personal navigation algorithm [43] to prefetch the result that was visited by the current searcher historically for the current query. Specifically, if the searcher has visited the same result for the previous two instances of the query, then that result will be prefetched for the current (third) in-stance of the query if it appears in the result ranking. URLs were normalized as with the previous baseline.
We now present our experimental results, starting with the descriptive statistics of the cursor movement data.
In this section, we characterize the features proposed in the previous subsection for building the prefetching models. Since our goal is to prefetch clicked results, we wanted to understand whether certain features could indicate future clicks. We report analysis across 41.6 million cursor sam-ples from 186k unique query impressions used for training our model. The results are summarized in Table 2 and the discussions are organized by the feature group. We focus on the two classes of cursor samples with clicked AOI and unclicked AOI to understand the changes of feature values across the two groups. Features in the two global groups are impacting at the impression-level, i.e., features that im-pact overall clickthrough for the entire impression would, in turn, impact clickthrough for individual AOIs on the SERP. In contrast, the features in the two local groups impact di-rectly regarding the AOI in question.

Static Global : All the features in this group are signif-icantly different among the two groups. Interestingly, the queries with less clicks have higher frequencies, which may be due to the more likely presence of answer results  X  this hypothesis is further supported by the averaged higher value of the has answer feature in the StaticLocal group.
Dynamic Global : Almost all the features in this group are significantly different among the two groups except for cur-sor ycoord and cursor total time. The total cursor distance of cursor for the unclicked impressions is higher, as is the number of non-hyperlink clicks (often associated with text selections) and evidence of reading behavior, which suggest that people are exploring and more deeply engaged with ex-amining the SERP (rather than clicking).

Static Local : All the features in this group are significantly different among the two classes. As we can see, certain types of AOIs are indeed more likely to attract more clicks. For example, the clicked class has larger AOI area , which makes sense, as larger AOI may be more likely to attract user at-tention. Other examples include the rate of AOI having the card attribute (e.g., additional information such as brand logo and/or deep links) which may increase both user confi-dence in document quality as well as attractiveness, resulting in more chance of clickthrough. In contrast, having answer directly in the AOI, as discussed earlier, reduces the chance of the AOI being clicked due to  X  X ood abandonment X  [20]. Also, as expected, the clicked AOI tends to have lower rank (demonstrated by both lower AOI rank and AOI ycoord ).
Dynamic Local : There are interesting differences in this group X  X  features. Cursor hovers on the AOI are a significant indicator of an impending click, as is dwell time on the AOI Table 2: Descriptive statistics of the proposed features for the two classes of cursor sample with clicked AOI and un-clicked AOI. The differences between the two classes are statistically significant for the majority of the features based on Welch X  X  t -test ( p &lt; 0 . 05) except for features that are noted with *. (s) denotes seconds, (px) denotes pixels. (which is much higher when a click is observed). The speed and acceleration toward the AOI suggests that the searcher is performing a focused movement before the click. This is also supported by the higher value of AOICursor ontarget .
We present the results of our prefetching experiments for different lead times in the precision-recall curves in Figure 4. As we can see from these curves, for comparable recall levels, we achieve substantial improvements over prefetch-ing based on all cursor-agnostic methods. Although per-formance drops when we require a conservative five second leadtime, our algorithm still outperforms baselines by a sig-nificant margin. We present the results of significance tests in Table 3 for a high precision model (with a high value of  X  ) and a high recall model (with a low value of  X  ).
One might suspect that pre-fetching decisions for naviga-Table 3: Comparison with baselines. Superscripts denote statistically significant improvements over a competing run using a Student X  X  t -test ( p &lt; 0 . 05) with respect to rank (r), p ( click all ) (a), p ( click user ) (u), high precision model (P), or high recall model (R).
 Table 4: The effectiveness of prefetching models trained suppressing the specified feature groups against the full model trained with all features the score threshold  X  of 3 and a leadtime ` of 500 ms. * represents statistically sig-nificant decreases in performance with respect to using all features using a Student X  X  t -test ( p &lt; 0 . 05). tional queries, because they have a single target result, can be made without cursor information. To test this, we exam-ined the performance of our algorithm on queries defined as navigational (i.e., with a click entropy less than or equal to one, as in [42]). Figure 4b demonstrates the higher perfor-mance of all methods, including baselines. The differences in performance are statistically significant ( p &lt; 0 . 05). We similarly investigated the performance of our model when evaluating only on informational queries (i.e., queries with a click entropy of two or more (again, as in [42]). These queries involve more thorough examination of the ranked list. This behavior can result from either multiple intents, poor re-trieval performance, or higher recall intent. Because of the diversity of the click patterns, we suspect that our baselines will perform less well on these queries. The results (Figure 4c) demonstrate the lower performance of all runs, including our model. Nevertheless, the model-based approach signifi-cantly improves over the baselines ( p &lt; 0 . 05).
We present the feature ablation experiments in Table 4, where we remove one feature group at a time to examine the effectiveness of each of them in the presence of other feature groups. To do this, we use the high precision model, whose overall results are reported in Table 3. Static fea-tures, taken as a whole, contribute substantially (removing them results in a 6.6% drop in precision and a 3.5% drop in recall). This reflects the importance of visual layout and attractiveness in successful prefetching. Importantly, the degradation is not observed when suppressing global or lo-Table 5: Feature importance. Five most and least impor-tant features and weights. Weights are normalized to be in unit range with respect to the most important feature (AOICursor hover from the Dynamic Local class).
 cal features alone, suggesting that static features perform best when using conjunctions of local and global features. Dynamic features, taken as a whole, also provide significant information (removing them results in a 5.2% drop in pre-cision and a 23.8% drop in recall). The majority of this contribution comes from local features, suggesting that in-formation about the AOI with respect to the cursor are crit-ical to high performance. The local features are important in recall since they provide signals about each of the AOIs that may be missed in general SERP-level analysis. For the high precision model, user deviation features appear to add no value over the other features, in combination.
Finally, we wanted to understand feature importance. We present the most and least informative features in Table 5. Unsurprisingly, hovering over an AOI is a strong signal that a click is imminent. Other Dynamic Local features, such as AOICursor ydistance, are also important but are not in the top five. While features such as the rank position of the result AOI and the distance between the cursor and the AOI might seem obvious, others are less so. The pres-ence of a card suggests that the visual attractiveness of an AOI may result in a higher clickthrough rate. Variations in searcher attention as a function of caption attractiveness have been noted in previous studies (e.g., [11]). Conversely, query click entropy appears to help the model distinguish be-tween more predictable behavior connected to navigational intentions and less predictable behaviors for informational intent. The less informative features involve more granular cursor movements (e.g., jerk, acceleration), suggesting that in order to perform effectively our prefetching model only requires a coarse model of search interaction behavior.
We have introduced the search result prefetching challenge and real-time prefetching methods to address it based pri-marily on cursor movements. Our approach significantly outperforms three strong baselines leveraging result rank-ings and historic clicks to prefetch search results. Analysis of the differences between the prefetch time and click time reveals that on average, our method could save searchers around 650ms per query for the 65% of queries where our model correctly prefetches the clicked result. The average prefetch lead time is 6-7 seconds (depending on  X  ), but we cap our time savings to the median landing page load time from earlier (672ms). 650ms in time savings per accurate prediction is a sizable efficiency increase, especially at Web scale, where prefetching could benefit millions of searchers.
Our findings showed that we can achieve strong prediction accuracy. As with other studies [8, 9] we noted evidence of task effects (e.g., our models do better for navigational queries, where search behavior is more predictable). We break out our findings by query type, but not by searcher type. Variability in interactions between searchers is high [8] and differences in model performance per individual or cohort should be considered. This could inform decisions about the selective application of cursor-based prefetching.
There are some limitations in this study. First, we con-sider prefetching only one result per SERP. In the future, we plan to extend our method to handle prefetching of multiple results during SERP examination. Second, in our evalua-tion we represent cost as the action of prefetching a result. In practice, cost is more nuanced: there is a variable cost associated with prefetching, depending on the nature of the prefetched page (e.g., size, content types). Third, we have not fully explored the trade-off between client-side code opti-mization and its impact on prefetching performance. In the current implementation, we optimize our client-side code to ensure no significant delay on PLT for SERPs with the log-ging enabled. This includes lazy loading of the logging code, enforcing mouse sampling (i.e., every 250ms or 8px of move-ment), and optimizing our prediction latency in both feature extraction (i.e., constant time in the number of cursor sam-ples) and model execution (i.e., using decision trees, which can be turned into heavily optimized binaries). However, we do not know whether these optimizations are truly optimal and plan to further explore this in future work. Finally, we focused on desktop devices (e.g., PCs) in this study. We will extend our methods to mobile devices (e.g., smartphones, tablets), where multi-touch interactions and viewport dy-namics would replace the mouse cursor. Bandwidth may be more limited in mobile settings and result prefetching could prove to be more valuable therein.

Looking ahead, the ability to prefetch visited resources has a number of implications. People on slow network con-nections (e.g., in developing countries or remote locations in developed countries) can benefit from faster landing page loading, as would those engaged in time-critical tasks [28]. Accurately prefetching clicked results also allows search en-gines to offer enhanced interaction capabilities such as aug-menting landing pages to better support transitions from SERPs (e.g., clickable snippets [15]). More broadly, our prefetching methods could help reduce latency in any Web-site, especially those with low traffic or large amounts of dynamically-generated content; both of which can hinder the use of historic activity in prefetching decisions.
We presented a dynamic framework for prefetching Web pages in response to SERP interaction behavior. Previous models have focused on static prediction using historic data. We show that incorporating aspects of the SERP visual lay-out and dynamic on-SERP behavior, such as cursor move-ment, can substantially improve the accuracy of real-time prefetching decisions and may improve searcher efficiency. In future work, we will experiment with more sophisticated models to better capture the temporal dynamics and per-sonal nature of cursor movements. While our technique gen-eralizes beyond SERPs, optimal performance may require task-specific models. We will extend our analysis to a di-verse range of non-SERP Web pages. Finally, we will adapt our models to mobile devices where viewport and touch in-teractions can be leveraged in place of cursor movements. [1] E. Agichtein and Z. Zheng. Identifying  X  X est bet X  web [2] I. Arapakis, X. Bai, and B.B. Cambazoglu. Impact of [3] M. Barreda- X  Angeles et al. Unconscious physiological [4] T. Asano, E. Sharlin, Y. Kitamura, K. Takashima, [5] R. Baeza-Yates et al. The impact of caching on search [6] R. Blanco et al. Caching search engine results over [7] C.J. Burges. From ranknet to lambdarank to [8] G. Buscher et al. Large-scale analysis of individual [9] E. Cutrell and Z. Guan. What are you looking for?: [10] J. Dean and L.A. Barroso. The tail at scale. CACM , [11] F. Diaz et al. Robust models of mouse movement on [12] Z. Dou, R. Song, and J.-R. Wen. A large-scale [13] T. Fagni et al. Boosting the performance of web [14] L. Fan et al. Web prefetching between low-bandwidth [15] H. Feild, R.W. White, and X. Fu. Supporting [16] Q. Guo and E. Agichtein. Towards predicting web [17] Q. Guo and E. Agichtein. Beyond dwell time: [18] E. Horvitz. Continual computation policies for [19] J. Huang, R.W. White, and G. Buscher. User see, user [20] J. Huang, R.W. White, and S. Dumais. No clicks, no [21] T. Joachims et al. Accurately interpreting clickthrough [22] S. Jonassen, L. Granka, and F. Silvestri. Prefetching [23] D. Lagun et al. Discovering common motifs in cursor [24] D.M. Lane et al. A process for anticipating and [25] S. Lee, J. Yoo, and G. Han. Gaze-assisted user [26] R. Lempel and S. Moran. Predictive caching and [27] R.B. Miller. Response time in man-computer [28] N. Mishra et al. Time-critical search. SIGIR , 747 X 756, [29] A. Murata. Improvement of pointing time by [30] F.F. Nah. A study on tolerable waiting time: how long [31] J. Nielsen. Usability Engineering . Morgan Kaufmann, [32] J. Nielsen. User interface directions for the web. [33] V.N. Padmanabhan and J.C. Mogul. Using predictive [34] P.T. Pasqual and J.O. Wobbrock. Mouse pointing [35] J. Psaroudakis and M. Khambatti. A deeper look at [36] J. Ramsay, A. Barbesi, and J. Preece. A psychological [37] K. Rodden et al. Eye-mouse coordination patterns on [38] E. Schurman and J. Brutlag. Performance related [39] B. Shneiderman. Response time and display rate in [40] A. Singhal and M. Cutts. Using site speed in web [41] J. Teevan et al. Slow search. CACM , 57(8):36 X 38, [42] J. Teevan, S.T. Dumais, and D.J. Liebling. To [43] J. Teevan, D.J. Liebling, and [44] R.W. White, P. Bailey, and L. Chen. Predicting user [45] Q. Yang, H.H. Zhang, and T. Li. Mining web logs for [46] B. Ziebart, A. Dey, and J.A. Bagnell. Probabilistic
