 Research efforts to increase search efficiency for phrase-based MT (Koehn et al., 2003) have ex-plored several directions, ranging from generalizing the stack decoding algorithm (Ortiz et al., 2006) to additional early pruning techniques (Delaney et al., 2006), (Moore and Quirk, 2007) and more efficient language model (LM) querying (Heafield, 2011). This work extends the approach by (Zens and Ney, 2008) with two techniques to increase trans-lation speed and scalability. We show that taking a heuristic LM score estimate for pre-sorting the phrase translation candidates has a positive effect on both translation quality and speed. Further, we intro-duce two novel LM look-ahead methods. The idea of LM look-ahead is to incorporate the LM proba-bilities into the pruning process of the beam search as early as possible. In speech recognition it has been used for many years (Steinbiss et al., 1994; Ortmanns et al., 1998). First-word LM look-ahead exploits the search structure to use the LM costs of the first word of a new phrase as a lower bound for the full LM costs of the phrase. Phrase-only LM look-ahead makes use of a pre-computed estimate of the full LM costs for each phrase. We detail the implementation of these methods and analyze their effect with respect to the number of LM computa-tions and hypothesis expansions as well as on trans-lation speed and quality. We also run comparisons with the Moses decoder (Koehn et al., 2007), which yields the same performance in B LEU , but is outper-formed significantly in terms of scalability for faster translation. Our implementation is available under a non-commercial open source licence  X  . We apply the decoding algorithm described in (Zens and Ney, 2008). Hypotheses are scored by a weighted log-linear combination of models. A beam search strategy is used to find the best hypothesis. During search we perform pruning controlled by the parameters coverage histogram size  X  N c and lexical histogram size  X  N l . 2.1 Phrase candidate pre-sorting In addition to the source sentence f J 1 , the beam search algorithm takes a matrix E (  X  ,  X  ) as input, where for each contiguous phrase  X  f = f j ... f j 0 within the source sentence, E ( j , j 0 ) contains a list of all candidate translations for  X  f . The candidate lists are sorted according to their model score, which was observed to speed up translation by Delaney et al. (2006). In addition to sorting according to the purely phrase-internal scores, which is common practice, we compute an estimate q LME (  X  e ) for the LM score of each target phrase  X  e . q LME (  X  e ) is the weighted LM score we receive by assuming  X  e to be a com-plete sentence without using sentence start and end markers. We limit the number of translation options per source phrase to the N o top scoring candidates (observation histogram pruning).

The pre-sorting during phrase matching has two effects on the search algorithm. Firstly, it defines the order in which the hypothesis expansions take place. As higher scoring phrases are considered first, it is less likely that already created partial hypothe-ses will have to be replaced, thus effectively reduc-ing the expected number of hypothesis expansions. Secondly, due to the observation pruning the sorting affects the considered phrase candidates and conse-quently the search space. A better pre-selection can be expected to improve translation quality. 2.2 Language Model Look-Ahead LM score computations are among the most expen-sive in decoding. Delaney et al. (2006) report signif-icant improvements in runtime by removing unnec-essary LM lookups via early pruning. Here we de-scribe an LM look-ahead technique, which is aimed at further reducing the number of LM computations.
The innermost loop of the search algorithm iter-ates over all translation options for a single source phrase to consider them for expanding the current hypothesis. We introduce an LM look-ahead score q
LMLA (  X  e |  X  e translation options. This score is added to the over-all hypothesis score, and if the pruning threshold is exceeded, we discard the expansion without com-puting the full LM score.
 First-word LM look-ahead pruning defines the LM look-ahead score q LMLA (  X  e |  X  e 0 ) = q LM (  X  e be the LM score of the first word of target phrase  X  e the full LM score, the technique does not introduce additional seach errors. The score can be reused, if the LM score of the full phrase  X  e needs to be com-puted afterwards.

We can exploit the structure of the search to speed up the LM lookups for the first word. The LM prob-abilities are stored in a trie , where each node cor-responds to a specific LM history. Usually, each LM lookup consists of first traversing the trie to find the node corresponding to the current LM history and then retrieving the probability for the next word. If the n -gram is not present, we have to repeat this procedure with the next lower-order history, until a probability is found. However, the LM history for the first words of all phrases within the innermost loop of the search algorithm is identical. Just be-fore the loop we can therefore traverse the trie once for the current history and each of its lower order n -grams and store the pointers to the resulting nodes. To retrieve the LM look-ahead scores, we can then directly access the nodes without the need to traverse the trie again. This implementational detail was con-firmed to increase translation speed by roughly 20% in a short experiment.

Phrase-only LM look-ahead pruning defines the LM score of phrase  X  e , assuming  X  e to be the full sen-tence. It was already used for sorting the phrases, is therefore pre-computed and does not require ad-ditional LM lookups. As it is not a lower bound for the real LM score, this pruning technique can intro-duce additional search errors. Our results show that it radically reduces the number of LM lookups. 3.1 Setup The experiments are carried out on the German  X  English task provided for WMT 2011  X  . The English language model is a 4-gram LM created with the SRILM toolkit (Stolcke, 2002) on all bilingual and parts of the provided monolingual data. newstest2008 is used for parameter optimization, newstest2009 as a blind test set. To confirm our results, we run the final set of experiments also on the English  X  French task of IWSLT 2011  X  . We evaluate with B LEU (Papineni et al., 2002) and T ER (Snover et al., 2006).

We use identical phrase tables and scaling fac-tors for Moses and our decoder. The phrase table is pruned to a maximum of 400 target candidates per source phrase before decoding. The phrase table and LM are loaded into memory before translating and loading time is eliminated for speed measurements. 3.2 Methodological analysis To observe the effect of the proposed search al-gorithm extensions, we ran experiments with fixed pruning parameters, keeping track of the number of hypothesis expansions and LM computations. The LM score pre-sorting affects both the set of phrase candidates due to observation histogram pruning and the order in which they are considered. To sepa-rate these effects, experiments were run both with histogram pruning ( N o = 100) and without. From Table 1 we can see that in terms of efficiency both cases show similar improvements over the baseline, which performs pre-sorting with respect to the trans-lation model scores only. The number of hypothesis expansions is reduced by  X  20% and the number of LM lookups by  X  50%. When observation pruning is applied, we additionally observe a small increase by 0.2% in B LEU .

Application of first-word LM look-ahead further reduces the number of LM lookups by 23%, result-ing in doubled translation speed, part of which de-rives from fewer trie node searches. The heuristic phrase-only LM look-ahead method introduces ad-ditional search errors, resulting in a B LEU drop by 0.3%, but yields another 85% reduction in LM com-putations and increases throughput by a factor of 2.2. 3.3 Performance evaluation In this section we evaluate the proposed extensions to the original beam search algorithm in terms of scalability and their usefulness for different appli-cation constraints. We compare Moses and four dif-ferent setups of our decoder: LM score pre-sorting switched on or off without LM look-ahead and both LM look-ahead methods with LM score pre-sorting. We translated the test set with the beam sizes set to N c = N l = { 1 , 2 , 4 , 8 , 16 , 24 , 32 , 48 , 64 } . For Moses we used the beam sizes 2 i , i  X  X  1 ,..., 9 } . Transla-tion performance in B LEU is plotted against speed in Figure 1. Without the proposed extensions, Moses slightly outperforms our decoder in terms of B LEU . However, the latter already scales better for higher speed. With LM score pre-sorting, the best B LEU value is similar to Moses while further accelerat-ing translation, yielding identical performance at 16 words/sec as Moses at 1.8 words/sec. Application of first-word LM look-ahead shifts the graph to the right, now reaching the same performance at 31 words/sec. At a fixed translation speed of roughly 70 words/sec, our approach yields 20.0% B LEU , whereas Moses reaches 17.2%. For phrase-only LM look-ahead the graph is somewhat flatter. It yields nearly the same top performance with an even better trade-off between translation quality and speed.
The final set of experiments is performed on both the WMT and the IWSLT task. We directly com-pare our decoder with the two LM look-ahead meth-ods with Moses in four scenarios: the best possi-ble translation, the fastest possible translation with-out performance constraint and the fastest possible translation with no more than 1% and 2% loss in B LEU on the dev set compared to the best value. Table 2 shows that on the WMT data, the top per-formance is similar for both decoders. However, if we allow for a small degradation in translation per-formance, our approaches clearly outperform Moses in terms of translation speed. With phrase-only LM look-ahead, our decoder is faster by a factor of 6 for no more than 1% B LEU loss, a factor of 11 for 2% B LEU loss and a factor of 22 in the fastest set-ting. The results on the IWSLT data are very similar. Here, the speed difference reaches a factor of 19 in the fastest setting. This work introduces two extensions to the well-known beam search algorithm for phrase-based ma-chine translation. Both pre-sorting the phrase trans-lation candidates with an LM score estimate and LM look-ahead during search are shown to have a pos-itive effect on translation speed. We compare our decoder to Moses, reaching a similar highest B LEU score, but clearly outperforming it in terms of scal-ability with respect to the trade-off ratio between translation quality and speed. In our experiments, the fastest settings of our decoder and Moses differ in translation speed by a factor of 22 on the WMT data and a factor of 19 on the IWSLT data. Our soft-ware is part of the open source toolkit Jane .
