 We consider the problem of automatically acquiring knowl-edge about the typical temporal orderings among relations (e.g., actedIn (person, film) typically occurs before wonPrize (film, award)), given only a database of known facts (rela-tion instances) without time information, and a large doc-ument collection. Our approach is based on the conjecture that the narrative order of verb mentions within documents correlates with the temporal order of the relations they rep-resent. We propose a family of algorithms based on this con-jecture, utilizing a corpus of 890m dependency parsed sen-tences to obtain verbs that represent relations of interest, and utilizing Wikipedia documents to gather statistics on narrative order of verb mentions. Our proposed algorithm, GraphOrder, is a novel and scalable graph-based label prop-agation algorithm that takes transitivity of temporal order into account, as well as these statistics on narrative order of verb mentions. This algorithm achieves as high as 38.4% ab-solute improvement in F1 over a random baseline. Finally, we demonstrate the utility of this learned general knowledge about typical temporal orderings among relations, by show-ing that these temporal constraints can be successfully used by a joint inference framework to assign specific temporal scopes to individual facts.
 I.2.4 [ Artificial Intelligence ]: Knowledge Representation Formalisms and Methods; I.2.6 [ Artificial Intelligence ]: Learning; I.2.7 [ Artificial Intelligence ]: Natural Language Processing Algorithms, Experimentation Contributed equally.
 Knowledge Bases, Temporal Ordering, Temporal Scoping, Narrative Ordering, Graph-based Semi-Supervised Learn-ing, Label Propagation
Harvesting temporal knowledge from Web sources is an important research challenge [23]. Web search and Question Answering (QA) systems can benefit from having knowledge about entities, their relationships, and the time at which the relationships hold (i.e., their time scopes) [1]. A closely re-lated task to time scoping is temporal ordering of relations. Instead of finding the temporal scopes of relation instances (or simply, facts), temporal ordering finds order (e.g., be-fore, simultaneous , etc.) between facts. For example, while temporal scoping aims to infer that presidentOf(Bill Clin-ton, USA) was true during 1993 -2001, temporal order-ing at the relation level aims to infer that, for the same person, wasBornIn(person, location) happens before presi-dentOf(person, country) . Knowledge of temporal order can be useful for temporal scoping. For example, if we know that Al Gore X  X  vice presidency was simultaneous with Bill Clinton X  X  presidency, and if we know the time scope of Clin-ton X  X  presidency, then we can infer the time scope of Gore X  X  vice presidency.

We address the problem of inferring temporal ordering of relations at the macro (corpus) level: a novel, important, and unexplored problem. Previous research has mostly fo-cused on temporal ordering at the micro level (i.e., sentence or intra-document level), and that too over verbs (or single-verb events). Instead of taking a verb-centric view, we aim to temporally order relations, where each relation may be expressed by different verbs in different sentences. For ex-ample, the actedIn ( person,film ) relation can be expressed by the verbs acted , starred , and many others.

This paper explores the feasibility of inducing typical tem-poral orderings between relations based on the narrative or-der of inferred mentions of these relations, averaged across many documents. We define the narrative order of relation mentions in a document as the textual order of sentences that contain verbs expressing these relations. We explore the conjecture that narrative order is correlated with tem-poral order frequently enough (especially when aggregated across a large number of documents) for it to serve as a probabilistic training signal for learning the temporal order. Such conjecture is appropriate especially in documents that are each about an entity and the relations incident on the entity. In such document, relations are frequently mentioned around the entity in chronological order. Wikipedia is an ex-ample of a large and broad coverage corpus containing such documents.

Our temporal order of relations is related to structured se-quences of participants and events in documents that have been called scripts [18] or narrative event chains [9], which are applied to or inferred from the narrative in the docu-ment. Since scripts are inherently temporal, e.g. restaurant script, employment script, or kidnapping script; we conjec-ture that narrative can also be useful for temporal ordering.
Our Contributions: We consider the problem of learn-ing typical temporal ordering between relations (e.g., acte-dIn (person, film) happens before wonPrize (film, award)), given a database of macro-read facts ( relation instances ) such as Yago2 [12] without time information. We present GraphOrder, a graph-based label propagation algorithm for this task and demonstrate in our experiments that the nar-rative order of mentions of these relation instances (i.e., verbs expressing the relations), aggregated over a large num-ber of documents, can be exploited as probabilistic train-ing signals to infer the temporal ordering of the relations. We also mine syntactic information in the form of subject-verb-object triples from a large corpus of 890m dependency parsed sentences 1 and show how such information can be ef-fectively used for finding these relation mentions . Through experiments on several domains of real-world datasets, we show the effectiveness of our method. We also propose CoTS Soft , which extends a recent collective inference frame-work for temporal scoping [20], to handle soft temporal or-dering constraints generated by GraphOrder. We incorpo-rate our automatically acquired constraints into this frame-work and show the effectiveness of our learned temporal or-der constraints for temporal scoping of relation instances.
Here we introduce our terminology and define the problem considered in this paper. We use the term relation to refer to any general typed binary relation r (e.g., directed(person, film) whose two arguments are of type person and film ), and the term relation instance (or simply, fact ) to refer to any instance of that relation (e.g., directed(George Lucas, Star Wars)). We say that a verb v expresses a relation r if mentions of v in documents frequently represent facts of relation r , with RelVerb( r,v ) measuring the strength of this association. Default( r ) is a function which returns a single default (or representative) verb (extracted from the relation name ) which represents relation r . For example, Default(actedIn) =  X  X ct X .

We use the term narrative order between verbs v 1 and v 2 within a document collection to refer to statistics about the sequence in which v 1 and v 2 are mentioned within the doc-uments: we use NBefore( v 1 ,v 2 ) to measure how often v curs before v 2 in a text narrative, and use NSimultaneous( v to measure how often v 1 occurs in the same sentence (i.e., simultaneously) as v 2 in the text narrative. We use the term temporal order between relations r 1 and r 2 to refer to the temporal sequence in which these relations occur in the real world. Specifically, we use TBefore(r 1 , r 2 ) to represent that
To the best of our knowledge, this is one of the largest parsed corpus ever used for temporal extraction task. facts of relation r 1 typically occur temporally before facts of r 2 involving at least one shared argument. Similarly, we use TSimultaneous(r 1 , r 2 ) to denote that facts of these two relations typically occur at the same time (i.e., there is a significant overlap between their temporal spans). We note that while TSimultaneous is a symmetric relation, TBefore is not. Our temporal orders are partial and uncertain (or soft). Given that many relations have only probabilistic temporal precedence, the non-negative values of TBefore(r 1 , r TSimultaneous(r 1 , r 2 ) are intended to capture a degree of confidence that increases monotonically with this probabil-ity. In this paper, we consider the following problem:
Problem Statement: Given (1) a set of typed binary relations R with one common argument type, which we shall refer to as the domain ; (2) for each relation r  X  R , a set of facts Q r ; and (3) an unlabeled text corpus C , where each document in the subset D  X  C describes one instance from the domain ; we would like to estimate the temporal rela-tionship for each pair of relations in R , i.e., estimate values for TBefore(r i , r j ), TSimultaneous(r i , r j )  X  R + ,  X  r where higher values represent higher confidence that the cor-responding temporal relationship holds. 2
Example: Let R = { actedIn ( person,film ), directed ( person,film ), wonPrize ( film,award ) } be the set of typed binary relations, with common argument type film . Hence, film will also be the the domain in this case. Let the set of facts for each relation be as follows: Q actedIn = { (Tom Hanks, Forrest Gump) } , Q directed = { (Robert Zemeckis, Forrest Gump) } , and Q wonPrize = { (Forrest Gump, Academy Awards) } . Let D be a singleton set consisting of the Wikipedia page on the film  X  X orrest Gump X , and let C be its superset consisting of additional documents sampled from a large text corpus such as [7]. Given these as input, we would like to determine the temporal relationship (i.e., estimate values for TBefore and TSimultaneous) for each pair of relations in R .
Please note that since our inputs do not consist of any labeled data with temporal orderings annotated (as in [17]), our proposed method is unsupervised with respect to this task.
The central thesis behind the method introduced here is that:  X  Statistics aggregated from a large number of docu-ments about narrative orders of verbs representing two rela-tions can serve as probabilistic training signals for learning the temporal order between the two relations.  X 
Let us consider two relations incident on film instances: directed and wonPrize , whose default verbs are  X  X irected X  and  X  X on, X  respectively. If in documents discussing film in-stances, we predominantly observe the narrative order di-rected  X  won , i.e., NBefore( directed,won ) NBefore( won , directed ), this should increase our estimate of TBefore( directed , wonPrize ). In other words, NBefore( directed,won ), which can be estimated directly from free text, can be exploited to estimate the unobserved TBefore(directed , wonPrize) (and similarly for TSimultaneous). Based on the thesis mentioned above, our proposed approach consists of three phases, which are outlined in Figure 1. We next describe each of these three phases.
R + is the set of non-negative real numbers relation pair is estimated (Section 3.4). Figure 2: Finding verbs that express relations by mining a collection of subject-verb-object (SVO) triples extracted from 890m dependency parsed sen-tences (Section 3.2).
To identify verbs that express a particular relation r , we consider the given set of input facts Q r representing known instances of relation r . Each fact f  X  Q r is a triple,  X  f f a 2 , f rel  X  containing the values of the first argument, second argument, and the name of relation (in this case r ), respec-tively. An example of a fact is  X  Tom Hanks, Forrest Gump, actedIn  X  3 . We draw on work on distributional similarity be-tween verbs [10] and other means of grouping verbs that share the same lexical items in subject/object positions [6]. In particular, for each relation r we identify verbs whose mentions occur with subject/object pairs that match the known instances of relation r in Q r .
Alternatively, we also write it as actedIn(Tom Hanks, For-rest Gump) Figure 3: Narrative orders of verbs (e.g., directed, starring, won , etc.) in documents from the film do-main (Section 3.3). Each document describes one instance (e.g.,  X  X orrest Gump X ) from the domain ( film in this case). These narrative orders provide guidance for learning temporal order of relations (e.g., actedIn(person, film) , directed(person, film) , wonPrize(film, award) ) (Section 3.4).

More specifically, to identify the verbs associated with relation r (as illustrated in Figure 2), we use a large col-lection of Subject-Verb-Object (SVO) triples which can be compared to the facts Q r about relation r . We construct this dataset of SVO triples by first parsing 50m Web doc-uments (890m sentences, 16B tokens) using the MALT de-pendency parser [16], and then extracting SVO triples from these parsed sentences 4 . We aggregate all the SVO triples and count their frequency using Hadoop. This yields in a dataset with 114m triples, which we shall refer to as S .
Details on the SVO triple extraction will appear in a longer version of this paper. Fields within each tuple s  X  S can be accessed using the fol-lowing: s sbj , s obj , s verb , and s cnt , which return the subject, object, verb, and the count of the tuple, respectively. To the best of our knowledge, this is one of the largest depen-dency parsed corpora that has ever been used in published research 5 .

Let S v = { s | s verb = v, s  X  S } be the set of tuples in S with v as its verb. Also, let U S = { u | | S u | &gt; 0 } be the set of unique verbs in S , and let U be the overall set of unique verbs under consideration. We have, We now compute RelVerb( r,v ) to measure whether verb v  X  U expresses relation r as follows: where I 1 ( r,u ) = 1 iff  X  r  X  R such that u = Default( r ), and 0 otherwise; and I 2 ( f,s ) is another indicator function which returns 1 whenever arguments of f and s match, i.e., ( f otherwise. We now define F r , the set of verbs used to express relation r as follows, where  X   X  R is a threshold, which is set to 0 for the exper-iments in this paper. Examples of the top-5 elements from the set F  X , actedIn relation, are as follows: stars, features,  X  X , costars, starring. This suggests that this method is able to automatically discover meaningful verbs to express a given relation.
Once we find the set of verbs that express relations, we find mentions of these verbs in documents in D , as illus-trated in Figure 3. Since each document in D is about a domain instance (Section 2), we make a simplifying assump-tion that verbs appearing in the same document are sharing the domain instance as subject/object argument. In the fu-ture, a dependency parse of sentences in the document can be used to find verbs sharing the same domain instance as subject/object.

We shall represent a document d  X  D as a sequence of sentences sorted as per the narrative order, with | d | denoting the total number of sentences in the document, and d k (1  X  k  X | d | ) returning the k th sentence in document d .
Now, for each sentence d k , we compute the sentence score ( SS ) of each ( r,v ) pair mentioned in the sentence. where VerbCount( d k ,r,v ) is the number of times verb v is used to express relation r in sentence d k . This can be estimated by matching the arguments of v in d k against Q the set of facts from relation r . However, we shall make a simplifying assumption and increment verb count by 1 whenever we come across v in d k , subject to the condition that v  X  F  X ,r (increasing  X  may improve precision of verbs
The SVO triples dataset is available from the authors on request. expressing a relation, but may affect recall). The scoring in Equation 3 will discount scores for verbs which express a large number of relations.

We then define the narrative order scores for two verbs v and v 2 in a document d as the sum of scores ( SS ) of v 1 in all pairs of sentences for which v 1 is before v 2 , and the sum of scores of v 1 and v 2 in all sentences where v 1 and v occur. We then aggregate these per-document narrative or-ders across all documents containing v 1 and v 2 by taking the average of these per-document scores as the final narrative order scores NBefore( v 1 ,v 2 ) and NSimultaneous( v 1 ,v the verbs v 1 and v 2 .

We note that Rel-grams [2], a recently proposed semantic resource, may also be used to estimate NBefore, although it is less clear how this resource can be used to estimate NSimultaneous.
Once we find narrative order of verbs as described in Sec-tion 3.3, we infer temporal order of relations expressed by these verbs using two methods. The first, Pairwise (Sec-tion 3.4.1), is a non-transitive method that determines the order for two relations r i and r j by making pairwise de-cisions. The other method, GraphOrder (Section 3.4.2), uses a novel graph-based label propagation algorithm to temporally order relations in a joint optimization, and also while taking transitivity into account.

Because we are the first to address the problem of tempo-rally ordering relations, there are no existing techniques that we could compare to. In our experiments, we compare the resulting temporal order of relations to a random baseline inspired by [9], which addresses a related but different prob-lem. We also use our non-transitive method, Pairwise, as baseline to compare to GraphOrder, our proposed transitive graph-based method. We now define scores for the the temporal order relations TBefore and TSimultaneous as follows: where TSimultaneous(r i , r j ) = TSimultaneous(r j , r Z = TBefore(r i , r j ) + TBefore(r j , r i ) + TSimultaneous(r is the normalizing factor. As is evident from the equations above, the temporal ordering scores for a pair of relations are obtained by marginalizing over all the verb pairs used to express the two relations. We call this method Pairwise.
The temporal ordering method described in previous sec-tion makes pairwise decision considering two relations at a time. However, it is conceivable that instead of ordering two relations at a time, a collective framework which looks at all relations at once could be useful. Transitivity is one such benefit that is possible in this collective ordering frame-work. For example, the pairwise method might infer that TBefore(r 1 , r 2 ) and TBefore(r 2 , r 3 ) are true, but it might fail to identify that TBefore(r 1 , r 3 ) is also true. However, if we considered all three relations at the same time, then it might be easier to establish the temporal relationship be-tween r 1 and r 3 . Although simple transitive closure would work well in case of hard orderings, it is not clear how such a closure could be computed with soft ordering constraints, as is the case in our setting, where there is inherent uncertainty in the inferred temporal orders. We note that the methods for inducing global ordering over soft orderings [5, 8] are not applicable in the current setting, as instead of a single global ordering, we are interested in finding the transitive closure over the soft orderings.

In order to overcome these challenges, we propose a novel graph-based label propagation algorithm GraphOrder, which is both parallelizable and scalable. We first create a graph G = ( X,E,W ) whose vertices X = R  X  U (see Equation 1), i.e., each vertex is either a relation or a verb. Edges in this graph consist of the following mix of directed and undi-rected edges 6 : (1) undirected edge between relation r and verb v with edge weight W r,v = RelVerb( r,v ); (2) directed edge between verb v i and v j with W v i ,v j = NBefore( v (3) undirected edge between verb v i and v j with W v i ,v NSimultaneous( v i ,v j ); (4) directed temporal order edge be-tween relations r i and r j with W r i ,r j = TBefore(ri , (5) undirected temporal order edge between r i and r j with W r i ,r j = TSimultaneous(r i , r j ). This graph is very similar to the one shown in Figure 1, except that there are no doc-ument nodes here. We have E = E D  X  E U where E D is the set of directed edges while E U is the set of undirected edges.
We shall now inject each relation node u  X  R  X  X with a unique node specific label l u . This seed information is stored in the initial label matrix Y with Y u,l u = 1. Each such relation specific label l u has another temporally-after variant ~ l , which can be interpreted as follows: if another relation node v  X  R is assigned the label ~ l u with high score, then we should infer that there is a before ( u,v ) relationship between the two relations. Otherwise, a high l u score on v should lead us to infer that there is a simultaneous ( u,v ). Following [19], we shall also have a none-of-the-above label, &gt; , which we shall use as an automatic threshold on the label scores. Let L be the total set of labels with total | L | = 2  X  n + 1 labels in it, where | R | = n is the total number of relations.
In order to perform collective node classification and thereby infer temporal ordering among relations, our proposed algo-rithm, GraphOrder, optimizes the following objective func-tion (Equation 6), where  X  1 , X  2 , X  3 are hyperparameters, and M is a regular-ization matrix, and  X  Y u,l  X  R is the output variable which
By directed, we refer to edges corresponding to NBefore and TBefore; and by undirected to edges corresponding to NSimultaneous and TSimultaneous orders. measure the score of label l assigned to node u . The ob-jective above is convex 7 and is similar in spirit to the QC criteria [3] and MAD [19] that tries to minimize difference between predicted and actual labels, with the critical dis-tinction that the second term in this objective involves di-rected edges with transformed labels across the edge ( l vs ~ l ). We develop a Jacobi iteration-based iterative algorithm to solve this objective 7 . However, in order to handle the directed edges, whenever a label l is to be spread over the directed edge ( u,v )  X  E D , instead of updating the score for label l on v , we transform the label and update the score for ~ l on v . We iterate this process for a fixed number of itera-tions 8 , after which the assigned labels are used to infer the temporal relations as described earlier in this section. The objective function and solution to the optimization problem are detailed in the Appendix (Section A and Section B).
The algorithm we propose above is novel as no previous la-bel propagation algorithm has considered the situation when there is a need to transform labels during propagation. Also, this algorithm is essentially self-supervised as no external hand labeled data is necessary. As we will see in the ex-perimental results in Figure 4, the proposed algorithm ef-fectively combines these strategies for the task of temporal order acquisition. We select entities in Yago2 [12] as our domain instances. We select relations in Yago2 that are incident on these enti-ties to temporally order and their relation instances (facts) to temporally scope . Yago2 is a knowledge base contain-ing entities and relations automatically extracted from the entities X  Wikipedia infoboxes. For this reason, we choose Wikipedia as the source of our unlabeled corpus D . Each document in D is thus about an entity (i.e., contains the entity in its infobox). Although Wikipedia pages, being gen-eral purpose, may seem more chronological and thus better suit the assumption that narrative follows temporal order, we show in experiments that the choice of Wikipedia pages does not in fact trivialize the problem. There is enough vari-ation in the narrative sequence of verb mentions in Wikipedia that we benefit from aggregation. We use Wikipedia pages without any special processing, omitting infoboxes, using only sentences in the page sorted by their narrative order in the page.

The relations we want to temporally order and whose in-stances we want to time scope are chosen from the domains: albums , cricketers , films , footballers , novels , operas , plays , politicians , and songs . For each domain, we select the top 25 relations that have at least one argument belonging to the domain, to temporally order (based on the number of re-lation instances/facts). We also select as domain instances , whose Wikipedia pages we use as our corpus D , the top 1000 Yago2 entities that participate the most in these relations. By doing so, we ensure that D contains a lot of information on the relations we want to temporally order. We define an argument of a relation as belonging to the domain films if it belongs to any Wikipedia category that ends with the word  X  films  X : e.g. wikicategory 1930s crime films , etc.
Please see appendix for details.
Set to 10 iterations for the experiments in this paper Figure 4: Comparison of performance of the tempo-ral constraint acquisition methods discussed in the paper. We observe that the graph-based inference method GraphOrder (Section 3.4.2) (i.e., the right-most bar) achieves the best performance on average.
For example, the top 5 relations selected for the domain films are actedIn , wasCreatedOnDate , directed , created , and produced . A lot of relations in Yago2 are biographical (birth, death) due to the nature of the infoboxes. We also notice that some entities are categorized incorrectly in Wikipedia, e.g., a person maybe categorized as films 9 . These may lead to irrelevant relations selected, e.g., wasBornIn for films .
Following [9], we use a random baseline which generated 300 random sets of pairwise orderings for each domain. In each set, we generate the same number of orderings as that learned by GraphOrder (described below), and the perfor-mance is averaged across these 300 sets. As there are no existing methods that output temporal orders at the re-lation level, we use the methods that are based on Pair-wise Temporal Ordering (Section 3.4.1): using default verbs only ( Pairwise ) or using also verbs derived from the SVO dataset ( Pairwise++ ) as additional baselines. We com-pare these baselines to our proposed method GraphOrder (Section 3.4.2), which performs collective temporal ordering using label propagation-based inference over a graph consist-ing of constraints from the Pairwise++ system. We observe that GraphOrder converges quite fast (10 iterations are usu-ally sufficient).

For each domain and each method, we obtain temporal orderings of relations and the scores of the orderings. Each ordering is between a pair of relations. For evaluation, for each domain and each method we create a directed acyclic graph (DAG) of the learned orderings, traversing over the orderings from the highest score to the lowest score, and adding an ordering to the DAG only if it does not conflict with previously added orderings.

Similar to temporal evaluation in [9], we hand identify a gold standard set of temporal orderings for each domain. All attempts were made to include every before and simul-taneous relation that can exist between the relations for the http://en.wikipedia.org/wiki/Joseph Kessel is a person that has a Wikipedia category:  X  X ovels adapted into films X  domain or that could be deduced through transitivity rules. We ignore temporal orders of relations that are ambiguous to humans. For example, isMarriedTo and actedIn , one can be before or after the other at the relation-level. We eval-uate each temporal ordering (i.e., each edge) in the DAG for correctness according to the gold standard. Precision is computed as the number of edges marked correct over the total number of edges marked unambiguous in the DAG. Since it is difficult to determine upfront the full set of valid temporal orderings, we decide to estimate the recall with respect to the largest number of correct edges returned by any one of the compared methods. Using precision and the estimated recall, we compute F1 (Figure 4).

Experimental results comparing these four systems are shown in Figure 4. We see that GraphOrder performs better than all three baselines on average and across all domains, showing the merit of our graph-based, unsupervised, macro learning of temporal orderings. On average, we observe a 38.4% improvement in F1 score over the random baseline for GraphOrder.

In Figure 4, we see that performance (F1) of Pairwise++ is higher than Pairwise on average and across all domains except in footballers . This indicates that adding verbs from SVO typically improves temporal ordering and demonstrates the benefits of having a more flexible representation of rela-tions that involves multiple verbs. We see that the highest increase in performance is in politicians , in which we find the largest number of SVO verbs (118 verbs) to represent the relations. In contrast, we only obtain one verb from SVO:  X  X ad visited X  that represents the influences relation in footballers . In this case, adding verbs from SVO does not help performance and may even degrade it if the verb found does not really express the relation, as in this case.
In Figure 4, we observe that performance of GraphOrder is higher than the non-transitive baselines (Pairwise++ and Pairwise). In terms of precision and estimated recall, we find that GraphOrder increases recall without hurting precision. On average, GraphOrder increases the number of correctly learned temporal orderings by 48.7% from that learned by Pairwise++ and this improves the F1 score by 12.2%. This shows how collective reasoning using label propagation can improve temporal ordering.

A few examples of temporal orders learned by GraphOrder in politics and in films are shown in Figure 5. As can be seen from the figure, in case of politics , the method is able to learn orderings pretty accurately: all relations happen after was-BornIn . In addition, a person must be a citizen of some country if he happens to hold a political position , and/or is a leader of something. We also notice that a politician or a leader must have an influence at least before he dies . In case of films , the method learns correctly that the process of creating a movie: producing , directing , acting must hap-pen in the same period of time and all before the movie can win a prize. However, there are still questionable ordering such as diedIn before hasWonPrize (possibly winning an award posthumously), and mistake such as wasBornIn si-multaneous produced . We note that all these orderings have scores attached to them which can be taken into account in any subsequent consumption of these constraints, as we do in temporal scoping (Section 4.4). Figure 6: Performance of GraphOrder as the num-ber of Wikipedia documents from which the narra-tive order of verbs are estimated is increased. We observe that the overall temporal ordering perfor-mance improves as increasing number of documents are used to estimate narrative order. All results are averaged over the 9 domains.
The goal in this section is to measure what effect aggre-gation of narrative order of verbs from a large number of Wikipedia documents has on final temporal ordering of re-lations. In Figure 6, we report the temporal ordering per-formance of GraphOrder, the best performing system from Section 4.2, as the fraction of Wikipedia documents from which narrative order of verbs are estimated is increased. All results are averaged across 9 domains. From Figure 6, we observe that performance improves sharply as the frac-tion of documents used is increased. This demonstrates the value of aggregation for temporal ordering. The results in Figure 6 suggest that a small number of documents (e.g., 1%) are not sufficient for optimal temporal ordering perfor-mance. This also suggests that the use of Wikipedia doesn X  X  trivialize the problem as otherwise a small number of doc-uments would have been sufficient for reliable estimation of Table 1: Results of temporal scoping experiment us-ing the CoTS Soft framework, with different sources of inter-relation temporal ordering constraints: None (i.e., no such constraints used), constraints inferred by GraphOrder (Section 3.4.2), and manually en-coded constraints. All results are averaged over five evaluation sets, with standard deviation indicated in brackets. For each domain, the setting with high-est F1 score is marked in bold. See Section 4.4 for details. narrative orders and thereby temporal order of relations. In-stead, these results suggest that document specific inconsis-tencies can be overcome through aggregation across a large set of documents, resulting in improved temporal ordering performance.
The experiments described above establish that GraphOrder, our proposed approach, can acquire knowledge of the typ-ical temporal ordering between different relations. In this section we explore how useful these learned temporal con-straints are, for the task of assigning specific temporal scope to individual facts (instances of these relations). In particu-lar, we use an extended version of the CoTS system [20] to temporally scope facts from the Yago KB. CoTS is a joint in-ference ILP framework for temporal scoping that combines the rising and falling of counts of occurrences of facts in documents over time (as a signal to indicate the period of validity of the facts), with constraints about the facts ( si-multaneous, before or after temporal orders) to infer the temporal scope of facts (i.e., the interval of time when a fact is true). The original CoTS system [20] can only han-dle hard and unweighted constraints, which is not adequate in our case as the temporal ordering constraints learned by GraphOrder are soft and weighted in nature. In order to overcome these limitations, we present an extension of the CoTS system, which we shall refer to as CoTS Soft .
CoTS Soft System : Let x r,s,t  X  { 0 , 1 } be a binary vari-able with x r,s,t = 1 whenever fact r from relation s is true at time t , and 0 otherwise. Let z b r,s,t  X  { 0 , 1 } be a binary variable with z b r,s,t = 1 indicating that fact s started to be true at time t . Similarly, z e r,s,t = 1 indicating that fact s ceased to be true at time t . Let b ( r,s,t ) be our initial belief that fact s from relation r is true at time t . Now, given a temporal order TBefore(a , c) with score q a,c , and two facts u and v from the relations a and c , respectively, CoTS Soft introduces the following constraint, where  X  a,u,c,v  X  X  0 , 1 } is a binary slack variable which allows this constraint to be violated, but not without suffering a penalty of q a,c in the objective as we shall see shortly. Please note that CoTS Soft enforces this soft-constraint iff facts u and v have a shared argument entity. CoTS Soft also uses the Consistency constraints from [20] to make sure the z b z e and x variables are consistent with one another at any given time. CoTS Soft uses the following set of inequalities to encode a TSimultaneous(a , c) ordering between the two facts u and v from above at any given time t .  X  be the set of all slack variables, with q  X  representing the penalty associated with slack variable  X  . We now present the objective optimized by CoTS Soft subject to the constraints presented above. 10 where  X  is a hyperparamater which we set to 1 for all the ex-periments in this section. We use the Mosek 11 optimization package to solve this Integer Linear Program (ILP).
We now describe the experimental setup used to evaluate the usefulness of temporal orderings learned by GraphOrder. In this section, we experiment with three domains: films , novels and songs . For each domain, we obtain facts from the Yago2 KB as in previous sections, a subset of which is already temporally scoped. We split this subset into two parts, with the first set consisting 80% of the facts. We keep the temporal scopes of the first set fixed as prior knowledge, and use the CoTS Soft system to predict temporal scopes for the other set. We then compare predicted temporal scope with truth to compute precision, recall and F1. We repeat this process five times for each domain, generating a separate
Please note that even though we discuss only two types of constraints here, slack variable-based extensions of all the constraints in the CoTS system have been implemented in-side CoTS Soft . We refer the reader to [20] for a complete listing of these constraints. http://www.mosek.com/ split each time. All results are averaged over these five runs. For each split, we evaluate temporal scoping performance of the CoTS Soft system when injected with inter-relation tem-poral ordering constraints obtained from three sources: (1) None (i.e., when no temporal ordering constraints are used); (2) temporal orders learned by GraphOrder in Section 4.2; and (3) manually crafted ordering rules. For all experimen-tal runs, we required CoTS Soft to make point predictions (i.e., unit length temporal scopes). For each fact r ( a,b ) we query the Gigaword corpus 12 with  X  a AND b  X  or  X  a AND Default(r)  X  if b is a date, to estimate b ( r,s,t ). We perform all evaluations at the year granularity, and since Gigaword X  X  coverage is limited to the period 1994-2008, we only consider facts which are true during this period.

Experimental results comparing these three ordering gen-erators are presented in Table 1. For all three domains, we observe that GraphOrder leads to improvement in perfor-mance (F1) compared to when no constraints are used. In-terestingly, constraints learned by GraphOrder outperforms manually crafted constraints in two out of the three domains. This might be due to the incomprehensiveness of the man-ual rule writing process leading to omission of certain use-ful orders, which were included in the orderings learned by GraphOrder.
 Next, we evaluate whether the temporal orders learned by GraphOrder, along with CoTS Soft , can be used to increase Yago2 X  X  temporal coverage, i.e., temporally scope Yago2 facts which are currently not scoped. We experiment with facts from the films domain (total 1831 facts), and fix all facts that are already temporally scoped (such as instances of produced ). We then use the CoTS Soft system to temporally scope facts from the actedin relation, which are currently not scoped. Using random sampling-based manual evalua-tion of 100 facts, we observe a temporal scoping precision of 69.8%. Thus, we are able to increase Yago2 X  X  temporal coverage at a modest precision, showing the merit of learn-ing temporal constraints by GraphOrder and using them for temporal scoping. A few examples of correct scoping are: 2001 for actedIn (Jason Biggs, American Pie 2); 1996 for actedIn (John Neville, High School High). Examples of an incorrect scoping is: 1998 for actedIn (Judy Davis, Decon-structing Harry) (correct: 1997).
Schank and Abelson [18] first proposed hand-coded scripts to describe frequently recurring social situations such as a visit to a restaurant. Our work is similar in spirit to scripts. However, instead of hand coding scripts, we automatically identify verbs that can express a relation, find sequence of these verbs in a document, and aggregate over similar se-quences in a large number of documents to infer temporal order of relations expressed by the verbs.

We also draw from Chambers and Jurafsky X  X  [9] automatic induction of script-like structure called narrative events chain. We use a related notion of protagonist (domain) overlap to relate the set of relations we temporally order. But we dif-fer in that we use narrative order to serve as probabilistic training signals for learning temporal order; and use unsu-pervised method of temporal ordering instead of a super-vised classifier. Also, their focus is to output the narrative chain of verbs in a document while ours is to output the
English Gigaword: http://bit.ly/KJMTYr typical temporal order of relations over a large number of documents.

Fujiki et al. [11] investigated script acquisition by arrang-ing first paragraphs of news articles based on their dates of issue. In contrast, we do not use such date of issue times to order paragraphs in documents, thus our method is applica-ble more widely even to documents without creation time. We also temporally order relations instead of just verbs.
Timely YAGO [22] harvests temporal facts using regular expressions in Wikipedia infoboxes, but is not applicable to arbitrary text. PRAVDA [21] harvests temporal facts using a combination of textual patterns and graph-based re-ranking techniques to extract facts and their temporal scopes at the same time. In contrast, our goal is to in-fer typical temporal order of relations , and not temporally scope individual facts. However, our experiments demon-strate that knowledge of typical temporal order can be ef-fective in improving temporal scoping performance.

There are several algorithms for classifying temporal rela-tions between events [15, 8, 24]; many of them trained on the TimeBank [17] dataset. Unlike these methods, we propose a self-supervised method for temporal ordering that does not require labeled data. While these previous works focused on temporal ordering of events with one verb per event, we temporally order relations that are represented by a group of verbs. To the best of our knowledge, there is no other work on temporal ordering of relations. However, these previous works are complementary in that their output can be used as additional verb ordering signals to augment our narrative order statistics.

Finally, CoTS [20] is a recently proposed system for tem-porally scoping relational facts which so far used manually edited temporal order constraints. While manual ordering is appealing, unfortunately it is not very scalable as in prac-tice hundreds of relations, whose numbers are ever increas-ing, may need to be ordered. As demonstrated in Section 4, our automatically acquired, scalable temporal orderings can be a replacement for the current manual rules in CoTS. We also extend CoTS framework to CoTS Soft which can handle weighted constraints through the use of slack variables.
Learning to model ranked data from partial [13] and pair-wise preferences [14] has been recently studied. While the pairwise orderings in [14] are unweighted, the orderings in GraphOrder are weighted. Also, these previous studies con-sidered only one type of ordering relationship between two items (viz., whether one item is preferred over the other), while GraphOrder uses two ordering relationships, TBefore and TSimultaneous, to compare two items (relations in our case). More importantly, in contrast to these previous ap-proaches, the goal in GraphOrder is not to learn a ranking model, but to expand an initial set of weighted pairwise temporal orderings by taking transitivity into account.
In this paper, we address the problem of learning typ-ical temporal orderings over relations (e.g., learning that directed(person, film) typically occurs before wonPrize(film, award) ). This problem is important because this type of knowledge can play a key role in assigning temporal scope to specific instances of these relations (e.g., to determine when the relation instance wonPrize(Gone With The Wind, Academy Award) occurred). We present both a method for acquiring this type of general ordering knowledge from dis-tant supervision, and a method for using this knowledge to infer temporal scope for specific instances of these rela-tions. To acquire this knowledge, we introduce GraphOrder, a novel graph-based collective label propagation method. Our approach is based on the assumption that relation men-tions in sentences are often expressed by verbs, the assump-tion that narrative order of verbs correlates with the tem-poral order of the relations they represent, and the fact that temporal orderings are transitive. Our approach mines a corpus of 890m dependency parsed sentences to discover verbs that express relations of interest, and collects statistics about mentions of these verbs over a large number of doc-uments. We show the effectiveness of our proposed method for learning temporal orders of relations, compared to non-transitive and random baselines. We also show the utility of this learned knowledge about temporal ordering, for the problem of assigning temporal scope to specific instances of these relations. In future work, we would like to explore alternative approaches of estimating narrative order. We would also like to extend the proposed algorithms to learn-ing orderings when some relation arguments are instanti-ated, and believe the knowledge our current system learns can serve as useful priors for acquiring this information. We thank Justin Betteridge (CMU) for help with parsing the corpus. We are thankful to Mosek 13 for their gener-ous academic licensing; and to the anonymous reviewers for their constructive comments. This research has been sup-ported in part by DARPA (under contract number FA8750-09-C-0179), Google, and Fulbright. Any opinions, findings, conclusions and recommendations expressed in this paper are the authors X  and do not necessarily reflect those of the sponsors. [1] O. Alonso, M. Gertz, and R. Baeza-Yates. On the [2] N. Balasubramanian, S. Soderland, Mausam, and [3] Y. Bengio, O. Delalleau, and N. Le Roux. Label [4] S. P. Boyd and L. Vandenberghe. Convex [5] P. Bramsen, P. Deshpande, Y. Lee, and R. Barzilay. [6] S. Brody. Clustering clauses for high-level relation [7] J. Callan, M. Hoy, C. Yoo, and L. Zhao. Clueweb09 [8] N. Chambers and D. Jurafsky. Jointly combining [9] N. Chambers and D. Jurafsky. Unsupervised learning
Mosek ApS: http://mosek.com/ [10] T. Chklovski and P. Pantel. Verbocean: Mining the [11] T. Fujiki, H. Nanba, and M. Okumura. Automatic [12] J. Hoffart, F. Suchanek, K. Berberich, [13] G. Lebanon and Y. Mao. Non-parametric modeling of [14] T. Lu and C. Boutilier. Learning mallows models with [15] I. Mani, M. Verhagen, B. Wellner, C. Lee, and [16] J. Nivre, J. Hall, J. Nilsson, A. Chanev, G. Eryigit, [17] J. Pustejovsky, P. Hanks, R. Sauri, A. See, [18] R. Schank, R. Abelson, et al. Scripts, plans, goals and [19] P. P. Talukdar and K. Crammer. New regularized [20] P. P. Talukdar, D. Wijaya, and T. Mitchell. Coupled [21] Y. Wang, B. Yang, L. Qu, M. Spaniol, and [22] Y. Wang, M. Zhu, L. Qu, M. Spaniol, and G. Weikum. [23] G. Weikum, S. Bedathur, and R. Schenkel. Temporal [24] K. Yoshikawa, S. Riedel, M. Asahara, and
We first reproduce the objective function minimized by the label propagation-based GraphOrder algorithm presented in Sec 3.2.2 (Equation 6), and write it as a sum of two func-tions as follows.
 where
J 1 ( {  X  Y ul } ) =  X  1 X where  X  1 , X  2 , X  3  X  0 are hyperparameters and From the construction of the graph, we have W u,v  X  0, i.e., all edge weights are non-negative. From the analysis in [19], we know that J 1 is a convex function. We also know that the non-negative weighted sum of two convex functions is also a convex function [4]. Putting all of these together, we shall be able to prove that J is convex if we can show that J 2 is a convex function.

We rewrite J 2 as the non-negative weighted sum of even smaller functions:
Now, reusing the non-negative weighted sum property of convex functions once again, we can show that J 2 is con-W u,v  X  0 ,  X  u,v ). We shall prove that J vex function by showing that its Hessian is a Positive Semi-Definite (PSD) matrix.

After some algebra, we can show that the Hessian matrix for any x = [ x 1 x 2 ] T , we have x T Hx = 2( x 1  X  x 2
From this we conclude that J 0 2 (  X  Y ul ,  X  Y v ~ l ) is convex, which implies that J 2 is convex, and which in turn implies that J , i.e., the objective function function optimized by the GraphOrder is indeed convex.

In the previous section, we proved that the objective J minimized by GraphOrder is convex. Hence, the optimal solution is obtained by setting  X J Following the analysis in [3], we develop a Jacobi itera-tion method and end up with the following iterative update, which updates the score for label l at node u at time instant t + 1 as follows: Above, S is a diagonal matrix with S u,u = p inj u when u  X  R (the set of relation nodes) and 0 otherwise, with p using the equations in Sec 2.2 of [19]; I ( l u ,l ) = 1 iff u  X  R and l is the node specific label for relation node u . While cess is updated until convergence or until a fixed number of iterations. We have found that the iterations usually con-verge with a small number of iterations. We also note that the iterative updates above can be easily parallelized in the MapReduce framework using Hadoop, and hence the opti-mization can be applied to large graphs.
