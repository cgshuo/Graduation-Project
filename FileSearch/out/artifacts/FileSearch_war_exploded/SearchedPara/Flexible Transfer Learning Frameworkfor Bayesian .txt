 Tinu Theckel Joy ( Whether it is the design of new products in manufacturing, or tuning hyper-parameters in machine learning algorithms, it is expensive to search for the best solution exhaustively because these functions are expensive to evaluate. Bayesian optimisation offers powerful solutions in this space [ 1 ]. It is efficient in terms of the number of function evaluations required, and is powerful to model objective functions without knowing its form [ 2 ]. Bayesian optimisation has been success-fully applied in many different fields including learning optimal robot mechanics [ 3 ], sequential experimental design [ 4 ], optimal sensor placement [ 5 ], etc. Recently, it has found popularity in tuning hyperparameters for machine learning algorithms [ 6 , 7 ]. A problem arises in the case of  X  X old start X , when a new tuning task is tackled. In initial trials, many bad set of hyperparameters may be recommended before a good region is found. When data is large and model is complex, tuning hyperparameters can be excruciatingly long. Reducing the time to optimally tune remains an important problem to solve.
 One solution is to induce transfer learning by leveraging the data from pre-vious tasks. Bardenet et al. [ 8 ] build a model from past experience by bias-ing search in a new problem towards the part of the hyperparameter space where optimal hyper-parameters can be found. Incorporating a surrogate based ranking method, they can collaboratively optimise similar objective functions. Yogatama and Mann [ 9 ] use a Bayesian optimisation setting to transfer knowl-edge from one dataset to the next by using a Gaussian process to model devia-tions from the per dataset mean. However, both assume that the transfer occurs where the source (previous dataset) and the target (current dataset) tasks are highly related e.g. [ 8 ] assumes strong similarity in terms of ranking behavior and [ 9 ] assumes strong similarity in the deviations from the respective means. Therefore, transfer learning approach for Bayesian optimisation that can han-dle different relatedness among the source and the target tasks in a principled manner is still an open problem.
 knowledge across tasks. Intuitively, we do this by modeling the source data as noisy observations of the target function. We achieve this through the mod-ification of the kernel of the Gaussian process, adding more noise variance to source observations. We start by assuming that the source and target functions lie within some envelope of each other. The width of the envelope is determined by the noise variance -smaller noise variance imply that the source observations provide a strong prior knowledge of the target function, larger noise variance imply that the source does not influence the target function. Former is required when tasks are related and the latter is desirable when the tasks are unrelated. We estimate the appropriate noise variance for the target and a source task from the data in a Bayesian setting.
 sample a small fraction of the data, treating it as a source. This source dataset is then evaluated exhaustively on a number of different hyperparameters. Since the number of data in the source is small, the cost of exhaustive evaluation is low. This knowledge is now used to tune the hyperparameters of the original dataset. We experiment on three benchmark classification datasets for finding the best hyperparameters for two machine learning approaches -elastic net and support vector machine with RBF kernel. In all the experiments, our method is able to find the best hyperparameters in the least amount of time (considering time taken by both source sampling and the target optimisation) over both the current state-of-the-art and the usual tuning algorithm without transfer learning.  X  Proposal of a new transfer learning algorithm for Bayesian optimisation in the most general setting that includes source and target tasks with different similarity. This is achieved by modeling the source as a noisy observation of the target function and automatically estimating the noise variance from data.  X  Proposal of a novel setting for tuning hyperparameters that exploits the pro-posed transfer learning framework to improve efficiency. Using a small fraction of the training dataset as a source task, we accelerate the hyperparameter tun-ing for the whole training set, which is modeled as the target task.  X  Evaluation of the proposed algorithm for the best hyperparameter search for two machine learning algorithms on three benchmark classification datasets.
Our method is around 6 times faster than methods that tune hyperparame-ters without transfer learning and around 3 times faster than the state-of-art transfer learning for Bayesian optimisation [ 9 ]. 2.1 Gaussian Process Gaussian processes (GPs) [ 10 ] are a way of specifying prior distributions over the space of smooth functions. The properties of the Gaussian distribution allow us to compute the predictive means and variances in closed form. It is specified by its mean function,  X  ( x ) and covariance function, k ( x, x ). A sample from a Gaussian process is a function as, where the value f ( x ) at an arbitrary x is a Gaussian distributed random variable specified by a mean and a variance. Without any loss in generality, the prior mean function can be assumed to be a zero function making the Gaussian process fully defined by the covariance function. A popular choice of covariance function is squared exponential function, Other choice of covariance functions include linear kernel, Mat  X  ern kernel etc. Let us assume that we have data points x 1: p and say that the function val-ues corresponding to those data points are sampled from the prior Gaussian process with mean zero and the covariance function k ( x i y = f ( X 1: p ) as the function values corresponding to the data points X The function values y 1: p jointly follow a multivariate Gaussian distribution y  X  X  ( 0 , K ), where is called the kernel matrix. For a new data point x p +1 , let the function value be y = f ( x p +1 ). Then, by the properties of Gaussian process, y jointly Gaussian as, where k =[ k ( x 1 , x p +1 ) k ( x 2 , x p +1 ) ... k ( x Woodburry [ 10 ] formula, the predictive distribution of the function value at a new location ( x p +1 ) can be written as, where the predicted mean and the variance is given by  X  p a noisy estimate of the actual function value i.e. y = f ( x )+  X  , where  X  N (0 , X  2 noise ) the modified predictive distribution becomes  X  2.2 Bayesian Optimisation Bayesian optimisation is an efficient method for the global optimisation of costly objective functions [ 2 ]. It is especially used in situations where one does not have access to the function form. The user only has the access to the noisy evaluations. Examples in machine learning include tuning hyperparameters of a machine learning model, where the function that relates the choice of hyperparameters to the model performance is unknown and can be very complex.
 misation then employs a simple strategy where it makes use of a surrogate utility function, which is easy to evaluate. The surrogate utility function is called acqui-sition function . The role of the acquisition function is to guide us to reach the optimum of the underlying function. Essentially, acquisition functions are defined in such a way that a high value of the acquisition function corresponds to the potential high value of the underlying function when the optimisation problem is a maxima problem. The new point ( e.g. new hyperparameter setting) to evaluate next, is then obtained by maximizing the acquisition function.
 Acquisition Functions. Acquisition function can be defined either using improvement based criteria or using confidence based criteria. Improvement based criteria such as Probability of Improvement (PI) [ 11 ] or Expected Improve-ment (EI) [ 12 ] results in maximizing the probability of improvement over the current best or the improvement in the expected sense. Confidence based cri-teria such GP-UCB [ 13 ] use the upper confidence bound of the GP predictive distribution as an acquisition function. Sometime, a mix of them can also be used as the acquisition function [ 4 ]. In this paper, we use EI as the criteria for its usefulness and simplicity. A brief description of EI is provided below. Expected Improvement (EI). Let us assume that our optimisation problem is optimizing arg max x f ( x ) and the current best is at x The improvement function is defined as, The acquisition function is then defined on the expected value of I ( x )[ 2 ]as, Algorithm 1. Generic Bayesian Optimisation Algorithm where z =(  X  ( x )  X  f ( x + )) / X  ( x ).  X  ( . )and  X  ( . ) are the CDF and PDF of a standard normal distribution respectively.
 The generic Bayesian optimisation algorithm is presented in Algorithm 1 . The generic Bayesian optimisation algorithm suffers from  X  X old start X  problem i.e. at the beginning it may take many trials before it reaches a good region. To improve the efficiency, we propose a novel Bayesian optimisation framework using transfer learning. We elaborate our framework in the context of hyperparameter optimisation.
 Let us denote the source observations as { x s i ,y s i } hyperparameter setting, y s i is the performance of the model built using the hyper-parameters x s i and N s is the size of source observations. The source observations are generated either on a grid or at random hyperparameter settings. No opti-misation is performed at this stage. We assume that the source and the target function lies within a close proximity with each other since they only differ in the amount of training data; source models the mapping from hyperparameter setting to the model performance on a small subset of the whole training data, whereas target function maps the same for the whole training data. We model the difference between the source and target. We use source data to provide us with a rough guideline about the target function, f t ( . ). To accomplish this, we model source observations as a noisy measurement of the target function as, where s N (0 , X  2 s ) is a random noise. This implies that the source function val-ues lies within 3  X  s ball of the target function values with a probability close to 1. Let us denote the observations from the target task as { x is the number of target observations so far. We combine data from both the source and target and create a combined observation set: X = y = { y s , y t } . The target GP is built using the combined observation. The kernel matrix for the combined data is computed and then it is updated to incorporate the noise of the source as, where  X  2 s models the closeness between the source and the target function and  X  t is the measurement noise for the target. The value of the  X  belief on how close the source and target functions are. If they are thought to be very close then we should set  X  2 s small and large otherwise. The value of  X  greatly effect the efficiency of the Bayesian optimisation. In the next section, we will provide a principled way to estimate its value from the target observations. Estimating the Source Noise Variance (  X  2 s ). We estimate the source noise variance from the data by placing an inverse gamma distribution with parameters  X  0 and  X  0 as the prior distribution as, We start with a wide prior and then update the posterior from the observation of output value of the target ( y t ) and the source ( y s ) for the same hyperparameter setting. We use the evaluated target value at the recommended settings and use the source predicted value ( X  y s ) at those settings. The source function is modeled with a Gaussian process. Since the inverse gamma is a conjugate prior to the variance, the posterior is also an inverse gamma distribution with updated parameters  X  n and  X  n as, Assuming the mean of the difference to be zero, the parameters  X  updated as follows, We use the mode of the posterior distribution as the value of source noise variance and it is given by, The kernel matrix is recomputed following ( 11 ) and the Bayesian optimisation is sequentially performed using this kernel matrix. The proposed algorithm is illustrated in Algorithm 2 . Algorithm 2. Proposed Transfer Learning Algorithm. 4.1 Experimental Setup We conduct experiments on both synthetic and real world datasets. Through two different synthetic datasets, we create two transfer learning situations by vary-ing the similarity between the source and the target functions and analyse the behavior of the proposed algorithm in those cases. For the experiment with syn-thetic data, we do not tune hyperparameters of any classifier, rather we assume that the source and the target functions are known and the task is to reach the maximum of the target function. Experiments with real world dataset are per-formed to evaluate our algorithm with respect to the baselines on the efficiency of tuning hyperparameters for two classification algorithms. For the experiment on tuning hyperparameters, a fraction of the training data is treated as the source and whole as the target task.
 We compare the proposed method with the following baselines:  X  Efficient-BO [ 9 ] : In this transfer learning approach, a common function for source and target is learnt where the common function is represented as devi-ations from their respective means.  X  Generic-BO: Algorithm 1 is used only on the target dataset.
 We evaluate based on both the number of iteration taken and total time taken to reach the maximum performance. All timings reported in the experiments, are computed for programs running on a workstation with 3.4 GHz quad-core processor having 8 GB RAM. 4.2 Experiment with Synthetic Data We generate two synthetic datasets: in synthetic dataset-I, we create a a target function that is highly similar to the source function and in synthetic dataset-II, we create the target function to be very different from the source function. The source function is always fixed in both the scenarios whilst the target func-tion is varied. The source function is a 2-variate normal probability distribution function with mean at [0,0] and covariance matrix as I 2  X  are also modeled by another 2-variate normal probability distribution function with the same covariance matrix but at different means. For synthetic dataset-I, the target task mean is set at (0 . 1 , 0 . 1) which is very close to the source func-tion mean. The two functions are shown in Fig. 1 a. For synthetic dataset-II, the target task mean is set at (1 . 5 , 1 . 5) which is far from the source mean. The two functions for this scenario is shown in Fig. 1 b. For both the scenarios source functions are represented with 25 data points sampled randomly between [ along both the dimensions.
 the iteration for the proposed method and the baselines for synthetic dataset-I. All the three methods start from the same position, but our proposed method is able to gain faster reaching 80 % of the maximum value within 7 and reaching at the maximum value by 22. In comparison, Efficient-BO reaches 80 % of the maximum value after 10, although finally reaching at the maximum around the same time as the source.
 knowledge, reaches 80 % of the maximum value only after 15 iterations and not reaching the maximum even after 30 iterations. Figure 1 e shows the noise variance estimate after each iteration for the proposed method. The variance starts with a high prior and decreases fast as the two functions are very close to each other.
 of the iteration for the proposed method and the baselines for synthetic dataset-II. We see that, even if all of them start from the same position, our proposed method is able to gain faster reaching close to the maximum value by 15th itera-tion. In comparison, the generic Bayesian optimisation reaches to a similar value only after 28th iteration. In this case, since the source and the target functions are quite different, Efficient-BO fails to reach beyond 60 % of the maximum with 35 iterations. Figure 1 f shows the noise variance estimate after each iteration for the proposed method. The variance starts at the same position as it started for synthetic dataset-I, but instead of decreasing, the variance increases with the iteration as the the source and the target functions are quite different for this scenario. 4.3 Experiment with Real World Datasets We experiment with three real world datasets for tuning hyperparameters for two classification algorithms: Elastic net and Support Vector Machines (SVM) with radial basis function (RBF) kernel. All three datasets are benchmark datasets net, the hyperparameters are the L 1 and L 2 penalty weights. The bounds for both of them are chosen to be within [10  X  5 , 10  X  1 ]. For SVM with RBF kernel, the hyperparameters are the cost parameter ( C ) of the SVM formulation and the width of the RBF kernel. We choose 10  X  3 to 10 3 and 10  X  C and the width of the kernel, respectively. As the ranges are high, we perform Bayesian optimisation on the logarithmic of the values of the hyperparameters. For each dataset, 5 separate training datasets are created by randomly sampling 70 % of the data for training. Average results over these training set are reported. on the different datasets are presented in Fig. 2 . The results show that the pro-posed method achieves the maximum accuracy in the least number of evaluations compared to both the methods. The transfer learning based Efficient-BO follows closely but have never been able to reach to the optimal hyperparameters faster than the proposed method. The Generic-BO without any transfer learning per-forms the slowest and mostly not being able to reach to the best within 30 iterations.
 in CPU seconds. On all the datasets and for both the classification algorithms, our proposed method reaches to the optimal hyperparameters the quickest. It is around 6 times faster than the no transfer method and around 3 times faster than the Efficient-BO [ 9 ]. This clearly demonstrate the usefulness of our proposed method for tuning hyperparameters.
 proposed method with respect to the size of the source data. Plots for all three datasets are shown. When a smaller percentage of data is used as source, the source to target difference may be higher leading to more optimisation iterations for target. This amounts to higher computational demand. Increasing source percentage implies larger computational demand for source but decreasing com-putational requirement for target. In other extreme, a large source means time taken for evaluating source itself is very high. This leads to a nice  X  X  X  shaped efficiency curve for our proposed algorithm. The optimal lies in the middle and at around 40 % for all three datasets for both classifiers. For much larger data it may be possible to reach to the maximum even with a smaller fraction of data but from our experience 20 X 40 % is a good starting point.
 In this paper, we proposed a novel transfer learning framework for Bayesian optimisation. We model source task as a noisy observation of the target function and use the source observations to avoid the cold start problem for target task optimisation. The noise variance is estimated from data based on the data in a Bayesian setting. This enabled us to address the limitations of the existing methods that only work when tasks are closely related. The proposed method performs around 6 times faster than the generic Bayesian optimisation method and around 3 times faster than the current state-of-art on the task of tuning hyperparameters.

