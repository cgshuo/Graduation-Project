 1. Introduction
When a person has an information need and decides to seek the assistance of a librarian, the person does not approach the librarian with two or three keywords. Instead, the person usually asks the librarian a ques-tion or poses a directive statement which indicates an interest in finding information. This initial question or directive might be sufficient for the librarian to take some action, or it might cause the librarian to start ques-tioning the person. Conversational norms guide the communication that takes place between a person and a librarian and communicating with keywords would be a clear violation of such norms. Even though questions contain both subject-related terms and syntactical fillers, librarians use all of the words that comprise the ques-tion when determining the most appropriate search strategies and materials. Given that this real-world process is optimized by having the user ask a question rather than state keywords, we propose that asking questions, rather than simply submitting keywords, is also potentially a better technique for posing information needs to online search services.

When a person asks a librarian a question it is common for the librarian to respond by finding information in books, or periodicals, or other resources. Being given a periodical or some other type of document in response to a question essentially answers the question if the document contains the answer. In this context an appropriate answer to the question is a document. In this paper, we seek to answer a question with a doc-ument, and we consider a document relevant if it contains the answer. For some questions the most appro-priate response may be several documents, but for the purpose of this paper, we consider only questions answerable with a single document.

Although people do not communicate with librarians using only keywords, this is the most common method for posing an information need to a search service. The process of translating an information need into a set of keywords is not considered optimal since it is believed to be an oversimplification of the user X  X  real information need ( Taylor, 1962 ). Furthermore, communicating with a system using keywords constrains the information need in a number of ways. Since users are frequently seeking information about something they do not already know, representing an information need as a set of keywords can result in an incomplete and often erroneous approximation of this need ( Belkin, 2000 ). It has been observed that the average length of queries submitted to search services is about 2.5 terms ( Spink &amp; Jansen, 2004 ). We can thus conjecture that requiring users to pose their information needs as keywords results in the loss of important information which might otherwise be used to improve retrieval. Recent work ( Kelly, Dollu, &amp; Fu, 2005 ) demonstrates that when users are probed about their information needs through an online form, they respond in natural language and provide more robust descriptions of their information problems, which results in better retrieval performance.
There is some evidence that when users pose their information needs as questions rather than key terms, they provide more terms ( Belkin et al., 2003 ). Along with the extra terms this question format encourages, there is also information contained within the structure of the question that can be used to classify question types ( Pomerantz, 2005 ). These classifications can, in turn, be used to improve retrieval ( Moldovan et al., 1999 ). However, traditional document retrieval systems often remove non-content words from questions, essentially treating the questions as a bag of words and ignoring the potentially important information con-tained within the question structure. Furthermore, traditional retrieval techniques typically use the content of documents only, ignoring other potentially important document elements such as structure, to retrieve docu-ments in response to queries. In searching a large collection of documents it would be helpful to identify highly useful documents using structural clues as well as content clues.

The importance of non-content words can be illustrated with the following two example questions:  X  X  X ow do I apply for an H1 visa? X  X  which requires an explanation of the visa application process and possibly an application form, and  X  X  X ow many people apply for H1 visas each year? X  X  for which the appropriate answer is a number. In this paper, we refer to the first question as a procedural question because it asks for a descrip-tion of a process. We refer to the second question as a fact question because it is answerable with a statement of fact. It is clearly impractical to treat  X  X  X ow many X  X  and  X  X  X ow do I X  X  as content terms, and score documents highly that contain these terms. Typical document retrieval systems represent documents and queries as dis-tributions of terms, and the terms  X  X  X ow X  X   X  X  X o X  X  and  X  X  X  X  X  are so frequent, they appear in many, if not most doc-uments, regardless of the topic of the document. Although they are integral to the meaning of the question, they are not useful indicators of the degree of topical overlap between the question and the document. Omit-ting the words  X  X  X ow many X  X  and  X  X  X ow do I X  X  strips both questions of key indicators of their meanings. In both cases the original intent of the questions and the distinction between the two has been lost. We examine procedural questions specifically because our initial retrieval results suggest that techniques developed for keyword queries are not as effective for procedural questions as they are for fact questions. We hypothesize that knowing that a person is asking about how a procedure is done will allow us to retrieve more appropriate documents.

In this paper, we focus on a classification of questions based on the  X  X  X h-words, X  X  and use this classification to characterize several thousand questions that have been posed to an online search service. We distinguish between questions that ask for a specific piece of information as indicated by question words such as  X  X  X ho is X  X  or  X  X  X hat is, X  X  and questions that ask for a description of a process, as indicated by question words such as  X  X  X ow do I X  X  or  X  X  X ow does X  X . Appropriate answers to the former are often a single value or a statement of fact.
Appropriate answers to the latter are often prescriptive and present a sequence of steps or instructions for accomplishing some task.

We speculate that questions can be classified into procedural and fact classes based on the words at the beginning of the question. We further speculate that answers to procedural questions take on a form distinct from fact questions, and that this form can be identified using structural features of documents. Finally, we propose that these features can be used to improve retrieval precision for procedural questions. Although there are many other types of questions, such as  X  X  X hy X  X  questions or  X  X  X es X  X o X  X  questions, they are beyond the scope of the current study.

In the next section we present three areas of related literature: questions and question classification, tradi-tional retrieval techniques for questions, and the use of document structure in retrieval. The following section presents our work on question classification. This includes a description of the question corpus used in this study, along with results from various techniques for classifying the questions. This section is followed by a description of our relevance assessment study, in which we identify relevant and non-relevant documents for each type of question. We then present an analysis of the features of documents relevant to each type of question and results from our experiments using these features to improve retrieval for procedural ques-tions. Finally, we conclude our work and discuss future directions. 2. Questions and classification
Some of the earliest research investigating interactive IR systems noted the difficulties that users have com-municating with systems ( Savage-Knepshield &amp; Belkin, 1999 ). For instance, in one of the first review articles written about user interfaces for interactive IR systems, Bennett (1972) observed that an effective language for human X  X omputer communication is a necessary prerequisite to improving interactive IR. At this point in IR development, users not only had to use Boolean syntax to communicate their information needs, but they were also limited to using the indexing language of the system. Bennett recognized that in most cases users were forced to communicate using the system X  X  vocabulary and not their own. He goes on to recommend that inter-face languages for IR should be habitable ( Watt, 1968 ), or  X  X  X he language should enable a person to address the computer without being exasperated by failure of the device to handle that language that he feels is appro-priate X  X . Among other research, this idea is further evident in the work of Meadow (1973) , who described inter-active IR as  X  X  X  process characterized by a conversation between man and machine, a give-and-take or bargaining situation X  X .

A number of features were included in IR systems to assist users in overcoming the language barrier, including automatic thesauri and word frequency statistics ( Lesk &amp; Salton, 1971; Thompson, 1971 ). The initial approach was to provide users 1 with tools to help them communicate in the language of the system. A second approach to addressing this problem was to allow users to communicate in natural language. Communicating in natural language includes communicating using a normal conversational syntax and an unlimited vocabu-lary. Some of the first work that investigated natural language queries was developed to address the problems that users had when posing queries in Boolean language. For example, Howard Turtle (1994) compared Bool-ean queries to natural language descriptions of an information need, posed to an index of legal documents.
The Boolean queries were constructed by expert searchers, and represented the best possible Boolean formu-lation of a given natural language query. This work demonstrated that natural language queries were more effective for retrieval as compared to Boolean queries, even for expert searchers who were comfortable using Boolean language.

Beyond querying in natural language, the idea of an interactive natural language dialogue occurring between the system and the user was also extensively investigated in IR, most notably during the 1980s.
During this time period a number of systems were developed which sought to simulate the activities performed by typical human search intermediaries (usually librarians) when interacting with users with information needs ( Brajnik, Guida, &amp; Tasso, 1987; Croft &amp; Thompson, 1987 ). These systems were, in part, based on research that analyzed real-life user-intermediary interactions ( Ingwersen, 1982 ). Although robust IR systems that could simulate the user-intermediary interaction did not come to fruition, the idea that retrieval systems should at least try to model some characteristics of search intermediaries continues to influence IR research.
Prior to the growing body of research in question-answering, which is discussed below, a number of systems were developed which accept queries in the form of questions, rather than keywords. Ask.com  X  X  X sk Jeeves X  X ) was one of the first systems to provide an alternative to key term queries for web searching.
Ask.com invited users to ask their expert (the fictional Jeeves ) a question. Question templates were provided if users needed help with this process. If the question was deemed ambiguous, an additional set of questions was provided, each of which linked to a set of results for that question. The user would choose the question most similar to the question they had in mind, and be directed to the results for this question. Although Ask
Jeeves accepts natural language queries, the search technology does little, if any, linguistic processing. The interface relies on previously answered questions to improve retrieval results and to guide users to construct more effective queries. Although the search technology uses no natural language processing, there is clearly a benefit to offering previously answered questions, in a natural language format, to guide users in their search. There are also a number of  X  X  X xpert X  X  sites that allow users to ask experts particular questions. In Google Answers, 3 users pose a question and propose a dollar amount that they are willing to pay for the answer. An expert searches for the answer and the user is charged the proposed fee for the answer only if it is found.
In this case, the questions are answered by people, and no part of the question answering technology is auto-mated. Questioners and experts frequently engage in a dialogue to clarify the question, much as a reference librarian would. In another approach, exemplified by ExpertCentral, claimed experts who are part of the knowledge network answer the questions. There are also more traditional examples of online experts who answer questions: it is very common for libraries to have online reference ser-vices. These services usually occur via email or online chat. In all these examples, users are able to communi-cate using a natural, question-based language, often in the form of a dialogue. The effectiveness of these systems, whether automated or not, is dependent on a natural, conversational query language.

There is some evidence that when users are asked to pose their information needs to IR systems as questions rather than key terms, they provide more terms ( Belkin et al., 2003 ). One reason for this is when articulating an information need as a question a user might include search terms that he or she might otherwise exclude from the query. It might be the case that users are overly selective when posing keyword queries and censor themselves based on their model of how the system works. The additional terms that are present in a question provide information that can potentially be used to improve retrieval, for instance, by disambiguating other terms in the query. The structure of the question might also provide information about relationships between terms. Part-of-speech tags might provide clues about term usage. Finally, the question words found at the beginning of the question can be used to identify the type of answer the user expects.

A number of studies have attempted to classify questions along various dimensions. Pomerantz (2005) sur-veys existing literature and identifies five types of question taxonomies: (1) wh-words; (2) subjects of questions; (3) functions of expected answers to questions; (4) forms of expected answers to questions; and (5) types of sources from which answers may be drawn.

The first approach to question classification uses the following wh-words for classification: who, which, what, when, where, why and how. These high-level classes have further been subdivided into smaller, more specific classes of questions. The wh-approach to question classification has received the most attention in the IR community with many early question answering systems using wh-word classification for question answering (an example of such a system is in Moldovan et al., 1999 ).

The second approach to question classification uses the subject terms contained within the question for classification. This approach is most like traditional retrieval where the query is treated as a bag of words, and stop-word lists and stemming remove non-subject words from questions and conflate terms to their com-mon stems.

The third approach classifies questions according to the answer type. This approach classifies questions according to the content of the information sought, and has been used more to understand question-asking behavior, and in later question answering systems.

The fourth approach classifies questions according to the form of expected answers and derives from attempts to quantify reference services provided in libraries. The most common method of classifying ques-tions using this approach is to sort them into reference transactions and directional transactions categories.
The final approach to question classification identified by Pomerantz (2005) is based on the types of sources from which answers may be drawn. This approach is also a classification of answer types rather than questions and is based on the types of sources used to answer the question (such as atlases, encyclopedias, dictionaries, etc.). 3. Retrieval for questions
Document retrieval systems often remove non-content words from the query, and/or conflate words to their common stems, so as to increase the degree of matching between the question and the set of relevant docu-ments. In so doing, verbs may be stemmed into nouns, and common words such as  X  X  X , X  X   X  X  X ho, X  X  or  X  X  X ow X  X  removed entirely. Treating the question as a bag of words eliminates the information about the question words which is vital to any question-based IR system. If the user is to pose a question to the system, we hypothesize that we can do a better job of responding to the question by using information in non-content words. Using information at the lexical and discourse levels allows much of the information inherent in questions to be accessed by shallow linguistic processing.

The process of automated question answering can be thought of as analogous to document retrieval, except that the query is assumed to be a well-formed question, and an answer is returned, rather than a ranked list of documents. Question answering has inspired a prolific body of research, mostly directed toward answering fact questions. This line of research is embodied in the Text Retrieval Conference (TREC) Question Answer-ing Track ( Voorhees, 2003 ). Each participating group is given a set of previously unseen questions provided by the National Institute for Standards (NIST). 5 The participants are not allowed to alter their systems after the release of the data. They run their systems on the questions, retrieve a list of answers to each question, and the results for each group are presented at the TREC conference, and published in its proceedings. In addition to TREC, ARDA 6 has funded the development of a number of question answering systems through its AQUA-
INT 7 project, which has resulted key developments in questions answering, including a push towards interac-tive question answering.

The body of work in question-answering, for which question classification is frequently employed, includes various rule-based and machine-learning systems. Rule-based systems typically classify questions according to their question words. In the case of  X  X  X hat X  X  questions, the question type is determined by the first noun fol-lowing the word  X  X  X hat X  X . Examples of such systems are MURAX ( Kupiec, 1993 ) and Abney, Collins, and Singhal (2000) . IBM X  X  Question Answering System ( Ittycheriah, Franz, Zhu, &amp; Ratnaparkhi, 2000 ) uses a
Maximum Entropy classifier trained on features of the question that include the word unigrams, bigrams, stemmed words with their parts of speech annotated, question words identified in the beginning, middle or end of the question, and an expansion of the question using the WordNet ontology ( Miller, 1995 ). Li and Roth (2002) use a hierarchical system to classify questions as one of six coarse-grained classes. In a second step, they identify fine-grained classes of which the question could potentially be a member, based on the output of the they have 50 fine-grained classes. Features include part of speech tags, noun phrases, the words in the ques-tion, sets of semantically related words, and named entities. Any given question may have several hundred active features.
Other approaches, such as Zhang and Lee (2003) and Metzler and Croft (2005) , use support vector machines to classify the TREC 10 fact-based questions into more than 30 classes. Zhang and Lee use a tree kernel, which determines similarities in the parse tree structure of questions. Metzler and Croft use surface text features in a radial basis function kernel, and demonstrate classification on the TREC 10 fact-based questions, as well as 250 fact questions from the MadSci question archive. science from users of varying ages and education levels. It is potentially more difficult to classify the questions because they contain spelling and grammar errors, or do not make sense when taken in isolation, and they often require more lengthy and detailed responses. The TREC 10 questions were taken from the MSNSearch query logs, and have been spell-corrected, and edited by the assessors whenever possible, although there are still occasional errors in the data. The questions were chosen because they required short, factual answers, typ-ically two or three tokens in length, and they are straightforward when taken in isolation. Examples of TREC 10 questions are  X  X  X ho was Monet? X  X ,  X  X  X hat is the capital of Sri Lanka? X  X ,  X  X  X ow far away is the moon? X  X 
Examples of MadSci questions are  X  X  X hich is hotter the sun or lightening? X  X ,  X  X  X hat is the h323 protocol and t30 protocol? X  X ,  X  X  X oes time go frame by frame like in a movie or is it an endless continuum? X  X . 4. Exploiting document structure
Just as the non-content words in questions may be useful, the HTML tags and structure information in documents may be of use for responding to questions. The HTML tags, and other structure information such as tables, links and lists, may provide valuable information that can be related to different types of questions. There is a persistent body of work that utilizes information about documents other than their textual content.
For example, information about the link structure of a document relates it to a database of documents as in the  X  X  X ubs and authorities X  X  idea in Kleinberg (1998) , and the PageRank algorithm (Brin et al., 1999) . Both algorithms make advantageous use of structural information to improve retrieval. Getoor, Segal, Taskar, and Koller (2001) demonstrate that exploiting the structure of scientific paper references among papers improves classification. Glover et al. (2001) identify categories of documents (research papers, personal home pages, FAQs, calls for papers, product reviews), and train an SVM for each category using features based on the text of the document, the HTML structure of the document, and the document discourse information.
They learn a set of query expansions appropriate for each category of document, and then submit the query in a meta-search strategy to produce the best ranking of documents from a variety of sources for each desired category. Others have also suggested using document genre to improve retrieval ( Crowston &amp; Williams, 1997;
Toms, Campbell, &amp; Blades, 1999 ). 5. Question classification
Many document retrieval and question answering systems are based on the assumption that questions are answerable with facts. In order to improve retrieval for procedural questions, it is a necessary first step to dis-tinguish procedural questions from other question types. In this section, we explore various techniques for classifying questions and demonstrate that standard machine learning techniques can be employed to distin-guish fact and procedural questions with at least 90% accuracy. 5.1. Fact and procedural questions
Our corpus of 4100 questions came from the query logs of the search engine, GovBot. 1000 queries to GovBot were well-formed questions, with an average length of eight words, and a total vocab-ulary of 3700 unique words pertaining to government. Variation in the grammatical structure of the sentence was reduced because every question began with a question word.
To get an initial estimate of the number of fact-oriented and procedure-oriented questions in our corpus, we classified the questions manually based on the question words. A question was deemed a  X  X  X rocedural ques-tion X  X  if it began with words such as  X  X  X ow do X  X ,  X  X  X ow can X  X ,  X  X  X ow would X  X . Questions were deemed  X  X  X act ques-tions X  X  if they began with words such as  X  X  X ho X  X ,  X  X  X hen X  X , or  X  X  X ow old (long, big, many, etc.) X  X . Questions beginning with other question words, such as  X  X  X hat X  X , were included in either the procedural or the fact set, depending on their most dominant orientation. Ambiguous questions (questions that could be considered either procedural questions or fact questions) were included in either the procedural or the fact set, depending on which set fit the question best. 10 We divided the 4100 questions into a fact-oriented set and a procedure-oriented set. According to this heuristic classification, 700 of the questions were procedure-oriented, and 3400 were fact-oriented. Examples of procedural questions posed to the GovBot search engine are  X  X  X ow do I reg-ister a trademark? X  X ,  X  X  X ow do I find an old birth certificate? X  X  and  X  X  X ow do I become a notary public? X  X . Exam-ples of fact questions posed to GovBot are  X  X  X ow many seats are in congress? X  X ,  X  X  X hat is the poverty level in Wyoming? X  X  and  X  X  X ow long can I stay in the United States without a visa? X  X .

Since GovBot was a search engine for government and military websites, the questions are not as diverse as questions to general purpose search engines. Also, there are a number of question rewrites, which would skew the results of any classifier relying on separate training and test data. To address these limitations, we created a second corpus of questions from Excite query logs. 11
The Excite queries were processed to be consistent with the way the GovBot queries were processed. From the Excite logs queries were selected that began with the words  X  X  X ho, X  X   X  X  X hat, X  X   X  X  X here, X  X   X  X  X hen, X  X  and  X  X  X ow X  X .
From this subset of queries, we eliminated any that were shorter than three words (as with the GovBot que-ries). Queries were also eliminated that were in Boolean query form (indicated by the symbol  X  X + X  X , or the word  X  X  X ND X  X ). To further reduce the number of non-grammatical queries, queries of the form  X  X  X ow to write cover letter X  X  are not considered well-formed questions, and were eliminated. In addition, song or movie titles begin-ning with question words (such as  X  X  X ow the Grinch Stole Christmas X  X ) were removed. Duplicates and ques-tion rewrites were removed, and then the questions were classified as fact or procedural by hand, in the same way as the GovBot questions described above. The resulting set had 2475 fact questions, and 480 procedural questions. Examples of fact queries from the Excite logs are  X  X  X ho writes small business loans in CT? X  X , X  X  X ho or what is Morphius? X  X  and  X  X  X ow many cardinals are there in the Roman Catholic church? X  X . Examples of procedural questions from the Excite logs are  X  X  X ow can I put a search engine for my site? X  X ,  X  X  X ow do I cap-ture video playback from a computer? X  X , and  X  X  X ow do I put together a golf tournament? X  X . 5.2. Classifying questions using Na X   X  ve Bayes and maximum entropy classifiers
In previous work, we demonstrated that questions can be classified as fact or procedural by comparing their syntax trees, and by using language models ( Murdock &amp; Croft, 2002 ). As a simple addendum to these approaches, we classified questions based on the presence of a person pronoun in the first four words. We looked for the words  X  X  X  X  X ,  X  X  X ou X  X ,  X  X  X ne X  X  or  X  X  X e X  X  in the first four words of each question. Of the original set of 3400 fact questions, 70 had a person pronoun in the first four terms. Of the original set of 720 procedure questions, 696 had a person pronoun in the first four terms. If we were to classify questions as procedural if they contained a person pronoun, we would have classified 696 questions correctly. Similarly, 3330 fact ques-tions would have been classified correctly based on the absence of a person pronoun. The total number of questions is 4120, so the classification accuracy in this case would be 97.7%.

Approaches such as these exploit the fact that this particular data has a high degree of structural predict-ability. This type of classification is not likely to generalize to questions that are more variable in their grammar. Standard classification techniques allow us to choose a more flexible feature set in determining the procedure-orientation of the questions so that we might generalize to noisier data.

We chose a Maximum Entropy classifier and a Na X   X  ve Bayes classifier, which are commonly used for text classification tasks. Na X   X  ve Bayes classification estimates the class given the data by computing the probability of the data given the class and applying Bayes Rule. A thorough discussion of Na X   X  ve Bayes classification and its applications in information retrieval is in Lewis (1998) . Maximum Entropy estimates the probability of the class given the data directly. Given a choice of distributions for the data, Maximum Entropy chooses the dis-tribution that maximizes the conditional entropy of the class given the data, subject to certain constraints. An example of using Maximum Entropy for text classification, with an explanation of Maximum Entropy clas-sification is Nigam, Lafferty, and McCallum (1999) . Appendix A gives a detailed explanation of Na X   X  ve Bayes and Maximum Entropy classifiers. We used the implementation of Na X   X  ve Bayes and Maximum Entropy in the
Mallet Toolkit ( McCallum, 2002 ). 5.3. Experiments and results
We report classification results using the Excite questions as the training set, and the GovBot questions as the testing set, and vice versa. Features considered include the text of the question tokens themselves, the first word, the first three words, and the length of the question. All questions were normalized to lower case, and trailing punctuation was removed. Various feature combinations were explored. Table 1 shows the results of question classification using various feature combinations.

Classification accuracy is computed as the number of questions correctly classified (as either procedural or fact) divided by the total number of questions. In Table 1 , the columns labeled  X  X  X est X  X  show the accuracy of the held-out testing set. The columns labeled  X  X  X rain X  X  are the results of training the classifier, and then testing the classifier on the data we trained on. The higher the number in the train column, the more the classifier fit the data, thus it is an indication of the degree of overfitting the data.
 Training on the text only gives reasonable performance when training on the more general Excite data.
However when the training and test sets are swapped, the results are poor because the classifiers fit the more specific GovBot data in training, and then are unable to generalize at test time. Adding features that do not depend on the domain of the data improves the generalization of the classifiers. Removing the text data com-pletely, and training on the length of the question does surprisingly well, and shows a reasonable generaliza-tion when training on the GovBot data and testing on the Excite data. Adding question word information improves the GovBot classification. 5.4. Discussion of question classification
We have shown that questions can be distinguished as fact-or procedure-oriented based on information independent of the complete text of the questions. Knowing that the information need implied by the question is different depending on its orientation allows us to develop techniques specific to each genre of question.
Using features that are not dependent on the domain of the questions (such as the wh-words or the length train a classifier on very general data, unrelated to the domain of the test data, and to produce a good clas-sification (better than 90% accuracy) on domain specific data. In the case that the only training data available is domain specific, a reasonably general question classifier can be trained with an accuracy of better than 80%.
Based on these results, our next step was to determine how we might use question classification to improve retrieval performance. In previous work ( Kelly, Murdock, Yuan, Croft, &amp; Belkin, 2002 ), we found evidence for the hypothesis that features of documents that are relevant to procedural questions are different from those that are relevant to fact questions. To further test this hypothesis and to explore how these differences might be exploited to improve retrieval, it was necessary for us obtain relevance assessments of documents retrieved in response to both types of questions. 6. Relevance assessments
To identify relevant and non-relevant documents for both procedural and fact questions, we recruited assessors to evaluate the relevance of documents retrieved for both types of questions. Assessors were each asked to consider six different questions and judge the relevance of a set of documents for each of these questions. 6.1. Assessors
We solicited assessors from a graduate program in library and information science because we wanted assessors who were experienced with mediated searching and who were experienced Internet users. The task of mediated searching (searching for and evaluating the relevance of documents for other people X  X  information problems) is a task commonly performed by this group of people. In addition, most of the questions used in this study were general knowledge questions similar to those that typical reference librarians are likely to receive. Thus, this assessor group is particularly well-suited for the experimental task that we had them per-form. Eighteen graduate students volunteered as assessors in this study. 6.2. Question collection
Using the 4100-item GovBot question corpus described in the preceding section as our starting point, we randomly selected a list of 120 potential questions (60 fact and 60 procedural) to use in our relevance assess-ment study. In addition to the previous manual and automatic classification of these questions as procedural or fact, we performed an additional manual classification to ensure that these questions were indeed humanly distinguishable as such, and to add to the reliability of the previous manual classification. Questions were inde-pendently classified by four people, who did not perform the first manual classification, as either fact or pro-cedural, depending upon the type of information requested by the question. The result of the manual classification was that all four people agreed on the classification of 113 of the 120 questions. We did not com-pute a Cohen X  X  Kappa statistic because we were primarily interested in using the results of this classification to identify questions where 100% agreement between human classifiers was found to use in our relevance assess-ment study. The seven questions for which there was no classification consensus were excluded. We further processed the 113 questions as described below. 6.3. Document collection
In our initial experiment ( Kelly et al., 2002 ), we built a document collection by submitting each of the 113 questions to GovBot. We followed each returned URL to ensure that all links functioned properly and were still accessible. From these efforts, we obtained the top 25 unique documents retrieved by GovBot for each question without regard to the document X  X  relevancy to the question. We further screened the list of questions for currency and document availability until we had 80 total questions, 40 procedural and 40 fact, which we then used in our relevance assessment study. To prepare for the present study, we had to repeat the screening process because a significant amount of time had passed since we last used the collection. This second screen-ing process resulted in the elimination of a small number of documents for some questions, such that each question had 20 X 25 documents associated with it. 6.4. Assessment task
Assessors were asked to judge the relevance of documents retrieved for three fact and three procedural questions. Each assessor was asked to evaluate 20 X 25 documents for each of the six different questions. In total, each assessor evaluated 120 X 150 documents. The presentation of questions alternated between proce-dural and fact, or fact and procedural, depending upon the assessor number. We designed the study such that odd numbered assessors began the evaluation with a procedural query, while even numbered assessors began with a fact query. The order in which documents were presented for evaluation for each question was random.
The study was administered online and assessors were allowed to complete the study at a location of their choice. Although assessors were not required to complete document assessments for all six questions during one session, they were required to complete assessments for any single question during a single session. Asses-sors were expected to complete their assessments within one week. The interface used to make relevance assessments is displayed in Fig. 1 . This interface presented assessors with the question for which they were assessing documents, the content of a single document and four choices for relevance: relevant, partially rel-evant, not relevant, and unsure. Definitions for each type of relevance were provided to assessors as part of the experimental instructions which they were shown each time they logged in to assess documents. These defini-tions were as follows: (1) Relevant : the information on this page (not a linked page) satisfies the query; (2) Par-tially Relevant : the information found on this page satisfies the query only in part; (3) Not Relevant : the information on this page does not satisfy the query; and (4) Unsure : unable to determine the relevance based on the information on this page.
 After assessors submitted their relevance assessment of a document, the next document was displayed.
Assessors were unable to return to the document or to change their relevance assessment once they clicked the submit button. If an error was encountered on the page, assessors were instructed to click the unsure but-ton, and continue with the evaluations. Upon completing assessments of all documents for a single question, assessors had an opportunity to either continue with the study or to end the study session.
 6.5. Results
The results that we present in this section are based on evaluations of 80 questions (40 fact and 40 proce-dural) made by 18 assessors. Because we were only able to solicit 18 volunteer assessors in this study, the num-ber of document assessments made for each question differed. Each of the 18 assessors evaluated documents for six questions, which resulted in a total of 108 unique question evaluations. However, since there were 80 total questions in the pool, this number of assessors was not quite enough to result in two independent doc-ument assessments for each of the 80 questions. Instead, for 52 questions, document assessments were only made by a single assessor. This number was equally divided among procedural and fact questions.

In cases where more than one assessor evaluated the relevance of the same document, we re-classified the relevance of each document based on inter-assessor agreement. We used a contingency table for doing this which displayed in Table 2 . We classified documents as relevant if two assessors marked the document as rel-evant, or in the case of a single assessor, if that assessor marked the document as relevant. We classified doc-uments as non-relevant if two assessors marked the document as non-relevant, or in the case of a single assessor, if that assessor marked the document as non-relevant. A document that was rated as relevant by one assessor and non-relevant by another was deemed to have ambiguous relevance and was excluded from analysis. Documents scored as unsure by one or more assessors were excluded in the analysis. A document scored as partially relevant by one assessor and relevant by another was classified as partially relevant.
Fig. 2 shows the frequency and distribution of relevance for each question type. There were no significant differences in the number of relevant documents for each question type. Further, the distributions of relevance across the three categories of relevant, partially relevant and non-relevant did not differ significantly either.
In order to analyze the features of relevant and non-relevant documents for procedural and fact questions, which we describe in the next section, we needed to further classify the documents so that only three classes of documents existed: relevant and non-relevant, and those excluded from analysis. Thus, the classification pre-sented in Table 3 was modified slightly to achieve this goal. This modification primarily concerned those doc-uments classified as partially relevant in Table 3 . Documents judged partially relevant by one assessor, or partially relevant by one assessor and relevant by a second assessor were included in the relevant set. Other-wise, partially relevant documents were classified as non-relevant. Documents judged non-relevant by one assessor and  X  X  X nsure X  X  by a second assessor were included in the non-relevant set. All other documents with at least one  X  X  X nsure X  X  were excluded. Documents judged relevant by one assessor and non-relevant by another assessor were excluded. In total, there were 328 documents relevant to procedural questions, 304 relevant to fact questions, 584 non-relevant to procedural questions and 605 non-relevant to fact questions. 7. Document features
Documents are most often retrieved based on the similarity between the content of the query and the con-tent of the document. It is possible certain genres of document or specific structures within a document are important evidence of the document X  X  relevance. For example, one can imagine that data presented in tabular form might be more appropriate in answer to a fact question than a procedural question. In this section, we present an examination of 24 features of document format and structure. We based feature selection on those features that are easily identifiable in HTML documents, and that are not dependent on the similarity between the question and the document. The distributions of these features in procedure-relevant and fact-relevant documents are compared. We identify seven features that distinguish fact-relevant from procedure-relevant documents. Within the procedural class, we found seven features that distinguish relevant from non-relevant documents. Within the fact class, there was only one feature that distinguished relevant from non-relevant documents. Comparing the complete set of procedural documents and the complete set of fact documents, we identify eleven distinguishing features. 7.1. Features of documents
Some candidate features indicated structures within a document such as tables, contact information, lists, and forms. These are easily identifiable by their HTML markup. In the case of lists, we can identify list tags these tags such as  X  X  X ype X  X  =  X  X  X ext X  X . Contact information is identifiable with street addresses and zip codes, which can be matched with regular expressions, and words such as  X  X  X ontact us X  X  or  X  X  X ontact information X  X .
Other features were properties of the document itself, such as the document is an FAQ, or a news group posting, or a press release. Newsgroups can be identified by terms such as  X  X  X ubject: X  X ,  X  X  X ate: X  X ,  X  X  X ewsgroup: X  X  ument. FAQ pages were identified as having  X  X  X AQ X  X  or  X  X  X requently Asked Questions X  X  in the title or url of a document. Pages that had three or more links whose text label is a question, or pages that had three or more instances of emphasized text containing questions were counted as FAQ pages. Emphasized text includes text that was boldface, italicized, or in a heading. Pages were also counted as FAQ pages if they had three or more sentences that started with  X  X  X : X  X  followed by a question. Finally, a page was counted as an FAQ if it had mul-tiple questions in the text. We set the number of questions in the text indicative of an FAQ at five, from our observations of FAQ pages.

Finally, there were features that counted various elements in the document, such as the number of widgets (radio buttons, checkboxes), list elements, links to files, or images. These can all be identified with their HTML tags.

HTML tables are frequently used for formatting, rather than as a tabular presentation of data. Algorithms for detecting and extracting information from tables (as in Pinto, McCallum, Wei, &amp; Croft, 2003 ) use features such as the number of rows and columns, or the presence of table headers and captions, to determine whether a table in an HTML document is a table of data, or a table used to format the page. Rather than creating a rule-based table extractor, we computed features that might be indicative of data tables, and incorporated these as features of our document. Table 3 shows a list of features of HTML tables. Text tables are relatively easy to detect based on the presence of text-based column and row delimiters. We looked for columns and rows delim-ited with pipes ( X  X  X  X  X  characters), dashes and underscores. We also looked for columns delimited with repeated tabs, and for left-justified or right-justified columns. These documents are pre-formatted and included  X  X  X s is X  X  in the HTML, or they are not HTML documents. In either case the white space used to delimit columns and rows could be computed reliably. See Appendix B for the entire set of features which were tested. 7.2. Results
Features were computed for fact-relevant documents, procedure-relevant documents, non-relevant fact documents and non-relevant procedural documents. Some of the features (such as features that detect prop-erties of the document itself) are binary features. Some features (such as features of HTML tables, or features that are counted) are continuous. We compared the differences in the means for each feature, using the feature value (binary or continuous) as it was computed. The distributions of non-binary features were compared between two classes, using a two-tailed T -test.

Table 4 shows features that differed significantly when comparing the fact-relevant set to the procedure-relevant set. The features  X  X  X AQ X  X  and  X  X  X owto in URL X  X  are binary features, and thus a T -test is not an appro-priate measure of the significance of the difference. FAQs are more common in procedure-relevant documents, as are words indicating  X  X  X ow to X  X  or  X  X  X ow do I X  X  in the URL.

The average number of links per table was not statistically significant. While this feature may not be a strong indicator of procedure-orientedness, it may help identify data tables, and the presence of data tables may be significant. We observed that links appear more often in formatting tables than in tabular data.
Table 5 shows features whose distributions differ significantly between the relevant set and the non-relevant set of documents for procedural questions. Four other features were different, but not statistically significant: the ratio of text in a table to text not in a table, the average number of links per table, the number of phone or fax numbers, and the number of forms. The number of widgets and the number of forms both attempt to iden-tify form elements in a document that are actual forms, and not site search buttons, or other formatting devices. As widgets are a significant feature, the form feature may also be useful in identifying procedural documents.
One feature X  X he average length of a list X  X istinguishes the relevant fact documents from non-relevant fact documents, but the difference between the means is not significant. And as a feature, average list length seems an improbable choice for providing evidence of relevance. We identified no other structural features to distin-guish relevance within the class of fact documents.

Comparing the entire set of fact documents (relevant and non-relevant) to the entire set of procedural doc-uments shows that the documents returned by the retrieval engine as relevant for each class are distinguishable by nine features, as shown in Table 6 . Two additional features (widgets, and average list length) are not sta-tistically significant. 7.3. Summary
The number of features that distinguish fact from procedural documents and relevant from non-relevant documents within the class of procedural documents suggests that there are interesting and significant differ-ences between the structure of fact and procedure documents. In the next section we show that we can exploit these differences to improve retrieval for procedural questions. We use the features found to be significant in a clustering system, and then use the clustering results to re-rank the documents. 8. Improving retrieval
Many retrieval approaches are content-based in the sense that documents with words or phrases matching those in the query are ranked higher. Processing questions in this manner may not be as effective for proce-dural questions as for fact questions. Our approach is to cluster documents based on their structural similarity, and re-rank the documents based on the results of clustering. We show that this approach benefits procedural questions, giving ranked lists that are comparable to the results for fact questions. We explored different com-binations of features, and different distance metrics. We present the results of clustering the documents based on the existence of a feature within a range of values for that feature, as well as clustering via the standard
K -Means clustering algorithm. We find that K -Means improves retrieval, and that the best distance metric in this context is Euclidean distance. 8.1. Evaluation
The metrics used to evaluate the retrieval are precision at rank one, mean reciprocal rank at rank five, mean reciprocal rank overall, and mean average precision. Precision at rank one evaluates the number of questions for which documents at rank one are relevant. It is commonly used to evaluate Question Answering systems, and in High Accuracy Retrieval of Documents (which attempts to get a relevant document at rank one for every query). The mean reciprocal rank (MRR) is the inverse of the rank of the first relevant document. Thus if the relevant documents occur in a ranked list at ranks 3 X 5, the MRR score for this list would be 1/3. We used two versions of MRR. The first considers the rank of the first relevant document in the top five docu-ments returned for a query. In this case, if the first relevant document is returned at rank 6, the score for that query is zero. The second version considers the MRR score over the entire ranked list. Thus if the first relevant document is at rank 6, the score for that query is 1/6. The final metric, mean average precision, attempts to evaluate the overall quality of the ranked list. It is the average precision for each question, averaged over all questions. We consider only the top 25 documents for each query, and we compare the re-ranking of these documents to their original ranking. 8.2. Experiments and results The original (baseline) results were obtained by posing the questions from the GovBot logs (described in
Section 7.2 ) to a database of government and military websites indexed by the GovBot search engine. The doc-uments from this original retrieval are the same documents described in Section 7.3 .

The GovBot search engine used Inquery ( Callan, Croft, &amp; Harding, 1992 ), a document retrieval system developed at the University of Massachusetts. Inquery first attempts to retrieve documents using a proximity window that preserves the order of stopped and stemmed query terms. If that fails, Inquery generates a list of
WordNet synonyms ( Miller, 1995 ), and uses that inside the ordered proximity window. If that fails to find relevant documents, then the list of synonyms is put in a proximity window, ignoring the order of the terms.
This will rarely fail to find documents because an unordered window of query terms requires that the docu-ment contain only one of the terms in the window, and the query has been further broadened to include syn-onyms of the original terms. Our baseline is the original ranked list generated by the Inquery system. The baseline results for procedural questions are lower than the baseline results for fact questions, as shown in 7, suggesting that content alone is not as effective for procedural questions. 8.2.1. Existential clustering
The structural cues in the documents are not definitive. For example, although it is the case that FAQs appear more often in procedural documents, it is not the case that all procedural documents have FAQs, or that no fact documents have FAQs. The structural features, as the textual content, provide evidence of the  X  X  X oodness X  X  of the document. Any document exhibiting any one of the features indicated for procedural documents may be a better document. We cannot say anything about documents that have an absence of one or more features. Our first approach ranks documents higher that have at least one of the features within a designated range of values for that feature. We call this approach  X  X  X xistential Clustering X  X .

For each feature we compute the mean and standard deviation across the set of procedural documents, and the set of fact documents. We construct a range of values for each feature that is one standard deviation above and below the mean, rounded to the nearest integer greater than zero. No feature value of zero was included in the range. Means and standard deviations were computed from non-zero feature values. For the procedural documents, if the value of any one of the features was within the range for that feature for procedural ques-tions, the document was added to the  X  X  X rocedural X  X  cluster. Analogously, if a fact document was in the des-ignated range for fact documents for any feature, the document was added to the  X  X  X act X  X  cluster. Since the questions have been classified, we consider only the cluster for the question type. Thus, for each document retrieved for a procedural question, it is either included in the procedure cluster or not. Documents are only added to the fact cluster if they were returned in response to a fact question, in which case no procedure cluster is computed. The original results were re-ranked by floating documents to the top of the ranked list that appeared in the  X  X  X rocedural X  X  (or  X  X  X act X  X ) cluster if the document was returned for a procedural (or fact) question.

Table 7 shows the results of this ranking method in the row labeled  X  X  X xistential 1 X  X , using the set of features shown to be significant in distinguishing the set of procedural documents from the set of fact documents (shown in Table 6 ). Row two of Table 7 ( X  X  X xistential 2 X  X ) shows the results of the same approach, using features shown to distinguish procedure-relevant documents and fact-relevant documents (as in Table 4 ).

It seems reasonable that documents exhibiting more than one feature in the designated range would be bet-ter documents. To test this, we clustered the documents as described above, and then ranked the documents according to their presence in the cluster and the count of positive feature-matches. We used the set of features that distinguish procedural documents from fact documents. The result of this is in Table 7 , in the row labeled  X  X  X xistential 3 X  X .

In examining the data, we noticed that more procedural documents than fact documents were FAQs. Rank-ing documents higher that were FAQs showed a significant improvement in the precision at rank one of the procedural documents. The results are shown in the last row of Table 7 . 8.2.2. Ranking by distance from the mean feature vector
The features computed for each document form a vector. We can measure the distance of this vector from the vector of mean values of the features, using a variety of vector similarity and distance metrics. We ranked the results according to their distance from the mean vector using Euclidean distance and Chebyshev distance. Eqs. (1) and (2) show Euclidean and Chebyshev distance.

Euclidean distance
Chebyshev distance
Euclidean distance takes into account the entire vector of features. Any feature that is found in the document is counted in computing the distance from the mean. Chebyshev distance has the property that it only counts the distance from the mean of the most distant feature. Intuitively, a document is only as close as its farthest feature. We would like to say a document is as close to the mean as its closest feature, since not all features are present in every document. We used a variation of Chebyshev distance which takes the minimum distance between any two features, rather than the maximum. We call this distance metric  X  X  X nverse Chebyshev Dis-tance X  X . Table 8 shows the results for ranking the documents according to their distance from the vector of mean values for each feature. The result for Euclidean distance, precision at rank one for procedural questions is statistically significant at the .05 level, using a T -test. 8.2.3. Ranking by distance from procedural/fact centroids
Since we can effectively classify questions as fact or procedural, the documents come to us pre-clustered in a sense. They have already been assigned to a procedural cluster or a fact cluster by being included in the ranked list for those questions. We hoped that finding documents that have structural features typical of procedural questions, and presenting those at the front of the ranked list would improve retrieval. We computed the fact and procedural cluster centroids by taking the mean of the feature set over the entire set of procedural doc-ument (or fact document) returns. Documents are assigned to the cluster whose centroid is closest. Within each cluster we ranked the documents by their distance from the centroid, and then set a threshold. Docu-ments within the cluster threshold were presented at the top of the ranked list, retaining their original order.
We investigated a variety of distance metrics, but found that Euclidean distance and Chebyshev distance per-formed the best. The results are shown in Table 9 . The precision at rank one for Inverse Chebyshev distance is significant for procedural questions at the .001 level, using a T -Test.

Clustering documents in this way assumes that there are two clusters of documents, whose centroids are the means of all documents returned for procedural questions (or fact questions). To find clusters of relevant and non-relevant documents in an unsupervised way, we used the standard K -Means clustering algorithm with
K = 4. We initialized the centroids of the two relevant clusters to be the means of the features computed over the top 50% of documents. The two non-relevant centroids were initialized to the means of the feature values for the bottom 50% of documents in the ranked lists. Documents in the two relevant clusters were presented in their respective ranked lists before the rest of the documents. This unsupervised clustering, using Euclidean distance proved to be the most effective. The results are shown in Table 10 . The results for precision at rank one for procedural questions is significant at the .001 level, using a T -Test.
 We tried a variety of distance metrics, including cosine similarity, Euclidean distance, Minkowski distance, Chebyshev distance, inverse Chebyshev distance, Czekanowski coefficient, city-block distance and found that
Euclidean distance and Chebyshev distance were consistently the best. We tried various settings for  X  X  K  X  X  (two clusters, three clusters, four clusters and five clusters) and found that four clusters produced the best results.
We also tried both supervised and unsupervised clustering. In supervised clustering, we created training and test sets, for five different splits of the data. The centroids were computed from the portion of the training set that was judged relevant. The supervised version produced a significantly worse result than the original ranked list. We believe this is because reducing the amount of data used to estimate the centroids for already sparse feature vectors produced biased estimates of the centroids. 8.3. Discussion
We are re-ranking documents whose content similarity has already been determined by the initial retrieval algorithm. Keeping the content-based ranking the same while we float documents with structural features typ-ical of procedural documents to the top of the ranked list allows us to utilize the structural features without placing too much weight on them. A separate set of experiments which ranked the documents according to their structural similarity to the mean for all procedural documents, without regard to the original ranking, did not produce good results. Documents relevant to procedural questions are similar in content as well as having higher incidence of particular structures.

All but one of the results presented for the precision at rank one were statistically significant. With only 40 procedural questions and 40 fact questions, the differences in results have to be fairly drastic to be statistically significant. It is possible that with larger samples, the results for the other metrics would also be statistically significant.
 Structures in an HTML document are not straightforward to identify as not all HTML is well-formed, and
HTML is fairly flexible in allowing alternative ways to express the same structures. In addition, although the presence or absence of a feature shows trends over large numbers of documents, for an individual document the absence of a feature is not indicative of its procedure-orientedness. Thus we can only consider the presence of features.

Many tasks such as High Accuracy Retrieval of Documents (HARD; Allan, 2003 ), Question Answering, and Summarization rely on high quality results at the top of the ranked list. In addition there is some evidence that users do not look far down the ranked list. While the overall quality of the ranked list is important, we focus on the quality of the top of the ranked list. Our results for the documents at rank one showed significant improvements over the baseline retrieval for procedural questions. It is also worth noting that results for pro-cedural questions on the other evaluation measures, although not statistically significant, were consistently better than the baseline for all of the techniques. Finally, of all of the techniques we tested, the best and most consistent improvement over baseline retrieval was achieved by using K -Means clustering with Euclidean distance.
 9. Conclusions
In daily life, people are very comfortable seeking information from other people using questions. As users have become more search X  X avvy they have adjusted their style of requesting information to the form most often required by search engines: keyword queries. When a user asks a well-formed question, we can analyze the question in a variety of linguistic ways. For our purpose of determining questions asking for a process, lexical information based on the first three question words is sufficient. With this information, we can classify questions as asking about a process or asking for a fact with very high accuracy (above 90%).

The results for procedural questions after the initial retrieval step were lower than for the fact questions. It is possible that processing designed to improve retrieval based on the assumption that questions are answer-able with facts is not as effective for questions answerable with a procedure description. That the documents relevant to procedural questions differ from both documents relevant to fact questions and non-relevant doc-uments is evident on examining structural features.

Once we have established that a question is asking for a description of a process, we can use structural char-acteristics of the documents to improve the ranked list returned for those questions. Although structure alone is not sufficient to produce a high quality ranked list, using structural features in conjunction with the content-based initial retrieval allows us to significantly improve the retrieval at the top of the ranked list. One tech-nique for improving the quality of the ranked list is to use K -Means clustering to find the cluster of relevant procedural documents (based on their structural similarity), and then to re-order the ranked list so as to float the procedure-relevant cluster to the top of the list, while retaining the original retrieval order within the pro-cedure-relevant cluster. We found that with this approach, Euclidean distance was most effective with four clusters (corresponding to procedure-relevant, procedure-non-relevant, fact-relevant, fact-non-relevant).
It remains to be seen which types of information need are better served with questions. We have presented one type of information need (description of a process) for which the question information was useful. We examined features of the HTML, but there remain many other non-content-based features that may also be indicative of procedural documents. We leave to future work using classification techniques to separate the relatively small class of relevant documents from the universe of non-relevant documents. It appears to be a simple two-class classification problem, but on closer inspection is a much more subtle issue, as it is not possible to make generalizations about the class of non-relevant documents. The structural features indi-cating that a document was an FAQ were useful to retrieval, and this suggests that identifying different genres of documents, and the questions they are most appropriate for, would be an interesting direction for future investigation, as would identifying and improving retrieval for other types of questions.
 Acknowledgements
This work was supported in part by the Center for Intelligent Information Retrieval, in part by NSF Grant #IIS-9907018, and in part by SPAWARSYSCEN-SD Grant number N66001-02-1-8903. Any opinions, find-ings and conclusions or recommendations expressed in this material are the author(s) and do not necessarily reflect those of the sponsor.
 Appendix A. Na X   X  ve Bayes classification
Na X   X  ve Bayes classifiers are based on Bayes Rule. Let h be the hypothesis that an example of data is a mem-ber of class, and let Q be the data. In the case of question classification, one example of data would be one question, and the possible classes would be  X  X  X rocedural X  X  or  X  X  X act X  X . By Bayes Rule,
The Na X   X  ve Bayes classifier seeks the hypothesis h from the set of all possible hypotheses H with the maximum a posteriori probability
We can ignore the prior probability of the question, P ( Q ), because it is a constant, and independent of the hypothesis, h
In our case, the questions are represented by vectors of features. We assign a class to each question that maxi-mizes the likelihood of the features given the class. Thus, the maximum likelihood hypothesis is where c is a class and f is a feature. From training data, estimating the prior probability of a class is simply a matter of counting the number of procedural (or fact) questions divided by the total number of questions. To estimate the conditional probability of the features given the class, we assume that the features are condition-ally independent given the class, (the conditional independence assumption is what makes this a  X  X  X a X   X  ve X  X  Bayes classifier). Thus, gives us this classifier
After the probabilities have been computed in data for which the class is known (the training set), those same probabilities are used to estimate previously unseen examples (the testing set). The class that maximizes this likelihood is the class assigned to the data. An excellent introduction to Bayesian decision theory is given in ( Duda, Hart, &amp; Stork, 2001 ).

Maximum entropy classification . Whereas Na X   X  ve Bayes classification estimates the class given the data by computing the probability of the data given the class and applying Bayes rule, maximum entropy estimates the probability of the class given the data directly. Given a choice of distributions for the data, maximum entropy chooses the distribution that maximizes the conditional entropy of the class given the data, sub-ject to certain constraints. Let the empirical joint distribution of ( x , y ) be the frequency of the co-occurrence of ( x , y ) in the training set, where x is the data and y is the class label. Then we maximize the conditional entropy subject to the following constraints:
For each binary feature f ( x , y ) the expected value of f under the model should equal the expected value of f under the empirical joint distribution The sum of the conditional probabilities of each class equals 1
For each feature function in the first constraint, and for the second constraint we introduce Lagrangian multipliers
By taking the derivative with respect to P ( y | x , k ), and solving it, we can show that the maximum entropy dis-tribution is given by where Z is a normalizing constant We assign the class label that maximizes P ( y | x , k ).
 Appendix B
The 24 features of documents examined: 1. The URL contains the terms  X  X  X owdoi X  X   X  X  X owto X  X   X  X  X nfo X  X . 2. Number of examples of contact information in the document. 3. Number of phone/fax numbers in the document. 4. Number of links in the document. 5. The document is not formatted with HTML. 6. Number of links to files. 7. Number of images. 8. Does the document contain a form. 9. Number of widgets. 10. Number of lists. 11. Average list length. 12. Is the document an FAQ. 13. Is the URL an ftp address. 14. Is the document a press release. 15. Is the document a news group posting. 16. Number of text tables. 17. Number of HTML tables. 18. Average number of rows in tables. 19. Average number of columns in tables. 20. Average number of text characters per table cell. 21. Average number of links per table. 22. Average number of images per table. 23. Ratio of text in a table to text not in a table. 24. Number of table labels (such as  X  X  Table 3  X  X ).
 References
