 Kazuyoshi Yoshii k.yoshii@aist.go.jp Ryota Tomioka tomioka@mist.i.u-tokyo.ac.jp Daichi Mochihashi daichi@ism.ac.jp Masataka Goto m.goto@aist.go.jp Matrix factorization (MF) has recently been an active research topic in the field of machine learning. Given amatrix X  X  R M  X  N as observed data, the objective is to find a low-rank approximation X  X  AB where A  X  R M  X  K , B  X  R K  X  N ,and K min { M,N } .This problem often arises in many application fields. Can-didates of X include a user-item rating matrix in col-laborative filtering ( Salakhutdinov &amp; Mnih , 2008 ), a set of face images in image processing ( Lee &amp; Seung , 2000 ), and a time-frequency spectrogram in audio pro-cessing ( Smaragdis &amp; Brown , 2003 ). Many variants of MF have been proposed by using various measures on the reconstruction error D ( X | AB ) and imposing con-straints on A and B . In terms of probabilistic model-ing, a specific model is defined by a likelihood function p ( X | A , B ) and prior distributions p ( A )and p ( B ). One of the popular classes of MF is nonnegative matrix factorization (NMF), in which all elements of A and B must be no less than zero. This constraint reflects the fact that some physical quantities, e.g. , pixel bright-ness and signal energy, cannot be negative. A typical way to impose this constraint is to place element-wise gamma priors on A and B ( Cemgil , 2009 ). D ( X | AB ) has often been defined in an element-wise manner by using the Bregman divergence ( Bregman , 1967 ), which includes as special cases the Kullback-Leibler (KL) di-vergence ( Kullback &amp; Leibler , 1951 ) and the Itakura-Saito (IS) divergence ( Itakura &amp; Saito , 1968 ). The as-sumption underlying the likelihood p ( X | A , B )isthat each element of X is independently Poisson distributed in KL-NMF or independently exponentially distributed in IS-NMF.
 In audio analysis, IS-NMF is more suitable for decom-posing a power spectrogram X over M frequency bins and N frames as the product of sound-source power spectra ( K columns of A ) and the corresponding tem-poral activations ( K rows of B )( F  X  evotte et al. , 2009 ). IS-NMF is theoretically justified if the frequency bins of source spectra are independent. Note that the au-dio signals of pitched instruments have clear periodici-ties, i.e. , those signals are highly autocorrelated at cer-tain time lags. However, the short-term Fourier trans-form (STFT) is unable to perfectly decorrelate the fre-quency components forming harmonic structures. A similar problem arises in electroencephalogram (EEG) analysis ( Lee et al. , 2006 ), in which cross-correlations between multichannel signals recorded at different po-sitions of the head are usually ignored. This indicates that it is not appropriate to place gamma priors on A and define D ( X | AB )inan element-wise manner. To solve this problem, we propose a new class of tensor factorization called positive semidefinite tensor factor-ization (PSDTF). As NMF decomposes N nonnegative vectors (a matrix) as the conic sums of K nonnegative vectors, PSDTF decomposes N PSD matrices (a ten-sor) as the conic sums of K PSD matrices. As shown in Figure 1 , each nonnegative vector is embeded into a PSD matrix that represents the covariance structure of the multivariate elements. We thus place matrix-wise Wishart priors on the basis matrices. In this paper the reconstruction error is defined by using a kind of the Bregman matrix divergence called the log-determinant (LD) divergence ( Kulis et al. , 2009 ), also in a matrix-wise manner. This implies that each slice of the ob-served tensor is assumed to have a Wishart likelihood. Since the resulting LD-PSDTF is a natural extension of IS-NMF, an inherited problem is that the number of bases, K , should be given in advance.
 To estimate an appropriate number of basis matri-ces, we propose a nonparametric Bayesian model of LD-PSDTF similar to one of IS-NMF ( Hoffman et al. , 2010 ). Although the Wishart prior-Wishart likelihood hierarchy does not satisfy the conjugacy condition, we can derive an elegant variational algorithm for closed-form posterior inference. A multiplicative update rule can also be used for maximum-likelihood estimation. In addition, we reveal that IS-NMF in the frequency domain approximates LD-PSDTF in the time domain that can consider periodic covariance structures of the audio signals. This explains why IS-NMF works well for music power-spectrogram decomposition. We propose a probabilistic model of positive semidefi-nite tensor factorization (PSDTF) and derive its non-parametric Bayesian extension that in theory allows observed data ( e.g. , a music signal) to contain an in-finite number of latent bases ( e.g. , sound sources) by using the gamma process. An effective number of bases required for representing the observed data can be ef-ficiently estimated in a data-driven manner. We then discuss how PSDTF is related to matrix factorization and how it is applied to music signal analysis. 2.1. Problem Specification We will formalize the problem. Suppose we have as ob-served data a three-mode tensor X =[ X 1 ,  X  X  X  , X N ]  X  symmetric positive semidefinite (PSD) matrix. Al-though PSDTF can be defined even if X n  X  C M  X  M is a complex Hermitian PSD matrix, we focus on the case of X n  X  R M  X  M for explanatory simplicity. The goal of factorization is to approximate each PSD matrix X n by a convex combination of PSD matrices { V k } K k =1 ( K bases) as follows: where  X  k  X  0isa global weight shared over all N slices and h kn  X  0isa local weight specific to the n -th slice. Eq. ( 1 ) can also be represented as X  X  K k =1  X  k h k  X  V k  X  Y , where  X  indicates the Kronecker product. To evaluate the reconstruction error between PSD ma-trices X n and Y n ,weproposetouseaBregmanma-trix divergence ( Bregman , 1967 )definedasfollows: =  X  ( X n )  X   X  ( Y n )  X  tr  X   X  ( Y n ) T ( X n  X  Y n ) , (2) where  X  is a strictly convex matrix function. In this paper we focus on the log-determinant (LD) divergence (  X  ( Z )=  X  log | Z | )( Kulis et al. , 2009 )givenby This divergence is always nonnegative and is zero if and only if X n = Y n holds. A well-known special case when M = 1 is the Itakura-Saito (IS) divergence over nonnegative numbers ( Itakura &amp; Saito , 1968 )givenby D in signal processing. Sivalingam et al. ( 2010 )formal-ized a similar tensor factorization problem that uses D Our goal here is to estimate unknown variables  X  = [  X  ,  X  X  X  , X  K ] T  X  R K , H =[ h 1 ,  X  X  X  , h K ]  X  R N  X  K ,and V =[ V 1 ,  X  X  X  , V K ]  X  R M  X  M  X  K such that the cost function C ( X | Y )= n D LD ( X n | Y n ) is minimized. Note that our model imposes the nonnegativity con-straint on  X  and H and the positive semidefiniteness constraint on V . We call this model LD-PSDTF. 2.2. Probabilistic Formulation We explain Bayesian treatment of LD-PSDTF defined by Eq. ( 1 ) in terms of probabilistic modeling. 2.2.1. Formulating a Probabilistic Model We first formulate a finite model based on a fixed num-ber of bases by specifying prior distributions on  X  , H , and V and a likelihood function of X .Inthismodel we assume  X  k = 1 because the effect of  X  k can be com-pensated by adjusting the scale of h k .
 Since h kn is nonnegative ( h kn  X  0) and V k is PSD ( V k  X  0 ), a natural choice is to place gamma and Wishart priors on h kn and V k as follows: where a 0 and b 0 are the shape and rate parameters of the gamma distribution and  X  0 and V 0 are the degree of freedom (DOF) and scale matrix of the Wishart distribution.
 We then assume PSD matrices {  X  X n } N n =1 to be inde-pendently Wishart distributed as follows: where  X  is a DOF of the Wishart distribution. Note that E [ X n ]= Y n and M [ X n ]=  X   X  M  X  1  X  Y n ,where means the mode. When  X  M , M [ X n ]  X  Y n holds. When  X &lt;M , X n is rank deficient. If M =  X  =1, the distribution reduces to an exponential distribution. The log-likelihood of X n is given by log p ( X n | Y n )= C (  X  )+ where C (  X  ) is a constant term depending only on  X  and thesecondtermcanalsobeconsideredtobeconstant because X n is the observed data. Therefore, the max-imization of the likelihood p ( X | Y )= n p ( X n | Y n ) with respect to Y is equivalent to the minimization of the cost function C ( X | Y )= n D LD ( X n | Y n )(com-pare Eq. ( 7 ) with Eq. ( 3 )).
 Consequently, a Bayesian model of LD-PSDTF is de-fined by Eqs. ( 4 ), ( 5 ), and ( 6 ). Given the data X ,our goal is to calculate a posterior distribution p ( H , V | X over unknown variables H and V . 2.2.2. Taking the Infinite Limit To overcome the limitation that the number of bases K should be specified in advance, we leverage Bayesian nonparametrics for taking the infinite limit of Eq. ( 6 ) as K diverges to infinity. Given the data X ,an effec-tive number of bases should be estimated in a data-driven manner. We thus aim to learn a sparse infinite-dimensional weight vector  X  =[  X  1 ,  X  X  X  , X   X  ] T as pro-posed by Hoffman et al. ( 2010 ).
 We place a gamma process (GaP) prior on  X  in a so-called weak-approximation manner as follows: where  X  and c are positive numbers, E prior [  X  k ]= c/K , and E prior [ k  X  k ]= c . When the truncation level K goes to infinity, the vector  X  approximates an infinite-dimensional discrete measure G that is stochastically drawn from the GaP over a space  X  as follows: where  X  is called a concentration parameter and G 0 a base measure. In our model we assumed that G 0 is a uniform measure such that G 0 (  X  )= c .The effective number of elements, K + , such that  X  k &gt; for some number &gt; 0 is almost surely finite. If we set K to be sufficiently larger than  X  , only a few of the K elements of  X  will be substantially greater than zero.
 A nonparametric Bayesian model of GaP-LD-PSDTF is defined by Eqs. ( 4 ), ( 5 ), ( 6 ), and ( 8 ) with a large truncation level K . Given the data X , our goal is to calculate a posterior distribution p (  X  , H , V | X )and estimate the value of K + at the same time. 2.3.  X  X ugmented X  Matrix Factorization We show here that LD-PSDTF naturally emerges from the standard problem of matrix factorization. Suppose as observed data, where  X  x n  X  R M is a feature vector of the n -th sample. Although the case of  X  x n  X  C M can be dealt with, as in Section 2.1 we here discuss the case of  X  x n  X  R M . In signal processing, for example, a local signal s n  X  R M in the n -th short window (called aframe)isoftenregardedas  X  x n (Figure 2 ). Alterna-tively,  X  x n can be a complex spectrum c n = Fs n  X  C M at the n -th frame, where F  X  C M  X  M is the unitary discrete Fourier transform matrix.
 The goal is to discover a limited number of bases hav-ing characteristic structures ( e.g. , instrument sounds) from the observed data  X  X ( e.g. , music audio signal), i.e. , to decompose each sample  X  x n into a linear sum of K variable bases {  X  w kn } K k =1 as follows: where  X   X  k is a global coefficient of the k -th basis,  X  local coefficient of the k -th basis, and  X  x nk =  X   X  k  X  is the k -th component in  X  x n . Those variables are al-lowed to take any real values. If we assume that  X  w kn is a fixed basis such that  X  w kn is equal to  X  w k for any n and define some symbols as  X   X  =[  X   X  1 ,  X  X  X  ,  X   X  K ] T  X  R K [  X  w 1 ,  X  X  X  ,  X  w k ]  X  R M  X  K ,and  X  H =[  X  h 1 ,  X  X  X  ,  X  h R N  X  K ,Eq.( 10 ) can be simply written as follows: where diag( z ) means a diagonal matrix having a vec-tor z as its diagonal elements. If diag(  X   X  )isanidentity matrix (  X   X  = 1 ), Eq. ( 11 ) reduces to the standard prob-lem of matrix factorization given by  X  X =  X  W  X  H optimal values of  X   X  ,  X  W ,and  X  H depend on what kinds of constraints are placed on those variables. 2.3.1. Formulating a Probabilistic Model We aim to formulate a Bayesian model of Eq. ( 10 ). A key feature is to consider essential correlations between M elements of basis  X  w kn . A natural choice is to put a multivariate Gaussian prior on  X  w kn as follows: where  X  V k  X  R M  X  M is a full covariance matrix. In general, the Gaussian mean is set to a zero vector. For example, an audio signal is recorded as real numbers distributed on both sides of zero (see Figure 2 ). lead to a likelihood of  X  x nk as follows: Then, using the reproducing property of the Gaussian and the linear relationship  X  x n = K k =1  X  x nk ,wegeta likelihood of  X  x n as follows: If we assume that  X  k =  X   X  2 k  X  0, h kn =  X  h 2 kn  X  0,  X  V k  X  0 ,and X n =  X  x n  X  x Eq. ( 7 )when  X  = 1 except for constant terms. We can put the same priors as Eqs. ( 4 ), ( 5 ), and ( 8 ). This is a special case of the general LD-PSDTF model in which each X n is restricted to a rank-1 PSD matrix (
X n =  X  x n  X  x T n ). In general, the DOF of X can be larger than MN because each X n is allowed to take any PSD matrix. In this section, the DOF of X is MN because X n is just an augmented representation of  X  x n . 2.3.2. Estimating the Latent Components In many applications such as source separation, latent component  X  x nk is of main interest. One might consider it necessary to calculate  X  x nk =  X   X  k  X  h kn  X  w kn .Instead, marginalizing out  X  w kn gives the posterior of  X  x nk as a Gaussian whose mean and covariance are matrices. For a Bayesian treatment, we need to calcu-H ,and V under a posterior over these variables, but this is analytically intractable. One alternative is to substitute maximum-a-posteriori (MAP) estimates of  X  , H ,and V into Eqs. ( 15 )and( 16 ). 2.4. Fourier Trick We here discuss the formulation of LD-PSDTF in the frequency domain. Using Eq. ( 14 ), the complex spec-trum F  X  x n (linear transformation of  X  x n ) is found to be complex-Gaussian distributed as follows: F  X  x n |  X   X  ,  X  W ,  X  H  X  X  c 0 , It is known that  X  V k can be diagonalized by using F if  X  V k is strictly a circulant matrix. A trivial example is a case that  X  V k is a scaled identity matrix, i.e. ,  X  is stationary white Gaussian noise. If  X  V k is a periodic kernel and its size M is much larger than its period,  X  V k can be roughly viewed as a circulant matrix. These facts justify IS-NMF for power-spectrogram de-composition. Since music audio signals roughly consist of pitched sounds and percussive sounds, it is reason-able to approximate  X  V k as a convex combination of periodic kernels (for pitched sounds) and identity ma-trices (for percussive sounds). In the frequency domain LD-PSDTF thus reduces to IS-NMF discarding the covariance between frequency bins, while in the time domain the full covariance structure is still taken into account. This approximation dramatically reduces the computational cost of LD-PSDTF from O ( M 3 NK )to O ( MNK ) as suggested in ( Liutkus et al. , 2011 ). We explain an inference method for a Bayesian model of GaP-LD-PSDTF defined by Eqs. ( 4 ), ( 5 ), ( 6 ), and ( 8 ). Given the observed data X , our goal is to calcu-late a posterior p (  X  , H , V | X )byusingtheBayesrule p (  X  , H , V | X )= p ( X ,  X  , H , V ) /p ( X ). Since p ( X analytically intractable, we use a variational Bayesian (VB) method for approximating p (  X  , H , V | X )bya factorizable distribution q (  X  , H , V )givenby q (  X  , H , V )= These factors can be alternately updated to monoton-ically increase a log-evidence lower bound L given by log p ( X )  X  E [log p ( X |  X  , H , V )] + E [log p (  X  )] + E [log p ( H )] + E [log p ( V )]  X  E [log q (  X  )]  X  E [log q ( H )]  X  E [log q ( V )]  X L . (19) Since the first term is still intractable, we need to take a further lower bound L such that L X L .Notethat L can be indirectly maximized by maximizing L .The updating formulas are q ( H )  X  p ( H )exp( E q (  X  , V ) [log q ( X |  X  , H , V q ( V )  X  p ( V )exp( E q (  X  , H ) [log q ( X |  X  , H , V where log q ( X |  X  , H , V ) is a variational lower bound of log p ( X |  X  , H , V ), which is given by Eq. ( 23 ). 3.1. Log-Evidence Lower Bound To derive the tractable bound L , we focus on the con-vexity and concavity of matrix-variate functions over PSD matrices. For example, f ( V ) = log | V | is con-cave and g ( V )=tr( ZV  X  1 ) is convex for any PSD matrix Z .Let M be the dimension of V .
 We first calculate a tangent plane of f ( V )byusinga first-order Taylor expansion as follows: where  X  is an arbitrary PSD matrix (tangent point) and the equality is satisfied when  X  = V .
 We then use the following matrix inequality, proposed by Sawada et al. ( 2012 ), regarding g ( V ): where { V k } K k =1 is a set of arbitrary PSD matrices, {
 X  k } K k =1 is a set of auxiliary matrices that sum to the identity matrix ( i.e. , k  X  k = I ), and the equality is satisfied when  X  k = V k ( k V k )  X  1 .
 Using Eqs. ( 21 )and( 22 ), we can derive the tractable lower bound of E [log p ( X |  X  , H , V )] (the first term of Eq. ( 19 )). A term regarding X n is bounded as follows:
E [log p ( X n |  X  , H , V )] (see Eq. ( 7 )) (23) =  X   X  X  X   X  where  X  n is a PSD matrix and {  X  nk } K k =1 is a set of auxiliary matrices that sum to an identity matrix. Let-ting the partial derivatives of Eq. ( 23 ) equal to be zero, we can obtain the optimal values of  X  n and {  X  nk } K k =1 that satisfy the equality as follows: 3.2. Variational Bayesian Update Here we discuss the functional forms of q (  X  k ), q ( h kn and q ( V k ). A problem lies in the non-conjugacy of the Bayesian model. Eq. ( 23 ) involves the expectations both of the scalar variables and of their reciprocals. This is, the sufficient statistics are x and x  X  1 , although those of the gamma prior are log( x )and x . This means that the functional forms of q (  X  k )and q ( h kn )aregiven by the generalized inverse Gaussian (GIG) distribu-tion, as shown in Hoffman et al. ( 2010 ). Note that the GIG distribution is defined as
GIG( x |  X , X , X  )= where  X  ,  X &gt; 0, and  X &gt; 0 are parameters and K  X  is the modified Bessel function of the second kind. The expectations E [ x ]and E [ x  X  1 ]aregivenby E [ x ]= As to matrix variable V k , we found that the functional form of q ( V k )isgivenbythematrixGIG(MGIG)dis-tribution ( Barndorff-Nielsen et al. , 1982 ). The MGIG distribution over PSD matrix X is defined as
MGIG( X |  X , R , T )= where  X  is a real number, R , T &gt; 0 are PSD matrices, M is the size of X ,and B  X  is the matrix Bessel func-tion of the second kind ( Herz , 1955 ). It includes the Wishart distribution as a special case ( Butler , 1998 ) and its sufficient statistics are log | X | , X ,and X  X  1 To calculate E [ X ]and E [ X  X  1 ],weuseaMonteCarlo method as described in the supplementary material. Consequently, we can assume the following forms: These parameters are iteratively updated as follows: 3.3. Multiplicative Update The multiplicative update (MU) is a well-known opti-mization technique often used for maximum-likelihood estimation of NMF. To show a clear connection of LD-PSDTF to IS-NMF, we derive closed-form MU rules for calculating the point estimates of H and V .Note that we assume  X  k =1andtr( V k )=1(unittrace) to remove the scale arbitrariness. If tr( V k )= s ,the scale adjustments V k  X  1 s V k and h k  X  s h k do not change the LD divergence D LD ( X n | Y n ).
 We aim to maximize the log-likelihood given by remov-ing the expectation operators from Eq. ( 23 ). Letting the partial derivative with respect to h kn equal to be zero, we get the following update rule: Then, letting the partial derivative with respect to V k equal to be zero, we get the following equation: where P k and Q k are PSD matrices given by Sawada et al. ( 2012 ) derived a complicated solution of Eq. ( 32 ), but we can solve it analytically by using the Cholesky decomposition Q k = L k L T k ,where L k is a lower triangular matrix. Finally, we get When all the matrices are diagonal, Eqs. ( 31 )and( 34 ) reduce to the one in IS-NMF ( Nakano et al. , 2010 ). We show that PSDTF has deep connections to nonneg-ative matrix factorization (NMF), tensor factorization (TF), and principal component analysis (PCA). 4.1. Nonnegative Matrix Factorization PSDTF includes NMF as a special case. If we restrict PSD matrices X n and V k to diagonal matrices ( i.e. , X n = diag( x n )and V k =diag( v k )forsome nonneg-ative vectors x n and v k )Eq.( 1 ) can be written as where  X  k and h kn are nonnegative numbers. If  X  k =1, this model reduces to the basic formulation of NMF. F  X  evotte et al. ( 2009 ) showed that the IS divergence is theoretically suitable for evaluating the reconstruction error of Eq. ( 35 ) in the task of audio source separation. Hoffman et al. ( 2010 ) proposed an infinite extension of IS-NMF (GaP-IS-NMF) using a gamma process prior on  X  . GaP-LD-PSDTF can therefore be viewed as a natural extension of GaP-IS-NMF.
 An interesting finding is that the positive semidefinite-ness (nonnegative definiteness) constraint on matrices in PSDTF induces sparse decomposition like the non-negativity constraint on vectors and scalars as in NMF. Positive semidefiniteness can therefore be considered a generalization of the nonnegativity concept. 4.2. Tensor Factorization PSDTF is related to a variant of TF called canonical polyadic (CP) decomposition ( Carroll &amp; Chang , 1970 ; Harshman , 1970 ). If we restrict a PSD matrix V k to a rank-1 matrix ( i.e. , V k = u k u T k for some vector u k Eq. ( 1 ) can be written as This can be viewed as CP decomposition in which basis vectors of the second mode, { u k } K k =1 , are constrained to be equal to those of the third mode, and  X  k and h k should be nonnegative. In addition, PSDTF uses the LD divergence for evaluating the reconstruction error while typical TF uses the Euclidean distance. There are some other related models. Tucker decom-position ( Tucker , 1966 ) is a generalization of CP de-composition and Xu et al. ( 2012 ) proposed its infinite extension ( K  X  X  X  ) based on the Gaussian or t pro-cess. Shashua &amp; Hazan ( 2005 ) proposed nonnegative TF (NTF) that, like NMF, imposes a nonnegativity constraint on all elements of factors. In this paper the nonnegativity constraint on  X  k and h k , not on u k ,led to a new class of TF. 4.3. Principal Component Analysis LD-PSDTF is related to a major class of matrix factor-ization (MF) using the Gaussian distribution as a core building block of probabilistic models. Let us recall the MF model given by Eq. ( 11 ): is the number of observations).
 Several probabilistic models are obtained by putting Gaussian priors in different ways. Placing an isotropic Gaussian prior on N columns of  X  H (latent-space co-ordinates corresponding to the observations) leads to probabilistic PCA (PPCA) ( Bishop , 1999 ). If we put an isotropic Gaussian prior on M rows of  X  W (mapping functions from the latent space to the observed space), the resulting model is called dual PPCA. Marginaliz-ing  X  W out, we can formulate a Gaussian process la-tent variable model (GPLVM) ( Lawrence , 2003 ). PS-DTF, on the other hand, puts a full-covariance Gaus-sian prior on K columns of  X  W . If we instead use a GP prior, PSDTF given by Eq. ( 14 ) can be viewed as mul-tiple kernel learning (MKL) ( Lanckriet et al. , 2004 ). This section reports experiments to evaluate the per-formance of LD-PSDTF. 5.1. Synthetic Data We evaluated the capability of GaP-LD-PSDTF to dis-cover basis PSD matrices { V k } K + k =1 used for generating an observed tensor X and to estimate the value of K + . In this experiment we use M =  X  = 10, N = 2000, and K + = 6. The synthetic data X was stochastically gen-erated according to the following process: To learn the GaP-LD-PSDTF model, we used the VB algorithm with a truncation level K = 100, and hy-perparameters  X  = c =1, a 0 = b 0 =0 . 1,  X  0 = 10, and V 0 = I / X  0 . Since Monte Carlo simulation of E [ V k ] and E [ V  X  1 k ] was found to be often unreliable, we in-stead calculated the maximum-a-posteriori estimates. For maximum-likelihood estimation of the LD-PSDTF model, we used the MU algorithm with K =6. The both methods were initialized randomly.
 As shown in Figure 3 , the experimental results showed that the both models successfully discovered the cor-rect basis matrices. The hyperparameters were not sensitive to the results. In GaP-LD-PSDTF, the true number of bases, K + = 6, was correctly estimated. 5.2. EEG Data We then tested LD-PSDTF on a popular EEG dataset ( Blankertz , 2001 ).Weaimedtopredictaleftorright hand movement (label -1 or 1) from 500 ms EEG sig-nals recorded at 28 channels of the brain ( M = 28) with a sampling rate of 100 Hz. There were 416 trials ( N = 416) of which 100 belong to the test set. For each trial we calculated a full-rank covariance matrix over 50 frames. The PSD basis matrices and their activa-tions were estimated by using the MU algorithm with K = 5 in an unsupervised manner. We used Fisher X  X  LDA for binary classification of the K -dimensional ac-tivations corresponding to the individual trials. The most significant principal component of each ba-sismatrixisshowninFigure 4 , in which each number indicates the correlation between the ground-truth la-bels and the estimated activations on the test set. The accuracies of classification were 73% ( K = 5) and 79% ( K = 10). The leftmost and rightmost bases with high correlations are compatible with the well-known physi-ological process called event-related desynchronization (ERD). We consider these results promising.
 Note that the best results from the competition were obtained by combining the first-order features (later-alized readiness potential: LRP) and the second-order ones (ERD), whereas our method used only the latter. Finding discriminative patterns without any supervi-sion in this context is a highly nontrivial task, because discriminative signals are much weaker than irrelevant oscillatory activities ( e.g. , occipital alpha waves). 5.3. Music Data We evaluated LD-PSDTF for music signal analysis. As discussed in Section 2.4 , LD-PSDTF formulated in the time domain is equivalent to IS-NMF formulated in the frequency domain if { V k } K k =1 are circulant ma-trices such as periodic kernels, identity matrices, and their conic sums. Since this is a reasonable assumption for music signals, we used the Fourier trick explained in Section 2.4 . We tested an infinite model with a trun-cation level K = 100 and finite models (  X  k = 1) with different K ranging from 1 to 100. For comparison, we tested an infinite model of KL-NMF (GaP-KL-NMF) and finite models with different K ranging from 1 to 100 for amplitude-spectrogram decomposition.
 We used three songs (No.1, 2, and 3) from the  X  X WC Music Database: Popular Music X  ( Goto et al. , 2002 ). The CD-quality audio signals were downsampled at 16 [kHz] and were analyzed by using short-time Fourier transform with a window size of 128 [ms] and a shifting interval of 64 [ms]. The size of X is specified as M = 2048, and N = 3237, 3447, and 3020, respectively. We used the VB algorithm with hyperparameters  X  =1, c =1, a 0 = b 0 =0 . 1,  X  0 = M ,  X  =1,and V 0 = I / X  0 . The experimental results showed that in each song the value of K + chosen by GaP-LD-PSDTF was close to the best value of K found by finite models (Figure 5 ). This proves that GaP-LD-PSDTF has an ability of au-tomatic model-order selection without expensive grid search. Similar results were obtained in KL-NMF, but GaP-LD-PSDTF achieved much higher log-evidence lower bounds than GaP-KL-NMF did. This supports the appropriateness of IS-NMF for music analysis. Ad-ditional results of source separation with sound sam-ples are given in the supplementary material. This paper presented positive semidefinite tensor fac-torization (PSDTF) as a natural extension of nonneg-ative matrix factorization (NMF). We used a Bregman matrix divergence called the log-determinant (LD) di-vergence as the reconstruction error. This LD-PSDTF can be viewed as a natural extension of NMF based on the Itakura-Saito (IS) divergence. We formulated a nonparametric Bayesian model that allows an observed tensor to contain an unbounded number of bases and derived a variational Bayesian algorithm and a multi-plicative update rule by using matrix inequalities. In addition, we showed the effectiveness of the Fourier trick, i.e., the frequency-domain formulation can dra-matically reduce the computational cost in some ap-plications such as music signal analysis.
 One interesting open question is what kind of PSDTF can be viewed as an extension of NMF based on the Kullback-Leibler (KL) divergence. The von-Neumann (vN) divergence ( Tsuda et al. , 2005 )iswellknownas another major type of the Bregman matrix divergence that includes the KL divergence as a special case. Sub-stituting  X  ( Z )=tr( Z log Z  X  Z )intoEq.( 2 ), the vN divergence is given by The assumption underlying p ( X n | Y n ), however, is not obvious although the Bregman divergence must corre-spond one-to-one to an exponential family. We plan to investigate vN-PSDTF to formulate a Bayesian model.
Acknowledgment: This study was partially sup-ported by JSPS KAKENHI 23700184, MEXT KAK-
ENHI 25870192, and JST OngaCREST project. We thank the reviewers for giving insightful comments. Barndorff-Nielsen, O., Bl X sild, P., Jensen, J. L., and J X rgensen, B. Exponential transformation models. Royal Society of London , 379(1776):41 X 65, 1982. Bishop, C. M. Variational principal components. In ICANN , pp. 509 X 514, 1999.
 Blankertz, B., Curio, G., and M  X  uller, K.-L. Classifying single trial EEG: Towards brain computer interfac-ing, In: NIPS , 2001. www.bbci.de/competition/ii/berlin desc.html Bregman, L. M. The relaxation method of finding the common points of convex sets and its application to the solution of problems in convex programming. USSR CMMP , 7(3):200 X 217, 1967.
 Butler, R. W. Generalized inverse Gaussian distribu-tions and their Wishart connections. Scandinavian Journal of Statistics , 25(1):69 X 75, 1998.
 Butler, R. W. and Wood, A. Laplace approximation for Bessel functions of matrix argument. J. of Com-putational and Applied Math. , 155(2):359 X 382, 2003. Carroll, J. D. and Chang, J. J. Analysis of individ-ual differences in multidimensional scaling via an
N-way generalization of  X  X ckart-Young X  decomposi-tion. Psychometrika , 35(3):283 X 319, 1970.
 Cemgil, A. T. Bayesian inference for nonnegative matrix factorisation models. Computational Intel-ligence and Neuroscience , Article ID 785152, 2009. F  X  evotte, C., Bertin, N., and Durrieu, J.-L. Nonnega-tive matrix factorization with the Itakura-Saito di-vergence: With application to music analysis. Neu-ral Computation , 21(3):793 X 830, 2009.
 Goto, M., Hashiguchi, H., Nishimura, T., and Oka, R.
RWC music database: Popular, classical, and jazz music database. In ISMIR , pp. 287 X 288, 2002. Harshman, R. A. Foundations of the PARAFAC pro-cedure: Models and conditions for an  X  X xplanatory X  multi-modal factor analysis. UCLA Working Papers in Phonetics , 16(1), 1970.
 Herz, C. S. Bessel functions of matrix argument. An-nals of Mathematics , 61(3):474 X 523, 1955.
 Hoffman, M., Blei, D., and Cook, P. Bayesian non-parametric matrix factorization for recorded music. In ICML , pp. 439 X 446, 2010.
 Itakura, F. and Saito, S. Analysis synthesis telephony based on the maximum likelihood method. In ICA , pp. C17 X  X 20, 1968.
 Kulis, B., Sustik, M., and Dhillon, I. Low-rank kernel learning with Bregman matrix divergences. JMLR , 10:341 X 376, 2009.
 Kullback, S. and Leibler, R. On information and suf-ficiency. Annals of Math. Stat. , 22(1):79 X 86, 1951. Lanckriet, G., Cristianini, N., Bartlett, P., Ghaoui,
L., and Jordan, M. Learning the kernel matrix with semidefinite programming. JMLR , 5:27 X 72, 2004. Lawrence, N. D. Gaussian process latent variable mod-els for visualisation of high dimensional data. In NIPS , 2003.
 Lee, D. and Seung, H. Algorithms for non-negative matrix factorization. In NIPS , pp. 556 X 562, 2000. Lee, H., Cichocki, A., and Choi, S. Nonnegative matrix factorization for motor imagery EEG classification. In ICANN , pp. 250 X 259, 2006.
 Liutkus, A., Badeau, R., and Richard, G. Gaus-sian processes for underdetermined source separa-tion. IEEE Trans. on ASLP , 59(7):3155 X 3167, 2011. Nakano, M., Kameoka, H., Roux, J. Le, Kitano, Y.,
Ono, N., and Sagayama, S. Convergence-guaranteed multiplicative algorithms for non-negative matrix factorization with beta divergence. In MLSP , pp. 283 X 288, 2010.
 Salakhutdinov, R. and Mnih, A. Bayesian probabilis-tic matrix factorization using Markov chain Monte Carlo. In ICML , pp. 880 X 887, 2008.
 Sawada, H., Kameoka, H., Araki, S., and Ueda, N. Efficient algorithms for multichannel extensions of Itakura-Saito nonnegative matrix factorization. In ICASSP , pp. 261 X 264, 2012.
 Shashua, A. and Hazan, T. Non-negative tensor factor-ization with applications to statistics and computer vision. In ICML , pp. 792 X 799, 2005.
 Sivalingam, R., Boley, D., Morellas, V., and Pa-panikolopoulos, N. Tensor sparse coding for region covariances. In ECCV , pp. 722 X 735, 2010.
 Smaragdis, P. and Brown, J. C. Non-negative matrix factorization for polyphonic music transcription. In WASPAA , pp. 177 X 180, 2003.
 Tsuda, K., R  X  atsch, G., and Warmuth, M. K. Matrix exponentiated gradient updates for on-line learning and Bregman projection. JMLR , 6:995 X 1018, 2005. Tucker, L. R. Some mathematical notes on three-mode factor analysis. Psychometrika , 31(3):279 X 311, 1966. Xu, Z., Yan, F., and Qi, Y. Infinite Tucker decomposi-tion: Nonparametric Bayesian models for multiway
