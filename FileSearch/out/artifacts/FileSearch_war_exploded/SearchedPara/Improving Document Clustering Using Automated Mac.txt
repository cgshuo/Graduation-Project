 With the development of statistical machine translation, we have ready-to-use tools that can translate documents from one language to many other languages. These trans-lations provide different yet correlated views of the same set of documents. This gives rise to an intriguing ques-tion: can we use the extra information to achieve a better clustering of the documents? Some recent work on multi-view clustering provided positive answers to this question. In this work, we propose an alternative approach to ad-dress this problem using the constrained clustering frame-work. Unlike traditional Must-Link and Cannot-Link con-straints, the constraints generated from machine translation are dense yet noisy. We show how to incorporate this type of constraints by presenting two algorithms, one parametric and one non-parametric. Our algorithms are easy to imple-ment, efficient, and can consistently improve the clustering of real data, namely the Reuters RCV1/RCV2 Multilingual Dataset. In contrast to existing multiview clustering algo-rithms, our technique does not need the compatibility or the conditional independence assumption, nor does it involve subtle parameter tuning.
 H.3.3 [ Information Search and Retrieval ]: Clustering Document clustering, constrained spectral clustering, ma-chine translation
Automated Machine Translation (MT) [17] allows docu-ments written in one language to be translated into other languages at very low cost. The field has made great strides recently and many online tools, such as Google Translate and Microsoft Translator, have been made available. This gives rise to an intriguing question: can machine translation help us to achieve better clustering of documents?
This problem has been recently explored from the multi-view learning perspective [3,15,16] and positive results have been reported. In the multiview learning framework, each original document and its translation are treated as two views of the same data object. Multiview learning makes the assumption that the two views are compatible and con-ditionally independent [7], thus combining them will lead to a better classification or clustering. However, in real-ity there is no principled way to check the validity of these assumptions. Another limitation of the existing multiview clustering techniques is that their performance is sensitive to parameter tuning, which relies heavily on the prior knowl-edge on the relative quality of each particular view.
An alternative approach we shall explore to incorporating machine translation into document clustering is constrained clustering [5]. The basic idea of constrained clustering is to convert side information into Must-Link and Cannot-Link pairwise constraints; then the constraints are enforced on the original dataset to help improving the clustering. Tra-ditionally, the constraints are assumed to be accurate but sparse :  X  X ccurate X  means that they come from domain ex-perts or ground truth, thus they are definite and correct;  X  X parse X  means only a small amount of constraints are in-corporated, given acquiring them in practice is costly.
However, the constraints generated from machine trans-lation are dense but noisy :  X  X ense X  means that we have access to large amounts of constraints with no cost or very low cost;  X  X oisy X  means that the constraints are noisy and may not necessarily reflect the ground truth clustering. As a result, existing constrained clustering algorithms cannot be directly applied to our problem. A new algorithm is needed to properly incorporate this type of constraints so that it can 1) fully exploit the useful information in the massive constraint set; 2) ignore constraints that are inaccurate and excessive; and 3) not be easily over-constrained as most con-strained clustering algorithms are [9].
We show how to incorporate massive amounts of noisy side information into spectral clustering and demonstrate that this approach outperforms multiview techniques for MT aided document clustering. We choose a spectral formula-tion for two reasons: 1) spectral clustering has been proven effective on high-dimensional data, particularly text [6], and 2) by converting the two views into two graphs with the same set of nodes we can transfer knowledge between two het-e rogeneous feature spaces (languages) in a principled man-ner. Our objective function extends earlier work by Wang and Davidson [21]. Their work was limited to clusterings for K = 2 and used a sparse constraint matrix generated from domain experts or ground truth labeling. In our prob-lem setting, the constraint matrix is generated from another graph. As a result, the constraint matrix we use is guar-anteed to be positive semi-definite. This difference enables us to extend the formulation in [21] from 2-way partition to K -way partition, and to propose a parametric as well as a non-parametric solution. As compared to existing work in MT aided document clustering, our algorithm has several unique benefits:
Our contribution to the field is to propose a spectral clus-tering formulation that can handle dense but noisy con-straints. We also show: 1. Our method is able to improve the clustering on the 2. Our approach outperforms many existing multiview 3. Our approach yields improvements in the vast major-4. Our algorithm can be extended to other applications
The remainder of the paper is organized as follows: Sec-tion 2 provides some background knowledge on spectral clus-tering and the constrained spectral clustering formulation proposed in [21]; we present our algorithm in Section 3; we test our algorithm on real data in Section 4 and compare it to existing techniques; related work is discussed in Sec-tion 5; we discuss future directions and conclude the paper in Section 6.
To make this paper self-contained, we provide some back-ground knowledge and also introduce notations that will be used throughout the rest of the paper (also summarized in Table 1).

W e first introduce the formulation and notation for spec-tral clustering. Readers who are familiar with the topic can skip to Section 2.2.
 Given a graph G with N nodes, A is the affinity matrix of G . A is symmetric and nonnegative. D is the degree matrix of G : L = D  X  A is the graph Laplacian of G , and  X  L = D  X  1 / 2 is called the normalized graph Laplacian [20].

The objective function for the normalized min-cut prob-lem is (after relaxation): Shi and Malik [18] showed that the optimal solution to Equa-tion (1) is the second smallest eigenvector of  X  L .
For K -way partition, the objective becomes tr is the matrix trace. The optimal solution to Equation (2) is the top-K smallest eigenvectors of  X  L , and the clustering assignment is derived from applying K -means to the rows of V [20].
Next we briefly summarize the constrained spectral clus-tering formulation proposed in [21].

Let Q  X  R N  X  N be a relaxed constraint matrix. Q is sym-metric and Given a cut v , the objective is to minimize its cost on  X  and to lower bound its satisfaction on Q with a parameter  X  . Specifically:
Algorithm 1: T he parametric version of our algorithm ( csp-p ) Input :  X  L ,  X  Q ,  X  , K ;
O utput : u ; 1 Solve the generalized eigenvalue problem  X  L v =  X   X  Q v ; 2 Let V = { v i } N i =1 be the set of all generalized eigenvectors; 3 for i = 1 to N do 4 if v T i  X  Q v i &lt;  X  then 5 R emove v i from V ; 6 end 7 end 8 V  X  [ ]; 9 for i = 1 to K do 10 v  X  = argmin 11 Remove v  X  from V ; 12 V  X  [ V, v  X  ]; 13 end 14 return u  X  Kmeans ( V, K ); Equation (3) can be solved by introduce the Karush-Kuhn-T ucker conditions [14]. The solution is among the eigenvec-tors of the following the generalized eigenvalue problem: After removing all negative eigenvectors (which fail to satisfy the lower bound  X  ), the one that minimizes v T  X  L v is the solution.

This formulation has several limitations: 1) it is for 2-way clustering instead of K -way; 2) setting the cut-off threshold  X  requires prior knowledge; and 3) it is unclear if it can handle huge amounts of constraints. In Section 3, we will adapt this formulation to our problem and address these limitations.
In this section, we present our constrained spectral clus-tering algorithm for MT aided document clustering. The objective function we use is derived from Equation (3). But unlike earlier work, since the constraint matrix Q in our problem setting is obtained from a distance metric, it is guar-anteed to be positive semi-definite (PSD). We first show how to construct the graph Laplacian and the constraint matrix in the document clustering setting; then we extend Equa-tion (3) from 2-way partition to K -way partition (Equa-tion (5)); we further develop a non-parametric version of the formulation (Equation (7)); efficient solutions are provided for both objectives (Algorithm 1 and 2).
Given a set of N documents, after standard tf-idf indexing and normalization, they can be represented by a set of d -Let X = [ x 1 , . . . , x N ], then is an N  X  N cosine similarity matrix and A is positive semi-definite.

In our problem setting, given a set of documents and their translation, we can construct two similarity matrices, A (1)
Algorithm 2: T he non-parametric version of our algo-rithm ( csp-n ) Input :  X  L ,  X  Q , K ;
O utput : u ; 1 Solve the generalized eigenvalue problem  X  L v =  X   X  Q v ; 2 Let V = { v i } N i =1 be the set of all generalized eigenvectors; 3 V  X  [ ]; 4 for i = 1 to K do 5 v  X  = argmin 6 R emove v  X  from V ; 7 V  X  [ V, v  X  ]; 8 end 9 return u  X  Kmeans ( V, K ); and A ( 2) , one for each language. We use A (1) as the affin-ity matrix of the graph, which is equivalent to A in the constrained spectral clustering formulation. We use A (2) the constraint matrix, which is equivalent to Q in the con-strained spectral clustering formulation. The interpretation of A (2) is: the greater A (2) ij is, the more likely document i and j are considered to belong to the same cluster.
Let D ( i ) be the degree matrix of A ( i ) , i = 1 , 2. We have:
Note that as long as A (1) and A (2) are guaranteed to be PSD, other similarity functions and preprocessing tech-niques can be freely used. For example, instead of cosine similarity, we can use a Gaussian kernel. We can also apply various dimensionality reduction techniques to improve the quality of the similarity matrix.
Given  X  L and  X  Q , we first present a parametric formulation for constrained spectral clustering. It is a natural exten-sion of Equation (3) from 2-way to K -way partition. Our objective function is:  X  is the only parameter in this objective. It serves as a cut-off threshold. Any cut v that fails to satisfy (in the relaxed sense) at least  X  constraints in  X  Q will be rejected. For the remaining cuts, which are called feasible cuts [21], the top-K ones with the lowest cost on  X  L will be chosen. To guarantee the existence of at least K feasible cuts to choose from, when setting the value of  X  , we require: since  X  Q is PSD in our problem setting, if  X  is set to 0, any v  X  R N is a feasible cut.
Given that Equation (6) is satisfied, raising the threshold  X  w ill reduce the range of the feasible cuts that we can choose from. As a result, the final partition will be more biased towards the constraint matrix  X  Q . Similarly, lowering the threshold  X  will give the algorithm more freedom to choose cuts that are favored by the graph  X  L . Therefore, in practice, the choice of  X  is determined by our preference between  X  and  X  Q . If we have confidence that  X  Q is more accurate, then we should set  X  to a larger value; and vice versa.
Next we present an alternative objective which is non-parametric. To get rid of the cut-off threshold  X  , we consider the following cost-satisfaction ratio for any cut v : S ince  X  Q is now guaranteed to be PSD, we have The goal of constrained spectral clustering is to maximize v
Q v and minimize v T  X  L v , which is equivalent to minimize f ( v ). Therefore f ( v ) becomes a unified measure for the quality of the cut v : smaller f ( v ) means better cut.
Formally, the non-parametric version of our objective is: We first show how to solve the non-parametric objective in Equation (7). Consider the generalized eigenvalue problem: Since both  X  L and  X  Q are Hermitian and PSD, we will have N real generalized eigenvectors [4], and they are the critical points for the generalized Rayleigh quotient [11]
Consequently, the solution to Equation (7) is the top-K generalized eigenvectors from Equation (8) that minimize the Rayleigh quotient in Equation (9). The full algorithm is presented in Algorithm 2.

On the other hand, the solution to Equation (5) is similar to that in [21]. We first solve the generalized eigenvalue problem and the non-negative eigenvectors are the feasible cuts. For efficiency consideration, in our implementation, we simply solve Equation (8), and choose eigenvectors associated with eigenvalues that are no smaller than  X  . This allows us to only solve the generalized eigenvalue problem once, and reuse the eigenvectors for multiple  X  values. The algorithm for the parametric objective in Equation (5) is summarized in Algorithm 1.
In Figure 1 we illustrate how our approach can improve over clustering using a single view. The dataset consists of 1200 documents on 6 topics originally written in English and their translation into French. We plot the top-6 eigenvectors (each color corresponds to one eigenvector). The y -axis is the entry values of the eigenvectors. Due to the sparsity of the feature space, for spectral clustering using a single view, we observe a couple of  X  X pikes X  among the top eigenvectors (Figure 1(a) and (b)), which are trivial cuts that separate a small number of outlying documents from the rest. In-cluding these trivial cuts will inevitably harm the quality of the final partition. In contrast, when we combined the two views together using our approach, the two views re-jected the trivial cuts proposed by each other, and reached agreement on 6 eigenvectors that are more informative (Fig-ure 1(c)). As a result, the quality of the final partition is substantially improved.

Note that Figure 1 is just one among many cases where our approach could help improve the clustering. It could be still effective even when there are no trivial cuts involved.
The runtime for our algorithm is dominated by that of solving the eigenvalue problem in Equation (8). Therefore, the complexity of our algorithm is on par with spectral clus-tering in big-O notation, which is O ( KN 2 ) for dense matri-ces and O ( KM ) for sparse matrices, M be the number of non-zeros entries.
The process of enforcing the constraint matrix  X  Q to the graph  X  L can be viewed as the transfer of knowledge. From Equation (5) we can see that the transfer is asymmetric. Therefore, given the original view and the translated view, we need to decide which view should be used to construct  X  and which view should be used to construct  X  L . The role of the constraint matrix  X  Q is to select N  X  feasible cuts from the N generalized eigenvectors ( N  X   X  N ); the role of the graph Laplacian is to select K min-cuts from the N  X  feasible cuts. Therefore,  X  Q plays a more critical role than  X  L does. If the quality of  X  Q is very poor, it will rule out eigenvectors that lead to good cuts; and once those cuts are ruled out,  X  L will not be able to recover them. On the other hand, if the quality of  X  Q is very high, it will select a set of good feasible cuts for  X  L to choose from.

Consequently, in practice we should use the better view to construct the constraint matrix. Although such prior knowledge is not always available, according to our observa-tion on real data, the original (language) view usually has better quality. Therefore in our experiments,  X  Q is always constructed from the original documents and  X  L is always constructed from the translation.
In this section, we empirically study the performance of our algorithm on real-world data and compare it to existing techniques. We aim to answer the following questions: 1. Effectiveness: Is our algorithm able to improve the 2. Consistency: Is the performance gain of our algo-3. C omparison: Does our algorithm outperform exist-
We used the Reuters RCV1/RCV2 Multilingual Dataset introduced by [3]. This dataset has been used by previous work [13, 15, 16] to evaluate the performance of multiview spectral clustering algorithm. The dataset contains docu-ments originally written in five different languages, namely English (EN), French (FR), German (GR), Spanish (SP) and Italian (IT). Each document, originally written in one language, was translated to the other four languages using the Portage system [19]. The documents are categorized into six different topics. The statistics of the dataset is summa-rized in Table 2. More detail can be found on the dataset homepage [1].

The dataset is provided in the form of tf-idf vectors. We did not apply additional preprocessing to the data. No di-mensionality reduction technique was applied. We used co-sine similarity to construct the graphs.
We used two common evaluation metrics to measure the quality of clustering, namely Adjusted Rand Index (ARI) [12] and Normalized Mutual Information (NMI). Both of them indicate the similarity between a given clustering and the ground truth partition: higher value means better clus-tering; 1 means perfect match.

Note that the dataset we chose is a very difficult dataset to work with. Even the state-of-the-art techniques can only achieve moderate clustering accuracy on this dataset [15,16]. The results we report in Section 4.2 may seem low, but they are consistent with the numbers reported by previous work.
We implemented both the parametric ( csp-p ) and the non-parametric ( csp-n ) version of our algorithm 1 . For the parametric version, we always set where  X  2 K (  X  Q ) is the 2 K -th largest eigenvalue of words, we provide 2 K feasible cuts and  X  L will choose the top-K with the lowest costs.

We also implemented five baseline algorithms in MATLAB to compare with:
T he MATLAB code is available by contacting the authors.
We first pick a language pair, say EN-FR, which means documents that are originally written in English along with their French translation. To maximize the diversity between the data samples we use, in each trial we randomly sample 1200 documents, which is less than 10% of all available docu-ments. We have 100 trials for each language pair. We apply our algorithm and the baseline algorithms to the sample and partition it into K = 6 clusters. We measure the resultant clusterings using both ARI and NMI. Since the last step of spectral clustering involves the K -means algorithm, in each trial, we repeat K -means algorithm 100 times with 100 ran-dom seeds and report the average performance.

For all language pairs, we report the average performance (Table 3), aggregated results over 100 trials (Figure 2), and the performance of each individual trial (Figure 3).
Since we have 5 different languages, there are 20 possible original-translation language combinations. We show our re-sults on 8 pairs, namely English to French (EN-FR), German (EN-GR), Italian (EN-IT), Spanish (EN-SP), and German to English (GR-EN), French (GR-FR), Italian (GR-IT), and Spanish (GR-SP). The conclusions we draw from these 8 pairs also hold for the other 12 pairs.

First we give an overview of the results in terms of av-erage performance. In Table 3, we report the average ARI of 7 different algorithms on 8 different language pairs. Our approach ( csp-p ) shows consistent and significant (99% con-fidence level) improvement over the clustering on the orig-inal view only ( orig ) for all language pairs. Also, for all language pairs, csp-p has the highest average ARI.
More detailed results are reported in Figure 2, with box plots for all 7 algorithms and 8 language pairs, in terms of ARI and NMI, respectively. Besides of showing the advan-tage of our approach ( csp-p ), as we have seen in Table 3, Figure 2 illustrates the diversity of the data samples we used in different trials. Some data samples were easier to clus-ter and others more difficult. This demonstrates that the effectiveness of our approach is not limited to a certain data distribution or a certain language.

Note that the performance of our non-parametric approach ( csp-n ) is not as good as the parametric one ( csp-p ). This is expected because csp-n uses zero prior knowledge. On the other hand, as shown in Table 3, csp-n managed to out-perform orig on all 8 language pairs. It also outperformed the multiview competitors on several language pairs. This result is non-trivial considering the approach is completely parameter-free.

To further demonstrate the consistency and reliability of our approach over different random samples, in Figure 3, we show the trial by trial breakdown of the performance gain of our approach ( csp-p and csp-n ) over the clustering on the original view only ( orig ), as measured by ARI. We can see that for 800 random trials over 8 different language pairs, our algorithm achieved positive gain in most cases, and it rarely caused large performance loss. This means our approach is reliable in practice. Practitioners can apply our approach to a dataset, with peace of mind that it is very likely that our algorithm will improve the result, and it is very unlikely that our algorithm will lead to a great performance loss.
Recent work in the multiview learning literature stud-ied the potential of using automated machine translation to improve both document classification [2, 3] and cluster-ing [13, 15, 16]. Given a set of documents, their translation in another language is modeled as a second view. These multiple views are considered as partial observations of the same set of data objects. When properly combined, these views will complement each other and improve the resultant classification or clustering. Several multiview learning algo-rithms have been proposed and tested on the same Reuters RCV1/RCV2 Multilingual Dataset [3] as we used in this pa-per. Empirical results confirmed the helpfulness of machine translation. Note that the effectiveness of multiview learn-ing is based on the assumption that the views are compatible and conditionally independent [7]. There is no practical way to validate either assumption for a specific sample.
In this work, we adopted an alternative approach, namely constrained clustering [5]. In contrast to the multiview for-mulation, constrained clustering does not make assumption about the underlying distribution of the data. Traditional constrained clustering algorithm cannot be directly applied to our problem because they can only deal with sparse and accurate constraints. If the number of constraints increases, or incorrect constraints are introduced, the clustering al-gorithm will be over-constrained [9]. It is also difficult to choose a small set of helpful constraints [10].

Wang and Davidson [21] proposed a spectral formulation for constrained clustering, which suits our problem setting well because it can incorporate soft constraints. Instead of enforcing each and every constraint given, they use a user-specified parameter to lower bound the number of satisfied constraints. As a result, noisy and incorrect constraints can be ignored by the final clustering. The difference be-tween our work and theirs is that they considered sparse constraints generated from ground truth or domain experts, whereas our constraints are dense, generated from a distance metric. Furthermore, the constraint matrix we use is always PSD. As a result, we are able to extend their objective to K -way partition, and develop a new non-parametric solution to the problem.

The focus of this work is to show how to use machine translation to improve document clustering. We assume that the translations of documents are readily available. Techni-cal detail of statistical machine translation [17] is not con-sidered, although it is expected that better translation will lead to greater performance gain.
Automated Machine Translation offers the ability to sup-plement existing document representations with additional information. Previous work has explored using this addi-tional information in a multiview clustering setting with some success. In this work, we take an alternative approach of encoding the additional information as constraints. This is a challenging problem since existing constrained clustering algorithms expect a small number of constraints generated original view ( orig ), and outperforms the competitors. but also in most of the individual trials. from the ground truth or domain experts, whereas MT pro-d uces dense and potentially inaccurate information. We pro-posed two algorithms that can be viewed as an extension of spectral clustering to encode many noisy constraints with-out being over-constrained and with the ability to ignore constraints. We showed with real data that our approach is effective (it improves the clustering using a single view (language), see Table 3), consistent (since it can ignore poor side information, see Figure 2 and 3), and outperforms other comparable techniques (see Table 3).

It is important to remember that existing work on multi-view clustering [15] showed that the performance gain from using more than two views is marginal. Our future work will revisit this question by clustering using constraints gen-erated from multiple translated views. Also, in this work we showed that adding constraints from translations does not hurt the clustering (since our approach ignores harm-ful constraints). However, an important problem we hope to address is determining a priori how much the transla-tion will improve the clustering. This will help address the problem:  X  X hich language should we translate into? X  We gratefully acknowledge support of this research via ONR grants N00014-09-1-0712 Automated Discovery and Explanation of Event Behavior, N00014-11-1-0108 Guided Learning in Dynamic Environments and NSF Grant NSF IIS-0801528 Knowledge Enhanced Clustering. [1] Reuters RCV1/RCV2 multilingual dataset: [2] M.-R. Amini and C. Goutte. A co-classification [3] M.-R. Amini, N. Usunier, and C. Goutte. Learning [4] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and [5] S. Basu, I. Davidson, and K. Wagstaff, editors. [6] M. W. Berry, editor. Survey of text mining: clustering, [7] A. Blum and T. M. Mitchell. Combining labeled and [8] C. Cortes, M. Mohri, and A. Rostamizadeh. Learning [9] I. Davidson and S. S. Ravi. Identifying and generating [10] I. Davidson, K. Wagstaff, and S. Basu. Measuring [11] R. Horn and C. Johnson. Matrix analysis . Cambridge [12] L. Hubert and P. Arabie. Comparing partitions. [13] Y.-M. Kim, M.-R. Amini, C. Goutte, and P. Gallinari. [14] H. Kuhn and A. Tucker. Nonlinear programming. [15] A. Kumar and H. D. III. A co-training approach for [16] A. Kumar, P. Rai, and H. D. III. Co-regularized [17] A. Lopez. Statistical machine translation. ACM [18] J. Shi and J. Malik. Normalized cuts and image [19] N. Ueffing, M. Simard, S. Larkin, and J. H. Johnson. [20] U. von Luxburg. A tutorial on spectral clustering. [21] X. Wang and I. Davidson. Flexible constrained [22] D. Zhou and C. Burges. Spectral clustering and
