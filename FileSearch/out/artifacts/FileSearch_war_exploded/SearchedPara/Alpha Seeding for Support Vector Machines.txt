 A k ey practical obstacle in applying supp ort v ector ma-chines to man y large-scale data mining tasks is that SVM training time generally scales quadratically (or w orse) in the n um b er of examples or supp ort v ectors. This complexit yis further comp ounded when a sp eci c SVM training is but one of man y , suc h as in Lea v e-One-Out-Cross-V alidation (LOOCV) for determining optimal SVM parameters or as in wrapp er-based feature selection. In this pap er w e explore new tec hniques for reducing the amortized cost of eac hsuc h SVM training, b y seeding successiv e SVM trainings with the results of previous similar trainings.
 I.5.5 [ Computing Metho dologies ]: P attern Recognition-Implemen tation supp ort v ector mac hines, classi cation, training sp eed-ups Recen t progress on sp eeding up the training time for supp ort v ector mac hines (e.g. [7],[5]) has made SVM's practical no w for training sets that are fairly large. Ho w ev er, the time complexities of those approac hes are still t ypically quadratic in the n um ber of examples ( N ) in the training data set. This is esp ecially problematic in a data mining con text, due b oth to commonalit y of enormous data set sizes and to the frequen t need for high-qualit y mo del selection o v er man y candidate SVM's.
 W e therefore seek metho ds whic h can reuse previous results from similar SVM's in order to amortize training costs. In the best case, this could lead to amortized SVM training costs whic h are linear in N . F or example, Lea v e-One-Out-Cross-V alidation (LOOCV) estimates of generalization er-ror for a data set of N examples in v olv e N trainings, eac h in v olving N 1 training examples , th us leading to cubic o v erall time complexit y . If eac h SVM for eac hof the size N 1 data sets could be in telligen tly initialized from the result of the SVM trained on all N examples, only a small amoun t of additional w ork migh t b e required for eac h. The o v erall cost migh tw ell remain quadratic in N (i.e. domi-nated b y the cost of the SVM trained on the full data set) | and th us e ectiv ely ha v e cost linear in N for eac hofthe N SVM's trained for the di eren t size N 1 data sets. An underlying motiv ation of our w ork is to try to bring SVM's substan tially closer to the fast near-linear complexit y of LOOCV using k nearest-neigh bors, (a factor in k-NN's p opularit y in practice), while retaining the adv an tages of SVM's (e.g. maxim um margins). 1 After reviewing the basic asp ects of SVM classi cation, w e presen tav ariet yof \alpha seeding" metho ds for reducing SVM training time. W e then presen t some empirical results whic h illustrate the p oten tial promise of suc h alpha seeding and help us b egin to understand the tradeo s in v olv ed. Al-though w eha v e not y et ac hiev ed linear amortized costs, our results app ear promising to w ards that e ort, as w ell as of practical use in their o wn righ t. Supp ort v ector mac hines [9, 10] represen t a relativ ely new and promising approac htomac hine learning. Recen tw ork has established SVM's as pro viding state-of-the-art p erfor-mance on classi cation and regression tasks across a v ariet y of real-w orld applications (e.g. see [9] and [4]). In this pap er, w e fo cus on SVM's for binary classi cation [1]. Eac h lab el is v alued either \+1" or \-1", indicating either a p ositiv e or negativ e example, resp ectiv ely .
 Let X A be an n A b y D matrix represen ting the training set and X B be an n B b y D matrix represen ting the test set, where D is the dimensionalit y of the input space (i.e. D features) and n A and n B are the n um ber of training and 1 F or Euclidian distance, complexit y logarithmic in N is of-ten ac hiev ed for k-NN, using indexing sc hemes suc has k-d trees. Ho w ev er, for the general distance metrics emplo y ed within SVM k ernel metho ds [8, 3] sub-linear p erformance for k-NN's is not as ob viously ac hiev ed.

Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 test examples, resp ectiv ely . Let L A be a v ector of the n kno wn lab els for the training set and L B be a v ector of the n B actual (often unkno wn) lab els for the test set. Let y be a v ector of the n B lab el predictions of the automated classi er for the test set X B .
 The follo wing constrained quadratic optimization (QP) prob-lem is commonly used to train a SVM classi er: using notational simpli cations: N = n A , L = L A , and x is i-th example (ro w) in X A .
 The SVM prediction, for example x (v ectorofsizeD),is: where scalar b (bias) and v ector (of size N ) con tains the v ariables determined b y the ab o v e QP optimization. Th us, the test predictions y B are f ( x ), for eac h x in X B . K ( x i ;x j ) represen ts a kernel whic h implicitly pro jects t w o giv en examples from D dimensional input space in to some (p ossibly in nite) feature space. The simplest is the line ar k ernel, implemen ted as a simple dot pro duct: The p olynomial k ernel is de ned b y a non-linearly squashed dot-pro duct of the follo wing form: with p olynomial degree parameter d . V arying the con tin u-ous o set parameter r c hanges the relativ ew eigh ting of the (implicit) terms in the nonlinear p olynomial feature space. Supp ort ve ctors are those training example v ectors for whic h i &gt; 0. As can b e seen from the ab o v e summation used to generate predictions, a zero i means that the i-th training example do es not con tribute to the prediction. In SVM ap-plications often only 10% or less of the training examples b ecome supp orts. Suc h sparsit yis a k ey prop ert yof SVM's that helps them a v oid o v er tting noise. A general rule of th um b is that the exp ected test error of the SVM is prop or-tional to the ratio of the n um b er of supp ort v ectors to the n um b er of all training examples.
 P arameter C &gt; 0 de nes a soft mar gin | a trade-o be-t w een regularization (sparsit y of non-zero alphas) and train-ing errors. All misclassi ed training examples, for example, end up with alphas at C. An appropriate v alue for C is t yp-ically determined via cross v alidation. Existing SVM metho ds initialize all alpha ( )v alues for the QP optimization to 0. W e use the term alpha se e ding to refer to an ymethodwhic hpro vides initial estimates of the alpha v alues. W e restrict ourselv es to metho ds whic h start eac h SVM training with fe asible alphas (i.e. whic h satisfy the b ounds and the single equalit y constrain t), although it is conceiv able that infeasible seeds ma y sometimes b e use-ful for sp eci c SVM training algorithms. Sp eci cally , w e in v estigate seeding metho ds whic hmak e use of nal output alphas from one training to initialize a similar one. T o motiv ate our w ork and establish a framew ork, b elo ww e discuss a v ariet yof w a ys in whic h alpha seeding can b e used to impro v ev arious asp ects of SVM training. In Section 4, w e empirically explore some of these in more detail. All the tasks for whic hw ein tro duce alpha seeding metho ds can b e solv ed without seeding (i.e. just start eac h with zero alphas). Th us, alpha seeding o ers no new theoretical ad-v ance, as, sa y , a new t yp e of SVM k ernel migh t. Instead, the goal of alpha seeding is drastically faster con v ergence to the nal alpha v alues for the SVM problem(s) of in terest. Ho w ev er, it is imp ortan ttok eep in mind that resource allo-cation is almost alw a ys a concern in practice. F or example, if one can sp eed the SVM training for one k ernel or C v alue b y a factor of 10, one ma y b e able to searc h for the optimal of ten di eren tt yp es of k ernels (or Cv alues) in the same xed a v ailable o v erall training time.
 It is also useful to k eep in mind that all of these approac hes to alpha seeding can amortize the cost of k ernel computa-tions across the en tire set of of SVM trainings. Dot-pro duct cac hes are common ev en for single SVM trainings, as in most practical SVM trainers (e.g. [5]). Our alpha seeding tec h-niques exploit dot-pro duct cac hes ev en further, with later trainings often requiring no additional k ernel computations. When input dimensionalit y D is large, these sa vings can b e substan tial (t ypically more than 200% v ersus no cac he). A fundamen tal issue is ho w alphas from a previous training should b e adapted in to appropriate seeds for the next train-ing. As w e shall explore, there are t ypically m uc h more e ec-tiv e approac hes than simply passing the alphas unc hanged bet w een trainings.
 The k ey issue determining whether a giv en alpha seeding metho d is e ectiv e for a giv en task is, of course, whether the sum of the training costs o v er the sequence of successiv ely seeded SVM's is lo w er than the cost of directly training the non-seeded SVM of in terest. W e will explore that issue in Section 4, after rst discussing the v arious metho ds. One of the simplest and y et e ectiv e alpha seeding metho ds is for e X cien t LOOCV estimation of generalization error. LOOCV requires N SVM trainings, where the i-th SVM is tested on only the i-th example and is trained on the N 1 other examples. Unlik e other metho ds b elo w, eac hsuc h case is for xed parameters (e.g. for giv en C ,t yp e of k ernel, etc.). Doing m ultiple LOOCV's, for v arious parameter v alues, pro-vides a p opular empirical-based means of mo del selection. SVM theory pro vides estimates of the w orst case b ounds on the LOOCV error, suc h as the fraction of training examples whic h b ecome supp ort v ectors. Ho w ev er, since suc h b ounds are necessarily lo ose, it can be useful for accurate mo del selection to compute the actual LOOCV error, esp ecially if it can b e obtained e X cien tly .
 Our alpha seeding approac h to LOOCV is as follo ws. First, train the SVM for all N examples. Denote the resulting al-phas as . F or eac h of the examples ( i ) out of the full N , pretend in turn that that i-th one is not in the data set. 2 If i is already 0, then simply classify this i-th example as the full SVM do es (and record if it disagrees with L i ). Oth-erwise, initialize the N alphas ( ) to b e those of and set i to 0 (i.e. forget it). In that case, the equalit y constrain t T o re-establish the equalit y ,w em ust distribute that resid-ual to some of the other alphas. Finally , after training the i-th SVM from the so-adjusted alphas ,w e classify the i-th example (and record if it disagrees with L i ).
 W eha v e found that a simple and y et rather e ectiv e metho d is to redistribute the residual among all the in-b ound alphas (i.e. those greater than 0 and less than C). A k ey motiv a-tion is that mo dern SVM trainers tend to w ork on in-b ound alphas b efore re-examining at-b ound ones. This is b ecause generally once an alpha reac hes 0 or C it will tend to sta y there during the remainder of an SVM training.
 W e ha v e explored v arious sc hemes for redistributing the residual among the in-b ound alphas. One whic h routinely p erforms w ell, although not the b est in ev ery case, is to uni-formly add an equal p ortion of i to eac h in-b ound alpha j for whic h its corresp onding example j is in same class as the hold-out (i.e. same lab el L i ). That is, add i where z is the n um b er of other examples of that class with in-b ound alphas. The exception is that if this causes some alpha to reac h (i.e. w an t to exceed) the limit C ,thenan y remaining residual is (uniformly) redistributed among the remaining in-b ound alphas of that class, un til all residual is gone. W e call this sc heme uniform same-class r esidual r e distribution , and rep ort results with it in Section 4.1. A more complex alpha seeding metho d in v olv es training SVM's using successiv ely larger C v alues. It is commonly observ ed in SVM literature that larger C v alues tend to re-quire substan tially more training time than smaller v alues. Ho w ev er, w e theorized that initial training with a smaller C could quic kly iden tify appro ximate alpha w eigh ts whic h later trainings with larger C's w ould b e able to re ne. More precisely , let S = [ C 1 ;::: ;C n ] where C i &lt; C atrainingsc hedule that pro duces correct alpha w eigh ts for C , the target v alue of C. W e will refer to the training phase that uses C i as S i . The Gro wC approac h tak es the alphas pro duced at the end of S i and uses them as seeds for S i +1 The heart of an ysuc h strategy relies on determining an ef-fectiv esc hedule for gro wing C. Our goal in this w ork is to establish that go o d sc hedules do exist. W e defer in-depth study in to automatically pro ducing them to future w ork. Another k ey issue in v olv es adjustmen ts to the alphas be-2 F or e X ciency ,w e do not actually destro y the original data set, but instead ha v e re ned our SVM algorithms to allo w ignoring one selected example during the QP optimization. t w een training phases. When mo ving from S i to S i +1 ,the range of allo w able alpha v alues expands from [0 : ::C i [0 :::C i +1 ]. There are sev eral options a v ailable. The alphas from S i can be passed unc hanged to S i +1 . Alternativ ely , the S i alphas that are at C i can b e mo v ed to C i +1 . Athird alternativ e is to scale all of the alphas in to the new range. Lastly , a more complex (p ossibly adaptiv e) metho d could adjust only those alphas that are lik ely to mo v e from their S i v alues. In Section 4.2, w e compare the results of the rst three options empirically and demonstrate the imp or-tance of go o d c hoices for alpha adjustmen tbet w een training phases. Another natural use of alpha seeding is for successiv e cross v alidations o v er some range of settings for a k ernel param-eter. Previous w ork with Kernel Adatron SVM trainers [2] sho w ed this can b e e ectiv e, often not costing m uc hmore to train for a large n um b er of parameter v alues than for the rst one. Alpha seeds need not b e based on previous trainings of v ery similar SVM's. F or example, they could b e based on geo-metrical argumen ts for wh ya giv en example is lik elytobe supp ort v ector or not, or lik ely to b e at C (i.e. a noisy ex-ample). Guessing whic h examples will b e at 0 or C can b e particularly useful for man ySVM training metho ds, since suc h at-b ounds cases can often be ignored in man y itera-tions of those algorithms.
 A particular metho d in this area whic hw eha v e explored is training a SVM using a linear k ernel and then using those alphas to seed training a SVM for some target nonlinear k ernel. The in tuition is that for problems whic h are only sligh tly nonlinear, suc h seeds can b e v ery close to optimal for the nonlinear case as w ell. This idea is esp ecially ap-p ealing giv en the substan tial time sa vings p ossible for linear k ernels, due to the feasibilit y of folding all N alphas in to only D w eigh ts necessary to ev alute the SVM output for an y example in the linear sp ecial case. T o empirically explore alpha seeding, w e mo di ed t w o com-mon SVM algorithms, our implemen tation of SM O [7] (with impro v emen ts of [6]) and the freely a v ailable SV M lig ht [5]. Our mo di cations include taking seed alphas as argumen ts, instead of b eginning training from (default) zero alphas. F or our initial exp erimen ts to rep ort in this pap er w e se-lected the UCI Adult data set, since a fair amoun tof re-lated w ork with this data set has already been published using the SM O and SV M lig ht algorithms. In particular, for direct comparison w e used Platt's discretized v ersions, consisting of 123 binary input attributes and v arious sub-sets of the full set of 32562 [7]. All tests w ere p erformed on a 450Mhz Sun Ultra 60 w orkstation with 2 Gb of RAM. F or LOOCV tests, w e used the smallest subset Platt re-p orted on in his w ork [7], whic h consists of 1604 examples. Figure 1 (left) sho ws the cum ulativ e run times for standard SVM (zero alphas for eac h of the N LOOCV retrainings) and our uniform same-class residual redistribution LOOCV alpha seeding metho d (as describ ed in Section 3.1). Our metho d w as nearly 5 times faster (1733 vs 380 secs). The training time for full data set w as 2.86 secs and re-sulted in 714 outof1604 examples b eing supp ort v ectors. The LOOCV training for eac h of the 714 hold-outs whic h w ere supp ort v ectors eac h to ok roughly the amoun tof time as that for full training: mean 2.943 secs, standard deviation .2923 secs, maxim um 4.51 secs, and minim um 2.24 secs. Us-ing our alpha seeding, training times for the supp ort v ector hold-outs w ere faster: mean 0.6452 secs, standard deviation .2245, maxim um 1.54 secs, and minim um 0.22secs.
 Both metho ds, of course, computed the same LOOCV er-ror rate (16.55%), since their only di erence is in sp eed of con v ergence. It is in teresting to con rm that this rate is far belo wthe(w ell-kno wn to b e lo ose) LOOCV error estimate b ounds (44.51%) that the standard ratio of supp ort v ectors divided b y the n um b er of examples w ould suggest. Figure 1 (righ t) illustrates wh y our metho d p erformed m uc h b etter than a standard non-seeded metho d. It plots all N training times, sorted from smallest to largest for b oth meth-ods. LOOCV for the 890 non-supp ort v ector examples re-quires no retraining, indicated b y a ma jorit y of zero train-ing times. F or the 714 supp ort v ector examples, there is substan tially more area under the curv e using zero seeds v ersus redistribution-based seeds. Most zero-seed trainings required roughly equal time | ab out the same as the initial training (2.86 secs). Eac h of the 714 redistribution-seeded trainings w ere faster (slo w est w as 1.54 secs). Left : Cum ulativ e time (y-axis) o v er N = 1604 LOOCV trainings (x-axis). Higher curv e is standard SVM (zero alpha seeds). Lo w er curv e is our alpha seeding metho d (Section 3.1). F or linear k ernel, with C=1, for UCI Adult1 data set. Righ t : sorted times for alpha seeds of zeros (top) vs redistribution-based (b ottom). Note di eren t y ranges. F or b oth our mo di ed SM O and SV M lig ht algorithms, w e exp erimen ted with sev eral sc hedules for gradually gro wing C. In general, w e observ ed that alpha seeding obtained dra-matic reductions in total run time for b oth algorithms. The Adult data set w e used for these exp erimen ts is referred to as \Adult small" in [7], consisting of 11221 training examples. W eha v ev eri ed that the n um ber of b ound and in-b ound alphas w e obtain corresp ond to those rep orted b y Platt. All runs used a linear k ernel and run times are a v eraged o v er v e trials. W e also made use of the cac he that stores k ernel computations, so that they need not b e recomputed. This cac he p ersists o v er eac h training phase S i (after the rst in a sequence of trainings), to mak e it comparable to training from scratc h (where the cac he is a v ailable throughout the course of training).
 Section 3.2 outlined four options for ho w to seed S i +1 the results of S i . Belo w rep orts ho w the rst three p erform.
Figure 2: Gro wC times ( SM O , SV M lig ht ): seedings. Using successiv ely larger v alues of C and seeding eac h it-eration with the alphas found at the end of the previous one do es not alw a ys yield run time b ene ts, as sho wn in Fig-ure 2. F orCv alues less than 0.3 for SM O and for al l tested Cv alues for SV M lig ht , it is actually more exp ensiv eto use this form of alpha seeding than to pro ceed from scratc h. A smaller C i restricts whic h p ossible alpha v alues are explored, th us limiting the initial run time. But when these alphas are used as seeds for S i +1 with a larger C i +1 , a lot of time can b e sp en t adjusting them gradually in to the larger range. This is esp ecially true for alphas that are at C i at the end of S |itis lik ely that they will end up b eing at C i +1 at the end of S i +1 , but it ma y tak e a long time to push them that far. This observ ation leads naturally to the second option: at the end of S i ,c hange all alphas that ha v eav alue of C \b ound" alphas) to the new C i +1 directly . The fact that an alpha is b ound in S i often indicates that it will b e b ound in S i +1 . If so, a lot of time can b e sa v ed b y immediately jump-ing to the new b oundary v alue, C i +1 . Figure 2 sho ws that this impro v es run time for SM O o v er Direct Alpha Reuse, but can still (for C less than 0.1) b e more exp ensiv ethan training from scratc h. Similar trends app ear for SV M lig ht . Our next option is to scale eac h alpha v alue pro duced b y S in to the new range allo w ed in S i +1 . This is accomplished b y m ultiplying eac h alpha v alue b y C i +1 sending all alphas at C i to the new v alue C i +1 and spread-ing the rest of the in-b ound alphas in to the new range. In addition, it k eeps zero-v alued alphas at 0. As sho wn in Fig-ure 2, this strategy ac hiev es the greatest impro v emen ts in run time. T raining SM O from scratc hfor C =1 : 0 requires ab out 175 seconds. Scaling All Alphas requires just 19 sec-onds, a sa vings of 89% of the total run time. F or SV M lig ht , training from scratc h requires 120 seconds, but Scaling All Alphas requires only 49 seconds (59% sa vings). As noted ab o v e, the c hoice of sc hedule S impacts the e ec-tiv eness of alpha seeding. The seeding results in Figure 2 w ere all pro duced using sc hedule S 1 =[0 : 01 ; 0 : 05 ; 0 : 1 ; 0 : 3 ; 0 : 5 ; 1 : 0], whic hw as exp erimen tally determined to w ork w ell with the Adult data. Exp erimen ts with other sc hed-ules indicate that more graduations tend to yield greater o v erall b ene ts for SM O , but the rev erse trend app ears for SV M lig ht . F urther in v estigation is required to fully under-stand what strategies for constructing training sequences are of most use to eac h algorithm.
 Clearly ,in telligen t adjustmen ts to alphas b et w een training phases are essen tial. It is p ossible that b etter alpha adjust-men t strategies could result in ev en larger run time impro v e-men ts for alpha seeding. In addition, these results w ere all gained while using a linear k ernel; other k ernel t yp es ma y require di eren t alpha seeding (or C sc heduling) strategies. Our results demonstrate signi can t impro v emen ts in p erfor-mance for SM O for C v alues less than or equal to 1.0. Most of those C v alues are accompanied b y a similar impro v emen t for SV M lig ht . Ho w ev er, it is not usually p ossible to predict ahead of time what a go o d C v alue will b e for a problem. Therefore, go o d p erformance o v er a v ariet yof C v alues is imp ortan t. In particular, large C v alues ha v e b een a c hal-lenge for SVM algorithms. In separate exp erimen ts, w ew ere able to train on the Adult data with a C of 500 in under 85 seconds. 3 It to ok SV M lig ht and SM O o v er 10 min utes to train with suc h a large C. 4 Clearly , alpha seeding reduces these previously computationally-exp ensiv e trainings to rea-sonable durations.
 Another b ene t of using a seeding approac his thatit can signi can tly reduce the time required to nd a go o d v alue for C on a new data set. Instead of p erforming a series of trainings, all from scratc h, with v arious v alues of C, w e instead obtain m ultiple results together, b y using a training sequence that con tains man yC v alues of in terest. The SVM pro duced for eac hin termediate C i can b e used to compute a test set error, selecting the v alue giving lo w est test error. Our results suggest that alpha seeding is a promising w a y for sp eeding up SVM training. Although our sp eedups are often essen tially constan t ones, these factors are often m uc h larger than the impact of other recen tly published metho ds for sp eeding up SVM's (e.g. bias in terv als in [6] and \shrinking" in [5]). Th us, they are of signi can t practical imp ortance. There are man y directions for future w ork. One is to un-derstand the nature of the b est alpha seedings b etter, to-w ard sp eedups that are t ypically more than nearly-constan t ones (ideally , with amortized linear time cost for eac hSVM training). Another is to understand sensitivit y issues, suc h as ho w close to the nal v alues the alpha seeds ha v eto be, for signi can t sp eedup gains to be realized. Y et another is to dev elop means for automatically nding go o d gro wth sc hedules for an ygiv en task, for our Gro wC metho d. 1 : 0 ; 3 : 0 ; 5 : 0 ; 10 ; 15 ; 20 ; 30 ; 50 ; 100 ; 500]. 4 W e terminated the training for eac h one at that p oin t. W e also plan to con trast our e X cien t LOOCV alpha seeding approac h with Lea v e-One-Out SVM's (LOOSVM's, [11]). Empirical studies of the computational costs of LOOSVM's are not y et a v ailable, so it is unclear when eac h is most e ectiv e | explicit selection from a set of C v alues as in our case v ersus folding the searc h for C within the optimization problem (as in LOOSVM's). This researc hw as carried out b y the Jet Propulsion Lab ora-tory , California Institute of T ec hnology ,undercon tract with the National Aeronautics and Space Administration. [1] C. Burges. A tutorial on supp ort v ector mac hines for [2] N. Cristianini, C. Campb ell, and J. Sha w e-T a ylor. [3] Dennis DeCoste and Mic hael Burl.
 [4] Isab elle Guy on. Online SVM application list. (See [5] T. Joac hims. Making large-scale supp ort v ector [6] S.S. Keerthi, S.K. Shev ade, C. Bhattac haryy a, and [7] John Platt. F ast training of supp ort v ector mac hines [8] B. Sc h X  o lk opf, A. Smola, and K.R. M  X  u ller. Nonlinear [9] B. Sc ho elk opf, C. Burges, and A. Smola. A dvanc es in [10] V. V apnik. The Natur e of Statistic al L e arning The ory . [11] Jason W eston. Lea v e-on-out supp ort v ector mac hines.
