 1. Introduction
Opinion extraction research is a promising field for public opinion and reputation analysis using mass media or word-of-mouth sources on the web. The relevant research is divided into three subcategories. The first subcategory is opinion detection, which involves detection of opinionated documents and sentence/phrase extraction (Wiebe, Wilson, Bruce, Bell, and positive, neutral, or negative sentence/phrase detection (Wilson, Wiebe, &amp; Hoffmann, 2005 ) The third subcategory is 2006a, 2006b ). Opinion holder identification research is important because news articles contain many opinions from differ-ent opinion holders. By grouping opinion holders, we can discriminate between the opinions that are viewed from different perspectives and bridge cultural gaps between groups, organizations, and countries.
 ditional random fields (CRFs) with features from part-of-speech information, such as nouns (for opinion holders), and from syntactic dependency information on the semantic classes (for opinionated phrases). For opinion holder annotation, Choi et 0.5. However, these researchers have only slightly focused on the inexplicit opinion holder elements, such as the anaphoric elements within a document or the exophoric elements 2 , such as the authors.
The first Opinion Analysis Pilot Task was conducted (Seki et al., 2007 ) at the sixth NTCIR (NII Test Collection for IR Sys-tems) Workshop in 2006 X 2007. 3 In the NTCIR-6 corpus, the opinion holder identification task was slightly more challenging but also the inexplicit noun phrases, such as the anaphoric elements within a document, or the exophoric elements, such as 2005) participated in this task as members of the Cornell team and they only attained F -values of 0.222 based on lenient standards, as shown in Table 6 in Section 3.3.

We propose a new method for the identification of opinion holders based on the differentiation between the author and authority viewpoints. Here,  X  X  X uthor X  means the writer of the document, and  X  X  X uthority X  means the third parties (not only who have the same opinions. Other opinion mining applications, such as the reputation analysis from blogs or review sites, also seek to find all the opinion units.

Previous research studies have tackled opinion holder identification using explicit clues and sequences related to opinion phrases or holders. They have focused very little on the discrimination between the author and authority viewpoints, be-cause of a lack of genre-independent author clues. To differentiate between the author and authority viewpoints, we focused  X  X  X ust X ) tend to be used in author-opinionated sentences, but not to be used in authority-opinionated sentences. This ap-proach is applicable to a wide variety of genres and multilingualy. We implemented two types of opinionated sentence clas-sifiers, for Japanese and English. This paper is organized as follows. In Section 2, we provide an overview of the NTCIR-6 uation results, and the postsubmission analysis of the opinion types. In Sections 4 and 5 , we give the revised approach and the comparison experiments. Finally, we present our conclusion in Section 6. 2. NTCIR-6 opinion analysis pilot task 2.1. Task and annotation overview
The opinion extraction task was conducted in Japanese, English, and Chinese. For opinion extraction, the participants submitted two mandatory results: opinionated sentence extraction and opinion holder identification; and two optional results: relevant sentence judgment and polarity judgment. Six teams in English, three in Japanese, and five in Chinese (14 teams in total) submitted 21 runs. The test collection sizes for Japanese and English, which used in this paper, are shown in Table 1 .
 Opinionated sentences and opinion holders in sentences with three opinion types were annotated using three assessors in
Japanese and English. One sample topic was used for the session to improve the agreement between the assessors. The Kappa coefficients between two assessors are shown in Table 2 .

For opinionated information annotation, we decided to annotate the following sentences as  X  X  X ot opinionated X  from the session results.
 Indirect hearsay evidence or opinions held by the general public were regarded as general facts.
Public announcements from the government/nation or declarative pronouncements from an organization were regarded as plans. 2.2. Evaluation methodology
The evaluation was based on the precision, recall, and F -measure values obtained for the numbers of opinionated sen-tences and opinion holders. These measures were defined as follows:
The correct answer set was defined using two standards for the agreements between the assessors. The lenient standard was based on an agreement upon the opinionated sentences between two out of three assessors. The strict standard was based on an agreement upon the opinionated sentences for three out of three assessors. The population parameters for the precision and recall values were computed from the total number of sentences assessed. We applied a sentence-based evaluation to evaluate the opinion holders. If multiple opinion holders existed in one sentence and the system detected one of them, then we regarded the system X  X  identification as valid.

In addition, we used the following five-grade evaluation of the matching of the system X  X  and the assessor X  X  detection based on leniently or strictly agreed upon opinionated sentences. (1) Matched semantically, and the strings matched almost completely. (2) Matched semantically, and the strings matched partially, but a proper name was not detected. (3) Matched semantically, but strings were not matched. (4) Matched partially in some respect, but a proper name could not be specified. (5) Did not match.

We counted the results using grades 1 X 3 for valid identifications, and computed the precision, recall, and F -measure values. 3. Our approach: opinion holder extraction based on author X  X  opinions and authority opinion extraction
To identify the author and authority opinion holders, we constructed the three-step system shown in Fig. 1 . In this sec-tion, we describe the author-and authority-opinionated sentence classification, holder identification rules, and evaluation ducted to investigate the effectiveness of our approach.
 3.1. Opinionated sentence classification using the author and authority viewpoints (in NTCIR-6 formal run)
An automatic opinionated sentence classification using author and authority viewpoints was implemented in terms of two types of opinionated sentence estimation using a support vector machine (SVM).
For the Japanese training data, we used four sample topics (Topic IDs: 001, 002, 003, and 026) in the NTCIR-6 Opinion Cor-pus , which was explained in Section 2. To discriminate between the author and authority viewpoints, we annotated the fol-lowing three opinion types (Wiebe, Wilson, and Cardie, 2005 ). (1) Explicit mentions of private states by a person, nation, or organization: (2) Speech events expressing private states by an agent: (3) Expressive subjective elements:
For training data for the author and authority viewpoints, we used this annotation information in Japanese. If the opin-ionated sentence was a type 3 opinion and contained its holder (an agent expressing expressive subjective elements), we regarded it as having the opinion-from-author viewpoint, because authors in Japanese tend to express their opinion without viewpoint.

For the English training data, we used the MPQA Corpus (Wiebe et al., 2006 ). An opinionated sentence was defined in terms of its strength, i.e. extreme , high , and middle in the following way.
 The sentence contains a  X  X  X ATE_direct-subjective X  annotation WITH attribute intensity NOT IN [ X  low  X ,  X  neutral  X  X  AND NOT WITH attribute insubstantial.
 The sentence contains a  X  X  X ATE_expressive-subjectivity X  annotation WITH attribute intensity NOT IN [ X  low  X  X .
For the author and authority viewpoints, we discriminated between opinionated sentence types using  X  X  X ested source X  point. Otherwise, we regarded it as having the opinion-from-authority viewpoint.

The feature set for the opinionated sentence classification was as follows: 155 (author) and 569 (authority) syntactic pairs (40 were shared) of grammatical subjects and predicates were used in
Japanese.  X  Subjects were categorized using named entities , semantic primitives , and key terms such as pronouns.  X  Predicates were categorized using the semantic primitives from a thesaurus, namely Bunrui-Goi-Hyou (National 565 (author) and 376 (authority) syntactic pairs (142 of which were shared) of the syntactic patterns such as nouns and adjectives/verbs were used in English. (Note: The features in English will be discussed in Section 4.2).
Syntactic dependency was checked using Cabocha (Kudo&amp; Matsumoto, 2002 ) in Japanese and Minipar (Lin, 2005 ) in Eng-marker. The features in English will be discussed in Section 4.2. They were specified from the analysis of the sample data (four topics) in the NTCIR-6 Opinion Corpus and the entire data of the MPQA Corpus (Wiebe et al., 2006 ).
In addition, we utilized the following features for the opinionated sentence extraction in Japanese, based on the observa-ated sentence extraction in English. (1) The number of sentences in the document was defined as a feature. (2) Sentence-based statistical features: (a) the sentence position in the document, and (b) the sentence length were (3) Sentence-based term frequency features: (c) the number of heading words in the sentence, (d) the number of words (4) Approximately 20 types of semantic primitives for the predicates extracted using the Bunrui-Goi-Hyou (National Insti-(5) The frequencies of approximately 40 types of keywords for the author and authority opinions were defined as features.
The training data and features explained in this section are summarized in Table 4 . Note that we discriminated between opinion types in the training data in Japanese, but not in English. 3.2. Opinion holder identification based on author and authority opinionated sentences
The authority opinion holder was identified by using a named entity extraction approach; that is, the distinction between the author holder and other authority holder was made by using the author and authority opinionated sentence discussed in
Section 3.1. For Japanese, the author X  X  name was determined and identified from the signatures.
To determine the authority holder elements, we set four grades for priority rules using the following three named entity elements: (1) Bracketed elements of person, organization, and location (prioritized in this order here and in 2, 3, and 4) in the (2) Grammatical subject elements of person, organization, and location in the sentence. (3) Grammatical subject elements of person, organization, and location in the previous sentences. (4) Person, organization, and location elements in the sentences other than those classified by (1) or (2). 3.3. Evaluation results in NTCIR-6 an abbreviation of the  X  X  X oyohashi University of Technology X . The letters represented the team identification (IDs) such as
University). If we needed to differentiate between various submission run IDs from the same team, the sequential number resulting F -values of TUT (our team) for the opinionated sentence extraction and opinion holder identification were reason-able for Japanese, but were not as good for English.

The opinion holder extraction methods of the participants were mainly categorized into three classes: (1) extraction using term sequences (Cornell, GATE), (2) lexicon-based heuristics (IIT), and (3) named entity extraction approach (TUT and other teams).

As we discussed in Section 1, the correct answer set of the opinion holders in the NTCIR-6 Opinion Analysis Pilot Task con-tained holder elements that referred to anaphoric or exophoric elements, such as the authors. To differentiate between the author and authority viewpoints, we focused on the writing style differences between the author-and authority-opinionated sentences. The participants took two different approaches from ours to identify the author opinion holders: (1) to utilize author-related clues such as verbs (ICU-IR) or (2) to detect author opinion holders when there were no holder candidates surrounding the opinionated sentences (EHBN, Cornell). Unlike our approach, the former approach must define explicit author clues and the latter may miss author opinion holders when there were incorrect opinion holder candidates surround-ing the opinionated sentences. 3.4. Evaluation results for author and authority opinionated sentence classification in NTCIR-6 submission results
We evaluated the author and authority classification results of our submission to the NTCIR-6 Opinion Analysis Pilot Task by opinion types, as shown in Table 7 .

From this table, we observed that: in Japanese, although author-and authority-opinionated sentence estimation was still not straightforwardly matched from the estimation of the opinion types, our system classified the author-opinionated sentences with a precision of 0.802 (for opinion type 3) and the authority-opinionated sentences with 0.976 and 0.998 (for opinion types 1 and 2, respectively), based on lenient standards; and in English, the opinion types difference was not utilized for clues in the training stage. The precision of the author-opin-ionated sentences with opinion type 3 was only 0.467. 3.5. Author-and authority-opinionated sentence distribution using opinion types in the NTCIR-6 and MPQA opinion corpuses
We also conducted a postsubmission analysis of the opinion holder identification to investigate the effectiveness of our approach. The author and authority distributions in the NTCIR-6 Opinion Corpus are shown in Table 8 , using three opinion types. The opinion types were defined from the discussion in Section 3.1. From this table, we found the following (see Table 9): In Japanese, the majority of elements with opinion types 1 and 2 often appeared as authority-opinionated sentences.
Sentences with opinion type 3, i.e. an agent expressing expressive subjective elements, usually appeared as author-opin-ionated sentences, and sometimes as authority-opinionated sentences.

In English, the tendency for opinion types 1 and 2 is similar to that in Japanese, but author-opinionated sentences appeared more often than in Japanese.

We also analyzed the MPQA Corpus using the opinion types. Following Wiebe et al. (2005) , the  X  X  X irect-subjective X  (cor-responding to opinion types 1 and 2) and  X  X  X xpressive-subjectivity X  (corresponding to opinion type 3) information was anno-tated to this corpus in phrasal units. We aligned this information to the sentence units (13,830 phrases to 11,112 sentences in 535 documents) and counted the distribution of the author-and authority-opinionated sentences by normalizing the weights to a unit weight per sentence. 7 We defined the sentences whose values of  X  X  X ested-sources X  attributes were nonn-ested  X  X  X  X  as author opinionated, similar to that in Section 3.1. From this table, we often found that the author also appeared as an opinion type 3 in English, which is similar to the
Japanese distribution of the NTCIR-6 Opinion Corpus in Table 8 . The main reason for the slightly different distribution in ated sentences in the MPQA Corpus analysis. 4. Revised approach based on postsubmission discussion
To investigate the effectiveness of our approach, we revised it, and conducted comparison experiments with other systems. 4.1. Opinion holder identification from authority opinion: revised approach Based on the results of NTCIR-6, we followed and extended the authority opinion holder extraction approach used by the
ICU-IR team ( Kim &amp; Myaeng, 2007 ) shown in Table 6 for the English side. We implemented the following opinion holder extraction rules: (1) We extracted the noun phrases that were followed by  X  X  X ccording to X . (3) We extracted the noun phrases that were followed by the word  X  X  X y X . (4) We extracted the phrases that were governed by the word  X  X  X y X . (5) We extracted the subjects governed by opinion verbs using lexicons (Wilson et al., 2005 ) and several communicative (6) We extracted the interviewer or interviewee markers using heuristic rules. (7) We extracted the  X  X  X erson X  elements from the sentence using a named entity tagger OAK. 4.2. Investigation of effective features to classify author and authority opinion sentences
As explained in Section 3.1, we used syntactic pairs following the five syntactic patterns in the NTCIR-6 formal run. We auxiliary verb &amp; verb) recognized by using Minipar (Lin, 2005 ) were effective in classifying the author and authority opinions.

We also investigated the effectiveness of the five additional features in English cases, which were used in Japanese cases, as explained in Section 3.1. Throughout the experiments, we found that (1)  X  X  X he number of sentences in the document X  and term types). These features were determined using adjective entries (Hatzivassiloglou &amp; Wiebe, 2000 ), which contained 1914 word entries, and the General Inquirer (Stone, 2000 ), which contained 1168 word entries. (5) Keyword list features 10. We clarified the entries in Table 10 , as follows: These features were selected based on v -square tests on the MPQA corpus.

The opinion verb types and the verb elements of syntactic pairs were defined by generalization using (A) communicative (Wilson et al., 2005 ) and Minipar ( Lin, 2005 ).

The grammatical subject elements in syntactic pairs were generalized with (C) ZeroProN (in case they were missing), (D) named entity types, such as GPE or PERCENT , (E) case-sensitive pronouns, and ( F ) parts-of-speech with regard to Minipar.
We used three count features: cntopnoun , cntopadj , and cntopadv that represented the numbers of the respective subjec-
We restricted the shared features between author and authority opinion extraction to only a few, compared with the approach in NTCIR-6 formal run : four syntactic pairs (grammatical subjects &amp; verbs) and two subjective verb types only.
From the analysis in Section 3.4, we conclude that our system did not classify the author and authority opinionated sen-improve our author and authority classifier by filtering the author-opinionated sentences with a direct-subjective (opinion types 1 and 2) classifier.

We implemented the direct-subjective classifier by analyzing the MPQA Opinion Corpus . This classifier was implemented bined with part-of-speech information. Polarity categories were also extended by adding appraisal attributes ( Bloom et al., 2006).

The new author and authority classification results obtained after filtering the author-opinionated sentences to the 3.4, all F -values were improved. From this result, we can tentatively conclude that the author and authority classification results were, to some extent, improved by using the direct-subjective classifier. 5. Comparison experiments based on revised approach 5.1. Comparison experiments
To investigate the improvements based on the revision in Section 4, we evaluated our revised approach for opinion holder extraction. For comparison, we implemented a baseline system based on opinionated sentences that did not differentiate between the author and authority opinion and the holder extraction algorithm from authority opinion, which was explained sentences in the MPQA corpus , which is in the same way as in Section 4.2. We also used three participant system results for the comparison, namely ICU-IR (the best system in Table 6 (Kim &amp; Myaeng, 2007 )), IIT (the second best system in Table 6 5.2. Results and discussion We presented the evaluations ( F -values) on opinionated sentence extraction and opinion holder identification in Table 13 . We inferred the following:
The revised approach was improved significantly over the baseline system, not only for the opinion holder identification, but also for the opinionated sentence extraction.

This result was also comparable with the best system in the NTCIR-6 formal run, especially for macro averaged values over all the topics. Note that this result was a postsubmission-revised approach.

We found that seven topics contained more than 30% of author-opinionated sentences of the twelve good-performance topics that attained higher F -values for the opinion holder identification than other comparative systems. 6. Conclusions
We have proposed an opinion holder identification system using the author and authority viewpoints in both Japanese and English. We participated in the NTCIR-6 Opinion Analysis Pilot Task and evaluated the effectiveness of our system. The results show that our system performed fairly well with respect to Japanese documents. We also found that our system per-formed less well with respect to the English documents in NTCIR-6, but that it could be improved by revising opinion holder ated sentences based on v -square tests, and filtering the author-opinionated sentences to the authority-opinionated sen-tences using a direct-subjective classifier.
 In future work, we plan to apply our author and authority classification approach to the analysis of public opinion (e.g. would seek the correct number of sentiments expressed by the texts in order to estimate the number of people who have the same opinion.
 Acknowledgements This work was partially supported by Grants-in-Aid for Young Scientists (B) (#18700241) from the Ministry of Education, Culture, Sports, Science, and Technology, Japan. We appreciate the valuable efforts of all the participants involved in the NTCIR-6 Opinion Analysis Task .
 References
