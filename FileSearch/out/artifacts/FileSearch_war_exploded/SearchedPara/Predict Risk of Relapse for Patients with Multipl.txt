 Depression is a serious mood disorder afflicting millions of people around the globe. Medications of different types and with different effects on neural activity have been developed for its treatments during the past few decades. Due to the heterogeneity of the disorder, many patients cannot achieve symptomatic remission from a single clinical trial. Instead they need multiple clinical trials to achieve remission, re-sulting in a multiple stage treatment pattern. Furthermore those who indeed achieve symptom remission are still faced with substantial risk of relapse. One promising approach to predicting the risk of relapse is censored regression. Tradi-tional censored regression typically applies only to situations in which the exact time of event of interest is known. How-ever, follow-up studies that track the patients X  relapse status can only provide an interval of time during which relapse occurs. The exact time of relapse is usually unknown. In this paper, we present a censored regression approach with a truncated l 1 loss function that can handle the uncertainty of relapse time. Based on this general loss function, we de-velop a gradient boosting algorithm and a stochastic dual coordinate ascent algorithm when the hypothesis in the loss function is represented as (1) an ensemble of decision trees and (2) a linear combination of covariates, respectively. As an extension of our linear model, a multi-stage linear ap-proach is further proposed to harness the data collected from multiple stages of treatment. We evaluate the proposed algo-rithms using a real-world clinical trial dataset. Results show that our methods outperform the well-known Cox propor-tional hazard model. In addition, the risk factors identified by our multi-stage linear model not only corroborate find-ings from recent research but also yield some new insights into how to develop effective measures for prevention of re-lapse among patients after their initial remission from the acute treatment stage.
 Major depressive disorder, relapse, censored regression, sur-vival analysis
Depression, clinically called Major Depressive Disorder (MDD), is a mood disorder that affects about one eighth of population in US [7] and an estimated 350 million people globally and is projected on track to be the second lead-ing cause of disability in the world by the year 2020 [11]. Medications of different types, such as selective serotonin reuptake inhibitors (SSRIs) and serotonin norepinephrine reuptake inhibitors (SNRIs), which are based on different biological mechanisms and have different effects on neural activity have been developed and tested in a number of clin-ical trials during the past few decades. Typically, a clinical trial on one medication for depression lasts somewhere from one to four months during which the antidepressant under study is vigorously dosed to tolerance with the goal of symp-tom remission due to its implications of better daily func-tion[19][8]. However, the interplay of multiple factors such as patients X  gene expression profile [5], chronicity of depres-sion [1], psychiatric and general heath comorbidities [16], intolerable side effects from medications, etc., makes many patients with MDD unlikely to respond to certain types of treatment. Thus they are unlikely to achieve remission with a single trial. For these patients, several sequential treat-ment stages are often necessary to obtain remission.
Another problem concerning the treatment of MDD is the potential risk of relapse among those who indeed achieve re-mission with one or several stages of treatment. Keller et al. [12] pointed out that there is a substantial probability of prompt relapse among patients without bipolar disorders who recovered from their first major depressive episode and should they relapse, they have an approximately 20% chance of remaining chronically depressed. Findings from [15] in-dicated that patients who achieved remission of MDD af-ter treatment with citalopram still continued to experience residual symptoms which put them at a higher risk of re-lapse in a 12-month follow-up phase. Interestingly, the risk of relapse seems to be inversely proportional to the survival time after remission. For instance, Ramana et al. [18] found that all the relapses of subjects that participated in their study occurred within 10 months after they achieved remis-sion. According to the results reported in [13], relapse occurs within four weeks for 12% of patients with remission. And it takes eight more weeks for the number to double. Thus it is of crucial importance to accurately predict the risk of relapse of MDD patients after their remission, especially those who are likely to relapse shortly after their remission as there is evidence showing that putting remitted patients under continuation and maintenance therapy would greatly reduce their risk of relapse [23] [17].

An example of clinical trial to treat depression involv-ing both multiple stages of treatment aimed at achieving symptom remission and a follow-up phase evaluating the long-term treatment outcome is the Sequenced Treatment Alternatives to Relieve Depression (STAR*D) 1 trial. It in-volves over 4,000 outpatients with nonpsychotic MDD from a broad spectrum of social demography. It consists of four sequential acute treatment stages. Patients who did not achieve remission or suffered from intolerable side effects of medications in one treatment stage were encouraged to go to the next stage. Those who had achieved remission or shown significant symptomatic improvement could en-ter a 12-month naturalistic follow-up phase. In the follow-up phase, the measurements concerning patients X  depressive symptoms were made on a monthly basis. However, in both acute treatment stages and follow-up, patients may miss cer-tain scheduled clinic visits or drop out, resulting in different patterns of missingness.

One class of the widely used methods that can be po-tentially employed for building models to predict the risk of relapse is survival analysis, e.g., Cox proportional haz-ards model [4]. However, traditional methods for survival analysis usually assume that for uncensored cases (e.g., sub-jects that relapsed), the exact time when the event of in-terest (e.g., relapse) occurs is known, which is not the case in STAR*D or for data collected through clinical trials of depression in general. In STAR*D, for instance, if a sub-ject does not relapse at the 5th month but is observed to have relapsed at the 6th month, we only know for certainty that the relapse occurs somewhere between the 5th month and the 6th month. Another issue with traditional survival analysis methods is that they apply only to the situation where all the subjects have the same covariates. They can-not be readily used for solving the problem in which certain patients have covariates from more treatment stages while others have less. An alternative is to build a sequence of dependent predictive models, one model for each time point with the dependency formulated through an explicit or im-plicit constraint that if a model built for a later time point predicts that the event of interest does not occur for a sub-ject, the models built for all the earlier time points should predict the same. However, such methods usually result in very complicated optimization problems to solve or have to resort to some kind of approximation [3].

In this paper, we present a censored regression approach to predict the risk of relapse based on information collected from patients X  acute treatment stages and their enrollment. Specifically, we employ a truncated l 1 loss to model the re-sponses (patients X  time of relapse) that are upper bounded and/or lower bounded. Based on this basic loss function, we consider the hypothesis that can be represented as (1) an ensemble of decision trees, and (2) a linear combination of covariates. For the hypothesis that takes the form of an ensemble of decision trees, we develop a gradient boosting approach to learn the base models and combination coeffi-cients. When the hypothesis takes the form of a linear com-http://www.nimh.nih.gov/funding/clinical-research/practical/stard/index.shtml bination of covariates, we develop a stochastic dual coordi-nate ascent algorithm, which is the state-of-the-art method for solving large-scale machine learning problems with a con-vex loss function [24] with a guaranteed fast convergence rate [20]. Furthermore, we assume that the treatment stage varying covariates collected on patients around the same time before they entered follow-up study should share some commonalities in terms of their relative contribution to pre-dicted risk of relapse and, at the same time covariates col-lected from different stages overall, contribute differently to the prediction. Based on this assumption, we propose a multi-stage linear approach that can simultaneously esti-mate multiple linear models for patients remitted after dif-ferent numbers of treatment stages. Based on data collected from STAR*D trial, we generated several datasets by se-lecting different cut-off points. Our results show that our proposed methods consistently outperform the Cox model on all datasets.
 Our major contributions in this paper are as follows: (1) We present a truncated l 1 loss based censored regression ap-proach to deal with uncertainties of responses. (2) We de-velop an efficient gradient boosting algorithm and stochas-tic dual coordinate descent algorithm to solve the proposed formulation. (3) Based on the linear model, we further pro-pose a multi-stage linear approach that can deal with covari-ates collected from different numbers of treatment stages. (4) We conduct experiments on both synthetic datasets and STAR*D to evaluate the effectiveness of our methods. (5) We identify risk factors which might provide some new in-sights into development of more effective therapies for pre-vention of relapse.
In recent literature, different types of loss functions have been proposed to handle censored data under different appli-cation scenarios. For instance, in [10], the prediction of time of occurrence of stroke among subjects was modeled through the Huber loss which basically enforces the predicted time of the uncensored subjects to be the same as the observed time. In [21], the loss function for learning from censored targets was absolute deviation of predicted value away from its tar-get interval. In our situation, the assessments of depressive status of all the patients were made on a monthly basis dur-ing the follow-up phase, which means that when a patient was regarded as having relapsed at the time of assessment, we only know for sure that relapse of the patient occurred at or before the time when the assessment was made. For this reason, the truncated l 1 loss, which includes the loss function used in [21] as a special case, is proposed to model the relapse risk of patients in the STAR*D cohort.

Suppose there are a total of n samples D = { x 1 ,  X  X  X  ,x Let S l be the set of indices of samples whose responses are bounded from below by some values, i.e. S l = { i | a i  X  l and S u be the set of indices of samples whose responses are bounded from above by some values, i.e. S u = { i | a i  X  u where a i is the real unknown response of the i -th sample; l its observed lower bound and u i is its observed upper bound. The real unknown responses are both upper bounded and lower bounded for uncensored cases and only lower bounded for those censored cases.
We formulate our censored regression with truncated l loss as the following optimization problem: arg min = arg min where ( z ) + = max(0 ,z ) ,  X  z  X  R ; F ( x i ) gives an estimate of the response for the i -th sample x i ; l , u are vectors com-prising of the lower and upper bounds for all the samples, respectively;  X   X  (0 , 1) is a pre-specified constant that bal-ances the tradeoff between the two terms.

Intuitively, if the response of an instance x i is lower bounded by l i , we would like the predicted response F ( x greater than or equal to l i . Otherwise, a penalty would be imposed. This works similarly if its response is upper bounded. No penalty is incurred if F ( x i )  X  [ l i ,u i
Although in practice, the response of an instance x both lower bounded and upper bounded if it is an uncen-sored case, in the following, to simplify our discussion, we simply replicate such instances with one for lower bound in-dex set S l and one for upper bound index set S u such that S  X  X  u =  X  and denote the new dataset as X . Furthermore, we use one vector c to denote the union of l and u such that c = l i if i  X  S l and c i = u i if i  X  S u . Thus, the resulting loss can be written as arg min = arg min where N = |S l | + |S u | . Note that replicating instances serves only to decouple the index set S l and S u and has no effect on model F ( x ) trained on the dataset.

To further simplify the problem (2), we introduce variables y  X  X  ( i = 1 , 2 ,  X  X  X  ,N ) which are defined as follows: Then the problem (2) could be reformulated as arg min = arg min = arg min Defining  X  c i ,  X  y i as we can rewrite the problem (4) as
In this section, we consider the case where F ( x ) can be represented as an ensemble of regression trees. Namely, for an ensemble consisting of M + 1 base learners: functions; each a i represents a specific set of joint param-eter values realizing a member of this function class and  X  ,  X  X  X  , X  m are the combination coefficients .

The gradient boosting decision tree [6] iteratively adds new regression trees that fit the negative gradient of the loss function with respect to F ( x ) at the most up-to-date estimate. Suppose that at the m -th step, we have in our ensemble a set of base learners { f ( x ; a i ) } m  X  1 i =0 takes the form of a regression tree and the corresponding weights {  X  i } m  X  1 i =0 . Then, the current estimation for the re-sponse of the j -th sample is given by The negative gradient of loss function with respect to F ( x at F m  X  1 ( x ) is  X  g j and The new base learner (which, in our case, is a regression tree) parameterized by a m to be added to the ensemble at the current step is typically obtained by solving the following optimization problem: For simplicity, let us denote f ( x i ,a m ) by f m ( x i fixed, the optimal line search step size for f m ( x ) is obtained via Let r Since (9) is a piece-wise linear function, it is straightforward that the optimal  X  is one of r i  X  X  that satisfies r i &gt; 0 , and could make (9) reach minimum.

When the number of instances N is very large, it is very time consuming to evaluate the function value for each r since each time of evaluation of the function value involves summing over N elements. However, since (9) is the dif-ference of two monotonically decreasing functions, this re-peated computation can be avoided by keeping track of the amount by which each function decreases each time when we increase  X  by a certain amount.
 Algorithm 1 Gradient boosting approach for censored re-gression with truncated l 1 loss 1: Input: X = { x 1 ,..., x N } , S l , S u , c,  X   X  (0 , 1) , M . 2: Output: F m ( x ) . 3: F 0 ( x )  X  argmin z  X  R L ( z |  X , c , S l , S u ,X ) 4: for m = 1 ,  X  X  X  ,M do 5: Compute negative gradient {  X  g i } N i =1 by (8) 6: a m  X  arg min P N i =1 (  X  g i  X  f ( x i ; a m )) 2 7:  X  t  X  arg min  X &gt; 0 L ( F m  X  1 ( x ) +  X f ( x ; a m 8: F m ( x )  X  F m  X  1 ( x ) +  X  m f ( x ; a m ) 9: end for
The description of the algorithm for solving censored re-gression with the truncated l 1 loss based on the gradient boosting approach is shown in Algorithm 1. The time com-plexity of the algorithm is O ( pMNlogN ), where p is the number of features, thus it may not be applicable for large-scale datasets. Next we introduce the linear model which can be applied to large-scale datasets.
In this section, we consider the case where the hypothesis takes the form of a linear combination of the covariates. We formulate the problem as follows: where  X  &gt; 0 is the regularization parameter. In the follow-ing, we simply denote the loss function for the sample x i L w T x i . To take into consideration the effects of bias, we can append ones at the end of x i  X  X .

By the definition of the loss function in (2), the above problem can be written as min
Using the definition of auxiliary variables y i ,  X  c i defined in (3), (5) and further defining  X x i as: we can transform (11) into
The loss function above looks similar to the hinge loss used in SVM. However, the fact that  X  c i can be both positive and negative makes the popular packages such as LIBSVM and LIBLINEAR not applicable to solve this problem. An alter-native is to transform the above problem into its dual form and use CVX to solve the resulting quadratic programming problem. However it is generally very slow and does not scale to large-scale datasets. Next, we show how to solve the problem with stochastic dual coordinate ascent which has been proven to achieve a fast convergence rate and can handle large-scale datasets.

Define  X  i : R  X  R as  X  i ( z ) = [ X  c i  X  z ] + . Its convex conju-gate  X   X  i : R  X  R [2] is
By plugging the definition of  X  i ( z ) into (14), we have
For w  X  R d , the convex conjugate of the regularization term g ( w )  X  1 2 k w k 2 2 is
The stochastic dual coordinate ascent (SDCA)[20] solves the dual problem which can be formulated as where and w  X  = w (  X   X  ) where w  X  and  X   X  are the primal and dual optimal solutions with both the loss function and the regu-larization term in the primal problem being convex.
The SDCA method in each iteration randomly chooses one  X  (1  X  j  X  N ) to update with the objective of increasing the dual function value as much as possible. That is  X   X  j = argmax
By expanding the l 2 -norm and discarding the terms unre-lated to  X   X  j , we can further get Then, by letting we have Algorithm 2 Proposed SDCA algorithm for truncated loss based censor regression with linear model 1: Input: X = { x 0 , x 1 ,..., x N } , S l , S u , c,  X   X  2: Output:  X  w ,  X   X  3: Initialize:  X  (0) = 0 , v (0) = 0 4: for i = 1 ,  X  X  X  ,N do 5: Compute  X  x i ,  X  c i by (12), (5) respectively 6: end for 7: for t = 1 ,  X  X  X  ,T do 8: Randomly pick j  X  [1 ,N ] 9: Compute  X   X  j based on (20) 12: end for
The algorithm for solving (13) is shown in Algorithm 2. In practice, the number of iterations T can be determined by the duality gap which is the difference between the function value of (13) and (16) at the attained primal and dual solu-tion. The iteration can be terminated when this gap drops below a predefined threshold.
Based on the linear model introduced in the previous sec-tion, we propose in this section a mutli-stage linear model that can simultaneously estimate multiple linear models, one for each group of patients sharing the same number of treat-ment stages.

The key idea underlying our simultaneous estimation of models is that although patients may achieve remission from different stages of treatment, the stage-varying covariates collected on them around the same time before they entered follow-up study should share some commonalities in terms of their relative contribution to the prediction of relapse time and, at the same time covariates collected from different stages overall, contribute differently to the prediction. In this section, by assuming that commonalities shared among the patients remitted across different treatment stages take the form of a linear combination of covariates, we show how these commonalities can be exploited toward simultaneous learning of the prediction model for multiple stages of treat-ment.
Suppose that patients in the clinical study of interest ex-perience at most M stages of treatment before they achieve remission. Let all the covariates of the i th patient who remit-ted after m treatment stages be x i = x T i 0 , x T i 1 ,  X  X  X  , x (1  X  m  X  M ), where each of the x T ik (0  X  k  X  m ) is a column vector and x T i 0 represents covariates that are not related to treatment such as those recording the patient X  X  demographic information and family medical history infor-mation; x T k ( k  X  1) represents the covariates from the treat-ment stage k  X  1 stages away from their last treatment stage. For instance, x T i 2 represents the covariates from second to the last treatment stage. Let w = w T 0 , w T 1 ,  X  X  X  , w T M w k represents the coefficients associated with x T k . Note that in our approach, the patients across all the treatment stages share the same weight vector as long as they have the co-variates that the specific segment of weights corresponds to. Also let  X  = [  X  10 , X  11 ,  X  X  X   X  k 0 ,  X  X  X   X  kk ,  X  X  X   X  M 0 a vector of coefficients balancing the influence of different blocks of the covariates for patients that remit from each of the specific treatment stages. Let S m l and S m u be the set of indices of patients that have covariates from their last m treatment stages and have a lower bound and an upper bound in their relapse time, respectively.

Our proposed approach for simultaneously estimating cen-sored regression models for patients remitting from multiple treatment stages can be formulated as follows:
In making prediction, the coefficients for x ik is  X  All the covariates from the stage that is k  X  1 stages away from the last stage share the same w k while for patients that experienced different number of stages, they have differ different  X  mk .
It is worth noting that although (21) is convex with re-spect to either  X  or w , it not jointly convex. The block coordinate descent algorithm that alternates between opti-mizing over  X  and optimizing over w is adopted to solve this problem. In the following, we show that each step of the op-timizing over w and optimizing over  X  can be transformed into a problem of the same form as (11).
 When w is fixed, the problem (21) can actually be decoupled into M separate problems. Let r ik = w T k x ik . The problem (21) becomes M separate optimization problems, each of which takes the form: +(1  X   X  m ) X
This problem is actually the same as (11), thus the al-gorithm introduced in the previous section can be used to solve it.
 When  X  is fixed, the problem (21) turns into min For i  X  S m u , denote p ik =  X  mk (1  X   X  m ) x ik ; p i = [ p and  X  u i = (1  X   X  m ) u i . For i  X  S m l , denote q ik  X  ) x ik ; q i = [ q i 0 ,  X  X  X  , q im ] and  X  l i =  X  m l dimension of x ik and I d k be an identity matrix of d k rows and columns. Also let I M = diag[ I d 0 ,  X  X  X  I d M ] which is also an identity matrix and I m be its first d 0 +  X  X  X  + d m rows. Then the problem above can be simplified as min = min By introducing  X  p i = I T m p i and  X  q i = I T m q i and making use of the fact that S l = S 1 l +  X  X  X  + S M l and S u = S 1 u the above problem can be further written as min which also has the same form as (11).

Note that although in the STAR*D dataset, all the pa-tients that achieved remission at a later stage have gone through all the previous treatment stages, this is not a pre-requisite for our model. Our model only requires that the covariates of all the patients are aligned in the reverse order of the treatment stages leading to remission.
In this experiment, we use simulated data to answer the following two questions: (1) How well can our linear model scale to large datasets? (2) If there is an underlying lin-ear relationship between the response and the covariates, to what extent that the linear model learned from minimizing the truncated l 1 loss recovers the true linear model when only an upper bound or a lower bound of the response of an instance is known?
We first generate our datasets as follows: Each instance is randomly drawn from a 100-dimensional standard normal distribution. For each dataset we generate n such instances where n ranges from 50k to 15,000k. The ground truth linear model w  X  is a 100 dimensional vector with each entry being 0 . 5. The ground truth response for each instance is y = w  X  T x i + e i where e i  X  N (0 , 1); For each instance, we draw a random number b i from U (0 , max i | y | i ) and use y as its upper bound or use y i  X  b i as its lower bound. We assign a lower bound for half of the randomly chosen samples and an upper bound for the other half. In the simulation, we fix  X  in (11) to be 1 e  X  6 and  X  to be 0 . 5. The algorithm terminates when the duality gap is less than 1 e  X  7. Figure 1: Change of the computational time and model error when the number of instances increases. The simulation was run on a machine with Intel Xeon(R) CPU E5-1620 v2 3.70GHz  X  8 processor, 32 GB memory and Ubuntu 14.04 LTS system. The results are shown in Fig-ure 1. The computational time grows approximately linearly with the number of instances. The model error, measured by p k  X  w  X  w  X  k 2 2 / 100, decreases as the number of instances in-creases. Note that as the number of instances increases, the perturbation range max i | y | i also increases, bringing in more uncertainty to the response of each instance. It is interest-ing to observe that even though only an upper bound or a lower bound of responses is provided, with enough samples, the linear model learned from minimizing the truncated l 1 loss can still to some extent recover the true model under certain conditions.
STAR*D is a study designed to identify the most effective treatment or combination of treatments for patients diag-nosed with nonpyschotic MDD. It lasted over a period of seven years and involved over 4,000 patients aging from 18-75 and has been so far the largest and longest study ever con-ducted for evaluating the effectiveness of treatments of de-pression. It consists of four treatment stages during each of which patients were treated with certain antidepressants and their depressive symptoms were evaluated every two to three weeks. Patients that could not achieve remission or suffered from intolerable side effects of medications in one treatment stage were encouraged to proceed to the subsequent stage. Those who did achieve remission or demonstrated significant symptomatic improvement were invited to enter a 12-month naturalistic follow-up phase where assessments of patients X  depressive symptom severity were made on a monthly basis.
Due to subjects dropping out without relapse in the follow-up phase, we considered in our experiments three cut-off points in the follow-up -10 months, 11 months, 12 months respectively, in order to, on one hand, keep our analysis in as much accordance with the original design of the study as possible, one the other hand, take into account the subjects that dropped out at later time points of the follow-up phase and see how our model performs in response to changes in the total number of right censored cases. For each cut-off point, we included into our analysis only the subjects that either have definitively relapsed at or before the chosen cut-off time point (uncensored cases) and the subjects whose relapse occurred later than the chosen cut-off time point (right censored cases). It is worth emphasizing here that we excluded from our analysis those who dropped out and did not relapse before the chosen cut-off time point so that risk of relapse for each sample is known.

As for the covariates, we included in our analysis those col-lected from the follow-up enrollment including demograph-ics (DM), Eligibility (EL), Psychiatric Diagnostic Screening (PDS), etc. For each treatment stage, we took into consid-eration the covariates from patients X  last observed record of Quick Inventory of Depressive Symptomatology -Clinician-rated (QIDS-C) and QIDS-SR (Self-rated), the baseline record of Interactive Voice Response (IVR) and Research Outcomes Assessments (ROA). According to [19], relapse is defined as an individual having an observed QIDS total score col-lected in IVR during the follow-up phase great than 10. All the subjects included in our analysis achieved remis-sion from the acute treatment stages. The number of sub-jects that relapsed and did not relapse by the cut-off time point from each treatment stage is shown in Table 1. Note that, for the cut-off time points before the 12th month,  X  X on-relapse X  cases included all subjects that had definitively not relapsed until that point, even if they eventually relapsed at some time later than that point. The Kaplan-Meier sur-vival curves for subjects with only one treatment stage and subjects with more than one treatment stage from different datasets are shown in Figure 2.

In training models, all the treatment stage varying co-variates were aligned based on the reverse order of stages leading to remission. As is shown in Table 1 the number of subjects that remitted from stage 3 and stage 4 were too small, we omitted the covariates from the first treatment stage for those achieved remission from stage 3 and stage 4 as well as the covariates from the second treatment stage for those who achieved remission from stage 4. Missing values were imputed with the column mean. All the covariates that were included were normalized with zscore.
The dataset included a large number of covariates, ranging from demographic information, medical and psychiatric co-morbidities to depressive symptom measurements. However, not all of them are related to the subjects X  relapse status at the end of the follow-up phase or risk of relapse, which ne-cessitates feature selection as one of the crucial steps to min-imize overfitting and ensure the quality of predictive models. In this work, l 1 sparse logistic regression-based stability se-lection [14] was employed to perform feature selection on each of the datasets determined by different cut-off points. The fitting target for sparse logistic regression is the relapse status by the cut-off point associated with each dataset. Since we have covariates from different stages of treatment, we ran stability selection twice, one on covariates from en-rollment and last treatment stage, the other on covariates from second to last treatment stage. The number of covari-ates selected was determined by the cross-validation.
A commonly used metric for evaluating the performance of survival models is concordance index [22] [10] which is a generalization of Area Under ROC Curve (AUC) for contin-uous response and censored data. It measures the probabil-ity of concordance between the predicted response and the observed response. A high concordance index means that there is a high likelihood that for two randomly sampled individuals, the order of their predicted response matches the order of their observed response. In our context, con-cordance index can be regarded as a measure of the propor-tion of the pairs of subjects for whom the relative order of predicted time of relapse is concordant with the order of ob-served time of relapse among all the pairs of subjects whose observed time of relapse can be ordered. Suppose we have n samples in our testing set. The observed time of relapse for the i -th subject is t i . Its predicted time of relapse is p Let A be the set of pairs that can be ordered, that is Then the concordance index can be defined as where I (  X  ) is the indicator function.
We can divide each of the datasets into two sets of sam-ples, one with data from one treatment stage only, the other with data from at least two treatment stages. For each dataset, we randomly selected 80% of samples from those who relapsed and those who did not relapse in each set as training set and the rest 20% as the testing set. A five fold cross-validation was carried out on the training set to se-lect parameters. This random split was repeated 10 times
Stage Stage 1 290 174 464 274 296 570 263 353 616 Stage 2 163 42 205 155 81 236 147 101 248 Stage 3 23 6 29 23 10 33 23 12 35 Stage 4 9 5 14 9 7 16 9 9 18
Total 485 227 712 461 394 855 442 475 917 and the mean performance on the test sets and the standard deviation were reported.

The linear model, gradient boosting model, and Cox model were all trained and tested on the covariates from enrollment and last treatment stage. The model parameters including the number of features selected for all the models,  X  ,  X  for linear models,  X  , the number of trees in the gradient boosting model were determined by cross validation. For the multi-stage linear model, the number of covariates selected from different stages were determined independently and the  X   X  X  were set to be the same.
 Table 2: Performance of different methods on dataset determined by cut-off at the 12th month Table 3: Performance of different methods on dataset determined by cut-off at the 11th month
The average concordance indices along with the standard deviation obtained by different methods on datasets deter-mined by different cut-off time points are reported in Tables 2, 3, 4. Overall the performance on datasets with cut-off Table 4: Performance of different methods on dataset determined by cut-off at the 10th month at the 10th and the 11th month is better, which is prob-ably due to an increase in the number non-relapse cases that largely comes from subjects dropping out late in the follow-up phase. On all the datasets, our methods perform better than the Cox model. In particular, the multi-stage stage linear model produces the best performance, which, we think, can be accounted for by the fact that it can take into consideration the distributional difference of covariates from subjects remitted from different treatment stages. The performance of the gradient boosting approach is worse than that of the linear approach probably due to overfitting.
One of the advantages of predicting relapse risk with lin-ear models is that with all the covariates normalized to zero mean and the same variance, the magnitude of the coefficient associated with a covariate indicates its marginal contribu-tion to the predicted risk, given all other covariates remain-ing unchanged. In this subsection, we use the multi-stage linear models obtained from previous subsection based on random splits of the dataset determined by the cut-off point at the 10th month to produce a ranking of the covariates. Although the models built on different random splits of data involve different numbers of covariates, the numbers clus-tered in small range, with the number of covariates selected from Enrollment and last treatment stage varying from 70 to 90 and the number of covariates selected from the second to last treatment stage ranging from 10 to 20. Similar to the method used in [10] to cope with variance arising from cross-validation design, we averaged coefficients associated with each covariate over ten models and used the magni-tude of the mean value minus their variance as a score to rank the coefficients.

Table 5 shows the top predictors of risk of relapse for sub-jects that experienced only one treatment stage. To our surprise, the  X  X cademic degree X  comes on top of the list, suggesting that a higher academic degree is associated with a lower risk of relapse, according to the coding of this at-tribute and the sign of its coefficient. The residual symp-toms, as mostly marked by  X  X ast observed X , are also ranked high on the list, which corroborates the findings from [9] that the presence of residual symptoms such as depressed mood, hopelessness is associated with an earlier short-term relapse. Although residual sleep disturbance did not appear in our list of top predictors, which is consistent with the ob-servation made in [15] that there is no significant different in Kaplan-Meier survival curves for those with and without the domain of residual sleep disturbance, we did find strong cor-relation between sleep onset insomnia at the baseline, which ranked second in our list of relapse risk factors. In addition, the subjective nature implied in some of the top-ranked pre-dictors such as  X  X ad mood X  and  X  X mpact of your family and friends X  also help explain that mindfulness-based cognitive therapy [23] can reduce the risk of relapse of MDD patients in remission or recovery.

The top predictors of risk of relapse for subjects that expe-rienced more than one treatment stage largely overlap with those in Table 5, thus we did not show a separate table here. However, it is worth mentioning that the scores of covariates for patients with more than one treatment stage are generally lower and more flat and among top predic-tors, predictors marked with  X * X  in Table 5 rise to a much higher place in the list. But there are some predictors with a relatively high magnitude of mean in their corresponding coefficients but did not make into the top list due to a great variance, which probably results from relatively scarce pres-ence of the conditions specified in those predictors among the subjects under study, implying that they should be con-sidered as potential high risk factors if they are found in subjects. Such predictors include  X  X isited emergency room in last three months X ,  X  X areless work due to emotional prob-lem X  from baseline IVR of second to last treatment stage,  X  X ired nearly every day past 2 weeks X  from PDS and  X  X am-ily history drug abuse X  from PHX. Overall, from the top risk factors we identified, we can see that therapies focusing on improving subjects X  outlook for the future, psychomotor functioning and negative thinking might be more effective in preventing the relapse among the patients that achieved remission from treatment with antidepressant.
In this paper, we proposed a censored regression approach for predicting the risk of relapse of patients after their ini-tial remission from one or multiple stages of antidepressant treatment. Since the patients X  relapse status was assessed once every month, we employed a truncated l 1 loss to model the response for which only a lower bound or an upper bound is observed. We considered the hypothesis in the loss func-tion that can be represented as (1) an ensemble of regres-sion trees; (2) a linear combination of covariates. We de-veloped an efficient gradient boosting algorithm when the hypothesis takes the form of an ensemble of regression trees and a stochastic dual coordinate ascent algorithm when the hypothesis is a linear model. Furthermore, we extend the linear model to deal with covariates collected from multi-ple stages of treatment. Our experiments on synthetic data and STAR*D datasets demonstrate the efficiency and effec-tiveness of the proposed methods. In all cases, our multi-stage linear method achieves the best performance. In addi-tion, the top risk factors identified by our multi-stage linear method are not only consistent with the findings from some of the recent research regarding relapse among patients with MDD who had initially achieved remission but also provided some insights into how to develop therapies for prevention of relapse.
 The authors would like to thank Dr. Qingqin Li from John-son &amp; Johnson for some helpful discussions. This work is supported in part by grants from NIH (RF1AG051710) and NSF (IIS-0953662 and III-1421057). [1] J. Alpert and M. Fava. Handbook of Chronic [2] S. Boyd and L. Vandenberghe. Convex Optimization . [3] H. chin Lin, V. Baracos, R. Greiner, and C. nam [4] D. R. Cox. Regression models and life-tables. Journal [5] H. A. Eyre, A. Eskin, S. F. Nelson, N. M. St. Cyr, [6] J. H. Friedman. Greedy function approximation: A [7] B. N. Gaynes, D. Warden, M. H. Trivedi, S. R.
 [8] J. E. Kelsey. Achieving remission in major depressive [9] N. Kennedy and K. Foy. The impact of residual  X  0.1588 PDS [10] A. Khosla, Y. Cao, C. C.-Y. Lin, H.-K. Chiu, J. Hu, [11] M. Marcus, M. T. Yasamy, M. van Ommeren, [12] K. MB, L. PW, L. CE, and K. GL. Predictors of [13] K. MB, S. RW, L. PW, and W. N. Relapse in major [14] N. Meinshausen and P. B  X  uhlmann. Stability selection. [15] A. A. Nierenberg, M. M. Husain, M. H. Trivedi, [16] C. Otte. Incomplete remission in depression: role of [17] E. S. Paykel. Continuation and maintenance therapy [18] R. Ramana, E. S. Paykel, Z. Cooper, H. Hayhurst, [19] A. J. Rush, M. H. Trivedi, S. R. Wisniewski, A. A. [20] S. Shalev-Shwartz and T. Zhang. Stochastic dual [21] P. K. Shivaswamy, W. Chu, and M. Jansche. A [22] H. Steck, B. Krishnapuram, C. Dehing-oberije, [23] J. D. Teasdale, Z. V. Segal, J. Mark, G. Williams, [24] K. Tran, S. Hosseini, L. Xiao, T. Finley, and
