 1. Introduction
Open MultiAgent Systems (OMAS) are characterised by being designed for a specific purpose and by an unknown population of agents at design time ( Hewitt, 1986 ). This population is composed of autonomous agents that may be heterogeneous in their pre-ferences and behaviour and may join and leave the system at runtime. With all these characteristics in mind, when designing such systems, one of the main problems consists of assuring that the  X  X  X  priori X  X  unknown population of agents behave according to the system X  X  global objectives and preferences.

The research community has tack led this problem by defining organisation based models to str ucture and regulate the agents X  actions in a system ( Ferber et al., 2004 ). This changes the focus in the design of such systems from an agent-centred approach to an organisation-oriented approach where the problem consists in designing the rules of the game ra ther than the individual compo-nents. Some examples are AGR ( Ferber et al., 2003 ), MESSAGE ( Caire et al., 2001 ), Roadmap ( Juan et al., 2002 ), Electronic Institutions ( Esteva et al., 2001b ), OMNI ( Dignum et al., 2004 ), MOISE  X  ( Hubner et al., 2002 ), GORMAS ( Argente, 2008 ) or the models proposed in
Deloach et al. (2008) and Boella et al. (2005) . In general, it can be considered that such models are b ased on normative systems. That is, they define norms  X  either explicitly or through the use of organisational concepts like roles, g roups, protocols, etc.  X  to govern the behaviour of the agents in the system. Such norms are often defined at design time and have the aim to assure that the system X  X  global objectives are achieved. The hypothesis behind that idea is that if the agents behave according the specified norms, then the global system will perform in an efficient way.

In our opinion, this approach is suitable for imposing the fulfilment of predefined rules and protocols, but has its limita-tions when the global objectives of a system (in terms of a global utility function) also depend on some efficiency measures. This occurs usually in rather complex domains like, for instance traffic regulation. The main objectives of traffic regulation can be roughly summarised as: (i) guaranteeing the security of the participants, and (ii) assuring an efficient traffic flow. Regarding such domains, one can identify the following shortcomings of standard normative systems: 1. Especially in complex domains, it may be difficult to define a set of norms at design time that actually affects the global utility in a positive way. Traffic authorities a re continuously looking for better traffic regulations what indicates that it is a hard task to specify a  X  X  X ood X  X  set of norms regarding the global objectives. 2. Within a standard normative system, agents may still have a certain degree of freedom, and correct normative behaviour may not always imply an efficient performance of the global system. For instance, a driver that goes much below the speed limit complies with the norms, but may provoke undesired effects like traffic congestions. 3. The fixed nature of predefined norms may imply less flexibility in certain unforeseen situations and, sometimes, it may be desirable that agents violate a norm. For instance, a driver should exceed the speed limit if this can avoid an accident. 4. It is difficult to define effective penalties/rewards in open systems where the agent population and the individual pre-ferences are unknown at design time and may change at runtime. In the traffic domain, for example, a fine of 100$ will have a different persuading effect on different drivers (maybe depending on the income of the driver).

In order to tackle the mentioned shortcomings this paper pro-poses to endow OMAS with a different regulation mechanism; a mechanism based on incentives that attempts to induce agents to act in each moment in a way that is beneficial regarding the global utility of the system. In particular, an adaptive incentive mechanism is proposed that: (i) given a current state of the system it is able to identify the appropriate actions agents should perform from the system X  X  point of view and (ii) provides incentives to induce agents to perform such actions by tuning penalties or rewards as a consequence of performing those actions.

The paper is organised as follows: Section 2 presents some background on our notion of incentives and provides some basic definitions and notations. Thereafter, Section 3 presents our propo-sal of an adaptive incentive mechanism. Section 4 presents the experimental validation that has been carried out in a p2p scenario.
Section 5 puts forward some related work. Finally, Section 6 gives some conclusions and points out lines of future work. 2. Background
This paper focuses on OMAS that are populated with rational agents. An OMAS is considered to be a system that provides a common environment in which a number of autonomous agents, denoted by A g , perform their actions. The system is said to be open because agents may join and leave at any time and their behaviour is unknown a priori. The set of possible actions agents can perform in an OMAS is denoted by A . An OMAS evolves by means of transitions in the environmental state space, denoted by
X . An OMAS may have some global objectives. It is assumed that these objectives are reflected by means of a preference relation over environmental states in X and which can be captured in form of a global utility function U O : X - X  0 :: 1 .
An OMAS regulates the set of actions an agent ag i A A g can perform in a given state x A X . This set is denoted by A
OMAS may be governed by some organisational infrastructure (see an example in Section 3.1 ) that enforces certain rules on agents X  actions. In this sense, the actions an agent can perform in a given state depend on (i) the agent X  X  individual abilities; (ii) the restrictions imposed by the environment; and (iii) the rules, protocols, norms, etc., imposed through some sort of organisa-tional infrastructure.

Implicitly it is assumed that an OMAS evolves at discrete time steps. In each step, all agents in the system perform one action, that is, the new state of the environment is produced through the joint actions of all agents. Agents can choose a  X  X  X kip X  X  action, which allows for modelling asynchronous behaviours. The environment defines the consequences of the agents X  actions in terms of a new environmental state. From the point of view of an individual agent, the consequences or results of taking an action do not only depend on its own action, but also on the actions performed by all other agents in the same time step, the characteristics of the resources embedded in the OMAS (e.g., the state of a road in a traffic scenario), and possibly on other external influences (e.g., weather conditions in a traffic scenario).

In the sequel, it is assumed that the environment of an OMAS can be represented by means of a finite set of attributes ATT  X  f Att 1 , ... , Att n g , where V i denotes the value domain for attribute Att i . Thus, an environmental state x A X can be modelled as a set of tuples, such that, x  X f / Att j , v j S9 Att j A ATT 4 v j A
In this work, only OMAS that are populated with rational utility maximisers are considered. That is, agents rule their behaviour based on some preferences over environmental states, which are represented by means of an utility function U ag : X - X  0 :: 1 . Using this function, an agent ag i decides its next action by means of an action selection function t that follows the principle of maximising the expected utility: t  X  x  X  X  argmax where P x  X  x 0 9 x , a k  X  is the agents X  estimate, at state x , of the probability that state x 0 will occur when executing action a state x .

As we have mentioned before, OMAS are usually designed with a general purpose in mind  X  the global objective of the system which is captured by the utility function U O . Our proposal is to use incentive mechanisms to regulate such systems with the goal to increase their global utility.

Following the notion adopted in Centeno et al. (2009) ,anincentive mechanism is considered to be some function that produces changes in the environment of a system. In this sense, our notion of  X  X  X ncentive X  X  is slightly different to the usual consideration of incentives to be something positive. For us, incentives are mod-ifications that are introduced in the environment by some author-ity as a result of agents taking certain actions. In particular, they are changes of the resulting states an agent may reach after taking an action. The idea behind incentives is that rational agents will react to the introduced changes. However, whether an agent will consider such changes as positive, negative or irrelevant, depends on its own subjective interpretation (its individual utility function). Accordingly, different agents may react in different ways to the same incentives. 3. A generic incentive mechanism for OMAS
This section presents our proposal of a generic incentive mechanism that tries to overcome the aforementioned short-comings of classical normative systems. The key points of this mechanism are the following:
The mechanism identifies and promotes those actions that actually improve the global utility of the system and prevents actions that do not. Moreover, these actions may be different in each environmental state and for each individual agent.
Incentives are adapted to each agent, in order to take into account that agents may have different preferences and, thus, may react in a different way to the same incentives.
 Both issues are resolved at runtime through a learning process, which assures an adaptation to changes in the agent population and/or in the environment.

As mentioned in Section 2 , incentives are seen as modifications that are introduced in the environment if an agent performs a certain action. Such modifications are denoted as tuples of the form / Att j , v j S , where v j A V j is the value that the attribute Att j A ATT will take as a consequence of performing a particular action. Furthermore, M D f / Att j , v j S9 Att j A ATT 4 v the set of possible modifications the incentive mechanism is able (and authorised) to apply.

Given an OMAS with A g , A , X and U O as specified in Section 2 , the proposed mechanism specifies a set of incentives for each environmental state and for each individual agent of the system.
A set of incentives is a set: f / a 1 , m 1 S , ... , / a a
A , m A M , and 8 / a k , m k S , / a j , m j S : a k a a assignment of modifications to some actions and such that each action is included at most once. I denotes the family of all sets of incentives.

Our mechanism defines a number of incentive policies, one for each agent ag i A A g . These policies can be described by means of the following function: inc : X -I  X  1  X 
The operational semantics of these functions is as follows: Given an environmental state x A X , inc ag i  X  x  X  determines the (valid) set of incentives that will be applied in this state to the agent ag agent ag i selects an action a k in state x and inc ag i  X  x  X  contains an incentive / a k , m k S , then the modification m k will be applied to the result state.

Instead of applying fixed incentive policies, the mechanism learns them at runtime using a Q -learning algorithm ( Watkins, 1989 ). This means, the policies are adapted to the behaviour of each particular agent and also to possible modifications in the system. Q -learning requires to store the Q -values in a two dimensional state/action table. In our case, the state dimension is determined by the environmental state space X . This space may be very large or even infinite. In order to cope with this issue, a space reduction algorithm is applied.

The following subsections describe, in the first place, our approach to deploy the proposed incentive mechanism in an
OMAS. Afterwards, the dynamics of the mechanism is explained in detail: the method applied to reduce the environmental state space; the learning algorithm; different incentive policies; and the way the global utility of a state is calculated during the learning process. 3.1. An infrastructure for deploying the incentive mechanism
The incentive mechanism is deployed by means of a network of institutional agents, called incentivators . This infrastructure could be easily integrated in rather flexible organisation-based multiagent architectures like, for instance the THOMAS architec-ture ( Carrascosa et al., 2009 ). The approach is similar to the use of governor agents that are used in Electronic Institutions to control the behaviour of external agents ( Esteva et al., 2004 ). Each external agent ag i is assigned to an incentivator who is in charge of learning and applying the incentive policy inc ag i for this agent.
Furthermore, incentivators can communicate with each other allowing them to coordinate their actions. Fig. 1 presents a schematic overview.

During the execution of the system, each agent can query its incentivator about the incentives assigned to a given action in the current state. It is assumed that rational agents will always use this  X  X  X ervice X  X  because it allows them to better estimate the consequences of their actions and, thus, to maximise their individual utilities. After querying its incentivator, an agent selects an action and executes it. The incentivator observes the action its agent performs. If the action belongs to the set of incentivised actions, the incentivator applies the corresponding modification to the new environmental state. The interaction protocol for this communication is presented in Fig. 2 . 3.2. Reducing the environmental states space
In our approach, each incentivator learns the incentive policy inc i for its external agent. As mentioned before, in Q -learning, the policy is represented through a two dimensional lookup table, where one dimension corresponds to the state space X . This space may be very large or even infinite. In order to deal with this obstacle, state aggregation algorithms can be applied to transform the states space X to a reduced space X 0 .

In our case study (see Section 4 ), a simple state aggregation technique has been used that works as follows. Let x  X f /
Att
ATT 4 v j A V j g be a state in X . x is transformed into a state x 0  X f / Att j , v 0 reduced set of  X  X  X elevant X  X  attributes. Here, attributes that are likely to have no influence on the global utility of the system nor on the agents preferences are eliminated. This is a heuristic decision that should be taken by the designer of the system based on the domain knowledge.
 D V j is a reduced value domain of the relevant attributes.
Values in V 0 j are obtained using the following formula:  X  floor with i  X  ceil floor ( k ) returns the largest integer not greater than k and ceil ( k ) returns the smallest integer not less than k . V max j and V maximum and minimum values of the value domain V j and o is a parameter that determines the cardinality of the new domain V 0
Eq. (2) divides the domain range of V j into o intervals and the values of each interval are represented as a single value in V 0
Thus, 9 V 0 j 9  X  o . The formula is valid for continuous and discrete domains that are bounded by V max j and V j min . A way to deal with unbounded domains is to chose V max j and V j min such that they includes the most significant values. Then, Eq. (2) can be used to transform the values in this range and values that are grater than
V j or smaller than V min j may be mapped to some additional values in V 0 j . Regarding attributes with non-numerical values, they can be either maintained or they can be mapped to natural number which are then transformed using Eq. (2) . This decision will depend on each particular attribute and on the application domain.

Fig. 3 shows an example of the transformation process. In this example, the original environmental states space is composed of five attributes. The reduced space (on the right hand side of the figure) contains three  X  X  X elevant X  X  attributes. That is, in this case the attributes #cars-road1 and #cars-road2 have been eliminated because they are not considered to be relevant. Using the same parameters o  X  3 for each attribute, the values of the attributes are grouped together in three intervals as shown in the figure.
Then, each state x i will be transform to a new state x 0 applying Eq. (2) to the values of each original attribute. Therefore, the state aggregation algorithm transforms the original state space X into a new space X 0 .

Accordingly, the incentive policies inc ag i are redefined as follows: inc 3.3. Learning incentives
Each incentivator is in charge of one external agent and its task consists in determining appropriate incentives for this agent in a given state x . Appropriate, in this sense, means that the incentives should persuade the agent to choose actions that lead to an improvement of the global utility of the system. Instead of using a predefined set of incentives, we suggest to learn appropriate incentives over time. In particular, the learning tasks of each incentivator consists of: (i) identifying the effects ( X  X  X ositive X  X  or  X  X  X egative X  X ) of the possible actions that its agent may perform on the global utility and (ii) identifying the changes in the environ-ment (attributes and values) that are preferred/undesired by an agent. Our proposal is to learn both issues together by observing the effect of given incentives on the agents X  behaviour and on the utility of the system.

In scenarios where the global outcome of the action performed by an agent does not depend on the actions performed by others, the effect of that actions on the global utility could be determined locally. However, in many common situations the outcome of an action depends on the joint action of all participating agents. In order to account for this fact, all incentivators should work as a team so as to coordinate the actions to be promoted. The main problem in order to carry out such a task is that incentivators have just a local view of the system  X  the result of the action performed by their agents.

Taking into account these characteristics, the learning mechanism should have the following capabilities: (i) dealing with immediate local rewards and (ii) dealing with the lack of information about the actions performed by other members of the team. With this in mind, incentivators are endowed with a reinforcement multiagent cooperative learning algorithm based on Q -learning with immedi-ate rewards and E -greedy action selection ( Watkins, 1989 ).
In the scope of Q -learning, and based on the above definitions, the action space Z of an incentivator can be defined as the set of all possible incentives Z  X  A  X  M [f | g X  X  4  X  where | represents an empty modification. Thus, an incentive / a k , | S implies that, if an agent performs action a k , then no modification will be applied to the environment. Furthermore, the state space is the reduced environmental state space X 0 .
In each state x 0 A X 0 , incentivators select a set of incentives f z , ... , z n g D Z . After an incentivator has selected such set in state x 0 (as described in Section 3.4 ), it observes its effect and updates the Q -values for all involved incentives z i in the state/action table using the standard update rule ( Watkins, 1989 ) Q  X  x 0 , z i  X   X  Q  X  x 0 , z i  X  X  a  X  R  X  x t  X  1  X  Q  X  x where a is the learning rate and R  X  x t  X  1  X  is the immediate reward, received in the next environmental state x t  X  1 .

Besides, in order to explore new incentives, incentivators use an E -greedy selection approach. That is, random incentives are selected with probability E and the incentives with the highest Q -values are chosen with a probability of  X  1 E  X  . 3.4. Incentive policies
Based on the Q -values in the state/action table, two different incentive policies for incentivators have been defined and tested: (1) BestAction and (2) AllActions . 3.4.1. BestAction policy
The idea of the BestAction policy is the following: Let ag agent in the system. In each state x agent ag i can perform one of the actions in A ag i , x . Without loss of generality, it is assumed that there is at least one action a k A A ag i , x that is the most appropriate action regarding the global utility of the system. In the BestAction policy, an incentivator tries to promote this action by assigning it a  X  X  X ositive X  X  incentive (an incentive desired by its external agent). This means, that the incentivators try to learn only  X  X  X ositive X  X  incentives. The reward function R  X  x t  X  1  X  for the BestAction policy is defined accordingly. Let ag i be an agent; z k  X  / a k incentive selected by the incentivator responsible for ag current reduced state x 0 ; and let state x t  X  1 be the next state in the system. The reward R  X  x t  X  1  X  for updating the Q -value Q  X  x defined as follows: the range  X  0 :: 1 and higher values imply higher utility, this function assigns high Q -values to  X  X  X ositive X  X  incentives (because the agent took the promoted action) for actions that are better for the global utility of the system.
 The incentive policy inc ag i for an agent ag i in the case of BestAction is defined by the Algorithm 1 . As it can be observed, the space (based on E ). If exploration is decided, a new incentive is randomly chosen (line 2). In the other case, the incentivator selects the best incentive found until that moment, that is, the incentive with the highest Q -value (line 4). Using this algorithm, an incentivator always selects only one incentive. Besides, Z denotes the subset of Z that contains all the actions agent ag only incentives for actions that the agent can actually perform in the current state.

Algorithm 1. Algorithm describing the incentive policy BestAc-tion for agent ag i .

Require: x 0 -the state in the reduced state space corresponding to the current state x 1: if explore  X  E  X  then 2: z  X  getRandomElement  X  Z ag i , x  X  3: else 4: z  X  argmax z 5: end if 6: return f z g
When using the BestAction policy, incentivators operate as follows: given the current environmental state x , they obtain the corresponding state x 0 in the reduced space X 0 . Then, the incenti-publishes this incentive to its agent. The agent selects and executes an action based on its own decision function. The incentivator observes the agent X  X  behaviour and updates the
Q -value Q  X  x 0 , z k  X  , accordingly. Finally, if the agent selected the action a k , the incentivator applies the modification m k environmental state. 3.4.2. AllActions policy
In the AllActions policy, the incentivator assigns incentives to all the actions an agent can choose from in a given state. The idea is that in this way an agent that has decided not to choose the  X  X  X est X  X  action, still can be persuaded to choose another  X  X  X ood X  X  action or, at least, to omit actions that are negative for the system.
Here, an incentive / a k , m k S may be  X  X  X ositive X  X , that is, a an action that should be promoted (from the point of view of the system) and m k is some environmental change preferred by the agent. Or it may be  X  X  X egative X  X , where a k is an action with negative effects to the global utility and m k some modification undesired by the agent.

The reward function for the AllActions policy is defined as follows (the parameters are the same as before)
R  X  x
In this case, each incentive receives a higher reward if the proposed set of incentives has led to a higher system utility.
Whether or not agent ag i has performed the corresponding action, does not have any influence on the reward. Thus, an incentive may receive a positive reward because it keeps agents from choosing a  X  X  X egative X  X  action or because it persuades agents to choose a  X  X  X ositive X  X  action.

The incentive policy inc ag i for AllActions is defined in Algorithm 2 . In this case, an incentivator specifies an incentive for each action the agent could select in a given state. As it can be observed, the incentivator iterates over all actions the agent ag is able to perform in the current state (line 2). With probability E , it selects a random incentive for the current action (line 4). On the other hand, with probability 1 E the incentive with the currently highest Q -value is selected for the current action (line 2). Finally, in line 8 the incentivator accumulates the set of incentives to be applied in the current state.

Algorithm 2. Algorithm describing the incentive policy AllActions for agent ag i .

Require x 0 -the state in the reduced state space corresponding to the current state x 1: I  X  | 2: for all a k A A ag i , x do 3: if explore  X  E  X  then 4: m  X  getRandomElement  X  M [f | g 5: else 6: m  X  argmax 8 m 7: end if 8: I  X  I [f / a k , m S g 9: end for 10: return I
The operation of the incentivator with the AllActions policy is similar to the one described before. However, now the incenti-vator updates the Q -values of all incentives belonging to the set obtained in Algorithm 2 .

It should be noted that applying modifications to parameters in the environment may entail a certain cost for the system. It can be considered that such a cost will have an effect on the global utility of the system. In this regard, both defined reward functions already take into account the possible costs of applying incentives in the environment. 3.5. Calculating the global utility for guiding the learning algorithm
Both types of reward functions rely on an estimation of the global utility of the system in a given state ( U O  X  x  X  ). This work assumes that incentivators have only a partial view on the system X  X  state, that is, they are only able to observe the state of the system as a result of the action actually selected by their agents. Furthermore, it is assumed that incentivators know the system X  X  utility function. Therefore, an incentivator can only estimate the global utility of the system based on its local perception of the environment. In order to obtain a more reliable estimation of the global utility, incentivators should take into account the estimations of other incentivators. This is obtained by endowing incentivators with the gossip-based aggregation algo-rithm presented in Jelasity et al. (2005) .

The idea is that each incentivator holds a local value, and by exchanging messages with its neighbours the local values are aggregated using some aggregation function. Each incentivator i executes two different threads (see Table 1 ). The active thread periodically initiates an information exchange with a random incentivator j by sending a message containing the local estima-tion of the global utility U O i  X  x  X  and waits for a response with the utility estimation U O j  X  x  X  from incentivator j . On the other hand, the passive thread waits for messages sent by other incentivators and replies with the local utility estimate. The update method updates the local utility estimation by aggregating the current value and the received value. In our particular case study (see Section 4 ), the average has been chosen as the update function. Therefore, update  X  U O i  X  x  X  , U O j  X  x  X  X  returns  X  U particular aggregation function decreases the variance over the set of all local estimates of the global utility. 4. Case study: a P2P system
A peer-to-peer (p2p) file sharing scenario has been chosen for evaluating our approach. In particular, a simulator has been implemented that allows to simulate the operation of rational agents in such scenario and monitors the state of the global system in each moment. This simulator has been used to carry out different experiments where the evolution of the global utility of the system, as well as, the average utility of the agents has been analysed under different conditions.

In p2p file sharing systems, usually, only few peers ( seeders ) have the whole information; and the rest of peers ( leechers ) download pieces using a particular protocol. Our simulation tool focuses on peers sharing a file using the BitTorrent protocol ( Cohen, ). Following this protocol, a file is split in pieces of 256 kB each and every piece is split in 16 sub-pieces called blocks .
Peers exchange blocks and each block can be downloaded from different peers. For the sake of simplicity the phases in which peers and data are identified and peers get a list of neighbours to communicate with have been left out. The focus is just on the phase carried out to get each block (see Fig. 4 ). In this phase, each peer sends a bitfield message to its neighbours asking for their file status. After that, the peer has to decide which neighbours it will ask for the next block to download. A request message is sent to the selected peers. When a peer receives a request message it has to decide whether the requested block will be uploaded or not.
Once a peer accepts the request, it sends a piece message contain-ing the requested block. Immediately, the receiver of the piece sends a cancel message to the other neighbours it asked for the same block. When the download is finished, a have message is sent to the agent X  X  neighbours in order to update their file status.
This kind of scenario is a clear example of an open system where the objectives of individuals may not coincide with the objectives of the system. That is, these systems usually suffer from non-collaborative peers because they do not share their files.
The majority of peers join the system with the aim of down-loading but not sharing information ( Adar and Huberman, 2000 ).
Besides, there is usually no direct control over peers, they are usually unknown beforehand and may join/leave the system at runtime. Therefore, taking into account all these characteristics this kind of systems may be considered as an Open MultiAgent
System. 4.1. P2P system model
Regarding the most common problems in p2p systems (e.g., non-cooperation of peers, flooding of the network, etc. Hales, 2004 ), the objectives of the system could be specified as follows: (i) peers should download/upload as many files and blocks as possible in order to increase the number of available files; (ii) the usage of the network should be as low as possible; and (iii) the time spent on downloading files should be as short as possible in order to avoid an overload of the network. Furthermore, another objective of the system might be to keep the average fee peer pay to connect to the system at a constant level. The idea is that the fees should be high enough to cover the costs of the system but should not be too high such that agents are scared to participate. In a certain way, the objective to keep the average fee constant allows us to penalise extreme incentives. In most settings, extreme incentives (positive or negative) will typically persuade agents to act in the desired way, but usually will have negative effects (e.g., a cost) on the global utility.

The specified objectives of the system are captured by the following multiattribute utility function for the global system U  X  x  X  X  U files  X  x  X  w 0  X  U blocks  X  x  X  w 1  X  U C n  X  x  X  w where U files  X  x  X  is the utility of the number of already downloaded files.
The objective is to maximise the number of already down-loaded files. To do that, U files  X  x  X  has been defined as follows: where v files , x is the number of files already downloaded by all peers in the current state and V files max is the maximum number of files that can be downloaded by all agents in the system.
U blocks  X  x  X  represents the utility of the number of downloading/ uploading blocks in the state x . The greater the number of downloading or uploading blocks, the greater is the utility. The following formula is used: where v odown , x and v oup , x standfortheoveralldownloading/ uploadingblocksinthecurrentstate. V max odown , x and V max the maximum number of downloading/uploading blocks in the current state (e.g., all blocks that are not yet downloaded). U C n  X  x  X  is the utility of the usage of the network in the state x .
Following the work presented in Miralles et al. (2009) , this parameter is defined based on the network cost C n ( x ) that represents the sum of the network usage of each message sent among agents in state x . It is calculated as follows: C n  X  x  X  X  where msg length is the length of a message, and Lat  X  msg msg dst  X  is the latency between the source and target of the message. Latency is assumed to be constant and symmetric. The objective of the system is to minimise the network usage. This is reflected by the following formula: U C n  X  x  X  X  where V max C values of the network usage in the current state. The minimum maximum is calculated assuming that all peers send a piece or request message to all their potential neighbours.
 U t  X  x j  X  is the utility of the time spent on downloading a file, denoted as time cost . The objective is to minimise this time, such that, the shorter the time cost, the greater is the utility U t  X  x  X  X  where V max C time, respectively. The maximum is estimated in terms of the latencies between peers and assuming that peers will request their blocks to the neighbours with the highest latency.
Regarding the minimum, its value is estimated assuming that peers would download each block from the neighbour with the lowest latency. Finally, C t ( x ) is the actual time the agent has already spend in downloading a file.

U avfee  X  x  X  is the utility assigned to the difference between a default fee default fee and the average fee paid by the peers in the current state. The objective is to minimise this difference, that is, the lower the difference between the actually fee and the default fee, the higher is the utility.

All individual attribute utilities are defined in the range  X  0 , 1 and the parameters w 0 , w 1 , w 2 , w 3 and w 4 allow us to weight the utility of each attribute. 4.2. Peer agent model
Peers are modelled as rational agents that follow their own preferences. Peers have to make two main decisions: (i) to decide to how many neighbours they will send a request message asking for the next block to download and (ii) to decide how many requests received from other peers are accepted (i.e., how many piece messages are send). Accordingly, the action space of a peer ag is: A ag i  X f send piece  X  P  X  N 1  X  X  , send request  X  P  X  N the set of neighbours that have already sent a request to ag the set of neighbours that have the block ag i is seeking to obtain; skip represents no action; and P  X  X  denotes the power set. Peers that join the system obtain a certain bandwidth.
Furthermore, they have to pay a regular fee in order to connect to the network. Besides, a peer has a file it is sharing and which can be partially or completely downloaded. When a peer joins the system it receives a list of peers (neighbours) it can contact. It is assumed that a peer knows the latency with all its neighbours; and their file status is updated when have messages are received.
The attributes that may have some influence on the prefer-ences of a peer are bandwidth, fee, number of downloading and uploading blocks of a file and time spent on downloading a file.
These preferences can be captured through the following multi-attribute utility function:
U  X  x  X  X  U bw  X  x  X  w 5  X  U fee  X  x  X  w 6  X  U down  X  x  X  w 7 where
U bw  X  x  X  represents the utility of the available bandwidth in state x . Usually, agents tend to maximise the bandwidth, which is represented by the following equation:
U bw  X  x  X  X  where V bw max stands for the maximum possible bandwidth of a peer. In order to calculate the current available bandwidth v bw , x the formula proposed in Frioli and Pedrolli (2008) for the PeerSim simulator has been used.

U fee  X  x  X  represents the utility of the fee the agent is paying for connecting to the network calculated as follows:
U fee  X  x  X  X  where V fee max and V fee min stand for the maximum and minimum fee defined in the system and, v fee , x is the fee the peer is paying in the current state. Usually, peers would tend to minimise the fee.

U down  X  x  X  stands for the utility of the number of downloading blocks. Agents tend to maximise this utility and it is calculated by
U where V max down , x stands for the number of blocks of the current file that are not downloaded yet (in x ). v down , x represents the blocks the agent is downloading in the current state. Blocks that are requested but not yet downloaded are weighted by 0.5.

U up  X  x  X  is the utility of the number of blocks a peer is uploading to other peers. This attribute is calculated similar to U
The higher the number of uploading blocks in a given state, the higher is the value of U up  X  x  X  .

U  X  x  X  represents the utility of the time spent on downloading a file and peers usually try to minimise it. It is calculated as follows:
U  X  x  X  X 
V max and V t min represent the maximum and minimum time required to download the current file. It is not possible to calculate these values exactly because they depend on the network usage and the available bandwidth of peers. Instead, upper and lower bounds are used. In case of the maximum, it is estimated assuming the peer will request all missing blocks to the neighbour with the highest latency. For calculating the minimum, it is assumed that the remaining blocks are down-loaded from the neighbour with the lowest latency.
 All individual utilities obtain values in the range  X  0 , 1 .
The parameters w 5 , w 6 , w 7 , w 8 and w 9 are the weights assigned to each attribute, such that the sum of all of them is 1. By means of the different weights, it is possible to modelled different types of peers, that is, peers with different preferences. 4.3. Regulating the system
In order to evaluate the proposed approach, the performance of the system has been analysed when it is regulated by the following regulation mechanisms: a standard normative mechan-ism and the two incentive mechanisms BestAction and AllActions .
Furthermore, the results are also compared to the case where no additional regulation is employed.

Regarding the normative mechanism, it is based on a set of norms (defined at design time) that are coupled with sanctions to be applied when norms are violated. The following three norms have been defined:
N1:  X  X  X t is prohibited to use more than 85% of the assigned bandwidth X  X ;
N2:  X  X  X  peer is obliged to upload a block when at least 25% of the bandwidth is available and at least one request has been received X  X ;
N3:  X  X  X t is prohibited to request a block to more than 85% of the neighbours X  X .

The different percentages have been selected after some tests, but they could be modified by the designer. Norm violations are detected with a 100% efficiency and sanctioned with an increase of the fee in 10 units (equally for all agents).

Regarding the incentive mechanism, it is deployed by taking advantage of the nature of p2p systems. That is, incentivators are located at network service providers. Thus, the communication among them will be faster. Incentivators are authorised to modify the bandwidth assigned to peers and the fees peers pay to connect to the network. That is, the space of possible modifica-tions the incentivator for the peer (agent) ag i is able to apply, is given by the values of the attributes, fee agi A  X  value min tions of the fee or of the bandwidth assigned to a peer.
The reduction of the environmental state space is performed following the method presented in Section 3.2 . Each incentivator focuses on the following  X  X  X elevant X  X  attributes ATT 0  X f blocks , C
C , fee g . Furthermore, the value domains of these parameters are reduced using o  X  3. 4.4. Experimental results
In the experiments, the simulation tool has been instantiated with the parameters presented in Table 2 . These parameters have been selected after some experimental test.

Using these parameters, three different experiments have been carried out in which agents are generally non-collaborative, that is, they are interested in downloading but not in uploading files.
In the first experiment, the system is populated with agents that are sensitive to modifications of the fee they are paying for connecting to the network. Thus, the sanctions established in the standard normative mechanism will be effective because they are defined in relation to the fee. In the second experiment, the system is populated with agents that are not very sensitive to modifications of the fee. This describes a case where the designed norms (all based on an increase of the fee as a punishment) will not be very effective for the given population of agents. In the third case, the system is populated at the beginning with agents that are sensitive to changes of the fee, e.g., the standard normative system is effective. However, after several time-steps, a group of agents joins the system that do not care so much about higher fees. Thus, these agents do not behave in a desired way when using the normative regulation mechanism.

All three experiments have been repeated four times with a different initial network of peers. The exposed results represent the average of the four executions. 4.4.1. Experiment 1: non-collaborative agents, sensitive to the fee
In this experiment the objective is to test if the proposed incentive mechanism is as effective as an standard normative mechanism when norms are designed in an appropriate way. The system is populated with 70 peers modelled as non-collaborative agents that are sensitive to an increase of the fee, that is, they are sensitive to the sanctions in the normative mechanism. This is reflected in the agents X  utility function by setting the weights as follows: w 5  X  0.2, w 6  X  0.399, w 7  X  0.3, w 8  X  0.001 and w
The performance of the system is compared for the following four configurations: (i) without any regulation ( NoMechanisms ), (ii) with the standard normative mechanism ( Normative ), (iii) with the incentive mechanism using the BestAction policy ( Incen-tivatorAgentsBestAction ), and (iv) with the incentive mechanism using the AllActions policy ( IncentivatorAgentsAllActions ).
Fig. 5 (b) plots the average utility obtained by all peers participating in the system. As the figure shows, agents obtain the highest utility when the system is regulated by our proposal working with the BestAction policy. This happens because the incentivator agents convince external agents by making the resulting states more attractive for them, i.e. giving them a  X  X  X ositive X  X  incentive. Thus, the utility obtained by the agents in those states increases. In contrast, the normative system works rather badly regarding the agents utility. The reason is that the normative mechanism forces agents to behave according to the systems preferences by punishing them with an increase of the fee. Since the agents are sensitive to the fee, an increase of this attribute causes them a loss of utility, working even worst than when the system is being regulated by no mechanism ( NoMechanisms ).
Regarding the overall system, Fig. 5 (a) plots the evolution of the global utility for each configuration. It shows that the utility of the system is low when no regulation takes place. This was expected because the analysed population does not behave according to the preferences of the system (agents are non-collaborative). The normative mechanism is able to improve the global utility quite fast due to the usage of appropriate norms and effective sanctions. Regarding the incentive mechanism, both versions are a bit  X  X  X lower X  X  at the beginning, due to the learning process, but they are able to converge with the normative system after some time. This means that incentivators are able to learn which actions are good for the system in each state, and how to convince agents to perform those actions.

Fig. 5 (c) compares the four mechanisms regarding the percen-tage of leechers that are able to download the whole file over time. When no regulation mechanism is used, almost none of the peers are able to obtain the file. With the normative mechanism, about a 60% of leechers download the file; and the number increases to about a 90% and a 80% in the case of the AllActions and BestAction mechanisms, respectively. Again, it can be observed that the normative mechanism is effective very fast, whereas the incentive mechanisms need some time to reach its effectiveness (due to the learning process).
 Regarding the two incentive policies, one can observe how the AllActions policy performs slightly better than the BestAction policy on the global utility and the number of leechers that are able to download the file. However, it is worse when considering the utility of the agents. This happens because with the AllActions policy negative incentives may be assigned to some actions, and when applied, will result in a decrease of agents X  utility.
Concluding, in the case of systems where previous knowledge about the agents X  preferences and the domain is available and, thus, effective norms can be defined at design time, our incentive mechanism is still able to regulate the system in a similar way than a standard normative system. The incentive mechanism requires some time to be effective, but after that learning period it leads to better results when considering the average agents X  utility and the number of peers that are able to download the whole file. 4.4.2. Experiment 2: non-collaborative agents, not sensitive to the fee
This experiment simulates a case where a standard normative system is not very effective because of taking wrong assumptions about agents X  preferences when defining sanctions. In particular, the sanctions in the norms are related to an increase of the fee, but the agents in the system do not care so much about the fee they are paying. The latter is reflected by the following weights of the attributes in the agents X  utility functions: w 5  X  0.299, w w  X  0.499, w 8  X  0.001 and w 9  X  0.2 (in particular the weight of the fee w 6  X  0.001).

Fig. 6 (b) plots the average utility obtained by all peers. As it can be observed, the Normative system is not effective. It obtains the same result as NoMechanisms (both lines overlap in the figure). On the other hand, better results are obtained when the incentive mechanism is employed (for both versions AllActions and BestAction ).
That happens because incentivators are able to identify that instead of the fee, the bandwidth has an influence on peers X  utility. It uses modifications in the available bandwidth to make the upload of blocks to other peers more attractive. Regarding the system X  X  utility, shown in Fig. 6 (a), as it was expected, the system improves its performance when it is regulated by any of the incentive mechan-ism because they are able to adapt to the given population and assure that agents share their bl ocks. In contrast, the normative approach performs very badly. The norms lose effectiveness, because they are enforced by punis hing agents with an increase in the fee and the agents are not sensitive to this attribute. The normative system performs even worse than the system without any regulation. This is basically due to the fact that agents violate the norms because they are not very sensitive to the sanctions (higher fees). As a consequence, fees are increased and this fact has a negative effect on the objective to maintain the sum of the fees at a constant level (reflected by the attribute U avfee of the global utility function).

In Fig. 6 (c) it can be observed that only about a 10% of leechers are able to download the whole file when using the normative mechanism, while 80% and 60% download the file when the incentive mechanisms are running.

The experiment makes clear that a normative system does not fulfil its goal if the agents are not sensitive to the specified sanctions. In contrast, the proposed incentive mechanism is able to adapt to such situations by learning good actions for each situation and discovering the attributes that have some influence on agents X  preferences. These results are especially important for open systems where the population of agents is unknown before-hand and, thus, it might be difficult to define effective sanctions to enforce norm-compliance. Besides, in very complex and dynamic domains, it might also be difficult to define, at design time, which actions should be performed by the agents in each situation in order to improve the efficiency of the system. 4.4.3. Experiment 3: new peers joining the system
In this experiment the objective is to test how the proposed incentive mechanism adapts to changes in the agent population.
In order to simulate such a situation, the system is populated at the beginning with 20 agents that are sensitive to an increase of the fee. This is reflected in the agents X  utility function by the weights: w 5  X  0.2, w 6  X  0.399, w 7  X  0.3, w 8  X  0.001 and w
After 75 time-steps, a group of 50 agents joins the system that are not sensitive to the fee. These agents use a utility function with the following weights: w 5  X  0.299, w 6  X  0.001, w 7  X  0.499, w  X  0.001 and w 9  X  0.2.

The result can be observed in Fig. 7 . It can be clearly seen how the performance of the system changes at the time step 75, due to the entrance of the new agents. The reason of the drops in the utility at that point is that the new agents do not yet have downloaded any blocks. After time step 75 the agents X  utility recovers smoothly because the new agents start to download blocks ( Fig. 7 (b)). Similar to the previous experiments, agents get less utility when no regulation exits and when the system is regulated by the standard normative mechanism. Again, this is due to the negative impact of the sanctions on the agents X  utility.
Regarding the utility of the system ( Fig. 7 (a)), it can be noted that the standard normative mechanism performs better than the incentive mechanisms at the beginning of the experiment. How-ever, this changes after the new agents have joined the system, and the incentive approaches start to outperform the normative mechanism. This is due to the capacity of the incentive mechan-isms to adapt to the new population, possibly discovering that a modification of the bandwidth is a more effective incentive for them than a modification of the fee.

Finally, as to the percentage of leechers that are able to download the whole file ( Fig. 7 (c)), it can be observed that with the normative approach, at the beginning, about 65% of leechers are able to download the file due to norms and sanctions are effective. However, when new peers join the system, only about 35% of leechers are able to download the file. On the other hand, with the incentive mechanisms this number increases to about a 75%, in case of the AllActions policiy and, about a 55% with BestAction policy. The drop at time step 75 is again due to the entrance of new agents that drastically reduce the percentage of peers in the system that has already downloaded the file.
In conclusion, this experiment shows how the incentive mechanism is able to adapt to changes in the system such as the entrance of a group of new agents. The mechanism is able to identify the actions these new agents should perform and the incentives that may persuade them to choose such actions.
In all three experiments, it can be observed that the AllActions policy works slightly better than the BestAction policy regarding the system X  X  utility. The reason is that with this policy, incenti-vators influence the whole agents X  action space, trying to per-suade them to perform actions that are positive for the system, or, at least, preventing them to select undesired actions. 5. Related work
Many approaches have been proposed, in the field of multia-gent organisations, with the aim of regulating the activities of agents in Open MultiAgent Systems (e.g., Esteva et al., 2001b ;
Dignum et al., 2004 ). In most of them, the concept of norm appears as a main piece. Some approaches, like Esteva et al. (2001a) , focus on defining the set of allowed actions in each possible state of the system and assure that agents are just allowed to perform the valid actions. Other approaches, like
Dignum et al. (2004) , propose to couple norms with penalties and/or rewards which give agents the possibility to violate a norm. In these approaches norms are defined at designed time and it cannot be assured that the defined norms lead actually to an improvement of the systems efficiency. Furthermore, normally these systems are not flexible in the sense that they do not consider the violation of a norm as something that might be desirable under certain conditions. Any norm violation is treated in the same way, regardless its effect on the global utility of the system. Furthermore, the sanctions defined for norm violation are usually the same for all agents and are defined based on some assumptions with regard to the preferences of the agents that will populate the system. The incentive mechanism proposed in this paper, does not assume a priori knowledge about agents X  prefer-ences. Furthermore, it applies incentives with regarding to the effect of actions on the global utility rather than in order to avoid norm violations.

Some researchers have worked on the adaptation of normative systems by modifying the set of norms. For example, in Campos et al. (2011) the authors propose a model for adapting the normative framework of an Electronic Institution, where the set of norms can be modified at runtime. Another example of this approach is Tinnemeier et al. (2010) , where authors propose a model for allowing designers to increase, reduce or modify the set of norms at runtime. They can specify when and how norms can be modified. In both cases, however, the modifications that can be applied are limited to some parameters of the particular norms and the adaptation is not directly driven by the effect on the global utility of the system. Furthermore, in these cases sanctions are also not individualised to agents and situations.

Other works focus on the adaptation of the enforcement mechanisms associated to normat ive systems. In particular, they propose to adapt the sanctioni ng mechanisms. For example, Lopes
Cardoso and Oliveira (2011) face this problem by adapting the sanctions to the population, taking into account the number of times a norm is violated. The main idea is that, the higher the number of violations of a norm, the higher should be the sanction, and vice versa. However, the same sanctions are applied to all agents. The approach in Centeno et al. (2011) is similar, but here in this case to each individual agent and taking into account a given situation. In both cases, however, the objective of the adaptation algorithms is to increase norm compliance (and not the improvement of some global utility). The norms themselves do not change.

All previously mentioned approaches rely on the use of norms to regulate the agents X  action space towards valid states. They implicitly assume that norm compliance will achieve the objec-tives of the system. However, this may not always be the case because the agents may still have the freedom to select different actions which may lead to different global utilities.

Another set of works has been proposed in the last years that propose to inducing agents to behave in a desirable way. These approaches are similar to ours in the sense that they can be used to regulate the space of freedom left by normative systems between the valid actions and the actions actually performed by agents. In Rabinovich et al. (2010) , for example, authors focus on how an interested party, called the teacher, is able to encourage an agent a adopt a desirable behaviour specified by a policy. This is obtained by modifying the dynamics of the environment in terms of small changes, called tweaks . One of the main difference with the incentive mechanisms proposed in this paper is that this work models the problem for just one agent. That is, changes produced by other agents in the environment are not taken into consideration. Besides, our mechanism is designed with the objective of being effective in systems where the individual objectives of the agents might be in conflict with the global objective of the system.

A similar work is proposed by Dufton and Larson (2009) , where the authors extend the idea of  X  X  X olicy-teaching X  X  proposed in Zhang and Parkes (2008a , b) , to a multiagent setting. This approach is quite similar to ours in the sense that the modifica-tions made by the interested party in the environment might have same influence on all agents in the system. However, the main difference with our work is that they assume that the interested party knows the reward functions of each agent, while in our case, this assumption is not made. Instead, each incentivator has to discover the preferences of its external agent in order to identify the most appropriate incentives. Furthermore, Dufton and Larson  X  X  approach is based on a centralised perspective because the interested party is able to observe the behaviour of all agents so as to calculate the optimal incentives. This view might be valid from a theoretical point of view, but might not be applicable in some practical systems due to the distributed nature of Open
MultiAgent Systems. The incentive mechanisms proposed in this paper tries to overcome this limitation by estimating the global utility based on the local few of each individual incentivator.
From a conceptual point of view, the notion of coordination artifacts ( Omicini et al., 2008 ) has been presented as a means to implement mechanisms that have the aim to improve the overall performance of a multiagent system. A coordination artifact is an abstract model that is conceived as a persistent entity specialised in providing a coordination service in a MAS. An example of this sort of entity is presented in Hermoso et al. (2010) , where an artifact is used to provide useful information to the agents in the system. The mechanism presented in this paper could be designed, implemented and deployed in a MAS using these type of abstract model, as it is presented in Esparcia et al. (2011) .
Finally, mechanism design ( Parsons and Wooldridge, 2002 ) also deals with identifying the appropriate (interactions) rules of the system regarding some global utility function. However, such approaches usually assume that the agents X  pay-off functions are known. In the domains focused on in our work, this assumption does usually not hold and the agents X  preferences have to be identified during runtime, as it is done in our proposed approach. 6. Conclusion
This paper has put forward an incentive mechanism that is able (1) to learn which actions should be performed by each agent populating an OMAS, in each individual state (from the perspec-tive of the global system) and (2) learns how agents can be incentivised to perform the desired actions. The mechanism is deployed using a network of institutional agents called incentiva-tors . Each incentivator is in charge of one external agent. Incenti-vators learn  X  in a cooperative way, using Q -learning and a gossip-based reward calculation algorithm  X  which joint actions should be incentivised in each situation and how agents may be induced to perform those actions.

Based on the learning process, two different incentive policies have been defined. With the BestAction policy an incentivator tries to persuade its agent to perform the best action  X  from the system X  X  point of view, in the current state  X  by assigning it a  X  X  X ositive X  X  incentive (i.e. an incentive desired by the agent). On the other hand, with the AllActions policy, incentivators assign incentives to all actions an agent can perform in a given state. In this case, the incentivator establishes a ranking of the available actions and sets up  X  X  X ositive X  X  or  X  X  X egativ e X  X  incentives accordingly.
The proposed incentive mechanism does not limit the auton-omy of agents in any way. It rather relies on changing the environment and assumes that rational agents will adapt their behaviour accordingly. The agents in the system are still free to chose the actions they consider.

The mechanism has been tested in a p2p file sharing scenario, showing that it is a valid alternative to standard normative systems. In particular, it performs in a similar way as a standard normative system in cases where the norms (and sanctions) are effective and improve the system utility. On the other hand, the incentive mechanism outperforms normative system with fixed norms in scenarios where the design assumptions of such norms are not fulfilled. Furthermore, the mechanism is appropriate for systems with changing populations of agents, because it adapts its incentive strategy to the particular preferences of each agent.
As future work, other learning techniques can be explored that might be more suitable in scenarios where the preferences of the agents may vary over time. In principle, Q -learning can deal with such situations, but it may be too slow to obtain the desired adaptation. Furthermore, the mechanism is currently only able to perform the modification of just one attribute at the same time. In this sense, it might be interesting to explore the possibility of providing incentives based on the modification of a set of attributes.
 Acknowledgments
We acknowledge ITMAS 2011 as the forum in which the main ideas behind this paper were preliminary discussed. The present work has been partially funded by the Spanish Ministry of Education and Science under projects: OVAMAH (Grant no. TIN2009-13839-C03-02 co-funded by Plan E) and  X  X  X greement Technologies X  X  (Grant no. CSD2007-0022, CONSOLIDER-INGENIO 2010) and by the Spanish Ministry of Economy and Competitiveness through the project iHAS (Grant no. TIN2012-36586-C03-02).
 References
