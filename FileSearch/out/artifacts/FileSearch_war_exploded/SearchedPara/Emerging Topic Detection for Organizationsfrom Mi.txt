 Microblog services have emerged as an essential way to strength-en the communications among individuals and organization-s. These services promote timely and active discussions and comments towards products, markets as well as pub-lic events, and have attracted a lot of attentions from orga-nizations. In particular, emerging topics are of immediate concerns to organizations since they signal current concerns of, and feedback by their users. Two challenges must be tackled for effective emerging topic detection. One is the problem of real-time relevant data collection and the other is the ability to model the emerging characteristics of detect-ed topics and identify them before they become hot topics. To tackle these challenges, we first design a novel scheme to crawl the relevant messages related to the designated orga-nization by monitoring multi-aspects of microblog content, including users, the evolving keywords and their temporal sequence. We then develop an incremental clustering frame-work to detect new topics, and employ a range of content and temporal features to help in promptly detecting hot e-merging topics. Extensive evaluations on a representative real-world dataset based on Twitter data demonstrate that our scheme is able to characterize emerging topics well and detect them before they become hot topics.  X 
This work was done when the first author was a visiting scholar in the National University of Singapore.
 H.0 [ Information Systems ]: General; H.3.5 [ Information Storage and Retrieval ]: Online Information Services Algorithms, Experimentation Microblog Service, Emerging Topic Detection, Brand Mon-itoring, Organization Monitoring
Microblog services such as Twitter and Weibo provide an essential platforms for users to convey their thoughts, exchange their opinions and share their experiences. One key reason leading to their popularity is their real-time na-ture. On these platforms, individuals update their status regarding various topics, spanning from  X  X hat are they do-ing X (Twitter), to X  what are on their mind X (Facebook), and this is conveyed instantaneously to their friends. This great-ly strengthens inter-personal exchange and cooperation.
Besides facilitating communications among individuals, microblog services also explicitly or implicitly contain rich information towards organizations, such as banks, universi-ties, and government organizations, etc. Many organizations are keen on continually mining and analyzing these user-generated social data due to the following reasons. First, social data contains the interests, concerns and criticism-s of their users, and provides pointers for organizations to improve their products or services. Second, social data im-plicitly contains invaluable market insights for the organiza-tions. The primary foundation of these high-level applica-tions is based on topic monitoring and tracking. Specifically, organizations would like to: (1) track the evolution of any identified relevant topics about them; and (2) be informed of any new emerging topics which are fast gathering momen-tum in microblogs.
 Figure 1: The key phases of an hot emerging topic.
Several threads of research have been done on emerging topic detection. They can roughly be grouped into three categories. The first is based on traditional topic detection methods to identify new terms [12, 6, 21]. The second is to utilize topic model to learn a dynamic word distribution or topic transition [24]. The last is to detect novel topic-related words by dictionary learning methods [11, 19]. However, the definition of  X  X merging X  in previous literature focuses only on the novelty of topics, and they mainly model the novel words based on word co-occurrences within the topics. In this work, we extend the definition of  X  X merging X  to incor-porate temporal aspect of timeliness. In other words, we want to detect emerging topics that are not only novel, but also those that will become hot and viral in the near future. This presents an additional challenge to model the temporal characteristics of topics in real-time.

Figure 1 presents the evolution of a hot topic starting fromthetimethatitisdetected( t s ), to the time that it becomes hot ( t hot ). The period from t s to t hot is known as the emerging phase. We expect to identify this topic as hot and emerging before t hot .

However, emerging topic detection for organizations from microblog context faces several challenges. The first chal-lenge is the dilemma of relevant data collection. This is a challenging issue as most live microblog services impose limits on the amount and frequency of data that can be crawled 1 . This, in conjunction with the low ratio of relevant data in microblog content, results in missing relevant data about the organizations. The second challenge is on model-ing of topics with effective features to facilitate the detection of hot topics during the emerging phase.

To address these two challenges, we design a novel scheme to monitor and collect microblog messages (tweets) about or-ganizations as well as detect the hot emerging topics prompt-ly. It comprises two stages as illustrated in Figure 2. The first stage aims to gather rich social data with good coverage for a designated organization. Specifically, given a specific organization, we collect data in multiple aspects from four sources including fixed keywords, emerging keywords, known accounts and key users of the organization. All the crawled data is then sent to a binary SVM classifier, which discrimi-nates the relevant tweets from the huge amount of irrelevant ones. During this process, an organization user network is also maintained based upon the existing relationships 2 be-tween users within the organization. The second stage first employs the well-known incremental clustering algorithm-s [25, 2] to discover topics in real time. It then analyzes the emerging topic-related features including user authori-ty, tweets influence, and organization attributes such as the
This limit is, for each request, X  X p to 1% X  X f Firehose tweets for the streaming API of Twitter.
The existing relationship includes the follower, followee and friends, etc.
 emerging keywords. These features are then incorporated into the topic learner to identify hot emerging topics in a timely manner.

The main contributions of this paper are as follows:
The rest of this paper is organized as follows: Section 2 introduces the related work of emerging topic detection. Sections 3 and 4 respectively detail organization-related data selection and emerging topic detection. Our experimental results are presented in Section 5, followed by concluding remarks in Sections 6.
The popularity of microblog portals like Twitter has en-abled hot topics to be quickly propagated to a large number of users over wide geographical regions. Research on de-tecting emerging and evolving [8, 25] topics 3 of live tweet streams has gained much interest in recent years, and has been applied to a wide variety of applications [7], such as detecting emergencies like earthquakes [20], predicting po-litical election outcomes [22], mining topic and evolution [9], discovering controversial topics from twitter [17], and so on. There are several lines of research in this direction.
One line of research is based on the traditional topic de-tection approach. From the feature pivot aspects, some key-words based approaches [12] work well on mining tweets about specific topics. While high frequency of terms may be a good indicator for hot topics or trends, it does not i-dentify new of emerging trends. Cataldi et al. [6] defined emerging keywords as those which are frequently used in a given time period, but not in previous ones. They pre-sented an approach to identify emergent keywords and uti-lized them together with frequently co-occurring words as emerging topics. From the document-pivot aspects, Sayyadi et al. [21] created a keyword graph, and used it to cluster tweets based on various distance and similarity metrics.
Probabilistic latent semantic analysis (PLSA) and Laten-t Dirichlet Allocation (LDA) [5] are probabilistic methods
Here, topic can be exchangeable with event. that have found remarkable success in building topic models of static text. Variants of PLSA and LDA have been pro-posed for online and dynamic topic modeling [4].Wang et al. [23] took advantage of temporal information, and tried to model the topics continuously over time. They learned the dynamic word distributions and trends of topics over time. Wang et al. [24] proposed a Temporal-LDA or TM-LDA method to mine streams of social text such as the Twit-ter stream for an author, by modeling the topics and topic transitions that naturally arise in such data. Different from the work of [23], TM-LDA focuses more on learning the re-lationship among topics.

Another line of related research is on dictionary learning and non-probabilistic matrix factorizations based methods. Kasiviswanathan et al. [11] proposed a two stage approach based on the detection and clustering of novel user-generated content. They derived a scalable approach by using the al-ternating directions method to effectively solve the result-ing optimization problems. By extending the above work, Saha and Sindhwani [19] adapted Non-negative Factoriza-tion to learn trending topics in the context of social media. They showed that better topic modeling performance can be achieved, when the continuity between topics matrices in consecutive time stamps is taken into account.

Previous research on emerging topic [18, 1] detection main-ly focused on keywords and textual content, whereas we aim to find emerging topics with respect to an organization. The major difference is that for entities like organizations, in ad-dition to textual content, user association to the organiza-tion and social relations among users of the organization will greatly affect the detection of emerging topics for the orga-nization. These features have not been utilized before due to the focus on general emerging topics.
To perform high-order analytics, it is desirable to collec-t a relatively complete set of relevant data for the target organization in an effective way. However, such a task is of-ten overwhelmed by the tremendous amount of relevant as well as irrelevant data. To ensure comprehensive data col-lection, two interconnected observations can be made: (a) users related to organizations are more likely to post tweets related to the organization; and (b) tweets on organization often contain organization related keywords. These two ob-servations enable us to generate descriptive keywords and cues, such as fixed keywords, dynamic keywords, known ac-counts, and organization keyusers. Accordingly, we design four intelligent crawlers to comprehensively crawl organiza-tion relevant data from multiple aspects, as shown in Fig-ure 3. The generation of four aspects of sources are detailed in the following subsections.
Given the name of the organization, we first manually se-lect a few fixed keywords that uniquely identify the organi-zation, such as the name of the organization, the key terms of its brands, and the name of its CEO, etc. These fixed keywords are used in the streaming based Fixed Keyword Crawler .

To elicit a live and more diverse set of relevant tweets about the organization, typically those that do not contain Figure 3: Crawling of data from multi-aspect sources. the organization keywords, we extract a list of temporal-ly relevant emerging terms about the organizations at each time point t . Emerging terms are defined as those newly introduced terms that are able to represent emerging topics about the organization. To identify these terms, two sets of foreground ( S t for ) and background ( S t bak )tweetsateach time point t are constructed. All the relevant tweets in the current time slot [ t  X  T,t ] are regarded as S t for tweets, where T is the time interval. While S t bak covers all the tweets sent during the three time slots of: [ t  X  2 T,t  X  T ] of current day, [ t  X  T,t ]ofpreviousday,aswellas[ t  X  T,t ] of one week ago. This definition aims to filter out the time related peri-odical terms, such as morning, Friday, etc. The vocabulary sets of these two sets of tweets are respectively denoted as, W goal is to identify the terms that have distinctively different distributions in S t for and S t bak . This signals that the term is changing its behavior over time. Specifically, in terms of statistics, given two distributions of a term w i in S t for S bak , we expect to disprove the null hypothesis that the two distributions are drawn from the same distribution function to a certain level of significance. Those terms with rising frequencies are the potential emerging keywords, whereas those with decreasing frequencies will gradually disappear. In this work, we use the chi-square test [14] to compare t-wo distributions due to its efficiency and ability to handle rapidly evolving microblog contents. Formally, for each word w i  X  W for , its chi-square distribution is: where f i and b i are the normalized word frequency values of w i in S t for and S t bak respectively, defined as:
Basedonthevalueof  X  2 , a list of ordered terms is gen-erated, and the terms in the top N positions are truncated as the dynamic keywords. Each dynamic keyword will be concatenated with the acronym of the target organization to form a specific query for data collection.
Similar to fixed keywords, we manually identify a set of organization related accounts. These are typically official accounts of the organization on microblog platform that of-ten post relevant tweets about the target organization, such as news about the target organization. These known ac-counts are monitored by the streaming based Known Ac-counts Crawler .
The above mentioned three kinds of crawlers explicitly i-dentify and collect data about the target organization. How-ever, they overlook some important tweets posted by the users related to the organization. The tweets are relevan-t to the target organization, but in implicit form, i.e., not containing the fixed or emerging keywords. Org Keyusers Crawler intends to plug this gap by exploring the sources from the perspective of users. At time t , given a time inter-val T (such as 24 hours), we obtain a subset of users U  X  who post at least one relevant tweets in the time window  X  t =[ t  X  T,t ]. Here, we regard org keyusers as those people who post more about the organization and have many fol-lowers (a larger influence) in the time window  X  t .Asmen-tioned before, an organization user relationship graph G 0 constructed in real time. Nodes of G 0 are obtained from the known accounts, and contain all the users who posted at least one organization relevant tweets, as well as their friends and followers. Edges of G 0 are obtained by crawling the social relationships between them. In addition, we want to incorporate their activity degree during  X  t .Theactivity degree of a user is proportional to the number of tweets the user sent during  X  t . We can compute the authority score of user u i by incorporating the activity degree of user into graph G 0 as follows. where  X   X  (0 , 1) is a damping factor, following ( u j ) stands for a set of users that u j follows, Tw  X  t represents the rele-vant tweets in  X  t time slot, and Tw u i  X  t is the relevant tweets of u i in  X  t . We calculate users X  authority score in an iter-ative manner, i.e., when we calculate the authority value of u i in the k th iteration, we utilize u j  X  X  ( k  X  1)th authority score. We then rank users in U  X  t by their authority scores and the top N users are selected as the org keyusers.
The data collected from the four sources are a mix of relevant and irrelevant tweets to the organization. In order to filter out the noisy data, we utilize a standard two-class SVM classifier. For the training data, we regard all the tweets from known account source and fixed keyword (rule based to select) source as relevant.
We first present an incremental clustering method to dis-cover topic collections. Through extracting features from topic and organization views, we train two semi-supervised hot emerging topic learners.
For our real-time scenario, we need to handle live and large volume of tweets about an organization to detect topics without any prior knowledge of the number of topics, since the topics are constantly evolving and growing in size. On-line or incremental clustering algorithms, which are able to Algorithm 1 Incremental Clustering for Topic Discovery 1: Input: tweet sets D ,topicclusterset C , cluster center 2: Output: update topic clusters C , and update cluster 3: Process: 4: if C =  X  then 5: random select N tweets from D and add into C and Center . 6: end if 9: for center j  X  Center do 10: compute Cosine Similarity sim between center j and d i 11: if sim &gt; max then 13: end if 14: end for 15: if max &gt;  X  then 17: else 18: new cluster and centroid and add to C and Center . 19: end if 20: end for 21: return C and Center . handle a constant stream of new tweets, are desirable in our setting, where new tweets are continually being produced. We employ a single-pass incremental clustering algorithm [3] with a threshold  X  .Ateachtime t , and within a time in-terval T ,weobtainalltweets D during [t-T, t] in a time order. Such a clustering algorithm considers each tweet in turn and determines the suitable cluster assignment based on a similarity function. The algorithm considers each tweet d in order, and computes its similarity ( d i ,Center j ) against each existing cluster C j . If the maximum similarity value is greater than  X  , the tweet will be distributed to the cluster, meanwhile the clustering center will be updated. Otherwise, we will generate a new cluster and cluster center. The details of this algorithm are shown in Algorithm 1.
In Section 3, we have extracted emerging keywords and org keyusers for the target organization from a global view point. To infer the importance of a topic within an organi-zation, we need to examine also the influence of tweets and users at the local (topic) level.
Given a topic tp , there is a set of users related to tp , which is denoted as U tp = { u 1 ,u 2 , ..., u i , ..., u that the authority score of a user with respect to a specific topic u i  X  U tp is related to three factors. u i will have a larger influence on topic tp if u i has: (a) posted many tweets about topic tp ; (b) posted more tweets retweeted by other users in U tp ; and (c) more followers in U tp . Based on this observation, at time t , we can compute the authority score for each topic user u i  X  U tp as follows. where r u i is the total number of relevant tweets posted by u ; f u i is the total number of u i  X  X  followers who exist in U tp ; q u i is the total number of u i  X  X  relevant tweets that has been retweeted by other users; and  X  ,  X  and  X  are weighting parameters i.e.,  X  +  X  +  X  =1.
We can also find a set of tweets related to a topic tp ,which are defined as Tw tp = { tw 1 ,tw 2 , ..., tw i , ..., tw tweets are posted by users U tp . There are two intuitions: (a) if a tweet tw i has a strong influence, it should be propa-gated to a large scope and be retweeted by a relatively higher number of times; and (b) if the tweet is posted by a topic authority user, it should also have the potential to influence more users. Thus, given a tweet tw i in topic tp ,weemploy the number of retweets and authority of users to evaluate its influence, defined as follows. where auth tp ( tw i ) is the influence of tweet tw i ; auth is the author of tw i  X  X  authority; and U rtw i represents the user group that retweets tw i .

Let W tp = { w 1 , ..., w i , ..., w r } be the set of words that appear in topic tp .Foreachword w i in topic tp , we compute its weight Weight tp ( w i ) through the influence of tweets that it appears in, as:
We use Weight tp ( w i ) to rank the list of topic-related key-words.
In order to identify the hot emerging topics from the topic collection at each time t , we should analyze and extract the emerging features of topics. Given a target organization at time t , from an organization view point, we extract key users and emerging keywords for the target organization, and from a local topic view point, we calculate the authority of users, tweets and keywords for a specific topic. By combining these two views, we extract six representative features for each topic at time t to train the emerging topic learner. The six features with respect to topic tp are defined as follows.
These six features were chosen to discriminate an hot e-merging topic from the topic collection. For an emerging topic, the number of participating users, the increasing rate of tweets, and the number of retweets are expected to be distinctively higher than the normal topics and that of pre-vious time. Moreover, there are some clues that the key topic users and keywords would have a large overlap with the current org keyusers and dynamic keywords. As tweets are likely to be retweeted, the increasing rate of accumulat-ed weight of tweets are comparatively higher than those in previous time period too.

The design of our learners considers two factors in the microblog domain. (1) Since there are hundreds of cluster-s for an organization at each time t , the labeling process can be time consuming and labor intensive for all training data. Hence there will be the problem of insufficient train-ing data. (2) There is a problem of imbalance of positive (emerging phase) and negative (not emerging topics or not emerging phase for emerging topics) data, since the vast majority of data are negative instances. Hence the learner must be able to achieve good accuracy in the face of im-balance training data. The above two factors require that the chosen learners should work well under the conditions of sparse and imbalance training data scenario. Thus, we adopt a co-training learner and a semi-supervised ensemble learner for our learning task. We define the training data as TrainSet = { ( X l ,Y ) ,X u } ,where X l and X u denote the la-beled and unlabeled dataset respectively; and Y represents the label set ( positive ( p )and negative ( n )). Our aim is to learn a learner c : X  X  Y . Algorithm 2 Co-training Learner 3: Process: 4: while Unlabeled dataset { X u } =  X  do 9: Remove these from the unlabeled dataset { X u } . 12: end while
Features of topic tp at time t are divided into two orthog-onal views: v(1) the number increasing rate features ( f 1 and f 3 ); and v(2) the overlap features ( f 4 and f 5 )andthe accumulated weights of increasing rate feature ( f 6 ). We as-sume that both v(1) and v(2) are orthogonal to each other, and they are sufficient to train reasonably strong classifiers. We then train two basic SVM classifiers based on these two views. The training process is described in Algorithm 2. It is worth noting that in order to account for the imbalance data scenario, the construction of training instance set L ( L 1 and L 2 ) are different from the previous co-training ap-proaches. Here we keep all the positive instances in L , while sample only an equal number of labeled negative instances from the training data into L at each iteration.
As an alternative to co-training, we employ the voting based ensemble learning to train a semi-supervised classifier. Three classifiers (Decision Tree, SVM, and Naive Bayesian) are chosen to learn from the labeled training data to predict the unlabeled training data independently. Their results are used to vote for each unlabeled data instance. The consis-tent data will be added to the next training iteration, until convergence. Algorithm 3 describes the training process. Algorithm 3 Ensemble Learner 3: Process: 4: while Unlabeled dataset X u is used up do 7: Voting. 9: Remove these from the unlabeled dataset X u . 13: end while 14: return c .
The overall process of hot emerging topic detection schema is detailed in Algorithm 4. At each time t ,topicsarediscov-ered by Algorithm 1. For each topic, we extract the desired features and classify it using algorithm 2 or 3. If it is an emerging topic and the cluster id does not exist in ETSet , we will record the id and t into ETSet , which holds the list of emerging topic candidates.
In order to evaluate our approach, experiments were con-ducted on a real life dataset crawled for three organizations Algorithm 4 Overall Algorithm of Emerging Topic Detec-tion 1: Input: Start time point t 0 . 2: Output: Emerging topic list ETSet . 3: Process: 5: while D =  X  do 6: Detect topic using algorithm 1, and get topic cluster set C 8: Extract features of C i . 12: end if 13: end for 14: t = t + X  t . 15: Get tweets as D from t  X   X  t . 16: end while 17: return ETSet . of different nature in Singapore. They are StarHub 4 ,De-velopment Bank of Singapore ( DBS ) 5 and National Univer-sity of Singapore ( NUS ) 6 . These three organizations cover local telecommunication company ( StarHub ), the universi-ty ( NUS ), and a cross-border bank ( DBS ). We use twitter API 7 to collect the datasets that contain 51K, 130K, and 142K tweets from 15K, 44K and 36K users for the above three organizations respectively. Because there is no bench-mark for our emerging topic detection task, we generate the ground truth of hot emerging topics by adopting the follow-ing procedures. (1) We manually align the topics with online news and labeled the topic as hot emerging topic if its relevant tweets number at least doubled in the future 24 hours after its detection. Finally we obtain 24, 17 and 5 hot emerging topics respectively for the three organizations. (2) For each hot emerging topic, we label two time points: t and t hot , the start of the topic and the time when topic becomes hot, respectively (see Figure 1). t s is the first time slot in which the emerging topic is detected. t hot represents a time slot in which tweets number exceeds a threshold. A middle point t mid between t s and t hot is also computed automatically.

The statistics about the datasets for the three organiza-tions are detailed in Table 1. We list the time duration of data collection as well as hot emerging topic numbers. Ta-ble 1 also lists the initial period of data that we used for training. In our system, the numbers of org keyusers and dynamic keywords are set to 100 and 50 respectively, in or-der to limit the crawling resources required to monitor the keyusers and keywords. We empirically set the thresholds  X  =0 . 7,  X  =0 . 6and  X  =  X  =  X  =0 . 33. The two-class SVM and dynamic keywords mining are performed at intervals of every half an hour, while the org keyusers mining are per-formed at every 24 hour intervals. The time interval used for topic and emerging topic detection (Algorithm 4) is 1-hour.
Because of the lack of space, we only detail our experi-ments on topic detection and emerging topic detection only. The effectiveness of our data collection strategy will not be discussed here. It is tested indirectly through the accuracy of hot emerging topic detection experiments.
The performance of emerging topic detection model is based on the premise that the approach can fetch more orga-nization related topics. In this work, we compare our topic detection approach with several baselines to demonstrate its effectiveness:
For ease in evaluation, we utilize two sources of informa-tion to construct the ground truth for topic detection. The first is the emerging topics as shown in Table 1. The second is based on the idea of pooling by aggregating all the new topics detected by the four methods (CL, TM, TOT, and NN-Dict). We asked three people to evaluate the new topics detected and come up with the final set as the ground truth. The ground truth contain 34, 31 and 24 topics for StarHub , DBS ,and NUS . In this work, we use several widely-used classifier performance metrics for evaluation [13]: recall, pre-cision and F 1 . Figure 4 presents the performance of CL and three baselines for three organizations.

It can be seen from the Figure that NN-Dict and CL achieve better performance than TOT and TM across all evaluation metrics for all organizations. The main reason for the poor performance of TM is that it constructs an undirected graph and groups bursty keywords by maximum connected components. This probably leads to multiple se-mantically separate topics being merged together and there-fore produces very large topics that reflect very little about the real world topics. For TOT, it discovers topics clusters and their evolution over time. However, it often identifies old or non-informative topics as compared to the previously appearing ones. Overall, our method and NN-Dict can de-tect over 90% of topics with a F 1 measure of 70%. Finally, it is worth noting that the higher recall but lower F 1 of NUS suggests that there are not many topics happened in NUS as compared to StarHub and DBS .
This subsection aims to detect whether a newly found topic is emerging and will become a hot topic at a later stage. Hence there is a temporal dimension to this task, that the hot topic should be detected during the emerging phase as shown in Figure 1. Here we want to test the ability of the method to identify a topic as  X  X ot topic X  before a time T L . For evaluation purpose, we select two time limits: a stringent one with T L = t mid , and a more relax one with T
L = t hot (see Figure 1). If the topic is identified as hot topic before T L , it is considered a positive detection; otherwise it is considered a failure.

As our baselines are designed for novel topic detection, but not emerging topic detection, we need to incorporate time dimension into these methods. Here, we incorporate our two emerging topic learners, Co (Co-training learner) and En (Semi-supervised ensemble learner) with our proposed incremental learning method (CL) and the three baselines, giving rise to 8 combination of methods as shown in Figure 5, which list the F 1 measure of hot emerging topic detection when T L = t hot . Three main observations can be drawn from Figure 5. First, the topic detection methods incorpo-rating semi-supervised ensemble learner (CL+En, TM+En, TOT+En, and NN-Dict+En) generally perform much better than those incorporating the co-training learner (CL+Co, T-M+Co, TOT+Co, and NN-Dict+Co). The reason for the poor performance of Co is because the emerging features are split into two views with each view being weaker than the overall combined feature set. Second, NN-Dict and our pro-posed CL incorporating the emerging topic learners perform much better than the other two methods. This shows that Table 2: Performance of emerging topic detection when T L = t hot Table 3: Performance of emerging topic detection when T L = t mid a strong topic detection baseline is needed to achieve good performance in hot emerging topic detection. Here we also observe that the recall performance is better than precision for most cases. This is important as in real applications, the ability to flag all possible hot topics is essential for or-ganizations to handle all possible eventualities. Third, it is observed that our CL performs comparably in recall as compared to NN-Dict method but much better in precision, resulting in superior F 1 measure. In general, our method can detect close to 90% of hot topics with a precision of over 70%. This is an encouraging results for hot emerging topic detection.
The efficiency problem is very important for a real-time schema. In this subsection, we evaluate the average run time Figure 6: The impact of incoming tweets size on Run Time with different sizes of incoming tweets for different meth-ods. We run the algorithms on a computer with 2.83GHz Intel(R) Core 2 Quad CPU, and 4GB memory. The exper-imental result of logarithm deviation of run time varying with increasing tweets size is shown in Figure 6. It can be seen from the figure that the run time increases with rising incoming tweet number. TOT performs poorly as it needs to sample the tweets and iterate to convergence in several steps in each iteration. For TM, it spends much time on bursty keyword graph mining. The main cost of NN-Dict is to maintain and update the term dictionaries and to find novel clusters by clustering methods. Overall, incremen-tal clustering is the most efficient for real-time scenarios, as it explicitly maintains topics and related tweets for further analysis of emerging topic features.
Because of the poor performance of CL+Co, it will not be included in further experiments. Here we conduct fur-ther experiments to compare the performance of CL+En against two start-of-the-arts methods: Transductive SVM (TSVM) [10] and Semi-supervised Naive Bayesian classifiers (Semi-NB) [16]. As with the previous approaches, the pa-rameters of the methods are carefully tuned. And we test the ability of the methods to detect emerging topic at T L = t and T L = t mid .

Tables 2 and 3 show the precision, recall and F 1measure of the methods for T L = t hot and T L = t mid respectively. From the Tables, it is observed that our proposed incremen-tal clustering with ensemble learner (CL+En) performs the best against all the other methods, with a high F 1 of 0.90. The results show that our semi-supervised ensemble learner can inherit the advantages of each learner to make up for the shortcomings of a single learner. It is also observed that the recall is higher than precision generally, which is an impor-tant attribute of emerging topic detection methods. Table 3 presents the comparison results when T L = t mid ,which has a much more stringent criterion than that in Table 2. It naturally shows a performance reduction for all the learner-s. Finally, it is observed that performance of StarHub and DBS is better than that of NUS for the classification based metrics shown in Tables 2 and 3. The poorer performance of NUS is mainly due to the low number of tweets and lack of labeled data during the emerging phase.
In this subsection, experiments are carried out to investi-gate the influence of various emerging features on F 1 .We progressively remove one feature for our semi-supervised en-semble learner. The experimental results on StarHub , DBS , and NUS are illustrated in Figure 7. Here,  X  X  X  on x-axis means all the features are used for the learner, and  X -f* X  means that all the features except feature  X  X * X  are used in the ensemble learner. It is observed that the performance of F 1 on the three organizations are all degraded to a certain degree with the absence of some features. This observa-tion verifies that all features have some contributions to the learners towards achieving good performance. However, it can be observed that the absence of features f 2 , f 3 and f cause the most degradation in performance. This shows that the rates of increasing tweets number ( f 2 ), re-tweets num-ber ( f 3 ), and overall accumulated influence of tweets ( f are very important, and comparatively, user factors are less influential. This could be due to the fact that topic has only a small number of users. More testing needs to be done for future work. Finally, as compared to the other two organi-zations, NUS shows a much larger performance degradation with the absence of the above three factors. This is again due to the small training size for NUS .
As mentioned in Section 4.3, training data imbalance is also a challenging problem in our task. In this section, we briefly look into the impact of imbalance training data for the two semi-supervised learners. The F 1 performance of t-wo learners for the balance and imbalance settings is shown in Figure 8. It can be seen from the Figure that the F 1 performance of the two learners for the imbalance data de-grades greatly as compared to that with balance data. As imbalance data makes our learners bias towards negative in-stances, which will increase the false positive errors for the methods. This is worse for the co-training learner. As false positive error will be accumulated into the next iteration, the co-training classifier is more easily affected by the noisy data.
In this paper, we proposed a real-time framework for de-tecting hot emerging topics for organizations in social media context. First, we introduced four sources of crawling orga-nization data from multiple perspectives to ensure a more complete set of dataset for the target organization. Sec-ond, we discovered emerging topics and extracted emerging features from both the organization and topic perspectives. Thirdly, we developed semi-supervised learners to facilitate timely identification of hot emerging topics for organization-s. We demonstrated the effectiveness of our proposed frame-work by comparing them with the state-of-the-arts methods. Empirical evaluation on the Twitter datasets on three orga-nizations ( StarHub , DBS and NUS ) illustrated the effective-ness of the proposed emerging topic detection framework.
One can envision several directions for future work. While the current work is based on organizations to detect emerg-ing and evolving topics, we can extend our framework to more general entities, such as the people and location, etc. The other important direction is to build human readable emerging topic summarization for organization users.
This work was supported by the National Natural Sci-ence Foundati on of China (60973105, 90718017, 61170189, and 61202239), the Research Fund for the Doctoral Program of Higher Education (20111102130003), the Fund of the S-tate Key Laboratory of Software Development Environmen-t (SKLSDE-2013Z X-19), and the Innovation Foundation of Beihang University for Ph.D. Graduates (YWF-13-T-YJSY-024). This research was also supported by the Singapore Na-tional Research Foundation under its International Research Center @ Singapore Funding Initiative and administered by the IDM Programme Office. [1] C. G. Akcora, M. A. Bayir, M. Demirbas, and [2] J. Allan, R. Papka, and V. Lavrenko. On-line new [3] H. Becker, M. Naaman, and L. Gravano. Learning [4] D. M. Blei and J. D. Lafferty. Dynamic topic models. [5] D. M. Blei, A. Y. NG, and M. I. Jordan. Latent [6] M. Cataldi, L. D. Caro, and C. Schifanella. Emerging [7] Y. Chen, Z. Li, L. Nie, X. Hu, X. Wang, T. S. Chua, [8] Q. He, K. Chang, and E.-P. Lim. Analyzing feature [9] L. Hong and B. D. Davison. Empirical study of topic [10] T. Joachims. Transductive inference for text [11] S. P. Kasiviswanathan, P. Melville, A. Banerjee, and [12] A. Kotov, C. Zhai, and R. Sproat. Mining named [13] S. B. Kotsiantis. Supervised machine learning: A [14] H. O. Lancaster and E. Seneta. Chi-square [15] M. Mathioudakis and N. Koudas. Twittermonitor: [16] K. Nigam, A. K. Mccallum, S. Thrun, and T. Mitchell. [17] A.-M. Popescu and M. Pennacchiotti. Detecting [18] K. Randinsky, S. Davidovich, and S. Markovitch. [19] A. Saha and V. Sindhwani. Learning evolving and [20] T. Sakaki, M. Okazaki, and Y. Matsuo. Earthquake [21] H. Sayyadi, M. Hurst, and A. Maykov. Event [22] A. Tumasjan, T. O. Sprenger, P. G. Sandner, and [23] X. Wang and A. McCallum. Topics over time: a [24] Y. Wang, E. Agichtein, and M. Benzi. Tm-lda: [25] Y. Yang, T. Pierce, and J. Carbonell. A study on
