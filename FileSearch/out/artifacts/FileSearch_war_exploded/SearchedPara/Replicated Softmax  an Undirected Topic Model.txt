 words, defined by each topic, is the same across all documents .
 All these models can be viewed as graphical models in which la tent topic variables have directed connections to observed variables that represent words in a document. One major drawback is that more precise. For example, distributed representations al low the topics  X  X overnment X ,  X  X afia X  and  X  X layboy X  to combine to give very high probability to a word  X  Berlusconi X  that is not predicted nearly as strongly by each topic alone.
 Machines (RBMs), in which word-count vectors are modeled as a Poisson distribution. While these learning very unstable and hard. This is perhaps the main rea son why these potentially powerful different-sized documents. For undirected models margina lizing over unobserved variables is gen-fix this problem by proposing a Constrained Poisson model tha t would ensure that the mean Poisson word counts.
 that the proposed model is able to generalize much better com pared to a popular Bayesian mixture model, Latent Dirichlet Allocation (LDA) [2], in terms of bo th the log-probability on previously unseen documents and the retrieval accuracy. Consider modeling discrete visible units v using a restricted Boltzmann machine, that has a two-of the state { V , h } as follows: where { W, a, b } are the model parameters: W k unit i that takes on value k , and hidden feature j , b k matrix V is: where Z is known as the partition function or normalizing constant. The conditional distributions are given by softmax and logistic functions: where  X  ( x ) = 1 / (1 + exp(  X  x )) is the logistic function.
 Now suppose that for each document we create a separate RBM wi th as many softmax units as there can share the same set of weights, connecting them to binary h idden units. Consider a document that contains D words. In this case, we define the energy of the state { V , h } to be: where  X  v k = P D units to behave sensibly when dealing with documents of diff erent lengths.
 Given a collection of N documents { V parameters W takes the form: where E Observed Softmax Visibles and E mum likelihood learning in this model is intractable becaus e exact computation of the expectation E a different objective function, called the  X  X ontrastive Di vergence X  (CD) ([7]): where  X  is the learning rate and P likelihood learning.
 The weights can now be shared by the whole family of different -sized RBM X  X  that are created for document that contains 100 words is computationally not muc h more expensive than computing the gradients for a document that contains only one word. A key ob servation is that using D softmax units with identical weights is equivalent to having a singl e multinomial unit which is sampled D [13], except for the scaling of the hidden biases with D . Assessing the generalization performance of probabilisti c topic models plays an important role in any specific application.
 computing the global normalization constant requires enum eration over an exponential number of an exponential number of possible topic assignments for the words.
 Recently, [14] showed that a Monte Carlo based method, Annea led Importance Sampling (AIS) [12], can be used to efficiently estimate the partition function of an RBM. We also find AIS attractive because it not only provides a good estimate of the partition function in a reasonable amount of allow us to properly measure and compare generalization cap abilities of Replicated Softmax and Algorithm 1 Annealed Importance Sampling (AIS) run. LDA models. We now show how AIS can be used to estimate the part ition function of a Replicated Softmax model. 3.1 Annealed Importance Sampling Suppose we have two distributions: p p ( x ) is defined to be some simple proposal distribution with known Z A , whereas p B represents is to use a simple importance sampling method: possibly infinite, unless p Annealed Importance Sampling can be viewed as simple import ance sampling defined on a much bution p probability distributions: p sequence is to set: with  X  X nverse temperatures X  0 =  X  distribution, a Markov chain transition operator T defined.
 the model X  X  partition function. Let us consider a Replicate d Softmax model with D words. Using Eq. 5, the joint distribution over { V , h } is defined as 1 : where  X  v k = P D distributions, parameterized by  X  , can now be defined as follows: Note that for s = 0 , we have  X  function evaluates to Z have  X  p ( V ) invariant. ple uniform distribution p that  X  X ove X  the sample through the intermediate distributi ons p p butions. After performing M runs of AIS, the importance weights w ( i ) unbiased estimate of our model X  X  partition function Z where Z In particular, if we were to choose dumb transition operator s that do nothing, T  X  ( V  X   X  V ) for all s , we simply recover the simple importance sampling procedur e of Eq. 7. sized document can be represented as a separate RBM that has i ts own global normalizing constant. pers, 20-newsgroups, and Reuters Corpus Volume I (RCV1-v2) [10], and report generalization per-formance of Replicated Softmax and LDA models. 4.1 Description of Datasets training data and the remaining 50 documents as test. The dat aset was already preprocessed, where each document was represented as a vector containing 13,649 word counts.
 The 20-newsgroups corpus contains 18,845 postings taken fr om the Usenet newsgroup collection. test sets were separated in time. We further preprocessed th e data by removing common stopwords, was done.
 The Reuters Corpus Volume I is an archive of 804,414 newswire stories 4 that have been manually common stopwords were removed and all documents were stemme d. We again only considered the 10,000 most frequent words in the training dataset.
 For all datasets, each word count w datasets. 4.2 Details of Training For the Replicated Softmax model, to speed-up learning, we s ubdivided datasets into minibatches, was carried out using Contrastive Divergence by starting wi th one full Gibbs step and gradually total number of parameter updates was set to 100,000, which t ook several hours to train. For the [5]. The hyperparameters were optimized using stochastic E M as described by [15]. For the 20-newsgroups and NIPS datasets, the number of Gibbs updates wa s set to 100,000. For the large Reuters dataset, it was set to 10,000, which took several day s to train. 4.3 Assessing Topic Models as Generative Models the Replicated Softmax and LDA models we used 10,000 inverse temperatures  X  from 0 to 1. For each held-out document, the estimates were av eraged over 100 AIS runs. The average test perplexity per word was then estimated as exp  X  1 / N P N N is the total number of documents, D n and v n are the total number of words and the observed word-count vector for a document n .
 perplexity of 3405, improving upon LDA X  X  perplexity of 3576 . The LDA with 200 topics performed much better on this dataset compared to the LDA-50, but its pe rformance only slightly improved upon the 50-dimensional Replicated Softmax model. For the 2 0-newsgroups dataset, even with 200 2208, achieved by a simple smoothed unigram model. The Repli cated Softmax further reduces the perplexity down to 986, which is comparable in magnitude to t he improvement produced by the LDA over the unigram model. LDA with 200 topics does improve upon LDA-50, achieving a perplexity model. Precision (%) corresponding LDA model. For the Reuters dataset, as expect ed, there are many documents that are modeled much better by the undirected model than an LDA. Clea rly, the Replicated Softmax is able to generalize much better. 4.4 Document Retrieval We used 20-newsgroup and Reuters datasets to evaluate model performance on a document retrieval Softmax, the mapping from a word-count vector to the values o f the latent topic features is fast, requiring only a single matrix multiplication followed by a componentwise sigmoid non-linearity. For the LDA, we used 1000 Gibbs sweeps per test document in ord er to get an approximate posterior measure their similarity, the Replicated Softmax significa ntly outperforms LDA, particularly when retrieving the top few documents. We have presented a simple two-layer undirected topic model that be used to model and automati-can be viewed as a family of different-sized RBM X  X  that share parameters. The proposed model have are designed to be run in a batch mode. Therefore one would hav e to make further approximations, and the retrieval accuracy.
 be extended in several ways. For example, similar to supervi sed LDA [1], the proposed Replicated Softmax can be easily extended to modeling the joint the dist ribution over words and a document model, where a prior on the document-specific topic distribu tions was modeled as a function of observed metadata of the document. Similarly, we can define a conditional Replicated Softmax count data. Once the Replicated Softmax has been trained, we can add more layers to create a Deep Belief Network [8], which could potentially produce a bette r generative model and further improve retrieval accuracy.
 Acknowledgments This research was supported by NSERC, CFI, and CIFAR.

