 The common way to overcome the language barrier in cross-language information document translation is too high, query translation becomes the mainstream though the former can get a better translation. The problem with query translation is how to overcome translation ambiguity, and many methods are proposed for disambiguation. 
All the methods use a corpus for training, and according to the language of the all these methods are very time-consuming and it X  X  hard to acquire large parallel bilingual corpus, especially of minor languages. On the other hand, monolingual corpus is easy to get and methods based on it always use term (or word) co-monolingual corpus is more widely used. 
Translation usually involves two aspects: term and grammar. In the case of query translation, the queries are short and often given in irregular grammar, sometimes even without any sentence structure, hence correct translation of a term is more important, while the grammar can be ignored somehow. In such situation, only a difficulty of translation because each term ma ybe has several entries in the dictionary and combinations of every translation candi date are large. To select the correct translation combination out of n k candidate combinations ( n is the average number of entries each term has and k is the number of terms a query has) is a very difficult task, and it X  X  very probable to be wrong if only the best translation combination is returned decision among the translations. 
In this paper, we propose a progressive algorithm for computing the translation probability. Its original form is from another unsatisfactory progressive algorithm we propose called max-sum model. By analyzing the model X  X  drawbacks, we replace its strategy on redistribution of translation probability by a more reasonable one. What X  X  more interesting is that the new progressive algorithm can be computed in a more direct way by solving an equation system. We inspect the performance of the algorithm on SougouT2.0. Experiments show it outperforms four other methods. 
The remainder of this paper is organized as follows. Section 2 describes the max-sum model in detail and its drawbacks. Section 3 proposes the new algorithm and how Section 4, we draw our conclusions and have an outlook of further work. way is to choose the most frequent transla tion combination. For example, a query has Then the number of all the possible translation combinations is 6. For every combination, we count the number of documents where all the translation terms of this combination appear and the highest one is regarded as the best translation can X  X  be done beforehand. Moreover, it needs a very large corpus to avoid data-sparseness when the query is long. To overcome this problem, we do it in an approximate way. We only compute the co-occurrences of each pair of terms, and sum them up to approximate the frequency of a translation combination. This approximate method is more understandable in a graph as shown in Fig.1. between translation terms of different source terms. We can define the graph in a rigid partition of T , and A be the adjacency matrix of the graph, then the graph G ( T,S ) can be described as a graph satisfies: For convenience, we define C ( i ) as the set of all the terms which have an edge with i , document. In G ( T,S ), each translation combination constitutes a k -complete subgraph (as shown score approximate to the frequency of th e combination can be obtained. Hence the k -As mentioned above, we prefer to compute the probability distribution of a source term rather than make a simple decision. Some modification should be done to the highest-score k -complete subgraph method to suit the probability form. We specify a maximum value of (3), which is also used in a similar form in [8]. In fact, the computation of the highest-score k -complete subgraph is a special case of (3) when the probab ilities are 0-1 distributions. Bu t very strangely, though the formula generalized to the probability form, the maximum doesn X  X  exceed the highest score of k -complete subgraph according to the following theorem. Theorem 1. In G ( T,S ), the maximum of formula (3) equals to the highest score of k -complete subgraph. Proof. Suppose there exists some probability distribution which makes the maximum probability is not 0-1 distribution. Rewrite (3) as following: 
That is to say, we can redistribute the probability to make the inequality above equal, by letting p j of the max j in the right side of inequality above be 1 and others be 0. Thus, the probability of S i becomes a 0-1 distribution, and the value of (3) is no less than the original. Because the number of S i is finite, we can redistribute the complete subgraph, and the supposition can X  X  be true.  X  From the proof of Theorem 1, a progressive algorithm for computing the maximum of (3) is derived. Algorithm 1 1. For every p i , i  X  T , specify an initial value 2. While Do for every j  X  S i , let 
The value of (3) increases after every loop of step 2 and terminate on the maximum compute it in a probability form, which makes the method perform poor in the experiment (see Section 4). The main reason is that the relationship between the value of (3) and the correctness of the translation is not linear, even though we think that the weight is not the highest. Hence the strategy of computing the maximum is not so translation probability. The main idea of Algorithm 1 is to readjust the probability distribution of each S i until which eventually makes the result 0-1 distribution. This strategy is not so fair because if the weights of two terms are close, the result is still the 0-1 distribution rather than specifying close probabilities to them. A fairer probability distribution seems more reasonable in practice. Hence, we use the following formula in place of (5) in Algorithm 1. (6) redistributes the probability according to the weight each term has, so we call it (4) should be modified as: If we let: Thus, we acquire a new algorithm, and besides, the new algorithm can be computed in a more direct way. Since (7) can be rewritten as  X  X f any of the following equalities is not satisfied: Actually, all these equalities constitute an equation system, and the new algorithm terminates on the solutions to these equations. Hence, we direct solve the equations of (9) to obtain the result. 3.1 Solving the Equations Since the equations in (9) are nonlinear, we use the Newton Method [10]. In brief, the Newton Method is divided into two steps. First, specify the initial value: Then iteratively calculate (11): Until (12) is met: Where P (k) is:  X  (k) is the solution to (14). Since (14) is a linear equation system, ther e are many ways to solve it. Here we use the common method called Gauss elimination [11]. The calculation of the Jacobian matrix in (14) seems very time-consuming, but the partial derivative can be simplified according to (8). Where A i is the vector form of C ( i ); B i , C i and d i,j are defined as follows: products of (15) are same in every row of the Jacobian matrix, the number of vector product calculation in each iteration is only 2 n . To further simplify the computation, price for computing the Jacobian matrix in (14) is roughly 2 n 2 times of product, which is not so high as it seems. Besides, the Gauss elimination needs ( n 2 +3 n -1) n /3 times of roughly n 3 /3+3 n 2 -n /3 times of product or division and 2 n times of vector product. Various methods are compared under the measure of precision curve by retrieving a given documents collection. There are three kinds of experiment data we need in the experiments: queries, corpus for training and retrieving, and on-line dictionary. 
The queries we use are in English, come from a manual English translation of the 70 topics (TD216-TD285) from the Chinese Web Information Retrieval Forum the other is the  X  X escription X . The  X  X itle X  is short and concise, while the  X  X escription X  is long and detailed. Both fields are used as queries. Since most words in a  X  X itle X  are highly relevant to each other, while the  X  X escription X  usually includes many irrelevant or only slightly relevant words, we expect translation disambiguation is a more challenging problem for the latter. 
The online bilingual dictionary we use is Lexiconer (http://www.lexiconer.com/), and the translation is done in a word-by-word way. Processing of phrases is not considered in the experiments. 
The corpus is in Chinese, comes from a simplified version of SogouT2.0. Its size is roughly 1.1 GB. We use it in two aspects: one is to compute the dice coefficient (2) between pairs of terms; the other is to retrieve the documents relevant to the queries. 4.1 Methods Compared Five methods for translation disambiguation are compared. In addition, the original model method of Section 2 and the weighted-average method of Section 3, the other three methods are: 
Include All [12] makes no difference between any translations of a term. In fact, it combination, but also include much more wrong ones. 
Simple Weighted is a simplified version of the weighted-average method. Instead of (6), it computes the probability distribution in a more direct way like (19). Maximum Coherence Model [8] computes the maximum of a formula similar to (3), but it treats it as a quadratic programming problem. Its implementation is quite complex and the price for computation is also high. 
The results of these methods are not all in probability form, but they are all treated vector, and we use standard vector space model [13] for calculating the relevance of retrieved documents. 4.2 Results and Remarks Precision curves are plotted using the precisions of top 5-100 ranked relevant lists the average precision of each method and the relative improvement our method achieves over others. 
In Fig.2 and Fig.3, the curve of monolingual (direct using Chinese queries to retrieve) represents the theoretic upper boundary of precision, so it always stays above other curves. Except for this, we can see our average-weighted algorithm performs better than other four and the average improvement over other methods is about 15%-60% from Tab.1. The max-sum model performs unexpectedly poor in the experiments, and this may be due to the 0-1 distribution of its results and the unreasonable strategy to redistribute the probabilities. Moreover, a better retrieval achieved by short queries proves that the long queries tend to include more irrelevant or only slightly relevant words and they are usually more difficult to disambiguate for algorithms based on co-occurrence statistics of terms. In this paper, we first introduce a progressive algorithm called the max-sum model. Its main idea is to compute the maximum value of a formula, but its result is always a 0-1 distribution which proves incompetent in the experiments. Then we modify the algorithm by taking a strategy called weighted-average probability distribution, which seems fairer because it takes terms ha ving close weights into account. The new algorithm outperforms four other methods in the experiments. Moreover, it can be computed in a more direct way by solving an equation system using Newton method. 
The running efficiency of our algorithm is not so satisfactory, and we plan to improve the algorithm in the following fields in the future: 1) accelerate the convergence rate of Newton method; 2) optimization of time and space in Gauss elimination and the Jacobian matrix; 3) design a parallel version of the algorithm for longer query. 
