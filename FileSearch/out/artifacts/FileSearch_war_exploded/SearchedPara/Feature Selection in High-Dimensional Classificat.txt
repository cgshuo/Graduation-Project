 Mladen Kolar mladenk@cs.cmu.edu Han Liu hanliu@princeton.edu High-dimensional discriminant analysis plays an im-portant role in multivariate statistics and machine learning. In a typical setting, a binary discriminant analysis problem can be formulated as follows: we ob-dependently drawn from a joint distribution of ( X , Y ), where X  X  R p and Y  X  { 1 , 2 } . Discriminant analysis aims at classifying the value of Y given a new data point x . Let p 1 ( x ) and p 2 ( x ) be the density functions of X | Y = 1 and X | Y = 2, and the prior probabilities  X  1 = P ( Y = 1),  X  2 = P ( Y = 2). It is well known that the Bayes rule classifies a new data point x to the second class if and only if One of the most commonly used settings is the condi-tional Gaussian model, where
X | Y = 1  X  X  (  X  1 ,  X  ) and X | Y = 2  X  X  (  X  2 ,  X  ) . Let  X  d =  X  2  X   X  1 and  X  a = (  X  1 +  X  2 ) / 2. The optimal classifier classifies a point to class 2 if and only if For the above Gaussian discriminant analysis, Bickel &amp; Levina ( 2004 ) show that the classical low di-mensional normal-based linear discriminant analysis (LDA) is asymptotically equivalent to random guess-ing when the dimension p increases at a rate compa-rable to the sample size n . To handle this problem, we generally assume the discriminant direction  X  =  X   X  1  X  d is sparse. In particular, it is assumed that  X  = (  X 
T , 0 sume  X  = I , including the nearest shrunken centroids ( Tibshirani et al. , 2002 ; Wang &amp; Zhu , 2007 ) and fea-ture annealed independence rules ( Fan &amp; Fan , 2008 ). More recently, numerous alternative approaches have been proposed by taking more complex covariance matrix structures into consideration ( Fan et al. , 2010 ; One particularly interesting proposal is the ROAD estimator (Regularized Optimal Affine Discriminant) due to Fan et al. ( 2010 ). Let S and b  X  d be empirical estimators of  X  and  X  d . The ROAD estimator is ob-tained by minimizing v  X  Sv with v  X  b  X  d restricted to be a constant value, i.e. min Here  X  is a regularization parameter, when  X  = 0, the ROAD estimator reduces to be the classical Fisher X  X  discriminant rule. Later, Cai &amp; Liu ( 2011 ) proposed a different version of the sparse LDA, which tries to make v close to the Bayes rule X  X  linear term  X   X  1  X  d in the  X   X  norm, i.e., Equation ( 1.4 ) turns out to be a linear program-ming rule highly related to the Dantzig selector ( Candes &amp; Tao , 2007 ; Yuan , 2010 ; Cai et al. , 2011 ). More recently, Mai et al. ( 2012 ) proposed a version of the sparse LDA based on an  X  1 -norm penalized least square formulation.
 To avoid the curse of dimensionality, an  X  1 penalty is added in all three methods to encourage a sparsity pattern of v , and hence nice theoretical properties can be obtained under certain regularity conditions. How-ever, unlike the high dimensional regression settings where sharp theoretical results exist for prediction, es-timation, and variable selection consistency, all exist-ing theories for high discriminant analysis are either on estimation consistency or risk consistency, but not on variable selection consistency. The main reason is that analyzing the variable selection consistency of a high dimensional Gaussian discriminant analysis procedure requires us to sharply characterize the sampling dis-tribution and tail behavior of the scaled discriminant oretical analysis and new proof technique. Mai et al. ( 2012 ) provide a variable selection consistency result for their procedure, however, as we will show later, the scaling they obtained is not optimal.
 In the current paper, we bridge the theoretical gap in understanding of variable selection in high-dimensional discriminant analysis. We provide a sharp analysis of the variable selection performance of the ROAD estimator. The proof technique is based on the characterization of the Karush-Kuhn-Tucker (KKT) conditions for the constrained optimization problem. Unlike the  X  1 -norm penalized least squares regression, which directly estimates the regression coefficients, the ROAD estimator is related to the scaled quan-timization problem. To sharply characterize the vari-able selection consistency, we carefully analyze the tail behavior of this scaled quantity by exploiting sophis-ticated multivariate analysis results. Sufficient con-ditions for the variable selection consistency of the ROAD estimator are complemented with information theoretic limitations on recovery of the feature set T . In particular, we provide lower bounds on the sample size and the signal level needed to recover the set of relevant variables T by any procedure. Some of the main results of this paper are summarized below. We show that if the sample size n &gt; C  X  max where C is a universal constant,  X  a | T =  X  aa  X   X  eigvenvalue of  X  , then the estimated vector b  X  has the same sparsity pattern as the true  X  , thus the ROAD is variable selection consistent (or sparsis-tent). This result suggests that the discriminant analysis has a similar theoretical scaling as the re-gression setting. To show Eq. ( 1.5 ), we need the assumptions that min j  X  T |  X  j | is not too small and ||  X  NT  X   X  1 T T sign(  X  T ) ||  X   X  1  X   X  with  X   X  (0 , 1). The latter assumption is the irrepresentable condi-tion, which takes a similar form as for the  X  1 -norm penalized least squares problem. Our analysis of in-formation theoretic limitations reveals that if n &lt; C  X   X  2 min log( p  X  s ), where  X  min is the magnitude of the smallest non-zero component of  X  , then no proce-dure can reliably recover the set T . In particular, for the case where  X   X  1  X  d has bounded  X  2 norm and  X  is optimal for the purpose of variable selection. An illustrative simulation demonstrates sharpness of our results.
 The rest of this paper is organized as follows. In the next section, we introduce the notation. In Section 3 , we characterize the solution to the population version of the ROAD estimator and outline the proof tech-nique to be used to characterize the solution to the problem in Eq. ( 1.3 ). In Section 4 , we derive sufficient conditions for the ROAD estimator to be sparsistent. An information theoretic lower bound is given in Sec-tion 5 . Numerical simulations are provided in Sec-tion 6 . We conclude the paper with some discussions in Section 7 . any index set T  X  [ p ], we denote  X  T to be the subvec-tor containing the components of the vector  X  indexed by the set T , and X T denotes the submatrix contain-ing the columns of X indexed by T . Similarly A T T de-notes a submatrix of A with rows and columns indexed by T . For a vector a  X  R n , we denote supp( a ) = { j : a j 6 = 0 } the support set, || a || q , q  X  [1 ,  X  ), the  X  defined as || a || q = ( tensions for q  X  { 0 ,  X  X  , that is, || a || 0 = | supp( a ) | and || a ||  X  = max i  X  [ n ] | a i | . For a symmetric matrix A  X  R p  X  p we denote  X  min ( A ) and  X  max ( A ) the small-est and largest eigenvalues, respectively. We also use the weighted norm || a || 2 A = a  X  Aa for a symmetric ma-trix A . For two sequences { a n } and { b n } , we use a n = O ( b n ) to denote that a n &lt; Cb n for some finite positive constant C . We also denote a n = O ( b n ) to be b n &amp; a n . If a n = O ( b n ) and b n = O ( a n ), we denote it to be a n  X  b n . The notation a n = o ( b n ) is used to denote that a n b  X  1 n  X  0. In this section, we characterize the solution to the population version of the optimization problem in Eq. ( 1.3 ). That is, we characterize the solution b w to the optimization problem In particular, we derive conditions under which the vector b w recovers the sparsity pattern of  X  . Recall that T = supp(  X  ) and N = [ p ] \ T . We have the fol-lowing result. We have the following result.
 Theorem 1. Under the assumption that and that there exists a constant  X   X  (0 , 1] such that Eq. ( 3.1 ) , where Furthermore, we have sign( b w T ) = sign(  X  T ) . Theorem 1 provides two conditions under which the so-lution to Eq. ( 3.1 ) recovers the support of  X  . Eq. ( 3.2 ) is a condition on the smallest component of  X  T , as well as a condition on the tuning parameter  X  . Define  X  min as Let  X  =  X  0 ||  X  T ||  X  2  X  we observe that b w T recovers the support as long as  X  min  X   X  0 ||  X   X  1 T T sign(  X  T ) ||  X  . The second con-dition, given in Eq. ( 3.3 ), is related to the irrep-resentable condition commonly used in the analysis of Lasso ( Zou , 2006 ; Meinshausen &amp; B  X uhlmann , 2006 ; Zhao &amp; Yu , 2007 ; Wainwright , 2009 ). Theorem 1 pro-vides an explicit form for the solution b w . It is clear that the ROAD optimization procedure estimates the scaled discriminant direction ||  X  T ||  X  2  X  tor b w is biased when  X  6 = 0, but nevertheless it can recover the set T of non-zero components of  X  . Theorem 1 is proven by analyzing the Karush-Kuhn-Tucker (KKT) conditions for the optimization problem in Eq. ( 3.1 ). The KKT conditions are given as where b  X  is the Lagrange multiplier for the constraint and b z  X   X  || b w || 1 is an element of the subdifferen-tial. In what follows, we will construct a vector b w = ( b w  X  T , 0  X  )  X  that satisfies the KKT conditions and sign( b w T ) = sign(  X  T ).
 The following lemma characterizes the vector  X  and will be useful in analysis that follows.
 Lemma 2. Under the model in Eq. ( 1.2 ) with  X  =  X  and The solution b w to Eq. ( 3.1 ) is constructed by consider-ing an oracle optimization problem, where the solution is forced to be non-zero only on the set T , and then showing that it also satisfies the KKT conditions for the full problem. The following lemma characterizes the solution to the oracle optimization problem. Lemma 3. Let e w
T = arg min be the oracle optimization problem. Then The next lemma shows that the vector ( e w  X  T , 0  X  )  X  is the solution to the unconstrained optimization problem under the assumptions of Theorem 1 .
 Lemma 4. Assume that conditions of Theorem 1 are satisfied. Then b w = ( e w  X  T , 0  X  ) is the solution to the Furthermore, we have sign( b w ) = sign(  X  ) . Theorem 1 follows directly from Lemma 3 and Lemma 4 . In the next section, we will establish suf-ficient conditions for the ROAD procedure to recover the non-zero components of  X  when the population quantities in Eq. ( 3.1 ) are replaced by their empirical estimates. The proof construction is going to follow the same line of reasoning, however, proving analo-gous results to Lemma 3 and Lemma 4 in the sample version of the problem is much more challenging. In this section, we characterize the solution b v to the optimization problem given in Eq. ( 1.3 ), which is a sample version of the optimization problem given in Eq. ( 3.1 ). We will derive conditions under which b v = ( b v
T , 0 We observe n independent and identically distributed (iid) data points { x i , y i } from the model in Eq. ( 1.2 ) with equal class probabilities, that is, with out loss of generality  X  1 =  X  2 = 1 2 . Denote n 1 = |{ i : y i = 1 }| and n 2 = n  X  n 1 . Let X 1  X  R n 1  X  p be the matrix with rows containing data points for which the label is one and similarly define X 2  X  R n 2  X  p . Let H be the centering matrices, where I n is the n  X  n identity matrix and 1 n is the n  X  1 vector with all components equal to 1. We define the following quantities S The matrix S  X  W p ( n  X  2)  X  1  X  , n  X  2 is the pooled sample covariance matrix, where W p ( n  X  2)  X  1  X  , n  X  2 denotes the Wishart dis-tribution with n  X  2 degrees of freedom and the scaling matrix ( n  X  2)  X  1  X  (see Theorem 3.4.2 in Mardia et al. , 1980 ). It is a standard result (see The-orem 3.1.2 in Muirhead , 1982 ) that S is independent of b  X  i  X  X  (  X  i , n  X  1 i  X  ), ( i = 1 , 2). The following theorem is the main result that charac-terizes the variable selection consistency of the ROAD estimator.
 Theorem 5. We assume that condition in Eq. ( 3.3 ) holds. Let the penalty parameter be  X  =  X  0 ||  X  T ||  X  2 with where C 0 is a sufficiently large constant indepen-dent of the problem parameters and  X  a | T =  X  aa  X   X   X  for some sufficiently large constant K independent of the problem parameters. If the sample size n &gt; C 1 max where C 1 is a constant independent of the problem pa-rameters, then b v = ( b v  X  T , 0  X  ) , with b v is the unique solution to the optimization problem in Eq. ( 1.3 ) with probability at least 1  X  X  (log  X  1 ( n )) . Theorem 5 characterizes the solution b v obtained by solving the ROAD optimization problem in Eq. ( 1.3 ). It is a sample version of Theorem 1 given in the pre-vious section. Again, we require a lower bound on  X  min in order to distinguish relevant components from the irrelevant ones and an irrepresentable condition to hold. Theorem 5 provides a lower bound on the sam-ple size n that is sufficient for the ROAD procedure to identify the set T with high probability. The sample size scales as n  X  s log( p  X  s ) assuming that there ex-ists a constant c such that 0 &lt; c  X   X  min (  X  T T ). This scaling is of the same order as for the Lasso procedure, where n &gt; 2 s log( p  X  s ) is needed for recovery of the relevant variables when  X  = I .
 Mai et al. ( 2012 ) analyze an  X  1 -norm penalized least squares approach for solving the discriminant analysis problem. They require the sample size n to satisfy which is suboptimal compared to our results. At this point, it is not clear how their analysis can be im-proved, but we conjecture some results developed in our current paper could be useful.
 The proof of Theorem 5 is outlined in the next section. 4.1. Proof of Theorem 5 The proof of Theorem 5 parallels the proof of Theo-rem 1 . We construct the solution b v to the optimiza-tion problem in Eq. ( 1.3 ) that recovers the sparsity pattern of the vector  X  T . To achieve that, we proceed in two steps. In the first step, we consider an oracle optimization problem (defined in Lemma 6 ), which is minimized at e v T . In the second step, we show that the vector ( e v  X  T , 0  X  )  X  satisfies the KKT conditions for the original optimization problem given in Eq. ( 1.3 ), thus showing that b v = ( e v  X  T , 0  X  )  X  is the global minimizer. The following lemma characterizes the solution to the constrained optimization problem, which is analogous to the population version of the constrained optimiza-tion problem given in Eq. ( 3.10 ).
 Lemma 6. Let e v T = arg min be the oracle optimization problem. Then e v Lemma 6 provides an explicit form for the solution of the oracle optimization problem. Note that the solu-tion is unique, since the objective is strongly convex due to the quadratic term as S T T is positive definite with probability 1. The next result shows that, un-der the conditions of Theorem 5 , the vector ( e v  X  T , 0 is the solution to the ROAD optimization problem in Eq. ( 1.3 ).
 Lemma 7. Let the penalty parameter be  X  =  X  where C 0 is a sufficiently large constant indepen-dent of the problem parameters and  X  a | T =  X  aa  X   X   X  ||  X  T || 1 &lt; C 1 , for some constant C 1 . If the sample size n &gt; C 2 max where C 2 is a constant independent of the problem pa-mization problem in Eq. ( 1.3 ) with probability at least 1  X  X  (log  X  1 ( n )) .
 Finally, we need to show that sign( e v T ) = sign(  X  T ). Lemma 8. Under the assumptions of Theorem 5 , e v T defined in Eq. ( 4.5 ) recovers the sign pattern of the vector  X  T with probability at least 1  X  X  (log  X  1 ( n )) . Under the assumption on  X  min in Eq. ( 4.2 ), conditions of Lemma 7 are satisfied and sign( e v T ) = sign(  X  T ). This completes the proof of Theorem 5 . In this section, we are interested in results of comple-mentary nature to those derived in Theorem 5 . The-orem 5 provides sufficient conditions for a particular procedure to recover the support set T of non-zero ele-ments of  X  . Here we discuss necessary conditions that must be satisfied for any method to succeed in reliable estimation of the support set.
 Let  X  be an estimator of T . We consider the maximum risk, corresponding to 0 / 1 loss, given as where  X  = (  X  1 ,  X  2 ,  X  ) denotes the problem parame-ters, P  X  denotes the joint law of { x i , y i } i  X  [ n ] ing  X  1 =  X  2 = 1 2 , T (  X  ) = supp(  X  ) (recall that  X  =  X   X  1 (  X  2  X   X  1 )), and  X  is a family of parame-ters. Let M ( s, Z ) be the class of all subsets of the set Z of cardinality s . We consider  X  =  X (  X  ,  X , s ) = The minimax risk is defined as the smallest risk over all possible estimators. The main result of this section provides a lower bound on the minimax risk where  X  min &gt; 0 determines the signal strength. Before stating the result, we introduce two quantities that will be used to state Theorem 9 . We define = min and The first quantity will be used to measure the difficulty of distinguishing two close support sets T 1 and T 2 that differ in only one position, while the second quantity measures the effect of a huge number of support sets that are far from the support set T .
 Theorem 9. Let If  X  &lt;  X  min , there exists some constant C &gt; 0 , such that inf The result can be interpreted in words in the follow-ing way: whatever the estimation procedure, when  X  &lt;  X  min there exists some distribution indexed by  X   X   X (  X  ,  X , s ) such that the probability of incorrectly identifying the set T (  X  ) is bounded away from zero. Remarks: 1. Expressions  X  close (  X  ) and  X  far (  X  ) simplify 2. As a consequence of Theorem 9 and Theorem 5 , In this section, we conduct a few illustrative simula-tions that show finite sample performance of our re-sults. Theorem 5 describes the sample size needed to recover the set of relevant variables. We consider the following three scalings for the size of the set T : 1. fractional power sparsity, where s ( p ) =  X  2 p 0 . 45  X  3. linear sparsity, where s ( p ) =  X  0 . 4 p  X  . For all three scaling regimes, we set the sample size as where  X  is the control parameter varied in the inter-val [0 . 1 , 4 . 5] and investigate how well can the ROAD procedure recover the support set T . We set P [ Y = 1] = P [ Y = 2] = 1 2 , X | Y = 1  X  N (  X  ,  X  ) and without loss of generality X | Y = 2  X  N ( 0 ,  X  ). We specify the vector  X  by choosing the set T of size | T | = s ( p ) randomly, and for each a  X  T setting  X  a equal to +1 or  X  1 with equal probability, and  X  a = 0 for all com-ponents a 6 X  T . We specify the covariance matrix  X  as so that  X  =  X   X  1  X  = (  X   X  T , 0  X  )  X  . We consider three cases for the block component  X  T T : 1. identity matrix, where  X  T T = I s , 2. Toeplitz matrix, where  X  T T = [ X  ab ] a,b  X  T and 3. equal correlation matrix, where  X  ab =  X  when Finally, we set the penalty parameter  X  as for all cases. For this choice of  X  , Theorem 5 predicts that the set T will be recovered correctly. For each setting, we report the Hamming distance between the estimated set b T and the true set T , averaged over 200 independent simulation runs. Figure 1 plots the Hamming distance against the con-trol parameter  X  , or the rescaled number of samples. Here the Hamming distance between b T and T is cal-culated by averaging 200 independent simulation runs. There are three subplots corresponding to different sparsity regimes (fractional power, sublinear and lin-ear sparsity), each of them containing three curves for different problem sizes p  X  { 100 , 200 , 300 } . Note that when the control parameter reaches  X  = 3 almost all the elements of the set T are recovered, without false positives, while when  X  &lt; 3 the recovery is very poor. Figure 2 and Figure 3 show similar behavior for two other cases, with  X  T T being a Toeplitz matrix with parameter  X  = 0 . 1 and the equal correlation matrix with  X  = 0 . 1. In this paper, we address the problem of variable se-lection in high-dimensional discriminant analysis prob-lem. The problem of reliable variable selection is im-portant in many scientific areas where simple models
Hamming distance
Hamming distance Equal correlation; Fractional power
Hamming distance are needed to provide insights into complex systems. Existing research has focused primarily on establish-ing results for prediction consistency, ignoring feature selection. We bridge this gap, by analyzing variable selection properties of the ROAD procedure and es-tablishing sufficient conditions required for successful recovery of the set of relevant variables. This analy-sis is complemented by analyzing the information the-oretic limits, which provide necessary conditions for variable selection in discriminant analysis. From these results, we are able to identify the class of problems for which the computationally tractable procedure ROAD is optimal.
 We thank anonymous reviewers who provided insight-ful comments that helped improve the paper. HL was supported by NSF Grant III X 1116730.
 Bickel, P. J. and Levina, E. Some theory for fisher X  X  linear discriminant function,  X  X aive bayes X , and some alternatives when there are many more variables than observations. Bernoulli , 10:989 X 1010, 2004. Cai, T. and Liu, W. A direct estimation approach to sparse linear discriminant analysis. Arxiv preprint arXiv:1107.3442 , 2011.
 Cai, Tony, Liu, Weidong, and Luo, Xi. A constrained l1 minimization approach to sparse precision matrix estimation. Journal of the American Statistical As-sociation , 106:594 X 607, 2011.
 Candes, Emmanuel and Tao, Terence. The dantzig se-lector: statistical estimation when p is much larger than n. The Annals of Statistics , 35:2313 X 2351, 2007.
 Fan, J. and Fan, Y. High dimensional classification using features annealed independence rules. Annals of statistics , 36(6):2605, 2008.
 Fan, J., Feng, Y., and Tong, X. A road to classi-fication in high dimensional space. Arxiv preprint arXiv:1011.6095 , 2010.
 Mai, Q., Zou, H., and Yuan, M. A direct approach to sparse discriminant analysis in ultra-high dimen-sions. Biometrika , 2012.
 Mardia, K.V., Kent, J.T., and Bibby, J.M. Multivari-ate analysis. 1980.
 Meinshausen, N. and B  X uhlmann, P. High dimensional graphs and variable selection with the lasso. Annals of Statistics , 34(3), 2006.
 Muirhead, R.J. Aspects of multivariate statistical the-ory , volume 42. Wiley Online Library, 1982.
 Shao, J., Wang, Y., Deng, X., and Wang, S. Sparse linear discriminant analysis by thresholding for high dimensional data. Arxiv preprint arXiv:1105.3561 , 2011.
 Tibshirani, R., Hastie, T., Narasimhan, B., and Chu,
G. Diagnosis of multiple cancer types by shrunken centroids of gene expression. Proceedings of the Na-tional Academy of Sciences , 99(10):6567, 2002. Wainwright, Martin. Sharp thresholds for high-dimensional and noisy sparsity recovery using  X  1 -constrained quadratic programming (lasso). IEEE
Transactions on Information Theory , 55(5):2183 X  2202, May 2009.
 Wang, S. and Zhu, J. Improved centroids estimation for the nearest shrunken centroid classifier. Bioin-formatics , 23(8):972, 2007.
 Yuan, Ming. High dimensional inverse covariance ma-trix estimation via linear programming. Journal of Machine Learning Research , 11:2261 X 2286, 2010. Zhao, P. and Yu, B. On model selection consistency of lasso. J. of Mach. Learn. Res. , 7:2541 X 2567, 2007. Zou, Hui. The adaptive lasso and its oracle proper-ties. Journal of American Statistical Association ,
