 In recent years, recommender systems have played an important role in helping match users with information resources [ 4 ]. Various recommendation algorithms have been developed in the past years [ 1 ], including collaborative filtering meth-ods, content-based methods, and hybrid methods. Collaborative filtering meth-ods build a model from a user X  X  past behaviors as well as decisions made by other similar users. Content-based methods extract a set of important features of an item in order to recommend new items with similar features. These two types of methods are often combined in practical systems to form the hybrid methods. Although previous methods have been shown to be effective to some extent, there exist several problems with these approaches. First, these methods are usually task-oriented and cannot serve as a general solution to multiple rec-ommendation settings. It may not be easy for existing methods to adapt to a different recommendation setting. Second, existing recommendation algorithms may not be scalable to large datasets. For example, the efficiency of classic item-based KNN recommendation algorithms is largely limited by the construction of the KNN graph [ 4 ]; matrix factorization involves eigen-decomposition of the data matrix which is expensive and usually with approximation calculation [ 13 ]. Thus, how to balance generality and scalability has become an important prob-lem in practice. The main research focus of our paper is to develop a general and scalable recommendation framework.
 of typical recommendation tasks can be formulated as graphs. For example, we have presented two illustrative examples for top-N item recommendation and tag recommendation respectively in Fig. 1 . For item recommendation, we have two sets of vertices, namely users and items; While for tag recommendation, we have three sets of vertices, namely users, items and tags. Once we have built the graph representation, the recommendation task can be considered as a relatedness or relevance evaluation problem: given a or more query vertices, we would like to identify the most related vertices. For example, for item recommendation, the query vertex can be set to a specific user, while for tag recommendation, the query vertices can be set to a combination of a user and an item. With such a formulation, the difficulty lies in how to develop an effective way to evaluate the relatedness on the graph.
 learning and deep learning [ 8 , 10 , 18 ]. Network representation learning charac-terizes a vertex in a graph with a low-dimensional dense vector, a.k.a. , embed-ding vector. Embedding representations provide a promising way to represent and extract structural patterns in the networks, and several pioneering works have shown the effectiveness of network embedding models [ 10 , 18 ]. Especially, vertex similarity or relatedness can be well measured with the embedding vec-tors. Following this point, our general solution to recommendation tasks has been developed as a three-step procedure. In the first step, we build a k -partite adoption network which is constructed with all the historical adoption records ( e.g., product purchase records). Here, the term of  X  X doption X  has been used because the recommendation task can be considered as modeling the adoption process of a user. Second, we apply the network embedding techniques to learn the embeddings for the vertices on the k -partite adoption network. Embeddings from different vertices are projected into the same latent space, where simi-larity measurements can be used to evaluate the relatedness between vertices ( e.g., cosine similarity). Finally, the recommendation task will be casted into a similarity evaluation process. For example, given a user, we can directly rank the candidate items by the cosine similarity values between the user and item embeddings.
 The proposed approach is both general and scalable. On one hand, our for-mulation ( i.e., k -partite adoption network) can be used to characterize multiple recommendation settings with rich contextual information. When new contex-tual information is needed to consider, we can simply discretize the contextual information into discrete variables and represent them as new vertices. On the other hand, our approach utilizes the neural network models to derive the vertex embedding representations. In this case, the model is designed to optimize within local neighborhoods instead of performing global computations. This allows us to develop scalable algorithms such as stochastic gradient descent with weight sam-pling [ 18 ]. To evaluate the effectiveness of the proposed approach, we construct extensive experiments on two different recommendation tasks using real-world datasets. The experimental results have shown the superiority of our approach. To the best of our knowledge, it is the first time that recommendation task has been addressed by a network representation learning approach. Comparing to traditional recommendation algorithm, our goal is to provide a unified approach to multiple recommendation tasks. In a general sense, the input of the recommendation task corresponds to the adoption behaviors of users. The main idea is to represent the adoption records using a k -partite network, and then embedding representations are used to represent vertices on the graph. With such representations, we can fulfill the recommendation task using simple similarity measurements. Next we introduce the preliminaries for this paper. Definition 1. k -Tuple Adoption Record . An adoption record can be modeled as a k -tuple: e 1 ,...,e j ,...,e k , where each entry e value for the j -th feature ( a.k.a. attribute) in an adoption record. Here we require that the value of each entry must be a positive discrete value.
 with rich context information. For example, in top-N item recommendation, a pair u, i is used to represent the record that user u has adopted item j . While in top-N tag recommendation, a triplet u, i, t is used to represent the record that tag t has been given by user u on item i . It is similar to the feature coding in context-aware recommendation models such as SVDFeature [ 3 ] and libFM [ 11 ]. Definition 2. k -Partite Adoption Network .A k -partite adoption network is a graph whose vertices are or can be partitioned into k different independent sets: edges only exist in vertices from the same set. The edge weight is a real number which indicates the importance of the corresponding link.
 network. The values for the j -th attribute are characterized by the j -th vertex set in the k -partite adoption network. With loss of generality, we next present the construction of adoption graph with the settings of k =2and k = 3. These two cases correspond to two classic and widely studied tasks, top-N item and tag recommendation. Other cases with large values for k can be solved in a similar way, and we leave it as future work. We assume that the k -partite network is an undirected graph.
 Definition 3. Bipartite User-Item (UI) Network .Let U denote the set of all the users, and I denote the set of all the items. A bipartite user-item network can be denoted by G ( bi ) =( V , E , W ) , where the vertex set set E X  X  X I , the weight matrix W stores the edge weights, and W the link weight between a user u and an item i .
 Definition 4. Tripartite User-Item-Tag (UIT) Network .Let the set of all the users, I denote the set of all the items, and of all the tags. A tripartite user-item-tag network can be denoted by (
V , E , W ) , where the vertex set V = U X  X  X  X  , the edge set T )  X  ( I X T ) , and the weight matrix W stores the edge weights.
 adopted an item. Different from a UI network, in a UIT network, there can be three different types of edges consisting of vertices from two out of the three vertex sets, which correspond to the edge weights W u,i , W these weights, we use a simple counting method, W e 1 ,e 2 that e 1 and e 2 occur in all the adoption records.
 evaluating the relatedness between query vertices and candidate vertices. For example, in top-N item recommendation, a given user will be treated as the query vertex, and we search over the candidate item vertices to find out the most related ones. Next, we introduce a network embedding approach. Recently, networking representation learning is widely studied [ 10 , 18 ], and it provides an effective way to explore the networking structure patterns using low-dimensional embedding vectors. Not limited to discover structure patterns, network representations have been shown to be effective to serve as important features in many network-independent tasks, such as text classification [ 17 ]. In our current task, we aim to learn low-dimensional representations for the vertices on the k -partite adoption network. The embedding representation should encode important topological information and the similarity can be evaluated by these embedding vectors. 3.1 The General Network Embedding Model Formally, we use a d -dimensional embedding vector v embedding representation for a vertex e on the k -partite adoption network. We first describe a general network embedding model.
 Let us start with studying how to model the generative probability for an undirected edge between two vertices e s and e t , formally denoted as P ( e The main intuition is if two vertices v i and v j form a link on the network, their networking representations should be similar. In other words, the inner product v  X  v e large similarity value for two linked vertices. We define the probability of a link ( e ,e ) by using a sigmoid function as follows The probability P ( e s ,e t ) indicates that the link strength between two vertices e and e t . Recall that we also have the real weights for edges, i.e., the weight matrix W . We can also derive an empirical estimation  X  P ( e Following previous study on networking representation learning [ 18 ], we min-imize the KL-divergence of two probability distributions This essentially models first-order proximity modeled in the LINE model [ 18 ]. Although we can also model the second-order proximity as in LINE, the empir-ical results showed that the second-order did not perform well on tag recom-mendation. A possible reason will be that the second-order proximity mainly captures the shared contexts between vertices, and such an indirect modeling method does not work well on recommendation task. Thus, we only model the first-order proximity of the k -partite adoption network in this work. We follow the learning method in [ 18 ] to optimize Eq. 3 , which applies stochastic gradient descent with negative sampling and weight sampling. The running complexity is about O ( n  X  d  X  # neg  X  M ), where n is the iteration number, d is the number of latent factors, # neg is the number of negative samples and M is the number of training instances. 3.2 Utilizing Embedding Representations for Recommendations In the above, we have presented how to learn networking embeddings on the k -partite adoption graph. After parameter learning, we can obtain the embeddings for each vertex on the graph. Next, we will study how to make recommenda-tions with these embeddings. We consider two tasks, namely the top-N item recommendation and the top-N tag recommendation.
 Top-N Item Recommendation with Bipartite Network Embedding.
 Given a user u , the first task aims to produce a candidate list of N items based on her adoption history. In this task, we have two kinds of entities in the recom-mendation setting, namely users and items. We follow Definition 3 to construct the bipartite user-item network G ( bi ) (See Fig. 1 (a)). Then we run the network embedding model shown in Sect. 3.1 , and derive the embedding representations for both users and items, denoted by v u and v i respectively. Given a query vertex, i.e., a user, we would like to identify the most related item vertices. Formally, the task can be fulfilled using the following ranking funciton dations. Here, we do not consider repetitive adoption behaviors of users, it will be easy to adapt to the case where repetitive adoptions are considered. Top-N Tag Recommendation with Tripartite Network Embedding.
 Given a user u and an adopted item i , the second task aims to produce a can-didate list of N tags based on the tagging history. In this task, we have three kinds of entities in the recommendation setting, namely users, items and tags. We first follow Definition 4 to construct the tripartite user-item-tag network G tri ) (See Fig. 1 (b)). Then run the network embedding model (in Sect. 3.1 )on the tripartite network, and derive the embedding representations for both users, items and tags, denoted by v u , v i and v t respectively. Formally, the task can be fulfilled using the following ranking function data by using pairwise interaction factorization. High-Order Recommendation with Network Embedding. In more com-plex tasks, we can have multiple kinds of entities ( i.e., attributes or features) to consider. Our approach is quite general to incorporate arbitrary types of discrete features into the recommendation setting. The procedure can be described as fol-lows. We first construct the k -partite adoption graph, and then learn the embed-ding representations for each vertex on the network. After obtaining the embed-ding representations, we can define the score functions, such as Eqs. 4 and 5 ,to rank candidate vertices for recommendation.
 In what follows, we will call top-N item recommendation as item recom-mendation , and call top-N tag recommendation as tag recommendation for short. 1 We refer to our method as Network Embedding based Recommendation Model (NERM) . In this section, we conduct extensive experiments to evaluate the effectiveness of the proposed approach in two tasks, namely item and tag recommendation. 4.1 Evaluation on Top-N Item Recommendation Dataset. We use two shared datasets for the evaluation of item recommenda-tion: the JD dataset in [ 24 ] and the MovieLens dataset 2 product purchase collection, in which each adoption record consists of a user ID, a product ID and an adoption timestamp. MovieLens dataset is a large movie rating collection, in which each adoption record consists of a user ID, a movie ID and an adoption timestamp. 3 Table 1 summarizes the basic statistics of the two datasets. We select these two datasets because they are large and represent different data applications.
 Evaluation Metrics. For item recommendation, we adopt four widely used evaluation metrics, including Precision@K, Recall@K, Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR). Usually, only top ranked recommen-dations are important to consider, thus we set K to 10 in the experiments. Experimental Setting. Since each adoption record is attached with a timestamp, we consider a time-sensitive evaluation. We split the entire dataset by timestamps: the first 80 % data is used as training data while the rest 20 % data is used as test data. 4 Baselines. We consider using the following methods as the comparison baselines  X  BPR [ 12 ]. BPR is a Bayesian personalized ranking method for learning with implicit feedback. It adopts a pairwise loss function which assumes that an adopted item should be more weighted compared with an unadopted item.  X  DeepWalk [ 10 ]. DeepWalk is a recently proposed network embedding method. It first generates multiple random paths based on a social network, and further employs the word2vec [ 8 ] to deal with vertex sequences. tion task, representing state-of-the-art. DeepWalk is also a network embedding method and we incorporate it as a comparison. There can be several parame-ters to tune in baselines and our method. We hold out 10 % of training data as the development set for parameter optimization. For BPR, the number of latent factors is set to 256, and the number of negative samples is set to 300. For DeepWalk, the number of embedding dimensions is set to 1024 and we use the hierarchal softmax algorithm to learn the parameters. For our method NERM, the number of embedding dimensions is set to 1024, and the number of negative samples is set to 8.
 Results and Analysis. Table 2 presents the experimental results of the com-pared methods on the task of item recommendation. Overall, we have made the following observations. First, both network embedding methods are much bet-ter than the competitive baseline BPR. Second, NERM is slightly better than DeepWalk. These results indicate the effectiveness of the network embedding approach. BPR employs a pairwise ranking function to learn the preference order using implicit feedback, and each training case is a pair consisting a positive item and a negative item. DeepWalk generates truncated random vertex sequences, and derive the embeddings by using a hierarchical softmax. Compared to these two methods, NERM directly optimizes each edge ( i.e., each user-item adoption record) in the network and adopt the negative sampling as the optimization method. 4.2 Evaluation on Top-N Tag Recommendation Dataset. We use two shared datasets in [ 13 ] for the evaluation of tag recom-mendation. These two datasets have been widely used for tag recommendation. Different from [ 13 ], we do not perform p -core filtering. Table 3 summarizes the basic statistics of the two datasets.
 Experimental Setting. For tag recommendation, we following the same exper-imental setting in [ 13 ] for evaluation. We adopt Precision@K, Recall@K and F@K as the evaluation metrics. For each user, we take the last annotated item together with the attached tags as the test data, while the rest tagging records are used as training data.
 Baselines. We consider using the following methods as the comparison baselines  X  PITF [ 13 ]. PITF is a factorization model for tag recommendation, that explic-itly models the pairwise interactions (PITF) between users, items and tags.
The model is learned with an adaption of the Bayesian personalized ranking (BPR) criterion which originally has been introduced for item recommenda-tion.  X  DeepWalk [ 10 ]. It is similar to that is described in previous experiments on item recommendation.
 PITF represents a competitive baseline for tag recommendation in [ 13 ], PITF is better than FolkRank [ 5 ] on the two datasets, thus we do not compare with FoldRank here. We hold out 10 % of items in the training set as the development set for parameter optimization. For PITF, the number of latent factors is set to 256, and the number of negative samples is set to 200. For DeepWalk, the number of embedding dimensions is set to 128 and we use the hierarchal softmax algorithm to learn the parameters. For our method NERM, the number of embedding dimensions is set to 128, and the number of negative samples is set to 200.
 Results and Analysis. Table 4 presents the experimental results of the com-pared methods on the task of tag recommendation. Overall, we have made the fol-lowing observations. First, the proposed method NERM nearly performs best for all the entries, slightly worse than the state-of-the-art method PITF on Last.fm in terms P@5 and F@5. Second, the network embedding method DeepWalk per-forms poorly on the tag recommendation task. By combining the results on item recommendation, we can conclude that NERM is effective to deal with rec-ommendation tasks as a general method. DeepWalk does not perform well on tag recommendation, a possible reason is that it may require more principled random walk methods on k -partite graphs. Currently, we follow [ 10 ]tousea uniform sampling method, however, such a method may not be suitable to k -partite graphs. For example, it is likely that vertices in some independent set cannot be well covered even with many random paths. We will investigate into it as a future work. 4.3 Parameter Tuning In our model NERM, an important parameter to tune is the number of embed-ding dimensions. We vary it in the set { 32 , 64 , 128 , 256 , 512 , 1024 it affects the performance. The tuning results are shown in Figs. 2 and 3 .Aswe can see that, for item recommendation, we need to set a large number; While for tag recommendation, the optimal number is set to 128. The major reason is that the two datasets used for item recommendation are much larger than those used in tag recommendation. Recommender Systems. In the literature of recommender systems, two widely studied tasks are rating prediction and top-N recommendation . Rating predic-tion aims to predict the ratings from users to items, while top-N recommenda-tion aims to generate a short list of recommendations for users [ 4 ]. Our focus in this paper is top-N recommendation. There are three typical approaches for top-N recommendation. First, rating prediction methods were directly applied where the predicted rating value was used for ranking [ 9 , 23 ]. Second, implicit feedback information was utilized to improve the recommendation performance, such as the weighting-based method [ 6 ]. Thirdly, specific loss function in the optimization objective was developed, including AUC-based loss function [ 7 , 12 ] and MAP-based loss function [ 14 ]. Tag recommendation can be considered as a special task for top-N recommendation, where tag are recommended to a user on a specific item. Various methods have been proposed for tag recommendation, including random walk methods [ 5 ], time-sensitive methods [ 15 ], higher order singular value decomposition [ 16 ], and pairwise interaction tensor factorization [ 13 ]. Recently, several context-aware models have been also proposed in order to utilize complex contextual information for rating prediction [ 3 , 11 ]. Different from previous studies, we aim to build a general and scalable recommendation framework for top-N recommendation, and present a new perspective by apply-ing the network embedding techniques.
 Distributed Representation Learning. Recent years have witnessed the great success of distributed representation learning and neural networks. It pro-vides an effective way to represent and extract useful knowledge in many tasks, including text classification [ 17 ], knowledge graph mining [ 22 ] and recommender systems [ 21 ]. Especially, a promising direction is network embedding with dis-tributed representation learning [ 10 , 18 ]. For example, DeepWalk [ 10 ] adapted Skip-Gram [ 8 ], a widely used language model in natural language processing area, for network representation learning on truncated random walks. LINE [ 18 ] is a scalable network embedding algorithm which modeled the first-order and second-order proximities between vertices. More recently, heterogenous network embedding [ 2 ] or focus on deep network embedding [ 19 ] have been studied. We are also aware that several works have applied distributed representation learn-ing [ 20 ] or neural network models [ 21 ] to recommendation tasks. Our work is highly built on these studies. The novelty lies in the idea which casts the recom-mendation task into a network embedding task. To our knowledge, it is the first time that network embedding methods have been applied to recommendation tasks. In this paper, we made the first attempt that utilized the network represen-tation learning techniques for recommendation tasks. We first transformed the adoption records into a k -partite adoption network, then learned distributed representations for the vertices, and finally calculated the embedding similarity for recommendation. To evaluate the effectiveness of the proposed approach, we constructed extensive experiments on two different recommendation tasks using real-world datasets. The experimental results have shown the superiority of our approach. To the best of our knowledge, it is the first time that a network rep-resentation learning approach has been applied to recommendation tasks. Cur-rently, we adopt a simple network architecture for efficient parameter learning. In the future, we consider employing more complex deep neural networks [ 2 , 19 ] for recommender systems. We will also test how the current framework performs on context-aware recommendation which involves multiple kinds of contextual information, such as users X  demographics and items X  reviews.

