 While much of the recommender systems literature has focused on the off-line evaluation of prediction performance, a few case studies using online controlled experiments that assess the performance of business indicators are available. In this article, we describe the methods and re sults of an ongoing investigation conducted on the business value impact of personalized recommendations on three different portals of Nova Pontocom, the second largest Latin Ameri can online retailer. An on-line controlled experiment (A/B te sting), conducted for one month and covering 600,000 distinct users, statistically points out to a general revenue increase in the order of 8-20%. In addition, other consumer behavior metrics such as the number of page views and the more diverse di stribution of sales among the products catalog also support the positive impact of personalized recommendations in terms of business value. H.3.3 [ Information Search and Retrieval ]; H.4.2 [ Decision Support Systems ] Case study, online controlled experiment, business value, collaborative filtering Recently, several studies have been pointing out some pitfalls with regard to the evaluation of recommender systems that are based purely on off-line prediction performance [9, 11, 12, 13]. Some of these argue that other dimensions, such as novelty and diversity, should be considered in a proper evaluation of the quality of a recommender environment [1, 5, 6, 8]. Moreover, it is also evident in literature that off-line evaluations themselves are of limited scope when one deals with real commercial applications, because there is no guarantee that prediction performance necessarily translates into business value [8, 11]. This is the motivation of some recent works that are trying to identify the impact of recommender systems on real commercial applications. However, some of these are not based on real on-line controlled experiments, such as [3, 7]. Some remarkable exceptions are [15], which tests the conversion performance of different recommendation algor ithms against a human-made control scenario in a mobile internet games context, and [10], which investigates the relevance of the number of clicks as a metric for the online evaluation of recommender systems in a movies website. This article presents the method and results of a controlled on-line experiment in the three different portals ({pontofrio, casasbahia, extra}.com.br) of Nova Pontocom, the second largest Brazilian online retailer having 17% of market share, 6 million distinct users, and sales totaling more than US$ 2 billion in 2011 figures. The experiment was run for one month, independently in the three portals, summing up to nearly 600,000 different users; 50 million page views, 1 million online orders generating revenues of US $ 230 million. To the best of our knowledge, this is the larges t scale controlled experiment aiming to assess the business value impact of personalized recommendations published so far. Users were randomly assigned to a treatment group, in which personalized recommendations were presented, and a control group, with no recommendations. At the end of the experiment, it could be observed that with 95% of statistical significance, there was an overall increase in revenues in the order of 8-20%. During the test period, from 02/10/2012 to 03/10/2012, the users who visited one of the three websites were randomly assigned to either a treatment group (with recommendations) or a control group (without recommendations) with 95% and 5% of chance, respectively. The test for each website is independent from the others, that is, the persons visiting two such websites are considered in separate tests as being two different users. As usual in an e-commerce environment, users can navigate and view the recommendations (if in the treatment group) anonymously, but logging in is required for purchasing. In order to keep track of interaction history, we keep a record of the association between anonymous and logged users. In order to guarantee navigation consistency, we choose to keep the user X  X  test group when he logs in. This might be problematic because the user can navigate anonymously (and log in) in more than one machine, what would cause hi m to participate in different test groups. The workaround applie d to this problem is the following: if an anonymous user that is already assigned to a test group logs in and the corre spondent logged user was already assigned to a different test gr oup, we reassign anonymous test group to the logged user, maintaining the user experience consistency but making the logged user to participate in different test groups. The same o ccurs when a logged user logs off: the logged user X  X  test gr oup is assigned to the anonymous user. The test group associations are summarized in Table 1. In the simple case, when it is the fi rst user X  X  interaction with the test, he is randomly assigned to a test group. In subsequent interactions the same test group is used. In addition to these actions taken on-line, when processing the results we also merge the users X  anonymous and logged interactions. 
Anonymous user already has a test Anonymous user does not have a test group While observing that no ratings are available for the majority of the products, personalized reco mmendations presented in the treatment group were generated by seven different collaborative filtering techniques based on the users X  interaction history (implicit feedbacks), such as product views, purchases, and shopping cart composition. These techniques are presented to the users in the following different features that attempt to fit specific moments of the purchasing cycle: (a)  X  X etter together X , (b) Ultimate Buy:  X  X hat other items do customers buy after viewing this item? X , (c) Simila r Items:  X  X ustomers who bought this item also bought X , (d) Shopping cart:  X  X hat costumers that bought the products in your cart also bought? X , (e) View personalized:  X  X ou visited this item. You might be interested in these other items X , (f) Purchase personalized:  X  X ou bought this item. You might be interested in these other items X , and (g)  X  X ost popular X . Figure 1 shows a screenshot presenting some of the features (a-c) in a product description page (text in Portuguese). Three different tests were performed with regard to the per user numbers of product views, purch ases, and revenue generated. Since the main focus in both experiments was to assess the business value of personalized r ecommendations, we also kept a record of the purchases frequencies per product in each group in order to evaluate sales diversity . In this study, diversity was assessed as the expected information content of the product sales distribution (Shannon entropy) and also by means of Lorenz curves [14]. In short, for each item n ( n = 1,...,N ), we take its share on total sales ( p n ) in order to get its information content p n *log( p n ), and, thus, diversity is assessed by the normalized expected information content defined as - X  { p n *log( p n )}/log( n ). A potential problem that is experienced when working with per-user metrics is their accumulation over time, due to returning users. While observing that user averages would always increase, the moments of the distributions are not stable and, thus, no trivial statistical test can be performed. As pointed out by [2], a possible solution for this is the concept of a per-user-day metric, which was implemented in this experiment by considering users who returned to the websites on different days as two separate users. We f ound no significant serial correlation among daily interactions of a sa me user, thus exchangeability assumption still holds. Although this workaround might lead to the loss of relevant information regarding costumer fidelity, we assume here that its impact would at most increase the results that are presented. Figure 1. Product description page with recommendations. Another important remark is rela ted to the diversity of product categories. Some of them, such as electronics and informatics, have longer interaction cycles (in terms of bigger ratio views/purchases) and higher prices . On the other hand, categories such as books and DVDs show bigger order sizes (number of products bought by the same use r) and more frequent purchases. Although the aim of this experiment was to evaluate the impact of personalized recommendations on the entire websites, we realized the importance of performing separate testes for each product category. The reason for this is related to confounding problems in statistics when a hidden factor in the observations (here product category affiliation) possesses a strong explanatory power on the user behavior. While observing that the impact of recommendations might not be the same along with a wide diversity of products (i.e. we found it to be stronger in cell phones/accessories than in household appliances), mixing up these categories would lead to misleading results. In a slightly different problem, but still considering on-line controlled experiments, [2] relates this situation to the Simpson X  X  paradox by arguing that mixing data collected on two diffe rent days and from different splitting proportions might lead to a negative result, although positive, on both days. We checked for the existence of this situation by considering a simple two-category aggregation problem. It could be seen that merging a category whose control/treatment group difference is positive and statistically significant with another in which the difference is not significant can lead to a negative and statistically significant result. The issue of category aggregation with the aim of building an impact measure of an entire website is further addressed in the next section. In such a kind of an experiment, it is well known that the pure mean comparison between treatment and control can lead to spurious results due to the play of chance. Hence, we decided to work with confidence intervals for each metric calculated by means of the bootstrapping procedure [4]. In spite of simpler and easy-to-use formulas based on the normality assumption being available, a considerable bias in the confidence intervals was detected when using these methods. These findings are also in accordance to the literature [2 ]. The existence of a large number of users who had either no or very few interactions along with a few users with lots of interactions (what might be pretty common in online retailing) imprints data with high kurtosis, and, therefore, straightforward, normality-based confidence intervals are inadequate. While observing that the method of non-overlapping confidence intervals is not a proper hypothesis testing, in order to check whether the two samples are indeed statistically different, we applied a second bootstrapping procedure as follows [4]: Let x and y m ( n = 1,...,N, m = 1,...,M ) be two independent samples with whether both samples are derived from the same generating process), we first constructed a third sample z m+n concatenating x n and y m . Then, we resampled with replacement elements x  X  b , and of its last m elements y  X  b . Finally, the test p-value is given by the proportion of resamples, where x  X  -y  X  &gt; x  X  then reject the null hypothesis when p &lt; 0.05. In order to get an estimate of the impact of recommendations on the whole website (and not only by product categories), we compare two aggregated control and treatment scenarios. The control scenario is simply th e product between the evaluation metrics mean per user obtaine d without recommendations (and its correspondent confidence interv als) and the total number of users during the experiment in that particular website and category. The same procedure is applied for the treatment scenario, but we only add the mean per user obtained with recommendations in case there is a positive and statistically significant result in the category-specific hypothesis test. Otherwise, we assume that there is no impact at all and the same value of the treatment for that category is used. Finally, in order to obtain the ratio of difference (and confidence interval) between scenarios, we take an extra caution step by dividing the confidence bounds of the treatment scenario by the upper bound of the control scenario. This is an arbitrary decision that is taken with the aim of reducing the ch ance of a false-positive result. With regard to the revenue per user, we found statistically significant results in most of the large product categories (by number of users). In fact, we found no significant result in categories with less than 1,000 users, meaning that for those categories, the experiment peri od was probably too short. Table 2 depicts particular results for representative categories. The overall figures for the three websites are 8-19%, 9-19%, and 8-22%. Considering page views, the numbers in parenthesis represent the number of user s (real and anonymous). For revenue and diversity, they show the total revenue generated and the number of distinct products sold, respectively. These numbers are higher than t hose found in [10], for example, at least partially due to the fact that in their experiment the control group contained manually edited recommendations. The analysis of the number of page vi ews per user showed a slightly different picture. While observi ng that there were many more page views than actual purchases, we found significant differences for all the large categories, as depicted in Table 2. Considering the entire websites, it could be proved that the existence of recommendations in creased the number of page views per user in the order of 5-8%, 7-9%, and 5-7%, respectively. Besides the business metrics presented so far, we have also tested for the difference in diversification between the test groups. With regard to the five biggest product categories of each portal (a total of 15 tests), we found statistically significant differences in ten of them in the order (average) of 4% to 15%. Table 2 also depicts these results. The same effect could also be seen by means of Lorenz curves based on the diversity of sales among existing products, as depicted in Figure 2. Recent literature has pointed to the relevance of on-line controlled experiments in assessing the business value impact of recommender systems. In spite of being extremely useful, off-line evaluation metrics present some pitfalls in the evaluation of business indicators, because there is no guarantee, for example, that better prediction performance translates into bigger conversion rates or revenue per user. This article presents the ongoing results from an on-line experiment in the context of three different portals of Nova Pontocom, a large Brazilian online retailer. Statistically si gnificant differences could be observed between the treatment (in which personalized recommendations were presented) and control (no recommendations) groups with regard to key business metrics such as revenue per user (8-20 %), sales diversity (4-15%), and page views per user (5-9%). Such results support the use of personalized recommendations in the context of a large multi-item online retailer, even though no product ratings are available. Future research is s till needed in order to study how recommendations affect consumer behavior and, thus, business value. We intend to investigate this issue by checking for the difference in performance between the CF algorithms (features), which are presented in different navigational situations through the purchasing cycle. Thanks for the staff of Nova Pontocom for kindly cooperating with the experiment. Table 2. Confidence intervals for selected product categories. Diversity Revenue Page views [1] Anderson, C. 2006. The Long Tail. Hyperion, New York. [2] Crook, T., Frasca, B., Kohavi, R., and Longbotham, R. [3] Dias, M. B., Locher, D., Li, M., El-Deredy, M., and [4] Efron, B., and Tibshirani, R. J. 1993. An Introduction to the [5] Fleder, D. M., and Ho sanagar, K. 2009. Blockbuster [6] Fleder, D. M., and Ho sanagar, K. 2007. Recommender [7] Garfinkel, R., Gopal, R. D., Pathak, B. K., Venkatesan, R., [8] Ge, M., Delgado-Battenfeld, C., and Jannach, D. 2010. [9] Herlocker, J. L., Konstan, J. A., Terveen, L. G., and Riedl, [10] Jannach, D., and Hegelich, K. 2009. A case study on the [11] McNee, S. M., Riedl, J., and Konstan, J. A. 2006. Being [12] Pu, P., Chen, L., and Hu, R. 2011. A user-centric [13] Shani, G., and Gunawardana, A. 2011. Evaluating [14] Vargas, S., and Castells, P. 2011. Rank and relevance in [15] Zheng, H., Wang, D., Zhang, Q. Li, H., and Yang, T. 2010. 
