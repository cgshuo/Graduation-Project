 1. Introduction
Aggregate queries summarize information in a database, by dividing tuples into groups, where some target attributes agree on their values, and applying an aggregate function (e.g., a data warehouse [1] that stores detailed information about the transactions of a company in a huge fact table [15] with SELECT ProdID, StoreID, SUM(Quantity) FROM Sales GROUP BY ProdID, StoreID
In practice, the number of product/store combinations can be large and the results could overwhelm the user. Besides, the gates in all product/store combinations. We could express  X  X mportance X  by a HAVING clause that selects only those groups urally. As an example, a top-k groups query can be expressed in SQL, by adding to the statement above the following lines: ORDER BY SUM(Quantity) DESC STOP AFTER k
Apart from finding heavy groups in data warehouses, the top-k groups query also finds application in other data mining
Web documents with the largest number of incoming (or outgoing) links. Finally, as demonstrated in our experiments, top-k queries can be used to identify pairs of network ports with high volume of information flow, from traffic traces of TCP packets.

The evaluation of top-k groups queries could be facilitated by exploiting materialized views [12] over the base data. How-ever, the selection of attributes in such queries could be ad hoc. Pre-computation, materialization, and maintenance of Specifically, we focus on the class of distributive aggregate functions (say, gate functions (e.g., MEDIAN ).

A straightforward solution is to keep a counter for each group in memory and update the corresponding count while scan-group in memory even though today X  X  machines have large memory size. For example, it is not uncommon to have attributes with domain size in the order of 1000. For a query with four group-by attributes on a peta-byte warehouse, the number of required counters is (1000) 4 , which translates to 4 tera-bytes (assuming 4 bytes per counter).
The traditional method (by an RDBMS) for evaluating iceberg and top-k groups queries (using limited memory) is to com-k . This method (implemented by hashing or sorting with early aggregation) can be quite expensive, since the group-by oper-ation may apply multiple passes over the data to compute the aggregates for all groups, while most of them are expected to be eventually pruned. Previous work on iceberg queries [7,17] employed sampling-and/or hash-based techniques to elim-inate groups having small aggregates early and minimize the number of passes over the base data. The extension of these a priori.
 as ad hoc ranges in continuous or spatial domains. Second, aggregate extensions of multidimensional indexes [16] were pre-assumption, since the number of attributes in a relation (or combination of relations) could be arbitrary and the query may involve any subset of them. There is also a number of theoretical studies on one-pass approximate top-k groups retrie-situations where exact retrieval of groups and their aggregates is essential.

The goal of this paper is to provide solutions for on-demand and exact top-k groups extraction, under bounded memory nario where neither specialized multidimensional indexes are available, nor approximation of results is acceptable. Specif-ically, we investigate three representative scenarios and develop comprehensive techniques for them:  X  For the case where tuples are physically ordered by measure, we propose the write-optimized multi-pass sorted access algorithm (WMSA), that exploits available memory to compute top-k groups efficiently.  X  Under the scenario of unordered data, we study the recursive hashing algorithm (RHA), coupled with branch-and-bound techniques and derivation heuristics for tight aggregation bounds of partitions.  X  For the special case where the tuples to be aggregated are clustered according to a subset of group-by attributes, we develop the clustered groups algorithm (CGA), which accelerates top-k groups processing.

Our algorithms are cross-compared with traditional RDBMS approaches for on-demand top-k groups retrieval, using real and synthetic data.

The remainder of the paper is organized as follows. Section 2 reviews related work. Section 3 discusses the different prob-evaluation of all above techniques are presented in the respective sections. Finally, Section 7 concludes the paper. 2. Related work review past research on these problems and discuss their relationship to our work. 2.1. Top-k aggregate queries in OLAP
Mamoulis et al. [20] proposed an algorithm for computing top-k groups queries from a data warehouse, assuming that the
Non-leaf entries of the tree are augmented with aggregates that summarize all measures in the sub-tree pointed by it. Group is presumed to be a continuous range in the corresponding dimensional domain (e.g., product-type  X  X oy X  consists of product-ids 100 X 250). The main idea behind the top-k groups retrieval method is to traverse the tree and maintain upper and lower groups found so far.
 the set of dimensions have to be indexed by an aggregate multidimensional index, and (ii) the group targets must be formed by continuous sub-domains of the dimensions. The first hypothesis requires numerous multidimensional indexes (one for each combination of dimensions), which are inefficient or infeasible to maintain if the total number of dimensions is large ploy a single multidimensional index consisting all the dimensions; however, such an index renders inefficient query pro-for a total value ordering of a base attribute.

Recently, Li et al. [18] proposed an RDBMS operator for top-k groups queries that schedules the accesses of groups and their tuples, in order to minimize the amount of accessed information before the query result is guaranteed to be found.
The proposed methodology has practical limitations because it is based on the following strong assumptions. First, it as-sumes that the algorithm has control on the order by which the groups will be examined and on the order by which the size of each group has similar cost to computing the aggregate of the group itself.

Loh et al. [19] propose methods for processing top-k range queries in OLAP cubes. Given an arbitrary query range, the are pre-computed. These values can then be used to compute the top-k results in query regions that cover multiple cells. gated values of groups (as opposed to top-k measures) in the whole space (as opposed to a particular range). 2.2. Iceberg queries
Iceberg queries were first addressed in [7] . A typical query optimizer [9] would compute the aggregates for all groups and then return the ones whose score exceeds the query threshold t . Fang et al. [7] present several methods based on sampling in a whole hierarchy of OLAP cubes. These methods aim at avoiding useless aggregations for groups, which disqualify the on the targets. Buckets with smaller count than the threshold are immediately pruned. Sampling is used to identify early potential targets that could end-up in the result, which are treated separately.

In [17] , these methods are compared with sorting and hashing with early aggregation [9] . The results show that the meth-and then disregard their tuples at later passes, thus shrinking the data to be processed. However, if the scores between groups do not vary much, the attempts to shrink the data prove futile and a simple hash-based or sort-based method be-comes better.

The extension of the techniques of [7] for rank-based retrieval of important groups is not straightforward, since the k th nique for this case. 2.3. Top-k algorithms for middleware
Middleware top-k algorithms have been proposed for combining different rankings of the same set of objects in order to return the k objects with the highest combined score according to an aggregate function. Assume for example that we wish based on their scores in each of the query components, the problem is to identify the k restaurants with the best combined ranked inputs by monotone aggregate functions. They identify two types of accesses to the ranked lists; sorted accesses ods that are asymptotically optimal in the number of accesses they perform, for both cases depending on whether random accesses are allowed or not.

The threshold algorithm (TA) accesses sequentially the ranked inputs and, for each object seen there, it performs random ject found so far has higher aggregate score than the aggregate score of the last values seen at each input. The no random score than the score that any other object can reach. In [4] , extensions of these methods for the case where only random accesses are possible for most inputs are presented. Ilyas et al. [14] show how to adapt NRA for joins followed by ranking in relational databases. Finally, Mamoulis [21] optimized the NRA algorithm to a version that minimizes computational cost, on their measure values. 3. Problem settings
In this section, we define the problem of top-k groups queries for various cases and motivate the design of the proposed algorithms. Then, we discuss the setting used for empirically evaluating our algorithms. 3.1. Problem definition
We consider a query with a set G of group-by attributes and an aggregate function agg  X  X  that applies on a single measure attribute v . The tuples to be aggregated may physically be stored in the disk or produced by underlying operators (e.g.,
We develop methods that minimize the number of passes over the data, aiming at exact query evaluation. In particular, we explore various orderings of the query input that may apply in practice, as follows.  X  The data are physically ordered or clustered with respect to the measure attribute v , i.e., supporting the sorted access to a central server, which merges them and performs the aggregation. For these cases, we adapt the NRA algorithm [6] to minimize the number of accessed tuples before the top-k result is finalized (see Section 4 ). rithms for iceberg queries and generic hash-based aggregation.  X  The data are clustered with respect to one or more group-by attributes. For stored data, this case may apply if a clus-tered index or a hash-file organization exists. In a data stream scenario, tuples could be produced ordered by a group-by attribute (e.g., time, location). We examine such cases in Section 6 , by reducing the top-k groups problem to finding the ing to that attribute. We show that this case can be reduced to multiple, smaller problems. Roughly speaking, for each batch of tuples, having the same value (or set of values) for the clustered attribute, we only have to solve an unordered data subproblem.
For the ease of exposition, throughout the presentation, we consider data aggregated from a single table T with three of group-by values and it is implemented as such in our evaluation tests. The domain of v is assumed to be the interval  X  0 ; 1 ; nevertheless, the extension for arbitrary domain interval  X  V ward. Unless otherwise stated, we consider the SUM function for aggregation, which is the most common in OLAP queries; nonetheless our methods can be easily adapted for other monotone distributive functions, e.g., listed in Table 1 will be used in the discussions throughout the paper. 3.2. Experimental setting
To enhance the readability, the experimental evaluation of our proposed algorithms have been decomposed into their respective sections ( Sections 4.4, 5.3, and 6.2 ). In the following, we discuss their common experimental setting. packets, containing 3.86 M tuples with the following attributes: timestamp, source host, destination host, source TCP port, a top-k query ( Q 1) that computes the connections with the largest traffic at 1-s intervals: SELECT Tstp,Sip,Dip,Sport,Dport,SUM(PktSz) FROM dec _ wrl _ 4 GROUP BY Tstp,Sip,Dip,Sport,Dport ORDER BY SUM(PacketSize) DESC STOP AFTER k
Our second query Q 10 is the same as Q 1, except that the timestamp is discretized at every 10 s. The number of distinct groups for Q 1 and Q 10 are 765 K and 232 K, respectively. sizes of groups follow a Zipfian distribution (at a fixed skewness h measure values v in individual tuples also follow Zipfian distribution (at skewness h We considered four distributive aggregate functions (i.e., tuples have the same value, thus the results can be evaluated by all our algorithms. On the other hand, the query for be reduced into that of MAX as follows: (i) in the beginning, each tuple value v is mapped to the value 1 v (assuming that 1 t for obtaining the final results.

We implemented all tested algorithms in C++ and the experiments were executed on a PC with a Pentium 4 CPU of 2.3 GHz. We used a default memory buffer whose size is 2% of the dataset. The page size is 4 K bytes so that each page operations) to the number of pages in the dataset. 4. Algorithms for inputs ordered by measure
In this section, we propose techniques for top-k groups queries, applicable when the tuples are physically ordered based on the aggregated value v . In Section 4.1 we propose an algorithm that applies on a descending order of the tuples by v , memory.

These algorithms enable early termination as long as the lower bound scores of the top-k groups found so far are guaran-out necessarily obtaining their exact scores. 4.1. Preliminaries
Our first algorithm, termed  X  X  X orted accesses X  (SA) is an adaptation of the  X  X  X o random accesses X  (NRA) top-k technique, discussed in Section 2.3 . It scans the input T , while updating lower and upper aggregate scores for the groups seen so remaining ones, the algorithm terminates. Fig. 1 shows an exemplary query and the table T , which we will use to describe SA.

Before presenting SA, we discuss the computation of the lower/upper bounds it utilizes while scanning the input, consid-ering a SUM aggregate function. Initially, we assume that the maximum cardinality C
C max  X  3 in our example). Later, we relax this assumption. Let g accessed tuples in the group. A lower bound lb  X  g x  X  and an upper bound ub  X  g where v max represents the maximum value v of unseen tuples, e.g., the value of the latest seen tuple. These equations are applicable to the COUNT function as well, by fixing each value t v to 1. Regarding the bounds are defined as follows:
Since the tuple values are accessed in their descending order, the first tuple value seen in a group must be the maximum value in the group. For the MIN function, the problem could be reduced into that of the 3.2 .

Fig. 2 illustrates a pseudo-code of SA. The algorithm organizes groups that have been seen so far and can end-up in the top-k result in a hash-table H (with group-id as search key). It also maintains an initially empty min-heap W groups of the highest lb  X  g x  X  so far. When a tuple in group g it can enter W k . If an existing group g y (in H ) satisfies both conditions (i) g groups after each tuple access only if s :  X  v max C max 6 q (see [21] for a theoretical proof in the NRA context), thus it is meaningless to attempt pruning. We say that the algorithm the algorithm checks whether the remaining (non-pruned) groups are k , in order to terminate. The pruning at lines 12 X 
Theoretically, the worst case tuple access cost of SA is
Example. We illustrate the functionality of SA on the table of Fig. 1 , assuming k  X  1. Note that C
First, we access a tuple of g 2 and set v max to its value 0.7. Then, we update j w  X  g bound score q is updated to 0.7 ( W k  X f g 2 g X  . The procedure continues since s  X  v g 0.41 and update j w  X  g 5  X j X  1, lb  X  g 5  X  X  0 : 41. Now, we have s  X  0 : 41 3 cannot end-up in the result. Hence, the result must be one of the groups that have been accessed (i.e., groups g this, we must access the remaining tuples until q is no smaller than the upper bound scores of all accessed groups not in W
Since ub  X  g 4  X  X  1 : 32 6 q and ub  X  g 5  X  X  1 : 23 6 q , the algorithm terminates and reports g replaces Eq. (2) by in the experimental evaluation, when C max is replaced by C forcing the algorithm to access more tuples before its termination.

Observe that SA (and its extensions presented in subsequent sections) compute the top-k groups and a lower bound of the exact scores of the query results.

We now discuss the computational requirement of SA. After s drops below q , the pruning step (Lines 13 X 15) is compu-tationally demanding as the whole hash table H in memory has to be traversed in each iteration. In order to speed up the accessed. 4.2. Multi-pass sorted access algorithm
SA becomes inapplicable in practical applications as the memory cannot accommodate all candidate groups for the top-k bounded settings.

The basic idea of MSA is to use up all available memory while scanning T and compute the top-k groups among those the groups that fit in memory has been finalized, the algorithm frees the memory, keeping only information about the top-k ing process is repeated, until the top-k groups are guaranteed to be found.

Fig. 3 shows a pseudo-code of MSA. Let M be the number of memory pages and B be the number of tuples that can fit in a page. MSA uses a hash table H of size bounded by M B for tracking candidate groups in memory, and a creates a temporary table T 0 for storing tuples in groups that cannot fit in memory. Initially, variable fullH is set to remainder of T ). In addition, tuples from T before the current cursor which have already been considered by the groups of
H will not be processed again. Note that at each loop, the algorithm initializes a new T next pass. Fig. 4 illustrates this virtual appendix of T 0 to T at each loop of MSA.

Note that it is possible that some tuples appended to T 0 may belong to groups that have already been pruned (i.e., in a previous pass). However, this does not affect the correctness of MSA. When those tuples are processed from T 0 in the next pass, their corresponding groups will only have smaller upper score bounds and eventually get pruned. MSA can also be adapted to a variant MSA 0 that does not use C max (in the same way as SA is adapted to SA 0 ).
We proceed to analyze the worst case tuple access cost of MSA, by using the notations in Table 1 . In the worst case, the minimum number of groups that have been completely aggregated in each pass is MB (assuming MB k ), leading to a reduction of MB  X  N = G  X  tuples in the data cardinality. There are at most G =  X  MB  X  passes. In the i th pass, at most access cost of MSA is
Example. We demonstrate the execution steps of MSA on the example in Fig. 1 , assuming that k  X  2 and the memory can fit at most 3 tuples. After reading the first four tuples, we have lb  X  g is inserted into the temporary table T 0 . Then, we read a tuple of g the next two tuples (of g 5 ), updating lb  X  g 5  X  X  0 : 87. Since s  X  0 : 13 3 with the process after the table T 0 is appended to the beginning of T . The group g 0.12) is read. Eventually, MSA returns the groups g 2 and g 4.3. Write-optimized MSA
Recall that when SA and MSA are in the shrinking phase (i.e., s reduced. On the other hand, while the top-k results for the currently processed groups are finalized in MSA, if the group enough memory to accommodate these temporary tuples, thus there is no need to flush them to disk immediately. More significantly, if we use the memory space freed from H to temporarily store tuples, we could apply early aggregation for some groups; two or more tuples of the same group temporarily held in memory can be aggregated and shrunk to a single tuple for the current group.

We propose WMSA, a write-optimized version of MSA, which utilizes the memory freed from H in the shrinking phase. In ity of H 0 is set to 1 memory page, and later extended by the freed memory pages, as H shrinks. When H 0 becomes full, we group-id (i.e., values of group-by attributes), the number of tuples seen in the group (e.g., sum). Note that T 0 may contain multiple records of the same group, which are not flushed at the same time.
In the next pass, the groups in T 0 are read and processed according to their flushed order. Nevertheless the aggregated values of the groups in T 0 may now not appear in sorted order, while we need this order to derive an appropriate value for v max (e.g., in Line 8 of MSA) to set a pruning and terminating condition. To solve this problem we apply the following heu-
Thus, if T 0 is created by flushing H 0 R times, it can be thought of as a sequence of R partitions T 0
T we know m i ; the minimum value of all (early-aggregated) tuples in it. When T 0 is processed after being appended to the beginning of T  X  X  remainder, we update v max to m i after each partition T 0
For example, assume that H 0 has become full and it was flushed three times to the T 0 shown in Fig. 5 . For the three par-titions we keep the minimum value of the original tuples from T there. When we process T 0 , for each tuple we read, we do not update v max as in Line 8, but only after the whole partition T 0 next partition T 0 2 . Similarly, when T 0 2 is complete, we set v mation from the original tuples between flushes.

If, while T 0 is read, H (and H 0 ) becomes full and tuples must be written to a T 00 , m m for the current partition T 00 j flushed to disk. The correctness of the algorithm is not affected, since m
As a remark, the worst case access cost of WMSA is the same as that of MSA. This occurs when the number G of groups in the data is much larger than the memory size; and thus no aggregation occurs in the extra hash table H 0 .
A final thing to note is that SA-based algorithms (MSA and WMSA) can also be used for cases where the input is partially ordered. Consider a case, where T is divided into multiple partitions T to SA (and its extensions), except that it operates on a partition-by-partition basis instead of a tuple-by-tuple basis. 4.4. Experimental results
Following the experimental setting of Section 3.2 , we now evaluate the performance of bounded-memory algorithms on
Fig. 6 shows their access costs with respect to k . MSA 0 and WMSA 0 do not use C and WMSA, respectively. The cost of MSA 0 (WMSA 0 ) is insensitive to k and converges to the cost of MSA (WMSA) for large k temporary tables T 0 . The algorithms have lower costs for query Q 10 than Q 1 because Q 1 considers a larger number of distinct groups, having higher memory requirements. Note also that MSA (MSA 0 ) has similar cost to WMSA (WMSA 0 )in Q 10, as the temporary tables T 0 are of negligible size (note the sub-linear costs of the methods). orders of both the measure values and other attributes (e.g., if multiple B methods were almost always found to be more expensive than SA and its extensions, mainly due to the overhead of required random accesses.

We now study how the performance of algorithms is affected by the available memory (see Fig. 7 ). As expected, their costs decrease as the memory size increases. With a larger memory buffer, more candidates can be stored in memory and fewer passes are performed over the data. Again, WMSA outperforms the other methods. Observe that with as little memory as 2% of the data size, the cost of this method becomes sublinear.

In the next two experiments, we study the behavior of the algorithms on synthetic datasets. Fig. 8 a shows the access costs of the algorithms with respect to the database size. Note that they are insensitive to this parameter, given a proportional memory buffer (2%). Fig. 8 b shows their access costs for different group size skew h at low group size skew, most of the groups have similar sizes and C of the groups become very tight and MSA/WMSA can terminate early. 5. Algorithms for inputs with random order In this section, we study the evaluation of top-k queries for generic inputs T , where the tuples appear in random order.
We assume that memory is bounded such that we cannot accommodate a counter for each group. Thus, multiple passes over algorithm [7] for our problem and (ii) an extension of the hash-based aggregation algorithm [9] that minimizes the number of accesses for top-k groups computation, using branch-and-bound techniques in combination with appropriate heuristics for deriving tight upper bounds for the candidate groups. 5.1. The bucket algorithm
Our bucket algorithm (BA) is an adaptation of the Defer-Count method of [7] , which was proposed for the evaluation of iceberg queries; i.e., the identification of groups with COUNT ference between Defer-Count and BA is the online tuning of a lower bound for q , during the execution of the algorithm.
Fig. 9 shows a detailed pseudo-code for BA. BA follows a two-step filter/refinement paradigm. The filter step (Lines 3 X 15) eliminated. During the refinement step (Lines 16 X 21), the remaining candidates are aggregated and compared. Parameter F filter cost but high refinement cost. We will discuss how to eliminate this parameter in Section 5.1.2 . Ratio of page accesses
First, the algorithm draws a random sample T 0 of T . The j P k groups with the highest aggregate scores in the sample (i.e., groups expected to be heavy targets) are written to a set C of candidates . times. In the j th filter step, an array A of counters and a bitmap E passes.
 known. The k groups in C with the highest scores are inserted into the candidate top-k result S
S k and is used as a pruning bound. Finally, for each bucket A  X  z , bitmap position E
In the subsequent filter passes, tuples that do not belong to groups in C (these groups have already been counted) are bitmaps. Such a bitmap manipulation technique has been introduced in [7] , and was shown to effectively reduce the number of candidate groups that cannot lead to the result.

During the refinement step, only groups having 1 in all bitmap positions of the filter passes are counted exactly and the step cannot fit in the memory, then they must be partitioned and counted over multiple passes of data.
We continue to analyze the worst case tuple access cost of BA, based on the symbols in Table 1 . Clearly, the filter step leads to F N tuple accesses. In the worst case, no pruning occurs and all the groups belong to the candidate set. Since
N tuple accesses. Therefore, the worst case tuple access cost of BA is 5.1.1. An example
As an example, assume that we run BA on the table of Fig. 10 , with parameters k  X  1, F  X  2 and j  X  1. Assume that in the random sample, the top j  X  1 group is g 5 , thus C  X f g 5 the buckets for the groups (e.g., function h 1 places groups g counters A for the two buckets and the exact aggregate scores (i.e., 0 : 91) of the groups in C (i.e., g q  X  0 : 91. After the pass, the first bit of the bitmap E q . In the second pass, we scan the table again and compute new buckets A , using another hash function. Since E only E 2  X  2 is set. Note that the only non-pruned group at this point (by either hash function) is g step, only g 2 will be counted, agg  X  g 2  X  will be compared to C  X f g 5.1.2. Optimizations
We developed two optimization techniques for terminating the filter step in BA early. The first one is to terminate when the number of distinct groups in the dataset. An upper bound of the candidate set size can be estimated by multiplying the above optimizations lead to automatic termination of the filter step so that users need not specify the number F of filter passes. 5.2. The recursive hash algorithm
One potential problem with BA is that it is not easy to determine an appropriate value for the number of buckets (i.e., bitmap length). In case there are too few buckets, many groups are hashed to the same bucket and the filter becomes inef-fective. The effectiveness of the filter step can be improved by employing more buckets, however, the main memory may not be large enough to accommodate a huge number of buckets.

To avoid this problem, we extend the recursive hash algorithm (RHA) [9] with early aggregation to retrieve the top-k groups. Graefe et al. [9] compute the aggregate scores of all groups unconditionally, even though most of them cannot be the top-k groups. Motivated by this, we present two carefully designed optimizations for enhancing the performance of
RHA on top-k groups extraction. First, during hashing and early aggregation, our RHA algorithm derives upper bound aggre-gate values for groups in hash partitions and uses them to eliminate partitions that cannot lead to any result. Second, we propose an appropriate ordering of processing the partitions, which leads to early discovery of groups with high aggregate values and significant access cost reduction of the algorithm.

Fig. 12 illustrates the pseudo code of RHA. Let B be the number of group counters that can fit in a page and M be the num-ber of memory pages. RHA first initializes an initially empty set S routine on the table T . The recursive routine consists of three phases: the hashing phase (Lines 1 X 11), the clearing phase (Lines 12 X 17), and the branching phase (Lines 18 X 26). At each recursion level i , RHA uses a hash function h the tuples to R &lt; M buckets.

In the hashing phase, for each bucket r , a variable ub r (early aggregation), otherwise we create a new memory slot for x .If M bound for a group in T r , by adding to ub r the maximum agg  X  x  X 2 M ing disk partition T r .

After scanning all tuples from T , the recursive routine enters the clearing phase. We update the set S groups so far (and the k th score q , used as a pruning bound), from the partitions for which no groups have been flushed to disk. For these groups, agg  X  x  X  is guaranteed to be their complete scores. The remaining memory partitions are flushed to the corresponding disk partitions for further processing and their ub
Next, in the branching phase, RHA is recursively applied to each partition in descending order of their ub to discover groups with high aggregate values as early as possible. We prune partitions for which ub contain groups with higher aggregate values than those currently in S is loaded and its groups are aggregated while S k and q are updated. Otherwise, the algorithm invokes the recursive routine for the disk partition T r . 5.2.1. Deriving tighter upper bounds
We can derive a tighter upper bound than ub r for the groups in a partition T ters. Specifically, for each partition r , we use an array of L counters and a secondary hash function h counters for partition r are set to 0. For each memory part M into L segments, by applying h 0  X  x  X  to each group x 2 M agg  X  x  X  and add this value to the j th counter. Thus, after T bound ub r for the maximum group aggregate in T r .

The use of multiple counters presents us with a trade-off between memory allocation and tightness of the computed bound. The more counters allocated for a partition, the less memory can be used for holding and early-aggregating groups.
Note that for a partition r , allocating space for counters is not necessary until the first time M point, L counters are preserved for them in the memory. Thus, the average memory allocated per partition in the worst-case R L . The impact of L to the overall performance of RHA is studied in the experimental section.
As a final note, the upper bound of RHA X  X  cost is the cost of hash-based aggregation [9] without any pruning: where R is the number of memory partitions, and the descriptions of other symbols can be found in Table 1 . This is indeed the worst case of RHA, where all groups have the same aggregate score. However, in practice, skew in the distribution of scores brings significant cost savings to the algorithm, as we demonstrate in the next section.
Example. We proceed to apply RHA for computing the top-1 group over the example of Fig. 10 . Assume that we have R  X  2 memory partitions, and each one is able to hold 2 groups at the same time. Suppose that the groups g partition M 1 whereas the groups g 1 ; g 4 ; g 5 are hashed to the partition M
T , we have: (i) the group g 2 in M 1 , and (ii) the groups g from the group g 1 , belonging to the memory partition M 2 following (partial) group scores: lb  X  g 4  X  X  0 : 9 and lb  X  g group g 1 can be hashed into M 2 . The remaining tuples are read iteratively and their corresponding main memory partitions are updated. Subsequently, M 1 contains lb  X  g 2  X  X  1 : 49 and lb  X  g non-empty disk partition(s): ub 2  X  0 : 9  X  max f 0 : 2 ; 0 : 38 g X  1 : 28. Since ub
T . Eventually, the algorithm reports g 2 to be the top-1 group, having the score 1.49. 5.3. Experimental results
In this section, we adopt the experimental setting of Section 3.2 and compare the relative performance of algorithms that operate on unordered data; BA and RHA. The algorithm BA is configured to use 10,000 bucket counters and j  X  2000 heavy targets in memory, after drawing a 1%-sample from the data. The remaining memory is used for computing the FM sketch in the first filter pass.

In addition these methods, we implemented two baseline algorithms SORT and HASH. These correspond to the sorting computes top-k groups while merging the runs of the previous pass. Similarly, HASH maintains a heap of the top-k groups, while computing the scores of the groups at its final pass. Thus, HASH corresponds to the worst-case of RHA, where no buck-et can be pruned until the last pass.
 We first assess the impact of the number of counters L and R performing the first pass and writing to disk the overflown memory partitions. The results show that HASH has cost insen-sitive to the number of partitions, which is expected, since, for a given memory, the same number of tuples will be hashed and early aggregated, no matter how many partitions we have. On the other hand, our RHA algorithm that employs pruning essential cost of hashing in the first pass. First, the upper bounds ub smaller. Second, after the algorithm is recursively applied to a large number of small disk partitions T counters for RHA and as many partitions as permitted by the available memory.
 to process the partitions T r in descending order of their upper bound values ub as early as possible. Observe that the descending ordering achieves 30% and 23% cost reduction over the ascending ordering, for queries Q 1 and Q 10, respectively.

We proceed to study the effectiveness of our optimization techniques for terminating BA early. For this, we run BA for the rithm has to execute F  X  10 filter passes. Then, we run BA with early termination techniques for the same experiment. Now,
BA performs only 1 X 3 filter passes (depending on the value of k ) as it estimates that the remaining groups (with possible aggregate value greater than q ) fit in memory. Eventually, those groups are aggregated in a single refinement pass. In the subsequent experiments, we apply early termination techniques for BA and set the maximum number of filter passes to F  X  10.

In the remainder of the experiments, we compare all algorithms together. Fig. 15 shows the access costs of BA, RHA, SORT, essentially examined. Both SORT and HASH are insensitive to k as they do not utilize the pruning bound. RHA outperforms methods could be worse than simple early aggregation techniques (as also shown in [17] ).

Fig. 16 plots the access costs of the algorithms as a function the memory size. For BA, the number of hash buckets, heavy targets, and samples are scaled in proportion to the memory size. RHA has the best performance and it degrades gracefully for limited memory. The performance gap between RHA and HASH shrinks as memory size increases because at high mem-(independently of the hash-function used) and groups are rarely pruned. So far, we only considered the SUM aggregate function. Our algorithms can also be applied for other distributive functions: COUNT , MAX , and MIN (see Section 3.2 ). For instance, both BA and RHA are applicable to both of RHA). Table 3 shows the access cost of the algorithms with respect to the aggregate function. We applied variants of Q 1 and Q 10 that replace the SUM function with some other aggregate. For the query Q 1, BA is more expensive for the worst performance for MIN because most of the tuples have low values, leading to a high number of groups with low rithms have similar relative performances.
 Fig. 17 shows the performance of the algorithms on synthetic datasets as a function of the data size and group size skew. behave differently for different group size skew. At low group size skew, many groups have similar scores as the top-k groups. RHA could hardly prune any partition and thus degenerates into HASH. Similarly, BA requires many data passes as its buckets cannot help pruning unqualified groups effectively. Since RHA, SORT, and HASH apply early aggregation to re-duce the data size during each pass, they perform much better than BA for the worst distribution of group sizes. 6. Data clustered by a subset of group-by attributes
In this section, we show how the case of data ordered or clustered based on some group-by attributes can be reduced to the random-order case, where the same algorithms discussed in the previous section can be used as modules for query eval-uation. At the end, we present an experimental study for the proposed method. 6.1. The clustered groups algorithm (or arrive, in case of a streaming input) continuously without any other tuples being interleaved.
For example, consider the query with G  X f a ; b ; c ; d g plied. Note that, in the table, for each combination of values for attributes which are continuous in the table). In other words, G 0  X f
To solve this top-k query we can apply the clustered groups (CGA) algorithm shown in Fig. 19 . The algorithm reads all tu-exactly the same value). The results of these batches are merged to derive the global top-k groups. In our implementation, batch is known to have been read completely, the next phases of the algorithm begin to compute the top-k groups for the of the currently processed batch.

We proceed to study the worst case tuple access cost of CGA, by using the symbols in Table 1 . Let G distinct groups per batch; there are G = G 0 batches in total. Thus, the cost of CGA is assuming that CGA applies RHA to compute the top-k groups within a batch of tuples. In order to simplify the equation, the hidden parameter R (specific to RHA) is not shown here. 6.2. Experimental results
We followed the experimental setting of Section 3.2 and performed two experiments to validate the performance of the clustered groups (CGA) algorithm. For this purpose, we used the real dataset (i.e., TCP packets trace). We assume that the would easily fit in memory and queries would be processed by a single database scan.
 batches. As the time-grouping becomes coarser, more groups are formed (the distinct combinations of the remaining group-by attributes increase), thus it becomes more likely that the number of groups exceed the available memory, increasing the problem to smaller ones that are solved separately with full utilization of the system resources. Fig. 20 b shows the cost breakdown per batch for each of the 18 batches in the Q 200 query. Note that the average cost per batch drops as we proceed becomes larger, thus q becomes more effective in pruning early hash buckets at the current batch. The reason for the fluc-tuation among the costs at different batches is that the number of distinct groups between batches may vary significantly, affecting the corresponding bucket sizes and causing variable memory overflows among RHA executions. 7. Conclusions
We studied an important data analysis operator that retrieves the k groups with the highest aggregate values. In data find the top-k groups in the case where the distinct number of groups exceeds the number of counters that can fit in mem-ory. In this paper, we addressed this problem by proposing algorithms that do not rely on pre-computation or materializa-tion, but apply on-demand retrieval of groups for ad hoc sets of group-by attributes.
 For the generic case of unordered tuples, we developed the bucket algorithm (BA) and the recursive hash algorithm (RHA).
BA adapts from an iceberg query algorithm [7] and utilizes count of buckets to reduce the candidate size. In addition, we proposed optimizations for early terminating BA. On the other hand, RHA applies the branch-and-bound paradigm to min-imize the number of buckets to be examined, until the top-k result is finalized. To improve the efficiency of RHA, we sug-gested two optimizations: (i) ordering the processing partitions in an appropriate way, leading to early discovery of bounds for partitions. Our experimental results on both synthetic and real datasets show that RHA outperforms BA and tra-ditional RDBMS methods by wide margin in terms of data access cost.

Assuming that the tuples to be aggregated are ordered by their measure values, we proposed the multi-pass sorted access algorithm (MSA) and its write-optimized version (WMSA). MSA extends the NRA solution [6] for the memory-bounded set-ting by writing a temporary output table to the disk when memory becomes full. WMSA is an optimized version of MSA that exploits the available memory more effectively and reduces unnecessary disk accesses. Our experimental results show that
WMSA is the most efficient method, if tuples are ordered by the measure to be aggregated and the distribution of top-k groups is skewed. However, RHA outperforms WMSA at small memory sizes or data of low skew, even though it does not take advantage of the ordering.

Next, for the case where the data are clustered or sorted based on some group-by attributes, we proposed CGA; a high-unordered method, like RHA. The top-k results of each sub-problem are progressively merged until the whole dataset is com-pletely scanned.

We proceed to summarize our experimental findings, providing a qualitative comparison of the proposed top-k groups algorithms, depending on the data characteristics. Our results show that RHA outperforms all other alternatives for unor-dered data. The algorithm is very robust and adaptive to the available memory.
If the input is ordered by the measure attribute, WMSA is the best algorithm for cases with skewed data and moderate memory size. However, even for ordered input, RHA has lower access cost than WMSA at small memory sizes or data of low skew, even though it does not take advantage of the ordering. An additional advantage of RHA over WMSA is that it computes the top-k groups as well as their exact scores, as opposed to WMSA which determines only lower bounds for them.
For the case of clustered or ordered data with respect to one or more group-by attributes, CGA utilizing RHA for each
CGA can also be used in combination with WMSA, if the input is ordered primarily by some group-by attributes and second-arily by the measure attribute.

In the future, we will investigate top-k groups retrieval subject to user-defined constraints. For instance, a user may be mizations can be developed to accelerate the retrieval process.

References
