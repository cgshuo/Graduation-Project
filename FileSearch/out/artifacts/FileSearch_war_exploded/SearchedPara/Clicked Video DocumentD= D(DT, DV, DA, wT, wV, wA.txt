 This paper presents a novel online video recommendation system called VideoReach , which alleviates users X  efforts on finding the most relevant videos according to current view-ings without a sufficient collection of user profiles as required in traditional recommenders. In this system, video recom-mendation is formulated as finding a list of relevant videos in terms of multimodal relevance (i.e. textual, visual, and au-ral relevance) and user click-through. Since different videos have different intra-weights of relevance within an individ-ual modality and inter-weights among different modalities, we adopt relevance feedback to automatically find optimal weights by user click-though, as well as an attention fusion function to fuse multimodal relevance. We use 20 clips as the representative test videos, which are searched by top 10 queries from more than 13 k online videos, and report supe-rior performance compared with an existing video site. Categories and Subject Descriptors: H.3.5 [Informa-tion Storage and Retrieval]: Online Information Services  X  Web-based services General Terms: Algorithms, Human Factors, Experimen-tation.
 Keywords: video recommendation, multimodal relevance.
Online video services have surged to an unprecedented level in recent years. Today X  X  online users always face a daunting volume of video content from video sharing and blog sites, or from IPTV and mobile TV. As a result, there is an increasing demand of an online video recommenda-tion system to find the most relevant videos according to users X  current viewings or preferences. While many existing video-oriented sites, such as YouTube [9], MSN Soapbox [5], Yahoo! [7], and Google Video [1], have already provided recommendation services, it is likely that most of them rec-ommend videos only based on surrounding text. However, it remains a challenging problem to leverage video content and user click-through for a more effective recommendation.
Most of previous work on traditional recommendation fo-cus on collaborate filtering based on a sufficient collection of user profiles, e.g. the famous movie recommender system  X   X  X oviefinder X  [4]. However, there are many cases that a user tends to visit a web page anonymously without provid-vided by users, and embedded text such as closed captions in video stream; (2) indirect text  X  referring to the categories obtained by automatic text categorization based on a set of predefined categories. We use the vector and probabilistic models to describe the direct and indirect text, respectively.
In vector model, a textual document is represented as a set of keywords and corresponding weights. We use tf instead of classic tf  X  idf to represent the weight for each keyword, and adopt cosine distance to measure the rele-vance of two documents. We further introduce probabilistic model to describe latent semantics. Support Vector Ma-chine based text categorization [8] is adopted to automat-ically classify a textual document into a set of predefined hierarchy that consists of more than 1 k categories. For two textual documents D x with a set of categories C x = ( C 1 , C 2 , . . . , C m x ) and the corresponding probabilities P x = ( P P y = ( P 1 , P 2 , . . . , P m y ), the relevance in probabilistic model is defined as and d ( C i ) denotes the depth of category C i in the category tree, ` ( C i , C j ) denotes the depth of the first common ances-tor of C i and C j ,  X  is a predefined parameter.
The visual information is described by color histogram, motion intensity and shot frequency (i.e. average number of shots per second), while the aural information is described by the mean and standard variation of aural tempo among all the shots. These features have proved to be effective to describe video content [2]. The relevance in terms of feature i between documents D x and D y is defined as 1 . 0  X  X  f i ( D x )  X  f ( D y ) | , where f i ( D x ) is i -th feature of D x .
In order to fuse the relevance from three modalities, we adopt three dimensional AFF [3], which simulates human at-tention nature. We first use linear combination to fuse the relevance from different features within each single modal-ity, and then fuse the relevance from three modality using AFF by considering both monotonicity and heterogeneity simultaneously. For more details, please refer to [3].
Since different videos have different characteristics, it is difficult to select a set of weights satisfying all kinds of videos. We adopt RF [6] to automatically adjust the intra-and inter-weights for each video. We get  X  X ositive X  and  X  X egative X  examples from user click-through. If a user opens a recommended video and closes it within a short time (e.g. less than five seconds), it is taken as a  X  X egative X  example; if a user views a recommended video for a relative long time, it is taken as a  X  X ositive X  example. Given  X  X ositive X  and  X  X egative X  examples, RF automatically adjusts the weights.
We used a collection of more than 13 k online videos from  X  X SN Soapbox X  [5]. Since it is not reasonable to evaluating our system over all these videos, we use 20 representative videos which are searched by 10 popular queries from our database. For each video, we recommended six different lists of videos, each containing 20 videos. The six lists are generated by the following schemes: 1. Soapbox. The recommendation results from  X  X SN
