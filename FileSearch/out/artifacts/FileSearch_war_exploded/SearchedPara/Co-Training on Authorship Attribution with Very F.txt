 Authorship attribution (AA) aims to identify the authors of a set of documents. Traditiona l studies in this area often assume that there are a large set of labeled documents available for training. However, in the real life, it is hard or expensive to collect a large set of labeled data. For example, in the online review domain, most reviewers (authors) only write a few reviews, which are not enough to serve as the training da ta for accurate classification. 
In this paper, we present a novel two-view co-training framework to iteratively identify the authors of a few unlabeled data to augment the training set. The key idea is to first represent each document as several distinct views, and then a co-training technique is adopted to exploit the large amount of unlabeled documents. Starting from 10 training texts per author, we systematically evaluate the effectiveness of co-training for authorship attribution with lim ited labeled data. Two methods and three views are investigated: logi stic regression (LR) and support vector machines (SVM) methods, and character, lexical, and syntactic views. The experiment al results show that LR is particularly effective for impr oving co-training in AA, and the lexical view performs the best among three views when combined with a LR classifier. Furthermore, the co-training framework does not make much difference between one classifier from two views and two classifiers from one view. Instead, it is the learning approach and the view that plays a critical role. H.3.1 [Information Storage and Retrieval]: Content Analysis and Indexing. Authorship attribution; co-train ing; very few labeled examples The problem of authorship attribution (AA) can be defined as follows: Let A = { a 1 , ..., a k } be a set of k authors and D = { D D } be k sets of documents with D i being the document set of author a i  X  A . Each (training or testing) document is represented as a feature vector. A model or cl assifier is then built from the training data and applied to the te st data to determine the author a of each test document d , where a  X  A . 
Existing approaches to authorship attribution are mainly based on supervised classification [16, 9, 13]. Although this is an effective approach, it has a major weakness, i.e., for each author a large number of his/her articles are needed as the training data. This is possible if the author has written a large number of articles, but will be difficult if he/she has not. For example, in the online review domain, most authors (revi ewers) only write a few reviews (documents). It was showed that on average each reviewer only has 2.72 reviews in amazon.com, and only 8% of the reviewers have at least 5 reviews [7]. To ma ke things worse, the labeling of training documents is even more difficult. Due to the anonymous posting behavior or multiple user-ids of an online user, it is hard or expensive to collect labeled do cuments for an author. The small number of labeled documents make s it extremely challenging for supervised learning to train an accurate classifier. 
In this paper, we consider the problem of authorship attribution with very few labeled examples. By exploiting the redundancy in the human languages, we tackle th e problem of very few training examples in a two view co-training framework. Specifically, we first represent the documents as several natural views, and then two classifiers are co-trained on every two of the views. The predictions of each classifier on unlabeled examples are used to augment the small training set. This process repeats until a termination condition is satisfied, and the enlarged labeled set is finally used to train a classifier to make predictions on the test data. The focus in this paper is on evaluation and comparison of the co-training framework on different views and learning approaches. Starting from 10 training texts per author, we use two classifiers, logistic regression (LR) and support vector machines (SVM), to assess the effectiveness of three views, character, lexical and syntactic views. 
So far limited work has been done in this area. The AA problem with limited training data was attempted in [15] and [12]. However, neither of them used a co-training framework. In a pioneer work [11], Kourtis and Stamatatos introduced the co-training technique into authorship identification. In their experimental setting, about 115 a nd 129 documents per author on average are used for two experimental corpora. This number of labeled documents is still very large. We consider a much more realistic problem, where the size of training samples is extremely small. Only 10 samples per author are used in training. Furthermore, instead of using two different views, co-training in [11] is conducted on the character 3-gram view using two different classifiers, common n-grams (CNG) and SVM. This contradicts with the standard co -training algorithm [2], which requires two sufficient and redunda nt views. This inspired our study, which aims to seek the answers to the following questions:  X  Can the co-training technique benefit more from two views  X  What is the more important factor to the co-training  X  What are the most promising le arning technique and view for 
The rest of this paper is organi zed as follows. Section 2 reviews the related work. Section 3 presents our co-training method for authorship attribution. Section 4 provides experimental results. Finally, Section 5 c oncludes the paper. Copyright  X  2014 ACM 978-1-4503-2257-7/14/07...$15.00. Existing methods for AA can be categorized into two main themes: finding appropr iate features and developing efficient and effective techniques. Various f eatures have been proposed for modeling writing styles, including function words, length and richness features, punctuation frequencies, character, word and POS n-grams, and rewrit e rules. On developing effective learning techniques, an early study used Bayesian statistical analysis, but later work focused exclusivel y on classification, including decision trees, logistic regression, and SVM, etc. 
The main problem in the traditional research is the unrealistic size of the training set. Basicall y, a size of about 10,000 words per author is regarded to be a reasonable size [1, 3]. Even when no long documents available, hundreds of short texts can be selected [6] such that the total amount of words is large enough for training. Two studies [15, 12] introduced the problem of AA with limited data. However, neither of them used the co-training technique, which was a representative learning mechanism to cope with limited training data [2]. More recently, Kourtis and Stamatatos employed the co-training paradigm for AA [11]. The large training set size there was far from the realities of modern authorship attribution. In additio n, their co-training method was implemented using two classifiers built on one character n-gram view. Our preliminary work also showed that co-training helped improve the performance of AA with few labeled data [4] under two views, i.e., lexical and synt actic views. Experiments were conducted on only 10 authors using SVM. p documents with the highest scores are used to augmen t the labeled set. Results showed that co-training outperformed supervised classification on either of the two views and the combined feature vectors, as well as the ensemble method based on subs pacing. However, when we experimented with more authors, the accuracy dropped quickly. Furthermore, co-training on different learning approaches and views remained unknown. Later, we found that LR performed significantly better than SVM, an d the data augmentation strategy was also crucial, which inspired this work. This work added a new view and also performed extensive experiments to study views vs. classification methods of SVM and LR. In the context of authorship attribution, each document can be represented as several views of features: views about character, lexical and syntactic features. A classifier can be learned from any of these views. To deal with the problem of very few training examples, we present a two-view co-training algorithm, which uses the logistic regression (LR) or SVM as the learner and co-trained on every two of the views. The overall framework is shown in Algorithm 1. 
Given the labeled, unlabeled, and test sets L , U , and T , step 1 extracts the character, lexical, and syntactic views from L , U , and T , respectively. Steps 2-9 iteratively co-train two classifiers by adding the data which are assigne d the same label by the two classifiers into the training set. The algorithm first randomly selects u unlabeled documents from U to create a pool U X  of examples. Note that we can directly select from the large unlabeled set U . However, it is shown in [2] that a smaller pool can force the classifier C 1 and C 2 to select instances that are more representative of the underlying distribution that generates U . Hence we set the parameter u to a size of about 1 percent of the whole unlabeled set, which allows us to observe the effects of different number of iter ations. It then iterates for the following steps. First, use two of the character, lexical and syntactic views on the current labeled set to train two classifiers C Second, allow each of these tw o classifiers to examine the unlabeled set U X  and choose p samples that they agree to label as positive. The selected examples are then added to the current labeled set L with the label assigned, and the u documents are removed from the unlabeled pool U X  . Steps 10-13 are used to assign the test document to a cate gory (author) using the classifier learned from the first and second view in the augmented labeled data, respectively. The character view is represented as the character n-gram features of a document. Character n-grams are simple and easily available for any natural language [11]. For a fair comparison with the previous work [11], we extract frequencies of 3-grams on the character-level. The vocabulary size for character 3-grams in our experiment is 28584. The lexical view consists of wo rd unigrams of a document. We represent each article by a vector of word frequencies. The vocabulary size for word unigrams in our experiment is 195274. We do neither word stemming nor stop word removal as in text categorization. This is because some of the stop words are actually function words which have been demonstrated discriminative for authorship iden tification. In addition, stemming can be also harmful to information extraction of an author. The syntactic view is composed of the syntactic features of a document. We use four typical c ontent-independent structures including n-grams of POS tags (n = 1..3) and rewrite rules [9]. The syntactic features are extracted from the parsed syntactic trees. Each POS n-gram or rewrite rule is encoded like a single pseudo-word and assigned a unique number id. The vocabulary sizes for POS 1-grams, POS 2-grams, POS 3-grams, and rewrite rules in our experiment are 63, 1917, 21950, and 19240, respectively. These four types of syntactic structures are merged into a single vector. Hence the syntactic view of a document is represented as a vector with 43140 components. 
Input : A small set of labeled documents L = { l 1 ,..., l
Parameters : the number of iterations k , the size of selected 
Output : t k  X  X  class assignment 1. Extract views L c , L l , L s , U c , U l , U s 2. Loop for k iterations: 3. Randomly select u unlabeled documents U' from U; 4. Learn the first view classifier C 1 from L 1 ( L 1 5 Use C 1 to label documents in U' based on U c , U 6 Learn the second view classifier C 2 from L 2 7 Use C 2 to label documents in U' based on U c , U 8 U p = { u | u  X  U' , u.label by C 1 = u.label by C 9 U = U -U ', L = L  X  U p ; 10. Learn the first view classifier C 1 from L ; 11. Use C 1 to label t k in T based on T c , T l , T 12. Learn the second view classifier C 2 from L ; 13. Use C 2 to label t k in T based on T c , T l , T
Figure 1: A two-view co-training algorithm for AA (TCA) All our experiments use the SVM multiclass classifier [8] with its default parameter settings and th e logistic regression classifier with L2 regularization [5]. We conduct experiments on the IMDB data set [14]. This data set contains the IMDb reviews in Ma y 2009. It has 62,000 reviews by 62 users (1,000 reviews per user). It is publicly available upon request from the authors of [14]. For each author/reviewer, we further split his/her documents into the labeled, unlabeled, and test sets. 1% of one author X  X  documen ts, i.e., 10 documents per author, are used as the labeled data for training, 79% are used as unlabeled data, and the rest 20% ar e used for testing. We extract and compute the character and lexical features directly from the raw data, and use the Stanford PCFG parser [10] to generate the grammar structures of sentences in each document for extracting syntactic features. We normalize each feature X  X  value to [0, 1] interval by dividing the maximum value of this feature in the training set. We use the micro-averaged classification accuracy as the evaluation metric. We give two baselines which co-t rain on one view but using two different classifiers. The first one is presented in [11], and the second one is our extension. CNG+SVM on Char3Gram: It co-trains two classifiers from the character 3-gram view using CNG and SVM classifiers [11]. 
CNG is a profile-based method. It first merges all the available training texts from each author into one file. And then a single representation is extracted for the author. The representation is based on the L most frequent character n -grams of the file. The test document is represented similarly. The classification model is based on the dissimilarity of the profile of the text from each of the profiles of the candidate authors. The distance function is defined as the one used in [11]. LR+SVM on Char3Gram: Since our preliminary results show that the performance of CNG+SVM is extremely poor, we are curious what the reason is. Can this be due to the classifier or to the one view? Hence we present this baseline. This method also uses a single character 3-gram view and the SVM algorithm as one of the two classifiers. For the other classifier, we use LR instead of CNG . We first evaluate the effectiveness of the classifiers and views in our two-view co-training algorithm (TCA). We experimented with 0, 10, 20, 30, 40, 50, and 60 itera tions. Note that an iteration number of 0 means that no co-training is done. Only the initial labeled set is used to learn a classifier and then classify the documents in the test set. 
We show the effects of SVM and LR on the lexical and character views in Figure 2. At the starting point, the performance of LR is very close to that of SVM, both about 0.33 and 0.45 for character and lexical views, respectively. However, after 10 iterations, the accuracy for LR-Lex is 70.45%, signifi cantly better than that of SVM-Lex, which is only 52.69%. When iterating 40 times, the accuracy of SVM-Lex re aches its highest value, i.e., 52.24%. In contrast, LR-Lex shows a clear upward trend. Its performance continuously grows from 0.81 to 0.91 when increasing the number of iterations from 20 to 60. 
From Figure 2, we also see that the lexical view performs much better than the character view in all cases. See the red curve with square mark and the blue curve with triangle mark. However, their trends are similar. The reason can be that the lexical view shares some common properties with the character view. Figure 2: TCA using SVM (left) and LR (right) on lexical and character views
The effects of SVM and LR on the lexical and syntactic views are shown in Figure 3. Similar to Figure 2, the LR classifier shows a dramatic performance improveme nt over SVM. And the lexical view is a much better representation of authorship than the syntactic view. Figure 3: TCA using SVM (left) and LR (right) on lexical and syntactic views 
The effects of SVM and LR on the syntactic and character views are shown in Figure 4. Once again, SVM is defeated by LR. Starting from the initial accuracy value of 0.33, SVM-Char ends increasing at k=20, with a small improved value of 0.40. On the contrary, LR-Char continues to grow with the increasing k . Its performance gets to 0.82 after 60 iterations. As for the views, the character view is better than the syntactic view for LR. However, the performance of SVM-Char drops quickly after 40 iterations. Figure 4: TCA using SVM (left) and LR (right) on syntactic and character views In summary, the LR classifier pe rforms significantly better than SVM in the co-training framework. As we can see in Figures 2-4, on the same two views, the accuracies in the right figures (LR) outperform those in the left figures (SVM) by a large margin. One possible reason is that LR with L2 regularization is more tolerant to the over-fitting problems caused by the small number of training samples compared to the large number of feature variables. It is also known that when the number of training examples is small, probabilistic classifiers such as LR have an edge while discriminative classifiers like SVM can easily find wrong separation hyperplanes. Furthermore, while the performance of SVM fluctuates with different numbers of iterations, the accuracy of LR goes up steadily. This again strongly indicates that the co-training framework favors the probabilistic classifier LR. Finally, the lexical view performs the best among three views when the LR approach is used as the learning technique. We now compare our proposed two view co-training framework with two baselines. The comparison results are given in Table 1. 
From the results, we can see that CNG is almost unable to correctly classify any test case. Its accuracy is only 1.26% at the start point. And this directly leads to the failure of the whole co-training framework. The reason is that the other classifier SVM can augment nearly 0 documents from the unlabeled set. The intersection of the documents predicted by SVM and CNG is close to the null set. We also tuned the parameter L for CNG , but it makes little difference. To disti nguish the effects of views from the classifiers, we conduct two ty pes of evaluations. First, we apply the CNG+SVM framework to the lexical view. The results are even worse. Its accuracy drops to 0.58%. The details are omitted due to space limitations. This suggests that CNG does not work well with very few labeled examples in our experiments. Secondly, we replace CNG with LR , which has been shown quite effective for co-training. Results show that co-training on the character 3-gram view can still im prove the performance, as long as two suitable learning approaches are used as the classifiers for co-training. For example, the accuracy for LR on the character view grows from 32.88% to 79.54%, an absolute accuracy increase of 46.66%. 
Table 1. Comparisons between co-training on one view two methods and co-training on two views one method
Initially, we expect to see th e proposed two-view co-training framework to achieve a signifi cant performance improvement over its corresponding one-view co-t raining method. However, as shown in Table 1, when using SVM to co-train on the syntactic and character views, its performance is actually worse than that of LR + SVM on the character view. Meanwhile, we observe a sharp performance increase, i.e., from 45.75% to 91.23%, in our TCA by using LR to co-train on the lexical and character views. All these findings strongly demonstrate that it is both the view and classifier that have big impacts on the co-training results. In this paper, we investigate the problem of authorship attribution with very few labeled examples. We present a novel two-view co-training framework which utilizes natural views of human languages, i.e., the character, le xical and syntactic views. We conducted a comparative study on the effectiveness of learning techniques and different views. Re sults show that the logistic regression classifier is much more effective for the co-training approach than the SVM classifier, and the lexical view performs the best among the three views. Anot her interesting finding is that co-training on one view or two views does not make too much difference. What is important is to choose an appropriate learning algorithm and an expressive view. 
Our current study focuses on co-training using two views. Our next step will extend the work by integrating more views such as stylistic or vocabulary richness vi ew into the system. Moreover, we evaluated the performance with only one data set in this paper. Further experiments will be conducted to determine the general behavior of the co-training approach for the AA problem. For example, while the LR technique stably outperforms SVM by a large margin, the superiority of the lexical view over the character and syntactic views may be due to the special characteristic of the data set we investigated.
 ACKNOWLEDGEMENTS Tieyun Qian was supported in part by the NSFC Projects (61272275, 61232002, 61202036, 61272110), and the 111 Project (B07037). Bing Liu was supported in part by a grant from National Sc ience Foundation (NSF) under no. IIS-1111092. [1] S. Argamon, C. Whitelaw, P. Chase, S. R. Hota, N. Garg, and S. [2] A. Blum, and T. Mitchell. Combining labeled and unlabeled data [3] J. Burrows. All the way through: Testing for authorship in [4] M. Fan, T. Qian, B. Liu, M. Zhong, and G. He. Authorship [5] R. Fan, K. Chang, C. Hsieh, K. Wang, and C. Lin. Liblinear: A [6] G. Hirst and O. Feiguina. Bigrams of syntactic labels for [7] N. Jindal and B. Liu. Opinion spam and analysis. WSDM . pp. 29 X  [8] T. Joachims. www:cs:cornell:edu/people/tj/svm_light/old/ svm [9] S. Kim, H. Kim, T. Weninger, J. Han, and H. D. Kim. Authorship [10] D. Klein and C. D. Manning. Accurate unlexicalized parsing. In: [11] I. Kourtis, and E. Stamatatos . Author identification using semi-[12] K. Luyckx, W. Daelemans. Au thorship attribution and verification [13] Y. Seroussi, F. Bohnert, and I. Zukerman. Authorship attribution [14] Y. Seroussi, I. Zukerman, a nd F. Bohnert. Collaborative inference [15] E. Stamatatos. Author identifi cation using imbalanced and limited [16] E. Stamatatos. A survey of modern authorship attribution methods. 
