 One of the goals of unsupervised learning is to discover the l atent structure expressed in observed data. The nature of the learning problem will vary depending on the form of the data and the kind of latent structure it expresses, but many unsupervised learn ing problems can be viewed as a form of matrix factorization  X  i.e. decomposing an observed data ma trix, X , into the product of two or more matrices of latent variables. If X is an N  X  D matrix, where N is the number of D -dimensional observations making up X . This can be done by assuming that X  X  ZY , where Z is a N  X  K matrix indicating which of (and perhaps the extent to which) K latent features are expressed in each of the N observations and Y is a K  X  D matrix indicating how those K latent features are manifest in the D dimensional observation space. Typically, K is less than D , meaning that Z and Y provide an efficient summary of the structure of X .
 A standard problem for unsupervised learning algorithms ba sed on matrix factorization is determin-ing the dimensionality of the latent matrices, K . Nonparametric Bayesian statistics offers a way to address this problem: instead of specifying K a priori and searching for a  X  X est X  factorization, non-parametric Bayesian matrix factorization approaches such as those in [1] and [2] estimate a posterior distribution over factorizations with unbounded dimensio nality (i.e. letting K  X  X  X  ). This remains computationally tractable because each model uses a prior t hat ensures that Z is sparse, based on the Indian Buffet Process (IBP) [1]. The search for the dimen sionality of the latent feature matrices thus becomes a problem of posterior inference over the numbe r of non-empty columns in Z . Previous work on nonparametric Bayesian matrix factorizat ion has used Gibbs sampling for poste-rior estimation [1, 2]. Indeed, Gibbs sampling is the standa rd inference algorithm used in nonpara-metric Bayesian methods, most of which are based on the Diric hlet process [3, 4]. However, recent work has suggested that sequential Monte Carlo methods such as particle filtering can provide an efficient alternative to Gibbs sampling in Dirichlet proces s mixture models [5, 6].
 In this paper we develop a novel particle filtering algorithm for posterior estimation in matrix fac-a conjugate prior, and the other without a conjugate prior bu t tractable in other ways. Our particle filtering algorithm is by nature an  X  X n-line X  procedure, whe re each row of X is processed only once, in sequence. This stands in comparison to Gibbs sampling, wh ich must revisit each row many times showing that our particle filtering algorithm can be signific antly more efficient than Gibbs sampling for each of the two models, and discuss its applicability to t he broad class of nonparametric matrix factorization models based on the IBP. Let X be an observed N  X  D matrix. Our goal is to find a representation of the structure e xpressed in this matrix in terms of the latent matrices Z ( N  X  K ) and Y ( K  X  D ). This can be formulated as a statistical problem if we view X as being produced by a probabilistic generative process, re -matrix factorization problem is that the distribution of X is conditionally dependent on Z and Y only through the product ZY . Although defining P ( X | Z , Y ) allows us to use methods such as maximum-likelihood estimation to find a point estimate, our goal is to instead compute a posterior distribution over possible values of Z and Y . To do so we need to specify a prior over the latent This constitutes Bayesian matrix factorization, but two pr oblems remain: the choice of K , and the computational cost of estimating the posterior distributi on.
 Unlike standard matrix factorization methods that require an a priori choice of K , nonparametric Bayesian approaches allow us to estimate a posterior distri bution over Z and Y where the size of these matrices is unbounded. The models we discuss in this pa per place a prior on Z that gives each  X  X eft-ordered X  binary matrix (see [1] for details) probabi lity where K k ,
N is the number of rows, H N = P N i =1 1 /i is the N th harmonic number, and K h is the number of columns in Z that when read top-to-bottom form a sequence of 1  X  X  and 0  X  X  corresponding to the binary representation of the number h . This prior on Z is a distribution on sparse binary matrices that favors those that have few columns with many ones, with t he rest of the columns being all zeros. This distribution can be derived as the outcome of a sequenti al generative process called the Indian buffet process (IBP) [1]. Imagine an Indian restaurant into which N customers arrive one by one and m Poisson (  X /i ) new dishes. If we record the choices of each customer on one ro w of a matrix whose columns correspond to a dishes on the buffet ( 1 if chosen, 0 if not) then (the left-ordered form of) that matrix constitutes a draw from the distribution in Eqn. 2. The order in which the customers enter the restaurant has no bearing on the distribution of Z (up to permutation of the columns), making this distribution exchangeable.
 In this work we assume that Z and Y are independent, with P ( Z , Y ) = P ( Z ) P ( Y ) . As shown in Fig. 1, since we use the IBP prior for P ( Z ) , Y is a matrix with an infinite number of rows and D columns. We can take any appropriate distribution for P ( Y ) , and the infinite number of rows will not pose a problem because only K Figure 1: Nonparametric Bayesian matrix factorization. Th e data matrix X is the product of Z and Y , which have an unbounded number of columns and rows respecti vely. matrices, through K for both continuous [1, 7] and binary [2] data matrices X .
 Since the posterior distribution defined in Eqn. 1 is general ly intractable, Gibbs sampling has pre-viously been employed to construct a sample-based represen tation of this distribution. However, generally speaking, Gibbs sampling is slow, requiring each entry in Z and Y to be repeatedly up-number of rows of X increases as a consequence of new observations being introd uced, where the Gibbs sampler would need to be restarted after the introduct ion of each new observation. Our approach addresses the problems faced by the Gibbs sampl er by exploiting the fact that the prior on Z is recursively decomposable. To explain this we need to intr oduce new notation, let X ( i ) be do so simply follow the IBP in choosing dishes for the i th customer given the record of which dishes were chosen by the first i  X  1 customers (see Algorithm 1). Applying Bayes X  rule, we can wr ite the from P ( Z (1: i ) , Y | X (1: i ) ) using importance sampling with a proposal distribution of and taking Following Eq. 4, we could then approximately generate a set o f weighted particles from This  X  X article filtering X  procedure defines a recursive impo rtance sampling scheme for the full pos-form this procedure can produce particles with extreme weig hts, so we resample the particles at each iteration of the recursion from the distribution given by th eir normalized weights and set w for all  X  , which is a standard method known as sequential importance r esampling [8]. factorization models based on the IBP. This procedure will w ork even when the prior defined on Algorithm 1 Sample P ( Z (1: i ) | Z (1: i  X  1) ,  X  ) using the Indian Buffet process Y is not conjugate to the likelihood (and is much simpler than o ther algorithms for using the IBP with non-conjugate priors, e.g. [9]). However, the procedu re can be simplified further in special models. In the first case, the prior over Y is conjugate to the likelihood which means that Y need not be represented. In the other case, although the prior is n ot conjugate and thus Y does need to be explicitly represented, we present a way to improve the effic iency of this general particle filtering in significant improvements in performance over Gibbs sampl ing in both models. In this model, explained in detail in [1], the entries of both X and Y are continuous. We report results on the modeling of image data of the same kind as was or iginally used to demonstrate the model in [1]. Here each row of X is an image, each row of Z indicates the  X  X atent features X  present associated with a latent feature.
 The likelihood for this image model is matrix Gaussian where  X  2 with each element having variance  X  2 sian, they form a conjugate pair and Y can be integrated out to yield the collapsed likelihood,
P ( X | Z ,  X  x ) = which is matrix Gaussian with covariance  X   X  1 = I  X  Z Z 4.1 Particle Filter In this case the particle filter recursion shown in Eqns. 3 and 4 reduces to and may be implemented as shown in Algorithm 2.
 Algorithm 2 Particle filter for Infinite Linear Gaussian Model Figure 2: Generation of X under the linear Gaussian model. The first four images (left t o right) correspond to the true latent features, i.e. rows of Y . The fifth shows how the images get combined, with two source images added together by multiplying by a sin gle row of Z , z sixth is Gaussian noise. The seventh image is the resulting r ow of X .
 Gaussian we can find the required conditional distribution b y following the standard rules for con-ditioning in Gaussians. Letting  X   X  1 we can partition this matrix into four parts where A is a matrix, c is a vector, and b is a scalar. Then the conditional distribution of X ( i ) is This requires inverting a matrix A which grows linearly with the size of the data; however, A is highly structured and this can be exploited to reduce the cos t of this inversion [10]. 4.2 Experiments We compared the particle filter in Algorithm 2 with Gibbs samp ling on an image dataset similar 6  X  6 latent images. A 100  X  4 binary ground-truth matrix Z was generated with by sampling from P ( z i,k = 1) = 0 . 5 . The observed matrix X was generated by adding Gaussian noise with  X  X = 0 . 5 to each entry of ZY .
 Fig. 3 compares results from the particle filter and Gibbs sam pler for this model. The performance of the models was measured by comparing a general error metric c omputed over the posterior distribu-tions estimated by each approach. The error metric (the vert ical axis in Figs. 3 and 5) was computed by taking the expectation of the matrix ZZ T over the posterior samples produced by each algorithm and taking the summed absolute difference (i.e. L E [ ZZ T ] computed over the samples and the upper triangular portion o f the true ZZ T (including the diagonal). See Fig. 4 for an illustration of the informat ion conveyed by ZZ T . This error metric measures the distance of the mean of the posterior to the grou nd-truth. It is zero if the mean of the distribution matches the ground truth. It grows as a functio n of the difference between the ground truth and the posterior mean, accounting both for any differ ence in the number of latent factors that are present in each observation and for any difference in the number of latent factors that are shared between all pairs of observations.
 The particle filter was run using many different numbers of pa rticles, P . For each value of P , the Figure 3: Performance results for particle filter vs. Gibbs s ampling posterior estimation for the and error bars indicate the standard deviation of the error. wall-clock computation time on 2 Ghz Athlon 64 processors ru nning Matlab for the corresponding number of particles P while the error bars indicate the standard deviation of the e rror. The Gibbs sampler was run for varying numbers of sweeps, with the initi al 10% of samples being discarded. The number of Gibbs sampler sweeps was varied and the results are displayed in the same way as significantly less time than the Gibbs sampler, with the diff erence being an order or magnitude or X on each iteration, reducing the cost of computing the likeli hood. In this model, first presented in the context of learning hidd en causal structure [2], the entries of both X and Y are binary. Each row of X represents the values of a single observed variable across D trials or cases, each row of Y gives the values of a latent variable (a  X  X idden cause X ) acro ss those variables influence which observed variables. Learning the hidden causal structure then corresponds to inferring Z and Y from X . The model fits our schema for nonparametric Bayesian matrix factor-ization model (and hence is amenable to the use of our particl e filter) since the likelihood function it uses depends only on the product ZY .
 The likelihood function for this model assumes that each ent ry of X is generated independently trial (expressed in Y ). The probability that x where z parameter  X  sets the probability that x how this probability changes as the number of relevant activ e hidden causes increases. To complete the model, we assume that the entries of Y are generated independently from a Bernoulli process with parameter p , to give P ( Y ) = Q 5.1 Particle Filter In this model the prior over Y is not conjugate to the likelihood, so we are forced to explic itly represent Y in our particle filter state, as outlined in Eqns. 3 and 4. Howe ver, we can define a more we call this model a  X  X emi-conjugate X  model.
 The basic particle filter defined in Section 3 requires drawin g the new rows of Y from the prior when we generate new columns of Z . This can be problematic since the chance of producing an assignment of values to Y that has high probability under the likelihood can be quite l ow, in effect wasting many particles. However, if we can analytically mar ginalize out the new rows of Y , we Algorithm 3 Particle filter for Infinite Binary Matrix Factorization representation of Z and ZZ T . The middle and right are particle filtering results; a singl e random particle Z and E [ ZZ T ] from a 500 and 10000 particle run middle and right respective ly. are introduced to match the new columns appearing in Z ( i ) , then we can write where Thus, we can use the particle filter to estimate P ( Z (1: i ) , Y (1: i  X  1) | X (1: i ) ) The procedure described in the previous paragraph is possib le in this model because, while our prior can derive an analytic solution to P ( X ( i ) | Z (1: i ) , Y (1: i  X  1) ) = Q 5.2 Experiments We compared the particle filter in Algorithm 3 with Gibbs samp ling on a dataset generated from the model described above, using the same Gibbs sampling algori thm and data generation procedure as developed in [2]. We took K until a matrix Z of correct dimensionality ( 6  X  4 ) was produced. This matrix is shown in Fig. 4 as a bipartite graph, where the observed variables are shaded. A 4  X  250 random matrix Y was generated with p = 0 . 1 . The observed matrix X was then sampled from Eqn. 8 with parameters  X  = . 9 and  X  = . 01 . Comparison of the particle filter and Gibbs sampling was don e using the procedure the posterior distribution in less time, as shown in Fig. 5. infinite binary matrix factorization model. Each point is an average over 10 runs with a particular to right, and error bars indicate the standard deviation of t he error. In this paper we have introduced particle filter posterior es timation for non-parametric Bayesian matrix factorization models based on the Indian buffet proc ess. This approach is applicable to any Bayesian matrix factorization model with a sparse recursiv ely decomposable prior. We have applied this approach with two different models, one with a conjugat e prior and one with a non-conjugate prior, finding significant computational savings over Gibbs sampling for each. However, more work needs to be done to explore the strengths and weakneses of the se algorithms. In particular, simple observations, although we are optimistic that methods for a ddressing this problem that have been developed for Dirichlet process mixture models (e.g., [5]) will also be applicable in this setting. By exploring the strengths and weaknesses of different meth ods for approximate inference in these models, we hope to come closer to our ultimate goal of making n onparametric Bayesian matrix factorization into a tool that can be applied on the scale of r eal world problems.

