 Issues related to Quality of Service (QoS) have been addressed in telecommunication, networking and multimedia delivery areas, and recently in web service area. Some of the common QoS attributes include response time, throughput, reliability, availability, reputation, cost, etc. In a web search engine, when a user submits a query, there might be millions of web pages relevant to the query. In most of popular search engines, the results are ranked based on their qualities, and the most commonly used and effective quality attributes are authority, popularity and connectivity, which in a way all reflect the reputation to a certain extent. Usually the quality attributes that are related to page delivery performance (e.g. response time, reliability) are not considered at all. How-ever, they are also very important factors to determine a user X  X  searching experiences. For instance, if the top ranked pages in the result list respond very slowly when users want to access them, or couldn X  X  be displayed correctly, or the hosting web site crashes when too many people try to access at the same time, it will impact users X  searching experiences negatively. If we take response time as an example, when two pages have similar content, page A has a faster response time than page B , definitely users would prefer A than B if the sponse time is normally not a concern at all. But if the hosting web site is very slow or improve their overall Internet experiences. When we say a page has a high quality, it and it has a good quality of delivery. 
In the paper, we define how we can measure and calculate the delivery QoS, and how we can rank web pages based on QoS. The basic idea is that if two pages are both relevant to a query with the same degree of relevancy, since usually the user prefers the page with a low response time, the page with a lower response time will be ranked higher than the other page. The same principle can be applied to other QoS attributes. 
Since the content, reputation and QoS are all important to decide the page ranking, it is necessary to find a way to combine them together. In this study, both content and reputation based ranking are obtained from the meta search engine, in which the nu-meric relevance score for each page is not available, and QoS based score is calcu-lated by our proposed QoS-based ranking algorithm. Usually there are two ways to do the combination, one is to combine the scores, and the other is to combine the ranks. We choose the rank aggregation as our way to combine them for several reasons: the original score; thirdly, the converted score would not be in line with the QoS val-ues because the ways to calculate them are different. 
In the experiment, we compare several popular rank aggregation algorithms to see which one has the best performance to combine QoS. In order to measure the per-formance, we consider both the improvement of QoS of the top ranked pages and the engine. We find that the median rank aggregation algorithm [7] is the best to improve forming algorithm. 
The rest of the paper is organized as follows. Section 2 reviews the related works measure them, and it also describes the QoS-based ranking algorithm. Then in section 4, we explain the rank aggregation process and the system architecture. In section 5, we explain our experiment design and analyze the results. Finally in section 6, we conclude the paper. As indicated in a number of research works [12], every search engine crawls different part of the web and thus has a different web page collection, and also indexes and ranks differently. As a consequence, meta search engines become popular because they are believed to be able to have a wider coverage of the web and provide a more accurate result than each of the individual search engines. When a user submits a query to a meta search engine, it will issue the same query to n search engines and get back n ranked lists, and then generate a single ranked list as the output, which is ex-pected to outperform any of the n lists. There are two types of meta search algorithms. the ranks from each search engine, and the other is the score combination algorithm [4] which requires the knowledge of relevance scores for each returned result. For the rank aggregation, there are works on unsupervised learning approach requiring no training data and supervised learning approach with the training data [9]. Since the score information is usually unavailable when building meta search engines, and it is unsupervised rank aggregation method in this study. In this section, we only review those algorithms that we have tested and compared in the experiment. 
Borda-fuse [1] [6] and Condorcet-fuse [10] are two commonly used rank aggrega-tion algorithms. Borda-fuse (or Borda Count) was proposed to solve the problem of voting. It works as follows. Suppose there are n search engines, given a query, each of the result set. For each search engine, the top ranked page is given c points, the sec-ond ranked page is given c-1 points, and so on. If one page is not ranked by a search engine, the remaining points are divided evenly among all the unranked pages. Then we could calculate the total points earned by each page, and rank all the c pages in the descending order. It is a very simple procedure, but it has been proven to be both efficient and effective. Weighted Borda-fuse was also proposed in [1], which assigns a weight to each search engine based on its average precision, and its performance is better than Borda-fuse. 
Condorcet-fuse is another major approach to solve the voting problem. The basic idea is that the Condorcet winner should win (or ties in) every possible pair-wise counter is set to zero, then for each search engine, if it ranks A higher than B , counter will be incremented, otherwise, it will be decremented, after the iteration over all search engines, if the counter is a positive value, A will be ranked better than B , oth-erwise B better than A . By going through all pages in the result set, a ranked order is obtained. In the weighted Condorcet-fuse, a weight is assigned to each search engine. 
Dwork et al. [6] explains that extended Condorcet criterion (ECC) has the excellent anti-spamming properties when used in the context of the meta search, and this target can be achieved by their proposed Kendall-optimal aggregation method with several heuristic algorithms to improve the efficiency. Since Kendall-optimal aggregation itself is a NP-complete problem, footrule-optimal aggregation is a good approxima-tion and is more efficient. Fagin et al. [7] uses a heuristic called median rank aggrega-tion for the footrule-optimal aggregation. The heuristic is to sort all objects based on the median of the ranks they receive from the voters. They also propose a very effi-cient algorithm MEDRANK to median rank aggregation, in which all the ranked lists are accessed one page at a time, the winner is the candidate that is seen more than half genetic algorithm, Beg and Ahmad [2] try to optimize the footrule distance when only time, and the progressive result presentation can compensate it. They also test the fuzzy rank ordering and several other soft computing techniques. 
QoS has been an active research topic in several areas. Our review focuses on the web service area because how they use QoS in web services is similar to how we can use QoS for web search. In [11], Ran described a list of QoS attributes and how they can be used in web service discovery. The QoS attributes are cat egorized into four types: runtime related, transaction support related, configuration management and cost related, and security related. It is quite a complete list, and we define the search-other types are not direct concerns of search engine users. Usually, QoS is a matter of personal preference or restrictions from the user device. Different users have different expectations [9]. So in this work, we only consider QoS attributes that users prefer. In a number of other works such as [13], several common QoS attributes they choose include reliability, availability, response time, cost, etc. 3.1 Delivery-Related QoS Attributes There are many QoS attributes that have been proposed in the communication area or web service area, but not all of them are applicable to the web search. Based on our observation on the average user behavior in the web searching process, we identify a performance. 
The delivery-related QoS attribute measures how well a page is delivered to the end user. Users normally want a page with a good delivery speed, a high reliability and availability, and from a web site that can handle a large volume of concurrent requests. Sometimes users need to balance between the speed and the rich multimedia experience. If a page has lots of embedded multimedia or flash objects, definitely the speed is slower compared with a page with only text information. In this study, we users have the same requirements, and the other is the personalized attribute, which are based on user X  X  personal preference or the capacity of the user device. 
We only choose several most important and representative delivery-related quality sponse time. The ability to handle a big volume of requests is not considered because it would be hard to measure it without the cooperation from the web site owner. For the personalized QoS attributes, we choose the file size and the number of embedded multimedia objects. The file size and multimedia objects can be reflected in the re-sponse time to a certain extent. The reasons we consider them as separate QoS attrib-utes are twofold: firstly, response time is determined not only by the file size, but also too, such as the cost of the Internet access. Also compared with the response time, they are more of a personal preference. 
We would not give an exhaustive list of all QoS attributes, but our system frame-work would be flexible and extensible enough so as to easily include new QoS attrib-utes. The granularity of these attributes can be adjusted if necessary. For example, if a user does not like any flash file in a web page, the multimedia objects can be divided into several types depending on the media types. The data type of these QoS attributes can also be adjusted. Currently, all of them are continuous numeric values, but they could also be a discrete or categorical value. For instance, instead of using a numeric value, the reliability can have three ranges: low, medium, and high. Below we explain the four QoS attributes we choose, and how we measure and calculate them. Reliability. It measures how reliable a web site is and whether it could deliver a page without error. It is defined as the percentage of successful delivery among all the same web page during different time of a day and different days of a week. Each time, Whether a delivery is successful is also dependent on the location, a request from one location might be successful, whereas the request for the same page from another location, even at the same time, might be unsuccessful. So ideally, we should also send requests from different locations in order to get a more accurate value. Currently, because of the restriction of the experiment, we only measure from one location. Response time. It measures how soon a user can see the web page after a request is submitted to the web server. It is defined as the time difference between the time when a user sends a request and the time when the user receives the response. Since the network speed is not a constant number and the web server is not in the same state all the time because of the change of the incoming traffic and the internal status of the server itself, the response time may vary for the same page when it is accessed at different times. In order to more accurately measure the response time, we submit the request to the same page several times, and get the average value. Same as the reliability, currently, we only measure it from one location. objects. It can be obtained from the HTTP header or by downloading the page. Media richness. It measures the number of embedded multimedia objects within a web page. 
Our selected QoS attributes are especially important for mobile users or users with slow Internet connections. For this type of users, the first two directly affect the satis-their Internet access. Although file size an d media richness are both based on personal preferences, for this particular type of users, we assume that they prefer a smaller file size and a smaller number of multimedia objects, which we believe is reasonable assumption. 3.2 QoS-Based Ranking Algorithm For any given page, we calculate all four QoS attributes based on the previous defini-tions, and normalize the values to (0, 1) range using a simple normalization method. The reliability value is in (0, 1) range already, and thus we don X  X  do the normalization for this attribute. For the response time, because the page with a lower response time should be ranked higher, the normalization formula is as below: Where QoS ij is the value of the j-th QoS attribute for page i , NQoS ij is the normalized value of QoS ij , Min and Max are operations to get the minimum and maximum values among all pages. In the current system, j is a value from 1 to 4 because we have only 4 QoS attributes, and it could be expanded later. 
For the other two attributes (file size and media richness), because they are prefer-ence related, the page with a lower value could be ranked higher or lower depending on user X  X  preference. If we take media richness as an example, when a user prefers a lower value on media richness, the normalization formula is the same as the previous one, when a user prefers a higher value, we would use a different formula as shown below: 
In the current experiment, we assume a lower value is preferred on these two at-tributes. After the normalization step, we do the linear combination of the four nor-malized values, and each page would have an overall QoS value. attribute. The value of this weight is set between 0 to 1, 0 means that a user does not care about this QoS attribute, 1 means the QoS attribute is important to the user, any value in between could indicate the relative importance of the QoS attribute, and currently we only consider two borderline cases  X  0 and 1. After we define the search-related QoS attributes and QoS-based ranking, we need to decide how we can integrate them into the final ranking score. The content of a page, whether it is relevant to a query, and whether it has a high quality based on its linkage Since most of search engines consider all of them, by building a meta search engine based on those existing search engines, we can deploy them directly, and then QoS-based ranking can be combined with the ranking from the meta search engine using the rank aggregation methods. 
The first step is to build the meta search engine. We choose the three most popular search engines  X  Google, Yahoo and MSN Search as the underlying search engines. Since only the rank information is available from each of these search engines, we decide to use the rank aggregation meth od to combine the three ranked lists. We choose Condorcet-fuse as the rank aggregation method for the meta search engine in order to get a single ranked list. 
Usually users won X  X  go beyond the first one or two pages when browsing the search results. Therefore, we only download the top-ranked n pages, and calculate their normalized QoS values. Then we rank these n pages based on each QoS attribute and hence get four rankings. Whether a QoS attribute is included in the final ranking is determined by the user preference. Finally we apply the rank aggregation on the original ranking from the meta search engine and the user preferred QoS rankings to get the final ranked list. Figure 1 shows the system architecture. First, a query is submitted to three underlying search engines: Google, Yahoo and MSN. The top ranked pages from each search engine are combined and duplications are removed. Secondly, for each page in this combined list, we need to calculate the values of its QoS attributes. We submit the request on each page a few times and get the reliability and the average response time. Then we download the actual page in order to calculate the file size and the number of embedded objects. Using equation (3), we can calculate the QoS value for each page and get the QoS-based ranking. Finally different rank aggregation algorith ms are tested to combine the QoS-based ranking with the ranking from the meta search engine. 5.1 Experiment Design We collected 36 queries from a group of users (most of them are graduate students), some example queries are Disney , schizophrenia research , image processing , Buenos Aires travel , sea monster video , Object Oriented programming language , Taiwan variety of subjects such as computer science, biology, tourism, etc. We specially chose some topics that are country related because the web sites hosted in different countries may have a different response time and reliability. We also did the same experiment on another query set [2] [6] to test whether we could get the same conclu-sion for different queries. 
Since users normally look at the first one or two pages from the search engine, and by default, there are 10 results in one page, we downloaded the top 20 results from the meta search engine and calculated their QoS values, and then re-ranked them trying to promote the pages with a good delivery quality to top 10 positions. Our assumption here is that by only re-ranking the top 20 pages, the result relevancy won X  X  be affected much. In order to calculate the QoS value more accurately, we downloaded each page ten times and then we used the average value for each QoS attribute. In the experiment, we tested four rank aggregation methods  X  Condorcet-fuse [10], Borda-fuse [1], MEDRANK [7], and GA-based [2] rank aggregation. We also com-pared them with the baseline  X  QoS-based re-ranking algorithm, in which the top 20 results are simply re-ranked based on their overall QoS values. The weight of the QoS combinations of four weights in the experiment, but we only showed the case when all of the weights are 1 in the figures below. 5.2 Result Analysis Since the main purpose of our proposed algorithms is to improve QoS of the search results, we use the top 10 QoS improvement to measure the effectiveness of the algo-rithm. It is defined as the improvement on the average QoS value (one attribute or overall) of the top 10 results, 
In the mean time, we don X  X  want to sacrifice the accuracy of the results. Usually, the accuracy is measured by the precision value, due to the lack of the user evaluated relevancy data, we assume the precision from the meta search engine is our baseline, and then take it as the reference, to see how many original top 10 can be retained by the new algorithm, the higher the value, the higher the precision of the top 10 results. 
We believe that improving QoS and keeping precision are equally important to our system. In our experiment, we use the following formula to measure the overall effec-tiveness of the tested algorithm. In the following figures, CF refers to Condorcet-fuse, MR refers to MEDRANK, BF refers to Borda-fuse, GA refers to GA-based rank aggregation, and RR refers to the QoS based re-ranking. We only look at the results that are averaged on all queries. 194 X. Chen and C. Ding 
Figure 2 shows  X  QoS on each QoS attribute for the average query when the weight of that particular QoS is set to 1 and all other weights are set to 0. RR is the ranking algorithm used here. 
From this figure, we can see that the improvement on reliability is very little. The room for improvement is little. The improvements on other QoS attributes are very good, on average, both response time and file size have over 60% increases on QoS values, and media richness could achieve almost 80% increase. 
Since the improvement on reliability is very low, almost negligible compared with other QoS attributes, when we do the compar ison on different rank aggregation algo-rithms in the following analyses, we only consider three attributes  X  response time, file size and media richness. Table 1 shows the improvement on the values of these three QoS attributes from the four aggreg ation algorithms and QoS-based re-ranking algorithm. We could see that QoS-based re-ranking algorithm is consistently the best for all three attributes, and among the four aggregation algorithms, MEDRANK is the best performing one. It is quite understand able because in QoS-based re-ranking, only QoS values are used to re-rank the results without considering the original ranks from the meta search engine. with the QoS-based re-ranking algorithm on their improvements on 3 QoS attributes 
Figure 3 shows three overall measurements as we defined above  X   X  QoS,  X  Top10 and the overall effectiveness (E). We could see that overall speaking, MEDRANK is the best performing method, it achieves the highest improvement on E, is the second best on  X  QoS and  X  Top10 and only slightly worse than the best one on these two measurements. Borda-fuse is the second best one on E and the best on  X  Top10. GA-based algorithm is the worst on all three measurements. Regarding  X  QoS, Condorcet-fuse is worse than QoS-based re-ranking, but is better in overall effectiveness. QoS-based re-ranking is the best on individual QoS improvement and also on overall QoS improvement, but it changes the top 10 results too much and thus its performance on  X  Top10 and E is not so good. 
Figure 4 shows the result on another query set [2] [6] and the distribution of the this figure. Among the four aggregation algorithms, MEDRANK performs consis-tently the best when combining the QoS-based ranking, Borda-fuse is the second best one, the next is Condorcet-fuse, and the worst performing one is GA-based algorithm. When we compare their efficiencies, the first three are more or less the same, whereas the GA-based algorithm is the slowest one. with the QoS-based re-ranking algorithm 5.3 Discussions Our current way of collecting QoS data may not work in a real system, because it will affect the response time to get the retrieved results due to the extra page downloading and QoS calculation time. There are two possible solutions. The first one is to imple-ment QoS based ranking in a search engine instead of a meta search engine. It wouldn X  X  affect the query response time because the QoS data can be collected when the document is crawled and downloaded. The second solution is to apply QoS based ranking only for queries which have been issued before. Whenever a query is submit-for the future use, and it could also be updated regularly. The query response time wouldn X  X  be affected because QoS processing is done offline. 
The current implementation of measuring reliability and response time is not very accurate. We send the request on the same page during different time period. How-ever, we didn X  X  consider the effect of different locations. If the request is from differ-with the QoS-based re-ranking algorithm on a new query set 196 X. Chen and C. Ding possible solution is to send requests via public proxy servers in different locations and then get the average response time or reliability, which makes result more accurate. Another solution is to implement the QoS ranking in personalized search, so that reliability or response time is only related to one particular user. In this paper, we define four delivery-related QoS attributes and how we could meas-ure them and calculate the QoS-based ranking scores. By including them in the ranking procedure to re-rank the top positioned pages, users could have chance to see pages with a high delivery quality earlier than pages with a low quality, and as a conse-quence, it could improve users X  searching experiences. The experiment shows the promising results. Generally speaking, the top ranked pages have a better delivery performance than before, and in the mean time also keep a high precision on relevancy. And by comparing four rank aggregation algorithms, MEDRANK achieves the best performance regarding the improvement on QoS values and the overall effectiveness. 
There are several directions of works we would like to work in the future such as the ones we listed in the previous section. We would also like to conduct some ex-periments to collect the user relevancy data and thus have a more accurate measure-ment on precision. Another direction we would like to work on is that we could test our algorithm on a wireless device such as a cell phone or PDA, and on Internet con-nections with different speed, to see whether it could improve the searching experi-can choose their own preferences on QoS attributes, and a user study on their satisfac-tion level can be done afterwards. Acknowledgments. This work is sponsored by Natural Science and Engineering Research Council of Canada (grant 299021-07). 2. Beg, M.M.S., Ahmad, N.: Soft Computing Techniques for Rank Aggregation on the 3. Chen, X.: QoS-based Ranking for Web Search. Master Thesis, Ryerson University, To-4. Croft, W.B.: Combining Approaches to Information Retrieval. In: Croft, W.B. (ed.) Ad-6. Dwork, C., Kumar, R., Naor, M., Sivakumar, D.: Rank Aggregation Methods for the Web. 10. Montague, M., Aslam, J.A.: Condorcet Fusion for Improved Retrieval. In: Proceedings of 11. Ran, S.P.: A Model for Web Services Discovery with QoS. ACM SIGecom Ex-12. Selberg, E.W., Etzioni, O.: On the Instability of Web Search Engines. In: Proceedings of 
