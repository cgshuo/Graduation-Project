 F or man y feature selection problems, ah uman de nes the features that are p oten tially useful, and then a subset is c ho-sen from the original pool of features using an automated feature selection algorithm. In con trast to sup ervised learn-ing, class information is not a v ailable to guide the feature searc h for unsup ervised learning tasks. In this pap er, w e in tro duce Visual-FSSEM (Visual F eature Subset Selection using Exp ectation-Maximization Clustering), whic h incor-p orates visualization tec hniques, clustering, and user in ter-action to guide the feature subset searc h and to enable a deep er understanding of the data. Visual-FSSEM, serv es b oth as an exploratory and m ultiv ariate-data visualization to ol. W e illustrate Visual-FSSEM on a high-resolution com-puted tomograph y lung image data set. Most researc h in unsup ervised clustering assumes that when creating the target data set, the data analyst in conjunction with the domain exp ert w as able to iden tify a small relev an t set of features. If this is not the case, then c ho osing a sub-set of these features will lead to b etter p erformance of the data mining algorithm when redundan t and/or irrelev an t features are remo v ed. Moreo v er, reducing the dimensional-it y of the data is desirable for reducing computation time and increasing the comprehensibilit y of the results of the clustering algorithm.
 F or unsup ervised learning, the goal is to nd the feature subset that best disco v ers \natural" groupings from data (also kno wn as clustering). T o select an optimal feature subset, w e need a measure to assess cluster qualit y . The choice of p erformance criterion is b est made b y considering the goals of the domain. In studies of p erformance criteria a common conclusion is: \Di eren t classi cations [clusterings] are righ tfor di eren t purp oses, so w e cannot sa yan y one classi cation is b est." { Hartigan, 1985 [9].
 F eature subset selection coupled with unsup ervised learning is a di X cult task, b ecause class lab els are not a v ailable to guide the feature searc h. The problem is made more di X -cult when w e do not kno wthen um b er of clusters, k . Figure 1 illustrates this problem. In tw o dimensions (sho wn on the left) there are three clusters, whereas in one-dimension (sho wn on the righ t) there are only t w o clusters. The dif-cult yis in kno wing whic h is b etter. Ultimately the only w a y to decide is to use a criterion tied to the nal use of the clustering. Figure 1: The n um b er of cluster comp onen ts v aries with dimension.
 In [5], w e studied the problem of totally automated feature subset selection for unsup ervised clustering. W ein tro duced a new algorithm, FSSEM, whic h wraps feature subset se-lection around the exp ectation maximization [3] of a nite Gaussian mixture mo del (whic hw e refer to as EM cluster-ing). In this pap er, w e in tro duce Visual-FSSEM (Visual F eature Subset Selection using EM Clustering), whic h in-corp orates visualization tec hniques, clustering and user in-teraction to guide the feature subset searc h. Previous in-v estigation on automated feature selection for unsup ervised learning rev eals that no single feature selection criterion is b est for ev ery application. Visual-FSSEM enables the user to ev aluate clusters and features based on visual p erception and ev aluation measures suc h as scatter separabilit y , max-im um lik elihood, cluster en trop y and probabilit y of error. Our implemen tation allo ws the user to p erform b oth forw ard and bac kw ard searc hes and p ermits bac ktrac king. The abil-ity to visualize clusters in di eren t feature subsets engenders a deep er understanding of the data. Visual-FSSEM, th us, serv es b oth as an exploratory and m ultiv ariate-data visual-ization to ol.
 In Section 2, w ein tro duce Visual-FSSEM, an in teractiv efea-ture selection visualization en vironmen t. Sp eci cally ,w ede-
Permission to make digital or hard copies of part or all of this work or permission and/or a fee.

KDD 2000, Boston, MA USA  X  ACM 2000 1 -58113 -233 -6/00/0 8 ...$5.00 scrib e the c hosen visualization metho d, w e review FSSEM, and w e describ e three w a ys in whic h the user can guide the feature subset selection searc h. In Section 3, w e illustrate the p oten tial b ene ts of Visual-FSSEM on an HR CT-lungs (high resolution computed tomograph y image of the lungs) data set. Finally in Section 4, w e conclude our pap er and pro vide directions for future w ork. Visual-FSSEM (Visual F eature Subset Selection using EM clustering) is an in teractiv e visualization en vironmen t for feature selection for unsup ervised data. As suc h it serv es as a to ol for disco v ering clusters on di eren t feature sub-sets. Our completely automated v ersion, FSSEM, p erforms a greedy sequen tial forw ard or bac kw ard searc h for the b est feature subset as measured b ythec hosen p erformance cri-terion. In Visual-FSSEM, w e giv e the user the abilit y to guide this pro cess. In the remainder of this section w e rst describ e our visualization metho d. W e then review FSSEM and t w o of the four a v ailable clustering criteria. W e con-clude with a description of three w a ys in whic h the user can tak e part in the searc h for the b est subset through in terac-tion with Visual-FSSEM. When the data has one dimension, w edispla y its estimated probabilit y distribution. When the data has t w o dimen-sions, it can b e displa y ed using scatterplots [10]. When the data has more than three dimensions, w e need to apply vi-sualization tec hniques. There are sev eral m ultiv ariate-data visualization tec hniques. These metho ds can b e categorized in to geometric, icon-based, hierarc hical, and pixel-orien ted tec hniques. Keim and Kriegel [11] pro vide a short descrip-tion on eac h of these metho ds.
 Curren tly in Visual-FSSEM, w e apply a geometric visualiza-tion tec hnique. T o displa y the data and clusters, w eproject the data to 2-D and displa y the data on a scatterplot. W e c ho ose linear discriminan t analysis (LD A) [8] to pro ject the data, b ecause it nds the linear pro jection that maximizes cluster scatter. W e pro ject the original data X on to Y us-ing a linear transformation: Y = A T X , where X is a d n matrix represen ting the original data with n samples and d features. Eac h column of X represen ts a single instance of dimension d . Y is an m n matrix represen ting the pro jected data. Eac h column of Y is a pro jected instance on to m di-mensions. Ais a d m matrix whose columns corresp ond to the largest m eigen v ectors of S 1 w S b (see Section 2.3). When S 1 w S b turns out to b e singular, w e pro ject the data using its principal comp onen ts (i.e., transformation matrix A corresp onds to the m eigen v ectors of the data co v ariance) [8]. Principal comp onen ts analysis (PCA) pro jects the data in the direction of the largest v ariance. W e prefer LD A b ecause it sho ws the pro jection that pro vides the greatest cluster separation. F or example, consider the data sho wn in Figure 2, when pro jecting this t w o dimensional data to one dimension, LD Ac ho oses the pro jection p erp endicular to the largest cluster separation (the y -axis in this case). PCA, on the other hand, selects the pro jection represen ting the largest v ariance of the data (the x -axis in this case). Figure 2: LD A c ho oses the y-axis, whereas PCA c ho oses the x-axis. FSSEM wraps feature subset selection around the cluster-ing algorithm. The basic idea is to searc h through feature subset space, ev aluating eac h subset, F t ,b y rst clustering in space F t using EM clustering and then ev aluating the re-sulting clusters and feature subset using the c hosen feature selection criterion. An exhaustiv e searc hofthe2 d p ossible feature subsets ( d is the n um ber of a v ailable features) for the subset that maximizes our selection criterion is compu-tationally in tractable. Therefore, either sequen tial forw ard or bac kw ard elimination searc h is applied [8].
 FSSEM assumes that the data comes from a nite mixture mo del of m ultiv ariate Gaussians. W e apply the EM algo-rithm to estimate the maxim um lik elihood mixture mo del parameters and the cluster probabilities of eac h data p oin t [3]. The EM algorithm can b ecome trapp ed at a lo cal max-im um, hence the initialization v alues are imp ortan t. W e used the sub-sampling initialization algorithm prop osed b y F a yy ad et al. [7] with 10% sub-sampling and J = 10 sub-sampling iterations. After initializing the parameters, EM clustering iterates un til con v ergence (i.e., the lik elihood does not c hange b y 0 : 0001) or up to n (default 50) iterations whic hev er comes rst. W e limit the n um ber of iterations b ecause EM con v erges asymptotically , i.e., con v ergence is v ery slo w near a maxim um. EM estimation is constrained a w a y from singular solutions in parameter space b y limiting the diagonal elemen ts of the comp onen tco v ariance matri-ces j to b e greater than  X  =0 : 000001 2 , where 2 is the a v erage of the v ariances of the unclustered data. When w e are not giv en the n um ber of clusters, w e apply FSSEM-k , whic h p erforms a feature subset selection searc h wrapp ed around EM-k (EM clustering with order iden ti -cation). F or a giv en feature subset, w esearc h for k and the clusters. EM-k curren tly applies Bouman et al.'s metho d [1], whic h adds a minim um description length p enalt yterm to the log-lik eliho o d criterion. A p enalt y term is needed b e-cause the maxim um lik eliho o d estimate increases as more clusters are used. Without the p enalt y ,thelik eliho o d is at a maxim um when eac h data p oin t is considered as an indi-vidual cluster. In our curren t implemen tation w e giv e the user the abilit yto c ho ose from four w ell-kno wn p erformance measures: scatter separabilit y , maxim um lik eliho o d, cluster en trop y and prob-abilit y of error. Whic h criterion is b est dep ends on the goals of the data mining task. Due to space limitations, w ede-scrib e t w o criteria here and the others can b e found in [4]. Among the man y possible separabilit y criteria, w e c ho ose the trace ( S 1 w S b ) criterion b ecause it is in v arian t under an y nonsingular linear transformation [8]. S w is the within-class scatter matrix and S b is the b et w een class scatter matrix, and they are de ned as follo ws: where j is the probabilit y that an instance b elongs to clus-ter ! j , X is a d -dimensional random feature v ector repre-sen ting the data, k the n um b er of clusters, j is the sam-ple mean v ector of cluster ! j , M o is the total sample mean across all data p oin ts or instances in the data set, j is the sample co v ariance matrix of cluster ! j , and E fg is the exp ected v alue op erator. S w measures ho w scattered the samples are from their cluster means and the a v erage co v ari-ance of eac h cluster. S b measures ho w scattered the cluster means are from the total mean. W ew ould lik e the distance bet w een eac h pair of samples in a particular cluster to b e as small as p ossible and the cluster means to b e as far apart as p ossible with resp ect to the c hosen similarit y metric. S 1 is S b normalized b y the a v erage cluster co v ariance. Hence, the larger the v alue of trace ( S 1 w S b ) is, the larger the nor-malized distance b et w een clusters is whic h results in b etter cluster discrimination. Separabilit y is a general criterion that can b e used for an y clustering algorithm, but is biased to w ards clusters with cluster means that are far apart, and biased against clusters with equal means ev en though these clusters ha v e di eren tco v ariance matrices. Maxim um lik eliho o d (ML) measures ho wlik ely our data are giv en the parameters and the mo del. Th us, it tells ho w w ell our mo del ts the data. Therefore, in addition to the criterion for clustering, w e can emplo yML to ndthefea-ture subset that mo dels the data b est using EM clustering. The maxim um log-lik eliho o d of our data, X , is log ML = where f j ( X i j j ) is the probabilit y densit y function for class j , j is the mixing prop ortion of class j (prior probabilit y of class j ), N is the n um b er of data p oin ts, k is the n um ber of clusters, X i is a d -dimensional random data v ector, the set of parameters for class j , =( ; ) is the set of all parameters and f ( X i j ) is the probabilit y densit y function of our observ ed data p oin t X i giv en the parameters . W e c ho ose the subset that maximizes this criterion. Curren tly ,w eha v e implemen ted three mo des of in teraction with FSSEM. The rst metho d allo ws the user to view the clustering results of a sequen tial forw ard or bac kw ard searc h at eac h step and select the b est subset. F or example, giv en a data set describ ed b y ten features, if the user selects an SFS (sequen tial forw ard searc h) and the trace criterion, then Visual-FSSEM presen ts the user with ten (or less as sp eci-ed b y the user) scatterplots; one for eac h step in the SFS. The user can then select whic h of these b est clusters the data. This rst metho d is motiv ated b y the fact that man y of the criteria commonly used in clustering are biased ei-ther to w ard lo w er or higher dimensionalit y [5]. Therefore automated criterion-based feature selection ma y not lead to the b est clustering. F or example, the separabilit y criterion increases as the n um b er of features (or dimension) increases when the clustering assignmen ts remain the same. The sep-arabilit y measure is biased this w a y b ecause trace ( S 1 basically adding d (the n um b er of dimension) terms. F uku-naga [8] pro v ed that a criterion of the form X T d monotonically increases with dimension, d , assuming the same clustering assignmen t. [4] relates trace ( S 1 w S b pro of. Allo wing the user to select the b est subset ameliorates this problem. In Section 3 w esho wanexample,inwhic h the user selects a b etter cluster than FSSEM run with the trace criterion.
 The second metho d allo ws the user to in termingle di eren t criteria during the searc h pro cess. A teac h step of SFS or SBE the user is presen ted with the b est feature to add/delete as c hosen b yeac h criterion. Our previous in v estigation [5] rev eals that no single criterion measure w orks best for all applications. This option presen ts the user the b est feature c hosen b yeac h criterion in an add/delete step, allo wing the user to select the cluster one prefers to explore at eac h step. Finally , the third metho d allo ws the user to p erform a b eam searc h of the feature subset space. A t eac h step of the SFS/SBE searc h, the user is presen ted with the b best fea-tures to add/delete as rank ed b ythe c hosen clustering cri-terion. The user can then select the b est of these b features. FSSEM then adds/deletes this feature to/from the set of selected features. The user is giv en the capabilit ytobac k-trac k in the searc h space. The utilit y of this approac his that the user ma yha v e a preference for some features based on his/her domain kno wledge and kno wing ho weac h ranks with resp ect to the other candidate features for addition/deletion can help fo cus the searc h. The user sp eci es the v alue for b . In all cases, the user can select an arbitrary initial feature subset and limit the p o ol features to searc h from. The user selects the direction forw ard or bac kw ard in ev ery step and the n um b er of steps. When p erforming a forw ard sequen tial searc hw e giv e the user an alternativ e visualization option. T o select the n th feature for addition, w e pro ject the n 1 previously selected features to one dimension, and use the new feature as the other dimension. This enables the user to view the e ect of the feature to b e added. W e illustrate Visual-FSSEM on the HR CT-lung data set [6]. HR CT-lung consists of 500 instances. Eac h of these instances are represen ted b y 110 lo w-lev el con tin uous fea-tures measuring geometric, gra ylev el and texture features. The data is classi ed in to v e disease classes (Cen trilobular Emph ysema, P araseptal Emph ysema, EG, IPF, and P anaci-nar). F eature selection is imp ortan t for this data set, be-cause EM clustering using all the features results in just one cluster. F urthermore, the data can also b e group ed in to dif-feren t p ossible groupings (for example, disease class, sev erit y or b y the p erceptual categories that are used to train radi-ologists).
 The rst example of Visual-FSSEM illustrates a situation in whic h the user has selected SFS and is examining the b est feature c hosen b y FSSEM-k in conjunction with eac hof the feature set. features are selected. criteria (in teraction metho d 2). In Figure 3 w e displa y the results for only t w o of the four criteria due to space limita-tions. A tthispoin t in the searc hw e are determining whic h feature to add to the set 10, 15 and 87 (a texture homogene-it yandt w o global gra ylev el histogram features). This set w as c hosen b y the three pro ceeding SFS steps guided b y the scatter separabilit y criterion. In the gure, the clustering results parameterized b y the means and co v ariance matri-ces are displa y ed as  X  's and ellipses resp ectiv ely . FSSEM-k selects a di eren t b est feature for scatter and ML. In Fig-ure 3a, the x -axis for b oth scatterplots represen ts a linear discriminan t transformation of the previously selected three features to one-dimension. The y -axis represen ts the new feature b eing considered. The feature c hosen b y trace adds the greatest separabilit y . Note that feature y adds separa-tion to the curren t subset, if clusters can b e separated in the y dimension (or clusters are spread in the y -axis). The fea-ture c hosen b y the ML criterion do es not aid in nding new clusters. The y -v alue is appro ximately constan t for all data p oin ts. Note that the di eren t feature subsets resulted in di eren tn um b ers of clusters (eigh t for the scatter and four for the ML criterion). Figure 3b sho ws the corresp onding displa ys using LD A pro jecting on to t w o-dimensions. Note that the plot on the ML c hosen set is transformed on the rst t w o principal comp onen ts b ecause the within cluster v ariance w as close to singular. Figure b con rms that the trace plot results in a higher separabilit y than the ML plot, since the trace plot has a lo w er cluster (ellipse) o v erlap than ML. The trace clustering results in a classi cation error of 24 : 2% (if data w as hard clustered and judged based on the v e disease classes). Note that class lab els are not used dur-ing feature searc h or during clustering. W e use the lab els here only to aid in our ev aluation of the results. The second example of visual-FSSEM illustrates the use of domain kno wledge in the in teractiv e searc h. W e divided the original pool of features in to the follo wing ten feature groups: gra y lev el means and standard deviation features (1, 2, 19-21, 46-51), gra y histogram features (3-18, 30-45, 52-67), area histogram (22-29), texture energy (68-72), tex-ture en trop y (73-77), texture homogeneit y (78-87), texture con trast (88-92), texture correlation (92-97), texture clus-ter (98-102), and edginess features (103-110). W ethen se-lected the single b est feature from eac h class of features b y running FSSEM-k ten times (once on eac h feature group) with the trace criterion. Limiting our feature subset searc h to these ten features, w e apply Visual-FSSEM (using the rst metho d of in teraction) to explore the structure of this \lump" of data. Figure 4 sho ws the rst, second, fth, sixth and ten th SFS steps. Note that an automated FSSEM-k using trace w ould select feature subset F [45], since it has the largest trace . Ho w ev er, to a user the other cluster-ings migh t lo ok more in teresting. Ev en though the cri-terion v alue in feature subset F [45] is the largest, it ob-tained the lo w est class error of 37 : 2% whic h is equiv alen tto that obtained b y a ma jorit y rule (there are 314 instances b elonging to the largest class). The set with six features F [45 ; 93 ; 88 ; 102 ; 110 ; 84] resulted in a class error of 26 : 8%. Figures 5a and b illustrate an in teractiv e b eam searc h for whic h b =4. W e start with the empt y set and apply for-w ard selection. Figure 5a sho ws the four best individual features as rank ed b y the trace criterion. In this case w e c ho ose to explore starting from feature 1, b ecause the data is nicely distributed among the clusters in feature 1 (lo w er left). Moreo v er, our domain kno wledge supp orts this c hoice. F eature 1 is the mean gra ylev el of the lung. The other three features corresp ond to gra ylev el histogram bins. The mean v alue t ypically captures more information than a single his-togram bin. Figure 5b sho ws the next step in the searc h when the highest ranking pairs with resp ect to the trace cri-terion are displa y ed. All four rev eal in teresting structures. The class error obtained b y these four clusterings are 26 : 6%, 26 : 8%, 26 : 2% and 32% according to rank. This metho d of features, and (b) scatterplots on the b est four feature pair. in teraction p ermits bac ktrac king and can con tin ue as long as the user wishes to explore other cluster structures. W e ha v e illustrated that feature selection and clustering tec hniques can b e utilized to aid in the exploration and vi-sualization of high-dimensional unsup ervised data. A t the same time, visualization aids the data miner in selecting fea-tures. The abilit y to visualize di eren t cluster structures on di eren t feature subsets pro vides insigh tin to understanding the data. Visual-FSSEM allo ws the user to select an yfea-ture subset as a starting poin t, select the p o ol of features to c ho ose from, searc h forw ard or bac kw ard, and visualize the results of the EM clustering. F our clustering criteria are a v ailable to guide the searc h: separabilit y , maxim um lik e-lihood, cluster en trop y and probabilit y of error. A more general approac hw ould incorp orate di eren t clustering al-gorithms and allo w the user to select the clustering metho d to apply . W ec hose to displa y the data and clusterings as 2-D scatterplots pro jected to the 2-D space using linear dis-criminan t analysis. Other visualization tec hniques ma ybe applied. An in teresting tec hnique is the man ual pro jection metho d b y Co ok and Buja [2] whic hallo ws the user to man-ually con trol the pro jection of data on to di eren t views. It w ould also b e in teresting to allo w the user to cluster data within clusters (hierarc hical clustering) so as to zo om in on dense groupings. The authors wish to thank the ML-lunc h group at Pur-due for helpful commen ts. This researc h is supp orted b y NSF Gran t No. IRI9711535, and NIH Gran t No. 1 R01 LM06543-01A1. [1] C. A. Bouman, M. Shapiro, G. W. Co ok, C. B.
 [2] D. Co ok and A. Buja. Man ual con trols for [3] A. P . Dempster, N. M. Laird, and D. B. Rubin.
 [4] J. G. Dy .In Pr eliminary R ep ort: F e atur e Sele ction for [5] J. G. Dy and C. E. Bro dley .F eature subset selection [6] J. G. Dy , C. E. Bro dley ,A.Kak,C.R.Sh yu Sh yu, [7] U. F a yy ad, C. Reina, and P . S. Bradley . Initialization [8] K. F ukunaga. Statistic al Pattern R e c o gnition (se c ond [9] J. A. Hartigan. Statistical theory in clustering. [10] R. A. Johnson and D. W. Wic hern. Applie d [11] D. A. Keim and H.-P . Kriegel. Visualization
