 ordering instances under binary label information. The problem of ranking binary classification data is known in the machine learning literature as the bipartite ranking problem ([FISS03], [AGH + 05], [CLV08]). A natural approach is to find a real-valued scoring function which mimics the order induced by the regression function. A classical performance measure for scoring functions is the Receiver Operating Characteristic (ROC) curve which plots the rate of true positive against false positive ([vT68], [Ega75]). The ROC curve offers a graphical display which permits to judge rapidly how a scoring rule discriminates the two populations (positive against negative). A scoring rule whose ROC curve is close to the diagonal line does not discriminate at all, while the one lying above performance maximization) strategies for bipartite ranking have been based mostly on a popular summary of the ROC curve known as the Area Under a ROC Curve (AUC -see [CLV08], [FISS03], [AGH + 05]) which corresponds to the L 1 -metric on the space of ROC curves. In the present paper, we propose a statistical methodology to estimate the optimal ROC curve in a stronger sense than the AUC sense, namely in the sense of the supremum norm. In the same time, we will explain how to build a nearly optimal scoring function. Our approach is based on a simple observation: optimal scoring functions can be represented from the collection of level sets of the regression function. Hence, the bipartite ranking problem may be viewed as a  X  X ontinuum X  of classification problems with asymmetric costs where the targets are the level sets. In a nonparametric setup, regression or density level sets can be estimated with plug-in methods ([Cav97], [RV06], [AA07], [WN07], ...). Here, we take a different approach based on a weighted empirical risk minimization principle. We provide rates of convergence with which an optimal point of the ROC curve can be recovered according to this principle. We also develop a practical ranking method based on a discretization of the original problem. From the resulting classifiers and their related empirical errors, we show how to build a linear-by-part estimate of the optimal ROC curve and a quasi-optimal piecewise constant scoring function. Rate bounds in terms of the supremum norm on ROC curves for these procedures are also established.
 The rest of the paper is organized as follows: in Section 2, we present the problem and give some minimization, and in Section 4, we develop the main results of the paper which describe the statisti-cal performance of a scoring rule based on overlaying classifiers as well as the rate of convergence of the empirical estimate of the optimal ROC curve. Setup. We study the ranking problem for classification data with binary labels. The data are assumed to be generated as i.i.d. copies of a random pair ( X,Y )  X  X  X { X  1 , +1 } where X is a random 1 | X = x } , x  X  X . We will also denote by p = P { Y = 1 } the proportion of positive labels. In the sequel, we assume that the distribution  X  is absolutely continuous with respect to Lebesgue measure.
 Optimal scoring rules. We consider the approach where the ordering can be derived by the means event  X  Y = +1  X  should be observed. The following definition sets the goal of learning methods in the setup of bipartite ranking.
 Definition 1 (Optimal scoring functions) The class of optimal scoring functions is given by the set Interestingly, it is possible to make the connection between an arbitrary (bounded) optimal scoring function s  X   X  X   X  and the distribution P (through the regression function  X  ) completely explicit. Proposition 1 (Optimal scoring functions representation, [CV08]) A bounded scoring function s is optimal if and only if there exist a nonnegative integrable function w and a continuous random variable V in (0 , 1) such that: A crucial consequence of the last proposition is that solving the bipartite ranking problem amounts Hence, the bipartite ranking problem can be seen as a collection of overlaid classification problems. This view was first introduced in [CV07] and the present paper is devoted to the description of a statistical method implementing this idea.
 ROC curves. We now recall the concept of ROC curve and explain why it is a natural choice of performance measure for the ranking problem with classification data. We consider here only true ROC curves which correspond to the situation where the underlying distribution is known. First, we need to introduce some notations. For a given scoring rule s , the conditional cdfs of the random variable s ( X ) are denoted by G s and H s . We also set, for all z  X  R : previous functions by G  X  , H  X  ,  X  G  X  ,  X  H  X  respectively.
 We introduce the notation Q ( Z, X  ) to denote the quantile of order 1  X   X  for the distribution of a random variable Z conditioned on the event Y =  X  1 . In particular, the following quantile will be of interest: inf { t  X  R | F ( t )  X  z } . We now turn to the definition of the ROC curve.
 Definition 2 (True ROC curve) The ROC curve of a scoring function s is the parametric curve: for thresholds z  X  R . It can also be defined as the plot of the function: points of H s or G s ) are connected by line segments, so that the ROC curve is always continuous. For s =  X  , we take the notation ROC  X  (  X  ) = ROC(  X , X  ) .
 the ROC curve is the plot of the true positive rate against the false positive rate. ing functions. Some scoring function might provide a better ranking on some part of the observation space and a worst one on some other. A natural step to take is to consider local properties of the ROC curve in order to focus on best instances but this is not straightforward as explained in [CV07]. We expect optimal scoring functions to be those for which the ROC curve dominates all the others for all  X   X  (0 , 1) . The next proposition highlights the fact that the ROC curve is relevant when evaluating performance in the bipartite ranking problem.
 Proposition 2 The class S  X  of optimal scoring functions provides the best possible ranking with respect to the ROC curve. Indeed, for any scoring function s , we have: and  X  s  X   X  X   X  ,  X   X   X  (0 , 1) , ROC(s  X  , X  ) = ROC  X  (  X  ) .
 The following result will be needed later.
 Proposition 3 We assume that the optimal ROC curve is differentiable. Then, we have, for any  X  such that Q  X  (  X  ) &lt; 1 : For proofs of the previous propositions and more details on true ROC curves, we refer to [CV08]. We consider here the problem of recovering a single point of the optimal ROC curve from a sample positive. For any measurable set C  X  X  , we set the following notations: We also define the weighted classification error: with  X   X  (0 , 1) being the asymmetry factor.
 for all C  X  X  : Also the optimal error is given by: The excess risk for an arbitrary set C can be written: where  X  stands for the symmetric difference between sets. The empirical counterpart of the weighted classification error can be defined as: This leads to consider the weighted empirical risk minimizer over a class C of candidate sets: best set in the class in terms of the two types of error  X  and  X  .
 Theorem 1 Let  X   X  (0 , 1) . Assume that C is of finite VC dimension V and contains C  X   X  . Suppose independent of  X  such that, with probability at least 1  X   X  : factor term of p (1  X  p )  X  in the denominator instead .
 slower rates. Main result. We now propose to collect the classifiers studied in the previous section in order to build a scoring function for the bipartite ranking problem. From Proposition 1, we can focus on optimal scoring rules of the form: where the integral is taken w.r.t. any positive measure  X  with same support as the distribution of  X  ( X ) .
  X  C which may be seen as an empirical version of a discrete version of the target s  X  . In order to consider the performance of such an estimator, we need to compare the ROC curve of  X  s to of the ROC curve as a function of the errors of the overlaying classifiers becomes complicated. The main result of the paper is the next theorem which is proved for a modified sequence which yields to a different estimator. We introduce: {  X  C  X  i } 1  X  i  X  K defined by: The corresponding scoring function is then given by: 0  X  i  X  K + 1 .
 The next result offers a rate bound in the ROC space, equipped with a sup-norm. Up to our knowl-Theorem 2 Under the same assumptions as in Theorem 1 and with the previous notations, we set 1  X   X  , we have: performance of classifiers in ROC space. Using combinations of such classifiers to improve perfor-mance in terms of ROC curves has also been pointed out in [BDH06] and [BCT07].
 R P Remark 3 (A DAPTIVITY OF THE PARTITION .) A natural extension of the approach would be to regularity of the ROC curve. For now, it is not clear how to extend the method of the paper to take into account adaptive partitions, however we have investigated such partitions corresponding to different approximation schemes of the optimal ROC curve elsewhere ([CV08]), but the rates of convergence obtained in the present paper are faster.
 Optimal ROC curve approximation and estimation. We now provide some insights on the pre-vious result. The key for the proof of Theorem 2 is the idea of a piecewise linear approximation of the optimal ROC curve.
  X  i =  X  ( C (concave) approximation/interpolation of the optimal ROC curve ROC  X  . In the spirit of the finite element method (FEM, see [dB01] for instance), we introduce the  X  X at functions X  defined by: of ROC  X  may then be written as: In order to obtain an empirical estimator of ] ROC true level set C  X   X  the following estimator of ] ROC The next result takes the form of a deviation bound for the estimation of the optimal ROC curve. It quantifies the order of magnitude of a confidence band in supremum norm around an empirical estimate based on the previous approximation scheme with empirical counterparts. Theorem 3 Under the same assumptions as in Theorem 1 and with the previous notations, set K = K We have provided a strategy based on overlaid classifiers to build a nearly-optimal scoring function. Statistical guarantees are provided in terms of rates of convergence for a functional criterion which is the ROC space equipped with a supremum norm. This is the first theoretical result of this nature. and we hope that the present contribution gives a flavor of possible research directions. risk in terms of weighted classification error. First we re-parameterize the weighted classification error. Set C (  X  ) = { x  X  X  |  X  ( x ) &gt; Q  X  (  X  ) } and: Since ROC  X  is assumed to be differentiable and using Proposition 3, it is easy to check that the of `  X  (  X  ) around  X   X  at the second order that there exists  X  0  X  [0 , 1] such that: Using also the fact that ROC  X  dominates any other curve of the ROC space, we have:  X  C  X  X We have obtained the desired inequality. It remains to get the rate of convergence for the weighted empirical risk.
 Tsybakov X  X  margin condition [Tsy04], for all  X   X  [0 , 1] , of the form: we have that, under the modified margin condition, there exists a constant c such that, with proba-bility 1  X   X  : decomposition, for any  X   X  [0 , 1] : ROC  X  (  X  )  X  ROC(  X s K , X  ) = ROC  X  (  X  )  X  ROC curve, then we can bound the first term (which is positive),  X   X   X  [0 , 1] , by: Now, to control the second term, we upper bound the following quantity: first term, the next lemma will be needed: Lemma 1 We have, for all k  X  X  1 ,...,K } : where the notation O P (1) is used for a r.v. which is bounded in probability.
 inequalities hold with the  X   X  X . It remains to control the quantity  X   X  i +1  X   X   X  i . We have: We have that: of the optimal ROC curve is bounded, the second term is of the order K  X  1 . Eventually, we choose As only the first and the third term matter, this leads to the choice of K = K n  X  n 1 / 8 . Proof of Lemma 1.
 Eventually, errors are stacked and we obtain the result.
 Proof of Theorem 3.
 We use the following decomposition, for any fixed  X   X  (0 , 1) : \ ROC  X  (  X  )  X  ROC  X  (  X  ) = \ ROC  X  (  X  )  X  Therefore, we have by a triangular inequality:  X   X   X  [0 , 1] , \ ROC  X  (  X  )  X  And, by the finite increments theorem, we have: For the other term, we use the same result on approximation as in the proof of Theorem 2: in Theorem 1, except that  X  is replaced by  X /K in the bound. Eventually, we get the generalization bound: K  X  2 + (log K/n ) 1 / 3 , which is optimal for a number of knots: K  X  n 1 / 6 . [AA07] J.-Y. Audibert and A.Tsybakov. Fast learning rates for plug-in classifiers. Annals of [AGH + 05] S. Agarwal, T. Graepel, R. Herbrich, S. Har-Peled, and D. Roth. Generalization bounds [BBL05] S. Boucheron, O. Bousquet, and G. Lugosi. Theory of Classification: A Survey of Some [BCT07] M. Barreno, A.A. Cardenas, and J.D. Tygar. Optimal ROC curve for a combination of [BDH06] F.R. Bach, D.Heckerman, and Eric Horvitz. Considering cost asymmetry in learning [Cav97] L. Cavalier. Nonparametric estimation of regression level sets. Statistics , 29:131 X 160, [CLV08] S. Cl  X  emenc  X on, G. Lugosi, and N. Vayatis. Ranking and empirical risk minimization of [CV07] S. Cl  X  emenc  X on and N. Vayatis. Ranking the best instances. Journal of Machine Learning [CV08] S. Cl  X  emenc  X on and N. Vayatis. Tree-structured ranking rules and approximation of the [dB01] C. de Boor. A practical guide to splines . Springer, 2001. [Ega75] J.P. Egan. Signal Detection Theory and ROC Analysis . Academic Press, 1975. [FISS03] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. An efficient boosting algorithm for [RV06] P. Rigollet and R. Vert. Fast rates for plug-in estimators of density level sets. Technical [Tsy04] A. Tsybakov. Optimal aggregation of classifiers in statistical learning. Annals of Statis-[vT68] H.L. van Trees. Detection, Estimation, and Modulation Theory, Part I . Wiley, 1968. [WN07] R. Willett and R. Nowak. Minimax optimal level set estimation. IEEE Transactions on
