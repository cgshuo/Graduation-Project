 In this paper, we address the problem of grading spon-taneous speech using a combination of machine learning and crowdsourcing. Traditional machine learning techniques solve the stated problem inadequately as automatic speaker-independent speech transcription is inaccurate. The fea-tures derived from it are also inaccurate and so is the ma-chine learning model developed for speech evaluation. We propose a framework that combines machine learning with crowdsourcing. This entails identifying human intelligence tasks in the feature derivation step and using crowdsourcing to get them completed. We post the task of speech tran-scription to a large community of online workers (crowd). We also get spoken English grades from the crowd. We achieve 95% transcription accuracy by combining transcrip-tions from multiple crowd workers. Speech and prosody fea-tures are derived by force aligning the speech samples on these highly accurate transcriptions. Additionally, we derive surface and semantic level features directly from the tran-scription. We demonstrate the efficacy of our approach by predicting expert graded speech sample of 566 adult non-native speakers across two different countries -India and Philippines. Using the regression modeling technique, we are able achieve a Pearson correlation of 0 . 79 on the Philip-pines set and 0 . 74 on the Indian set with expert grades, an accuracy much higher than any previously reported machine learning approach. Our approach has an accuracy that rivals that of expert agreement. We show the value of the system through a case study in a real-world industrial deployment. This work is timely given the huge requirement of spoken English training and assessment.
 I.2.7 [ Artificial Intelligence ]: Natural Language Process-ing X  Speech Recognition and synthesis c  X  Spontaneous speech grading; Spoken English evaluation; Crowd-sourcing; Speech recognition; Machine Learning
The grading of constructed (open) response items, popu-larly known as subjective evaluation, provides a more holis-tic and accurate assessment of a candidate X  X  skills as com-pared to selected response items (multiple-choice questions) [1]. The primary limitation of a selected response item is that it asks the candidate to choose the right answer, pro-viding implicit hints and the structure of the solution [2]. However they are traditionally preferred over constructed response as their evaluation can be automated fairly easily. With the recent interest in MOOCs (Massively Online Open Courseware) [3], scalable education/training and automated recruitment assessment [4, 5], the interest in reducing man-ual effort in the assessment of constructed responses has increased manifolds.

Machine learning has been used to automatically grade these responses. There are many examples of successfully using machine learning for constructed response grading [2, 6]. A framework and general principles for using machine learning to grade constructed responses are presented in [2].
However machine learning techniques fall short of provid-ing accurate assessment for a number of problems [3, 7]. Sec-ondly, these automated approaches have come under criti-cism since the test-takers can fake high-scoring responses [8]. For instance, automated assessment of spontaneous speech for spoken language evaluation remains largely an unsolved problem [6]. On the other hand, automatic essay grading algorithms can be tricked by inserting the right words in random order or writing long essays [8].

We have identified that one of the key limitations of the current automated techniques is their inability to automat-ically derive the right set of features with high precision for assessing the response. Interestingly, many of these features can be easily derived by (or with help of) humans , even if they are non-experts. For instance, word sense disambigua-tion [9] or gibberish identification for essay grading, text transcription from noisy handwriting [10] and speech tran-scription [11] for speech grading are easily doable tasks by humans through crowdsourcing [12].

With this insight, we propose an extended machine learn-ing framework with a crowdsourcing layer for constructed response assessment. In this approach, one identifies human intelligence tasks in the feature derivation step and posts them to be completed by a non-expert crowd. 1 The re-sponses from the human intelligence tasks are then used to create relevant features for machine learning.
 We illustrate our technique for evaluation of spontaneous English speech 2 .

Evaluation of spoken English is in high demand for recruit-ment in the industry and admissions to the universities [17]. Universities teaching in English expect a level of English proficiency in incoming students whereas various job roles require different levels of spoken English capabilities. With the massive increase in the number of applicants applying for both university and companies, there is a great to scale this process by automating it. Having an automated/semi-automated way of evaluation for the purpose of giving feed-back to candidates cannot be overstated.

The problem acquires far reaching significance given the evidence that better English leads to better employment out-comes, wages and promotions [18, 19].

Figure 1 shows the application of technique to spoken En-glish grading. In the task of spontaneous speech evaluation, the text of the speech is not known a priori. We post the task of transcription of the sample speech to the crowd who transcribe it fairly accurately [11]. We force-align [18, 20] the speech of the candidate on this text to derive various fea-tures which go into a machine learning engine. We also col-lect non-expert grades of the speech from the crowd, which can be optionally used as additional features. With these accurately identified features with the help of the crowd, machine learning (specifically the modeling step) becomes a powerful technique for constructed response grading.
In summary, our approach is to identify human-intelligence tasks in the feature derivation step and let the crowd com-plete these in an accurate and reliable way. This enables machine learning to grade spoken English accurately. Our approach is an interesting addition to the extant machine learning only approaches and peer/crowd grading approaches. The approach could help in other grading problems such as that of essay X  X  by outsourcing word sense disambigua-tion [9], detecting insensible sentences or grammar checks to the crowd. It could also be used to grade handwritten assignments of students [10], where the crowd digitizes the responses. It needs to be seen how one creatively gleans hu-man intelligence tasks for other grading tasks and what is the incremental predictive value added by our technique.
In this paper, we show how we can solve a hitherto un-solved problem [6] of English spontaneous speech evaluation using our approach.

The paper makes the following contributions:
Our approach is different from peer grading [13] or crowd grading [14, 15, 16] approaches. These approaches directly ask the crowd to grade the response. The primary feature of our technique is using the crowd in the feature extraction step of machine learning. We provide a detailed comparison in Section 2.
Spontaneous speech refers to text X  X ndependent speech sam-ples.
The paper is organized as follows X  Section 2 describes pre-vious work, compares our technique with current incumbent techniques; Section 3 describes the procedure and aim of the speech assessment task; Section 4 describes the features classes used in the prediction algorithm; Section 5 describes the crowdsourcing framework which is used as input to ma-chine learning methods; Section 6 shows how this framework was used with machine learning techniques to predict a spo-ken English composite score and a case study of a real world implementations and results; Section 7 discusses the future work and concludes the paper.
Machine learning and human intelligence have been used in tandem since a long time. The classic application is to use the crowd to label samples and then use supervised learn-ing to predict the labels. Within grading of constructed responses, recently [21] learnt a grade using ML and used its confidence score to determine the number of expert eval-uations required for the task. We believe that our work is the first proposal and implementation of using crowdsourc-ing specifically for the feature derivation step of ML in the context of grading. To our best knowledge, this has not been attempted for other problems either. Some loosely related works include [22, 23, 24].

Our technique, when compared to a ML only approach, has a promise of higher accuracy, but makes some trade-offs. Firstly, there is a cost for every assessment done and the scalability depends on the number of non-expert workers available. These drawbacks are strictly speaking, valid, but they didn X  X  lead to practical challenges in our case. We paid 24 cents for each completely graded sample, a very affordable cost (per assessment) which is significantly lower than that of any expert grading efforts. Recently we have had crowd rate hundreds of samples in a day without any challenge or the need of altering the payment significantly.

Secondly, machine learning based assessments generally provide evaluations in real-time, whereas there is a time lag when using crowdsourcing. This works fine in many sce-narios, but doesn X  X  cater well to provide real-time feedback. Real time crowdsourcing has been an active area of research [25, 26] and a focus area for our group as well.

In comparison to peer grading methods [13] or crowd-sourcing assessments [14, 15, 16], our technique works dif-ferently. In these techniques, the crowd/peers directly eval-uates/grades the response on a rubric and a combination of their grades mimics the grades given by experts. We, on the other hand, use the crowd to help complete a human intel-ligence task, which helps in better feature derivation. One may additionally combine crowd grades too, if needed. This offers a new trade-off point among techniques with interest-ing possibilities. Crowd grades do not work for evaluating expert tasks, say a computer program or an advanced elec-tronics question and such evaluations need peers with some exposure to the subject. Our technique has the possibil-ity to make these amenable with cheaper and more scalable crowd, non-expert raters. It may also help improve the ac-curacy of peer/crowd grading techniques or reduce workers, given that we use a complementary set of information from the crowd. Futuristically, our technique could be more de-fensible for mid/high stake testing, given it uses the crowd for human intelligence tasks only and not direct evaluation.
In summary, our approach lies somewhere in between the machine learning only approaches and that of drawing grades from the consensus of crowd/experts.
We want to assess the quality of spoken English of can-didates based on their spontaneous speech samples. The speech samples of the candidates were collected using As-piring Minds X  automated speech assessment tool X  SVAR [5]. SVAR is conducted over phone as well as on a computer. The test has multiple sections where the candidate is re-quired to: read sentences aloud, listen and repeat sentences, listen to a passage or conversation and answer multiple choice questions and finally spontaneously speak on a given topic. In the spontaneous speech section, the candidates 3 are pro-vided with a topic and given 30 seconds to think, take notes and then speak on the topic for 45 seconds. The topic is re-peated to ensure task clarity. The complete test takes 16-20 minutes to complete, depending on the test version.
Currently, SVAR evaluates speech samples from the read and repeat sections with high accuracy [5]. Our goal in this paper is to evaluate the spontaneous speech of the candidate and provide a composite score based on it.
The subjects of our study use English as their second lan-guage and hail from various backgrounds, dialects and edu-cational qualifications.

A 5 point rubric for the composite score, similar to CEFR [27], was prepared with the help of experts. This score is a function of the pronunciation, fluency, content organiza-tion and grammar quality of the speech sample. Broadly speaking, Pronunciation [28] refers to the correctness in the utterance of the phonemes of a word by the students as per neutral accent. Fluency [29] refers to a desired rate of speech along with the absence of hesitations, false starts and stops etc. Content organization [30] measures the can-didate X  X  ability to structure the information disposition and present it coherently. Grammar [31] measures how well the syntax of the language was followed by the candidate.
In the next section we discuss the features which are used in the prediction algorithm. We use three classes of features X  Crowd Grades (CG), Force Alignment features (FA) and Natural Language Pro-cessing features (NLP). The spoken English samples are posted to the crowd to get the transcription and spoken English grades (Figure 1). Each task was completed by three workers. The crowd grades become one set of fea-tures. A second set, i.e., FA features, are derived by aligning [32] the speech sample on the crowdsourced transcriptions. A third set, i.e., NLP features, are also derived from the crowdsourced text. These are explained in the succeeding paragraphs.
Advanced Expectation-Maximization techniques [33] may also be used for an aggregation strategy, once the number of tasks done by every individual worker increases. In our current experiments, this number wasn X  X  very high. Figure 2: Our intuition of how different features predict the holistic score.
All the features described above were obtained for the spontaneous speech sample. We also derived features simi-lar to FA features for the candidate X  X  read and repeat speech samples collected during his/her SVAR test. The speech and prosody features are calculated by force aligning the speech on the known text. One of the models (RS/LR) in our ex-periments is based on these features and has been included for comparison. These features do not have any bearing on our final model for spontaneous speech evaluation.
The spoken English sample was given to the crowd to transcribe and provide grades. The task was posted on a popular crowdsourcing platform X  Amazon Mechanical Turk (AMT) [40]. AMT is a popular crowdsourcing marketplace. It is inspired by the famous 18 th century automated chess playing machine, running on the intelligence of a hidden
We were looking at prompt independent features only, at this point. human operator. It has more than 500 , 000 online workers from 190 countries [41]. One can post tasks on the platform online and offer fixed remuneration for their completion.
A clean and simple interface was provided to the worker with standard features needed for transcription. Addition-ally, an advanced audio player was embedded with the abil-ity to play the speech sample in repeat mode, rewind and forward, apart from standard play/pause functionality to help the worker. The different transcriptions were combined using the ROVER algorithm [42]. ROVER is a sophisti-cated voting algorithm to combine multiple transcriptions with errors, to obtain the best estimate of the correct tran-scription. It is reported to lead to an error reduction of 20-25%. ROVER proceeds in two stages: first the outputs are aligned and a single word transcription network (WTN) is built. The second stage consists of selecting the best scor-ing word (with the highest number of votes) at each node.
Several methods have been used in the past for increasing the reliability of the grades given by the crowd by identify-ing and correcting any biases and removing non-serious/low quality workers [43]. One of the key techniques for this in-volves inserting gold standard tasks with known answers to get an estimate of the worker X  X  ability [44]. The gold stan-dard tasks are similar to real tasks and the workers have no way to distinguish between the two. Our tasks took workers a reasonable amount of time (8-10 minutes). It wasn X  X  hence feasible to insert a gold standard task, as done typically, with every task to be completed.

To overcome this problem, we propose an innovative ap-proach where a risk is assigned to a worker based on his/her performance on the gold standard tasks. We conceptualized this system as a state machine that determines the risk level of a worker and proposes actions based on it (Refer to Fig-ure 3). All workers started with an initial risk level of 0 . 2. Gold standard tasks were probabilistically inserted among real tasks based on the worker X  X  risk level. Workers with a higher risk level saw more gold standard tasks. Also, the risk level of the worker was updated based on his/her perfor-mance on the gold standard tasks. Workers who consistently performed poorly on gold standard tasks were allocated a higher risk level and a notification was sent to them with a corrective course of action. Beyond a certain level, the worker was barred from attempting future work. We did not do any retrospective correction of the barred worker X  X  completed tasks and simply stopped him/her from attempt-ing newer tasks. This approach allowed us to control for the quality of workers, provide feedback, remove unsuitable workers and also adaptively control the balance between real and gold standard tasks. 6
We describe the experimental setup and the results in the next section.
We conducted the experiments to answer the following questions:
Specific details of the implementation are beyond the scope of the paper. Figure 3: Risk Level State Diagram: In the above fig-ure, each node corresponds to a risk level associated with a worker. The values range between 0 (min) -1 (max). The worker is either assigned a gold standard task (G) or a normal task (N) on the basis of his/her present risk level. The risk level changes every time a task is Accepted (A) or Rejected (R). Additionally worker may be warned (W) or blocked (B) in case of rejection.
We conducted the experiments on 566 spontaneous speech samples which were graded by expert assessors. Out of these, 319 were of native Indians (IN set), while the remain-ing 247 were of native Philippines (PH set). To answer the questions stated above, we developed models for the IN and PH sets separately. 7
In the model building phase, we used different set of fea-tures to develop the models and compared their accuracy. The models were built against expert grades using super-vised learning techniques. We experimented with three ma-chine learning techniques X  Ridge Regression, SVMs and Neu-ral Networks. The data set used in the experiments is dis-cussed in the next section.
Our data set contains a total of 566 spontaneous speech re-sponses comprising of 319 samples from India and 247 sam-ples from Philippines. The speech samples in both sets were from seniors (non X  X ative English speakers in final year of un-dergraduate education) pursuing bachelor X  X  degree in their respective countries. The candidates were asked to describe one of the following six scenes: a hospital, flood, a crowded market, an airport, favorite holiday destination and a school
We tried to build a single model over both samples, but it did not get good results. This could be because a good speech in one accent may not be equivalent to the other quantitatively. It may need further techniques for normal-ization. playground . The candidates were given 30 seconds to think and take notes and were then asked to speak for the next 45 seconds. The responses were collected on the phone during the SVAR test [5]. Apart from the spontaneous speech re-sponse, each candidate was asked to read 12 given sentences and repeat 9 given sentences immediately after listening to each of them. Empty or very noisy responses (not humanly discernible) were not included in the final 566 sample set.
Sample Set Inter correlation IN 0 . 83 0 . 86 PH 0 . 80 0 . 82
The responses in both sets were rated by one common rater with more than thirty years X  experience in grading spo-ken English and one specific rater each who had more than fifteen years X  experience. There were two set of scores. The first was a holistic score on the spontaneous speech samples based on its pronunciation, fluency, content characteristics and grammar. The second was a score on the pronunciation and fluency quality of the read/repeat sentences. The cor-relation between grades given by the two experts is shown below in Table 1. For each of the two scores, the average of the scores by the two expert grades was used for further purposes.

The correlation between the expert scores on spontaneous speech and automated read/repeat speech for IN set was 0 . 54 and for PH set was 0 . 56. This shows that there is a considerable unexplained variance (approx. 70%) in the spontaneous speech score, not addressed by the read/repeat scores. This could be due to a difference in the pronunciation quality and fluency of the candidates in reading/repeating text vs. speaking spontaneously and also due to the ad-ditional parameters of grammar and content characteristics in the spontaneous speech score. Thus, an automatic score mimicking the read/repeat expert grades, which is a solved problem, is inadequate for our task.

The first score is used for all subsequent discussion and development of models. The 566 speech sample assessment task was posted on Amazon Mechanical Turk (AMT). Each task was completed by three workers. In total, 312 unique workers completed the tasks. The majority of workers (90%) belonged to USA and India.

The task took on an average 8 X 9 minutes to complete and a worker was paid between 6 X 10 cents per task includ-ing a bonus which was paid on completion of every 4 tasks. The average transcription accuracy for a worker was 79 . 7% This significantly improved to 94 . 6% when the transcrip-tions of the three workers were combined using the ROVER algorithm. In comparison, the accuracy of automatic tran-scription of a speech recognition engine was 51 . 6%.
PHP similar text function was used as similarity metric.
Regression Modeling and the steps described herein were performed separately for IN and PH set. Each set was split into two sets: train and validation. The train set had 67% of the sample points whereas the validation set had 33%. The split was done randomly making sure that the grade distribution in both the sets was similar. While learning the model, a 3-fold cross validation was performed on the train sample.

Linear ridge regression, Neural Networks and SVM regres-sion with different kernels were used to build the models. The least cross-validation error was used to select the mod-els. We used some simple techniques for feature selection including forward feature selection and the algorithm which removes all but the k highest correlating features.
In next paragraphs, we explain the steps performed in tuning regression parameters and feature set used to build regression model.

Regression parameters: For linear regression with reg-ularization, optimal ridge coefficient  X  , between 1 and 1000, was selected based on the least RMS error in cross-validation. For support vector machines we tested two kernels: linear and radial basis function. In order to select the optimal SVM model, we varied the penalty factor C , parameters  X  and , the SVM kernel and the selected set of values that gave us the lowest RMS error in cross-validation. The Neural Networks model had one hidden layer and 5 to 10 neurons.
Feature sets used: The experiments were carried on five sets of features: first, features generated by automatic speech transcription of spontaneous speech using a speech recognizer (Pure ML approach); second, a set of features generated by force aligning read/repeated by candidates (an-other ML only approach referred to as RS/LR approach, see Section 4); third, a set of features pertaining to grades given by the crowd; fourth, NLP and FA features gener-ated by force aligning spontaneous speech on crowdsourced transcription (ML-CS approach) and fifth, NLP, FA features from crowdsourced transcription and Crowd Grades.
 In the following subsection, the features pertaining to ML-CS approach are referred to as ML-CS, those pertaining to natural language processing on crowdsourced transcription are referred to as NLP features while the one pertaining to crowd grades are referred to as Crowd Grades.
The results of the experiments are tabulated in Table 2. We report the Pearson coefficient of correlation ( r ) for the different models against the expert grades for IN and PH samples. These are the results for the models selected according to least cross-validation error. The best cross-validation error in case of SVMs was obtained for the linear kernel.

All the following observations are based on the valida-tion error. All the three techniques perform similarly with Neural Networks doing slightly worse in some cases. The broad trends across feature X  X ets remain similar across dif-ferent modeling techniques. We will be referring to the ridge regression results for further discussion.

Firstly, it is observed that the read/repeat features predict the spontaneous speech score with low accuracy ( r PH = 0 . 47, r
IN = 0 . 47) for both samples. This implies that read/repeat speech and derived features are inadequate to grade a per-son X  X  spontaneous speech, the ultimate test of a person X  X  spo-ken language skills. The second observation is that the ML-only approach using spontaneous speech features (Model IN RR-2, PH RR-2) is also inadequate to grade sponta-neous speech and does worse than approaches that uses fea-tures from crowdsourced transcription (Model PH/IN RR-4). This clearly shows the value of getting accurate tran-scription from workers towards better features and model.
Further, among the crowdsourcing approaches, we find that the crowd grades (Model RR-3) does equivalently well (and sometimes worse) than the model using features de-rived from the crowdsourced speech (Model IN/PH RR-4). However, when we combine all the features from crowdsourc-ing including the crowd grades, we find much better predic-tion accuracy ( r IN = 0 . 74 and r PH = 0 . 79). This shows that the crowd grades feature provides some orthogonal infor-mation as compared to the features from the crowdsourced transcription, towards predicting the grade given by experts. The validation r for Model IN RR-5 is 0 . 74 and Model PH RR-5 is 0 . 79. We find that the expert agreement on the validation sample is 0 . 77 and 0 . 82 respectively for each model. Thus, our predicted score rivals the agreement of experts. This shows great promise for the technique to be used in a high stake test setting.

In summary, we show the following:
We studied the deployment of our spontaneous speech scoring algorithm at a hiring event of a potential customer in Philippines. The company was using another automated speech evaluation product. The scoring of this product was based on the read and repeat speech of the candidate and could not do spontaneous evaluation. The customer was not satisfied with the product and experienced high type-1 error. The recruitment event had 500 applicants for the role of a customer support executive, who had to talk to native English speakers. The new scoring algorithm was tested on a subset of 150 students. All these candidates took the complete SVAR test along with the spontaneous speech section. The same set of students also took the ex-isting speech grading product of the other vendor. These candidates were also evaluated by an expert designated by the company. The expert graded each candidate X  X  speech as hirable or not-hireable based on the requirements for the position.

We sweep thresholds across the scores from our spon-taneous speech algorithm, read speech scores used by the company and SVAR read speech scores, to enable hirable/ not-hireable decisions. Figure 4 shows the trade-off curve between rejecting good candidates (type-2 error) and select-ing bad candidates (type-1 error). The spontaneous speech score ROC curve completely dominates the read speech by both SVAR and the existing product.

Based on one of the trade-off points on the curve, we are correctly able to identify 90% of the hirable candidates while allowing 21% of not-hireable candidates into the final hiring pool. The extant approach however could only identify 69% of the hirable candidates while allowing approximately the same amount of not-hireable candidates. Another trade-off point we consider is with a much lower type-1 error. By se-lecting an appropriate point, we were able to bring down the percentage of not-hireable candidates in the final set to 15%, while rejecting around 30% of the hirable candidates. On a similar selection percentage of not-hireable candidates, the other scoring algorithms were rejecting around 50% hirable candidates.

The above case study clearly shows the superior candi-date selection capabilities of the spontaneous speech scoring Figure 4: Classification errors of different scoring algorithms algorithm over the score based on the read speech of the candidate. This preliminary case study demonstrates how our approach gives immediate benefit and saves time of the interviewers by massively reducing the number of good can-didates being rejected while also reducing the number of bad candidates selected, thus improving the overall hiring quality.
We addressed the problem of evaluating spontaneous speech using a combination of machine learning and crowdsourcing. To achieve this, we post the task of speech transcription to the crowd. Additionally, we also get spoken English grades from the crowd. We are able to derive accurate features by force aligning the speech sample on the crowdsourced text. We experimented our technique on expert X  X raded speech samples of adult non X  X ative speakers from India and Philip-pines. Using these features in a regression model, we are able to predict expert grades with much higher accuracy than a machine learning only approach. These features also predict equivalent or better than crowd grades and a combination of these two outperforms all other approaches. Our approach shows an accuracy that rivals that of expert agreement.
Our technique has a promise of higher accuracy but has some trade-offs compared to fully automated approaches. First, there is a cost for every assessment done and the scal-ability depends on the number of non-expert workers avail-able. Though these drawbacks exist, we were able get tasks done inexpensively. We recently had the crowd rate a hun-dred samples in a day without any challenge. Second, our approach doesn X  X  provide instant grades. This works fine in many scenarios, but doesn X  X  cater well to providing real-time feedback. Real time crowdsourcing has been an active area of research [25, 26] and is an area for future work for us as well. [1] Menucha Birenbaum and Kikumi K Tatsuoka.
 [2] Varun Aggarwal, Shahank Srikant, and Vinay [3] P Mitros, Vikas Paruchuri, John Rogosic, and Diana [4] Cliff E Beevers and Jane S Paterson. Automatic [5] SVAR. 2014. http://www.aspiringminds.in/talent-[6] Klaus Zechner, Derrick Higgins, Xiaoming Xi, and [7] Sumit Basu, Chuck Jacobs, and Lucy Vanderwende. [8] Donald E Powers, Jill C Burstein, Martin Chodorow, [9] Cem Akkaya, Alexander Conrad, Janyce Wiebe, and [10] ASID Lang and Joshua Rio-Ross. Using amazon [11] Matthew Marge, Satanjeev Banerjee, and Alexander I [12] Thierry Buecheler, Jan Henrik Sieg, Rudolf M [13] Mark Lejk and Michael Wyvill. The effect of the [14] Nathan Van Houdnos. Can the internet grade math? [15] Joel R Tetreault, Elena Filatova, and Martin [16] Nitin Madnani, Joel Tetreault, Martin Chodorow, and [17] Sherrie A Kossoudji. English language ability and the [18] Elizabeth J Erling and Philip Seargeant. English and [19] Cahit Guven and Asadul Islam. Age at migration, [20] K  X are Sj  X  olander. An hmm-based system for automatic [21] Chinmay E Kulkarni, Richard Socher, Michael S [22] Marina Boia, Claudiu Cristian Musat, and Boi [23] Ece Kamar, Severin Hacker, and Eric Horvitz. [24] Sivan Sabato and Adam Kalai. Feature multi-selection [25] Michael S Bernstein, Joel Brandt, Robert C Miller, [26] Walter S Lasecki, Christopher D Miller, and Jeffrey P [27] Cambridge EOCL Examinations. Using the CEFR : [28] Eric John Dobson. English Pronunciation, 1500-1700: [29] Christopher Brumfit and Christopher J Brumfit. [30] Robert Stalnaker. The problem of logical omniscience, [31] David Brazil. A grammar of speech . Oxford University [32] Voxforge. 2014. [33] Mehdi Hosseini, Ingemar J Cox, Nata X sa [34] Steve Young, Gunnar Evermann, Mark Gales, Thomas [35] John S Garofolo, Lori F Lamel, William M Fisher, [36] Leonardo Neumeyer, Horacio Franco, Mitchel [37] Catia Cucchiarini, Helmer Strik, and Lou Boves. [38] LightSide. 2013. http://lightsidelabs.com/. [39] AfterTheDeadline. 2014. [40] Gabriele Paolacci, Jesse Chandler, and Panagiotis G [41] Amazon Mechanical Turk. 2014. [42] Jonathan G Fiscus. A post-processing system to yield [43] Ahmet Aker, Mahmoud El-Haj, M-Dyaa Albakour, [44] Quoc Viet Hung Nguyen, Tam Nguyen Thanh, Tran
