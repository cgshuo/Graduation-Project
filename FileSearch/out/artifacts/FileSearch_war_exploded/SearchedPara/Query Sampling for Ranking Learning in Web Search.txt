 Learning to rank has become a popular approach to build a ranking model for Web search recently. Based on our obser-vation, the constitution of the training set will greatly in flu-ence the performance of the learned ranking model. Mean-while, the number of queries in Web search is nearly infinite and the human labeling cost is expensive, hence a subset of queries need to be carefully selected for training. In thi s paper, we develop a greedy algorithm to sample the queries, by simultaneously taking the query density, difficulty and diversity into consideration. The experimental results on a collected Web search dataset comprising 2024 queries show that the proposed method can lead to a more informative training set for building an effective model.
 H.3.3 [ Information Storage and Retrieval ]: Retrieval models Algorithms, Experimentation.
 Learning to Rank, Training Set Construction.
Recently,  X  X earning to rank X  [5], which learns from labeled queries for a highly flexible ranking model, has attracted in -creasingly more attentions from not only the academia but also the industry. The process of learning to rank is as fol-lows. Firstly, some documents are labeled with the relevanc e to some queries by human oracle. Then the ranking model will be trained using the labeled data. These two steps are performed offline. Finally the learned ranking model is used to predict the relevance of documents to a given query, based on which the search results can be ranked and presented to the users.

It is obvious that the performance of learning to rank methods will greatly depend on the constitution of the train -ing data. A training set comprising informative samples wil l lead to a highly-performing ranking model, and vice versa. On the other hand, the human labeling cost is expensive and there are a huge amount of samples available on the Web. estimated as the following, where D R is the function to estimate the ranking distance between two rankings. In this paper, we use the inverse of Kendall tau rank correlation coefficient [6] to estimate the distance between two ranking lists, since it captures the di f-ference between the rankings instead of the ranking scores.
For a given candidate query set Q , we would like to select the queries which are more representative, i.e., the querie s that locate in the highly dense areas in the query space. To derive the query density measurement, we propose to utilize kernel density estimation (KDE) to estimate the density of each query in the whole query space, with the kernel taken as the standard Gaussian function with zero mean and unit variance, as follows, where h is the bandwidth parameter, and |Q| denotes the number of queries in the candidate set Q . Here, D Q ( q i , q j ) is the query distance between q i and q j . Since in this paper, the purpose of computing the query distance is to select some representative and diverse queries for ranking learni ng, we use the ranking list derived from the ranking features to represent the query. Formally, query q can be represented as q = ( r 1 , r 2 , . . . , r M ). The query distance between the query q and q j is defined as the average ranking distance between the lists derived from the corresponding features,
Given the candidate query set Q and the selected query set Q 0 , we propose to select the next query q  X  X  X  X  0 with maximal diversity into Q 0 , i.e., q is the most dissimilar to the queries in Q 0 , so that after the selection, Q 0 is diverse and informative to represent different kinds of queries in Q . Formally, the query diversity is defined as the minimal distance between q and the queries in Q 0 , i.e.,
We collect a dataset comprising 2024 queries from a com-mercial search engine, where we randomly select 5 queries for validation and 500 for testing. The training set are con-structed from the remaining queries, with the size varying from 5 to 30 gradually. Ranking SVM [5] and ListNet [1] are adopted to demonstrate the effectiveness of the proposed query selection method, by comparing with the random se-lections (average of 10 trials). We utilize the Mean Average Precision (MAP) and the Normalized Discounted Cumula-tive Gain (NDCG) [4] for the performance evaluation. To normalize the query difficulty, density and diversity, we uti -lize two methods: LinearNorm (linearly rescaling to [0 , 1]) and GaussianNorm (nonlinearly transforming to zero mean and unit variance). The results are shown in Fig.1, which
