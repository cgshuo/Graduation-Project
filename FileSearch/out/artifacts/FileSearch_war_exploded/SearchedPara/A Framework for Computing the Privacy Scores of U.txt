
Social-networking sites have grown tremendously in pop-ularity in recent years. Services such as Facebook and MySpace allow millions of users to create online profiles and share details of their personal lives with vast networks of friends, and often, strangers. As the number of users of these sites and the number of sites themselves explode, securing individuals X  privacy to avoid threats such as identity theft and digital stalking becomes an increasingly important issue.
Unfortunately, even sophisticated users who value privacy will often compromise it to improve their digital presence in the virtual world. They know that loss of control over their personal information poses a long-term threat, but they cannot assess the overall and long-term risk accurately enough to compare it to the short-term gain. Even worse, setting the privacy controls in online services is often a complicated and time-consuming task that many users feel confused about and usually skip.

Past research on privacy and social networks ( e.g. ,[1], [3], [4], [7], [9]) mainly focuses on corporate-scale privacy concerns, i.e. , how to share a social network owned by an organization without revealing the identity or sensitive relationships among the registered users. Not much attention has been given to individual users X  privacy risk posed by their information-sharing activities.

In this paper, we address the privacy issue from the user X  X  perspective: we propose a framework that estimates a privacy score for each user. This score measures the user X  X  potential privacy risk due to his online information-sharing behaviors. With this score, a user can monitor his privacy risk in real time and compare it with the rest of the population to see where he stands. In the case where the overall privacy risks of a user X  X  social graph are lower than that of the user, the system can recommend the user stronger privacy settings based on the information from his social neighbors. Our ultimate objective is to enhance public awareness of privacy, and to help users to easily manage their information sharing in social networks.

From the technical point of view, our definition of privacy score satisfies the following intuitive properties: The score increases with the i) sensitivity of the information being revealed and ii) with the visibility of the revealed information gets in the network. We develop mathematical models to estimate both sensitivity and visibility of the information, and we show how to combine these two factors in the calculation of the privacy score .

Contribution: To the best of our knowledge, we are the first to provide an intuitive and mathematically sound methodology for computing users X  privacy scores in online social networks. The two principles stated above are rather general, and many models would be able to satisfy them. In addition, the specific model we proposed in this paper exhibits two extra advantages: i) it is container independent, meaning that scores calculated for users belonging to differ-ent social networks ( e.g. , Facebook, LinkedIn and MySpace) are comparable, and ii) it fits the real data. Finally, we give algorithms for the computation of privacy score that scale well and indicative experimental evidence of the efficacy of our framework.

Overview of the technical framework: For a social-network user we compute his privacy score as a combination of the partial privacy scores of each one of his profile items, e.g., user X  X  real name, email, hometown, mobile-phone number, relationship status, sexual orientation, IM screen name, etc. The contribution of each profile item in the total score depends on the sensitivity of the item and the visibility it gets due to the user X  X  privacy settings. Therefore, the primary input to our framework is an n  X  N response matrix that shows the privacy settings of each one of the N users for each one of the n profile items. The values appearing in the response matrix are natural numbers; the higher the value of the cell R ( i, j ) , the more willing user j is to disclose information about item i . Our approach explores this information to compute the privacy scores of users. We do so by employing theories from Item Response Theory (IRT) [2].

In this paper, we do not consider how to conduct inference attacks to derive hidden information about a user based on his publicly disclosed data. We deem this inference problem as an important, albeit orthogonal to our work. Some profile items such as hobbies are composite since they may contain many different sensitive information. We decompose this kind of items into primitive ones. Again determining the granularity of the profile items is considered an orthogonal issue to the problem we study here.

Organization of the material: After the presentation of the related work in Section II and the description of the notational conventions in Section III, we present our defini-tions of privacy score in Section IV. Models and algorithmic solutions are described in Sections V, VI and VII. Experi-mental comparison of our methods is given in Section VIII. We conclude in Section IX.

To the best of our knowledge, we are the first to present a framework that formally quantifies the privacy risk of online social-network users. What we consider as the most relevant work is the work on scoring systems for measuring popularity , creditworthiness , trustworthiness ,and identity verification . We briefly describe these scores here.
QDOS score: Garlik, a UK-based company, launched a system called QDOS 1 for measuring people X  X  digital popularity. The primary purpose is to encourage people to share more information to enhance their rankings, which is opposite to ours. Their algorithms are not disclosed.
Credit score: A credit score 2 is used to estimate the likelihood that a person will default on a loan. The credit score is different from our privacy-risk score not only because it serves different purposes but also because the input data used for estimating the two scores as well as the estimation methods themselves are different.

Tr ust sc o re : A trust score is a measure of how a member of a group is trusted by the others ( e.g. , [6]). Trust scores could be used by social-network users to determine who can view their personal information. However, our system is used to quantify the privacy risk after the information has been shared.

Identity score: An identity score 3 is used by financial-service firms for tagging and verifying the legitimacy of one X  X  public identity. Our privacy risk score is different from identity score since it serves a different purpose. We assume there exists a social-network G that consists of N nodes, every node j  X  X  1 ,...,N } being associated with a user of the network. Every user has a profile consisting of n profile items. For each profile item, users set a privacy level that determines their willingne ss to disclose information associated with this item. The n  X  N response matrix R stores the privacy levels of all N users for all n profile items; R ( i, j ) refers to the privacy setting of user j for item i .If the entries in R take values in { 0 , 1 } , we say that R is a dichotomous . Otherwise, if the entries in R take any non-negative integer values in { 0 , 1 ,..., } we say that matrix R is polytomous .

In a dichotomous response matrix R , R ( i, j )=0 means that user j has made the information associated with profile item i private, whereas R ( i, j )=1 means that j has made item i publicly available. In a polytomous response matrix, R ( i, j )=0 means that user j keeps profile item i private; whereas R ( i, j )= k with k  X  1 means that j discloses information regarding item i to users that are at most k -links away in G .

In general, R ( i, j )  X  R ( i ,j ) means that j has more conservative privacy settings for item i than item i .The i -th row of R , denoted by R i , represents the settings of all users for profile item i . Similarly, the j -th column of R , denoted by R j , represents the profile settings of user j .
We often consider users X  settings for different profile items as random variables described by a probability dis-tribution. In such cases, the observed response matrix R is just a sample of responses that follow this probability distribution. For dichotomous response matrices, we use P to denote the probability that user j selects R ( i, j )=1 . That is, P ij = Prob R ( i, j )=1 . In the polytomous case, we use P ijk to denote the probability that user j sets R ( i, j )= k .Thatis, P ijk = Prob R ( i, j )= k .
In order to allow the readers to build intuition, we start by defining the privacy score for dichotomous response matrices in Section IV, and present algorithms for computing it in Sections VII and V. Polytomous settings are handled in Section VI.

The privacy score of a user is an indicator of his potential privacy risk; the higher the score of a user, the higher the threat to his privacy. Our basic premises for the calculation of privacy score are the following: (a) the more sensitive the information revealed by a user, the higher his privacy score; and (b) the wider the information about a user spreads, the higher his privacy score. Therefore, the privacy score of a user is a monotonically increasing function of two parameters: the sensitivity of the profile items and the visibility these items get.

Sensitivity of a profile item: The sensitivity of item i  X  { 1 ,...,n } is denoted by  X  i . This property depends on the item itself. Some profile items are, by nature, more sensitive than others.

Visibility of a profile item: The visibility of a profile item i that belongs to user j is denoted by V ( i, j ) .It captures how known this item becomes in the network; the widely it spreads, the higher the visibility. Naturally, V ( i, j ) depends on the value R ( i, j ) , which is the explicit privacy level chosen by the user. Thus, in the dichotomous case, we can simply define V ( i, j )= I ( R ( i,j )=1) ,where I is an indicator variable that becomes 1 when  X  X ondition X  is true. We call this the observed visibility for item i that belongs to user j . From a statistics point of view, one can assume that R is a sample from a probability distribution over all possible response matrices. Then, the true visibility is computed as V ( i, j )= P ij  X  1+(1  X  P ij )  X  0= P ij where P ij = Prob R ( i, j )=1 . Probability P ij depends both on the item i and the user j .

Privacy score of a user: The privacy score of individual j due to item i , denoted by P R ( i, j ) ,isdefinedasP R ( i, j )=  X  i V ( i, j ) .Operator is used to represent any arbitrary combination function that r espects the fact that P R ( i, j ) is monotonically increasing with bot h sensitivity and visibility. For simplicity, we use the product operator.

In order to evaluate the overall privacy score of user j , denoted by P R ( j ) , we can sum the privacy score of j due to different items. 4 That is, In the above, the privacy score can be computed using either observed visibility or true visibility . For the rest of the discussion we use the true visibility since we believe that the specific privacy settings of a user are just an instance of his possible setting described by probability distribution P R ( i, j ) .

In the next two sections we show how to compute the privacy score of a user in a social network based on his privacy settings on his profile items. .

We first introduce some basic concepts from Item Re-sponse Theory (IRT) [2]. Then, we show how these concepts are applicable in our setting.
 A. Introduction to IRT
IRT has its origins in psychometrics where it is used to analyze data from questionnaires and tests. The goal there is to measure the abilities of the examinees, the difficulty of the questions and the probability of an examinee to correctly answer a given question.

Every examinee j is characterized by his ability level  X  j  X   X  (  X  X  X  ,  X  ) . Every question q i is characterized by a pair of parameters  X  i =(  X  i , X  i ) . Parameter  X  i ,  X  i  X  ( represents the difficulty of q i . Parameter  X  i ,  X  i  X  ( quantifies the discrimination power of q i . The intuitive meaning of these two paramet ers will become clear shortly. The basic random variable of the model is the response of examinee j to a particular question q i . If this response is marked as either  X  X orrect X  or  X  X rong X  (dichotomous response), then t he probability that j answers q i correctly is given by Thus, P ij is a function of parameters  X  j and  X  i =(  X  i For a given question q i with parameters  X  i =(  X  i , X  i ) ,the plot of the above equation as a function of  X  j 5 is called the Item Characteristic Curve (ICC).
The ICCs obtained for differ ent values of parameters  X  i (  X  i , X  i ) are given in Figures 1(a) and 1(b). These figures make the intuitive m eaning of parameters  X  i and  X  i easier to explain.

Figure 1(a) shows the ICCs obtained for two questions q 1 and q 2 with parameters  X  1 =(  X  1 , X  1 ) and  X  2 =(  X  2 such that  X  1 =  X  2 and  X  1 &lt; X  2 . Parameter  X  i , the item difficulty, is defined as the point on the ability scale at which P ij =0 . 5 . We can observe that IRT places  X  i and  X  j on the same scale (see the x-axis of Figure 1(a)) so that they can compare. If an examinee X  X  ability is higher than the difficulty of the question, then he has better chance to get the answer right, and vice versa. This also indicates a very important feature of IRT called group invariance , that is, the item X  X  difficulty is a property of the item itself, not of the people that responded to the item. We will elaborate on this in the experiments section.

Figure 1(b) shows the ICCs obtained for two questions q 1 and q 2 with parameters  X  1 =(  X  1 , X  1 ) and  X  2 =(  X  2 such that  X  1 &gt; X  2 and  X  1 =  X  2 . Parameter  X  i , the item discrimination, is proportional to the slope of P ij = P i at the point where P ij =0 . 5 ; the steeper the slope, the higher the discriminatory power of a question, meaning that this question can well differentiate among examinees whose abilities are below and above the difficulty of this question.
In our IRT-based computation of the privacy score, we estimate the probability Prob R ( i, j )=1 using Equa-tion (2). However, we do not have examinees and questions, but we rather have users and profile items. Thus, each examinee is mapped to a user, and each question is mapped to a profile item. The ability of an examinee corresponds to the attitude of a user: for user j , his attitude  X  j quantifies how concerned j is about his privacy; low values of  X  j indicate a conservative/introvert user, while high values of  X  j indicate a careless/extrovert user. We use the difficulty parameter  X  i to quantify the sensitivity of profile item i . In general, parameter  X  i can take any value in (  X  X  X  ,  X  ) . In order to maintain the monotonicity of the privacy score with respect to items X  sensitivity we need to guarantee that  X   X  0 for all i  X  X  1 ,...,n } . This can be easily handled by shifting all items X  sensitivity values by a big constant value.
In the above mapping, parameter  X  i is ignored. Due to space constraints, we cannot elaborate on this topic, but the general idea is that this parameter allows us to do a finer-grained analysis of items and users.

For computing the privacy score we need to compute the sensitivity  X  i for all items i  X  X  1 ,...,n } and the probabilities P ij = Prob R ( i, j )=1 , using Equa-tion (2). For the latter computation, we need to know all the parameters  X  i =(  X  i , X  i ) for 1  X  i  X  n and  X  j for 1  X  j  X  N . In the following sections, we show how we can estimate these parameters using as input the response matrix R and employing Maximum Likelihood Estimation (MLE) techniques. All these techniques exploit the following three independence assumptions that are inherent in IRT models: (i) independence between items; (ii) independence between users; and (iii) independence between users and items. Experiments in Section VIII show that parameters learned based on these assumptions fit the real-world data very well. We refer to the privacy score computed using these methods as the Pr IRT score.
 B. IRT-based computation of sensitivity
In this section, we show how to compute the sensitivity  X  i of a particular item i . 6 Since items are independent, the computation of parameters  X  i =(  X  i , X  i ) is done separately for every item; thus all methods are highly parallelizable.
In Section V-B1 we first show how to compute  X  i assum-ing that the attitudes of the N individuals  X  =(  X  1 ,..., X  are given as part of the input. The algorithm for the com-putation of items X  parameters when attitudes are not known is discussed in Section V-B2. 1) Item-parameters estimation: The maximum likelihood estimation of  X  i =(  X  i , X  i ) sets as our goal to find  X  that the likelihood function is maximized. Recall that P ij is evaluated as in Equation (2) and depends on  X  i , X  i and  X  j .

The above likelihood function assumes different attitude per user. In practice, online social-network users form a grouping that partitions the set of users { 1 ,...,N } into K non-overlapping groups { F 1 ,...,F K } such that K g =1 F { 1 ,...,N } .Let  X  g be the attitude of group F g (all members of F g share the same attitude  X  g )and f g = | F g | .Also,for each item i let r ig be the number of people in F g that set R ( i, j )=1 ,thatis, r ig = j | j  X  F g and R ( i, j )= 1 . Given such grouping, the likelihood function can be ignoring the constants, the corresponding log-likelihood function is L = Our goal is now to find item parameters  X  i =(  X  i , X  i ) to maximize this log-likelihood function. For this we use the Newton-Raphson method [8]. The Newton-Raphson method is a numerical algorithm that, given partial derivatives L L iteratively. At iteration ( t +1) , the estimates of the param-eters  X  i ,  X  i denoted by corresponding estimates at iteration t as follows:
Discussion: The overall process starts with the partition-ing of the set of N users into K groups based on users X  attitude. This routine implements an 1-dimensional cluster-ing, and can be done optimally using dynamic programming in
O N 2 K time. The result of this procedure is a grouping of users into K groups { F 1 ,...,F K } , with group attitudes  X  , 1  X  g  X  K . Given this grouping, the values of f g and r ig for 1  X  i  X  n and 1  X  g  X  K are computed. These computations take time O ( nN ) . Given these values, the Newton-Raphson estimation is performed for each one of the n items. This takes O ( nIK ) time in total, where I is the number of iterations for the estimation of one item. There-fore, the total running time of item-parameters estimation is
O N 2 K + nN + nIK . Note that the N 2 K complexity can be reduced to linear using heuristics-based clustering, though the optimality is not guaranteed. Moreover, since items are independent of each ot her, the Newton-Raphson estimation of each item can be done in parallel, which makes the computation much more efficient than the theoretical complexity. 2) The EM algorithm for item-parameter estimation: Here, the goal is again to find  X  =(  X  1 ,..., X  n ) to maximize P R |  X  . The only difference is that the elements of vector  X  are unknown. We tackle this problem using an Expectation-Maximization ( EM ) procedure.

Expectation Step: In this step, we calculate the expected grouping of users using previously estimated  X  .Inother words, for 1  X  i  X  n and 1  X  g  X  K , we compute E [ f g ] and E [ r ig ] as follows: The computation relies on the posterior pr obability dis-tribution of a user X  X  attitude P  X  g | R j ,  X  . Assume for now that we know how to compute these probabilities. It is easy to observe that the membership of a user in a group is probabilistic. That is, every individual belongs to every group with some probability; the sum of these membership probabilities is equal to 1 .

Maximization Step: Knowing the values of f g and r ig for all groups and all items allows us to compute a new esti-mate of  X  by invoking the Newton-Raphson item-parameters estimation procedure ( NR_Item_Estimation ) described in Section V-B1.

The pseudocode for the EM algorithm is given in Al-gorithm 1. Every iteration of the algorithm consists of an Expectation and a Maximization step.

The Posterior Probability of Attitudes: By the definition of probability, this posterior probability is: Algorithm 1 The EM algorithm for estimating item parameters
Function g (  X  j ) is the probability density function of attitudes in the population of users. It is used to model our prior knowledge about user attitudes and its called the prior distribution of users attitude. Following standard conventions [5], we assume that the prior distribution g ( . ) is Gaussian and is the same for all users. Our results indicate that this prior fits the data well.

The term P R j |  X  j ,  X  in the numerator is the likelihood of the vector of observations R j given items X  parame-ters and user j  X  X  attitude. This term can be computed using the standard likelihood function P R j |  X  j ,  X  =
The evaluation of the posterior probability of every at-titude  X  j requires the evaluation of an integral. We bypass this problem as follows: Since we assume the existence of K groups, we only need to sample K points X 1 ,...X K on the ability scale. Each of these points serves as the common attitude of a user group. For each t  X  X  1 ,...,K } we compute g ( X t ) , the density of the attitude function at attitude value X t . Then, we let A ( X t ) be the area of the rectangle defined by the points ( X t  X  0 . 5 , 0) , ( X t values are normalized such that K t =1 A ( X t )=1 .Inthat way, we can obtain the posterior probabilities of X t P X t | R j ,  X  =
Discussion: The estimation of privacy score using the IRT model requires as input the number of groups of users K . In our implementation we follow standard conventions [5] and set K =10 . However, we have found that other values of K fit the data as well. The estimation of the  X  X orrect X  number of groups is an interesting model-selection problem for IRT models, which is not the focus of this work. The running time of the EM algorithm is O ( I R ( T E XP + T M AX )) ,where I R is the number of iterations of the repeat statement, and T E XP and T M AX the running times of the Expectation and the Maximization steps respectively. Lines 9 and 11 require O ( Nn ) time each. Therefore, the total time of the expectation step is T Section V-B1 we know that T M AX = O ( nIK ) ,where I is the number of iterations of Equation (5). Again, Steps 12, 13, 14 can be done in parallel due to the independence assumption of items.
 C. IRT-based computation of visibility The computation of visibility requires the evaluation of P ij = Prob R ( i, j )=1 given in Equation (2). Apparently if vectors  X  =(  X  1 ,..., X  N ) ,  X  =(  X  1 ,..., X  n ) and  X  = (  X  1 ,..., X  n ) are known, then computing P ij ,forevery i and j ,istrivial.

Here, we describe the NR_Attitude_Estimation algorithm, which is a Newton-Raphson procedure for com-puting the attitudes  X  of individuals using the item param-eters  X  =(  X  1 ,..., X  n ) and  X  =(  X  1 ,..., X  n ) . These item parameters could be given as input or they can be computed using the EM algorithm (Algorithm 1). For each individual j , the NR_Attitude_Estimation computes  X  j that max-corresponding log-likelihood L = Since  X  and  X  are part of the input, the only variable to maximize over is  X  j . Using Newton-Raphson method, the estimate  X  j at iteration ( t +1) is computed using the estimate at iteration t as follows:
Discussion: For I iterations of the Newton-Raphson method, the running time for estimating a single user X  X  attitude  X  j is O ( nI ) . Due to the independence of users, each user X  X  attitude is estimated separately; thus the estima-tion for N users requires O ( NnI ) time. Once again, this computation can be parallelized due to the independence assumption of users.
 D. Putting it All Together
The sensitivity  X  i computed in Section V-B and the visibility P ij computed in Section V-C can be applied to Equation (1) to compute the privacy score of a user.
The advantages of the IRT framework can be summarized as follows: 1) The quantities IRT computes, i.e. , sensitivity, attitude and visibility, have an intuitive interpretation. For example, the sensitivity of information can be used to send early alerts to users when the sensitivities of their shared profile items are out of the comfortable region. 2) Due to the independence assumptions, many of the computations can be parallelized, which makes the computation very efficient in practice. 3) As our experiments will demonstrate later, the probabilistic model defined by IRT in Equation (2) can be viewed as a generative model, and it fits the real response data very well in terms of  X  2 goodness-of-fit test. 4) Most importantly, the estimates obtained from IRT framework satisfy the group invariance property. We will further discuss this property in the experimental section. At an intuitive level, this property means that the privacy scores computed across different social networks are comparable.

In this section, we show how the definitions and methods described before can be extended to handle polytomous response matrices. Recall that i n polytomous matrices, every entry R ( i, j )= k with k  X  X  0 , 1 ,..., } . The smaller the value of R ( i, j ) , the more conservative the privacy setting of user j with respect to profile item i . The definitions of sensitivity and visibility of items in the polytomous case generalize as follows.
 Definition 1. The sensitivity of item i  X  X  1 ,...,n } respect to privacy level k  X  X  0 ,..., } , is denoted by  X  Function  X  ik is monotonically increasing with respect to k ; the larger the privacy level k picked for item i the higher its sensitivity.

Similarly, the visibility of an item becomes a function of its privacy level.
 Definition 2. The visibility of item i that belongs to user j at level k is denoted by V ( i, j, k ) .The observed visibility is computed as V ( i, j, k )= I ( R ( i,j )= k )  X  k .The true visibility is computed as V ( i, j, k )= P ijk  X  k ,where P ijk = Prob R ( i, j )= k .

Given Definitions 1 and 2 we compute the privacy score of user j using the following generalization of Equation (1): Again, in order to keep our framework more general, in the following sections, we will discuss true rather than observed visibility for the polytomous case.
 A. IRT-based privacy score: polytomous case
Computing the privacy score in this case boils down to a transformation of the polytomous response matrix R into ( +1) dichotomous response matrices R  X  0 , R  X  1 ,..., R Each matrix R  X  k , k  X  X  0 , 1 ,..., } , is constructed so that R k ( i, j )=1 if R ( i, j ) Let P  X  ijk be the probability of setting R  X  k ( i, j )=1 , i.e. , P ijk = Prob R k =0 ,matrix R  X  ik has all its entries equal to one, we have that P  X  ijk =1 for all users. When k  X  X  1 ,..., } , P  X  ijk given as in Equation (2). That is,
By construction, for every k ,k  X  X  1 ,..., } and k &lt;k we have that matrix R  X  k contains only a subset of the 1-entries appearing in matrix R  X  k . Therefore, P  X  ijk  X  P and ICC curves ( P  X  ijk ) of the same profile item i at different privacy levels k  X  X  1 ,..., } do not cross . This observation results in the following corollary.
 Corollary 1. For items i and privacy levels k  X  X  1 ,..., } curves P  X  ijk do not cross, we also have that  X   X  i  X  ik = ... =  X  are not defined.

The computation of privacy score in the polytomous case, however, requires computing  X  ik and P ijk = Prob R ( i, j )= k (see Definition (2) and Equation (12) ). These parameters are different from  X   X  ik and P  X  ijk the latter are defined on dichotomous matrices. Now the question is, if we can estimate  X   X  ik and P  X  ijk ,howto transform them to  X  ik and P ijk .

Fortunately, since by definition P  X  ijk is the cumulative probability P  X  ijk = k = k P ijk ,wehavethat P Also, by [2], we also have the following proposition for  X  Proposition 1. ([2]) For k  X  X  1 ,...,  X  1 } it holds that  X 
From Proposition 1 and Corollary 1 we have the follow-ing.
 Corollary 2. Fo r k  X  X  0 ,..., } it holds that  X  i 0 &lt; X  ...&lt; X  i .

The above corollary verifies our intuition that the sensi-tivity of an item is a monotonically increasing function of the privacy level k .

Estimating the parameters: The estimation of the pa-erative Newton-Raphson procedure, which is pretty much the same as we did in Section V. Once these parameters are calculated, it is easy to compute P  X  ijk , P ijk and  X  The overall privacy score is then computed by applying sensitivity values  X  ik and visibility values P ijk  X  k to Equation (12). We refer to the score thus obtained as the Pr IRT score. The distinction between polytomous and dichotomous IRT scores becomes clear from the context.
In this section we describe a simple way of computing the privacy score of a user. We call this approach Naive and it serves as a baseline methodology for computing privacy scores. We also demonstrate some of its disadvantages.
Naive computation of sensitivity: Intuitively, the higher the sensitivity of an item i , the less number of people who are willing to disclose it. So, if | R i | denotes the number of users who set R ( i, j )=1 , then the sensitivity  X  i for dichotomous matrices can be computed as the proportion of users that are reluctant to disclose item i .Thatis, The higher the value of  X  i , the more sensitive item i .
For the polytomous case, the above equation generalizes as follows:
Note that the  X  ik values are then computed from  X   X  ik following Proposition 1.

Naive computation of visibility: The computation of visibility in the dichotomous case requires an estimate of the probability P ij = Prob R ( i, j )=1 . Assuming inde-pendence between items and individuals, we can compute P ij to be the product of the probability of an 1 in row R i times the probability of an 1 in column R j .Thatis,if R j is the number of items for which j sets R ( i, j )=1 ,we have Probability P ij is higher for less sensitive items and for users that have the tendency/attitude to disclose lots of their profile items.

The visibility in the polytomous case requires the compu-tation of probability P ijk = Prob R ( i, j )= k . By assum-ing independence between items and users, this probability can be computed as follows: The Naive computation of privacy score requires applying Equations (15) and (17) to Equation (1), or Equations (16) and (18) to Equation (12). We refer to the privacy-risk score computed in this way as the Pr Naive score.

Discussion: The Naive computation can be done effi-ciently in O ( Nn ) time. But the disadvantage is that the sensitivity values obtained are significantly biased by the user population contained in R . If the users happen to be quite conservative and they rarely share anything, then the estimated sensitivity values can be very high, otherwise the values can be very low if the users are very extrovert. Moreover, as we will show in the experimental section, the probability model defined b y Equation (17) and (18), though simple and intuitive, fails to fit the real-world response matrices R (in terms of  X  2 goodness-of-fit).

The purpose of the experimental section is to illustrate the properties of the different methods for computing users X  pri-vacy scores and pinpoint their advantages and disadvantages. From the data-analysis point of view, our experiments with real data show interesting facts about the users X  behavior. A. Datasets
We start by giving a brief description of the synthetic and real-world datasets we used for our experiments.

Dichotomous Synthetic dataset: This dataset consists of a dichotomous n  X  N response matrix R S where the rows correspond to items and the columns correspond to users. The response matrix R S is generated as follows: for each item i , of a total of n =30 items, we pick parameters  X  i and  X  i uniformly at random from intervals (0 , 2) and [6 , 14] respectively. We assume that the items are sorted based on their  X  i values, i.e.,  X  1 &lt; X  2 &lt; ... &lt;  X  n .Next, K = 30 different attitude values are picked uniformly at random from the real interval [6 , 14] . Each such attitude value  X  is associated with a group of 200 users (all 200 users in a group have attitude  X  g ). Let the groups be sorted so that  X  1 &lt; X  2 &lt;  X  X  X  &lt; X  K . For every group F g ,user j  X  item i ,weset R s ( i, j )=1 with probability Prob R ( i, j )=
Survey dataset: This dataset consists of the data we collected by contacting an online survey 7 . The goal of the survey is to collect users X  inform ation-sharing preferences. Given a list of profile items that span a large spectrum of one X  X  personal life ( e.g. , name, gender, birthday, political views, interests, address, phone number, degree, job, etc.), the users are asked to specify the extent they want to share each item with others. The privacy levels a user can allocate to items are { 0 , 1 , 2 , 3 , 4 } .Value 0 means that a user wants to share this item with no one ; 1 means that he wants to share it with some of his immediate friends , 2 with all of his immediate friends , 3 with all immediate friends and friends of friends ,and 4 with everyone . This setting simulates most of the privacy-setting options used in real online social networks. Along with users X  privacy settings, we also collect information about their location, educational background, age etc. The survey spans 49 profile items. We have received 153 complete responses from 18 c ountries/political regions. Among the participants, 53 . 3% are male and 46 . 7% are female, 75 . 4% are in the age of 23 to 39, 91 . 6% hold a college degree or higher, and 76 . 0% spend 4 hours or more everyday surfing online.

From the Survey dataset we construct a polytomous response matrix R (with =4 ). This matrix contains the privacy levels picked by the 153 respondents for each one of the 49 items. We also construct four dichotomous matrices R  X  k with k = { 1 , 2 , 3 , 4 } as follows: R  X  if R ( i, j )  X  k ,and0otherwise.
 B. Experiments with the Dichotomous Synthetic data
The goal of the experiments in this section is to demon-strate the group invariance property of the IRT model using the Dichotomous Synthetic dataset.

We conduct the experiment as follows: first, we cluster the 6000 users into 3 groups F L =  X  g =1 ... 10 F g , F M  X  cluster consists of all users in the 10 lowest-attitude groups F ,...,F 10 , the second cluster consists of all users in the 10 medium-attitude groups and the third one consists of all users in the 10 highest-attitude groups. Given users X  attitudes assigned in the data-generation, we estimate three sets of item parameters  X  L i =  X  L i , X  L i ,  X  M i =  X  M i , X  M  X  i =  X  using Equation (5) with input response matrix that only contains the columns of R S associated with the users in F F
M and F H , respectively. We use Equation (5) to compute estimates  X  all i =  X  all i , X  all i using the whole response matrix R
Figure 2(a) shows the estimated sensitivity values of the items. Since the data was generated using IRT model, the true sensitivity parameters  X  i for each item are also known (and plotted). The x-axis of the figure shows the different items sorted in increasing order of their true  X  i values. It can be seen that for the majority of the items, the estimated to the true  X  i value. This indicates one of the interesting features of IRT that item parameters are not dependent upon the attitude level of the users responding to the item. Thus, the item parameters are what is known as group invariance . The validity of this property is demonstrated in Frank Baker X  X  book [2] and online tutorial: http://echo.edres.org: 8080/irt/baker/chapter3.pdf. At an intuitive level, since the same item was administrated to all groups, each of the three parameter estimation p rocesses was dealing with a segment of the same underlying item characteristic curve (see Figure 1). Consequently, the item parameters yielded by the three estimations should be identical.

It should be noted that even though the item parameters are group invariant, this does not mean that in practice values of the same item parameter estimated from different groups of users will always be exactly the same. The obtained values will be subject to variation due to group size, the goodness-of-fit of the ICC curve to the data. Nevertheless, the estimated values should be in  X  X he same ballpark X . This explains why in Figure 2(a) there are some items for which the estimated parameters deviate from the true one more.
We repeat the same experiment for the Naive model. That is, for each item we estimate sensitivities  X  L i ,  X  M i  X  i using the Naive approach (Section VII). Figure 2(b) shows the obtained estimates. The plot demonstrates that the Naive computation of sensitivity does not have the group-invariance property. For most of the items, sensitivity  X  obtained from users with low attitude levels ( i.e. , conserva-tive, introvert) are much higher than the  X  all i estimates since these users rarely share anything, whereas  X  H i obtained from users with high attitude levels ( i.e. , careless, extrovert) are much lower than  X  all i .

Note that since sensitivities estimated by Naive and IRT are not in the same scale, one should consider the relative error instead of absolute error when comparing the results in Figures 2(a) and 2(b).
 C. Experiments with the Survey data The goal of the experiments in this section is to use the Survey dataset to show 1) IRT is a good model for the real-world data, whereas Naive is not; and 2) IRT model provides us an interesting estimation of the sensitivity of information being shared in online social networks. 1) Testing  X  2 goodness-of-fit: We start by illustrating that the IRT model fits the real-world data very well, whereas the Naive model does not. For that we use  X  2 goodness-of-fit test , a commonly-used test for accepting or rejecting the null hypothesis that a data sample comes from a specific distribution. Our input data consists of dichotomous matrix R k ( k
First we test whether IRT model is a good model for data in each R  X  k . We test this hypothesis as follows: first use the EM algorithm to estimate both items X  parameters and users X  attitudes. Then, we use an 1-dimensional dynamic-programming algorithm to group the users based on their estimated attitudes. The mean attitude of a group F g serves as the group attitude  X  g . Next, for each matrix R  X  k and each item i in the matrix, we compute In the above equation, f g is the number of users in group F p ig (resp.  X  p ig ) is the expected (resp. observed) proportion of users in F g that set R  X  k ( i, j )=1 . Finally, q ig =1 (and  X  q ig =1  X   X  p ig ). For the IRT model p ig = P i and it is computed using Equation (2) for group attitude  X  g and item parameters estimated by EM .ForIRT,the test statistic follows, approximately, a  X  2 -distribution with ( K  X  2) degrees of freedom since there are 2 estimated parameters.

For testing whether the responses in R  X  k can be described by the Naive model we follow a similar procedure. First we compute, for each user, the proportion of items that the user sets equal to 1 in R  X  k . This value serves as the user X  X   X  X seudo-attitude X . Then we construct K groups of users F ,...,F K , using an 1-dimensional dynamic-programming algorithm based on these attitude values. Given this group-ing, the  X  2 statistic is computed again. The only difference here is that where | R  X  k in R  X  k ,and | R  X  j k | denotes the number of items being shared by a user j in R  X  k . For Naive, the test statistic approximately follows a  X  2 -distribution with ( K  X  1) degrees of freedom.
Table I shows the number of items for which the null hypothesis that their responses follow the IRT or Naive model is rejected. We show results for dichotomous matrices R 1 , R cases, the null hypothesis that the responses follow the Naive model were rejected for all 49 items. On the other hand, the null hypothesis that responses follow the IRT model was rejected only for small number of items in all configurations. This gives strong evidence that the IRT model fits the real data better. All results reported here are for confidence level . 95 .
 2) Sensitivity of profile items: In Figure 3 we visualize, using a tag cloud, the sensitivity of the profile items used in our survey. The evaluation of sensitivity values is done using the EM algorithm (Algorithm 1) with input the dichotomous response matrix R  X  2 . The larger the fonts used to represent a profile item in the tag cloud, the higher its estimated sensitivity value. It is easily observed that Mother X  X  Maiden Name is the most sensitive item, while one X  X  Gender ,which locates just right above the letter  X  X  X  of  X  X other X  has the lowest sensitivity; too small to be visually identified. 3) Geographic distribution of privacy scores: Here we present some interesting findings we get by further analyzing the Survey dataset. We compute the the privacy scores of the 153 respondents using the polytomous IRT-based computations (Section VI-A).

After evaluating the privacy scores of individuals using as input the whole response matrix R ,wegroupthere-spondents based on their geographic location. Figures 4(a) and 4(b) show the average values of the users X  Pr IRT scores and attitudes per location. The results indicate that people from North America and Europe have higher privacy scores than people from Asia and Australia. The privacy scores and the attitude values are highly correlated. This experimental finding indicates that people from North Amer-ica and Europe are more comfortable to reveal personal information on the social networks they participate. This can be either a result of inherent attitude or social pressure. Since online social-networking is more widespread in these regions, one can assume that people in North America and Europe succumb to the social pressure to reveal things about themselves online in order to appear  X  X ool X  and become popular.

We have presented models and algorithms for computing the privacy score of users in online social networks. Our methodology takes into account th e privacy settings of users with respect to their profile items and has its mathematical underpinnings in Item Response Theory. We have described the bases of our approach and presented a set of experiments on synthetic and real data that highlight the properties of our model and the current trends in users behavior. Our framework gives a new way of dealing with privacy in social networks. Although it is by no means perfect, it opens door to various possib ilities of privacy management that will make our online environment more comfortable.
 The authors would like to thank Tyrone Grandison and Michael Maximilien for their valuable comments and sug-gestions on this work.

