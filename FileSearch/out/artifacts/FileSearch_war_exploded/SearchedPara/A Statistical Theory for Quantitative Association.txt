
Association Rules. The goal of data mining is to extract higher level information from an abundance of raw data. Association rules are a key tool used for this purpose. An association rule is a rule of the form 
X + Y, where X and Y are events, which states that when X occurs in the database so does Y, with a certain probability (coined the confidence of the rule). A well-known application of association rules is in market basket data analysis. 
The problem of mining association rules was first introduced by Agrawal et. al. in [l], and later broadened by Agrawai et. al. in [2], for the case of databases consisting of categorical attributes alone. Categorical association rules are rules where the events X and Y, leading. For example, the rule  X  X eight E [lOOcm,150cm] =s age E [0,14] (70%) X  may be true even though few, if any, children under the age of one are 100cm tall (see Section 4.3 for real-world examples). In addition, the [7] definition often results in an exponential blowup of the number of rules, as the left-hand side of any given rule can always be enlarged. Hence, they place an a priori restriction on the maximum support of a rule (ma~~p), partially solving this problem. Finally, the discretiza-tion employed in the mining algorithm results in loss of information. In particular, the algorithm can only approximate the best rules (see [7] for details). 
Other work on this problem includes Zhang et. al. in [lo] who use clustering methods to improve the partitioning of the quantitative attributes in the algorithm. Fukuda et. al. in [4] and Yoda et. al. in [9] also worked on the quantitative associations problem. However, their work is related to a different version of the problem and is focused more on prediction, rather than association rules. A New Definition. In this paper we introduce a new definition of quantitative association rules, based on the distribution of values of the quantitative attributes. The new definition is a natural generalization of the categorical definition, when interpreted in the proper statistical terms. An example of a rule according to our new definition would be: saying that the average wage for females is $7.90 dollars per hour. This rule is interesting as it reveals a group of people earning a significantly lower than average wage ($9.02 p/hr). Our definition captures the notion of finding  X  X nteresting behavior X , generating rules revealing extraordinary phenomena. We use accepted statistical tests to confirm the validity of the discovered rules. We present algorithms that do not use discretization, but rather view the quantitative attributes as continuous. Finally, we validate our definition through an in-depth evaluation of results. 
Before giving our new definition, it would be helpful to backtrack a little and discuss the goal and structure of association rules in general. Association rules are designed to help us discover  X  X nteresting X  phenomena or behavior in databases. This is accomplished by locating sets of transactions containing unexpected behavior. Each rule is comprised of a left-hand side and a right-hand side: l The left-hand side of the rule is a description of a l The right-hand side of the rule is a description of 
In the coming sections we will present exact definitions, algorithms and evaluation for our new concepts. How-ever, before delving into these details, we first present some sample results, obtained from an actual database. 
We applied our algorithm to a database called De-terminants of Wages from the 1985 Current Population 
Survey in the United States (the database may be found at http://lib.stat.cmu.edu/datasets). The database con-tains 534 transactions and 11 attributes (7 categorical and 4 quantitative). Here are some of the rules dis-covered with 95% statistical confidence. Note that the mean wage overall is $9.02 p/hr. 
The second rule is a sub-rule (defined in section 2.2) of the first and shows that although females overall are paid lower wages, in the South of the USA the situation is much worse. We hope that things have improved somewhat since 1985. The other side of the coin of these two rules is the next rule also found: 
Other rules linked Education (as in years of formal education) to Wage and justify the argument that on average, education improves earning power: 
In a different and somewhat unexpected direction, a rule connecting Education to Age was found. The mean age of the population was 37 years. 
This rule shows us that those with very little schooling are on average far older, a sign of positive progress in so-ciety. Usually, most research on this database would be limited to factors affecting a person X  X  wage. Through our data-mining technique we exposed interesting in-formation which we would not initially have thought to look for. The Wages database provided us with inter-esting results. Clearly, this gives only a flavor of the rules. A rigorous evaluation of the quality of the rules discovered is provided in Section 4. 
Outline. In the next section we develop a formal framework for our definition. In section 3 we present efficient algorithms for some cases of quantitative rules and demonstrate the computational viability of mining Significance. A rule is only interesting if the mean for the subset TX is significantly different from the rest and is therefore unexpected. We therefore compare the mean in TX to the mean of the complement, i.e. D -TX. Note, however, that although the two means may be numerically different in the database, we may not have any statistical evidence to infer a difference in the real populations. Thus, we use statistical tests to establish the significance level of the difference. In the case of mean-values we use the standard Z-test, to establish significance of the inequality of the means. We test the hypothesis that the mean of the two subsets are not equal (the null hypothesis) against the hypothesis claiming a difference of means. A rule is then considered significant if the null hypothesis is rejected with confidence above a set threshold (usually 95%). Formally, we say that MeanJ(Tx) is significantly different from MeanJ(Tr), denoted MeunJ(Tx) $ MeanJ(Ty), if for every e E J the means of attribute e in TX and Ty are different, based on the appropriate statistical test. 
We are now ready to define mean-based quantitative association rules. Definition 1 A (mean-based) categorical to quantita-tive association rule is of the form X + MeanJ(Tx), EC x C), J is a set of quantitative attributes (J s EQ), and MeanJ(Tx) $ MeunJ(D -TX). Minimum Difference. Sometimes, finding popula-tions for which the means are merely different does not lead to interesting information. If we were to discover, for example, a group of people with life expectancy three days more than the overall population, it may not be of interest to us even if it passes a statistical test. We therefore allow a user-defined minimum dif-ference parameter, denoted mindif, and we say that Meani $ Meani if there is statistical support for inferring that jMeuni(Tx)-Meani &gt; mindif. other Distribution Measures. The rules defined here provide a tool to discover interesting behavior of the distribution with regards to its mean value. An analogous definition can be provided given any other measure of the distribution, e.g. variance, median. For a given measure M (e.g. M = Variance), an M-based association rule is of the form X + Mi(Tx). The rest of the definitions carry over directly from the basic mean-based rules by changing Mean to M throughout. For a given measure M, significance of inequality, denoted MJ(Tx) $ MJ(TY), is defined with the proper test for this measure, e.g. the F-test for variance. The algorithm outlined in section 3.2 is correct for any measure and has been implemented for variance-based rules as well. the middle and obtain an interval which is not above average. This property ensures that we do not have non-interesting regions appended to the edge of the rule (the third rule in the above example is reducible). A rule is maximal if we cannot enlarge the interval [a, b] either to the right or the left and still remain with an irreducible rule with above average distribution. Maximal rules are therefore the largest  X  X ood X  rules and provide the most concise presentation (the second rule in the above example is not maximal). 
Formally, we say that the rule is irreducible if for any both Meanj(Ty) and Meanj(Tz) are above average. setting Y = (e,c, b) (Y = ( e,a,c)) then Meanj(Ty) is either not above average or reducible. The definition for below average rules is analogous. A rule of this type must be both maximal and irreducible. 
We now have all the necessary concepts to define rules of this type: 
Definition 2 A (mean-based) quantitative to quantita-tive association rule is a maximal and irreducible rule of the form X 3 Meanj(Tx) where X is a profile for a single quantitative attribute (X E EQ x R x R) , j is a quantitative attribute (j E EQ), and Meanj(Tx) $ Meanj(D -TX). 
Remark. The profile of a rule of this type is in the form of a range. The profile classifies the transactions for which the rule refers to and a range provides a clear partition into those belonging and those not. 
For example, by saying that a phenomenon occurs for people between the ages of 10 and 20, it is clear to whom it applies. However, if we were to say that a phenomenon occurs for people of average age 15, then it would not be clear exactly what population this rule is based on and who it describes. 2.2 Sub-Rules 
We have so far provided a framework for defining rules, and definitions for two important categories of them. 
However, not all rules are desired. We are interested in finding the key factors of extraordinary behavior of a population. Consider the following set of rules, where the overall life expectancy is 70 years: Bernoulli random variable, the mean of  X  X  given X X  is exactly the confidence of the rule. The rules defined by [3] are defined in the same way with a different signifi-cance measure (a statistical X2 test is used). Efficient algorithms for finding quantitative association rules are provided for two types of rules: 1. Rules X + MeanJ(Tx) where both X and J 2. Rules X + MJ(T,Y.) where X C EC (only cate-3.1 Finding rules from one Numerical to In this section we introduce an algorithm that finds rules based on the mean distribution measure. Our algorithm finds rules between two given quantitative attributes. This is then applied for every pair, thereby obtaining all rules of this kind. We enable the user to specify a minimum support parameter here. This is not necessary at all for computational reasons as will be shown later. However, rules based on very few data points may be inconsequential, even if they are true. Furthermore, it can be shown that multiple hypothesis testing with a very small minimum support can lead to a high error (see section 5). Algorithm Motivation. Let i and j be a pair of quantitative attributes. Note that if we sort the database by attribute i, then any above or below-average continuous region of values in j is a rule (provided it passes the necessary statistical test). This is because attribute i is sorted, and therefore any continuous region is a range. However, we must also ensure that the rule is irreducible and maximal. 
Our algorithm is based on the following simple idea: if the regions [a, b] and [b, c] are both above or below average, then so too is the region [a, c]. It is completely symmetrical to search for above or below average regions, we will therefore refer only to above-average from now on. Further note than when we say  X  X bove-average X  we mean above the overall mean plus mindif The Window Procedure. The following data-driven procedure, called  X  X indow X , accepts as input an array of values and the average of the values in the array (plus mindifi. The input array is the array of values of attribute j, sorted by i. We execute a single pass to find all rules from i to j. The procedure works with two windows or regions: A and B. A is an irreducible above-average region (this remains invariant throughout). B Figure 1: Window Procedure for finding  X  X umerical + Numerical X  Rules We note that with very large databases, this sort may take considerably longer as it needs to be executed in secondary memory. Due to lack of space in this abstract we provide an outline for this algorithm only. We note that the left-hand side of a rule in this case is essentially a frequent set and this property forms the basis of our algorithm. Algorithm Outline. The algorithm has three distinct stages: 1. Find all frequent sets of categorical items only, using 2. For all quantitative attributes, calculate the distri-3. Find all non-contained rules and sub-rules. For 
We note that the ideas in sections 3.1 and 3.2 may be combined in order to find rules with profiles containing many categorical and a single numerical attribute. For a given frequent set X, we run Window on TX. We The Evaluation Results. The results of the quanti-tative evaluation by the domain expert are summarized in Table 1 (the numbers in brackets are the number of rules that would not have otherwise been found). 
Overall we see that 36% of rules were interesting or very interesting, 26% of all rules would not have been found using the standard hypothesis checking model. 7% of the rules were graded very interesting and would not have otherwise been found. This is vemJ high for an automated tool and the result is critical to the usefulness of the method. Users are unlikely to use tools which provide interesting results hidden amongst endless junk. Rule Complexity. If we further look at the break-down of interesting rules within the Categorical =s-Nu-merical rules, more than 50% of rules with one cate-gorical attribute in the profile were graded interesting! On the other hand, those with more than one attribute in the profile were judged not-interesting in 86% of the cases. This is most likely due to the difficulty in under-standing complex rules and shows that most interesting rules have simple profiles. This supports our claim that our algorithms cover most of the interesting cases. A Qualitative Evaluation. The strength of our technique can be seen by viewing a number of results judged to be interesting by our evaluator. Due to lack of space, we bring only one example. Some participants in the study were given a source text and were asked to base their essay on it and others were not. We present a suprising rule regarding the effect of these source texts. Our evaluator judged the following rule to be interesting and claimed that he would not have found it using standard statistical tools: This rule tells us that Russians who were not presented with a source text used the word the well below average. It is a known fact to Linguists that the Russian language has no definite article. Therefore, we are not surprised to see that Russians use the word the less. However, this was not inferred from the database. Rather, we found that only when the participants had no source text to base on, they fell back on their Russian habit of not using a definite article. On the basis of this rule 4.2 Scalability 
We also checked the scalability of our algorithms. For this we used Synthetic Data Sets (created by the IBM 
Quest Synthetic Data Generation Code). We created a database with 9 attributes, 3 categorical, 6 quantitative. 
We used a minimum support of 40. The results are depicted in Table 2 below. transactions time Window rules found 4.3 A Comparative Evaluation of [7] 
Remember that in [7] a quantitative association rule is defined as a rule X + Y with a certain support and confidence, where X and Y contain categorical items or numerical ranges. Their algorithm is based on mapping the problem to the categorical case by way of discretization, finding all association rules and then filtering superfluous nested rules. We now present an evaluation (rendered by our expert) of the rules generated by [7] and examples of some of the problems. 
We used the Linguistics dat,abase as our basis for the evaluation. 
A Quantitative Evaluation. In order to create a fair comparison of [7] rules to ours, we limited the search for rules with one attribute on each side of the rule. This was so we could choose a relatively high maximum support, low minimum support and low K (otherwise we would encounter extreme computational difficulties, especially with a database of 42 attributes). 
We also wished to limit the number of rules we found to something that could be realistically evaluated. In order to do this, we chose 10 pairs of attributes uniformly randomly (with the condition that at least one of the attributes was quantitative) and obtained rules from these pairs only. We then evaluated these rules as a sample of the set of all rules of this type. 
After some fine tuning we settled on the following input parameters: 
The minimum support was chosen to be the same as for our algorithm and we chose K=9 in order to reduce the number of rules found. length in the entire study was 172, for  X  X dvanced X  was 108 (way below average, i.e. low proficiency) and for  X  X luent X  was 222 (a strong sign that this group is well skilled). This example strongly confirms our claim that describing a numerical distribution with a range and probability may be very misleading. Not only may we not find interesting behavior, we may be presented with rules which lead us to erroneous conclusions. We introduced a general definition for quantitative association rules in the form of  X  X rofile + Significant Distribution X . However, we developed the definitions and algorithms necessary for rules of two specific cases only. These cases proved to be very useful and are important categories of rules. However, we would like to see a truly general definition of quantitative association rules, combining categorical attributes in the distribution as well. We note the difficulty in expanding the profile to include two quantitative attributes. Firstly, it is easily shown that it is not possible to use one-dimensional rules in order to find all two-dimensional rules. Secondly, a conceptual difficulty arises in that the union of two overlapping, above-average rules is not necessarily above-average. This may lead to the undesirable property of many overlapping rules. Multiple Hypothesis Testing. A general problem in data mining is that of multiple hypothesis testing. In statistical terms, these are called multiple comparison procedures and results are likely to be obtained even on random databases. We ran the following experiment in this context. We applied independent random permutations on each column of our test database (Linguistics) obtaining a database with no correlations, yet with the same original distributions for each attribute separately. This was done 50 times obtaining 50 random databases. We then ran our algorithm on these databases and counted the number of rules obtained. We found that the random variable of the number of rules has a Poisson distribution with mean 16.7. Thus the probability of obtaining more than 30 rules if correlations do not really exist in the database is less than 0.001. As we found more than 300 rules this is overwhelming evidence that the results are not random. However, we stress that in-depth statistical research is still required in order to provide a strong theoretical background for these types of data-mining procedures. Other Statistical Tests. An interesting question for future work is that of the effect of the specific statistical tests used. We used the Z-test as it is the most natural mean test, especially as we need not assume anything about the distribution of the values. However, the 
