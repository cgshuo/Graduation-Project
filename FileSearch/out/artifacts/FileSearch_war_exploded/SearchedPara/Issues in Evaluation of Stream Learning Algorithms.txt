 Learning from data streams is a research area of increasing importance. Nowadays, several stream learning algorithms have been developed. Most of them learn decision models that continuously evolve over time, run in resource-aware environments, detect and react to changes in the environ-ment generating data. One important issue, not yet con-veniently addressed, is the design of experimental work to evaluate and compare decision models that evolve over time. There are no golden standards for assessing performance in non-stationary environments. This paper proposes a gen-eral framework for assessing predictive stream learning algo-rithms. We defend the use of Predictive Sequential methods for error estimate  X  the prequential error. The prequential error allows us to monitor the evolution of the performance of models that evolve over time. Nevertheless, it is known to be a pessimistic estimator in comparison to holdout es-timates. To obtain more reliable estimators we need some forgetting mechanism. Two viable alternatives are: sliding windows and fading factors. We observe that the prequen-tial error converges to an holdout estimator when estimated over a sliding window or using fading factors. We present illustrative examples of the use of prequential error estima-tors, using fading factors, for the tasks of: i) assessing per-formance of a learning algorithm; ii) comparing learning algorithms; iii) hypothesis testing using McNemar test; and iv) change detection using Page-Hinkley test. In these tasks, the prequential error estimated using fading factors provide reliable estimators. In comparison to sliding windows, fad-ing factors are faster and memory-less, a requirement for streaming applications. This paper is a contribution to a discussion in the good-practices on performance assessment when learning dynamic models that evolve over time. H.2.8 [ Database Management ]: Database applications X  data mining ; I.2.6 [ Artificial Intelligence ]: Learning X  classifiers design and evaluation Copyright 2009 ACM 978-1-60558-495-9/09/06 ... $ 5.00. Experimentation, Measurement, Performance Data Streams, Evaluation Design, Concept Drift
The last twenty years or so have witnessed large progress in machine learning and in its capability to handle real-world applications. Nevertheless, machine learning so far has mostly centered on one-shot data analysis from homo-geneous and stationary data, and on centralized algorithms. Most of Machine Learning and Data Mining approaches as-sume that examples are independent, identically distributed and generated from a stationary distribution. A large num-ber of learning algorithms assume that computational re-sources are unlimited, e.g., data fits in main memory. In that context, standard data mining techniques use finite training sets and generates static models. Nowadays we are faced with tremendous amount of distributed data that could be generated from the ever increasing number of smart devices. In most cases, this data is transient, and may not even be stored permanently.

Our ability to collect data is changing dramatically. Nowa-days, computers and small devices send data to other com-puters. We are faced with the presence of distributed sources of detailed data. Data continuously flow, eventually at high-speed, generated from non-stationary processes. Examples of data mining applications that are faced with this scenario include sensor networks, social networks, user modeling, ra-dio frequency identification, web mining, scientific data, fi-nancial data, etc.

Most recent learning algorithms [6, 1, 10, 20, 15, 12] main-tain a decision model that continuously evolve over time, taking into account that the environment is non-stationary and computational resources are limited. Examples of pub-lic available software for learning from data streams include: the VFML [19] toolkit for mining high-speed time-changing data streams, the MOA [21] system for learning from mas-sive data sets, Rapid-Miner [24] a data mining system with plug-in for stream processing, etc. For illustrative purposes, consider a sensor network. Sensors are geographically dis-tributed and produce high-speed distributed data streams. They measure some quantity of interest, and we can be inter-ested in predicting that quantity for different time horizons. Suppose that at time t our predictive model made a pre-diction  X  y t + k for time t + k , where k is the desired horizon forecast. Later on, at time t + k the sensor measures the quantity of interest y t + k , then we can estimate the loss of our prediction L ( X  y t + k ,y t + k ).

Although the increasing number of streaming learning al-gorithms, the metrics and the design of experiments for as-sessing the quality of learning models is still an open issue. The main difficulties are: In a referenced paper, T. Diettrich [9] proposes a straightfor-ward technique to evaluate learning algorithms when data is abundant:  X  X earn a classifier from a large enough training set and apply the classifier to a large enough test set. X  Data streams are open-ended. This could facilitate the evaluation methodologies, because we have train and test sets as large as desired. The problem we address in this work is: Is this sampling strategy viable in the streaming scenario?
In this work we argue that the answer is no . Two as-pects, in the emerging applications and learning algorithms that have strong impact in the evaluation methodologies are the continuous evolution of decision models and the non-stationary nature of data streams. The approach we propose is based on sequential analysis . Sequential analysis refers to the body of statistical theory and methods where the sample size may depend in a random manner on the accumulating data [16]. The paper is organized as follows. The next sec-tion presents an overview of the main lines presented in the literature, in learning from data streams and the most com-mon strategies for performance assessment. In Section 3 we discuss the pros and cons of the prequential statistics in re-lation to the hold-out sampling strategy. Section 4 presents the main contributions of the paper, while the last section concludes the exposition, presenting the lessons learned.
G. Hulten and P. Domingos [18] identify desirable prop-erties of learning systems for efficient mining continuous, high-volume, open-ended data streams:
From this desiderata, we can identify 3 dimensions that influence the learning process: Figure 1: Performance evolution of VFDT in a web-mining problem. The accuracy (in percentage) in-creases for increasing number of training examples. For illustrative purposes we present the accuracy of C4.5 using the maximum number of examples that fit in memory (100k examples).
 Work Eval. Mem Examples Drift
VFDT holdout Yes Artif 1M 50k No CVFDT holdout Yes Artif 1M Yes Yes VFDTc holdout No Artif 1M 250k No UFFT holdout No Artif 1.5M 250k Yes FACIL holdout Yes Artif 1M 100k Yes MOA holdout Yes Artif 1G No ANB Prequen No Artif Yes Table 1: Resume of evaluation methods in stream mining literature. In this work we focus in the generalization power of the learning algorithm, although we recognize that the two first dimensions have direct impact in the generalization power of the learned model.

We are in presence of a potentially infinite number of examples. Is this fact relevant for learning? Do we need so many data points? Sampling a large training set is not enough? Figure 1 intends to provide useful information to answer these questions, showing the accuracy X  X  evolution of VFDT [10] in a web-mining problem. One observes, in this particular problem, a rapid increase of the accuracy with the number of examples; using more than 1e+07 examples will not affect the accuracy, it will remain stable near 80%.
The fact that decision models evolve over time has strong implications in the evaluation techniques assessing the effec-tiveness of the learning process. Another relevant aspect is the resilience to overfitting: each example is processed once.
A brief look at the stream mining literature shows the diversity of evaluation methods. Table 1 resumes the evalu-ation methods found in a diverse set of well-known stream learning algorithms. The algorithms under analysis are de-scribed in [10, 20, 15, 14, 11, 21, 4] and are presented in that order.
A key point in any intelligent system is the evaluation methodology. Learning systems generate compact represen-tations of what is being observed. They should be able to improve with experience and continuously self-modify their internal state. Their representation of the world is approxi-mate. How approximate is the representation of the world? Evaluation is used in two contexts: inside the learning sys-tem to assess hypothesis, and as a wrapper over the learning system to estimate the applicability of a particular algorithm in a given problem. Three fundamental aspects are:
For predictive learning tasks (classification and regres-sion) the learning goal is to induce a function  X  y = f ( ~x ). The most relevant dimension is the generalization error . It is an estimator of the difference between  X  f and the unknown f , and an estimate of the loss that can be expected when applying the model to future examples.
One aspect in the design of experiments that has not been conveniently addressed, is that learning algorithms run in computational devices that have limited computational power. For example, existing learning algorithms assume that data fits in memory; a prohibit assumption in the pres-ence of open-ended streams. This issue becomes much more relevant when data analysis must be done in situ . An illus-trative example is the case of sensor networks, where data flows at high-speed and computational resources are quite limited.

Very few algorithms address the bounded memory con-strain. A notable exception is VFDT [10] that can save memory by freezing less promising leaves whenever memory reaches a limit. VFDT monitors the available memory and prune leaves (where sufficient statistics are stored) depend-ing on recent accuracy. An interesting framework to eval-uate learning algorithms under memory constrains appears in [21]. The author proposes 3 environments using increas-ing memory, for evaluating stream mining algorithms: The memory management is more relevant for non-parametric decision models like decision trees or support vector ma-chines because the number of free parameters evolve with the number of training examples. For other type of mod-els, like linear models that typically depend on the number of attributes, memory management is not so problematic in the streaming context because the size of the model does not depend on the number of examples. In [21] the authors defend that general purpose streaming algorithms should be evaluated in the 3 mentioned scenarios.

In batch learning using finite training sets, cross-validation and variants (leave-one-out, bootstrap) are the standard methods to evaluate learning systems. Cross-validation is appropriate for restricted size datasets, generated by sta-tionary distributions, and assuming that examples are in-dependent. In data streams contexts, where data is poten-tially infinite, the distribution generating examples and the decision models evolve over time, cross-validation and other sampling strategies are not applicable. Research communi-ties and users need other evaluation strategies.
To evaluate a learning model in a stream context, two viable alternatives, presented in the literature, are:
We should point out that, in the prequential framework, we do not need to know the true value y i , for all points in the stream. The framework can be used in situations of limited feedback, by computing the loss function and S i for points where y i is known.

The mean loss is given by: M = 1 n  X  S . For any loss function, we can estimate a confidence interval for the prob-ability of error, M  X   X  , using Chernoff bound [5]: where  X  is a user defined confidence level. In the case of bounded loss functions, like the 0-1 loss, the Hoeffding bound [17] can be used: where R is the range of the random variable. Both bounds use the sum of independent random variables and give a relative or absolute approximation of the deviation of X from its expectation. They are independent of the distribution of the random variable.
Prequential evaluation provides a learning curve that mon-itors the evolution of learning as a process. Using holdout evaluation, we can obtain a similar curve by applying, at regular time intervals, the current model to the holdout set. Both estimates can be affected by the order of the examples. Moreover, it is known that the prequential estimator is pes-simistic: under the same conditions it will report somewhat higher errors (see Figure 2). The prequential error estimated over all the stream might be strong influenced by the first part of the error sequence, when few examples have been Figure 2: Comparison of error evolution as esti-mated by holdout and prequential strategies, in a stationary stream (Waveform data set). The learn-ing algorithm is VFDT. used for train the classifier. This observation leads to the following hypothesis: compute the prequential error using a forgetting mechanism, like time windows of the most recent observed errors or fading factors that weight previous er-rors using a decay factor. The following sections show that the error estimates using the proposed forgetting methods converge to the holdout estimator.
The objective of this experiment is to study convergence properties of the prequential statistics using sliding window error estimates. The learning algorithm is VFDT as im-plemented in VFML [19]. The experimental work has been done using the Waveform [3] dataset, because the Bayes-error is known: 14%. This is a three class problem defined by 21 numerical attributes.

Figure 3 (top) plots the holdout error, the prequential error, and the prequential error estimated using sliding-windows of different size. All the plots are means from 30 runs of VFDT on datasets generated with different seeds. The most relevant fact is that the window-size does not matter too much: the prequential error estimated over a sliding-window always converge fast to the holdout estimate. Figure 3 (bottom) plots the holdout error, the prequential error, the prequential error estimated using sliding-window (50k), and the prequential error estimated using fading fac-tor (0.975). Again, the prequential error estimated using fading factor converge fast to holdout estimate.
In this section we discuss methods to compare the perfor-mance of two algorithms ( A and B ) in a stream. Our goal is to distinguish between random and non-random differences in experimental results.

Let S A i and S B i be the sequences of the prequential ac-cumulated loss for each algorithm. A useful statistic that can be used with almost any loss function is: Q i ( A,B ) = tive performance of both models, while its value shows the strength of the differences. In a experimental study using real data from a electrical load-demand forecast problem, Figure 3: Comparison of error evolution between holdout, prequential and prequential over sliding windows of different sizes (top). The bottom figure present similar results using also fading factors. plotted in Figure 4, Q i reflects the overall tendency but ex-hibit long term influences and is not able to fast capture when a model is in a recovering phase. Two feasible al-ternatives are sliding windows, with the known problems of deciding the window-size, and fading-factors. Both methods have been used for blind adaptation, e.g. without explicit change detection, of decision models in drift scenarios [22, 23]. The formula for using fading factors with the Q i statis-tic is: It is interesting to observe that these two alternatives exhibit similar plots (see Figure 5).

The fading factors are multiplicative, corresponding to an exponential forgetting. At time-stamp t the weight of ex-ample t  X  k is  X  k . For example, using  X  = 0 . 995 the weight associated with the first term after 3000 examples is 2 . 9 E  X  7. In general, assuming that we can ignore the examples with weights less than , an upper bound for k (e.g. the set of  X  X mportant X  examples) is log ( ) /log (  X  ). The fading factors are memoryless, an important property in streaming scenar-ios. This is a strong advantage over sliding-windows that require maintaining in memory all the observations inside the window. Figure 4: Comparison between two different neural-networks topologies in a electrical load-demand problem. The loss function is the mean-squared er-ror . The figure plots the evolution of the Q i statis-tic. The sign of Q i is always negative, illustrating the overall advantage of one method over the other.
For classification problems, one of the most used tests is the McNemar test 1 . To be able to apply this test we only need to compute two quantities n i,j : n 0 , 1 denotes the num-ber of examples misclassified by A and not by B, whereas n 1 , 0 denotes the number of examples misclassified by B and not by A. The contingency table can be updated on the fly, which is a desirable property in mining high-speed data streams. The statistic M = sign ( n 0 , 1  X  n 1 , 0 )  X  ( n has a  X  2 distribution with 1 degree of freedom. For a con-fidence level of 0.99, the null hypothesis is rejected if the statistic is greater than 6.635 [9].
We have used the dataset SEA concepts [27], a benchmark problem for concept drift. Figure 6 (top panel) shows the evolution of the error rate of two naive-Bayes variants: a standard one and a variant that detects and relearn a new decision model whenever drift is detected. The McNemar test was performed to compare both algorithms. The bot-tom panel shows the evolution of the statistic test computed over all the stream. As it can be observed, once this statis-tic overcomes the threshold value (6.635), it never decreases below it, which is not informative about the dynamics of the process under study. Again, the problem is the long term influences verified with the Q i statistic. It is well known, that the power of statistical tests, the probability of signal-ing differences where they do not exist, are highly affected by data length. Data streams are potentially unbounded, which might increase the number of Type II errors.
To overthrow this drawback, and since the fading factors are memoryless and proves to exhibit similar behaviors to sliding windows, we compute this statistic test using differ-ent windows size and fading factors. Figure 7 illustrates a comparison on the evolution of a signed McNemar statistic between the two algorithms, computed over a sliding window
We do not argue that this is the most appropriate test for comparing classifiers. An in depth analysis on statistical tests to compare classifiers in batch scenario appears in [8]. Figure 5: Plot of the Q i statistic over a sliding win-dow of 250 examples (top). The bottom figure plots the Q i statistic using a fading factor of  X  = 0 . 995 . Figure 6: The evolution of signed McNemar statistic between two algorithms. Vertical dashed lines indi-cates drift in data, and vertical solid lines indicates when drift was detected. The top panel shows the evolution of the error rate of two naive-Bayes vari-ants: a standard one and a variant that detect and relearn a new model whenever drift is detected. The bottom panel shows the evolution of the signed Mc-Nemar statistic computed for these two algorithms. Figure 8: Experiments in SEA dataset illustrating the first drift at point 15000. The top figure shows the evolution of the naive-Bayes prequential error. The bottom figure represents the evolution of the Page-Hinkley test statistic and the detection thresh-old  X  . threshold parameter we establish a tradeoff between the false positive alarms and the miss detections.

As described before in this paper, the use of fading factors, as a smooth forgetting mechanism, may be an advantage in change detection algorithms. In a drift scenario, as new data is available, older observations are less useful. Using fading factors, e.g. attributing less weight to past observations, the change detection algorithm will focus in the most recent data, which in a drift scenario may lead to fast detections. For detection purposes, we monitor the evolution of the er-ror rate of a naive-Bayes classifier (using again the SEA con-cepts dataset). The formula we use to embed fading factors in the Page-Hinkley test is: m T =  X   X  m T  X  1 + ( x t  X   X  x To detect increases in the error rate (due to drifts in data) we compute the PHT, setting  X  and  X  parameters to 10  X  3 and 2.5, respectively. Figure 9 shows the delay time for this test: (a) without fading factors, (b) and (c) using different fading factors. The advantage of the use of fading factors in the PHT can be easily observed in this figure. The exponential forgetting results in small delay times without compromise miss detections.

We can control the rate of forgetting using different fading factors; as close to one is the  X  value of the fading factor the less it will forget data. We evaluate the PHT using different fading factors to assess the delay time in detection. Figure 9 (b) and c)) shows the delay time in detection of concept drifts. We have used the PHT and different fading factors (  X  = 1  X  10  X  5 and  X  = 1  X  10  X  4 , respectively) to detect these changes. Table 2 presents the delay time in detection of concept drifts in the same dataset used in figure 9. As the fading factor increases, one can observe that the delay time also increases, which is consistent with the design of experiments. As close to one is the  X  value of the fading 1st drift 1045 (1) 1609 2039 2089 2094 2095 2nd drift 654 (0) 2129 2464 2507 2511 2511 3rd drift 856 (1) 1357 1609 1637 2511 1641 Table 2: Delay times in drift scenarios using differ-ent fading factors . We observe false alarms only for 1  X   X  = 10  X  4 . The number of false alarms is indicated in parenthesis. factor the greater is the weight of the old data, which will lead to higher delay times. The feasible values for  X  are between 1  X  10  X  4 and 1  X  10  X  8 (with  X  = 1  X  10  X  8 the delay times are not decreased and with  X  = 1  X  10  X  4 the delay time decreases dramatically but with false alarms). We may focus on the resilience of this test to false alarms and on its ability to reveal changes without miss detections. The results obtained with this dataset were very consistent and precise, supporting that the use of fading factors improves the accuracy of the Page-Hinkley test.
The prequential method is a general methodology to eval-uate learning algorithms in streaming scenarios. The con-tributions of this paper are:
More than the technical contribution, this paper is a con-tribution to a discussion in the good-practices on perfor-mance assessment and differences in performance when learn-ing dynamic models that evolves over time.
The main problem in evaluation methods when learning from dynamic and time-changing data streams consists of monitoring the evolution of the learning process. In this work we defend the use of Predictive Sequential error esti-mates using fading factors to assess performance of stream learning algorithms in presence of non-stationary data. The prequential method is a general methodology to evaluate learning algorithms in streaming scenarios. In those appli-cations where the observed target value is available later in time, the prequential estimator can be implemented inside [5] H. Chernoff. A measure of asymptotic efficiency for [6] Graham Cormode, S. Muthukrishnan, and Wei [7] A. P. Dawid. Statistical theory: The prequential [8] Janez Demsar. Statistical comparisons of classifiers [9] T. Dietterich. Approximate statistical tests for [10] Pedro Domingos and Geoff Hulten. Mining High-Speed [11] F.J. Ferrer-Troyano, J.S. Aguilar-Ruiz, and J.C. [12] Francisco Ferrer-Troyano, Jesus S. Aguilar-Ruiz, and [13] Jo  X ao Gama, Pedro Medas, Gladys Castillo, and Pedro [14] Jo  X ao Gama, Pedro Medas, and Ricardo Rocha. Forest [15] Jo  X ao Gama, Ricardo Rocha, and Pedro Medas. [16] B. Ghosh and P. Sen. Handbook of Sequential [17] W. Hoeffding. Probability inequalities for sums of [18] Geoff Hulten and Pedro Domingos. Catching up with [19] Geoff Hulten and Pedro Domingos. VFML  X  a toolkit [20] Geoff Hulten, Laurie Spencer, and Pedro Domingos. [21] Richard Kirkby. Improving Hoeffding Trees . PhD [22] Ralf Klinkenberg. Learning drifting concepts: [23] I. Koychev. Gradual forgetting for adaptation to [24] I. Mierswa, M. Wurst, R. Klinkenberg, M. Scholz, and [25] H. Mouss, D. Mouss, N. Mouss, and L. Sefouhi. Test [26] E. S. Page. Continuous inspection schemes.
 [27] W. Nick Street and YongSeog Kim. A streaming [28] Gerhard Widmer and Miroslav Kubat. Learning in the
