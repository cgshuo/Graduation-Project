 Users frequently pose comparison queries (e.g., ibm vs ap-ple) on web search engines. However, little research has been done on understanding these queries. To fill in this gap, this paper describes a first solution to discovering and mining comparison queries. We present a novel snowballing algorithm that  X  X rawls X  comparison queries from search en-gines via their query autocompletion services. We propose a novel modeling approach that represents comparison queries in a comparison graph and develop a novel algorithm that mines closely related concepts from comparison graphs via spectral clustering. Initial experiments indicate that our ap-proach can reveal the inherent semantic relationship among the concepts and discover different senses of a concept, e.g.,  X  X oyota X  as a car brand or a company name.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval X  X uery Analysis Algorithms, Experimentation, Performance Comparison queries, query analysis, clustering
Life is full of choices. Since search engines are arguably the most popular and comprehensive information resources, we would expect many people to frequently turn to search engines for helps in making informed decisions. As a re-sult, there would be many comparison queries (e.g.,  X  X bm vs apple X  and  X  X pple or pc X ) posed on search engines. In this paper, we focus on the comparison queries expressed using  X  X s X  which is commonly used in English for comparison.
One way of verifying this conjecture is to check with query logs of search engines. However, such logs are often not read-ily available. Instead, we leverage the query autocompletion Y ahoo!. In the first set of experiments, we checked how many words in WordNet [7] were compared in search engines. (1) We start by considering different parts of speech (POS) in-cluding noun, verb, adjective, and adverb. For each POS, we conducted 6 experiments. First, we randomly selected 1% of words in that POS. The sample sizes are shown in the Rand column of Table 2 (the  X  X ny X  row will be explained soon). Next, we selected all words in that POS whose famil-iarity score is at least a , 1  X  a  X  5. Since WordNet does not have frequency counts of words, we use familiarity scores of words to approximate their popularity. The familiarity score is used in WordNet to indicate the number of senses that a word has. Intuitively, a word with a large number of senses (e.g., 51 senses for  X  X ake X ) is more likely to appear in texts. (2) Next, we performed similar 6 experiments over all words in the WordNet (i.e., without distinguishing their parts of speech). But due to the large number of words in the Word-Net, sample size was set to 2000 for the Rand experiment. The last ( X  any  X ) row shows the sample sizes.

Table 3 shows percentages of words appearing in compar-ison queries. We observe that except for adverb (17.8%), there are about 50% of words for all other POS X  X  that have ever been compared (from 42.6% for adjective to 52.3% verb). Furthermore, 38.4% of words randomly selected from the entire WordNet appeared in comparison queries. We also observe that the larger the familiarity score of a word, the more likely it will be compared. For example, the percent-ages range from 72.2 to as high as 91.8 for f &gt; 3. In the second set of experiments, we gathered from the Web lists of the most popular nouns [1], verbs [2], adjec-tives [3], and general words [4]. Since these lists are either provided by domain experts [1, 2, 3] or compiled based on the actual occurrence statistics [4], they may provide more direct evidences on the word popularity than the familiarity scores in the WordNet. Table 4 shows the numbers of words for different POS X  X  and the percentages of words appear-ing in comparison queries. We observe that about or over 90% of words were compared in all categories. This further confirms the prevalence of comparison queries.
 Observations: Intuitively, if a concept x is compared with another concept y , then x and y must somehow be related. This provides some clue to the meaning of y if we only know x . For example, consider a comparison query  X  X bm vs infor-matica X . Suppose that we know nothing about X  X nformatica X  but we do know  X  X bm X  is an IT company. Accordingly, we might infer that  X  X nformatica X  is likely also an IT company.
On the other hand, natural language is known for its rich-ness and often a word might have multiple senses (i.e., pol-ysemous). For example,  X  X pple X  could mean a company or a fruit. As a result, we might expect queries that compare  X  X pple X  with another company (e.g.,  X  X pple vs hp X ) or fruit (e.g.,  X  X pple vs orange X ). So from the concepts that  X  X pple X  is compared with, we might discover its different senses and concepts in each sense.
 C omparison Query and Graph: Consider a set of con-cepts C . A comparison query q takes the form of  X  c i vs c  X , where c i , c j  X  C . q may be simply represented as ( c i , c ). Note that in this paper, we do not consider variations of comparison queries that use X  X ersus X  X nd X  X r X  X nstead of X  X s X , e.g.,  X  X pple versus android X  and  X  X pple or pc X . A compari-son graph G = ( C , Q ) is a undirected graph whose nodes are concepts in C and edges ( Q ) represent comparison queries. The Crawling Problem: Given a seed concept c , find a set of concepts that are compared with c . A concept c  X  is compared with c if (1) direct comparison : there exists a comparison query ( c, c  X  ) or ( c  X  , c ); or (2) indirect comparison : there exists ( c, c  X  X  ) or ( c  X  X  , c ) and c  X  is compared with c  X  X  . The Mining Problem: Given a comparison graph G = ( C , Q ), find clusters of concepts in C such that concepts in the same cluster are closely related, while concepts in different clusters are semantically disjoint.
Figure 1 shows SnowballCrawler , the algorithm for crawling comparison queries from search engines via snow-balling. The algorithm takes as the input a seed concept seed , a search engine s that provides query autocompletion service, and a stopping criterion  X  that limits the number of levels that the crawler snowballs.

SnowballCrawler snowballs level by level. It starts at level 1 where there is only one seed . For every seed x at the current level l , SnowballCrawler poses a partial query q =  X  x vs  X  (i.e., x followed by  X  X s X  and a space, e.g.,  X  X bm vs  X ) to the search engine s . s responds with a list of completions to q , e.g.,  X  X bm vs apple X  and  X  X bm vs hp X . From the completions, SnowballCrawler extracts the concepts that x is compared with, e.g.,  X  X pple X  X nd X  X p X . Note that in this paper, we only consider the concepts that are represented in single words and ignore cases such as X  X bm vs apple laptop X  and  X  X bm pc vs apple pc X . As an ongoing work, we are considering these more general cases which essentially give rise to an interesting problem of normalizing and segmenting comparison queries.

The extracted new concepts (i.e., they are not existing seeds at the current and previous levels) will be added to the list of seeds for the next level. SnowballCrawler then repeats the same crawling process at the level l +1 and stops the snowballing once it reaches the maximum level  X  .
Figure 2 shows SpecClsr , the algorithm for discovering related concepts from comparison queries via spectral clus-tering. SpecClsr takes as the input a comparison graph G , which is constructed from the comparison queries as de-scribed in Section 2, and the desired number of clusters k . It outputs clusters of closely related concepts.

The key idea of SpecClsr is to transform G into data points in a k -dimensional eigenspace (steps 1 X 6) and then employ an off-the-shelf k -means algorithm to find clusters (step 7) [8]. SpecClsr first computes the Laplacian matrix L using the adjacency matrix of G (and the diagonal matrix D ). It then finds the k eigenvectors of L with the small-est eigenvalues. These eigenvectors form an eigenspace for representing the nodes in G (the matrix U in Figure 2). walmart g ines typically return up to 10 suggestions for a query, the number completions ( Com ) ranges from 105 to 472. The yield ratio ranges from 6.9 to 10. The number of unique concepts discovered ( Con ) ranges from 40 to 187. The cor-responding yield ratio ranges from 2.3 to 4.3. These indicate that SnowballCrawler can discover a significant number of new concepts with a relatively small number of queries. Evaluating SpecClsr: We gauge the performance of Spec-Clsr in clustering the concepts that SnowballCrawler gathered. To measure the performance, we manually clus-tered the concepts which forms a gold standard. The perfor-mance of SpecClsr is measured by the precision ( P ), recall ( R ), and F 1 (2 PR/ ( P + R )) of the concept pairs discovered by SpecClsr compared to the pairs in the gold standard.
Results are shown in Table 6 whose third column shows the number of desired clusters k where SpecClsr produced the best results (in F 1 ). We observe that the precision ranges from .45 to as high as .93, the recall ranges from .27 to 1, and F 1 ranges from .4 to .92. We note that precisions are typically higher than recalls since there are a relatively fewer number of clusters in the gold standard compared to k .
Table 7 shows clusters and example concepts in the clus-ters discovered by SpecClsr . We observe that SpecClsr can discover clusters of concepts closely related to the seed, e.g., company cluster for  X  X bm X , and measurement cluster for  X  X oot X . It may also discover related concepts for different senses of a concept, e.g., a cluster of cars and a cluster of companies for  X  X oyota X . Note that the algorithm also discov-ered some clusters that are seemingly unrelated to the seed. For example, a cluster of countries was discovered for the seed  X  X bm X . This is due to the comparison queries:  X  X bm vs sco X ,  X  X co vs usa X , and  X  X sa vs canada X . Note that  X  X co X  is both a company name and an abbreviation for  X  X cotland X . In other words,  X  X co X  may be regarded as a polysemous word which may represent a company or a country.
Our work is closely related to [6] which proposes an ag-glomerative clustering algorithm to mine related entities from comparison queries. However, there are several key dif-ferences. First, [6] assumes a log of comparison queries is available while we discover comparison queries from search engines via query autocompletion. Second, [6] focuses on real-world entities such as products and people, while ours
