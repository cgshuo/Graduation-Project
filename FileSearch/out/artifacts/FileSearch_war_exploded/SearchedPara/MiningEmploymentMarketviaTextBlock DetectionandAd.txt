 We have dev elop ed an approac h for analyzing online job ad-vertisemen ts in di eren t domains (industries) from di eren t regions worldwide. Our approac h is able to extract precise information from the text con ten t supp orting useful emplo y-men t mark et analysis locally and globally . A ma jor comp o-nen t in our approac h is an information extraction framew ork whic h is comp osed of two challenging tasks. The rst task is to detect unformatted text blo cks automatically based on an unsup ervised learning mo del. Iden tifying these useful text blo cks through this learning mo del allo ws the genera-tion of highly e ectiv e features for the next task whic h is text fragmen t extraction learning. The task of text frag-men t extraction learning is form ulated as a domain adapta-tion mo del for text fragmen t classi cation. One adv antage of our approac h is that it can easily adapt to a large num ber of online job adv ertisemen ts in di eren t and new domains. Extensiv e exp erimen ts have been conducted to demonstrate the e ectiv eness and exibilit y of our approac h. I.5.1 [ Pattern Recognition ]: Mo dels| Statistic al Algorithms information extraction, text blo ck detection
The work describ ed in this pap er is substan tially sup-ported by gran ts from the Researc h Gran t Council of the Hong Kong Special Administrativ e Region, China (Pro ject No: CUHK4128/07) and the Direct Gran t of the Fac-ulty of Engineering, CUHK (Pro ject Codes: 2050391 and 2050442). This work is also aliated with the Microsoft-CUHK Join t Lab oratory for Human-cen tric Computing and Interface Technologies.

There are a large amoun t of online job adv ertisemen ts, posted by recruiters or companies in di eren t industries and from di eren t regions worldwide. For instance, Figure 1 sho ws a sample of online job adv ertisemen t documen t posted on a recruitmen t Web site. Supp ose an emplo ymen t ocer wishes to analyze the emplo ymen t mark et of di eren t indus-tries in di eren t regions. This user could man ually bro wse and acquire detailed and precise job information. Obviously , it tak es an enormous amoun t of time and e ort. As a re-sult, it raises the need for information extraction systems, whic h can automatically extract precise job information in di eren t industries from di eren t Web sites facilitating both automatic information mining and human analysis.

Sev eral approac hes have been dev elop ed to extract infor-mation, in particular job information, from di eren t kinds of documen ts suc h as job announcemen ts in newsgroup. For example, RAPIER is a rule-induction metho d applied to ex-tract job information from job announcemen ts in Usenet [8]. One limitation of RAPIER is that the qualit y of the ex-tracted information is quite low with an average recall around 0.6, though the average precision is about 0.8. Hence, suc h qualit y of the extracted data cannot be used for further anal-ysis or information mining pro cess. Kauc hak et al. extended the Boosted Wrapp er Induction (BWI) metho d and applied to extract job elds in the same dataset [12]. One limitation of their approac h is that it just considers three elds, namely , job ID , job title , and comp any . It cannot extract other more complicated elds suc h as job duty , working experienc e , etc. One common shortcoming of all the above approac hes is that they can only extract information from a single domain, whic h refers to the job announcemen t of computer-related jobs in Usenet. All these metho ds, in principle, cannot han-dle cross-domain information. A lot of extra work is needed to extract information from other domains.

The above men tioned approac hes also su er another com-mon shortcoming is that they cannot handle job adv ertise-men ts con taining a mix of layout format tags (e.g., HTML tags), unformatte d text blocks , and text fragmen ts. An un-formatted text blo ck essen tially refers to a certain large piece of texts organized as a blo ck of information and it usually con veys the same kind of information suc h as the require-men t of a job. For example, Figure 1 depicts some unfor-matted text blo cks. An unformatted text blo ck can exist in di eren t layout formats suc h as paragraphs, lists, or tables. One characteristic is that an unformatted text blo ck consists of two ma jor comp onen ts, namely , blo ck heading and blo ck con ten t. For instance, the rst unformatted text blo ck in Figure 1 con tains a blo ck heading line of \Description" , and two paragraphs of blo ck con ten t starting with \Our clien t, a Big 4 ... " and \You will be resp onsible for ... ". The sec-ond unformatted text blo ck con tains a blo ck heading line of \Quali cation" and a list of blo ck con ten t suc h as \quali ed or nearly quali ed CA ...". Other unformatted text blo cks include the rows of the table. For instance, the rst row of the table con tains the blo ck heading \Date:" and the blo ck con ten t \30 Decem ber 2008" . Accurate iden ti cation of un-formatted text blo cks is very useful for inferring the mean-ing of texts, thus con tributing to the automatic extraction of elds of a job adv ertisemen t. However, due to the high variation of layout format and con ten t, this iden ti cation problem is very challenging.

We aim at dev eloping a metho d whic h is able to extract precise con ten t related to a num ber of useful elds of online job adv ertisemen t documen ts. The elds are job title , com-pany , location , salary , post-date , educ ation , experienc e , and duty . The job adv ertisemen ts can be in di eren t layout for-mat and collected from sev eral Web sites in di eren t coun-tries and regions worldwide suc h as North America, Europ e, and Asia. They are also related to a variet y of domains (industries or categories) suc h as accoun ting, logistic, etc. Our metho d can extract precise texts of these elds. Use-ful information related to emplo ymen t mark et can then be obtained by mining over the extracted data. For example, one may wish to kno w the exp ected salary of an auditor in di eren t coun tries, or the exp erience required to become a manager in di eren t industries.

The extraction of the eld information is tackled by an information extraction framew ork whic h is a ma jor comp o-nen t in our approac h. This information extraction frame-work is comp osed of two challenging tasks. The rst task is to automatically detect unformatted text blo cks and dis-cover the corresp onding headings and con ten ts based on an unsup ervised learning mo del. Iden tifying these unformatted text blo cks through this learning mo del allo ws the genera-tion of highly e ectiv e features for the next task whic h is the text fragmen t extraction learning. The task of unformatted text blo ck detection is related to the table detection problem whic h aims at locating tables con tained in text and iden ti-fying the heading cells and the data cells [15]. However, table detection problem is less challenging since there exist clues like table separators and con tinuous white space and they are very indicativ e. Moreo ver, the approac h presen ted in [15] is a sup ervised approac h and hence man ual e ort is needed to prepare the training examples. Unformatted text blo ck detection is also related to the visual blo ck detection prop osed by Cai et al. [7]. However, one limitation of their prop osed VIPS approac h is that it is an iterativ e algorithm con tinuously segmen ting a Web page con trolled by a thresh-old. As a result, this threshold must be carefully decided in adv ance in order to iden tify the blo cks related to job ad-vertisemen t elds. In addition, our approac h also aims at disco vering the heading of an unformatted text blo ck.
The task of text fragmen t extraction learning in our frame-work is form ulated as a domain adaptation mo del for text fragmen t classi cation. Essen tially , an online job adv ertise-men t documen t after prepro cessing can be considered as a sequence of text fragmen ts characterized by con ten t, layout format, as well as unformatted text blo ck information. The objectiv e of our text fragmen t extraction mo del is to lab el eac h text fragmen t to one of the elds. For example, the text fragmen ts \Income tax free salary of US$60,000" and \Georgeto wn, Cayman Island" are lab eled as salary and loca-tion resp ectiv ely. One adv antage of our information extrac-tion framew ork is that it can easily adapt to a large num ber of online job adv ertisemen ts in di eren t and new domains, without any man ually lab eled training examples in the new domains. Therefore, it can signi can tly reduce the man ual e ort in extracting job information from multiple domains. Blitzer et al. [5] prop osed the Structural Corresp ondence Learning (SCL) metho d, whic h rst heuristically iden ti es some common domain indep enden t frequen t features from both source and target domains. It represen ts other uncom-mon features by exploring the co-o ccurrence information be-tween those features and common features. The intuition is that similar represen tation for the features from the source and target domains should have similar prediction power. Ben-Da vid et al. [3] analyzed the bound of the general do-main adaptation problem and concluded that the adaptiv e classi cation error in the target domain is bounded by the error in the source domain and the distribution gap between the two domains. It pro vides the solid theoretical guaran tee for our prop osed domain adaptation metho d that extracts feature subspace. Our metho d directly aims at minimiz-ing the distribution gap between the source domain and the target domain by pro jecting them to a shared feature sub-space, at the same time, minimizing the empirical loss on the lab eled data in the source domain.
We have dev elop ed a learning framew ork for extracting texts related to di eren t elds con tained in online job adv er-tisemen t documen ts, whic h are collected from di eren t Web sites worldwide with di eren t layout formats. Unlik e most existing works, whic h can only extract information from a single domain, one characteristic of our framew ork is that it can adapt to extract information across di eren t domains. To achiev e this, we form ulate our extraction learning as a domain adaptation problem.

One ma jor task of our framew ork is an unsup ervised learn-ing mo del for unformatted text blo ck detection with heading disco very. Our metho d can e ectiv ely iden tify the blo cks related to job elds. The headings disco vered pro vide very useful feature information for the text extraction task.
We have conducted extensiv e exp erimen ts for online job adv ertisemen t documen ts collected from over ten real-w orld Web sites originated from di eren t regions of the world to evaluate our framew ork. The documen ts are related to a variet y of industries demonstrating the e ectiv eness of our approac h in extracting information from di eren t domains. The exp erimen tal results sho w that our metho d outp erforms existing works and the qualit y of extraction can supp ort fur-ther information disco very pro cess. In particular, we have applied our metho d to mine some useful patterns in the em-ploymen t mark et in di eren t regions and di eren t industries. Consider a set of domains denoted as D = f D 1 ;D 2 ;::: g . Let R i denote the i -th job adv ertisemen t from a particular domain in D . For example, one domain can be accoun ting and Figure 1 is a sample job adv ertisemen t collected from this domain. Eac h job adv ertisemen t is comp osed of a set of elds. Let A = f A 1 ;A 2 ;::: g be the elds of interest in our extraction task. For instance, a eld may refer to the salary , and the eld value of salary for the job adv ertisemen t in Figure 1 is \Income tax free salary of US$60,000" . With-out losing generalit y, there is a special elemen t in A denoted as A represen ting \not-a-eld" . Let P i be the online job ad-vertisemen t documen t con taining the information including the values of some elds about the job adv ertisemen t R i Essen tially , an online job adv ertisemen t documen t, whic h is basically a HTML documen t, can be treated as a documen t object mo del (DOM) structure 1 . The text nodes, whic h are the leaves of the DOM structure, refer to the texts dis-played while their ancestors, whic h are called HTML nodes, embody to the layout format of the texts. These text nodes can be automatically extracted. Text nodes consisting of paragraphs are also automatically segmen ted into sen tences by a sen tence segmen tator to pro vide ner gran ularit y. Af-ter suc h prepro cessing, the documen t P i becomes a sequence of text fragmen ts, whic h are basically the texts in the DOM text nodes and may be further sen tence segmen ted. We de-note f i j the j -th text fragmen t in the documen t P i . Eac h text fragmen t f i j is comp osed of two parts, namely , y i j A k 2 A and x i j 2 R n , whic h refer to the lab el and the fea-ture vector of f i j resp ectiv ely. y i j = k essen tially represen ts that the text fragmen t f i j belongs to the eld A k . For exam-ple, the lab el of the text fragmen t \Income tax free salary of US$60,000" is the eld \salary" for the job adv ertisemen t sho wn in Figure 1; x i j refers to a vector whic h captures di er-ent kinds of features suc h as the layout features, the con ten t features, as well as the blo ck heading features whic h will be describ ed later, to characterize the text fragmen t. Unformatted Text Block Detection with Heading Disco very: As describ ed above, x i j is a feature vector char-acterizing the text fragmen t f i j . It includes di eren t kinds of features. The rst kind of features is called con ten t fea-
The details of the documen t object mo del can be found in http://www.w3.or g/DOM . tures, whic h represen t the con ten t of the text fragmen t. The second kind of features is called layout features, whic h rep-resen t the layout format of the text fragmen t sho wn in the documen t P i . Another kind of features, whic h is called head-ing features, is related to unformatted text blo cks. Recall that an unformatted text blo ck is a certain large blo ck of texts con veying the same kind of information. It con tains a text fragmen t called blo ck heading and the remaining text fragmen ts called blo ck con ten ts. Let H ( f i j ;f i k called heading relation between the text fragmen ts f i j f , where f i j and f i k refer to the blo ck heading and a blo ck con ten t of the same unformatted text blo ck resp ectiv ely. H(\Description" , \Our clien t, a Big 4 ..."), H(\Description" , \You will be ..."), and H(\Quali cation" , \quali ed or nearly ...") are examples of heading relations disco vered in Fig-ure 1. Since the set of blo ck headings and the set of blo ck con ten ts are disjoin t, and the blo cks will not be overlapp ed in an online job adv ertisemen t documen t, there must not exist H ( f i j ;f i k ) suc h that f i l is a blo ck heading in another relation and j l k . The unformatted text blo ck detec-tion with heading disco very problem can be de ned as iden-tifying the heading relations in documen ts. f i j in H ( f can then be utilized to deriv e the heading features of f Existing Metho ds for Job Information Extraction: Existing metho ds for job information extraction can only extract job elds from a single domain. Formally , given a set of online job adv ertisemen t documen ts L S in a domain D S . The text fragmen ts con tained in the documen ts in L are all man ually lab eled with the asso ciated lab els in A . In other words, The lab el y i j is kno wn for eac h x i j P i 2 L S . A mo del M S is then learned from L S . M S can then be applied to another set of documen ts, denoted as V in the same domain D S , to automatically lab el all the text fragmen ts. Essen tially , given x l k of the k -th text fragmen t in documen t P l 2 V S , M S is able to predict the value of y . However, supp ose another set of documen ts is collected from another domain, say D T , M S cannot be applied to extract job elds in these documen ts, since the con ten t of the eld values are largely di eren t in the two domains. As a result, man ual e ort is required to prepare another set of training examples, namely L T in the domain D T to learn a new mo del M T to extract information. Unfortunately , this is infeasible if we need to handle a large num ber of domains. Cross-Domain Job Information Extraction: To ad-dress the limitation of existing metho ds, we aim at dev el-oping a metho d to extract precise job information across domains. Formally , given a set of online job adv ertisemen t documen ts L S , in whic h the text fragmen ts are all lab eled, in a source domain D S and another set of online job adv er-tisemen t documen ts U T , in whic h the text fragmen ts are all unlab eled data, in a target domain D T . In other words, the lab el y i j is kno wn for eac h x i j , where P i 2 L S . y is unkno wn, where P l 2 U T . Our objectiv e is to exploit both L S and U T to learn a mo del, denoted by M T , suc h that the newly learned mo del can be applied to a new set of documen ts V T in the target domain D T . Therefore, one adv antage is that we only need to prepare one set of train-ing examples in a source domain. We can mak e use of this metho d to learn mo dels for a num ber of di eren t domains to extract information, substan tially reducing the human e ort. Optionally , users can pro vide a small amoun t of la-beled training examples in the target domain as additional information to impro ve the extraction performance.
We have dev elop ed a Bayesian learning approac h to auto-matically detecting the unformatted text blo cks within an online job adv ertisemen t documen t, and disco vering the cor-resp onding headings and con ten ts in an unsup ervised man-ner. Speci cally , we aim at iden tifying the heading rela-tions, denoted as H ( f i j ;f i k ), where f i j and f i j -th and k -th text fragmen ts within the documen t P i . To achiev e this, our approac h rst iden ti es possible heading candidates within the online job adv ertisemen t based on Bayesian learning. A heading candidate is basically a text fragmen t whic h is likely to be the heading of an unformat-ted text blo ck. Next, heading relations regarding the iden ti-ed heading candidates and the con ten ts of the unformatted text blo ck will be automatically constructed. The relations having the same heading candidate eventually constitute an unformatted text blo ck.

We observ e that there are two kinds of clues whic h are very useful in iden tifying the headings. The rst kind of clues is that normally the headings of di eren t unformatted text blo cks of an online job adv ertisemen t documen t share high similarit y in their layout format. For example, the head-ings of the rst two unformatted text blo cks in Figure 1 are \Description:" and \Quali cations:" . Both are short phrases ending with a single colon. However, the layout formats of the headings can be di eren t across di eren t documen ts, even the documen ts originate from the same Web site. We call the layout format of a heading documen t dep enden t. The second kind of clues comes from the terms app eared in the heading. There are some phrases suc h as \quali cations" and \requiremen ts" whic h are commonly found in the head-ing of a blo ck. As a result, the terms of some headings of di eren t job adv ertisemen ts share certain similarit y and they are regarded as documen t indep enden t. One may prepare a list of terms related to headings and iden tify the headings of a job adv ertisemen t using a keyw ord matc hing approac h to nd all text fragmen ts con taining the terms. However, there may exist some terms whic h are rare or not common in headings. One example is the term \Descriptions" sho wn in Figure 1. This leads to a poor performance for this simple metho d.

Our heading candidate iden ti cation metho d is an unsu-pervised approac h based on Bayesian learning. Giv en a text fragmen t f i j in the documen t P i , we aim at computing the probabilit y that f i j is a heading of an unformatted text blo ck given f i j . We denote the probabilit y as P ( h j f i j be equal to either 1 or 0 represen ting \heading" or \not-a-heading" resp ectiv ely. Based on Bayes rule, we can obtain accordingly . Recall that a text fragmen t is comp osed of doc-umen t indep enden t con ten t information and documen t de-penden t layout information. The con ten t of a text fragmen t can be represen ted by a set of con ten t features, whic h are essen tially the terms found in the text fragmen ts. To char-acterize the layout format of the text fragmen ts, we design a set of binary features to accomplish the task. Examples of the layout features include boldness , italic , capitalization , single-wor d , double-wor d , etc. Assuming that these features are indep enden t, we can deriv e the expression where L m ( f i j ) and C n ( f i j ) refer to the binary feature func-tions that are equal to 1 if f i j possesses the m -th layout fea-ture and the n -th con ten t feature resp ectiv ely, and j L j and j C j refer to the total num ber of layout and con ten t features resp ectiv ely.
 In ordinary sup ervised Bayesian learning metho ds, P ( h ), P ( L m ( ) j h ), and P ( C n ( ) j h ) can be found from a set of la-beled training examples and hence P ( h j f 0 ) can be computed readily for an unseen text fragmen t f 0 . However, since the layout format of a heading is documen t dep enden t and it is imp ossible to prepare training examples for all adv ertise-men t documen ts from di eren t sites. Moreo ver, using the con ten t features alone also leads to the incapabilit y of dis-covering headings only con taining terms that are rare or not only common in headings. To tackle this problem, we have dev elop ed an unsup ervised learning metho d based on the documen t indep enden t prop erty of the con ten t features and the documen t dep enden t prop erty of the layout features. In eac h documen t, our metho d emplo ys the exp ectation-maximization (EM) technique to iden tify the text fragmen ts whic h likely belong to headings. In essence, the E-step and the M-step are as follo ws: E-Step: M-Step: where F i refers to the set of all text fragmen ts in P i a smo othing factor to avoid zero probabilit y. In the E-Step, we aim at computing the P t +1 ( h j f ) for all f 2 F i based on the parameters estimated at the t -th iteration. In the M-Step, we aim at estimating the parameters including the con-ditional probabilities P t +1 ( C n ( ) j h ) and P t +1 ( L well as the prior probabilit y P t +1 ( h ) based on P t +1 F i obtained in the E-Step. Note that our metho d is applied on individual documen ts. P t +1 ( L m ( ) j h ), whic h is related to the documen t dep enden t layout features, is automatically updated in the EM pro cedure based on the results in the E-Step. As a result, man ually lab eled training examples are not needed to obtain the values of P t +1 ( L m ( ) j h ) and the resulting P t +1 ( L m ( ) j h ) will be tailor-made for the corre-sponding documen t.

Figure 2 sho ws the outline of our metho d. Since we do not have any training examples and P 0 ( h j f ) is unkno wn for all f 2 F i , we initialize our heading iden ti cation algorithm by making use of a very few num ber of terms whic h are commonly con tained in headings. These terms can be easily determined in adv ance. In particular, we used less than 10 common terms suc h as \requiremen t" in our exp erimen ts. If a text fragmen t f 2 F i con tains any of the terms, we set P 0 ( h = 1 j f ) to a higher value. Next, we invoke our EM iteration. Up on con vergence, those text fragmen ts f with P t ( h = 1 j f ) greater than a certain threshold will be considered as heading candidates of unformatted text blo cks.
After obtaining the set of heading candidates, we can dis-cover the heading relation H ( f i j ;f i k ), where f i j to a blo ck heading and a blo ck con ten t of an unformatted text blo ck. We observ e that for a given unformatted text blo ck, the layout format of the con ten t shares high similar-ity. Therefore, we disco ver the heading relation as follo ws: Let f i j be a heading candidate iden ti ed using the algorithm sho wn in Figure 2. The heading relation H ( f i j ;f i j +1 created if f i j +1 is not a heading candidate. Next, by scan-ning through the next W text fragmen ts, a heading relation H ( f i j ;f i j + k ) is obtained if the layout formats of f f j + k are the same, where k = 2 ;:::;W , and W is a prede-ned windo w size. The pro cess stops once the layout format ing candidate. As describ ed in Section 2, once the heading relations are disco vered, the unformatted text blo cks can be obtained accordingly .
The ma jor idea of our cross-domain information extrac-tion metho d is to iden tify the feature subspace on whic h the distribution di erence between the source domain D S and the target domain D T is minimized. At the same time, the empirical loss on the lab eled data is also minimized. The lab eled data can be entirely collected from the source do-main. Optionally , it can also con tain a very small amoun t of lab eled data from the target domain.

Recall that given a feature vector x i j of a text fragmen t, we aim at predicting the value of y i j in extraction. For easy illustration in this section, we use ~x to replace x i j as the feature vector of any arbitrary text fragmen t. We consider the follo wing predictiv e function with resp ect to ~x : where : R n ! R d is a kno wn feature mapping that belongs to the Repro ducing Kernel Hilb ert Space (RKHS) 2 ; u l 2 R
In particular, we set ( ~x ) = ~x in our exp erimen t and mak e no additional transformation in the input feature space. In principle, can be other feature mappings. is the corresp onding weigh t vector; 2 R r d is the linear transformation whic h pro jects the input space to the linear feature subspace shared by the source domain D S and the target domain D T ; v l is the corresp onding weigh t vector. In essence, is the parametric family whic h parameterizes the data represen tation for domain adaptation. Di eren t from existing domain adaptation techniques, we do not need to rely on the covarian t shift assumption, whic h assumes that the conditional distribution of the lab el values is unc hanged between D S and D T given the unlab eled data. In con trast, we sacri ce the good prediction power on D S for the possible domain adaptation by nding the data embedding, whic h can well parameterize the learnabilit y in D S and adaptivit y between D S and D T . As a result, we aim at learning the linear data embedding subspace ~x where the regularized empirical risk and distribution gap are both minimized.
Supp ose there are n lab eled text fragmen ts, whic h can be either collected from D S or D T , as the training examples. We de ne the eld lab el indicator matrix Y 2 R n j A j , and the entry Y ik equals 1 if the i -th training example is lab eled as k where A k 2 A , and equals 1 otherwise. Let be or-thogonal on rows so that T = I . Then our framew ork can be form ulated as minimizing the follo wing trade-o function between the two criteria: sub ject to T = I . Note that L ( ) represen ts the empirical loss on the lab eled data; ( ~x i ) is a simple weigh t function, whic h generally equals 1 if ~x i is sampled from D S and a value greater than 1 if it is from D T . d ( ) measures the distri-bution gap between the embedding of D S and D T after the same pro jection . In particular, we apply the standard sampled from D T ; and use the Maxim um Mean Discrepancy discussed below for d ( ). r ( u l ;v l ; ) is the regularization con trolling the mo del complexit y. As we aim at maximizing the prediction power on the shared feature subspace, we em-ploy the 2-norm regularization r ( u l ;v l ; ) = u T l u the weigh t of the classi cation vector on the original feature space, but allo w larger weigh t on the shared feature space.
The fundamen tal question is how to evaluate the distribu-tion gap based on the nite observ ations of the two groups. Maxim um Mean Discrepancy (MMD) [6] is a recen t statis-tical test strategy whose principle is to obtain a function that holds di eren t means on di eren t distributions in D and D T . Once we restrict the function within the unit ball in RKHS, then we can bound the mean discrepancy by a form ulation with resp ect to the kernel matrix of the data in both domains. Consequen tly MMD can be applied to evalu-ate the distribution gap between the two transformed space on D S and D T by the same pro jection .

Next, Equation 7 can be expressed using the variables f u variable with the other variables xed, similar to the strat-egy applied in [1]. The detailed form ulation and optimiza-tion strategy of our cross-domain learning metho d are pre-sen ted in [9].
We have conducted extensiv e exp erimen ts on di eren t do-mains to evaluate our framew ork. We have collected online job adv ertisemen t documen ts from recruitmen t Web sites in 11 di eren t domains (or industries) from di eren t regions of the world. Table 1 sho ws the details of the collected data. The rst four domains sho wn in Table 1 are used to evaluate our framew ork, while the remaining domains, to-gether with the rst four, are all used in the application of our framew ork presen ted in Section 6. The rst, second, and third columns refer to the domain lab el, domain name, and the num ber of job adv ertisemen ts collected in the do-main resp ectiv ely. The fourth column of the table sho ws the num ber of text fragmen ts in the domain after segmen tation. In our exp erimen ts, the elds of interest are job title , com-pany , location , salary , post-date , educ ation , experienc e , and duty . Therefore, eac h text fragmen t should be lab eled as one of the above elds, or the \not-a-eld" lab el. For eval-uation purp ose, all text fragmen ts in the rst four domains are man ually lab eled by two human accessors. If there is a disagreemen t on the judgmen t of the two human accessors, it is resolv ed by a discussion among them.

In eac h domain, we have conducted di eren t sets of exp eri-men ts to demonstrate the performance of our framew ork and compare with the performance of existing metho ds. The rst set of exp erimen t is to use the lab eled training examples in the source domain, the unlab eled data in the target domain, and with/without a very limited num ber of lab eled data (30 job adv ertisemen ts) in the target domain to learn the extrac-tion mo del using our framew ork. The learned mo del is then applied to the testing data in the target domain and the per-formance is measured. For example, let D1 and D2 be the source and target domains resp ectiv ely. We use the lab eled training examples in D1 and the unlab eled data in D2 to learn a mo del. The learned mo del is applied to predict the lab els of the text fragmen ts in the testing data. The other sets of exp erimen ts are designed similar to the rst set. In the second set of exp erimen ts, we use transductiv e supp ort vector mac hine for mo del training. In the third set of exp er-imen ts, we apply our framew ork in the same manner as the rst set, except that the unformatted text blo ck detection comp onen t is not used. The objectiv e of the fourth set of exp erimen ts is to compare the e ectiv eness of our blo ck de-tection with other existing work. In this set of exp erimen ts, we apply VIPS prop osed in [7] to iden tify the information blo cks within a job adv ertisemen t documen t. The rst text fragmen t in an information blo ck is considered as the blo ck heading and the remaining text fragmen ts are considered as the blo ck con ten ts. The heading relations can then be con-structed by relating the blo ck con ten ts and the blo ck heading within the same information blo ck. These heading relations are then used to generate the heading features of the text fragmen ts. We call this metho d VIPS-Approac h.

We adopt the recall, precision, and F1-measure as the evaluation metrics. Recall is de ned as the num ber of text fragmen ts that are correctly lab eled by our framew ork, di-vided by the actual num ber of text fragmen ts. Precision is de ned as the num ber of text fragmen ts that are correctly lab eled by our framew ork, divided by the num ber of pre-dicted text fragmen ts using our framew ork. F1-measure is de ned as the harmonic mean of recall and precision.
In eac h set of exp erimen ts, we have conducted 24 runs us-ing di eren t com bination of the source and target domains. Table 2 depicts the performance of the exp erimen ts. In eac h run, we measure the recall, precision, and F1-measure for eac h eld. The gure in eac h cell of Table 2 is the aver-age performance among the 8 elds of interest in the corre-sponding exp erimen t. For example, our approac h achiev es an average precision, recall, and F1-measure of 0.814, 0.845, and 0.825 resp ectiv ely without using any lab eled training example in the target domain when the source and target domains are D1 and D2 resp ectiv ely. The table also sho ws the average performance of the exp erimen ts for not using and using a very small amoun t of lab eled training examples in the target domain. Our approac h achiev es an average precision, recall, and F1-measure of 0.798, 0.761, and 0.760 resp ectiv ely for not using any training examples in the target domain. It outp erforms TSVM whic h obtains a F1-measure of 0.723. When a few amoun t of training examples is pro-vided in the target domain, our approac h achiev es a better result, with an average precision, recall, and F1-measure of 0.818, 0.800, and 0.802 resp ectiv ely, whereas TSVM only ob-tains an average F1-measure of 0.746. The impro vemen t of our approac h is more signi can t than TSVM for using a few lab eled training examples in the target domain. The perfor-mance of our approac h is also better than VIPS, whic h is an existing metho d for text blo ck detection. VIPS achiev es an average F1-measure of 0.726 and 0.758 for not using and using a few lab eled training examples in the target domain resp ectiv ely. It illustrates that our unformatted text blo ck detection with heading disco very approac h is more e ectiv e. Table 2 also sho ws the t-statistics tests comparing the F1-measure of our approac h with that of other approac hes. It can be observ ed that the performance of our approac h is signi can tly better than that of other approac hes.
Table 3 depicts the extraction results among di eren t elds for using D3 as the source domain and others as the target domains. It can be observ ed that our approac h can outp er-form other approac hes in the extraction of most of the elds of interest. For example, when adapting D3 to D1, D2, and D4, the F1-measures for extracting title using our approac h are 0.733, 0.763, and 0.733 resp ectiv ely, while TSVM, can only achiev e F1-measures of 0.546, 0.453, and 0.484 resp ec-tively. The extraction performance of the eld educ ation is generally less satisfactory . The ma jor reason is that this eld values of educ ation in di eren t domains share very lit-tle similarit y. For example, an accoun tan t may require some professional certi cates suc h as CA, CPA, ACCA whic h are very speci c to the accoun ting domain. Nev ertheless, our approac h achiev es the best performance in the extraction of this eld among all the approac hes.

We have also conducted exp erimen ts for learning the mo del from the very few num ber of lab eled training examples and the unlab eled data in the target domain using TSVM. The average F1-measure among the four domains is 0.682. It sho ws that just using the few num ber of training examples in the target domain cannot obtain a good extraction mo del.
We have applied our framew ork to dev elop an applica-tion to deriv e useful kno wledge and information from on-line job adv ertisemen t documen ts in over 10 di eren t do-mains as describ ed describ ed in Table 1. The documen ts are collected form di eren t coun tries worldwide including Aus-tralia, Canada, Hong Kong, India, Singap ore, United King-dom, USA. Speci cally , our application can automatically analyze the trend of the emplo ymen t mark ets, for example, the salary of the same kind of job in di eren t coun tries.
We apply our framew ork to extract the elds of interest as describ ed in Section 5. The extracted values of di eren t elds of eac h job adv ertisemen t are stored in a database, as well as the coun try from whic h the job adv ertisemen t doc-umen t originates. Our application is to analyze the salary level of the same kind of job in di eren t coun tries. To achiev e this, we retriev e the salary range of the same job title. Ta-ble 4 sho ws the salary range of two jobs, namely , receptionist in the Administration domain and auditor in the Accoun t-ing domain resp ectiv ely. We disco ver that the salary can vary a lot. The salaries for receptionists are about CAD$ 13.00 (ab out USD$ 11.0) per hour , GBP$ 16,000.00 (ab out USD$ 22,000.00) per ann um, and INR$ 4,000 (ab out USD$ 82.00) per mon th in Canada, United Kingdom, and India re-spectiv ely. The salaries for accoun tan ts are similar in some dev elop ed coun tries. For example, the salary of a senior au-ditor is between USD$ 5,000 and 7,000 in Australia, USA, Canada, and United Kingdom; An auditor manager can ob-tain a salary of USD$ 8,000 in USA and United Kingdom.
Sev eral approac hes have been dev elop ed to extract infor-mation from di eren t kinds of texts [17] and mak e use of the extracted data to mine useful kno wledge. Freitag and Mc-Callum dev elop ed a hidden Mark ov mo del based metho d to extract the elds title and comp any from job announcemen ts in Usenet [10]. Approac hes have been dev elop ed to extract job related information from Web pages [2]. For example, Blei et al. dev elop ed a learning framew ork exploiting global features and scop e limited features to extract job title from Web pages [4]. One common limitation of these approac hes is that they are sup ervised approac hes and hence training examples are required to be prepared man ually . Moreo ver, the learned mo del can only be applied on the domain from whic h the training examples come. If we want to extract information from other domains, additional man ual work is required to prepare di eren t sets of training examples.
Domain adaptation is a widely studied area. It addresses a common situation when applying the trained mo del to a di eren t domain. Raina et al. [16] learned the sparse basis from the unlab eled data whic h does not necessarily come from the same domain as the lab eled data. Recen tly, Pan et al. [14] applied the Kernel Maxim um Mean Discrepancy to learn the embedded space where the distribution between the source domain and the target domain is minimized. As-signing separate weigh ts in the empirical loss function for the instances from separate domains is another direction. Jiang and Zhai [11] suggested to apply the instance weigh ting tech-nique for domain adaption in natural language pro cessing. Note that most of these works rely on the covariance shift assumption, whic h assumes the conditional distribution of the lab els given the unlab eled data is unc hanged between the source and target domains.

The unformatted text blo ck detection comp onen t of our framew ork is related to blo ck detection or record detection. Liu et al. dev elop ed an approac h to disco vering data records in a Web page by pro cessing the tag tree of the page using sev eral heuristics [13]. VIPS is an approac h to detecting visual blo cks of a Web page [7]. However, one limitation of VIPS is that it con tinuously segmen ts a Web page con trolled by a threshold. The level of segmen tation (or the threshold) is page dep enden t and is required to be carefully determined for eac h page man ually in adv ance.
We have dev elop ed a learning framew ork to extract job-related information from online job adv ertisemen t documen ts across di eren t domains. One characteristic of our frame-work is that we have dev elop ed an unsup ervised metho d to automatically detect unformatted text blo cks of a documen t and disco ver the corresp onding headings. The headings pro-vide useful features for our second task whic h is the text frag-men t extraction learning. We form ulate the extraction task as a domain adaptation problem and hence our metho d can handle adv ertisemen ts across di eren t domains. We have conducted extensiv e exp erimen ts and applied our framew ork to mine useful information from documen ts collected from over 10 di eren t real-w orld recruitmen t Web sites in di er-ent industries and in di eren t regions. Our approac h sho ws promising results and outp erforms existing metho ds.
