 Keyphrases in a document are often regarded as a high-level summary of the document. It not only helps the readers quickly capture the main topics of a document, but also plays an essential role in a variety of natural language pro-cessing tasks such as digital library [1], document retrieval [2] and content-based advertisement [3]. Since only a minority of documents have manually assigned keyphrases, there is great need to extrac t keyphrases from documents automat-ically. Recently, several automatic ke yphrase extraction approaches have been proposed, most of them leveraging supervised learning techniques [4] [5]. In these approaches, the task of automatic keyphrase extraction is basically formalized as a binary classification problem where a set of documents with manually la-beled keyphrases are used as training set an d a classifier is learned to distinguish keyphrases from all the candidate phrases in a given document.

One of the general assumptions made by traditional supervised learning algo-rithms is the balance between distributions of different classes. However, as we have observed, the assumption may not be hold in the real settings of automatic keyphrase extraction. The more common case is that the number of keyphrases (positive samples) is much fewer than that of non-keyphrases (negative samples) appearing in the same document. Taking a document of length n as an example, the number of possible phrases of lengths between p to q ( p  X  q ) would amount to O (( q  X  p +1)  X  n ), while the number of keyphrases in a document is often less than ten. It has since been proven that the e ffectiveness of most traditional learn-ing algorithm would be compromised by the imbalanced class distribution [6], we argue that the performance of automatic keyphrase extraction could be pro-moted by exploring the characteristics of imbalanced class distribution explicitly. However, to the best of our knowledge, the issue of class-imbalance in automatic keyphrase extraction has not been well studied in the literature.

In this paper, we adopt under-sampling mechanism to deal with the imbal-anced data in supervised automatic keyphrase extraction. Particularly, the use-less samples in training set are removed and thus traditional supervised learning approaches could be utilized more efficiently and effectively.

One of the reasons accounts for the successes of supervised keyphrase ex-traction approaches is that the saliency of each candidate phrase could be de-scribed using various features. In general, these features are calculated based on either the morphology (e.g., phrase length , part-of-speech tag ) or the occur-rence (e.g., first occurrence , PageRank value ) of candidates. In another word, the dataset in supervised keyphrase ext raction are comprised of two views. In-spired by the advantageous of multi-view learning approaches, we propose a novel under-sampling approach, named co-sampling , which aims to exploit the multiple views in the task of keyphrase ext raction to remove the useless samples.
The rest of the paper is organized as follows. After a brief overview of related work in Section 2, we present the details of co-sampling algorithm in Section 3. Section 4 reports experimental results on a keyphrase extraction dataset. Finally, we conclude the paper and discuss the fur ther work of co-sampling in Section 5. 2.1 Keyphrase Extraction Generally, automatic keyphrase extraction approaches can be categorized into two types: supervised and unsupervised.

In most supervised approaches, the task of keyphrase extraction is formulated as a classification problem. The accuracy of extracting results relies heavily on the features describing the saliency of candidate phrases. TF  X  IDF and first oc-currence of candidate phrase are the two features used in early work [4]. Besides, the features such as part-of-speech tag pattern, length and frequency of candidate phrase have shown their benefits to recogn ize keyphrases in a given document [8]. Recently, much work has been conducted on extracting keyphrases from partic-ular types of documents including scientific literatures [9], social snippets [10], web pages [11] and etc. One of the keys in the work is to calculate domain-specific features that capture the special salient characteristics of keyphrase in the particular type of documents. For ex ample, section occu rrences and acronym status play a critical role in finding keyphrases in scientific publications [9].
The basic idea of most unsupervised approaches is to leverage graph-based ranking techniques like PageRank [12] and HITS [13] to give a rank of all the candidate phrases. In general, the ranking scores are computed via random walk over co-occurrence graph of a given document. Recent extensions of unsupervised approaches mainly focus on building multiple co-occurrence graphs to reflect the characteristics of various keyphrase extraction settings. Wan et al. [14] built a global affinity graph on documents within a cluster in order to make use of mutual influences between documents. Liu et al. [15] took the semantic topics of document into account and built co -occurrence graph with respect to each topics. 2.2 Imbalanced Classification Imbalanced class distribution is a common phenomenon in many real machine learning applications. With imbalanced data, the classifiers can be easily over-whelmed by the majority class and thus ignore the minority but valuable one.
A straightforward but effective way to handle the imbalanced data is to re-balance the class distribution through sampling techniques, including removing a subset of samples from the majority class and inserting additional artificial samples in the minority class, which are referred to as under-sampling and over-sampling, respectively. EasyEnsemble and BalanceCascade [16] are the two typ-ical under-sampling approaches, which learn an ensemble of classifiers to select and remove the useless majority samples . SMOTE is an example of over-sampling approaches [17]. The algorithm randomly selects a point along the line joining a minority sample and one of its k nearest neighbors as a synthetic sample and adds it into the minority class. For the r ecent extensions of SMOTE, see [18], [19].
Cost-sensitive learning is another solutions to the problem of imbalanced clas-sification. The basic idea is to define a cost matrix to quantify the penalties of mis-classifying samples from one class to another. For most traditional learning algorithms, the cost-sensitive versions have been proposed for imbalanced clas-sification. For example, Yang et al. proposed three cost-sensitive boosting algo-rithms named AdaC1, AdaC2 and AdaC3 [20], Zhou et al. studied empirically the effect of sampling and threshold-moving strategy in training cost-sensitive neural networks [21].

Different from most existing work focusing on either sampling technique or cost-sensitive learning, there have been se veral proposed imbalance classification approaches combining the above two types of mechanisms [22]. 3.1 Problem Formulations and Algorithm Sketch Let X X  R d denote the input feature space of all possible candidate phrases and Y = { +1 ,  X  1 } denote the output space. Because of the two-view characteristic of keyphrase extraction, the input space X can be written as X = X 1  X X 2 ,where X 1 and phrases, respectively. That is, the fe ature vector of each candidate phrase x can be denoted as x =( x 1 , x 2 ).

Given a set of training samples S = P  X  N where P = { (( x 1 i , x 2 i ) , +1) | i = 1 ,  X  X  X  ,m } and N = { (( x 1 problems, m ! n ), the goal of co-sampling is to boost the performances of the classifiers through excluding redundant negative samples from training process.
As a co-training [7] style algorithm, co-sampling works in an iterative manner as shown in Table 1. During the iterations, redundant negative samples are removed from the training set of a classifier according to the predicted results of the classifier learned on another view. Figure 1 gives an illustration of the procedure of co-sampling. The iterative process stops when one of the following criteria is met: 1. The number of iterations exceeds a predefined maximum number. 2. The confidences of the predictions of either classifier on any negative samples 3. The redundancies of any reliably predicted negative samples is below a pre-At the end of co-sampling, the two classifiers f 1 and f 2 learned on the under-sampled training sets are combined to give prediction for a new sample x = ( x 1 , x 2 ). In particular, the final output is calculated as follows: where P (+1 | f i ( x i )) is the posterior probability of x to be a positive sample based on the prediction result f i ( x i )( i =1 , 2).

During the iterations, we employ an online learning algorithm, i.e., Percep-tron with uneven margins [23], to learn the classifiers on each view for its easy implementation, efficient training and theoretical soundness.

The key issue of co-sampling is to select the appropriate negative samples to be removed. To address the problem, we adopt a two-stage method. The first stage is to estimate the confidence of either classifier X  X  predictions on the negative samples and take these reliably predicted ones as the candidates for further removal. The second stage is to quantify the redundancy of each candi-date samples and remove these most redundant ones. The above two stages will be introduced in Section 3.2 and 3.3 detailedly, respectively.
 3.2 Prediction Confidence Estimation Intuitively, the sample with high posterior class probability P ( y =  X  1 | f ( x )) can be viewed as the reliably predicted ne gative sample. We thus leverage poste-rior class probability to estimate the confidence of either classifier X  X  prediction on each negative sample. As the perceptron algorithm outputs a linear predic-tion function f ( x )= w , x + b , we need to map the real valued outputs to probabilities.

Following [24], the posterior class probability of each predicted negative sam-ple is derived through fitting a sigmoid function: To find the best parameters A and B , we generate a set of training samples divergence between  X  P and its empirical approximation derived from T , i.e., This optimization can be solved by usi ng a second order gradient descent algo-rithm [24].

After the posterior class probabilities of each samples are calculated by (1), the samples with  X  P ( x ) higher than a predefined threshold  X  are selected as the candidates for further removal. 3.3 Redundant Sample Identification In co-sampling, we quantify the redundancy of each sample by using the cut edge weight statistic [25]. Particularly, our approach is a two-stage process.
Firstly, a complete undirected weighted graph is built on a set of training samples in which the vertex corresponds to a sample and the weight of the edge reflects the similarity between two samples . In particular, the weight is calculated as: where r ij is the rank of vertex x j among all the vertices according to its distance the specific values of distance are taken in to consideration during the calculation of edge weight, making the edge weight be less sensitive to the choice of distance measure. In the paper, Euclidean distance is chosen as the distance measure for simplicity.

Secondly, the cut edge weight statistic are defined and calculated based on the graph. Intuitively, a sample is supposed to be less informative for training if it appears to be easily distinguished from the samples of other classes. As each sample is represented as a vertex in graph, this implies that a redundant sample would be the one whose sum of the weights of the edges linking to the vertices of the same class is significantly larger than that of the edges linking to the vertices of different classes. The edges of the latter type are often referred to as cut edges. In order to identify the redundant samples, we define a null hypothesis H 0 as that the classes of the vertices of the graph are drawn independently from the probability distribution P ( Y = k )( k =  X  1 , +1). Usually, P ( Y = k )isestimated empirically as the proportion of the class k in the training set. Then, under H 0 , the cut edge statistic of a sample ( x i ,y i ) is defined as: where n is the number of training samples and I ij is an independent and iden-tically distributed random variables drawn from a Bernoulli distribution, i.e., Accordingly, P ( I ij =1)=1  X  P ( Y = y i ).

According to the de Moivre-Laplace th eorem, we can derive that the distri-bution of the cut edge statistic J i is approximately a normal distribution with In another words, As for a negative sample, it would be supposed to be redundant if its value of the cut edge statistic is significantly smaller than the expected under H 0 . Consequently, the redundancy of a negative sample x i is quantified by using the right unilateral p -value of  X  J i , i.e., Finally, the candidate negative samples selected from Section 3.2 with r ( x ) higher than a predefined threshold  X  will be excluded from the next co-sampling iterations of the peer classifier. 3.4 Two Views for Keyphrase Extraction In the section, we describe the partition of feature set to generate the two-view representations of the task of supervised keyphrase extraction.
 In general, each candidate phrase has views of morphology and occurrence. For example, ending with a noun is a feature in the morphology view of candidate phrases while appearing in title is a feature in the occurrence view. Basically, the co-training style algorithms require that each view is sufficient for learning a strong classifier and the views are conditionally independent to each other given the classes. However, the conditions are so strong that it hardly holds in most real applications. Therefore, we partition the features describing the saliency of candidate phrases into two disjoint sets a ccording to whether the feature carries either morphology or occurrence information about the phrase, together with practical concerns about the above sufficient and redundant conditions.
Particularly, the morphology view consists of the following features: 1. Length . That is, the number of words in a candidate phrase. 2. Part of speech tags . The type of features include a number of binary fea-3. TFIDF .TF,IDFandTF  X  IDF of every words in the candidate phrase are 4. suffix sequence . The suffices like -ment and -ion of each word are extracted, 5. Acronym form . Whether the candidate is an acronym may be a good indica-The occurrence view consists of the following features: 1. First occurrence . The feature is calculated as the number of words between 2. Occurrence among sections . The type of features include a number of 3. PageRank values . As in [12], the PageRank values of every words in the 4.1 Dataset To avoid manually annotation of keyphrases which is often laborious and erro-neous, we constructed an evaluation dataset using research articles with author provided keyphrases. Specifically, we collected the full-text papers published in the proceedings of two conferences, n amed ACM SIGIR and SIGKDD, from 2006 to 2010. After removing the papers without author provided keyphrases, there are totally 3461 keyphrases appear in 997 papers in our evaluation dataset. For each paper, tokenization, pos tagging, stemming and chunking were performed using NLTK (Natural Language Toolkit) 1 . We observed that the keyphrases make up only 0.31% of the total phrases in the dataset, which practically con-firms that there exists the problem of extreme class-imbalance in the task of supervised keyphrase extraction. 4.2 Baselines Since co-sampling is a multi-view supervised keyphrase extraction approach, several supervised approaches that make use of single view consisting of all the features referred in Section 3.4 are used as the baselines.

The first baseline is the supervised keyphrase extraction approach that learns the classifier using SVM on the original dataset without sampling. One of the existing under-sampling approaches, named EasyEnsemble [16], are taken as the second baseline. Besides, we also compare co-sampling with random under-sampling approach. In the random under-sampling approaches, a balanced train-ing set is generated by sampling a subset of negative samples randomly according to a class-imbalance ratio defined as: where | N | and | P | are the size of sampled negative set and original positive set, respectively. The class-imbalance ratio is set to 50, 20, 10, 5 and 1 experimentally. SVM is employed to learn the classifiers on the sampled training set. 4.3 Evaluation Measures The traditional metrics namely Precision , Recall and F1-score ,areusetoeval-uate our approach and all the baselines. 4.4 Experimental Results For co-sampling, the parameters  X  and  X  are tuned using an independent vali-dation set randomly sampled from the total dataset and the result with highest F1-score are reported. For the base line EasyEnsemble, the parameter T (the number of subsets to be sampled from negative set) is set to 4 as in [16]. For all the baselines, the learner, i.e. SVM, is implemented using SVM light 2 toolkit. As for the slack parameter C in SVM, the default values given by SVM light are used. Comparison of Keyphrase Extraction Accuracies. Table 2 shows the per-formances of co-sampling and baselines which are averaged over 10 fold cross validation. We can see that co-sampling outperforms all the baselines in terms of Precision, Recall and F1-score. We conducted paired t -test over the results of the 10 folds and found the improvements of co-sampling over all the base-lines (except the improvement over Eas yEnsemble in terms of Precision) are significant at the 0.01 level. Moreover, the baseline employing SVM on the orig-inal dataset without sampling is failed to give a nontrivial classifier because all the candidate phrases are classified as non-kephrase, which gives an example of class-imbalance problem on traditional learning algorithms.
 Training Performance versus Number of Iterations. We investigated how the performances of each classifiers lear ned on different views varies while the training set is under-sampled continuously during the iteration process. Particu-larly, we measured the training accuracy of each classifier by calculating F1-score of the predicted results obtained through performing 10-fold cross validation as in Table 1. Figure 2 gives the plots of training accuracies versus number of iterations. We can see that the classifier learned on either the morphology or the occurrence views can be boosted into a  X  X tronger X  classifier during the co-sampling process. This gives an evidence that the two classifiers are capable of mutually reinforcing through removing the reliably predicted redundant samples for each other. In this paper, we have argued that it is more essential to formalize the task of su-pervised keyphrase extraction as an imb alanced classification problem and have proposed a novel under-sampling approach, named co-sampling, to tackle the class-imbalance problem. Co-sampling is by nature a multi-view learning algo-rithm in which the keyphrase extraction dataset is partitioned into the morphol-ogy and the occurrence views. Experimen tal results on a key phrase extraction dataset verified the advantages of co-sampling.

In the future, we will evaluate co-sampling experimentally by using more im-balanced multi-view datasets from vari ous domains. Furthermore, it is necessary to give an in-depth theoretical analy sis of co-sampling as has been done for co-training style algorithms.
 Acknowledgments. This paper is supported partly by Chinese National Natu-ral Science Foundation ( 61170079); Shandong Prov ince Higher Educational Sci-ence and Technology Program (J12LN45); Key Research Program of Statistics Science of Shandong Province (KT11017); Research Project of  X  X DUST Spring Bud X  (2010AZZ179); Sci. &amp; Tech. Development Fund of Shandong Province (2010GSF10811); Sci. &amp; Tech. Development Fund of Qingdao(10-3-3-32-nsh); Excellent Young Scientist Foundation of Shandong Province (BS2010DX009 and 2010KYJQ101); China Postdoctoral Science Foundation (2011M501155).

