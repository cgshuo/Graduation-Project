 This paper describes how the Bootstrap approach to sta-tistics can be applied to the evaluation of IR effectiveness metrics. First, we argue that Bootstrap Hypothesis Tests deserve more attention from the IR community, as they are based on fewer assumptions than traditional statistical sig-nificance tests. We then describe straightforward methods for comparing the sensitivity of IR metrics based on Boot-strap Hypothesis Tests. Unlike the heuristics-based  X  X wap X  method proposed by Voorhees and Buckley, our method esti-mates the performance difference required to achieve a given significance level directly from Bootstrap Hypothesis Test results. In addition, we describe a simple way of examining the accuracy of rank correlati on between two metrics based on the Bootstrap Estimate of Standard Error. We demon-strate the usefulness of our methods using test collections and runs from the NTCIR CLIR track for comparing seven IR metrics, including those that can handle graded relevance and those based on the Geometric Mean.
 A typical IR paper claims that System X is better than System Y in terms of an effectiveness metric M computed based on a test collection C : How reliable is this paper? More specifically, (a) What happens if C is replaced with another set of data C ? (b) How good is M ? Question (a) posed above is usually dealt with as follows: Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00. 1. Use two or more test collections of  X  X espectable X  size, 2. Make sure that the performance difference between X 3. Conduct statistical significance tests to claim that the All of the above are arguably necessary conditions for mak-ing a good IR paper that involves comparative experiments, although surprisingly many IR papers do not satisfy them [13].
Unfortunately, there has been a controversy as to which statistical significance tests should be used for IR evaluation, as well as whether such tests should be used at all [4, 6, 13, 14]. It is known that a typical IR evaluation environment of-ten violates the underlying assumptions of significance tests, but it is also known that some significance tests work well even when some of the assumptions are violated. Parametric tests rely on the normality assumption and generally have higher power than nonparametric ones. (That is, it is eas-ier to detect significant differ ences with parametric tests.) But even nonparametric tests are not assumption-free: the Paired Wilcoxon Test depends on both the symmetry and the continuity assumptions [4]. An IR researcher who wants to be conservative (i.e., who wants to minimise the risk of jumping to wrong conclusions) might, for example, choose the two-tailed Sign Test, which generally has little power.
However, as Savoy [14] points out, there is a very attrac-tive alternative called the Bootstrap [3]. Invented in 1979, the Bootstrap is the approach to statistics for the computer age, and has strong theoretical foundations. While classical statistics rely on mathematical derivations that often require several assumptions on the underlying distributions of data, the Bootstrap tries to achieve the same goal by directly esti-mating the distributions through resampling from observed data. The Bootstrap Hypothesis Tests are free from the nor-mality and symmetry assumptions, and it is known that they often show power comparable to that of traditional paramet-ric significance tests. Moreover, the Unpaired Bootstrap Hy-pothesis Test is directly applicable even to unconventional summary statistics that are not Arithmetic Means over a topic set (e.g., the  X  X rea X  measure based on the worst N topics for each system [16] and Geometric Means [11, 16]). We therefore believe that Bootstrap Hypothesis Tests de-serve more attention from the IR community.

This paper concerns Question (b) posed above: How  X  X ood X  is IR metric M ? More specifically, we use the Bootstrap Hypothesis Tests to assess and compare the sensitivity of different IR metrics. This is related to the  X  X wap X  method proposed by Voorhees and Buckley [12, 13, 15] (and the  X  X tability X  method proposed by Buckley and Voorhees [2, 12]): The swap method derives the performance difference required for declaring that a s ystem is better than another, and that the chance of obtaining a contradictory result with another topic set (the swap rate ) is below a given threshold. However, while the swap method is not directly related to statistical significance tests (See Section 3.5), our method estimates the performance difference required to achieve a given significance level directly from Bootstrap Hypothesis Test results. In addition, we describe a simple way of exam-ining the accuracy of rank co rrelation between two metrics based on the Bootstrap Est imate of Standard Error.
To demonstrate the usefulness of our Bootstrap-based meth-ods, we use two test collections with submitted runs from NTCIR [9] and compare seven IR metrics, including those basedongradedrelevanceandthosebasedontheGeometric Mean [11, 16]. The swap, stability and our Bootstrap-based methods agree that, for these data sets, the most sensitive IR metrics are Q-measure [12], normalised Discounted Cu-mulative Gain at cut-off 1000 [5, 7] and Average Precision (AveP), while the least sensitive one is Precision at cut-off 1000. In the middle lie normalised Cumulative Gain at cut-off 1000 and Geometric Mean AveP / Q-measure.

Section 2 describes the IR metrics and the NTCIR data we use. Section 3 discusses our new methods for comparing the sensitivity of IR metrics based on the Bootstrap Hypoth-esis Tests. Section 4 describes a method for examining the accuracy of rank correlation between two IR metrics based on the Bootstrap Estimate of Standard Error. Section 5 discusses previous work and Section 6 concludes this paper. The IR metrics we consider in this paper are Average Precision (AveP), Precision at cut-off 1000 (PDoc 1000 ), Q-measure [12, 17], and normalised (Discounted) Cumulative Gain at cut-off 1000 (n(D)CG 1000 ) [5, 7].

AveP represents a very sensitive IR metric based on bi-nary relevance, while PDoc 1000 represents a very insensitive one [12]. (PDoc 1000 rewards a system with 10 relevant docu-ments at Ranks 1-10 and one with 10 relevant documents at Ranks 991-1000 equally. Note also that it does not average well.) Let R denote the number of relevant documents for a topic, and let L (  X  1000) denote the size of a ranked out-put. Let count ( r ) denote the number of relevant documents within top r (  X  L ), and let isrel ( r )be1ifthedocument at Rank r is relevant and 0 otherwise. Clearly, Precision at Rank r is given by P ( r )= count ( r ) /r .Hence:
We can also use IR metrics based on graded relevance, since the NTCIR data contain S-, A-and B-relevant (highly relevant, relevant and partially relevant) documents. Let R ( L ) denote the number of L -relevant documents so that P
L R ( L )= R ,andlet gain ( L )denotethe gain value for retrieving an L -relevant document [5]. (Throughout this paper, we use gain ( S )=3 ,gain ( A )=2 ,gain ( B )=1.) Let cg ( r )= Rank r of the system X  X  output, where g ( i )= gain ( L )ifthe document at Rank i is L -relevant and g ( i )=0otherwise. In particular, consider an ideal ranked output, such that isrel ( r )=1for1  X  r  X  R and g ( r )  X  g ( r  X  1) for r&gt; 1, and let cg I ( r ) denote the ideal cumulative gain at Rank r . i&gt;a , we can obtain the (ideal) discounted cumulative gain dcg ( r )and dcg I ( r )[5]. (Weuse a = 2 throughout this pa-per.) Then we have: where BR ( r )isthe blended ratio given by:
It is known that nCG l has a problem: it cannot pe-nalise late arrival of relevant documents after Rank R (as cg
I ( r )= cg I ( R )holdsfor r  X  R ) [12]. nDCG l solves this problem by discounting the gains, while Q-measure solves it by including r in the denominator of BR ( r ). nDCG l is a stable and sensitive metric provided that l is large [12]. Mi-crosoft reportedly uses a variant of this metric for improving their Web search engine [1]. Q-measure is also stable and sensitive [12], and it has been applied to XML retrieval [17] as well as Question Answering. It is more highly corre-lated with AveP than nDCG is; In a binary relevance en-vironment, Q -measure = AveP holds for any ranked out-put if there is no relevant document below Rank R ,and Q -measure &gt; AveP holds otherwise.

By default, we use the Arithmetic Mean over a given topic set with any IR metric. However, this paper also considers the Geometric Mean versions of AveP and Q-measure, which we denote by G AveP and G Q-measure. Let x i denote the value of a metric for the i -th topic (down to four significant figures). Then the actual method we use for obtaining the Geometric Mean ( GM )isasfollows[16]: The 0.00001 X  X  are necessary because limiting the ranked out-put size to L  X  1000 implies that x i may be zero. Geometric Mean AveP was used at the TREC Robust Track in order to focus on the  X  X ardest X  topics. Sakai et al. [11] used Geomet-ric Mean Q-measure for analysing their results at NTCIR-5. Geometric Means are arguably more practical than Arith-metic Means for building robust IR systems, which can pro-duce a decent output whatever the query is.
Our experiments use two sets of data (test collections and submitted runs) from the NTCIR-3 CLIR track [9], provided by National Institute of Informatics, Japan. Table 1 pro-vides some statistics of the data. From each data set, only the top 30 runs as measured by relaxed AveP (i.e., AveP that treats S-, A-and B-relevant documents just as  X  X elevant X ) were used in our experiments, since  X  X ear-zero X  runs are un-likely to be useful for discussing the sensitivity of metrics. Table 1: Statistics of the NTCIR-3 CLIR Chinese and Japanese data. Thus, for each data set, we have a set of 30  X  29 / 2 = 435 system combinations, which we shall denote by C . Sections 3.1 and 3.2 describe how Paired and Unpaired Bootstrap Hypothesis Tests can be applied to IR evaluation. Section 3.3 proposes methods for comparing the sensitivity of IR metrics and for estimating the performance difference required for achieving a given significance level. Section 3.4 presents our experimental results, and Section 3.5 compares them with those based on the stability and swap methods.
We first describe the Paired Bootstrap Hypothesis Test, which can be used for comparing two IR strategies run against a common test collection. This is similar to the one de-scribed earlier by Savoy [14], except that we use a Stu-dentised test statistic to enhance accuracy. We encourage other IR researchers to try it as it is based on fewer assump-tions than standard significance tests such as paired t -and Wilcoxon tests, is easy to apply, yet has high power.
Let Q be the set of topics provided in the test collection, and let | Q | = n .Let x =( x 1 ,...,x n )and y =( y 1 ,...,y denote the per-topic performance values of System X and Y as measured by some performance metric M . A standard method for comparing X and Y is to measure the difference between sample means  X  x = as Mean AveP values. But what we really want to know is whether the population means for X and Y (  X  X and  X  Y ), computed based on the population P of topics, are any dif-ferent. Since we can regard x and y as paired data, we let z =( z 1 ,...,z n )where z i = x i  X  y i ,let  X  =  X  X  X   X  up the following hypotheses for a two-tailed test: Thus the problem has been reduced to a one-sample prob-lem [3]. As with standard significance tests, we assume that z is an independent and identically distributed sam-ple drawn from an unknown distribution.

In order to conduct a Hypothesis Test, we need a test statistic t and a null hypothesis distribution .Consider: where  X   X  is the standard deviation of z ,givenby Moreover, let w =( w 1 ,...,w n )where w i = z i  X   X  z ,inorder to create bootstrap samples w  X  b of per-topic performance differences that obey H 0 . Figure 1 shows the algorithm for obtaining B bootstrap samples of topics ( Q  X  b )andthe corresponding values for w  X  b .(Welet B = 1000 through-out this paper.) For example, let us assume that we only for b =1to B Figure 1: Algorithm for creating Bootstrap samples Q  X  b and w  X  b =( w  X  b 1 ,...,w  X  b n ) for the Paired Test. count =0; for b =1to B ASL = count/B ; Figure 2: Algorithm for estimating the Achieved Significance Level based on the Paired Test. have five topics Q = (001, 002, 003, 004, 005) and that pling with replacement from Q yields Q  X  b = (001, 003, 001, 002, 005). Then, w  X  b =(0 . 2 , 0 . 1 , 0 . 2 , 0 . 0 , 0 . 0).
For each b ,let  X  w  X  b and  X   X   X  b denote the mean and the stan-dard deviation of w  X  b . Figure 2 shows how to compute the Achieved Significance Level (ASL) using w  X  b . In essence, we examine how rare the observed difference would be under H .If ASL &lt;  X  , where typically  X  =0 . 01 (very strong evi-dence against H 0 )or  X  =0 . 05 (reasonably strong evidence against H 0 ), then we reject H 0 . That is, we have enough evidence to state that  X  X and  X  Y are probably different.
The above test relies on the fact that the difference be-tween two Arithmetic Means equals the Arithmetic Mean of individual differences. But then how should we discuss statistical significance in terms of Geometric Mean AveP / Q-measure?
There are at least two ways to handle the problem. One is to use the Unpaired Bootstrap Hypothesis Test described in Section 3.2 instead, which is directly applicable to virtu-ally any metric, such as the  X  X rea X  measure based on the worst N topics for each system [16]. The other is to stick to the Paired Test: Instead of examining z i = x i  X  y i as men-tioned earlier, we could examine log( x i +0 . 00001)  X  log( y 0 . 00001). This is because testing the significance in terms of the Arithmetic Mean inside Eq. (7) should be equivalent to testing that in terms of the entire Geometric Mean formula. For convenience,  X  X rithmetic Mean inside the Geometric Mean X  will be denoted by the prefix  X  X G  X : We shall test AG AveP and AG Q-measuretodiscussthesensitivityof G AveP and G Q-measure in Section 3.4. As mentioned above, the Unpaired Bootstrap Hypothesis Test is more widely applicable than the Paired one, and it can handle Geometric Means directly. The downside is that the Unpaired Test has much less power than the Paired one since it uses less information. For this reason, the Paired Test should be preferred wherever it is applicable.
The Unpaired Test treats x and y as unpaired data, nat-urally. (In this paper, x and y are data obtained by running two IR strategies against a common test collection and there-fore | x | = | y | = n . But more generally, the two sets of obser-let v =( x 1 ,...,x n ,y 1 ,...,y m ); for b =1to B Figure 3: Algorithm for creating Bootstrap samples x paired Test. (We let m = n throughout this paper.) count =0; for b =1to B ASL = count/B ; Figure 4: Algorithm for estimating the Achieved Significance Level based on the Unpaired Test. vations may come from different test collections, so | y | be denoted by m rather than n hereafter.) As with standard significance tests, we assume that x and y are independently and identically distributed samples from unknown distribu-tions F and G , respectively. The test statistic we consider in this case is where, for example, M ( x )isthevalueofmetric M computed based on x . (Note that M does not have to be an Arith-metic Mean metric.) But what we really want to know is d , which represents the  X  X rue X  absolute performance difference between Systems X and Y when the whole population of topics is taken into account. Thus the hypotheses we can setupforatwo-tailedtestare:
We now need a null distribution for the data under H 0 .A natural choice would be to assume that F = G , i.e., that the observed values x i and y i actually come from an identical distribution. (In fact, F = G itself is commonly used as the null hypothesis.) First, let v denote a vector of size n + m obtained by concatenating the two per-topic performance vectors x and y . Figure 3 shows how to generate Bootstrap samples for the Unpaired Test. For simplicity, let us assume that x =(0 . 1 , 0 . 3) and y =(0 . 2 , 0 . 0), and therefore that v =(0 . 1 , 0 . 3 , 0 . 2 , 0 . 0). Then we generate random integers that range between 1 and 4: Suppose that we have obtained (1,4,1,2) for b = 1. Then, by splitting this vector into (1,4) In this way, Figure 3 shuffles the observed values without looking at whether they come from x or y .

Figure 4 shows how to compute the two-tailed ASL based on the Unpaired Test, in a way similar to Figure 2.
We now propose straightforward methods for assessing and comparing the sensitivity of IR effectiveness metrics.
DIF F =  X  ; for each system pair ( X, Y )  X  C estimated dif f =max { dif f  X  DIF F } (rounded to two significant figures); Figure 5: Algorithm for estimating the performance difference required for achieving a given significance level with the Paired Test.

DIF F =  X  ; for each system pair ( X, Y )  X  C estimated dif f =max { dif f  X  DIF F } (rounded to two significant figures); Figure 6: Algorithm for estimating the performance difference required for achieving a given significance level with the Unpaired Test.
 The idea is simple: Perform a Bootstrap Hypothesis Test for every system pair, and count how many of the pairs sat-isfy ASL &lt;  X  . Moreover, since each bootstrap replicate of the difference between two summary statistics (i.e.,  X  w the Paired Test and d  X  b for the Unpaired Test) is derived from n = | Q | topics, we can obtain a natural estimate of the performance difference required for guaranteeing ASL &lt;  X  , given n . This may be useful for informally guessing whether two systems are significantly different by just looking at the difference between two summary statistics.

Let  X  w  X  b X,Y and d  X  b X,Y explicitly denote the above bootstrap replicates for a particular system pair ( X, Y ). Figures 5 and 6 show our algorithms for estimating the performance difference required for achieving ASL &lt;  X  given n ,based on the Paired Test and the Unpaired Test, respectively. For example, if  X  =0 . 05 is chosen for Figure 6, the algorithm obtains the B X  = 1000  X  0 . 05 = 50-th largest value among | d
X,Y | for each ( X, Y ). Among the | C | = 435 values thus obtained, the algorithm takes the maximum value just to be conservative. Figure 5 is almost identical to Figure 6, although it looks slightly more complicated: Since we used Studentisation with the Paired Test, the bootstrap replicate  X  w
X,Y is not equal to the test statistic t ( w  X  b X,Y ) (See Figure 2), which we are using as the sort key . Figures 7 and 8 plot, for each IR metric, the Paired and Unpaired Bootstrap ASLs of system pairs from the NTCIR-3 Chinese data. The horizontal axis represents 435 system pairs sorted by the ASL. Since the figures may be too small for the reader, here we summarise the results in words: Ac-cording to Figure 7, Q-measure, AveP and nDCG 1000 are the most sensitive metrics, while PDoc 1000 is clearly the least sensitive. Whereas, nCG 1000 ,AG AveP and AG Q-measure (which represent G AveP and G Q-measure, respectively) lie in the middle.

The trends are similar in Figure 8, but it can be observed that the Unpaired Test has considerably less power for any metric. (The  X  X utlier X  curve represents PDoc 1000 : it fails to Figure 7: ASL curves based on Paired Bootstrap Hypothesis Tests (NTCIR-3 CLIR Chinese data). Figure 8: ASL curves based on Unpaired Bootstrap Hypothesis Tests (NTCIR-3 CLIR Chinese data). satisfy ASL &lt;  X  =0 . 05 for all system pairs.) By compar-ing the two figures, it can be observed that our AG AveP / AG Q-measure results based on the Paired Test (i.e., our  X  X ndirect X  assessments of G AveP and G Q-measure) are consistent with our G AveP / G Q-measure results based on the Unpaired Test. We have similar graphs for the NTCIR-3 Japanese data, which we omit due to lack of space.
Table 2(a) cuts Figure 7 in half to show, for each IR met-ric, how many pairs of Chinese systems satisfied ASL &lt;  X  = 0 . 05 with the Paired Test. The metrics have been sorted by this measure of sensitivity (Column 2). Table 2(b) shows similar results for the Japanese data, based on a Japan-ese version of Figure 7 not shown in this paper. It can be observed that Q-measure, nDCG 1000 and AveP are the most sensitive while PDoc 1000 is the least sensitive for both data sets. Moreover, the sensitivity values are substan-tially higher for the Japanese data: This is because the performance variance for the Japanese run set is greater than that for the Chinese run set. For example, the maxi-mum/mininum observed AveP values for the Japanese run set are .4932 and .0617, while those for the Chinese run set are .4166 and .2224.
 Column 3 of Table 2 shows, for each metric, the estimated Table 2: Sensitivity and the estimated differences basedonPairedTestsBootstrapTests(  X  =0 . 05 ; NTCIR-3 CLIR data). difference required for satisfying ASL &lt;  X  =0 . 05, given the topic set size n = 42. The algorithm we described in Fig-ure 5 was used to obtain these estimates. For AG AveP and AG Q-measure, the estimates represent differences be-tween two Arithmetic Means of logs rather than differences between two Geometric Means. Hence we tried to translate these estimates back into the latter by looking up the sys-tem pair ( X, Y ) and the bootstrap replicate number b that actually yielded the log-space differences, but our results did not always look accurate: For the purpose of estimating the required difference between Geometric Means, the Unpaired Testmaybemorereliable(Seebelow).

Our table based on the Unpaired Test, which we omit due to lack of space, is generally similar to Table 2: The sensitivity values (Column 2) are naturally much lower, but the estimated differences (Column 3) are comparable to the Paired case. According to the Unpaired Test, the estimated differences for G AveP and G Q-measure based on the Chi-nese data are 0.16 and 0.17, respectively. However, as Fig-ure 8 suggests, the Unpaired Test table appears to be less useful for discussing which metrics are more sensitive than others.

Note that Column 3 is not for comparing different IR met-rics: some metrics tend to take small values while others tend to take large values, so such comparisons are not nec-essarily valid. Relative performance differences [13] could be used for comparison across metrics, but this is beyond the scope of our work. We prefer to regard, for example, M ( x )= 0 . 0001 ,M ( y )=0 . 0002 and M ( x )=0 . 1000 ,M ( y )=0 . 2000 as quite different situations.
Readers familiar with the stability method [2] and the swap method [15] will note that our Bootstrap-based meth-ods for comparing the sensitivity of metrics is related to them. The crucial difference is that our methods are based on the Bootstrap which has time-honoured theoretical foun-dations. At the implementation level, the main difference is that the stability and swap methods use sampling without replacement whereas the Bootstrap samples are obtained by sampling with replacement. Sanderson and Zobel [13] and Sakai [10] explored variations of the swap method and the latter showed that with-and without-replacement yield similar results for the purpose of ranking metrics according for each system pair ( X, Y )  X  C Figure 9: Algorithm for computing EQ ( X, Y ) , GT ( X, Y ) and GT ( Y,X ) . Figure 10: MR-PT curves based on the stability method (NTCIR-3 CLIR Chinese data). to sensitivity (See Section 5). In light of this, this section uses the with -replacement versions of the stability and swap methods, and show that they yield results that are in agree-ment with our Bootstrap-based results. More specifically, we reuse our paired-test Bootstrap samples Q  X  b with these two methods.
 The essence of the stability method is to compare systems X and Y in terms of metric M using B different topic sets and count how often X outperforms Y ,howoften Y out-performs X and how often the two are regarded as equiva-lent. Our version works as follows: Let M ( X, Q  X  b )denote the value of metric M for System X computed based on Q  X  b .Givena fuzziness value f [2, 12], we count GT ( X, Y ), GT ( Y, X )and EQ ( X, Y ) as shown in Figure 9, where GT ( X, Y )+ GT ( Y, X )+ EQ ( X, Y )= B . Then, the minority rate ( MR ) and the proportion of ties ( PT )for M are computed as:
MR estimates the chance of reaching a wrong conclusion about a system pair, while PT reflects lack of discrimina-tive power. Thus, for a good performance metric, both of these values should be small. As a fixed fuzziness value implies different trade-offs for different metrics, we vary f (= 0 . 01 , 0 . 02 ,..., 0 . 20) for comparing the stability.
Figure 10 shows our MR -PT curves based on the NTCIR-3 CLIR Chinese data. (A similar graph for the Japanese for each system pair ( X, Y )  X  C for each bin i Figure 11: Algorithm for computing the swap rates. data is omitted due to lack of space.) The results are gener-ally in agreement with those based on Bootstrap Hypothesis Tests: nDCG 1000 , Q-measure and AveP are the most sta-ble, while PDoc 1000 is the least stable. In the middle lie nCG 1000 ,G AveP and G Q-measure.

The essence of the swap method is to estimate the swap rate , which represents the probability of the event that two experiments are contradictory given an overall performance difference. Our version works as follows: First, in addition to the set of B Bootstrap samples { Q  X  b } ,wecreate an-other set of B Bootstrap samples { Q  X  b } by sampling with replacement from Q .Let D denote the performance dif-ference between two systems as measured by M based on a topic set; we prepare 21 performance difference bins [12, 15], where the first bin represents performance differences such that 0  X  D&lt; 0 . 01, the second bin represents those such that 0 . 01  X  D&lt; 0 . 02, and so on, and the last bin represents those such that 0 . 20  X  D .Let BIN ( D ) denote the mapping from a difference D to one of the 21 bins where it belongs. The algorithm shown in Figure 11 calculates a swap rate for each bin. Note that D  X  b is not the same as our d  X  b from Fig-ure 4: D  X  b is the performance difference between X and Y as measured using the Bootstrap topic sample Q  X  b ;whereas, d  X  b is the Bootstrap replicate of the observed performance difference under the assumption that the per-topic values of X and Y come from an identical distribution .

We can thus plot swap rates against performance differ-ence bins. By looking for bins whose swap rates do not exceed (say) 5%, we can estimate how much absolute dif-ference is required in order to conclude that System X is better than Y with 95%  X  X onfidence X : But, as mentioned earlier, the swap method is not directly related to statistical significance tests.

Table 3 summarises the results of our  X  X wap X  experiments using the NTCIR-3 CLIR Chinese and Japanese data. The  X  X bs X  column shows the absolute difference required for guar-anteeing that the swap rate does not exceed 5%. The  X  X el X  column translates these values into relative values using the maximum performance record ed among all trials (the  X  X ax X  column). The  X  X ensitivity X  column, by which the IR met-rics have been sorted, shows the percentage of comparisons (among the total of 435*1000 comparisons) that actually satisfied the difference threshold shown in the  X  X bs X  column. Again, the results are generally consistent with the Boot-strap ones: For both data sets, nDCG 1000 ,Q-measureand AveP are the most sensitive metrics, while PDoc 1000 is the least sensitive. In the middle lie nCG 1000 ,G AveP and G Q-measure. The results generalise those by Voorhees [16] who Table 3: Differences and sensitivity based on the swap method (swap rate  X  5%; NTCIR-3 CLIR Chi-nese and Japanese data).
 compared AveP and G AveP. Note also that the estimated performance differences for guaranteeing 5% swap rate or less are lower than those required for achieving ASL &lt;  X  = 0 . 05 with the Bootstrap Hypothesis Tests.
The previous section discussed how the Bootstrap Hy-pothesis Tests can be used to assess the sensitivity of indi-vidual IR metrics. We now show a different application of the Bootstrap to the evaluation of IR metrics.

Kendall X  X  rank correlation is often used for comparing the system rankings according t otwodifferentIRmetrics[7, 12, 17], but the statistical significance of rank correlation is rarely discussed in the IR community. In fact, there is a traditional significance test available for Kendall X  X  rank correlation: Let the number of systems be n s ,andlet  X  be the value of Kendall X  X  rank correlation based on two system rankings. Then, it is known that obeys a normal distribution. Thus, a normal test can easily be applied. (In the case of Spearman X  X  rank correlation s , the test statistic would be t 0 =( | s | X  ( n s  X  2) 1 / 2 which obeys a t -distribution with n s  X  2 degrees of freedom.) Note that the test statistic Z 0 is proportional to |  X  | n : In terms of a two-tailed test with n s = 30 runs, the rank correlation is significant at  X  =0 . 01 if it is over 0.34.
However, there is another way to critically assess the rank correlation values: We could use the Bootstrap Estimate of Standard Error , which reflects the accuracy of an observed statistic. Let  X   X  b be the Bootstrap replicate of  X  computed using the Bootstrap sample Q  X  b instead of Q ,andlet m  X  for Kendall X  X  rank correlation is given by: We can thus quantify the accuracy of an observed rank cor-relation using  X  se boot .
 Figure 12: Kendall X  X  rank correlations with NTCIR-3 CLIR Chinese data.
 Figure 13: Bootstrap estimates of standard error for the rank correlations shown in Figure 12.
 Figure 12 shows Kendall X  X  rank correlations with the NTCIR-3 CLIR Chinese data for all pairs of IR metrics con-sidered in this study. All of the correlation values exceed 0.6, and therefore are statistically highly significant. In particu-lar, the most highly correlated pairs are (Q-measure, AveP) and (G Q-measure, G AveP); Also, (Q-measure, nDCG 1000 ), (nDCG 1000 ,AveP)and(nCG 1000 ,PDoc 1000 ) are highly cor-related. Figure 13 shows the corresponding Bootstrap Es-timates of Standard E rror. These values are approximately ten times smaller than the observed rank correlation val-ues, suggesting that the observed values are quite accu-rate. The results are generally consistent with the tradi-tional significance tests based on Z 0 : For example, the graph shows that the correlations between (Q-measure, AveP) and (G Q-measure, G AveP) are highly accurate, while those be-tween (AveP, G Q-measure), (AveP, G AveP), (Q-measure, G Q-measure) and (Q-measure, G AveP) are less accurate. These results probably reflect the fact that, unlike Arith-metic Means, Geometric Means produce quite different sys-tem rankings when there is change in the topic set as this change yields a  X  X ew X  set of worst-performing topics that will be emphasised, and that the new worst-performing top-ics are different across systems.

The Bootstrap Estimate of Standard Error is widely ap-plicable and useful for assessing the accuracy of any com-putable statistic. For example, it is applicable to the ob-served performance difference  X  d between two IR systems (See Eq. 8). In this case, one can compute using D  X  b from Figure 11 and m D = example,  X  d is not much larger than  X  se boot , then the difference is probably not substantial. However, Bootstrap Hypothesis Tests address such questions more formally.
Savoy [14] used the paired Bootstrap Hypothesis Test and Confidence Intervals, along with traditional significance tests, for comparing two IR strategies. Vu and Gallinari [17] compared, for an XML retrieval task, the Bootstrap Confi-dence Interval of the top run with those of the rest in order to compare the sensitivity of metrics such as Q-measure. In contrast, our methods perform a Bootstrap Hypothesis Test for every system pair and is less dependent on a single run. Furthermore, our methods can estimate the performance dif-ference required for achieving a given significance level, and we examined a wider variety of metrics.

Unlike our new methods, the swap method [8, 15] is not di-rectly related to significance tests: Sanderson and Zobel [13] used significance tests for filtering out some system pairs be-fore applying the swap method; Sakai [12] reported that the system pair ranking according to significance tests and that according to the swap method are not very highly correlated.
The original swap method used sampling without replace-ment; Sanderson and Zobel [13] also used sampling without replacement (although they called their method  X  X election with replacement X  in their paper) but ensured that the two topic sets Q i and Q i were independent; Sakai [10] showed that sampling with and without replacement yield similar re-sults for comparing different IR metrics. While all of these studies used resampled topic sets that are half the size of Q , we used Bootstrap samples with the swap method, so that |
Q  X  b | = | Q | . Hence extrapolation was not required.
As for rank correlation: Previous work that used rank correlation (e.g., [7, 16, 17]) often did not question statisiti-cal significance, possibly because the correlations values are generally very high. This paper showed that the Bootstrap Estimate of Standard Error p rovides a simple way of exam-ining the accuracy of rank correlation.
This paper showed that Bootstrap Hypothesis Tests are very useful not only for comparing IR strategies, but also for comparing the sensitivity of IR metrics. The paired Boot-strap Test is directly applicable to any Arithmetic Mean metric. The unpaired Bootstrap Test has less power, but is directly applicable even to unconventional metrics. This paper also showed that the Bootstrap Estimate of Stan-dard Error provides a simple way of quantifying the accu-racy of rank correlation between two IR metrics. Through experiments with the NTCIR-3 data, we showed that Q-measure, nDCG 1000 and AveP are all very sensitive metrics, while PDoc 1000 is very insensitive. nCG 1000 and Geometric Mean AveP / Q-measure lie in the middle. (But, as men-tioned in Section 2.1, nCG l has a defect.) Moreover, these Bootstrap-based results are in agreement with those based on the heuristics-based stability and swap methods.
Finally, it should be noted that the Bootstrap is not assumption-free: the most basic assumption that it relies on is that the original topics of the test collection are indepent and identi-cally distributed samples from the population P .Moreover, the Bootstrap is known to fail when the empirical distrib-ution based on the observed data is a poor approximation of the true distribution. Clarifying the limitations of our approach will be one of the subjects of our future work. I thank Steve Robertson for his insightful comments and advice on an early version of this paper, Ian Soboroff for guiding me to the Bootstrap Book [3], and the anonymous reviewers for their suggestions. [1] Asakawa, S. and Selberg, E.: The New MSN Search [2] Buckley, C. and Voorhees, E. M.: Evaluating [3] Efron, B. and Tibshirani, R.: An Introduction to the [4] Hull, D.: Using Statistical Testing in the Evaluation [5] J  X  arvelin, K. and Kek  X  al  X  ainen, J.: Cumulated [6] Johnson, D. H.: The Insignificance of Statistical [7] Kek  X  al  X  ainen, J.: Binary and Graded Relevance in IR [8] Lin, W.-H. and Hauptmann, A.: Revisiting the Effect [9] NTCIR: http://research.nii.ac.jp/ntcir/ [10] Sakai, T.: The Effect of Topic Sampling on Sensitivity [11] Sakai, T. et al. : Toshiba BRIDJE at NTCIR-5 CLIR: [12] Sakai, T.: On the Reliability of Information Retrieval [13] Sanderson, M. and Zobel, J.: Information Retrieval [14] Savoy, J.: Statistical Inference in Retrieval [15] Voorhees, E. M. and Buckley, C.: The Effect of Topic [16] Voorhees, E. M.: Overview of the TREC 2004 Robust [17] Vu, H.-T. and Gallinari, P.: On Effectiveness
