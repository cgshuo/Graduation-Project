 This article surveys the contents of the workshop Post-Pr ocessing in Machine Learning and Data Mining: Interpr e tation, V isualiza-tion, Integration, and Related T opics within KDD -2000 : The Sixth ACM SIGKDD Internationa l Confer ence on Knowledge Discovery and Data Mining , Boston, MA, USA, 20-23 August 2000 . The corresponding web site is on First, this s urvey paper introduces the state of the a rt of the work-shop topics, emphasizing that postprocessing forms a significant component in Know ledge Discovery in Databases (KDD ). Next, the a rticle brings up a report on the contents, analysis, discussion, and other aspects regarding this workshop. Afterwards, we survey all the workshop papers. They can be found at (and do wnloaded from) The au thors of this report worked as the or g anizers of the work-shop; the p rogramm e comm ittee was formed b y additional three researches in this field. Knowledge Discovery in Databa ses (KDD ) has become a very attractive discipline both for research and indu stry within the last few years. Its goal is to extract "pieces" of knowledge from usually very lar ge databases. It portray s a robust sequence of procedures that have to be carried out so as to derive reasonable a nd und er-standable results [1 1], [16].
 Th e data that are to be processed b y a knowledge a cquisition algorithm are usually noisy and often inconsistent [4]. Many steps must be performed before the actual data analysis starts. Therefore, certai n p r epr ocessing procedures have to precede the actual data ana lysis process. Next, a result of a knowledge a cquisition algo-rithm, such as a de c ision tree, a set of decision rules, or weights and topology of a neural net, may not be appropriate from the view of custom or comm ercial applications. As a re sult, a concept de-scription (model, knowledge base) produced b y such an indu ctive p rocess has to be usually postprocessed. Postpr ocessing proc edures usually inclu de various pruning routines, rule filtering, or even knowledge integration. All these pr o cedures provide a kind of symbolic filter for noisy and imprecise knowledge derived by an indu ctive algorithm. Therefore, some preprocessing routines as well as postprocessing ones s hould fill up the e ntire chain of data processing.
 Research in kn owledge discovery is s upp osed to develop methods and techniques to process lar ge databases in order to acquire knowledge (which is "hidden" in these databases) that is compact, more or less abstract, but und erstandable, and u seful for further applications. The paper [12] defines knowledge discovery as a nontrivial process of identifying valid, novel, and ultimately und er-standable knowledge in d ata.
 In our und er stand ing, knowledge discovery refers to the overall process of determining useful kno wledge from databases, i.e. extracting high-level knowledge from low-level data in the context of lar ge databases. Knowledge discovery can be viewed as a multi-disciplinary activity because it explo its several research d isciplines of artificial intelligence such as machine learning, pattern recogni-tion, expert systems, knowledge acquisition, as well as mathemati-cal disciplines such as statistics, theory of information, uncertainty processing.
 The e ntire chain of knowledge discovery consists of the following steps: (1) S electing the pr oblem ar ea . Prior to any processing, we first have to find and specify an application domain, and to identify the goal of the knowledge discovery process from the customer's view-point. Also, we need to choose a suitable re p resentation for this goal. (2) C ollecting the data. Next, we have to choose the object repre-sentation, and collect data as formally represented objects. If a domain expert is available, then h e/she could suggest what fields (attributes, features) are the most informative. If not , then the simplest method is to measure e verything available. (3) Pr epr ocessing o f the data. A data set collected is not directly s uitable for indu ction (knowledge acquisition); it comprises in most cases noise, missing v alues, the data a re not consistent, the data set is too lar ge, and so on. Therefore, we need to minimize the noise in data, choose a strategy for handling missing (unkn own) attribute values (see e.g. [5], [7], [14]), use any suitable method for selecting and ordering attributes (features) according to their informativity (so-called attribute mining), discretize/ fuzzify nu-merical (continu ous) attributes [3], [10], and eventually , process continuous classes. (4) Data mining: Extracting pieces of knowledge. W e reach the stage of selecting a paradigm for e xtracting pieces of knowledge (e.g., statistical methods, neural net approach, symbolic/logi cal learning, genetic algorithms). First, we have to realize that there is no optimal algorithm which would be able to process correctly any database. Second, we are to follow the criteria of the end-user; e.g., he/she might be more interested in und erstanding the model ex-tracted rather than its predictive capabiliti es. Afterwards, we a pply the selected algorithm and d erive (extract) new knowledge. (5) P ostpr ocessing of the knowledge derived. The pieces of know-ledge extracted in the previous step c o uld be further processed. One option is to simplify the extracted knowl edge. Also, we can evaluate the extracted knowledge, visualize it, or merely document it for the end user . They are v arious techniques to do that. Next, we may interpret the knowledge and incorporate it into an existing system, and check for potential conflicts w ith previously indu ced knowledge.
 Most research work has been d one in the step 4 . However , the other steps are also important for the successful applicati o n of knowledge discovery in p ractice.
 Postpr o cessing as an important component of KDD consist s of many various procedures and methods that can be categorized into the following g roups. (a) Knowledge filt ering: Rule truncation and po stpruning. If the training data is noisy then the indu ctive algorithm generates leaves of a decision tree or decision rules that cover a very small number of training o bjects. This happens because the indu ctive (learning) algorithm tries to split subsets of training objects to even smaller subse ts that would b e genuinely consistent. T o ov ercome this problem a tree or a decision set of rules must be shrunk , by either postpruning (decision trees) or truncation (decision rules); see e.g. [17]. (b) Interpr e tation and explana tion. Now , we may use the a c-quired knowledge dire ctly for prediction or in an expert system shell as a knowledge base. If the knowledge discovery process is performed for an end-user , we us ually document the derived re-sults. Another possibilit y is to visualize the knowledge [9], or to transform it to an und erstandable form for the user-end. Also, we may check the new knowledge for potential conflicts with previ-ously indu ced kn owledge. In this s tep, we can also summ arize the rules and combine them with a domain-specific knowl edge pro-vided for the given task. (c) Evaluation. After a learning system indu ces concept hypothe-ses (models) from the training set, their evaluation (or testing) should take place. There are se v eral widely used criteria for this purpose: classification accuracy , comprehensibilit y , computational complexity , and so o n. (d) Knowledge integration. The traditional decision-making s ystems have been dependant on a single technique, st rategy , model. New sophisticated decision-supp orting systems combine or refine results obtained from several models, produced u sually by dif ferent methods. This process increases accuracy and the likeli-hood of success. This workshop was addressing an important aspect related to the Data Mining (DM) and Machine Learning (ML) in postprocessing and analyzing knowledge bases indu ced from rea l-world d atabases. Results of a genuine ML algorithm, such as a decision tree or a set of decision r ules, need n ot be perfect from the view of custom or comm ercial applications. It is quite known that a concept descrip-tion (knowledge base, model) discovered b y an indu ctive (knowl-edge acquisition) proc ess has to be usually processed b y a postpruning procedure. Most exist ing procedures evaluate the extracted knowledge, visualize it, or merely docum ent it for the e nd user . Also, they may interpret the knowledge and incorporate it into an existing system, and check it for potential conflicts with p revi-ously derived knowledge (mode ls). Postprocessing procedures thus provide a kind of "symbolic filter" for noisy , imprecise, or "non-user-friendly" knowledge derived b y an indu ctive a lgorithm. Con s equently , the postprocessing tools are complementary to the DM algorithms and always h elp the DM algorithms to refine the acquired knowledge. Usually , these tools exploit t echniques that are not genuinely logical, e.g., statistics, neural nets, and others. The presentation and discussion within this workshop revealed the following: Famili: "Post-processing: The real challenge") t hat provided an overview of postprocessing. It discussed some typical applications of these techniques to real-world data and explained why we need and wher e we use the results of postprocessing. Some e xamples from his past experience were given, too.
 Fourteen research papers were submitted to this workshop. Each paper was reviewed b y three members of the programm e comm it-tee. Afte r r eviewing, eight of them were selected for pub lication, i.e. the a cceptance rate was 57%.
 The authors of this report worked as t he or g anizers of the work-shop; the programm e comm ittee was also formed b y additional three researches in this field: Petr Berka , Laboratory of Intelligent Systems, University of Eco-nomics, Prague, Czech Repub lic email: berka@vse.cz http://li sp.vse.cz/~berka M arko Bohanec, Institute Jozef Stephan, Jamova 37, Ljub ljana, Slovenia email: marko.bohanec@ijs.si http://www -ai.ijs.si/MarkoBohanec/mare.html W .F .S. (Skip) Poehlman, McMaster University , Hamilton, Canada email: skip@church.cas.mcmaster .ca The a uthors of this paper exp lain the motivation for a post-processing phase to the association rule mining algorithm when p lugged into the knowledge discovery in d atabases process. They focus on p rocessing o f lar ge sets of association rules. The technique of association rules allows one to discover intra-transactional records [1]. Over the last couple of years, one could see a sur ge in research on improving the a lgorithmic performance o f the original algorithms, among them the a uthor selected the Apriori algorithm [2] as a starting point.
 A strong element of the a ssociation rule mining is its abilit y to disco ver all associations that exist in the transaction d atabase. Unfortunately , this leads to sets of very lar ge number of rules that are hard to und erstand. T o ov ercome this drawback, the a uthors exploit the postprocessing and provi de a basic rationale for post-processin g the patterns generated b y an association rule mining process. Chun g and Lui also work in the field of postprocessing of associa-tion rules. They discuss the problem of mining association rules with multiple minimum supp ort. Their algorithm is applied in such a way that the low-level rules have enough minimum supp ort while the high-level rules are prevented from combinatorial explosion. The authors utili ze the generalized associat ion rules [15] and multiple-level association rules [13]. They developed a postpro-cessing framework for finding frequent itemsets with multiple m inimum supp orts. The explanation is accompanied by man y graphs, tables, and illustrative e xamples. This paper introduces two meta-learning methods of combiner and stacked generalizer [8] in the indu ctive algorithm CN4 [6] with six routines for unkn own attribute values processing.
 In order to improve the performance of learning algorithms the idea of multistrategy (meta-strategy) learning was initiated. The princi-ple of the combiner and stack generalizer consists of combining the decisions of several classifiers by a meta-classifier (a  X  X upervisor X  classifier). The experiments proved that such a knowledge combi-nation exhibits better performance than that of single classifiers. The author exploits the above knowledge combination mechanism for processing o f unkn own (missing) attribute values. It is known that no routine for unkn own attribute values processing is the best for all potential databases. One possible solution to this problem is to try experimentally which routine fits a given d atabase. Another solution was proposed by the author . A database is processed by all six routines (that are available in the covering algorithm CN4). As a result we get six classifiers; the meta-classifier ( combiner or stack generalizer) combines the decisions of these classifiers to get the final decision. This paper introduces a new strategy that allows one to m odify (refine) rule qualiti es during the classification of unseen objects. If a classifier uses an unordered se t of decision rules a problem a rises concerning what to do if the classification of an un seen object  X  X ires X  rules of dif ferent classes. One pos s ible solution consists in calculating a numerical factor that explicitly indicates a quality (predictive power) of each rule, giving thus a higher priority to the rule (s) with a higher quality . In existing models, the rule qualiti es are calculated b y a learning (data mining) algorithm and remain constant during the phase of classification.
 The refinement is carried out in a feed-back loop so that it can be viewed as a postprocessing procedure. This is another paper bringing up the a ssociation rules. Interpret-ing the discovered kn owledge to g ain a goo d und erstanding o f the domain is one of the important phases of KDD postprocessing. T o expound a set of association rules is not a trivial task since the size of the complete set of these rules is usually very lar ge. The a uthors describe their system DS-W e b that assists users in interpreting a set of association rules. They firstly summ arize the set of rules in order to build a hierarchical structure for easy brows-ing o f the complete set of rules. Then they propose to pub lish this hierarchy of rules via multiple web pages connected by hypertext links. This paper introduces V izLearn, a visually-interactive machine learning system. This exploratory system can visualize machine learning models and data. It treats data as if it were from a geo-graphical source by augmenting the original model with so-called fields. The system visualizes certain p atterns at-a-glance that would otherwise be dif ficult to grasp by using non-visual methods. V izLearn uses Bayesian networks for the knowledge representa tion because it permits flexibilit y in queries for classification. Also, it can handle both discrete and real-value data . It can b e used to process unkn own (missing) values. The author is currently extend-ing V izLearn b y probabilistic data brushing and abstraction. The paper comprises quite a few figures that ill ustrate the charac-teristics of the a uthor X  X  s ystem. Intelligen t t utoring systems are based on a user and d iagnostic models. These models must be validated by using a data base of user test results. Consequently , the e ntire model is to be learned from a database of the user test data.
 The authors propose a new model for intelligen t tutorial system and discuss how to obtain data that specify this model, including their refinement. Another paper dealin g with the a ssociation rules. The a uthors realized that the size of a set of association rules is usually ex-tremely lar ge. Therefore, there exists a n eed to prune the discov-ered rules according to their degree of interesti ngness. The inter-estingness is, in fact, equivalent to the idea of the ru l e qualiti es discussed in another paper of this workshop.
 The authors in this paper present and compare various interesting-ness measures for association patterns that are proposed in statis-tics, machine learning, and data mining. They also introduce a new metric and show that it i s highly linear with respect to the correla-tion coef ficient for many interesting association p atterns. [1] Agrawal, R., Imielinski, T ., and Swami, A. Mining associa-[2] Agrawal, R., and Srik ant. R. Fast algorithms for mining [3] Berka, P ., and Bruha, I. Empirical comparison of various [4] Brazdil , P ., and Clark, P . Learning from imperfect data. In: [5] Bruha, I., and Franek, F . Comparison of various routines for [6] Bruha, I., and Kockova, S. A supp ort for decision making: [7] Cestnik, B., Kononenko, I., and Bratko, I. A SS IST ANT [8] Chan, P ., and Stolfo, S. Experiments on multistrategy learn-[9] Cox, K., Eick, S ., and W i lls, G. V i sual data mining: recog-[10] Fayyad, U., and Irani, K.B. On the handling o f continuous-[1 1] Fayyad, U., Piatetsky-Shapiro, G., and Smyth, P . From data [12] Fayyad, U., Piatetsky-Shapiro, G., Smyth, P ., and [13] Han, J., and Fu, Y . Discovery of multiple-level association [14] Quinlan, J.R. Unkn own attribute values in ID3. International [15] Srikant, R., and Agrawal, R. Mining g eneralized association [16] Stolorz, P . et a l. Fast spatio-temporal data mining o f lar g e [17] T oivonen, H .et al. Pruning and group ing o f discovered
