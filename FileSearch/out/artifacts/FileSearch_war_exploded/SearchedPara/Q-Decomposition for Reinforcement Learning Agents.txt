 Stuart Russell  X  X  X  X  X  X  X  X  X  X  @  X  X  X  .  X  X  X  X  X  X  X  X  X  X  X  .  X  X  X  X  Andrew L. Zimdars  X  X  X  X  X  X  X  X  X  X  @  X  X  X  .  X  X  X  X  X  X  X  X  X  X  X  .  X  X  X  X  tasks is to decompose the monolithic agent architecture int o a collection of simpler subagents and provide an arbitrator that combines the outputs of these subagents. The principal architectural choices in such a design concern the nature of the information communicated between the arbitrator and the subagents and the method by which the arbitrator se-lects an action given the information it receives. ing a very simple environment (Figure 1). The agent starts in state S 0 and can attempt to move Left , Up , or Right , or it can stay put. With probability , each movement action has no e ff ect; otherwise, the agent reaches a terminal state with rewards of dollars and / or euros as shown. If we assume rough parity between dollars and euros, then the optimal policy is clearly to go Up . The question is how to achieve this with a distributed architecture in which one subagent cares only for dollars and the other only for euros. requires each subagent to recommend an action to the arbi-trator. In the simplest such scheme, the arbitrator chooses one of the actions and executes it (Brooks, 1986). The problem with this approach is that each subagent may sug-gest an action that makes the other subagents very unhappy; there is no way to find a  X  X ompromise X  action that is rea-sonable from every subagent X  X  viewpoint. In our example, the dollar-seeking subagent will suggest Left whereas the euro-seeking subagent will suggest Right . Whichever ac-tion is chosen by command arbitration, the agent is worse o ff than it would be if it went Up .
 mand fusion , whereby the arbitrator executes some kind of combination (such as an average) of the subagents X  recom-mendations (Sa ffi otti et al., 1995; Ogasawara, 1993; Lin, 1993; Goldberg et al., in press). Unfortunately, fusing the subagents X  actions may be disastrous. In our example, aver-aging the direction vectors for Left and Right yields NoOp , which is the worst possible choice. Furthermore, command fusion is often inapplicable X  X s, for example, when two chess-playing subagents recommend a knight move and a bishop move respectively.
 pointed out previously by proponents of utility fu-sion (Rosenblatt, 2000; Pirjanian, 2000). In a utility-fusion agent, each subagent calculates its own outcome probabili-ties for actions and its own utilities for the outcome states . The arbitrator combines this information to obtain a global estimate of the utility of each action. Although the seman-tics of probability combination is somewhat unclear, the method does make it possible to produce meaningful com-promise actions. Rosenblatt reports much-improved per-formance for an autonomous land vehicle, compared to command arbitration. Unfortunately, his paper does not identify the semantics or the origin of the local utility fun c-tions. We will see below that global optimality requires some attention to communicating global state when updat-ing local utilities, if fusion is to work.
 quires each subagent to indicate a value, from its perspec-tive, for every action. That is, subagent j reports its action values Q j ( s , a ) for the current state s to the arbitrator; the arbitrator then chooses an action maximizing the sum of the Q j values. In this way, an ideal compromise can be found. decomposition is that the agent X  X  overall reward function r ( s , a , s 0 ) can be additively decomposed into separate re-wards r j ( s , a , s 0 ) for each subagent X  X hat is, r ( s P j r j ( s , a , s 0 ). Thus, for a mobile robot, one subagent might be concerned with obstacle avoidance and receive a nega-tive reward for each bump, while another subagent might be concerned with navigation and receive a positive reward for making progress towards a goal; the agent X  X  overall re-ward function must be a sum of these two kinds of rewards. Of course, additive decomposition can always be achieved by choosing the right subagent reward functions. Heuris-tically speaking, we are interested in decompositions that meet two criteria. First, we want to be able to arrange the  X  X hysical X  agent design so that each subagent receives just its own reward r j . Second, each subagent X  X  action-value function Q j ( s , a ), which predicts the sum of r j rewards the agent expects to receive over time, ought to be simpler to express, and easier to learn, than the global Q -function for the whole agent. The arbitrator receives no reward signals and maintains no Q -functions; it only sums the subagent Q values for a particular state to determine the optimal actio n. In many cases, it should be possible to design subagents so that they need sense only a subset of the state variables and need express preferences only over a subset of the compo-nents that describe the global actions.
 globally optimal, even if it results from a distributed de-liberation process. We show that this is achieved if each subagent X  X  Q j -function correctly reflects its own future r rewards assuming future decisions are made according to the global arbitration policy . The next question is how to arrange for each subagent to learn the right Q j -function us-ing a local reinforcement learning procedure, ideally one that does not need to access the Q j -functions or rewards of the other subagents.
 conventional Q -learning algorithm (Watkins, 1989), global optimality is not achieved. Instead, each subagent learns the Q j values that would result if that subagent were to make all future decisions for the agent. This  X  X llusion of control X  means that the subagents converge to  X  X elfish X  es-timates that overestimate the true values of their own re-wards with respect to a globally optimal policy. For our dollar / euro example, local Q -learning leads in some cases to a global policy that chooses NoOp in state S 0 . simple observation that global optimality is achieved by lo -cal reinforcement learning with the Sarsa algorithm (Rum-mery &amp; Niranjan, 1994), provided that on each iteration the arbitrator communicates its decision to the subagents. This information allows the subagents to become realistic, rather than optimistic, about their own future rewards. that are somewhat less trivial than the dollar / euro exam-ple. The first is the well-known  X  X acetrack X  problem with two subagents: one wants to make progress and the other wants to avoid crashes. The second example is a simulated fishery conservation problem, in which several subagents (fishing boats) must learn to cooperate to extract the maxi-mum sustainable catch. This example illustrates how con-flicts between selfish actors can lead to a  X  X ragedy of the commons X . Markov decision process  X  X  , A , P , r , X   X  , with (finite) state space S , actions A , transition measure P , bounded reward function r : S  X  A  X  S  X  R , and discount factor  X   X  (0 , 1]. From a  X  X ocal X  viewpoint, however, we assume that the reward signal is decomposed into an n -element vector r of bounded reward components r j , each defined over the full state and action space, such that subagent j receives r and such that r = P n j reward component r j and policy  X  the expected discounted future value Q  X  function, it follows that the action-value function Q  X  for the entire system, given policy  X  , is the sum of the subagents X  action-value functions:
Q  X  ( s , a ) = E r ( s , a , s 0 ) +  X  Q  X  ( s 0 , X  ( s 0 )) It also follows that an arbitrator that has to evaluate the global action-value function for a particular state s i only needs to receive the vector Q j ( s i ,  X  ) from each subagent. fixed point Q  X  : S X A X  R satisfying the Bellman equation
Q  X  ( s , a ) = E h P n j The policy  X   X  : S  X  A corresponding to this action-value function is the optimal policy for the MDP. 3.1. Local Q learning: The illusion of control is updated under the assumption that the policy followed by the agent will also be the optimal policy with respect to Q In this case, the value update is the usual Q -learning update (Watkins, 1989), but relies only on value information local to each subagent. In detail, (  X  that a t is used only to associate the reward signal r j with a particular action; the learner does not require a t to evaluate the discounted future value  X  max a dure by  X  Q j and the corresponding policy by  X   X  j . 3.1.1. C  X  X  X  X  X  X  X  X  X  X  X  X  X  X  over all subagent values, and even though the arbitra-tor may never execute this policy, the following theorem demonstrates that this sort of o ff -policy update leads to the convergence of the Q j estimates to a collection of locally greedy ( X  X elfish X ) estimates.
 Theorem 1. (Theorem 4 in (Tsitsiklis, 1994).) Suppose that each ( s , a )  X  S X A is visited infinitely often. Under the update scheme described in equation (2) , each Q j will converge a.s. to a  X  Q j satisfying  X 
Q j ( s , a ) = X which are satisfied in the current setting, and all of which are omitted for brevity. This update is analogous to that proposed by Stone and Veloso (1998), who assume that the action space may be partitioned into subspaces, one for each subagent, and that rewards realized by each subagent are independent given only the local action of each sub-agent. 3.1.2. Q  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X   X  X  X  X  X  X  X  X  X  its action-value estimates according to the selfish Q update of equation (2). Let  X  Q d denote the action-value function for the dollars subagent, and  X  Q e the action value function for the euros subagent. The policy s 0 7 X  Left optimizes  X  Q The values are symmetric for  X  Q e , with the optimal action for  X  Q e from s 0 being Right .
 a perverse thing happens. For certain values of and  X  , the  X  X ptimal X  behavior is NoOp , even though an agent that never tries to escape can never achieve a reward: When the discount factor is su ffi ciently large (  X  &gt; the sum of selfish action-value estimates indicate that the agent is better o ff doing nothing than settling on one of the absorbing states. Informally, the dollars subagent prefer s Left , but assigns a fairly high value to NoOp because it can go Left at the next time step. The euros agent like-wise prefers Right , but assigns a fairly high value to NoOp because it can go Right after that. As a result, the expected value of receiving both the dollars and the euros on the next time step dominates the value of receiving one or the other on the current time step, even though only one can occur. The  X  X llusion of control X  leads to an incorrect policy. 3.2. Local Sarsa: Global realism (discounted) future rewards by assuming that it could have exclusive control of the system. While this may be a use-ful approximation (as when value functions depend on sub-spaces of S X A with small overlap), it does not in general guarantee that  X  Q will have the same values, or yield the same policy, as Q  X  . The relationship between  X  Q and Q discussed below. For now, consider an alternative that give s a collection of Q j functions whose sum converges to Q  X  . reward function to the optimal value function Q  X  defined in Equation (1): To converge to Q  X  must reflect the globally optimal policy. Update schemes that do this must replace the locally selfish updates de-scribed above with updates that are asymptotically greedy with respect to Q  X  .
 which requires on-policy updates, suggests one approach. Rather than allowing each subagent to choose the succes-sor action it uses to compute its action-value update, each subagent uses the action a t the successor state s t This requires that the arbitrator inform each subagent of th e successor action it actually followed, but the communica-tion overhead for this is linear in the dimension of A . 3.2.1. C  X  X  X  X  X  X  X  X  X  X  X  X  X  X  enforces convergence to Q  X  in the case when an agent maintains a single action-value function and acts greedily in the limit of infinite exploration.
 Lemma 1. (Rummery &amp; Niranjan, 1994) Suppose that an agent receives a single reward signal r : S X A X S  X  R , and maintains a corresponding Q function by the update If all ( s , a )  X  X  X A are visited infinitely often, S and finite, and the policy pursued by the agent is greedy in the limit of infinite exploration, then under the update scheme of equation (6) , Q will converge a.s. to Q  X  as defined in equation (1) .
 procedure yields estimates converging to Q  X  in equation (4), when the arbitrator asymptotically choose s the optimal action.
 Theorem 2. Suppose that all ( s , a )  X  S  X  A are visited infinitely often and the policy pursued by the arbitrator is greedy in the limit of infinite exploration. Suppose also tha t S and A are finite. Under the update scheme of equation (5), each Q j will converge a.s. to a Q  X  (4).
 vides convergence to the global optimum, it su ffi ces to ob-serve that the local Sarsa update is just the monolithic Sars a update in algebraic disguise: The individual Q j converge to Q  X  which they are updated converges to  X   X  , so that in the limit, they are updated under a fixed policy. 3.2.2. S  X  X  X  X  X  X   X  X  X  X   X  X  X  X  X  X  X  X  X  icy for the dollars-and-euros world. The optimal policy is to choose Up from s 0 , and the net value of NoOp in s 0 van-ishes because the update method of equation (5) requires that all subagents assume a single policy.
 for all  X  [0 , 1] and all  X   X  (0 , 1). In the undiscounted case, the supervisor incurs no penalty for repeatedly choos -ing NoOp for a very long time, but it will never achieve a 3.3. Remarks definition of the selfish action-value function:  X  Q ( s , a ) = X The sum of selfish components is therefore an optimistic estimate of the optimal value over all components. This can lead to overestimates of future value, as in the case of the three-state gridworld. For equality of the selfish and globally-optimal policies, we require because equivalent policies will converge to identical action-value estimates. 4.1. Description trade o ff his speed against the cost in time and money of damaging his equipment by colliding with the wall or with other racers. However, these goals are opposed to one an-other: the safest race car driver is the one who never starts his first lap, and the fastest one looks to win at all costs. excluded region (the  X  X nfield X ) in the center, so that the sur -rounding open spaces (the  X  X rack X ) are of uniform width. Represent the state of an agent by its position (the ( x , dex of the grid square it currently occupies) and velocity (in squares per unit of time). At each time step, the agent may alter each component of its velocity by  X  1, 0, or + 1 unit / second, giving a total of nine actions. Actions succeed with probability 0 . 9; the agent accelerates 45  X  to the left or right of the desired vector with probability 0 . 03 each, the agent accelerates 90  X  to the left or right of the desired vec-tor with probability 0 . 01 each, and the agent does nothing with probability 0 . 02. If an agent collides with a wall, its position is projected back onto the track, and each com-ponent of its velocity is reduced by 1. If the agent does not accelerate away from the wall, it will continue to  X  X lip X  parallel to the wall, but will not cross into the infield. lap, and a penalty of  X  1 for each collision with the wall. An agent also receives a shaping reward (Ng et al., 1999) proportional to the measure of the arc swept by its action. 4.2. Implementation track with a 15  X  20 infield. Training consisted of 4000 episodes, with ten test episodes occurring after every ten training episodes. To provide an incentive for finishing quickly, experiments assumed a penalty of  X  0 . 1 per time step, but no discount factor. Exploration occurred uni-formly at random; the exploration rate decreased from 0 . to 0 . 0625 over the first 2500 episodes, and remained con-stant thereafter. The update step size  X  diminished from 0 . 3 to 0 . 01 over the 4000 episodes. Both training and test episodes were truncated at 1000 steps; preliminary tests in -dicated that an agent could get stuck by hitting a wall at low speed, then choosing not to accelerate in any direction. 4.3. Results ing local Q and local Sarsa updates, as well as global Q and global Sarsa updates. This is not an ideal domain to illustrate the suboptimality of local Q updates, because the dynamics of the racetrack world couple the objectives of completing a lap quickly and avoiding collisions. Not only does a racer su ff er a negative reward when it collides with a wall, but it loses speed, which diminishes the discounted value of its eventual completion reward. However, the local Sarsa learner outperformed the local Q learner by roughly 1 standard deviation after 4000 training episodes. tween global and local Sarsa. As the above algebra sug-gests, the two methods should yield identical performance under identical representations, and this is borne out by Figure 2. Global Q learning, although not hobbled by the  X  X llusion of control X , underperforms Sarsa. The results of individual episodes suggest that the global Q learner suf-fered more collisions than the Sarsa learners, even though the number of steps required to complete a lap compared favorably, and this resulted in the di ff erence in value. 5.1. Description mercial fishery. A commercial fishing fleet wishes to max-imize the aggregate discounted value of its catch over time, which requires that it show at least some concern for the sustainability of the fish population. Individual fishermen , however, may choose to act selfishly and maximize their own profit, assuming that others in the fleet will reduce their catch for the viability of the fishery. If all fishermen follow the selfish policy, the result is a  X  X ragedy of the com-mons X : fish stocks collapse and the fishery dries up. the local Q and Sarsa algorithms, consider a fishery with n boats that alternates between a  X  X ishing X  season and a  X  X at-ing X  season. Assume that a fish population of size f ( t ) re-produces according to a density-dependent model (Ricker, 1954): where R for a fish population without immigration or em-igration is the di ff erence between the birth rate and the death rate, and f max is the  X  X arrying capacity X  of the en-vironment (the population at which growth diminishes to 0). At the beginning of each fishing season, the fish are assigned to one of n regions with equal probability. Let f ( t ) denote the number of fish in the j th region at time t , with P j f j ( t ) = f ( t ). The  X  X isheries commissioner X  (arbi-trator) selects the proportion of the season a j ( t ) that each boat will fish, based on the total fish population f ( t ). Let  X   X  (0 , 1) denote the e ffi ciency of each fishing boat. The number of fish caught by the j th boat, c j ( t ), is distributed alized by boat j at time t is given by for some constant  X  . The cost of fishing increases quadrat-ically with a j to reflect the increase in crew and equipment fatigue over the course of a season. 5.2. Implementation 1 . 5  X  10 5 fish, a population growth rate of R = 0 . 5, a carrying capacity of f max = 2  X  10 5 fish, an e ffi ciency of  X  = 0 . 98, and a maximum fishing cost of  X  = 10 3 . Ex-periments proceeded over 1000 episodes, and each episode terminated when fewer than 200 fish remained or when the fishery had survived 100 years. With a discount factor of  X  = 0 . 9, the contribution of the 100th episode to the initial value was reduced by a factor of 2 . 65  X  10  X  5 . The step size for learning updates was 0 . 1 for the first 100 episodes, and 0 . 05 thereafter. Exploration occurred uniformly at random; the exploration rate decreased from 0 . 4 to 0 . 05 over the first 400 training episodes, and remained constant thereafter. monolithic agents. For the decomposed agents, each action-value estimate Q j depended on three features: the current population f ( t ), the proportion a j ( t ) fished by the j th boat, and the total proportion a by the other boats, so that Q j was defined over a 3-imator represented the action-value estimates Q j for the de-composed agents, with values updated by bounded gradient steps. The monolithic agents required that their value func -tions span the full ( n + 1)-dimensional space, a task that would have been impossible using an RBF approximator with the same resolution as was used in the 3-dimensional layer of 100 nodes was used. 5.3. Results selfish and optimal decomposed learners in the fishery problem, as well as monolithic Q and Sarsa learners. Ev-ery ten training episodes, ten test episodes were executed and the values averaged. As anticipated, selfish updates quickly fell victim to the  X  X ragedy of the commons X  (Fig-ure 4). Each boat exhausted the fish stocks in its region because the fishery had computed the value of this self-ish policy without regard for the actions of the other boats. As a result, the fish population crashed within a couple of years. Concurrent Sarsa X  X   X  X ealistic X  updates led to a sustainable policy. Each boat only harvested a fraction of the fish in its region, so enough remained for the popula-tion to recover in the next mating cycle. Both monolithic learners demonstrated slower value improvement than the decomposed Sarsa learner because they represented exam-ples over the joint state and action space and not the re-duced subspaces of the decomposed learners. However, the monolithic Q learner did not su ff er from the di ffi faced by the selfish decomposed learner. forcement learning in two directions: it identifies a natura l decomposition of action-value estimates in terms of addi-tive reward signals, and considers the tasks of action selec -tion and value-function updates in terms of communication between an arbitrator and several subagents. Both direc-tions provide broad avenues for future work.
 ing notion of  X  X ubagents, X  may seem superficially related to particular methods for representing value functions, or for aligning the interests of multiple subagents. Q -decomposition only requires that value function updates assume a particular form to guarantee optimal agent be-havior. In some cases, like the fishery world, this additive decomposition results in a more compact value function. Other authors (Koller &amp; Parr, 1999) have explored approx-imations that represent the true value function as a linear combination of basis functions, with each basis function defined over a small collection of state variables. In order to maintain these approximations, value function updates must be projected back onto the basis at every time step, because the evolution of the MDP over time can introduce dependencies not represented in the basis. However, some subagent rewards may be unconditionally independent of some components of the state, or may depend only on ag-gregate values, as in the fishery world. By exposing these independencies, Q -decomposition furnishes a means of se-lecting basis components without sacrificing accuracy. ing Q -decomposition with graphical models of conditional utilities. While it may be possible to elicit and main-tain conditional utilities for one-step problems (Bacchus &amp; Grove, 1995; Boutilier et al., 2001), the dependencies introduced by both the transition model and reward de-composition become more di ffi cult to manage over time in sequential tasks. These dependencies limit the extent to which a problem may be exactly decomposed. Guestrin et al. (2002) avoid the di ffi culties of exact updates by fix-ing an approximation basis and a  X  X oordination graph X  ex-pressing dependencies shared between Q -components. Q -values may be computed in this setting by summing the factors of the coordination graph. The coordination struc-ture allows optimal selections to be communicated only to those components that require them, eliminating a cen-tral arbitrator for action selection. Gradient updates for a parametric basis follow because the gradient of the sum of components is a sum of gradients, one per basic term. Message-passing techniques for value estimation and value updating have clear advantages over methods requiring a central arbitrator, and deserve exploration in the context of Q -decomposition and exact updates.
 all the time to choose actions. In this sense, it di ff ers from  X  X elegation X  techniques like feudal reinforcement learni ng (Dayan &amp; Hinton, 1993), MAXQ (Dietterich, 2000), and the hierarchical abstract machines of Parr (1997) and An-dre (2002). These methods also decompose an agent into subagents, but only one subagent (or one branch in a hier-archy of subagents) is used at a time to select actions, and only one subagent receives a reward signal. Feudal RL sub-divides the state space at multiple levels of resolution, an d each of the subagents at a particular resolution assigns re-sponsibility for a subset of its state space to one of its  X  X as -sals X . MAXQ-decomposed agents and hierarchical abstract machines partition the decision problem by tasks, giving a hierarchical decomposition analogous to a subroutine call graph. Each component in this procedural decomposition maintains a value function relative to its own execution, an d does not receive a reward when it is not in the call stack. Q -decomposition complements these methods: a subagent in a delegation architecture could maintain a Q -decomposed value function.
 reinforcement learning into the  X  X ulti-body X  setting, and suggests a spectrum of learning methods distinguished by the degree of communication between modules and a cen-tral arbitrator. At one extreme, traditional RL methods as-sume closely-coupled components: a single value function defining a monolithic policy. Q -decomposition allows for multiple value functions, each residing in a subagent, but still requires each subagent to report its value estimates t o an arbitrator, and receive in turn the action that the arbi-trator chooses. Further relaxations of the communications requirements of Q -decomposition include action decom-position and partial observability of actions. In the for-mer case the action space A may be partitioned into sub-spaces A j corresponding to subagent reward components; in the latter, subagents maintain histories of observation s  X  o (1) ,..., o ( t  X  1) , o ( t )  X  from which they must estimate a to compute value updates.
 arbitrator, making optimality more di ffi cult to achieve, but similar issues of communication between participants arise. Traditional game theory considers the uncommu-nicative extreme of this spectrum, where participants do not share policies or value functions. Claus and Boutilier (1998) have proposed the  X  X ndividual learner X  and  X  X oint action learner X  concepts to distinguish between agents in cooperative games that choose actions to maximize indi-vidual rewards, and agents that choose actions to maximize joint rewards. A joint action learner observes the actions of its peers, and maintains belief state about the strate-gies they follow, with the goal of maximizing the joint reward. An independent learner ignores the actions of its peers when  X  X ptimizing X  its policy, analogous to a local Q learner. There is still no central arbitration mechanism, b ut inverse reinforcement learning techniques (Ng &amp; Russell, 2000) might be used to deduce the policies of other agents and bridge the communications gap.
 paper has evaluated two points in the continuum of pos-sible representations. These fairly simple-minded ap-proaches nonetheless provide evidence of the value of Q -decomposition as a tool for functional decomposition of agents, and suggest a variety of future work.

