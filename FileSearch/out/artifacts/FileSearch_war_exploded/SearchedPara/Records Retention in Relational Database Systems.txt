 The recent introduction of several pieces of legislation man-dating minimum and maximum retention periods for cor-porate records has prompted the Enterprise Content Man-agement (ECM) community to develop various records re-tention solutions. Records retention is a significant subfield of records management, and legal records retention require-ments apply over corporate records regardless of their shape or form. Unfortunately, the scope of existing solutions has been largely limited to proper identification, classification and retention of documents, and not of data more generally.
In this paper we address the problem of managed records retention in the context of relational database systems. The problem is significantly more challenging than it is for doc-uments for several reasons. Foremost, there is no clear def-inition of what constitutes a business record in relational databases; it could be an entire table, a tuple, part of a tu-ple, or parts of several tuples from multiple tables. There are also no standardized mechanisms for purging, anonymizing and protecting relational records. Functional dependencies, user defined constraints, and side effects caused by triggers make it even harder to guarantee that any given record will actually be protected when it needs to be protected or ex-punged when the necessary conditions are met. Most im-portantly, relational tuples may be organized such that one piece of data may be part of various legal records and subject to several (possibly conflicting) retention policies.
We address the above problems and present a complete so-lution for designing, managing, and enforcing records reten-tion policies in relational database systems. We experimen-tally demonstrate that the proposed framework can guaran-tee compliance with a broad range of retention policies on an off-the-shelf system without incurring a significant per-formance overhead for policy monitoring and enforcement. H.2.7 [ Database Administration ]: Security, integrity, and protection; K.5.2 [ Legal Aspects of Computing ]: Gov-ernmental Issues -Regulations Legal Aspects, Management Privacy, records retention, legal compliance, business records, relational systems
Storage, management, and efficient access to data have traditionally been of prime importance for businesses. Any loss of records has meant the loss of valuable business intel-ligence. However in the past decade, increasing awareness of privacy related issues and the introduction of several pieces of legislation mandating strict records retention periods have forced organizations to move away from the  X  X tore every-thing X  paradigm. Customers demand that their personal information not be indefinitely retained by businesses, and government legislations require that certain types of records, such as those related to taxation, be preserved for specified periods of time. This puts database systems in an awkward situation of having to store business records in tuples and tables that are relationally intertwined (e.g., through foreign keys) but with a diverse set of policies applicable to them.
The problem is further exacerbated by the fact that there is no clear notion of a  X  X usiness record X  in a database sys-tem, which means that there is no meaningful way of pro-tecting records and deleting them after they are no longer required. 1 For example, a typical physical document such as an invoice may be identified by a row in one table, but its contents in a database with a normalized schema may be spread across various tables. Various departments of the business may consider selected attributes of the invoice or its related line item tuples as sensitive information and may want to expunge or protect them as necessitated by law. With conflicting requirements arising through the intertwin-ing of complex records, the problem of mediating these re-quirements and determining whether they can be enforced becomes a significant challenge. As more and more policies are integrated from different departments, the problem of maintaining a consistent and enforceable set of data reten-tion rules can easily become unmanageable.

Not complying with legally mandated records retention policies can lead to criminal charges against individuals or corporations, and this greatly increases the gravity of the problem. Even accidentally destroying records and being
It is important to note that in this paper we distinguish between  X  X ecords, X  which can be thought of as  X  X usiness doc-uments, X  and  X  X uples X  that are stored in relations. unable to reproduce them in civil litigation scenarios can lead to the imposition of heavy fines [14]. At the same time retaining certain records for too long can be a source of li-ability or a violation of published records retention policy. Since records management solutions for relational databases do not exist, organizations that choose to enforce retention policies on relational data are forced to do so in an ad-hoc manner. Legislation is interpreted by a privacy officer who is aware of the database schema, and then data deletion specifications are handed down to a programmer. Deleting records when they have expired is most likely done through the use of scheduled SQL execution tasks. To implement protective data retention measures, for example those leg-islated by the Sarbanes-Oxley Act [1] mandating corporate financial records to be maintained for specified periods, an elaborate set of logging (or replicating) triggers is the only workable solution for database administrators.

Such an approach to hand-coding a corporate retention policy in a database system has several shortcomings. First, it is only viable if there is a single corporate privacy offi-cer providing the interpretation of data retention rules. In scenarios involving large, complex and possibly federated schemas, it is unlikely that one administrator will be re-sponsible for understanding and interpreting legal records retention requirements for all functional areas of the orga-nization. Second, administrators are not able to verify the effect of their policies automatically. For example, batched execution of data purging queries may attempt to delete records that are protected by data retention triggers. Policy conflicts are nearly impossible to detect beforehand when dealing with arbitrarily written queries. Consequently, ad-hoc conflict resolution becomes inevitable. The task of man-aging and maintaining a large number of scheduled tasks and retention triggers is not only challenging, but can also rep-resent a significant overhead cost in business situations with evolving schemas.

In this paper we present a complete end-to-end solution for managed data retention in relational database systems. Our key contribution is that of answering three fundamen-tal questions pertaining to systematic records retention in database systems: Our proposed framework answers the above questions, and it builds an expressive and flexible solution for records reten-tion that can be implemented on top of existing mainstream database management systems.
Information in a database that should be protected or deleted under a policy must be identified by subject experts in formats that are meaningful to them. Since each policy maker can interpret individual pieces of data differently, we propose a view based records management system.

Definition 1. A relational record is a logical view over a fixed relational schema.

Note that this definition is suitably expressive, as it gives policy makers the ability to define a record for policy en-forcement at any level of granularity. Although this defini-tion obscures the distinction between a single record and a collection of records, we believe that this is the most suitable method of identifying valuable business information that caters to the needs of all users. Such a framework allows each user to specify his or her own interpretation of a given record and the valuable information contained therein. Once important records are identified by users, retention policies can then be systematically implemented on the relevant view definitions.

To delimit the scope of the obligations on the data stored by an organization in a relational database, we propose that all records be explicitly declared. For example, if a busi-ness has very specific retention obligations on the taxation information contained in paid invoices issued to Jones Cor-poration, it can declare a record as follows: DEFINE RECORD R 1 AS SELECT * FROM Invoice NATURAL JOIN LineItem WHERE Recipient =  X  X ones Corp. X  AND Paid = true This example of a record declaration captures the data con-tained in all paid invoices issued to Jones Corp. Observe that relational records are mu ch more flexible than tradi-tional physical records (such as a single invoice or telephone bill) and can encapsulate a collection of related traditional physical records in a very comp act definition. Furthermore, using views provides a declarative means for policy makers to specify policy-relevant pieces of data that may be spread across numerous physical records.

An important requirement for any records retention frame-work is to accommodate the notion of temporality. The vast majority of data retention policies are time driven and warrant that records be destroyed after (or protected for) a fixed time after an event. We therefore propose the use of a temporal function NOW , which will denote the current system-wide time. Such a function is presently implemented in all modern database systems, and it is simply treated as a special timestamp. Support for comparative operators, date-time functions, and even its use in view definitions is also widespread. An example of a time-varying record would be to add a temporal predicate to the Jones Corporation record, R 1 , to identify those invoices and line items that were paid within the last five years: DEFINE RECORD R 2 AS SELECT * FROM Invoice NATURAL JOIN LineItem WHERE Recipient =  X  X ones Corp. X  AND Paid = true AND Years (NOW -PaidDate) &lt; 5
It must be clarified that our model for records and policies only considers the current state of the database. Here we do not deal with enforcing retention constraints on backup (of-fline) copies of the database or on tuples in older snapshots. Our aim is to ensure that the active database is always in a compliant state with respect to retention: all records that warrant protection are not affected by user initiated trans-actions and no records physically exist that have outlived their maximum retention period.
In the context of business documents such as invoices on which tax has been collected, preservation typically implies that the invoices are not destroyed, not modified, and that no new details are added. The natural equivalents of these protective requirements in databases are to ensure that cer-tain insertions, deletions, and updates are rejected. We pro-pose the use of a retention condition to express these pro-tective requirements in user defined records.

Definition 2. A retention condition is a boolean for-mula on the attributes of a record to denote tuples in the record on which a retention policy must be enforced.
If the policy applies to all tuples in the record, the reten-tion condition is simply the logical value true .Wedenote policies that prevent modification of records as protective re-tention policies . For these policies, we propose two levels of protection, namely update and append . A protective reten-tion policy on a record is defined as a pair Ret p :( protective retention condition , level of protection ). For a record under update protection, a DML statement is aborted if it modi-fies (deletes or updates) tuples in the record for which the protective retention condition is true. Similarly, for a record under append protection, a statement is aborted if it in-serts tuples in the record for which the protective retention condition is true.

A protective policy on all invoices issued to Jones Cor-poration and paid within the last five years ( R 2 )couldbe Ret p 1 :( ItemTotalTax &gt; 0 , update ) and a simple syntax for defining such a policy is as follows: DEFINE PROTECTIVE POLICY
ProtectItemTotalTax ON R 2 AS ( ItemTotalTax &gt; 0, update) The effect of the above policy would be that any update lead-ing to a change in a taxable amount for a tuple in the record will be rejected if the initial taxable amount was greater than zero.

Update protection by itself is not adequate for avoiding phantom inserts. For example an employee could create new line items for older invoices with non-zero tax amounts. Ap-pend protection ensures that such inserts cannot happen, and the record cannot increase in size because of user ini-tiated transactions. A policy such as ( ItemTotalTax &lt;&gt; 0 , append ), which can be instantiated using a similar syntax as described above, will reject all DML statements that lead to a new tuple being inserted in the record for which the taxable amount of the line item is non-zero.

Several retention policies can be enacted on a single record definition. For example, independent update and append policies can be placed at the same time to make sure that no change to a record can take place.

Note that changing the record definition slightly can have significant consequences in terms of these protection levels. For example consider the following record: DEFINE RECORD R 3 AS SELECT Sum(ItemTotalTax) FROM Invoice NATURAL JOIN LineItem WHERE Recipient =  X  X ones Corp. X  AND Paid = true AND Years (NOW -PaidDate) &lt; 5 R 3 adopts a different, summari zedviewoftherecordthat was examined in R 2 and focuses on the total tax collected on behalf of Jones Corporation in the last five years. It is also an example of a record that may not correspond to a physical document, but on which retention policies can be enforced. The intent of a policy protecting R 3 could be to ensure that the total tax owed (paid in the past on behalf of Jones Corporation) is never changed in the database. An important difference in protecting R 3 is that append protec-tion for such a record will have no effect. By definition, R will always contain a single tuple, therefore the cardinality of this view can never increase.

It is important to recognize that because of the flexibility in record definitions, many diffe rent records can be created over the same data, each with its own retention objectives. In fact, a significant benefit of our approach of using views to define records is that policy makers can leverage existing definitions of documents derived from a relational database. Queries used to generate day to day documents such as in-voices and sales reports can be taken directly from applica-tions and used as the view definitions for defining records that are used to enforce retention policies.

We note that protection for records is only meaningful (and enforceable) in the context of user initiated transac-tions. Consider the following example of a record and an append protection policy: DEFINE RECORD R 4 AS SELECT * FROM Invoice WHERE Days (NOW -CreateDate) &gt; 5 DEFINE PROTECTIVE POLICY
TemporalPolicy ON R 4 AS (true, append)
In this example, a user has defined a record as  X  X ll invoices created more than 5 days ago X  and attempted to enforce append protection for the contents of the record. In such a situation, as time passes tuples representing new invoices will automatically slide into the view and become part of the record definition. Since we are unable to abort the passage of time, we cannot protect records against trivial temporal alterations to the view, and we consider this as acceptable behavior that does not violate the retention policy. Details on how such trivial temporal alterations can be detected [5] are omitted due to space considerations.

Our framework for protecting r ecords resembles integrity constraints on arbitrarily defined views. Consequently we are able to benefit from the support for views provided by typical database systems and many of the theoretical results in the area of static query analysis.

Protecting records from modifications can be addressed using methods from efficient maintenance of materialized views. The notion of detecting whether a modification to a database will impact a particular view applies to both problems. Specifically, given a record definition and a policy, we identify precisely the view that needs to be monitored for relevant updates as follows:
Definition 3. For a given record definition R andare-tention condition for a policy C(Ret), a critical view C v defined as C v =  X  C ( Ret ) R
Critical views specified by protective retention policies are denoted as protective critical views . A critical view is a subset of a record that contains rows on which policy ac-tions need to be enforced. Consequently, protecting the contents of critical views is reduced to identifying modifi-cations to a base table that will change the contents of a protective critical view and r ejecting such statements. In our running example, the record and policy pair R 2 and P rotectItemT otalT ax generate the critical view specified by C p 1 as follows: C p 1 = SELECT ItemTotalTax
Updates and mechanisms for monitoring the effect of up-dates, such as alerters, assertions and triggers, have long been studied in the database community. A wide array of techniques, such as static analysis of updates to deter-mine relevance for a view [7] and incrementally maintaining and monitoring the impact of updates on views [8] can be adopted in our context. However, the efficiency of any se-lected approach depends on a large number of factors spe-cific to real-world data retention obligations. We provide a discussion of these factors and highlight the key decision making criteria that can lead to an efficient retention policy monitoring system. Our tests (Section 6) were developed with these principles in mind, and they demonstrate that the correct choice of event detection mechanism is critical for a view based records retention system.

The most important design consideration leading to effi-cient detection of policy violations is how we constrain the record definitions themselves. With the flexibility of defining records using arbitrary views with the full expressiveness of SQL comes the risk of having to incur the worst case cost of full re-computation of the record at every update. Thus, to ensure that our policies are practical and can be efficiently enforced, we need to restrict the record definition language. Our examination of data retention laws has led us to con-clude that policies are typically defined on what law-makers would consider practical business records, which typically map in our framework to sets of related but simple parame-terized queries. We observed that in most business schemas these  X  X ractical X  records (or the data contained therein) can be expressed as conjunctive queries .Consequently,were-strict ourselves in this paper to the analysis of records spec-ified as conjunctive queries with support for aggregation.
When enforcing policies, we should consider overlap among them. For example, given an update that is relevant for critical view C v 1 and is irrelevant for critical view C all database instances, there is clearly no need to check for violations on C v 2 for the update in question. This prob-lem of determining exclusive relevance has been addressed in the context of query disjointness, and Elkan gives an effi-cient decision procedure to detect disjointness of conjunctive queries [10]. Generally, if updates can easily be checked for relevance against a large number of views, then the task of capturing violations becomes significantly easier. The com-plexity of the schema and the density of policies over a given set of relations also play a critical role in determining pol-icy overlap. If the majority of data retention policies are clustered over unrelated relations, then there will certainly be less overhead in determining the impact of a particular update on all protected records.

We can also benefit from various static optimizations of policy actions on records. Given a set of record (view) defini-tions and a set of protective retention policies, we propose to derive an optimized set of policies with the same protective characteristics. One such optimization is that of eliminating redundancies in protective requirements that can naturally arise because of the hierarchical nature of business records. For example, at an abstract level, the data contained within a monthly sales report will typically be a subset of the data contained in the relevant yearly sales report. Therefore, when dealing with a large number of record definitions and policies, we may be able to exploit such correlations and check several policy violations against similar records.
Another property of a wide array of business records is that of temporal stability: as a record becomes older it is less likely to be modified. In many transactional business applications, records can be expected to be in the  X  X ctive state X  for a fixed period of time after their creation and then gradually achieve temporal stability. It may be beneficial in these situations to identify active records in order to isolate the passive or protected records. Temporal stability can also be used in conjunction with the fact that business records are often referenced with monotonically increasing identifier values. For example, if Invoice #500 was the first invoice created on Jan 01, 2007, and Invoice #1000 was the last invoice created on Dec 31, 2007, then it is very likely that all invoices created in the year 2007 lie in that particular range of identifiers. In such cases maintaining a few pointers can substantially reduce the overhead involved in monitoring whether a particular update will affect the contents of a record. Mechanisms to infer such correlations automatically or the ability for users to specify them must be supported by the underlying database system for these optimizations to be useful (e.g., range partitioning in Oracle and DB2).
So far we have focused on protecting records as long as retention conditions are met. The flip side of records reten-tion is to ensure that sensitive information is removed from a database as soon as it has outlived its purpose.
The two options available to achieve compliance with poli-cies that dictate maximum retention periods for records are destruction and transformation. Deleting a sensitive busi-ness record altogether is not a strategy that is widely prac-ticed, nor is it suitable in many situations. Instead, tech-niques such as partial deletion and anonymization are prefer-able, as they ensure that records retain their business value and yet pose no liability for the organization.

We denote policies that mandate removal (deletion or anonymization) of records as destructive retention policies and extend our view based records management framework to support such policies. In this section, we develop a for-malization for deletion and anonymization of user defined records over a fixed relational schema.

We define a destructive retention policy on a record as apair Ret d :( destructive retention condition ,  X  ( R )). A de-structive critical view is defined similar to a protective crit-ical view, as a conjunction of the destructive retention con-dition and the record definition. In contrast to their protec-tive counterparts, however, destructive critical views con-tain a subset of the rows of the record that need to be ex-punged from the database. In a retention compliant state all destructive critical views are empty, reflecting that there is no sensitive information in the database that needs to be deleted or anonymized. Monitoring destructive critical views is also very similar to monitoring protective critical views, except that we only pay attention to whether the view is empty or not. If any destructive critical view in a database is non-empty, then the database is in a retention unsafe state , and some action needs to be taken to remove the critical rows from the de structive critical views. De-structive actions , denoted by  X  ( R ), are associated with each destructive retention policy, and on detection of a non-empty destructive critical view, the execution of the specified ac-tions is intended to make the critical view empty, taking the database into a retention compliant state.

One approach to moving a database into a retention com-pliant state through destructive actions is to associate a stored procedure with every destructive policy, and to ex-ecute the procedure when a non-empty destructive criti-cal view is detected. This technique essentially offers un-bounded flexibility in the actions that can be performed to purge outdated records, but verification and ensuring cor-rectness of actions performed by arbitrarily written proce-dures over all database instances is undecidable. Conse-quently, we focus on a restricted class of destructive actions and thereby present a provably correct mechanism for policy enforcement.
We now introduce the first formal requirement for any destructive action on critical views to be provably correct.
Definition 4. A destructive policy P is weakly correct if upon any tuple becoming part of the destructive critical view V d , the destructive actions specified by P, denoted by  X  ( R ) , will ensure that V d wil l become empty.
Weak correctness implies that the invocation of the de-structive actions specified by the policy must remove criti-cal tuples from the critical view. Weak correctness by itself does not provide any guarantees for termination of policy actions. This is because actions of a particular destructive policy may introduce tuples in critical views of other de-structive policies. Consequently, the notion of non-invasive policies is presented so that we can reason about policy ex-ecution patterns.

Definition 5. A destructive policy P is non-invasive with respect to another policy P if the destructive actions specified by P, denoted by  X  ( R ) , cannot affect the critical view of P . P is cal led invasive with respect to P if  X  ( R ) has the potential to change the contents of the critical view of P .

This definition specializes the definition of relevant up-dates presented by Blakeley et al. in the context of materi-alized views [7]. If  X  ( R )of P cannot impact the contents of the critical view of P ,then  X  ( R ) is irrelevant to the critical view of P and P is non-invasive with respect to P .
An important observation is that the definition of invasive policies does not restrict P to being a destructive retention policy, and the notion of invasive policies can be used to detect delete-protect conflicts (Section 4.4). The definition also encompasses side effects that could be caused through destructive actions, for example through foreign key con-straints with cascading deletes.

Invasiveness among policies need not arise directly be-cause of data shared among records. As an example, con-sider two relations X 1 and X 2 , each with a single attribute called id , and two record definitions, R 1 which selects all id X  X  in X 1 that are not in X 2 ,and R 2 which selects all id X  X  in X 2 that are not in X 1 . Note that, by definition, R 1 R 2 are always disjoint ( R 1  X  R 2 =  X  ) but the definitions of R 1 and R 2 are inseparable. Performing insertions or dele-tions on either of these records has a direct impact on the other. Consequently, we must derive sufficient conditions to guarantee consistency of execution among the destructive actions of several destructive retention policies. The follow-ing requirement ensures that a policy will not only expunge all tuples from its critical view but also not lead to the in-sertion of new tuples in the critical views of other policies.
Definition 6. A destructive policy P is strongly cor-rect if it is weakly correct and either (i) P is non-invasive with respect to other destructive policies or (ii) the destruc-tive actions specified by P can only delete tuples from the critical views of other destructive policies.

Finally, using the notion of invasiveness and strong cor-rectness, we can offer an algorithm to determine whether a given set of destructive policy actions can be correctly en-forced. We rely on constructing a policy interference graph , which can also be used to visualize the impact of actions performed by each destructive policy on contents of critical views of other policies:
Definition 7. A policy interference graph is a directed graph constructed using a set of policies P = { P 1 ,P 2 , ..., P as nodes and edges from P i to P j when P i is invasive with respect to P j .

Lemma 1. A set of destructive policies is guaranteed to be terminating if all cycles in the relevant policy interference graph involve strongly correct policies.

The above lemma specifies a sufficient condition for avoid-ing circular enforcement of destructive retention policies. This result is similar to the results from trigger termina-tion and cyclic execution of rules in database systems [4]. If all interfering actions caused by policies only cause removal of records from destructive critical views, and the number of tuples in a database is finite, then indefinite execution of destructive actions is not possible.
It is important to note that the correctness criteria that we developed for our proposed view based records retention framework is general enough for all record definitions and all possible destructive actions that expunge tuples from criti-cal views. Unfortunately, determining correctness for arbi-trary record definitions and destructive actions is undecid-able. Therefore, we restrict ourselves to simple destructive actions on updatable conjunctive views. We argue (again) that business records can usually be expressed as conjunctive queries, and the destructive actions required to expunge or anonymize these records can be relatively simple. Such ac-tions include setting a particular attribute value to a default value or to null, or deleting a tuple altogether; we denote these as simple destructive actions .

Before presenting a decision procedure for correctness, we introduce the notion of an extended retention condition and a post-condition of the actions performed by a destructive retention policy. An extended retention condition is the boolean formula that defines the destructive critical view, constructed as the conjunction of the predicates of the record and the destructive retention condition of the policy, as de-scribed earlier. The post-condition of a destructive action is a boolean formula on the tuples of the critical view that must hold on a critical tuple (i.e., a tuple in the critical view) after the action is successfully applied to this tuple. For updatable conjunctive views, if the action involves delet-ing the tuple from the critical view, the post-condition is trivially false, since the deleted tuple will not exist in the record. On the other hand, if the action involves updating the value of one or more attributes in the critical tuple, say setting a i to v i for i =1 ...n , then the post-condition is a = v 1  X  a 2 = v 2  X  ...  X  a n = v n .

Lemma 2. A destructive retention policy with an extended retention condition R p and a destructive action having post-condition  X  p ( R ) is weakly correct if and only if R p  X  is unsatisfiable.

The above lemma reduces the problem of verifying cor-rectness of policies to that of satisfiability of conjunctive predicates, and it states that enforcement is guaranteed if and only if R p  X   X  p ( R ) is unsatisfiable. To illustrate the usability of this result, consider the following schema and a simple destructive retention policy that monitors completed sales orders that were placed more than a year ago: Order(OID , Delivered)
Transaction(Txn ,OID , TxnDate, CreditCard) where singly underlined attributes are keys and doubly un-derlined attributes are foreign keys.

Assume that the administrator has defined a record and a destructive policy as follows: DEFINE RECORD OldT xns AS SELECT OID, Txn, TxnDate, CreditCard FROM Order NATURAL JOIN Transaction WHERE Delivered = true AND CreditCard IS NOT null DEFINE DESTRUCTIVE POLICY
AnonymizeCreditCard ON OldT xns AS ( Months (NOW-TxnDate)&gt;12, SET CreditCard = null )
Observe that the extended retention condition R p on this record and policy pair is ( Months ( NOW -TxnDate) &gt; 12  X 
Delivered = true  X  CreditCard IS NOT null )andallrows in the critical view must satisfy this condition. As a particu-lar transaction for a delivered order becomes a year old, the destructive critical view will become non-empty, and con-sequently  X  ( R ) will be triggered. In this case the destruc-tive action is simply to set the CreditCard attribute to null, and it is obvious that no tuple can simultaneously satisfy both the post-condition and the extended retention condi-tion: the CreditCard attribute cannot be null and non-null at the same time. The unsatisfiability of R p  X   X  p ( R )proves that  X  ( R ) will always expunge tuples from the critical view.
Note that more general destructive actions can also be handled by this correctness condition. For example if  X  ( R )is changed to SET CreditCard = Anonymize ( CreditCard ), and if the post-condition of Anonymize ( CreditCard )al-ways invalidates the extended retention condition, then the destructive action is, again, verifiably correct.

If R p  X   X  p ( R ) is satisfiable, the process of proving this fact can be used to help administrators debug incorrect de-structive actions. For if R p  X   X  p ( R ) is satisfiable, there will exist at least one possible tuple in the critical view that can serve as a counterexample in which performing the destruc-tive action will not remove the tuple that violates the policy from the critical view. This counterexample serves to illus-trate that the destructive action is not correct. At the same time, presenting this counterexample to the administrator may help in debugging this incorrect destructive action.
The implications of being able to statically verify that a record X  X  life cycle will always terminate in destruction (re-moval from the critical view) are substantial. This proof of correctness of destructive policy actions provides the highest level of assurance for automated records retention compli-ance. The only downside is that, in the general case, check-ing for satisfiability is NP-complete. This is an improvement over the case of arbitrary reco rd definitions and destructive actions, in which static verification is undecidable, but we still need to pay the cost of solving an NP-complete problem. We note that in most practical situations where the number of predicates in the extended retention condition will be lim-ited, modern SAT solvers will be able to prove correctness for individual policies efficiently. Furthermore, the cost of verifying correctness has to be incurred only once (offline) for any given set of policies and destructive actions. Re-stricting the expressiveness of records to select-project-join views also allows us to verify non-invasiveness of actions and avoid cyclic execution of policy actions, since the problem of isolating policy interference is equivalent to statically de-termining relevance of actions on destructive critical views [7].
Yet another challenge in providing support for data reten-tion policies is to ensure that actions performed by retention policies never compromise the integrity of the database.
Definition 8. A destructive policy P is integrity pre-serving if its destructive actions  X  ( R ) when applied to any valid instances of the database will always lead to a valid instance of the database with respect to all integrity con-straints.

The notion of integrity preservation for a destructive ac-tion is distinct from the notion of correctness defined earlier. For example, the action can set a primary key value to null, which would satisfy the correctness criteria stated in Lemma 1, but would violate integrity. Conversely, a destructive ac-tion can be integrity preserving without being correct. For example, the action can simply do nothing, which would leave the integrity of the database intact but would not elim-inate the tuples from the critical view. Once again, statically verifying integrity compliance is undecidable in general, but the class of records specified by updatable conjunctive views and simple destructive actions on them can be restricted further to be made integrity preserving, as described in the remainder of this section.
Modifications to primary key and unique attributes can-not be checked for uniqueness without executing additional queries. Similarly, we can not statically determine the non-existence of foreign key references to a primary key value being deleted. Hence, for proving correctness of destructive actions on such attributes independently of the database instance, we must restrict these actions to deletions. Fur-thermore, if the action requires eliminating a primary key value, the only option that can be statically proven to be integrity preserving is to delete the entire tuple from the critical view, and only if this deletion is guaranteed to be non-invasive (through possible cascading delete foreign key constraints) for all protected critical views (Section 4.4).
Modifying foreign key attributes is slightly less restrictive, and if the foreign key attribute is nullable then it can always be set to null. However, in cases where null is not suitable, we can overwrite the foreign key value with a different but valid one. To motivate a scenario where this may be useful, let us assume that the CreditCard attribute in the example of Section 4.2 was a foreign key reference and that customers in our database are uniquely identified by their credit card numbers. An extended schema and a different destructive policy is described below: Order(OID , Delivered) Transaction(Txn ,OID , TxnDate, CreditCard ) Customer(CreditCard ,CustomerName) DEFINE DESTRUCTIVE POLICY
AnonymizeCustomer ON OldT xns AS ( Months (NOW-TxnDate) &gt; 12, SET CreditCard = 1111-1111-1111-1111 )
Note that this policy assumes the existence of an artificial customer (say John Doe) identified by the fictitious credit card number that is being used for anonymization. However, to ensure correctness of this policy, the foreign key value (the tuple for John Doe) must exist in the Customer relation at the time of policy instantiation, and the system must guar-antee that the statically used primary key will itself never be modified or deleted. This requirement ensures that the destructive action remains valid with respect to the integrity constraint throughout the lifetime of the policy. This con-straint on the artificial customer (statically used primary key value) can itself be modeled as a protective retention policy as follows: DEFINE RECORD Customers AS SELECT CreditCard FROM Customer DEFINE PROTECTIVE POLICY
ProtectJDoe ON Customers AS (CreditCard = 1111-1111-1111-1111 , update)
With modern database systems supporting triggers and check constraints of arbitrary complexity, it becomes signif-icantly harder to determine statically the consequences of actions taken in response to policy violations. For exam-ple, if our attempts to remove ou tdated records are rejected by a user programmed trigger, we will be unable to offer compliance guarantees unless the trigger is removed (or sus-pended) and the destructive policy actions repeated. Sus-pending trigger invocation altogether while retention actions take place is a simple solution, but then extra care must be taken in programming retention actions.
The final requirement for proving correctness for a given set of protective and destructive retention policies is to show that they are conflict free. Inter-policy conflicts are caused when expired data that is to be removed or modified ac-cording to a destructive policy is to be retained under a protective policy at the same time. Detecting conflicts can be easily accomplished using th e framework for correctness that we described earlier.

Definition 9. A destructive policy D is conflict free with respect to a protective policy P if D is non-invasive with respect to P.

If the actions specified by a destructive policy cannot im-pact the critical view of a protective policy then the two policies are guaranteed not to conflict with each other. As before, the problem of isolating conflicts is reduced to that of determining whether a given update (destructive policy ac-tion) is irrelevant to a given relational expression (protected critical view).

As in Section 4.2 we rely on the result that detecting ir-relevant updates for select-project-join view based records is decidable (although it is NP-complete), whereas in the gen-eral case detecting conflicts is undecidable [7]. To prove total correctness for a given set of policies, we have to show that all pairs { P , D } ,where P is a protective policy and D is a destructive policy, are conflict free. If conflicts are detected, the retention manager(s) responsible for the policies can be informed, so that appropriate and defensible resolutions can be devised.
Our proposal for records retention uses active integrity constraints and actions on views. We assert that the task of periodically monitoring destructive critical views and purg-ing records from them will not be a significant source of performance overhead in our model. This is because of the temporal flexibility available in destructive data reten-tion requirements: as long as records are deleted by well-documented regular maintenance routines, the legal reten-tion requirements will be met. For example, this flexibility allows us to execute daily or weekly batch jobs that enforce destructive retention policies without interfering with the online operation of the database system.

However, if a record is accidentally deleted when it is sup-posed to be protected by an organization, the consequences are more serious. Naturally, there is no flexibility in protect-ing records and every modification to a database has to be checked against the relevant policies to ensure compliance. Consequently, the cost of computing the effect of updates on protected records will be the most significant source of overhead. Thus, the aim of our experimental evaluation is twofold: first, to measure the overhead of continuous record protection for a broad mix of protective policies in a high-update and heavily regulated business scenario; and second, to determine and recommend means of minimizing this over-head using features already present in database systems.
There are two widely used mechanisms to detect events specified by arbitrary relational expressions (such as a tuple becoming critical) in database systems: incremental com-putation or total re-computation. More specifically, to de-tect changes in the contents of a critical view, we can either materialize the view completely and implement triggers on involving an aggregate value (A). the materialized view (a feature present in some commercial database systems such as Oracle) or implement triggers on base relations to detect whether the relevant critical view will be impacted by a triggeri ng statement (possibly after execution of an additional query) [8]. A detailed examina-tion of support for triggers on views, including an algorithm for mapping triggers on views to an equivalent set of triggers on base relations, has been presented in the literature [17].
We note that for monitoring critical views, there are two well known optimizations that can be exploited to reduce overhead. First, we can benefit from the observation that certain policies naturally favor a particular mechanism to detect violations. For example, using triggers to monitor a critical view with an aggregate value is ill-advised, since this value requires re-computation at every relevant update. Generally, the decision to materialize or to re-compute de-pends on the average cost incurred per relevant update, and most database optimizers can quite easily assess the cost of a typical update and re-computation query, to give a rea-sonable estimate of which technique will be better than the other. The second optimization relies on the observation that policies can quite often be clustered around a small number of related tables. Instead of instantiating a large number of triggers on base relations, the approach of trig-ger grouping [12, 17] can be used to reduce the number of triggers per table and to exploit the fact that multiple poli-cies on similar predicates can be checked by a single trigger invocation. The result of these optimizations can lead to a significant reduction in the cost of view monitoring.
Our tests use the TPC-H benchmark database represent-ing a business scenario involving the sale of parts to cus-tomers worldwide. We developed 12 different protective data retention policies and translated them into critical views that would need to be monitored. These policies were di-rectly derived from real-world r ecords retention requirements imposed on TPC-H like businesses by various security, ex-port, and taxation agencies. Examples of such policies in-clude protecting purchase orders involving the sale of special parts like uranium fuel rods, p rotecting order details with particularly large monetary sums, and protecting sales tax totals for specific countries. Our policy set consists of 3 sim-ple policies on single relations, 6 policies leading to the mon-itoring of conjunctive views, and 3 policies that protect an aggregate amount. We define a policy X  X  critical row coverage as the fraction of tuples subject to protection/monitoring in the base relations. In choosing our experimental policies we ensured that all but one has a low tuple coverage, and yet every tuple is protected against modification by at least one policy (Figure 1). A detailed description of the motivations behind these policies, their parameters, and the resulting critical views is available in our full report [5]. Figure 2: Average completion (commit) time for an update to a single tuple in a base relation protected under a single policy implemented using triggers over base relations and using the incremental view maintenance approach. The horizontal line repre-sents the cost of an update without any monitoring mechanisms in place.
 Our tests were conducted using a 1GB dataset on an Intel Core 2 Duo (1.8Ghz) machine with 1.5GB of RAM. All tests were performed on a warm database using DB2 v9.5 and involved issuing several thousand individual update state-ments on base relations that impact critical views defined by protective retention policies. We issued update statements until the confidence intervals for expected update time were very small, which typically required issuing 3,000-5,000 up-dates. The standard error in mea sured wall-clock response times in our tests was usually less than 5% of the average time to commit for an update.

Figure 2 summarizes the resu lts of the overhead incurred by each of the 12 protection policies when they are individ-ually implemented as a materialized view and as a set of (base relation) triggers. All updates were performed ran-domly over the dataset (each tuple was equally likely to be modified). Policies involving aggregated information (poli-cies 10, 11 and 12) clearly favor incremental computation (the materialized view approach), whereas the maintenance overhead caused by materialization for other policies is far greater than simply checking the relevance of updates using triggers on base tables.

The reason for triggers individually performing better than materialized views for event detection in most policies is largely due to the TPC-H schema and data specification. For example, a purchase order in the TPC-H schema is related to only one customer, nation and region. Furthermore, the data contained in any purchase order includes at most 7 line items. Consequently, the majority of simple policy decisions Response Time (ms) Figure 3: The scalability of incremental computa-tion (materialized views) and total re-computation (triggers) for detecting changes in the contents of critical views. The values on the x-axis represent the total number of protective policies being enforced on the database. For example at 7, Policies 1 through 7 are all being enforced at the same time. on updates pertaining to individual purchase orders can be made by examining a small number of tuples. Only when a large number of tuples has to be examined (e.g., for policies involving aggregated totals) does the active re-computation required when using triggers pay a heavy price.

Figure 3 demonstrates how both the incremental (materi-alized views) and total re-computation (triggers) approaches scale as more and more policies are implemented. Mate-rialization suffers from the overhead of view maintenance whereas triggers have scalability problems arising from one policy on a view being translated into multiple triggers on base tables. It is worth mentioning that most commercial database systems have strict limits on the number of triggers that can be instantiated on a relation (typically fewer than 64). Consequently, it is very unlikely when dealing with a large number of policies that triggers would be able to ac-commodate the monitoring of all protective critical views. Note that the policies where triggers incur a high overhead are deliberately introduced as the last three policies in the mix. Thus, the figure misleadingly seems to favor the use of triggers for critical view monitoring. However, the total cost associated with monitoring all policies using triggers is far greater than that of using materialized views. It is clear that neither triggers nor incremental view maintenance alone can work well in general for monitoring a broad variety of records and policies.

We can instead combine the two monitoring methods into a hybrid critical view monitoring technique that attempts to take advantages of the positive aspects of both approaches. Simply choosing the correct and more suited event moni-toring mechanism for each policy can substantially reduce the cost associated with monitoring a collection of critical views. Figure 4 shows that the simple hybrid strategy per-forms better than either technique alone. However it still suffers from having to instantiate a large number of trig-gers which greatly reduces its scalability. Since the TPC-H schema is relatively compact, and a large number of our policies revolve around the Orders and LineItem tables, we also recommend trigger grouping [21], which provides a sig-Response Time (ms) Figure 4: The scalability of the hybrid approaches to event detection on views. The values on the x-axis represent the total number of protective policies being enforced on the database. For example at 7, Policies 1 through 7 are all being enforced at the same time. nificant improvement in response times by minimizing the number of trigger invocations per update on a base table.
Finally, we note that our tests stress retention monitoring much more than real world scenarios would. The foremost difference is the non-uniform pattern for updates: all pro-tected tuples are not equally likely to be modified. As noted earlier, many businesses do not actively modify temporally stable records (for e xample very old purchase orders). Con-sequently, the use of temporal or range based indexing and partitioning will substantially reduce monitoring costs. Sec-ond, we note that our tests were done using unrealistically high record coverage (each tuple was protected under at least one policy) and typical coverage can be expected to be much lower than what we tested against.

In practice, we recommend that the decision to use a par-ticular strategy for monitoring views be made by the query optimizer after considering the expected number of policy vi-olations, expected number of relevant updates, level of tem-poral stability exhibited by records, and types of views to be monitored. Given a particular workload, modern database systems are able to recommend materializations of views that can improve query performance. Much of the infras-tructure to measure the cost and benefit of incremental com-putation of views versus re-computation costs of queries al-ready exists. Therefore we believe that by using these ex-isting features a  X  X etention policy advisor X  can be built such that given a set of record definitions, protective policies on records and an expected workload profile, the best monitor-ing mechanism can easily be determined.
Limited and managed data retention has long been of prime importance in the field of records management. Re-tention solutions for unstructured data, such as documents and email, are widely deployed in large organizations, and the importance of limited retention has also been acknowl-edged as a fundamental requirement in privacy aware Hip-pocratic databases [3]. The problem of unintentional data retention in relational database systems, even after explicit deletion by users, has been also been examined recently [18]. Unfortunately the need to have a formal framework for sys-tematically creating, managing and enforcing records reten-tion policies has been ignore d for structured records.
The work by Garcia-Molina et al. [11] and Toman [19] in removing tuples from large fact tables in data warehouses when they are not required in answering a given set of queries is similar in spirit to ours. The problems being addressed, however, are quite different, and our framework extends the notion of removal of sensitive data to apply to records as views and takes into consideration the interaction between protective and destructive policies.

Other suggested approaches to solving the problems as-sociated with data retention in relational databases have generally taken a very simplistic approach to the problem of managed records retention, where a piece of metadata, such as a purpose or expiry timestamp, is attached to abstractly defined facts that have retention obligations [2, 3, 16]. Such proposals also trivialize the problem to that of deleting records when they have outlived their purpose, whereas the vast majority of legal retention obligations faced by organi-zations are protective and not destructive. Needless to say that for provable regulatory compliance, the interaction be-tween protective and destructive policies has to be examined carefully, and our research looks at the problem from both perspectives.

Our proposed framework for protective policies on records is related to several features present in traditional integrity constraint and materialized view mechanisms, which have been exhaustively examined in the past several decades. Ef-ficient monitoring of records i s directly related to several problems in efficient materialized view maintenance [7] and incremental evaluation [8]. The problem of policy enforce-ment and actions on views relies heavily on the notion of updatable views [9]. Verifying the impact of user defined actions on views [15] and assisting users in creating mean-ingful actions on views [13] have also been examined in the literature.
In this paper we present a view based framework for sys-tematically creating and enforcing records retention policies in relational database systems. Using such a framework en-ables users to define records at any granularity and enforce a rich set of protective and destructive retention policies on these records. A significant benefit of our approach is that it specifies a formal criterion for correctness for any user defined record and policy action. We demonstrate that for the class of records specified by updatable conjunctive views and simple destructive actions on them, users can statically verify the effect of their policies and be certain that pol-icy execution will always guarantee regulatory compliance. The requirement to use such views can be enforced syntacti-cally. The layer of formal reasoning that we have developed can also be extended to encompass more expressive records (e.g., outer joins) and can be combined with a wide choice of anonymization functions. Our framework can be efficiently implemented using the infrastructure for triggers and mate-rialized views that is part of almost all commercial database systems.
 This work was funded by the University of Waterloo, Open Text Corporation, and the Natural Sciences and Engineering Research Council of Canada. [1] United States Public Company Accounting Reform [2] Rakesh Agrawal, Paul Bird, Tyrone Grandison, Jerry [3] Rakesh Agrawal, Jerry Kiernan, Ramakrishnan [4] Alexander Aiken, Jennifer Widom, and Joseph M. [5] Ahmed Ataullah. A Framework for Records [6] Lars B X kgaard and Leo Mark. Incremental [7] Jos  X  e A. Blakeley, Neil Coburn, and Per- X  Ake Larson. [8] Stefano Ceri and Jennifer Widom. Deriving [9] Umeshwar Dayal and Philip A. Bernstein. On the [10] Charles Elkan. A decision procedure for conjunctive [11] Hector Garcia-Molina, Wilburt Labio, and Jun Yang. [12] Eric N. Hanson, Chris Carnes, Lan Huang, Mohan [13] Arthur M. Keller. Choosing a view update translator [14] Bill Lipner. The million-dollar backup tape. [15] Claudia Bauzer Medeiros and Frank Wm. Tompa. [16] Marco C. Mont and Robert Thyne. A System to [17] Feng Shao, Antal Novak, and Jayavel [18] Patrick Stahlberg, Gerome Miklau, and Brian Neil [19] David Toman. Expiration of historical databases. In
