 This paper investigates the problem of generating on-demand recommendations over a dataset of items where the input is a selection of a few of the items. As an example in the context of a movie dataset, the user may wish to see a list of movies related to the three animation movies  X  X inding Nemo X ,  X  X p X  and  X  X pirited Away X . In this case, it would be expected that the list returned would contain other animation movies like  X  X all-E X ,  X  X rincess Mononoke X  etc. Thus, this problem can be viewed as a type of X  X lustering on demand X  X roblem [1]. It is the set form of input that distinguishes this problem from a standard information retrieval problem where the query is usually a single item or an abstraction of a single item in the dataset. In this paper, we present several new ap-proaches to dealing with this problem. We also show some representative results on a movie text dataset.
 H.4 [ Information Systems Applications ]: Miscellaneous Algorithms Bayesian Sets, Set-based retrieval, Probabilistic models
Technological advances have made large corpora of infor-mation ubiquitous. Recent trends in personalization and on-demand access of various types of media (image collec-tions, songs, movies, TV shows etc.) have resulted in large collections of these items being made available for access. More and more, this leaves users with the daunting task of browsing or retrieving items in a large corpus.

Recommender systems have proved useful tools in this re-gard. By various techniques such as keeping track of popular content, extrapolating similar users X  behavior, and finding content similar to what the user has accessed previously, these systems provide personalized views of the items in the collection. The popularity and demand of such systems show the value of making content access less challenging.
However sophisticated such systems may be, they can-not reasonably be expected to anticipate a user X  X  every data access need. We argue that in such cases, an interactive mechanism that allows for simple and intuitive user-directed exploration of the collection is valuable. A particularly in-tuitive retrieval paradigm for such user-directed browsing is one where a user wishes to find items that are related to a set of items that he has in mind currently. For instance, consider the task of finding songs in a music collection. Sup-pose a user wants to know which songs in a collection are similar to the songs Stairway to Heaven (by the band Led Zeppelin) and Purple Haze (by Jimi Hendrix). Since both of these songs are guitar-heavy and of the classic rock genre, other classic rock songs in the collection are reasonable items to return. As another example, in browsing a television guide, the user might want to find sitcoms similar to Ev-erybody Loves Raymond and Home Improvement . In this case, the returned TV shows might reasonably be expected to be comedies that are family-focused. Bayesian Sets [1] and Google Sets[4], are examples of techniques working in this paradigm, and in this work we present extensions and alternative formulations in the same paradigm.

Concretely, we will focus on the following on demand rec-ommendation problem: given a set of data items as input, we will concentrate on the task of returning a relevancy score over elements in the database. The scores can then be used to define a top-k ranked list of items recommended based on the input set. Note that this differs from the standard infor-mation retrieval problem in a significant way: The  X  X uery X  consists of a set of items. Further, the query input set con-sists of complete items in the collection (not abstractions of items in the collection). Thus the challenge in this paradigm is finding and abstracting any relevant concepts that ex-press commonality of the given few input items, and then efficiently returning other items in the collection that con-form to these concepts. Note that on demand nature of the input is in contrast to typical content based recommender systems, that are static and try to infer relations between item features and a user profile.
As mentioned in Ghahramani and Heller [1], this problem can be viewed as a  X  X lustering on demand X  problem, where the input set is considered a subset of some underlying con-cept or cluster and the objective of the algorithm is to pro-vide a completion of this cluster. We also find approaching the problem more tractable from a ranked retrieval perspec-tive and will concentrate on approaches in this direction.
We now introduce some terms and notation. We formu-late the set retrieval problem in terms of a given data collec-tion/dataset D . The data collection is a set of items/examples x  X  X  . Each item x in the dataset is described by a fixed tor). We will refer to the components x j as features. We will sometimes require a fixed ordering over the data collection, and will refer to the matrix containing the entire dataset as (the item by feature matrix).

The set-based recommendation problem is: given a (typi-cally small) subset of elements S X  X  from the collection, re-turn relevancy scores over all the elements in D . The scores should reflect the similarity of elements in the dataset to the items in the set S . Since the application we target is inter-actively browsing a collection, we require that the various approaches we pursue are efficient in terms of computation.
All the techniques we will describe in detail approach the problem by computing a score per item in the collection. The score will evaluate how close any particular item x is to the input set S , as compared to the entire dataset, D .
The  X  X ayesian Sets X  approach [1] examines the set retrieval problem from a probabilistic viewpoint. We briefly outline this approach here. In their main algorithm, features are assumed to be binary, i.e. x j  X  X  0 , 1 } . Given the input set S , a probabilistic relevancy score is computed over all the elements in the dataset, which is then used to generate a ranked list. A generative probabilistic model is assumed to hold over the data. In particular, for each item, every bi-nary feature is assumed to be drawn from a fixed Bernoulli distribution with parameter  X  j , which equals the probabil-ity of feature j being non-zero. We denote by  X  the vector collecting all n Bernoulli parameters for the features. The items x are assumed independently and identically gener-ated. With the model specified, quantities like p ( x |  X  ) and p ( D|  X  ) = for each item  X  ( x ) is defined to be: Insight into why this is a reasonable score is obtained by rewriting it as  X  ( x ) = p ( x  X S ) /p ( x ) p ( S ) and referring to the generative model for the (assumed i.i.d.) data items, p ( x |  X  ) . The score for every item is the ratio of the prob-ability of the item x and S modeled jointly (using a com-mon parameter vector  X  for both) divided by the same joint probability but assuming they are parametrically indepen-dent (in this case the generative model for x and S can have separate parameters). Intuitively, if the item x and the set S are related, having a model where the same pa-rameters generate both x and S will result in the joint prob-ability dominating the model where x and S are generated with separate parameters. In calculating the score, Ghahra-mani and Heller [1] adopt a Bayesian approach, and place a prior distribution over the parameters p (  X  ). Evaluating the score then requires integrating over this prior, so that, p ( x ) = model for the feature presences, and a conjugate Beta prior on the Bernoulli model parameters, Ghahramani and Heller [1] show that the log-score can be computed analytically and efficiently, via a simple matrix-vector multiplication so that finally: , where c = log  X  j , q j = log  X   X  j  X  log  X  j + log  X   X  j  X  log  X   X   X  ,  X  j are the Beta hyperprior parameters for each feature. In experimental results that follow, we use the same broad empirical priors used by Ghahramani and Heller [1], namely,  X  = t x m ,  X  = t (1  X  x m ) where x m is a mean vector over all items x m =
While the Bayesian Sets approach was shown to have good results, the efficient matrix-vector multiplication operation for computing the score is limited to binary data and there is a hyperparameter t that needs specification. We exam-ine a maximum-likelihood (ML) variant of this approach to address these restrictions. In particular, we use the same score function (Equation 1):  X  ( x ) = p ( x |S ) /p ( x ). The dif-ference is that we use the maximum-likelihood parameter values in evaluating this expression. In other words we use the approximation that: If the distribution p (  X  |S ) is sharply peaked around  X  approximation will be good. Similarly we approximate p ( x ) = p ( x |  X  D ). Assuming we can estimate  X  S and  X  D , this leads to a simple expression for the score. In the case of binary data and assuming i.i.d. Bernoulli draws, it is easy to show an-other matrix-vector multiplication results for the log score: where q = log  X  S  X  log  X  D + log(1  X   X  D )  X  log(1  X   X  S The Bernoulli parameters are estimated using their ML ex-pressions, namely,  X  S ,j = P mates in two reasonable ways. One, we cap the ML estimate of any Bernoulli parameter to 0 . 9999. This is necessary be-cause when dealing with a small input set, frequently all elements of the set may have a particular feature set to 1. In this case, the shrinkage keeps the score valid. Two, we set  X  S ,j =  X  D ,j for all features j not seen in the input set S . Again, this is reasonable and amounts to setting feature occurrence probabilities to those seen in the overall dataset when we have no information about such features in S .
Intuitively, the score compares modeling the item by pa-rameters that best fit the items in set S vs. parameters that best fit the entire dataset D . There are two advan-tages to this approach. First, we have no additional pa-rameters to set/specify. Second, the score can be computed efficiently for count data in addition to binary data X  X his is especially useful in our context since ratings data is quite of-ten of this form. Note that the required integrals render this
Here, I ( . ) is an indicator function which returns the value 1 if the argument is true and 0 otherwise. score analytically intractable for Bayesian Sets. Assuming a Multinomial generative model for the integer count data x , the expression for the score is even simpler than be-fore:  X  MLMN = Xq where q = log  X  S  X  log  X  D . Here,  X  are the multinomial model parameter ML estimates. We perform a similar shrinkage of the  X  S estimates in this case also. This could be of tremendous practical advantage when items have count values which would result in lost informa-tion if represented in binary format (an example is for long documents, or numeric ratings data).
Maxent [3], is a regularized density estimation procedure originally conceived for species distribution estimation. In the typical maxent problem setup, you are given a data col-lection D (consisting of items and their associated features), and subset of these examples S which are known  X  X resence X  locations 2 . The maxent problem is set up to estimate a dis-tribution over the finite set of items such that the expected feature values under this distribution are close to the empir-ical expected feature values for the given set S . The name maxent derives from the fact that the distribution that is chosen, is the one which is most uniform (or has maximum entropy)while satisfying the expected feature constraints. Once the distribution is estimated, it can be used to score the items in the dataset (the assumption being that S was sampled independently from this distribution). The max-ent formulation is intuitively appealing to the set retrieval problem as it balances two criteria. By seeking the most uniform distribution we remain agnostic to the relevancy of the items not in S . On the other hand, the expected fea-ture value constraint enforces that items with similar feature profiles as the items in S will be more relevant.
Mathematically, the problem is to estimate a distribu-tion  X  over the items D , given a set S , consisting of d el-ements from D . We will use the expression  X   X  to denote our estimated distribution. For clarity, in what follows, we will denote the n length feature vector by f , with f j = x .,j . Denote the empirical distribution over D by  X   X  ( z ) = | 1  X  i  X  d : z i = z | /d . As Phillips et al. [3] point out, in general  X   X  may not be a good approximation to  X  , but it seems reasonable to expect  X   X  [ f j ] to be reasonably close to  X  [ f j ] for all n features. They suggest the following con-trol how closely we want the empirical estimate to be to the true expectation. In our experiments, we found re-sults to not be extremely sensitive to the  X  j values. We set them to be equal for all components and in the 10  X  5 to 10  X  7 range. From amongst all the distributions satisfy-ing these constraints, the maxent problem is cast as choosing the distribution with maximum entropy (and hence choosing the distribution closest to uniform). If the set of distribu-tions considered are Gibbs distributions, that is, distribu-tions of the form  X   X  ( x ) = q  X  ( x ) = e  X  . f ( x ) /Z et al. [3] prove that the maximum likelihood Gibbs distri-bution is the required maxent distribution. In the above,
In species modeling, the  X  X tems X  are geographical locations (assumed to be of finite number). The dataset D consists of observed features of each geographical location, for example measurements like average rainfall, temperature etc. The set S corresponds to presence locations, i.e., locations where sightings of the animal/plant have been confirmed. Z  X  = tion function, and  X   X  R n are the model parameters. The resulting convex optimization problem is: In the above, RE ( . k . ) denotes the relative entropy (or KL di-vergence) and is defined as RE ( p k q ) = Since the objective function is a convex, there are many ef-ficient algorithms that find a minimum. We employ a co-ordinate wise gradient descent approach [3] that scales well with large numbers of features. Finally, after the optimiza-tion is complete and we have obtained the density estimate  X   X  = q  X  over D , we set the relevance score exactly equal to it  X  Maxent ( x ) = q  X  ( x ).
As an alternative technique working in the original feature space, we pose the problem in terms of learning a distance metric over the feature space. We examine an approach where we assume that the set S provides us information about a distance metric, which we try to estimate. The in-tuition behind the approach is that the given set of items S , can thought of as examples which should affect how we measure distance between arbitrary items in the dataset. In particular, the fact that two specific items are in S implies that we believe that the distance between these items is small (the user believes the items are somehow connected). Consequently, in computing distances between items, this implies certain features are more important than others. In-tuitively, features where the items in S mostly agree are likely important, and features where they disagree substan-tially are deemed unimportant (since despite their disagree-ment on such features, the user still considers them related).
We pose the distance metric learning problem as one of learning the Mahalanobis distance matrix M , such that the squared distance between two items x i and x j is given by x
M x j . For efficiency sake, we focus on learning diagonal matrices of the form M (where m is the vector on the diag-onal of M ). This approach has been studied in the past in a slightly different context [2]. Mathematically, the problem is set up as the following: Note that the determinant constraint is required in order to keep solution non-degenerate (Otherwise M = 0 solves the unconstrained problem). In this setting the optimal solution to the problem can be found [2], and the solution is to set x in distance-based sets) and to set m jj  X  1 P a constant factor). In our experimental results, we set the constant of proportionality to |S| X  1. Hence,  X  DML ( x ) = ( x i  X  x S ) T M ( x i  X  x S ), with M, x S as defined above.
We collected a dataset which comprised the text descrip-tions associated with each movie in the Netflix challenge dataset (17,770 titles). While we use text based movie fea-tures in this work mainly for interpretability, the procedure applies equally well to ratings data, where movie features would be user-based. The text data for each movie includes a synopsis, the year of production, actor and director infor-mation, as well as some user-tag data. We processed the data by stemming, removing stopwords, and keeping the 13,233 most frequent tokens.

Representative results on two queries for this dataset are shown. The first query is rather broad-based, S = { Doctor Zhivago, Gone with the Wind } , which are two classic epic style dramas set in a war/revolutionary background. The re-sults returned from all methods are reasonable, with many other classic epic war-time dramas being amongst the top results (note the war/fighting theme is not well represented in the Google sets result). Amongst the methods, maxent, Bayesian Sets and the distance metric learning techniques return some potentially irrelevant films on the list (e.g., Darkside Blues , which is a science-fiction anime movie, My Voyage to Italy , which is a movie about classic Italian films etc.).

The second query is constructed more specifically, to probe feature importance. Essentially, methods that make stronger individual feature assumptions (ML Sets and DML Sets) should do better on these queries than the other methods (Bayesian Sets and Maxent). The input we use consists of three movies starring the actor Tom Cruise, S = { Magnolia, Jerry Maguire, Days of Thunder } . Since the genres and plots of these movies are quite different, the only common thread is the actor X  X  presence. The results again show how under these assumptions, techniques that exploit strong feature-based relevance in their scores perform better than those that do not. In particular, Bayesian Sets appears to include some movies (e.g., Boogie Nights, Hellcab ) which don X  X  star Tom Cruise. So does Google sets ( The Pelican Brief ).
In this paper we explore an on-demand set-based recom-mendation problems from various perspectives. We believe this problem is interesting and practically relevant as it ad-dresses a very different kind of need in current recommender system technology. While in current recommenders they sys-tem tries to influence user behavior (via recommendations), here instead, we assume the user drives a a search for con-tent and is only aided by the system. We formulate this problem in terms of some well established machine learning techniques. We present ML Sets, an extension to Bayesian Sets that is tuning parameter free and can be applied to count data. We also present novel applications of maxent [3] and DML [2] to this problem. Finally, we show some anecdotal results on a movie dataset illustrating that tech-niques making relatively mild assumptions on the feature strengths as they relate to the relevance of the items in the dataset perform well on general/vague input concepts. In contrast, these same techniques are less effective than tech-niques based on strong feature-relevance assumptions when faced with tight concept set queries. An interesting future direction would be to combine these strengths possibly by an informed blending of techniques, by further understanding the input set.
