 Many cognitive theories of conceptual organisation assume that concepts are distributed representations over semantic primitives, often referred to as fea-2004). That is, we can understand the meaning of a concept through its properties. For example, under-standing the meaning of BANANA is closely related to understanding that it has properties such as is a fruit , is yellow , is long , is sweet , and knowing how these properties overlap with or differ from the prop-erties of other concepts.

A number of property norm datasets, where hu-mans were asked to list attributes of given concepts, have been collected to test this hypothesis (McRae et al., 2005; Vinson and Vigliocco, 2008; Devereux et al., 2013). After having been used to test mod-els of conceptual representation in cognitive science for decades (Randall et al., 2004; Cree et al., 2006), these datasets have proved to be useful in a wide range of semantic NLP tasks as well, including text simplification for limited vocabulary groups. More recently, property norms have been used as a proxy for perceptual information in a number of studies on multi-modal semantics (Andrews et al., 2009; Ri-ordan and Jones, 2011; Silberer and Lapata, 2012; Roller and Im Walde, 2013; Hill and Korhonen, 2014). Such models aim to addres the grounding problem (Harnad, 1990) that distributional semantic models of language (Turney and Pantel, 2010; Clark, 2015) suffer from.

Property norms are a valuable source of seman-tic information, and can potentially be applied to a variety of NLP tasks, but are expensive to obtain because they involve intensive human annotation. The largest property norm dataset to date consists of just 638 concepts (Devereux et al., 2013), and the most widely cited one presents properties for only 541 concepts (McRae et al., 2005). If we are to use these datasets in large-scale semantic tasks, we would need to extend the currently available prop-erty norms by obtaining annotations for more than just a few hundred words.

The alternative to collecting more data through human annotation is to increase the coverage of property norms datasets by automatically inferring properties of concepts from easily accessible re-sources, such as textual data. Considering the fact that concepts, as well as their properties, are in lin-guistic form, the task then becomes a bootstrapping one where we take advantage of the abundance of freely available textual corpora.

There are two strands of research that attempt to automatically obtain property norm data for new concepts. One approach is to automatically generate feature norms from text corpora by mining text data for a set of generalised property patterns (Kelly et al., 2014; Baroni et al., 2010; Barbu, 2008). An-other avenue of research is inspired by Lazaridou et al. (2014) and Mikolov et al. (2013b) and tries to increase the coverage of feature norms through cross-modal mapping from linguistic information (Fagarasan et al., 2015).

Here, we follow recent trends in multi-modal se-mantics and explore automatic property norm ex-traction from visual, rather than textual, data. Ob-taining property norms from visual information makes intuitive sense: information contained in the property norm datasets can often be attributed to extra-linguistic modalities X  X  large proportion of relevant properties are visual, auditory or tactile, rather than linguistic (e.g. is round, makes noise, is yellow ).

We show that such conceptual properties can be more accurately predicted through cross-modal mappings from raw perceptual information (i.e. im-age data) or multi-modal models (i.e. text and image data combined) rather than from purely textual in-formation (Section 3). Furthermore, we analyse the quality of human collected property norm datasets and conclude that these are sparse and incomplete, meaning that there will be a lot of property annota-tions missing for a given concept (e.g. has legs is not listed as a property of TORTOISE ). We show that having a complete dataset can drastically increase the performance of automatic feature prediction, re-sulting in a truer evaluation (Section 3.5). Lastly, we demonstrate how property norm datasets could be used in an image retrieval task (Section 4), which opens up intriguing possibilities for retrieving con-cepts based on their visual properties. Property norming studies are set up in the following way: participants are asked to freely write down a list of properties for a given concept, whilst being encouraged to consider different kinds of properties Table 1: Examples of features together with their production frequencies from MCRAE (how the concept feels, smells, what it is used for etc ).

Besides collecting lists of properties for the con-cepts of interest, a number of useful property statis-tics are also collected during these studies. For ex-ample, the number of participants that have pro-duced the same property for a given concept (also called production frequency ) and the number of con-cepts for which a particular property is listed in the dataset (number of concepts per feature ) have been proposed as fundamental organising principles of cognitive models (Cree et al., 2006).

One of the most widely used property norm datasets is the one collected by McRae et al. (2005), henceforth MCRAE . It contains feature norms for a set of 541 concrete nouns. Each concept was seen by 30 participants and only features that were listed by at least 5 participants were recorded. The pub-lished dataset contains a total of 2526 features, with a mean of 13.7 features per concept. The numbers of features registered for a given concept range be-tween 6 (for concepts like COLANDER or HARMON -ICA ) and 26 (for FAWN ). Table 1 lists some exam-ples of properties that have been produced for BA -NANA and CELLO , taken from the MCRAE dataset.
The largest feature norm dataset published to date was developed by the Cambridge Centre for Speech, Language and Brain (Devereux et al., 2013). It contains semantic properties for 638 concrete con-cepts, with 415 of these also appearing in MCRAE . The data collection experiment was done similarly to McRae et al. (2005), using a production frequency cutoff of 5. The final dataset lists a total of 4359 fea-tures for the 638 concepts, with an average of 2.15 features per concept more than MCRAE . Although most property norm datasets have only collected property norms for nouns, Vinson and Vigliocco Table 2: Property types and associated examples (2008) also include verbs in their study.

All the experiments presented in this paper were conducted on MCRAE . Our choice is motivated by the fact that this dataset has also been used in pre-vious work on automated property norm prediction (Kelly et al., 2014; Fagarasan et al., 2015), besides being one of the largest publicly-available property norm datasets.

One aspect of feature norms that previous work (Kelly et al., 2014; Baroni et al., 2010; Barbu, 2008; Fagarasan et al., 2015) fails to capture is their multi-modal nature. Even though the attributes are elicited in a linguistic form, and some properties (e.g. what things look like) are easier to verbalise than others (e.g. what things smell like), these datasets contain a variety of property types, ranging from visual and auditory to encyclopaedic and behavioural. Table 2 shows some examples for each of the 10 property types as defined and annotated in MCRAE . More than 25% of all features are visual (e.g. is yellow, is round, made of metal ); hence a natural question that follows is whether images can be used in the property norm prediction task and how their per-formance compares to that of predicting properties from text. Cross-modal maps represent a formalisation of the reference problem. For example, by inducing cross-modal maps between visual vectors and linguistic ones we can learn which images (represented as vi-sual vectors) refer to which concepts (represented as Table 3: Subspace of PROPNORM . Important to note that MCRAE is not complete, meaning that even though some properties are true of a given concept, they have not been produced by the human partic-ipants (e.g. the is edible property for APPLE holds the value 0). text-based distributional vectors) (Lazaridou et al., 2014). This represents an extension of the object recognition problem, since we want to associate im-ages with semantic representations of their depicted objects, rather than just with their label (Frome et al., 2013; Socher et al., 2014).

The benefit of this approach lies in its generali-sation power: once a function between the two se-mantic spaces is learnt, it can be used to see how an unseen concept relates to other concepts, just by looking at an image of that concept. This is referred to as the zero-shot learning task (Palatucci et al., 2009; Lazaridou et al., 2014). Our task is to increase the coverage of the property norm datasets, meaning that we want to predict properties for new (unseen) concepts. For example, the concept WOLF is not in-cluded in MCRAE , but it would be desirable to know which of the properties in the dataset apply to it (e.g. is animal, has 4 legs ) and which don X  X  (e.g. a bird, made of metal ). 3.1 Building modality-specific representations We obtain distributed representations of concepts in the property-norm semantic space (henceforth PROPNORM ) by simply treating MCRAE as a bag of 2526 properties, with the production frequencies representing the  X  X o-occurrence counts X  (Table 3).
Our visual space (henceforth VISUAL ) consists of visual representations for all the 541 concepts in MCRAE , built as follows. First, we retrieve 10 im-previous work (Bergsma and Goebel, 2011; Kiela and Bottou, 2014). The image representations are then obtained by extracting the pre-softmax layer from a forward pass in a convolutional neural net-work that has been trained on the ImageNet classifi-cation task using Caffe (Jia et al., 2014). We aggre-gate images associated with a concept into an overall visually grounded representation by taking the mean of the individual image representations. The dimen-sionality of the visual vectors is 4096.
 We also build three linguistic spaces ( DISTRIB , SVD and EMBED ), along the lines of Fagarasan et al. (2015). DISTRIB is a 10K-dimensional  X  X anilla X  distributional semantic space, where the contexts are the top 10K most frequent lemmatised words (excluding stopwords) from the October 2013 Wikipedia dump. We use raw frequency counts with context windows being defined as sentence bound-aries. SVD is a 300-dimensional SVD-reduced ver-sion of DISTRIB where PPMI has been applied to the raw counts. EMBED stands for the continu-ous vector representations from the log-linear skip-gram model of Mikolov et al. (2013a). We used the on part of the Google News dataset (about 100 bil-lion words).

We will also employ three multi-modal seman-SUAL + EMBED ), in which the visual ( VISUAL ) and respective linguistic representations ( DISTRIB , SVD , EMBED ) are combined into a multi-modal rep-resentation by concatenating their respective L2-normalized representations. 3.2 Method and evaluation Following previous work (Fagarasan et al., 2015; Kiela et al., 2015) we use partial least squares re-property-norm space ( PROPNORM ) from the visual ( multi-modal semantic spaces ( VISUAL + DISTRIB , we take advantage of the fact that we possess both visual/linguistic/multi-modal and property norm in-formation for the concepts in MCRAE . Let X  X  con-sider the VISUAL  X  PROPNORM setting as an exam-ple. We use this cross-modal vocabulary to learn a mapping function between VISUAL and PROP -NORM : this function will learn to map visual dimen-sions to property dimensions. During testing, we use the learnt function to map the visual informa-tion of a previously unseen concept (e.g. CAT ) to the property norm space and obtain a predicted prop-erty vector for that concept. Ideally, we want this predicted property vector to be closer to the gold-standard property vector for CAT than to any other property vector (i.e. the label of its nearest neigh-
We use the standard evaluation metric for this task: average percentage correct at N (P@N) (Fa-garasan et al., 2015; Lazaridou et al., 2014; Kiela et al., 2015). This measures how many of the test in-stances were ranked within the top N highest ranked nearest neighbors (using the cosine measure). All the results reported in Table 4 use the zero-shot learning procedure X  X or each of the 541 concepts in MCRAE , we train a mapping on the remaining 540 concepts and record whether the correct label is retrieved among the top N neighbours X  X nd are averaged over the entire dataset. We also compare to a random baseline, for which a concept X  X  nearest neighbours list is obtained by randomly ranking the list of target words.

Since the cross-modal map allows us to obtain property vectors for any concept, we were also able to evaluate these semantic representations on a stan-dard NLP task, such as the well known conceptual similarity and relatedness task. The MEN test col-lection (Bruni et al., 2014) contains human similar-ity ratings for 3000 concept pairs. Performance on this dataset is usually measured by computing the Spearman  X  s correlation between the ranking pro-duced by the similarity scores of the learnt property vectors and that produced by the human-annotated concept similarity scores. Similarity between con-cept pairs is calculated using cosine similarity.
For each of the semantic spaces presented in Table 5 we learn a cross-modal map to PROPNORM using all the concepts in MCRAE at training time. Dur-ing testing, we predict property vectors for all con-cepts in MEN -NOUNS , a subset of the MEN dataset consisting of 1285 noun pairs that don X  X  occur in MCRAE . Table 5 reports the Spearman  X  s correla-tion of the predicted property vectors and the gold-standard relatedness scores on MEN -NOUNS (col-umn  X  PROPNORM ), as well as the correlation of the Table 4: Zero-shot learning performance when map-ping to the property-norm space ( PROPNORM ) Table 5: Performance (Spearman  X  s correlation) of various uni-modal and multi-modal semantic spaces (column SS ), together with that of the property vec-tors they predict (column  X  PROPNORM ) on a se-mantic relatedness task ( MEN -NOUNS ) original semantic spaces (e.g. DISTRIB or SVD ) and the gold standard scores (column SS ). 3.3 Quantitative results The results presented in Table 4 show that visual in-formation is a overall better predictor of a concept X  X  properties than linguistic information. The cross-modal maps from the visual space VISUAL outper-form all those from linguistic spaces DISTRIB , SVD , EMBED , and the addition of linguistic information to the visual one (maps from VISUAL + DISTRIB , VI -improve the performance.

It is also important to point out that, even though the P@1 numbers may appear small, similar results have been reported for other zero-shot cross-modal maps (Lazaridou et al., 2014; Kiela et al., 2015). Overall results are good for higher values of N and the qualitative results (Table 6) demonstrate how well the mapping is performing.

A model will achieve a perfect score on this task if it is able to predict, for a given concept, exactly those features (and associated production frequen-cies) listed in MCRAE . However, close-to-perfect performance in this task is impossible, since almost 30% of the features only occur with one concept, and hence can X  X  be reconstructed for that particular concept. Consider the case of the a baby deer prop-erty: this only occurs in the MCRAE dataset as an attribute of FAWN . When predicting properties of FAWN as part of the zero-shot learning procedure, the a baby deer property can X  X  be learned, since it doesn X  X  occur with any other concept.

Columns SS and  X  PROPNORM in Table 5 re-port correlations with the MEN -NOUNS ratings. The predicted property vectors obtain a high correlation with the MEN scores, showing that the property vec-tors do capture lexical similarity well, although not as well as the linguistic vectors, which was expected (Bruni et al., 2012). An useful finding is that in some cases, the predicted property vectors obtain a better correlation with the MEN scores than their predic-tors (i.e. the VISUAL and multi-modal vectors). This shows a potential strength of the attribute-centric se-mantic representations: their capability to perform better on some lexical similarity/relatedness tasks than representations that contain raw perceptual in-formation. 3.4 Qualitative results In order to gain more insight into the differences be-tween the from vision and from language mappings, we performed two types of qualitative analysis: we looked at the differences in nearest neighbours of the predicted property-norm representations (Table 6), as well as the top predicted properties of a concept (Table 6). In the from language setting we learned the mapping using the EMBED space, as it was the best performing linguistic space at P@1 and P@5 as shown in Table 4. We obtained the list of near-est neighbours as follows: at test time, we use the learnt cross-modal map to project the visual or lin-guistic representation of the unseen concept onto a property-norm representation. Using cosine similar-ity, we then obtain a ranked list of neighbours from all the 541 gold-standard property vectors. By in-specting the nearest neighbour predictions, we can check where the unseen concept is mapped to (e.g. BANANA is mapped close to yellow fruits). In or-der to retrieve the top predicted properties of a con-cept, we rank the dimensions of PROPNORM accord-ing to the weights in the predicted property vector (e.g. the predicted property vector for BANANA has high weights for a fruit and is green when mapped from language).

By looking at the nearest neighbour predictions, we observe that, when mapping from visual input, the predicted vector will be mapped into a subspace containing visually-similar things. When mapping from linguistic input, the neighbours tend to be con-cepts that are semantically related or denoted by words that occur in the same context as the target concept (e.g. worms are found in plums and nec-tarines).

A notable result is that when mapping from vi-sion, the top neighbours tend to share the same colour (top neighbours for BANANA are yellow fruits, for SWAN are white birds) or shape as the target concept (top neighbours for WORM are long things with no legs). One possible clue as to why vision is better at predicting a concept X  X  properties is given by the fact that it obtains better results on concepts such as PANTS or STOOL , where the only difference to very similar concepts like TROUSERS or CHAIR are visual (a STOOL has no backrest as opposed to a CHAIR ).

However, there are cases in which the visual at-tributes of an object are not very useful in predicting its most important features: e.g. DRUM is mapped into a subspace of round objects (from vision), and not instruments (from language).
 Besides the difference in top predicted features, Table 6 also indicates a shortcoming of MCRAE , specifically that this is not complete, meaning that not all properties that apply to a given concept were produced by the human annotators. Most of the top predicted attributes that don X  X  occur in the dataset (those marked with * in Table 6) are highly plausi-ble properties for the given concepts: tastes sweet for BANANA or has legs for TORTOISE . This also means that the model is being unfairly penalised.
In order to obtain a complete version of MCRAE , every possible ( CONCEPT , property ) pair would have to be checked for validity and annotated ac-cordingly depending on whether property is a valid attribute of CONCEPT . 3.5 Importance of complete data We were interested in measuring the impact that a complete dataset of features would have on the performance of the cross-modal zero-shot learning task. Silberer et al. (2013) conducted a study using a subset of the concepts and properties in MCRAE , whereby every property was annotated if it was a plausible attribute of the concept.

The published dataset ( SILBERER ) consists of vi-sual attribute annotations for 512 concepts (that also occur in MCRAE ) and 693 visual properties. The an-Table 7: Comparison of various datasets, according to the number of concepts and properties covered, as well as the pairs of ( CONCEPT , property ) contained Table 8: Zeroshot learning performance for the vi-sion to norms cross-modal map on different training and test sets notation was done on a per-concept basis by looking at 10 images retrieved from ImageNet (Deng et al., 2009) and selecting all the attributes that were con-sidered to be generally true for the given concept, even if not depicted in the images. For example, has a pit is a valid visual attribute for PLUM , even though not all retrieved images of plums show the pit.

Since not all of the 693 visual properties covered in
SILBERER can be found in MCRAE , we will only be concerned with the subset of SILBERER which contains only those visual properties that also occur in
MCRAE , henceforth SILB -VIS . These datasets are complete , since they were exhaustively annotated as explained above.

Let us also define M -VIS as the subset of MCRAE that contains the 512 concepts listed in SILBERER and the 283 properties that are common to SIL -BERER and MCRAE , together with their produc-tion frequencies as in MCRAE . This will act as our incomplete dataset. Table 7 lists all the afore-mentioned datasets, together with statistics related to their number of concepts, features and concept-feature pairs. It also demonstrates the sparsity of MCRAE : it contains fewer ( CONCEPT , property ) pairs than SILBERER , even though it contains 4 times more properties.

All the experiments performed using these datasets are listed in Table 8. These are identical in methodology to the zero-shot cross-modal maps from VISUAL to PROPNORM ; the only difference be-ing the datasets that these are run on.

The row (train: M -VIS , test: M -VIS ) represents the setting where the cross-modal map learning and test-ing are both done on the incomplete set of data, just like we would do using MCRAE . We notice a huge improvement in performance by using the complete data only at test-time (row (train: M -VIS , test: SILB -VIS )). Note that, in this scenario, the learning is carried out in the same way, but the model can X  X  be penalised for ranking plausible features near the top during test time, since we are testing against a complete dataset. This new setting provides a truer evaluation scenario and demonstrates the weakness in using MCRAE as a test set.

Performance improves even more if the complete dataset is used at training time as well (the row (train: SILB -VIS , test: SILB -VIS )), showing the bene-fit of also learning the mapping from complete data, as well as evaluating on it.
 Table 9: Zero-shot learning performance when map-ping to the visual space ( VISUAL ) An interesting question follows from the good per-formance of the cross-modal mapping in Section 3, and that is whether we can reliably predict what con-cepts look like based on their semantic properties. For example, does something that flies, has wings and a beak look like a bird?
This task could be formalised as a property-based query engine, where we can train the cross-modal mapping to learn which concepts refer to which im-ages. We follow the same experimental setup as detailed in Section 3.2 in order to learn a cross-modal map from PROPNORM to VISUAL . We also learn cross-modal maps from the linguistic spaces ( whether conceptual properties or linguistic input are better at predicting visual information.

Table 9 shows the results of our quantitative eval-uation: the average precentage of correctly retrieved mean visual representations at N.
 A qualitative analysis of the PROPNORM to VI -SUAL cross-modal map is shown in Figure 1. Be-cause there are no images associated with the predicted mean visual representation, we retrieve and display the top neighbouring images. These images look surprisingly good, considering that the representation for TAXI in PROPNORM is a sparse vector where only the features is yellow, requires drivers, used for transportation, a car, requires money, found in New York, is expensive, used for passengers, a cab, is fast are activated. We have studied the automatic prediction of prop-erty norms for unseen concepts, through learning the cross-modal mapping from image data. Following previous work, we evaluated on a zero-shot learning task and show that raw visual information (images) is a better predictor for conceptual properties than linguistic input (text). We also presented a short case study demonstrating the importance of having com-plete annotations in the property norm datasets, for both testing and training. Lastly, we demonstrated a possible use case for property norm datasets in an image retrieval task.

Our contributions are two-fold: first, we show that property norms can be successfully predicted from non-linguistic modalities and secondly, we quantify the need to have complete property norm datasets, where a production frequency of 0 for a ( CONCEPT , property ) pair can always be interpreted as  X  property is not true of CONCEPT  X .
 LB is supported by an EPSRC Doctoral Train-ing Grant. DK is supported by EPSRC grant EP/I037512/1. SC is supported by ERC Start-ing Grant DisCoTex (306920) and EPSRC grant EP/I037512/1. We thank the anonymous review-ers for their helpful comments. A link to the data used for the experiments in this paper is available at http://www.cl.cam.ac.uk/  X  ltf24/ .

