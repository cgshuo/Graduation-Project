 Rapid influx of digital data has galvanized the use of empirical methods in many fields including Ma-chine Translation (MT). The increasing availabil-ity of bilingual corpora has made it possible to automatically learn translation rules that required years of linguistic analysis previously. While ad-ditional data is often beneficial for a general pur-pose Statistical Machine Translation (SMT) sys-tem, a problem arises when translating new do-mains such as lectures (Cettolo et al., 2014), patents (Fujii et al., 2010) or medical text (Bojar et al., 2014), where either the bilingual text does not exist or is available in small quantity. All do-mains have their own vocabulary and stylistic pref-erences which cannot be fully encompassed by a system trained on the general domain.

Machine translation systems trained from a sim-ple concatenation of small in-domain and large out-domain data often perform below par be-cause the out-domain data is distant or over-whelmingly larger than the in-domain data. Ad-ditional data increases lexical ambiguity by in-troducing new senses to the existing in-domain vocabulary. For example, an Arabic-to-English SMT system trained by simply concatenating in-and out-domain data translates the Arabic phrase  X  problem of unwanted pregnancy X . This translation is incorrect in the context of the in-domain data, where it should be translated to  X  X bout the prob-lem of choice overload X . The sense of the Ara-bic phrase taken from out-domain data completely changes the meaning of the sentence. In this paper, we tackle this problem by proposing domain adap-tation models that make use of all the data while preserving the in-domain preferences.

A significant amount of research has been car-ried out recently in domain adaptation. The com-plexity of the SMT pipeline, starting from cor-pus preparation to word-alignment, and then train-ing a wide range of models opens a wide horizon to carry out domain specific adaptations. This is typically done using either data selection (Mat-soukas et al., 2009) or model adaptation (Foster and Kuhn, 2007). In this paper, we further re-search in model adaptation using the neural net-work framework.

In recent years, there has been a growing in-terest in deep neural networks (NNs) and word embeddings with application to numerous NLP problems. A notably successful attempt on the SMT frontier was recently made by Devlin et al. (2014). They proposed a neural network joint model (NNJM), which augments streams of source with target n -grams and learns a NN model over vector representation of such streams. The model is then integrated into the decoder and used as an additional language model feature.

Our aim in this paper is to advance the state-of-the-art in SMT by extending NNJM for domain adaptation to leverage the huge amount of out-domain data coming from heterogeneous sources. We hypothesize that the distributed vector rep-resentation of NNJM helps to bridge the lexical differences between the in-domain and the out-domain data, and adaptation is necessary to avoid deviation of the model from the in-domain data, which otherwise happens because of the large out-domain data.

To this end, we propose two novel extensions of NNJM for domain adaptation. Our first model minimizes the cross entropy by regularizing the loss function with respect to the in-domain model. The regularizer gives higher weight to the training instances that are similar to the in-domain data. Our second model takes a more conservative ap-proach by additionally penalizing data instances similar to the out-domain data.

We evaluate our models on the standard task of translating Arabic-English and English-German language pairs. Our adapted models achieve bet-ter perplexities (Chen and Goodman, 1999) than the models trained on in-and in+out-domain data. Improvements are also reflected in BLEU scores (Papineni et al., 2002) as we compare these mod-els within the SMT pipeline. We obtain gains of up to 0 . 5 and 0 . 6 on Arabic-English and English-German pairs over a competitive baseline system.
The remainder of this paper is organized as fol-lows: Section 2 gives an account on related work. Section 3 revisits NNJM model and Section 4 dis-cusses our models. Section 5 presents the experi-mental setup and the results. Section 6 concludes. Previous work on domain adaptation in MT can be broken down broadly into two main categories namely data selection and model adaptation . 2.1 Data Selection Data selection has shown to be an effective way to discard poor quality or irrelevant training in-stances, which when included in an MT system, hurts its performance. The idea is to score the out-domain data using a model trained from the in-domain data and apply a cut-off based on the re-sulting scores. The MT system can then be trained on a subset of the out-domain data that is closer to in-domain. Selection based methods can be helpful to reduce computational cost when train-ing is expensive and also when memory is con-strained. Data selection was done earlier for lan-guage modeling using information retrieval tech-niques (Hildebrand et al., 2005) and perplexity measures (Moore and Lewis, 2010). Axelrod et al. (2011) further extended the work of Moore and Lewis (2010) to translation model adaptation by using both source-and target-side language mod-els. Duh et al. (2013) used a recurrent neural lan-guage model instead of an ngram-based language model to do the same. Translation model features were used recently by (Liu et al., 2014; Hoang and Sima X  X n, 2014) for data selection. Durrani et al. (2015a) performed data selection using operation sequence model (OSM) and NNJM models. 2.2 Model Adaptation The downside of data selection is that finding an optimal cut-off threshold is a time consuming pro-cess. An alternative to completely filtering out less useful data is to minimize its effect by down-weighting it. It is more robust than selection since it takes advantage of the complete out-domain data with intelligent weighting towards the in-domain.
Matsoukas et al. (2009) proposed a classification-based sentence weighting method for adaptation. Foster et al. (2010) extended this by weighting phrases rather than sentence pairs. Other researchers have carried out weighting by merging phrase-tables through linear interpolation (Finch and Sumita, 2008; Nakov and Ng, 2009) or log-linear combination (Foster and Kuhn, 2009; Bisazza et al., 2011; Sennrich, 2012) and through phrase training based adaptation (Mansour and Ney, 2013). Durrani et al. (2015a) applied EM-based mixture modeling to OSM and NNJM models to perform model weighting. Chen et al. (2013b) used a vector space model for adaptation at the phrase level. Every phrase pair is represented as a vector, where every entry in the vector reflects its relatedness with each domain. Chen et al. (2013a) also applied mixture model adaptation for reordering model.

Other work on domain adaptation includes but not limited to studies focusing on topic models (Eidelman et al., 2012; Hasler et al., 2014), dy-namic adaptation without in-domain data (Sen-nrich et al., 2013; Mathur et al., 2014) and sense disambiguation (Carpuat et al., 2013).

In this paper, we do model adaptation using a neural network framework. In contrast to pre-vious work, we perform it at the (bilingual) n -gram level, where n is sufficiently large to cap-ture long-range cross-lingual dependencies. The generalized vector representation of the neural net-work model reduces the data sparsity issue of tra-ditional Markov-based models by learning better word classes. Furthermore, our specially designed loss functions for adaptation help the model to avoid deviation from the in-domain data without losing the ability to generalize. In recent years, there has been a great deal of ef-fort dedicated to neural networks (NNs) and word embeddings with applications to SMT and other areas in NLP (Bengio et al., 2003; Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Gao et al., 2014; Schwenk, 2012; Collobert et al., 2011; Mikolov et al., 2013a; Socher et al., 2013; Hin-ton et al., 2012). Recently, Devlin et al. (2014) proposed a neural network joint model (NNJM) and integrated it into the decoder as an additional feature. They showed impressive improvements in Arabic-to-English and Chinese-to-English MT tasks. Let us revisit the NNJM model briefly.
Given a source sentence S and its correspond-ing target sentence T , the NNJM model computes the conditional probability P ( T | S ) as follows: where, s i is a q -word source window for the tar-get word t i based on the one-to-one (non-NULL) alignment of T to S . As exemplified in Figure 1, this is essentially a ( p + q ) -gram neural network LM (NNLM) originally proposed by Bengio et al. (2003). Each input word i.e. source or target word in the context is represented by a D dimensional vector in the shared look-up layer L  X  R | V i | X  D , where V i is the input vocabulary. 1 The look-up layer then creates a context vector x n representing the context words of the ( p + q ) -gram sequence by concatenating their respective vectors in L . The concatenated vector is then passed through non-linear hidden layers to learn a high-level represen-tation, which is in turn fed to the output layer. The output layer has a softmax activation over the output vocabulary V o of target words. Formally, the probability of getting k -th word in the output given the context x n can be written as: where  X  ( x n ) defines the transformations of x n through the hidden layers, and w k are the weights from the last hidden layer to the output layer. For notational simplicity, henceforth we will use ( x n ,y n ) to represent a training sequence.
 By setting p and q to be sufficiently large, NNJM can capture long-range cross-lingual de-pendencies between words, while still overcom-ing the data sparseness issue by virtue of its dis-tributed representations (i.e., word vectors). A ma-jor bottleneck, however, is to surmount the com-putational cost involved in training the model and applying it for MT decoding. Devlin et al. (2014) proposed two tricks to speed up computation in decoding. The first one is to pre-compute the hid-den layer computations and fetch them directly as needed during decoding. The second technique is to train a self-normalized NNJM to avoid compu-tation of the softmax normalization factor (i.e., the denominator in Equation 2) in decoding. How-ever, self-normalization does not solve the compu-tational cost of training the model. In the follow-ing, we describe a method to address this issue. 3.1 Training by Noise Contrastive Estimation The standard way to train NNLMs is to maximize the log likelihood of the training data: where, y nk = I ( y n = k ) is an indicator vari-able (i.e., y nk =1 when y n = k , otherwise 0 ). Op-timization is performed using first-order online methods, such as stochastic gradient ascent (SGA) with standard backpropagation algorithm. Unfor-tunately, training NNLMs are impractically slow because for each training instance ( x n ,y n ) , the softmax output layer (see Equation 2) needs to compute a summation over all words in the output (Gutmann and Hyv  X  arinen, 2010) provides an effi-cient and stable way to avoid this repetitive com-putation as recently applied to NNLMs (Vaswani et al., 2013; Mnih and Teh, 2012). We can re-write Equation 2 as follows: where  X  ( . ) is the un-normalized score and Z ( . ) is the normalization factor. In NCE, we consider W Z ( . ) as an additional model parameter along with the regular parameters, i.e., weights, look-up vec-tors. However, it has been shown that fixing Z ( . ) to 1 instead of learning it in training does not affect the model performance (Mnih and Teh, 2012). For each training instance ( x n ,y n ) , we add M noise samples ( x n ,y m n ) by sampling y m n from a known noise distribution  X  (e.g., unigram, uniform ) M many times (i.e., m = 1 ...M ); see Figure 1. NCE loss is then defined to discriminate a true instance from a noisy one. Let C  X  { 0 , 1 } denote the class of an instance with C = 1 indicat-ing true and C = 0 indicating noise . NCE maxi-mizes the following conditional log likelihood: where Q = P ( y n ,C = 1 | x n , X , X  ) + P ( y m n ,C = 0 | x n , X , X  ) is a normalization constant. After re-moving the constant terms, Equation 6 can be fur-ther simplified as: where  X  nk = P ( y m n = k | x n , X  ) is the noise dis-tribution,  X  nk =  X  ( y n = k | x n , X  ) is the unnormal-ized score at the output layer (Equation 4), and y nk NCE reduces the number of computations needed at the output layer from | V o | to M + 1 , where M is a small number in comparison with | V o | . In all our experiments we use NCE loss with M = 100 samples as suggested by Mnih and Teh (2012). The ability to generalize and learn complex se-mantic relationships (Mikolov et al., 2013b) and its compelling empirical results gives a strong mo-tivation to use the NNJM model for the problem of domain adaptation in machine translation. How-ever, the vanilla NNJM described above is limited in its ability to effectively learn from a large and diverse out-domain data in the best favor of an in-domain data. To address this, we propose two neu-ral domain adaptation models (NDAM) extending the NNJM model. Our models add regularization to its loss function either with respect to in-domain or both in-and out-domains. In both cases, we first present the regularized loss function for the nor-malized output layer with the standard softmax, followed by the corresponding un-normalized one using the noise contrastive estimation. 4.1 NDAM v 1 To improve the generalization of word embed-dings, NNLMs are generally trained on very large datasets (Mikolov et al., 2013a; Vaswani et al., 2013). Therefore, we aim to train our neural domain adaptation models (NDAM) on in-plus out-domain data, while restricting it to drift away from in-domain. In our first model NDAM v 1 , we achieve this by biasing the model towards the in-domain using a regularizer (or prior) based on the in-domain model. Let  X  i be an NNJM model al-ready trained on the in-domain data. We train an adapted model  X  a on the whole data, but regular-izing it with respect to  X  i . We redefine the normal-ized loss function of Equation 3 as follows: where  X  y nk (  X  a ) is the softmax output and p nk (  X  i is the probability of the training instance accord-ing to the in-domain model  X  i . Notice that the loss function minimizes the cross entropy of the cur-rent model  X  a with respect to the gold labels y n and the in-domain model  X  i . The mixing param-eter  X   X  [0 , 1] determines the relative strength of the NCE loss of Equation 7 as: We use SGA with backpropagation to train this model. The derivatives of J (  X  a ) with respect to the final layer weight vectors w j turn out to be: 4.2 NDAM v 2 The regularizer in NDAM v 1 is based on an in-domain model  X  i , which puts higher weights to the training instances (i.e., n -gram sequences) that are similar to the in-domain ones. This might work better when the out-domain data is similar to the in-domain data. In cases where the out-domain data is different, we might want to build a more conservative model that penalizes training instances for being similar to the out-domain ones.
Let  X  i and  X  o be the two NNJMs already trained from the in-and out-domains, respectively, and  X  o is trained using the same vocabulary as  X  i . We de-fine the new normalized loss function as follows: where y nk ,  X  y nk (  X  a ) , p nk (  X  i ) and p nk (  X  o ilarly defined as before. This loss function min-imizes the cross entropy of the current model  X  a with respect to the gold labels y n and the differ-ence between the in-domain model  X  i and the out-domain model  X  o . Intuitively, the regularizer as-signs higher weights to training instances that are not only similar to the in-domain but also dissim-ilar to the out-domain. The parameter  X   X  [0 , 1] determines the strength of the regularization. The corresponding NCE loss can be defined as follows: The derivatives of the above cost function with re-spect to the final layer weight vectors w j are:
In a way, the regularizers in our loss functions are inspired from the data selection methods of Axelrod et al. (2011), where they use cross entropy between the in-and the out-domain LMs to score out-domain sentences. However, our approach is quite different from them in several aspects. First and most importantly, we take the scoring inside model training and use it to bias the training to-wards the in-domain model. Both the scoring and the training are performed at the bilingual n -gram level rather than at the sentence level. Integrating scoring inside the model allows us to learn a robust model by training/tuning the relevant parameters, while still using the complete data. Secondly, our models are based on NNs, while theirs utilize the traditional Markov-based generative models. 4.3 Technical Details In this section, we describe some implementation details of NDAM that we found to be crucial, such as: using gradient clipping to handle vanish-ing/exploding gradient problem in SGA training with backpropagation, selecting appropriate noise distribution in NCE, and special handling of out-domain words that are unknown to the in-domain. 4.3.1 Gradient Clipping Two common issues with training deep NNs on large data-sets are the vanishing and the exploding gradients problems (Pascanu et al., 2013). The er-ror gradients propagated by the backpropagation may sometimes become very small or very large which can lead to undesired (nan) values in weight matrices, causing the training to fail. We also ex-perienced the same problem in our NDAM quite often. One simple solution to this problem is to truncate the gradients, known as gradient clipping (Mikolov, 2012). In our experiments, we limit the gradients to be in the range [  X  5; +5] . 4.3.2 Noise Distribution in NCE Training with NCE relies on sampling from a noise distribution (i.e.,  X  in Equation 5), and the performance of the NDAM models varies consid-erably with the choice of the distribution. We ex-plored uniform and unigram noise distributions in this work. With uniform distribution, every word in the output vocabulary has the same probability to be sampled as noise. The unigram noise dis-tribution is a multinomial distribution over words constructed by counting their occurrences in the output (i.e., n -th word in the n -gram sequence). In our experiments, unigram distribution delivered much lower perplexity and better MT results com-pared to the uniform one. Mnih and Teh (2012) also reported similar findings on perplexity. 4.3.3 Handling of Unknown Words In order to reduce the training time and to learn better word representations, NNLMs are often trained on most frequent vocabulary words only and low frequency words are represented under a class of unknown words, unk . This results in a large number of n -gram sequences containing at least one unk word and thereby, makes unk a
Our NDAM models rely on scoring out-domain sequences (of word Ids) using models that are trained based on the in-domain vocabulary. To score out-domain sequences using a model, we need to generate the sequences using the same vo-cabulary based on which the model was trained. In doing so, the out-domain words that are un-known to the in-domain data map to the same unk class. As a result, out-domain sequences contain-ing unk s get higher probability although they are distant from the in-domain data.

A solution to this problem is to have an in-domain model that can differentiate between its own unk class, resulted from the reduced in-domain vocabulary, and actual unknown words that come from the out-domain data. We intro-duce a new class unk o to represent the latter. We train the in-domain model by adding a few dummy sequences containing unk o occurring on both source and target sides. This enables the model to learn unk and unk o separately, where unk o is a less probable class according to the model. Later, the n -gram sequences of the out-domain data contain both unk and unk o classes depending on whether a word is unknown to only pruned in-domain vocabulary (i.e., unk ) or is un-known to full in-domain vocabulary (i.e., unk o ). In this section, we describe the experimental setup (i.e., data, settings for NN models and MT pipeline) and the results. First we evaluate our models intrinsically by comparing the perplexities on a held-out in-domain testset against the base-line NNJM model. Then we carry out an extrinsic evaluation by using the NNJM and NDAM models as features in machine translation and compare the BLEU scores. Initial developmental experiments were done on the Arabic-to-English language pair. We carried out further experiments on the English-to-German pair to validate our models. 5.1 Data We experimented with the data made publicly available for the translation task of the Interna-tional Workshop on Spoken Language Translation (IWSLT) (Cettolo et al., 2014). We used TED talks as our in-domain corpus. For Arabic-to-English, we used the QCRI Educational Domain (QED)  X  A bilingual collection of educational lec-multiUN (UN) (Eisele and Chen, 2010) as our out-domain corpora. For English-to-German, we used the News, the Europarl (EP), and the Com-mon Crawl (CC) corpora made available for the Table 1 shows the size of the data used.

Training NN models is expensive. We, there-fore, randomly selected subsets of about 300K sentences from the bigger domains (UN, CC and tuned on concatenation of the dev. and test2010 and evaluated on test2011-2013 datasets. The tun-ing set was also used to measure the perplexities of different models. 5.2 System Settings NNJM &amp; NDAM: The NNJM models were 2013) with the following settings. We used a tar-get context of 5 words and an aligned source win-dow of 9 words, forming a joint stream of 14-grams for training. We restricted source and tar-get side vocabularies to the 20K and 40K most frequent words. The word vector size D and the hidden layer size were set to 150 and 750, respec-tively. Only one hidden layer is used to allow faster decoding. Training was done by the stan-dard stochastic gradient ascent with NCE using Table 1: Statistics of the Arabic-English and English-German training corpora in terms of Sen-tences and Tokens (Source/Target). Tokens are represented in millions. 100 noise samples and a mini-batch size of 1000. All models were trained for 25 epochs. We used identical settings to train the NDAM models, ex-cept for the special handling of unk tokens. Machine Translation System: We trained a Moses system (Koehn et al., 2007), with the following settings: a maximum sentence length of 80, Fast-Aligner for word-alignments (Dyer et al., 2013), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield, 2011), lexicalized reordering model (Galley and Manning, 2008), a 5-gram operation sequence model (Durrani et al., 2015b) and other default pa-rameters. We also used an NNJM trained with the settings described above as an additional feature in our baseline system. In adapted systems, we replaced the NNJM model with the NDAM mod-els. We used ATB segmentation using the Stanford ATB segmenter (Green and DeNero, 2012) for Arabic-to-English and the default tokenizer pro-vided with the Moses toolkit (Koehn et al., 2007) for the English-to-German pair. Arabic OOVs were translated using an unsupervised transliter-ation module in Moses (Durrani et al., 2014). We used k-best batch MIRA (Cherry and Foster, 2012) for tuning. 5.3 Intrinsic Evaluation In this section, we compare the NNJM model and our NDAM models in terms of their perplexity numbers on the in-domain held-out dataset (i.e., dev+test2010). We choose Arabic-English lan-guage pair for the development experiments and train domain-wise models to measure the related-ness of each domain with respect to the in-domain. We later replicated selective experiments for the English-German language pair.

The first part of Table 2 summarizes the results for Arabic-English. The perplexity numbers in the second column (NNJM b ) show that NEWS is the Table 2: Comparing the perplexity of NNJM and NDAM models. NNJM b represents the model trained on each individual domain separately. most related domain from the perspective of in-domain data, whereas UN is the farthest having the worst perplexity. The third column (NNJM cat ) shows results of the models trained from concate-nating each domain to the in-domain data. The perplexity numbers improved significantly in each case showing that there is useful information avail-able in each domain which can be utilized to im-prove the baseline. It also shows the robustness of neural network models. Unlike the n -gram model, the NN-based model improves generalization with the increase in data without completely skewing towards the dominating part of the data.

Concatenating in-domain with the NEWS data gave better perplexities than other domains. Best results were obtained by concatenating all the data together (See row ALL ). The third and fourth columns show results of our models (NDAM v  X  ). Both give better perplexities than NNJM cat in all cases. However, it is unclear which of the two is better. Similar observations were made for the English-to-German pair, where we only did exper-iments on the concatenation of all domains. 5.4 Extrinsic Evaluation Arabic-to-English: For most language pairs, the conventional wisdom is to train the system with all available data. However, previously re-ported MT results on Arabic-to-English (Mansour and Ney, 2013) show that this is not optimal and the results are often worse than only using in-domain data. The reason for this is that the UN domain is found to be distant and overwhelmingly large as compared to the in-domain IWSLT data. We carried out domain-wise experiments and also found this to be true.
 We considered three baseline systems: ( i ) B in , Table 3: Results of the baseline Arabic-to-English MT systems. The numbers are averaged over tst2011-2013. which is trained on the in-domain data, ( ii ) B cat , which is trained on the concatenation of in-and out-domain data, and ( ii ) B cat,in , where the MT pipeline was trained on the concatenation but the NNJM model is trained only on the in-domain data. Table 3 reports average BLEU scores across three test sets on all domains. Adding QED and NEWS domains gave improvements on top of the in-domain IWSLT baseline. Concatenation of UN with in-domain made the results worse. Concate-nating all out-domain and in-domain data achieves +0.4 BLEU gain on top of the baseline in-domain system. We will use B cat systems as our baseline to compare our adapted systems with.

Table 4 shows results of the MT systems S v 1 and S v 2 using our adapted models NDAM v 1 and NDAM v 2 . We compare them to the baseline sys-tem B cat , which uses the non-adapted NNJM cat as a feature. S v 1 achieved an improvement of up to +0.4 and S v 2 achieved an improvement of up to +0.5 BLEU points. However, S v 2 performs slightly worse than S v 1 on individual domains. We speculate this is because of the nature of the NDAM v 2 , which gives high weight to out-domain sequences that are liked by the in-domain model and disliked by the out-domain model. In the case of individual domains, NDAM v 2 might be over pe-nalizing out-domain since the out-domain model is only built on that particular domain and always prefers it more than the in-domain model. In case of ALL , the out-domain model is more diverse and has different level of likeness for each domain.
We analyzed the output of the baseline system (S cat ) and spotted several cases of lexical ambigu-ity caused by out-domain data. For example, the lated to choice overload or unwanted pregnancy . The latter translation is incorrect in the context of in-domain. The bias created due to the out-domain data caused S cat to choose the contextually incor-rect translation unwanted pregnancy . However, the adapted systems S v  X  were able to translate it ( How about fitness? ), the word  X   X  AJ  X  is translated to proprietary by S cat , a translation frequently ob-served in the out-domain data. S v  X  translated it correctly to fitness , as preferred by the in-domain. English-to-German: Concatenating all training data to train the MT pipeline has been shown to give the best results for English-to-German (Birch et al., 2014). Therefore, we did not do domain-wise experiments, except for training a system on the in-domain IWSLT data for the sake of com-pleteness. We also tried B cat,in variation, i.e. training an MT system on the entire data and using in-domain data to train the baseline NNJM. The baseline system B cat gave better results and was used as our reference for comparison.

Table 5 shows the results of our systems, S v 1 and S v 2 , compared to the baselines, B in and B cat . Unlike Arabic-to-English, the baseline system B in is much worse than B cat . Our adapted MT systems S v 1 and S v 2 both outperformed the best baseline system (B cat ) with an improvement of up to 0.6 points. S v 2 performed slightly better than S v 1 on one occasion and slightly worse in others.
 Comparison with Data Selection: We also compared our results with the MML-based data Table 6: Comparison with Modified Moore-Lewis selection approach as shown in Table 6. The MML-based baseline systems (B mml ) used 20% selected data for training the MT system and the NNJM. On Arabic-English, both MML-based se-lection and our model (S v 1 ) gave similar gains on top of the baseline system (B cat ). Further results showed that both approaches are complementary. We were able to obtain an average gain of +0.3 BLEU points by training an NDAM v 1 model over the selected data (see S v 1+ mml ).

However, on English-German, the MML-based selection caused a drop in the performance (see Table 6). Training an adapted NDAM v 1 model over selected data gave improvements over MML in two test sets but could not restore the baseline performance, probably because the useful data has already been filtered by the selection process. We presented two novel models for domain adap-tation based on NNJM. Adaptation is performed by regularizing the loss function towards the in-domain model and away from the unrelated out-of-domain data. Our models show better perplex-ities than the non-adapted baseline NNJM mod-els. When integrated into a machine translation system, gains of up to 0.5 and 0.6 BLEU points were obtained in Arabic-to-English and English-to-German systems over strong baselines.
