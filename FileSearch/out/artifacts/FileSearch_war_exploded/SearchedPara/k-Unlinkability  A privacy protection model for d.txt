 1. Introduction
As people hustle and bustle through their daily activities, they leave behind personal data in an increasing number of databases. Individuals are not always endowed with the ability to exert control over whether or not their data is collected; however, the use and sharing of personal data is regulated. In the United States, where there is no centralized legal statute for data protection, a patchwork of laws are tailored to oversee the han-dling and disclosure of specific types of personal data, such as the HIPAA Privacy Rule for health data [9] and the Graham-Leach-Bliley Act for financial data [13] . In contrast, in the European Union, the Data Protection used, and shared. Nonetheless, regardless of locale, many data protection regulations permit the sharing of anonymized data. For instance, the Data Protection Directive, which strictly prohibits secondary uses of per-son-specific data without individual consent, provides an exception to the ruling in Recital 26, which states that the
However, what does it mean for data to be  X  X  X dentifiable X  X ? How do we know when it is no longer identifi-longer sufficient to say that data  X  X  X ooks anonymous X  X . Rather, we need to prove it. To do so, we need to build computational foundations and technologies that formally model the identifiabilitiy of data and ensure its pro-tection in a way that adheres to legal specifications. In this paper, we introduce a novel computational model of data protection designed to solve a privacy problem that manifests when an individual X  X  data is collected and disclosed by many different data collectors. 1.1. Protection perception versus reality
In the past, an individual X  X  identity was protected through the application of techniques that made data look anonymous, but provided little proof to back up such claims. For example, data holders often pro-tected the identity of subjects by partitioning sensitive and identifying information. A data holder could disclose a collection of sensitive data that was  X  X  X e-identified X  X  by the removal of explicit identifiers, such as the subject X  X  name, phone number, or Social Security Number. Similarly, the data holder could disclose a collection of identified data that was stripped of sensitive information. Though ad hoc, this practice was considered an acceptable form of privacy protection for sensitive data because the attributes of the disclosures were unrelated. For instance, imagine you were shown a DNA sequence and asked to determine the name of the individual to whom the DNA belonged. Without prior knowledge, you cannot do it. There is no central registry on the Internet that matches people with their DNA sequences. Thus, the DNA appears anonymous because there is no clear relationship between an individual X  X  name and their DNA.

However, in today X  X  data-centric society, an individual sheds similar, and often the same, piece of data to independently-operated organizations. When data holders follow na X   X  ve protection strategies, such as parti-
The trail can be observed in the shared databases of the sensitive, as well as the identified, data; and the uniqueness of an individual X  X  trail often re-identifies seemingly anonymous data to the name of the individual from whom it was derived. The phenomenon of trails manifests in a number of environments [22] , ranging from healthcare to the Internet to public surveillance. For instance, the trail of an individual X  X  DNA sequence individual X  X  identifying information could be extracted through separately released databases, such as hospital concern, where it was demonstrated that DNA could be uniquely matched to patient names in 33 X 100% of various patient populations. 1.2. Contributions of this research ductive, and often impossible, to simply prohibit all data disclosures. Data sharing is needed for a wide array of notable endeavors, ranging from basic scientific research to system quality assurance to public policy anal-sharing? X  X  Existing privacy protection models tend to focus on data disclosed from a single database or a set of databases that have attributes in common. However, the trails model of re-identification is an extension to the traditional privacy concerns regarding a single location X  X  releases [6,14,41] . As such, prior protection models tection models can provide false assurances.

As a solution to the trail re-identification problem, we introduce a novel model of privacy protection for data trails that we term k -unlinkability. In short, a piece of data, such as a DNA sequence, is k -unlinkable when its trail leads to no less than k identities via the observed trails. We frame this research from a graph-based perspective, which enables us to derive formal trail re-identification and protection models. The graph-based framework is an abstraction that allows us to compare k -unlinkability to alternative models of privacy protection.

The organization of this paper is as follows. We begin by reviewing related prior research on models of pri-vacy protection. Next, we introduce the k -unlinkability privacy protection model. Finally, we compare this model to pre-existing models, methods, and theories of privacy protection. We conclude with a discussion on the limitations and possible extensions to this research. 2. Related research and insights
A number of models to protect the identities of subjects in a disclosed database have been proposed in var-we present k-map [35] , k-anonymity [29,36] , and k-ambiguity [39] .

We borrow our notation from relational database and set theory. Let s ( A tuple, which we represent as t =[ a 1 , ... , a p ], such that a
Databases provide the syntax for information storage, but privacy is concerned with the semantics of the information itself. This research is concerned with the ability to relate de-identified information to the individual from whom it was derived, which raises the question,  X  X  X hat constitutes  X  X dentifying informa-tion X ? X  X  To answer this question, we paraphrase Sweeney [36] , who defines explicit identifying information as that which enables a data holder to (1) uniquely distinguish an individual in a population and (2) have a direct route of communication with the individual. Thus, in a database table, an explicit identifier is an attribute, or combination of attributes, for which data values permit direct communication with an individual. For instance, the combination of { First Name , Last Name , Residential Address } constitutes an explicit identifier because it enables targeted communication via the postal system with a specific individ-ual. Though explicit identifiers permit direct communication, the lack of such identifiers from a data collection does not mean that communication with an individual is impossible. Beyond the explicit iden-population. For instance, the combination of { Date of Birth , Gender , and 5-Digit Residential Zip Code } is a quasi-identifier for the majority of US citizens [15,34] . Unto themselves, quasi-identifiers do not permit direct communication with an individual; however, quasi-identifiers enable us to uniquely pinpoint an individual X  X  record in a de-identified collection. When the same quasi-identifier resides in de-identified and identified resources, then we can link de-identified records to enable communication with an individual. For instance, Sweeney used the aforementioned demographics as a quasi-identifier to re-iden-tify publicly available hospital discharge records to publicly available identified voter registration records [37] .

Now that we have defined tables and the notion of identifying information in such tables, we can begin to formalize the notion of privacy protection models. For a formal treatment of the topic, we model the conver-sion of a table to protect privacy as a function. Specifically, let f ( s )= s tion that manipulates data. For example, the function f could be defined as a process that suppresses all names.

Finally, to evaluate the formal properties of protection functions, we need to model the ground truth between information stored on entities and the entities from whom it was derived. This is also specified as a function. Formally, let a : s ! S be a function that maps tuples back to their underlying entities. Without loss of generality, let us assume that a has the power to distinguish between tuples of duplicate values. For example, imagine there is a database with two occurrences of the name  X  X  X ohn Doe X  X , each of which corre-sponds to a different entity. Given this information, a maps the first and second occurrence of  X  X  X ohn Doe X  X  to the first and second entity, respectively. 2.1. k-Map
The k-map protection model [35] provides guidelines regarding the relationships between observed features in a protected table and the population from which it was derived. It is a formal protection model in that exact guarantees of re-identification, or lack thereof, can be derived. Informally, k -map protection means that each record in a table refers to at least k entities in the population.
 isfied for s if there exists a s R S , such that " t 2 s : (1) j { s jh t , s i2 s R S } j P k , and (2) h t , a ( t ) i2 s R S .

Basically, k -map protection requires every tuple to be related to no less than k entities, one of which is the entity to whom the data was derived. The k -map protection model does not describe an algorithm that aug-ments the quasi-identifier for protection. It only specifies the conditions for which a dataset must adhere in order to satisfy the protection model.

In this paper, we propose k -unlinkability as an enforcement of k -map as applied to trails. 2.2. k-Anonymity
The k -map protection model specifies what must be true regarding the disclosed data and the population from which it was derived. However, we do not always know the information that is external to the shared dataset that can be linked to a quasi-identifier. As a consequence, a second formal protection model known as k-anonymity was introduced [29,36] . The k -anonymity protection model is a more restrictive case of k -map population external to the dataset.

Formally, let s D s be a relation, such that h t i , t j i2 decidable criterion. Table s is said to be protected if:
The k -anonymity model defines indistinguishability as strict equality . Two tuples t equal when Thus, a tuple is k -anonymous when the combination of quasi-identifying values occurs in k tuples in the table.
Akin to the k -map protection model, k -anonymity does not define a method for augmenting the quasi-iden-tifier for protection. Rather, k -anonymity defines when a database satisfies protection. 2.3. k-Ambiguity
Another model that has been proposed for privacy protection is k-ambiguity [39] . In a similar manner to k -anonymity, the k -ambiguity method specifies protection via indistinguishability. However, k -anonymity and k -ambiguity differ in their definition of the relation s ambiguity defines indistinguishability in the form of indiscernability [31] .
 text of cell suppression, a protected tuple t 0  X  a 0 1 ; ... ; a original table when where 6 implies a generalization of the value in a hierarchy of the domain for A tuple. In other words a x is a more specific value than a more general value 1521 * that corresponds to all zip codes in the set {15210,15211, ... ,15219}. Notice that in ambiguity model, it must be made less specific in the form of a generalized value.

It is important to recognize that k -ambiguity is not a generalized form of k -anonymity. Ambiguation is a method for data protection, but there are many alternative schemas by which k -anonymous data can be gen-erated. For instance, work presented by Domingo-Ferrer and Torra [11] and Newton et al. [27] propose aver-aging techniques to achieve k -anonymity. Thus, Eq. (3) is a fundamental deviation from the k -map and k -anonymity protection models. The latter specify architectures for protection, but not how the data must be protected.

In this research, we consider a specific case of k -ambiguity, known as ambiguity through suppression, such that indiscernability is defined as where * is a wildcard value that represents the union of all values in the domain of attribute A is indiscernible with at least k records in the original table, then it is said to be k -ambiguous. 2.4. Generalization and suppression
A popular method to achieve k -anonymity and k -ambiguity in practice is through generalization and sup-pression [28,37,39] . Suppression is the replacement of a value with a wildcard value, such as the
Fig. 1 provides an example of the difference in protected tables that can arise when k -anonymity and k -ambiguity protection models are satisfied through generalization and suppression. The original table s is shown to the left in Fig. 1 a. The middle and right tables depict protections that satisfy 2-anonymity and 2-ambiguity, respectively. In this example, the table produced by k -ambiguity in Fig. 1 c leaks inferences. Let us explore how this leakage occurs. First, recognize that the second tuple in Fig. 1 c, [33, uous with respect to the second and third tuple in the original table in Fig. 1 a. Similarly, recognize that the ginal table. However, it can be deduced that the second tuple in Fig. 1 a must correspond to the second tuple in sequently deduce that the third tuple in Fig. 1 a must correspond to the third tuple in Fig. 1 c.
In contrast, such exact inferences can not be made from the 2-anonymous table in Fig. 1 b. 3. Trail re-identification
To develop a protection model for trails, we must first understand how trails are re-identified. In prior research, we designed and evaluated a suite of trail re-identification algorithms [22,24] . However, we did not derive the general basis for the phenomenon. In this section we develop a graph-based framework to char-acterize how, and exactly when, trail re-identification will be achieved.
 3.1. Terminology and framework
We begin the formalization of trail re-identification with a general model for trails and linkage. We call the basic data structure a trail matrix ( Definition 1 ). This matrix summarizes information for data tracked over a patients, while the columns of a trail matrix can correspond to the set of hospitals that were visited by the patient population. Each value of a trail is selected from a set of two unambiguous values, 0 and 1, and an ambiguous value, * , which symbolizes either 0 or 1. The value 1 indicates an entity appeared at a location, whereas 0 indicates an entity failed to appear at the location, and appearance was made.

Definition 1 ( trail matrix/trail ). A trail matrix X is an m  X  p matrix with cell values drawn from the range {0,1, * }. A trail is a row x =[ a x ,1 , ... , a x , p ]in X , where a
Fig. 2 provides an example of trail matrices. Trail matrix X alerts us that data element Charlie was at hos-pital A and hospital D and was not at hospital B . It is unknown if Charlie visited hospital C .
We concentrate on the relationship between disparate trail matrices and study a specific type of scenario, the duplicate of the other trail matrix with missing, or ambiguous, values. Both trail matrices are defined over a common and closed population of people and locations. In the trail matrices, columns are aligned, such that each particular location is in the same column for both matrices. In Fig. 2 , for instance, hospital A is in the first column of both trail matrices X and Y . Rows, on the other hand, are not aligned, so the same person X  X  data can be represented by different rows of the two trail matrices. In Fig. 2 , an individual X  X  information, which is unambiguously represented by person 1 , is in the fourth row of trail matrix X as the name Ali and is in the third row of trail matrix Y as the DNA sequence actg .

Definition 2 (Association Relationship). We say there exists an association relationship between two trail matrices X and Y , if there exists a trail matrix W with range {0,1}, such that: 1. X is a copy of W , such that cell values may be replaced with 2. Y is a copy of W , such that cell values may be replaced with 3. row ordering in X and Y are permutations of row ordering in W .

Let f : W ! X and g : W ! Y be bijective functions that map row positions in W to their permuted positions in X and Y , respectively.

When there exists an association relationship, we can specify when two trails are derived from the same ancestor. Furthermore, when two trails are derived from the same ancestor, we say the two trails make up an association . In this sense, we do not necessarily imply the two trails are derived from the same trail, but from the same underlying concept. This distinction, while subtle, is necessary to discriminate between 3 . An example of an association relationship for trail matrices X and Y is shown in Fig. 2 . There is an asso-ciation between the name Ali and the DNA sequence actg .

Definition 3 ( Association ). Given an association relationship exists between trail matrices X and Y , we say there exists an association between row x 2 X and row y 2 Y when f
We know an association relationship exists, however, in the real world we are not provided with the func-tions f and g . The goal of trail linkage is to match those trails in X and Y that have the same ancestor ( Def-inition 4 ).

Definition 4 ( Trail Linkage Problem ). Given trail matrices X and Y with an association relationship, such that functions f and g are unknown, find the relation X R Y with the largest membership, such that ( x , y ) 2 and only if, f 1 ( x )= g 1 ( y ).

In our prior research [22 X 24] , we introduced a family of data mining algorithms that achieve trail linkage deterministically. In other words, we group as many associations together as possible without grouping any tification could occur and did not worry about maximizing the number of re-identifications. To develop for-mal privacy protections for every piece of data we must know when it can be re-identified. Therefore, in the following sections we show how optimal trail re-identification strategies can be derived using a graph-based perspective. 3.2. Graph-based model
How can we leverage trail matrices with an association relationship to discover associations? The answer to this question is best understood in the context of graph theory. For completeness, we review relevant graph theory definitions. Let G =( V X [ V Y , E ) be a bipartite graph, such that V matching in G is a subgraph H =( V X [ V Y , F E ), in which no two edges in F share an endpoint. A maximum matching is a matching with the largest number of edges. 2 In the stable marriage problem [16] , vertices in V X and V mity, we adopt these terms. Each man x 2 V X ranks the women according to his preference, so edge e directed from vertex x to y 2 V Y and weighted according to rank. Similarly, women rank the men. The goal is to find a matching, such that each edge connecting a man and woman, or marriage, is stable . A marriage is said to be stable if no man and woman would both prefer to be married to each other in comparison to their current state. More precisely, if x and y both rank each other higher than their current partners, then their marriage is said to be unstable . Otherwise, the matching is said to be stable.

There are many variants of the stable marriage problem. Of most relevance to the trail linkage problem, it is known that when the following constraints hold true: Incomplete Not every man/woman is an acceptable mate for a member of the opposite sex, Unweighted Every man/woman is indifferent between its list of acceptable mates,
Undirected Every acceptable mate reciprocates (i.e., man likes woman and woman likes man), then solving the stable marriage problem reduces to finding a maximum matching in G , i.e., the matching with the largest number of edges. There can simultaneously exist more than one maximum matching in a graph and, consequentially, more than one stable marriage is possible for a vertex. For instance, in the graph in
Fig. 3 a, both of the maximum matchings ( Figs. 3 b and 3c) depicted are stable marriages. The maximum matchings shown in Fig. 3 correspond to a special case called perfect matchings , which are characterized by the fact that every vertex in the graph participates in the matching.

We claim an association can be represented by a specific type of marriage that we call a true love . A true graph in Fig. 3 a.

Definition 5 ( True Love ). Let T XY  X  T 1 XY ; T 2 XY ; ... ; T graph G =( V X [ V Y , E ). Vertices x 2 V X and y 2 V Y make up a true love when edge e 3.3. Optimal re-identification
We prove our claim regarding true loves and associations in a step-wise manner. First, we summarize the link matrix communicates when two trails potentially correspond to the same unambiguous pattern of values. matrix L XY is an m  X  m matrix, such that:
Edges in a link matrix satisfy an indiscernability criteria different than k -anonymity and k -ambiguity, such that the quasi-identifier consists of the location specific attributes. By this formulation, the link matrix in Eq. (5) is tantamount to the bipartite graph G =( V X [ V rows of trail matrices X and Y , respectively, and the edgeset E ={ e
Next, we state an important observation regarding the translation of associations into the link matrix ( Lemma 1 ). Specifically, if two trails are an association, then they must be linked in the link matrix.

Lemma 1 (Associations are Links). Given that trail matrices X and Y have an association relationship, if x 2 X and y 2 Y are an association, then L XY [x,y] = 1.

Proof. By Definition 3 , when trails x and y are an association, f single unambiguous trail in common. As a result, trails x and y can be made equivalent by changing ambig-uous to unambiguous values only, which satisfies the definition of a link. h that every association is mutually exclusive, such that no two associations have an ancestral trail in common ( Corollary 1 ).

Lemma 2 (One Association Per Trail). Given trail matrices X and Y have an association relationship, every trail participates in one, and only one, association.
 Proof. This follows directly from the existence of bijective functions f and g in Definition 2 . h
Corollary 1 (Associations are Exclusive). Given trail matrices X and Y have an association relationship, all associations are mutually exclusive.

Finally, we have the necessary tools to prove true loves represent associations in trail matrices ( Theorem 1 ).
 Theorem 1 (True Loves Are Associations). Let G = (V X [ V such that there exists an association relationship between trail matrices X and Y. Every true love in G is an association for X and Y.

Proof. By Lemmas 1, 2 , and Corollary 1 it directly follows that there exists one, and only one, perfect match-ing that captures all associations between X and Y without capturing any non-associations. Therefore, if there exists a true love in the set of perfect matchings, it must correspond to an association. h
Theorem 1 allows us to state every true love implies an association. Unfortunately, we can not claim the reverse implication holds true. In other words, not every association corresponds to a true love. This begs the important question,  X  X  X an we construct a procedure to discover associations that are not true loves in
G without including non-associations? X  X  Interestingly, we can prove that this is impossible. This has impor-tant ramifications because it implies that the set of true loves in G is the largest set of associations any deterministic algorithm can discover using only trails ( Theorem 2 ). Informally, every edge not associated with a true love in the bipartite graph can represent a feasible alternative solution to the trail linkage problem.
 Theorem 2 (True Loves Are the Optimal Trail Solution). Let G = (V participates in an association.
 no edges connect to vertices in a true love. Let T AB represent the set of perfect matchings in graph H . Let Z be the union of perfect matchings associations, there exists a matching in H that captures every remaining association.
 It follows that for each vertex a 2 V A there exists a vertex b 2 V association. Similarly for V B . Moreover, since no vertex in H participates in a true love, every vertex must must exist at least two perfect matchings in H that have zero edges in common. One of the perfect matchings captures all associations without any non-associations. However, it is not possible to discern which perfect matching is the correct one. In other words, every edge in Z as an association. Therefore, if a vertex does not participate in a true love, no computational method can consistently correctly decide when it is in an association. h
True loves illustrate how unique re-identifications can be made; e.g., the correct and discriminative link-age of a piece of DNA to a named individual. Yet, in many instances, an adversary is willing to accept non-unique re-identification. For example, imagine that the adversary is an insurance company that wants to learn if any of their consumers have withheld information. The insurance company performs a trail re-iden-tification on all of the DNA databases that it can gather. Upon completion, the insurance agency finds that the trail of John Doe links to two DNA sequences  X  one of which contains a mutation for a debilitating disorder and the other one does not. Based on this information, there is a 50% chance that John Doe has the disorder. As such, the insurance company may rule that 50% is enough risk to revise John Doe X  X  premium.

Given this scenario, and others like it, we require a more general definition of trail re-identification. For-tunately, the graph-based framework is amenable to such a need. Beyond true loves, if a piece of data can be k -trail re-identified. Thus, we say all true loves are 1-trail re-identified.

The generalization to k -trail re-identification enables administrators to make informed policy decisions believes that the protected data is of minimal risk to the corresponding individual, then k can be set to a low
DNA-based disease), then they can choose to set k at a higher level. Choosing an appropriate k is a policy decision, and is beyond the scope of this paper, however, in other work, we show how different k affect the quantity of data an organization is able to disclose [23,25] . 4. k -Unlinkability
When considering generalization and suppression to satisfy models, such as k -anonymity and k -ambiguity, can define a formal privacy model that accounts for the relationships between the trail matrices and, by tran-sitivity, for the underlying population. We base our model of data privacy on the notion of unlinkability .
Informally, we define unlinkability as the degree to which data corresponding to the same entity can not be discriminately related.
 These are trails that correspond to data that is never disclosed by any location.

Definition 7 ( Null Trail ). A trail is said to be a null trail if it consists only of Next, we define k -unlinkability ( Definition 8 ) from the perspective of perfect matchings in bipartite graphs. Definition 8 ( k -Unlinkability ). Let X and Y be trail matrices with an association relationship, and let L the corresponding link matrix. Let G =( V X [ V Y , E ) be the bipartite graph for link matrix L the union of perfect matchings in G . We say G satisfies k -unlinkability when each vertex v in G either: 1. participates in at least k different edges in T XY ,or 2. all edges in T XY that end at v are connected to a null trail.

By Definition 8 , there are two ways an element can satisfy k -unlinkabilty. The first way occurs when the ele-ment is matched to at least k different elements in the set of perfect matches. For example, consider the trail matrices X and Y in Fig. 4 . These matrices have an association relationship, as demonstrated with trail matrix
W . Based on the trail matrices in Fig. 4 we derive a link matrix with the corresponding bipartite graph shown in Fig. 5 a. The perfect matchings of this graph are shown in Fig. 5 b X  X . Each of these graphs is a feasible un-ique trail linkage solution. The union of these solutions corresponds to the original graph, and the vertices corresponding to Alice , Bob , actg , and ctga each have a degree of two or greater, which implies the corre-sponding trails satisfy 2-unlinkability.
 Elements are not always disclosed, so there is a second way in which an element can satisfy k -unlinkability.
The second way occurs when the element participates in edges in the perfect matchings, such that every edge is fication is never completed because every element corresponds to a null trail. In other words, we know that a re-identification could be made if the elements were disclosed, but we cannot complete the re-identification due to the fact that such elements are never shared by any location. Again, consider the example in Figs. 4 and 5 .
We know that an element exists for  X  X ???? X  X , but we do not know the real world values for this element; thus the question marks. Though, Charlie and ???? both participate in a single edge, it is incident with a null trail, so
Charlie and his mystery DNA data satisfy k -unlinkability by the second criteria. 5. Comparison of k -unlinkability and related models
In this section, we address several aspects of the k -unlinkability model in more depth. Specifically, we inves-tigate its relationship to (1) k -anonymity and k -ambiguity as achieved through generalization and suppression and (2) alternative models for unlinkability quantification. 5.1. k-Unlinkability versus k-anonymity If we use the strict equality criterion in Eq. (2) with respect to X (or Y ), then the link matrix L unlinkability. In other words, if a trail matrix is k -anonymous, as achieved through cell suppression, then any link matrix constructed from the trail matrix is k -unlinkable. For example, consider Fig. 6 . This link matrix can be derived from the link matrix.

However, k -anonymity is unnecessary to satisfy k -unlinkability. As an example, consider the trail matrices in Fig. 7 . There are 12 distinct perfect matchings that can be derived from the link matrix. Given the perfect matchings, it can be validated that the link matrix is 3-unlinkable. This is shown in matrix U resents the union of perfect matchings, where every rowsum and columnsum equals 3. Yet, in contrast to Fig. 6 , neither trail matrix is 3-anonymous.

The primary difference between k -anonymity, from a generalization and suppression perspective, and k -unlinkability is that the latter does not require equivalence of trails to satisfy protection. With respect to the bipartite graph representation, k -anonymity protection implies that every connected component of the bipartite graph is complete, such that every vertex of one data type is connected to every vertex of the other data type in the component. In contrast, k -unlinkability protection does not imply how many components a map formal protection model and preserve more variation in trails in comparison to k -anonymity. 5.2. k-Unlinkability versus k-ambiguity
In the previous subsection, it was shown there exists an explicit relationship between k -anonymity and k -unlinkability. The relationship between k -ambiguity and k -unlinkability is more subtle. Specifically, we show an important non-implication regarding the relationship between k -ambiguity and k -unlinkability. Consider, for a pair of trail matrices. Though one trail matrix may contain ambiguous values, it can be linked to a data-base that does not satisfy k -ambiguity. This theorem formalizes one of the limitations of the k -ambiguity model.

Theorem 3 ( k -Ambiguity does not imply k -unlinkability). Given trail matrices X and Y have an association X.

Proof. We prove this theorem by contradiction. In Fig. 8 , we show two trail matrices and are provided with k responds to the original and unambiguous trail matrix. However, upon computation of the union of perfect matchings we observe that the first and fourth trail in Y can be uniquely re-identified to a trail in X , which violates 2-unlinkility. h 5.3. k-Unlinkability versus entropy unlinkability
This research is not the first to quantify the concept of unlinkability. Steinbrecher and Ko  X  psell [32] inte-grated various definitions from communications anonymity [7,8,30] and introduced a formal model of unlink-ability based upon information theory. To paraphrase Steinbrecher and Ko  X  psell, let X and Y be two sets of concept. Their model uses a probabilistic framework, such that for a recipient of the data, P (( x , y ) 2 ent X  X  posterior probability of correctly claiming two arbitrary elements are unrelated is P (( x , y ) 6 2 fundamental assumption of this model is
Based on this assumption, the degree of unlinkability between two elements x and y is quantified as the entropy complete confidence in whether or not x and y correspond to the  X  X  X ame concept X  X , whereas an ent -unlinkability of 1 means the attacker is equally confident in stating x and y correspond to the  X  X  X ame concept X  X  and  X  X  X ot the same concept X  X .
 The k -unlinkability model can be translated into an ent -unlinkability setting. Consider the scenario in Fig. 8 . Let E y be the set of elements in Y for which P (( x , y ) 2 and Y are one-to-one, the recipient knows that for an arbitrary trail y there is only one trail x for which it probability of correctly and discriminately relating y and x is lated is As a consequence, the ent -unlinkability of two arbitrary trails is calculated as Though k -unlinkability can be rewritten in ent -unlinkability form, there is a disparity between the models.
Since X and Y are one-to-one, it can be derived that the maximum ent -unlinkability score of 1 is only achieved when P (( x , y ) 2 X R Y ) = 1/2. However, as k grows larger, the unlinkability of two elements should become more pronounced. We depict this paradox in Fig. 9 . The reason this paradox exists is that ent -unlinkability places equal dependence on P (( x , y ) 2 X R Y ) and P (( x , y ) 6 2 ity P (( x , y ) 2 X R Y ) never dominates, such that P (( x , y ) 6 2
However, for k -unlinkability it does not matter if a recipient can unequivocally discern that two elements are unrelated. We only want to ensure that the recipient can not discriminately determine when two elements are related. So, we use 1 1/ k to represent the degree of unlinkability between two elements. Therefore, k -and ent-unlinkability scores are monotonically inversely proportional as shown in Fig. 10 . 6. Discussion
One of the more significant benefits of the k -unlinkability model of privacy protection is that it provides exact guarantees of privacy for every disclosed tuple of data in the context of trails. Yet, k -unlinkability is a general model of privacy and this paper does not define any particular algorithm for transforming trails into nymity and k -ambiguity, we believe that it may be possible to adapt generalization and suppression algorithms that have been proposed in existing literature. Nonetheless, there remain challenges to overcome in the devel-opment of efficient strategies for the alternative protection models. 6.1. Alternative graph-based models
Graph-based frameworks beyond k -unlinkability have been proposed to investigate privacy problems in data disclosures. Though certain models use similar graph-based approaches, such as bipartite analysis, they impose different assumptions on the disclosure environment, the type of information available to attackers, and address different privacy concerns in general. Despite their differences, it is possible that theories and methods developed for related graph-based privacy models may be adapted and applied in our setting. Here, we compare and contrast graph-based models with the k -unlinkability framework.

Beresford and Stajano [4] applied bipartite graphs to link vertices of pseudonyms in location-based systems to vertices of their owners. They constructed a graph of weighted edges and proposed an approximation algo-rithm to maximize the weights for the set of pseudonyms-owner matchings. This model adopts a probabilistic approach, such that certain matchings are more likely than others. However, this model does not account for the scenario where both set of vertices are manifestations, or pseudonyms per se, of the underlying owner, the unambiguous trail. Recall, in the trail re-identification problem either set of vertices can consist of missing information, in the form of the ambiguous * value. To apply Beresford and Stajano X  X  model for trails, their bipartite graph would need to be extended to a tripartite graph to relate both types of data, identified and de-identified, to the originator. When trail matching moves into a probabilistic realm, efficient graph matching algorithms such as those proposed by Beresford and Stajano may provide a basis for privacy protection evaluation.

In Lakshmanan et al. [19] , a bipartite graph model is used by an attacker to link vertices of protected data function to relate transformed tuples to beliefs of the original tuples X  values. This work also proposes a heu-ristic-based algorithm to estimate the number of associations that can be made given a bipartite graph that can be executed in O( n log n ) time. In practice, the estimation method achieves good performance, and is within 10% of the actual number of associations. However, this method uses a probabilistic basis, such that if there are x DNA samples and there is a uniform probability of p of 1-trail re-identification for each, then the esti-mate will claim there are np re-identifications. Yet, while the attacker may have achieved np correct re-iden-tifications, he will not necessarily know which are the correct cases.

Nonetheless, their work goes beyond ours in the context of the adversarial model. In this paper, we assumed that an attacker is na X   X  ve and does not have additional knowledge beyond the observed trails. Yet, in the real world, it is possible that an adversary has additional knowledge, possibly in the form of its own database (e.g., a hospital that did not disclose its DNA database). The model used in [19] is generalized, such that it can incorporate private knowledge and it is possible that such generalizations can be applied to our bipartite graph representation of the trail re-identification problem. 6.2. Finding re-identifications efficiently
The k -unlinkability model is based on the enumeration of perfect matchings. However, based on prior and current research in graph theory, the enumeration of perfect matchings is a non-trivial feat. The most efficient technique to find a single perfect matching is the alternating paths algorithm [17] , which has worst case com-plexity O( j E j  X  j V X [ V Y j 1/2 ). With respect to trail matrices, since j V matching discovery is O( n 5/2 ). Moreover, the fastest known algorithm to enumerate all perfect matchings has complexity O( j E j  X  j V X [ V Y j 1/2 + l G  X  log j V [38] . The form of this complexity statement is similar to that of finding a single perfect matching. However, there are potentially on the order of n ! perfect matchings in the graph, so enumeration is O( n This latter term dominates for n P 3, so enumeration requires O( n !log n ).

While k -unlinkability may be difficult to detect in the general case, there is research that shows true loves all edges in a bipartite graph that must be in all perfect matchings. This algorithm has O( nm ) runtime com-plexity, where n is the number of vertices and m is the number of edges in the bipartite graph. Though this algorithm does not discover all true loves, it does guarantee that all discovered true loves are correct.
In real world data distributions, we have observed that trail matrices tend to be sparse, which may open the door for efficient approximation and graph partitioning algorithms. 6.3. Building k-unlinkable databases
Though k -unlinkability specifies a model of privacy protection, for the model to be useful in practice, we in theory, discovering the function that minimizes the number of suppressions to satisfy k -anonymity, or k -ambiguity, appears to require exhaustive search strategies [3,20,28,37] . This comment is made in light of the problem X  X  computational hardness. First, Meyerson and Williams [26] proved achieving k -anonymity by minimizing the number of attributes suppressed is an NP-hard problem. Second, Aggarwal et al. [1,2] proved minimizing the number of cells to suppress, regardless of attribute, to satisfy k -anonymity is an
NP-hard problem as well. Third, Vinterbo [39] proved minimizing the number of cell suppressions to satisfy k -ambiguity is an NP-hard problem.

However, in real world populations, quasi-identifying values are non-uniformly distributed. evidence suggests heuristics, such as those in [18,33,40,42] , which use genetic and simulated annealing algo-rithms, may work well in practice [20,40] . Along this line, we have initiated the development and evaluation of efficient heuristic-based algorithms that generate k -unlinkable trails with promising results [23,25] . 7. Conclusions
This paper proposed a formal protection model to address a real world privacy problem in distributed envi-ronments. Specifically, this work studied how seemingly anonymous data (e.g., a DNA sequence devoid of identifiers) is linked back to named entities by common patterns in the set of locations visits, or  X  X  X rails X  X .
To prevent this privacy problem, we introduced a computational model called k -unlinkability, which enables administrators to determine when an individual X  X  data is linkable to at least k identities given the observed trails. We proved that k -unlinkability provides privacy protection that is more appropriate to this problem than probabilistic and information theoretic solutions. We further demonstrated that alternative computa-tional protection models, such as k -ambiguity, can leak inferences in this environment, which can compromise privacy. This research presented the formal properties of k -unlinkability, but does not address how to render k -unlinkable data. We have initiated the design and evaluation of algorithms to transform data to satisfy this model and in future research we intend to build upon probabilistic frameworks to extend and improve the efficiency of our solutions.
 Acknowledgements The author greatly acknowledges the support, discussions, and insightful suggestions provided by Latanya
Sweeney. The author is also thankful for discussions with Edoardo Airoldi, Michael Benisch, Alastair Beres-ford, Kathleen Carley, Christopher Clifton, Christos Faloutsos, Ralph Gross, and Yiheng Li as well as the helpful comments provided by the anonymous referees. This research was funded in part by NSF IGERT grant 9972762 in CASOS and the Data Privacy Laboratory at Carnegie Mellon University.

References
