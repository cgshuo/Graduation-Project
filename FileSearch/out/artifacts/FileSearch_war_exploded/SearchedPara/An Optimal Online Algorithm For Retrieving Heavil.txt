 We give the first  X  O ( 1  X  structing noisy statistical databases, where T is the num-ber of (online) sample queries received. The algorithm is optimal up to the poly (log( T )) factor in terms of the er-ror and requires only O (log T ) memory. It aims to learn a hidden database-vector w  X   X  R D in order to accurately answer a stream of queries regarding the hidden database, which arrive in an online fashion from some unknown distri-bution D . We assume the distribution D is defined on the neighborhood of a low-dimensional manifold. The presented algorithm runs in O ( dD )-time per query, where d is the di-mensionality of the query-space. Contrary to the classical setting, there is no separate training set that is used by the algorithm to learn the database  X  the stream on which the algorithm will be evaluated must also be used to learn the database-vector. The algorithm only has access to a binary oracle O that answers whether a particular linear function of the database-vector plus random noise is larger than a threshold, which is specified by the algorithm. We note that we allow for a significant O ( D ) amount of noise to be added while other works focused on the low noise o ( For a stream of T queries our algorithm achieves an aver-age error  X  O ( 1  X  threshold values given to the oracle based on its previous answers and, as a consequence, recovering with high preci-sion a projection of a database-vector w  X  onto the manifold defining the query-space. Our algorithm may be also applied in the adversarial machine learning context to compromise machine learning engines by heavily exploiting the vulnera-bilities of the systems that output only binary signal and in the presence of significant noise.
 c  X  H.2 [ Database management ]: Security, integrity, and pro-tection; H.3 [ Information storage and retrieval ]: Infor-mation Search and Retrieval X  Retrieval models ; I.1 [ Symbolic and algebraic manipulation ]: Analysis of algorithms Algorithms, Theory, Security statistical databases, low-dimensional querying model, bi-section method, database privacy, online retrieval algorithms, adversarial machine learning
Protecting databases that contain sensitive information has become increasingly important due to its crucial prac-tical applications, such as the disclosure of sensitive health data. Privacy preservation plays a key role in this setting since such data is often published in anonymized form so it can be used by analysts and researchers. Several mecha-nisms have been proposed, such as differential privacy, that allow for learning from a database while preserving privacy guarantees ([1, 2, 3, 4, 5, 6]). At the other extreme are many results showing how database privacy can be compromised by an adversary who is able to collect perturbed answers to a large number of queries regarding the database ([7, 8, 9, 10, 6]). Existing results related to breaking the privacy of a database have several key limitations. For example, most as-sume that each query is represented by a vector q of D inde-pendent entries taken from some fixed distribution (such as the Gaussian distribution or a specific discrete distribution), and that this structure is known to the privacy-breaking al-gorithm. Also, most methods learn an approximation of the unknown database-vector w  X  that has L 2 error D for some small constant &gt; 0. Such precision is not sufficient to ob-tain o (1)-error on the stream of T queries for T D , as is the case in our model. Further, the focus has typically been on the offline setting, where the adversary first collects all the queries, then applies some privacy-breaking algorithm, and finally uses the reconstructed database-vector to com-pute good approximations of the statistics he needs. From the machine learning point of view this means that the over-all protocol for the adversary consists of two distinct phases: a training phase and a testing phase. Finally, the memory resources used by privacy-breaking algorithms are typically not analyzed, even though this is a crucial issue for the set-ting considered here, where the number of all the queries q coming in the stream may be huge.

The goal of this paper is to present and analyze a database privacy-breaking algorithm for a more realistic setting in which the limitations described above are lifted. The en-tries of the query-vector are not necessarily independent. The distribution D of the query-vector is not known to the adversary. The adversary is not able to first learn the database-vector before being evaluated. Our algorithm uses only O (log( T ))-size memory to process the entire stream of T queries and therefore is well-suited to the limited resources scenario. To make life of the adversary even more difficult, we assume that the database mechanism provides only a bi-nary oracle O that answers whether the perturbed value of a dot-product between the database-vector w  X  and the query-vector q is greater than a threshold that is specified by the adversary. Thus the algorithm has very limited access to the database even in the noiseless scenario. Dot-products between query-vector and a database-vector are considered in most of the settings analyzing database privacy-breaking algorithms. Considering this more challenging setting, we will show that much less than the noisy answer is needed to carry out an effective attack and compromise data privacy.
In some of the mentioned papers ([5, 6]) an effort is made to learn a good approximation of the database vector with a small number of queries that is only linear in the size of the database D . We use many more queries but our task is more challenging -we need much more accurate approximation, and get the information only about the sign of the perturbed product as opposed to the perturbed product itself. Finally, we are penalized whenever we are making a mistake. Our goal is to minimize the average error of the algorithm over a long sequence of queries so we need to learn this more accurate approximation very fast.

In this paper we present the first online algorithm that an adversary can use to reconstruct a noisy statistical database protected by a binary oracle O that achieves average error  X  rithmic memory. The algorithm is optimal, i.e. we show that any other algorithm (not necessarily online and with arbitrary running time) solving this problem achieves er-ror at least  X  O ( 1  X  on we will call this algorithm a learning algorithm . The learning algorithm is given a set of queries taken from some unknown distribution D defined on a neighborhood of the low-dimensional manifold that it needs to answer in the or-der that they arrive (note that the entries of a fixed query do not have to be independent). The learning algorithm can use the information learned from previously collected queries but cannot wait for other queries to learn a more ac-curate answer. Every received query can be used only once to communicate with a database. The database mechanism calculates a perturbed answer to the query and passes the result to the binary oracle O . The binary oracle uses the threshold provided by the adversary and passes a  X  X es/No X -answer to him. The error made for a single query is de-fined as: | z t  X  w  X   X  q t | , where q t and z t are the query and answer, respectively, provided by the learning algorithm in round t . As a byproduct of our methods, we recover with high precision the projection of the database-vector w  X  onto the query-space. Our approximation is within  X  O ( 1  X  distance from the exact projection. By comparison, most of the previous papers focused on approximating/recovering all but at most a constant fraction D of all the entries of w  X  which is unacceptably inaccurate in our learning setting where T D . The assumption that queries are taken from a low-dimensional manifold is in perfect agreement with re-cent development in machine learning (see: [11], [12], [13]). It leads to the conclusion that, as stated in [11]:  X  X  lot of data which superficially lie in a very high-dimensional space R
D actually have low intristic dimensionality , in the sense of lying close to a manifold of dimension d D  X . Assume that the queries are taken from a truly high-dimensional space. Then as long as the number of all queries is polynomial in D , the average distances between them are substantial. In this scenario any nontrivial noisy setting prevents the adversary from learning anything about the database since a single perturbed answer does not give much information and the probability that a close enough query will be asked in the fu-ture is negligible in D . In practice we observe however that noise can be very often filtered out and a significant number of queries can give nontrivial information about a database-vector w  X  . In this paper we explain this phenomenon from the theoretical point of view. Our algorithm accurately re-constructs the part of the database that regards the lower-dimensional space used for querying. We show that this suf-fices to achieve average  X  O ( 1  X  queries. In our model, the number of queries significantly exceeds the dimensionality of the database, and therefore we focus on optimizing our algorithm X  X  time complexity and accuracy as a function of T . Having said that, in most of the formulas derived in the paper we will also explicitly give the dependence on other parameters of the model such as the dimensionality of the database D and the dimensional-ity of the query-space d . We are mainly interested in the setting: d D T . If we use the O -notation, where the dependency is not explicitly given then we treat all missing parameters as constants.

It should be also emphasized that, contrary to most pre-vious work on reconstructing databases based on the per-turbed statistics, the proposed algorithm does not use linear programming and thus gives better theoretical guarantees regarding running time than most existing methods. The algorithm uses a subroutine whose goal is to solve a linear program, however we show this program has a closed-form solution. Therefore we do not need to use any techniques such as simplex or the ellipsoid method. The algorithm is very fast: it needs only O ( dD )-time per query. More de-tailed analysis of the running time of the algorithm as well as memory usage will be given later.
One important related setting for our model as well as the algorithm we are going to present comes from adversar-ial machine learning. In many real world machine learning applications an adversary can use data in order to reveal information about the machine learning classifier and con-sequently, use this information to trick the classifier. This happens for instance in such domains as fraud, malware and spam detection, biometric recognition, auction alloca-tion/pricing and many more. Thus, the need arises to es-tablish secure learning in the adversarial setting.
Some introduced methods include repeated manual recon-structions of the classifier or randomizing the classifier. For example, an adversary-robust classifier in proposed in [14], where the adversarial learning problem is formally defined. In [15], a randomized approach is analyzed as a tool do de-fend against the adversary. The paper in fact precisely char-acterizes all optimal randomization schemes in the presence of both classifier manipulations and adversarial inverse en-gineering. On the other hand, many practical methods to violate machine learning system security have also been in-troduced, for example using evasion attacks. In the spam detection framework these may involve, for instance, obfus-cating spam emails X  content. In general, malicious samples are modified and used to exploit the vulnerabilities of the system. Other techniques take advantage of the frequent retraining phases, where data can be possibly poisoned. In that setting the goal is usually to compromise the process of learning. Most of these aforementioned scenarios fit nat-urally in the online framework.

The random noise in these settings setup may come from many different sources (noise introduced to protect the ma-chine learning framework, as it is in [15] or noisy channels as is the case for various geophysical signals such as sonar signals that are subject to noise depending on the ocean ge-ometry and weather). The role of noise in machine learning setting was extensively studied (see: [16]) and must be taken into account in many real world settings.

We can think of the database-vector w  X  in our model as a linear machine learning classifier. The dot-product given to the oracle has a natural interpretation since it says on which side of the hyperplane the given query is, and conse-quently determines its classification. The dot-product out-put has been extensively used in the statistical database setting, however (contrary to many other approaches), the techniques used in our algorithm can be used for various types of outputs, not necessarily linear in the query q (giv-ing rise to applications for nonlinear classifiers). This is due to the generalization of the bisection idea in the presence of noise, that we fully explore in this paper and which consti-tutes the core of the proposed algorithm, is independent of the particular form of the output. The perturbation added before this dot-product is given to the oracle may be in-terpreted as a noise introduced to challenge the adversary or an effect of the noisy channel, which was also the case in the database setting described before. The oracle is like the machine learning classification mechanism that outputs the classification of the given query (for instance: spam/no-spam). The answer given by the oracle in our model is bi-nary, however, straightforward extensions of the approach proposed by us lead to models, where the oracle chooses an answer from the larger discrete set.

Finally, let us comment on the interpretation of our algo-rithm X  X  error in the adversarial classification setting. Note, the goal of our algorithm is to retrieve the vector w  X  , which is a stronger goal than what is usually considered in the usual adversarial classification setting, i.e. forcing false classifica-tions. The exact value of the classifier dot-product measures how far a given query is from the boundary of the binary classification, i.e. how confident the system is that a given query should be classified in a particular way. This informa-tion is not given explicitly by the engine but is nevertheless of great importance. It is reasonable to assume that the adversary does not have unlimited resources and that is-suing queries has a cost. Thus, the goal of the adversary is to minimize the average per-round error during the sequence of queries, which is equivalent to compromising the classifier (within some additive error) with as few queries as possible. We will now describe in detail our database access model. We assume that the database can be encoded by the database-vector w  X   X  R D . For definiteness we will consider: w [0 , 1] for i = 1 ,...,D . Our method can be however used in the much more general setting, as long as w  X  is taken from some fixed ball in L  X  . Each query can be repre-sented as a vector q = ( q 1 ,...,q D ), where: 0  X  q i  X  1 and q +  X  X  X  + q 2 D &gt; 0. Queries are taken independently at ran-dom from the unknown distribution D (notice that entries of a fixed query do not have to be independent). The distri-bution D is defined on some d -dimensional linear subspace U  X  R d ( d &lt; D ). The exact answer to the query is given as a = P D i =1 w  X  i q i . For the t th coming query q t algorithm L selects the threshold value  X  t and passes q t the database mechanism M which computes a t = w  X   X  q t . The noisy version  X  a t of a t as well as  X  t is passed by M and L to the binary oracle O :
The value O (  X  a t , X  t ) is then given to L . The learner records this value and can also use the information obtained from previously received queries to give an answer z t to the query q . However it has only O (log( T ))-memory available. Fur-ther, for a fixed query the learner only has one-time access to the binary oracle O .

The noise t =  X  a t  X  a t is generated independently at ran-dom and is of the form D E , where E is some known distribu-tion producing values from some bounded range [  X  u,u ]. The boundedness assumption is not crucial. Technically speak-ing, as long as the random variable is not heavy-tailed (which is a standard assumption), our approach works. In fact even this condition is unnecessarily strong. This will become ob-vious later when we describe and analyze our method.
This setting covers standard scenarios where computing every single product in the sum of d terms for w  X   X  q t gives an independent bounded error. We should note here that in most of the previous papers the magnitude of the noise added was of the order o ( stance, in [8] the authors reconstruct a database that agrees with the groundtruth one on all but (2 c X  ) 2 entries, where  X  is a noise magnitude and c &gt; 0 is a constant. Thus, even though previous works do not assume that noise was added independently for every query, the average error per single product in the dot-product sum was only of the mag-nitude o ( 1  X  range of possible applications. This is no longer the case in our setting, where some mild and reasonable assumptions regarding independence of noise added to different queries and low-dimensionality of querying space leads to a model much more robust to noise. We will assume that t do not have singularities, i.e. P ( t = c ) = 0 for any fixed c .
We need a few more definitions.
Definition 2.1. We say that a vector w computed by the learning algorithm -approximates database-vector w  X  if |  X 
U ( w )  X   X  U ( w  X  ) |  X   X  , where  X  U ( v ) stands for the projec-tion of v onto d -dimensional querying space U .

Definition 2.2. Let Q be a probability distribution on the unit sphere S (0 , 1) in L 2 . For a fixed vector q  X  X  (0 , 1) we denote by p Q q, X  the probability that a vector x selected ac-cording to Q satisfies: q  X  x  X  cos(  X  ) .

Definition 2.3. Take a distribution D from which queries are taken. Assume that D is defined on the d -dimensional space U with orthonormal basis B . Denote by D n the nor-malized version of D and by B n the normalized version of B (all vectors rescaled to length 1 in the L 2 -norm). Then we
The error q the algorithm is making on each query q is defined as the absolute value of the difference between the exact answer to the query and the answer that is provided by the algorithm. The average error on the set of queries: q ,...,q T is defined as av = 1 T P T i =1 q i . Let us state now main result of this paper.

Theorem 2.1. Let q 1 ,...,q T be a stream of query-vectors coming in an online fashion from some d -dimensional sub-space, where: 0  X  q t i  X  1 for i = 1 ,...,d and each q a nonzero vector. Then there exists an algorithm A lg using O (log( T )) -memory, acting according to the protocol defined above, and achieving average error:
We will give this algorithm, called OnlineBisection algo-rithm, in the next section. Notice that  X  is well approxi-mated by 1 worst-case scenario it suffices to analyze the setting where q is chosen uniformly at random from the query-space U .
If this is the case then one can notice that p D , X  is of the exists a basis of U such that most of the mass of D is con-centrated around vectors from the basis then standard anal-bound on r (where poly ( d ) is a polynomial function of d ).
Theorem 2.1 implies a corollary regarding the batch ver-sion of the algorithm, where test and training set are clearly separated (the proof of that corollary will be given later):
Corollary 2.1. Let w T denote the final hypothesis con-structed by the OnlineBisection algorithm after consuming T queries drawn from an unknown distribution D . Then the following inequality holds with probability at least 1  X  D :
In the subsequent sections we will prove Theorem 2.1 and conduct further analysis of the algorithm. Unless stated otherwise, log denotes the natural logarithm.
We will now present an algorithm (Algorithm 1) that achieves theoretical guarantees from Theorem 2.1. Our algo-rithm, called OnlineBisection , maintains a tuple of intervals ( I 1 ,..., I d ) which encode a hypercube that contains the database-vector w  X  (projected onto U ) with very high prob-ability.

For each coming query-vector q t the algorithm outputs an answer w approx  X  q t , where w approx is an arbitrarily se-lected vector in the current hypercube. The query-vectors received by the algorithm are used to progressively shrink the hypercube.
 Algorithm 1 -OnlineBisection
Input: Stream q 1 ,...,q T of T queries, database mechanism M and binary oracle O .

Output: A sequence of answers ( w 1  X  q 1 ,...,w T  X  q t ), returned online. begin end
As the hypercube shrinks, vector w approx -approximates w  X  for smaller values of . When the hypercube is large the errors made by the algorithm will be large, but on the other hand larger hypercubes are easier to shrink since they require fewer queries to ensure that hypercube continues to contain w  X  (with very high probability) after shrinking. This observation plays a crucial role in establishing upper bounds on the average error made by the algorithm on the sequence of T queries.

After outputting an answer for query-vector q t , the algo-rithm checks whether q t has a large inner product with at least one vector in an orthonormal basis C = { e 1 ,...,e U . If so, q t represents an observation for that basis vector; whether it is a positive or negative observation depends on the response of the binary oracle O . The threshold given by the algorithm to O is chosen by solving the linear pro-gram max y  X  X C q  X  y for q = q t and q =  X  q t , where HC is the current hypercube. As we will see in Section 5, this linear program is simple enough that there is a closed-form expression for its optimal value. So we do not need to use the simplex method or any other linear programming tools. Algorithm 2 -ShrinkHyperCube
Input: I 1 = [ x 1 ,y 1 ] ,..., I d = [ x d ,y d ], N + 1
Output: Updated hypercube ( I 1 ,..., I d ). begin end
The optimal values m and M of the linear programs solved by the OnlineBisection algorithm represent the smallest and largest possible value of the inner product of the query-vector and a vector from the current hypercube. The true value lies in the interval [ m,M ]. By choosing the average of these two values as a threshold for the oracle we are able to effectively shrink direction i  X  . The intuition is that if the query-vector forms an angle  X  = 0 with this direction and there is no noise added then by choosing the average we ba-sically perform standard binary search for q . Since  X  is not necessarily 0 but is relatively small (and noise is added that perturbs the output), the search is not exactly binary. In-stead of two disjoint subintervals of I i  X  we get two intervals whose union is I i  X  but that intersect. Still, each of them is only of a fraction of the length of I i  X  and that still enables us to significantly shrink each dimension whenever a suffi-cient number of observations have been collected for each basis vector  X  specifically, N crit observations  X  by calling the ShrinkHyperCube subroutine (Algorithm 2).

Every shrinking of the hypercube decreases each edge by a factor  X  for some 0 &lt;  X  &lt; 1. A logarithmic number of shrinkings is needed to ensure that any choice of w approx the hypercube will give an error of the order  X  O ( 1  X  that N crit grows with T , which reflects the fact that for smaller hypercubes more observations are needed to further shrink the hypercube while preserving the property that it contains the database-vector w  X  with very high probability. This is the case since if the hypercube is small we already know a good approximation of the database vector so it is harder to find even more accurate one under the same level of noise. When the hypercube is small enough (condition: |I anymore since each vector taken from the hypercube is a precise enough estimate of the database vector.

Note that choosing an orthonormal basis C = { e 1 ,...,e d of U does not require the knowledge of the distribution D from which queries are taken. We only assume that queries are from a low-dimensional linear subspace U of d dimen-sions. It suffices to have as { e 1 ,...,e d } some orthonormal basis of that linear subspace. There are many state-of-the-art mechanisms (such as PCA) that are able to extract such a basis, and thus we will not focus on that, but instead assume that such an orthonormal system is already given. Notice that in practice those techniques should be applied before our algorithm can be run. Since such a preprocess-ing phase requires sampling from D but does not require an access to the database system, we can think about it as a preliminary period, where evaluation is not being conducted.
In this section we prove Theorem 2.1. We start by in-troducing several technical lemmas. First we will prove all of them and then we will show how those lemmas can be combined to obtain our main result.

We denote: h T = for shrinking the hypercube is of the form: |I i |  X  1  X  i = 1 ,...,d .

We start with the concentration result regarding binomial random variables.

Lemma 4.1. Let Z m = Bin ( m,p 1 ) , W m = Bin ( m,p 1 +  X  p ) and  X  1 = mp 1 .
 Then the following is true: Proof. The proof follows from standard concentration in-equalities. Let  X  1 , X  2 &gt; 0. Note that E ( Z m )  X  mp E ( W m )  X  mp 1 + m  X  p . Denote  X  2 = E ( W m ). Note that by Chernoff X  X  inequality we have: Similarly,  X  and  X  2 , we obtain: Similarly, P ( W m &lt;  X  1 + m  X  p
Notice that  X  1 , X  2  X   X  p 2 (the latter inequality holds be-cause obviously:  X  2  X  m ). Thus we get: and Since  X  p  X  1, the proof is completed. Definition 4.1. Let HC be a d -dimensional hypercube in R
D . We denote by l ( HC ) the length of its side measured percube have the same length).

Next lemma is central for finding an upper bound on the average error made by the algorithm.
 Lemma 4.2. Let ( q 1 ,...,q T ) be a sequence of T queries. Let HC 0 ,..., HC s be a sequence of d -dimensional hypercubes in
R D . Assume that l ( HC i +1 )  X   X l ( HC i ) for i = 0 ,...,s  X  1 and some 0 &lt;  X  &lt; 1 . Denote l ( HC 0 ) = L  X  D and assume that s = 1 log function of T . Assume that w  X   X  X C 0 T  X  X  X  T HC s . Let E be a random variable defined on the interval [  X  u,u ] for some constant u &gt; 0 , with density  X  continuous at 0 , and such that  X  (0) &gt; 0 . Define for some constant 0 &lt;  X  1 8 . Let m i = 1  X  2 ( i ) for some constant C &gt; 0 and let k i = m i r for some other constant r &gt; 0 and i = 0 ,...,s . Assume that learning algo-rithm uses a vector w approx  X  X C 0 to answer first k 0 queries, a vector w approx  X  X C 1 to answer next k 1 queries, etc. As-sume also that an algorithm uses a vector w approx  X  X C s to answer remaining T  X  P s i =0 k i queries. Then the following is true about the cumulative error cum made by the algorithm: Proof. Note first that for any d -dimensional hypercube HC  X  R D of side length l , two vectors: w 1 ,w 2  X  HC and a vector q = ( q 1 ,...,q D ) such that: q i = 1 for i = 1 ,...,d the following is true: | w 1  X  q  X  w 2  X  q | X  l the fact that: k w 1  X  w 2 k 2  X  l Schwarz inequality. Thus we see that the cumulative error cum made by the algorithm for the first P s i =0 k i satisfies: Therefore we have: We can write: 0, t is well-defined. Notice that t does not depend on d , D and T , but only on the random variable E and constant  X  . Observe that:
CL and where the last inequality follows immediately from the defi-nition of t (density  X  on the interval considered in the defi-at least: the length of that interval times  X  (0)  X  2 , i.e.: Therefore the considered expression is of the order O ( L R = CL we get: where Therefore we have: Thus we get: so we also get: Using the formula on s , we get: Combining this upper bound on R with the upper bound on the previous expression, we obtain: Next let us focus on the cumulative error 2 cum made by the algorithm for the remaining T  X  P s i =0 k i queries. By the definition of s we know that This implies that for any w  X  X C s we have: Thus clearly for any query coming in this phase the learning algorithm makes an error at most Schwarz inequality) and we have at most T queries in this phase. Therefore 2 cum = O ( entire proof.
In the following lemma we analyze cutting the hypercube according to some linear threshold.

Lemma 4.3. Let w  X  R D , let { v 1 ,...,v d } be a system of pairwise orthogonal vectors such that v i  X  R D , k v i k i = 1 ,..,d and let HC = { w + P d i =1 f i v i : f 1 ,...,f be a d -dimensional hypercube. Let e be a unit-length vector in L 2 that is parallel to v 1 , i.e. e = 1  X  length vector satisfying: z  X  e  X  cos(  X  ) for some 0 &lt;  X  &lt; . Let 0 &lt;  X  &lt; 1 . Define m = min y  X  X C y  X  z and M = max y  X  X C y  X  z . Let HC l = { y  X  X C : z  X  y  X  m +  X  ( M  X  m ) } and HC r = { y  X  HC : z  X  y &gt; m +  X  ( M  X  m ) } . Then for = 8 sin(  X  2 ) and Proof. Denote:  X  = e  X  z . Note that Take first y  X  X  C l . We have: Thus we also get: Define:  X  m = min y  X  X C y  X  e and  X  M = max y  X  X C y  X  e . Notice that: and besides: This follows directly from: and Cauchy-Schwarz inequality. Thus we obtain: and e  X  y  X   X  m +2 sin(  X  Since, from the definition of  X  M,  X  m and HC we have: we obtain: and e  X  y  X   X  m + 2 sin(  X  Therefore This completes the proof of inequality 25. The proof of inequality 26 is completely analogous.
 We are ready to prove Theorem 2.1.
 Proof. Let L = 2 can be divided into s + 1 phases, where in the i th ( i = 0 ,...,s ) all the intervals I i are of length L X   X  i Indeed, whenever the shrinking is conducted, the length of each side of the hypercube decreases by a factor 1  X  (see sub-routine ShrinkHyperCube ), the initial lengths are 2 the shrinking is not performed anymore if the side of each length is at most 1  X  phase , 2nd-phase , etc. Notice also that the value of the parameter N crit is constant across a fixed phase since this number changes only when ShrinkHyperCube subroutine is performed. Let us denote the value of N crit during the i phase of the algorithm as n i . Notice that where  X  p i is the value of the parameter  X  p of the algorithm used in the i th phase. Denote by k i the number of queries that need to be processed in the i th phase for i = 0 ,...,s  X  1. Parameter k i is a random variable but we will show later that with high probability: k i  X  n i r for i = 0 ,...,s  X  1, where: Assume now that this is the case. Denote by HC 0 ,..., HC the sequence of hypercubes constructed by the algorithm. Assume furthermore that w  X   X  HC 0  X   X  X  X   X  HC s . Again, we have not proved it yet, we will show that this happens with high probability later. However we will prove now that under these two assumptions we get the average error pro-posed in the statement of Theorem 2.1. Notice that under these assumptions we can use Lemma 4.2 with L = 2 following bound on the cumulative error: Thus the average error is at most By using the expression h ( T ) = log( T )  X  we obtain the bound from the statement of Theorem 2.1.
It remains to prove that our two assumptions are correct with high probability and find a lower bound on this proba-bility that matches the one from the statement of the theo-rem. We will do it now. Let us focus on the i th phase of the algorithm. First we will find an upper bound on the prob-ability that the number of queries processed in this phase is greater than k i . Fix a vector e j from the orthonormal basis C . The probability that a new query q is within angle  X  from e j is at least p = p D , X  , by the definition of p Assume that u i queries were constructed. By standard con-centration inequalities, such as Azuma X  X  inequality, we can conclude that with probability at least 1  X  e  X  2 u i ( p 2 of those queries will be within angle  X  from e take: then we conclude that with probability at least 1  X  e  X  2 u at least n i of those queries will be within angle  X  from e Denote u i = n i r , where r &gt; 2 p . We see that the considered n we get that this probability is at least 1  X  e  X  30 r p 2 Notice that when n i queries within angle  X  from a given vector e j  X  C are collected, the j th dimension is ready for shrinking. Thus taking union bound over O (log( dT )) phases and all d dimensions we see that if we take k i = rn i , where: r = 2 p 2 , then with probability at most d log( dT ) phase of the algorithm for i  X  X  0 ,...,s  X  1 } will require more than k i queries. Now let us focus again on the fixed i th of the algorithm. Assume that ShrinkHyperCube subroutine is being run. Fix some dimension j  X  { 1 ,...,d } . We know that, with high probability, at least n i queries q that were within angle  X  from the vector e j  X  X  were collected. Denote by w  X  j the j th coordinate of w  X  . Let I j = [ x j ,y that w  X  j  X  [ x j ,y j ]. Let us assume that the ShrinkHyperCube subroutine replaced I j = [ x j ,y j ] by  X  I j . We want to show that with high probability segment  X  I j is constructed in such a way that w  X  j  X   X  I j . Denote and Notice first that if w  X  j  X  [(1  X   X  ) l, X l ] then w  X  j will be in since no matter how  X  I j is constructed, it always contains Thus we have either or Let us assume first the former. Consider a query-vector q within angle  X  of e j that contributed to N + j . Let us denote by p + the probability of the following event F q : for q the oracle O gives answer:  X  X reater than 0 X . Observe that the total error made by the database mechanism M while com-puting the dot-product: w  X   X  q is D E . Now notice, that by Lemma 4.3 and the definition of E , probability p + is at most P ( D E &gt;  X   X  l ), where: = 8 sin(  X  2 ) Notice that in the i th phase the hypercube under consider-ation has the side of length exactly  X  i . Thus, since  X  = we get: Let us assume now that w  X  j  X  [ y j  X  (1  X   X  ) l,y j ]. We proceed with the similar analysis as before. We see that the proba-bility P + of an event F q is at least P ( D E  X  X  X   X  + l ). Thus we obtain: But now we see, by Lemma 4.1, using: m = N i , p 1 = P ( E &gt; that N + i &gt; N i p 1 + N i  X  p 2 is satisfied if w  X  j  X  )( y j  X  x j )] with probability at most e  X  n i ( X  p ) N  X  )( y j  X  x j )] with probability at most e  X  n i ( X  p ) Lemma 4.1 since (as it is easy to notice) in the i th phase  X  p is exactly and p 1 is exactly We obtain the following: the probability that there exists i such that w  X  /  X  X C 0  X  X  X  X  X  X HC i is at most: O ( P s i =0 e Substituting in that expression the formula on n i , and notic-ing that the number of all the phases of the algorithm is logarithmic in T , D and d , we get the bound O ( log( dDT ) Thus, according to our previous remarks, we conclude that section algorithm makes an average error at most: As mentioned before, we complete the proof by using the formula:
We start with the analysis of the running time of On-lineBisection . First we will show that the linear program used by the algorithm to determine the threshold in each round has a closed form solution.

Lemma 5.1. For any query-vector q , I 1 = [ x 1 ,y 1 ] ,..., I [ x d ,y d ] and orthonormal basis C = { e 1 ,...,e d } the value is given by where J + = { i  X  { 1 ,...,d } : e i  X  q  X  0 } and J  X  = { i  X  { 1 ,...,d } : e i  X  q &lt; 0 } .
 Proof. Take some point: c 1 e 1 +  X  X  X  + c d e d , where: x c  X  y i for i = 1 ,...,d . For j  X  J + the following is true: c j  X  J  X  we have: c j e j  X  q  X  x j e j  X  q , again by the definition of J  X  . Combining these inequalities we get that for every point v in the hypercube HC induced by I 1 ,..., I d and C the following is true: v  X  q  X  opt . Besides clearly there exists v  X  X C such that: v  X   X  q = opt .

Now let us fix a query q . It is easy to notice that q is being processed by the algorithm in O ( dD ) time. Indeed, a single query requires updating O ( d ) variables of the form N i , N Lemma 5.1 in O ( dD ) time. Computing dot product of the query with the given approximation of the database vector clearly takes O ( D ) time. Thus OnlineBisection runs in the O ( dD )-time per query. Notice that OnlineBisection algo-rithm does not store any nontrivial data structures, only segments: I 1 ,..., I d , counts: N + i , N  X  i for i = 1 ,...,d and a constant number of other variables. The counts can be represented by O (log( T ))-digit numbers thus we conclude that OnlineBisection runs in the O (log( T ))-memory.
In this section we prove a negative result showing that up to the poly (log( T )) factor no algorithm (offline or online) can beat OnlineBisection in respect to the achieved error. We prove that this is the case even if the oracle is turned off and the perturbed answer is given directly to the adversary.
Theorem 6.1. In the considered database model (but with online mode possibly turned off ) any algorithm achieves er-ror at least  X  O ( 1  X  Proof. The proof is a direct consequence of the tightness of Azuma X  X  inequality. We will prove that the result holds even for D = 2. Our queries q will be of the form ( x, 1). Consider a database vector w = ( a,b ) for some b &gt; 0. The perturbed answer that the oracle receives is of the form ax + b + q where q is the error added to the exact answer for the query q . Obviously, it suffices to show our result if the oracle is turned off and the adversary receives ax + b + x instead of the binary signal.
 Intuitively, we want to show that for T queries there al-ways exists a vector w near = ( a,b near ), where: b near  X  T log( T ) ) such that with probability almost one the ad-versary will not be able to distinguish between w and w near based on the outcome for these T queries.
 Let us analyze the following expression: E = P T i =1 q i Notice that from Azuma X  X  inequality we know that P ( E &gt; heavily used this fact before. However, from the tightness of Azuma X  X  inequality (see: [17]) we also know that there exists a symmetric bounded distribution Z such that if every is taken from Z then P ( E &gt; 1  X  f ( T ) = log( T ).

Let us assume that this is the case, i.e. E &gt; 1 But then one can easily check that there exists w near (and related distribution determining the amount of error being added to each answer) that gives the same perturbed an-swers as w . Thus, given this set of queries the adversary will incur an error of at least  X  ( 1  X  given by at least one of the two database mechanisms deter-mined by vectors: w and w near for each asked query (since the distance between w and w near is of order  X  ( 1  X  Thus, by the Pigeonhole Principle, for at least one of the two mechanisms the adversary will achieve average error at least
That completes the proof since both database mechanisms are legitimate mechanisms that can communicate with the adversary.
Throughout the paper we have considered the challeng-ing online scenario, where the algorithm both learns and is evaluated on a single set of streaming queries. However, we note that the OnlineBisection algorithm also works well in the batch setting, i.e. when there is a separate train and test phase. We prove here Corollary 2.1, that for clarity we state once more:
Corollary 7.1. Let w T denote the final hypothesis con-structed by the OnlineBisection algorithm after consuming T queries drawn from an unknown distribution D . Then the following inequality holds with probability at least 1  X  D :
Proof. This simply follows from the fact that, as ar-gued in the proof of Theorem 2.1, w  X   X  HC s with at least the probability indicated in the statement of this corollary. Furthermore, by definition of the algorithm, we have w T  X  HC s and the length of the side of the hypercube HC log( T ) / | w We presented in this paper the first  X  O ( 1  X  rithm for database reconstruction. It is adapted to the highly challenging, yet very realistic setting, where the an-swers given by the database are heavily perturbed by a ran-dom noise and besides there exists strong privacy mechanism (binary oracle O ) that aims to protect the database against an adversary attempting to compromise it. We show that even if the learning algorithm receives only binary answers on the database side and needs to learn database-vector w with high precision at the same time it is being evaluated, it can still achieve very small average error. We assume that the query-space is low-dimensional but this fact is needed only to guarantee that the term r = 2 p 2 on the error is not exponential in D . The low-dimensionality assumption is indispensable here if one wants to achieve av-erage error of the order o (1) and considers nontrivial models with random noise. It is also worth to mention that if no noise is added, the low-dimensionality query-space is not re-quired and a simple modification of our algorithm enables to get rid of the 2 p 2 very limited (logarithmic) memory and is very fast ( O ( d ) processing time per query). By not using linear program-ming we obtain better theoretical bounds regarding running time of the algorithm than previously considered methods. OnlineBisection algorithm adapts next threshold values sent to the binary oracle O to its previous answers in order to obtain good approximation of the projection of a database-vector w  X  onto the low-dimensional query-space U .
It would be interesting to know whether the assumption about the low-dimensional querying model is indeed nec-essary or may be at least relaxed. Another area that can be explored is the application of the presented method to machine learning settings other than adversarial machine learning. It seems that presented technique provides a gen-eral mechanism for efficient online information retrieval. Fi-nally, the authors plan also to extend presented algorithm to work in the setting, where no independence assumption for the mechanism of noise addition is required. [1] Cynthia Dwork. Differential privacy. In ICALP (2) , [2] Cynthia Dwork. Differential privacy in new settings. [3] Cynthia Dwork, Moni Naor, Toniann Pitassi, and [4] Cynthia Dwork, Moni Naor, Toniann Pitassi, Guy N. [5] Kobbi Nissim, Rann Smorodinsky, and Moshe [6] Krzysztof Choromanski and Tal Malkin. The power of [7] Sergey Yekhanin. Private information retrieval. [8] Cynthia Dwork, Frank McSherry, and Kunal Talwar. [9] Irit Dinur and Kobbi Nissim. Revealing information [10] Cynthia Dwork and Sergey Yekhanin. New efficient [11] Sanjoy Dasgupta and Yoav Freund. Random [12] Mikhail Belkin and Partha Niyogi. Laplacian [13] Richard G. Baraniuk, Volkan Cevher, and Michael B. [14] Nilesh Dalvi, Pedro Domingos, Mausam, Sumit [15] Yevgeniy Vorobeychik and Bo Li. Optimal randomized [16] Kevin G. Jamieson, Maya R. Gupta, Eric Swanson, [17] S. Ross. Stochastic processes. Wiley , 1996.
