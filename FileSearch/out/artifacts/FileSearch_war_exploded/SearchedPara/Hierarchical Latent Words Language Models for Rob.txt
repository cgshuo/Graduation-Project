 Language models (LMs) are essential for auto-matic speech recognition or statistical machine translation (Rosenfeld, 2000). The performance of LMs strongly depends on quality and quantity of their training data. Superior performance is usu-ally obtained by using enormous domain-matched training data sets to construct LMs (Brants et al., 2007). Unfortunately, in many cases, large amounts of domain-matched training data sets are not available. Therefore, LM technology that can robustly work for domains that differ from that of the training data is needed (Goodman, 2001).
For robust language modeling, several tech-nologies have been proposed. Fundamental tech-niques are smoothing (Chen and Goodman, 1999) and clustering (Brown et al., 1992). Other solu-tions are Bayesian modeling (Teh, 2006) and en-semble modeling (Xu and Jelinek, 2004; Emami and Jelinek, 2005). Moreover, continuous rep-resentation of words in neural network LMs can also support robust modeling (Bengio et al., 2003; Mikolov et al., 2010). However, previous works are focused on maximizing performance in the same domain as that of the training data. In other words, it is uncertain that these technologies ro-bustly support out-of domain tasks.

In contrast, latent words LMs (LWLMs) (De-schacht et al., 2012) are clearly effective for out-of domain tasks. We employed the LWLM to speech recognition and the resulting performance was significantly superior in out-of domain tasks while the performance was comparable in domain-matched task to conventional LMs (Masumura et al., 2013a; Masumura et al., 2013b). LWLMs are generative models that employ a latent word space. The latent space can flexibly take into ac-count relationships between words and the model-ing helps to efficiently increase the robustness to out-of domain tasks (Sec. 2).

In this paper, we focus on LWLMs and aim to make them more flexible for greater robustness to out-of domain tasks. To this end, this paper takes note of a fact that standard LWLM simply repre-sents the latent space as n-gram model of latent words. However, function and meaning of words are essentially hierarchical and upper layers ought to be useful to increase the robustness to out-of domain tasks. The conventional LWLMs do not model the hierarchy, while the latent words are used to represent function and meaning of words. Thus, we tried to model the hierarchy in the latent space by estimating a latent word of a latent word recursively.

This paper proposes a novel LWLM with mul-tiple latent word spaces that are hierarchically structured; we call it the hierarchical LWLM (h-LWLM). The proposed model can be regarded as a generalized form of the standard LWLMs. The hierarchical structure can take into account the abstraction process of function and meaning of words. Therefore, it can be expected that h-LWLMs flexibly calculate generative probability for unseen words unlike non-hierarchical LWLMs. To create the hierarchical latent word structure from training data sets, we also propose a layer-wise inference. The inference is inspired by a deep Boltzmann machine (Salakhutdinov and Hin-ton, 2009) that stacks up restricted Boltzmann ma-chines (Hinton et al., 2006). In addition, we detail an n-gram approximation technique to apply the proposed model to practical natural language pro-cessing tasks (see Sec. 3).

In experiments, we construct LMs from sponta-neous lecture task data and apply them to a contact center dialogue task and a voice mail task as out-of domain tasks. The effectiveness of the proposed method is shown by perplexity and speech recog-nition evaluation (Sec. 4). LWLMs are generative models with single latent word space (Deschacht et al., 2012). The latent word is represented as a specific word that is se-lected from the entire vocabulary. Thus, the num-ber of latent words equals the number of observed words.

Bayesian modeling of LWLM produces the gen-erative probability of observed word sequence w = w 1 ,  X  X  X  ,w K as: P ( w ) = where  X  indicates a model parameter of the LWLM, h = h 1 ,  X  X  X  ,h K denotes a latent word sequence and l k denotes context latent sents the transition probability which can be ex-pressed by an n-gram model for latent words, and P ( w k | h k ,  X  ) represents the emission probability that models the dependency between the observed word and the latent word. More details are shown in previous works (Deschacht et al., 2012; Ma-sumura et al., 2013a; Masumura et al., 2013b). 3.1 Definition This paper introduces h-LWLM. The proposed model has multiple latent word spaces in a hier-archical structure. Thus, it assumes that there is
Figure 1: Graphical representation of h-LWLM. a latent word behind a latent word. The proposed model can be regarded as a generalized form of the standard LWLM. Thus, standard LWLMs cor-respond to h-LWLMs with just one layer. The la-tent words in all layers are represented as a specific word that is selected from the entire vocabulary. A graphic rendering of h-LWLM is shown in Figure 1. In a generative process of the h-LWLM, a latent word in the highest layer is first generated depending on its context latent words. Next, a la-tent word in a lower layer is recursively generated depending on the latent word in the upper layer. Finally, an observed word is generated depending on the latent word in the lowest layer.

Bayesian modeling of h-LWLM produces the following generative probability: P ( w ) = where M is the number of layers and  X  indi-h in the m -th layer. P ( h ( M ) the transition probability which is expressed by n-gram model for latent words in the highest layer. the emission probabilities that respectively model the dependency between latent words in two layers and the dependency between the observed word and the latent word in the lowest layer.

As the integral with respect to  X  is analytically Algorithm 1 : Inference procedure for h-LWLM.
 Input: Training data w , number of instances T , Output: Model parameters  X  1 ,  X  X  X  ,  X  T 1: for t = 1 to T do 3: for m = 1 to M do 5: end for 7: end for 8: return  X  1 ,  X  X  X  ,  X  T intractable, the equation can be approximated as:
P ( w ) = The probability distribution can be approximated using T instances of point estimated parameter;  X  t indicates the t -th point estimated parameter. 3.2 Parameter Inference
This paper proposes a layer-wise inference pro-cedure for constructing h-LWLMs from training data. The detailed procedure is shown in Algo-rithm 1 , and Figure 2 shows an image representa-tion of the procedure as increased with the number of layers. In the procedure, LWLM structure is re-cursively accumulated by estimating a latent word sequence in an upper layer from a latent word se-quence in the lower layer.

Line 4 in Algorithm 1 denotes the key proce-dure of estimating a latent word sequence in an up-per layer from a latent word sequence in the lower structure in m -th layer; it can be defined from both George, 1992; Robert et al., 1993; Scott, 2002). Gibbs sampling picks a new value for h ( m ) cording to its probability distribution which is es-timated from both h ( m ) sents all latent words in the m -th layer except for h For the inference, the prior distribution is neces-sary for each probability distribution. Usually, a hierarchical Pitman-Yor prior (Teh, 2006) is used for each transition probability and a Dirichlet prior (MacKay and Peto, 1994) is used for each emis-sion probability.

As shown in line 6, t -th point estimated param-eter  X  t indicates parameters of each LWLM for all layers in t -th iteration. The transition proba-bilities except for M -th layer are only used in the layer-wise inference procedure. 3.3 Usage It is impractical to directly apply the h-LWLM to natural language processing tasks since the pro-posed model has multiple latent word spaces and we have to consider all possible latent word as-signment for calculating generative probabilities. Therefore, this paper introduces an n-gram ap-proximation technique as well as that for standard LWLM (Masumura et al., 2013a). Algorithm 2 : Random sampling for trained h-LWLM.
 Input: Model parameters  X  1 ,  X  X  X  ,  X  T , Output: Sampled data w 1: for k = 1 to K do 4: for m = M  X  1 to 1 do 6: end for 8: end for 9: return w = w 1 ,  X  X  X  ,w K
The n-gram approximation is conducted as fol-lowing steps. First, a lot of text data that permit h-LWLMs to be approximated by n-gram structure is generated by random sampling using trained h-LWLM. Next, an n-gram model is constructed from the generated data. The random sampling is based on Algorithm 2 . The sampled data w in line 9 is only used for n-gram model estimation. 4.1 Experimental Conditions Our basic assumption is domain-matched train-ing data is not available. Thus, for LM train-ing, we used the Corpus of Spontaneous Japanese (CSJ) whose domain is a spontaneous lecture task (Maekawa et al., 2000). We divided CSJ into a training set and a small validation set (Valid). The validation set was used for optimizing several hy-per parameters of LMs. For evaluation, a contact center dialogue task (Test 1) and a voice mail task (Test 2) were prepared. In contact center dialogue task, two speakers, an operator and a customer, talked to each other as in call center dialogues. 24 phone calls (24 operator channels and 24 customer channels) were used in the evaluation. In the voice mail task, a person spoke small voice messages us-ing a smart phone. 237 messages are used in the evaluation. The training data had about 7M words, the vocabulary size was about 80K. The validation data size and test data size (both tasks) were about 20K words.

For speech recognition evaluation, we prepared an acoustic model based on hidden Markov mod-els with deep neural networks (DNN-HMM) (Hin-ton et al., 2012). The DNN-HMM had 8 hidden layers with 2048 nodes. The speech recognizer was a weighted finite state transducer (WFST) de-coder (Mohri et al., 2001; Hori et al., 2007). As a baseline, 3-gram LM with interpolated Kneser-Ney smoothing ( MKN ) (Kneser and Ney, 1995) and 3-gram hierarchical Pitman-Yor LM ( HPY ) (Huang and Yor, 2007) were constructed from the training data. We also trained a class-based recurrent neural network LM with 500 hid-den nodes and 500 classes ( RNN ) for comparison to state-of-the art language modeling (Mikolov et al., 2011). In addition, we constructed 3-gram standard LWLM and 3-gram h-LWLMs ( LW ). LW with 1 layer represents standard LWLM, and LW with 2-5 layers represent proposed h-LWLMs. The number of instances was set to 10 for each LW . For their n-gram approximation, we generated one billion words and approximated each as a 3-gram HPYLM. Moreover, we constructed interpolated model with LW and HPY ( LW+HPY ). 4.2 Results Figure 3 shows the relation between number of layers in h-LWLM and perplexity (PPL) reduc-tion for each condition. In addition, Table 1 shows speech recognition results in terms of word error rate (WER) for each condition. RNN was only tested in PPL evaluation as RNN cannot be con-verted into WFST format.

For the validation set (same domain as that of training set), PPL was not improved by the hier-archical structure in LW . LW is comparable to MKN and HPY , and inferior to RNN in terms of PPL. On the other hand, in test sets (out-of domain tasks), PPL improved with the increase in the number of layers in LW . LW with 5 layers was superior to 1 layer in terms of PPL and WER. The best re-sults were obtained by LW+HPY with 5 layers. In fact, when we generated one billion words using a trained LWLM or trained h-LWLM, the num-ber of observed trigrams in h-LWLM with 5 lay-ers was 101M while the number of observed tri-grams in non-hierarchical LWLM was 94M. Thus, h-LWLM can generate unseen words unlike non-hierarchical LWLM. Moreover, trigram coverage in each test data slightly increased with number of layers. These results show that h-LWLM with multiple layers offers robust performance not pos-sible with other models while its performance in the same domain as that of training data was not improved. As a result, LW+HPY with 5 layers performed significantly better than MKN , HPY and RNN in the out-of domain tasks. This paper proposed h-LWLM for robust model-ing and detailed its definition, inference proce-dure, and approximation method. The proposed model has a hierarchical latent word space and it can flexibly handle linguistic phenomena not present in the training data. Our experiments showed that h-LWLM offers improved robustness to out-of domain tasks; h-LWLM is also superior to standard LWLM in terms of PPL and WER.
 Furthermore, our approach is significantly supe-rior to the conventional n-gram models or the re-current neural network LM in out-of domain tasks.
