 In this paper we address the issue of learning to rank for document retrieval. In the task, a model is automatically created with some training data and then is utilized for ranking of documents. The goodness of a model is usually evaluated with performance mea-sures such as MAP (Mean Average Precision) and NDCG (Nor-malized Discounted Cumulative Gain). Ideally a learning algo-rithm would train a ranking model that could directly optimize the performance measures with respect to the training data. Existing methods, however, are only able to train ranking models by mini-mizing loss functions loosely related to the performance measures. For example, Ranking SVM and RankBoost train ranking mod-els by minimizing classification errors on instance pairs. To deal with the problem, we propose a novel learning algorithm within the framework of boosting, which can minimize a loss function directly defined on the performance measures . Our algorithm, re-ferred to as AdaRank, repeatedly constructs  X  X eak rankers X  on the basis of re-weighted training data and finally linearly combines the weak rankers for making ranking predictions. We prove that the training process of AdaRank is exactly that of enhancing the per-formance measure used. Experimental results on four benchmark datasets show that AdaRank significantly outperforms the baseline methods of BM25, Ranking SVM, and RankBoost.
 H.3.3 [ Information Search and Retrieval ]: Retrieval models Algorithms, Experimentation, Theory Information retrieval, Learning to rank, Boosting
Recently  X  X earning to rank X  has gained increasing attention in both the fields of information retrieval and machine learning. When applied to document retrieval, learning to rank becomes a task as follows. In training, a ranking model is constructed with data con-sisting of queries, their corresponding retrieved documents, and rel-evance levels given by humans. In ranking, given a new query, the corresponding retrieved documents are sorted by using the trained ranking model. In document retrieval, usually ranking results are evaluated in terms of performance measures such as MAP (Mean Average Precision) [1] and NDCG (Normalized Discounted Cumu-lative Gain) [15]. Ideally, the ranking function is created so that the accuracy of ranking in terms of one of the measures with respect to the training data is maximized.

Several methods for learning to rank have been developed and applied to document retrieval. For example, Herbrich et al. [13] propose a learning algorithm for ranking on the basis of Support Vector Machines, called Ranking SVM. Freund et al. [8] take a similar approach and perform the learning by using boosting, re-ferred to as RankBoost. All the existing methods used for docu-ment retrieval [2, 3, 8, 13, 16, 20] are designed to optimize loss functions loosely related to the IR performance measures, not loss functions directly based on the measures. For example, Ranking SVM and RankBoost train ranking models by minimizing classifi-cation errors on instance pairs.

In this paper, we aim to develop a new learning algorithm that can directly optimize any performance measure used in document retrieval. Inspired by the work of AdaBoost for classification [9], we propose to develop a boosting algorithm for information re-trieval, referred to as AdaRank. AdaRank utilizes a linear com-bination of  X  X eak rankers X  as its model. In learning, it repeats the process of re-weighting the training sample, creating a weak ranker, and calculating a weight for the ranker.

We show that AdaRank algorithm can iteratively optimize an ex-ponential loss function based on any of IR performance measures. A lower bound of the performance on training data is given, which indicates that the ranking accuracy in terms of the performance measure can be continuously improved during the training process.
AdaRank o ff ers several advantages: ease in implementation, the-oretical soundness, e ffi ciency in training, and high accuracy in ranking. Experimental results indicate that AdaRank can outperform the base-line methods of BM25, Ranking SVM, and RankBoost, on four benchmark datasets including OHSUMED, WSJ, AP, and .Gov.
Tuning ranking models using certain training data and a perfor-mance measure is a common practice in IR [1]. As the number of features in the ranking model gets larger and the amount of train-ing data gets larger, the tuning becomes harder. From the viewpoint of IR, AdaRank can be viewed as a machine learning method for ranking model tuning.

Recently, direct optimization of performance measures in learn-ing has become a hot research topic. Several methods for classifi-cation [17] and ranking [5, 19] have been proposed. AdaRank can be viewed as a machine learning method for direct optimization of performance measures, based on a di ff erent approach.

The rest of the paper is organized as follows. After a summary of related work in Section 2, we describe the proposed AdaRank algorithm in details in Section 3. Experimental results and discus-sions are given in Section 4. Section 5 concludes this paper and gives future work.
The key problem for document retrieval is ranking, specifically, how to create the ranking model (function) that can sort documents based on their relevance to the given query. It is a common practice in IR to tune the parameters of a ranking model using some labeled data and one performance measure [1]. For example, the state-of-the-art methods of BM25 [24] and LMIR (Language Models for Information Retrieval) [18, 22] all have parameters to tune. As the ranking models become more sophisticated (more features are used) and more labeled data become available, how to tune or train ranking models turns out to be a challenging issue.

Recently methods of  X  X earning to rank X  have been applied to ranking model construction and some promising results have been obtained. For example, Joachims [16] applies Ranking SVM to document retrieval. He utilizes click-through data to deduce train-ing data for the model creation. Cao et al. [4] adapt Ranking SVM to document retrieval by modifying the Hinge Loss function to better meet the requirements of IR. Specifically, they introduce a Hinge Loss function that heavily penalizes errors on the tops of ranking lists and errors from queries with fewer retrieved docu-ments. Burges et al. [3] employ Relative Entropy as a loss function and Gradient Descent as an algorithm to train a Neural Network model for ranking in document retrieval. The method is referred to as  X  X ankNet X .
There are three topics in machine learning which are related to our current work. They are  X  X earning to rank X , boosting, and direct optimization of performance measures.

Learning to rank is to automatically create a ranking function that assigns scores to instances and then rank the instances by us-ing the scores. Several approaches have been proposed to tackle the problem. One major approach to learning to rank is that of transforming it into binary classification on instance pairs. This  X  X air-wise X  approach fits well with information retrieval and thus is widely used in IR. Typical methods of the approach include Rank-ing SVM [13], RankBoost [8], and RankNet [3]. For other ap-proaches to learning to rank, refer to [2, 11, 31].

In the pair-wise approach to ranking, the learning task is formal-ized as a problem of classifying instance pairs into two categories (correctly ranked and incorrectly ranked). Actually, it is known that reducing classification errors on instance pairs is equivalent to maximizing a lower bound of MAP [16]. In that sense, the exist-ing methods of Ranking SVM, RankBoost, and RankNet are only able to minimize loss functions that are loosely related to the IR performance measures.

Boosting is a general technique for improving the accuracies of machine learning algorithms. The basic idea of boosting is to re-peatedly construct  X  X eak learners X  by re-weighting training data and form an ensemble of weak learners such that the total perfor-mance of the ensemble is  X  X oosted X . Freund and Schapire have proposed the first well-known boosting algorithm called AdaBoost ( Ada ptive Boost ing) [9], which is designed for binary classifica-tion (0-1 prediction). Later, Schapire &amp; Singer have introduced a generalized version of AdaBoost in which weak learners can give confidence scores in their predictions rather than make 0-1 deci-sions [26]. Extensions have been made to deal with the problems of multi-class classification [10, 26], regression [7], and ranking [8]. In fact, AdaBoost is an algorithm that ingeniously constructs a linear model by minimizing the  X  X xponential loss function X  with respect to the training data [26]. Our work in this paper can be viewed as a boosting method developed for ranking, particularly for ranking in IR.

Recently, a number of authors have proposed conducting direct optimization of multivariate performance measures in learning. For instance, Joachims [17] presents an SVM method to directly opti-mize nonlinear multivariate performance measures like the F proximately optimize the ranking performance measure DCG [15]. Metzler et al. [19] also propose a method of directly maximizing rank-based metrics for ranking on the basis of manifold learning. AdaRank is also one that tries to directly optimize multivariate per-formance measures, but is based on a di ff erent approach. AdaRank is unique in that it employs an exponential loss function based on IR performance measures and a boosting technique.
We first describe the general framework of learning to rank for document retrieval. In retrieval (testing), given a query the system returns a ranking list of documents in descending order of the rel-evance scores. The relevance scores are calculated with a ranking function (model). In learning (training), a number of queries and their corresponding retrieved documents are given. Furthermore, the relevance levels of the documents with respect to the queries are also provided. The relevance levels are represented as ranks (i.e., categories in a total order). The objective of learning is to construct a ranking function which achieves the best results in ranking of the training data in the sense of minimization of a loss function. Ideally the loss function is defined on the basis of the performance measure used in testing.

Suppose that Y = { r 1 , r 2 ,  X  X  X  , r ` } is a set of ranks, where ` denotes the number of ranks. There exists a total order between the ranks r ` r `  X  1  X  X  X  r 1 , where  X   X  denotes a preference relationship.
In training, a set of queries Q = { q 1 , q 2 ,  X  X  X  , q m query q i is associated with a list of retrieved documents d denotes the sizes of lists d i and y i , d i j denotes the j th document in d , and y i j  X  Y denotes the rank of document d i j . A feature vec-tor ~ x i j =  X  ( q i , d i j )  X  X is created from each query-document pair ( q , d i j ) , i = 1 , 2 ,  X  X  X  , m ; j = 1 , 2 ,  X  X  X  , n ( q can be represented as S = { ( q i , d i , y i ) } m i = 1
The objective of learning is to create a ranking function f : X7 X  &lt; , such that for each query the elements in its corresponding doc-ument list can be assigned relevance scores using the function and then be ranked according to the scores. Specifically, we create a permutation of integers  X  ( q i , d i , f ) for query q ing list of documents d i , and the ranking function f . Let d { d n ( q i ) } to itself. We use  X  ( j ) to denote the position of item j (i.e., d ). The learning process turns out to be that of minimizing the loss function which represents the disagreement between the per-
Notations Explanations q i  X  Q i th query y i j  X  X  r 1 , r 2 ,  X  X  X  , r ` } Rank of d i j w.r.t. q i
S = { ( q i , d i , y i ) } m i = 1 Training set ~ x i j =  X  ( q i , d i j )  X  X  Feature vector for ( q i , d f ( ~ x i j )  X &lt; Ranking model  X  ( q i , d i , f ) Permutation for q i , d i , and f h t ( ~ x i j )  X &lt; t th weak ranker E (  X  ( q i , d i , f ) , y i )  X  [  X  1 , + 1] Performance measure function
In the paper, we define the rank model as a linear combination of weak rankers: f ( ~ x ) = is its weight, and T is the number of weak rankers.

In information retrieval, query-based performance measures are used to evaluate the  X  X oodness X  of a ranking function. By query based measure, we mean a measure defined over a ranking list of documents with respect to a query. These measures include MAP, NDCG, MRR (Mean Reciprocal Rank), WTA (Winners Take ALL), and Precision@n [1, 15]. We utilize a general function sures. The first argument of E is the permutation  X  created using the ranking function f on d i . The second argument is the list of ranks y i given by humans. E measures the agreement between  X  and y i . Table 1 gives a summary of notations described above.
Next, as examples of performance measures, we present the def-initions of MAP and NDCG. Given a query q i , the corresponding list of ranks y i , and a permutation  X  i on d i , average precision for q is defined as: where y i j takes on 1 and 0 as values, representing being relevant or irrelevant and P i ( j ) is defined as precision at the position of d where  X  i ( j ) denotes the position of d i j .

Given a query q i , the list of ranks y i , and a permutation  X  NDCG at position m for q i is defined as: where y i j takes on ranks as values and n i is a normalization con-stant. n i is chosen so that a perfect ranking  X   X  i  X  X  NDCG score at position m is 1.
Inspired by the AdaBoost algorithm for classification, we have devised a novel algorithm which can optimize a loss function based on the IR performance measures. The algorithm is referred to as  X  X daRank X  and is shown in Figure 1.

AdaRank takes a training set S = { ( q i , d i , y i ) } m takes the performance measure function E and the number of itera-tions T as parameters. AdaRank runs T rounds and at each round it creates a weak ranker h t ( t = 1 ,  X  X  X  , T ). Finally, it outputs a ranking model f by linearly combining the weak rankers.

At each round, AdaRank maintains a distribution of weights over the queries in the training data. We denote the distribution of weights Input: S = { ( q i , d i , y i ) } m i = 1 , and parameters E and T Initialize P 1 ( i ) = 1 / m .
 For t = 1 ,  X  X  X  , T End For Output ranking model: f ( ~ x ) = f T ( ~ x ).
 at round t as P t and the weight on the i th training query q t as P t ( i ). Initially, AdaRank sets equal weights to the queries. At each round, it increases the weights of those queries that are not ranked well by f t , the model created so far. As a result, the learning at the next round will be focused on the creation of a weak ranker that can work on the ranking of those  X  X ard X  queries.

At each round, a weak ranker h t is constructed based on training data with weight distribution P t . The goodness of a weak ranker is measured by the performance measure E weighted by P t : Several methods for weak ranker construction can be considered. For example, a weak ranker can be created by using a subset of queries (together with their document list and label list) sampled according to the distribution P t . In this paper, we use single features as weak rankers, as will be explained in Section 3.6.

Once a weak ranker h t is built, AdaRank chooses a weight  X  for the weak ranker. Intuitively,  X  t measures the importance of h
A ranking model f t is created at each round by linearly com-bining the weak rankers constructed so far h 1 ,  X  X  X  , h t  X  ,  X  X  X  , X  t . f t is then used for updating the distribution P
The existing learning algorithms for ranking attempt to minimize a loss function based on instance pairs (document pairs). In con-trast, AdaRank tries to optimize a loss function based on queries. Furthermore, the loss function in AdaRank is defined on the basis of general IR performance measures. The measures can be MAP, NDCG, WTA, MRR, or any other measures whose range is within [  X  1 , + 1]. We next explain why this is the case.

Ideally we want to maximize the ranking accuracy in terms of a performance measure on the training data: where F is the set of possible ranking functions. This is equivalent to minimizing the loss on the training data continuous function and thus may be di ffi cult to handle. We instead attempt to minimize an upper bound of the loss in (5) because e  X  x  X  1  X  x holds for any x  X &lt; . We consider the use of a linear combination of weak rankers as our ranking model: The minimization in (6) then turns out to be where H is the set of possible weak rankers,  X  t is a positive weight, and ( f t  X  1 +  X  t h t )( ~ x ) = f t  X  1 ( ~ x ) +  X  t h coe ffi cients  X  t and weak rankers h t may be considered. Following the idea of AdaBoost, in AdaRank we take the approach of  X  X orward stage-wise additive modeling X  [12] and get the algorithm in Figure 1. It can be proved that there exists a lower bound on the ranking accuracy for AdaRank on training data, as presented in Theorem 1.
T  X  X  X  X  X  X  X  X  1. The following bound holds on the ranking accu-racy of the AdaRank algorithm on training data: where  X  ( t ) = for all i = 1 , 2 ,  X  X  X  , m and t = 1 , 2 ,  X  X  X  , T .
 A proof of the theorem can be found in appendix. The theorem implies that the ranking accuracy in terms of the performance mea-holds.
AdaRank is a simple yet powerful method. More importantly, it is a method that can be justified from the theoretical viewpoint, as discussed above. In addition AdaRank has several other advantages when compared with the existing learning to rank methods such as Ranking SVM, RankBoost, and RankNet.

First, AdaRank can incorporate any performance measure, pro-vided that the measure is query based and in the range of [  X  1 , + 1]. Notice that the major IR measures meet this requirement. In con-trast the existing methods only minimize loss functions that are loosely related to the IR measures [16].

Second, the learning process of AdaRank is more e ffi cient than those of the existing learning algorithms. The time complexity of AdaRank is of order O (( k + T )  X  m  X  n log n ), where k denotes the num-ber of features, T the number of rounds, m the number of queries in training data, and n is the maximum number of documents for queries in training data. The time complexity of RankBoost, for example, is of order O ( T  X  m  X  n 2 ) [8].

Third, AdaRank employs a more reasonable framework for per-forming the ranking task than the existing methods. Specifically in AdaRank the instances correspond to queries, while in the existing methods the instances correspond to document pairs. As a result, AdaRank does not have the following shortcomings that plague the existing methods. (a) The existing methods have to make a strong assumption that the document pairs from the same query are inde-pendently distributed. In reality, this is clearly not the case and this problem does not exist for AdaRank. (b) Ranking the most relevant documents on the tops of document lists is crucial for document re-trieval. The existing methods cannot focus on the training on the tops, as indicated in [4]. Several methods for rectifying the problem have been proposed (e.g., [4]), however, they do not seem to fun-damentally solve the problem. In contrast, AdaRank can naturally focus on training on the tops of document lists, because the perfor-mance measures used favor rankings for which relevant documents are on the tops. (c) In the existing methods, the numbers of docu-ment pairs vary from query to query, resulting in creating models biased toward queries with more document pairs, as pointed out in [4]. AdaRank does not have this drawback, because it treats queries rather than document pairs as basic units in learning. AdaRank is a boosting algorithm. In that sense, it is similar to AdaBoost, but it also has several striking di ff erences from AdaBoost.
First, the types of instances are di ff erent. AdaRank makes use of queries and their corresponding document lists as instances. The la-bels in training data are lists of ranks (relevance levels). AdaBoost makes use of feature vectors as instances. The labels in training data are simply + 1 and  X  1.

Second, the performance measures are di ff erent. In AdaRank, the performance measure is a generic measure, defined on the doc-ument list and the rank list of a query. In AdaBoost the correspond-ing performance measure is a specific measure for binary classifi-cation, also referred to as  X  X argin X  [25].

Third, the ways of updating weights are also di ff erent. In Ad-aBoost, the distribution of weights on training instances is calcu-lated according to the current distribution and the performance of the current weak learner. In AdaRank, in contrast, it is calculated according to the performance of the ranking model created so far, as shown in Figure 1. Note that AdaBoost can also adopt the weight updating method used in AdaRank. For AdaBoost they are equiva-lent (cf., [12] page 305). However, this is not true for AdaRank.
We consider an e ffi cient implementation for weak ranker con-struction, which is also used in our experiments. In the implemen-tation, as weak ranker we choose the feature that has the optimal weighted performance among all of the features:
Creating weak rankers in this way, the learning process turns out to be that of repeatedly selecting features and linearly combining the selected features. Note that features which are not selected in the training phase will have a weight of zero.
We conducted experiments to test the performances of AdaRank using four benchmark datasets: OHSUMED, WSJ, AP, and .Gov. Table 2: Features used in the experiments on OHSUMED, WSJ, and AP datasets. C ( w , d ) represents frequency of word w in document d ; C represents the entire collection; n denotes number of terms in query; | X | denotes the size function; and id f (  X  ) denotes inverse document frequency. 1 3 5 7 ln(BM25 score)
Ranking SVM [13, 16] and RankBoost [8] were selected as base-lines in the experiments, because they are the state-of-the-art learn-ing to rank methods. Furthermore, BM25 [24] was used as a base-line, representing the state-of-the-arts IR method (we actually used
For AdaRank, the parameter T was determined automatically during each experiment. Specifically, when there is no improve-ment in ranking accuracy in terms of the performance measure, the iteration stops (and T is determined). As the measure E , MAP and NDCG@5 were utilized. The results for AdaRank using MAP and NDCG@5 as measures in training are represented as AdaRank.MAP and AdaRank.NDCG, respectively.
In this experiment, we made use of the OHSUMED dataset [14] to test the performances of AdaRank. The OHSUMED dataset con-sists of 348,566 documents and 106 queries. There are in total 16,140 query-document pairs upon which relevance judgments are made. The relevance judgments are either  X  X  X  ( definitely relevant ),  X  X  X  ( possibly relevant ), or  X  X  X ( not relevant ). The data have been used in many experiments in IR, for example [4, 29].
 As features, we adopted those used in document retrieval [4]. Table 2 shows the features. For example, tf (term frequency), idf (inverse document frequency), dl (document length), and combina-tions of them are defined as features. BM25 score itself is also a feature. Stop words were removed and stemming was conducted in the data.

We randomly divided queries into four even subsets and con-ducted 4-fold cross-validation experiments. We tuned the parame-ters for BM25 during one of the trials and applied them to the other trials. The results reported in Figure 2 are those averaged over four trials. In MAP calculation, we define the rank  X  X  X  as relevant and 1 http: // www.lemurproject.com the other two ranks as irrelevant. From Figure 2, we see that both AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures. We conducted sig-nificant tests (t-test) on the improvements of AdaRank.MAP over BM25, Ranking SVM, and RankBoost in terms of MAP. The re-sults indicate that all the improvements are statistically significant (p-value &lt; 0.05). We also conducted t-test on the improvements of AdaRank.NDCG over BM25, Ranking SVM, and RankBoost in terms of NDCG@5. The improvements are also statistically significant.
In this experiment, we made use of the WSJ and AP datasets from the TREC ad-hoc retrieval track, to test the performances of AdaRank. WSJ contains 74,520 articles of Wall Street Journals from 1990 to 1992, and AP contains 158,240 articles of Associ-ated Press in 1988 and 1990. 200 queries are selected from the TREC topics (No.101  X  No.300). Each query has a number of doc-uments associated and they are labeled as  X  X elevant X  or  X  X rrelevant X  (to the query). Following the practice in [28], the queries that have less than 10 relevant documents were discarded. Table 3 shows the statistics on the two datasets.

In the same way as in section 4.2, we adopted the features listed in Table 2 for ranking. We also conducted 4-fold cross-validation experiments. The results reported in Figure 3 and 4 are those aver-aged over four trials on WSJ and AP datasets, respectively. From Figure 3 and 4, we can see that AdaRank.MAP and AdaRank.NDCG outperform BM25, Ranking SVM, and RankBoost in terms of all measures on both WSJ and AP. We conducted t-tests on the im-provements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost on WSJ and AP. The results indi-cate that all the improvements in terms of MAP are statistically sig-nificant (p-value &lt; 0.05). However only some of the improvements in terms of NDCG@5 are statistically significant, although overall the improvements on NDCG scores are quite high (1-2 points).
In this experiment, we further made use of the TREC .Gov data to test the performance of AdaRank for the task of web retrieval. The corpus is a crawl from the .gov domain in early 2002, and has been used at TREC Web Track since 2002. There are a total
Table 4: Features used in the experiments on .Gov dataset. 1 BM25 [24] 2 MSRA1000 [27] 3 PageRank [21] 4 HostRank [30] 5 Relevance Propagation [23] (10 features) of 1,053,110 web pages with 11,164,829 hyperlinks in the data. The 50 queries in the topic distillation task in the Web Track of TREC 2003 [6] were used. The ground truths for the queries are provided by the TREC committee with binary judgment: relevant or irrelevant. The number of relevant pages vary from query to query (from 1 to 86).

We extracted 14 features from each query-document pair. Ta-well-known algorithms (systems). These features are di ff erent from those in Table 2, because the task is di ff erent.

Again, we conducted 4-fold cross-validation experiments. The results averaged over four trials are reported in Figure 5. From the results, we can see that AdaRank.MAP and AdaRank.NDCG out-perform all the baselines in terms of all measures. We conducted t-tests on the improvements of AdaRank.MAP and AdaRank.NDCG over BM25, Ranking SVM, and RankBoost. Some of the improve-ments are not statistically significant. This is because we have only 50 queries used in the experiments, and the number of queries is too small.
We investigated the reasons that AdaRank outperforms the base-line methods, using the results of the OHSUMED dataset as examples.
First, we examined the reason that AdaRank has higher perfor-mances than Ranking SVM and RankBoost. Specifically we com-Figure 6: Accuracy on ranking document pairs with OHSUMED dataset. Figure 7: Distribution of queries with di ff erent number of doc-ument pairs in training data of trial 1. pared the error rates between di ff erent rank pairs made by Rank-ing SVM, RankBoost, AdaRank.MAP, and AdaRank.NDCG on the test data. The results averaged over four trials in the 4-fold cross validation are shown in Figure 6. We use  X  X -n X  to stand for the pairs between  X  X efinitely relevant X  and  X  X ot relevant X ,  X  X -p X  the pairs be-tween  X  X efinitely relevant X  and  X  X artially relevant X , and  X  X -n X  the pairs between  X  X artially relevant X  and  X  X ot relevant X . From Fig-ure 6, we can see that AdaRank.MAP and AdaRank.NDCG make fewer errors for  X  X -n X  and  X  X -p X , which are related to the tops of rankings and are important. This is because AdaRank.MAP and AdaRank.NDCG can naturally focus upon the training on the tops by optimizing MAP and NDCG@5, respectively.

We also made statistics on the number of document pairs per query in the training data (for trial 1). The queries are clustered into di ff erent groups based on the the number of their associated docu-ment pairs. Figure 7 shows the distribution of the query groups. In the figure, for example,  X 0-1k X  is the group of queries whose num-ber of document pairs are between 0 and 999. We can see that the numbers of document pairs really vary from query to query. Next we evaluated the accuracies of AdaRank.MAP and RankBoost in terms of MAP for each of the query group. The results are reported in Figure 8. We found that the average MAP of AdaRank.MAP over the groups is two points higher than RankBoost. Furthermore, it is interesting to see that AdaRank.MAP performs particularly better than RankBoost for queries with small numbers of document pairs (e.g.,  X 0-1k X ,  X 1k-2k X , and  X 2k-3k X ). The results indicate that AdaRank.MAP can e ff ectively avoid creating a model biased to-wards queries with more document pairs. For AdaRank.NDCG, similar results can be observed.

Figure 8: Di ff erences in MAP for di ff erent query groups. Figure 9: MAP on training set when model is trained with MAP or NDCG@5.

We further conducted an experiment to see whether AdaRank has the ability to improve the ranking accuracy in terms of a measure by using the measure in training. Specifically, we trained ranking models using AdaRank.MAP and AdaRank.NDCG and evaluated their accuracies on the training dataset in terms of both MAP and NDCG@5. The experiment was conducted for each trial. Figure 9 and Figure 10 show the results in terms of MAP and NDCG@5, respectively. We can see that, AdaRank.MAP trained with MAP performs better in terms of MAP while AdaRank.NDCG trained with NDCG@5 performs better in terms of NDCG@5. The results indicate that AdaRank can indeed enhance ranking performance in terms of a measure by using the measure in training.

Finally, we tried to verify the correctness of Theorem 1. That is, the ranking accuracy in terms of the performance measure can be an example, Figure 11 shows the learning curve of AdaRank.MAP in terms of MAP during the training phase in one trial of the cross validation. From the figure, we can see that the ranking accuracy of AdaRank.MAP steadily improves, as the training goes on, until it reaches to the peak. The result agrees well with Theorem 1.
In this paper we have proposed a novel algorithm for learning ranking models in document retrieval, referred to as AdaRank. In contrast to existing methods, AdaRank optimizes a loss function that is directly defined on the performance measures . It employs a boosting technique in ranking model learning. AdaRank o ff ers several advantages: ease of implementation, theoretical soundness, e ffi ciency in training, and high accuracy in ranking. Experimental results based on four benchmark datasets show that AdaRank can significantly outperform the baseline methods of BM25, Ranking SVM, and RankBoost. Figure 10: NDCG@5 on training set when model is trained with MAP or NDCG@5.
Future work includes theoretical analysis on the generalization error and other properties of the AdaRank algorithm, and further empirical evaluations of the algorithm including comparisons with other algorithms that can directly optimize performance measures. We thank Harry Shum, Wei-Ying Ma, Tie-Yan Liu, Gu Xu, Bin Gao, Robert Schapire, and Andrew Arnold for their valuable com-ments and suggestions to this paper. [1] R. Baeza-Yates and B. Ribeiro-Neto. Modern Information [2] C. Burges, R. Ragno, and Q. Le. Learning to rank with [3] C. Burges, T. Shaked, E. Renshaw, A. Lazier, M. Deeds, [4] Y. Cao, J. Xu, T.-Y. Liu, H. Li, Y. Huang, and H.-W. Hon. [5] D. Cossock and T. Zhang. Subset ranking using regression. [6] N. Craswell, D. Hawking, R. Wilkinson, and M. Wu.
 [7] N. Du ff y and D. Helmbold. Boosting methods for regression. [8] Y. Freund, R. D. Iyer, R. E. Schapire, and Y. Singer. An [9] Y. Freund and R. E. Schapire. A decision-theoretic [10] J. Friedman, T. Hastie, and R. Tibshirani. Additive logistic [11] G. Fung, R. Rosales, and B. Krishnapuram. Learning [12] T. Hastie, R. Tibshirani, and J. H. Friedman. The Elements of [13] R. Herbrich, T. Graepel, and K. Obermayer. Large Margin [14] W. Hersh, C. Buckley, T. J. Leone, and D. Hickam.
 [15] K. Jarvelin and J. Kekalainen. IR evaluation methods for [16] T. Joachims. Optimizing search engines using clickthrough [17] T. Joachims. A support vector method for multivariate [18] J. La ff erty and C. Zhai. Document language models, query [19] D. A. Metzler, W. B. Croft, and A. McCallum. Direct [20] R. Nallapati. Discriminative models for information retrieval. [21] L. Page, S. Brin, R. Motwani, and T. Winograd. The [22] J. M. Ponte and W. B. Croft. A language modeling approach [23] T. Qin, T.-Y. Liu, X.-D. Zhang, Z. Chen, and W.-Y. Ma. A [24] S. E. Robertson and D. A. Hull. The TREC-9 filtering track [25] R. E. Schapire, Y. Freund, P. Barlett, and W. S. Lee. Boosting [26] R. E. Schapire and Y. Singer. Improved boosting algorithms [27] R. Song, J. Wen, S. Shi, G. Xin, T. yan Liu, T. Qin, X. Zheng, [28] A. Trotman. Learning to rank. Inf. Retr. , 8(3):359 X 381, 2005. [29] J. Xu, Y. Cao, H. Li, and Y. Huang. Cost-sensitive learning [30] G.-R. Xue, Q. Yang, H.-J. Zeng, Y. Yu, and Z. Chen. [31] H. Yu. SVM selective sampling for ranking with application Here we give the proof of Theorem 1.
 P  X  X  X  X  X  . Set Z T =  X  ( t )). According to the definition of  X  t , we know that e  X  t = Z Moreover, if E (  X  ( q i , d i , h T ) , y i )  X  [  X  1 , + 1] then, Z  X  m
