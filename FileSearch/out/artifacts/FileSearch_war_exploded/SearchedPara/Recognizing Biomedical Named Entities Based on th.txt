 With the rapid development of computational and biological technology, biomedical literatures are gro wing exponentially , and abundant literatures about biomedical fundamental step, the biomedical named entity recognition ( Bio -NER ) plays a critical medical field . Over the past years , though various methods have been proposed for Bio -NER , there is still a large gap on recogni tion performance between the biomed i-cal and general field . 
Currently , the most widely used methods to recognize biomedical named entity can focus on dictionary -based methods, rule -based methods and statistical machine lear n-ing methods [1] . Compared with the other two methods, the machine learning m e-biomedical entities which are not previously included in standard dictionaries. T here have been many attempts to develop machine learning techniques such as Hidden Markov Model (HMM) [2] , Maximum Entropy (ME) [3] , Conditional Random Fiel d (CRF ) [4] , Support Vector Machine (SVM) [5] and etc . 
However , these shallow machine learning methods are require d to extract the m a-nual features as the intermediate representation of each word in the text. Theref ore, the recognition performance may be affected by some common drawbacks as follo w-domain knowledge. Besides , selecting an optimal subset of features needs tremendous exp eriments. Furthermore , some complex features with syntactic information may be obtained from other NLP modules , like Part-of-Speech, and the inevitable cascading errors can lead to the final recognition errors. Meanwhile, enormous manual effort s may lead t o over -design of the system and reduce the ability of generalization.
Aiming to overcome the problems described above, deep learning has been applied on the entity recognition in recent years. Collobert et al. [6] proposed a unified neural network architecture for various natural languages processing tasks which also achieved a better result in the NER task. Chen el al. [7] proposed deep belief network (DBN) to extract unsupervised and multi -level feature representation for entity reco g-nition and classifica tion, out perform ing SVM, CRF and ANN classifiers. In order to integrate longer range of contextual effects and flexibly use the context inform ation, Li et al. [8, 9] adopted the combined and extended recurrent neural networks (RNNs) which ha d better perfor mance than CRF models with some simple features . However, some limitations still existed in their system . For example, the back propa gated error in long sentence either blows up or decays exponentially so that long time lags are inaccessible in RNNs. There fore, Long Short Term -Memory (LSTM) as a RNN arch i-tecture is motivated to deal with long range dependencies.

In this paper, we extend the bidirectional LSTM (BLSTM) on biomedical named entity recognition. Firstly, the twin word embeddings are used to rich input inform a-tion. Then, the sentence vector can be obtained by calculating the difference s of two embeddings to get the whole sentence information, which can accurately encodes the the illogical label sequences . The experimental results on the Bio Creative II GM co r-pus show that our S entence vector/Twin word embeddings con ditioned BLSTM (ST-BLSTM) without any manual features can achieve an F -score of 88.61% which is better than (or close to) other state -of-the -art Bio -NER systems. We explore a so -called ST-BLSTM architecture, in which the twin word embeddings and sentence vector are introduced to the BLSTM . The system architecture for named entity recognition base d on ST-BLSTM can be summarized in Fig. 1. Firstly, the word embeddings are obtained by lookup tables and the vectors in the word -context window are concatenated together to feed into the recurrent neural network. T hen, we establish a recurrent neural netw ork with ST -BLSTM unit to acquire the hidden layer . And the recurrent connection is also added into the output layer to associate previous phase to further improve the re cognition capability.
 2.1 LSTM A standard architecture of LSTM mainly consists of an input layer, a recurrent LSTM layer and an output layer. Based on this structure, the input, output and stored information can be partially adjusted by the gates, which enhance the flexibility of the model. Such structures are more capable to learn a complex composition of word vectors than simple RNNs. While numerous LSTM variants have been descr ibed, here the forward pass for the LSTM model used in this paper is as follows: Where  X  denotes the logistic sigmoid function and  X  denotes the element -wise mu l-gate, forg et gate , output gate and the proposed value s, all of which are the same size tions and bias values respectively.  X  forget gate controls the extent to which the previous memory cell is forgotten, the input gate co ntrols what proportion of the current input to pass into the memory cell , and the output gate co ntrols the exposure of the internal memory state. Therefore, the Since the value of the gating variables var ies for each vector element , the model can learn to represent i nformation of long range dependencies . 2.2 BLSTM One shortcoming of conventional RNNs is that they are only able to make use of the previous context. Bidirectional RNNs (BRNNs ) [10] can do this by processing the data in both directions with two separate hidden layers, which are then fed forwards to the same output layer. In order to efficiently make use of the past features and future features, we construct bidirectional LSTM . Since there are no interactions between the two type s of state neurons, the BLSTM network can be unfolded into a general feed forward network. I n our implementation, we respectively do forward and bac k-ward for the whole sentences and reset the hidden states to random values at the b e-ginning of each sentence. 2.3 ST -BLSTM Since the original word embeddings are trained by the unsupervised learning a p-embe ddings. In this paper, fine tuning is added into the training process to retrain the word embeddings. Furthermore, we construct the twin word embeddings to rich the input inform ation and the sentence vector is introduced to extend the neural network, whose memory cell is shown in Fig 2.
 Twin Word Embeddings . The supervised fine -tun ing process can further improve the performance of BLSTM and the retrained word embeddings can also be obtained in the fine -tuning process . The retrained wor d embeddings contain richer information associated with Bio -NER and the pre -trained word embeddings learned from large-account the advantage of both feature information, we use two independent word embeddings to extend the BLSTM network. S ince they share the same initial values, we call them twin word embeddings. The only difference between them is that one kind of word embedding s keeps constant over the whole process, i .e. x t . In this work, we use the new LSTM architecture that is precisely specified below. using the word -level embeddings , the sentence -level feature representation applied twin word embeddings, we can generate the sentence vector d 0 by averaging or maximizing all the word embeddings in the sentence. B esides, we use reading gate r t to control what information should be retrained for future time steps. T hen Equation (5) is modified so that the cell value c t also depends on the sentence vector, which can accurat ely encode the input information. 2.4 Extension at the O utput Layer
Considering the recurrent connection at the output layer can also improve the p er-formance of recognition [8] , the probability information from the previous state to-Equation (15). where W hs and W ss are the weight matrices between the hidden layer and output layer, and between the previous output node and current output node, respectively. h t-1 produces a probability distribution over labels. b s represent s the bias of each layer. 2.5 Training All the neural network models used in this paper are trained by treating each se ntence as a mi ni-batch. The objective function i s the cross entropy error between the pr e-backward networks in the BLSTM are structured to share the same set of word e m-beddings. Adadel ta [11] is used for gradient descent and optimizing the parameters. Besides, dropout [12] is adopted in our experiments to address the overfitting pro b-lem. 2.6 Viterbi Algorithm in the T est ing Phase During the testing phase , Viterbi algorithm is executed to make sure that the illog ical label sequence will not be selected . Since the generated label y i does not involve the example, it is obvious that the label I should not follow the label O . In this paper, we use the similar method as shown in Chen et a l. X  X  [13] , the initial probabilities of illog i-cal entity label is assigned 0, while the others reset to 1. And the transition probabil i-ties of the illogical entity label path should be 0. Thus, the partial probabilities of path containing  X  X  I X  will be 0 and this path will be discarde d. Our experiments are carried out on three different datasets including the BioCreative II GM , JNLPBA2004 and BioC reative V DNER . Firstly, we experimentally demo n-strate that the improvements based on our ST -BLSTM are effective on t he BioCre a-tive II GM corpus. T hen, the comparison with other approaches is conducted on the three corpora. I n experiments, all the deep networks are based on the common Theano neural network toolkit 1 and the RNN models are trained with the same hyper -parameters. All the experiments are based on a set of 200 dimensional word embe d-definition of Precision ( P ), Recall ( R ) and F -score ( F ) are shown as E quation (18-20). negative s. 3.1 Data Set BioCreative V DNER (abstract) 500 500 500
We test our system on three biomedical datasets as shown in Tab le 1. And BioCr e-ative II GM is mainly composed of sentences, while JNLPBA 2004 and BioCreative V development and test sets, respectively. BILOU tagging scheme is selected to find the entity boundary in our experiment. B refers to the beginning word of a gene name, I and L respectively indicate inside tokens and the last token in a gene name if it co n-name, and finally U represents the unit -length chunks. 3.2 BioCreative II GM Corpus The Result s of I mprovement s Based on ST -BLSTM .
 Our experiments are carried out on the BioCreative II GM corpus and the models X  performance is reported in Tab le 2. The effects of improvements are analyzed as following s. Pre-train ed Word Embeddings (PWE) . In order to explore the impact of richer text information on LSTM architecture, we use two ways to initialize the word embedding: random and pre -train ed. And the results reveal that the pre -train ed word embeddings have better performance than the random word embeddings by rising 5.87% F -score (77.98% vs 83.85%). Recurrent Connection (RC) at the output layer . Recurrent connection at the output layer can take advantage of the previous probabili stic information of labels and apply can be considered to further improve the performance . The experimental results show that the F -score can incr ease from 83.85% to 84.43%.
 BLSTM. In order to efficiently make use of the past and future features, bidirectional LSTM networks are trained in our work. From Table 2, we can see that the BLSTM can have better performance which rises 2.77% compared with t he unidirectional LSTM . Twin Word Embeddings . Based on the BLSTM, the twin word embeddings are added to rich the input information and the F -score reaches 88.43% . W e can improve the performance by 1.23% . Sentence Vector. Sentence vector is also combined wi th our BLSTM, which is generated by maximizing the difference of twin word embeddings in a sentence . The F-score reaches 88.54% which rises by 0.11%. Viterbi Algorithm . In the test ing phase, we also use Viterbi algorithm to filter illogical 88.61%.
 Comparison with E xisting System s.
 We make the comparisons between our system and some state -of-the -art work s in Tab le 3. As the best system in competition at that time , Ando [14] mainly used a semi -supervised learning method, combined classifiers with dictionary as well as the post -processing , the final F -score reached 87.21%. I n Li et al.  X  X  system [15] , they extracted rich hand -designed features such as part -of-speech, s temmed wor d, o rthographic fea-ture etc. and u nigram, bigram, trigram types of features based on CRF model as well they increased three kinds of distributed word representation besides the rich hand -designed features , and used the combined methods to reach a better F-score of 88.44%. However, in our approach the complex hand -designed features and domain dictionary kno wledge are skipped as well as 0.17% F-score higher compared with Li et al.  X  X  [16]. 
In Li et al. X  s system [8], the conventional RNNs are adopted to Bio -NER task and demo nstrate s that our model outperforms the conventional RNN.
 Though our system can outperform most of the shallow approaches, compared with Li et al.  X  X  system [17] which perform s the best until now , our F -score is 0.44% lower . construct the dictionary, rich dom ain knowledge and hand -designed fe atures . 3.3 JNLPBA2004 Corpus
Table 4 lists the comparison with other systems on the JNLPBA2004 corpus. Yao of feature s, achieving 71.01% F -score. Chang et al. [19] used some hand -designed features and word embeddings as the input of CRF model as well as the post -processing; they achieved 71.85% F -score. Wang et al. [20] verified that the Gimli method based on CRF model c ould achieve the best performance with 72.23% F-score among six different Bio -NER methods on JNLPBA2004 corpus . Besides, as the score [21] . The abundant resources knowledge and common hand -designed features , such as abbreviation, alias and dictionary, we re used, which greatly enhanced its per-formance. However, t he experimental results show that the F -score of our S T-BLSTM model can reach 72.76% which outperforms all of them by 0.91%, 1.75%, 0.53% and 0.21% , respectively . Meanwhile, no hand -designed features and rules are used in our system. 3.4 Bio Creative V DNER Corpus
We also apply our system on the Bio Creative V DNER corpus . Tab le 5 shows the comparison result with CRF model . In the case of evaluating the test set, we combine training set and development set as the training set. T he CRF model need s to extract ture etc. As shown in T able 5, our method can reach 78.91% F -score on the develo p-ment set, which is 2.5% higher than CRF. And CRF achieve s 76.91% on the test set, while our method is 5.69% higher than CRF instead of any manual features . For e x-ample, in development set , the standard entity  X  X ricuspid valve regurgitation  X  is re c-ognized by our method, while the CRF model could not recognize it . The main re a-son is that the ne ural network can learn more potential characteristic information , and train more compl ex model s; however, the shallow machine learning method s have strong dependency on the art ificial features and hard to represent the complex models . Therefore our method can achieve a better result in the NER task . From the above experimental results, we can conclude that our S T-BLSTM model outperforms most state -of-the -art Bio -NER systems and mainly includes the follo w-ing important advantages: No Hand-designed Features. We skip the step of extracting complex hand -designed features, and replace it with word embeddings trained off -line. Since high -quality word embeddings can catch a large number of precise syntactic and semantic word relationships, th e deep learning architecture can fully utilize th is information and extract the high -level features for the Bio-NER.
 Additional Extension at the Output Layer. Considering that predicted result (i.e. probability of labeling) from the prior node can have an important impact on the current prediction , we extend the original LSTM model by adding a reconnection at the ou tput layer. F rom the experimental results, we can see that the extended method can produce positive impact on the Bio -NER.
 Combining Twin Word Embeddings and S entence Vector. Considering the fine -tun ed word embeddings contain richer information asso ciated with Bio -NER , and the pre -trained word embeddings contain the feature information learning from large -scale unlabeled corpus, we extend the bidirectional LSTM by adding t win word embeddings . For the input, the extended features are more abundant, and the multiplication gates can control more accurate information. B esides, the sentence vector could contain complementary information of twin word embeddings . The experimental results show that both twin word embeddings and sentence vector could have posi tive effects on the BLSTM architecture to recognize biomedical named entities.
 Viterbi Algorithm in the Testing Phase . The results show that the added Viterbi effectively . This is mainly because that t he algorithm is based on dynamic programming and can find the most likely label sequences . In this paper, we propose ST -BLSTM architecture to identify biomedical entities. The t win word embeddings and sentence vect or are added into the bidirectional LSTM to obtain more abundant contextual information. Simultaneously, we extend the mo del by adding recurrent connection at the output layer and in the testing phase the Viterbi algorithm is app lied to filter the illogica l label sequence s. The experime n-tal results show that our model on Bio Creative II GM corpus can achieve 88.61% F -score without using any hand -designed features and exte rnal resource, higher than almost all systems . And on JNLPBA2004 and BioC reative V DNER datasets , we also can achieve a rather better rec ognition performance . Acknowledgment. The authors gratefully acknowledge the financial support pr o-vided by the National Natural Science Foundation of China under No. 61173101, 61173100. The Tesla K40 used f or this research was donated by the NVIDIA Corpo-ration.

