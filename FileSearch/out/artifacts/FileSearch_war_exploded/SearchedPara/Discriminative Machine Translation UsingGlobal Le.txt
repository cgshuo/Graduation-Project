 SRIRAM VENKATAPATHY IIIT-Hyderabad and SRINIVAS BANGALORE AT&amp;T Labs-Research 1. INTRODUCTION The problem of machine translation can be viewed as consisti ng of two sub-problems: (a) lexical selection, where appropriate target language lexical items 8: 2  X  S. Venkatapathy and S. Bangalore are chosen for each source language lexical item and (b) lexi cal reordering, where the chosen target language lexical items are rearrang ed to produce a meaningful target language string. Both these problems are extremely chal-lenging, especially in the case of translation from English to Indian languages and vice versa. In addition to the lack of a large sentence ali gned corpora, lexi-cal choice for Indian languages is difficult because of (1) mo rphological richness of Indian languages and (2) agreement between features of sy ntactic related words in Indian languages. Lexical reordering is also a majo r issue between English and Indian languages as the degree of word-reorderi ng is extremely high. This is in contrast to relatively local lexical reorde ring between English and French. However, Indian languages are relatively free-word order lan-guages where the grammatical role of the content words is lar gely determined by its case markers and not by their positions in the sentence . Hence, predict-ing appropriate function words associated with the content words is perhaps more crucial than ordering the words in the target language s entence. emplified in Brown et al. [1993], employ word-alignment algo rithms [Och et al. 1999; Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore 2005; Blunsom and Cohn 2006] that provide local associations between sour ce words and tar-get words. The source-to-target word-alignments are somet imes augmented with target-to-source word alignments in order to improve t he precision of these local associations. Further, the word-level alignme nts are extended to phrase-level alignments in order to increase the extent of l ocal associations. The phrasal associations compile some amount of (local) lex ical reordering of the target words X  X hose permitted by the size of the phrase . Most of the state-of-the-art machine translation systems use these ph rase-level associa-tions in conjunction with a target language model to produce the target sen-tence. There is relatively little emphasis on (global) lexi cal reordering other than the local reorderings permitted within the phrasal ali gnments. A few ex-ceptions are the hierarchical (possibly syntax-based) tra nsduction models [Wu 1997; Alshawi et al. 1998; Yamada and Knight 2001; Chiang 200 5] and the string transduction models [Kanthak et al. 2005]. Recently , factored models for machine translation [Koehn and Hoang 2007] have been pro posed which use additional linguistic factors associated with words fo r building models for machine translation. Unlike most approaches for machine tr anslation which allow training of only limited number of features, Direct Tr anslation Model2 [Ittycheriah and Roukos 2007] relies on training all its par ameters using the Maximum Entropy Model. However, it still uses only local ass ociations and mild context information (previous word and next word) duri ng translation. associations to produce the target sentence. These models g enerate every lex-ical unit in the target sentence by considering the context o f the entire source sentence. We term this approach of selecting target languag e lexical items by considering features of the entire source sentence as globa l lexical selection (GLS). Although, in this article, we only consider lexical f eatures of the source sentence, in general, the GLS approach can incorporate synt actic information as well as determine the lexical items of a target sentence. T hese lexical items are then reordered using a language model.
 source sentence, each of the target words are chosen by consi dering the lexical features of the entire source sentence. The selected target language words are then permuted in various ways and each permutation is ranked using a target language model and the best permutation is chosen as the targ et language translation. The size of the search space of these permutati ons can be set by a parameter called the permutation window. This model does n ot allow long distance reorderings of target words unless a very large per mutation window is chosen X  X  computationally very expensive proposition.
 This model generates words in an order which is faithful to th e order of words in the source sentence. Now, the number of permutations that need to be ex-amined to obtain the best target language strings is much les s when compared to the bag-of-words model. This model is expected to give goo d results for lan-guage pairs such as English-French for which only local word order variations exist between sentences.
 (LWGs) in the target language. LWGs are groups of words havin g one con-tent word and the associated function words. This model is im portant for two reasons. Besides selecting appropriate words in the target sentence, it also predicts the association between the content words and the f unction words. This association conveys the grammatical roles of the conte nt words. To obtain a well-formed target sentence, these local word groups are o rdered appropri-ately using a target language model.
 phological richness, lexical choice, and association of fu nctions words to appro-priate content words and issues of data sparsity. In contras t to the bag of lwgs, where content words and associated function words are predi cted together, in this model they are predicted independently. The morpholog ical features and the function words of the content words are treated as factor s of the content words. The factors are predicted independently and associa ted to the source positions using sequential lexical choice model, thereby a ssociating them with each other. The factors are then combined using a morpho logical gen-erator to obtain the target language local word groups, whic h are then ordered appropriately.
 to global lexical selection. Section 3 describes four model s for global lexical selection and reordering. In Sections 4 and 5, we report the r esults of the translation models on English-Hindi language pair and high light the strengths and limitations of the models.
 8: 4  X  S. Venkatapathy and S. Bangalore 2. GLOBAL LEXICAL SELECTION For global lexical selection, in contrast to the local appro aches of associating target words to the source words, the target words are associ ated with the entire source sentence. There are two primary advantages in doing this:  X  X or languages such as English-Hindi or English-Japanese, there is a high degree of word-reordering (see Figure 1). Hence, it is diffic ult to obtain high-quality alignments which consequently leads to poor-quality local associations.  X  X here are lexico-syntactic features of the source sentenc e (not necessarily a single source word) that trigger the presence of a target wo rd in the tar-get sentence. For example, in Hindi and Japanese, the gender of the verb should agree with the gender of the subject in certain cases. So, while trans-lating a sentence from English to Hindi/Japanese, the lexic al form of the verb in Hindi/Japanese depends both on the English verb as we ll as the sub-ject/object of the English sentence. So, this is one of the ca ses where global lexical selection is helpful in picking the right lexical it ems in the target language. This phenomenon cannot be captured by a system whi ch relies only on local associations, as is the case in a phrase-based S MT system. In
Figure 2, we can see two sentences where the translations of t he words  X  X et X  and  X  X ings X  have different word forms in the target language . In the first sentence pair, we can see that  X  X ings X  translates in Hindi to form a feminine word form  X  X aatii X  while in the second pair,  X  X ings X  appeare d as the mascu-line form  X  X aataa. X  get word to a source word/phrase: (a) when sentence translat ions are not exact but are paraphrases, and (b) the target language does not hav e one lexical item to express the same concept that is expressed in the source wo rd. The exten-sions of word alignments to phrasal alignments attempt to ad dress some of these situations in addition to alleviating the noise in wor d-level alignments. have a tight association between source language words/phr ases and target language words/phrases. The result of lexical selection is simply a bag of words/phrases in the target language and the target sentenc e has to be re-constructed using this bag of words/phrases.
 information that could aid in the reconstruction of the targ et sentence. This approach to lexical selection and sentence reconstruction has the potential to circumvent the limitations of word-alignment based method s for translation between significantly different word order languages. 3. TRAINING THE DISCRIMINATIVE MODELS FOR LEXICAL SELECTIO N In this section, we present our approach to a global lexical s election model which is based on discriminatively trained classification t echniques. Discrim-inant modeling techniques have become the dominant method f or resolving ambiguity in speech and natural language processing tasks, outperforming generative models for the same task. We expect the discrimin atively trained global lexical selection models to outperform generativel y trained local lexical selection models as well as provide a framework for incorpor ating rich morpho-syntactic information.
 8: 6  X  S. Venkatapathy and S. Bangalore target sequence that maximizes P ( T | S ), where S is the source sentence and T is the target sentence. Ideally, P ( T | S ) should be estimated directly to maximize the conditional likelihood on the training data (d iscriminant model). However, T corresponds to a sequence with an exponentially large combi na-tion of possible labels, and traditional classification app roaches cannot be used directly. Although Conditional Random Fields (CRF) [Laffe rty et al. 2001] train a discriminative model at the sequence level, in trans lation tasks such as ours the computational requirements of training such mod els are currently prohibitively expensive.
 the local associations. Instead, we take the sentence-alig ned corpus as before but we treat the target sentence as a bag of words (BOW) assign ed to the source sentence. The goal is, given a source sentence S , to estimate the probability that we find a given word ( t j ) in its translation, that is, we need to estimate the probabilities P ( true | t j , S ) and P ( false | t j , S ) (see Figure 3). in the target language vocabulary. The probability distrib utions of these bi-nary classifiers are learnt using maximum entropy model [Ber ger et al. 1996; Haffner 2006]. For the word t j , those training sentence pairs are considered as positive examples where the word appears in the target, and n egative other-wise. Thus, the number of training examples for each binary c lassifier equals the total number of training source sentences. In this model , classifiers are trained using word n -grams of the source sentence as features ( BOgrams ( S )). For example, consider the following training data to train t he global lexical selection model. data will be modified as shown below: (1) s 1 s 2 s 3 s 4 s 5 s 6 s 7 s 8 =  X  true p ( false | t 14 , BOgrams ( S )). BOgrams ( S ) are the n -gram features (unigram, bigram and trigram features are usually considered) in the s ource sentence S . For the source sentence S 1 above, the features ( BOgrams ( S )) are: s 1 , s 2 , s 3 , s 4 , s 5 , s 6 , s 7 , s 8 , s 1 -s 2 , s 2 -s 3 , s 3 -s 4 , s 4 -s 5 , s 5 -s 6 , s 6 -s 7 , s 7 -s 8 , ion as shown in Equation 1. In Equation 1, W ( t ) represents the weight vector learned by training a maxent model for a target word t , and F ( X , BOgrams ( S )) represents the feature vector that associates the input fea tures with the class label ( X ). sentences is associated with its repetition index. In the tr aining data (1) above, the target word t 2 has occurred twice, the first occurrence represented as t 1 2 and the second occurrence as t 2 2 . t 1 2 models the occurrence of t 2 once in the target sentence given a source sentence, while t 2 2 models the occurrence of t 2 the second time in the target sentence given a source sentenc e. The training data for modeling repetition of t 2 is shown below. Here, t 2 has occurred twice in sentence (1). (1) s 1 s 2 s 3 s 4 s 5 s 6 s 7 s 8 =  X  true (1) s 1 s 2 s 3 s 4 s 5 s 6 s 7 s 8 =  X  true 3.1 Bag of Words Model the target sentence directly, what we initially obtain is th e target bag of words. Each word in the target bag is detected independently, so we h ave here a very 8: 8  X  S. Venkatapathy and S. Bangalore simple use of binary static classifiers. Given a sentence S , the bag of words ( BOW ( T ) contains those words whose distributions have the positiv e proba-bility greater than a threshold (  X  ).
 in the target vocabulary and the words which satisfy Equatio n 2 are included in the predicted target language bag associated with the sou rce sentence. bag of target words which do not contain any word order inform ation. In order to incorporate word order among the words, we could (a) start with a random order of words or (b) start with an order of words determined b y their lexical choice probability. However, this order has no advantage ov er the random order as the lexical choice probabilities do not carry any se quence information. GLS on an input sentence S . The initial order of words is represented as a finite-state automata (FSA) (see Figure 4).
 might be erroneous or redundant. In order for these words to b e deleted dur-ing the construction of a well-formed sentence in the target language, we intro-duce deletion arcs (epsilon arcs). These deletion arcs comp ete with the words in the bag and provide alternate paths bypassing the words. T hese deletion arcs are associated with a deletion cost ( delc ). A high deletion cost results in fewer words being deleted while a low deletion cost results i n more words being deleted. Figure 5 shows the FSA with deletion arcs.
 we consider various permutations of words in BOW ( T ) and weight them by a target language model. However, considering all possible p ermutations of the words in BOW ( T ) is computationally infeasible. The number of permutation s examined can be reduced by using a heuristic forward pruning or by constrain-ing the permutations to be within a local window ( w ) of adjustable size (also see Kanthak et al. [2005]). In this article, we have chosen to constrain permu-tations using a local window. Figure 6 shows an FSA represent ing the permu-tation of an initial string (with deletion arcs) taking the l ocal window size of 2. This means that a word occurring in position i in the initial order can only occur at positions i  X  1 and i +1. In the experiments, we consider a permutation window of 10, which is reasonable.
 language model [Goffin et al. 2005]. 1 The target language string having the highest probability according to the language model is then retrieved. One can also retrieve the k -best paths from the resulting FSA to obtain the k -best translations of the source sentence. 3.2 Sequential Lexical Choice Model of words which is then permuted to obtain the best target stri ng. If we were able to search the entire space of permuted strings, this ini tial order of words would not have mattered. However, given that it is computati onally expensive to search the complete space of permuted strings, it would be helpful if we could start searching various permutations using a less-arbitra rily ordered bag. One such ordering is the order of source words itself, in which th e target words are associated with source word positions. In this model, du ring the lexical selection, we try to place the target words in an order which i s faithful to the source sentence. 2 sentence and yet retains the power of global lexical selecti on. For every posi-tion ( i ) of the source sentence, a prefix string is formed which consi sts of the sequence of words from positions 1 to i . Each of these prefix strings are used to predict bags of target words using the global lexical sele ction. Now, the bags generated using the prefix strings are processed in the o rder of source positions. Let T i be the bag of target words generated by prefix string i (see Figure 7).
 T that a target word t is associated with a position i if there was some information present at the i th source position that triggered the probability of t to exceed the 8: 10  X  S. Venkatapathy and S. Bangalore these source strings predict the following bags of words.  X  s 1 s 2 s 3 s 4 s 5 =  X  { t 1 , t 4 , t 9 , t 5 , t 12 } six in the source sentence. If a word in T i  X  1 is deleted in T i , which is a rare occurrence, it is ignored. The initial target string is thus the sequence of target language words associated with the sequence of source langu age positions. re-construction procedure described in Section 3.1.2. 3.3 Bag of LWGs Model In this model, we predict local-word-groups (LWGs) in the ta rget language instead of words. Each local-word-group consists of a conte nt word and its associated function words. For example,  X  X as been playing X  is a local-word-group where the content word is  X  X laying X  and the other words just convey the tense and modality of the verb  X  X laying. X  Similarly,  X  X n India X  is an LWG where the function word  X  X n X  conveys syntactic information about the content word  X  X ndia. X  Unlike English, where the syntactic informat ion is expressed through word-order, the grammatical relations of the conte nt words in Hindi are largely expressed through the case-markers associated with these words. For example, the case-marker  X  X e X  always comes with a conten t word which acts as the subject of the verb and  X  X o X  occurs with a content w ord which gen-erally the object of the verb. Hence, both the sentences  X  X ut te (dog) ne billi (cat) ko maaraa (killed) X  and  X  X illi (cat) ko kutte (dog) ne m aara X  mean the same. However, there is a preference of word order between va rious possible sentences.
 chunk, more than one content word can be part of the group wher eas every LWG contains only one content word. In Hindi, the function wo rds succeed the content words which causes local word grouping to be a str aightforward process. However, in English, a dependency parser is used to perform this grouping as the function words might not be adjacent to the co ntent word. For example, in the phrase  X  X n the big house, X  local word groups w ould be  X  X ig X  and  X  in the house. X  Also, the same holds in case of verb groups too. For a p hrase  X  X as been quickly going, X  the local word groups shall be  X  X as been going X  and  X  X uickly. X  Hence, the dependency links between the functio n words and the content word need to be taken into consideration for local wo rd grouping. 3 obtained by applying GLS on an input sentence S . The initial random order of words is represented as an FSA (see Figure 8).
 (with cost = 11) are added to obtain FSA shown in Figure 9.
 path is selected to obtain the best target language string. F or the lexical choice of the local word groups, different types of features can be c onsidered in the source. We considered three types of source sentence featur es, (1) words, (2) local word groups, and (3) local word groups without any dete rminers. 3.4 Factored Sequential Model A word form can be decomposed into a set of factors. For exampl e, a word  X  X ings X  can be represented using three factors, root = sing, category = verb and number = plural. Given the three factors, the original word c an be generated using a morphological generator. We follow this approach, w here instead of generating the word form directly, we first independently pr edict the factors and then use a morphological generator to obtain the actual w ord forms. This approach is crucial in the context of translation to and from Indian languages as Indian languages are morphologically rich and there isn X  t a large parallel corpora between English and Indian languages.
 ical information associated with the content words as facto rs. The function 8: 12  X  S. Venkatapathy and S. Bangalore words are used in language largely to convey grammatical inf ormation and tense of the content words which they qualify. Hence, we trea t the function words as factors of the content words. This decouples the lex ical choice of content words and function words, and both can be predicted i ndependently. Figure 10 shows the various factors of an English sentence  X  X  eople of these islands have adopted Hindi as a means of communication. X  [Bangalore and Joshi 1999] can be considered for each word. W e have not experimented with using supertags for sentence reconstruc tion in this article. source sentence is estimated. This procedure is similar to t he training proce-dure of a simple word based model. One difference is that unli ke the word-based model where only source n-grams were used as features, all the source factors are considered for computing the probability of tar get factor. Obtaining the factors given a sentence involves local word grouping as discussed in the previous section. ate sequences of target language factors. As each factor is a ssociated with a particular source position, the factors that are predicted at that position are in turn associated with each other to form a factor-vector. T he sets of target language factor-vectors are given to the morphological gen erator to obtain the word-forms in the target language. using the re-construction procedure described in Section 3 .1.2. The additional factors that have been predicted in the previous step can als o be used to help the sentence construction. However, we do not conduct this e xperiment in this article. 4. DATASET We conduct experiments on the English-Hindi language pair. We compute the translation accuracies in both the directions for the ba g of words model, whereas for other models, we present the translation accura cy from English to Hindi. The corpus (EILMT Tourism Corpus) on which we condu ct the ex-periments is a domain specific corpus collected as part of Eng lish-to-Indian-Languages consortium (EILMT) project. 4 A total of 12300 English sentences from various Indian tourism web sites were collected and the n translated to Hindi by the participants of the EILMT consortium project. 5 The average length of the English sentences is 24.61 words while that of t he Hindi sen-tences is 26 words. The vocabulary of the English corpus is 21 761 words and the vocabulary of the Hindi corpus is 27221 words. The token/ type ratio of English in the dataset is 13.9 and that of Hindi is 11.8. Here i s a sample sentence pair from the corpus.  X  X ity palace is a magnificent structure, the palace occupies one seventh of the walled city of Jaipur and is a wonderful blend of Rajput an d Mughal architecture.  X  X itI/ city pElesa/ palace viSAla/ magnificient saMracanA/ structure hEM/ is , mahala/ palace jayapura/ Jaipur Sahara/ city kI/ of eka battA sAwa xIvAra/ wall ko GerawA/surrounds hE Ora/ and mugala/ Mughal waWA/ and rAjapUwa/ Rajput sWApawya kalA/ architecture kA/ of ArScajanaka/ wonderful milana/ blend hEM/ is . 6 of 11,300 sentence pairs, (2) development set of 500 sentenc e pairs, and (3) test set of 500 sentence pairs. Tables I and II contain the tar get language statistics of the dataset considered with Hindi and English as target languages respectively. The n -gram target language model ( n = 5, for our experiments) is constructed using the Hindi portion of training set of 11,30 0 sentences. 8: 14  X  S. Venkatapathy and S. Bangalore 5. RESULTS 5.1 Moses We compare our results with the translation accuracies (lex ical accuracy and BLEU score 7 ) of Moses 8 [Koehn et al. 2007]. Lexical accuracy measures the similarity between the unordered bag of words in the refe rence sentence against the unordered bag of words in the hypothesized trans lation. We report the F-score that is computed from the Recall and Precision me asures between the two bags of words. We believe that lexical accuracy measu res the fidelity of the lexical transfer from the source to target sentence, i ndependent of the syntax of the target language. It is worth noting that BLEU-1 metric (lim-iting BLEU metric to only unigrams, instead of the conventio nal four-gram overlap), a component of the BLEU metric essentially measur es the fidelity of lexical transfer. However, we choose to make this explicit t hrough the lexical accuracy metric. Also, tuning to improve the BLEU metric mig ht indirectly improve the lexical accuracy metric.
 system. We run Moses 9 with various source and target phrase lengths 10 on both the datasets, English-to-Hindi and Hindi-to-English . The models built us-ing the training sets are tuned using the development sets. T he tuning process optimizes the BLEU metric on the development set. The tuned m odel is used to test the system. Table III shows the results on both the test a nd development sets, and for both the datasets (English-to-Hindi and Hindi -to-English). 5.2 Bag-of-Words Model results of the bag-of-words model without handling repetit ions on the devel-opment set of the English-Hindi dataset. To determine the be st  X  (threshold) value, we obtain the F-score, Recall and Precision values fo r various probabil-ity thresholds ranging from 0 to 0.5 (see Figure 11). The best lexical accuracy is 0.609 at a threshold of 0.25, and the number of tokens predi cted are 10,572 words. When the repetition of target words is modeled, the le xical accuracy improves further. The results of the bag-of-words model wit h repetitions is presented in Figure 11. The best lexical accuracy is 0.636 at a threshold of 0.25. The number of predicted tokens are 11,913 words. It is i nteresting to note that we obtained a much higher lexical accuracy than the tuned model of Moses (see Table III). Another observation was that our mo del was able to handle repetition of words quite well. For the rest of the exp eriments, repeti-tions are always modeled. Also, for training the models, a co unt cut-off of 2 is imposed on source language features. results of the bag-of-words model on the Hindi-English data set. The best lexi-cal accuracy is 0.644 at a threshold of 0.25, and the number of tokens predicted are 9,668 words (see Figure 12). When the repetitions of targ et words is mod-eled, the best lexical accuracy is 0.680 at a threshold of 0.2 6 (see Figure 12). The number of predicted tokens are 11,140 words. The lexical accuracy of the bag-of-words model was higher than Moses on the Hindi-Engli sh dataset as well (see Table III). words obtained through global lexical selection to constru ct the target lan-guage strings. We considered a permutation window of 10 for e xploring the various possible target language strings. While reranking these permutations using the language model, some of the noisy words from the bag can be deleted by setting a deletion cost (  X  ). We experimented with various deletion costs, and tuned it to optimize the BLEU score on the development set. Fi gure 13 shows 8: 16  X  S. Venkatapathy and S. Bangalore the best BLEU scores obtained by reordering the bags associa ted with vari-ous threshold values on the English-Hindi dataset. Figure 1 3 also shows the BLEU scores at various deletions costs by considering the th reshold at which BLEU score was maximum.
 English-Hindi corpus are 0.1469 and 0.636 respectively at a threshold of 0.26 and a deletion cost of 21. We use these parameter settings to t ranslate the test set. On the test set, we obtained a BLEU score of 0.1547 and a le xical accuracy of 0.631. It is interesting to note that despite the tuning of the BLEU metric, Moses X  lexical accuracy on the development and test sets is l ower than that of the bag-of-words model. This might suggest that the BLEU m etric tuning process emphasizes improving lexical reordering over lexi cal selection. Also, our observation from Table III suggests that there may be met hods to combine the lexical selection from the bag-of-words model with Mose s to improve the results further. lexical accuracies on development set of the Hindi-English are 0.1483 and 0.643 respectively at a threshold of 0.25 and a deletion cost of 19. On the test set, we obtained a BLEU score of 0.1895 and a lexical accu racy of 0.676. We observe a similar trend in terms of comparison to Moses as w e did for the English-Hindi dataset. the words in the target language, but does not perform as well as Moses when the order of the words is considered. As the bag-of-word s is a word-based model, it is interesting to compare its results with re sults from Moses restricted to phrase length of 1. The bag-of-words model giv es better lexical accuracy as well as better BLEU score in such a comparison. 5.3 Sequential Lexical Choice Model model, the predicted target language words are associated w ith the source positions. Figure 15 presents the lexical accuracies of the words obtained us-ing the sequential lexical choice at various thresholds and different fertilities. Fert = 2 means that there could be two words associated at a sou rce position. 8: 18  X  S. Venkatapathy and S. Bangalore is 0.591 at a threshold of 0.26. The best lexical accuracy fro m Hindi to English is 0.631 at a threshold of 0.28 and fertility of 3. If the ferti lity is restricted to 1, the best lexical accuracy from Hindi to English is 0.611. BLEU score on the English to Hindi development set is 0.1974 a t a thresh-old of 0.24, fertility of 3 and deletion cost of 15. The lexica l accuracy for this parameter set is 0.608. The BLEU score and lexical accuracy o n the test set are 0.2054 and 0.607 respectively.
 model in terms of BLEU score, as expected. However, its BLEU s core is still slightly less than the higher order models of Moses. 5.4 Bag-of-LWGs Model In these experiments, we compute the prediction accuracy of target local-word-groups instead of words. The prediction of a target local-wo rd-group is correct only if the entire local word group is predicted correctly. E ach local-word-group contains one content word and its associated function words . In Hindi which is a relatively free word order language, the function words associated with the content words convey the grammatical role of the content words to a large extent. Hence, predicting the local word groups directly in stead of words is a useful task.
 local-word-groups and (c) local-word-groups -No determin er. Figure 17 shows the accuracies of prediction of local-word-groups in Hindi with words as the features in English. The best local-word-group accuracy we obtain is 0.580 at a threshold of 0.23. The number of local-word-groups predic ted is 7,839 while the number of local-word-groups in the reference set is 10,0 74.
 compute the lexical accuracy of the words. This metric is com parable to the simple bag-of-words model where words are predicted direct ly. Figure 17 shows the accuracies of prediction of words in Hindi using th e bag-of-LWGs model. The best lexical accuracy is 0.601 at a threshold of 0. 22. The number of tokens predicted is 9,442 words. The tokens in the referen ce set are 12,999. The lexical accuracy is slightly less than the bag-of-words model, but the ad-vantage of this model is that the function words are associat ed with the content words, which should lead to easier interpretation of the out put.
 on the three types of source language features. A key observa tion here is words as features give the best lexical accuracies when compared t o local word groups as source language features. This might be due to the limited corpus available for training.
 and 0.447 respectively at a threshold of 0.14 and a deletion c ost of 19. On the test set, the BLEU score and the lexical accuracy were 0.2212 and 0.588 re-spectively. As expected, the association of function words with content words before reordering is really helpful. These results demonst rate that linguisti-cally derived multi-word units such as LWGs can have a signifi cant impact in improving the accuracy of the translation. While Moses uses multi-word units, also known as phrases, these units are not derived based on li nguistic struc-ture. We contrast the LWG-based model X  X  accuracy against th at obtained using the Moses system, limited to 3 11 word source and target phrases in Table III. 5.5 Factored Sequential Model In this model, the goal is to predict various factors of targe t language local-word-groups independently and then combine them using the t arget language morphological generator. We consider four factors of the lo cal word groups for our experiments: (a) content word lemmas, (b) post-posi tions (equivalent of preposition in English), (c) auxiliary sequences, and (d ) number (plural or 8: 20  X  S. Venkatapathy and S. Bangalore singular). The motivation for the factored model is similar to the one for Moses. The attempt here is to alleviate sparseness due to limited co rpus and pre-dict the features of the target word first from which the surfa ce word form is synthesized.
 we use a local word grouper and a morphological analyzer to ob tain the four factors for Hindi sentences. For example, consider the sent ence in Hindi,  X  X acche thakh gaye. X  Here the local word groups are  X  X acche X  / boys and  X  X hakh gaye X  became-tired . The four factors of each of these two local word groups are (bachhaa, -, -, plural) and (thakh, -, ROOT gaye, plural). We first present the bag-of-words accuracy and sequential lexical choice accur acies of each of the factors.
 0.29 (see Figure 19). When the sequential lexical choice mod el is considered, the best lexical accuracy are 0.616 and 0.598 for fertilitie s 3 and 1 respectively. The accuracies are better than the accuracies for predictin g local word groups. An accurate association of various factors with each other u sing a sequential model will not only produce good local word groups but also a m ore accurate word sequence. The best lexical accuracies for the individu al factors and the conjoined factors is presented in Table V.
 racy of the bag-of-words model. The second column shows the l exical accuracy when the target words are associated to the source positions using the sequen-tial lexical choice model. The third column reports the resu lts when NULL information is also modeled and considered during evaluati on. Here, the un-informative factors are considered as NULL. For example, he re is a sample of the outputs produced by the three models for the post-positi on factor. For the sequential lexical choice model, the words are prefixed with source positions. For  X 2:meM X  conveys that the word  X  X eM X  is associated with th e second source position. (1) bow : meM ke vAlI (2) seq : 2:meM 3:ke 8:vAlI (3) bow-null : X X meM X X X X ke vAlI X X conjoined feature, Conjoin3, groups the post-position, au xiliary, and number information together. A sample output of such a model is:  X  X e-X-sg meM-X-sg vAlI-X-sg  X  X ara-X-sg ke-X-sg kI-X-sg X-hE-sg Here, X denotes the lack of any information for that particul ar factor. not present these experiments in this article. 5.6 Summary of Results In this section, we present a summary of the results of all the models. Tables VI and VII contains the results on English-Hindi and Hindi-Eng lish datasets respectively.
 8: 22  X  S. Venkatapathy and S. Bangalore 6. CONCLUSION In this article, we presented discriminative models for mac hine translation that decouple the steps of lexical selection and lexical reo rdering with the aim of minimizing the role of word-alignment in machine transla tion. Indian lan-guages are morphologically rich and have relatively free-w ord order where the grammatical role of content words is largely determined by t heir case markers and not entirely by their positions in the sentence. Hence, l exical selection plays a far greater role than lexical reordering. The bag-of -words model per-formed very well in predicting lexical items but was not as go od as Moses at ordering them. The bag-of-LWGs (bag of local-word-groups) model associates content words and function words which is crucial in languag es such as Hindi. This in turn reduces the search space for ordering words ther eby improving the word order in a sentence. The bag-of-LWGs model gives encour aging results when compared with the state-of-art phrase based system whe n the phrase limit is restricted to 3, demonstrating that linguisticall y (LWGs in our case) derived multi-word units can result in improved translatio n. As future re-search, we plan to extend this model with linguistically mot ivated reordering models in contrast to the permutation-based models reporte d in this article. We would like to thank the Moses team for making the Moses tool available. We would also like to thank the Department of Information Tec hnology, Gov-ernment of India, and the English-Indian Languages Consort ium for providing the EILMT Tourism Corpus to us. We thank Karthik Gali, IIIT-H yderabad, for implementing the local-word-grouper for English and Hindi .

