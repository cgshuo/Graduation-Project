 Lixin Duan S080003@ntu.edu.sg Dong Xu DongXu@ntu.edu.sg Ivor W. Tsang IvorTsang@ntu.edu.sg In real-world applications, it is often expensive and time-consuming to collect the labeled data. Transfer learning (a.k.a., domain adaptation), as a new machine learning strategy, has attracted growing attention because it can learn robust classifiers with very few labeled data from the target domain by leveraging a large amount of labeled data from other existing domains (a.k.a., source domains).
 Domain adaptation methods have been successfully used for different research fields such as natural lan-guage processing and computer vision ( Blitzer et al. , 2006 ; 2007 ; Daum  X e III , 2007 ; Duan et al. , 2010 ; 2012b ; a ; Wu &amp; Dietterich , 2004 ). However, all those methods assume that the data from different domains are represented by the same type of features with the same dimension. Thus, they cannot deal with the problem where the dimensions of data from the source and target domains are different, which is known as heterogeneous domain adaptation (HDA) ( Dai et al. , 2009 ; Yang et al. , 2009 ).
 In the literature, a few works have been proposed for the HDA problem. Dai et al. ( 2009 ) proposed to learn a feature translator between the source and target domains by assuming that the data from both domains share co-occurrence attributes (i.e., text data). The same assumption was also used in ( Yang et al. , 2009 ; Zhu et al. , 2011 ) for text-aid image clustering and classification. However, this assumption may not be well satisfied in many appli-cations such as the object recognition task where only visual features are used. Based on structural corre-spondence learning ( Blitzer et al. , 2006 ), two method-recently proposed to extract the so-called pivot fea-tures from the source and target domains, which is specifically designed for the cross-language text classi-fication task. And these pivot features are constructed by text words which have explicit semantic meanings. For more general HDA tasks, Shi et al. ( 2010 ) proposed a method called Heterogeneous Spectral Mapping (HeMap) to discover a common feature subspace by learning two feature mapping matrices as well as the optimal projection of the data from both domains, in which the valuable label information is not exploited. Harel and Mannor ( 2011 ) learned rotation matrices to match source data distributions to that of the target domain. However, this method does not use the valu-able training labels, either. Wang et al. ( 2011 ) used the class labels of the training data to learn the manifold alignment by simultaneously maximizing the intra-domain similarity and the inter-domain dissimilarity. By kernelizing the method in ( Saenko et al. , 2010 ), Kulis et al. ( 2011 ) proposed to learn an asymmetric kernel transformation to transfer feature knowledge between the data from the source and target domains. In this work, we propose a new method called Het-erogeneous Feature Augmentation (HFA) for heteroge-neous domain adaptation. Considering the data from different domains are represented by features with different dimensions, we first transform the data from the source and target domains into a common subspace by using two different projection matrices P and Q . Then, we propose two new feature mapping functions to augment the transformed data with their original features and zeros. With the new augmented feature representations, we propose to learn the projection matrices P and Q by using the standard SVM with the hinge loss function in a linear case. We also describe its kernelization in order to efficiently cope with the data with very high dimension. To simplify the nontrivial optimization problem in HFA, we introduce an intermediate variable H called as a transformation metric to combine P and Q . We then develop an alternating optimization algorithm to simultaneously solve for the dual problem of SVM and the optimal transformation metric H .
 We summarize the main contributions of this work:  X  The newly proposed augmented features in our  X  We simplify the nontrivial optimization problem  X  Promising results on two benchmark datasets In the remainder of this paper, we use the superscript to denote the transpose of a vector or a matrix. We define I n as the n  X  n identity matrix and O n  X  m as a n  X  m matrix of all zeros. We also define 0 as the n  X  1 column vectors of all zeros and all ones, respectively. The inequality a  X  b means that a i  X  b i for i = 1 , . . . , n . Moreover, a  X  b denotes the element-wise product between vectors a and b , i.e., a  X  b = [ a 1 b 1 , . . . , a n b n ] H is positive semidefinite.
 In this work, we assume there are only one source domain and one target domain. For some given class, we are provided with a set of labeled training samples { limited number of labeled samples { ( x t i , y t i ) | n the target domain, where y s i and y t i are the labels of the samples x s i and x t i , respectively, and y s i , y { 1 ,  X  1 } . The dimensions of x s i and x t i are d s and d respectively. Note that in the HDA problem, d s  X  = d t . 2.1. Heterogeneous Feature Augmentation Daume III ( 2007 ) proposed Feature Replication (FR) to augment the original feature space R d into a larger space R 3 d by replicating the source and target data for homogeneous domain adaptation. Specifically, for any data point x  X  R d , the feature mapping functions  X  s and  X  t for the source and target domains are defined as  X  s ( x ) = [ x  X  , x  X  , 0 d ]  X  and  X  t ( x ) = [ x  X  that it is not meaningful to directly use the method in ( Daum  X e III , 2007 ) for the HDA task by simply padding zeros to make the dimensions of the data from two domains become the same, because there would be no correspondences between the heterogeneous fea-tures in this case.
 To effectively utilize the heterogeneous features from two domains, we first introduce a common subspace for the source and target data for our HDA task, in which the heterogeneous features from two domains can be compared. We define the common subspace as R x t can be projected onto it by using two projection ly. Note that promising results have been shown by incorporating original features into feature augmenta-tion ( Daum  X e III , 2007 ; Pan et al. , 2010 ) to enhance the similarities between data from the same domain. Motivated by ( Daum  X e III , 2007 ; Pan et al. , 2010 ), we also incorporate original features in this work and then augment any source and target domain samples x feature mapping functions  X  s and  X  t as follows: After introducing P and Q , the data from two domains can be readily compared in the common subspace. It is worth mentioning that our newly proposed augmented features for the source and target samples in ( 1 ) can be readily incorporated into different methods (e.g., SVM and SVR), making these methods applicable for the HDA problem.
 In the next subsection, we take SVM with the hinge loss as a showcase of our H eterogeneous F eature A ugmentation method (HFA for short). As it is nontrivial to solve for the projection matrices P and Q in our learning problem, we simplify the optimization problem by introducing an intermediate variable H = [ P , Q ]  X  [ P , Q ] such that we only need to solve for H rather than P and Q . In this way, the common subspace becomes invisible to us, which is therefore referred to as latent common subspace in this work. 2.2. Proposed Method We define feature weight vector w = [ w  X  c , w  X  s , w  X  for the augmented feature space, where w c , w s and w t are also weight vectors that are defined for the common subspace, the source domain and the target domain, respectively. We then propose to learn the projection matrices P and Q as well as the weight vector w by minimizing the structural risk functional of SVM. Formally, we present the formulation of our HFA method for the HDA problem as follows: min where C &gt; 0 is a regularization parameter that regu-lates the loss on the training samples, and  X  p ,  X  q &gt; 0 are predefined to control the complexities of P and Q , respectively.
 To solve ( 2 ), we first derive the dual form of the inner optimization problem in ( 2 ) with respect to w , b,  X  s i and  X  t i . Specifically, we introduce dual variables {  X  ( 4 ), respectively. By setting the derivatives of the Lagrangian of ( 2 ) with respect to w , b,  X  s i and  X  t i zeros, we obtain the Karush-Kuhn-Tucker (KKT) con-ditions as: w =  X  With the KKT conditions, we arrive at the alternative optimization problem as follows: min where = [  X  s 1 , . . . ,  X  s n a vector of the dual variables, y = [ y 1 , . . . , y K is the derived kernel matrix.
 A straightforward solution to the optimization prob-lem in ( 5 ) would be to iteratively update one of the variables , P and Q by fixing the others. However, the dimension of the common subspace (i.e., d c ) must be given beforehand in this case, and it is nontrivial to determine the optimal d c . Observing that in the kernel matrix K P , Q in ( 5 ), the projection matrices P and Q always appear in the forms of P  X  P , P  X  Q , Q  X  P and Q  X  Q , we then replace these multiplications by defining an intermediate variable H = [ P , Q ]  X  [ P , Q ] R nite, i.e., H  X  0. With the introduction of H , we can throw away the parameter d c . Moreover, the common subspace becomes latent , because we do not need to explicitly solve for P and Q any more.
 With the definition of H , we convert the optimization problem in ( 5 ) to the final formulation of our proposed HFA method as follows: min where K H = L  X  p +  X  q . Note that given , the optimization problem in ( 6 ) becomes the following Semidefinite Program-ming (SDP) problem ( Vandenberghe &amp; Boyd , 1996 ) by defining =  X  y : Thus far, we have successfully converted our original HDA problem, which learns two projection matri-ces P and Q , into a new problem of learning a transformation metric H . We emphasize that this new problem has two main advantages: i) it avoids determining the optimal dimension of the common subspace beforehand; and ii) as the common subspace becomes latent after the introduction of H , we only have to optimize and H for our proposed method. Discussion: There are two major limitations to the current formulation of HFA in ( 6 ): i) The transforma-tion metric H is linear, which may not be effective for some tasks. ii) The size of H grows with the dimensions of the source and target data (i.e., d s and d ). Therefore, it is computationally infeasible to learn the linear metric H in the SDP problem ( 7 ) for some real-world applications (e.g., text categorization) with very high dimensional data. In order to effectively deal with high dimensional data, inspired by ( Kulis et al. , 2011 ), in the next subsection we will apply kerneliza-tion to the data from the source and target domains and show that ( 7 ) can be solved in a kernel space by learning a nonlinear transformation metric with its size independent of the feature dimension. 2.3. Nonlinear Feature Transformation Note that the size of the linear transformation metric H is proportional to the feature dimension, and thus it is computationally infeasible for data with a very high dimension. In this section, we will show that by applying kernelization, the transformation metric is independent of the feature dimension and grows only with the number of training data.
 As any arbitrary feature mapping function  X  can be used to derive a corresponding kernel space for the source and target data, we can replace their linear inner products with some kernel function k . Let us denote  X  s = [  X  ( x s 1 ) , . . . ,  X  ( x s n [  X  ( x t 1 ) , . . . ,  X  ( x t n target training data after mapping them into a non-linear feature space by using  X  , respectively. We also define K s =  X   X  s  X  s and K t =  X   X  t  X  t as the kernel matrices of the training data from the source and target domains, respectively. Moreover, we denote the corresponding projection matrices for the source and target data respectively as P  X  and Q  X  .
 Theorem 1. Assume K s and K t be positive de nite. There exist two matrices  X  P  X  R d c  X  n s and  X  Q  X  R d such that any feasible solution P  X  and Q  X  to the kernelized version of ( 2 ) can be written in the form of P Proof. The proof can be analogously derived as for Lemma 3.1 in ( Kulis et al. , 2011 ).
 With Theorem 1 , it is easy to verify that  X   X  P  X  2 F =  X  we apply the same trick as in Section 2.2 to avoid determining d c for the latent common subspace. That is, we define the nonlinear transformation metric  X  H = [  X  Algorithm 1 Heterogeneous Feature Augmentation Input: Labeled source samples { ( x s i , y s i ) | n s i =1 labeled target samples { ( x t i , y t i ) | n t i =1 } Initialization:  X   X  1,  X  H [  X  ]  X   X  n With  X  H [  X  ] , solve for [  X  ] in the inner optimization problem of ( 8 ) by using SVM; while  X  &lt; T max do end  X  H is independent of the feature dimension. We also have  X  H  X  0 and trace(  X  H )  X   X  p +  X  q =  X  . Therefore, the formulation of our proposed HFA method after applying kernelization becomes: min where K ~ H = [ , we also arrive at an SDP problem as follows by defining =  X  y : 2.4. Detailed Solution For our proposed HFA method, we develop an alter-nating optimization algorithm by iteratively updating Specifically, when updating at the  X  -th iteration, we fix  X  H [  X  ] and solve for [  X  ] in ( 8 ) by using the standard SVM with the kernel matrix K ~ H optimization in ( 9 ). The optimization procedure will be terminated when the value of the objective function in ( 8 ) converges.
 In order to efficiently solve the SDP problem in ( 9 ), we also develop a simple projected gradient descent method to update  X  H . Let us define s = [  X  1 , . . . ,  X  objective function of ( 9 ), we first obtain the derivative of G (  X  H ) with respect to  X  H as follows: Then at the  X  -th iteration,  X  H will be updated by using the following equation: where  X  [  X  ] is the step size at the  X  -th iteration, which can be found by using the standard line search method ( Boyd &amp; Vandenberghe , 2004 ).
 We summarize the proposed alternating optimization algorithm for HFA in Algorithm 1 . After obtaining the optimal solution and  X  H to ( 8 ), we can predict any test data point x from the target domain by using the following target decision function: f ( x ) = w  X   X  ( x ) + b where k t = [ k ( x t 1 , x ) , . . . , k ( x t n samples x i and x j with the same feature dimension. The pioneer works ( Dai et al. , 2009 ; Prettenhofer &amp; Stein , 2010 ; Wei &amp; Pal , 2010 ; Yang et al. , 2009 ; Zhu et al. , 2011 ) are limited to some specific HDA tasks, because they required additional information to transfer the source knowledge to the target domain.
 To handle more general HDA tasks, other methods have been proposed to explicitly discover a com-mon subspace ( Shi et al. , 2010 ; Wang &amp; Mahadevan , 2011 ). Shi et al. ( 2010 ) proposed to learn feature mapping matrices without using the valuable data label information. While Wang et al. ( 2011 ) used the class labels of data, they assumed the data should have a manifold structure. Such manifold assumption may not exist in real-world applications.
 Recently, Kulis et al. ( 2011 ) proposed a nonlinear metric learning method to learn an asymmetric feature transformation for the source and target data with high dimensions. And the learned transformation metric is universal for all classes. However, when there exist many classes, a universal metric may not be suf-ficiently good for feature transformation between data from all classes. In contrast, our method incorporates the proposed augmented features into SVM to learn an individual model for each class.
 In this section, we evaluate our proposed HFA method for object recognition and multilingual text catego-rization. We focus on the heterogeneous domain adaptation problem where there exist only one source domain and one target domain in which only a limited number of labeled target training samples are avail-able. Moreover, we assume that the test data from the target domain are unseen during the training phase. 4.1. Setup Object recognition: We employ a recently released dataset 1 used in ( Saenko et al. , 2010 ; Kulis et al. , 2011 ) for this task. This dataset contains a total of 4106 images with 31 categories from three sources: amazon (web images downloaded from an online mer-chant), dslr (high-resolution images taken from a digital DLR camera) and webcam (low-resolution im-ages taken from a web camera). We follow the same protocols in the previous work ( Kulis et al. , 2011 ). Specifically, SURF features ( Bay et al. , 2006 ) are ex-tracted for all the images. The images from amazon and webcam are clustered into 800 visual words by using k-means. After vector quantization, each image is represented as a 800 dimensional histogram feature. Similarly, we represented each image from dslr as a 600-dimensional histogram feature.
 In the experiments, dslr is used as the target do-main, while amazon and webcam are considered as two individual source domains. We strictly follow the setting in ( Saenko et al. , 2010 ; Kulis et al. , 2011 ) and randomly select 20 (resp., 8) training images per category for the source domain amazon (resp., webcam ). For the target domain dslr , 3 training images are randomly selected from each category, and the remaining dslr images are used for testing. See Table 1 for a summarization of this dataset. Text categorization: We use the Reuters multilin-gual dataset 2 ( Amini et al. , 2009 ), which is collected by sampling parts of the Reuters RCV1 and RCV2 collections. It contains about 11K newswire articles from 6 classes in 5 languages (i.e., English , French , German , Italian and Spanish ). While each document was also translated into the other four languages in this dataset, we do not use the translated documents in this work. All documents are represented as a bag of words and the TF-IDF are extracted.
 We take Spanish as the target domain in the experi-ments and other four languages as individual source domains. For each class, we randomly sample 100 training documents from the source domain and m training documents from the target domain, where m = 5 , 7 , 10 , 15 and 20. And the remaining doc-uments in the target domain are used as the test data. Note that the method ( Wang &amp; Mahadevan , 2011 ) cannot handle the original high dimensional TF-IDF features. In order to compare our HFA method with theirs ( Wang &amp; Mahadevan , 2011 ), for documents written in each language, we perform PCA with 60% energy preserved on the TF-IDF features. We summarize this dataset in Table 2 .
 Baselines: As the source and target data have differ-ent dimensions, they cannot be directly combined to train any classifiers for the target domain. Considering the number of training samples is much lower than the feature dimension, we compare our HFA method by applying kernelization with a number of baseline algorithms listed below:  X  SVM T: It utilizes the labeled samples only  X  KCCA ( Shawe-Taylor &amp; Cristianini , 2004 ):  X  HeMap ( Shi et al. , 2010 ): It finds the projec- X  DAMA ( Wang &amp; Mahadevan , 2011 ): It  X  ARC-t ( Kulis et al. , 2011 ): It uses the labeled For KCCA, HeMap and DAMA, after learning the projection matrices, we apply SVM to train their final classifiers by using the projected training data from both domains for a given category/class. For ARC-t, we construct the kernel matrix based on the learned asymmetric transformation metric, and then SVM is also applied to train its final classifier. For all methods, we set the regularization parameter C = 1 in SVM and use the RBF kernel for fair comparison. As we only have a very limited number of labeled training samples in the target domain, the cross-validation technique cannot be effectively employed to determine the optimal parameters. Instead, for our HFA method, we empirically fix the parameter  X  as 100 for the object dataset and 1 for the Reuters multilingual dataset. For other methods, we validate all their parameters chosen report their best results.
 Evaluation metric: Following ( Kulis et al. , 2011 ), for each method we measure the classification accuracy over all categories/classes on both datasets. We randomly sample the training data for ten times and report the mean classification accuracies of all methods over the ten rounds of experiments. 4.2. Classi cation Results Object recognition: We report the mean and s-tandard deviations of classification accuracies for all methods on the object dataset ( Saenko et al. , 2010 ) in Table 3 . From the results, SVM T outperforms KCCA and HeMap by using only 3 labeled training samples from the target domain. The explanation is that KCCA and HeMap do not utilize the label information of the target training data to learn the feature mapping matrices. As a result, the learned common subspace is not sufficient to preserve a similar data structure as in the original feature spaces of the source and target data, which results in poor classification performances. DAMA performs only slightly better that SVM T, possibly due to the lack of  X  2 . 4 53 . 3  X  2 . 3 53 . 1  X  2 . 4 (53 . 2) 55 . 4  X  2 . 8  X  2 . 6 53 . 2  X  3 . 2 53 . 0  X  3 . 2 54 . 3  X  3 . 7  X  3 . 1 72 . 4  X  2 . 4 72 . 9  X  2 . 0 75 . 3  X  1 . 7  X  4 . 2 72 . 8  X  2 . 0 73 . 5  X  1 . 8 75 . 7  X  1 . 6  X  3 . 6 72 . 9  X  2 . 3 74 . 7  X  1 . 6 76 . 1  X  1 . 5  X  2 . 3 73 . 3  X  2 . 1 74 . 0  X  2 . 0 75 . 8  X  1 . 8 the strong manifold structure on this dataset. Both re-sults of ARC-t implemented by ourselves and reported in ( Kulis et al. , 2011 ) are only comparable with those of SVM T, which shows that ARC-t is less effective for HDA on this dataset. Our HFA method outperforms the other methods under both settings, which clearly demonstrate the effectiveness of our proposed method for HDA by learning with augmented features. Text categorization: Table 4 shows the mean and standard deviations of classification accura-cies for all methods on the Reuters multilingual dataset ( Amini et al. , 2009 ) by using m = 20 labeled training samples per class from the target domain. We have a similar observation as on the object dataset that SVM T still outperforms HeMap in terms of classification accuracy. It is interesting to observe that KCCA is generally better than SVM T, which shows that it can learn a good common feature subspace on this dataset. Moreover, by using the label information, both DAMA and ARC-t perform better than SVM T under almost all the settings. Our proposed HFA method still achieves significantly better performances than others on this dataset, when judged by the t-test with a significance level at 0.05.
 We also plot the classification results of SVM T, KC-CA, DAMA, ARC-t and our HFA method with respect to the number of target training samples per class (i.e., m = 5 , 7 , 10 , 15 and 20) for each source domain in Figure 2 . We do not report the results of HeMap, as they are much worse than the other methods. From the results, the performances of all methods increase when using a larger m . And the two HDA methods DAMA and ARC-t generally achieve better mean classification accuracies than SVM T except for the setting using English as the source domain. Our HFA method generally outperforms all other methods according to mean classification accuracy. 4.3. Convergence Analysis To analyze the convergence of the proposed Algorith-m 1 for our HFA method, we take one setting from each of the datasets as the showcase. For the object dataset, we use the category  X  X ack pack X  and the source domain amazon ; for the Reuters multilingual dataset, the class  X  X 15 X  is used together with the source domain English . From the results, Algorithm 1 generally takes less than 80 (resp., 40) iterations before its convergence on the object dataset (resp., the Reuters multilingual dataset). We have similar observations for other categories/classes on the two datasets as well. We have proposed a new method called Heterogeneous Feature Augmentation (HFA) for heterogeneous do-main adaptation. In HFA, we augment the heteroge-neous features from the source and target domains by using two newly proposed feature mapping functions, respectively. With the augmented features, we propose to find the two projection matrices for the source and target data by using the standard SVM with the hinge loss in both linear and nonlinear cases. Moreover, a so-called transformation metric is introduced to simply our formulated optimization problem of HFA such that it can be effectively solved by our developed alternating optimization algorithm. Promising results of HFA have been achieved on two benchmark datasets for object recognition and text classification. This work is supported by the Singapore National Research Foundation under its Interactive &amp; Digital Media (IDM) Public Sector R&amp;D Funding Initiative (administered by the IDM Programme Office) and AcRF Tier-1 Research Grant (RG15/08).

