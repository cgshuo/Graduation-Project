 A fundamental task in Information Retrieval (IR) is term weighting. Early IR theory considered both the presence or absence of all terms in the lexicon for ranking and needed to weight them all. Yet, as the size of lexicons grew and models became too complex, common weighting models preferred to aggregate only the weights of the query terms that are matched in candidate documents. Thus, unmatched term contribution in these models is only considered indirectly, such as in probability smoothing with corpus distribution, or in weight normalization by document length. In this work we propose a novel term weighting model that directly as-sesses the weights of unmatched terms, and show its ben-efits. Specifically, we propose a Learning To Rank frame-work, in which features corresponding to matched terms are also  X  X irrored X  in similar features that account only for un-matched terms. The relative importance of each feature is learned via a click-through query log. As a test case, we con-sider vertical search in Community-based Question Answer-ing (CQA) sites from Web queries. Queries that result in viewing CQA content often contain fine grained information needs and benefit more from unmatched term weighting. We assess our model both via manual evaluation and via auto-matic evaluation over a clickthrough log. Our results show consistent improvement in retrieval when unmatched infor-mation is taken into account. This holds both when only identical terms are considered matched, and when related terms are matched via distributional similarity.

One of the fundamental tasks in Information Retrieval (IR) is term weighting, which refers to the assessment of a weight for each term appearing in the document collection, and similarly in the input query. Early Probabilistic IR the-ories considered the presence or absence of all terms in the lexicon for ranking, both in the query and the documents [29, 26]. However, term weights in these models were found difficult to compute, and the main line of research around weighting models chose to consider mainly the weights of the query terms that are matched in candidate documents. In-deed, weighting schemes such as TF-IDF [31], BM25 [28] and statistical language models [33, 24, 40], as well as Learning To Rank (LTR) methods [20], are primarily based on con-sidering the contribution of the matched terms , those query terms that appear also in the document. Though unmatched terms , i.e. terms that appear only in the query or only in the document, are not completely ignored, they are considered indirectly in these models, such as by using the document length for weight normalization or via corpus-based smooth-ing of maximum likelihood estimations.

As suggested by early probabilistic models we argue that analyzing directly unmatched terms may provide additional cues to the relevance of a candidate document to the query. Indeed, while the contribution of stop-words, such as de-terminers and modals, can be largely ignored, unmatched named entities are strong indicators of semantic differences between the query and the document. For example, for the query  X  most deadliest snake  X , the document title  X  where can I find a list of the deadliest snakes  X  is more relevant than  X  which is the most deadliest snake in Russia  X , though the first title is longer than the second, and second title con-tains all of the query terms in the right order.

Another intuition regarding direct modeling of unmatched terms refers to the percentage of query terms that are cov-ered in the document. We would like to explicitly indicate that for two queries, a short one and a long one, if both match the same set of terms within a candidate document, this document is likely to be of less relevance for the longer query, which contains more unmatched terms, compared to the shorter one. As an example consider the queries  X  most deadliest snake  X  and  X  most deadliest snake in Russia  X  and the candidate document  X  where can I find a list of the dead-liest snakes  X . We would like to explicitly express the lack of relevance of the document to the second query due to unmatched query terms.

We expect the subtleties between different types of un-matched terms to show especially for Web queries with fine-grained information-need. Therefore, we focus in this paper on Web queries with question intent, which constitute  X  10% of the Web queries issued to a search engine [36]. Exam-ples for such Web queries are those resulting in the searcher clicking on a question page belonging to Community-based Question Answering (CQA) sites, such as Yahoo Answers, StackExchange and Quora, and are called here CQA queries . Our retrieval scenario is vertical search [25, 2], in which con-tent of a CQA sub-collection should be retrieved on top of general Web search.

In this work, we address this vertical search task by in-troducing a term weighting model that directly considers the contribution of unmatched terms for ranking. However, instead of a probabilistic framework, we utilize LTR as a ranking framework. To this end, we employ a large set of state-of-the-art features, which capture various attributes of matched terms, both statistical ones (such as variants of TF-IDF) as well as syntactic ones (such as Part-Of-Speech (POS) tags) [20, 9]. We then design  X  X irror X  features that evaluate similar attributes, but for the unmatched terms. These novel mirror features are provided, together with the features that correspond to matched terms, as input to an LTR algorithm, which learns the relative weights of the dif-ferent features using click-through training data.

Prior work in document ranking noted that, occasionally, different terms in the query and the document actually con-vey related meanings or even the same meaning ( e.g.  X  guy  X   X   X  man  X ,  X  drink  X   X   X  alcohol  X ) and should be considered as matching for document ranking. One common approach to handle this lexical gap is via translation models, which in-clude some similarity measure between query and document terms as part of term matching [4, 17, 39, 14]. In order to analyze the contribution of our unmatched term modeling under such  X  X oft matching X  schemes, we introduce a  X  X oft X  variant of all the features we compute for matched terms in our LTR framework, which is based on distributional simi-larity between terms. We then provide a similar soft variant of our unmatched-term features that complement the soft matched-term features. This should enable the unmatch-based features to better account for only semantically un-matched terms instead of terms that have similar meanings.
We conducted experiments on a vertical search setting that searches a Web query over a large collection of ques-tion pages from Yahoo Answers. The contribution of our unmatch-based features for term weighting was evaluated under two setups: a) large-scale automatic evaluation over a click-through query log; b) manual evaluation of the top re-trieved documents for a set of tested queries. We compared our model to a state-of-the-art LTR model that utilizes only features that correspond to matched terms. The tested mod-els were assessed both under the exact matching modeling, in which only identical terms are considered matched, and the soft matching modeling, where terms may be partially matched via distributional similarity. Our novel features provided consistent improvement in document ranking on both scenarios, showing the benefit of directly considering unmatched terms for term weighting.
Unmatched terms were addressed in prior IR ranking mod-els in different ways, both for general search and for CQA search. We distinguish between two types of unmatched terms: a) terms that appear in the candidate document but not in the query, denoted as excessive terms ; b) terms that appear in the query but not in the candidate document for ranking, denoted as missing terms .

Probabilistic information retrieval theory accounts for pres-ence or absence of all terms in the lexicon, both in the query and in a candidate document for ranking [29]. Similarly, early reformulation of language models for IR (LMIR) [26] considered the query as a set of words, and modeled ex-cessive terms in the document by their ability to generate terms not in the query. However, term weighting compu-tation becomes a difficult problem under these frameworks [30, 33, 40], especially when no relevance feedback is con-sidered. Therefore, recent ranking models, and specifically term weighting models, focus mainly on the matched terms between the query and a candidate document.

The overall ranking score of a document is typically the sum of the weights of the terms in the document that match (to some extent) the query terms. Therefore, document term weights in popular weighting schemes are non-negative and the effect of missing terms in a candidate document is con-sidered indirectly by not contributing their weights to the document ranking score. This is the case in common prob-abilistic and vector space models, such as Binary Indepen-dence Model (BIM) [30], TF-IDF [31], Okapi BM25 [28], divergence from randomness [1], and multinomial language models, which view the query as a sequence of terms [33, 24, 41, 40]. Specifically, language models were extensively explored for CQA retrieval and were extended in different ways to incorporate meta data like categories [7], and the question focus and topic [13]. The same principle of scor-ing documents by summing matching term weights is also behind different weighting terms at the query side [3, 43].
In another line of ranking research, Learning to rank (LTR) approaches [20, 19] were introduced for learning to com-bine many features in a supervised way. Various learning algorithms were proposed, such as SVMRank [8] and Lamb-daMart [38], which may assign negative weights to some features. Still, the features themselves are typically derived for the matched terms, and therefore LTR algorithms learn the relative contribution of each feature with respect to the matched terms. Most derived features are statistical in na-ture, such as variants of term frequency and document fre-quency scores [20], and were also utilized in supervised rank-ing models in CQA [18, 37]. Carmel et al. [9] showed that utilizing features derived from syntactic analysis of the doc-ument title improves ranking performance for CQA queries. In a related task of answer sentence ranking within the field of Question Answering, tree kernels that incorporate seman-tic and syntactic features of the words provide state-of-the-art performance [34]. Still, the overall approach weigh in the number of matched sub-trees but not the unmatched ones.
Missing terms, which appear in the query but not in the document, received considerable attention within attempts to address the lexical gap problem: improving the match-ing between query and document terms that are not lexi-cally identical but convey similar meaning. One common approach incorporates a translation model as part of term weighting [4, 17]. This approach was found useful also for retrieving CQA content, where translation models were used within LMIR for retrieving related questions [16, 39, 42] as well as for ranking CQA documents for Web queries [37]. Recently, lexical semantic similarity between terms via dis-tributed representations, such as word2vec [23], was found helpful in several IR tasks, including query term weighting [43] and as features in a LTR framework for answer retrieval [10]. Ganguly et al [14] employed similarity between word embedding vectors within a translation model for LMIR as means to overcome the lexical gap between queries and doc-uments, where it outperformed a language model extended with latent topics. Figure 1: Sum of IDF values (normalized) for the matched, excessive and missing terms, computed separately over all non-clicked documents ranked at positions 1 to 20 (the three plots) and over the clicked documents (three horizontal lines).

Prior work captured the effect of excessive terms (appear-ing only in the document) on the ranking score mainly by their contribution to overall document context or structure. Many vector space and probabilistic models ( e.g. TF-IDF, BM25, language models) utilize the document X  X  length as a degrading parameter for term weights, e.g. as the de-nominator of maximum likelihood estimation or in ` 2 vector normalization. Other models include all document terms when modeling a global latent representation for each doc-ument. One line of works uses latent topics ( e.g. LDA [5]) as additional smoothing elements within LMIR [35]. This extension was shown useful also in retrieval of related CQA questions [6]. Another approach is to embed the document in a latent space. Latent Semantic Indexing [12] utilizes SVD to represent documents and queries within a reduced dimension space based on the main singular values of a term/document co-occurrence matrix. Lately, deep learn-ing was shown useful for ranking by embedding the query and document texts into a shared latent space, within Web search [15] and within Question Answering [32]. Such ap-proaches were not evaluated under the CQA vertical search setting yet, whose query length distribution and query at-tributes is quite distinguishable from general Web search and from question/answer datasets [9, 36].
In this work we perform our analysis and experiments on a large document collection taken from Yahoo Answers. Ya-hoo Answers is a popular CQA website containing questions about diverse topics, such as sports, healthcare, politics, sci-ence and many others. Each question page in the site con-sist of: a) a title, which is typically a short summary of the question, b) a body, containing a detailed description of the question, and c) all the answers provided for this question. We collected 54 million question pages from Yahoo Answers (referred to as our corpus ) and indexed them using Lucene
We also randomly sampled Web queries that were issued to a popular search engine and resulted in a click on one of the pages in our corpus by analyzing the search log . For each sampled query the top 100 results from our corpus were re-lucene.apache.org Figure 2: POS tagging and dependency parse tree for the question Can someone suggest fun party games? . The upper label of each token is its POS tag and the lower label is its syntactic role. trieved using Lucene X  X  BM25 ranking function over all fields (title, body, answers). We retained the set of queries for which the clicked page for the query (as extracted from the search log) was found among the top 100 Lucene results. After this process, our click-based query collection consists of 136,000 queries.

Since search-engines show mostly the title of a question page on the search result page, Carmel et al. [9] reason that the relevance of the title to the query is one of the main rea-sons for a user to click on the page. This especially makes sense as the title is often a good summary of the question in the page. Furthermore, both title and query are concise and usually do not contain redundant information. There-fore, we expect that unmatched term weighting would help in retrieval under this scenario. Following them [9], we an-alyze and model unmatched terms only between the title of a Yahoo Answers question page and the target query.
To further motivate our modeling approach we analyze the properties of the unmatched terms: terms that appear in the candidate document but not in the query ( exces-sive ), and terms that appear in the query but not in the candidate document ( missing ). To this end, we sampled 20,000 queries 2 from our query collection and examined the matched and unmatched terms between each query and the titles of the retrieved documents (using Lucene). We com-pared the analysis statistics between documents that were clicked by the user who issued the query, denoted as clicked documents, and the other top retrieved documents, denoted as non-clicked documents.
First we examined the distribution of IDF values among the matched and unmatched terms. To this end, we com-puted the sum of IDF values for the matched terms and the excessive terms in each title (normalized by the title length). We also computed the IDF sum for the query X  X  missing terms (normalized by the query length). These three indicators are plotted in Fig. 1, where each point in the plot repre-sents the value averaged over all non-clicked titles ranked at the i X  X h position by the BM25 ranking, for i=1..20 . The three horizontal lines in Fig. 1 correspond to the values of the three indicators averaged over the clicked documents. Note, higher values intuitively reflect more relevance in the matched term curve but less relevance in excessive and miss-ing term statistics.
Taken from our training set  X  see Sec. 5.4 Figure 3: Probability of a term in the title not to match any query term given its POS tag (top chart) or syntactic role (bottom chart)
The matching-term IDF values (the diamond shape points) indicate that the clicked document (the horizontal line) is comparable, on average, only to the document at the 4 0 position. This means that, with respect to matched terms, top ranked non-clicked documents usually contain as much and even more IDF volume compared to the clicked docu-ment. Yet, excessive and missing term analyses reveal com-plementing phenomena. First, on average, only non-clicked documents that are ranked first have the excessive term indi-cator lower than the value for clicked documents. This may indicate that while several non-clicked documents contain  X  X mportant X  (high-IDF) query terms in their title, driving them to high ranking positions, they also contain additional  X  X mportant X  terms that do not appear in the query and may change the meaning of the title compared to the query.
Second, the indicator for missing terms in clicked docu-ments stands out even more, as it is lower compared to all non-clicked documents. This could indicate that in CQA queries it takes more than one term to express the gist of the information needed. While some non-clicked documents may include in their title high-IDF query terms, which cor-respond to  X  X mportant X  terms, they also tend to leave-out more  X  X mportant X  terms compared to the clicked document.
Assuming that click analysis is a useful approximation of relevance analysis, these results suggest that matched term statistics reveal only some aspects of the title X  X  relevance to a searcher X  X  information need. More relevance aspects may be further exposed by explicitly modeling unmatched terms.
Carmel et al. [9] showed that document terms with differ-ent syntactic properties should be weighted differently for retrieval. Hence, we examine similar syntactic properties of excessive terms, namely POS tags and dependency roles.
To this end, all titles in our corpus were syntactically an-alyzed using the Stanford parser 3 under the  X  X ll typed de-pendencies X  setting. Then, for each title term in a retrieved document we extracted its POS tag and syntactic role (the http://nlp.stanford.edu/software/lex-parser.shtml dependency relation in which the term is the dependent). Fig. 2 presents an example for this analysis. Finally, for each syntactic property we counted its total occurrences in each title and its occurrences within excessive terms, and com-puted their ratio. The ratio of these two counts represents the probability of each syntactic property to be an excessive term. In Fig. 3 we depict two probability families, averaged across all analyzed queries: a) for all clicked documents; and b) for the three highest ranking non-clicked documents (representing the  X  X oughest X  competitors to beat for rank-ing the clicked documents on top of non-clicked ones). For clarity, the charts contain only the results for the 15 most common POS tags and syntactic roles, and they are sorted in decreasing order of the probability value.

Looking at Fig. 3 we observe large differences in excessive term probability between different syntactic tags. For exam-ple, in clicked items, this probability for pronouns (PRP -usually a low IDF stopword) is  X  0.75, while for proper nouns (NNP) it is only  X  0.35. Such large differences echoes previ-ous observation [9] that there is a possible gain in modeling differently terms with different syntactic tags.

Comparing the statistics between clicked titles and the top non-clicked titles, we can see that in quite a few prop-erties there are distinguishable differences between clicked and non-clicked titles. These differences suggest that ex-cessive term weighting may improve if syntactic properties will be considered. As an example we look at verbs, which are important terms in a query (usually capturing the main activity asked about). The syntactic roles that are often associated with verbs in the bottom chart are  X  aux  X ,  X  cop  X ,  X  root  X  and sometimes  X  conj  X . These roles can be partitioned into two groups. The first group contains  X  aux  X  and  X  cop  X , whose excessive probability is higher in non-clicked titles. The second group contains  X  root  X  and  X  conj  X , which are more likely to be excessive in clicked titles. This shows that syn-tactic properties can provide more fine-grained distinctions between similar terms or even for the same term when as-suming different roles.

As another example, the charts in Fig. 3 also show that nouns (POS tags: NN*, Dep roles: conj, nsubj, dobj, pobj, nn) are more likely to be excessive in non-clicked documents. As nouns typically contain the main participants of a ques-tion, it is important to match all (or most) of them to align the exact semantics of the query to that of a title. There-fore, directly assessing both matched and unmatched nouns could improve retrieval. In Fig. 3 the only reverse case is with  X  conj  X , which is more likely to be excessive in clicked titles. Yet, conjunctive nouns, such as in the example  X  good websites that stream movies and tv shows  X , may be skipped (and become excessive terms) while maintaining the same semantic gist of the question.
 The analysis in this section suggests that modeling statisti-cal properties as well as syntactic properties of unmatched title terms may lead to better assessment of document rele-vance for CQA queries. We next explicitly construct features for unmatched terms, within a Learning To Rank (LTR) framework, which take these properties into consideration.
The task of our ranking algorithm is to rank a set of can-didate documents D given a CQA query q . We follow a stan-dard LTR scheme [19, 20, 9] and define a mapping function Table 1: Matched term features used in [9]; c ( t,X )  X  term frequency of t in X ; df ( t )  X  document frequency of t in our corpus C ; idf ( t ) = log | C | df ( t ) ; | X |  X  total number of terms in X ; POS ( t ) , CPOS ( t ) and SR ( t ) are the POS tag, coarse-POS tag and syntactic role of t respectively;  X  () is the indicator function.
  X  ( q,d )  X  R n from pairs of a query q and a candidate docu-ment d to the vector space R n . Our algorithm uses a weight vector w to compute a score for each d  X  D via the inner product s ( q,d ) = w  X   X  ( q,d ). Finally, the candidate docu-ments are ranked according to the value of s ( q,d ), where the higher the score for some document d , the higher its rank in the retrieved list. The goal of the learning algorithm is to find weights w such that more relevant documents will have high score compared to less relevant ones.
 Our work focuses in the design of a new feature mapping  X  ( q,d ) that captures both missing and excessive terms. We build on the work of Carmel et al [9] who proposed only fea-tures that consider matched terms and extend their mapping in two ways: (1) taking into account unmatched terms; (2) relaxing the notion of matched/unmatched terms, and al-lowing soft-matching between terms in the query and the candidate, based on distributional similarity.

We describe in Sec. 5.1 the state-of-the-art features pro-posed by Carmel et al [9], which are our baseline and starting point. Additionally, we present in Sec. 5.2 an abstraction of these features having in mind our goal to introduce their corresponding new features for unmatched terms. Finally, in Sec. 5.3, we incorporate soft-matching into all features presented until then. We use a previous weight learning scheme [9] (Sec. 5.4) in order to replicate their work as a baseline and have a fair comparison of our new features.
Carmel et al [9] also addressed the task of vertical search for CQA queries within an LTR framework (see Sec. 2). They proposed two types of features that analyze matched terms: standard statistical features [20], such as TF-IDF, and new syntactic-based features that collect matched term statistics for each POS tag and syntactic role separately. All these features are summarized in Tab. 1.

Carmel et al mainly analyzed the performance of the doc-ument title for matching the query, arguing that in CQA content, the title is a good summary of the question be-ing answered within the document. Therefore, all statistics are derived only from the document X  X  title, except for the BM25-related features ( H 1  X  2 ) which are computed over the whole document. Under this formulation, which we follow, q and d represent the list of terms in the query and the doc-ument X  X  title respectively. Features L 1  X  10 and H 1  X  3 standard statistical features, while G 1  X  4 ( p ) and G are feature families that are generated for each POS tag p and syntactic role sr , respectively. For example G 1 ( IN ) is the feature generated for the IN (preposition) POS tag and G ( root ) is the feature generated for the root syntactic role.
Each of the features L 1  X  10 and G 1  X  6 can be viewed more abstractly as (possibly conditional) term summing of a prod-uct of two terms: (1) a count (or a function of it) of some event, denoted by f F i ; and (2) a boolean predicate or a nu-meric value indicating some matching between q and d :
The matching indication part in L i is  X  ( t  X  d )  X  whether the query term appears in the document. The indication part of G i is c ( t,q )  X  the occurrence count of the title term in the query, which is 0 for unmatched. Examples for in-stantiating these abstractions with specific statistics are: a) for L 2 , f L 2 := log( c ( t,d ) + 1); and b) for G 1 , f cond G 1 := ( POS ( t ) = p ).
We now introduce our novel features, which induce the f F signals in parallel to their counterpart matched term features L 1  X  10 and G 1  X  6 . Yet, instead for the matched terms, the new feature families do so for the set of excessive terms, de-noted by EXL and EXG , and for the set of missing terms, denoted by MIL and MIG . We present the generic repre-sentations of these feature families similarly to (1) and (2): and,
The differences between these new feature families and their matched term counterparts are in: a) the matching indicators, which turn into unmatching indicators; and b) the term sets over which the summation is performed. For example, the unmatching indicator in the excessive feature family EXL i is (1  X   X  ( t  X  q )), which is 1 only if the docu-ment term is not in the query. In addition, the summation in EXL i is over u ( d ), which stands for the set of unique terms in d from which we pick the excessive terms.

We note that for MIG i , syntactic analysis of the query is required. We used the Stanford parser for query parsing as well, but found that the query dependency trees were of low quality. Therefore, in our experiments we only use the POS tags for queries, and hence only features MIG 1  X  4 , leaving the reliable generation of features MIG 5  X  6 for future work.
As discussed in Sec. 2, some mismatches in exact matching scheme should actually be accounted as (at least partial) matches, such as in the case of synonyms or related words, e.g.  X  guitar  X  and  X  riff  X . We extend our proposed features and present a novel soft matching formulation of all our matched and unmatched features (except BM25-related H 1  X  2 ). To the best of our knowledge, this is the first formulation in the context of the standard set of LETOR features [20].
We start with a lexical similarity function sim ( t [0 , 1] between a query term t q and a document term t d . The closer the function X  X  value is to 1 the more similar the two terms are. We follow recent successes with word embedding similarity and use in this work: where sg ( t ) is the word embedding vector of term t learned by the SkipGram algorithm [23]. We define sim ( t,t ) = 1 for every word similarity with itself and sim ( t,u ) = 0 if t 6 = u and either t or u are not in the lexicon.

To incorporate the similarity score sim ( t q ,t d ) into our fea-tures we find the best matching counterpart term for each query term and for each document term:
Term repetition is avoided since the number of occurrences of the term t in d is already counted in f L i . where  X  s ( t,d ) and  X  s ( q,t ) are soft indicator functions that capture how well a query (document) term is matched against the document (query) via its similarity score with its best match. We note that if sim () would only return 1 for exact match and 0 otherwise,  X  s () would become  X  ().
 Finally, we extend our features using bm () and  X  s (): EXL s i ( q,d ) = X EXG s i ( q,d ) = X
MIL s i ( q,d ) = X MIG s i ( q,d ) = X where we simply replace (or augment where necessary) the indicator function with the soft indicator variant, and in-stead of gathering statistics from exact match occurrences, we gather them from the occurrences of the best-match. If a query term appears as-is in the document (exact match), our feature scores are exactly as for exact matching. Yet, when a query term does not exactly appear in the docu-ment (or vice versa) instead of returning a matched feature value of zero, we resort to counting with respect to its best soft match instead. We note that a similar formulation us-ing best-matches is utilized by Liu et al [21] for computing similarity between short documents.
Prior work showed that extending LMIR with some sim-ilarity notion between terms improves retrieval results [39, 14]. We therefore extend our language model feature H 3 in a similar way, following the formalism of Xue et al [39]: H 3 ( q,d ) =
P ( t | d ) =
P tt ( t | d ) = X where q is the query term list; d is the document term list and | d | is its length; c ( t,d ) is the term-frequency of t in d ; P ( t ) is maximum likelihood estimation (MLE) of t in our corpus; Z is a probability normalizer; and  X  and  X  are hyper parameters to be tuned.
 We note that H s 3 is a variant of Xue et al X  X  language model. Instead of using a translation table as P tt , we followed Gan-guly et al [14], who suggested a variant of P tt based on a similarity function sim () (normalized into a probability dis-tribution). Note, when sim () represents exact matching, H becomes H 3 .
We learned the weights w of our ranking algorithm in a semi-supervised manner based on clickthrough data. The goal of the learning algorithm is to find a vector w such that for each query in the training data, the corresponding clicked document will be ranked as high as possible. We used the online variant of SVMRank [8] with the AROW update [11] as done before [9]. Specifically, for each training query the algorithm first re-ranks the top 100 documents retrieved by Lucene using the currently learned ranker. Then, it se-lects the top K ranking documents, excluding the clicked document. The algorithm then updates w such as for this query the clicked document would increase its ranking score compared to the selected K documents.

We split our query collection (see Sec. 3) into a 61,000 query training-set, a 14,000 query validation-set and a 61,000 query test-set. The validation-set was used to tune the various hyper-parameters for each tested model separately , namely, the number of training rounds, the value of K , and the AROW hyper-parameter r . The only hyper parameters that were tuned once for all models are  X  = 1,  X  = 0 . 5 for the H 3 and H s 3 LMIR features. Specifically,  X  was tuned on the LETOR model and  X  was then tuned on a soft version of the LETOR . See Sec. 6.1 for details on the configuration of each tested model. Finally, to compute term similarity we used publicly available 5 pre-trained word embedding vectors.
We evaluated our proposed models against several base-lines via two settings: first, based on a large scale click-through data, and second, based on manual judgments.
We consider six baseline models:
We compare the baselines to our proposed models: https://code.google.com/p/word2vec/
Using RedSVD: http://code.google.com/p/redsvd/
We trained separately each of the LTR-based models using the algorithm in Sec. 5.4.
We conducted a large scale automatic evaluation using our 61,000 query test-set (see Sec. 5.4). For each query we retrieved the top 100 results from the document collection using Lucene, and then re-ranked the top results using each of the tested models. We report Mean Reciprocal Rank (MRR) and Binary Recall at position K (R@ K ), all derived from the rank position of the clicked document associated with each query. Fig. 4 depicts the query length distribution of our test-set. We remind the reader that CQA queries are usually longer than typical Web queries.
We randomly sampled 1,000 queries of length 3 or more words from our test-set (shorter queries are scarce in our query collection -see Fig. 4). For each query we collected the top 10 documents as ranked by each of the tested models. Professional editors assessed the relevance of each document in the pool on three Likert-scale levels: (1) non-relevant, (2) partially-relevant, and (3) highly-relevant. Inspecting the evaluations, we found that usually only highly-relevant doc-uments refer to relevant content. Hence, we report NDCG with weights of 0 for non-relevant , 1 for partially-relevant and 10 for highly-relevant . We also report Precision at K (P@ K ) taking only highly-relevant documents as relevant.
The results for the automatic and manual evaluations are summarized in Tab. 2 and Tab. 3, respectively. All sta-tistical significance figures are computed using t-test. The results in both tables indicate a trend similar to the one re-ported by Cramel et el [9]. Namely, LETOR outperforms BM25 by a large margin ( e.g. 10.5% increase in MRR) and adding syntactic features ( G i ) on top of statistical features ( L i , H i ) in the Matched model consistently provides addi-tional improvement, e.g. 1.5% increase in MRR across all query lengths (Fig. 5). We thus refer to Matched as our main baseline. We note in passing that the performance of the LSI and BIM models was significantly lower than the LTR models ( e.g. MRR of 0.202 for LSI , 0.164 for BIM ) and adding them as additional features did not help either. We therefore excluded their performance report.

We next observe that adding soft term-matching to ad-dress the lexical gap between queries and documents ( Soft-Matched model) shows a nice improvement under the click-through automatic evaluation. For example, MRR is in-creased by 2 . 1% compared to Matched . In addition, manual evaluation also shows some improvement using soft match-ing, specifically at high rank positions. For example, P@3 is increased by 2 . 9% compared to Matched . On the other hand, the results for P@5 and P@10 are comparable to Matched . Analyzing our soft matching model, we found quite a few queries where exact matching provided better ranking than soft matching. For example, under the au-tomatic evaluation setting, SoftMatched ranked the clicked are statistically significant at p &lt; 0 . 001 .
 to the Matched model. Table 4: Examples where Full promoted better con-tent at the top compared to Matched document higher than Matched on 25% of the queries but that Matched ranked the clicked document higher than Soft-Matched on 18.3% of the test-set. This may indicate that our current similarity function is noisy and could be improved in future work. While soft matching for retrieval was studied before, this is the first time it is applied in the CQA vertical search scenario. In addition, we are not aware of prior work that directly applies it to a large set of standard LTR fea-tures, specifically using similarity between word embedding vectors for lexical semantics (compared to the well studied translation models for this usage).

We now get to our main result, which is split into two parts, corresponding to the exact matching and soft match-ing settings. Under the exact matching setting, when adding features that directly address unmatched terms ( Full model) we see a significant improvement in performance in the au-tomatic evaluation compared to only using matched term features ( Matched model). For example, MRR is increased by 2 . 9%, and similar trends occur for R@ K . In Fig. 5 we plot MRR vs query length from which we observe that the MRR gap is maintained across all query lengths. The gap is slightly decreasing towards longer queries, perhaps because matching many of the terms for longer queries within a ques-tion title contains enough information to indicate relevant content. Under the manual evaluation, some improvement is shown. Specifically P@3 and P@10 show an increase of  X  1% compared to Matched while P@1 and P@5 show com-parable results. Tab. 4 shows examples where Full promoted better content at the top compared to Matched . These ex-amples demonstrate how Full downgrades titles containing excessive information that changes the meaning of the title, such as  X  Angles  X  and  X  Dodgers  X  in the first example.
Both unmatched term features ( Full ) and soft matched term features ( SoftMatched ) provide a rather similar im-provement over exact matching ( Matched ). However, they capture different aspects of query/document ranking, one is addressing the lexical gap between the two and the other is addressing the importance of terms that were not matched from either side. Combining both model approaches to-gether ( SoftFull model) shows that they convey somewhat complementing elements for ranking. Indeed, SoftFull is the best performing model under all metrics. Under automatic evaluation the improvement is rather additive with a nice gap in performance maintained across all query lengths from both SoftMatched and Matched . Under manual evaluation the improvement is more significant. It seems that under all metrics, but P@3, the combination of soft matching with di-rect unmatched term assessment is more powerful than each of its parts. This result may indicate that soft matching helps pinpointing the  X  X rue X  unmatched terms and therefore improves the modeling of their contribution to ranking.
Our unmatched term features are composed of two types: a) those that address excessive terms, which occur only in the document ( EX { L,G } i ); and b) those that address miss-ing terms, which occur only in the query ( MI { L,G } i ). We evaluated the contribution of each feature type indepen-dently by constructing two auxiliary models, both augment-ing all matched term features ( Matched ). The first model adds only EX { L,G } i features, denoted MatchedAndExces-what is candys american dream no virgin birth No Virgin birth? Was the Virgin 92 accord misfire My  X 92 Accord misfires and loses Table 5: Examples where Full ranked a relevant clicked document low due to many excessive terms sive , and the second model adds only the MI { L,G } tures, denoted MatchedAndMissing .

Fig. 6 presents the performance of the two new models compared to Matched and to Full on the automatic evalu-ation setting, measured via the MRR metric. The graph shows that while missing term features are not as strong in-dicators for irrelevance compared to excessive term features, they still directly improve MRR compared to Matched . In addition, while excessive term features contribute more to detecting irrelevant documents, a small improvement in MRR is gained when excessive and missing features are combined in the Full model.

Another observation from Fig. 6 is that MatchedAndEx-cessive improves over Matched for short queries much more than for long queries. One reason may be that for short queries there are more candidates in the corpus that contain the query terms, but many of them may have a lot of exces-sive information. As opposed to short web queries, in which the information need is generally wide, CQA queries tend to refer to very specific information needs even in such short queries, e.g.  X  characteristics of enzymes  X . If this hypothesis is true and our algorithm learned that a lot of excessive in-formation indicates an irrelevant candidate, we expect the model to be particularly effective in filtering out such candi-dates. We note that for short queries of length 1-2 there is no improvement using missing features. This is not surprising, since there are no missing terms when matching queries of length 1, and the amount of missing information in queries of length two is at most a single term.
 Figure 6: MRR by query length with the addition of Excessive vs. Missing Features
To better understand the performance of our features, we conducted error analysis on cases in which Full , which con-siders both matched and unmatched term features, ranks a clicked document significantly lower than Matched , which employs only matched term features. To this end, we con-sidered queries from our validation set in which Matched ranked the clicked document for the query in one of the top 3 positions while Full ranked it far below. We sorted the ex-amples by the rank margin between Matched and Full and analyzed the 100 queries with the largest rank margin.
We found out that in 35 of the analyzed queries the top document ranked by Full was relevant and in 46 queries at least one of the top 3 documents was relevant. This is a known issue when using clickthrough logs as proxy to doc-ument relevance, as some unclicked documents may also be relevant to the query. Thus absolute model performance under such evaluation is biased. Yet, comparing ranking al-gorithms over a large-scale click-based gold labeling is useful for differentiating between their ranking performance [27].
Out of the 65 queries where the top candidate by Full was not relevant, we recognized two main reasons for this rank-ing failure. The most prominent reason, which occurred in 36 cases, is that some terms in the document title were not matched due to the exact-matching scheme used in Full , but would have considered matched under proper soft matching. Out of these 36 case, 13 queries had spelling errors and other phenomena, such as unigram/bigram variations ( e.g.  X  coun-tertop  X  vs.  X  counter top  X ). Such lexical variations are not recognized by our current soft term matching which uses word embedding.

The second phenomenon occurred in 16 out of the 65 queries. The clicked document title contains a lot of ex-cessive terms, yet still fulfills the information need behind the query. Tab. 5 presents such examples. While our results show the potential in directly modeling unmatched terms, and specifically excessive terms as negative signals, a large number of such terms may accumulate into an unnecessary downgrading of the ranking score, and further research is required to develop more robust models. In this work we proposed novel features in a Learning To Rank framework that directly assess the importance of missing and excessive terms within the task of term weight-ing for vertical search on CQA content. To better model truly unmatched terms we also presented a  X  X oft matching X  variant of all our features, basing it on distributional sim-ilarity between terms, where similar terms are considered partially both matched, and unmatched. Our experiments show improvement in document retrieval in all settings when unmatched information is taken into account.

In future research we plan to test whether our approach may contribute to other types of Web documents. One di-rection could be to explore how unmatched terms can be modeled in other parts of the document, which may require different features than the ones used in this paper. The resrach was supported in part by the Yahoo Faculty Research and Engagement Program.
