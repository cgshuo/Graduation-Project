 Bruno Castro da Silva bsilva@cs.umass.edu George Konidaris gdk@csail.mit.edu Andrew G. Barto barto@cs.umass.edu One approach to dealing with the complexity of apply-ing reinforcement learning to high-dimensional control problems is to specify or discover hierarchically struc-tured policies. The most widely used hierarchical re-inforcement learning formalism is the options frame-work (Sutton et al., 1999), where high-level options (also called skills ) define temporally extended policies that can be used directly in learning and planning but abstract away the details of low-level control. One of the motivating principles underlying hierarchical rein-forcement learning is the idea that subproblems recur, so that acquired or designed options can be reused in a variety of tasks and contexts.
 However, the options framework as usually formulated defines an option as a single policy. An agent may in-stead wish to define a parameterized policy that can be applied across a class of related tasks. For exam-ple, consider a soccer playing agent. During a game the agent might wish to kick the ball with varying amounts of force, towards various different locations on the field; for such an agent to be truly competent it should be able to execute such kicks whenever neces-sary, even with a particular combination of force and target location that it has never had direct experience with. In such cases, learning a single policy for each possible variation of the task is clearly infeasible. The agent might therefore wish to learn good policies for a few specific kicks, and then use this experience to synthesize a single general skill for kicking the ball X  parameterized by the amount of force desired and the target location X  X hat it can execute on-demand. We propose a method for constructing parameterized skills from experience. The agent learns to solve a few instances of the parameterized task and uses these to estimate the topology of the lower-dimensional mani-fold on which the skill policies lie. This manifold mod-els how policy parameters change as task parameters vary. The method identifies the number of charts that compose the manifold and then applies non-linear re-gression in each chart to construct a parameterized skill by predicting policy parameters from task param-eters. We evaluate the method on an underactuated simulated robotic arm tasked with learning to accu-rately throw darts at a parameterized target location. In what follows we assume an agent which is presented with a set of tasks drawn from some task distribution. Each task is modeled by a Markov Decision Process (MDP) and the agent must maximize the expected re-ward over the whole distribution of possible MDPs. We assume that the MDPs have dynamics and reward functions similar enough so that they can be consid-ered variations of a same task. Formally, the goal of such an agent is to maximize: where  X   X  is a policy parameterized by a vector  X   X  R N ,  X  is a task parameter vector drawn from a | T | -dimensional continuous space T , J (  X , X  ) = E P K t =0 r t |  X , X  is the expected return obtained when executing policy  X  while in task  X  and P (  X  ) is a prob-ability density function describing the probability of task  X  occurring. Furthermore, we define a parameter-ized skill as a function mapping task parameters to policy parameters. When using a parameterized skill to solve a distribution of tasks, the specific policy parameters to be used depend on the task currently being solved and are specified by  X . Under this definition, our goal is to construct a parameterized skill  X  which maximizes: 2.1. Assumptions We assume the agent must solve tasks drawn from a distribution P (  X  ). Suppose we are given a set K of pairs {  X , X   X  } , where  X  is a | T | -dimensional vector of task parameters sampled from P (  X  ) and  X   X  is the cor-responding policy parameter vector that maximizes re-turn for task  X  . We would like to use K to construct a parameterized skill which (at least approximately) maximizes the quantity in Equation 2.
 We start by highlighting the fact that the probability density function P induces a (possibly infinite) set of skill policies for solving tasks in the support of P , each one corresponding to a vector  X   X   X  R N . These poli-cies lie in an N -dimensional space containing sample policies that can be used to solve tasks drawn from P . Since the tasks in the support of P are assumed to be related, it is reasonable to further assume that there exists some structure in this space; specifically, that the policies for solving tasks drawn from the dis-tribution lie on a lower-dimensional surface embedded in
R N and that their parameters vary smoothly as we vary the task parameters.
 This assumption is reasonable in a variety of situa-tions, especially in the common case where the policy is differentiable with respect to its parameters. In this case, the natural gradient of the performance J  X   X  , X  is well-defined and indicates the direction (in policy space) that locally maximizes J but which does not change the distribution of paths induced by the policy by much. Consider, for example, problems in which performance is directly correlated to how close the agent gets to a goal state; in this case one can interpret a small perturbation to the policy as defining a new policy which solves a similar task but with a slightly different goal. Since under these conditions small pol-icy changes induce a smoothly-varying set of goals, one can imagine that the goals themselves parameterize the space of policies: that is, that by varying the goal or task one moves over the lower-dimensional surface of corresponding policies.
 Note that it is possible to find points in policy space in which the corresponding policy cannot be further locally modified in order to obtain a solution to a new, related goal. This implies that the set of skill policies of interest might be in fact distributed over several charts of a piecewise-smooth manifold. Our method can automatically detect when this is the case and construct separate models for each manifold, essen-tially discovering how many different skills exist and creating a unified model by which they are integrated. Our method proceeds by collecting example task in-stances and their solution policies and using them to train a family of independent non-linear regression models mapping task parameters to policy parameters. However, because policies for different subsets of T might lie in different, disjoint manifolds, it is necessary to first estimate how many such lower-dimensional sur-faces exist before separately training a set of regression models for each one.
 More formally, our method consists of four steps: 1) draw | K | sample tasks from P and construct K , the set of task instances  X  and their corresponding learned policy parameters  X   X  ; 2) use K to estimate the geom-etry and topology of the policy space, specifically the number D of lower-dimensional surfaces embedded in R
N on which skill policies lie; 3) train a classifier  X  mapping elements of T to [1 ,...,D ]; that is, to one of the D lower-dimensional manifolds; 4) train a set of ( N  X  D ) independent non-linear regression models  X  i,j , i  X  [1 ,...,D ], j  X  [1 ,...N ], each one mapping elements of T to individual skill policy parameters  X  i , i  X  [1 ,...N ]. Each subset [ X  i, 1 ,...,  X  i,N ] of regression models is trained over all tasks  X  in K where  X  (  X  ) = i . We therefore define a parameterized skill as a vector function: Figure 1 depicts the above-mentioned steps. Note that we have described our method without specifying a particular choice of policy representation, learning algorithm, classifier, or non-linear regression model, since these design decisions are best made in light of the characteristics of the application at hand. In the following sections we present a control problem whose goal is to accurately throw darts at a variety of tar-gets and describe one possible instantiation of our ap-proach. In the dart throwing domain, a simulated planar un-deractuated robotic arm is tasked with learning a pa-rameterized policy to accurately throw darts at targets around it (Figure 4). The base of the arm is affixed to a wall in the center of a 3-meter high and 4-meter wide room. The arm is composed of three connected links and a single motor which applies torque only to the second joint, making this a difficult non-linear and underactuated control problem. At the end of its third link, the arm possesses an actuator capable of holding and releasing a dart. The state of the system is a 7-dimensional vector composed by 6 continuous features corresponding to the angle and angular velocities of each link and by a seventh binary feature specifying whether or not the dart is still in being held. The goal of the system is to control the arm so that it executes a throwing movement and accurately hits a target of interest. In this domain the space T of tasks con-sists of a 2-dimensional Euclidean space containing all ( x,y ) coordinates at which a target can be placed X  X  target can be affixed anywhere on the walls or ceiling surrounding the agent. To implement the method outlined in Section 3 we need to specify methods to 1) represent a policy; 2) learn a policy from experience; 3) analyze the topol-ogy of the policy space and estimate D , the number of lower-dimensional surfaces on which skill policies lie; 4) construct the non-linear classifier  X  ; and 5) con-struct the non-linear regression models  X . In this sec-tion we describe the specific algorithms and techniques chosen in order to tackle the dart-throwing domain. We discuss our results in Section 6.
 Our choices of methods are directly guided by the char-acteristics of the domain. Because the following ex-periments involve a multi-joint simulated robotic arm, we chose a policy representation that is particularly well-suited to robotics: Dynamic Movement Primi-tives (Schaal et al., 2004), or DMPs. DMPs are a framework for modular motor control based on a set of linearly-parameterized autonomous non-linear dif-ferential equations. The time evolution of these equa-tions defines a smooth kinematic control policy which can be used to drive the controlled system. The spe-cific trajectory in joint space that needs to be followed is obtained by integrating the following set of differen-tial equations: where x and v are the position and velocity of the system, respectively; x 0 and g denote the start and goal positions;  X  is a temporal scaling factor; and K and Q act like a spring constant and a damping term, respectively. Finally, f is a non-linear function which can be learned in order to allow the system to generate arbitrarily complex movements and is defined as where  X  i ( s ) = exp(  X  h i ( s  X  c i ) 2 ) are Gaussian basis functions with adjustable weights w i and which de-pend on a phase variable s . The phase variable is constructed so that it monotonically decreases from 1 to 0 during the execution of the movement and is typi-cally computed by integrating  X   X  s =  X   X s , where  X  is a pre-determined constant. In our experiments we used a PID controller to track the trajectories induced by the above-mentioned system of equations.
 This results in a 37-dimensional policy vector  X  = [  X ,g,w 1 ,...,w 35 ] T , where  X  specifies the value of the phase variable s at which the arm should let go of the dart; g is the goal parameter of the DMP; and w ,...,w 35 are the weights of each Gaussian basis function in the movement primitive.
 We combine DMPs with a policy learning method known to perform well with this type of policy rep-resentation. PoWER (Kober &amp; Peters, 2008) is a policy search technique that collects sample path ex-ecutions and updates the policy X  X  parameters towards ones that induce a new success-weighted path distri-bution. We choose PoWER due to its simplicity and because it has been shown to outperform other pol-icy learning algorithms in a variety of standard bench-marks and on real robotics problems (Kober &amp; Peters, 2010). PoWER works by executing rollouts  X  con-structed based on slightly perturbed versions of the current policy parameters; perturbations to the pol-icy parameters consist of a structured, state-dependent exploration  X  t T  X  ( s ,t ), where  X  t  X  X  (0 ,  X   X  ) and meta-parameter of the exploration;  X  ( s ,t ) is the vec-tor of policy feature activations at time t . By adding this type of perturbation to  X  we induce a stochas-tic policy whose actions are a = (  X  +  X  t ) T  X  ( s ,t ))  X  ing such a stochastic policy, the policy parameters are updated as follows: where  X  Q  X  ( s , a ,t ) = P T  X  t = t r ( s  X  t , a  X  unbiased estimate of the return, W ( s ,t ) =  X  ( s ,t )  X  ( s ,t ) T  X  ( s ,t ) T  X   X   X  ( s ,t )  X  1 and  X  X  X  an importance sampler which can be chosen depending on the domain. A useful heuristic when defining  X  is to discard sample rollouts with very small importance weights; importance weights, in our experiments, are proportional to the relative performance of the rollout in comparison to others.
 To analyze the geometry and topology of the pol-icy space and estimate the number D of lower-dimensional surfaces on which skill policies lie we used the ISOMAP algorithm (Tenenbaum et al., 2000). ISOMAP is a technique for learning the underlying global geometry of high-dimensional spaces and the number of non-linear degrees of freedom that under-lie it. This information provides us with an estimate of D , the number of disjoint lower-dimensional mani-fold where policies are located; ISOMAP also specifies to which of these disconnected manifolds a given in-put policy belongs. This information is used to train the classifier  X  , which learns a mapping from task pa-rameters to numerical identifiers specifying one of the lower-dimensional surfaces embedded in policy space. For this domain we have implemented  X  by means of a simple linear classifier. In general, however, more powerful classifiers could be used.
 Finally, we must choose a non-linear regression algo-rithm for constructing  X  i,j . We use standard Support Vector Machines (SVM) (Vapnik, 1995) due to their good generalization capabilities and relatively low de-pendence on parameter tuning. In the experiments presented in Section 6 we use SVMs with Gaussian kernels and a inverse variance width of 5.0. As pre-viously mentioned, if important correlations between policy and task parameters are known to exist, mul-tivariate regression models might be preferable; one possibility in such cases are Structure Support Vector Machines (Tsochantaridis et al., 2005). Before discussing the performance of parameterized skill learning in this domain, we present some empiri-cally measured properties of its policy space. Specifi-cally, we describe topological characteristics of the in-duced space of policies generated as we vary the task. We sampled 60 tasks (target positions) uniformly at random and placed target boards at the correspond-ing positions. The policies for solving each one of these tasks were computed using PoWER; the learning algo-rithm was configured to perform a policy update every 20 rollouts and to run until a minimum performance threshold was reached. In our simulations, this crite-ria corresponded to the moment when the robotic arm first executed a policy that landed the dart within 5 centimeters of the intended target. In order to speed up the sampling process we initialize policies for subse-quent targets with ones computed for previously sam-pled tasks.
 We first analyze the structure of the policy manifold by estimating how each dimension of a policy varies as we smoothly vary the task. Figure 2a presents this information for a representative subset of policy pa-rameters. On each subgraph of Figure 2a the x axis corresponds to a 1-dimensional representation of the task obtained by computing the angle at which the target is located with respect to the arm; this is done for ease of visualization, since using x,y coordinates would require a 3-D figure. The y axis corresponds to the value of a selected policy parameter. The first im-portant observation to be made is that as we vary the task, not only do the policy parameters vary smoothly, but they tend to remain confined to one of two dis-joint but smoothly varying lower-dimensional surfaces. A discontinuity exists, indicating that after a certain point in task space a qualitatively different type of pol-icy parameterization is required. Another interesting observation is that this discontinuity occurs approx-imately at the task parameter values corresponding to hitting targets directly above the robotic arm; this implies that skills for hitting targets to the left of the arm lie on a different manifold than policies for hit-ting targets to its right. This information is relevant for two reasons: 1) it confirms both that the manifold assumption is reasonable and that smooth task varia-tions induce smooth, albeit non-linear, policy changes; and 2) it shows that the policies for solving a distri-bution of tasks are generally confined to one of several lower-dimensional surfaces, and that the way in which they are distributed among these surfaces is correlated to the qualitatively different strategies that they im-plement. Figures 2b and 2c show, similarly, how a selected sub-set of policy parameters changes as we vary the task, but now with the two resulting manifolds analyzed separately. Figure 2b shows the variations in policy parameters induced by smoothly modifying tasks for hitting targets anywhere in the interval of 1.57 to 3.5 radians X  X hat is, targets placed roughly at angles be-tween 90  X  (directly above the agent) and 200  X  (lowest part of the right wall). Figure 2c shows that same in-formation but for targets located on one of the other two quadrants X  X hat is, targets to the left of the arm. We superimposed in Figures 2a-c a red curve repre-senting the non-linear fit constructed by  X  while mod-eling the relation between task and policy parameters in each manifold. Note also how a clear linear sepa-ration exists between which task policies lie on which manifold: this separation indicates that two qualita-tively distinct types of movement are required for solv-ing different subsets of the tasks. Because we empiri-cally observe that a linear separation exists, we imple-ment  X  using a simple linear classifier mapping tasks parameters to the numerical identifier of the manifold to which the task belongs.
 We can also analyze the characteristics of the lower-dimensional, quasi-isometric embedding of poli-cies produced by ISOMAP. Figure 3 shows the 2-dimensional embedding of a set of policies sampled from one of the manifolds. Embeddings for the other manifold have similar properties. Analysis of the resid-ual variance of ISOMAP allows us to conclude that the intrinsic dimensionality of the skill manifold is 2; this is expected since we are essentially parameteriz-ing a high-dimensional policy space by task parame-ters, which are drawn from the 2-dimensional space T . This implies that even though skill policies themselves are part of a 37-dimensional space, because there are just two degrees-of-freedom with which we can vary tasks, the policies themselves remain confined to a 2-dimensional manifold. In Figure 3 we use lighter-colored points to identify embeddings of policies for hitting targets at higher locations. From this obser-vation it is possible to note how policies for similar tasks tend to remain geometrically close in the space of solutions.
 Figure 4 shows some types of movements the arm is ca-pable of executing when throwing the dart at specific targets. Figure 4a and Figure 4b present trajectories corresponding to policies aiming at targets high on the ceiling and low on the right wall, respectively; these were presented as training examples to the parameter-ized skill. Note that the link trajectories required to accurately hit a target are complex because we are us-ing just a single actuated joint to control an arm with three joints.
 Figure 4c shows a policy that was predicted by the pa-rameterized skill for a new, unknown task correspond-ing to a target in the middle of the right wall. A total of five sample trajectories were presented to the pa-rameterized skill and the corresponding predicted pol-icy was further improved by two policy updates, after which the arm was capable of hitting the intended tar-get perfectly.
 Figure 5 shows the predicted policy parameter error, averaged over the parameters of 15 unknown tasks sampled uniformly at random, as a function of the number of examples used to learn the parameterized skill. This is a measure of the relative error between the policy parameters predicted by  X  and parameters of a known good solution for the same task. The lower the error, the closer the predicted policy is (in norm) to the correct solution. After 6 samples are presented to the parameterized option it is capable of predicting policies whose parameters are within 6% of the correct ones; with approximately 15 samples, this error sta-bilizes around 3%. Note that this type of accuracy is only possible because even though the spaces analyzed are high-dimensional, they are also highly structured; specifically, solutions to similar tasks lie on a lower-dimensional manifold whose regular topology can be exploited when generalizing known solutions to new problems. Since some policy representations might be particu-larly sensitive to noise, we additionally measured the actual effectiveness of the predicted policy when di-rectly applied to novel tasks. Specifically, we measure the distance between the position where the dart hits and the intended target; this measurement is obtained by executing the predicted policy directly and before any further learning takes places. Figure 6 shows that after 10 samples are presented to the parameterized skill, the average distance is 70cm. This is a reason-able error if we consider that targets can be placed anywhere on a surface that extends for a total of 10 meters. If the parameterized skill is presented with a total of 24 samples the average error decreases to 30cm, which roughly corresponds to the dart being thrown from 2 meters away and landing one dartboard away from the intended center.
 Although these initial solutions are good, especially considering that no learning with the target task pa-rameters took place, they are not perfect. We might therefore want to further improve them. Figure 7 shows how many additional policy updates are re-quired to improve the policy predicted by the param-eterized skill up to a point where it reaches a perfor-mance threshold. The dashed line in Figure 7 shows that on average 22 policy updates are required for find-ing a good policy when the agent has to learn from scratch. On the other hand, by using a parameterized skill trained with 9 examples it is already possible to decrease this number to just 4 policy updates. With 20 examples or more it takes the agent an average of 2 additional policy updates to meet the performance threshold. The simplest solution for learning a distribution of tasks in RL is to include  X  , the task parameter vec-tor, as part of state descriptor and treat the entire class of tasks as a single MDP. This approach has sev-eral shortcomings: 1) learning and generalizing over tasks is slow since the state features corresponding to task parameters remain constant throughout each episode; 2) the number of basis functions needed to ap-proximate the value function or policy needs to be in-creased since the policy representation has to be pow-erful enough to not only solve the current MDP but to capture all the non-trivial correlations between task policy parameters; 3) sample task policies cannot be collected in parallel and later on combined in order to accelerate the construction of the parameterized skill; and 4) if the distribution of tasks is non-stationary, there is no simple way of adapting a single estimated policy in order to deal with a new pattern of tasks. Alternative, more efficient approaches have been pro-posed under the general heading of skill transfer. Konidaris and Barto (2007) introduce a method for constructing reusable options by learning them in an agent-centered state space instead of in the original problem-space. This technique does not, however, con-struct generalized skills capable of solving a family of related tasks. Soni and Singh (2006) create adapt-able options whose meta-parameters, e.g., their termi-nation criteria, can be adapted on-the-fly in order to deal with unknown, changing aspects of a task. How-ever, this technique does not directly predict a com-plete parameterization of the policy for new tasks. Liu and Stone (2006) propose a method for transferring a value function between a specific given pair of tasks but require prior knowledge of the task dynamics in the form of a Dynamic Bayes Network.
 Several other similar methods have been proposed in which the goal is to transfer a model or value function between a given pair of tasks, but not necessarily to reuse a set of learned tasks and construct a generalized, parameterized solution. It is also often assumed that a mapping between features and actions of the source and target tasks exists and is known a priori, as in Tay-lor and Stone (2007). Hausknecht and Stone (2011) propose a way of estimating a parameterized skill for kicking a soccer ball with varying amounts of energy. They exhaustively test variations of a control policy by varying one of its parameters, known a priori to be relevant for the skill, and measuring the resulting net effect on the distance that the ball travels. By assum-ing a quadratic relation between these variables, they are able to construct a regression model and invert it, thereby obtaining a closed-form for the value that the policy parameter needs to assume whenever a given type of kick is desired. This is an interesting example of the type of parameterized skill that we would like to construct, albeit a very domain-dependent one. Fi-nally, Braun et al. (2010) discuss how Bayesian model-ing can be used to explain experimental data from cog-nitive and motor neuroscience that supports the idea of structure learning in humans, a concept very simi-lar in nature to the one of parameterized skills. The authors do not, however, propose a concrete method for properly identifying and constructing such skills. We have presented a general framework for construct-ing parameterized skills. The idea underlying our method is to sample a small number of task instances and generalize them to new problems by combining classifiers and non-linear regression models. This ap-proach is effective in practice because it exploits the intrinsic structure of the policy space and because skill policies for similar tasks typically lie on a lower-dimensional manifold. Our framework allows for the construction of effective parameterized skills and is also able to identify the number of qualitatively differ-ent strategies required for solving a given distribution of tasks.
 This work can be extended in several important di-rections. First, the question of how to actively select training tasks in order to improve the overall readiness of a parameterized skill, given a distribution of tasks expected in the future, needs to be addressed. Another important open problem is how to properly deal with a non-stationary distribution of tasks. If a new task distribution is known exactly it might be possible to use it to resample instances from K and thus recon-struct the parameterized skill. However, more general strategies are needed if the task distribution changes in a way that is not known to the agent.
 Another important question is how to analyze the topology and geometry of the policy space more effi-ciently. Methods for discovering the underlying global geometry of high-dimensional spaces typically require dense sampling of the manifold, which could, in case of very irregular spaces, require solving an unreason-able number of training tasks. Note, however, that most local policy search methods like the Natural Ac-tor Critic and PoWER move smoothly over the mani-fold of policies while searching for locally optimal solu-tions. Therefore, at each policy update during learn-ing they provide us with a new sample which can be used for further train the parameterized skill; each task instance therefore results in a trajectory through pol-icy parameter space. Integrating this type of sam-pling into the construction of the skill essentially cor-responds to a type of off-policy learning method, since samples collected while estimating one policy could be used to generalize it to different tasks.
 This research was partially funded by the European Community 7th Framework Programme (FP7/2007-2013), grant agreement No. ICT-IP-231722, project  X  X M-CLeVeR -Intrinsically Motivated Cumulative Learning Versatile Robots X .

