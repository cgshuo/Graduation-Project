 Kernel machines such as the Support Vector Machine are attractive because they can approximate any function or decision boundary arbitrarily well with enough training data. Unfortunately, meth-ods that operate on the kernel matrix (Gram matrix) of the data scale poorly with the size of the training dataset. For example, even with the most powerful workstation, it might take days to train a nonlinear SVM on a dataset with half a million training examples. On the other hand, lin-ear machines can be trained very quickly on large datasets when the dimensionality of the data is small [1, 2, 3]. One way to take advantage of these linear training algorithms for training nonlinear machines is to approximately factor the kernel matrix and to treat the columns of the factor matrix as features in a linear machine (see for example [4]). Instead, we propose to factor the kernel func-tion itself. This factorization does not depend on the data, and allows us to convert the training and evaluation of a kernel machine into the corresponding operations of a linear machine by mapping data into a relatively low-dimensional randomized feature space. Our experiments show that these random features, combined with very simple linear learning techniques, compete favorably in speed and accuracy with state-of-the-art kernel-based classification and regression algorithms, including those that factor the kernel matrix.
 The kernel trick is a simple way to generate features for algorithms that depend only on the inner product between pairs of input points. It relies on the observation that any positive definite function k ( x , y ) with x , y  X  R d defines an inner product and a lifting  X  so that the inner product between is that the algorithm accesses the data only through evaluations of k ( x , y ) , or through the kernel matrix consisting of k applied to all pairs of datapoints. As a result, large training sets incur large computational and storage costs.
 Instead of relying on the implicit lifting provided by the kernel trick, we propose explicitly mapping the data to a low-dimensional Euclidean inner product space using a randomized feature map z : evaluation: Unlike the kernel X  X  lifting  X  , z is low-dimensional. Thus, we can simply transform the input with z , and then apply fast linear learning methods to approximate the answer of the corresponding nonlinear kernel machine. In what follows, we show how to construct feature spaces that uniformly approximate popular shift-invariant kernels k ( x  X  y ) to within with only D = O ( d  X  2 log 1 2 ) dimensions, and empirically show that excellent regression and classification performance can be obtained for even smaller D .
 In addition to giving us access to extremely fast learning algorithms, these randomized feature maps also provide a way to quickly evaluate the machine. With the kernel trick, evaluating the machine compute and requires retaining much of the dataset unless the machine is very sparse. This is often unacceptable for large datasets. On the other hand, after learning a hyperplane w , a linear machine can be evaluated by simply computing f ( x ) = w 0 z ( x ) , which, with the randomized feature maps presented here, requires only O ( D + d ) operations and storage.
 We demonstrate two randomized feature maps for approximating shift invariant kernels. Our first randomized map, presented in Section 3, consists of sinusoids randomly drawn from the Fourier transform of the kernel function we seek to approximate. Because this map is smooth, it is well-suited for interpolation tasks. Our second randomized map, presented in Section 4, partitions the input space using randomly shifted grids at randomly chosen resolutions. This mapping is not smooth, but leverages the proximity between input points, and is well-suited for approximating ker-nels that depend on the L 1 distance between datapoints. Our experiments in Section 5 demonstrate that combining these randomized maps with simple linear learning algorithms competes favorably with state-of-the-art training algorithms in a variety of regression and classification scenarios. The most popular methods for large-scale kernel machines are decomposition methods for solving Support Vector Machines (SVM). These methods iteratively update a subset of the kernel machine X  X  coefficients using coordinate ascent until KKT conditions are satisfied to within a tolerance [5, 6]. While such approaches are versatile workhorses, they do not always scale to datasets with more than hundreds of thousands of datapoints for non-linear problems. To extend learning with kernel machines to these scales, several approximation schemes have been proposed for speeding up operations involving the kernel matrix.
 The evaluation of the kernel function can be sped up using linear random projections [7]. Throwing away individual entries [7] or entire rows [8, 9, 10] of the kernel matrix lowers the storage and computational cost of operating on the kernel matrix. These approximations either preserve the separability of the data [8], or produce good low-rank or sparse approximations of the true kernel matrix [7, 9]. Fast multipole and multigrid methods have also been proposed for this purpose, but, while they appear to be effective on small and low-dimensional problems, they have not been demonstrated on large datasets. Further, the quality of the Hermite or Taylor approximation that these methods rely on degrades exponentially with the dimensionality of the dataset [11]. Fast nearest neighbor lookup with KD-Trees has been used to approximate multiplication with the kernel matrix, and in turn, a variety of other operations [12]. The feature map we present in Section 4 is reminiscent of KD-trees in that it partitions the input space using multi-resolution axis-aligned grids similar to those developed in [13] for embedding linear assignment problems. Our first set of random features project data points onto a randomly chosen line, and then pass the resulting scalar through a sinusoid (see Figure 1 and Algorithm 1). The random lines are drawn from a distribution so as to guarantee that the inner product of two transformed points approximates the desired shift-invariant kernel.
 The following classical theorem from harmonic analysis provides the key insight behind this trans-formation: Theorem 1 (Bochner [15]) . A continuous kernel k ( x, y ) = k ( x  X  y ) on R d is positive definite if and only if k (  X  ) is the Fourier transform of a non-negative measure. Figure 1: Random Fourier Features. Each component of the feature map z ( x ) projects x onto a random If the kernel k (  X  ) is properly scaled, Bochner X  X  theorem guarantees that its Fourier transform p (  X  ) is a proper probability distribution. Defining  X   X  ( x ) = e j X  0 x , we have so  X   X  ( x )  X   X  ( y )  X  is an unbiased estimate of k ( x , y ) when  X  is drawn from p . To obtain a real-valued random feature for k , note that both the probability distribution p (  X  ) and z k ( x , y ) , since z  X  ( x ) 0 z  X  ( y ) = cos  X  0 ( x  X  y ) . Other mappings such as z  X  ( x ) = b ) , where  X  is drawn from p (  X  ) and b is drawn uniformly from [0 , 2  X  ] , also satisfy the condition E [ z  X  ( x ) 0 z  X  ( y )] = k ( x , y ) .
 We can lower the variance of z  X  ( x ) 0 z  X  ( y ) by concatenating D randomly chosen z  X  into a column vector z and normalizing each component by z j ( x ) z  X  j ( y ) and is therefore a lower variance approximation to the expectation (2). Since z  X  ( x ) 0 z  X  ( y ) is bounded between -1 and 1, for a fixed pair of points x and y , Hoeffd-ing X  X  inequality guarantees exponentially fast convergence in D between z ( x ) 0 z ( y ) and k ( x , y ) : assertion can be proven for every pair of points in the input space simultaneously: Claim 1 (Uniform convergence of Fourier features) . Let M be a compact subset of R d with diam-eter diam( M ) . Then, for the mapping z defined in Algorithm 1, we have where  X  2 p  X  E p [  X  0  X  ] is the second moment of the Fourier transform of k . Fur-ther, sup x,y  X  X  | z ( x ) 0 z ( y )  X  k ( y , x ) |  X  with any constant probability when D = -net over M  X  M . This result is then extended to the entire space using the fact that the feature map is smooth with high probability. See the Appendix for details.
 By a standard Fourier identity, the scalar  X  2 p is equal to the trace of the Hessian of k at 0 . It quantifies the curvature of the kernel at the origin. For the spherical Gaussian kernel, k ( x , y ) = exp  X   X  k x  X  y k 2 , we have  X  2 p = 2 d X  . Algorithm 1 Random Fourier Features.
 Require: A positive definite shift-invariant kernel k ( x , y ) = k ( x  X  y ) .
 Ensure: A randomized feature map z ( x ) : R d  X  R 2 D so that z ( x ) 0 z ( y )  X  k ( x  X  y ) . Compute the Fourier transform p of the kernel k : p (  X  ) = 1 2  X  R e  X  j X  0  X  k ( X ) d  X  . Draw D iid samples  X  1 ,  X   X   X  ,  X  D  X  R d from p .
 Our second random map partitions the input space using randomly shifted grids at randomly chosen resolutions and assigns to an input point a binary bit string that corresponds to the bin in which it falls (see Figure 2 and Algorithm 2). The grids are constructed so that the probability that two points x and y are assigned to the same bin is proportional to k ( x , y ) . The inner product between a pair of transformed points is proportional to the number of times the two points are binned together, and is therefore an unbiased estimate of k ( x , y ) . Figure 2: Random Binning Features. (left) The algorithm repeatedly partitions the input space using a ran-We first describe a randomized mapping to approximate the  X  X at X  kernel k hat ( x, y ;  X  ) = max 0 , 1  X  | x  X  y |  X  on a compact subset of R  X  R , then show how to construct mappings for more general separable multi-dimensional kernels. Partition the real number line with a grid of pitch  X  , and shift this grid randomly by an amount u drawn uniformly at random from [0 ,  X  ] . This grid partitions the real number line into intervals [ u + n X , u + ( n + 1)  X  ] for all integers n . The probability that two points x and y fall in the same bin in this grid is max 0 , 1  X  | x  X  y |  X  [13]. In other words, if we number the bins of the grid so that a point x falls in bin  X  x = b x  X  u  X  c and y vector z ( x ) over the bins, z ( x ) 0 z ( y ) = 1 if x and y fall in the same bin and zero otherwise, so Pr Now consider shift-invariant kernels that can be written as convex combinations of hat kernels on a E drawn uniformly from [0 ,  X  ] the probability that x and y are binned together is k ( x, y ) . Lemma 1 in  X  exp(  X   X  ) . For the Gaussian kernel,  X  k is not everywhere positive, so this procedure does not yield a random map.
 Random maps for separable multivariate shift-invariant kernels of the form k ( x  X  y ) = Q if each k m can be written as a convex combination of hat kernels. We apply the above binning pro-cess over each dimension of R d independently. The probability that x m and y m are binned together in dimension m is k m ( | x m  X  y m | ) . Since the binning process is independent across dimensions, the d -dimensional grid as a binary indicator vector. In practice, to prevent overflows when computing z ( x ) when d is large, our implementation eliminates unoccupied bins from the representation. Since there are never more bins than training points, this ensures no overflow is possible. We can again reduce the variance of the estimator z ( x ) 0 z ( y ) by concatenating P random binning functions z into a larger list of features z and scaling by p 1 /P . The inner product z ( x ) 0 z ( y ) = claim is that this convergence holds simultaneously for all points: Claim 2. Let M be a compact subset of R d with diameter diam( M ) . Let  X  = E [1 / X  ] and let L k denote the Lipschitz constant of k with respect to the L 1 norm. With z as above, we have
Pr sup of the claim (see the appendix) partitions M  X  M into a few small rectangular cells over which throughout M  X  M .
 Algorithm 2 Random Binning Features.
  X   X  k m ( X ) is a probability distribution on  X   X  0 .
 Ensure: A randomized feature map z ( x ) so that z ( x ) 0 z ( y )  X  k ( x  X  y ) . for p = 1 . . . P do end for The experiments summarized in Table 1 show that ridge regression with our random features is a fast way to approximate the training of supervised kernel machines. We focus our comparisons against the Core Vector Machine [14] because it was shown in [14] to be both faster and more accurate than other known approaches for training kernel machines, including, in most cases, random sampling of datapoints [8]. The experiments were conducted on the five standard large-scale datasets evaluated in [14], excluding the synthetic datasets. We replicated the results in the literature pertaining to the CVM, SVM light , and libSVM using binaries provided by the respective authors. 1 For the random feature experiments, we trained regressors and classifiers by solving the ridge regression problem Table 1: Comparison of testing error and training time between ridge regression with random features, Core min w k Z 0 w  X  y k 2 2 +  X  k w k 2 2 , where y denotes the vector of desired outputs and Z denotes the matrix of random features. To evaluate the resulting machine on a datapoint x , we can simply compute w 0 z ( x ) . Despite its simplicity, ridge regression with random features is faster than, and provides competitive accuracy with, alternative methods. It also produces very compact functions because only w and a set of O ( D ) random vectors or a hash-table of partitions need to be retained. Random Fourier features perform better on the tasks that largely rely on interpolation. On the other hand, random binning features perform better on memorization tasks (those for which the standard SVM requires many support vectors), because they explicitly preserve locality in the input space. This difference is most dramatic in the Forest dataset.
 Figure 3(left) illustrates the benefit of training classifiers on larger datasets, where accuracy con-tinues to improve as more data are used in training. Figure 3(middle) and (right) show that good performance can be obtained even from a modest number of features. We have presented randomized features whose inner products uniformly approximate many popular kernels. We showed empirically that providing these features as input to a standard linear learning algorithm produces results that are competitive with state-of-the-art large-scale kernel machines in accuracy, training time, and evaluation time.
 It is worth noting that hybrids of Fourier features and Binning features can be constructed by con-catenating these features. While we have focused on regression and classification, our features can be applied to accelerate other kernel methods, including semi-supervised and unsupervised learn-ing algorithms. In all of these cases, a significant computational speed-up can be achieved by first computing random features and then applying the associated linear technique. We thank Eric Garcia for help on early versions of these features, Sameer Agarwal and James R. Lee for helpful discussions, and Erik Learned-Miller and Andres Corrada-Emmanuel for helpful corrections.
 Lemma 1. Suppose a function k ( X ) : R  X  R is twice differentiable and has the form R 0 p (  X  ) max(0 , 1  X  Proof. We want p so that p ( X ) /  X  . are shift invariant, as their arguments we use  X   X  x  X  y  X  M  X  for notational simplicity. M  X  is compact and has diameter at most twice diam( M ) , so we can find an -net that covers M  X  using at most T = (4 diam M /r ) d balls of radius r [17]. Let {  X  i } T i =1 denote the centers of these balls, and let L f denote the Lipschitz constant of f . We have | f ( X ) | &lt; for all  X   X  M  X  if | f ( X  i ) | &lt; / 2 and L f &lt; Since f is differentiable, L f = k X  f ( X   X  ) k , where  X   X  = arg max  X   X  X   X  k X  f ( X ) k . We have by Markov X  X  inequality, Pr[ L 2 f  X  t ]  X  E [ L 2 f ] /t , or The union bound followed by Hoeffding X  X  inequality applied to the anchors in the -net gives Combining (5) and (6) gives a bound in terms of the free variable r : This has the form 1  X   X  1 r  X  d  X  k 2 r 2 . Setting r =  X  1  X  assuming that  X  p diam( M )  X  1 and diam( M )  X  1 , proves the first part of the claim. To prove the second part of the claim, pick any probability for the RHS and solve for D .
 Proof of Claim 2. M can be covered by rectangles over each of which z is constant. Let  X  pm be the pitch of the p th grid along the m th dimension. Each grid has at most d diam( M ) / X  pm e bins, and P overlapping grids produce at most N m = P P g =1 d diam( M ) / X  gm e  X  P + diam( M ) P P p =1 1  X  partitions along the m th dimension. The expected value of the right hand side is P + P diam( M )  X  . By Markov X  X  inequality and the union bound, Pr  X  d m =1 N m  X  t ( P + P diam( M )  X  )  X  1  X  d/t . That is, with probability 1  X  d/t , along every dimension, we have at most t ( P + P diam( M )  X  ) one-dimensional cells. Denote by d mi the width of the i th cell along the m th dimension and observe that P N m i =1 d mi  X  diam( M ) . We further subdivide these cells into smaller rectangles of some small width r to ensure that the kernel k varies very little over each of these cells. This results in at most P upper bound for N m , setting t  X  1  X P and assuming  X  diam( M )  X  1 , with probability 1  X  d/t , M and z ( x ) is constant throughout each rectangle. With r d = 2 L Hoeffding X  X  inequality gives Combining this with the probability that z ( x ) is constant in each cell gives a bound in terms of t :
Pr sup This has the form 1  X   X  1 t  X  1  X   X  2 t d . To prove the claim, set t =  X  1 2  X  upper bound of 1  X  3  X  1  X 
