 With ever more capable smartphones connecting users to cloud-based computing, voice has been a rapidly growing modality for searching for infor-mation online. Our voice search application con-nects a speech recognition service with a search engine, providing users with structured answers to questions, Web results, voice actions such as set-ting an alarm, and more. In the multimodal smart-phone interface, users can press a button to ac-tivate the microphone, and then speak the query when prompted by a beep; after receiving results, the microphone button is available if they wish to follow up with a subsequent voice query.

Traditionally, the evaluation of speech recogni-tion systems has been carried by preparing a test set of annotated utterances and comparing the ac-curacy of a system X  X  transcripts of those utterances against the annotations. In particular, we seek to measure and minimize the word error rate (WER) of a system, with a WER of zero indicating perfect transcription. For voice search interfaces such as the present one, though, query-level metrics like WER only tell part of the story. When a user is-sues two queries in a row, she might be seeking the same information for a second time due to a sys-tem failure the first time. When this happens, from an evaluation standpoint it is helpful to break down why the first query was unsuccessful: it might be a speech recognition issue (in particular, a mis-taken transcription), a search quality issue (where a correct transcript is interpreted incorrectly by the semantic understanding systems), a user interface issue, or another factor. As a second voice query may also be a new query or a follow-up query, as opposed to a retry of the first query, the detection of voice search retry pairs in the query steam is non-trivial.

Correctly identifying a retry situation in the query stream has two main benefits. The first involves offline evaluation and monitoring. We would like to know the rate at which users were forced to retry their voice queries, as a measure of quality. The second has a more immediate ben-efit for individual users: if we can detect in real time that a new voice search is really a retry of a previous voice search, we can take immediate cor-rective action, such as reranking transcription hy-potheses to avoid making the same mistake twice, or presenting alternative searches in the user inter-face to indicate that the system acknowledges it is having difficulty.

In this paper, we describe a method for the clas-sification of subsequent voice searches as either retry pairs of a certain type, or non-retry pairs. We identify four salient types of retry pairs, describe a test set and identify the features we extracted to build an automatic classifier. We then describe the models we used to build the classifier and their rel-ative performance on the task, and leave the issue of real-time corrective action to future work. Previous work in voice-enabled information re-trieval has investigated the problem of identifying voice retries, and some has taken the additional step of taking corrective action in instances where the user is thought to be retrying an earlier utter-ance. Zweig (2009) describes a system switching approach in which the second utterance is recog-nized by a separate model, one trained differently than the primary model. The  X  X ackup X  system is found to be quite effective at recognizing those utterances missed by the primary system. Retry cases are identified with joint language modeling across multiple transcripts, with the intuition that retry pairs tend to be closely related or exact dupli-cates. They also propose a joint acoustic model in which portions of both utterances are averaged for feature extraction. Zweig et al. (2008) similarly create a joint decoding model under the assump-tion that a discrete sent of entities (names of busi-nesses with directory information) underlies both queries. While we follow this work in our usage of joint language modeling, our application encom-passes open domain voice searches and voice ac-tions (such as placing calls), so we cannot use sim-plifying domain assumptions.

Other approaches include Cevik, Weng and Lee (2008), who use dynamic time warping to de-fine pattern boundaries using spectral features, and then consider the best matching patterns to be re-peated. Williams (2008) measures the overlap be-tween the two utterances X  n-best lists (alternate hy-potheses) and upweights hypotheses that are com-mon to both attempts; similarly, Orlandi, Culy and Franco (2003) remove hypotheses that are seman-tically equivalent to a previously rejected hypoth-esis. Unlike these approaches, we do not assume a strong notion of dialog state to maintain per-state models.

Another consequence of the open-domain na-ture of our service is that users are conditioned to interact with the system as they would with a search engine, e.g., if the results of a search do not satisfy their information need, they rephrase queries in order to refine their results. This can happen even if the first transcript was correct and the rephrased query can be easily confused for a retry of a utterance where the recognition failed. For purposes of latently monitoring the accuracy of the recognizer from usage logs, this is a signifi-cant complicating factor. Our data consists of pairs of queries sampled from anonymized session logs. We consider a pair of voice searches (spoken queries) to be a potential retry pair if they are consecutive; we assume that a voice search cannot be a retry of another voice search if a typed search occurs between them. We also exclude pairs for which either member has no recognition result. For the purpose of our analy-sis, we further restricted our data to query pairs whose second member had been previously ran-domly selected for transcription. A set of 8,254 query pairs met these requirements and are consid-ered potential retry pairs. 1,000 randomly selected pairs from this set were separated out and anno-tated by the authors, leaving a test set of 7,254 po-tential retry pairs. Among the annotated develop-ment set, 18 inaudible or unintelligible pairs were discarded, for a final development set of 982 pairs.
The problem as we have formulated it requires a labeling system that identifies repetitions and rephrases as retries, while excluding query pairs that are superficially similar but have different search intents. Our system includes five labels. Figure 1 shows the guidelines for annotation that define each category.

The first distinction is between query pairs with the same search intent ( X  X s the user looking for the same information? X ) and those with different search intents. We define search intent as the re-sponse the user wants and expects from the sys-tem. If the second query X  X  search intent is differ-ent, it is by definition no retry .

The second distinction we make is between cases where the first query was recognized cor-rectly and those where it was not. Although a query that was recognized correctly may be retried X  X or example, the user may want to be reminded of information she already received ( other ) X  X e are only interested in cases where the system is in error.

If the search intent is the same for both queries, and the system incorrectly recognized the first, we consider the second query a retry. We dis-tinguish between cases where the user repeated the query exactly, repetition , and where the user rephrased the query in an attempt to adapt to the system X  X  failure, rephrase . This category includes many kinds of rephrasings, such as adding or drop-ping terms, or replacing them with synonyms. The rephrased query may be significantly differ-ent from the original, as in the following example:
The rephrased query dropped a term ( X  X avigate to X ) and added another ( X  X ittle Italy Baltimore X ).
This example illustrates another difficulty of the data: the unreliability of the automatic speech recognition (ASR) means that terms that are in fact identical ( X  X hiapparelli X  X  X ) may be recog-nized very differently ( X  X haparral ease X  or  X  X hip-per rally X  X  X ). In the next example, the recognition hypotheses of two identical queries have only a single word in common:
Conversely, recognition hypotheses that are nearly identical are not necessarily retries. Often, these are  X  X erial queries, X  a series of queries the user is making of the same form or on the same topic, often to test the system.

These complementary problems mean that we cannot use na  X   X ve text similarity features to identify retries. Instead, we combine features that model the first query X  X  likely accuracy to broader similar-ity features to form a more nuanced picture of a likely retry.

The five granular retry labels were collapsed into binary categories: search retry, other, and no retry were mapped to NO RETRY; and repetition and rephrase were mapped to RETRY. The label distribution of the final dataset is shown in Figure 2. The features we consider can be divided into three main categories. The first group of features, sim-ilarity , is intended to measure the similarity be-tween the two queries, as similar queries are (with the above caveats) more likely to be retries. We calculate the edit distance between the two tran-scripts at the character and word level, as well as the two most similar phonetic rewrites. We include both raw and normalized values as features. We also count the number of unigrams the two tran-scripts have in common and the length, absolute and relative, of the longest unigram overlap.
As we have shown in the previous section, sim-ilarity features alone cannot identify a retry, since ASR errors and user rephrases can result in recog-nition hypotheses that are significantly different from the original query, while a nearly identical pair of queries can have different search intents. Our second group of features, correctness , goes up a level in our labeling decision tree (Figure 1) and attempts to instead answer the question:  X  X as the first query transcribed incorrectly? X  We use the confidence score assigned by the recognizer to the first recognition hypothesis as a measure of the system X  X  opinion of its own performance. Since this score, while informative, may be inaccurate, we also consider signals from the user that might indicate the accuracy of the hypothesis. A boolean feature indicates whether the user interacted with any of the results (structured or unstructured) that were presented by the system in response to the first query, which should constitute an implicit ac-ceptance of the system X  X  recognition hypothesis. The length of the interval between the two queries is another feature, since a query that occurs imme-diately after another is likely to be a retry. We also include the difference and ratio of the two queries X  speaking rate, roughly calculated as the number of vowels divided by the audio duration in sec-onds, since a speaker is likely to hyperarticulate (speak more loudly and slowly) after being misun-derstood ((Wade et al., 1992; Oviatt et al., 1996; Levow, 1998; Bell and Gustafson, 1999; Soltau and Waibel, 1998)).

The third feature group, recognizability , at-tempts to model the characteristics of a query that is likely to be misrecognized (for the first query of the pair) or is likely to be a retry of a previ-ous query (for the second query). We look at the language model (LM) score and the number of al-ternate pronunciations of the first query, predicting that a misrecognized query will have a lower LM score and more alternate pronunciations. In ad-dition, we look at the number of characters and unigrams and the audio duration of each query, with the intuition that the length of a query may be correlated with its likelihood of being retried (or a retry). This feature group also includes two heuristic features intended to flag the  X  X erial queries X  mentioned before: the number of capital-ized words in each query, and whether each one begins with a question word (who, what, etc.). 5.1 Experimental Results A logistic regression model was trained on these features to predict the collapsed binary categories of NO RETRY (search retry, other, no retry) vs. RETRY (rephrase, repetition). The results of run-ning this model with each combination of the fea-ture groups are shown in Table 1.

Individually, each feature group peformed sig-nificantly better than the baseline strategy of al-ways predicting NO RETRY (62.4%). Each pair of feature groups performed better than any indi-vidual group, and the final combination of all three feature groups had the highest precision, recall, and accuracy, suggesting that each aspect of the retry conceptualization provides valuable informa-tion to the model.

Of the similarity features, the ones that con-tributed significantly in the final model were char-acter edit distance (normalized) and phoneme edit distance (raw and normalized); as expected, re-tries are associated with more similar query pairs. Of the correctness features, high recognizer con-fidence, the presence of a positive reaction from the user such as a link click, and a long inter-val between queries were all negatively associated with retries. The significant recognizability fea-tures included length of the first query in charac-ters (longer queries were less likely to be retried) and the number of capital letters in each query (as our LM is case-sensitive): queries transcribed with more capital letters were more likely to be retried, but less likely to themselves be retries. In addition, the language model likelihood for the first query was, as expected, significantly lower for retries. Interestingly, the score of the second query was lower for retries as well. This accords with our finding that retries of misrecognized queries are themselves misrecognized 60%-70% of the time, which highlights the potential value of corrective action informed by the retry context.

Several features, though not significant in the model, are significantly different between the RETRY and NO RETRY categories, which affords us further insight into the characteristics of a retry. T -tests between the two categories showed that all edit distance features X  X haracter, word, reduced, and phonetic; raw and normalized X  X re signifi-Similarly, the number of unigrams the two queries have in common is significantly higher for retries. The duration of each member of the query pair, in seconds and word count, is significantly more similar between retry pairs, and each member of a retry pair tends to be shorter than members of a no retry pair. Finally, members of NO RETRY query pairs were significantly more similar in speaking rate, and the relative speaking rate of the second query was significantly slower for RETRY pairs, possibly due to hyperarticulation. 5.2 Analysis Figure 3 shows a breakdown of the true granular labels versus the predicted binary labels. The pri-mary source of error is the REPHRASE category, which is identified as a retry with only 16.5% ac-curacy. This result reflects the fact that although rephrases conceptually belong in the retry cate-gory, their characteristics are materially different. Most notably, all edit distance features are signif-icantly greater for rephrases. Differences in du-ration between the two queries in a pair, in sec-onds and words, are significantly greater as well. Rephrases also are significantly longer, in seconds and words, than strict retries. The model includ-ing only correctness and recognizability features does significantly better on rephrases than the full model, identifying them as retries with 25.6% ac-curacy, confirming that the similarity features are the primary culprit. Future work may address this issue by including features crafted to examine the similarity between substrings of the two queries, rather than the query as a whole, and by expand-ing the similarity definition to include synonyms.
To test the model X  X  performance with a larger, unseen dataset, we looked at how many retries it detected in the test set of potential retry pairs (n=7,254). We do not have retry annotations for this larger set, but we have transcriptions for the first member of each query pair, enabling us to cal-culate the word error rate (WER) of each query X  X  recognition hypothesis, and thus obtain ground truth for half of our retry definition. A perfect model should never predict RETRY when the first query is transcribed correctly (WER==0). As shown in Figure 4, our model assigns a RETRY label to approximately 14% of the queries follow-ing an incorrectly recognized search, and only 2% of queries following a correctly recognized search. While this provides us with only a lower bound on our model X  X  error, this significant correlation with an orthogonal accuracy metric shows that we have modeled at least this aspect of retries correctly, and suggests a correlation between retry rate and tradi-tional WER-based evaluation.
 We have presented a method for characterizing re-tries in an unrestricted voice interface to a search system. One particular challenge is the lack of simplifying assumptions based on domain and state (as users may consider the system to be stateless when issuing subsequent queries). We introduce a labeling scheme for retries that en-compasses rephrases X  X ases in which the user re-worded her query to adapt to the system X  X  error X  as well as repetitions.

Our model identifies retries with 81% accuracy, significantly above baseline. Our error analysis confirms that user rephrasings complicate the bi-nary class separation; an approach that models typical typed rephrasings may help overcome this difficulty. However, our model X  X  performance to-day correlates strongly with an orthogonal accu-racy metric, word error rate, on unseen data. This suggests that  X  X etry rate X  is a reasonable offline quality metric, to be considered in context among other metrics and traditional evaluation based on word error rate.
 The authors thank Daisy Stanton and Maryam Kamvar for their helpful comments on this project.
