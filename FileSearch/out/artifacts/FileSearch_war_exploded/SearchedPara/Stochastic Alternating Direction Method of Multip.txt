 Hua Ouyang y houyang@cc.gatech.edu Niao He z nhe6@isye.gatech.edu Long Q. Tran y ltran3@gatech.edu Alexander Gray y agray@cc.gatech.edu
School of Computational Science and Engineering, Georgia Tech The Alternating Direction Method of Multipliers (ADMM) ( Glowinski &amp; Marroco , 1975 ; Gabay &amp; Mercier , 1976 ) is a very simple computational method for optimization proposed in 1970s. It stemmed from the augmented Lagrangian method (also known as the method of multipliers) dating back to late 1960s. The theoretical aspects of ADMM have been studied since 1980s, and its global convergence was established in Eckstein &amp; Bertsekas , 1992 ). As reviewed in the com-prehensive paper ( Boyd et al. , 2010 ), with the ability of dealing with objective functions separately and syn-chronously , ADMM turned out to be a natural t in the eld of large-scale data-distributed machine learn-ing and big-data related optimization, and therefore received signi cant amount of attention in the last few years. Considerable work was conducted thereafter. On the theoretical side, ADMM was shown to have an O (1 =N ) rate of convergence for convex problems &amp; Banerjee , 2012 ), where N stands for the number of iterations. When objective functions are strongly convex and Lipschitz smooth, linear convergence rates were reported very recently ( Hong &amp; Luo , 2012 ; Deng &amp; Yin , 2012 ). On the practical side, ADMM has been applied to a wide range of application domains, such as compressed sensing ( Yang &amp; Zhang , 2011 ), image restoration ( Goldstein &amp; Osher , 2009 ), video process-ing and matrix completion ( Goldfarb et al. , 2010 ). Be-sides that, many variations of this classical method have been recently developed, such as linearized ( Gold-farb et al. , 2010 ; Zhang et al. , 2011 ; Yang &amp; Yuan , 2012 ), accelerated ( Goldfarb et al. , 2010 ) and online ( Wang &amp; Banerjee , 2012 ) ADMM. However, most of these variants including the classic one implicitly as-sume full accessibilty of true data values, while in reality one can hardly ignore the existence of noise. A more natural way of handling this issue is to con-sider unbiased or even biased observations of true data, which leads us to the stochastic setting. 1.1. Stochastic Setting for ADMM In this work, we study a family of convex optimization problems where our objective functions are stochastic and composite. Speci cally, we are interested in the following equality-constrained stochastic optimization: R m , X is a convex compact set, and Y is a closed convex set. We use the notation 1 for both the instance function value 1 ( x ; ) and its expectation ( x ) E 1 ( x ; ). We are able to draw a sequence of identical and independent (i.i.d.) observations from the random vector that obeys a xed but unknown distribution P . When is deterministic, we can re-cover the traditional problem formulation of ADMM ( Boyd et al. , 2010 ). In our most general setting, real-valued functions 1 ( ) and 2 ( ) are convex but not necessarily continuously di erentiable. We will make additional assumptions in Section 4 , in which we sug-gest more structural information on 1 . 1.2. Motivations The stochasticity of the proposed setting is inspired by the structural risk minimization principle ( Vap-nik , 2000 ). Under this principle, a statistical learn-ing system's goal is to minimize the regularized ex-pected risk function : R ( x ) E L ( x ; ) + ( x ), where L ( x ; ) is the loss incurred when applying prediction rule x on a sample , and is a regularizer. In the batch learning setting, one uses a set of training sam-ples to minimize the regularized empirical risk func-probability, R and R emp are close when the number of samples is large ( Vapnik , 2000 ). However, to mini-mize R emp one has to handle larger amount of samples which becomes less ecient under time and resource constraints. In the stochastic setting, in each iteration x is updated based on one noisy sample drawn from P instead of a nite training set. One obvious advan-tage is that the update costs much less time and re-sources than in the batch setting. Another advantage we will show later in this paper is that, when care-fully designed, our algorithm optimizes the expected risk directly with good rates of convergence. The proposed stochastic ADMM setting ts per-fectly with the regularized expected risk minimization. Putting it into our canonical form ( 1 ): 1 ( x ; ) = L ( x ; ) ; 2 ( y ) = ( y ), and the constraint becomes x = y . Beyond this simple formulation, the objective separation of ADMM is so exible that one can use a more general linear constraint A x + B y = b to model the complex structural information encoded in the reg-ularizer ( x ). For example, if ( v 1 ; v 2 ) = j v 1 v could add a variable v 12 , a linear constraint v 1 v 2 = v 12 , and simply minimize ( v 12 ) = ier to handle under our stochastic setting for ADMM. More examples will be given in Section 5 . 1.3. Our Contributions We propose a stochastic setting of the ADMM prob-lem and also design the Stochastic ADMM algorithm to solve this problem. A key algorithmic feature of our Stochastic ADMM that distinguishes our method from previous ADMM and its variants is the rst-order approximation of 1 that we used to modify the augmented Lagrangian. This simple modi cation is not only necessary for the convergence analysis of our stochastic method, but also makes our method appli-cable to a more general class of convex objective func-tions which might not have a closed-form solution in minimizing the augmented 1 directly. Moreover, the linearization makes the updates simpler and faster, as demonstrated by the examples in Section 5 .
 We establish convergence rates under various struc-tural assumptions of 1 : O (1 = tions and O (log t=t ) for strongly convex functions in terms of both the objective value and the feasibility violation. By contrast, recent research ( He &amp; Yuan , 2012a ; b ; Wang &amp; Banerjee , 2012 ) only show the con-vergence of ADMM indirectly in terms of the satisfac-tion of variational inequalities. We also demonstrate the usefulness of our algorithm with a novel applica-tion in Graph-Guided Support Vector Machine. 1.4. Related Work A related setting named online ADMM was proposed in ( Wang &amp; Banerjee , 2012 ). In this setting, one does not assume to be i.i.d., nor the objective to be stochastic, and the minimization of regret is concerned: R ( x [1: t ] ) also di ers from our stochastic ADMM algorithmi-cally: a nonlinearized 1 is used in online ADMM, while a linearized one is adopted in our algorithm. In an independent work ( Suzuki , 2013 ), the author also linearized 1 , and proposed dual averaging and proximal gradient methods for problem ( 1 ). The pro-posed OPG-ADMM algorithm enjoys the same order of convergence rates as our stochastic ADMM. 1.5. Notations Throughout this paper, we denote a subgradient of a function f as f 0 . When f is di erentiable, we will use r f . We denote by ( u ) 1 ( x ) + 2 ( y ) the sum of the stochastic and the deterministic functions. For simplicity and clarity, we will use the following notations to denote stacked vectors or tuples: the For a positive semide nite matrix G 2 R d 1 d 1 , we de ne the G -norm of a vector as k x k G := k G 1 = 2 x k p x T G x . We use h ; i to denote the inner prod-uct in a nite dimensional Euclidean space. When there is no ambiguity, we often use k k to denote the Euclidean norm kk 2 . For a di erentiable func-tion ! ( ), Bregman divergence is de ned as D ( u ; v ) ! ( u ) ! ( v ) hr ! ( v ) ; u v i .
 We assume that the optimal solution of ( 1 ) exists, and is denoted as u ( x T ; y T ) T . The following quantities appear frequently in our convergence analysis.
D X sup 1.6. Assumptions Before presenting the algorithm and convergence re-sults, we list the following assumptions that will be used in our statements. These assumptions provide bounds for the magnitude and variance of subgradi-ents for the stochastic function.
 Assumption 1. For all x 2X , E Assumption 2. For all x 2X , Assumption 3. For all x 2X , Directly solving problem ( 1 ) can be nontrivial, even if is deterministic and the equality constraint is as simple as x y = 0 . For example, using the aug-mented Lagrangian method, one has to minimize the augmented Lagrangian : h ; A x + B y b i + where is a pre-de ned penalty parameter. This prob-lem is at least not easier than solving the original one. The (deterministic) ADMM (Alg. 1 ) solves this prob-lem in a one-sweep Gauss-Seidel manner: minimizing L w.r.t. x and y alternatively given the other xed, followed by a penalty update over the Lagrangian mul-tiplier .
 Algorithm 1 Deterministic ADMM 0. Initialize y 0 and 0 = 0. for k = 0 ; 1 ; 2 ; : : : do 1. x k +1 arg min 2. y k +1 arg min end for A variant deterministic algorithm named linearized ADMM replaces Line 1 of Alg. 1 by ant can be regarded as a generalization of the original ADMM. When G = 0 , it is the same as Alg. 1 . When G = rI d 1 A T A , it is equivalent to the following linearized proximal point method: Note that the linearization is only applied to the quadratic function k ( A x + B y k b ) k = k 2 , but not to 1 . This approximation helps in some cases when Line 1 of Alg. 1 does not produce a closed-form solution given the quadratic term. For example, let ( x ) = k x k 1 and A not identity.
 As given in Alg. 2 , we propose a Stochastic Alternating Direction Method of Multipliers ( Stochastic ADMM ) algorithm. Our algorithm shares some features with the classical and the linearized ADMM. One can see that Line 2 and 3 are essentially the same as before. However we have a di erent updating rule for x as shown in Line 1, where we de ne an approximated aug-mented Lagrangian : ^ L ;k ( x ; y ; ) 1 ( x k ) + h ; A x + B y b i + There are two di erences between L ( 4 ) and ^ L ;k ( 5 ). First, we replace 1 ( x ) with a rst-order approxi-mation of 1 ( x ; k +1 ) at x k : 1 ( x k ) + x T 0 1 ( x This approximation has the same avour of the stochastic mirror descent ( Nemirovski et al. , 2009 ) used for solving a one-variable stochastic convex prob-lem. Second, similar to the linearized ADMM, we add an l 2 -norm prox-function k x x k k 2 but scale it by a time-varying stepsize k +1 . As we will see in Section 3 , the choice of this stepsize is crucial in guaranteeing a convergence.
 Algorithm 2 Stochastic ADMM 0. Initialize x 0 ; y 0 and set 0 = 0. for k = 0 ; 1 ; 2 ; : : : do 1. x k +1 arg min 2. y k +1 arg min end for In this section, we will show that our Stochastic ADMM given in Alg. 2 exhibits a rate O (1 = vergence in terms of both the objective value and the feasibility violation:
E [ All proofs in this section are provided in a longer ver-sion of this paper that is available at arXiv.org. Before we address the main theorem on convergence rates, we will start with the following simple lemma, which is a very useful result by implementing Bregman divergence as a prox-function in proximal methods. Lemma 1. Let l ( x ) : X ! R be a convex di erentiable function with gradient g . Let scalar s 0 . For any vector u and v , denote their Bregman divergence as D ( u ; v ) . If 8 u 2X , then h g ( x ) ; x x i s [ D ( x ; u ) D ( x ; x ) D ( x ; u )] : Utilizing the above lemma, we are able to obtain an upper bound of the variation of the Lagrangian func-tion and its rst order approximation based on each iteration points.
 Lemma 2. 8 w 2W ; k 0 we have 2 h In what follows, we will present our main theorem of the convergence in two fashions, both in terms of ex-pectation and probability satis cation.
 Theorem 1. Let k = D X
M 1 ( t ) Then 8 &gt; 0 and t 1 we have: (i) Under Assumption 1 (ii) Under Assumption 1 and 2 , 8 &gt; 0 Remark 1. Observe that our proof techniques can also be adapted to the deterministic case where no noise takes place. We are able to obtain a similar result for the classic deterministic ADMM: ( u t ) ( u ) + k A x t + B y t b k 2 The positive in the preceding results controls the trade-o between the objective value reduction and the feasibility satisfaction. For a xed , one can set an optimal = =D y ;B such that the upper bound is minimized.
 While the resulting O (1 =t ) rate for the deterministic ADMM is the same as those in the existing literature, the above nding is an advance in the theoretical as-pects of ADMM. Our convergence rate for general con-vex functions is proved in terms of both the objec-tive value and the feasibility violation. By contrast, Banerjee , 2012 ) only shows the convergence of ADMM in terms of the satisfaction of variational inequalities, which is not a direct measure of how fast an algorithm reaches the optimal solution.
 4.1. Strongly Convex 1 When function 1 ( ) is strongly convex, the conver-gence rate of Stochastic ADMM can be improved to O ( Theorem 2. When 1 is -strongly convex with re-spect to kk , taking k = 1 k in Alg. 2 , under Assump-tion 1 we have 8 &gt; 0 ; t 1 , 4.2. Lipschitz Smooth 1 Since the bounds given in Theorem 1 are related to the magnitude of subgradients, they do not provide any intuition of the performance in low-noise scenarios. With a Lipschitz smooth function 1 , we are able to obtain convergence rates in terms of the variations of gradients, as stated in Assumption 3 . Besides, under this assumption we are able to replace the unusual de nition of u k in ( 2 ) with the following: Theorem 3. When 1 ( ) is L -Lipschitz smooth with respect to kk , taking k = 1 Assumption 3 we have 8 &gt; 0 ; t 1 , 5.1. Lasso As one of the many motivating examples given in the review of ADMM ( Boyd et al. , 2010 ), the l 1 -regularized sparse least squares problem, also known as lasso , ts the general class of ( 1 ) very naturally. The composite functions can be written as: where the training sample contains feature-label pair f s ; l g and is a regularization parameter. The con-straint simply becomes A = I; B = I; b = 0 . Same as in the deterministic case, applying stochas-tic ADMM to l 1 -regularized problem produces closed-form updating rules. The three updates for ( 14 ) are actually very simple: x y where the soft-thresholding operator S ( x ) is de ned in the same way as in ( Boyd et al. , 2010 ): Some vector-scaling operations can be saved by by re-placing k with k in ( 15 ): x y For simple problems like lasso, it is indeed not nec-essary to formulate it as a two-variable equality-constrained optimization. Instead, we can directly minimize E ( l x T s ) 2 + k x k 1 without any constraint. A popular class of methods for solving this composite-objective problem is called proximal gradient ( Tseng , 2008 ; Nemirovski &amp; Yudin , 1983 ) or proximal splitting ( Combettes &amp; Pesquet , 2011 ), which was investigated in various communities ( Daubechies et al. , 2004 ; Com-2007 ; Wright et al. , 2009 ). Stochastic and online vari-ants of these methods have also been developed re-cently, mainly in the large-scale machine learning and optimization literature ( Langford et al. , 2009 ; Lan , 2010 ; Lan &amp; Ghadimi , 2011 ; Duchi &amp; Singer , 2009 ; Hu et al. , 2009 ; Xiao , 2010 ). For comparison pur-poses, here we take the online forward-backward split-ting method (FOBOS) ( Combettes &amp; Pesquet , 2011 ; Duchi &amp; Singer , 2009 ) as a rst example. The FOBOS can be regarded as a proximal method with lineariza-tion of 1 : x Comparing this method with our Alg. 2 , we can see that ( 16 ) is actually a special Stochastic ADMM that enforces x k = y k (hence k = k = 0 ) in every iter-ation k . Note that this constraint feasibility is easy to enforce only because lasso comes with an extremely simple constraint x = y . One of the most attractive features of ( 16 ) is its closed form solution for lasso in terms of soft-thresholding: As we will see in our next example (Sec. 5.2 ), with com-plex constraints, applying proximal splitting methods might not produce closed-form updates.
 The second algorithm we are going to compare with is the online ADMM ( Wang &amp; Banerjee , 2012 ), which was proposed under a related but di erent setting of online learning. In this algorithm, the rst-order ap-the exact function 1 ( x ; k ), which is a very straight-forward \stochastization" of the deterministic ADMM. Applying this algorithm to lasso yields the following update for x : dates for y and remain the same as our stochas-tic ADMM. Comparing the x updates of online and stochastic ADMM, it is clear that the linearization of our algorithm results in a much simpler inner prod-uct calculation, while a rank-1 matrix inversion is re-quired for the online ADMM. Even with the Sherman-Morrison formula, this inversion process is still slower than the stochastic ADMM.
 In the following experiments, we investigate two real-world datasets to examine the eciency of our algo-rithm. Table 1 shows the statistics of these datasets and parameters we used for lasso. The rst dataset, Abalone , obtained from the UCI Machine Learning Repository 1 , is used to predict the age of abalones from physical measurements. The second dataset, E2006-tf-idf , a part of the 10K-Corpus 2 , is used to predict the volatility of stock returns, an empirical measure of the nancial risk of a company. The features are tf-idf of unigrams extracted from the nancial reports of com-panies during the years 1996-2006 ( Kogan et al. , 2009 ). The prediction results are shown in Fig. 1 and 2 . One can observe that all algorithms converge reasonably well, as expected from our discussions above. The stochastic ADMM performs slightly better than the other two in Abalone . For E2006-tf-idf , an acceptable accuracy is achieved with a fast sweep of merely 2 ; 000 samples, less than 25% of the entire dataset. 5.2. Graph-Guided SVM Stochastic ADMM is more powerful for problems with complex equality constraints, for which proximal split-ting methods such as FOBOS are no longer applicable, since there will be no closed-form for it. An important class of these problems is called the generalized lasso ( Tibshirani &amp; Taylor , 2011 ): where the linear transformation F 2 R f d 1 encodes the structural prior of a speci c problem. When F = I , one recovers lasso. We can write ( 17 ) in our canonical form ( 1 ) with where = f s ; l g is a feature-label pair.
 As a concrete example of the generalized lasso, we eval-uate our algorithm based on the graph-guided fused lasso (GFlasso) framework ( Kim et al. , 2009 ), a graph-ical extension of the well-known fused lasso ( Tibshi-rani et al. , 2004 ). Denote graph G fV ; Eg , in which V = f x 1 ; : : : ; x d g is a set of the d variables of x and the set of edges among V . Each edge f i; j g is assigned with a weight w ij . The optimization of GFlasso can thus be formulated as: min E The only di erence between GFlasso and lasso is the last term, referred as the fusion penalty ( Tibshirani et al. , 2004 ), which penalizes the di erences among variables connected in G . A carefully designed fusion penalty helps in further reducing the risk of over t-ting of our model over the training data. To imple-ment Alg. 2 to this problem, we only need to formu-late the linear transformation F , which is very simple for GFlasso: F ij = w ij and F ji = w ij for any edge f i; j g2E .
 According to our convergence analysis, the loss 1 and regularizator 2 are allowed to be any convex func-tions. To meet the goal of classi cation, we replace the least squares loss in ( 19 ) by a nonsmooth hinge loss L ( x ; ) max f 0 ; 1 l s T x g and the l 1-norm by an Euclidean norm to enforce the maximum margin. The resulting combination is also known as support vector machine (SVM). With the additional graph-guided fu-sion penalty, we name our formulation Graph-Guided SVM (GGSVM) : Before presenting the penalty term, we rst give an algorithmic solution of ( 20 ). Applying our stochastic ADMM to GGSVM, we obtain the following updates: Without the graph-guided regularization, the stochas-tic ADMM becomes exactly the same as the clas-sic stochastic gradient descent (SGD): x k +1 The rst two updates of ( 21 ) have close-forms: Note that this simple x -update is exactly the bene t that stochastic ADMM brings. In contrast, neither the classic ADMM nor its variants have closed-forms due to the nonseparable form of the hinge loss.
 In each x -update of ( 22 ), due to the time-varying k +1 one has to solve a symmetric linear system with a dif-ferent system matrix. This can be carried out using standard methods, e.g. conjugate gradient, where the sparsity of F T F can help in reducing the time com-plexity. However, for large-scale problems we can re-move this computational burden completely by replac-ing k +1 with a xed t , if we want to run t itera-tions. This indeed leads to a convergent algorithm, although the proof is not shown in Section 3 due to limited space. By this means we only need to solve the linear system once , and save the result for succes-sive iterations.
 The data is the publicly available 20newsgroups dataset 3 , which contains binary occurrences of 100 popular words counted from 16 ; 242 newsgroup post-ings. On the top level of these postings are 4 main cat-egories: computer, recreation, science and talks. We are interested in a multi-class classi cation task: to predict the category that a posting belongs to. We split the original data into a training set and a testing set. In each posting category, 80% postings are used for training and the rest 20% for testing. We use the one-vs-rest scheme for the multi-class classi cation. The graphical structures we want to explore are the dependencies among these 100 words. Speci cally, if two words i and j are strongly dependent, the di er-ence between x i and x j in the linear predictor x 2 R 100 should be penalized. In order to obtain F , we use the sparse inverse covariance selection ( Banerjee et al. , 2008 ) (also known as graphical lasso ( Friedman &amp; Tib-shirani , 2007 ; Boyd et al. , 2010 )) and determine the sparsity pattern of the inverse covariance matrix 1 . By properly thresholding the components of 1 to 0 and 1, we obtain the anity matrix of G and plot the relations of these 100 words accordingly in Fig. 3 . For simplicity, we take all the weights in F to be 1 and 1 whenever there is an edge. We compare the prediction accuracies with and with-out graphical regularization. Fig. 4 shows the ex-perimental results. The x-axis stands for the number of epochs for stochastic algorithms. For this dataset, each epoch means 12 ; 994 iterations. We calculate the mean and the standard deviation of all the accuracies based on 10 runs of experiments under the same set-ting. This gure clearly indicates that GGSVM out-performs the classical SVM consistently in every set-ting. After a single epoch, which corresponds to 1 iteration for the deterministic ADMM, the prediction accuracy is already very close to the best. This is a further evidence for the eciency of our stochastic ADMM. In this paper, we have proposed the stochastic set-ting for ADMM along with our stochastic linearized ADMM algorithm. As a bene t of the rst-order ap-proximation on the stochastic function, our algorithm is applicable to a very broad class of problems even with functions that have no closed-form solution to the subproblem of minimizing the augmented 1 . We have also established convergence rates under various struc-tural assumptions of 1 : O (1 = and O (log t=t ) for strongly convex functions. We are working on integrating Nesterov's optimal rst-order methods ( Nesterov , 2004 ) to our algorithm, which will help in achieving optimal convergence rates. More in-teresting and challenging applications will be carried out in our future work.

