
Although mixed-membership models have achieved great success in unsupervised learning, they have not been we propose a family of discriminative mixed-membership models for classification by combining unsupervised mixed-membership models with multi-class logistic regression. I n particular, we propose two variants respectively applica-ble to text classification based on latent Dirichlet alloca-tion and usual feature vector classification based on mixed-membership naive Bayes models. The proposed models al-low the number of components in the mixed membership to be different from the number of classes. We propose two variational inference based algorithms for learning th e models, including a fast variational inference which is sub -stantially more efficient than mean-field variational appro x-imation. Through extensive experiments on UCI and text classification benchmark datasets, we show that the models are competitive with the state of the art, and can discover components not explicitly captured by the class labels.
In recent years, mixed-membership (MM) models have been found wide application in a variety of domains, such as topic modeling [6], bioinformatics [1] and social networ k analysis [12]. A key advantage of such models is that they provide a succinct and interpretable representation of oth -erwise large and high-dimensional datasets. However, one important restriction of most existing MM models is that they are unsupervised models and cannot leverage class la-bel information for classification. On the other hand, while popular classification algorithms, such as support vector machines (SVM) [7] and logistic regression (LR) [17], per-to interpret. The above observation motivates our current work on designing accurate discriminative classification a l-gorithms while leveraging mixed-membership models for interpretability.

Supervised latent Dirichlet allocation (SLDA) [5] is such a mixed-membership model which takes response variables into account. However, the response variables in SLDA are real numbers assumed to be generated from a normal linear model, which is different from categorical labels in the con -text of classification. In principle, the authors proposed a general framework to extend SLDA to deal with other types of response variables, including categorical labels, base d on generalized linear models (GLM) [14]. However, efficient inference in the general case is difficult without the good properties of Gaussian distribution. In addition, SLDA is only designed to handle text data or a sequence of homoge-lems involve heterogenous features with measured values, e.g., most datasets in the UCI benchmark.
 membership (DM) models by combining multi-class logis-tic regression with unsupervised MM models. In particular, we consider two variants X  X iscriminative latent Dirichlet allocation (DLDA) and discriminative mixed-membership naive Bayes (DMNB). DLDA is applicable to text classi-the underlying MM model. DMNB is applicable to non-text classification involving numerical feature vectors an d uses mixed-membership naive Bayes (MNB) [2] as the un-derlying MM model. The mixed-membership representa-tion generated by DM is biased by class labels and can be viewed as a supervised dimensionality reduction. Further, since DM allows the number of components k in the mixed membership to be different from the number of classes c , the model often discovers additional latent structure beyo nd what implies by the class labels with a larger k . To learn the model, we propose two families of variational inference algorithms: one is based on ideas originally proposed in [6] and the other is more efficient in space and time complexity by using a significantly less number of parameters. Unlike Taylor expansion based approximations suggested in [5], the proposed inference algorithms maintain the lower bound maximization strategy used in variational inference.
Recently, there has been an increasing interest in mixed-membership models combining supervision information. Other than SLDA, [10] proposed labeled latent Dirichlet allocation to incorporate functional annotation of known genes to guide gene clustering. [13] proposed DiscLDA which determines document position on topic simplex with guidance of labels. [15] proposed a Dirichlet-multinomial regression which accommodates different types of meta-data, including labels. [19] proposed a correlated labelin g model for multi-label classification. [18] extends SLDA for image classification and annotation.

The rest of the paper is organized as follows: In Sec-tion 2, we give a brief overview of mixed-membership models. In Section 3, we propose discriminative mixed-membership models. In Section 4, a variational approach for learning DM is given. We present the experimental re-sults in Section 5 and conclude in Section 6. In the follow-ing sections, mixed-membership models particularly refer to LDA and MNB.
In this section, we give an overview on two mixed-membership models X  X atent Dirichlet allocation and mixed-membership naive Bayes models. We also briefly in-troduce supervised latent Dirichlet allocation.
LDA [6] is a three-level Bayesian model as an extension of finite mixture models (FMM) for topic modeling. Instead of having a fixed component proportion  X  for all data points as in FMM, LDA maintains a separate component propor-tion  X  over k components for each document x is sampled from a Dirichlet distribution Dir (  X  ) . For a se-quence of words in a document x sequence of components (topics) z of the form where  X  of parameters for k component distributions, each of which is a Discrete distribution over all words in the dictionary.
Getting a closed form expression for the marginal den-sity p ( x and Gibbs sampling [11] are two most popular approaches proposed to address the problem.
Supervised latent Dirichlet allocation (SLDA) [5] is an extension of LDA which accommodates the response vari-ables other than the documents. The response variable is assumed to be generated from a normal linear model covariates  X  z = P N quencies of each latent topic in the document. The density function of SLDA is given as follows: =
Z Since y is assumed to be generated from a univariate normal linear model, SLDA is constrained to deal with one dimen-sional real-valued response variables.
Although LDA achieves a good performance in topic modeling, it suffers from two limitations [2]: (1) LDA can-not deal with data points with measured feature values. (2) LDA cannot deal with data points with heterogenous fea-tures. MNB relaxes these limitations by introducing a sepa-rate exponential family distribution [3] for each feature. It is designed to deal with sparse and heterogenous feature vec-tors. Given a data point x model with k components is as follows: p ( x 1: N |  X ,  X )= where  X  x non-missing features,  X  = {  X  for the distributions of N features respectively, and each  X  n = {  X  ni , [ i ] k 1 } feature n . p bution with a form of p where  X  is the natural parameter,  X  ( ) is the cumulant func-tion, and p mines a particular family, such as Gaussian, Poisson, etc., and  X  determines a particular distribution in that family.
We motivate discriminative mixed-membership models by considering two important limitations of SLDA [5] which prevent it from being used as a discriminative clas-sification model: First, the response variables in SLDA are univariate real numbers assumed to be generated from a nor-mal linear model, whereas the response variables, i.e., la-though the authors pointed out that the response variables can be of various types obtained from generalized linear models, variational inference is difficult in the general ca se. While a Taylor expansion is recommended [5] to obtain an approximation of the log-likelihood, such an approach forgoes the lower bound guarantee of variational inference . Second, like LDA, SLDA is designed for text data viewed as a sequence of homogeneous tokens. However, most non-text classification tasks, e.g., the UCI benchmark datasets , have features with measured values. Further, the features could be heterogenous with different semantics, different ranges of values, etc., such as a customer X  X  age, occupation and zip code. SLDA is not designed for such data.
The proposed family of discriminative mixed mem-bership models overcome both limitations. In particular, DLDA is a variant of SLDA which accommodates cate-classification tasks. Further, DMNB is a variant of MNB, i.e., a generalization suitable for discriminative classi fica-tion with (non-text) heterogenous feature vectors. In prin ci-ple, DMNB works for sparse data, but the sequel only con-siders the non-sparse case for ease of exposition.
Assuming there are c classes and the number of com-ponents we choose is k , the graphical model for DLDA is given in Figure 1(a), where  X  is a k -dimensional parame-ter of a Dirichlet distribution,  X  k component distributions over the words with each com-ponent referring to a topic, and  X  matrix with c k -dimensional logistic regression parameters as the rows, where  X  use  X  process for each document x 1. Choose a component proportion  X   X  Dirichlet (  X  ) . 2. For each word in the document, 3. Choose the label from a multi-class logistic regression  X  z is an average of z z being 1 if it denotes the i th component. The categorical re-sponse variable y can be considered as a sample generated from the Discrete distribution ( p where p i.e., the model needs only one  X  in the two-class case.
There are two important properties of DLDA and DM models in general: (1) The k -dimensional mixed member-ship  X  z effectively serves as a low dimensional representa-tion of the original document. While  X  z in LDA is inferred in an unsupervised way, it is obtained from a supervised di-mensionality reduction in DLDA. We give the explanation in Section 4. (2) DLDA allows the number of classes c and the number of components k in the generative model to be different. If k was forced to be equal to c , for problems with a small number of classes,  X  z would have been a rather coarse representation of the document. In particular, for two-cla ss problems,  X  z would lie on the 2-simplex which may not be an informative representation for classification purposes. D e-coupling the choice of k from c prevents such pathologies. In principle, we may find a proper k using Dirichlet process mixture models [4].

From the generative model, the joint distribution of latent and observable variables for DLDA is given by = p (  X  |  X  ) Integrating (1) over  X  and summing it over z marginal distribution of ( x
X The probability of the entire data set of D documents and their labels ( X = { x
X
Discriminative MNB is similar with DLDA except that it keeps separate distributions for each feature. Given the graphical model in Figure 1(b), the generative process for x 1. Choose a component proportion  X   X  Dirichlet (  X  ) . 2. For each feature in the data point 3. Choose the label from a multi-class logistic regression p tion [3]. Comparing DMNB with DLDA, for each com-ponent/topic i , DLDA has only one discrete distribution to generate features (words), while DMNB has separate dis-tributions p ponent distributions for DMNB could be of different types, or a same type with different parameters, just like in naive Bayes. Therefore, DMNB is more flexible than DLDA to deal with heterogenous features with measured values by choosing a proper distribution for each feature.
 DMNB could be considered as a generalization of naive Bayes (NB) classifier extended in the following aspects: First, NB shares a component among all features, but DMNB has a separate component for each feature and maintains a Dirichlet-multinomial prior on all possible combination of component assignments. Therefore the components for different features might be different in DMNB, and NB could be considered as a special case when z component as a class indicator, whereas DMNB uses the mixed membership over separate components as inputs to a logistic regression model which finally generates the class label. Third, NB requires k = c while DMNB does not. In principle, DMNB could be applied whenever naive Bayes is applicable.

A special case of DMNB is when each feature n is assumed to be generated from one of k Gaussian distri-butions with the mean  X   X  = {  X  2
X The probability of the entire data set ( X = { x { y d , [ d ] D 1 } )
X
Since DM models assume a generative process for both labels as well as the data points, instead of using labels di-rectly to train a classifier, we use both X and Y as samples from the generative process to estimate the parameters of DM models such that the likelihood of observing ( X , Y ) is maximized. Unlike naive Bayes [9], the parameters can-not be directly estimated from the class labels due to the latent mixed memberships. In particular, due to the latent variables, the computation of the likelihood in (2) and (4) i s intractable. In this section, we present two alternative ap -proaches to obtain a variational approximation of the log-likelihood and propose an expectation maximization (EM)-style algorithm to iteratively obtain better estimates of t he model parameters. Finally, we show how the estimated pa-rameters can be used to do prediction on test data.
For each data point, to obtain a tractable lower bound to distribution q (  X ,z posterior distribution p (  X ,z variables, where  X  is the set of variational parameters. By bound to log p ( x We use L to denote the lower bound. Following [6] and noticing that x given z L = E q [log p (  X  |  X  )]+ E q [log p ( z 1: N |  X  )] (7)
We propose two different variational distributions q (  X ,z 1: N ) . Following [6], we consider where q q pose where q is a Discrete distribution for all z have a k -dimensional Dirichlet (  X  ) for each data point, but we have a k -dimensional Discrete (  X  tures in q By keeping a substantially smaller number of parameters, q ber of parameters to optimize over. q two different variational inference algorithms. We call th e first one  X  X tandard variational inference X  and the second on e  X  X ast variational inference X , which accordingly yield sta n-dard DM/MM (Std DM/MM) models as opposed to Fast DM/MM models respectively. In the sequel, we use q to denote q
Given the variational distribution as in (8) or (9), the first five terms in the lower bound (7) can be easily obtained fol-lowing LDA or MNB depending on which DM model we are using. The most difficult part is the last term, which cannot be computed exactly even after introducing the vari-ational distribution q , so further approximation is needed. We give the expression for the last term here, the details of derivation could be found in the Appendix. For standard DM models, we have  X  1 N and for Fast DM models, we have  X 
X where  X  &gt; 0 is a new variational parameter introduced to obtain a lower bound for the last term in (7). 4.1.1 Inference Given a choice of model parameters (  X  ( t ) ,  X  ( t ) the lower bound to the log-likelihood for each data point in (7) becomes a function of the variational parame-ence step is to obtain the tightest lower bound to the true log-likelihood, which is achieved by maximizing results of the variational parameters are given in Table 1, where 1 ( x word of the document in the dictionary and 0 otherwise, and V is total number of the words in the dictionary.

From Table 1,  X   X  = [ P N for DM and  X  = [  X  each data point. Note that the last term in all expressions of  X  contains y , showing that the low-dimension represen-tation not only depends on x which means DM models achieve supervised dimension re-duction. Removing the last term gives the expression of  X  in the corresponding unsupervised settings. 4.1.2 Parameter estimation Variational parameters (  X   X  , X   X  , X   X  ) from the inference step gives the optimal lower bound to the log-likelihood of each pair of ( x gate lower bound P D data points with respect to  X ,  X  and  X  obtain the estimated parameters. The estimations of  X  and  X  are the same as in the corresponding MM models [6, 2]. As for  X  , we have for standard DM models, and for Fast DM models.
We propose an EM-style algorithm to find out the optimal model parameters alternatively. Given (  X  eration. The algorithm alternates between the following tw o steps until convergence: point, find the variational parameters (  X  log p ( x d ,y d |  X ,  X  , X  1: c  X  1 ) . 2. M-step: Maximizing the aggregate lower bound yields an improved estimate of model parameters: (  X  After t iterations, the objective function becomes P The first inequality holds because (  X  ( t +1) maximizes L (  X  the second inequality holds because (  X  ( t +1) ,  X  ( t +1) maximizes P D step. Therefore, the objective function is guaranteed to be non-decreasing until convergence.
Once we have the model parameters from EM, we can use  X  diction. Given a data point x
E [log p ( y = h | x 1: N , X ,  X  , X  1: c  X  1 )] = Since the second term for [ h ] c  X  1 we only need to compare  X  T h th term is the largest, the predicted class is h .
The computation for E [  X  z ] is intractable, so we again introduce variational distribution q (  X ,z E [  X  z ] as an approximation of E [  X  z ] . In particular, Table 7. Running time (seconds) of standard DLDA and
We present experimental results for DMNB on UCI data, and for DLDA on text. Two types of experiments are in-cluded: First, we compare DM to corresponding MM mod-els. Second, we compare DM with other classification algo-rithms. Experiments are run with a 10-fold cross validation .
We pick 9 datasets from UCI machine learning reposi-tory for DMNB. The number of data points ( D ), features (
N ) and classes ( c ) in each dataset are in Table 2. We pick five text datasets for DLDA. The number of documents ( D ), the number of words in the dictionary ( V ), and the classes are in Table 3. Nasa is a subset of Aviation Safety Report-of flight problems originated by three sources. Others are commonly used benchmark datasets for text classification.
In this section, we compare DM models with k = c to corresponding MM models. We initialize model parame-in particular, we set the number of components k to be the number of classes c ; use the mean and standard deviation (for Gaussian case only) of the data points in each class to initialize  X  ; and use D number of data points in class i and D is the total number of data points. For  X  tion by holding out 10% of training data as the validation set and use the parameters generating the best results on the validation set. In particular, each  X  ru ing 1 and others being 0, and r takes values from 0 to 100 in steps of 10. In principle, MM models are not used for clas-sification, but given the initialization we have introduced , there is a one-to-one mapping between the component and the class. Therefore, given the mixed membership on a test data point, we pick the component i with the largest proba-bility as the predicted component, if the corresponding cla ss of component i is the same with the class label, we consider the data point as correctly classified, otherwise it is mista k-enly classified. We use the percentage of correctly classifie d data points, i.e., the accuracy, to compare DM and MM.
The results for DMNB and DLDA are presented in Ta-ble 4 and 6 respectively. We make two observations: (1) Fast DM/MM models have a higher accuracy than the cor-responding standard DM/MM models, with a few excep-tions. (2) Standard DM models are not necessarily better than standard MM models, but Fast DM models are usu-ally better than Fast MM models. The higher accuracy of Fast DM demonstrates the effects of logistic regression in accommodating label information for DM models.

We further investigate on the mixed memberships gen-erated by DM and MM models. In particular, we com-pute the Shanon entropy for the mixed membership as a Discrete distribution and compare the entropy among different algorithms. A low entropy implies almost a  X  X ole membership X , whereas a higher entropy implies a real mixed membership. Figure 2 is an example show-ing the histogram of mixed membership entropy on text data of Cmusim using four variants of LDA. We can see that for Fast LDA/DLDA, almost all data points have ex-tremely small mixed-membership entropies, while for stan-dard LDA/DLDA, the entropies fall into different ranges. Similar results are obtained on UCI data. The interesting observation indicates that fast variational inference act ually generates  X  X ole membership X  while standard mean-field variational inference generates real  X  X ixed membership X . The fact that Fast DM/MM generates sole membership, as well as the previous observation that Fast DM/MM are bet-ter than standard DM/MM in terms of accuracy, shows the correlation between  X  X ole membership X  and higher classi-fication accuracy, although we are not sure about the exis-tence of causality between them.  X  X ixed membership X  may be useful in various real applications, but it does not seem to help in terms of classification accuracy.
 We compare the running time between standard DM and Fast DM. The results for DMNB and DLDA are presented in Table 5 and 7 respectively. In Table 5, although most of datasets are small, Fast DMNB is already faster than the standard DMNB, especially on the largest dataset Seg, where Fast DMNB is about 5 times faster than standard DMNB. Fast DM X  X  advantage increases when it comes to the larger and higher-dimensional text data as in Table 7, where Fast DLDA is about 20 to 150 times faster than the standard DLDA, showing Fast DM models X  absolute supe-riority in terms of time efficiency. Combining the results with the accuracy comparison in Table 4 and Table 6, we can see that Fast DM models are generally more accurate and substantially faster than standard DM and MM models.
Since Fast DM models have better performance than standard DM models, in this subsection, we use Fast DM to compare with other classification algorithms. In particu -lar, we compare Fast DMNB with support vector machine (SVM) [8], logistic regression (LR) and naive Bayes (NB) models on UCI data; and compare Fast DLDA with SVM, NB, LR and mixture of von Mises-Fisher (vMF) model on text data. Since DM models are combination of logistic re-gression and mixed-membership model, we also compare the results from DM with those from MM and logistic re-gression in two steps sequentially.

For Fast DM models, we run the experiments with an increasing k . In particular, for Fast DMNB, we use k = ( c,c + 5 ,c + 10) , and for Fast DLDA, we use k = ( c,c + 15 ,c + 30 ,c + 50 ,c + 100) . For initialization of  X  , we use the mean and standard deviation (for Gaussian case only) of the training data in given classes plus some perturbation if k &gt; c ; for  X  , we set it to be 1 /k on each dimension; and for  X  For SVM, we use linear and RBF kernel with same cross validation strategy on the penalty parameter and the kernel parameter (for RBF only) taking values from 10  X  5 to 10 5 in multiplicative steps of 10 respectively.
 The results for Fast DMNB and DLDA are presented in Table 8 and 10. The top parts of the tables are the results from the generative models, and the bottom parts are the SVM, we report the highest accuracy of linear and RBF kernels with different parameters. We use bold for the best results among the generative models and use bold and italic for the best results among all algorithms. Three parts of in-formation could be read from the tables: (1) Overall, on text datasets, Fast DLDA does better than all other algorithms, including SVM, on almost all datasets, which is a promis-ing result although more rigorous experimentations may be needed to make a further investigation; on UCI datasets, Fast DMNB also achieves higher accuracy than all other
Table 11. Accuracy on text from Fast LDA and logistic algorithms on most of datasets except SVM, which beats Fast DMNB six out of nine times. (2) The better perfor-mance of Fast DM models compared with LR on original datasets indicates that the low-dimensional representati on we generate helps the classification. (3) Interestingly, fo r Fast DMNB, the accuracy increases monotonically with k from c to c + 10 on most of the datasets. For Fast DLDA on text data, an increasing of accuracy with a larger k is also observed, although the result goes up and down without a clear trend. One possible reason for the increasing accurac y is as follows: When k is too small, we are performing a drastic dimension reduction to represent each data point in a k -dimensional mixed membership representation, which may cause a huge loss of information, but the loss may de-crease when k increases.

DM models do dimensionality reduction and classifica-tion in one shot via a combination of MM models and logis-tic regression. In principle, we may also use these two algo-rithms sequentially in two steps, i.e., first using MM mod-plying logistic regression on the low-dimensional represe n-tation for classification. The results with different choic es of k following this two-step strategy are presented in Ta-ble 9 and 11 for UCI and text data respectively. Comparing these results with Table 8 and 10, it is clear that DM models outperform the algorithm using MM and logistic regression sequentially, which means, by combining MM and logistic regression together, DM achieves supervised dimensional-ity reduction to obtain a better low-dimensional represent a-tion than MM, which further helps classification. Compar-ing these results with the accuracy of logistic regression o n original data, we can see that there is no clear winner, which may depend on the quality of low-dimensional representa-tion generated from MM.

As we have mentioned, DM models generate inter-pretable results. We give an example of several topic word lists on Nasa generated by Fast DLDA ( k = c + 30 ) in Table 12. It is also an interesting result demonstrating the effect of allowing a larger number of components than the number of classes, that is, Fast LDA may discover topics which are not explicitly specified in class labels. The first three topics in Table 12 correspond to three classes in Nasa respectively, but topic 4, which we call  X  X assenger medical emergency X , could be considered as a subcategory of the  X  X assenger X  class, and it is not specified in the labels. Nei-ther NB nor SVM is able to generate this type of results.
In this paper, we have proposed discriminative mixed-membership models, as a combination of unsupervised mixed-membership models and multi-label logistic regres-sion. We proposed a fast variational inference algorithm mation used in LDA. An important property of DM models is that they allow the number of components k to be differ-ent from the number of classes c . Interestingly, a larger helps to discover the components not specified in labels and increase classification accuracy. In addition, DM models are competitive with the state of the art classification algo -rithms in terms of the accuracy, especially on text data, and are able to generate interpretable results. Future work in-cludes using Dirichlet process mixture models to find out the proper value for k and extending the model to accom-modate kernels.
In this section, we give the derivation for variational in-ference in Section 4. Given the lower bound function as (7), the first five terms could easily obtained following LDA or MNB depending on which DM model is used, so we only work on the last term E
The class label y is from a multi-class logistic regres-distribution with  X  .
 Accordingly, = E q [ = The second term of (12) could be expanded as follows: second inequality is also from Jensen X  X  inequality noticin g equality is from  X  log( x )  X  1  X  x ducing a new variational parameter  X  &gt; 0 . Given (13),  X 
X where in standard DM models E in fast DM models, E
Putting E complete expression for L . By maximizing (7) with respect to the variational and model parameters alternatively as in Section 4, we find the optimal value for (  X ,  X  , X  Acknowledgements The research was supported by NASA grant NNX08AC36A and NSF grant IIS-0812183.

