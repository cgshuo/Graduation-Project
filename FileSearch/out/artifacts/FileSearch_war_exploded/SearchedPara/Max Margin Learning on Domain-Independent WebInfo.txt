 Domain-independent web information extraction can be ad-dressed as a structured prediction problem where we learn a mapping function from an input web page to the structured and interdependent output variables, labeling each block on the page. In this paper, built upon an HTML parser of In-ternet Explorer that parses and renders a web page based on HTML tags and visual appearance, we propose a max margin learning approach for web information extraction. Specifically, the output of the parser is a vision tree, which is similar to a DOM tree but with visual information, i.e., how each node is displayed. Based on this hierarchical struc-ture, we develop a max margin learning method for labeling each of its nodes. Due to the rich connections between blocks on the web page, we further introduce edges that connect spatially adjacent nodes on the vision tree, complicating the problem into a cyclic graph labeling task. A max margin learning method on cyclic graphs is developed for this prob-lem, where loopy belief propagation is used for approximate inference. Experimental results on web data extraction show the feasibility and promise of our approach.
 I.2.6 [ Artificial Intelligence ]: Learning Algorithms, Performance Max Margin Learning, Web Information Extraction, Belief Propagation, Cutting Plane
Web search has become a necessary part of our everyday life, and search engines such as Google and Bing can help us find the right web pages for majority of queries. How-ever, the understanding of web pages by computers is still very limited. There have been studies on categorizing web pages [4], identifying the main entity of a page [5], and ex-tracting particular types of information from pages [1]. But no approach can assign semantics to majority of contents on web pages. There have not been mature approaches for understanding the functionality and semantics of different sections of a web page. Even tasks such as distinguish-ing main contents from side contents (e.g., user comments, navigational bars, and advertisements) have remained tough challenges.
 Figure 1 shows a web page about Britney Spears on last.fm. It has a typical layout among web pages covering multiple aspects of an entity. The entity name appears in bold at top, followed by a short biography. The page is divided into many sections, each having a title in red and bold font and contents under that. Each piece of content (e.g., an album, a song, or a video) occupies a small region contain-ing text and possibly image/video that are close to each other. Without any knowledge about music, we can see this page is about Britney Spears , her similar artists include Christina Aguilera and Lindsay Lohan , her top albums in-clude Circus and Blackout , etc. In fact we do not need to understand any word to get such information. For exam-ple, http://www.lastfm.es/music/Ricky+Martin?setlang=es contains a Spanish page about Ricky Martin .Evenifwe do not know Spanish at all, we can see his  X  A lbumes m  X a s escuchados (albums) include Ricky Martin and Vuelve ,and there are seven Artistas similares (similar artists) listed on the page.
If a human does not need to understand a page to extract such information, a computer can probably do it as well. A human tells the role of an element on a page mainly by its position, size, font, color, boldness, relative position to other elements, etc. All these features can be utilized by a machine learning algorithm, especially structured learning method which considers the relationships between related elements in a graph. In this paper we study the problem of domain-independent web information extraction ,whichaims at extracting information from a generic web page, with se-mantics provided by the page itself. Take the page in Figure 1 as an example. With existing approaches in [5] and [13], a computer is capable of figuring out that the main entity of this page is Britney Spears . Then our approach will be able to extract attributes and values. For example, Top Albums is an attribute of the main entity, and its values are Circus , Blackout , etc .

In this paper, domain-independent web information ex-traction is formulated as a structured prediction problem, with input as a web page and output as labeling for each block on the page. Specifically, we first use the HTML parser of Internet Explorer to parse and render a web page. The output of the parser is a vision tree, which is similar to a DOM tree but with visual information, i.e., how each node is displayed, such as the location and size of the surround-ing rectangle of each node. Based on the web page parsing result, we build a hierarchical model, where nodes corre-spond to blocks on the page, and a smaller block residing in another block is represented as a child node of the node corresponding to that larger block. We first propose a hier-archical model where a node is only connected to its parent, and as a result, the hierarchical model for the web page is a tree. To label each node on this tree, we propose a max margin learning approach, where cutting plane method is used for parameter learning and a Viterbi decoding style ap-proach is developed for inference. Moreover, due to the rich connections between blocks on the web page, we argue that modeling the web page structure as a tree encounters loss of crucial information, such as the adjacency relationship be-tweenattributenameblock Top Albums and attribute value block Circus in Figure 1. Therefore, we further advance the model by introducing edges that connect spatially adja-cent blocks in the hierarchical structure. The introduction of these edges complicates the structured prediction prob-lem as the output structure is no longer a tree, but a cyclic graph. To perform inference on this graph, we introduce a loopy belief propagation method for approximate inference.
Our approach for domain-independent web information extraction could be separated into two steps. Given a web page, we first perform a web page segmentation that builds a hierarchical model for the page. Based on this hierarchical structure, a mapping function that maps the web page to a structured output is learned with max margin criterion.
We use the HTML parser of Internet Explorer to parse and render a web page. The output of the parser is a vision tree, which is similar to a DOM tree but with visual information, i.e., how each node is displayed. The vision tree provides the location and size of the surrounding rectangle of each node. For each leaf node containing text, image or another type of rich content, it provides the formatting information such as font, font-size, color, boldness, etc. Each node in the vision tree corresponds to a node in the original DOM tree. Sometimes the contents (especially textual contents) of a leaf node in the DOM tree may occupy multiple lines, and the vision tree will contain the display information for each line. As mentioned before, the display information such as position, font-size and boldness are very important for a human to understand the role of each piece of content on a web page, and similarly they are crucial for our system to label each node.
Given the segmentation results, we formulate the web in-formation extraction problem as a structured classification task, where the task is to assign labels for each node in the previously obtained vision tree. Given the predicted la-bel for each node in the tree structure, useful information could therefore be extracted. Before getting into details of the structured prediction approach, we first define the la-bel space employed in our approach. Specifically, there are mainly two types of nodes in the hierarchical structure corre-sponding to a web page, leaf nodes and non-leaf nodes. The leaf nodes correspond to those smallest blocks on the web page that have specific contents. On the other hand, the non-leaf nodes in the hierarchy structure define the struc-tural layout of the web page by grouping leaf nodes and other non-leaf nodes.
Due to the definition of our information extraction prob-lem, we only need to decide whether a leaf node is an at-tribute name, an attribute value or something else. It should be noted that we do not target to identifying the exact meaning of attribute name, like price, product name, date of birth, etc. In the information extraction problem, we only care about whether a leaf node is an attribute name, and exact meaning of the attribute name will be resolved in the query matching phase, which is beyond the scope of this paper. We also define labels for page title, navigational bar, page tail, images, etc. Anything not belonging to these classes is classified as non attribute . Consequently, the leaf node label space is composed of 8 elements: L leaf = { attribute name, attribute value,non attribute,
Non-leaf nodes in the web page hierarchical structure cor-respond to blocks composed of leaf nodes or other non-leaf nodes. We separate these non-leaf nodes into two groups: data record related nodes, and web page structural nodes. Specifically, the parent node of attribute name and attribute value nodes are data record nodes, and the node contain-ing data record node as its descendent are also data record nodes. The web page structural nodes include navigational bar, advertisement, web page tail, etc. With the above defi-nition, the non-leaf node label space could hence be defined as follows L nonl = { structure block, data record,nav bar block, non attribute block, value block, page tail block, name block, image block, image caption block, main title block } Specifically, value block is the node whose children nodes are all labeled with attribute value or value block .Forex-ample, in Figure 1, the block right below Top Tracks con-taining a table of several tracks by Britney Spears is defined as a value block .Moreover, structure block denotes a large block containing multiple types of non-leaf blocks. Other labels are rather self-explanatory.
In order to provide labels for each node in the hierarchical model of a web page, we employ structured max margin learning approach.
In supervised structured prediction problems, given input variables { x l } m l =1  X  X  and output variables { y l } m generally infeasible to model each possible value of output y as an individual class. Moreover, multiple way dependencies might exist between the components of x l , between the com-ponents of y l , and between the components of x l and y l Therefore, a feature map  X ( x l , y l ): X X Y X  X  d that allows to capture these multiple way dependencies among the input and output variables is widely employed [7]. Specifically, a discriminant function F : X X Y X  X  is usually employed to map the feasible input-out put pairs to a com patibility score of the pair. The response for input x is therefore com-puted as the one that maximizes this score over the set of all possible outputs [7], The score of an ( x , y ) pair is computed from local fragments. For example, in hidden Markov model, x is a chain, y pro-vides label for each node in the chain x and a local fragment of ( x , y ) is a clique comprised of x i and its labeling y
Specifically, in structured SVM [7], a generalized linear model F ( x , y )= w ,  X ( x , y ) is employed to decode the top-scoring output for any input x . To measure the accuracy of a prediction, a symmetric, non-negative loss function  X  : Y X Y  X  R that details the distance between the true output variable y and the prediction is employed. Different from conventional SVM, where regularized 0-1 loss is minimized, structural SVM minimizes a loss function that could capture the com-plex structure within label variables y [2]. This is due to the fact that a label sequence with only one incorrect prediction is obviously more preferable than a label sequence with no correct predictions. Hence, the expected risk of f could be calculated as where input-output pairs ( x l , y l ) are generated i.i.d. accord-ing to some fixed distribution P X X Y .As P X X Y is unknown, structured prediction is addressed by minimizing the em-pirical risk on the training input-output pairs { ( x l , y X X Y : l =1 ,...,m } , regularized with the inverse margin || w || 2 . Therefore, structured SVM could be formulated as follows [7] In this paper, we define the loss function  X ( y , y )asthe number of disagreements between y and y , i.e.,  X ( y , y )= phase of structural SVM targets to learn the weight vector w , while in the inference phase, dynamical programming is usually employed to find the most compatible label structure with the underlying observation.
The definition of a proper parametric discriminant func-tion F requires specifying the joint input output mapping  X ( x , y ), which extracts features from a web page/hierarchical structured labeling pair ( x , y ). In this section, we provide detailed definition of features extracted in our hierarchical model for web information extraction.
Inspired by sequence prediction approaches, such as hid-den Markov model and conditional random field ,wede-fine features for the two types of cliques in the hierarchi-cal model respectively: cliques C b covering observation-state node pairs and cliques C p covering state-state node pairs. Note that in current hierarchical model, we only consider parent-child connections, resulting in the only group of state-state links connecting a node to its parent. Consequently,  X ( x , y ) is defined over the set C = C b  X  X  p and its com-ponents could be categorized into two groups according to which type of cliques they are defined over. Specifically, each component defined over clique in C b conjunctively combines an input attribute  X  ( x i ) d  X  X  (i.e., the d -th entry for the feature vector  X  ( x i )ofthe i -th node in the graph) with a state l where ( x i ,y i ) is a node in the web tree structure. Besides, each component defined over clique in C p deals with a pair of adjacent states y i and y j
We first define the features  X  ( x )extractedforeachblock x in the hierarchical structure. As mentioned in the previous section, the feature represen-tation for this block with label y is where e y is a vector with all its elements being 0, except the y -th element being 1, and  X  is the Kronecker product.
Besides the aforementioned features extracted from each block independently, we also extract features that capture the relationship between connected nodes in the hierarchi-cal structure. Specifically, this feature captures the label co-occurrence patterns for child and parent parent nodes. Therefore, we construct the following feature capturing the label co-occurrence pattern for a node s and its parent t in the hierarchical structure Since the root node has no parent,  X  l,  X  l :  X  p ( l,  X 
All the features extracted at node i are simply stacked together to form  X ( x i ,y i ). Finally, this feature map is ex-tended to the entire hierarchical structure involving n nodes in an additive manner as As a result of the two types of features in  X ( x , y ), we sepa-rate the weight vector w also into two independent groups w b and w p , such that
Givenawebpage X = { x 1 ,..., x n } and learned param-eter w , we need to solve the following inference problem to obtain the optimal labeling y  X  y  X  =argmax where  X  i is the parent of node i . A closer look into the above formulation reveals that each term in w b ,  X  b ( x i ,y i dependent of others, and could therefore be optimized sepa-rately. On the other hand, the terms in w p ,  X  p ( y i ,y interdependent. However, as the only edges in the hierar-chical structure are those connecting a node with its parent node, we could employ a Viterbi decoding approach that starts from the leaf nodes in the tree and traverses through thetreeuntilreachingtherootnode.Foreachnode,wede-fine  X  i ( y )for  X  y  X  X  that stores the optimal value of terms in (12) involving the i -th node x i with label y . Specifically, for a leaf node,  X  i ( y ) is defined as On the other hand, for non-leaf node x j ,  X  j ( y ) is defined as  X  j ( y )= w b ,  X  b ( x j ,y ) + where C ( j )isthesetofnodeswhoseparentnodeis x j . Clearly, the  X  function for a node is only computed when all its child nodes have been handled. The computation continues until the root node is reached. Among those |Y| values computed for the root node, the maximal one is se-lected and corresponding y is selected as the label for root node. After the root node has been handled, we perform a standard backtracking to obtain the optimal labeling for the entire hierarchical structure.
Learning parameter w requires solving a quadratic pro-gramming problem with exponential number of constraints. A standard approach for tackling this problem is cutting plane algorithm [7], a.k.a., constraint generation. Specifi-cally, we construct a nested sequence of successively tighter relaxations of the original problem, and each optimization problem in this sequence could be efficiently solved via quadratic programming.
For a block on the web page, spatially adjacent blocks encrypt important information about its functionality and semantics. For example, in Figure 1, the blocks listing tracks of Britney Spears appear below the block Top Tracks , which serves as the attribute name for those blocks of track name. Correctly identifying the functionality of the block Top Tracks is of great help in assigning labels to those blocks below it, and vise versa. The aforementioned hierarchical model only encodes parent-child relationship, without any encryption of the relationship between spatially adjacent blocks. In this section, we improve the hierarchical model by introducing edges connecting adjacent blocks.

Specifically, for each leaf node in the web page parsing hierarchy, our approach finds its k nearest neighbor leaf nodes and adds edges between the leaf node and its k neigh-bors. After adding these edges, the hierarchical model for a web page becomes a cyclic graph, instead of a tree struc-ture where edges only appear between parent-child nodes. Feature representation for this improved hierarchical model includes the two types of features defined previously for tree structure model. Moreover, we introduce one more group of features that directly model the relationship between spa-tially adjacent nodes.
This feature is designed to encrypt the relationship among labels for connected nodes. Therefore, a similar feature as used for parent-child relationship could be adopted here. Specifically, the feature captures the co-appearance pattern of labels for spatially adjacent blocks. If node i is connected to node j , we define the following boolean-valued feature for block pair ( i, j ) We define the weight vector for this feature type as w s . With w b and w p defined similarly as in the tree structure model, the prediction function for this cyclic graph hierar-chical model is
F ( x , y )= w b ,  X  b ( x , y ) + w p ,  X  p ( x , y ) + w where  X  b ( x , y ) captures features of a block itself,  X  models the relationship between a block with its direct en-closing block, and  X  s ( x , y ) represents features computed be-tween spatially adjacent blocks. Assume the cyclic graph hierarchical model for a web page is G =( V , E ), where V the set of nodes and E denotes the set of edges. Since both  X  ( x , y )and X  s ( x , y ) model label co-occurrence patterns of connected nodes, we could unify these two types of features. Specifically, for each edge e =( s, t )  X  X  that connects two nodes s and t , define a feature  X  e ( y s ,y t )as Consequently, the prediction function becomes
Similar as the tree structured hierarchical model, given a web page X = { x 1 ,..., x n } and learned parameter w , inference targets to finding the most probable labeling for each node on the page. Specifically, inference in the cyclic graph renders into solving the following problem where the first term w b ,  X  b ( x i ,y i ) computes features re-garding a block in isolation, and w e ,  X  e ( y i ,y j ) computes features involving two nodes in the graph, either parent-child nodes or spatially adjacent nodes.

Different from the tree structured hierarchical model, where an exact inference based on Viterbi decoding could be em-ployed, in the cyclic graph hierarchical model, exact infer-ence would take exponential time. Therefore, we employ a loopy belief propagation algorithm in this section to perform approximate inference. Specifically, for nodes { x s } with la-bels { y s } , we define the following two functions At each iteration, each node t passes a  X  X essage X  to each of its neighbors s  X  X  ( t ), which is connected to t with edge ( s, t )  X  X  . This message, which we denote by M ts ( y s function of the possible states y s  X  X  . On the full graph there are totally 2 |E| messages, one for each direction of each edge. This full collection of messages is updated according to the following recursion [10] For tree-structured graphs, it can be shown that iterates generated by (22) will converge to a unique fixed point after a finite number of iterations. Given these messages M ts ( y a standard backtracking procedure could be used to find the most probable labeling variables y  X  . Moreover, this fixed point is precisely equal to the result obtained using the Viterbi decoding method introduced in previous sections for tree structured hierarchical model.

The idea of loopy belief propagation [10] is to run the above procedure on a graph with cycles. Specifically, message up-dates (22) can be applied at a node while ignoring the pres-ence of cycles -essentially pretending that any given node is embedded in a tree. Due to the presence of cycles, informa-tion could flow many times around the graph. This approach is known to perform well in many real-world problems [6, 9, 8, 12, 11].
We test our web information extraction approach on two groups of experiments: labeling each block on the page and extracting attribute name-value pairs. The extraction of attribute name-value pairs is of particular interests as they cover the majority of search queries.
We crawled 1000 web pages from around 200 web sites, covering a large variety of various domains. For the block-level prediction task, we provide detailed labels for each block in 50 randomly selected web pages, resulting in a to-tal of 13640 blocks. Moreover, we also manually label all attribute name-value pairs in the 1000 crawled web pages.
The first experiment tests the performance of our ap-proach in assigning semantic labels for each block on the web pages.
To evaluate block-level prediction results, we employ the following three criteria Leave-one-out cross validation is performed on the training set to calculate these performance measures.
We use multi-class SVM [3] as baseline, and compare the block-level prediction performances of structured SVM with tree hierarchical structure and structured SVM on cyclic graphs. Table 1 compares the block-level prediction results of multiclass SVM , structured SVM with tree hierarchical model (SVM-Tree) and structured SVM with cyclic graph hierarchical model (SVM-Cyclic) . Specifically, overall accu-racy on all categories, precision and recall on attribute name and value classes are provided. According to Table 1, both SVM-Tree and SVM-Cyclic algorithms achieve much higher accuracy than multiclass SVM , demonstrating the effective-ness of our structured prediction framework. Moreover, the results for SVM-Cyclic are roughly 3% higher than those for SVM-Tree . This shows that connecting spatially adjacent blocks in the hierarchical model improves our structured prediction framework, even though only edges connecting leaf nodes are incorporated in current model. Although no edges are adding between non-leaf blocks in SVM-Cyclic , connected leaf blocks affect the label assignment of their parent nodes through the child-parent edges in the model. With better prediction for the leaf nodes, prediction results for non-leaf nodes are also improved.

To better understand the prediction results, we also pro-vide confusion matrix for SVM-Cyclic . Specifically, Table 2 shows confusion matrix for leaf node labels, with rows corre-sponding to predicted labels, and columns corresponding to manually generated true labels. The 8 leaf node labels are organized in the same order as in L leaf ,fromlefttoright, and from top to bottom. Similarly, Table 3 shows confusion matrix for non-leaf node labels. The 10 non-leaf node labels are organized in the same order as in L nonl ,fromleftto right, and from top to bottom.
Automatically assigning semantic labels on blocks of a web page reveals great promise for intelligent web page un-derstanding. However, how to make use of the extracted data is also an interesting problem. In this section, we take a step further and focus on extracting attribute name and value pairs. For example, in Figure 1, Top Albums and Blockout compose an attribute name-value pair. Given the block-level prediction results, our algorithm matches each attribute value block with a  X  X losest X  attribute name block. In order to measure the closeness between two blocks, we define a distance measure composed of two parts: tree dis-tance and Euclidean distance. Specifically, we define the tree distance as length of the shortest path connecting these two blocks in the tree hierarchical model. Moreover, Euclidean distance is calculated between centers of the two blocks. Fi-nally, block distance is computed as a weighted sum of tree distance and Euclidean distance, i.e., where d Tree is the tree distance, d 2 is the Euclidean distance and  X  is a weight parameter. We label all attribute name and value pairs in 1000 web pages crawled from more than 200 web sites and apply the SVM-Tree approach for block-level prediction. We then match each attribute value block to the closest attribute name block. Using this approach for extracting attribute name-value pairs, we achieve precision of 56 . 91 %, and recall of 59 . 37 %.
In this paper, we propose a max margin learning approach for domain-independent web information extraction. Specif-ically, we propose a tree structured hierarchical model, and a cyclic graph hierarchical model. For both models, ef-ficient inference and parameter learning methods are dis-cussed. Experimental results for both block-level prediction and attribute name-value pair extraction demonstrate the effectiveness of our proposed method.
 E. P. Xing is supported by NSF IIS-0713379, DBI-0546594, Career Award, ONR N000140910758, DARPA NBCH1080007 and Alfred P. Sloan Foundation.
