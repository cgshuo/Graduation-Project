 In this paper we propose Instance Filtering as preprocess-ing step for supervised classification-based learning systems for entity recognition. The goal of Instance Filtering is to reduce both the skewed class distribution and the data set size by eliminating negative instances, while preserving pos-itive ones as much as possible. This process is performed on both the training and test set, with the effect of reduc-ing the learning and classification time, while maintaining or improving the prediction accuracy. We performed a com-parative study on a class of Instance Filtering techniques, called Stop Word Filters , that simply remove all the tokens belonging to a list of stop words. We evaluated our approach on three different entity recognition tasks (i.e. Named En-tity, Bio-Entity and Temporal Expression Recognition) in English and Dutch, showing that both the skewness and the data set size are drastically reduced. Consequently, we reported an impressive reduction of the computation time required for training and classification, while maintaining (and sometimes improving) the prediction accuracy. The objective of Information Extraction (IE) is to identify a set of relevant domain-specific classes of entities and their relations in textual documents. In this paper we focus on the problem of Entity Recognition (ER). Recent evaluation campaigns on ER, such as the CoNLL-2002, CoNLL-2003, TERN 2004 and JNLPBA 2004 shared tasks, have shown that most of the participating systems approach the task as a supervised classification problem, assigning an appro-priate classification label for each token in the input docu-ments. However, two problems are usually related to this approach: the skewed class distribution and the data set size . The unbalanced distribution of examples, due to the entity sparseness in texts, yields in many systems a drop off in classification accuracy. In additon, supervised learn-ing from large data sets is a time-consuming process, as the complexity of many supervised algorithms is proportional to the number of examples.
 To address these problems, we propose a technique called Instance Filtering (IF). The goal of IF is to reduce both the skewness and the data set size by removing negative instances and preserving the positive ones. The main pecu-liarity of this technique is that it is performed on both the training and test sets. This reduces the computation time and the memory requirements for learning and classification, and improves the classification performance.
 We present a comparative study on Stop Word Filters , a class of IF techniques that remove from the data set all in-stances (i.e. tokens) belonging to a list of stop words. The basic assumption behind this approach is as follows: stop words do not provide any specific information about the text in which they appear, therefore their expectation of being (part of) relevant entities is very low. Lists of stop words can be easily acquired by computing statistics on the training corpus. Furthermore, Stop Word Filters are easy to implement and can be successfully used as preprocessing modules by most of the supervised systems for ER, allow-ing to scale the IE process from toy problems to real-world applications. In this work, we compare three different met-rics to select stop words: Information Content, Correlation Coefficient, and Odds Ratio.
 To evaluate our filtering techniques we used the SIE sys-tem, a supervised system for ER developed at ITC-irst. SIE is designed to achieve the goal of being easily and quickly portable across tasks and languages. SIE is based on Sup-port Vector Machines and uses a (standard) general purpose feature set.
 We performed experiments on three different ER tasks (i.e. Named Entity, Bio-Entity and Temporal Expression Recog-nition) in two languages (i.e. English and Dutch). The results show that both the skewness and the data set size are drastically reduced, while the overall performance is in-creased in the bio-entity recognition task and maintained in the other tasks. Consequently, we reported a considerable reduction of the computation time required for training and classification. Overall, the system performs comparably to the state-of-the-art, demonstrating the general applicability of this technique.
 The paper is structured as follows. In the next section, we discuss the background of the proposed approach and provide an overview of the related work. In Section 3 we define a general framework for IF and we introduce the class of Stop Word Filters. In Section 4 we briefly describe the SIE system. In Section 5 results of a comparison of three Stop Word Filters on several benchmark data sets are presented. Finally, Section 6 is reserved for conclusions and topics for future research.
 Learning with skewed class distributions is a very well-known problem in machine learning. It has been experimentally shown that an unbalanced class distribution leads to poor performance on the minority class [16] (i.e. the positive class). The error rate of a classifier trained on a skewed data set is typically very low for the majority class (i.e. the nega-tive class), while it is unacceptable for the minority class in most of the cases. This phenomenon causes biased estima-tion [11] and suboptimal classification performance [2]. The poor performance on the minority class reflects on a drop off in both precision and recall [12] because both measures estimate the ability of the classifier to predict the positive class.
 The most common technique for dealing with skewed data sets is sampling. It consists in reducing the skewness by altering the distribution of training examples [17]. The basic sampling methods are under-sampling and over-sampling. Under-sampling eliminates majority class examples, while over-sampling increases the minority class. IF is a technique for performing uniform under-sampling on both the training and the test sets.
 An additional problem is the huge size of the data sets. In fact, to collect a sufficient amount of positive examples, it is inevitable to accumulate a large number of negative ones. The complexity of many supervised algorithms is strongly related to the data set size. For example, in the Support Vector Machines algorithm the number of support vectors increases linearly with the number of training examples [15], causing a loss of efficiency during both learning and pre-diction [4]. The large size of the training set is a rele-vant problem also for instance-based algorithms. For ex-ample, the complexity of kNN in classification is linear in the number of training examples. The problem of reducing the number of instances used for training while maintaining (and sometimes improving) the generalization accuracy, has been called Instance Pruning [18]. Instance Pruning tech-niques have been mainly applied to instance-based learning algorithms (e.g. kNN), to speed up the classification pro-cess while minimizing the memory requirements. The main drawback of many Instance Pruning techniques is their time complexity, which is generally quadratic in the data set size. In [19] the authors give a good overview of these method-ologies. IF is a technique for performing uniform Instance Pruning on both the training and the test sets.
 IF techniques have been proposed in the IE literature to alleviate the computational load and to reduce the data skewness in ER tasks. They can be considered as a way to perform instance pruning by under-sampling the nega-tive class. In contrast to sampling and pruning, IF is per-formed on both the training and the test sets. Below, we will briefly review a set of IF techniques 1 . In [13] the au-thors approach IE as a two-step classification strategy of text fragments. The aim of the first step is to filter out most of the negative examples without eliminating positive examples, so it is an IF in its proper sense. This filter can be perceived as a classifier that achieves high recall, while the second classifier tends to high precision. The filter was able to discard around 90% of the data set instances while
Note that the term Instance Filtering is introduced for the first time in this paper, while in most of the cited works filtering techniques have been referred to by different names. losing only about 2% of positive examples in a standard IE benchmark. A similar approach is reported in [5]. The au-thors implemented a two-step supervised algorithm for ER. The first step is performed by a  X  X entence filter X , that dis-cards all the sentences that are not likely to contain relevant information. Entity boundaries are then assigned to all the instances in the remaining sentences by adopting a standard supervised approach for ER. The filter is implemented by a supervised text classification system, tuned in order to max-imize recall. In [14] IF is performed as a preprocessing step, by eliminating all the tokens that are not located inside a noun phrase and whose part-of-speech (PoS) belongs to a stop list. This approach was applied to the JNLPBA shared task, allowing about 50% of the tokens to be filtered out, while losing 0,05% of positive instances. The main limita-tion of this technique is its portability across languages and domains. In [8], we investigated an IF technique for IE that filters uninformative words from both the training and the test sets. Uninformative words have been identified by es-timating the ratio between their probability of being (part of) an entity and their probability of lying outside. In this paper we extend and generalize such previous work. IF is a preprocessing step performed to reduce the number of instances given as input to a supervised classifier for ER. In this section, we describe a formal framework for IF and introduce two metrics to evaluate an Instance Filter. In ad-dition, we define the class of Stop Word Filters and propose an algorithm for their optimization. Let T = ( t 1 ,t 2 ,... ,t n ) be the list of all the tokens contained in a corpus T , let  X  ( t ) be a function that returns the type w associated to the token t 2 , let V = { w |  X  ( t i ) = w } be the vocabulary of T , and let  X  ( t i ) be a function such that An Instance Filter is a function  X ( t i ,T ) that returns 0 if the token t i is not expected to be part of a relevant entity, 1 otherwise. When  X  is applied to the whole data set T , it returns a Filtered data set T 0 such that T 0 =  X ( T ) = { t |  X ( t i ,T ) = 1 } . If labeled data is required to define the Filtering Function  X , we say that  X  is a Supervised Instance Filter. Otherwise, when unlabeled data is used or the filter is rule-based,  X  is an Unsupervised Instance Filter 3 Instance Filter  X  can be evaluated using the two following functions:
The terms  X  X oken X  and  X  X ype X  are used herein in accor-dance with their usual meanings in linguistics. A token is any word found in the corpus, a type is a set of identical tokens. A token is then defined by a type, together with a location at which that type occurs.
Following this distinction, [14] implements an Unsuper-vised Instance Filter, while both [5] and [13] implement a Supervised Instance Filter.
 and where  X  ( X  ,T ) is called the Filtering Rate and denotes the total percentage of filtered tokens in the data set T , and  X  + ( X  ,T ) is named as Positive Filtering Rate and denotes the percentage of positive tokens (wrongly) removed.  X  is a good filter if  X  + ( X  ,T ) is minimized and  X  ( X  ,T ) is maximized, allowing to reduce as much as possible the data set size while preserving most of the positive instances. In order to avoid over-fitting, the Filtering Rates among the training and test set ( T L and T T , respectively) have to be preserved: In addition, we use the skewness ratio given by Equation 5 to evaluate the ability of an Instance Filter to reduce the data skewness. We are therefore interested in Instance Filters that produce a filtered data set T 0 whose skewness ratio  X  ( T 0 ) is consid-erably lower than its original one: In this subsection, we present a class of Instance Filters called Stop Word Filters . They are implemented in two steps: first, Stop Words are identified from the training cor-pus T and collected in the set of types U  X  V ; then all their tokens are removed from both the training and the test set Formally, a Stop Word Filter is defined by A Stop Word Filter  X  U is fully specified by a list of stop words U . To find such a list, we borrowed from the text classification community a number of feature selection me-thods. In text classification, feature selection is used to re-move non-informative terms from bag-of-words representa-tions of texts. In this sense, IF is closely related to feature selection: in the former non-informative words are removed from the instance set , while in the latter they are removed from the feature set . Below, we present a set of metrics used to collect a stop word list U from a training corpus T . These metrics are functions of the following probabilities.
Note that only instances are removed from the data set, while words in U still appear in the feature description of the remaining tokens. The most commonly used feature selection metric in text classification is based on document frequency (i.e the num-ber of documents in which a term occurs). The basic as-sumption is that occurrences of very frequent types in texts are non-informative for document indexing. Our approach consists in removing all the tokens whose type has a very low information content. The IC filter 5 is specified by: In text classification the  X  2 statistic is used to measure the lack of independence between a type w and a category [20]. In our approach we use the correlation coefficient CC 2 =  X  of a term w with the negative class, to find those types that are less likely to express relevant information in texts. The CC filter is specified by: where
CC ( w,  X  ) = Odds ratio measures the ratio between the probability of a type to occur in the positive class, and its probability to occur in the negative class. In text classification the idea is that the distribution of the features on the relevant docu-ments is different from the distribution on non-relevant doc-uments [21]. Following this assumption, our approach is that a type is non-informative when its probability of being a negative example is sensibly higher than its probability of being a positive example [8]. The OR filter is specified by: In this section we describe how to find the optimal threshold  X  for a Stop Word Filter. We derive the optimal threshold value by resolving the following optimization problem: To solve this problem, we observe the behaviors of  X  and  X  + which, for construction, are decreasing functions of  X  , as shown in Figure 1. As a consequence, the optimization problem can be reformulated as the problem of finding the minimum  X   X  R such that  X  + ( X  U  X  ,T ) &lt; , that can be easily solved by progressively decreasing  X  until the upper IC is an unsupervised filter.
 bound for the positive filtering rate  X  + is reached 6 . Fil-tering Rates can be estimated by performing n -fold cross-validation on the training set. SIE (Simple Information Extraction) is a supervised system for Entity Recognition [7]. It casts the IE task as a classi-fication problem by applying Support Vector Machines for detecting entity boundaries in texts. SIE is designed with the goal of being easily and quickly portable across different tasks and languages.
 The architecture of the system is represented in Figure 2. In the training phase, SIE learns off-line a set of data mod-els from a corpus prepared in IOBE format (see 4.1). In the classification phase, these models are applied to tag new documents. In both the phases, the IF module is used to remove instances from the data sets, and the Feature Extrac-tion module is applied to all the unfiltered instances. Each entity boundary is independently identified by the Classifi-cation Module, and the final output is provided by the Tag Matcher module. In the following subsections we will briefly describe each module.
We implemented an algorithm for finding the optimal threshold with logarithmic time complexity. The corpus must be prepared in IOBE notation, an ad-hoc extension of the IOB notation. Both notations do not allow nested and overlapping entities. Tokens outside entities are tagged with O , the first token of an entity is tagged with B-entity type , the last token is tagged E-entity type , and all the tokens inside the entity boundaries are tagged with I-entity type , where entity type is the type of the marked entity (e.g. protein , person ). The IF module implements the three different Stop Word Filters described in Section 3.2 7 . Two different Stop Word Lists are provided for the beginning and the end boundaries of each entity, as SIE learns two distinct classifiers for them. The Feature Extraction module is used to extract a pre-defined set of features for each unfiltered token in both the training and the test sets. Each instance is represented by encoding all the following basic features for the token itself and for all the tokens in a context window of size  X  3 tokens: Type The type of the token.
 POS The Part of Speech of the token.
 Orthographic These features map each token into equiva-Moreover, the Feature Extraction module encodes all the bigrams of tokens and PoSs in a local window. SIE approaches the IE task as a classification problem by assigning an appropriate classification label to unfiltered to-kens. We use SVM light for training the classifiers 8 . In par-ticular, SIE identifies the boundaries that indicate the begin-ning and the end of each entity as two distinct classification tasks, following the approach adopted in [3; 6]. All tokens that begin(end) an entity are considered positive instances for the begin(end) classifier, while all the remaining tokens are negative instances. In this way, SIE uses 2 n binary clas-sifiers, where n is the number of entity types for the task. All the positive predictions produced by the begin and end classifiers are paired by the Tag Matcher module, that pro-vides the final output of the system. To perform this op-eration, the Tag Matcher module assigns a score to each candidate entity. If nested or overlapping entities occur, it selects the entity with the maximal score.
 The score of each entity is proportional to the entity length probability (i.e. the probability that an entity has a certain length) and to the confidence provided by the classifiers to the boundary predictions. The entity length distribution is estimated from the training set, as reported in Table 2.
All the IF techniques described in this paper are im-plemented in the java Instance Filtering tool (jInFil), freely available at http://tcc.itc.it/research/textec/ tools-resources/jinfil.html . SVM light is available at http://svmlight.joachims.org/ . Table 1: A corpus fragment with multiple predictions. The left column shows the actual label, while the right column shows the predictions and their normalized scores For example, in the corpus fragment reported in Table 1, the Classification Module have identified four possible bound-aries for the entity protein . The matching algorithm chooses among three mutually exclusive candidates:  X  X appa B en-hancer X ,  X  X appa B X , and  X  X  enhancer X . The scores for each candidate are 0 . 23  X  0 . 12  X  0 . 33 = 0 . 009108, 0 . 23  X  0 . 34  X  tively. In the example, the matcher correctly extracts the candidate that maximizes the score function.
 Table 2: The length distribution for the entity protein in the JNLPBA task In order to assess the portability and the language inde-pendence of our filtering techniques, we performed a set of comparative experiments on three different tasks in two dif-ferent languages (see Subsection 5.1). SIE exploited exactly the same configuration for all the tasks. We report results for three different Stop Word Filters: Information Content (IC), Odds Ratio (OR) and Correlation Coefficient (CC). In Subsection 5.2 we report the filtering rates obtained by vary-ing the parameter 9 . The results show that IF drastically decreases the computation time (see Subsection 5.3), while preserving or improving the overall accuracy of the system (see Subsection 5.4). The experiments reported in this paper have been performed in the following evaluation tasks: JNLPBA The JNLPBA shared task [10] is an open chal-
We tried = 0 , 0 . 01 , 0 . 025 , 0 . 05. = 0 means that the original data set has not been filtered. http://research.nii.ac.jp/~collier/workshops/ JNLPBA04st.htm . CoNLL-2002 The CoNLL-2002 shared task is to recog-TERN The TERN (Time Expression Recognition and Nor-Figure 3 displays the averaged filtering rates in the training and test sets on JNLPBA , CoNLL-2002 and TERN using IC, CC and OR filters, respectively. The results indicate that both CC and OR do exhibit good performance and are far better than IC in all the tasks. For example, in the JNLPBA data set, OR allows to remove more than 70% of the instances, losing less than 1% of the positive examples. These results pinpoint the importance of using a supervised metric to collect stop words. The results also highlight that our optimization strategy is robust against overfitting, be-cause the difference between the filtering rates in the train-ing and test sets is minimal, satisfying the requirement ex-pressed by equation 4. We also report a significant reduction of the data skewness. Table 3 shows that all the IF tech-niques reduce sensibly the data skewness on the JNLPBA data set 13 . As expected, both CC and OR consistently out-perform IC. Figure 4 displays the impact of IF on the computation time required to perform the overall IE process. The computa-tion time includes both IF optimization and training and testing the boundary classifiers for each entity. It is impor-tant to note that the cost of the IF optimization process is negligible: about 230, 128 and 32 seconds for the JNLPBA , CoNLL-2002 and TERN shared tasks, respectively. The curves indicate that both CC and OR are far superior to IC, allowing a drastic reduction of the time. Supervised http://cnts.uia.ac.be/signll/shared.html . http://timex2.mitre.org/tern.html .
We only report results for this data set as it exhibits the highest skewness ratio. All the experiments have been performed using a dual 1.66 GHz Power Mac G5.
 Figure 3: Averaged Filtering Rates (  X  ) compared on train-ing and test sets for JNLPBA , CoNLL-2002 and TERN IF techniques are then particularly convenient when dealing with large data sets. For example, the time required by SIE to perform the JNLPBA task decreased from more than 600 minutes to about 150 minutes (see Table 4). Figure 5 plots the values of the micro-averaged F 1 15 . Both OR and CC allows to drastically reduce the computation time and maintain the prediction accuracy with small values of . Using OR, for example, with = 0 . 025 on JNLPBA , F 1 augments from 66.7% to 67.9%. On the contrary, for CoNLL-2002 and TERN , for &gt; 0 . 025 and &gt; 0 . 01 respec-tively, the performance of all the filters rapidly declines. The explanation for this behavior is that, for the last two tasks, the difference between the filtering rates on the training and test sets becomes much larger for &gt; 0 . 025 and &gt; 0 . 01, re-spectively. That is, the data skewness changes significantly
All results are obtained using the official evaluation soft-ware made available by the organizers of the tasks. from the training to the test set. It is not surprising that an extremely aggressive filtering step reduces too much the information available to the classifiers, leading the overall performance to decrease.
 Table 4: Filtering Rate, Micro-averaged Recall, Precision, F 1 and Time for JNLPBA Tables 4, 5 and 6 summarize the performance of SIE com-pared to the baselines and to the best systems in all the tasks. SIE largely outperforms all the baselines, achieving performances close to the best systems in all the tasks It is worth noting that state-of-the-art IE systems exploit external information (e.g. gazeteers [1] and lexical resources [22]) while SIE adopts exactly the same feature set and does not use any external or task dependent knowledge source. The original motivation of our work was to approach the entity recognition task by using string kernels in a support vector machine based learning system. The high complexity of these algorithms suggested us to investigate in the direc-tion of Instance Filtering as a preprocessing technique to alleviate two relevant problems of classification-based learn-ing: the skewness of the class distribution and the data set Note that the TERN results cannot be publicly compared. Figure 4: Computation time required by the SIE system to perform the overall IE process for JNLPBA , CoNLL-2002 and TERN size. An important advantage of Instance Filtering is the drastic reduction of the computation time required by the entity recognition system to perform both training and clas-sification. We presented a class of instance filters, namely Stop Word Filters, based on feature selection metrics. We did an extensive set of comparative experiments on different tasks in different languages, using a set of Instance Filtering techniques as preprocessing modules for the SIE system. In all the experiments we performed, the results are close to the state-of-the-art.
 The portability, the language independence and the effi-ciency of SIE suggest its applicability in practical problems (e.g. user modeling, semantic web, information extraction from biological data) in which huge collections of texts have to be processed in a very limited time. In addition, the improved efficiency will allow us to experiment with kernel methods. For the future, we plan to implement more ag-gressive instance filtering schemata for Entity Recognition, by performing a deeper semantic analysis of the texts. Figure 5: Micro-Averaged F 1 values for JNLPBA , CoNLL-2002 and TERN Claudio Giuliano is supported by the IST-Dot.Kom project sponsored by the European Commission (Framework V grant IST-2001-34038). Raffaella Rinaldi is supported by the Web-FAQ project, funded by the Provincia Autonoma di Trento. We are grateful to Bruno Caprile, Cesare Furlanello and Alberto Lavelli for the helpful suggestions and comments. Thanks to Bonaventura Coppola for the preprocessing of CoNLL data set. [1] X. Carreras, L. M  X arques, and L. Padr  X o. Named entity [2] N. V. Chawla, N. Japkowicz, and A. Kotcz. Editorial: Table 5: Filtering Rate, Micro-averaged Recall, Precision, F 1 and total computation time for CoNLL-2002 Table 6: Filtering Rate, Micro-averaged Recall, Precision, F 1 and total computation time for TERN [3] F. Ciravegna. Learning to tag for information extrac-[4] C. Cortes and V. Vapnik. Support-vector networks. Ma-[5] A. De Sitter and W. Daelemans. Information extraction [6] D. Freitag and N. Kushmerick. Boosted wrapper induc-[7] C. Giuliano, A. Lavelli, and L. Romano. Simple infor-[8] A. M. Gliozzo, C. Giuliano, and R. Rinaldi. Instance [9] J. Kim, T. Ohta, Y. Tateishi, and J. Tsujii. Ge-[10] J. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-[11] S. Kotsiantis and P. Pintelas. Mixture of expert agents [12] J. Leskovec and J. Shawe-Taylor. Linear program-[13] D. Roth and W. Yih. Relational learning via propo-[14] Y. Song, E. Yi, E. Kim, and G. G. Lee. POSBIOTM-[15] I. Steinwart. Sparseness of Support Vector Machines X  [16] G. Weiss and F. Provost. The effect of class distribu-[17] G. M. Weiss. Mining with rarity: a unifying framework. [18] D. R. Wilson and T. R. Martinez. Instance pruning [19] D. R. Wilson and T. R. Martinez. Reduction techniques [20] Y. Yang and J. O. Pedersen. A comparative study [21] Z. Zheng, X. Wu, and R. Srihari. Feature selection for [22] G. D. Zhou and J. Su. Exploring deep knowledge re-
