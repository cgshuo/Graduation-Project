 With the ever-increasing growth of the Internet, numer-ous copies of documents become serious problem for search engine, opinion mining and many other web applications. Since partial-duplicates only contain a small piece of text taken from other sources and most existing near-duplicate detection approaches focus on document level, partial dupli-cates can not be dealt with well. In this paper, we propose a novel algorithm to realize the partial-duplicate detection task. Besides the similarities between documents, our pro-posed algorithm can simultaneously locate the duplicated parts. The main idea is to divide the partial-duplicate de-tection task into two subtasks: sentence level near-duplicate detection and sequence matching. For evaluation, we com-pare the proposed method with other approaches on both English and Chinese web collections. Experimental results appear to support that our proposed method is effectively and efficiently to detect both partial-duplicates on large web collections.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval -Information Search and Retrieval; H.3.7 [ Digital Libraries ]: Collection, Systems Issues Algorithms, Experimentation.
 Partial-Duplicate Detection, Sequence Matching, MapRe-duce
Because of the explosion of Internet and the fact that digi-tal documents can be easily replicated, enormous duplicated web pages and mirrored documents cause serious problem for search engine, product review, and many other Web ap-plications. Along with the increasing requirements, near-duplicate detection has received much attentions in recent years [24, 25, 11, 26, 20].

Existing studies on near-duplicate detection usually focus on the whole document level to figure out web pages that have the same content but only differ in the framing, nav-igation bar, advertisements, footer, and so on. Thus there are several factors that can not be well processed by existing methods.
 Collection: Figure 1 shows a pair of Web pages 1 2 which Multiple-page: In order to facilitate user X  X  browsing, some Threads in Forum: Millions of people contribute more than Besides the factors listed above, there are a number of prob-lems like, plagiarize sentences, non-cleaned web pages, sen-tences/paragraphs quotation, can also be generalized to par-tial duplicate . If a pair of documents are partial-duplicate with each other, it means they contain a number of sentences or paragraphs with similar content. With requirements of applications such as plagiarism detection, information flow tracking, opinion mining, and so on, partial-duplicate detec-tion task is proposed and studied in this paper. Local text reuse detection [23] can be used to partially address this task. However, we argue that only similarities and category types do not provide sufficient information for all applica-http://iphandroid.com/ http://www.chinapost.com.tw/business/company-focus/2009/11/25/234147/Droid-is.htm tions and are not convenient enough for user to easily find the duplications in dozens of lines.
 Figure 1: Examples of partial-duplicate web pages
In this paper, we present an efficient algorithms for detect-ing partial-duplicates and locating their positions. Figure 2 shows an example on partial-duplicates. As shown in the graph, a sequence of sentences in Page A are similar with a number of sentences in Page B. Page A and C also con-tains duplicated text. From these pairs, we try to get the following results: Since the proposed method can not only detect duplicates but also locate their positions, the near-duplicates of the whole document level can also be precisely detected. As the Web collections contain hundreds of millions pages, the algo-rithm is explored with MapReduce [8], which is a framework for large-scale distributed computing. We implement our method and compare it with the state-of-the-art approaches on four web collections and one manually constructed evalu-ation corpus. The experimental results show that it achieves good performance, both effectiveness and efficiency are sig-nificantly improved.

The contributions of this work are as follows: 1) We con-vert the partial-duplicate detection task into sentence level near-duplicate detection task and sequence matching task. 2) In order to handle hundreds of millions documents, the al-gorithm is designed and implemented under the MapReduce framework. 3) Shingles, I-Match, and Spotsigs are compared and evaluated in experiments, and experimental analyses of the signatures for sentences are provided. 4) Evaluations on manually labeled  X  X racle Set X  and four large web collections are used to measure the effectiveness and efficiency.
The remaining of the paper is organized as follows: In section 2, we review a number of related work and the state-of-the-art approaches in related areas. Section 3 provides an brief introduction of MapReduce. Section 4 presents the proposed method. Experimental results in test collections and analyses are shown in section 5. Section 6 concludes this paper.
Near-duplicate detection has received considerable atten-tions over the past several years. Previous studies on du-plicate and near-duplicate detection can be roughly divided into two research directions: document representation and efficient detection. The first one focuses on representing doc-uments with or without linguistic knowledge. Since collec-tion contains hundreds of millions of documents, the second one, efficiency, has also received lots of attentions. This sec-tion introduces related approaches briefly.

Broder [3] defined the resemblance and containment be-tween two documents. He used shingles to represent doc-uments and Jaccard overlap to calculate the similarity be-tween documents. In order to reduce the complexity of shin-gling, Broder [4] proposed to use meta-sketches for this task.
Indyk and Motwani[15] proposed the notion of locality-sensitive hashing and applied it to sublinear-time similarity searching. LSH maintains a number of hash tables, which each of is parameterized by the number of hashed dimen-sions. Points close to each other in some metric space have the same hash value with high probability. Gionis et al. [11] also used LSH for approximate similarity search.
I-Match [7] hinges on the premise that removal of very infrequent terms and very common terms results in good document representations for the near-duplicate detection task. They filter the input document based on collection statistics and compute a single hash value for the remainder text. The documents with same hash value are duplicates.
Schleimer et al. [22] proposed a local document finger-printing algorithm, which is called winnowing. They de-scribed and analyzed the winnowing algorithm for selecting fingerprints from hashes of k-grams. They also presented the complexity of any local document fingerprinting algorithm and gave the non-trivial lower bound.

Henzinger [13] performed an evaluation of Border et al. X  X  [4] shingling and Charikar X  X  [6] random projection near-duplicate algorithms on 1.6B web pages. The results showed that nei-ther of the algorithms works well for detecting near-duplicate pairs on the same site, while both of them achieve high pre-cision for near-duplicate pairs on different sites.
Manku et al. [19] proposed an approach for both online and batch types near-duplicate detection. They used Charikar X  X  fingerprinting technique [6] and demonstrated it X  X  effective-ness. They also presented an algorithmic technique for iden-tifying existing f-bit fingerprints that differ from a given fin-gerprint in at most k bit-positions, for small k.
Theobald et al. [26] presented their work SpotSigs, which combine stopword antecedents with short chains of adjacent content terms. Through demonstrating the upper bounds of Jaccard similarity, they also proposed several pruning condi-tions, which could ignore all pairs of documents safely during the matching process when SpotSig vectors exceed a certain difference in length.

Besides the approaches focused on Web pages or docu-ments, Muthmann et al. [20] proposed their work to iden-tify threads with near-duplicate content and to group these threads in the search results. They incorporated text-based features, features based on extracted entities for products, and structure-based features to capture the near-duplicate threads.

Local text reuse detection proposed by Seo and Croft [23] is also related to our method. Different from duplicate de-tection, text reuse tries to capture the loose restatements of the information from the previous sources [2]. They de-fined six categories of text reuse and a general framework for text reuse detection. Several fingerprinting techniques for the framework were evaluated under the framework.
Lin [18] explored the problem of pairwise similarity on large document collections and introduced three MapReduce algorithms to solve this problem, which are based on brute force, large-scale ad hoc retrieval, and the Cartesian product of postings lists. Different with us, the granularity of this work is also document level.

Kolak and Schilit [16] described an approach to mine pop-ularly quoted passages and add links among them on a dig-ital library. They use shingle table method to find repeated sequences between different books. Since the storage com-plexity of shingle methods is huge and extracting shared shingles is timing consuming tasks, the method can not be directly used for partial-duplicate detection task.
In order to handle hundreds of millions web collections, we also use MapReduce framework in this work, which is introduced by Dean and Ghemawat [8]. It is used an asso-ciated implementation for processing and generating large data sets. The MapReduce programming model has been successfully used at Google for many different purposes.
As number of data such as web pages, web request logs, and so on grows rapidly, applications have to be distributed across thousands of machines in order to finish in time. Bulk-synchronous parallel (BSP) model [27] and some higher-level abstractions(MPI [12]) have been supported program-mers to write parallel programs. However, because of its higher-level abstractions, programmers usually spend too much time on details. MapReduce [8], which is difference from these systems, exploits a restricted programming model to parallelize the user program automatically. And the trans-parent fault-tolerance and load balancing are also provided, because of the restrictions.

The key concept behind MapReduce is inspired by the map and reduce primitives present in many functional lan-guages. Dean and Ghemawat [8] presented the observation that most information processing computations share the same two-stage structure, which contains map and reduce operations. The map operation is applied to every logical  X  X ecord X  of input to compute a set of intermediate key/value pairs. Then the reduce operation is applied to all the values that shared the same key, in order to combine the derived data. Figure 3 shows the two-stage structure.

Under this framework, the computation takes a set of in-put key/value pairs, and produces a set of output key/value pairs. A programmer only needs to implement two opera-tions: map and reduce . The intermediate key/value pairs will be grouped and sorted by the key automatically.
Many different implementations of MapReduce interface are available now. Google X  X  MapReduce implementation is coupled with Google File System (GFS) [10], a kind of dis-tributed file system. Apache X  X  MapReduce implementation, Hadoop 3 , which follows the same architecture, uses a dis-tributed file system named Hadoop Distributed File System (HDFS) to store data and the intermediate results. Hadoop tries to schedule the MapReduce computation tasks to the node where the data locates in order to reduce the overall network I/O. Besides Hadoop, MapReduce has also been implemented by many corporations, such as Greenplum, GridGain, Cell Broadband Engine, and so on.

In this paper, we implement our algorithms under the open-source implementation Hadoop 0.20. HDFS is used to provide the distributed storage.
A partial-duplicate is a pairwise relationship. Given a pair of documents, we need to identify and locate the duplicated parts between them. To make questions simple, we limit granularity to sentence level. Based on this assumption, we propose the algorithm PDC-MR , which converts the partial-duplicate detection task into three MapReduce jobs (illustrated in Figure 4 and Figure 5). 1) Indexing : We use a MapReduce job to build a stan-dard inverted index [9] for collections. Signatures used as terms in the inverted index are extracted from each sen-tences in map procedure. The map procedures emit the sig-nature as the key, and a tuple consists of the document id and sentence id. After grouping and sorting, the reduce pro-cedures take the tuples as input and write out the inverted index to the disk. Since signatures would highly impact the final result, a detail description of it will be given in the Section 4.1. 2) Sentence Duplication Detecting : Jaccard coeffi-cient is used to measure the similarities between sentences. If the Jaccard similarity between a sentence pair is over a threshold, they are considered duplicates. Another MapRe-duce job is used to detect the sentence duplicates. The map procedures read the inverted index from disks and emit a pair of sentences which both contain a same signature as the key. After grouping and sorting, all signature ids belonging to the same sentence pair are brought together. The reduce procedures take them as inputs, and emit the sentence du-plications. The procedure is shown in the right of Figure 4. http://hadoop.apache.org/ 3) Sequence Matching : With the results of sentence duplicate detection, matrixes representing sentence dupli-cates for each pair of documents are generated. Figure 5 shows an example of the sentence duplicates between page d and page d j . The dot plots in the figure represents dupli-cated sentence pairs. The sequences of duplicated sentences are partial duplications we try to extract and locate. Based on that, the problem can be straightly converted to the se-quence matching task, which aims to find all diagonals in the matrix. We also use a MapReduce job to do that. The out-puts of the job include partial duplicates among documents and their locations. Since numerous of document pairs are needed to be processed, Section 4.3 gives detail descriptions about the efficient sequence matching method.
As described in the Section 2, a number of signature ex-traction methods have been proposed for document level near-duplicate detections. Since the average number of words per sentence is much fewer than document, we introduce sev-eral signature methods in this section. Shingles is the simplest method, which is proposed by Broder et al. [5]. It tokenizes documents into a list of words and extracts all word sequences of adjacent words to repre-sent the document.  X  n-shingles  X  represents the number of n adjacent words in a shingle. As the shingles uses all chunks, it might not be suitable for large collections because of too many signatures.
I-Match [7] uses SHA1 hash function over concatenation of terms filtered by stopwords and infrequent terms. It hinges on the assumption that removal of very infrequent terms and stop words results in good document representations for the near-duplicate detection task. Although the compu-tationally of I-Match is attractive, it usually unstable even to small perturbations of content.
SpotSigs [26] combines stopword antecedents with short chains of adjacent content terms. A spotsig s i of a location in a document consists of a chain of words that follow an antecedent word a i at a fixed spot distance d i . Antecedent words are predefined and typically chosen to be stop words. Experimental results in [26] show that SpotSigs with five common terms as antecedent achieve better result than a full stopword list. However, we observe that signatures can not be extracted from more than 15.2% sentences in English collection with the five common terms. The experimental results about selecting the number of antecedents are shown in Section 5.3.
As shown in Figure 4, the sentence duplicate detection algorithm, which is implemented by a MapReduce job, ex-tracts near duplicated sentence pairs whose Jaccard similar-ity are higher than a threshold. Sentences are represented by a group of signatures. The upper bounds for Jaccard similarity [26] is For | A | X | B | , we can get: With the upper bound and vector representation of docu-ments, we observe that only similar length sentence pairs can be near duplicate. If we set the threshold to  X  , sen-tence pairs where | A | | B |  X   X  can be safely removed. Based on that, the pseudo-code of this method is show in Algo-rithm 1. The input of the procedure map is the signature id ( sig i ) and associated postings list ( [ d 1 s 1 , d d s j represents document id and sentence id). Inside each mapper, all candidate sentence pairs, which follow the upper bound of the Jaccard similarity, are emitted to the key-value nature ids belonging to the same sentence pair are brought together. With the list, Jaccard similarity can be easily calculated. The procedure reduce takes the sentence pair and corresponding list as input and emit the duplication judgments based on the Jaccard similarity and predefined threshold  X  .
 Algorithm 1 Pseudo-code of sentence duplication detection algorithm in MapReduce MAP ( sig i , [ d 1 s 1 , d 2 s 2 , ... ]) 1: for all d i s j  X  [ d 1 s 1 , d 2 s 2 , ... ] do 2: for all d k s l  X  [ d 1 s 1 , d 2 s 2 , ... ] do 3: if d i s j 6 = d k s l then 5: EMIT (  X  d i s j , d k s l  X  , sig i ) 6: end if 7: end if 8: end for 9: end for REDUCE (  X  d i s j , d k s l  X  , [ sig 1 , sig 2 , ... ]) 2: EMIT (  X  d i , d k  X  ,  X  s j , s l  X  ) 3: end if
As described in the previous sections, the sequence match-ing procedure aims to find all diagonals in the matrix. Al-gorithm 2 shows the pseudo-code of the MapReduce job. Inputs to the procedure map consists document pairs (keys,  X  d , d j  X  ) and a corresponding list of duplicated sentence pairs between these documents (values, [  X  s k , s l  X  ,  X  s p , s each duplicated sentence pair, the longest diagonal whose root is the pair is extracted and emitted. Extracted sen-tence pairs will be eliminated.  X  is used as the threshold for the diagonal length. The final output, which contains document pair, respective start positions, and length, are generated in the procedure reduce . In practical, the reducer can also be merged into the mapper to trim the intermediate data. We evaluate our methods with four corpora WT10g, TREC Blogs06 4 , SogouT 2.0 5 , and ClueWeb09-T09B 6 . Table 1 shows the statistics of the four collections. WT10g is used by TREC Web tracks, which contains more than 1.6 million documents collected from about 11,000 servers. Besides that, BLOGS06 corpus, which is used by TREC 2006 and TREC 2007 blog tracks, is also selected to evaluate sys-tems. It is a big sample of the blogsphere, and contains more than 3.2 millions documents including spam as well as http://ir.dcs.gla.ac.uk/test collections http://www.sogou.com/labs/dl/t.html http://boston.lti.cs.cmu.edu/Data/clueweb09/ Algorithm 2 Pseudo-code of sequence matching algorithm in MapReduce MAP (  X  d i , d j  X  , [  X  s k , s l  X  ,  X  s p , s q  X  , ... ]) 1: P  X  [  X  s k , s l  X  ,  X  s p , s q  X  , ... ] 2: for all s i s j in P do 4: if | D | &gt;  X  then 5: EMIT (  X  d i , d j  X  , D ) 6: P  X  P  X  D 7: end if 8: end for
IAGONAL E XTRACT ( s i s j ) 1: while s i s j in P do 2: D  X  D 5: end while REDUCE (  X  d i , d j  X  , [ D 1 , D 2 , ... ]) 1: for all D  X  [ D 1 , D 2 , ... ] do 2: EMIT(  X  d i , d j  X  , Start.d i , Start.d j , D.Length  X  ) 3: end for possibly non-blogs. SogouT 2.0 corpus is made up of 24.8M Chinese Web pages and crawled from all domains. TREC Category B dataset(ClueWeb09-T09B), which is a subset of the ClueWeb09, contains 50 million English pages and has been used in various TREC tracks.
 WT10g English 1,692,096 11GB Blogs06 English 3,215,171 88.8GB SogouT 2.0 Chinese 24,833,521 372.5GB
ClueWeb09-T09B English 50,220,423 490.4GB All the MapReduce jobs were implemented in Java for Hadoop framework. HDFS was used to provide the dis-tributed storage. All experiments were evaluated on a 16 machines cluster. Each machine contains two Xeon quad core CPUs (2.0GHz), and 32GB RAM. Software stack of the experiments used Java 1.6 and Hadoop version 0.20. For web page cleaning, we just removed all HTML markup tags from the collections. Since the impact of sentence boundary detection X  X  performance would not be heavy and a number of manually written rules can achieve good result [1] with lit-tle attractive computational consumption, we used around 50 rules to do that in our experiment.
In order to compare the performances of different signa-tures, we manually select 2000 documents, which contain 57135 sentences totaly, from ClueWeb09-T09B ( Oracle Eng is used to represent the corpus in the following section for simple). For Chinese corpus SogouT, we also constructed a manually labeled corpus ( Oracle Chn ), which contains Figure 6: Shingles X  performances of varying the threshold  X  for corpora Oracle Eng and Oracle Chn Figure 7: I-Match X  performances of varying IDF for corpora Oracle Eng and Oracle Chn 80516 sentences extracted from 2000 documents. Six in-dividuals were asked to label them. The average Kappa statistic among them is around 91.6%, which shows good agreement.

Figure 6 shows the performances comparison of 2-Shingles, 3-Shingles, and 4-Shingles. We observe that 4-Shingles con-sistently performs better than 2-Shingles and 3-Shingles in both English and Chinese collections. Different with results in document level [18, 26], threshold  X  = 0 . 9 achieves the best performance in both of the collections. The reason is that around 91% of duplicated sentences in Oracle Chn and 89% of them in Oracle Eng are exactly same with each other in our evaluation collections. However, this kind of factor is rare in the document level.
 Figure 7 shows the performances of I-Match with different IDF ranges. Tokens exceeding IDF range were filtered. We where N is the corpus size, df j is the document frequency of the token. Since the similarities calculated by I-Match are either 0 or 1, the threshold  X  does not need to adjusted. The best result is achieved by [0.1, 1.0] in both Oracle Chn and Oracle Eng. It means that most of the tokens should be kept and used to calculate the hash result. The main reason is that sentences usually contain a small number of tokens and most of the duplicated sentences are same with each other. When tokens whose IDF is lower than 0.4 are filtered, most of the sentences have less than 2 tokens left in Oracle Chn. Because of that, the recall for [0.4, 1.0] is almost perfect 100%, but the precision is only 1.4%.
The impacts of the number of antecedents for Spotsig are shown in Figure 8. The x-axis represents the number of an-tecedents and varies from 5 to 500 in Oracle Chn and 10 to 20K in Oracle Eng. The numbers below each point represent the average number of signatures per sentences with corre-sponding antecedents. It shows that the antecedents X  num-Figure 8: Spotsigs X  performances of varying the number of antecedents for corpora Oracle Eng and Oracle Chn Figure 9: Spotsigs X  performances of varying the threshold  X  for corpora Oracle Eng and Oracle Chn ber would highly impact the performance. We think that the main reason is that sentences cannot be well represented by a small number of signatures. By trading-off between effi-ciency and effectiveness, we choose # antecedent = 60 to achieve 95.2% F1 score in Chinese collection. For English one, we choose # antecedent = 10 K . We observe that the number of antecedents is much different between English and Chinese collections. However the best results are both achieved at the similar average number of signatures per sentence. It shows that a sentence can be well described by around 4 signatures. Figure 9 shows the performances with different thresholds. Comparing with shingles, spotsigs show the similar trends. We achieve the best result with  X  = 0 . 9 in Oracle Chn and Oracle Eng.

In summary, 4-Shingles achieve the best result in the sen-tence level duplicate detection. However, the performances of 2-Shingles, 3-Shingles, Spotsigs, and I-Match are compa-rable. The parameters used for sentence level are much dif-ferent with document level ones. We think that it is caused by the characters of sentence collections, such as length, standard for labeling and so on. We also observe that al-though all three signature extraction methods are highly tunable, the results prove to be robust for a large variety of parameters.
After evaluating three different methods to extract du-plicated sentences, we now consider the impact of sequence matching. Table 2 summaries the sequence matching re-sults with different signatures. The configurable parameters IDF range, similarity threshold  X  , and # antecedent are se-lected by the previous experiments and listed in the brack-ets. We use Precision, Recall, and F 1 -Score as our choice of evaluation metric to measure how accurately the dupli-Table 2: Summary of sequence matching results with Shingles, I-Match and Spotsigs for Oracle sets cation is located. From analyzing the Oracle collections, we observe that lengths of most duplications are bigger than three. Hence,  X  , which is the threshold of diagonal length, is set to 3 in all the experiments. We observe that the final results are heavily related to the performances of sentence duplicate detection. Since the performances of 2-Shingles, 3-Shingles, 4-Shingles, I-Match and Spotsigs are similar, the final F 1 -scores do not have significant difference. In order to evaluate the impact of  X  , we also evaluate the perfor-mances at  X  = 1 . In Oracle Eng, the F 1 -score of 2-Singles is only 0.952, which is significantly 7 different from the results shown in the Table 2. 3-Shingles, 4-Shingles, I-Match and Spotsigs have the similar results. By trading-off efficiency and effectiveness, we determine to use I-Match method to extract signatures in our method.

Figure 10 summarizes our results of PDC-MR versus the document level near-duplicate detection. For convenient comparison among copra, the top one million documents of each corpus are used in this experiment. For document level near-duplicate detection, the state-of-the-art method Spot-sig is used, whose parameters are set up based on [26]. The y-axis represents the number of unique documents. The bot-tom parts of each bar represent results of Spotsig. The top parts represent the number of documents which can be de-tected by our PDC-MR method but can not be detected by document level Spotsig. In WT10g, Spotsig extracts around 31K documents which contain duplications in the same cor-pus. They compose more than 1.89 million duplication pairs. Besides those documents, through our method, another 94K documents which contain partial-duplicates are detected. In Blogs06, ClueWeb09-T09B, and Chinese corpus SogouT2.0, we get similar results. It shows that partial-duplications are common in web collections and our proposed method can effectively detect them.

In order to evaluate the validity of the extracted partial duplicates, we random select 200 documents from the de-tection results of each corpus and manually classify them into four types as listed in the Table 3.  X  X ews Collection X  and  X  X ultiple Page X  are described in the Section 1.  X  X artial Quotation X  represents all types of short piece of text quo-tation. Banner, copyright notice, navigation bar, and other non-content parts are classified into  X  X ther X . The results show that  X  X artial quotation X  account for the majority of all instances. The average length of this kind of duplications
The paired  X  -test (  X &lt; 0.05) is used to measure the signifi-cance.
 Table 3: Partial duplicates in the web collections Figure 10: Summary of PDC-MR vs. document level Spotsig in four web collections is around 6 sentences. While the average length of docu-ment is more than 23 sentences in SogouT 2.0 and 26 sen-tences in WT10g. Thus those partial duplications can not be easily detected by the existing document level detection methods. The results show that most of extracted partial duplications are useful and meaningful. Except ClueWeb09-T09B, the percentages of  X  X ther X  type in other collections are less than 15%. While, there are 32% instances belonging to this type in ClueWeb09-T09B. We think the main reason is that ClueWeb09-T09B is not well cleaned and contains lots of advertisements.
Figure 11 plots the running times of spotsigs based near-duplicate detection and our proposed PDC-MR method for different corpus size. ClueWeb09-T09B is used in this exper-iment. All Hadoop jobs in the efficiency experiments were configured with 60 mappers and 60 reducers. The graph suggests that although the number of sentence is huger than the number of documents, our proposed method is more ef-ficient than Spotsig. We think that it makes sense since I-match is efficient and its performance is also comparable in sentence level.
This paper presents our work on partial-duplicate detec-tion task. A number of factors like news collection, multiple pages, threads in forums, plagiarize sentences, non-cleaned web pages, and sentences/paragraphs quotation belong to it. In order to address this problem, we propose a novel MapReduce algorithm, which converts the task into three MapReduce jobs. Except for the similarities between doc-uments, the algorithm can simultaneously output the posi-tions where the duplicated parts occur. The contributions of the work include both empirical analysis of signatures for Figure 11: Running time of the PDC-MR and Spot-sig with different corpus size sentence and algorithm design. Experimental results in four real-world web collections show that the proposed method can be effectively and efficiently used to detect partial-and near-duplicate.
The author wishes to thank the anonymous reviewers for their helpful comments. This work was partially funded by 973 Program (2010CB327906), Shanghai Leading Academic Discipline Project (B114), Doctoral Fund of Ministry of Ed-ucation of China (200802460066), and Shanghai Science and Technology Development Funds (08511500302). [1] J. Aberdeen, J. Burger, D. Day, L. Hirschman, [2] M. Bendersky and W. B. Croft. Finding text reuse on [3] A. Z. Broder. On the resemblance and containment of [4] A. Z. Broder. Identifying and filtering near-duplicate [5] A. Z. Broder, S. C. Glassman, M. S. Manasse, and [6] M. S. Charikar. Similarity estimation techniques from [7] A. Chowdhury, O. Frieder, D. Grossman, and M. C. [8] J. Dean and S. Ghemawat. Mapreduce: Simplified [9] W. B. Frakes and R. A. Baeza-Yates. Information [10] S. Ghemawat, H. Gobioff, and S.-T. Leung. The [11] A. Gionis, P. Indyk, and R. Motwani. Similarity [12] W. Gropp, E. Lusk, and A. Skjellum. Using MPI: [13] M. Henzinger. Finding near-duplicate web pages: a [14] S. C. Herring, L. A. Scheidt, I. Kouper, and [15] P. Indyk and R. Motwani. Approximate nearest [16] O. Kolak and B. N. Schilit. Generating links by [17] A. Ko X  X cz, A. Chowdhury, and J. Alspector. Improved [18] J. Lin. Brute force and indexed approaches to pairwise [19] G. S. Manku, A. Jain, and A. Das Sarma. Detecting [20] K. Muthmann, W. M. Barczy  X nski, F. Brauer, and [21] R. Ramakrishnan and A. Tomkins. Toward a [22] S. Schleimer, D. S. Wilkerson, and A. Aiken. [23] J. Seo and W. B. Croft. Local text reuse detection. In [24] N. Shivakumar and H. Garcia-Molina. Scam: A copy [25] N. Shivakumar and H. Garcia-Molina. Finding [26] M. Theobald, J. Siddharth, and A. Paepcke. Spotsigs: [27] L. G. Valiant. A bridging model for parallel
