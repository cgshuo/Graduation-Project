 A/B testing, also known as bucket testing, split testing, or controlled experiment, is a st andard way to evaluate user engagement or satisfaction from a new service, feature, or product. It is widely used among online websites, including social network sites such as Facebook, LinkedIn, and Twitter to make data-driven decisions. At LinkedI n, we have seen tremendous growth of controlled experiments over time, with now over 400 concurrent experiments running per day. General A/B testing frameworks and methodologies, incl uding challenges and pitfalls, have been discussed extensivel y in several previous KDD work [7, 8, 9, 10]. In this paper, we describe in depth the experimentation platform we have built at LinkedIn and the challenges that arise particularly when running A/B tests at large scale in a social network setting. We start with an introduction of the experimentation platform and how it is built to handle each step of the A/B testing process at LinkedIn, from designing and deploying experiments to analyzing them. It is then followed by discussions on several more sophis ticated A/B testing scenarios, such as running offline experiments and addressing the network effect, where one user X  X  action can influence that of another. Lastly, we talk about features a nd processes that are crucial for building a strong experimentation culture. G.3 Probability and Statistics/Experimental Design: controlled experiments, randomized experiments, A/B testing.
 Measurement, Design, Experimentation Controlled experiments, A/B tes ting, social network, online experiments, network A/B testing, measurement. A/B testing, also called controlled experiment, has become the gold standard for evaluating new product strategies and approaches in many internet companies, including Amazon, eBay, Etsy, Facebook, Google, Groupon, Li nkedIn, Microsoft, Netflix and Yahoo [9, 10]. As experimentation gains popularity, so does the need for properly designing, managing and analyzing experiments. The theory of controlled experiment dates back to Sir Ronald A. Fisher X  X  experiments at the Rothamsted Agricultural Experimental Station in England in the 1920s [11]. Since then, many textbooks and papers from different fields have provided theoretical foundations [20, 21, 32, 33] for running controlled experiments. While the theory may be straightforward, the deployment and mining of experiments in practice and at scale can be complex and challenging [13, 14]. In particular , several past KDD papers have discussed at length the experimentation systems used at Microsoft Bing and Google [8, 9], including be st practices and pitfalls [7, 10]. Facebook also introduces the PlanOut language which provides a toolkit for parameter-based experiments [12]. At a high level, we follow sim ilar practices and methodology for experimentation at LinkedIn. Ho wever, many of the challenges we face arise particularly because LinkedIn is a member-based social network (we call our logged-in users  X  X embers X ). In this paper, we focus on how we address these challenges as we scale to run more experiments. We share how we built XLNT (pronounced  X  X xcellent X ), the end-to-end A/B testing platform at LinkedIn, to not only meet the day-to-day A/B testing needs across the company, but to also address more sophisticated use cases that are prevalent in a social network setting. When we launched XLNT, the platform only supported about 50 experiments per day. Today, that number has increased to more than 400. The number of metrics supported has grown from 60 to more than 1000. Such tremendous gr owth is not only attributed to our scalable platform but also to our continuous emphasis on embedding experimentation deeply into LinkedIn X  X  decision-making process and culture. We include in the paper several XLNT features we built to enable us to take education and evangelization past the  X  X lassroom X . A/B testing is truly the driver behind LinkedIn X  X  product innovation. The areas we experime nt on are extremely diverse, ranging from visual changes on our home page, to improvements on our job recommendation algorithm, to personalizing the subject line of our emails. We begin this paper with two example experiments that we recently ran. The first experiment was on member s X  profile pages (Figure 1). In order to encourage members to better establish their professional identity, we displayed a small modul e at the top of their profile. The experiment was to include an additional line of text to call out the benefits and values a complete profile provides. For instance, the example in Figure 1 encourages members to add volunteer experience to their profile. This small change turned out to be extremely successful. Results from the A/B test showed a 14% increase in profile edits on volunteer experience! Another experiment was an entire redesign of the Premium Subscription X  X  payment flow (F igure 2). Apart from providing a cleaner look and feel, we re duced the number of payment checkout pages and added an FAQ. The experiment showed an increase of millions of dollars in annualized bookings, about 30% reduction in refund orders and over 10% lift in free trial orders. In both cases, the XLNT platform made it feasible to quickly measure the impact of both small and large changes at low cost. This enables us to identify features with a high Return-On-Investment (ROI) and moreover, to quantify the impact in a scientific and controlled manner. A screen shot of our analysis dashboard is shown in Figure 3. Here is a summary of our contributions in this paper:  X  We share the details of how we built our experimentation  X  We discuss several challenging A/ B testing scenarios we face  X  We discuss several concepts and novel features we  X  Many real A/B test examples are shared for the first time in The paper is organized as follows. Section 2 introduces XLNT and how it is built to address several fundamental challenges. Section 3 discusses several more sophisticated A/B testing use cases. Section 4 discusses the XLNT features that help create a stronger experimentation culture and Section 5 concludes. We realized early on that ad-hoc A/B testing was not a scalable approach to sustain the high speed of innovation at LinkedIn. We required an A/B testing platform to allow us to quickly quantify the impact of features. Additionally, this needed to be achieved in a scientific and controlled manner across the company. Thus, we built XLNT. The XLNT platform is aimed at encompassing every step of the testing process, from designing and deploying experiments to analyzing them. In particular, it was built to address the following concerns and challenges: 1. Scalability. We continue to see tremendous growth in both 2. Incorporating Existing Practices . Over time LinkedIn 3. Engineering Integration . The platform has to be well 4. Flexibility . Although the basic A/B testing requirements are 5. Usability. A/B testing is not limited only to the R&amp;D Taking these challenges into consid eration, we share the details of the XLNT platform in this section, with the overall architecture outlined in Figure 4 below. Experimental design is arguably th e most important step in the testing workflow to get good and meaningful results. As Sir R. A. Fisher put it [27]  X  X o consult the st atistician after an experiment is finished is often merely to ask him to conduct a post mortem examination. He can perhaps say what the experiment died of. X  To this end, we have built the platform to incorporate the standard practice at LinkedIn while providing capabilities to enable better designs and prevent common pitfalls. In this section, we first start with introducing a few key concepts that are fundamental to our experiment model and then fo cus on targeting, a critical component used in designing experiments at LinkedIn. Most experimentation terminologie s used at LinkedIn are standard and can be found in any experimental design textbooks [28]. We focus here on only a few definiti ons that are key to our platform. To run an experiment, one starts by creating a testKey , which is a unique identifier that represents the concept or the feature to be tested. An actual experiment is then created as an instantiation of the testKey. Such hierarchical structure makes it easy to manage experiments at various stages of the testing process. For example, we want to investigate the bene fits of adding a background image. We begin by diverting only 1% of US users to the treatment, then increasing the allocation to 50% and eventually expanding to users outside of the US market. Even though the feature being tested remains the same throu ghout the ramping process, it requires different experiment inst ances as the traffic allocations and targeting changes. In other words, an experiment acts as a realization of the testKey, and only one experiment per testKey can be active at a time. Every experiment is comprised of one or more segments , with each segment identifying a subpopulation to experiment on. A common practice is to set up an experiment with a  X  X hitelist X  segment containing only the team members developing the product, an  X  X nternal X  segmen t consisting of all LinkedIn employees and additional segments targeting external users. Because each segment defines its own traffic allocation, the treatment can be ramped to 100% in the whitelist segment, while still running at 1% in the external segments. Note that segment ordering matters because members are only considered as part of the first eligible segment. Afte r the experimenters input their design through an intuitive User Interface, all the information is then concisely stored in a DS L (Domain Specific Language). For example, the line below indicate s a single segment experiment targeting English-speaking users in the US where 10% of them are in the treatment variant while the rest in control. It is important to mention that each experiment is associated with a hashID , which serves as an input to an MD5 based algorithm used to randomize users into variants. By default, all experiments of the same testKey share the same hashID, and different testKeys have different hashIDs. This ensures that a user receives consistent experience as we ramp up a treatment. More importantly, as we have hundred s of experiments running in parallel, different hashIDs imply that the randomizations between active experiments are orthogonal. The platform also allows manually overwriting the hashIDs, and the applicable usage cases will be discussed in Section 3.1. We recognize that not only are ou r products diverse, each one of our users is special and unique. With that in mind, many of the experiments we run at LinkedIn focus on how to provide the most improved user experience possible for specific user groups. This is achieved by creating different segments in an experiment targeting different subpopulations, as we have mentioned in Section 2.1.1. Deciding on the right population to target is the most important part of experiment design. There are three targeting capabilities provided by the platform: Built-in Member Attributes . The platform provides more than 40 built-in member attributes for experimenters to leverage. They range from static attributes su ch as a member X  X  country to dynamic attributes such as a member X  X  last login date. These attributes are computed daily as part of our data pipelines and pushed to Voldemort, a distributed key-value data storage system [22], for real-tim e targeting. Customized Member Attributes. Frequently experimenters need a targeting criterion beyond the default ones provided by XLNT. The platform provides a seamless onboarding process to include member attributes generated regularly from external data pipelines. It is even more straight forward if this is a static list generated from a one-off job, as one can simply  X  X pload X  it to the platform. These customized attributes are pushed to Voldemort on a daily basis and can be used the same way as any of the built-in ones. Real-time Attributes . These attributes are only available at runtime, such as the browser type or mobile device. XLNT provides an integrated way to target using these attributes, or any parameters passed during a runtim e request. For example, to target only requests coming from iPhones, one just needs to inform the platform that an attribute called  X  X sName X  is to be evaluated at runtime, and target only those with the value equal to  X  X Phone X . This feature is used extensively for mobile experiments, as new mobile features are usuall y only rolled out for particular mobile app versions. This is also beneficial when experimenting on guest users where no information is available prior to the request. Section 3.2.1 includes more discussi ons on this case. The XLNT A/B testing platfo rm is a key component of LinkedIn X  X  Continuous Deployment framework [23]. It spans across every fabric and stack of LinkedIn X  X  engineering infrastructure, providing A/B testing capabilities universally. Once the design is completed, deploying an experiment involves the following two components: 1. Application Layer. This includes any production artifacts, 2. Service Layer. This is a distributed cache and experiment At runtime, a simple experiment that does not involve targeting on pre-defined attributes can be executed locally at the application layer, which takes no time delay at all. Experiments that require member attributes for targeting (see Section 2.1.2) are sent to execute at the service layer. The results are then communicated back to the application client with a total delay of 1msec on average. Because these are high throughput services with about 20k to 30k QPS, we need to estab lish strict SLAs and enforce it with timeouts. These timeout durations can be fully customized according to experiment specific latencies. To support monitoring and analysis , an event is logged during the  X  X etTreatment X  call at the application layer, with information such as the timestamp, testKey, experiment name, ID (experimental unit), variant, etc. These events are stored in Kafka topics [25] and periodically ETL X  X  to our HDFS clus ters to be used in our data workflows. It is important to note that these experiment events are fired only when the  X  X etTreatment X  code is called, and not for every request to LinkedIn.com. This not only reduces the logs footprint, but also enables us to do triggered analysis, where only users who were actually impacted by the experiment are included in the A/B test analysis. For example, LinkedIn.com could have 20 million daily users, but only 2 million of them visited the  X  X obs X  page where the experiment is actually on. Without such trigger information, it is hard to isolate the real impact of the experiment from the noise, especially for experiments with low trigger rates. Even with triggered logging, th e volume of events generated presents a challenge. Currently, an average of 10 billion events are generated daily, and that number is growing quickly as more experiments are run on an increasi ng user base. We have visited the event schema and generation conditions several times in the past to remove derivable attributes and encode values. Continued effort is necessary for incremental improvement and to identifying new solutions addressing this challenge going forward. Automated analytics is crucial in popularizing experimentation. It not only saves teams from time-c onsuming ad hoc analysis, but also ensures that the methodology behind the reports is solid, consistent and scientifically founded. To paint with a broad brush, the analytics pipeline computes user engagement metrics such as pageviews and clicks, and joins them with the experiment assignment information from online logs described in Section 2.2.1. The da ta are then aggregated based on the experiment and time range to produce summary statistics that are sufficient to compute not only the difference between any two variants, but also the statistical significance information such as p-values and confidence intervals. Approximately 4TB of metrics data and 6TB of experiment assignment data are processed every day to produce over 150 million summary records. Much of this computation utilizes the large scale joins-and-aggregations solution provided by the Cubert framework [34, 35]. All these data are stored in Pinot [26], our in-house distributed storage system, to be queried by the UI applications. LinkedIn has many diverse products. Even though there are a handful of company metrics that everyone optimizes towards, every product has several product-s pecific metrics that are most likely impacted by experiments in their  X  X rea X . As LinkedIn X  X  products evolve and new products emerge, it is impossible for the experimentation team to create and maintain all metrics for all products (currently more than 1000 of them). Therefore, to maintain the metrics, we follow a hybrid of centralized and decentralized model. Metrics are categorized into 3 tiers: 1) Company wide 2) Product Specific 3) Feature Specific. A central team maintains tier 1 metrics. Ownership of tier 2 &amp; 3 metrics is decentralized  X  each team owns the logic for these metrics while the central team is responsible for the daily com putation and operations. XLNT computes all tier 1 and tier 2 metr ics for all experiments while tier 3 metrics are only computed on an ad-hoc basis. When a metric is impacted, experimenters frequently want to dig deeper and get more actionable in sights. For this reason, XLNT provides several slicing and dicing capabilities. Experimenters can leverage both user and non-user based dimensions and even apply multiple dimensions at once. As an example, the metric total pageviews can be narrowed down to homepage pageviews on the iOS app for Spanish speaking members across the US, South America and Spain. As one can easily imagine, this is computational extremely heavy especially when over 1000 metrics are involved across multiple days for hundreds of experiments. At a high level there are two use cases we need to address within experiment reporting: 1) Enable a broad understanding of the impact across LinkedIn 2) Enable a deep understanding in the areas most heavily impacted by the experiment. Knowing that this functionality clearly falls in the second use case, we decided to provide multi-dimensional drill-down only for the subset of metrics likely to be impacted by the experiment. Furthermore, from the dozens of dimensions available, teams decide what dimensions are most relevant for their experiments and at what level of combinations. This saves us from unnecessarily crunching data that is not relevant and no one uses. Even with such savings, our pipeline still generates about 150 million records daily on average, where each record includes summary statistics for the tuple of experiment, variant, date range, metric and dimension combination. We have described the components in XLNT that enable the basic workflow for running experiments at LinkedIn. In this section, we discuss, with real examples, how we address a few challenging scenarios. Even though the examples may be LinkedIn specific, most of the lessons and best practi ces we share here are applicable to experimenting on social networks in general. As mentioned in Section 2.1, experiments at LinkedIn are fully overlapping by default . In other words, a member is simultaneously in all applicable experiments. Uni que hashIDs are used for each experiment to en sure the randomizations between experiments are orthogonal. The si mple parallel experimentation structure allows us to scale the number of experiments easily in a de-centralized manner. It is sufficient for most of our A/B testing needs as many of our experiments affect entirely different products and are unlikely to interfere or interact with each other. However, there are cases where interactions are expected. For example, one experiment was tes ting whether or not to include a LinkedIn Pulse module on the hom epage, while simultaneously we had another experiment investigating the number of stories to include in the same module. Clearly, when the module does not exist, the second experiment is ineffective. Another example of potential interaction is between two email experiments. When a member receives two emails from LinkedIn, he or she is likely to open only one of them. Hence two email experiments both improving the subject line are in fact competing with each other. Each of them would have enjoyed a larger gain if the experiments were run on two disjoint user spaces. Google X  X  experimentation system uses layers and domains to divide up the user space to avoid such conflicts or interactions [8]. Microsoft Bing has a similar appr oach but their system includes detection in addition to prevention [9]. Different from these two systems, the XLNT platform takes a de-centralized approach that is closely integrated with the Li nkedIn engineering infrastructure. We describe here how we use XL NT to address the three most common concerns and use cases related to interactions at LinkedIn. Gating Key. Using the LinkedIn Pulse module example, a testKey is created that acts as a gating key to control whether the module is on or off. A second testKey is then used to split the traffic to test different number of stories. The second testKey would only be evaluated if the gating key indicates that the module is  X  X n X . This nested structure ensures that users do not see five stories without the module. The same gating key concept is also used to create disjoint traffic for multiple experiments. For instance, we always have several experiments running simultaneously on the homepage; some are improving the feed relevance while others are modify ing UI elements. When one is concerned about potential interaction between N of them, he or she can create a gating key with N variants and each variant acts as a bucket that sends traffic only to one of the N experiments. Factorial Design. Even though LinkedIn has over 300 million members, there are product areas where the user base is relatively small in comparison to other parts of the site. Some of these products are heavily experimented on, for example, the Subscription acquisition page . Small UI changes on the subscription page can make a bi g difference monetarily. With multiple UI experiments running in parallel, there is valid concern regarding interference. However, splitting up the traffic makes it even harder to have sufficient power to detect changes. It is also not practical to run experime nts sequentially since each experiment runs for at least a month to monitor long-term user impact. On the other hand, if we set up these experiments independently (the default setup) each experiment becomes a factor in a full factorial design. We can then analyze to see if these experiments do interact and if so, what their effects would be without interaction. Of course, if there is no significant interaction, each experiment can be analyzed separately and each gets to enjoy the full amount of traffic available for maximum power. Dependent Experiments. There are cases where people are interested in only certain combinations of different factors, but not all. For instance, we have one experiment testing the maximum number of sponsored updates to include in one X  X  feeds per day, while another experime nt var ying the gap between two consequent sponsored updates. What is the best combination of these two factors? Some combinations, for example, a maximum of 25 sponsored updates with a gap of 3 organic updates in between, should never be launched due to bad user experience. Testing these combinations not onl y wastes valuable traffic, but also hurts the member experience. In most cases, creating a single test that combines the two factors is impractical, but we can align them using the same hashID so that members are randomized identically in both experiments. We can then create the variants for the second experiment such that when combined with the first they generate the right combinations. Figure 5 illustrates the idea. 
Figure 5: Using the same hash ID to align two experiments. The first decision to make when running an experiment is whom to run it on. The experimental unit refers to both the entity randomly assigned to treatment and control (randomization unit) and the unit metrics are calculated and averaged over (analysis unit). We assume these two units are the same in our discussion. The most common case when they do differ (e.g. computing the metric Click-Through-Rate when e xperimenting on users) is well discussed in the literature [8, 13] and hence omitted in this paper. In this section, we use guest e xperimentation as an example to discuss how XLNT supports diffe rent experimental units and some of the challenges we encounter. In addition, we introduce an interesting problem that arises particularly in a social network setting where the same user can play two different roles with each needing to be tested separately. Most of our experiments fo cus on optimizing the member experience. However, we also run experiments on public pages such as the registration page and public profiles. Providing an excellent user experience on these pages is critical because they ultimately drive sign-ups. The XLNT platform makes it easy to run an experiment on guest users (or any other unit the experi menter requires). A browser ID, instead of a member ID, is usually used to identify guest users and is passed to the  X  X etTreatment X  call the same way as a member ID (see Section 2.2). The browser ID is associated with a different URN type called  X  X uest X  which is then recorded as part of the experiment logs (Section 2.2.1). Ta rgeting is also possible. Even though there is no rich profile data associated with a browser ID, properties that are available at run time such as the browser type, or the geographic location computed by reversing the IP address can all be used for targeting (Section 2.1.2). Our offline analysis pipeline uses the URN type to di fferentiate betwee n experimental units and automatically joins metrics with the corresponding experiment assignment information. This enables us to have a single workflow to handle analysis on various experimental units. One challenge is to handle tran sitions between member and guest status of the same user during an experiment. There are mainly two cases when this happens: a guest user becoming a member, or a member transitioning between l ogged-in and logged-out states. The latter is less of a concer n since a member X  X  logged out activities on the site is very limited and has little influence over our experiments. The former, however, is of particular concern for experiments optimizi ng registration and onboarding flow as one unified experience. For example, we have one guest experiment promoting registration using the  X  X ignup with Facebook X  option, in which case certain information can then be automatically filled during the onboarding process to provide a better member experience. Even though the experiment is randomized on the browserID, we need to have a measurement based on both browserID and memberID. This is a common use case as information collected on the guest side helps us tailor the experience on the member side. To achieve the continuity, we can make a small customization on the application client (Section 2.2) to emit two pieces of tracking events, one based on the browserID (the default one) and the other on the memberID. The analysis workflow then automatically picks up both events and generates two reports, one for each ID. Other experimental units have th eir unique challenges as well. Take experimenting on LinkedIn groups as an example. A member can belong to multiple groups, so we can no longer assume independence between un its. This presents a similar challenge as the network A/B tes ting problem to be discussed in Section 3.4. There are different experimental units even when we only consider members on the site. This is the case because every member holds two different roles in a social network as both the one performing an action and the one receiving an action. For example, a member can view others X  profiles or receive profile views. Similarly, they can endorse someone or be endorsed. It is important to know which role we are experimenting on and analyze accordingly. If an experiment is randomizing on endorsers, endorsees are going to be evenly distributed across treatment and control. Therefore, analyzing the number of endorsements received gives no i ndication on how the experiment performs. Similarly, an experiment that encourages members to send more invitations should not be evaluated based on the number of invitations received. Essentially, even though the total number of actions is the same c ounting by either role, how the actions are attributed needs to be aligned with the experimental unit. Moreover, some attributions, such as the number of invitations accepted, are time-lagged, in which case we need to create additional metrics to ca pture the time dimension (e.g. invitation accepted by day X). We need to be even more careful when we have experiments aiming at assessing impacts on bot h sides. As an example, we recently considered allowing members to include a custom background image on their profile page. We wanted to evaluate whether members would view more profiles and receive more profile views. Two experiments (testKeys) were created, one allowing a member to upload an image (viewee experiment) and the other allowing a member to see the image if there is one (viewer experiment). However, these two experiments cannot be fully independent because viewer and viewee are simply opposite perspectives of the same member. Members who are able to upload background images should also be able to see them, especially on their own profiles. To address this issue, we applied the same hashID to both experiments to ensure the same treatment assignments. A more subtle but important point is that the experiment effect depends on the percentage of members receiving the treatment. Clearly, if only 1% of profiles have the background image, a viewer X  X  be havior is less likely to be impacted compared to if all profiles had a background image. Therefore, the bias is smallest when the treatment is ramped close to 100%. On the other hand, running the experiments at 50/50 gives the least variance. It is thus a bias and variance trade-off that the team has to keep in mind when evaluating the experiment results. In an online experiment, users trickle into the A/B test as they visit LinkedIn. The experimenter has no prior information on who will be in the experiment, and users X  engagement is measured through their activities on the site , e.g. pageviews and clicks. Many of the experiments we run at LinkedIn are such online experiments. However, because LinkedIn is a member-based social network, there are also many experiments that are  X  X ffline X , in which case the experiment us ers do not necessarily have an online activity. We walk through three scenarios below. Email Experiments. Emails are usually sent to members independent of whether they visit the online site or not. For example, a member would receive a notification email if another member sends them an invitation to connect. Not everyone who gets an email would come to the site. Therefore, if we only consider active members when analyzing an email experiment we are likely to introduce a bias. For instance, if the treatment email has a more attractive subject line that encourages more members to click and visit the online site, we are likely to see a drop in many key metrics (e.g. average pageviews) if only active members are taken into account because the email tends to drive less engaged members to visit Li nkedIn. In other words, the experiment population cannot be determined only with online user activity. It needs to take into account all users who received the email. To make it even more complex, we also have experiments that attempt to reduce the email footprint by filtering out emails that are less relevant to members. In these cases, it is not sufficient to only look at users who actually receive the email, but to also include users who would have rece ived the email had there been no relevance filtering. Email Campaign. We have recently revamped the Who Viewed My Profile page. The product team wants to measure through an A/B test if the new changes are indeed better, and if so, by how much. The marketing team wants to create a buzz around the new page by starting an email campa ign. This is a very common scenario, but how can the A/B test and the email campaign coexist? Clearly, we can only send campaign emails to the treatment group as there is nothing new for members in control. However, this would contaminate the online A/B test because the campaign encourages more members from the treatment to visit. To make sure the experiment sa mples are unbiased, similar to an email experiment discussed earlier, we need to identify the matching population who would have received the email in the control group as well. For instance , if the emails are sent to treatment members selected thro ugh process X, the same process needs to be applied to the control group to select those who would have received the email if they were in treatment. It is important to note that even though these two populations are unbiased, the difference between the two populations is no longer just the effect of the newer page, but a com pounded effect of both the email campaign and the new page. It ma y be a subtle point, but this combination is in fact a better representation of the final impact we would like to measure from launching the new page. It is, however, crucial to realize that email campaigns have a strong novelty effect that dies out over time, so we need to keep the experiment running for a longer time period till the effect stabilizes. There are also occasions where one wants to measure the impact of these two factors separately, in which case we would either create a separate treatment bucket that receives no email campaign, or measure the A/ B test first before setting off the campaign. Cohort Experiments . It is common practice at LinkedIn to run A/B tests on a cohort of members w ho are pre-selected offline. As an example, we tested a new feature where members get a push notification on their LinkedIn profile page if they have not visited the homepage for a while. The first thing the experimenter did was to gather a list of members who have not visited the homepage for more than 7 days and used it to create a customized targeting attribute (Section 2.1.2). These targeted members are then split between treatment and control, and the notification was presented to only members in the treatment cohort. Such cohort experimentation is a great way to leverage the capability of the XLNT platform to test out ideas without having to build a full-fledged product. Another advantage of cohort experiments is to be able to measure long-term member impact without the dilution of members newly added to experi ments. Howeve r, one common pitfall is that people try to update the cohort selection during the experiment, while the selection criteria is directly related to the experiment outcome. In the pus h notification example, the treatment encourages more members to visit LinkedIn homepage, though once a member visits the homepage, he or she would disappear from the updated cohort list. Therefore the updated treatment bucket would end up with fewer members which creates a bias. Our suggestion is to use a static cohort or to include the previous cohort in the analysis if an update is necessary. In our discussion thus far, we have assumed the Rubin causal model [1], a standard machiner y of testing framework, when conducting and analyzing A/B test s. A key assumption made in Rubin causal model is the  X  X table Unit Treatment Value Assumption X  (SUTVA), which states that the behavior of each sample in the experiment depends only on their own treatment and not on the treatments of others. This is a plausible assumption in most practical applications. For example, a user who is served better search results is more likely to click, and that behavior is en tirely independent of others using the same search engine. Howe ver, such assumption does not always hold in experiments run on social networks. In a social network setting, a user X  X  behavior is likely impacted by that of their social neighborhood [2, 3, 4]. In most cases, a user would find a new feature more valuable and hence more likely to adopt it as more of their neighbors adopt it . For example, video chat is a useless feature unless one X  X  fri ends use it too. In an A/B experiment, this implies that if the treatment has a significant impact on a user, the effect would spill over to his/her social circles, regardless whether his/her neighbors are in treatment or control. This poses a special challenge for running A/B tests in many online social and professional networks like Facebook, Twitter and LinkedIn. Many features tested there are likely to have network effects. For example, a better recommendation algorithm in treatment for the People Y ou May Know module on LinkedIn encourages a user to send more invitations. However, users who receive such invitations can be in the control variant and when they visit LinkedIn to accept the invitation they may discover more people they know. If the primary metric of interest were the total number of invitations sent, we would see a positive gain in both the treatment and the contro l groups. The treatment effect estimated ignoring netw ork effect would be biased and would not fully capture the benefit of the ne w algorithm. Such bias exists in testing almost any features that involve social interactions, which is truly ubiquitous in a so cial network environment. To address this challenge, we need to take users X  network connections into consideration wh en sampling them into treatment and control [19]. Essentially, we take the following two-stage procedure: 1. Partition the users into clusters. 2. Treat each cluster as a unit fo r randomization so all users in The estimation for the treatment effect can then be based on either the sample means, or more sophisticated estimators summarized in [19]. We have noticed strong network effects from the network A/B tests we have run at Linke dIn based on this sampling and estimation framework. Running large scale A/B tests is not just a matter of infrastructure and best practices, establishing a strong experimentation culture is also key to embedding A/B testing as part of the decision making process. Apart from building the basic functionalities any A/B testing platform requires, we have identified four necessary ingredients that have helped us instill experimentation deeply into our culture at LinkedIn. We share th em in the rest of the section. XLNT produces over 1000 metrics for A/B test reports. Similar metrics are often used in busin ess reporting for decision making as well. These two sets of metrics used to be computed separately and much effort was spent on investigating differences when they happened. We realized the importance of using the same definitions of metrics across reporting and experimentation, and hence have worked with various teams at LinkedIn to evolve our metrics pipeline to be the unified metrics pipeline leveraged by the entire company. By making our experiment results and business reports  X  X omparable X , we have made it possible for R&amp;D teams to relate changes in business numbers with experiment launches. Moreover, the integration also provides the foundation that enables other organizations such as Finance to bake A/B test results into business forecasting. In addition to providing product development with a directional signal, A/B testing is often utilized to measure impact and assess ROI (Return-On-Investment). A report based on triggered analysis (Section 2.2.1) is great at provid ing a directional signal, however, it does not accurately represent the global lift that will occur when the winning treatment is ramped to 100% (holding everything else constant). The reason is two-fold: 1) Most experiments only target a subset of the entire user population. 2) Most experiments only trigger for a subset of their targeted population. In other words, triggered analysis only provides evaluation of the local impact, not the global impact of an experiment. To address this concern, we compute the Site Wide Impact, defined as the percentage delta between two parallel universes: one with treatment applied to only targeted users and control to the rest, the other with control applied to all. With site wide impact provided for all experiments, teams are able to compare results across experiments rega rdless of their targeting and triggering conditions. Moreover, Site Wide Impact from multiple segments of the same experiment can be added up to give an assessment of the total impact, which is particularly helpful when each segment is running at different percentages and has to be analyzed separately to a void Simpson X  X  Paradox [31]. However, computing Site Wide Impact explicitly for every metric and report we generate would be more than doubling our current computation effort. It turns out that Site Wide Impact depends only on (1) the summary statistics that are already computed as part of the triggered report, and (2) the global total with a matching analysis date range as the experiment report (see Appendix A). For most of our metrics that are additive across days, we can simply keep a daily counter of the global total and add them up for any arbitrary date range. However, there are metrics, such as the number of unique visitors, which are not additive across days. Instead of computing the global total for all estimate them based on the daily totals (see Appendix A for more details), saving more than 99% of the computation cost without sacrificing much accuracy. One interesting phenomenon we obser ve is that the local impact (percentage delta from triggered analysis) and the Site Wide Impact can sometimes disagree directionally for ratio metrics such as CTR. For example, in a recent experiment testing  X  X onnection request X  emails targeting Englis h speaking members with fewer than 50 connections, we observed a -9.4% local impact on CTR while a +0.5% Site Wide Impact. Mathematically, this is an instance of Simpon X  X  Paradox as the overall site wide CTR is essentially a linear combination of the within-segment and outside-segment CTR. The real ch allenge, however, is to decide which number to use for decision making. We believe that even though Site Wide Impact has a st ronger business im plication, the local impact is a better indication of user satisfaction in this case. Multiple testing is a common problem in the field of A/B testing [30] a nd LinkedIn is no exception. With over 1000 metrics computed for each experiment, this issue is so prevalent that the most common question we hear fro m experimenters is  X  X hy is this irrelevant metric significant? X  Even though we have tried to educate people on the topic of multiple testing, many are still clueless as what they should do when a metric is unexpectedly significant. What X  X  worse, some people even lost trust in our A/B reports because they couldn X  X  tell whether a metric is moved for real or simply due to multiple testing. To this end, we have come up with a two-step rule-of-thumb for experimenters to follow: 1. Divide all metrics into three groups: 1) First order metrics are 2. Apply tiered significance le vels to each group. We Our rule-of-thumb is in fact based on an interesting Bayesian interpretation. It boils down to how much we believe the null hypothesis (H 0 ) is true before we even run the experiment. Believing that the posterior probability on H 0 should be consistent across all metrics, given different priors on H 0 , we essentially vary the probability of observing the data given H 0 , which is the definition of p-value in the frequentist interpretation. Since the significance level is the cutoff used for p-values, this leads to adjusting the thresholds accordingly. In particular, the three significance levels 0.05, 0.01 and 0.001 reflect that we believe an experiment having a prior probab ility of 1/2, 1/6 and 1/51 impacting each of the three metric groups. This interpretation establishes a simple mathematical equivalence between people X  X  prior belief on whether a metric is impacted and the significance level. Thus for those who are a bit more sophisticated they can easily come up with their own significance level tiers to match their prior beliefs. LinkedIn is a deeply interconnected site. When it comes to experimentation, each team in fact plays two different roles. On the one hand, they run experiment s to improve their metrics; on the other hand, they are also responsible for the global health of the same metrics, even when they may be impacted by experiments from other teams. Recognizing th e importance of the second responsibility, we launched a feature called  X  X ost Impactful Experiments X  (MIE) to allow metric owners to follow experiments that impact the metrics they care about. MIE is a major milestone for e xperimentation at LinkedIn. Not only does it drive greater transparency regarding experiment launch decisions, it also encour ages more discussions to occur which in turn increases the overall knowledge of experimentation in the company. To ensure MIE only listed the most relevant experiments, we use the following three-step algorithm: 1. Filter out all the experiments that have potential quality 2. For each metric, control False Discovery Rate using the 3. Score the experiments from step 2 based on three factors: There are a couple of interestin g observations and lessons learnt while developing this capability:  X  Large Initial Impact . We observed that it is common for  X  Personalizing Threshold . Picking the optimal threshold In this paper, we shared the details of building a powerful and flexible experimentation platform that enables teams across LinkedIn to make informed deci sions faster and at scale. Furthermore, we discussed se veral challenging A/B testing scenarios and shared many real examples applicable to social networks. Realizing the importan ce of a strong experimentation culture, we discussed four ingredients that helps us instill experimentation deeply into ev ery decision making process at LinkedIn, even across roles outside of R&amp;D. One challenge we have not disc ussed here is how to provide guided insights to the experiment owners. Traditionally, controlled experiments have been used to answer  X  X hat X  has been impacted, but not to answer  X  X hy  X . By utilizing the amount of information we have available for each experiment, in terms of both the metrics and dimensions, we hope to automatically generate insights that can bett er guide product development. There are also cases where it is not possible to run an A/B test due to practical reasons, so we have to leverage quasi-experimental designs where users in treatment and control groups are matched post hoc using techniques such as propensity score matching. Lastly, we hope that the topics we covered will lead to further research and development of A/B testing in large scale social networks. The authors wish to thank June Andrews, Drew Moxon, Karan Ahuja, Alexis Baird, Caroline Gaffney, Udi Milo and Xin Fu for insightful discussions on several of the examples. We have been fortunate to be part of the t eam developing the experimentation platform and we wish to thank all members of the XLNT team, including Adam Smyczek who made significant contributions before leaving LinkedIn. Finally, we wish to thank many people for being strong advocates of experimentation at LinkedIn, especially Igor Perisic. [1] Rubin, Donald B. Estimating causal effects of treatments in [2] Ugander, Johan, Karrer, Brian, Backstrom, Lars and [3] Katzir, Liran, Liberty, Edo and Somekh Oren.
 [4] Toulis, Panos and Kao, Edward. Estimation of causal peer [5] Eckles, Dean, Karrer, Br ian and Ugander, Johan. Design [6] Aronow, Peter M, and Samii, Cyrus. Estimating average [7] Kohavi, Ron, et al. Trustworthy online controlled [8] Tang, Diane, et al. Overlapping Experiment Infrastructure: [9] Kohavi, Ron, et al. Online Controlled Experiments at Large [10] Kohavi, Ron, et al. Seven Rules of Thumb for Web Site [11] Yates, Frank, Sir Ronald Fisher and the Design of [12] Bakshy, Eytan, Echles, Dean and Bernstein, Michael S.
 [13] Kohavi, Ron. et al. Controlled experiments on the web: [14] Crook, Thomas, et al. Seven Pitfalls to Avoid when [15] Ioannidis, John PA. "Why most published research findings [16] Wacholder, Sholom, et al. "Assessing the probability that a [17] Benjamini, Yoav, and Yosef Hochberg. "Controlling the [18] Saaty, Thomas L. "How to make a decision: the analytic [19] Gui, Huan, Xu, Ya, Bhas in, Anmol, Han Jiawei. Network [20] Box, George EP, J. Stuart Hunter, and William G. [21] Gerber, A. S., and Green, D. P. Field Experiments: Design, [22] Sumbaly, Roshan, et al. "Serving large-scale batch [23] Tate, Ryan. The Software Revolution Behind LinkedIn X  X  [24] Auradkar, Aditya, et al. "Data infrastructure at [25] Kreps, Jay, Neha Narkhede, and Jun Rao. "Kafka: A [26] Naga, Praveen Neppalli, Real-time Analytics at Massive [27] Fisher, Ronald A. Presidential Address. Sankhya: The [28] Montgomery, Douglas C. Design and analysis of [29] Betz, Joe, Tagle, Moira. Rest.li:RESTful Service [30] Romano, Joseph P. Azeem M. Shaikh and Michael Wolf. [31] Wikipedia. Simpson X  X  Paradox. [Online] [32] McFarland, Colin. Experiment!: Website conversion rate [33] Eisenberg, Bryan. How to Improve A/B Testing. ClickZ [34] Vemuri, Srinivas, Varshney, Maneesh, Puttaswamy, [35] Varshney, Maneesh, Vemuri, Srinivas. Open Sourcing We use the average number of clicks as an example metric to show how we compute Site Wide Impact. Let Let  X   X ,  X   X ,  X  X  X  X   X   X  X  X   X  X  X  X  X  X  X  X  X  denote the total number of clicks in the treatment group, the control group, the whole segment (including the treatment, the control and potentially other variants) and  X   X ,  X   X ,  X  X  X  X   X   X  X  X   X  X  X  X  X  X  X  X  X  denote the sample sizes for each of the four groups mentioned above. It is easy to see that the total number of clicks in the treatment (control) universe can be estimated as Then the Site Wide Impact is computed as which indicates that the Site Wide Impact is essentially the local impact  X  scaled by a factor of  X  , and  X   X  X  X  X  X  X  X  X  X  is the only ingredient that is not already computed as part of summary statistics for the triggered analysis. For metrics such as average number of clicks,  X   X  X  X  X  X  X  X  X  X  arbitrary date range can be computed by summing over clicks from corresponding single days. Howe ver, for metrics such as average number of unique visitors , de-duplication is necessary across days. To avoid having to compute  X  for all date ranges we generate reports for, we estimate cross-day  X  by averaging the single-day  X   X  X . There is one other group of metrics that are ratio of two metrics. One example is Click-Through-Ra te, which equals Clicks over Impressions. The derivation of Site Wide Impact for ratio metrics is similar, with the sample size replaced by the denominator metric.
