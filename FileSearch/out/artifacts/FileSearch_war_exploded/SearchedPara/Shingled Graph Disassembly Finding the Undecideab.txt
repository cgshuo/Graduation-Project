
Richard Wartell 1 , Yan Zhou 2 , Kevin W. Hamlen 2 , and Murat Kantarcioglu 2 Statistical data mining techniques have found wide application in domains where statistical information is valuable for solving problems. Examples include com-puter vision, web search, natural langu age processing, and more. A recent addition to this list is static disassembly [1,2]. Disassembly is the process of translating byte sequences to human-readable assembly co de. Such translation is often deemed a crucial first step in software rev erse engineering and analysis.

Although all binary-level debuggers perform dynamic disassembly to display assembly code for individual runs of target programs, the much more challenging task of static disassembly attempts to provide assembly code for all possible runs (i.e., all reachable instructions). Static disassembly is therefore critical for ana-lyzing code with non-trivial control-flows, such as branches and loops. Example applications include binary code optimiza tion, reverse engineering legacy code, semantics-based security analysis, malware analysis, intrusion detection, and dig-ital forensics. Incorrectly disassembled binaries often lead to incorrect analyses, and therefore bugs or security vulnerabilities in mission-critical systems.
Static disassembly of binaries that target Intel-based architectures is par-ticularly challenging because of the architecture X  X  heavy use of variable-length, unaligned instruction encodings, dynamically computed control-flows, and inter-leaved code and data. Unalignment refers to the fact that Intel chipsets consider all memory addresses to be legal instruction starting points. When some pro-grams compute the destinations of jumps dynamically using runtime pointer arithmetic, statically deciding which bytes are part of reachable instructions and which are (non-executed) static data reduces from the halting problem. As a result, the static disassembly problem for Intel architectures is provably Turing-undecidable in general.

Production-level disassemblers and rev erse engineering tools have therefore applied a long history of evolving heuristics to generate best-guess disassem-blies. Such heuristics include fall-through disassembly, various control-flow and dataflow analyses, and compiler-specific pattern matching. Unfortunately, even after decades of tuning, these heuristics often fail even for non-obfuscated, non-malicious, compiler-generated software. As a result, human analysts are often forced to laboriously guide the disassembly process by hand using an interactive disassembler [3]. When binaries are tens or hundreds of megabytes in size, the task quickly becomes intractable.

Wartell et al. recently proposed to appl y machine learning and data mining to address this problem [1]. Their approach uses statistical data compression techniques to reveal the semantics of a binary in its assembly form, yielding a segmentation of code bytes into assembly instructions and a differentiation of data bytes from code bytes. Although the technique is effective and exhibits improved accuracy over the best commercial disassembler currently available [4], the compression algorithm suffers high memory usage. Thus, training on large corpora can be very slow compared to other disassemblers.

In this paper, we present an improved disassembly technique that is both more effective and more efficient. Rather than relying on high-order context seman-tic information (which leads to long training times), we leverage a finite state machine with transitional probabilities to infer likely execution paths through a sea of bytes. Our main contributions include a graph-based static disassembly technique; a simple, efficient, but effective disassembler implementation; and an empirical demonstra tion of the effectiveness of the approach.

Our high-level strategy involves two lin ear passes: a preprocessing step which recovers a conservative superset of potential disassemblies, followed by a filtering step in which a state machine selects the best disassembly from the possible candidates. While the resulting disassembly is not guaranteed to be fully correct (due to the undecidability of the general problem), it is guaranteed to avoid certain common errors that plague mainstr eam disassemblers. Our empirical analysis shows our simple, linear approach is faster and more accurate than the observably quadratic-time approaches adopted by other disassemblers.
The rest of the paper proceeds as follows . Section 2 discusses related work in static disassembly. Section 3 presents ou r graph-based static disassembly tech-nique. Section 4 presents experimental results, and Section 5 concludes and sug-gests future work. Existing disassemblers mainly fall into three categories: linear sweep disassem-blers, recursive traversal disassemblers, and the hybrid approach. The GNU util-ity objdump [5] is a popular example of the linear sweep approach. It starts at the beginning of the text segment of the binary to be disassembled, decoding one instruction at a time until everything in executable sections is decoded. This type of disassembler is prone to errors wh en code and data bytes are interleaved within some segments. Such interleaving is typical of almost all production-level Windows binaries generated by non-GNU compilers.

IDA Pro [3, 4] follows the recursive traversal approach. Unlike linear sweep disassemblers, it decodes instructions by traversing the static control flow of the program, thereby skipping data bytes that may punctuate the code bytes. However, not all control flows can be predicted statically. When the control flow is constructed incorrectly, some reachable code bytes are missed, resulting in disassemblies that omit significant blocks of code.

The hybrid approach [6] combines linea r sweep and recursive traversal to detect and locate disassembly errors. The basic idea is to disassemble using the linear sweep algorithm and verify the output using the recursive traversal algorithm. While this helps to eliminate some disassembly errors, in general it remains prone to the shortcomings of both techniques. That is, when the sweep and traversal phases disagree, there is no clear indication of which is correct; the ambiguous bytes therefore receive an error-prone classification.

Wartell et al. recently presented a mac hine learning-and data mining-based approach to the disassembly problem [1]. Their approach avoids error-prone control-flow analysis heuristics in favor of a three-phase approach: First, executa-bles are segmented into subsequences of by tes that constitute valid instruction encodings as defined by the architecture [7]. Next, a language model is built from the training corpus with a statistical data model used in modern data compres-sion. The language model is used to classify the segmented subsequence as code or data. Finally, a set of pre-defined heuristics refines the classification results. The experimental results demonstrate substantial improvements over IDA Pro X  X  traversal-based approach. However, it has the disadvantage of high memory us-age due to the large statistical compression model. This significantly slows the disassembly process relative to simple sweep and traversal disassemblers.
Our disassembly algorithm presented in this paper instead adopts a prob-abilistic finite state machine (FSM) [8, 9] approach. FSMs are widely used in areas such as computational linguistics, speech processing, and gene sequencing. Although the transitions of probabilistic FSMs are non-deterministic, they are labeled with probabilities given training data. For any given byte stream, there is more than one trace through the FSM. By querying the FSM, the likelihood of each trace can be computed, revealing the most probable path of reachable opcode and operand sequences in an executable. Our machine learning approach to disassembly frames the disassembly problem as follows: Problem Definition. Givenanarbitrarystringofbytes,whichsubsetofthebytes is the most probable set of potentially reachable instruction starting points, where  X  X robable X  is defined in terms of a given corpus of correct binary disassemblies?
Figure 1 shows the architecture of our disassembly technique. It consists of a shingled disassembler that recovers the (overlapping) building blocks ( shingles ) of all possible valid execution paths, a finite state machine trained on binary executables, and a graph disassembler that traces and prunes the shingles to output the maximum-likelihood classification of bytes as instruction starting points, instruction non-starting points, and data. source binary 3.1 Shingled Disassembler Since computed branch instructions in x86 have their targets established at runtime, every byte within the code section can be a target and thus must be considered as executable code. This aspect of the x86 architecture allows for instruction aliasing , the ability for two instructions to overlap each other. Therefore, we refer to a disassembler tha t retains all possible execution paths through a binary as a shingled disassembler.
 Definition 1 Shingle A shingle is a consecutive sequence of bytes that decodes to a single machine instruction. Shingles may overlap.

The core functionality of the shingled disassembler is to eliminate bytes that are clearly data (because all flows that contain them lead to execution of bytes that do not encode any valid instruction), and to compose a byte sequence that retains information for generating every possible valid shingle of the source binary. This is a major benefit of this approach since the shingled disassembly encodes a superset of all the possible valid disassemblies of the binary. In later sections, we discuss how we apply our graph disassembler to prune this superset until we find the most probable byte classifications. In order to define what consists of a valid execution path, we m ust first discuss a few key concepts. Definition 2 Fal l through 1 Shingle x (conditionally) falls through to shingle y ,denoted x X y ,ifshingle y is located adjacent to and after instruction x , and the semantics of instruction x do not (always) modify the program counter. In this case, execution of instruction x is (sometimes) followed by execution of instruction y at runtime.
 Definition 3 Unconditional Branch A shingle is an unconditional branch if it only falls through when its operand ex-plicitly targets the immediately following byte. Unconditional branch instructions for x86 include jmp and ret instructions.

Unconditional branch instructions are important in defining valid disassem-blies because the last instruction in any disassembly must be an unconditional branch. If this is not the case, the program could execute past the end of its virtual address space.
 Definition 4 Static Successor A control-flow edge ( x,y ) is static if x X y holds or if x is a conditional or un-conditional branch with fixed (i.e., non-computed) destination y . An instruction X  X  static successors are defined by S ( x )= { y | ( x,y ) is static } . Definition 5 Postdominating Set The (static) postdominating set P ( x ) of shingle x is the transitive closure of S on { x } . If there exists a static control-flow from x to an illegal address (e.g., an address outside the address space or whose bytes do not encode a legal instruc-tion), then P ( x ) is not well defined and we write P ( x )=  X  .
 Definition 6 Valid Execution Path All paths in P ( x ) are considered valid execution paths from x .
The x86 instruction set does not make u se of every possible opcode sequence; therefore certain bytes cannot be the beginning of a code instruction. For ex-ample, the 0xFF byte is used to distinguish the beginning of one of 7 different instructions, using the byte that follows to distinguish which instruction is in-tended. However, 0xFFFF is an invalid opcode that is unused in the instruction set. This sequence of bytes is common because any negative offset in two X  X  com-plement that branches less than 0xFFFF bytes away starts with 0xFFFF .The shingled disassembler can immediately mark any shingle whose opcode is not supported under the x86 instruction set as data . A shingle that is marked as data is either used as the operand of another instruction, or it is part of a data block within the code section. Execution of the instruction would cause the pro-gram to crash.
 Lemma 1. Invalid Fall-through  X  x,y :: x X y  X  y :=  X  X  X  x :=  X  ,inwhich  X  stands for data bytes.

Any time that we encounter an address that is marked data, all fall-throughs to that instruction can be marked as data as well. Direct branches also fall into this definition. All direct call and jmp instructions imply a direct executional relationship between the instruction and its target. Therefore, any shingle that targets a shingle previously marked as data is also marked as data.
 Definition 7 Sheering Ashingle x is sheered from the shingled disassembly when  X  y :: x X y , x and all y are marked as data in the shingled disassembly.

Figure 2 illustrates how our shingled disassembler works. Given a binary of byte sequence 6A 01 51 56 8B C7 E8 B6 E6 FF FF ... , the shingled disassem-bler performs a single-pass, ordered sca n over the byte sequence. Data bytes and invalid shingles are marked along the way. Figure 2(a) demonstrates the first se-ries of valid shingles, beginning at the first byte of the binary. Figure 2(b) starts at the second byte, which falls through to a previously disassembled shingle. The shingle with byte C7 is then marked as data (shaded in Figure 2(c)) since it is an invalid opcode. Figure 2(d) shows an invalid shingle since it falls through to an invalid opcode FF FF . Our shingled disassembler marks the two shingles B6 and FF as invalid in the sequence. Figure 2(e) shows another valid shingle that begins at the ninth byte of the binary. After completing the scan, our shingled disassembler has stored information necessary to produce all valid paths in P ( x ).
The secondary function of the shingled disassembler is to collect local statistics called code/data modifiers that are speci fic to the executable. These modifiers keep track of the likelihood that a shingle is code or data in this particular executable. The following heuris tics are used to update modifiers: 1. If the shingle at address a is a long direct branch instruction with a as 2. If three shingles sequentially fall-through to each other and match one of the 3. If bytes at address a and a +4 both encode addresses that reference shingles The pseudocode for generating a shingled disassembly for a binary is shown in Figure 3. For simplicity, the heuristics used to update modifiers are not described in the pseudocode. Lines 1 X 17 construct a static control-flow graph G in which all edges are reversed. A distinguished node bad is introduced with outgoing edges to all shingles that do not encode any valid instruction, or that branch to static, non-executable addresses. Lines 18 X 20 then mark all addresses reachable from bad as data. The rest are possible instruction starting points. 3.2 Opcode State Machine The state machine is constructed from a large corpus of pre-tagged binaries, disassembled with IDA Pro v6.3. The byte s equences of the training executables are used to build an opcode graph, consisting of opcode states and transitions from one state to another. For each opcode state, we label its transition with the probability of seeing the next opcode in the training instruction streams. The opcode graph is a probabilistic finite state machine (FSM) that encodes all the correct disassemblies of the training byte sequences annotated with transition probabilities. The accepting state of the FSM is the last unconditional branch seen in the binary.

Figure 4 shows what this transition graph might look like if the x86 instruction set only contained four opcodes: 0x01 through 0x04. Each directed edge in the graph between opcode x i and x j implies that a transition between x i and x j has been observed in the corpus, and the edge weight of x i  X  x j is the probability that given x i , the next instruction is x j . It is also important to note the node db in the graph which represents data bytes. Any transition from an instruction to data observed in the corpus will be re presented by a directed edge to the db node. The graph for the full x86 instruction set includes more than 500 nodes, as each observed opcode must be included. 3.3 Maximum-Likelihood Execution Path We name the output of the shingled disassembler a shingled binary .Theshingled binary of the source executable encodes within it up to 2 n possible valid disas-semblies. Our graph disassembler is designed to scan the shingled binary and prune shingles with lower probabilities. By using our graph disassembler, we can find the maximum-likelihood set of byte classifications by tracing the shingled binary through the opcode finite state m achine. At every receiving state, we check which preceding path (predecessor) has the highest transition probability. For example in Figure 2, the 5th byte ( 8B ) is the receiving state of two preceding addresses: byte 1 (see Figure 2(a)) and byte 2 (see Figure 2(b)). We compute the transition probability from each of the two addresses and sheer the one with a lower probability.
 Theorem 1. The graph disassembler always returns the maximum-likelihood byte classifications among the set S of all valid shingles.
 Proof. Each byte in the shingled binary is a po tential receiving state of multiple predecessors. At each receiving state, we keep the best predecessor with the highest transition probability. Therefore, when we reach the last receiving state X  the accepting state, which represents the last unconditional brach instruction X  we find the shingle with the highest probability as the best execution path.
The transition probability of a predecessor consists of two parts: the global transition probability taken from the opcode state machine and the local modi-fiers, and local statistics of each byte being code or data based on several heuris-tics. This is important because runtime reference patterns specific to the binary being disassembled are included in distinguishing the most probable disassembly path.

Let r be a receiving state of a tr ansition triggered at x i in the shingled bi-and let cm and dm be the code and data modifiers computed during shingled disassembly. The transition probability to r is as follows: if x i is a fall-through instruction, or if x i is a branch instruction, where Pr ( db i ) is the probability that x i is followed by data and Pr ( db r ) is the probability that r is proceeded by data. Every branch instruction can possibly be followed by data. To account for this, when determin-ing the best predecessor for each instruct ion, branch instructions are treated as fall-throughs to their following instruction and to data. Each branch instruction can be a predecessor to the following instruction or to any instruction that is on a 4-byte boundary and is reachable via data bytes.
Therefore, the transition probability of any valid shingle-path s resulting in a trace of r 0 ,...,r i ,...,r k is: and the optimal execution path s  X  is: 3.4 Algorithm Analysis Our disassembly algorithm is much quicker than other approaches of comparable accuracy due to the small amount of info rmation that needs to be analyzed. The time complexity of each of th e three steps is as follows:  X  Shingled disassembly: Lines 1 X 17 of Figure 3 complete in O ( n ) time (where  X  Sheering: Pruning invalid shingles also requires O ( n )time.  X  Graph disassembly: The graph-based disassembler performs a single-pass Therefore, our disassembly algorithm runs in time O ( n ), that is, linear in the size of the source binary executable. A prototype of our shingled disassembler was developed in Windows using Mi-crosoft .NET C#. Testing of our disassembly algorithm was performed on an Intel Xeon processor with six 2.4GHz cores and 24GB of physical RAM. We tested 24 difficult binaries with very positive results. 4.1 Broad Results Table 1 shows the different programs on which we tested our disassembler, as well as file sizes and code section sizes. It also d isplays the number of instructions that the graph disassembler identified that IDA Pro didn X  X  identify as code. Figure 5 shows the percentage of instructions that IDA Pro identified as code that our disassembler also identified as code.

Our disassembler runs in linear time in the size of the input binary. Figure 6 shows how many times longer IDA Pro took to disassemble each binary relative to our disassembler. Our disassembler is increasingly faster than IDA Pro as the size of the input grows.

Finally, for each binary we used Ollydbg to create and save the traces of ex-ecutions. Tracing executions in this way does not reveal the ground truth of non-executed bytes (which may be data or code), but the bytes that do execute are definitely code. We compared these results to the static disassembly yielded by our disassembler, by IDA Pro, and by the dynamic disassembly tool VDB/-Vivisect [10]. Both our disassembler and IDA Pro were 100% accurate against the execution paths that actually executed during the tests, but VDB/Vivisect exhibited much lower accuracies of around 15 X 35%. We also used VDB/Vivi-sect to dynamically trace command line tools, such as the Spec2000 benchmark suite and Cygwin, and obtained similar code coverages. This provides signifi-cant evidence that purely dynamic disassembly is not a viable solution to many disassembly problems where high code coverage is essential. We presented an extremely simple yet highly effective static disassembly tech-nique using probabilistic finite state machines. It finds the most probable set of byte classifications from all possible v alid disassemblies. Compared to the cur-rent state-of-the-art IDA Pro, our disassembler runs in time linear in the size of the input binary. We achieve greater efficiency, and experiments indicate that our resulting disassemblies are more accurate than those yielded by IDA Pro.
We are currently working on extending our disassembler to instrument and record the actual execution traces of ex ecutables, for better estimation of ground truth and therefore more com prehensive evaluation of accuracy. One major chal-lenge is to get high code coverage X  X he percentage of the code sections covered during each execution X  X specially for large applications. The instrumented ex-ecution traces would give us the advantage to verify all identified code sections in a controlled and automatic fashion.

