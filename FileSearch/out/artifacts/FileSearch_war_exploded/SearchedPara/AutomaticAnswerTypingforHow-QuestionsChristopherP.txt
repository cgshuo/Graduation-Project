 Question answering (QA) systems are emer ging as a viable means of obtaining specific information in the face of lar ge availability . Answer typing is an important part of QA because it allo ws the system to greatly reduce the number of potential answers, using general kno wledge of the answer form for a specific question. For example, for what , wher e , and who questions lik e  X  X hat is the capital of Canada? X , answer typing can filter the phrases which might be proposed as candidate answers, perhaps only identi-fying those textual entities kno wn to be cities.
We focus on answer typing for how -questions, a subset of questions which have recei ved little spe-cific attention in the QA community . Rather than seeking an open-ended noun or verb phrase, how-questions often seek a numerical measurement ex-pressed in terms of a certain kind of unit, as in the follo wing example: Example 1:  X  X o w hea vy is a grizzly bear? X  An answer typing system might expect answers to include units lik e kilo grams , pounds , or tons . Enti-ties with inappropriate units, such as feet , meter s , or hone y pots , would be excluded as candidate answers.
We specifically handle the subset of how-questions that we call how-adjective questions; that is, questions of the form  X  X o w adjective ...? X  such as Example 1. In particular , we do not address  X  X o w man y X  questions, which usually specify the units di-rectly follo wing many , nor  X  X o w much X  questions, which generally seek a monetary value.

Hand-crafting a comprehensi ve list of units ap-propriate to man y dif ferent adjecti ves is time-consuming and lik ely to miss important units. For example, an annotator might miss gigabytes for a measure of  X  X o w lar ge.  X  Instead of compiling a list manually , we propose a means of automatically gen-erating lists of appropriate units for a number of real-world questions.

Ho w-adjecti ve questions represent a significant portion of queries sent to search engines; of the 35 million queries in the AOL search query data set (Pass et al., 2006), over 11,000 are of the form  X  X o w adjective ... X   X  close to one in every three thou-sand queries. Of those 11,000 queries, 152 dif ferent adjecti ves are used, ranging from the expected  X  X o w old X  and  X  X o w far X  to the obscure  X  X o w orwellian.  X 
This high proportion of queries is especially strik-ing given that search engines pro vide little sup-port for answering how-adjecti ve questions. Indeed, most IR systems work by keyw ord matching. En-tering Example 1 into a search engine returns doc-uments discussing the grizzly X  s  X  X ea vy fur , X   X  X ea vy, shaggy coat X  and  X  X ea vy stout body . X  When faced with such results, a smart search engine user kno ws to inject answer units into their query to refine their search, perhaps querying  X  X rizzly pounds.  X  The y may also con vert their adjecti ve ( heavy ) to a related concept ( weight ), for the query  X  X rizzly weight.  X 
Similarly , our approach disco vers unit types by first con verting the adjecti ve to a related concept, us-ing information in a structured ontology . For exam-ple,  X  X ig X  can be used to obtain  X  X ize,  X  and  X  X all X  can deri ve  X  X eight.  X  We then use an online search engine to automatically find units appropriate to the con-cept, given the assumption that the concept is explic-itly measured in terms of specific units, e.g., height can be measured in feet, weight can be measured in pounds, and size can be measured in gigabytes.
By automatically extracting units, we do not re-quire a set of prior questions with associated an-swers. Instead, we use actual questions as a source of realistic adjecti ves only . This is important be-cause while lar ge sets of existing questions can be obtained (Li and Roth, 2002), there are man y fewer questions with available answers.

Our experiments demonstrate that how-question-specific unit lists consistently achie ve higher answer identification performance than fix ed-type, general-purpose answer typing (which propose all numeri-cal entities as answer candidates). Furthermore, our precomputed, automatically-genera ted unit lists are sho wn to consistently achie ve better performance than baseline systems which deri ve unit lists at run-time from documents rele vant to the answer query , even when such documents are gathered using per -fect kno wledge of the answer distrib ution.
The outline of the paper is as follo ws. In Section 2 we outline related work. In Section 3 we pro vide the frame work of our answer -typing model. Section 4 describes the implementation details of the model. Section 5 describes our experimental methodology , while Section 6 sho ws the benefits of using auto-matic how-question answer -typing. We conclude with possible directions of future research opened by this novel problem formulation. Answer typing is an important component of any QA system, but varies greatly in the approach tak en (Prager et al., 2003; Harabagiu et al., 2005). Basically , answer typing pro vides a means of filter -ing answer candidates as either appropriate or in-appropriate to the question. For example, Li and Roth (2002) assign one of fifty possible types to a question based on features present in the question. Answer candidates can then be selected from text by finding entities whose type matches that of the input question. Similarly , Ittycheriah et al. (2000) assign one of the MUC named-entity types to each input question. In these fix ed-cate gory approaches, how-questions are assigned a fix ed type in the same manner as other questions. For how-questions, this corresponds to a numerical type. Ho we ver, retrie v-ing all numerical entities will pro vide lower answer identification precision than a system that only pro-vides those specified with the expected answer units.
Pinchak and Lin (2006) propose a dynamic an-swer typing system which computes a unique score for the appropriateness of any word to a particu-lar question. Unfortunately , their question conte xt-mapping is limited to what , wher e , and who ques-tions, and thus is not defined for how-questions.
Wu et al. (2005) handle how-questions dif ferently than other questions. The y use special hand-crafted rules to assign a particular answer tar get during the answer typing phase. In this way, the y tak e adv an-tage of the structure inherent in how-questions rather than just treating them as general queries. Ho we ver, manually hand-crafting types is costly , and would have to be repeated if the system was mo ved to a new language or a new query domain. Our auto-matic approach does not suf fer from this dra wback.
Light et al. (2001) sho wed that for a small fix ed set of answer types, multiple words tagged with the same type will exist even with perfect passage retrie val, sentence retrie val, and type assignment. For example, Example 1 may be answered with a sentence such as  X  X ears range in weight from the smaller black bear at 400 pounds to the gigantic griz-zly at over 1200 pounds X  in which two answers have appropriate units but only one of which is correct. We pro vide results in Section 6 confirming the lim-its of answer typing at narro wing answer focus, us-ing varying levels of perfect information.

Our approach mak es use of the web as a lar ge corpus of useful information. Exploiting the vast amount of data on the web is part of a gro wing trend in Natural Language Processing (K eller and Lapata, 2003). Indeed, man y QA systems have been devel-oped using the web (to varying degrees) to assist in finding a correct answer (Brill et al., 2001; Cucerzan and Agichtein, 2005; Rade v et al., 2001), as the web is the lar gest available corpus even if its information can be dif ficult to harness. Rather than relying on the web to find the answer to a question, we rely on it as a source of information on appropriate units only . Should the domain of the question answering system change from general factoid questions, units may be extracted from a smaller , domain-specific corpus. The objecti ve of our model is to create a list of rel-evant units for an adjecti ve that may be found in a how-question. We wish to create these lists a pri-ori and off-line so that the y are applicable to future questions. Although the model described here can be applied on-line at the time of question answering, the resources and time required mak e off-line gener -ation of unit lists the preferred approach.
We wish to automatically learn a mapping from how-questions and associated with these adjecti ves. For example, an element of this mapping might be: high which assigns height measurements to  X  X o w high X  questions. Inducing this mapping means establish-ing a connection, or co-occurr ence , between each adjecti ve and its units sections, we sho w how to establish this connection. 3.1 Using WordNet for Adjecti ve Expansion In common documents, such as news articles or web pages, the co-occurrence of an adjecti ve and its units may be unlik ely . For example, the co-occurrence between  X  X ea vy X  and  X  X ounds X  may not be as pre valent as the co-occurrence between  X  X eight X  and  X  X ounds.  X  We therefore propose us-ing WordNet (Fellbaum, 1998) to expand the how-adjecti ve to a set of related concepts the adjecti ve may be used to describe. We denote a related con-cept of as . In the abo ve example,  X  X ea vy X  can be used to describe a  X  X eight.  X  Two useful WordNet re-lations are the attrib ute relation, in which the adjec-tive is an attrib ute of the concept, and in cases where no attrib ute exists, the derivationally-r elate d words.  X  X ea vy X  is an attrib ute of  X  X eight X  whereas the deri vationally-relate d form is  X  X ea viness,  X  a plausi-ble but less useful concept. Ne xt we describe how the particular co-occurrence of the related concept and unit is obtained. 3.2 Using Google to Obtain Counts We selected the Google search engine as a source of co-occurrence data due to the lar ge number of in-dexed documents from which co-occurrence counts can be deri ved. To further enhance the quality of co-occurrence data, we search on the specific phrase  X  is measured in X  in which is one of the related concepts of . This allo ws for the simultaneous dis-covery of unkno wn units and the retrie val of their co-occurrence counts.

Sentences in which the pattern occurs are parsed using Minipar (Lin, 1998b) so that we can obtain the word related to  X  X easured X  via the preposi-tional in relation. This allo ws us to handle senten-tial constructions that may interv ene between  X  X ea-sured X  and a meaningful unit. For each unit that is related to  X  X easured X  via in , we increment the co-occurrence count , thereby collecting fre-quenc y counts for each with .

The pattern X  s precision pre vents incidental co-occurrence between a related concept and some unit that may occur simply because of the general topic of the document. For example,  X  X ize is measured in X  matches  X  X ize is measured in gigabytes, and per -formance is measured in milliseconds X . In this ex-ample, the co-occurrence count gigabytes size would be incremented by one, whereas there is no co-occurrence between  X  X ize X  and  X  X illiseconds.  X  Due to the lar ge amount of data available to Google, we can afford to restrict ourselv es to a single pattern and still expect to find meaningful units.

To gather the co-occurrence counts between an adjecti ve and a unit , we first expand to the set of related concepts and then compute: These frequencies can then be used by the scoring functions described in the follo wing section. 3.3 Filtering the Unit List For a given adjecti ve and a particular unit with co-occurrence , we define two impor -tant statistics:
The first equation measures the lik elihood of a unit being an answer unit for a how-question with the given adjecti ve . The second equation mea-sures, for a given unit , how lik ely a how question with adjecti ve ask ed the question answered by . The second measure is particularly useful in cases where a unit co-occurs with a number of dif fer -ent adjecti ves. These units are inherently less useful for answer typing. For example, if the word  X  X erms X  occurs on the unit list of adjecti ves such as  X  X igh,  X   X  X ong,  X  and  X  X ea vy, X  it may indicate that  X  X erms X  is not an appropriate measure for any of these con-cepts, but rather just a word lik ely to co-occur with nouns that can be measured.

We propose using the measures and unit lists. mance on the development set and so will not be further considered. the standard -measure (Salton and Buckle y, 1988). list and sures, we can create a unit list for an adjecti ve as in which jecti ve (either some threshold imposed to deal with the amount of noise present in the co-occurrence data. This thresh-old allo ws us to vary the required strength of the as-sociation between the unit and the question in or-der to consider the unit as appropriate to the how-adjecti ve. In Section 6, we demonstrate this flexi-bility by sho wing how answer identification preci-sion and recall can be traded off as desired by the given application. The value be passed to downstream modules of the question answering process (such as the answer extractor), which may then exploit the association value di-rectly . 4.1 Automatic Ho w-Adjecti ve Disco very An initial step in implementing answer typing for how-adjecti ve questions is to decide which adjec-tives would benefit from types. WordNet gives a set of all adjecti ves, but pro viding answer type units for all these adjecti ves is unnecessary and poten-tially misleading. Man y adjecti ves would clearly never occur in a how-adjecti ve query (i.e.,  X  X o w ve-hicular ...? X ), and even some that do, lik e the  X  X o w orwellian X  query mentioned abo ve, are dif ficult to quantify . For these, a simple search with keyw ord matching as in typical information retrie val would be preferable.

We have a two-stage process for identifying unit-typable how-adjecti ves. First, we examine the AOL query data (Pass et al., 2006) and extract as candi-dates all 152 adjecti ves that occur with the pattern  X  X o w adjective is/are/w as/were.  X  Second, we fil-ter adjecti ves that do not have a related concept in WordNet (Section 3.1). We built unit lists for the 104 adjecti ves that remained.

Given that both the query database and WordNet may lack information, it is important to consider the coverage of actual how-adjecti ve questions that unit lists collected this way may have. Reassuringly , ex-periments have sho wn 100% coverage of the 96 ad-jecti ves in our development and test question set, tak en from the TREC QA corpus (see Section 5). 4.2 Similar Word Expansion Unfortunately , we found that search results obtained using the pattern described in Section 3.2 do not pro-duce a wide variety of units. Web pages often do not use a slang term when mentioning the unit of measurement; a search for  X  X ize is measured in gigs X  on Google returns zero pages. Also, searching with Google X  s API and obtaining rele vant documents can be time consuming, and we must limit the number of pages considered. Thus, there is strong moti va-tion to expand the list of units obtained from Google by automatically considering similar units.
We gather similar units from an automatically-constructed thesaurus of distrib utionally similar words (Lin, 1998a). The similar word expansion can add a term lik e gigs as a unit for size by virtue of its association with gigabytes , which is on the original list.

Unit similarity can be thought of as a mapping is sets of related units. If is an element of a particular adjecti ve , the mapping us a way to add new words to the unit list for . For example, the similar word list for  X  X igabytes X  might be GB, me gabytes, kilobytes, KB, byte , GHz, gigs ... , which can all be added to the unit list for the adjecti ve  X  X ar ge.  X 
After expanding each element of the unit list for adjecti ve , we have a new set of units : where
For each there is an associated score . We define the score of units that do not co-occur on similar word lists to be zero and the similarity of two identical units to be one. We can then use these scores to assign estimated co-occurrence counts for any unit in the expanded unit list : If a unit also occurs in the set of expanded similar units for another another unit , that is, , then the original co-occurrence fre-quenc y of and , , will be boosted by the similarity-weighted frequenc y of on the ex-panded unit list of , . 4.3 Selection of Answer Candidates For a given how-adjecti ve question and a document of interest, we use a two-stage process to identify the entities in the document that are suitable answers for the question. First, the named entity recognizer of Minipar is used to identify all numerical entities in text, labeled as NUM . Minipar labels times, dates, monetary amounts, and address numbers with types other than NUM and so we can correctly exclude these from consideration. We then inspect the con-text of all NUM entities to see if a unit exists on the pre-computed unit list for the given how-adjecti ve. Textual entities that pass both stages of our identifi-cation process are considered as candidate answers. This section presents experiments comparing our how-adjecti ve answer typing approach to alterna-tive schemes on an answer identification task. We compare our two unit ranking functions and using the similar unit expansion (Section 4.2). 5.1 Ev aluation Questions The clearest way to test a QA system is to evalu-ate it on a lar ge set of questions. Although our an-swer typing system is not capable of fully answer -ing questions, we will mak e use of the how-adjecti ve questions from TREC 2002-2005 (Vorhees, 2002) as a set of test data. We tak e eight of the questions as a development set (used for preliminary investigations of scoring functions  X  no parameters can be set on the development set specifically) and 86 of the ques-tions as a final, unseen test set. Seventeen dif ferent adjecti ves occur in the test questions. 5.2 Ev aluation Methodology We evaluate our system with an approach we call Answer -Identification Precision Recall (AIPR). For a particular scoring threshold (Section 3.3), each ad-jecti ve has a corresponding unit list, which is used to extract answer candidates from documents (Section 4.3). To ensure the performance of the IR-engine is not an issue in evaluation, we only use documents judged to contain the correct answer by TREC.
Answer -identification precision corresponds to the number of correct answers among the candi-date answers extracted by our system. Answer -identification recall is the number of correct answers extracted among the total number of correct answers in the answer documents.
 A plot of AIPR allo ws the designer of a particular QA system to decide on the optimum PR-tradeof f for the answer typing task. If other stages of QA rely on a lar ge number of candidates, a high recall value may be desired so no potential answers are missed. If answer typing is used as a means of boost-ing already-lik ely answers, high precision may in-stead be favoured. 5.3 Comparison Systems This section describes the various systems we com-pare with our approach. Recall that perfect AIPR performance is not possible with typing alone (Sec-tion 2, (Light et al., 2001)), and thus we pro-vide some of our comparison systems with varying amounts of perfect answer information in order to establish the highest performance possible in dif fer -ent scenarios on the given dataset.

Query-specific Oracle : The best possible system creates a unit list for each specific how-question in-dividually . This list is created using only those units in the answer pattern of the TREC-pro vided judge-ment for this specific question.

Adjecti ve-specific Oracle : This system is de-signed, lik e ours, to pro vide a unit list for each how-adjecti ve, rather than for a specific question. The unit list for a particular adjecti ve contains all the units from all the test set answers of how-adjecti ve questions containing that adjecti ve. It is optimal in the sense it will identify every correct answer for each how-adjecti ve, but only contains those units necessary for this identification.

Fixed-Category : This system gives the perfor -mance of a general-purpose, fix ed-cate gory answer typing approach applied to how-questions. In a fix ed-cate gory approach, all how-questions are clas-sified as seeking numerical answers, and thus all nu-merical answers are returned as answer candidates.
IR-Document Inferr ed : Here we infer question units from documents belie ved to be rele vant to the question. An IR system (TREC X  s PRISE) is given a how-adjecti ve question, and returns a set of doc-uments for that query . Ev ery numerical digit in the documents can be considered a possible answer to the question, and the units associated with those val-ues can be collected as the unit list, rank ed (and thresholded) by frequenc y. We remo ve units that oc-cur in a list of 527 stopw ords, and filter numerical modifiers lik e  X  X undred, thousand, million, etc.  X 
Answer -Document Inferr ed : This approach is identical to the IR-Document Inferred approach, except the documents are only those documents judged by TREC to contain the answer . In this way the Answer -Document Inferred approach pro vides some what of an upper bound on Document Inferred unit typing, by assuming perfect document retrie val. Figure 1: Microa veraged AIPR with dif ferent scor -ing functions, unit lists.

Inferring the answer units from the set of rele-vant documents is similar in spirit to (Daum  X  e III and Marcu, 2006). In one of their experiments in query-focused summarization, the y sho w competiti ve sum-marization performance without even providing the query , as the query model is inferred solely from the commonality in rele vant documents. In our case, good performance will also be possible if the actual answers have the highest commonality among the numerical values in the rele vant documents. The microa veraged Answer -Identification Precision Recall over all question-answer pairs is plotted in Figures 1 and 2. Macroa veraged results are similar .
For our own automatic answer typing approaches, our first observ ation is the benefit of ranking with Ov er most of the recall range, both the une xpanded (automatic) unit lists and the expanded unit lists impro ve in precision by a few percent when using both probabilistic scoring statistics. Secondly , note that both systems using the expanded unit lists can achie ve almost 20% higher maximum recall than the une xpanded unit list systems. This pro vides strong justification for the small overhead of looking up similar words for items on our unit list.

We next examine the AIPR performance of our comparison systems versus our best-performing au-tomatic unit typing approach (Figure 2). The query-specific oracle is able to achie ve the highest perfor -mance because of perfect kno wledge of the units ap-propriate to a given question. Ho we ver, its preci-sion is only 42.2%. That is, the answer identifica-tion accurac y is limited because the correct answer shares its units with other numerical entities in the answer documents. Slightly worse, the adjecti ve-specific oracle is limited to 34.2% precision. Un-lik e the query-specific oracle, if the question is  X  X o w long did WWII last? X , the entities with the irrele-vant units  X  X eters X  and  X  X ilometers X  must also be proposed as candidate answers because the y occur in answers to other  X  X o w long X  questions. This ora-cle thus pro vides a more appropriate upper bound on automatic unit-typing performance as our automatic approaches also build unit lists for adjecti ves rather than questions. Note again that unit lists for adjec-tives can be generated off-line whereas unit lists for specific questions need the query before processing.
In terms of recall, both upper -bound systems top out at around 78% (with our expanded systems reaching close to this at about 72%). At first, this number seems fairly disappointing: if how-adjecti ve questions only have answer units in 78% of the cases, perhaps our typing approach is not entirely appropriate. On inspecting the actual misses, how-ever, we find that 10 of the 16 missed questions cor -respond to  X  X o w old X  questions. These are often answered without units (e.g.  X  X t age 52.  X ). Higher recall would be possible if the system def aults to ex-tracting all numerical entities for  X  X o w old X  ques-tions. On the remaining questions, high recall can indeed be obtained.

Also of note is the clear disadv antage of using the standard fix ed-cate gory approach to how-question answer typing (Figure 2). Its precision runs at just under 5%, about a quarter of the lowest precision of any of our unit-list approaches at any recall value. Ho we ver, fix ed-cate gory typing does achie ve high recall, roughly 96%, missing only numerical entities unrecognized by Minipar . This high recall is possi-ble because fix ed-cate gory typing does not miss an-swers for  X  X o w old X  questions.

Both inferred approaches also perform worse than Figure 2: Microa veraged AIPR of our approach ver-sus comparison systems. our system (Figure 2). Thus inferring units from rele vant documents does not seem promising, as even the unrealistic approach of inferring only from kno wn answer documents cannot achie ve as high in answer -identification precision. Also, realistically using IR-retrie ved documents has uni versally lower AIPR. As expected, answer -document inferred re-call plateaus at the same spot as the oracle systems, as it also requires a unit after each numerical en-tity (hurting it, again, on the  X  X o w old X  questions). Despite their lower performance, note that these in-ferred approaches are completely orthogonal to our offline automatic answer -typing, so a future pos-sibility remains to combine both kinds of systems within a unified frame work. Although it is dif ficult to evaluate the impact of our approach until it is inte grated into a full QA-system, we have clearly demonstrated the adv antages of au-tomatic answer typing for how-questions. We have sho wn the impro vements possible by ranking with dif ferent co-occurrence statistics, and the benefit of expanding unit lists with similar words. Experi-mental results sho w our approaches achie ve superior AIPR performance over all realistic baselines.
In addition to proposing a competiti ve system, we belie ve we have established a frame work and eval-uation methodology that may be of use to other re-searchers. For example, although manual typing re-mains an option, our approach can at least pro vide a good set of candidate units to consider . Further -more, a similar -w ord database can expand the list obtained by manual typing. Finally , users may also wish to rank the manual types in some way, and thus configure the system for a particular level of answer -identification precision/recall.

Our success with these unit lists has encouraged two main directions of future work. First, we plan to mo ve to a discriminati ve approach to combin-ing scores and weighting unit features using a small labeled set. Secondly , we will look at incorporat-ing units into the information retrie val process. Our moti vating example in Section 1 retrie ved irrele vant documents when given to a search engine, and this seems to be a general trend in how-question IR. Less than 60% of the TREC how-questions have a unit of the correct type anywhere in the top ten docu-ments returned by the PRISE IR engine, and less than half correspondingly had a correct answer in the top ten at all. Making the information retrie val process aware of the desired answer types will be an important future direction of QA research.
 We gratefully ackno wledge support from the Natu-ral Sciences and Engineering Research Council of Canada, the Alberta Ingenuity Fund, and the Alberta Informatics Circle of Research Excellence.

