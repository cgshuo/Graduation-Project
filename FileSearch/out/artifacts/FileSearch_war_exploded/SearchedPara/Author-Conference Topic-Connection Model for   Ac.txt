 This paper proposes a novel topi c model, Author-Conference Topic-Connection (ACTC) Model fo r academic network search. The ACTC Model extends the author-conference-topic (ACT) model by adding subject of the conference and the latent mapping information between subjects and topics. It si multaneously models topical aspects of papers, authors and conferences with two latent topic layers: a subject layer corresponding to conference topic, and a topic layer corresponding to the word topic. Ea ch author would be associated with a multinomial distribution over subjects of conference (eg., KM, DB, IR for CIKM 2012), the confer ence(CIKM 2012), and the topics are respectively generated from a sampled subject. Then the words are generated from the sampled t opics. We conduct experiments on a data set with 8 , 523 authors, 22 , 487 papers and 1 , 243 conferences from the well-known Arnetminer website, and train the model with different number of subjects and t opics. For a qualitative evaluation, we compare ACTC with three others models LDA, Author-Topic (AT) and ACT in academic search services. Experiments show that ACTC can effectively capture the semantic connection between different types of information in academic network and perform well in expert searching and conference searching. H.2.8 [ Database Management ]: Database applications  X  data mining ; H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  retrieval models. Algorithms, Experimentation.
 Academic Network Search, Gi bbs Sampling, Topic Model This paper aims at the academic network search [1,2,3], in which the object is to find the expertise experts, papers and conferences for a given query. In this paper we discuss a new solution for the academic search. In particular, an Author-Conference Topic-Connection (ACTC) Model is proposed which could mine the latent subjects and topics. This paper focuses on academic search with ACTC, which extends author-conference-topic (ACT) model by adding another latent variable which indicates the subject information of the conference and the latent mapping informati on between subjects and topics.
 ACT is a unified topic model for simultaneously modeling the topical distribution of paper, author and conferences, as shown in Figure 1. In ACT, each topic is characterized by a distribution over words and also associated with a multinomial distribution over the conferences. Therefore, the topic of words is made equivalent with the topic that generate the conference. In fact, they are not equivalent. Generally speaking, each conference has its main research areas called subjects. Furthermore, there are some topics of interest in each subject. For example, the Conference on Information and Knowledge Management ( CIKM ) is related to the areas of  X  information retrieval  X  ( IR ),  X  knowledge management  X  ( KM ) and  X  databases  X  ( DB ). The topics of interest in thes e subjects are promulgated on the homepage of CIKM 1 . The examples of topics under the subject of DB to CIKM are shown in Figure 2. Besides, the subjects and topics are rich in semantic information, the association between which is especially significant for the academic network modeling. Figure 1. ACT Model. Figure 2. The topics under subject DB . For better explaining the problem, let X  X  take a simple example in Figure 3. Assuming there is a paper which is a vector of 8 words Furthermore, the number of topics is set to 3, and the topic 1 has high probability generating the words of  X  database  X ,  X  multimedia  X  and  X  data  X . The topic 2 is most likely to generate  X  gene  X ,  X  biological  X ,  X  database  X  and  X  data  X . The topic 3 has high probability generating  X  database  X ,  X  data  X , and  X  distributed  X . Besides, all the topics are most likely to be related to the conferences such as SIGMOD , CIKM and VLDB . In the sense of words feature, the topic 2 may be most likely to represent the meaning of  X  biological database  X . While in the sense of conference representa tion, it may also be sampled but most likely to be interpreted as  X  database  X . In fact, the semantic meaning of  X  biological database  X  belongs to  X  database  X . Therefore, the latent topic represented by word s is not equivalent with the topic represented by conferences as assumed in ACT model. To overcome the limitation of AC T as explained, we introduce another latent topic layer called  X  subject  X  to indicate conference topic. Then we model the topic-connection between the word-topic and the conference-topic. As shown in Figure 4, the subject 2 is most related to the conference like SIGMOD , CIKM and VLDB , which may indicate the meaning of  X  database  X . The topic presentation by the words is the same as in Figure 3. In terms of words feature, the topic 1, http://www.cikm2012.org 2 and 3 are respectively representing the meaning of  X  multimedia database  X ,  X  biological database  X  and  X  distributed database  X . Obviously, the subject 2 is mostly associated with all three topics. The remainder of this paper is organized as follows: Section 2 reviews related work on academic network search and topic modeling. Section 3 presents the proposed ACTC m odel and introduces the parameter estimation process. Section 4 desc ribes the experiments, empirical evaluation of our model, and comparison with three other models. We conclude the paper with future res earch developments in section 5. Academic network contains many di fferent types of information, like papers, conferences and authors, wh ich are significant in modeling the academic network. At pres ent, academic network modeling approach normally separate the modeling of different types of information, which have neglecte d the dependencies between them. The dependencies are often import ant for improving academic search, so a unified modeling approach for the academic network is needed. Academic network search, which de votes to finding out the expertise author, influential papers, and top conferences in the academic network for a given query, have be en intensively investigated. The academic network search has drawn a lot of attention, and some specialized academic search engines have been in service, such as Google Scholar, Citeseer, Libra, and ArnetMiner. Recently many probabilistic topic models have been proposed to represent the documents, such as Probability La tent Semantic Indexing (PLSI) [4], Latent Dirich let Allocation (LDA) model [5] and so on. LDA is a probabilistic generative model that can be used to estimate the properties of multinom ial observations via unsupervised learning. LDA represents each document as a mixture of probabilistic topics and each topic as a multinomial distribution over words. As an extension of LDA by adding an author layer, Author-Topic (AT) model is originally proposed by Steyvers and Rosen-Zvi in 2004 [6], and assumes that the topic proportion of a document is generated by the chosen author. By simultan eously modeling the content of documents and the interests of aut hors, the set of topics could be obtained, as well as the most related topics of the authors.
 In particular, ACT model has furt her extended AT model by adding publication venue as one extra piece of contextual information. By simultaneously modeling the papers , author and conferences, the learned topic distribution could be used for further estimating the inter-dependences between differen t types of information [2]. ACT model has been applied to the academic search services. Besides, it could be used for author interest finding, conference suggestion, and association search by accumulating the distance between authors. Our method follows the line of work in ACT model. It is enhanced to capture the association between authors, conferences and words in paper using the connection between conference-topic and word-topic. Basically, the academic network is composed of three different types of objects: authors, papers, and conferences. Our goal is to discover the latent topic distribution associated with each object. ACTC Model extends previously proposed ACT model by explicitly modeling the subject information of conference during the generative process. It represents each paper with a mixture of authors, each author is associated with a multinomial distri bution over subjects, each subject is associated with a multinomial distribution over topics, each conference is generated from the sa mpled subject and each word is generated from the sampled topic. Th is model associates the authors and conferences with the words in a paper via the latent subject and topic as well as their connection. For a given paper, we could not only map it to a conference which improves the ideology of the ACT model. In addition, the accepted paper by one conference could also be mapped to a subject of the conference, and the words in paper could be used for representing the topics under the subjects. It should be noted that the subjects of conference are coarse-grained. The topics under the subject, in contrast, are fine-grained. Furt hermore, the subjects could be considered as the topic information of conference-hierarchy, simultaneously the topics could be regarded as the topic information of word-hierarchy. There exists a mapping relationship between the subjects and the topics. So we add an additional subject layer between the conference layer and original topic layer. We can just use this mapping relationship to build the ACTC model while ACT only uses the topic of words to en hance the topic connection between the conference and papers. Compared with ACT, ACTC adds the latent subject layer and mapping information between subjects and topics, so that it could strength the topic connection between the conference-topic and the word-topic in a paper.
 The intuition behind the ACTC model is: first, authors formulate the rough research direction, which is a named subject and determines a proportion of the conference based on the subject, and a topic need to be ascertained according to the priori distribution of corresponding subject connecting to the topic, then generates the words in the paper. The proposed ACTC model is illustrated in Figure 5, and the notations used are summarized in Tabl e. 1. The dotted red frame is the innovation part of our proposed model. Each author is represented as mixtures over all subjects (with a prior distribution  X  ), and each subject is a multinomial distribution  X  over all conferences, as well as a multinomial distribution  X  over all topics. Lastly, each topic of words is associated with a multinomial distribution  X  over all words. Four symmetric Dirichlet conjugate priors,  X  ,  X  ,  X  and  X  , are de fi ned for each of these four multinomial distributions in ACTC. The formal definition of the generative process in the ACTC model can be described as follows: At present, there are a variety of inference algorithms which could be used for estimating the parameters of topic models, such as basic expectation maximization, variati onal EM, expectation propagation and Gibbs sampling [7,8]. In this paper, we choose Gibbs sampling for parameter estimation, which is a special case of Markov Chain Monte Carlo sampling and often yiel ds relatively simple algorithms for approximate inference in high dimensional models . In the Gibbs Sampling process, we would construct a Markov chain which converges to the posterior distribution on latent topics. What we need to estimate in the model are four sets of parameters:(1) conference given a subject  X  , (3) the probability of a word given a topic  X  , (4) the probability of a topic given a subject  X  . According to the independence assumptions shown in Figure 5, the joint distribution of the authors, s ubjects (conference-topics), topics (word-topics), conferences, and words based on A hyper-parameters could be form ally derived in Equation 1. For simplicity, the hyper-parameters  X  ,  X  ,  X  and  X  , are initialized empirically (i.e.,  X  =50/T,  X  =0.01,  X  =0.1 and  X  =0.01). In the Gibbs sampling procedure, we fi rstly estimate the posterior distribution on posterior probability is calculated in Equation 2. The equation draws the hidden variable for the current state i , in which the meaning of the total numbers of the subject s allocated to the author x except the current assignment (the exception is denoted by addition, all the standard notations are described in detail in Table 1. After Gibbs sampling, the parameters can be estimated as follows: We used the DBLP version which is released by ArnetMiner website as the data set [2,9,10]. We organize data set by considering each paper as a unit. Each paper contai ns its title, author, abstract, and conference information. Th e data set consists of 8 , 523 authors, 22 , 487 papers and 1 , 243 conferences. For the sake of efficiency, we conducted some degree of preprocessing on the data set. We only keep the authors and conferences whose occurrence frequency is above a certain threshold of 1, so that it could reduce the sparsity. Perplexity is a standard measur e for estimating the performance of topic model. Lower perplexity score indicates better generalization performance [11,12]. For a set of test words, ( w perplexity is defined as the exp onential of the ne gative normalized predictive likelihood as follows: As there are two latent topic laye rs in the model, we conduct the experiments in two steps. Firstly, we test the perplexity for different subject numbers (S) with fixing the numbers of topic (T) to a certain number, and choose the smallest num ber of subject which leads to approximating the minimum perple xity. Secondly, based on the selected number of subject, we con tinue test the perplexity for the different number of topics, then select it with the same principle. Figure 6 shows that the change of perplexity for different number of subjects when topics number is fixe d to 100. As the increasing of the number of iterations, the perplexity is decreasing with different subject numbers. Furthermore, at some extend, the larger subjects leads to the smaller value of perplexity. But the perplexity will goes up when the subject numbers is set to 50. Figure 7 shows the results that the change of perplexity for a function of the number of topics with the subjects number S=30. The principle of selecting the topics number is similar to the subject. Still the minimum perplexity is obtained when the topic is set to 100. So we set th e subject number S=30 and the topic number T=100 in our experiments. 
Figure 6. The perplexity for different number of subject when As shown in Table 2, we list the ex amples of four topics discovered by ACTC. Each topic is shown with the top 8 words and top 5 authors, along with their own probabilities. In Table 2, with regard to topic #74, the top 8 words are  X  natural  X ,  X  language  X ,  X  technique  X ,  X  knowledge  X ,  X  process  X ,  X  discourse  X ,  X  analysis  X , and  X  semantic  X , so topic #74 may represent the semantic meaning of  X  Natural Language Processing  X . Moreover, the top 5 authors related to topic #74 are listed as  X  Robert Gaizauskas  X ,  X  Robert Wilensky  X ,  X  Eric Brill  X ,  X  Yo r i c k Wi l k s  X , and  X  Ewan Klein  X . In fact, the listed top 5 authors are really interested in the research of  X  Natural Language Processing  X , and  X  Robert Gaizauskas  X ,  X  Eric Brill  X , and  X  Yorick Wilks  X  are also really considered expert in the benchmark set. Furthermore, we discovered the subj ects of the conferences with the top 3 topics and the top 5 conf erences, as shown in Table 3. According to the top 3 topics and the top 5 conferences, we could obtain the interpretations of the subjects. As shown in Table 3, the subject 18 is most related with the conferences like  X  AAAI  X ,  X  EMNLP  X ,  X  Machine Learning  X ,  X  COLI  X , and  X  SIGKDD  X . Therefore, the subject 18 may represent the semantic meaning of  X  Artificial Intelligence  X . According to Table 2, the topic 5, 74, 32 could be respectively considered as the meaning of  X  Machine Learning  X ,  X  Natural Language Processing  X , and  X  Pattern Recognition  X . We could find that  X  Machine Learning  X ,  X  Natural Language Processing  X , and  X  Pattern Recognition  X  is fin e-grained, while in contrast,  X  Artificial Intelligence  X  is coarse-grained. In fact,  X  Machine Learning  X ,  X  Natural Language Processing  X , and  X  Pattern Recognition  X  indeed belong to  X  Artificial Intelligence  X . Another interesting result is that the author  X  Vladimir Vapnik  X  is related on both the topic 5 ( X  Machine Learning  X ) and the topic 32 ( X  Pattern Recognition  X ). It could also prove that the topic represented by words is fine-grained. The mapping between subjects and topics has provided further evidence for the topic connection. Because of this, the model could be able to more accurately characterize the semantic association between the words, authors with conferences. After modeling, we could acquire many distributions between the authors, words and conferences on the foundation of the topic connection between topics and subj ects. ACTC model could be used for a variety of applications, such as expert searching and conference searching. Furthermore, we conduc ted evaluation in terms of P@5, P@10, P@20, R-pre, and MAP based on the application. For evaluation, we collected a pool of authors and conferences returned from Arnetminer and Libra. Specifically, for each query, we get the top 50 returned results from these websites. All the queries we adopt in our experiments are:  X  data-mining  X ,  X  machine-learning  X ,  X  planning  X ,  X  semantic-web  X ,  X  support-vector-machine  X . In order to better reflect the modeling effect, we compare the performance of ACTC with L DA, AT model and ACT model. We applied the same strategy to estimate these models. As lacking in modeling the authors and conferences, we treated these information as the normal words in LDA, and treated the conferences as normal words in AT. We set the topics number as T=100 for all models. Additionally, the number of subjects is set as S=30 for ACTC. Given a research area as a query, expert searching returns lists of experts which are most related to this area. For example, when given the query  X  data mining  X ,  X  Jiawei Han  X  would be retrieved as one of the top names. Such a task requires computing the similarity between authors and the query. To illustrate how the model could be used in this respect, we rank the posterior probability of the target author ( a ) given the query words ( w ), P(a|w) . This probability will be computed based on the parameters of ACTC model. Formally, we define the ranking function of our retrieval system in Equation 4. W is the input query, which may contain one or more words. For each word, we use w i to denote it. When computing the value of P(a|w we use the two distributions P(a|s j ) , P(s j |z t ) and P(z which are derived from ACTC model. What should be noted is that, although the conference information can not be found in the Equation 4, the subjects of them has been used to enhance the association between the author, word, and conference via the distribution P(s between the latent conference topi c and the word topic. The final score is the product of all words in this query. The performance of ACTC model and other baseline models are shown in Table 4. We can also see that ACTC works better than other models. Model P @ 5 P @ 10 P @ 20 R-p re MAP AT 0.43 0.45 0.36 0.22 0.28 Conference searching could be analogous to the expert searching. When given a research area as a query, lists of conferences which are most related to this area woul d be recommended. The posterior probability of the conference ( c ) given the query words ( w ), P(c|w) , will be computed as Equation 5 shown. Specifically, the distribution P(c|s j ) represents the posterior probability of the target conference ( c ) on the basis of subject ( s P(z t |w i ) stands for the posterior probability of the topic ( z query word ( w i ). Especially P(s j |z t ), the connection from conference topic to word topic, has helped to improve the precision of the distribution P(c|w i ) . The final score is the product of all words in this query. The performance of the ACTC model and other baseline models is shown in Table 5. The ACTC model achieves the best performance in all evaluation measures. It has made greater progress than the performance in expert searching, which further reflects this model could enhance the modeli ng capabilities of conference. Model P @ 5 P @ 10 P @ 20 R-p re MAP AT 0.36 0.38 0.35 0.23 0.25 In this paper, we propose a novel author-conferecne topic-connection (ACTC) model, which could be app lied for academic network search. Specially, a new latent layer named  X  subject  X  has been introduced, which is associated with the confer ences information, may connect to multiple  X  topic  X  layer each represented by the words. ACTC has been demonstrated more effective in improving coherence in topic clustering and the association of authors, words and conferences. Furthermore, there are many potentia l further developments. It would be interesting to further study to dynamically set the number of the subjects corresponding to the different conferences. In addition, the model could be further extended with citation information [13]. Citation information is also significant for academic network modeling and analysis. We could also use ACTC for the analysis of topic trends and new topic detection. This work was supported by the NSF of China (No.90920005, No.61003192), The Major Project of State Language Commission in the Twelfth Five-year Plan Period (No.ZDI125-1), Project in the National Science &amp; Technology Pillar Program in the Twelfth Five-year Plan Period (No. 2012BAK24B01), the Program of Introducing Talents of Discipline to Universities (No.B07042), the NSF of Hubei Province (No.2011C DA034) and the self-determined research funds of CCNU from the colleges X  basic research and operation of MOE (No.CCNU10A02009 and No.CCNU10C01005). [1] Yi Fang, Luo Si and Aditya Mathur. 2008. FacFinder: Search [2] Jie Tang, J.Zhang, L.Yao, J.Li, L.Zhang, and Z.Su. 2008. [3] Jie Tang, J.Zhang, Ruoming Jin, Zi Yang, Keke Cai, L.Zhang, [4] T.Hofmann. 1999, Probability Latent Semantic Analysis. 15 [5] D.M.Blei, A.Y.Ng, and M.I. Jordan. 2003. Latent dirichlet [6] M.Rosen-Zvi, T.Grif fi ths, M.Steyvers, P.Smyth. 2004. The [7] T. Minka and J. Laerty. 2002. Expectation-propagation for the [8] Y.Teh, M.Jordan, M.Beal&amp; D.Bl ei. 2006. Hierarchical Dirichlet [9] Jie Tang, Limin Yao, Duo Zhang, and Jing Zhang. 2010. A [10] Jie Tang, Duo Zhang, and Limin Yao. 2007. Social Network [11] Fang Li, Huiyu Shen, Tingting He. 2011. Tag-Topic Model for [12] Xin Chen, Xiaohua Hu, Zhongna Zhou, Caimei Lu, Gail Rosen, [13] Y. Tu, N. Johri, D. Roth, and J. Hockenmaier. 2010. Citation 
