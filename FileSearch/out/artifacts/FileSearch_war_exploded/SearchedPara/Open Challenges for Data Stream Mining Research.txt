 Every day, huge volumes of sensory, transactional, and web data are continuously generated as streams, which need to be analyzed online as they arrive. Streaming data can be considered as one of the main sources of what is called big data. While predictive modeling for data streams and big data have received a lot of at-tention over the last decade, many research approaches are typi-cally designed for well-behaved controlled problem settings, over-looking important challenges imposed by real-world applications. This article presents a discussion on eight open challenges for data stream mining. Our goal is to identify gaps between current re-search and meaningful applications, highlight open problems, and define new application-relevant research directions for data stream mining. The identified challenges cover the full cycle of knowledge discovery and involve such problems as: protecting data privacy, dealing with legacy systems, handling incomplete and delayed in-formation, analysis of complex data, and evaluation of stream min-ing algorithms. The resulting analysis is illustrated by practical applications and provides general suggestions concerning lines of future research in data stream mining. The volumes of automatically generated data are constantly in-creasing. According to the Digital Universe Study [18], over 2 . 8 ZB of data were created and processed in 2012, with a projected in-crease of 15 times by 2020. This growth in the production of dig-ital data results from our surrounding environment being equipped with more and more sensors. People carrying smart phones produce data, database transactions are being counted and stored, streams of data are extracted from virtual environments in the form of logs or user generated content. A significant part of such data is volatile, which means it needs to be analyzed in real time as it arrives. Data stream mining is a research field that studies methods and algo-rithms for extracting knowledge from volatile streaming data [14; 5; 1]. Although data streams, online learning, big data, and adapta-tion to concept drift have become important research topics during
Figure 1: CRISP cycle with data stream research challenges. Mining big data streams faces three principal challenges: volume , velocity ,and volatility . Volume and velocity require a high volume of data to be processed in limited time. Starting from the first arriv-ing instance, the amount of available data constantly increases from zero to potentially infinity. This requires incremental approaches that incorporate information as it becomes available, and online processing if not all data can be kept [15]. Volatility, on the other hand, corresponds to a dynamic environment with ever-changing patterns. Here, old data is of limited use, even if it could be saved and processed again later. This is due to change, that can affect the induced data mining models in multiple ways: change of the target variable , change in the available feature information ,and drift . Changes of the target variable occur for example in credit scor-ing, when the definition of the classification target  X  X efault X  versus  X  X on-default X  changes due to business or regulatory requirements. Changes in the available feature information arise when new fea-tures become available, e.g. due to a new sensor or instrument. Similarly, existing features might need to be excluded due to regu-latory requirements, or a feature might change in its scale, if data from a more precise instrument becomes available. Finally, drift is a phenomenon that occurs when the distributions of features x and target variables y change in time. The challenge posed by drift has been subject to extensive research, thus we provide here solely a brief categorization and refer to recent surveys like [17]. In supervised learning, drift can affect the posterior P ( y | x ) ,the P ( y ) distribution. The distinction based on which distribution is assumed to be affected, and which is assumed to be static, serves to assess the suitability of an approach for a particular task. It is worth noting, that the problem of changing distributions is also present in unsupervised learning from data streams.
 A further categorization of drift can be made by: The second important challenge for privacy preservation is concept drift. As data may evolve over time, fixed privacy preservation rules may no longer hold. For example, suppose winter comes, snow falls, and much less people commute by bike. By knowing that a person comes to work by bike and having a set of GPS traces, it may not be possible to identify this person uniquely in summer, when there are many cyclists, but possible in winter. Hence, an im-portant direction for future research is to develop adaptive privacy preservation mechanisms, that would diagnose such a situation and adapt themselves to preserve privacy in the new circumstances. Most of the data stream research concentrates on developing pre-dictive models that address a simplified scenario, in which data is already pre-processed , completely and immediately available for free . However, successful business implementations depend strongly on the alignment of the used machine learning algorithms with both, the business objectives, and the available data. This section discusses often omitted challenge s connected with streaming data. Data preprocessing is an important step in all real world data anal-ysis applications, since data comes from complex environments, may be noisy, redundant , contain outliers and missing values. Many standard procedures for preprocessing offline data are available and well established, see e.g. [33]; however, the data stream setting in-troduces new challenges that have not received sufficient research attention yet.
 While in traditional offline analysis data preprocessing is a once-off procedure, usually done by a human expert prior to modeling, in the streaming scenario manual processing is not feasible, as new data continuously arrives. Streaming data needs fully automated pre-processing methods, that can optimize the parameters and operate autonomously. Moreover, preprocessing models need to be able to update themselves automatically along with evolving data, in a sim-ilar way as predictive models for streaming data do. Furthermore, all updates of preprocessing procedures need to be synchronized with the subsequent predictive models, otherwise after an update in preprocessing the data representation may change and, as a result, the previously used predictive model may become useless. Except for some studies, mainly focusing on feature construction over data streams, e.g. [49; 4], no systematic methodology for data stream preprocessing is currently available.
 As an illustrative example for challenges related to data preprocess-ing, consider predicting traffic jams based on mobile sensing data. People using navigation services on mobile devices can opt to send anonymized data to the service prov ider. Service providers, such as Google, Yandex or Nokia, provide estimations and predictions of traffic jams based on this data. First, the data of each user is mapped to the road network, the speed of each user on each road segment of the trip is computed, data from multiple users is aggregated, and finally the current speed of the traffic is estimated. There are a lot of data preprocessing challenges associated with this task. First, noisiness of GPS data might vary depending on location and load of the telecommunication network. There may be outliers , for instance, if somebody stopped in the middle of a segment to wait for a passenger, or a car broke. The number of pedestrians using mobile navigation may vary, and require adaptive instance selection . Moreover, road networks may change over time, leading to changes in average speeds, in the number of cars and even car types (e.g. heavy trucks might be banned, new optimal routes emerge). All these issues require automated preprocessing Class imbalance, where the class prior probability of the minor-ity class is small compared to that of the majority class, is a fre-quent problem in real-world applications like fraud detection or credit scoring. This problem has been well studied in the offline setting (see e.g. [22] for a recen t book on that subject), and has also been studied to some extent in the online, stream-based setting (see [23] for a recent survey). However, among the few existing stream-based approaches, most do not pay attention to drift of the minority class, and as [23] pointed out, a more rigorous evaluation of these algorithms on real-world data needs yet to be done. Latency means information becomes available with significant de-lay. For example, in the case of so-called verification latency, the value of the preceding instance X  X  target variable is not available be-fore the subsequent instance has to be predicted. On evolving data streams, this is more than a mere problem of streaming data inte-gration between feature and target streams, as due to concept drift patterns show temporal locality [2]. It means that feedback on the current prediction is not available to improve the subsequent pre-dictions, but only eventually will become available for much later predictions. Thus, there is no recent sample of labeled data at all that would correspond to the most-recent unlabeled data, and semi-supervised learning approaches are not directly applicable. A related problem in static, offline data mining is that addressed by unsupervised transductive transfer learning (or unsupervised do-main adaptation): given labeled data from a source domain, a pre-dictive model is sought for a related target domain in which no labeled data is available. In principle, ideas from transfer learning could be used to address latency in evolving data streams, for ex-ample by employing them in a chunk-based approach, as suggested in [43]. However, adapting them for use in evolving data streams has not been tried yet and constitutes a non-trivial, open task, as adaptation in streams must be fast and fully automated and thus cannot rely on iterated careful tuning by human experts. Furthermore, consecutive chunks constitute several domains, thus the transitions between several subsequent chunks might provide exploitable patterns of systematic drift. This idea has been in-troduced in [27], and a few so-called drift-mining algorithms that identify and exploit such patterns have been proposed since then. However, the existing approaches cover only a very limited set of possible drift patterns and scenarios. The challenge of intelligently selecting among costly pieces of in-formation is the subject of active learning research. Active stream-based selective sampling [38] describes a scenario, in which in-stances arrive one-by-one. While the instances X  feature vectors are provided for free, obtaining their true target values is costly, and the definitive decision whether or not to request this target value must be taken before proceeding to the next instance. This corresponds to a data stream, but not necessarily to an evolving one. As a result, only a small subset of stream-based selective sampling algorithms is suited for non-stationary environments. To make things worse, many contributions do not state explicitly whether they were de-signed for drift, neither do they provide experimental evaluations on such evolving data streams, thus leaving the reader the ardu-ous task to assess their suitability for evolving streams. A first, re-cent attempt to provide an overview on the existing active learning strategies for evolving data streams is given in [43]. The challenges for active learning posed by evolving data streams are: Albeit the members of each stream are entities, we use the term  X  X ntity X  only for stream T  X  the target of learning, while we denote the entities in the other streams as  X  X nstances X . In the unsupervised setting, entity stream clustering encompasses learning and adapting clusters over T , taking account the other streams that arrive at dif-ferent speeds. In the supervised setting, entity stream classification involves learning and adapting a classifier, notwithstanding the fact that an entity X  X  label may change from one time point to the next, as new instances referencing it arrive. The first challenge of entity stream mining task concerns informa-tion summarization: how to aggregate into each entity e at each time point t the information available on it from the other streams? What information should be sto red for each entity? How to deal with differences in the speeds of the individual streams? How to learn over the streams efficiently? Answering these questions in a seamless way would allow us to deploy conventional stream mining methods for entity stream mi ning after aggregation. The information referencing a rel ational entity cannot be held per-petually for learning, hence aggregation of the arriving streams is necessary. Information aggregation over time-stamped data is tra-ditionally practiced in document stream mining, where the objec-tive is to derive and adapt content summaries on learned topics. Content summarization on entities, which are referenced in the doc-ument stream, is studied by Kotov et al., who maintain for each entity the number of times it is mentioned in the news [26]. In such studies, summarization is a task by itself. Aggregation of information for subsequent learning is a bit more challenging, be-cause summarization implies information loss -notably informa-tion about the evolution o f an entity. Hassani and Seidl monitor health parameters of patients, modeling the stream of recordings on a patient as a sequence of events [21]: the learning task is then to predict forthcoming values. Aggregation with selective forget-ting of past information is proposed in [25; 42] in the classification context: the former method [25] slides a window over the stream, while the latter [42] forgets entities that have not appeared for a while, and summarizes the information in frequent itemsets, which are then used as new features for learning. Even if information aggregation over the streams T 1 ,...,T m is performed intelligently, entity stream mining still calls for more than conventional stream mining methods. The reason is that enti-ties of stream T re-appear in the stream and evolve. In particular, in the unsupervised setting, an entity may be linked to conceptu-ally different instances at each time point, e.g. reflecting a cus-tomer X  X  change in preferences. In the supervised setting, an entity may change its label; for example, a customer X  X  affinity to risk may change in response to market changes or to changes in family sta-tus. This corresponds to entity drift , i.e. a new type of drift beyond the conventional concept drift pertaining to model  X  T . Hence, how should entity drift be traced, and how should the interplay between entity drift and model drift be captured? In the unsupervised setting, Oliveira and Gama learn and monitor clusters as states of evolution [32], while [41] extend that work to learn Markov chains that mark the entities X  evolution. As pointed out in [32], these states are not necessarily predefined  X  they must be subject of learning. In [43], we report on further solutions to the entity evolution problem and to the problem of learning with forgetting over multiple streams and over the entities referenced by them.
 Conventional concept drift also occurs when learning a model over is the probability of an event to occur before time t . Correspond-ingly, S ( t | x )=1  X  F ( t | x ) is the probability that the event did not occur until time t (the survival probability). It can hence be used to model the probability of the right-censoring of the time for an event to occur.
 A simple example is the Cox proportional hazard model [9], in which the hazard rate is constant over time; thus, it does depend on the feature vector x =( x 1 ,...,x n ) but not on time t .More specifically, the hazard rate is modeled as a log-linear function of the features x i : The model is proportional in the sense that increasing x i by one unit increases the hazard rate  X  ( x ) by a factor of  X  i =exp(  X  i ) . For this model, one easily derives the survival function S ( t | x )= 1  X  exp(  X   X  ( x )  X  t ) and an expected survival time of 1 / X  ( x ) . Although the temporal nature of event data naturally fits the data stream model and, moreover, event data is naturally produced by many data sources, EHA has been considered in the data stream scenario only very recently. In [39], the authors propose a method for analyzing earthquake and Twitter data, namely an extension of the above Cox model based on a sliding window approach. The authors of [28] modify standard classification algorithms, such as decision trees, so that they can be trained on a snapshot stream of both censored and non-censored data.
 Like in the case of clustering [35], where one distinguishes between clustering observations and clust ering data sources, two different settings can be envisioned for EHA on data streams: 1. In the first setting, events are generated by multiple data sources 2. In the second setting, events are produced by a single data Statistical event models on data streams can be used in much the same way as in the case of static data. For example, they can serve predictive purposes, i.e., to an swer questions such as  X  X ow much time will elapse before the next email arrives? X  or  X  X hat is the probability to receive mo re than 100 emails within the next hour? X . What is specifically interesting, however, and indeed distinguishes the data stream setting from the static case, is the fact that the model may change over time. This is a subtle aspect, because the hazard model h ( t | x ) itself may already be time-dependent; here, how-ever, t is not the absolute time but the duration time, i.e., the time elapsed since the last event. A change of the model is compara-ble to concept drift in classification, and means that the way in which the hazard rate depends on time t and on the features x i changes over time. For example, consider the event  X  X ncrease of a stock rate X  and suppose that  X  i =log(2) for the binary feature x = energysector in the above Cox model (which, as already mentioned, does not depend on t ). Thus, this feature doubles the hazard rate and hence halves the expected duration between two events. Needless to say, however, this influence may change over time, depending on how well the energy sector is doing. account expert knowledge and user preferences.
 Clearly, evaluation of data stream algorithms is a fertile ground for novel theoretical and algorithmic solutions. In terms of pre-diction measures, data stream mining still requires evaluation tools that would be immune to class imbalance and robust to noise. In our opinion, solutions to this problem should involve not only met-rics based on relative performance to baseline (chance) classifiers, but also graphical measures similar to PR-curves or cost curves. Furthermore, there is a need for integrating information about con-cept drifts in the evaluation process. As mentioned earlier, possible ways of considering concept drifts will depend on the information that is available. If true concepts are known, algorithms could be evaluated based on: how often they detect drift, how early they de-tect it, how they react to it, and how quickly they recover from it. Moreover, in this scenario, evaluation of an algorithm should be dependent on whether it takes place during drift or during times of concept stability. A possible way of tackling this problem would be the proposal of graphical methods, similar to ROC analysis, which would work online and visualize concept drift measures alongside prediction measures. Additionally, these graphical measures could take into account the state of the stream, for example, its speed, number of missing values, or class distribution. Similar methods could be proposed for scenarios where concepts are not known in advance, however, in these cases measures should be based on drift detectors or label-independent stream statistics. Above all, due to the number of aspects which need to be measured, we believe that the evaluation of data stream algorithms requires a multi-criterial view. This could be done by using i nspirations from multiple crite-ria decision analysis, where trade-offs between criteria are achieved using user-feedback. In particular, a user could showcase his/her criteria preferences (for example, between memory consumption, accuracy, reactivity, self-tuning, and adaptability) by deciding be-tween alternative algorithms for a given data stream. It is worth noticing that such a multi-criterial view on evaluation is difficult to encapsulate in a single number, as it is usually done in traditional offline learning. This might suggest that researchers in this area should turn towards semi-qualitati ve and semi-quantitative evalua-tion, for which systematic methodologies should be developed. Finally, a separate research direction involves rethinking the way we test data stream mining algorithms. The traditional train, cross-validate, test workflow in classification is not applicable for sequen-tial data, which makes, for instance, parameter tuning much more difficult. Similarly, ground truth verification in unsupervised learn-ing is practically impossible in data stream environments. With these problems in mind, it is worth stating that there is still a short-age of real and synthetic benchmark datasets. Such a situation might be a result of non-uniform standards for testing algorithms on streaming data. As community, we should decide on such matters as: What characteristics should benchmark datasets have? Should they have prediction tasks attached? Should we move towards on-line evaluation tools rather than datasets? These questions should be answered in order to solve evaluation issues in controlled envi-ronments before we create measures for real-world scenarios. While a lot of algorithmic methods for data streams are already available, their deployment in real applications with real streaming data presents a new dimension of challenges. This section points out two such challenges: making models simpler and dealing with legacy systems. from online events to optimize an immediate pay-off. In the online advertisement application mentioned above, the user-click predic-tion is done within a context, defined for example by the currently viewed page and the profile of the cookie. The decision which banner to display is done online, but the context can be prepro-cessed offline. By deriving meta-information such as  X  X he profile is a young male, the page is from the sport cluster X , the offline com-ponent can ease the online decision task. Domain knowledge may help to solve many issues raised in this paper, by systematically exploiting particularities of application do-mains. However, this is seldom considered, as typical data stream methods are created to deal with a large variety of domains. For in-stance, in some domains the learning algorithm receives only par-tial feedback upon its prediction, i.e. a single bit of right-or-wrong, rather than the true label. In the user-click prediction example, if a user does not click on a banner, we do not know which one would have been correct, but solely that the displayed one was wrong. This is related to the issues on timing and availability of informa-tion discussed in section 4.2.
 However, building predictive models that systematically incorpo-rate domain knowledge or domain specific information requires to choose the right optimization criteria. As mentioned in sec-tion 6, the data stream setting requires optimizing multiple criteria simultaneously, as optimizing only predictive performance is not sufficient. We need to develop learning algorithms, which mini-mize an objective function including intrinsically and simultane-ously: memory consumption, predictive performance, reactivity, self monitoring and tuning, and (explainable) auto-adaptivity. Data streams research is lacking methodologies for forming and opti-mizing such criteria.
 Therefore, models should be simple so that they do not depend on a set of carefully tuned parameters. Additionally, they should com-bine offline and online techniques to address challenges of big and fast data, and they should solve the right problem, which might consist in solving a multi-criteria optimization task. Finally, they have to be able to learn from a small amount of data and with low variance [37], to react quickly to drift. In many application environments, such as financial services or health care systems, business critical applications are in operation for decades. Since these applications produce massive amounts of data, it becomes very promising to process these amounts of data by real-time stream mining approaches. However, it is often impos-sible to change existing infrastructures in order to introduce fully fledged stream mining systems. Rather than changing existing in-frastructures, approaches are required that integrate stream mining techniques into legacy systems. In general, problems concerning legacy systems are domain-specific and encompass both technical and procedural issues. In this section, we analyze challenges posed by a specific real-world application with legacy issues  X  the ISS Columbus spacecraft module. Spacecrafts are very complex systems, exposed to very different physical environments (e.g. space), and associated to ground sta-tions. These systems are under constant and remote monitoring by means of telemetry and commands. The ISS Columbus mod-ule has been in operation for more than 5 years. For some time, it is pointed out that the monitoring process is not as efficient as previously expected [30]. However, we assume that data stream the flight control team) are responsible for decisions and conse-quent actions. This problem poses the following question: How to integrate data stream mining into legacy systems when automation needs to be increased but the human expert needs to be maintained in the loop? Abstract discussions on this topic are provided by ex-pert systems [44] and the MAPE-K reference model [24]. Expert systems aim to combine human expertise with artificial expertise and the MAPE-K reference model aims to provide an autonomic control loop. A balance must be struck which considers both afore-mentioned aspects appropriately.
 Overall, the Columbus study has shown that extending legacy sys-tems with real time data stream mining technologies is feasible and it is an important area for further stream-mining research. In this paper, we discussed research challenges for data streams, originating from real-world applications. We analyzed issues con-cerning privacy, availability of information, relational and event streams, preprocessing, model complexity, evaluation, and legacy systems. The discussed issues were illustrated by practical applica-tions including GPS systems, Twitte r analysis, earthquake predic-tions, customer profiling, and spacecraft monitoring. The study of real-world problems highlighted shortcomings of existing method-ologies and showcased previous ly unaddressed research issues. Consequently, we call the data stream mining community to con-sider the following action points for data stream research: As our study shows, there are challenges in every step of the CRISP data mining process. To date, modeling over data streams has been viewed and approached as an extension of traditional meth-ods. However, our discussion and application examples show that in many cases it would be beneficial to step aside from building upon existing offline approaches, and start blank considering what is required in the stream setting.
 We would like to thank the participants of the RealStream2013 workshop at ECMLPKDD2013 in Prague, and in particular Bern-hard Pfahringer and George Forman, for suggestions and discus-sions on the challenges in stream mining. Part of this work was [17] J. Gama, I. Zliobaite, A. Bifet, M. Pechenizkiy, and [18] J. Gantz and D. Reinsel. The digital universe in 2020: Big [19] A. Goldberg, M. Li, and X. Zhu. Online manifold regular-[20] I. Guyon, A. Saffari, G. Dror, and G. Cawley. Model selec-[21] M. Hassani and T. Seidl. Towards a mobile health context [22] H. He and Y. Ma, editors. Imbalanced Learning: Founda-[23] T. Hoens, R. Polikar, and N. Chawla. Learning from stream-[24] IBM. An architectural blueprint for autonomic computing. [25] E. Ikonomovska, K. Driessens, S. Dzeroski, and J. Gama. [26] A. Kotov, C. Zhai, and R. Sproat. Mining named entities [27] G. Krempl. The algorithm APT to classify in concurrence of [28] M. Last and H. Halpert. Survival analysis meets data stream [29] F. Nelwamondo and T. Marwala. Key issues on computational [30] E. Noack, W. Belau, R. Wohlgemuth, R. M  X  uller, S. Palumberi, [31] E. Noack, A. Luedtke, I. Schmitt, T. Noack, E. Schauml  X  offel, [32] M. Oliveira and J. Gama. A framework to monitor clusters WiththeinceptionoftheTwittermicrobloggingplatform in2006,amyriadofresearcheffortshaveemergedstudying differentaspectsoftheTwittersphere.Eachstudyexploits itsowntoolsandmechanismstocapture,store,queryand analyzeTwitterdata.Inevitably,platformshavebeende-velopedtoreplacethisad-hocexplorationwithamorestruc-turedandmethodologicalformofanalysis.Anotherbody ofliteraturefocusesondevelopinglanguagesforquerying Tweets.Thispaperaddressesissuesaroundthebigdatana-tureofTwitterandemphasizestheneedfornewdataman-agementandquerylanguageframeworksthataddresslimi-tationsofexistingsystems.Wereviewexistingapproaches thatweredevelopedtofacilitatetwitteranalyticsfollowed byadiscussiononresearchissuesandtechnicalchallenges indevelopingintegratedsolutions. Themassivegrowthofdatageneratedfromsocialmedia sourceshaveresultedinagrowinginterestonefficientand effectivemeansofcollecting,analyzingandqueryinglarge volumesofsocialdata.Theexistingplatformsexploitsev-eralcharacteristicsofbigdata,includinglargevolumesof data,velocityduetothestreamingnatureofdata,andva-rietyduetotheintegrationofdatafromthewebandother sources.Hence,socialnetworkdatapresentsanexcellent testbedforresearchonbigdata.
 Inparticular,onlinesocialnetworkingandmicroblogging platformTwitterhasseenexponentialgrowthinitsuser basesinceitsinceptionin2006withnowover200million monthlyactiveusersproducing500milliontweets( Twitter-sphere ,thepostingsmadetoTwitter)daily 1 .Awidere-searchcommunityhasbeenestablishedsincethenwiththe hopeofunderstandinginteractionsonTwitter.Forexam-ple,studieshavebeenconductedinmanydomainsexploring differentperspectivesofunderstandinghumanbehavior.
 Priorresearchfocusesonavarietyoftopicsincludingopin-ionmining[12,14,38],eventdetection[46,65,76],spreadof pandemics[26,58,68],celebrityengagement[74]andanalysis ofpoliticaldiscourse[28,40,70].Thesetypesofeffortshave enabledresearcherstounderstandinteractionsonTwitter relatedtothefieldsofjournalism,education,marketing,dis-asterreliefetc. http://tnw.to/s0n9u WiththeinceptionoftheTwittermicrobloggingplatform in2006,amyriadofresearcheffortshaveemergedstudying differentaspectsoftheTwittersphere.Eachstudyexploits itsowntoolsandmechanismstocapture,store,queryand analyzeTwitterdata.Inevitably,platformshavebeende-velopedtoreplacethisad-hocexplorationwithamorestruc-turedandmethodologicalformofanalysis.Anotherbody ofliteraturefocusesondevelopinglanguagesforquerying Tweets.Thispaperaddressesissuesaroundthebigdatana-tureofTwitterandemphasizestheneedfornewdataman-agementandquerylanguageframeworksthataddresslimi-tationsofexistingsystems.Wereviewexistingapproaches thatweredevelopedtofacilitatetwitteranalyticsfollowed byadiscussiononresearchissuesandtechnicalchallenges indevelopingintegratedsolutions. Themassivegrowthofdatageneratedfromsocialmedia sourceshaveresultedinagrowinginterestonefficientand effectivemeansofcollecting,analyzingandqueryinglarge volumesofsocialdata.Theexistingplatformsexploitsev-eralcharacteristicsofbigdata,includinglargevolumesof data,velocityduetothestreamingnatureofdata,andva-rietyduetotheintegrationofdatafromthewebandother sources.Hence,socialnetworkdatapresentsanexcellent testbedforresearchonbigdata.
 Inparticular,onlinesocialnetworkingandmicroblogging platformTwitterhasseenexponentialgrowthinitsuser basesinceitsinceptionin2006withnowover200million monthlyactiveusersproducing500milliontweets( Twitter-sphere ,thepostingsmadetoTwitter)daily 1 .Awidere-searchcommunityhasbeenestablishedsincethenwiththe hopeofunderstandinginteractionsonTwitter.Forexam-ple,studieshavebeenconductedinmanydomainsexploring differentperspectivesofunderstandinghumanbehavior.
 Priorresearchfocusesonavarietyoftopicsincludingopin-ionmining[12,14,38],eventdetection[46,65,76],spreadof pandemics[26,58,68],celebrityengagement[74]andanalysis ofpoliticaldiscourse[28,40,70].Thesetypesofeffortshave enabledresearcherstounderstandinteractionsonTwitter relatedtothefieldsofjournalism,education,marketing,dis-asterreliefetc. http://tnw.to/s0n9u  X  Datamanagementframeworks. Inadditiontopro- X  Languagesforqueryingtweets. Agrowingbody Wehaveidentifiedtheessentialingredientsforaunified Twitterdatamanagementsolution,withtheintentionthat ananalystwilleasilybeabletoextenditscapabilitiesfor specifictypesofresearch.Suchasolutionwillallowthe dataanalysttofocusontheusecasesoftheanalyticstask byconvenientlyusingthefunctionalityprovidedbyanin-tegratedframework.InSection5wepresentourposition andemphasizetheneedforintegratedsolutionsthataddress limitationsofexistingsystems.Section6outlinesresearch issuesandchallengesassociatedwiththedevelopmentofin-tegratedplatforms.FinallyweconcludeinSection7. ResearchershaveseveraloptionswhenchoosinganAPIfor datacollection,i.e.theSearch,StreamingandtheREST API.EachAPIhasvaryingcapabilitieswithrespecttothe typeandtheamountofinformationthatcanberetrieved.
 TheSearchAPIisdedicatedforrunningsearchesagainstan indexofrecenttweets.Ittakeskeywordsasquerieswiththe possibilityofmultiplequeriescombinedasacommasepa-ratedlist.ArequesttothesearchAPIreturnsacollection ofrelevanttweetsmatchingauserquery.
 TheStreamingAPIprovidesastreamtocontinuouslycap-turethepublictweetswhereparametersareprovidedto filtertheresultsofthestreambyhashtags,keywords,twit-teruserids,usernamesorgeographicregions.TheREST APIcanbeusedtoretrieveafractionofthemostrecent tweetspublishedbyaTwitteruser.AllthreeAPIslimitthe numberofrequestswithinatimewindowandrate-limitsare posedattheuserandtheapplicationlevel.Responseob-tainedfromTwitterAPIisgenerallyintheJSONformat.
 Thirdpartylibraries 2 areavailableinmanyprogramming languagesforaccessingtheTwitterAPI.Theselibrariespro-videwrappersandprovidemethodsforauthenticationand otherfunctionstoconvenientlyaccesstheAPI.
 PubliclyavailableAPIsdonotguaranteecompletecoverage ofthedataforagivenqueryasthefeedsarenotdesigned forenterpriseaccess.Forexample,thestreamingAPIonly providesarandomsampleof1%(knownasthe Spritzer https://dev.twitter.com/docs/twitter-libraries  X  Datamanagementframeworks. Inadditiontopro- X  Languagesforqueryingtweets. Agrowingbody Wehaveidentifiedtheessentialingredientsforaunified Twitterdatamanagementsolution,withtheintentionthat ananalystwilleasilybeabletoextenditscapabilitiesfor specifictypesofresearch.Suchasolutionwillallowthe dataanalysttofocusontheusecasesoftheanalyticstask byconvenientlyusingthefunctionalityprovidedbyanin-tegratedframework.InSection5wepresentourposition andemphasizetheneedforintegratedsolutionsthataddress limitationsofexistingsystems.Section6outlinesresearch issuesandchallengesassociatedwiththedevelopmentofin-tegratedplatforms.FinallyweconcludeinSection7. ResearchershaveseveraloptionswhenchoosinganAPIfor datacollection,i.e.theSearch,StreamingandtheREST API.EachAPIhasvaryingcapabilitieswithrespecttothe typeandtheamountofinformationthatcanberetrieved.
 TheSearchAPIisdedicatedforrunningsearchesagainstan indexofrecenttweets.Ittakeskeywordsasquerieswiththe possibilityofmultiplequeriescombinedasacommasepa-ratedlist.ArequesttothesearchAPIreturnsacollection ofrelevanttweetsmatchingauserquery.
 TheStreamingAPIprovidesastreamtocontinuouslycap-turethepublictweetswhereparametersareprovidedto filtertheresultsofthestreambyhashtags,keywords,twit-teruserids,usernamesorgeographicregions.TheREST APIcanbeusedtoretrieveafractionofthemostrecent tweetspublishedbyaTwitteruser.AllthreeAPIslimitthe numberofrequestswithinatimewindowandrate-limitsare posedattheuserandtheapplicationlevel.Responseob-tainedfromTwitterAPIisgenerallyintheJSONformat.
 Thirdpartylibraries 2 areavailableinmanyprogramming languagesforaccessingtheTwitterAPI.Theselibrariespro-videwrappersandprovidemethodsforauthenticationand otherfunctionstoconvenientlyaccesstheAPI.
 PubliclyavailableAPIsdonotguaranteecompletecoverage ofthedataforagivenqueryasthefeedsarenotdesigned forenterpriseaccess.Forexample,thestreamingAPIonly providesarandomsampleof1%(knownasthe Spritzer https://dev.twitter.com/docs/twitter-libraries taggingaresomeofthetextprocessingtasksthatbetter preparetweetsfortheanalysistask.Theplatformpro-videsseparatebuilt-inmodulestoextractinformationsuch aslocation,language,sentimentandnamedentitiesthat aredeemedveryusefulindataanalytics.Thecreationofa pipelineofthesetoolsallowsthedataanalysttoextendand reuseeachcomponentwithrelativeease.
 TwitIE[17]isanotheropen-sourceinformationextraction NLPpipelinecustomizedformicroblogtext.Forthepur-poseofinformationextraction(IE),thegeneralpurposeIE pipelineANNIEisusedanditconsistsofcomponentssuch assentencesplitter,POStaggerandgazetteerlists(forloca-tionprediction).Eachstepofthepipelineaddressesdraw-backsintraditionalNLPsystemsbyaddressingtheinherent challengesinmicroblogtext.Asaresult,individualcom-ponentsofANNIEarecustomized.Languageidentification, tokenisation,normalization,POStaggingandnamedentity recognitionisperformedwitheachmodulereportingaccu-racyontweets.
 Baldwin[11]presentsasystemdesignedforeventdetection onTwitterwithfunctionalityforpre-processing.JSONre-sultsreturnedbytheStreamingAPIareparsedandpiped throughlanguagefilteringandlexicalnormalisationcompo-nents.Messagesthatdonothavelocationinformationare geo-located,usingprobabilisticmodelssinceit X  X acritical issueinidentifyingwhereaneventoccurs.Informationex-tractionmodulesrequireknowledgefromexternalsources andaregenerallymoreexpensivetasksthanlanguagepro-cessing.Platformsthatsupportreal-timeanalysis[11,76] requireprocessingtaskstobeconductedon-the-flywhere thespeedoftheunderlyingalgorithmsisacrucialconsider-ation. Thereareseveralproposalsinwhichresearchershavetried todevelopgenericplatformstoprovidearepeatablefoun-dationforTwitterdataanalytics.TwitterZombie[15]isa platformtounifythedatagatheringandanalysismethods bypresentingacandidatearchitectureandmethodological approachforexaminingspecificpartsoftheTwittersphere. Itoutlinesarchitectureforstandardcapture,transforma-tionandanalysisofTwitterinteractionsusingtheTwitter X  X  SearchAPI.ThistoolisdesignedtogatherdatafromTwit-terbyexecutingaseriesofindependentsearchjobsona continualbasisandthecollectedtweetsandtheirmetadata iskeptinarelationalDBMS.Oneoftheinterestingfeatures ofTwitterZombieisitsabilitytocapturehierarchicalrela-tionshipsinthedatareturnedbyTwitter.Anetworktrans-latormoduleperformspost-processingonthetweetsand storeshashtags,mentionsandretweets,separatelyfromthe tweettext.Rawtweetsaretransformedintoarepresenta-tionofinteractionstocreatenetworksofretweets,mentions andusersmentioninghashtags.Thisfeaturecapturedby TwitterZombie,whichotherstudiespaylittleattentionto, ishelpfulinansweringdifferenttypesofresearchquestions withrelativeease.Socialgraphsarecreatedintheformof aretweetormentionnetworkandtheydonotcrawlforthe usergraphwithtraditionalfollowingrelationships.Italso drawsdiscussiononhowmulti-bytetweetsinlanguageslike ArabicorChinesecanbestoredbyperformingtranslitera-tion.
 Morerecently,TwitHoard[69]suggestsaframeworkofsup-portingprocessorsfordataanalyticsonTwitterwithem-taggingaresomeofthetextprocessingtasksthatbetter preparetweetsfortheanalysistask.Theplatformpro-videsseparatebuilt-inmodulestoextractinformationsuch aslocation,language,sentimentandnamedentitiesthat aredeemedveryusefulindataanalytics.Thecreationofa pipelineofthesetoolsallowsthedataanalysttoextendand reuseeachcomponentwithrelativeease.
 TwitIE[17]isanotheropen-sourceinformationextraction NLPpipelinecustomizedformicroblogtext.Forthepur-poseofinformationextraction(IE),thegeneralpurposeIE pipelineANNIEisusedanditconsistsofcomponentssuch assentencesplitter,POStaggerandgazetteerlists(forloca-tionprediction).Eachstepofthepipelineaddressesdraw-backsintraditionalNLPsystemsbyaddressingtheinherent challengesinmicroblogtext.Asaresult,individualcom-ponentsofANNIEarecustomized.Languageidentification, tokenisation,normalization,POStaggingandnamedentity recognitionisperformedwitheachmodulereportingaccu-racyontweets.
 Baldwin[11]presentsasystemdesignedforeventdetection onTwitterwithfunctionalityforpre-processing.JSONre-sultsreturnedbytheStreamingAPIareparsedandpiped throughlanguagefilteringandlexicalnormalisationcompo-nents.Messagesthatdonothavelocationinformationare geo-located,usingprobabilisticmodelssinceit X  X acritical issueinidentifyingwhereaneventoccurs.Informationex-tractionmodulesrequireknowledgefromexternalsources andaregenerallymoreexpensivetasksthanlanguagepro-cessing.Platformsthatsupportreal-timeanalysis[11,76] requireprocessingtaskstobeconductedon-the-flywhere thespeedoftheunderlyingalgorithmsisacrucialconsider-ation. Thereareseveralproposalsinwhichresearchershavetried todevelopgenericplatformstoprovidearepeatablefoun-dationforTwitterdataanalytics.TwitterZombie[15]isa platformtounifythedatagatheringandanalysismethods bypresentingacandidatearchitectureandmethodological approachforexaminingspecificpartsoftheTwittersphere. Itoutlinesarchitectureforstandardcapture,transforma-tionandanalysisofTwitterinteractionsusingtheTwitter X  X  SearchAPI.ThistoolisdesignedtogatherdatafromTwit-terbyexecutingaseriesofindependentsearchjobsona continualbasisandthecollectedtweetsandtheirmetadata iskeptinarelationalDBMS.Oneoftheinterestingfeatures ofTwitterZombieisitsabilitytocapturehierarchicalrela-tionshipsinthedatareturnedbyTwitter.Anetworktrans-latormoduleperformspost-processingonthetweetsand storeshashtags,mentionsandretweets,separatelyfromthe tweettext.Rawtweetsaretransformedintoarepresenta-tionofinteractionstocreatenetworksofretweets,mentions andusersmentioninghashtags.Thisfeaturecapturedby TwitterZombie,whichotherstudiespaylittleattentionto, ishelpfulinansweringdifferenttypesofresearchquestions withrelativeease.Socialgraphsarecreatedintheformof aretweetormentionnetworkandtheydonotcrawlforthe usergraphwithtraditionalfollowingrelationships.Italso drawsdiscussiononhowmulti-bytetweetsinlanguageslike ArabicorChinesecanbestoredbyperformingtranslitera-tion.
 Morerecently,TwitHoard[69]suggestsaframeworkofsup-portingprocessorsfordataanalyticsonTwitterwithem-streamonatopicofinterest.Theresultsarepipedthrough textprocessingcomponentsandthegeo-locatedtweetsare visualisedonamapforbetterinteraction.Clearly,platforms ofthisnaturethatdealwithincidentexplorationneedto makeprovisionsforreal-timeanalysisoftheincomingTwit-terstreamandproducesuitablevisualizationsofdetected incidents. Datamodelsarenotdiscussedindetailinmoststudies, asasimpledatamodelissufficienttoconductbasicform ofanalysis.Whenstandardtweetsarecollected,flatfiles [11,72]isthepreferredchoice.Severalstudiesthatcap-turethesocialrelationships[15,19]oftheTwittersphere, employstherelationaldatamodelbutdonotnecessarily storetherelationshipsinagraphdatabase.Asaconse-quence,manyanalysesthatcanbeperformedconveniently onagrapharenotcapturedbytheseplatforms.Only TwitHoard[69]intheirpapermodelsco-occurrenceofterms asagraphwithtemporallyevolvingproperties.Twitter Zombie[15]andTwitHoard[69]shouldbehighlightedfor capturinginteractionsincludingtheretweetsandtermas-sociationsapartfromthetraditionalfollower/friendsocial relationships.TrendMiner[61]drawsexplicitdiscussionon makingprovisionsforprocessingmillionsofdataandtakes advantageofApacheHadoopMapReduceframeworktoper-formdistributedprocessingofthetweetsstoredaskey-value pairs.CoalMine[72]alsohasApacheHadoopatthecore oftheirbatchprocessingcomponentresponsibleforefficient processingoflargeamountofdata. Therearemanyplatformsdesignedwithintegratedtools predominantlyforvisualization,toanalysedatainspatial, temporalandtopicalperspectives.OnetoolistweetTracker [44],whichisdesignedtoaidmonitoringoftweetsforhu-manitariananddisasterrelief.TweetXplorer[54]alsopro-videsusefulvisualizationtoolstoexploreTwitterdata.For aparticularcampaign,visualizationsintweetXplorerhelp analyststoviewthedataalongdifferentdimensions;most interestingdaysinthecampaign(when),importantusers andtheirtweets(who/what)andimportantlocationsinthe dataset(where).SystemslikeTwitInfo[48],Twitcident[8] andTorettor[65]alsoprovideasuiteofvisualisationca-pabilitiestoexploretweetsindifferentdimensionsrelating tospecificapplicationslikefightingfireanddetectingearth-quakes.Web-mashupslikeTrendsmap[5]andTwitalyzer[6] provideawebinterfaceandenterprisebusinesssolutionsto gainreal-timetrendandinsightsofusergroups.
 Table1illustratesanoverviewofrelatedapproachesand featuresofdifferentplatforms. Pre-processing inTable1in-dicatesifanyformoflanguageprocessingtaskssuchasPOS taggingornormalizationareconducted. Informationextrac-tion referstothetypesofpostprocessingperformedtoinfer additionalinformation,suchassentimentornamedentities (NEs).Multipleticks( )correspondtoataskthatiscar-riedoutextensively.Inadditiontocollectingtweets,some studiesalsocapturetheuser X  X socialgraphwhileotherspro-posetheneedtoregardinteractionsofhashtags,retweets, mentionsasseparateproperties.Backenddatamodelssup-portedbytheplatformshapethetypesofanalysisthatcan beconvenientlydoneoneachframework.Fromthesum-maryinTable1,wecanobservethateachstudyondata streamonatopicofinterest.Theresultsarepipedthrough textprocessingcomponentsandthegeo-locatedtweetsare visualisedonamapforbetterinteraction.Clearly,platforms ofthisnaturethatdealwithincidentexplorationneedto makeprovisionsforreal-timeanalysisoftheincomingTwit-terstreamandproducesuitablevisualizationsofdetected incidents. Datamodelsarenotdiscussedindetailinmoststudies, asasimpledatamodelissufficienttoconductbasicform ofanalysis.Whenstandardtweetsarecollected,flatfiles [11,72]isthepreferredchoice.Severalstudiesthatcap-turethesocialrelationships[15,19]oftheTwittersphere, employstherelationaldatamodelbutdonotnecessarily storetherelationshipsinagraphdatabase.Asaconse-quence,manyanalysesthatcanbeperformedconveniently onagrapharenotcapturedbytheseplatforms.Only TwitHoard[69]intheirpapermodelsco-occurrenceofterms asagraphwithtemporallyevolvingproperties.Twitter Zombie[15]andTwitHoard[69]shouldbehighlightedfor capturinginteractionsincludingtheretweetsandtermas-sociationsapartfromthetraditionalfollower/friendsocial relationships.TrendMiner[61]drawsexplicitdiscussionon makingprovisionsforprocessingmillionsofdataandtakes advantageofApacheHadoopMapReduceframeworktoper-formdistributedprocessingofthetweetsstoredaskey-value pairs.CoalMine[72]alsohasApacheHadoopatthecore oftheirbatchprocessingcomponentresponsibleforefficient processingoflargeamountofdata. Therearemanyplatformsdesignedwithintegratedtools predominantlyforvisualization,toanalysedatainspatial, temporalandtopicalperspectives.OnetoolistweetTracker [44],whichisdesignedtoaidmonitoringoftweetsforhu-manitariananddisasterrelief.TweetXplorer[54]alsopro-videsusefulvisualizationtoolstoexploreTwitterdata.For aparticularcampaign,visualizationsintweetXplorerhelp analyststoviewthedataalongdifferentdimensions;most interestingdaysinthecampaign(when),importantusers andtheirtweets(who/what)andimportantlocationsinthe dataset(where).SystemslikeTwitInfo[48],Twitcident[8] andTorettor[65]alsoprovideasuiteofvisualisationca-pabilitiestoexploretweetsindifferentdimensionsrelating tospecificapplicationslikefightingfireanddetectingearth-quakes.Web-mashupslikeTrendsmap[5]andTwitalyzer[6] provideawebinterfaceandenterprisebusinesssolutionsto gainreal-timetrendandinsightsofusergroups.
 Table1illustratesanoverviewofrelatedapproachesand featuresofdifferentplatforms. Pre-processing inTable1in-dicatesifanyformoflanguageprocessingtaskssuchasPOS taggingornormalizationareconducted. Informationextrac-tion referstothetypesofpostprocessingperformedtoinfer additionalinformation,suchassentimentornamedentities (NEs).Multipleticks( )correspondtoataskthatiscar-riedoutextensively.Inadditiontocollectingtweets,some studiesalsocapturetheuser X  X socialgraphwhileotherspro-posetheneedtoregardinteractionsofhashtags,retweets, mentionsasseparateproperties.Backenddatamodelssup-portedbytheplatformshapethetypesofanalysisthatcan beconvenientlydoneoneachframework.Fromthesum-maryinTable1,wecanobservethateachstudyondata ingtimegranularities,todiscovercontextofcollecteddata. Operatorsalsoallowretrievingasubsetoftweetssatisfying thesecomplexconditionsontermassociations.Thisenables theendusertoselectagoodsetofterms(hashtags)that drivethedatacollectionwhichhasadirectimpactonthe qualityoftheresultsgeneratedfromtheanalysis.
 Spatialfeaturesareanotherpropertyoftweetsoftenover-lookedincomplexanalysis.Previouslydiscussedworkuses thelocationattributeasamechanismtofiltertweetsin space.Tocompleteourdiscussionwebrieflyoutlinetwo studiesthatusegeo-spatialpropertiestoperformcomplex analysisusingthelocationattribute.Doytsher etal. [31]in-troducedamodelandquerylanguagesuitedforintegrated dataconnectingasocialnetworkofuserswithaspatialnet-worktoidentifyplacesvisitedfrequently.Edgesnamed life-patterns areusedtoassociatethesocialandspatialnet-works.Differenttimegranularitiescanbeexpressedfor eachvisitedlocationrepresentedbythelife-patternedge. Eventhoughtheimplementationemploysapartiallysyn-theticdataset,itwillbeinterestingtoinvestigatehowthe socio-spatialnetworksandthelife-patternedgesthatare usedtoassociatethespatialandsocialnetworkscanberep-resentedinarealsocialnetworkdatasetwithlocationinfor-mation,suchasTwitter.GeoScope[18]findsinformation trendsbydetectingsignificantcorrelationsamongtrending location-topicpairsinaslidingwindow.Thisgivesriseto theimportanceofcapturingthenotionofspatialinforma-tiontrendsinsocialnetworksinanalysistasks.Real-time detectionofcrisiseventsfromalocationinspace,exhibits thepossiblevalueofGeoscope.Inoneoftheexperiments Twitterisusedasacasestudytodemonstrateitsuseful-ness,whereahashtagischosentorepresentthetopicand cityfromwhichthetweetoriginateschosentocapturethe location. Relational,RDFandGraphsarethemostcommonchoices ofdatarepresentation.Thereisacloseaffiliationinthese datamodelsobservingthat,forinstance,agraphcaneasily correspondtoasetofRDFtriplesorviceversa.Infact, somestudieslikePlachourasandStavrakas[59]haveput forwardtheirdatamodelasalabeledmultidigraphandhave chosenarelationaldatabaseforitsimplementation.None ofthesequerysystemsmodelsTwittersocialnetworkwith followingorretweetrelationshipsamongusers.Doytsher et al. [31]implementtheiralgebraicqueryoperatorswiththe useofbothgraphandarelationaldatabaseastheunderlying datastorage.Theyexperimentallycomparerelationaland ingtimegranularities,todiscovercontextofcollecteddata. Operatorsalsoallowretrievingasubsetoftweetssatisfying thesecomplexconditionsontermassociations.Thisenables theendusertoselectagoodsetofterms(hashtags)that drivethedatacollectionwhichhasadirectimpactonthe qualityoftheresultsgeneratedfromtheanalysis.
 Spatialfeaturesareanotherpropertyoftweetsoftenover-lookedincomplexanalysis.Previouslydiscussedworkuses thelocationattributeasamechanismtofiltertweetsin space.Tocompleteourdiscussionwebrieflyoutlinetwo studiesthatusegeo-spatialpropertiestoperformcomplex analysisusingthelocationattribute.Doytsher etal. [31]in-troducedamodelandquerylanguagesuitedforintegrated dataconnectingasocialnetworkofuserswithaspatialnet-worktoidentifyplacesvisitedfrequently.Edgesnamed life-patterns areusedtoassociatethesocialandspatialnet-works.Differenttimegranularitiescanbeexpressedfor eachvisitedlocationrepresentedbythelife-patternedge. Eventhoughtheimplementationemploysapartiallysyn-theticdataset,itwillbeinterestingtoinvestigatehowthe socio-spatialnetworksandthelife-patternedgesthatare usedtoassociatethespatialandsocialnetworkscanberep-resentedinarealsocialnetworkdatasetwithlocationinfor-mation,suchasTwitter.GeoScope[18]findsinformation trendsbydetectingsignificantcorrelationsamongtrending location-topicpairsinaslidingwindow.Thisgivesriseto theimportanceofcapturingthenotionofspatialinforma-tiontrendsinsocialnetworksinanalysistasks.Real-time detectionofcrisiseventsfromalocationinspace,exhibits thepossiblevalueofGeoscope.Inoneoftheexperiments Twitterisusedasacasestudytodemonstrateitsuseful-ness,whereahashtagischosentorepresentthetopicand cityfromwhichthetweetoriginateschosentocapturethe location. Relational,RDFandGraphsarethemostcommonchoices ofdatarepresentation.Thereisacloseaffiliationinthese datamodelsobservingthat,forinstance,agraphcaneasily correspondtoasetofRDFtriplesorviceversa.Infact, somestudieslikePlachourasandStavrakas[59]haveput forwardtheirdatamodelasalabeledmultidigraphandhave chosenarelationaldatabaseforitsimplementation.None ofthesequerysystemsmodelsTwittersocialnetworkwith followingorretweetrelationshipsamongusers.Doytsher et al. [31]implementtheiralgebraicqueryoperatorswiththe useofbothgraphandarelationaldatabaseastheunderlying datastorage.Theyexperimentallycomparerelationaland vestigatedineachsystemaredepictedintheTable2.Sys-temsthathavemadeprovisionforthereal-timestreaming natureofthetweetsareindicatedinthe Real-time column. Multipleticks( )correspondtoadimensionexploredinde-tail.Notethatthesystemsmarkedwithanasterisk(*)are notimplementedspecificallytargetingtweets,thoughtheir applicationismeaningfulandcanbeextendedtotheTwit-tersphere.Weobservepotentialfordevelopinglanguagesfor queryingtweetsthatincludequeryingbydimensionsthat arenotcapturedbyexistingsystems,especiallythesocial graph. Thereisaneedtoassimilateindividualeffortswiththegoal ofprovidingaunifiedframeworkthatcanbeusedbyre-searchersandpractitionersacrossmanydisciplines.Inte-gratedsolutionsshouldideallyhandletheentireworkflow ofthedataanalysislifecyclefromcollectingthetweetsto presentingtheresultstotheuser.Theliteraturewehave reviewedinprevioussectionsoutlineseffortsthatsupport differentpartsoftheworkflow.Inthissection,wepresent ourpositionwiththeaimofoutliningsignificantcompo-nentsofanintegratedsolutionaddressingthelimitationsof existingsystems.
 Accordingtoareviewofliteratureconductedonthemi-crobloggingplatform[25],themajorityofpublishedwork onTwitterconcentratesontheuserdomainandthemes-sagedomain.TheuserdomainexplorespropertiesofTwit-terusersinthemicrobloggingenvironmentwhilethemes-sagedomaindealswithpropertiesexhibitedbythetweets themselves.Incomparisontotheextentofworkdoneon themicrobloggingplatform,onlyafewinvestigatesthede-velopmentofdatamanagementframeworksandquerylan-guagesthatdescribeandfacilitateprocessingofonlineso-cialnetworkingdata.Inconsequence,thereisopportunity forimprovementinthisareaforfutureresearchaddressing thechallengesindatamanagement.Weelicitthefollowing high-levelcomponentsandenvisageaplatformforTwitter thatencompassessuchcapabilities: Focusedcrawler: Responsibleforretrievalandcollection ofTwitterdatabycrawlingthepubliclyaccessibleTwit-terAPIs.Afocusedcrawlershouldallowtheusertode-fineacampaignwithsuitablefilters,monitoroutputand iterativelycrawlTwitterforlargevolumesofdatauntilits coverageofrelevanttweetsissatisfactory.
 Pre-processor: AshighlightedinSection3.2,thisstage usuallycomprisesofmodulesforpre-processingandinfor-vestigatedineachsystemaredepictedintheTable2.Sys-temsthathavemadeprovisionforthereal-timestreaming natureofthetweetsareindicatedinthe Real-time column. Multipleticks( )correspondtoadimensionexploredinde-tail.Notethatthesystemsmarkedwithanasterisk(*)are notimplementedspecificallytargetingtweets,thoughtheir applicationismeaningfulandcanbeextendedtotheTwit-tersphere.Weobservepotentialfordevelopinglanguagesfor queryingtweetsthatincludequeryingbydimensionsthat arenotcapturedbyexistingsystems,especiallythesocial graph. Thereisaneedtoassimilateindividualeffortswiththegoal ofprovidingaunifiedframeworkthatcanbeusedbyre-searchersandpractitionersacrossmanydisciplines.Inte-gratedsolutionsshouldideallyhandletheentireworkflow ofthedataanalysislifecyclefromcollectingthetweetsto presentingtheresultstotheuser.Theliteraturewehave reviewedinprevioussectionsoutlineseffortsthatsupport differentpartsoftheworkflow.Inthissection,wepresent ourpositionwiththeaimofoutliningsignificantcompo-nentsofanintegratedsolutionaddressingthelimitationsof existingsystems.
 Accordingtoareviewofliteratureconductedonthemi-crobloggingplatform[25],themajorityofpublishedwork onTwitterconcentratesontheuserdomainandthemes-sagedomain.TheuserdomainexplorespropertiesofTwit-terusersinthemicrobloggingenvironmentwhilethemes-sagedomaindealswithpropertiesexhibitedbythetweets themselves.Incomparisontotheextentofworkdoneon themicrobloggingplatform,onlyafewinvestigatesthede-velopmentofdatamanagementframeworksandquerylan-guagesthatdescribeandfacilitateprocessingofonlineso-cialnetworkingdata.Inconsequence,thereisopportunity forimprovementinthisareaforfutureresearchaddressing thechallengesindatamanagement.Weelicitthefollowing high-levelcomponentsandenvisageaplatformforTwitter thatencompassessuchcapabilities: Focusedcrawler: Responsibleforretrievalandcollection ofTwitterdatabycrawlingthepubliclyaccessibleTwit-terAPIs.Afocusedcrawlershouldallowtheusertode-fineacampaignwithsuitablefilters,monitoroutputand iterativelycrawlTwitterforlargevolumesofdatauntilits coverageofrelevanttweetsissatisfactory.
 Pre-processor: AshighlightedinSection3.2,thisstage usuallycomprisesofmodulesforpre-processingandinfor-turnsanarrayoflocations.Anotherinterestingavenueto exploreistheintroductionofa rankingmechanism on thequeryresult.Rankingcriteriamayinvolverelevance, timelinessornetworkattributeslikethereputationofusers inthecaseofasocialgraph.Rankingfunctionsareastan-dardrequirementinthefieldofinformationretrieval[23,39] andstudieslikeSociQL[30]reporttheuseofvisibilityand reputationsmetricstorankresultsgeneratedfromasocial graph.AquerylanguagewithagraphviewoftheTwitter-spherealongwithcapabilitiesforvisualizationsandranking willcertainlybenefittheupcomingdataanalysiseffortsof Twitter.
 Here,wefocusedonthekeyingredientsrequiredforafully developedsolutionanddiscussedimprovementswecanmake onexistingliterature.Inthenextsection,weidentifychal-lengesandresearchissuesinvolved. Tocompleteourdiscussion,inthissectionwesummarize keyresearchissuesindatamanagementandpresenttech-nicalchallengesthatneedtobeaddressedinthecontextof buildingadataanalyticsplatformforTwitter. OnceasuitableTwitterAPIhasbeenidentified,wecan definea campaign withasetofparameters.Thefocused crawlercanbeprogrammedtoretrievealltweetsmatching thequeryofthecampaign.Ifasocialgraphisnecessary, separatemoduleswouldberesponsibletocreatethisnet-workiteratively.Exhaustivelycrawlingalltherelationships betweenTwitterusersisprohibitivegiventherestrictions setbytheTwitterAPI.Henceitisrequiredforthefocused crawlertoprioritizetherelationshipstocrawlbasedonthe impactandimportanceofspecificTwitteraccounts.Inthe casethattheplatformhandlesmultiplecampaignsinpar-allel,thereisaneedtooptimizetheaccesstotheAPI.
 Typically,theimplementationofacrawlershouldaimto minimizethenumberofAPIrequests,consideringthere-strictions,whilefetchingdataformanycampaignsinparal-lel.Hencebuildinganeffectivecrawlingstrategyisachal-lengingtask,inordertooptimizetheuseofAPIrequests available.
 Appropriatecoverageofthecampaignisanothersignificant concernanddenoteswhetheralltherelevantinformationhas beencollected.Whenspecifyingtheparameterstodefine thecampaign,auserneedsaverygoodknowledgeonthe relevantkeywords.Dependingonthespecifiedkeywords, acollectionmaymissrelevanttweetsinadditiontothe tweetsremovedduetorestrictionsbyAPIs.Plachourasand Stavrakaswork[60]isaninitialstepinthisdirectionasitin-vestigatesthisnotionofcoverageandproposesmechanisms toautomaticallyadaptthecampaigntoevolvinghashtags. Manyproblemsassociatedwithsummarization,topicde-tectionandpart-of-speech(POS)tagging,inthecaseof well-formeddocuments,e.g.newsarticles,havebeenex-tensivelystudiedintheliterature.Traditionalnamedentity recognizers(NERs)heavilydependonlocallinguisticfea-tures[62]ofwell-formeddocumentslikecapitalizationand POStaggingofpreviouswords.Noneofthecharacteristics holdfortweetswithshortutterancesoftweetslimitedto140 turnsanarrayoflocations.Anotherinterestingavenueto exploreistheintroductionofa rankingmechanism on thequeryresult.Rankingcriteriamayinvolverelevance, timelinessornetworkattributeslikethereputationofusers inthecaseofasocialgraph.Rankingfunctionsareastan-dardrequirementinthefieldofinformationretrieval[23,39] andstudieslikeSociQL[30]reporttheuseofvisibilityand reputationsmetricstorankresultsgeneratedfromasocial graph.AquerylanguagewithagraphviewoftheTwitter-spherealongwithcapabilitiesforvisualizationsandranking willcertainlybenefittheupcomingdataanalysiseffortsof Twitter.
 Here,wefocusedonthekeyingredientsrequiredforafully developedsolutionanddiscussedimprovementswecanmake onexistingliterature.Inthenextsection,weidentifychal-lengesandresearchissuesinvolved. Tocompleteourdiscussion,inthissectionwesummarize keyresearchissuesindatamanagementandpresenttech-nicalchallengesthatneedtobeaddressedinthecontextof buildingadataanalyticsplatformforTwitter. OnceasuitableTwitterAPIhasbeenidentified,wecan definea campaign withasetofparameters.Thefocused crawlercanbeprogrammedtoretrievealltweetsmatching thequeryofthecampaign.Ifasocialgraphisnecessary, separatemoduleswouldberesponsibletocreatethisnet-workiteratively.Exhaustivelycrawlingalltherelationships betweenTwitterusersisprohibitivegiventherestrictions setbytheTwitterAPI.Henceitisrequiredforthefocused crawlertoprioritizetherelationshipstocrawlbasedonthe impactandimportanceofspecificTwitteraccounts.Inthe casethattheplatformhandlesmultiplecampaignsinpar-allel,thereisaneedtooptimizetheaccesstotheAPI.
 Typically,theimplementationofacrawlershouldaimto minimizethenumberofAPIrequests,consideringthere-strictions,whilefetchingdataformanycampaignsinparal-lel.Hencebuildinganeffectivecrawlingstrategyisachal-lengingtask,inordertooptimizetheuseofAPIrequests available.
 Appropriatecoverageofthecampaignisanothersignificant concernanddenoteswhetheralltherelevantinformationhas beencollected.Whenspecifyingtheparameterstodefine thecampaign,auserneedsaverygoodknowledgeonthe relevantkeywords.Dependingonthespecifiedkeywords, acollectionmaymissrelevanttweetsinadditiontothe tweetsremovedduetorestrictionsbyAPIs.Plachourasand Stavrakaswork[60]isaninitialstepinthisdirectionasitin-vestigatesthisnotionofcoverageandproposesmechanisms toautomaticallyadaptthecampaigntoevolvinghashtags. Manyproblemsassociatedwithsummarization,topicde-tectionandpart-of-speech(POS)tagging,inthecaseof well-formeddocuments,e.g.newsarticles,havebeenex-tensivelystudiedintheliterature.Traditionalnamedentity recognizers(NERs)heavilydependonlocallinguisticfea-tures[62]ofwell-formeddocumentslikecapitalizationand POStaggingofpreviouswords.Noneofthecharacteristics holdfortweetswithshortutterancesoftweetslimitedto140 Oneofthepredominantchallengesisthemanagementof largegraphsthatinevitablyresultsfrommodelingusers, tweetsandtheirpropertiesasgraphs.Withthelargevolume ofdatainvolvedinanypracticaltask,adatamodelshould beinformationrich,yetaconciserepresentationthaten-ablesexpressionofusefulqueries.Queriesongraphsshould beoptimizedforlargenetworksandshouldideallyrunin-dependentofthesizeofthegraph.Therearealreadyap-proachesthatinvestigateefficientalgorithmsonverylarge graphs[41 X 43,66].Efficientencodingandindexingmecha-nismsshouldbeinplacetakingintoaccountvariationsofin-dexingsystemsalreadyproposedfortweets[23]andindexing ofgraphs[75]ingeneral.Weneedtoconsidermaintaining indexesfortweets,keywords,users,hashtagsforefficientac-cessofdatainadvancequeries.Incertainsituationsitmay beimpracticaltostoretheentirerawtweetandthecom-pleteusergraphanditmaybedesirabletoeithercompress ordropportionsofthedata.Itisimportanttoinvestigate whichpropertiesoftweetsandthegraphshouldbecom-pressed.
 Besidestheabovechallenges,tweetsimposegeneralresearch issuesrelatedtobigdata.Challengesshouldbeaddressed inthesamespiritasanyotherbigdataanalyticstask.In thefaceofchallengesposedbylargevolumesofdatabeing collected,theNoSQLparadigmshouldbeconsideredasan obviouschoiceofdealingwiththem.Developedsolutions shouldbeextensibleforupcomingrequirementsandshould indeedscalewell.Whencollectingdata,auserneedtocon-siderscalablecrawlingastherearelargevolumesoftweets received,processedandindexedpersecond.Withrespect toimplementation,itisnecessarytoinvestigateparadigms thatscalewell,likeMapReducewhichisoptimizedforoffline analyticsonlargedatapartitionedonhundredsofmachines. Dependingonthecomplexityofthequeriessupported,it mightbedifficulttoexpressgraphalgorithmsintuitivelyin MapReducegraphmodels,consequentlydatabasessuchas Titan[4],DEX[3],andNeo4j[2]shouldbecomparedfor graphimplementations. Inthispaperweaddressedissuesaroundthebigdatanature ofTwitteranalyticsandtheneedfornewdatamanagement andquerylanguageframeworks.Byconductingacareful andextensivereviewoftheexistingliteratureweobserved waysinwhichresearchershavetriedtodevelopgeneralplat-formstoprovidearepeatablefoundationforTwitterdata analytics.Wereviewedthetweetanalyticsspacebyexplor-ingmechanismsprimarilyfordatacollection,datamanage-mentandlanguagesforqueryingandanalyzingtweets.We haveidentifiedtheessentialingredientsrequiredforaunified frameworkthataddressthelimitationsofexistingsystems. Thepaperoutlinesresearchissuesandidentifiessomeof thechallengesassociatedwiththedevelopmentofsuchin-tegratedplatforms. [1]FaceBookQueryLanguage(FQL)overview. [2]Neo4j:Theworld X  X leadinggraphdatabase. http:// Oneofthepredominantchallengesisthemanagementof largegraphsthatinevitablyresultsfrommodelingusers, tweetsandtheirpropertiesasgraphs.Withthelargevolume ofdatainvolvedinanypracticaltask,adatamodelshould beinformationrich,yetaconciserepresentationthaten-ablesexpressionofusefulqueries.Queriesongraphsshould beoptimizedforlargenetworksandshouldideallyrunin-dependentofthesizeofthegraph.Therearealreadyap-proachesthatinvestigateefficientalgorithmsonverylarge graphs[41 X 43,66].Efficientencodingandindexingmecha-nismsshouldbeinplacetakingintoaccountvariationsofin-dexingsystemsalreadyproposedfortweets[23]andindexing ofgraphs[75]ingeneral.Weneedtoconsidermaintaining indexesfortweets,keywords,users,hashtagsforefficientac-cessofdatainadvancequeries.Incertainsituationsitmay beimpracticaltostoretheentirerawtweetandthecom-pleteusergraphanditmaybedesirabletoeithercompress ordropportionsofthedata.Itisimportanttoinvestigate whichpropertiesoftweetsandthegraphshouldbecom-pressed.
 Besidestheabovechallenges,tweetsimposegeneralresearch issuesrelatedtobigdata.Challengesshouldbeaddressed inthesamespiritasanyotherbigdataanalyticstask.In thefaceofchallengesposedbylargevolumesofdatabeing collected,theNoSQLparadigmshouldbeconsideredasan obviouschoiceofdealingwiththem.Developedsolutions shouldbeextensibleforupcomingrequirementsandshould indeedscalewell.Whencollectingdata,auserneedtocon-siderscalablecrawlingastherearelargevolumesoftweets received,processedandindexedpersecond.Withrespect toimplementation,itisnecessarytoinvestigateparadigms thatscalewell,likeMapReducewhichisoptimizedforoffline analyticsonlargedatapartitionedonhundredsofmachines. Dependingonthecomplexityofthequeriessupported,it mightbedifficulttoexpressgraphalgorithmsintuitivelyin MapReducegraphmodels,consequentlydatabasessuchas Titan[4],DEX[3],andNeo4j[2]shouldbecomparedfor graphimplementations. Inthispaperweaddressedissuesaroundthebigdatanature ofTwitteranalyticsandtheneedfornewdatamanagement andquerylanguageframeworks.Byconductingacareful andextensivereviewoftheexistingliteratureweobserved waysinwhichresearchershavetriedtodevelopgeneralplat-formstoprovidearepeatablefoundationforTwitterdata analytics.Wereviewedthetweetanalyticsspacebyexplor-ingmechanismsprimarilyfordatacollection,datamanage-mentandlanguagesforqueryingandanalyzingtweets.We haveidentifiedtheessentialingredientsrequiredforaunified frameworkthataddressthelimitationsofexistingsystems. Thepaperoutlinesresearchissuesandidentifiessomeof thechallengesassociatedwiththedevelopmentofsuchin-tegratedplatforms. [1]FaceBookQueryLanguage(FQL)overview. [2]Neo4j:Theworld X  X leadinggraphdatabase. http:// [19]C.Byun,H.Lee,Y.Kim,andK.K.Kim.Twitter [20]S.Carter,W.Weerkamp,andM.Tsagkias.Microblog [21]M.Cha,H.Haddadi,F.Benevenuto,andK.P.Gum-[22]S.Chandra,L.Khan,andF.B.Muhaya.Estimating [23]C.Chen,F.Li,C.Ooi,andS.Wu.TI:Anefficient [24]Z.Cheng,J.Caverlee,K.Lee,andC.Science.A [25]M.CheongandS.Ray.Aliteraturereviewofrecent [26]Chew,Cynthia,andG.Eysenbach.Pandemicsinthe [27]B.O.Connor,N.A.Smith,andE.P.Xing.Alatent [28]Conover,Michael,J.Ratkiewicz,M.Francisco, [29]J.David.Thatswhatfriendsareforinferringlocation [30]DiegoSerrano,EleniStroulia,DenilsonBarbosaand [31]Y.DoytsherandB.Galon.Queryinggeo-socialdataby [32]A.Dries,S.Nijssen,andL.DeRaedt.Aquerylanguage [33]M.Efron.Hashtagretrievalinamicrobloggingenviron-[19]C.Byun,H.Lee,Y.Kim,andK.K.Kim.Twitter [20]S.Carter,W.Weerkamp,andM.Tsagkias.Microblog [21]M.Cha,H.Haddadi,F.Benevenuto,andK.P.Gum-[22]S.Chandra,L.Khan,andF.B.Muhaya.Estimating [23]C.Chen,F.Li,C.Ooi,andS.Wu.TI:Anefficient [24]Z.Cheng,J.Caverlee,K.Lee,andC.Science.A [25]M.CheongandS.Ray.Aliteraturereviewofrecent [26]Chew,Cynthia,andG.Eysenbach.Pandemicsinthe [27]B.O.Connor,N.A.Smith,andE.P.Xing.Alatent [28]Conover,Michael,J.Ratkiewicz,M.Francisco, [29]J.David.Thatswhatfriendsareforinferringlocation [30]DiegoSerrano,EleniStroulia,DenilsonBarbosaand [31]Y.DoytsherandB.Galon.Queryinggeo-socialdataby [32]A.Dries,S.Nijssen,andL.DeRaedt.Aquerylanguage [33]M.Efron.Hashtagretrievalinamicrobloggingenviron-[50]M.S.Mart  X  X nandC.Gutierrez.Representing,querying [51]P.T.W.MauroSanMart  X  X n,ClaudioGutierrez.SNQL [52]M.McglohonandC.Faloutsos.Statisticalproperties [53]P.Mendes,A.Passant,andP.Kapanipathi.Twarql: [54]F.Morstatter,S.Kumar,H.Liu,andR.Maciejew-[55]P.Noordhuis,M.Heijkoop,andA.Lazovik.Mining [56]I.Ounis,C.Macdonald,J.Lin,andI.Soboroff.
 [57]A.PakandP.Paroubek.Twitterasacorpusforsen-[58]Paul,M.J,andM.Dredze.In ICWSM ,pages265 X 272. [59]V.PlachourasandY.Stavrakas.Queryingtermasso-[60]V.Plachouras,Y.Stavrakas,andA.Andreou.Assess-[61]D.Preotiuc-Pietro,S.Samangooei,andT.Cohn.
 [62]L.RatinovandD.Roth.Designchallengesandmiscon-[50]M.S.Mart  X  X nandC.Gutierrez.Representing,querying [51]P.T.W.MauroSanMart  X  X n,ClaudioGutierrez.SNQL [52]M.McglohonandC.Faloutsos.Statisticalproperties [53]P.Mendes,A.Passant,andP.Kapanipathi.Twarql: [54]F.Morstatter,S.Kumar,H.Liu,andR.Maciejew-[55]P.Noordhuis,M.Heijkoop,andA.Lazovik.Mining [56]I.Ounis,C.Macdonald,J.Lin,andI.Soboroff.
 [57]A.PakandP.Paroubek.Twitterasacorpusforsen-[58]Paul,M.J,andM.Dredze.In ICWSM ,pages265 X 272. [59]V.PlachourasandY.Stavrakas.Queryingtermasso-[60]V.Plachouras,Y.Stavrakas,andA.Andreou.Assess-[61]D.Preotiuc-Pietro,S.Samangooei,andT.Cohn.
 [62]L.RatinovandD.Roth.Designchallengesandmiscon-Tumblr, as one of the most popular microblogging platforms, has gained momentum recently. It is reported to have 166.4 millions of users and 73.4 billions of posts by January 2014. While many arti-cles about Tumblr have been published in major press, there is not much scholar work so far. In this paper, we provide some pioneer analysis on Tumblr from a variety of aspects. We study the social network structure among Tumblr users, analyze its user generated content, and describe reblogging patterns to analyze its user be-havior. We aim to provide a comprehensive statistical overview of Tumblr and compare it with other popular social services, including blogosphere, Twitter and Facebook, in answering a couple of key questions: What is Tumblr? How is Tumblr different from other social media networks? In short, we find Tumblr has more rich content than other microblogging platforms, and it contains hybrid characteristics of social networking, traditional blogosphere, and social media. This work serves as an early snapshot of Tumblr that later work can leverage. Tumblr, as one of the most prevalent microblogging sites, has be-come phenomenal in recent years, and it is acquired by Yahoo! in 2013. By mid-January 2014, Tumblr has 166.4 millions of users social site among young generation, as half of Tumblr X  X  visitor are sites in United States, which is the 2nd most dominant blogging site, the 2nd largest microblogging service, and the 5th most preva-recent press, little academic research has been conducted over this burgeoning social service. Naturally questions arise: What is Tum-blr? What is the difference between Tumblr and other blogging or social media sites? have high quality content but little social interactions. Nardi et al. [17] investigated blogging as a form of personal communica-tion and expression, and showed that the vast majority of blog posts are written by ordinary people with a small audience. On the con-Both Twitter and Tumblr are considered as microblogging plat-forms. Comparing with Twitter, Tumblr exposes several differ-ences: Specifically, Tumblr defines 8 types of posts: photo , text , quote , audio , video , chat , link and answer . As shown in Figure 1, one has the flexibility to start a post in any type except answer . Text , photo , audio , video and link allow one to post, share and comment any multimedia content. Quote and chat , which are not available in most other social networking platforms, let Tumblr users share quote or chat history from ichat or msn. Answer occurs only when one tries to interact with other users: when one user posts a ques-tion, in particular, writes a post with text box ending with a question mark, the user can enable the option for others to answer the ques-tion, which will be disabled automatically after 7 days. A post can also be reblogged by another user to broadcast to his own follow-ers. The reblogged post will quote the original post by default and allow the reblogger to add additional comments.
 Figure 2 demonstrates the distribution of Tumblr post types, based on 586.4 million posts we collected. As seen in the figure, even though all kinds of content are supported, photo and text dominate the distribution, accounting for more than 92% of the posts. There-fore, we will concentrate on these two types of posts for our content analysis later.
 Since Tumblr has a strong presence of photos, it is natural to com-Flicker users can add contact, comment or like others X  photos. Yet, different from Tumblr, one cannot reblog another X  X  photo in Flickr. Pinterest is designed for curators, allowing one to share photos or videos of her taste with the public. Pinterest links a pin to the commercial website where the product presented in the pin can be purchased, which accounts for a stronger e-commerce behavior. Therefore, the target audience of Tumblr and Pinterest are quite different: the majority of users in Tumblr are under age 25, while Pinterest is heavily used by women within age from 25 to 44 [16]. We directly sample a sub-graph snapshot of social network from Tumblr on August 2013, which contains 62.8 million nodes and edges. Within this social graph, 41.40% of nodes have 0 in-degree, and the maximum in-degree of a node is 4.06 million. By con-trast, 12.74% of nodes have 0 out-degree, the maximum out-degree of a node is 155.5k. Top popular Tumblr users include equipo 11 , acteristic of Tumblr: the most popular user has more than 4 million audience, while more than 40% of users are purely audience since they don X  X  have any followers.
 Figure 3(a) demonstrates the distribution of in-degrees in the blue curve and that of out-degrees in the red curve, where y-axis refers to the cumulated density distribution function (CCDF): the proba-bility that accounts have at least k in-degrees or out-degrees, i.e., P ( K&gt; = k ) . It is observed that Tumblr users X  in-degree follows a power-law distribution with exponent  X  2 . 19 , which is quite sim-ilar from the power law exponent of Twitter at  X  2 . 28 [11] or that of traditional blogs at  X  2 . 38 [21]. This also confirms with earlier empirical observation that most social network have a power-law exponent between  X  2 and  X  3 [6].
 In regard to out-degree distribution, we notice the red curve has a big drop when out-degree is around 5000, since there was a limit that ordinary Tumblr users can follow at most 5000 other users. Tumblr users X  out-degree does not follow a power-law distribution, which is similar to blogosphere of traditional blogging [21]. If we explore user X  X  in-degree and out-degree together, we could generate normalized 3-D histogram in Figure 3(b). As both in-degree and out-degree follow the heavy-tail distribution, we only zoom in those user who have less than 2 10 in-degrees and out-degrees. Apparently, there is a positive correlation between in-degree and out-degree because of the dominance of diagonal bars. In aggregation, a user with low in-degree tends to have low out-degree as well, even though some nodes, especially those top pop-ular ones, have very imbalanced in-degree and out-degree. Reciprocity. Since Tumblr is a directed network, we would like to examine the reciprocity of the graph. We derive the backbone of the Tumblr network by keeping those reciprocal connections only, i.e., user a follows b and vice versa. Let r-graph denote the correspond-ing reciprocal graph. We found 29.03% of Tumblr user pairs have reciprocity relationship, which is higher than 22.1% of reciprocity on Twitter [11] and 3% of reciprocity on Blogosphere [21], indicat-ing a stronger interaction between users in the network. Figure 3(c) shows the distribution of degrees in the r-graph. There is a turning As Tumblr is initially proposed for the purpose of blogging, here we analyze its user generated contents. As described earlier, photo and text posts account for more than 92% of total posts. Hence, we concentrate only on these two types of posts. One text post may contain URL, quote or raw message. In this study, we are mainly interested in the authentic contents generated by users. Hence, we extract raw messages as the content information of each text post, by removing quotes and URLs. Similarly, photo posts contains 3 categories of information: photo URL, quote photo caption, raw photo caption. While the photo URL might contain lots of addi-tional meta information, it would require tremendous effort to ana-lyze all images in Tumblr. Hence, we focus on raw photo captions as the content of each photo post. We end up with two datasets of content: one is text post , and the other is photo caption . What X  X  the effect of no length limit for post? Both Tumblr and Twitter are considered microblogging platforms, yet there is one Topic Topical Keywords Pop music song listen iframe band album lyrics Music video guitar
Sports game play team win video cookie
Internet internet computer laptop google search online
Pets big dog cat animal pet animals bear tiny
Medical anxiety pain hospital mental panic cancer
Finance money pay store loan online interest buying lows us to run topic analysis over the two datasets to have an overview of the content. We run LDA [4] with 100 topics on both datasets, and showcase several topics and their corresponding keywords on Tables 3 and 4, which also show the high quality of textual content on Tumblr clearly. Medical, Pets, Pop Music, Sports are shared in-terests across 2 different datasets, although representative topical keywords might be different even for the same topic. Finance, In-ternet only attracts enough attentions from text posts, while only significant amount of photo posts show interest to Photography, Scenery topics. We want to emphasize that most of these keywords are semantically meaningful and representative of the topics. Who are the major contributors of contents? There are two po-tential hypotheses. 1) One supposes those socially popular users post more. This is derived from the result that those popular users are followed by many users, therefore blogging is one way to at-tract more audience as followers. Meanwhile, it might be true that blogging is an incentive for celebrities to interact or reward their followers. 2) The other assumes that long-term users (in terms of registration time) post more, since they are accustomed to this ser-vice, and they are more likely to have their own focused commu-nities or social circles. These peer interactions encourage them to generate more authentic content to share with others. Do socially popular users or long-term users generate more con-tents? In order to answer this question, we choose a fixed time window of two weeks in August 2013 and examine how frequent each user blogs on Tumblr. We sort all users based on their in-degree (or duration time since registration) and then partition them into 10 equi-width bins. For each bin, we calculate the average blogging frequency. For easy comparison, we consider the maxi-mal value of all bins as 1, and normalize the relative ratio for other bins. The results are displayed in Figure 6, where x-axis from left to right indicates increasing in-degree (or decreasing duration time). Figure 6: Correlation of Post Frequency with User In-degree or Duration Time since Registration ables information to be propagated through the network. In this section, we examine the reblogging patterns in Tumblr. We exam-ine all blog posts uploaded within the first 2 weeks, and count re-blog events in the subsequent 2 weeks right after the blog is posted, so that there would be no bias because of the time window selection in our blog data.
 Who are reblogging? Firstly, we would like to understand which users tend to reblog more? Those people who reblog frequently serves as the information transmitter. Similar to the previous sec-tion, we examine the correlation of reblogging behavior with users X  in-degree. As shown in the Figure 8, social celebrities, who are the major source of contents, reblog a lot more compared with other users. This reblogging is propagated further through their huge number of followers. Hence, they serve as both content contrib-utor and information transmitter. On the other hand, users who registered earlier reblog more as well. The socially popular and long-term users are the backbone of Tumblr network to make it a vibrant community for information propagation and sharing. Reblog size distribution. Once a blog is posted, it can be re-blogged by others. Those reblogs can be reblogged even further, which leads to a tree structure, which is called reblog cascade, with the first author being the root node. The reblog cascade size indi-cates the number of reblog actions that have been involved in the cascade. Figure 9 plots the distribution of reblog cascade sizes. Not surprisingly, it follows a power-law distribution, with majority Figure 8: Correlation of Reblog Frequency with User In-degree or Duration Time since Registration likely at first glimpse, considering any two users are just few hops away. Indeed, this is because users can add comment while reblog-ging, and thus one user is likely to involve in one reblog cascade multiple times. We notice that some Tumblr users adopt reblog as one way for conversation or chat.
 Reblog Structure Distribution. Since most reblog cascades are few hops, here we show the cascade tree structure distribution up to size 5 in Figure 11. The structures are sorted based on their cov-erage. Apparently, a substantial percentage of cascades ( 36 . 05% ) are of size 2 , i.e., a post being reblogged merely once. Generally speaking, a reblog cascade of a flat structure tends to have a higher probability than a reblog cascade of the same size but with a deep structure. For instance, a reblog cascade of size 3 have two vari-ants, of which the flat one covers 9 . 42% cascade while the deep one drops to 5 . 85% . The same patten applies to reblog cascades of size 4 and 5. In other words, it is easier to spread a message widely rather than deeply in general. This implies that it might be acceptable to consider only the cascade effect under few hops and focus those nodes with larger audience when one tries to maximize influence or information propagation.
 Temporal patten of reblog. We have investigated the information propagation spatially in terms of network topology, now we study how fast for one blog to be reblogged? Figure 12 displays the dis-tribution of time gap between a post and its first reblog. There is a strong bias toward recency. The larger the time gap since a blog content analysis. McGlohon et al. [14] found topology features can help us distinguish blogs, the temporal activity of blogs is very non-uniform and bursty, but it is self-similar. Bakshy et al. [3] investigated the attributes and relative influence based on Twitter follower graph, and concluded that word-of-mouth diffusion can only be harnessed reliably by targeting large numbers of potential influencers, thereby capturing average effects. Hopcroft et al. [9] studied the Twitter user influence based on two-way reciprocal rela-tionship prediction. Weng et al. [23] extended PageRank algorithm to measure the influence of Twitter users, and took both the topi-cal similarity between users and link structure into account. Kwak et al. [11] study the topological and geographical properties on the entire Twittersphere and they observe some notable properties of Twitter, such as a non-power-law follower distribution, a short effective diameter, and low reciprocity, marking a deviation from known characteristics of human social networks.
 However, due to data access limitation, majority of the existing scholar papers are based on either Twitter data or traditional blog-ging data. This work closes the gap by providing the first overview of Tumblr so that others can leverage as a stepstone to investigate more over this evolving social service or compare with other related services. This snapshot research is by no means to be complete. There are several directions to extend this work. First, some patterns de-scribed here are correlations. They do not illustrate the underlying mechanism. It is imperative to differentiate correlation and causal-ity [2] so that we can better understand the user behavior. Secondly, it is observed that Tumblr is very popular among young users, as half of Tumblr X  X  visitor base being under 25 years old. Why is it so? We need to combine content analysis, social network analysis, together with user profiles to figure out. In addition, since more than 70% of Tumblr posts are images, it is necessary to go beyond photo captions, and analyze image content together with other meta information. [1] N. Agarwal, H. Liu, L. Tang, and P. S. Yu. Identifying the in-[2] A. Anagnostopoulos, R. Kumar, and M. Mahdian. Influence [3] E. Bakshy, J. M. Hofman, W. A. Mason, and D. J. Watts. Ev-[4] D. M. Blei, A. Y. Ng, and M. I. Jordan:. Latent dirichlet allo-[5] S. Chang, V. Kumar, E. Gilbert, and L. Terveen. Specializa-[6] A. Clauset, C. R. Shalizi, and M. E. J. Newman. Power-law [7] E. Gilbert, S. Bakhshi, S. Chang, and L. Terveen.  X  X  need to Big Data is identified by its three Vs , namely velocity, volume, and variety. The area of data stream processing has long dealt with the former two Vs velocity and volume. Over a decade of intensive research, the community has provided many important research discoveries in the area. The third V of Big Data has been the result of social media and the large unstructured data it generates. Streaming techniques have also been proposed re-cently addressing this emerging need. However, a hidden factor can represent an important fourth V , that is variability or change. Our world is changing rapidly, and accounting to variability is a crucial success factor. This paper provides a survey of change detection techniques as applied to streaming data. The review is timely with the rise of Big Data technologies, and the need to have this important aspect highlighted and its techniques catego-rized and detailed. Today X  X  world is changing very fast. The changes occur in every aspects of life. Therefore, the ability to detect, adapt, and react to the change play an important role in all aspects of life. The physical world is often represented in some model or some infor-mation system. The changes in the physical world are reflected in terms of the changes in data or model built from data. Therefore, the nature of data is changing.
 The advance of technology results in the data deluge. The data volume is increasing with an estimated rate of 50% per year [39]. Data flood makes traditional methods including traditional dis-tributed framework and parallel models inappropriate for pro-cessing, analyzing, storing, and understanding these massive data sets. Data deluge needs a new generation of computing tools that Jim Gray calls the 4 th paradigm in scientific computing [25]. Re-cently, there have been some emerging computing paradigms that meet the requirements of Big Data as follows. Parallel batch pro-cessing model only deals with the stationary massive data [17]. However, evolving data continuously arrives with high speed. In fact, online data stream processing is the main approach to deal-ing with the problem of three characteristics of Big Data includ-ing big volume, big velocity, and big variety. Streaming data pro-cessing is a model of Big Data processing. Streaming data is tem-poral data in nature. In addition to the temporal nature, streaming data may include spatial characteristics. For example, geographic information systems can produce spatial-temporal data stream. Streaming data processing and mining have been deploying in Big Data is identified by its three Vs , namely velocity, volume, and variety. The area of data stream processing has long dealt with the former two Vs velocity and volume. Over a decade of intensive research, the community has provided many important research discoveries in the area. The third V of Big Data has been the result of social media and the large unstructured data it generates. Streaming techniques have also been proposed re-cently addressing this emerging need. However, a hidden factor can represent an important fourth V , that is variability or change. Our world is changing rapidly, and accounting to variability is a crucial success factor. This paper provides a survey of change detection techniques as applied to streaming data. The review is timely with the rise of Big Data technologies, and the need to have this important aspect highlighted and its techniques catego-rized and detailed. Today X  X  world is changing very fast. The changes occur in every aspects of life. Therefore, the ability to detect, adapt, and react to the change play an important role in all aspects of life. The physical world is often represented in some model or some infor-mation system. The changes in the physical world are reflected in terms of the changes in data or model built from data. Therefore, the nature of data is changing.
 The advance of technology results in the data deluge. The data volume is increasing with an estimated rate of 50% per year [39]. Data flood makes traditional methods including traditional dis-tributed framework and parallel models inappropriate for pro-cessing, analyzing, storing, and understanding these massive data sets. Data deluge needs a new generation of computing tools that Jim Gray calls the 4 th paradigm in scientific computing [25]. Re-cently, there have been some emerging computing paradigms that meet the requirements of Big Data as follows. Parallel batch pro-cessing model only deals with the stationary massive data [17]. However, evolving data continuously arrives with high speed. In fact, online data stream processing is the main approach to deal-ing with the problem of three characteristics of Big Data includ-ing big volume, big velocity, and big variety. Streaming data pro-cessing is a model of Big Data processing. Streaming data is tem-poral data in nature. In addition to the temporal nature, streaming data may include spatial characteristics. For example, geographic information systems can produce spatial-temporal data stream. Streaming data processing and mining have been deploying in Streaming computational model is considered one of the widely-used models for processing and analyzing massive data. Stream-ing data processing helps the decision-making process in real-time. A data stream is defined as follows.

D EFINITION 1. A data stream is an infinite sequence of ele-ments Each element is a pair X j , T j where X j is a d-dimensional vec-is defined over discrete domain with a total order. There are two types of time-stamps: explicit time-stamp is generated when data arrive; implicit time-stamp is assigned by some data stream pro-cessing system.
 Streaming data includes the fundamental characteristics as fol-lows. First, data arrives continuously. Second, streaming data evolves overtime. Third, streaming data is noisy, corrupted. Forth, timely interfering is important. From the characteristics of streaming data and data stream model, data stream process-ing and mining pose the following challenges. First, as streaming data arrives rapidly, the techniques of streaming data process and analysis must keep up with the data rate to prevent from the loss of important information as well as avoid data redundancy. Sec-ond, as the speed of streaming data is very high, the data volume overcomes the processing capacity of the existing systems. Third, the value of data decreases over time, the recent streaming data is sufficient for many applications. Therefore, one can only capture and process the data as soon as it is generated. This section presents concepts and classification of changes and change detection methods. To develop a change detection method, we should understand what a change is.

D EFINITION 2. Change is defined as the difference in the state of an object or phenomenon over time and/or space [52; 1].
 In the view of system, change is the process of transition from a state of a system to another. In other words, a change can be de-fined as the difference between an earlier state and a later state. An important distinction between change and difference is that a change refers to a transition in the state of an object or a phe-nomenon overtime while the difference means the dissimilarity in the characteristics of two objects. A change can reflect the short-term trend or long-term trend. For example, a stock analyst may be interested in the short-term change of the stock price. Change detection is defined as the process of identifying differ-ences in the state of an object or phenomenon by observing it at different times [54]. In the above definition, a change is detected on the basis of differences of an object at different times without considering the differences of an object in locations in space. In many real world applications, changes can occur both in terms of both time and space. For example, multiple spatial-temporal data streams representing triple (latitude, longitude, time) are created in traffic information systems using GPS [23]. Hence, change de-tection can be defined as follows.

D EFINITION 3. Change detection is the process of identify-ing differences in the state of an object or phenomenon by ob-serving it at different times and/or different locations in space. A distinction between concept drift detection and change detec-tion is that concept drift detection focuses on the labeled data while change detection can deal with both labeled and unlabeled Streaming computational model is considered one of the widely-used models for processing and analyzing massive data. Stream-ing data processing helps the decision-making process in real-time. A data stream is defined as follows.

D EFINITION 1. A data stream is an infinite sequence of ele-ments Each element is a pair X j , T j where X j is a d-dimensional vec-is defined over discrete domain with a total order. There are two types of time-stamps: explicit time-stamp is generated when data arrive; implicit time-stamp is assigned by some data stream pro-cessing system.
 Streaming data includes the fundamental characteristics as fol-lows. First, data arrives continuously. Second, streaming data evolves overtime. Third, streaming data is noisy, corrupted. Forth, timely interfering is important. From the characteristics of streaming data and data stream model, data stream process-ing and mining pose the following challenges. First, as streaming data arrives rapidly, the techniques of streaming data process and analysis must keep up with the data rate to prevent from the loss of important information as well as avoid data redundancy. Sec-ond, as the speed of streaming data is very high, the data volume overcomes the processing capacity of the existing systems. Third, the value of data decreases over time, the recent streaming data is sufficient for many applications. Therefore, one can only capture and process the data as soon as it is generated. This section presents concepts and classification of changes and change detection methods. To develop a change detection method, we should understand what a change is.

D EFINITION 2. Change is defined as the difference in the state of an object or phenomenon over time and/or space [52; 1].
 In the view of system, change is the process of transition from a state of a system to another. In other words, a change can be de-fined as the difference between an earlier state and a later state. An important distinction between change and difference is that a change refers to a transition in the state of an object or a phe-nomenon overtime while the difference means the dissimilarity in the characteristics of two objects. A change can reflect the short-term trend or long-term trend. For example, a stock analyst may be interested in the short-term change of the stock price. Change detection is defined as the process of identifying differ-ences in the state of an object or phenomenon by observing it at different times [54]. In the above definition, a change is detected on the basis of differences of an object at different times without considering the differences of an object in locations in space. In many real world applications, changes can occur both in terms of both time and space. For example, multiple spatial-temporal data streams representing triple (latitude, longitude, time) are created in traffic information systems using GPS [23]. Hence, change de-tection can be defined as follows.

D EFINITION 3. Change detection is the process of identify-ing differences in the state of an object or phenomenon by ob-serving it at different times and/or different locations in space. A distinction between concept drift detection and change detec-tion is that concept drift detection focuses on the labeled data while change detection can deal with both labeled and unlabeled Figure 1: A general diagram for detecting changes in data stream ing a data stream into different segments by identifying the points where the stream dynamics changes [53].
 As data streams evolve overtime in nature, there is growing em-phasis on detecting changes not only in the underlying data dis-tribution, but also in the models generated by data stream process and data stream mining. As can be seen in Figure 1, a change can occur in the data stream, or the streaming model. There-fore, there are two types of the problems of change detection: change detection in the data generating process and change detec-tion in the model generated by a data stream processing, or min-ing. The fundamental issues of detecting changes in data streams includes characterizing and quantifying of changes and detect-ing changes. A change detection method in streaming data needs a trade-off among space-efficiency, detection performance, and time-efficiency. Over the last 50 years, change detection has been widely studied and applied in both academic research and industry. For exam-ple, it has been studied for a long time in the following fields: statistics, signal processing, and control theory. In recent years many change detection methods have been proposed for stream-ing data. The approaches to detecting changes in data stream can be classified as follows. Figure 1: A general diagram for detecting changes in data stream ing a data stream into different segments by identifying the points where the stream dynamics changes [53].
 As data streams evolve overtime in nature, there is growing em-phasis on detecting changes not only in the underlying data dis-tribution, but also in the models generated by data stream process and data stream mining. As can be seen in Figure 1, a change can occur in the data stream, or the streaming model. There-fore, there are two types of the problems of change detection: change detection in the data generating process and change detec-tion in the model generated by a data stream processing, or min-ing. The fundamental issues of detecting changes in data streams includes characterizing and quantifying of changes and detect-ing changes. A change detection method in streaming data needs a trade-off among space-efficiency, detection performance, and time-efficiency. Over the last 50 years, change detection has been widely studied and applied in both academic research and industry. For exam-ple, it has been studied for a long time in the following fields: statistics, signal processing, and control theory. In recent years many change detection methods have been proposed for stream-ing data. The approaches to detecting changes in data stream can be classified as follows. The first work on model-based change detection proposed by [21; 22] is FOCUS. The central idea behind FOCUS is that the models The first work on model-based change detection proposed by [21; 22] is FOCUS. The central idea behind FOCUS is that the models reference is initialized with the first batch of transactions from data stream. The current window moves on the data stream and captures the next batch of transactions. Two frequent item sets are constructed from two corresponding windows by using the Apriori algorithm. A statistical test is performed on two absolute support values that are computed by the Apriori from the refer-ence window and current window. Based on the statistical test, the deviation can be significant or insignificant. If the deviation is significant then a change in the data stream is reported. Chang and Lee [8] have presented a method for monitoring the recent change of frequent item sets from data stream by using sliding window. There are two design methodologies for developing the change detection algorithms in streaming data. The first methodology is to adapt the existing change detection methods for streaming data. However, many traditional change detection methods can-not be extended for streaming data because of the high compu-tational complexity such as some kernel-based change detection methods, and density-based change detection methods. The sec-ond methodology is to develop new change detection methods for streaming data.
 There are two common approaches to the problem of change de-tection in streaming data distributions: distance-based change de-tectors and predictive model-based change detectors. In the for-mer, two windows are used to extract two data segments from the data stream. The change is quantified by using some dissimilar-ity measure. If the dissimilarity measure is greater than a given threshold then a change is detected. Similar to distance-based change detectors, two windows are used for detecting changes. Instead of comparing the dissimilarity measure between two win-dows with a given threshold, a change is detected by using the prediction error of the model built from the current window and the predictive model constructed from the reference window. Knowledge discovery from massive amount of streaming data can be achieved only when we could develop the change detec-tion frameworks that monitor streaming data created by multiple sources such as sensor networks, WWW [13]. The objectives of designing a distributed change detection scheme are maximizing the lifetime of the network, maximizing the detection capability, and minimizing the communication cost [58].
 There are two approaches to the problem of change detection in streaming data that is created from multiple sources. In the cen-tralized approach: all remote sites send raw data to the coordi-nator. The coordinator aggregates all the raw streaming data that is received from the remote sites. Detection of changes is per-formed on the aggregated streaming data. In most cases, com-munication consumes the largest amount of energy. The lifetime of sensors therefore drastically reduces when they communicate raw measurements to a centralized server for analysis. Central-ized approaches suffer from the following problems: communi-cation constraint, power consumption, robustness, and privacy. Distributed detection of changes in streaming data addresses the challenges that come from the problem of change detection, data stream processing, and the problem of distributed computing. The challenges coming from the distributed computing environ-ment are as follows reference is initialized with the first batch of transactions from data stream. The current window moves on the data stream and captures the next batch of transactions. Two frequent item sets are constructed from two corresponding windows by using the Apriori algorithm. A statistical test is performed on two absolute support values that are computed by the Apriori from the refer-ence window and current window. Based on the statistical test, the deviation can be significant or insignificant. If the deviation is significant then a change in the data stream is reported. Chang and Lee [8] have presented a method for monitoring the recent change of frequent item sets from data stream by using sliding window. There are two design methodologies for developing the change detection algorithms in streaming data. The first methodology is to adapt the existing change detection methods for streaming data. However, many traditional change detection methods can-not be extended for streaming data because of the high compu-tational complexity such as some kernel-based change detection methods, and density-based change detection methods. The sec-ond methodology is to develop new change detection methods for streaming data.
 There are two common approaches to the problem of change de-tection in streaming data distributions: distance-based change de-tectors and predictive model-based change detectors. In the for-mer, two windows are used to extract two data segments from the data stream. The change is quantified by using some dissimilar-ity measure. If the dissimilarity measure is greater than a given threshold then a change is detected. Similar to distance-based change detectors, two windows are used for detecting changes. Instead of comparing the dissimilarity measure between two win-dows with a given threshold, a change is detected by using the prediction error of the model built from the current window and the predictive model constructed from the reference window. Knowledge discovery from massive amount of streaming data can be achieved only when we could develop the change detec-tion frameworks that monitor streaming data created by multiple sources such as sensor networks, WWW [13]. The objectives of designing a distributed change detection scheme are maximizing the lifetime of the network, maximizing the detection capability, and minimizing the communication cost [58].
 There are two approaches to the problem of change detection in streaming data that is created from multiple sources. In the cen-tralized approach: all remote sites send raw data to the coordi-nator. The coordinator aggregates all the raw streaming data that is received from the remote sites. Detection of changes is per-formed on the aggregated streaming data. In most cases, com-munication consumes the largest amount of energy. The lifetime of sensors therefore drastically reduces when they communicate raw measurements to a centralized server for analysis. Central-ized approaches suffer from the following problems: communi-cation constraint, power consumption, robustness, and privacy. Distributed detection of changes in streaming data addresses the challenges that come from the problem of change detection, data stream processing, and the problem of distributed computing. The challenges coming from the distributed computing environ-ment are as follows framework for mining streaming data should be robust to net-work partitions, and node failures.
 The advantage of local approaches is the ability to preserve pri-vacy [20]. A drawback of the local approach to the problem of distributed change detection is the synchronization problem. For example, the local change approach can meet the principle of lo-calized algorithms in wireless sensor networks in which data pro-cessing is performed at node-level as much as possible in order to reduce the amount of information to be sent in the network. Over the last decades, the problem of decentralized detection has received much attention. There are two directions of research on decentralized detection. The first approach focuses on aggre-gating measurements from multiple sensors to test a single hy-pothesis. The second focuses on dealing with multiple dependent testing/estimation tasks from multiple sensors [51]. Distributed change detection usually involves a set of sensors that receive observations from the environment and then transmit those obser-vations back to fusion center in order to reach the final consensus of detection. Decentralized detection and data fusion are there-fore two closely related tasks that arise in the context of sensor networks [48; 47]. Two traditional approaches to the decentral-ized change detection are data fusion, and decision fusion. In data fusion, each node detects change and sends quantized version of its observation to a fusion center responsible for making deci-sion on the detected changes, and further relaying information. In contrast, in decision fusion, each node performs local change detection by using some local change algorithm and updates its decision based on the received information and broadcasts again its new decision. This process repeats until consensus among the nodes are reached. Compared to data fusion, decision fusion can reduce the communication cost because sensors need only to transmit the local decisions represented by small data structures. Although there is great deal of work on distributed detection and data fusion, most of work focuses on the one-time change detec-tion solutions. One-time query is defined as a query that needs to proceed data once in order to provide the answer [12]. Likewise, one-time change detection method is a change detection that re-quires to proceed data once in response to the change occurred. In real-world applications, we need the approaches capable of con-tinuously monitoring the changes of the events occurring in the environment. Recently, work on continuous detection and mon-itoring of changes has been started receiving attention such as [49; 13; 50]. Das et al. [13] have presented a scalable distributed framework for detecting changes in astronomy data streams us-ing local, asynchronous eigen monitoring algorithms. Palpanas et al. [49] proposed a distributed framework for outlier detection in real-time data streams. In their framework, each sensor esti-mates and maintains a model for its underlying distribution by using kernel density estimators. However, they did not show how to reach the global detection decision. We argued in this paper that variability, or simply change, is cru-cial in a world full of affecting factors that alter the behavior of the data, and consequently the underlying model. The ability to detect such changes in centralized as well as distributed system plays an important role in identifying validity of data models. The paper presented the state-of-the-art in this area of paramount importance. Techniques, in some cases, are tightly coupled with application domains. However, most of the techniques reviewed in this paper are generic and could be adapted to different do-mains of applications.
 With Big Data technologies reaching a mature stage, the future framework for mining streaming data should be robust to net-work partitions, and node failures.
 The advantage of local approaches is the ability to preserve pri-vacy [20]. A drawback of the local approach to the problem of distributed change detection is the synchronization problem. For example, the local change approach can meet the principle of lo-calized algorithms in wireless sensor networks in which data pro-cessing is performed at node-level as much as possible in order to reduce the amount of information to be sent in the network. Over the last decades, the problem of decentralized detection has received much attention. There are two directions of research on decentralized detection. The first approach focuses on aggre-gating measurements from multiple sensors to test a single hy-pothesis. The second focuses on dealing with multiple dependent testing/estimation tasks from multiple sensors [51]. Distributed change detection usually involves a set of sensors that receive observations from the environment and then transmit those obser-vations back to fusion center in order to reach the final consensus of detection. Decentralized detection and data fusion are there-fore two closely related tasks that arise in the context of sensor networks [48; 47]. Two traditional approaches to the decentral-ized change detection are data fusion, and decision fusion. In data fusion, each node detects change and sends quantized version of its observation to a fusion center responsible for making deci-sion on the detected changes, and further relaying information. In contrast, in decision fusion, each node performs local change detection by using some local change algorithm and updates its decision based on the received information and broadcasts again its new decision. This process repeats until consensus among the nodes are reached. Compared to data fusion, decision fusion can reduce the communication cost because sensors need only to transmit the local decisions represented by small data structures. Although there is great deal of work on distributed detection and data fusion, most of work focuses on the one-time change detec-tion solutions. One-time query is defined as a query that needs to proceed data once in order to provide the answer [12]. Likewise, one-time change detection method is a change detection that re-quires to proceed data once in response to the change occurred. In real-world applications, we need the approaches capable of con-tinuously monitoring the changes of the events occurring in the environment. Recently, work on continuous detection and mon-itoring of changes has been started receiving attention such as [49; 13; 50]. Das et al. [13] have presented a scalable distributed framework for detecting changes in astronomy data streams us-ing local, asynchronous eigen monitoring algorithms. Palpanas et al. [49] proposed a distributed framework for outlier detection in real-time data streams. In their framework, each sensor esti-mates and maintains a model for its underlying distribution by using kernel density estimators. However, they did not show how to reach the global detection decision. We argued in this paper that variability, or simply change, is cru-cial in a world full of affecting factors that alter the behavior of the data, and consequently the underlying model. The ability to detect such changes in centralized as well as distributed system plays an important role in identifying validity of data models. The paper presented the state-of-the-art in this area of paramount importance. Techniques, in some cases, are tightly coupled with application domains. However, most of the techniques reviewed in this paper are generic and could be adapted to different do-mains of applications.
 With Big Data technologies reaching a mature stage, the future [14] T. Dasu, S. Krishnan, D. Lin, S. Venkatasubramanian, and [15] T. Dasu, S. Krishnan, S. Venkatasubramanian, and K. Yi. [16] S. Datta, K. Bhaduri, C. Giannella, R. Wolff, and H. Kar-[17] J. Dean and S. Ghemawat. Mapreduce: Simplified data pro-[18] N. Dindar, P. M. Fischer, M. Soner, and N. Tatbul. Effi-[19] G. Dong, J. Han, L. Lakshmanan, J. Pei, H. Wang, and [20] A. R. Ganguly, J. Gama, O. A. Omitaomu, M. M. Gaber, [21] V. Ganti, J. Gehrke, and R. Ramakrishnan. A framework for [22] V. Ganti, J. Gehrke, R. Ramakrishnan, and W. Loh. A [23] S. Geisler, C. Quix, and S. Schiffer. A data stream-based [24] L. Golab, T. Johnson, J. S. Seidel, and V. Shkapenyuk. [25] A. J. Hey, S. Tansley, and K. M. Tolle. The fourth [26] S. Hido, T. Id X , H. Kashima, H. Kubo, and H. Matsuzawa. [27] S. Ho and H. Wechsler. Detecting changes in unlabeled data [28] W. Huang, E. Omiecinski, and L. Mark. Evolution in Data [14] T. Dasu, S. Krishnan, D. Lin, S. Venkatasubramanian, and [15] T. Dasu, S. Krishnan, S. Venkatasubramanian, and K. Yi. [16] S. Datta, K. Bhaduri, C. Giannella, R. Wolff, and H. Kar-[17] J. Dean and S. Ghemawat. Mapreduce: Simplified data pro-[18] N. Dindar, P. M. Fischer, M. Soner, and N. Tatbul. Effi-[19] G. Dong, J. Han, L. Lakshmanan, J. Pei, H. Wang, and [20] A. R. Ganguly, J. Gama, O. A. Omitaomu, M. M. Gaber, [21] V. Ganti, J. Gehrke, and R. Ramakrishnan. A framework for [22] V. Ganti, J. Gehrke, R. Ramakrishnan, and W. Loh. A [23] S. Geisler, C. Quix, and S. Schiffer. A data stream-based [24] L. Golab, T. Johnson, J. S. Seidel, and V. Shkapenyuk. [25] A. J. Hey, S. Tansley, and K. M. Tolle. The fourth [26] S. Hido, T. Id X , H. Kashima, H. Kubo, and H. Matsuzawa. [27] S. Ho and H. Wechsler. Detecting changes in unlabeled data [28] W. Huang, E. Omiecinski, and L. Mark. Evolution in Data [45] W. Ng and M. Dash. A test paradigm for detecting changes [46] D. Nikovski and A. Jain. Fast adaptive algorithms for abrupt [47] R. Niu and P. K. Varshney. Performance analysis of dis-[48] R. Niu, P. K. Varshney, and Q. Cheng. Distributed detec-[49] T. Palpanas, D. Papadopoulos, V. Kalogeraki, and [50] D.-S. Pham, S. Venkatesh, M. Lazarescu, and S. Budha-[51] R. Rajagopal, X. Nguyen, S. C. Ergen, and P. Varaiya. [52] J. Roddick, L. Al-Jadir, L. Bertossi, M. Dumas, [45] W. Ng and M. Dash. A test paradigm for detecting changes [46] D. Nikovski and A. Jain. Fast adaptive algorithms for abrupt [47] R. Niu and P. K. Varshney. Performance analysis of dis-[48] R. Niu, P. K. Varshney, and Q. Cheng. Distributed detec-[49] T. Palpanas, D. Papadopoulos, V. Kalogeraki, and [50] D.-S. Pham, S. Venkatesh, M. Lazarescu, and S. Budha-[51] R. Rajagopal, X. Nguyen, S. C. Ergen, and P. Varaiya. [52] J. Roddick, L. Al-Jadir, L. Bertossi, M. Dumas, Most data analytics applications are industry/domain specific, e.g., predicting patients at high risk of being admitted to intensive care unit in the healthcare sector or predicting malicious SMSs in the telecommunication sector. Existing solutions are based on  X  X est practices X , i.e., the systems X  decisions are knowledge-driven and/or data-driven . However, there are rules and exceptional cases that can only be precisely formulated and identified by subject-matter experts (SMEs) who have accumulated many years of experience. This paper envisions a more intelligent database management system (DBMS) that captures such knowledge to effectively address the industry/domain specific applications. At the core, the system is a hybrid human-machine database engine where the machine interacts with the SMEs as part of a feedback loop to gather, infer, ascertain and enhance the database knowledge and processing. We discuss the challenges towards building such a system through examples in healthcare predictive analysis  X  a popular area for big data analytics. Most data analytics applications are industry or domain specific. For example, many prediction tasks in healthcare require prior medical knowledge, such as, identifying patients at high risk of being admitted to the intensive care unit, or predicting the probability of the patients being readmitted into the hospital within 30 days after discharge. Another example from the telecommunication sector is the identification of malicious SMSs requiring inputs from security experts. Building competent tools to effectively address these problems are important, as industrial organizations face increasing pressures to improve outcomes while reducing costs [3].
 Existing solutions to industry or domain specific tasks are based on  X  X est practices X . These solutions are knowledge-driven (i.e., utilizing general guidelines such existing clinical guidelines or literature from medical journals) and/or data-driven (i.e., deriving rules from observational data) [31]. Let us consider the task of identifying the risk factors related to heart failure. The knowledge-driven solution uses risk factors identified from existing clinical knowledge or literature, such as, age, hypertension and diabetes status. However, it may miss out other unknown risk factors specific to the population of interest. The reason is that the guidelines are generic and based on existing knowledge, which results in models that may not adequately represent the underlying complex disease processes in the population with a comprehensive list of risk factors [31]. The be provided by internal domain experts. In contrast, experts in Data Tamer are not the users of the system and hence there is a need to customize/localize the system for different use-cases. In order to entrench the crowd intelligence into the DBMS, the system needs to keep SMEs as part of the feedback loop. The system can then further utilize feedback provided from the SMEs to infer, ascertain and enhance its processing, thus continuously improving the effectiveness of the system. For example, when predicting the risk of unplanned patient readmissions, the system asks the doctors to label patients who the system has low confidence in predicting their readmissions, and the rules/hypotheses that the doctors used to do the labeling. One example of such an expert rule is that an elderly patient who lives alone and have had several severe diseases is likely to be readmitted into the hospital frequently. The system would then verify or adjust these rules/hypotheses and revert back to the doctors with evidence to support or reject their rules/hypotheses. Such interactions are beneficial to both the system and the doctors. Eventually, the application system evolves over time. SMEs become part of this evolving process by sharing their domain knowledge and rich experience, thereby contributing to the improvement and development of the system. Hence, the experts are more willing and comfortable to use the system to alleviate the burden of their duties.
 This work is part of our CIIDAA project on building large scale, Comprehensive IT Infrastructure for Data-intensive Applications and Analysis [2]. Our collaborators are clinicians in the National University Health System (NUHS) [5]. The project aims to harness the power of cloud computing to solve big data problems in the real world, with healthcare predictive analytics being a popular area for big data analytics [26].
 Organization. The remainder of this paper is organized as follows. Section 2 presents motivating examples in healthcare predictive analytics. Section 3 discusses the architecture of an intelligent DBMS that aims to embed contextual crowd intelligence. Section 4 elaborates on research problems that we need to address in order to build an intelligent DBMS. Section 5 presents our preliminary results on the problem of predicting the risk of unplanned patient readmissions. Section 6 presents the related work. Finally, Section 7 concludes our work. Let us consider a hospital that has an integrated view of the medical care records of patients as shown in Table 1. The table contains two types of information: The tuples in this table are extracted from real cases of patients admitted to the National University Hospital (NUH) in Singapore. Healthcare professionals often have queries relating to predicting the severity of patients X  condition, such as, identifying patients at The three above mentioned aspects call for a new generation of intelligent DBMSs that can provide effective solutions for big data analytics. Our proposition of exploiting contextual crowd intelligence is, we believe, a big step towards this goal. The central theme of crowd intelligence is to get domain experts engaged as both the participants to fine tune the system and the end-users of the system. Figure 1 presents an intelligent system that exploits contextual crowd intelligence for big data analytics. The system first builds a knowledge base that will be subsequently used for the analytics tasks based on historical data, domain knowledge from SMEs (e.g., doctors), and other sources such as general clinical guidelines. Each source contributes to build some  X  X eak classifiers X . The system needs to combine these classifiers to derive a final classifier that achieves a high level of accuracy for prediction purposes. The system also needs to go through several iterations of interaction with the experts to refine, for example, the final classifier. As such, the experts participate in the entire process in fine tuning the system and decide on the  X  X est practices X . When real-time data or feed arrives, the system performs the prediction on-the-fly and alerts the experts immediately. Hence, the experts become the end-users of the system.
 We have developed the epiC system [1; 10; 19] to support large scale data processing, and are extending it to support healthcare analytics. Figure 2 shows the software stack of epiC. At the bottom, the storage layer supports different storage systems (e.g., We can also ask the same kind of questions for the analytics tasks in other domains. For instance, to predict malicious SMSs, we need to select a small set of messages (by utilizing some clustering algorithms) and ask the experts to provide labels for these samples. We also collect rules and heuristics that the experts utilize to label the SMSs. Feature selection is very important for any machine learning task and can greatly affect the algorithm X  X  quality. Processing doctor X  X  notes for extracting important features is an inevitably important step for healthcare analytics problems. There are several state-of-the-art Natural Language Processing (NLP) engines for processing clinical documents, such as, MedLEE [14] and cTAKES [27]. These engines process clinical notes, identifying types of clinical entities (e.g., medications, diseases, procedures, A hybrid human-machine approach. To infer the correct entities from unstructured data, a hybrid human-machine solution should be employed. The system can leverage the information from the knowledge base (e.g., UMLS) together with the implicit information (signals) inherent in the unstructured data (e.g., doctor X  X  notes) to improve the accuracy of its inference process and enhance the knowledge base as well. The system will pose questions to the healthcare professionals for verification. Based on the answers from the experts, the system adjusts its inference results. The inference process gets more accurate and complete as the system runs more iterations. Meanwhile, the knowledge base becomes more comprehensive and customized to each organization X  X  practice. More specifically, in our running example: We can obtain different classifiers from multiple sources such as classifiers built based on the observational data, rules used by the doctors and general clinical guidelines. Each source of knowledge can be considered as a  X  X eak classifier X  and the task is to combine these classifiers to derive a final classifier that achieves a very high accuracy in prediction.
 Clearly , to perform such tasks, we need to consult the domain experts as different hospitals/doctors may have different opinions/reasoning in performing this task. This is, again, an example of getting the domain experts involved in building the systems. As the system needs to interact with SMEs frequently, it is important to engage the experts along the process of building and using the system. The system should provide several functionalities for this purpose: We are studying the problem of predicting the probability of patients being readmitted into the hospital within 30 days after discharge. We refer to the task as readmission prediction for short. We use the clinical data drawn from the National University Hospital X  X  Computerized Clinical Data Repository (CCDR) and focus only on the elderly patients (i.e., patients with age older than 60) admitted to the hospital in 2012. The table used for the prediction task is the medical care table 1 that has similar schema as the one presented in Table 1. There are in total 29049 elderly patients admitted to NUH in 2012, where 5658 patients readmitted within 30 days, i.e., the proportion of patients who were readmitted (i.e. class label 1) is 0 : 188. We have been getting the doctors involved in the following tasks. Hypothesis/Rules. Our clinician collaborators have suggested a hypothesis that the following features (indicators) might be important for the readmission prediction:
T o derive the medical care table, we joined information from var-ious relations in CCDR, including: Discharge Summary, Patient Demographics, Visit and Encounter, Lab Results and Emergency Department. We used WEKA [15] to run a 10-fold cross-validation and the Bayesian Network classifier to construct a readmission classifier 2 . Table 2 reports the accuracy of the prediction across all the 10 validation data. If only structured features are used to build the classifier (Table 2(a)), the resulting classifier can correctly predict 1071 cases that are readmitted (within 30 days). The precision and recall in this case are 0 : 448 and 0 : 189, respectively. Meanwhile, if both structured and derived features are used to build the classifier (Table 2(b)), the resulting classifier can correctly predict 2679 cases that are readmitted. The precision and recall are 0 : 387 and 0 : 473 respectively. Clearly, the recall has been improved significantly with the usage of the derived features from the free-text doctor X  X  notes. The result is also very promising when we compared it to the result handled manually by domain experts such as physicians, case managers, and nurses [7]. The recall reported in [7] is in the range [0.149, 0.306]. The conclusion in [7] is that care-providers were not able to accurately predict which patients were at highest risk of readmission. However, we believe that a hybrid machine-human solution would greatly alleviate the problem.
 We would like to emphasize that there are many rooms to further improve the accuracy of the prediction such as enhancing the feature extraction process, employing additional features, such as, disease status, specific diagnoses, medications, and using special classifiers for highly-imbalanced data set. Related works to our proposition can be broadly classified into the following three categories.
 Existing solutions for industry/domain specific applications. Existing solutions are currently built based on  X  X est practices X . One direction is knowledge-driven approach that is based on general guidelines such as clinical guidelines, e.g., IBM Watson [3]. Another direction is data-driven approach that is based on  X  X ules X  extracted from the observational data, e.g., [16; 18; 20]. Recently, IBM proposes to combine the strengths of the two directions [31]. However, these solutions have not explored the exceptionally complicated rules/patterns that can only be provided by internal domain experts with years of working experience. Our research aims to fill this gap: we seek to engage the experts as users of the system, and tap on their expertise to enhance the database knowledge and processing. There are several benefits of employing internal domain experts. First, we do not need to customize/localize the system for different use-cases; they themselves define the  X  X est practices X  for the system. Second, in terms of the data used to build the knowledge base, our system mainly bases on observational data and knowledge provided by domain experts; whereas others (e.g., IBM Watson) need to process a much larger amount of inputs such as medical journals, white papers, medical policies and practices, information in the web, etc. Third, the system should become more  X  X ntelligent X  over times when the expert users continuously enhance the system with their expert knowledge.
W e also used other classifiers such as decision tree, rule-based classifier, SVM, etc and observe that the Bayesian Network classi-fier provides the best result.
 NRF-CRP8-2011-08. We thank Associate Professor Gerald C.H. Koh and Dr. Chuen Seng Tan (Saw Swee Hock School of Public Health, National University Health System) for sharing with us domain knowledge in healthcare. [1] http://www.comp.nus.edu.sg/ epic. [2] The comprehensive it infrastructure for data-[3] Ibm big data for healthcare. http://www.ibm.com. [4] The minority report: Chicago X  X  new police [5] National university health system. http://www.nuhs.edu.sg/. [6] Unified medical language system. [7] N. Allaudeen, J. L. Schnipper, E. J. Orav, R. M. Wachter, and [8] Y. Cao, C. Chen, F. Guo, D. Jiang, Y. Lin, B. C. Ooi, H. T. [9] G. Chen, K. Chen, D. Jiang, B. C. Ooi, L. Shi, H. T. Vo, [10] G. Chen, H. Jagadish, D. Jiang, D. Maier, B. Ooi, K. Tan, and [11] J. Dean and S. Ghemawat. Mapreduce: Simplified data pro-[12] J. Fan, M. Lu, B. C. Ooi, W.-C. Tan, and M. Zhang. A hybrid [13] M. J. Franklin, D. Kossmann, T. Kraska, S. Ramesh, and [14] C. Friedman, P. O. Alderson, J. H. Austin, J. J. Cimino, and [15] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, [16] J. Han. Data Mining: Concepts and Techniques . Morgan [17] S. C. H. Hoi, R. Jin, J. Zhu, and M. R. Lyu. Batch mode active Inmanyofthelarge-scalephysicalandsocialcomplexsys-temsphenomenafat-taileddistributionsoccur,forwhichdif-ferentgeneratingmechanismshavebeenproposed.Inthis paper,westudymodelsofgeneratingpowerlawdistribu-tionsintheevolutionoflarge-scaletaxonomiessuchasOpen DirectoryProject,whichconsistofwebsitesassignedtoone oftensofthousandsofcategories.Thecategoriesinsuch taxonomiesarearrangedintreeorDAGstructuredcon-figurationshavingparent-childrelationsamongthem.We firstquantitativelyanalysetheformationprocessofsuch taxonomies,whichleadstopowerlawdistributionasthe stationarydistributions.Inthecontextofdesigningclassi-fiersforlarge-scaletaxonomies,whichautomaticallyassign unseendocumentstoleaf-levelcategories,wehighlighthow thefat-tailednatureofthesedistributionscanbeleveraged toanalyticallystudythespacecomplexityofsuchclassi-fiers.Empiricalevaluationofthespacecomplexityonpub-liclyavailabledatasetsdemonstratestheapplicabilityofour approach. Withthetremendousgrowthofdataonthewebfromvar-ioussourcessuchassocialnetworks,onlinebusinessser-vicesandnewsnetworks,structuringthedataintoconcep-tualtaxonomiesleadstobetterscalability,interpretability andvisualization.Yahoo!directory,theopendirectory project(ODP)andWikipediaareprominentexamplesof suchweb-scaletaxonomies.TheMedicalSubjectHeading hierarchyoftheNationalLibraryofMedicineisanother instanceofalarge-scaletaxonomyinthedomainoflife sciences.Thesetaxonomiesconsistofclassesarrangedin ahierarchicalstructurewithparent-childrelationsamong themandcanbeintheformofarootedtreeoradirected acyclicgraph.ODPforinstance,whichisintheformofa rootedtree,listsover5millionwebsitesdistributedamong closeto1millioncategoriesandismaintainedbycloseto 100,000humaneditors.Wikipedia,ontheotherhand,rep-resentsamorecomplicateddirectedgraphtaxonomystruc-tureconsistingofoveramillioncategories.Inthiscontext, large-scalehierarchicalclassificationdealswiththetaskof automaticallyassigninglabelstounseendocumentsfroma setoftargetclasseswhicharerepresentedbytheleaflevel nodesinthehierarchy.
 Inthiswork,westudythedistributionofdataandthehi-Inmanyofthelarge-scalephysicalandsocialcomplexsys-temsphenomenafat-taileddistributionsoccur,forwhichdif-ferentgeneratingmechanismshavebeenproposed.Inthis paper,westudymodelsofgeneratingpowerlawdistribu-tionsintheevolutionoflarge-scaletaxonomiessuchasOpen DirectoryProject,whichconsistofwebsitesassignedtoone oftensofthousandsofcategories.Thecategoriesinsuch taxonomiesarearrangedintreeorDAGstructuredcon-figurationshavingparent-childrelationsamongthem.We firstquantitativelyanalysetheformationprocessofsuch taxonomies,whichleadstopowerlawdistributionasthe stationarydistributions.Inthecontextofdesigningclassi-fiersforlarge-scaletaxonomies,whichautomaticallyassign unseendocumentstoleaf-levelcategories,wehighlighthow thefat-tailednatureofthesedistributionscanbeleveraged toanalyticallystudythespacecomplexityofsuchclassi-fiers.Empiricalevaluationofthespacecomplexityonpub-liclyavailabledatasetsdemonstratestheapplicabilityofour approach. Withthetremendousgrowthofdataonthewebfromvar-ioussourcessuchassocialnetworks,onlinebusinessser-vicesandnewsnetworks,structuringthedataintoconcep-tualtaxonomiesleadstobetterscalability,interpretability andvisualization.Yahoo!directory,theopendirectory project(ODP)andWikipediaareprominentexamplesof suchweb-scaletaxonomies.TheMedicalSubjectHeading hierarchyoftheNationalLibraryofMedicineisanother instanceofalarge-scaletaxonomyinthedomainoflife sciences.Thesetaxonomiesconsistofclassesarrangedin ahierarchicalstructurewithparent-childrelationsamong themandcanbeintheformofarootedtreeoradirected acyclicgraph.ODPforinstance,whichisintheformofa rootedtree,listsover5millionwebsitesdistributedamong closeto1millioncategoriesandismaintainedbycloseto 100,000humaneditors.Wikipedia,ontheotherhand,rep-resentsamorecomplicateddirectedgraphtaxonomystruc-tureconsistingofoveramillioncategories.Inthiscontext, large-scalehierarchicalclassificationdealswiththetaskof automaticallyassigninglabelstounseendocumentsfroma setoftargetclasseswhicharerepresentedbytheleaflevel nodesinthehierarchy.
 Inthiswork,westudythedistributionofdataandthehi-erarchicallarge-scalewebtaxonomiesbystudyingtheevo-lutiondynamicsthatgeneratethem.Morespecifically,we presentaprocessthatjointlymodelsthegrowthinthesize ofcategories,aswellasthegrowthofthehierarchicaltree structure.Wederivefromthisgrowthmodelwhytheclass sizedistributionatagivenlevelofthehierarchyalsoex-hibitspowerlawdecay.Buildingonthis,wethenappeal toHeaps X  X awinSection4,toexplainthedistributionof featuresamongcategorieswhichisthenexploitedinSec-tion5foranalysingthespacecomplexityforhierarchical classificationschemes.Theanalysisisempiricallyvalidated onpubliclyavailableDMOZdatasetsfromtheLargeScale HierarchicalTextClassificationChallenge(LSHTC) 1 and patentdata(IPC) 2 fromWorldIntellectualPropertyOr-ganization.Finally,Section6concludesthiswork. Powerlawdistributionsarereportedinawidevarietyof physicalandsocialcomplexsystems[22],suchasininter-nettopologies.Forinstance[11;7]showedthatinternet topologiesexhibitpowerlawswithrespecttothein-degree ofthenodes.Alsothesizedistributionofwebsitecate-gories,measuredintermsofnumberofwebsites,exhibitsa fat-taileddistribution,asempiricallydemonstratedin[32; 19]fortheOpenDirectoryProject(ODP).Variousmod-elshavebeenproposedforthegenerationpowerlawdistri-butions,aphenomenonthatmaybeseenasfundamental incomplexsystemsasthenormaldistributioninstatistics [25].However,incontrasttothestraight-forwardderivation ofnormaldistributionviathecentrallimittheorem,models explainingpowerlawformationallrelyonanapproxima-tion.Someexplanationsarebasedonmultiplicativenoise orontherenormalizationgroupformalism[28;30;16].For thegrowthprocessoflarge-scaletaxonomies,modelsbased onpreferentialattachmentaremostappropriate,whichare usedinthispaper.Thesemodelsarebasedontheseminal modelbyYule[33],originallyformulatedforthetaxonomy ofbiologicalspecies,detailedinsection3.Itappliestosys-temswhereelementsofthesystemaregroupedintoclasses, andthesystemgrowsbothinthenumberofclasses,and inthetotalnumberofelements(whichareheredocuments orwebsites).Initsoriginalform,Yule X  X modelservesas explanationforpowerlawformationinanytaxonomy,irre-spectiveofaneventualhierarchyamongcategories.Similar dynamicshavebeenappliedtoexplainscalingintheconnec-tivityofanetwork,whichgrowsintermsofnodesandedges viapreferentialattachment[2].Recentfurthergeneraliza-tionsapplythesamegrowthprocesstotrees[17;14;29].
 Inthispaper,describetheapproximatepower-lawinthe child-to-parentcategoryrelationsbythemodelbyKlemm etal.[17].Furthermore,wecombinethisformationprocess inasimplemannerwiththeoriginalYulemodelinorderto explainalsoapowerlawincategorysizes,i.e.weprovide acomprehensiveexplanationfortheformationprocessof large-scalewebtaxonomiessuchasDMOZ.Fromthesec-ond,weinferathirdscalingdistributionforthenumberof featurespercategory.ThisisdoneviatheempiricalHeaps X  X  law[10],whichdescribesthescalingrelationshipbetween textlengthandthesizeofitsvocabulary.
 Someoftheearlierworksonexploitinghierarchyamongtar-http://lshtc.iit.demokritos.gr/ http://web2.wipo.int/ipcpub/ erarchicallarge-scalewebtaxonomiesbystudyingtheevo-lutiondynamicsthatgeneratethem.Morespecifically,we presentaprocessthatjointlymodelsthegrowthinthesize ofcategories,aswellasthegrowthofthehierarchicaltree structure.Wederivefromthisgrowthmodelwhytheclass sizedistributionatagivenlevelofthehierarchyalsoex-hibitspowerlawdecay.Buildingonthis,wethenappeal toHeaps X  X awinSection4,toexplainthedistributionof featuresamongcategorieswhichisthenexploitedinSec-tion5foranalysingthespacecomplexityforhierarchical classificationschemes.Theanalysisisempiricallyvalidated onpubliclyavailableDMOZdatasetsfromtheLargeScale HierarchicalTextClassificationChallenge(LSHTC) 1 and patentdata(IPC) 2 fromWorldIntellectualPropertyOr-ganization.Finally,Section6concludesthiswork. Powerlawdistributionsarereportedinawidevarietyof physicalandsocialcomplexsystems[22],suchasininter-nettopologies.Forinstance[11;7]showedthatinternet topologiesexhibitpowerlawswithrespecttothein-degree ofthenodes.Alsothesizedistributionofwebsitecate-gories,measuredintermsofnumberofwebsites,exhibitsa fat-taileddistribution,asempiricallydemonstratedin[32; 19]fortheOpenDirectoryProject(ODP).Variousmod-elshavebeenproposedforthegenerationpowerlawdistri-butions,aphenomenonthatmaybeseenasfundamental incomplexsystemsasthenormaldistributioninstatistics [25].However,incontrasttothestraight-forwardderivation ofnormaldistributionviathecentrallimittheorem,models explainingpowerlawformationallrelyonanapproxima-tion.Someexplanationsarebasedonmultiplicativenoise orontherenormalizationgroupformalism[28;30;16].For thegrowthprocessoflarge-scaletaxonomies,modelsbased onpreferentialattachmentaremostappropriate,whichare usedinthispaper.Thesemodelsarebasedontheseminal modelbyYule[33],originallyformulatedforthetaxonomy ofbiologicalspecies,detailedinsection3.Itappliestosys-temswhereelementsofthesystemaregroupedintoclasses, andthesystemgrowsbothinthenumberofclasses,and inthetotalnumberofelements(whichareheredocuments orwebsites).Initsoriginalform,Yule X  X modelservesas explanationforpowerlawformationinanytaxonomy,irre-spectiveofaneventualhierarchyamongcategories.Similar dynamicshavebeenappliedtoexplainscalingintheconnec-tivityofanetwork,whichgrowsintermsofnodesandedges viapreferentialattachment[2].Recentfurthergeneraliza-tionsapplythesamegrowthprocesstotrees[17;14;29].
 Inthispaper,describetheapproximatepower-lawinthe child-to-parentcategoryrelationsbythemodelbyKlemm etal.[17].Furthermore,wecombinethisformationprocess inasimplemannerwiththeoriginalYulemodelinorderto explainalsoapowerlawincategorysizes,i.e.weprovide acomprehensiveexplanationfortheformationprocessof large-scalewebtaxonomiessuchasDMOZ.Fromthesec-ond,weinferathirdscalingdistributionforthenumberof featurespercategory.ThisisdoneviatheempiricalHeaps X  X  law[10],whichdescribesthescalingrelationshipbetween textlengthandthesizeofitsvocabulary.
 Someoftheearlierworksonexploitinghierarchyamongtar-http://lshtc.iit.demokritos.gr/ http://web2.wipo.int/ipcpub/ Variables
N i Numberofelementsinclass i dg i Numberofsubclassesofclass i d i Numberoffeaturesofclass i  X  Totalnumberofclasses
DG Totalnumberofin-degrees(=subcategories) p
N, X  Fractionofclasseshaving N elements
Constants m Numberofelementsaddedtothesystemaf-w  X  [0 , 1]Probabilitythatattachmentofsub-
Indices i Indexfortheclass fromclasseswhichhavegainedanelementandhavebecome ofsize( N +1),thisisshownbytherightarrow(pointing rightwards)inFigure4.Theequationfortheclassofsize1 isgivenby: Asthenumber  X  ofclasses(andthereforethenumberof elements  X  ( m +1))inthesystemincreases,theprobability thatanewelementisclassifiedintoaclassofsize N ,givenby Equation3,isassumedtoremainconstantandindependent of  X  .Underthishypothesis,thestationarydistributionfor classsizescanbedeterminedbysolvingEquation4and usingEquation5astheinitialcondition.Thisisgivenby where B ( .,. )isthebetadistribution.Equation6hasbeen termed Yuledistribution [26].Writtenforacontinuousvari-able N ,ithasapowerlawtail: Fromtheaboveequationtheexponentofthedensityfunc-tionisbetween2and3.Itscumulativesizedistribution P ( N k &gt;N ),asgivenbyEquation1,hasanexponentgiven by whichisbetween1and2.Thehigherthefrequency1 /m atwhichnewclassesareintroduced,thebigger  X  becomes, andthelowertheaverageclasssize.Thisexponentisstable overtimealthoughthetaxonomyisconstantlygrowing. Asimilarmodelhasbeenformulatedfornetworkgrowthby Barab  X asiandAlbert[2],whichexplainstheformationofa powerlawdistributioninconnectivitydegreeofnodes.It assumesthatthenetworksgrowintermsofnodesandedges, andthateverynewlyaddednodetothesystemconnects withafixednumberofedgestoexistingnodes.Attachment isagainpreferential,i.e.theprobabilityforanewlyadded Variables
N i Numberofelementsinclass i dg i Numberofsubclassesofclass i d i Numberoffeaturesofclass i  X  Totalnumberofclasses
DG Totalnumberofin-degrees(=subcategories) p
N, X  Fractionofclasseshaving N elements
Constants m Numberofelementsaddedtothesystemaf-w  X  [0 , 1]Probabilitythatattachmentofsub-
Indices i Indexfortheclass fromclasseswhichhavegainedanelementandhavebecome ofsize( N +1),thisisshownbytherightarrow(pointing rightwards)inFigure4.Theequationfortheclassofsize1 isgivenby: Asthenumber  X  ofclasses(andthereforethenumberof elements  X  ( m +1))inthesystemincreases,theprobability thatanewelementisclassifiedintoaclassofsize N ,givenby Equation3,isassumedtoremainconstantandindependent of  X  .Underthishypothesis,thestationarydistributionfor classsizescanbedeterminedbysolvingEquation4and usingEquation5astheinitialcondition.Thisisgivenby where B ( .,. )isthebetadistribution.Equation6hasbeen termed Yuledistribution [26].Writtenforacontinuousvari-able N ,ithasapowerlawtail: Fromtheaboveequationtheexponentofthedensityfunc-tionisbetween2and3.Itscumulativesizedistribution P ( N k &gt;N ),asgivenbyEquation1,hasanexponentgiven by whichisbetween1and2.Thehigherthefrequency1 /m atwhichnewclassesareintroduced,thebigger  X  becomes, andthelowertheaverageclasssize.Thisexponentisstable overtimealthoughthetaxonomyisconstantlygrowing. Asimilarmodelhasbeenformulatedfornetworkgrowthby Barab  X asiandAlbert[2],whichexplainstheformationofa powerlawdistributioninconnectivitydegreeofnodes.It assumesthatthenetworksgrowintermsofnodesandedges, andthateverynewlyaddednodetothesystemconnects withafixednumberofedgestoexistingnodes.Attachment isagainpreferential,i.e.theprobabilityforanewlyadded oneforthesizedistributionofleafcategoriesand(b)onefor theindegree(child-to-parentlink)distributionofcategories (showninFigure2).Thesetwoscalinglawsarelinkedina non-trivialmanner:acategorymaybeverysmalloreven notcontainanywebsites,butneverthelessbehighlycon-nected.Sinceontheotherhand(a)and(b)arisejointly, weproposehereamodelgeneratingthetwoscalinglaws inasimplegenericmanner.Wesuggestacombinationof thetwoprocessesdetailedinsubsections3.1and3.2tode-scribethegrowthprocess:websitesarecontinuouslyadded tothesystem,andclassifiedintocategoriesbyhumanref-erees.Atthesametime,thecategoriesarenotamereset, butformatreestructure,whichgrowsitselfintwoquanti-ties:inthenumbernodes(categories)andinthenumberof in-degreesofnodes(child-to-parentlinks,i.e.subcategory-to-categorylinks).Basedontherulesforvoluntaryreferees oftheDMOZhowtoclassifywebsites,weproposeasim-plecombineddescriptionoftheprocess.Altogether,the databasegrowsin three quantities: (i) Growthinwebsites. Newwebsitesareassignedinto Figure5:(i):Awebsiteisassignedtoexistingcategories with p ( i )  X  N i . (ii) Growthincategories. Withprobability1 /m ,theref-Figure6:(ii):Growthincategoriesisequivalenttogrowth ofthetreestructureintermsofin-degrees. (iii) Growthinchildrencategories. Finally,thehierarchy oneforthesizedistributionofleafcategoriesand(b)onefor theindegree(child-to-parentlink)distributionofcategories (showninFigure2).Thesetwoscalinglawsarelinkedina non-trivialmanner:acategorymaybeverysmalloreven notcontainanywebsites,butneverthelessbehighlycon-nected.Sinceontheotherhand(a)and(b)arisejointly, weproposehereamodelgeneratingthetwoscalinglaws inasimplegenericmanner.Wesuggestacombinationof thetwoprocessesdetailedinsubsections3.1and3.2tode-scribethegrowthprocess:websitesarecontinuouslyadded tothesystem,andclassifiedintocategoriesbyhumanref-erees.Atthesametime,thecategoriesarenotamereset, butformatreestructure,whichgrowsitselfintwoquanti-ties:inthenumbernodes(categories)andinthenumberof in-degreesofnodes(child-to-parentlinks,i.e.subcategory-to-categorylinks).Basedontherulesforvoluntaryreferees oftheDMOZhowtoclassifywebsites,weproposeasim-plecombineddescriptionoftheprocess.Altogether,the databasegrowsin three quantities: (i) Growthinwebsites. Newwebsitesareassignedinto Figure5:(i):Awebsiteisassignedtoexistingcategories with p ( i )  X  N i . (ii) Growthincategories. Withprobability1 /m ,theref-Figure6:(ii):Growthincategoriesisequivalenttogrowth ofthetreestructureintermsofin-degrees. (iii) Growthinchildrencategories. Finally,thehierarchy editionandtwodatasetswereextractedfromtheLSHTC-2011edition.ThesearereferredtoasLSHTC1-large,LSHTC2-aandLSHTC2-brespectivelyinTable2.Thefourthdataset (IPC)comesfromthepatentcollectionreleasedbyWorld IntellectualPropertyOrganization.Thedatasetsareinthe LibSVMformat,whichhavebeenpreprocessedbystemming andstopwordremoval.Variouspropertiesofinterestforthe datasetsareshowninTable2.
 Dataset #Tr./#Test#Classes#Feat.
 LSHTC1-large 93,805/34,88012,294347,255 LSHTC2-a 25,310/6,4411,789145,859 LSHTC2-b 36,834/9,6053,672145,354 IPC 46,324/28,9264511,123,497 Table2:Datasetsforhierarchicalclassificationwiththe properties:Numberoftraining/testexamples,targetclasses andsizeofthefeaturespace.Thedepthofthehierarchytree forLSHTCdatasetsis6andfortheIPCdatasetis4.
 Table3showsthedifferenceintrainedmodelsize(actual valueofthemodelsizeontheharddrive)betweenthetwo classificationschemesforthefourdatasets,alongwiththe valuesdefinedinProposition1.Thesymbol referstothe Table3:Modelsize(inGB)forflatandhierarchicalmodels alongwiththecorrespondingvaluesdefinedinProposition AsshownforthethreeDMOZdatasets,thetrainedmodel forflatclassifierscanbeanorderofmagnitudelargerthan forhierarchicalclassification.Thisresultsfromthesparse andhigh-dimensionalnatureoftheproblemwhichisquite typicalintextclassification.Forflatclassifiers,theentire featuresetparticipatesforalltheclasses,butfortop-down classification,thenumberofclassesandfeaturesparticipat-inginclassifiertrainingareinverselyrelated,whentravers-ingthetreefromtheroottowardstheleaves.Asshownin Proposition1,thepowerlawexponent  X  playsacrucialrole inreducingthemodelsizeofhierarchicalclassifier. Inthisworkwepresentedamodelinordertoexplainthe dynamicsthatexistinthecreationandevolutionoflarge-scaletaxonomiessuchastheDMOZdirectory,wherethe categoriesareorganizedinahierarchicalform.Morespecif-ically,thepresentedprocessmodelsjointlythegrowthin thesizeofthecategories(intermsofdocuments)aswellas thegrowthofthetaxonomyintermsofcategories,which toourknowledgehavenotbeenaddressedinajointframe-work.Fromoneofthem,thepowerlawincategorysize distribution,wederivedpowerlawsateachlevelofthehier-archy,andwiththehelpofHeaps X  X lawathirdscalinglaw inthefeaturessizedistributionofcategorieswhichwethen editionandtwodatasetswereextractedfromtheLSHTC-2011edition.ThesearereferredtoasLSHTC1-large,LSHTC2-aandLSHTC2-brespectivelyinTable2.Thefourthdataset (IPC)comesfromthepatentcollectionreleasedbyWorld IntellectualPropertyOrganization.Thedatasetsareinthe LibSVMformat,whichhavebeenpreprocessedbystemming andstopwordremoval.Variouspropertiesofinterestforthe datasetsareshowninTable2.
 Dataset #Tr./#Test#Classes#Feat.
 LSHTC1-large 93,805/34,88012,294347,255 LSHTC2-a 25,310/6,4411,789145,859 LSHTC2-b 36,834/9,6053,672145,354 IPC 46,324/28,9264511,123,497 Table2:Datasetsforhierarchicalclassificationwiththe properties:Numberoftraining/testexamples,targetclasses andsizeofthefeaturespace.Thedepthofthehierarchytree forLSHTCdatasetsis6andfortheIPCdatasetis4.
 Table3showsthedifferenceintrainedmodelsize(actual valueofthemodelsizeontheharddrive)betweenthetwo classificationschemesforthefourdatasets,alongwiththe valuesdefinedinProposition1.Thesymbol referstothe Table3:Modelsize(inGB)forflatandhierarchicalmodels alongwiththecorrespondingvaluesdefinedinProposition AsshownforthethreeDMOZdatasets,thetrainedmodel forflatclassifierscanbeanorderofmagnitudelargerthan forhierarchicalclassification.Thisresultsfromthesparse andhigh-dimensionalnatureoftheproblemwhichisquite typicalintextclassification.Forflatclassifiers,theentire featuresetparticipatesforalltheclasses,butfortop-down classification,thenumberofclassesandfeaturesparticipat-inginclassifiertrainingareinverselyrelated,whentravers-ingthetreefromtheroottowardstheleaves.Asshownin Proposition1,thepowerlawexponent  X  playsacrucialrole inreducingthemodelsizeofhierarchicalclassifier. Inthisworkwepresentedamodelinordertoexplainthe dynamicsthatexistinthecreationandevolutionoflarge-scaletaxonomiessuchastheDMOZdirectory,wherethe categoriesareorganizedinahierarchicalform.Morespecif-ically,thepresentedprocessmodelsjointlythegrowthin thesizeofthecategories(intermsofdocuments)aswellas thegrowthofthetaxonomyintermsofcategories,which toourknowledgehavenotbeenaddressedinajointframe-work.Fromoneofthem,thepowerlawincategorysize distribution,wederivedpowerlawsateachlevelofthehier-archy,andwiththehelpofHeaps X  X lawathirdscalinglaw inthefeaturessizedistributionofcategorieswhichwethen [10]L.Egghe.Untanglingherdan X  X lawandheaps X  X aw: [11]M.Faloutsos,P.Faloutsos,andC.Faloutsos.Onpower-[12]R.-E.Fan,K.-W.Chang,C.-J.Hsieh,X.-R.Wang, [13]T.GaoandD.Koller.Discriminativelearningofre-[14]M.M.Geipel,C.J.Tessone,andF.Schweitzer.Acom-[15]S.Gopal,Y.Yang,B.Bai,andA.Niculescu-Mizil.
 [16]G.Jona-Lasinio.Renormalizationgroupandprobabil-[17]K.Klemm,V.M.Egu  X  X luz,andM.SanMiguel.Scaling [18]D.KollerandM.Sahami.Hierarchicallyclassifying [19]T.-Y.Liu,Y.Yang,H.Wan,H.-J.Zeng,Z.Chen,and [20]B.Mandelbrot.Anoteonaclassofskewdistribution [21]C.MetzigandM.B.Gordon.Amodelforscalingin [10]L.Egghe.Untanglingherdan X  X lawandheaps X  X aw: [11]M.Faloutsos,P.Faloutsos,andC.Faloutsos.Onpower-[12]R.-E.Fan,K.-W.Chang,C.-J.Hsieh,X.-R.Wang, [13]T.GaoandD.Koller.Discriminativelearningofre-[14]M.M.Geipel,C.J.Tessone,andF.Schweitzer.Acom-[15]S.Gopal,Y.Yang,B.Bai,andA.Niculescu-Mizil.
 [16]G.Jona-Lasinio.Renormalizationgroupandprobabil-[17]K.Klemm,V.M.Egu  X  X luz,andM.SanMiguel.Scaling [18]D.KollerandM.Sahami.Hierarchicallyclassifying [19]T.-Y.Liu,Y.Yang,H.Wan,H.-J.Zeng,Z.Chen,and [20]B.Mandelbrot.Anoteonaclassofskewdistribution [21]C.MetzigandM.B.Gordon.Amodelforscalingin We discuss the most important database research advances, industry developments, role of relational and NoSQL databases, Computing Reality, Data Curation, Cloud Computing, Tamr and Jisto startups, what he learned as a chief Scientist of Verizon, Knowledge Discovery, Privacy Issues, and more.
 Data Curation, NoSQL, Data Curation, Cloud Computing, Verizon, Privacy, Computing Reality. both at GTE Laboratories in 1990s, where he was already a world-famous researcher and a department manager. I recently met him these questions while flying from Boston to Doha, Qatar where he is advising Qatar Computing Research Institute. Parts of this interview were published in KDnuggets [1-3]. advanced, academic research and large-scale industrial practice deployment of innovative technologies while helping research to Information Technology Challenges in Healthcare Reform. GP: What about the most important database industry developments? database industry development came from outside the database database industry has been insular and protected its relational turf for FAR too long. Smart folks at Yahoo!, Google and other places saw value in data, non-database data, and thus emerged MapReduce, Hadoop, and NoSQL-generally crappy database ideas but it woke up the database industry 1 . Hadoop and NoSQL amazing for a very specific problem domain, embarrassingly importance of MapReduce is that it forced the database industry to get out of their hammocks. GP: What is the role of Relational Databases, NoSQL databases, Graph databases, and other databases today? Relational Databases have two extremely well established roles. Conventional row stores serve the OLTP community as the transaction processors are moving in-memory. OLTP stores are lock step with business growth and decline. Column stores, OLAP, are the backbone of data warehouses and until recently business intelligence. In general there are huge numbers of these, often of very large size in the Petabyte and Exabyte range. This is where Big Data battle lines are being drawn. What fun!! This is also where we turn from polishing the relational round ball Taking over is relative; none of the 12 other categories has more than 3% of the database market. Graph databases serve graph applications like networking in communications, telecom, social networks, and of course NSA applications! But what is wonderful about these emerging classes of data-domain specific DBMSs is serve. The use cases define the DBMSs and the DBMSs help formulate data and computation at scale. It is awkward for both communities scientists who only speak R. Exciting times. For a little fun look at the database-engines list [6]. DB-Engines lists 216 different database management systems, which are classified according to their database model (e.g. On June 25, 2014 Google launched Cloud Dataflow replacing 
MapReduce and marking the decline of MR and Hadoop as predicted at launch in 2010 by Mike Stonebraker in 2010. waking up in the morning and thinking you might change the world. That requires that I conceive the world not just differently, but so that it solves someone X  X  REAL problem. Even more cool. Gregory Piatetsky: Currently you are an adviser at a startup called Tamr [7], co-founded by another leading DB researcher and serial entrepreneur Michael Stonebraker. What can you tell us about Tamr and its product? Michael Brodie: Consider the data universe. Since the 1980 X  X  I with less than 10% of the world X  X  data most of which is structured, discrete, and conforms to some schema. With the Web inconceivable while shrinking database data to less than 8%. zettabytes. [ If you are constantly amazed at the growth of the Digital World, you don X  X  understand it yet  X  A profound, casual comment of my departed friend, Gerard Berry, Academie Francais. ] In 1988 or so you, Gregory, and a few others saw the potential of data with your knowledge discovery in databases  X  then a radical now named Big Data. Even though Big Data is hot in 2014, almost 30 years later, it X  X  application, tools, and technologies are early 1990s. Just as the Web has and is changing the world, so too will Big Data. ________________________________________ Compared with database data, Big Data is crazy. It X  X  largely not understood hence it is schema-less or model-less. Big Data is inconceivably massive, dirty, imprecise, incomplete, and heterogeneous beyond anything we X  X e seen before. Yet it trumps finite, precise, database data in many ways hence is a database data that is a small subset of Big Data -EMC/IDC claims requires different thinking, tools, and techniques. Database data is approached top-down. Telco billing folks know billing inside out that does not comply is erroneous. Database data, like Telco bills approached bottom up. More fundamentally, we should let data speak; see what models or correlations emerge from the data, e.g., sense (a known unknown) or to discover something we never preconceived, possibly biased, model on data we should understand it. For more on Data Curation at scale, see Stonebraker [8]. GP: You also advise another startup Jisto. What can you tell us about your role there? MB: I am having a blast with Jisto [9]  X  some amazingly talented young engineers [PhDs actually] with lots of energy and a killer idea. Jisto is an exceptional example of the quality you ask about in the next question. Cloud computing enabled by virtualization is radically changing the world by reducing the cost and increasing the availability of computing resources. Can you imagine that only 50% of the world X  X  servers are virtualized? Pop quiz [do not cheat and read ahead]. What is the average CPU utilization of physical servers, worldwide? Of virtual servers? Answer: Virtual machine CPU utilization is typically in the 30-50% range while physical servers are 10-20%, due to risk and scheduling ,but mostly cultural challenges. Jisto enables enterprises to transparently run more compute-intensive workloads on these paid-for but unused resources whether on premises or in public or private clouds, thus reducing costs by 75  X  90% over acquiring more hardware or cloud resources. Jisto provides a high-performance, virtualized, elastic cloud-computing environment from underutilized enterprise or cloud computing resources (servers, laptops, etc.) without impacting the primary task on those resources. Organizations that will benefit most from Jisto are those that run parallelized compute-intensive applications in the data center or in private and public clouds (e.g., Amazon Web Services, Windows Azure, Google Cloud Platform, IBM SmartCloud). _ who will gain significant reduction in the cost of their computing possibly avoiding costly data center expansion. GP: You are also a Principal at First Founders Limited. What do you look for in young business ventures -how do you determine quality? --------------------------------------------startups. The professional ones are called "Venture Capitalists (VCs). The retired ones are called Angels. Like any serious problem there is due diligence to determine and evaluate the business plan, etc. as the many books [10] and formulas suggest. ---------------------------------------------------------comes down to good taste developed over years of successful ought to know, Tamr is about his 25th startup. not going out until you do! GP: Around 1989 when you were a manager at GTE Labs and I was a member of technical staff there, you were somewhat skeptical of the idea I proposed for research into Knowledge Discovery in Databases (then called KDD or Data Mining, and more recently Predictive Analytics, and Data Science). The field has progressed significantly since then. From your point of view, what are the main successes and disappointments of KDD/Data Mining/Predictive Analytics and can Data Science become an actual science? philosophical underpinnings of Big Data and Data Science. With Big Data we are undergoing a fundamental shift in thinking and in computing. Big Data is a marvelous tool to investigate What  X  correlations or patterns that suggest that things might have or will occur. Big Data X  X  weakness is that it says nothing about Why  X  causation or why a phenomenon occurred or will occur. A pernicious aspect of What are the biases that we bring to it. On a personal note, my biased recall of 1989 was how marvelous your ideas were and the amazing potential of data mining. I accept your view that I was skeptical rather than enthusiastic as I recall. side, which I was not then. Hence, what we think that we thought people X  X  reality. As Richard Feynman said, are t he easiest person to fool. X  is. The World of What is phenomenal  X  machines proposing rather simple model, while models, such as Machine Learning, meaningless. For example, ~99% of credit card transactions are apparently anomalous behavior  X  as it is happening! market, electoral predictions, marketing success, and many more that underlie the Big Data explosion. Yet there is a potential Big Data Winter ahead if people blindly apply Big Data and more specifically Machine Learning. The failures concern limited models of phenomena and the human tendency of bias. People can climate change or lack of human impact on climate change, rather to be able to detect attackers and issues well after they have made it through their gates, find them, and stop them before damage can deploying it as part of the very core of the defense mechanisms. Big Data has rendered obsolete the current approach to protecting privacy and civil liberties.  X  Hence, Big Data requires a shift from a focus on top-down on data usage. Not only do top-down methods not scale,  X  Tightly hugely valuable resource [13] X  . Adequate let alone complete solutions will take years to develop. GP: What interesting technical developments you expect in Database and Cloud Technology in the next 5 years? MB: I call the Big Picture Computing Reality in which we model the world from whatever reasonable perspectives emerge from the symbiotically with machines and people collaborating to optimize resources while achieving measures of veracity for each result. One subspace of this world is what we currently know with high relational databases. Another encompassing space is what we know but forgot or don X  X  want to remember ( unknown knowns ) and a third is what we speculate but do not know ( known know in science, business, and life. otherwise learning would be at an end. That is the space of discovery. I am investigating Computing Reality to investigate the entire space with the objective of accelerating Scientific Discovery. discrete, bounded, finite, or involves a single version of truth, yet that is the world of most computing. With Computing Reality we and theoretically interesting because we have almost no mathematical or computing models in these areas. Those that exist You see what old retired guys get to do? GP: What do you like to do in your free time? What recent book you liked? MB: Free time  X  what a concept! My yoga teacher, Lynne recommended that I should try to do nothing one day, and I will. I will. Soon. Really. Life is such a blast; it X  X  hard to keep still. One step in this direction by the Qatar Foundation was to create the Qatar Computing Research Institute (QCRI). In less than three computer science research group seeded with world-class Computing, Data Analysis, Cyber Security, and Arabic Language Technologies (e.g., Machine Learning and Translation) amongst others. Each group already has multiple publications over several years in the leading conferences in their areas, e.g., SIGMOD and VLDB for Data Analysis. I spent my time reviewing with them what I consider to be some of the most challenging issues in Big Data. Leading Database Researcher, Industry Leader, Thinker, in KDnuggets, April 2014, http://www. kdnuggets.com/2014/04/michael-brodie-database-researcher-leader-thinker.html [2] Gregory Piatetsky, Interview (part 2): Michael Brodie on Data Curation, Cloud Computing, Startup Quality, Verizon, in KDnuggets, May 2014, http://www. kdnuggets.com/2014/04/interview-michael-brodie-2-data-curation-cloud-computing-verizon.html [3] Gregory Piatetsky, Interview (part 3): Michael Brodie on Industry Lessons, Knowledge Discovery, and Future Trends, in KDnuggets, May 2014, http://www.kdnuggets.com/ 2014/05/interview-michael-brodie-3-industry-lessons-knowledge-discovery-trends-qcri.html [4] www.michaelbrodie.com/michael_brodie.asp [5] Michael Stonebraker. Are We Polishing a Round Ball? Panel Abstract. ICDE, page 606. IEEE Computer Society, 1993 [6] Database Engines List, http://db-engines.com/en/blog_post/23 http:/db-engines.com/en/blog_post/23 
