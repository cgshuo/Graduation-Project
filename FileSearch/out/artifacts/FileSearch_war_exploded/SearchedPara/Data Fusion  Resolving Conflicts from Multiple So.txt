 The amount of useful information available on the Web has been growing at a dramatic pace in recent years. In a variety of domains , such as science, business, technology, arts, entertainment, politics, government , sports, tourism, there are a huge number of data sources that seek to provide information to a wide spectrum of information users. In addition to enabling the availability of useful information, the Web has also eased the ability to publish and spread false infor mation across multiple sources. Widespread availability of conflicting information (some true, some false) makes it hard to separate the wheat from the chaff. Simply using the information that is asserted by the largest number of data sources ( i.e. , naive voting) is clearly inadequate since biased (and even malicious) sources abound, and plagiarism ( i.e. , copying without proper attribution) be-tween sources may be widespread. Data fusion aims at resolving conflicts from different sources and find values that reflect the real world.

Ideally, when applying voting, we would like to give a higher vote to more trustwor-thy sources and ignore copied information; however, this raises many challenges. First, we often do not know apriori the trustworthiness of a source and that depends on how much of its provided data are correct, but the correctness of data, on the other hand, needs to be decided by considering the number and trustworthiness of the providers; thus, it is a chicken-and-egg problem. Second, in many applications we do not know how each source obtains its data, so we have to discover copiers from a snapshot of data. The discovery is non-trivial: sharing common data does not in itself imply copying X  accurate sources can also share a lot of indepe ndently provided correct data; not shar-ing a lot of common data does not in itself imply no-copying X  X  copier may copy only a small fraction of data from the original source; even when we decide that two sources are dependent, it is not always obvious which one is a copier. Third, a copier can also provide some data by itself or verify the correctness of some of the copied data, so it is inappropriate to ignore all data it provides.

In this paper, we present novel approaches for data fusion. First, we consider copying between data sources in truth discovery. Our technique considers not only whether two sources share the same values, but also whether the shared values are true or false. In-tuitively, for a particular object, there are often multiple distinct false values but usually only one true value. Sharing the same true value does not necessarily imply copying between sources; however, sharing the same false value is typically a low-probability event when the sources are fully independent. Thus, if two data sources share a lot of false values, copying is more likely. Based on this analysis, we describe Bayesian mod-els that compute the probability of copying between pairs of data sources and take the result into consideration in truth discovery.

Second, we also consider accuracy in voting: we trust an accurate data source more and give values that it provides a higher weight. This method requires identifying not only if two sources are dependent, but also which source is the copier. Indeed, accuracy in itself is a clue of direction of copying: gi ven two data sources, if the accuracy of their common data is highly different from that of one of the sources, that source is more likely to be a copier.
 Example 1. Consider the five data sources in Table 1. They provide information on affiliations of five researchers and only S 1 provides all correct data. Sources S 4 and S 5 copy their data from S 3 ,and S 5 introduces certain errors during copying.
First consider the three sources S 1 , S 2 , and S 3 . For all researchers except Carey ,a naive voting on data provided by these three sources can find the correct affiliations. For Carey , these sources provide three different affiliations, resulting in a tie. However, if we take into account that the data provided by S 1 is more accurate (among the rest of the 4 researchers, S 1 provides all correct affiliations, whereas S 2 provides 3 and S 3 provides only 2 correct affiliations), we will consider UCI as most likely to be the correct value.
Now consider in addition sources S 4 and S 5 . Since the affiliations provided by S 3 are copied by S 4 and S 5 , naive voting would consider them as the majority and so make wrong decisions for three researchers. Only if we ignore the values provided by S 4 and S , we will be able to again decide the correct affiliations. Note however that identifying the copying relationships is not easy: while S 3 shares 5 values with S 4 and 4 values with S , S 1 and S 2 also share 3 values, more than half of all values. If we knew which values are true and which are false, we would suspect copying between S 3 , S 4 and S 5 , because they provide the same false values. On the other hand, we would suspect the copying between S 1 and S 2 much less, as they share only true values.
 The structure of the rest of the paper is as follows. Section 2 presents how we can lever-age source accuracy in data fusion. Section 3 presents how we can leverage copying relationships in data fusion. Section 4 presents a case study of these techniques on a real-world data set, and Section 5 concludes. We first formally describe the data fusion problem and describe how we leverage the trustworthiness of sources in truth discovery. In this section we assume no-copying between data sources and defer discussion on copying to the next section. 2.1 Data Fusion We consider a set of data sources S and a set of objects O . An object represents a particular aspect of a real-world entity, such as the affiliation of a researcher; in a relational database, an object corres ponds to a cell in a table. For each object O  X  O , a source S  X  S can (but not necessarily) provide a value . Among different values provided for an object, one correctly describes the real world and is true ,andtherest are false . In this paper we solve the following problem: given a snapshot of data sources in S , decide the true value for each object O  X  O .

We note that a value provided by a data source can either be atomic, or a set or list of atomic values ( e.g. , author list of a book). In the latter case, we consider the value as true if the atomic values are correct and the set o r list is complete (and order preserved for a list). This setting already fits many real-world applications and we refer our readers to [13] for solutions that treat a set or list of values as multiple values.
We consider a core case that satisfies the following two conditions (relaxation of these assumptions is discussed in [7]):  X  Uniform false-value distribution: For each object, there are multiple false values  X  Categorical value: For each object, values that do not match exactly are considered Note that this problem definition focuses on static information that does not evolve over time, such as authors and publishers of books, and we refer our readers to [8] for data fusion for evolving values.
 2.2 Accuracy of a Source Let S  X  S be a data source. The accuracy of S , denoted by A ( S ) , is the fraction of true values provided by S ; it can also be considered as the probability that a value provided by S is the true value.

Ideally we should compute the accuracy of a source as it is defined; however, in real applications we often do not know for sure which values are true, especially among values that are provided by similar number of sources. Thus, we compute the accuracy of a source as the average probability of its values being true (we describe how we compute such probabilities shortly). Formally, let  X  V ( S ) be the values provided by S and that v is true. We compute A ( S ) as follows.
 We distinguish good sources from bad ones: a data source is considered to be good if for each object it is more likely to provide the true value than any particular false value; otherwise, it is considered to be bad. Assume for each object in O the number of false values in the domain is n . Then, in the core case, the probability that S provides unless otherwise specified. 2.3 Probability of a Value Being True Now we need a way to compute the probability that a value is true. Intuitively, the computation should consider both how many sources provide the value and accuracy of those sources. We apply a Bayesian analysis for this purpose.

Consider an object O  X  O .Let V ( O ) be the domain of O , including one true value and n false values. Let  X  S o be the sources that provide information on O . For each v  X  V We denote by  X  ( O ) the observation of which value each S  X   X  S o votes for O .
To compute P ( v ) for v  X  V ( O ) , we need to first compute the probability of  X  ( O ) conditioned on v being true. This probability should be that of sources in  X  S o ( v ) each providing the true value and other sources each providing a particular false value: Among the values in V ( O ) , there is one and only one true value. Assume our apriori belief of each value being true is the same, denoted by  X  .Wethenhave Applying the Bayes Rule leads us to To simplify the computation, we define the confidence of v , denoted by C ( v ) ,as C ( v )=  X  by summing up the accuracy scores of its providers. Finally, we can compute the prob-ability of each value as P ( v )= 2 C ( v )  X  a higher probability to be true; thus, rathe r than comparing vote counts, we can just compare confidence of values. The following theorem shows three nice properties of Equation (4).
 Theorem 1. Equation (4) has the following properties: 1. If all data sources are good and have the same accuracy, when the size of  X  S o ( v ) Note that the first property is actually a justification for the naive voting strategy when all sources have the same accuracy. The third property shows that we should be careful not to assign very high or very low accuracy to a data source, which has been avoided by defining the accuracy of a source as the average probability of its provided values. Example 2. Consider S 1 , S 2 and S 3 in Table 1 and assume their accuracies are .97, .6, .4 respectively. Assuming there are 5 false values in the domain ( i.e. , n = 5), we can
Now consider the three values provided for Carey .Value UCI thus has confidence 8, AT&amp;T has confidence 5, and BEA has confidence 4. Among them, UCI has the highest confidence and so the highest probability to be true. Indeed, its probability is Computing value confidence r equires knowing accuracy of data sources, whereas com-puting source accuracy requires knowing value probability. There is an inter-dependence between them and we solve the problem by computing them iteratively. We give details of the iterative algorithm in Section 3. Next, we describe how we detect copiers and leverage the discovered copying relation-ships in data fusion.
 3.1 Copy Detection We say that there exists copying between two data sources S 1 and S 2 if they derive the same part of their data directly or transitively from a common source (can be one of S and S 2 ). Accordingly, there are two types of data sources: independent sources and copiers .An independent source provides all values independently. It may provide some erroneous values because of incorrect knowledge of the real world, mis-spellings, etc. A copier copies a part (or all) of its data from other sources (independent sources or copiers). It can copy from multiple sources by union, intersection, etc., and as we focus on a snapshot of data, cyclic copying on a particular object is impossible. In addition, a copier may revise some of the copied values or add additional values; though, such revised and added values are considered as independent contributions of the copier.
To make our models tractable, we consider only direct copying. In addition, we make the following assumptions.  X  Assumption 1 (Independent values). The values that are independently provided by  X  Assumption 2 (Independent copying). The copying between a pair of data sources  X  Assumption 3 (No mutual copying). There is no mutual copying between a pair of Our experiments on real world data show that the basic model already obtains high accuracy and we refer our readers to [6] for how we can relax the assumptions. We next describe the basic copy-detection model.

Consider two sources S 1 , S 2  X  S . We apply Bayesian analysis to compute the prob-ability of copying between S 1 and S 2 given observation of their data. For this purpose, we need to compute the proba bility of the observed data, conditioned on independence of or copying between the sources. We denote by c ( 0 &lt; c  X  1 ) the probability that a value provided by a copier is copied . We bootstrap our algorithm by setting c to a default value initially and iteratively refine it according to copy detection results.
In our observation, we are interested in three sets of objects:  X  O t , denoting the set of objects on which S 1 and S 2 provide the same true value,  X  O f , denoting the set of objects on which they provide the same false value, and  X  O d , denoting the set of objects on and  X  O d , the more common false values that S 1 and S 2 provide, the more likely that they are dependent. On the other hand, if we fix  X  O t and  X  O f , the fewer objects on which S 1 and S 2 provide different values, the more likely that they are dependent. We denote by  X  describe how we compute the conditional probability of  X  based on these intuitions.
We first consider the case where S 1 and S 2 are independent, denoted by S 1  X  S 2 .Since there is a single true value, the probability that S 1 and S 2 provide the same true value for object O is On the other hand, the probability that S 1 and S 2 provide the same false value for O is Then, the probability that S 1 and S 2 provide different values on an object O , denoted by P for convenience, is Following the Independent-values assumption, the conditional probability of observing  X  is We next consider the case when S 2 copies from S 1 , denoted by S 2  X  S 1 .Therearetwo cases where S 1 and S 2 provide the same value v for an object O . First, with probability c , S 2 copies v from S 1 and so v is true with probability A ( S 1 ) and false with probability 1  X  A ( S so its probability of being true or false is the same as in the case where S 1 and S 2 are independent. Thus, we have Finally, the probability that S 1 and S 2 provide different values on an object is that of S 1 providing a value independently and the value differs from that provided by S 2 : We compute Pr (  X  | S 2  X  S 1 ) accordingly; similarl y we can also compute Pr (  X  | S 1  X  S ) . Now we can compute the probability of S 1  X  S 2 by applying the Bayes Rule. independent. As we have no apriori preference for copy direction, we set the apriori probability for copying in each direction as 1  X   X  2 .

Equation (12) has several nice properties that conform to the intuitions we discussed earlier in this section, formalized as follows.
 Theorem 2. Let S be a set of good independent sources and copiers. Equation (12) has the following three properties on S .
 1. Fixing k t + k f and k d ,whenk f increases, the probability of copying (i.e., Pr ( S 1  X  2. Fixing k t + k f + k d ,whenk t + k f increases and none of k t and k f decreases, the 3. Fixing k t and k f ,whenk d decreases, the probability of copying increases. Example 3. Continue with Ex.1 and consider the possible copying relationship between S and S 2 . We observe that they share no false valu es (all values they share are correct), ysis goes as follows.
 Thus, Pr (  X  | S 1  X  S 2 )= . 582 3  X  P 2 d = . 2 P 2 d .
 Similarly, Pr (  X  | S 2  X  S 1 )= . 028 P 2 d .

According to Equation (12), Pr ( S 1  X  S 2 |  X  )= . 5 independence is very likely. 3.2 Independent Vote Count of a Value Since even a copier can provide some of the values independently, we compute the independent vote for each particular value. In th is process we consider the data sources one by one in some order. For each source S , we denote by Pre ( S ) the set of sources that have already been considered and by Post ( S ) the set of sources that have not been considered yet. We compute the probability that the value provided by S is independent of any source in Pre ( S ) and take it as the vote count of S . The vote count computed in this way is not precise because if S depends only on sources in Post ( S ) but some of those sources depend on sources in Pre ( S ) , our estimation still (incorrectly) counts S  X  X  vote. To minimize such error, we wish that the probability that S depends on a source S  X  Post ( S ) and S depends on a source S  X  Pre ( S ) be the lowest. Thus, we use a greedy algorithm and consider data sources in the following order. 1. If the probability of S 1  X  S 2 is much higher than that of S 2  X  S 1 , we consider S 1 2. For each subset of sources between which there is no particular ordering yet, we sort We now consider how to compute the vote count of v once we have decided an order of the data sources. Let S be a data source that votes for v . The probability that S provides the probability that S provides v independently of any data source in Pre ( S ) , denoted by I ( S ) ,is The total vote count of v is  X  S  X   X  S
Finally, when we consider the accuracy of sources, we compute the confidence of v as follows.
 In the equation, I ( S ) is computed by Equation (13). In other words, we take only the  X  X ndependent fraction X  of th e original vote c ount (decided by source accuracy) from each source. 3.3 Iterative Algorithm We need to compute three measures: accuracy of sources, copying between sources, and confidence of values. Accuracy of a source depends on confidence of values; copying between sources depends on accuracy of sources and the true values selected according to the confidence of values; and confidence o f values depends on both accuracy of and copying between data sources.

We conduct analysis of both accuracy and copying in each round. Specifically, Al-gorithm A CCU C OPY starts by setting the same accuracy for each source and the same probability for each value, then iteratively (1) computes copying based on the con-fidence of values computed in the previous round, (2) updates confidence of values accordingly, and (3) updates accuracy of sour ces accordingly, and stops when the accu-racy of the sources becomes stable. Note that it is crucial to consider copying between sources from the beginning; otherwise, a data source that has been duplicated many times can dominate the voting results in the first round and make it hard to detect the copying between it and its copiers (as they share only  X  X rue X  values). Our initial deci-sion on copying is similar to Equation (12) except considering both the possibility of a value being true and that of the value being false and we skip details here.
We can prove that if we ignore source accuracy ( i.e. , assuming all sources have the same accuracy) and there are a finite number of objects in O , Algorithm A CCU C OPY cannot change the decision for an object O back and forth between two different values forever; thus, the algorithm converges.
 Theorem 3. Let S be a set of good independent sources and copiers that provide information on objects in O . Let l be the number of objects in O and n 0 be the maximum number of values provided for an object by S .The A CCU V OTE algorithm converges in at most 2 ln 0 rounds on S and O if it ignores source accuracy.
 Once we consider accuracy of sources, A CCU C OPY may not converge: when we select different values as the true values, the di rection of the copying between two sources can change and in turn suggest different true values. We stop the process after we detect oscillation of decided true values. Finally, we note that the complexity of each round is O ( | O || S | 2 log | S | ) . We now describe a case study on a real-world data set 1 extracted by searching computer-science books on AbeBooks.com . For each book, AbeBooks.com returns information provided by a set of online bookstores. Our goal is to find the list of authors for each book. In the data set there are 877 bookstores, 1263 books, and 24364 listings (each listing contains a list of authors on a book provided by a bookstore).

We did a normalization of author names and generated a normalized form that pre-serves the order of the authors and the first name and last name (ignoring the middle name) of each author. On average, each book has 19 listings; the number of different author lists after cleaning varie s from 1 to 23 and is 4 on average.

We used a golden standard that contains 100 randomly selected books and the list of authors found on the cover of each book. We co mpared the fusion r esults with the golden standard, considering missing or add itional authors, mis-ordering, misspelling, and missing first name or last name as errors; however, we do not report missing or misspelled middle names. Table 2 shows the number of errors of different types on the selected books if we apply a naive voting (note that the result author lists on some books may contain multiple types of errors).

We d e fi n e precision of the results as the fraction o f objects on which we select the true values (as the number of true values we return and the real number of true values are both the same as the number of objects, the recall of the results is the same as the precision). Note that this definition is different from that of accuracy of sources. Precision and Efficiency. We compared the following data fusion models on this data set.  X  V OTE conducts naive voting;  X  S IM conducts naive voting but considers similarity between values;  X  A CCU considers accuracy of sources as we described in Section 2, but assumes all  X  C OPY considers copying between sources as we described in Section 3, but as- X  A CCU C OPY applies the A CCU C OPY algorithm described in Section 3, considering  X  A CCU C OPY S IM applies the A CCU C OPY algorithm and considers in addition sim-When applicable, we set  X  = . 2 , c = . 8 ,  X  = . 2and n = 100. Though, we observed that ranging  X  from .05 to .5, ranging c from .5 to .95, and ranging  X  from .05 to .3 did not change the results much. We compared similarity of two author lists using 2-gram Jaccard distance.

Table 3 lists the precision of results of each algorithm. A CCU C OPY S IM obtained the best results and improved over V OTE by 25.4%. S IM ,A CCU and C OPY each extends V
OTE on a different aspect; while each of them increased the precision, C OPY increased it the most.

To further understand how considering copying and accuracy of sources can affect our results, we looked at the books on which A CCU C OPY and V OTE generated different results and manually found the correct authors. There are 143 such books, among which A
CCU C OPY gave correct authors for 119 books, V OTE gave correct authors for 15 books, and both gave incorrect authors for 9 books.

Finally, C OPY was quite efficient and finished in 28.3 seconds. It took A CCU C OPY and A CCU C OPY S IM longer time to converge (3.1, 3.3 minutes respectively); however, truth discovery is often a one-time process and so taking a few minutes is reasonable. Copying and Sour ce Accuracy: Out of the 385,000 pairs of bookstores, 2916 pairs provide information on at least the same 10 books and among them A CCU C OPY S IM found 508 pairs that are likely to be dependent. Among each such pair S 1 and S 2 ,if the probability of S 1 depending on S 2 is over 2/3 of the probability of S 1 and S 2 being dependent, we consider S 1 as a copier of S 2 ; otherwise, we consider S 1 and S 2 each has .5 probability to be a copier . Table 4 shows the bookstores whose information is likely to be copied by more than 10 bookstores. On av erage each of them prov ides information on 460 books and has accuracy .75. Note that among all bookstores, on average each provides information on 28 books, conformi ng to the intuition that small bookstores are more likely to copy data from large ones. Interestingly, when we applied V OTE on only the information provided by bookstores in Table 4, we obtained a precision of only .58, showing that bookstores that are large and copied often actually can make a lot of mistakes.

Finally, we compare the source accuracy co mputed by our algorithms with that sam-pled on the 100 books in the golden standard. Specifically, there were 46 bookstores that provide information on more than 10 books in the golden standard. For each of them we computed the sampled accuracy as the fraction of the books on which the book-store provides the same author list as the golden standard. Then, for each bookstore we computed the difference between its accu racy computed by one of our algorithms and the sampled accuracy (Table 5). The source accuracy computed by A CCU C OPY S IM is the closest to the sampled accuracy, indicatin g the effectiveness of our model on com-puting source accuracy and showing that cons idering copying between sources helps obtain better source accuracy. This paper presented how to improve truth d iscovery by analyzing accuracy of sources and detecting copying between sources. We d escribe Bayesian models that discover copiers by analyzing values shared between sources. A case study shows that the pre-sented algorithms can significantly improve accu racy of truth discovery and are scalable when there are a large number of data sources.

Our work is closely related to Data Provenance , which has been a topic of research for a decade [4, 5]. Whereas research on data provenance is focused on how to represent and analyze available provenance information, our work on copy detection helps detect provenance and in particular copying rela tionships between dependent data sources.
Our work is also related to analysis of trust and authoritativeness of sources [1 X 3, 10, 9, 12] by link analysis or source behavior in a P2P network. Such trustworthi-ness is not directly related to source accuracy.

Finally, various fusion models have been proposed in the literature. A comparison of them is presented in [11] on two real-world Deep Web data sets, showing advantages of considering source accuracy togeth er with copying in data fusion.

