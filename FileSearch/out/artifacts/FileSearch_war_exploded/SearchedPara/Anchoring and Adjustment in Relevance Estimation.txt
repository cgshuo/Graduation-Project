 People X  X  tendency to overly rely on prior information has been well studied in psychology in the context of anchoring and adjustment . Anchoring biases pervade many aspects of human behavior. In this paper, we present a study of anchoring bias in information retrieval (IR) settings. We provide strong evidence of anchoring during the estimation of document relevance via both human rele-vance judging and in natural user behavior collected via search log analysis. In particular, we show that sequential relevance judgment of documents collected for the same query could be subject to an-choring bias. That is, the human annotators are likely to assign different relevance labels to a document, depending on the qual-ity of the last document they had judged for the same query. In addition to manually assigned labels, we further show that the im-plicit relevance labels inferred from click logs can also be affected by anchoring bias. Our experiments over the query logs of a com-mercial search engine suggested that searchers X  interaction with a document can be highly affected by the documents visited imme-diately beforehand. Our findings have implications for the design of search systems and judgment methodologies that consider and adapt to anchoring effects.
 Information Systems [ Information Retrieval ]: Evaluation of re-trieval results X  Relevance assessment
Consider a simple experiment; which involves estimating the to-tal value of the following mathematical equation in five seconds: What was your estimate? Now, how about the following equation? It is immediately apparent that the second equation is the same as the first but in reverse order, thus the estimate should not change. c In 1974, Tversky and Kahneman [ 12 ] conducted the same exper-i me nt and presented the first sequence to a group of subjects and the second to another group. They noted that the median estimate was 512 for the subjects in the first group and 2250 for those in the second. 1 This drastic difference between the median estimates of t he two groups can be explained by the anchoring effect  X  also referred to as anchoring bias [12 ]. Anchoring  X  or focalism  X  is a cog nitive bias that explains the human tendency to rely heavily on first presented information ( anchor ) when making decisions. In the example above, given the short amount of time permitted for cal-culation, the subjects were subconsciously biased by the first few numbers in equations and reached significantly different estimates across the two groups. Anchoring effects have been studied in a range of settings. Northcraft and Neale [7 ] showed that the listed pri ce of a property affects how much people  X  including experts  X  are willing to pay for it despite having access to comprehensive information about the quality factors. Wansink et al. [ 14 ] demon-str ated that consumers are likely to purchase more products when they are presented in multiple-unit prices and purchase limits (e.g.,  X  X n sale  X  6 cans for $3  X  versus  X  X n sale  X  50  X   X ). Here, the number of units acts as the anchor and biases consumer behavior.
In this paper, we study the anchoring effect in IR. In particular, we focus on how anchoring affects the relevance ratings of docu-ments. Relevance labels for documents are usually obtained either (1) in a batch form by soliciting explicit judgments from human assessors with respect to a query, or (2) in an online form where the document relevance is inferred by using implicit feedback e.g., time spent on a document (dwell time) from search log data. We first demonstrate that the relevance label of a document is affected by the judgment that was made on an immediately preceding doc-ument. We further focus on relevance inferred from dwell time and show that the relevance of the previously-clicked document can also have a significant impact on the time searchers spend on the current document, which could significantly affect relevance in-ferred via dwell time. While the notion of relevance in IR has been studied for decades [ 8], we believe that our study is the first inves-t ig ation of anchoring bias on implicitly inferred relevance labels.
Our results suggest that (1) the biases introduced by the rele-vance of the last labeled/clicked document should be considered both for batch and online evaluation, (2) that dwell time estimates should be used with caution as proxy for relevance, in particular for the documents that are presented towards the bottom of a ranked list (which are likely to be considered only after higher-ranked al-ternatives), and (3) the models that infer relevance from dwell time could be improved by incorporating the relevance label (or dwell time) of the previously judged/clicked document, if available.
Th e correct value is 40320 . (c) Anchor-Type: Good the same subset of documents based on the unanchored judgments.
Relevance judgments for a query are typically collected in batches where each assessor may rate multiple documents for the same query [ 13 ]. The problem with this common practice is that the doc-ume nt(s) judged early for a given query can anchor judges X  remain-ing judgments on that topic. Carterette and Soboroff [ 1] were the firs t to report that sequential labeling of documents may affect the assigned relevance label. They observed that sequentially judged documents tend to receive the same labels, and referred to this phenomenon as  X  X utocorrelation X . However, the authors did not provide any explanation for their observations nor compare their results with unanchored datasets to establish a ground truth.
Scholer et al. [ 9] performed the first analysis of anchoring in rel-eva nce assessments by measuring the  X  X riming effect X  in relevance judgments. 2 They used the relevance of the top k lab eled docu-ments by an assessor as the anchor and analyzed how the relevance of these documents may affect the labels assigned to the documents judged subsequently. They showed that if the judges are presented with many documents of high relevance when they start assessing for a query, they tend to assign lower relevance labels to documents labeled later on (and vice versa). In our work, part of our focus is to analyze how the relevance of the previously-labeled document can affect the relevance label assigned to the current document. In contrast to Scholer et al. [9 ], our results show that when labeling a do cument, judges tend to assign the same label as the previous document they have labeled.

Our conclusions do not necessarily reject those reached by Sc-holer et al. [ 9]. The differences are caused by the type of anchor-ing that is considered in the two studies. Scholer et al. focus on long-term anchoring (top k labeled documents as the anchor) and analyze how this affects the relevance labels assigned to the doc-uments judged later. In our work, we focus on the short-term an-choring (last labeled document as the anchor) and analyze how this affects the relevance labels assigned to the document judged im-mediately after. The relevance of a single document is unlikely to have a significant effect on judges X  overall expectation of relevance. However, having seen a document of a certain relevance level, the judges might subconsciously expect the relevance level of the next
The re is a subtle difference between the anchoring and priming paradigms; in the latter, the priming information (or anchor) is of-ten externally provided by the experimenter, while in the former it is internally generated by the participants themselves [ 11 ]. do c ument judged to be similar, and hence their judgments may be affected.

For our experiments, we sampled 400 queries from a ranker train-ing dataset of the Bing search engine. For each query in this dataset there are tens of documents from which we randomly selected three. The first two documents are used as anchors ( A 1 , A 2 ) and the last document is used as the target (anchored) (  X  ) document to measure the impact of the anchoring effect. We hired three mutually ex-clusive groups of professional relevance assessors from the crowd-sourcing platform of the Bing search engine, and provided them with an identical judging guidelines for rating the relevance of docu-ments in three levels (Bad, Good, Perfect). In total we collected our judgments from 220 assessors. We used the first group of judges (81 assessors) to collect unanchored relevance labels for our target documents. Each document is judged by three different assessors from this group and is annotated based on the majority vote (al-though other consensus models can be also used instead). In 8.5% of cases (34 queries) there were ties, which were broken by select-ing one of the labels at random. Figure 1(a) depicts the distribution o f r elevance labels for target documents assigned by these judges.
The assessors in the other two groups are used to collect an-chored judgments. We refer to these assessors as anchored judges . The anchored judges first rate an anchor document ( A 1 or A pending on the group) and then rate the target document (  X  ) for a query. Therefore, the target documents are rated by both groups of anchored judges, while the anchor documents are different across the two groups. Again, each anchor and target document is rated by three different judges in each group, and the final label is de-termined by the majority vote and tie-breaking as with the unan-chored set. Judges rated a maximum of 20 (query, anchor, target) tuples. They always judged an anchor document (  X  A 1 or A then the target (anchored) document (  X   X  ). For each task, a tu-ple was randomly sampled, without replacement, from the pool of 400 total. To ensure that there are no systematic differences be-tween the anchored groups we compared the overall distribution of labels. An unpaired t -test found no statistically significant differ-ences ( p = 0 . 979 ).

The first question that we investigate is if relevance judgments are subject to anchoring effects. That is: Does anchor quality af-fect the labels assigned to target documents? We grouped the an-chored judgments collected for the target documents according to the labels assigned to their anchor documents. The dark blue bars in Figure 1 represent the probability of observing a relevance label for a target document, depending on the rating assigned to its ancho r. For instance, Figure 1(b) includes only cases where the target docu-men t was judged immediately after an anchor page which was rated as Bad by the assessor. Figure 1(c-d) are generated in a similar man-ner but for Good and Perfect anchors respectively. The red bars in Figure 1(b-d) represent the distribution of labels assigned to the sam e set of documents but by unanchored judges. It is clear from these graphs that the anchored and unanchored judgments have dif-ferent distributions despite the fact that they are computed over the same set of target documents. We applied Chi-squared tests to com-pare the label distributions for each of the three anchor types in Figure 1(b-d). The results confirm that the differences between the anc hored and unanchored groups are statistically significant (Bad anchor:  X  2 (2) = 8 . 91 , p = 0 . 0028 ; Good anchor:  X  2 (2) = 80 . 49 , p &lt; 0 . 0001 ; Perfect anchor:  X  2 (2) = 6 . 76 , p = 0 . 0093 ). This provides supporting evidence for the presence of anchoring bi-ases that can be introduced in collecting sequential relevance judg-ments from human assessors. Overall, there is a stronger anchoring effect for Good and Perfect judgments. This is expected as Good and Perfect documents both represent relevant documents and are more similar to each other than to Bad documents, which are irrel-evant. The next question that naturally arises from these findings is: Can the anchoring direction be predicted? That is, given the label of the anchor document can one make any predictions about the label that will be assigned to the target (anchored) document? The conditional probabilities in Figure 1(c-d) clearly suggest that t he most likely label for a relevant (Good, Perfect) target document is the rating assigned to its anchor. In other words, the anchoring direction is towards the anchor. However, Figure 1(b) shows that non -relevant anchors (Bad) have the opposite effect, with anchored judges being observed to be less likely to select a rating of Bad. Understanding all factors that affect the anchoring direction in rel-evance judgments is an interesting direction for future research.
Search result clickthrough rate was once commonly used to infer the relevance of documents [ 3]. Subsequently, it has been shown tha t clickthrough statistics are often highly affected by issues such as presentation bias and perceived relevance of the documents. The perceived relevance of a document is mainly based on the sum-mary (snippet) of the document presented on the result page, and can be different than the actual relevance of the document; hence, searchers may end up clicking on a document and discover that it is not relevant [ 2]. In order to overcome this problem, dwell time, the tim e spent examining a document, has been proposed as an implicit signal of relevance. Dwell time has been examined in a number of previous studies to infer searcher satisfaction [ 4, 6] and relevance [5] from observed search behavior. Over many years, a dwell time of 3 0 seconds has become a standard threshold from which to in-fer document relevance from document examination behavior [ 10 ]. Doc ument visits with dwell times exceeding that threshold have been regarded as implicit indications of relevance.

Beyond controlled experimental settings, such as that employed in the previous section, we were also interested in whether there was any evidence of anchoring effects in the wild, in naturalistic search settings such as Web search. In such settings, anchoring might affect relevance labels inferred implicitly from search behav-iors. We used six weeks of search logs from the Bing search engine. These logs contained millions of query-URL pairs on which to per-form our study. To remove geographic and linguistic variations we focused on queries generated by searchers within the United States geographic locale. The logs contained queries, the time-ordered se-quence of result clicks for each query, and dwell time on each of the landing pages reached through a result click.

In this analysis, we focused on the effects of anchoring on land-ing page dwell time. We calculated dwell time based on the time be-tween subsequent search engine interactions, including re-visits to the search result page and query reformulations. Dwell times could not be computed for the last clicks for impressions (since there was no subsequent event on which to base dwell time estimates), and these clicks were ignored in our analysis. We divided the landing page dwell times into two groups: (i) Quickback: Dwell time of 15s or less, and (ii) Satisfied: Dwell time of 30s or more. These thresh-olds were derived from previous research on implicit feedback and satisfaction modeling in Web search settings [ 4]. Documents with d we ll times ranging from 16-29s (inclusive) were excluded in or-der to simplify our analysis since it is less clear whether such dwell times are associated with satisfaction or dissatisfaction. We utilize similar terminology as used in the previous section. For queries with multiple clicks, we define the set of anchor clicks as the first clicks and the set of target clicks as the second clicks. Note that we may observe separate instances of the same (query, clicked URL) pair in both sets depending on search activity, e.g. for a given query q , a searcher may click on a document d and termi-nate the search session (unanchored), while another searcher may click on a few documents first before clicking on d (anchored). The average rank position ( r ) of the anchor clicks and the target click is significantly different (anchor click: r = 2.28, target (anchored) click: r = 4.15, independent measures t -test: p &lt; 0 . 001 ), signaling that searchers often adhere to a top-down examination strategy.
The first question that we sought to answer was whether there were differences in the dwell times for the target clicks given the nature of the anchor. Since we consider clicks within a query, we could be more confident that the searcher had the same intention with each observed click. Table 1 shows the distribution of dwell t im es (as percentages of the total count of query-click pairs in the four groups) across the four combinations of dwell times assigned to the first and the second click for a query. The table shows that in this analysis, consistent satisfaction (i.e., pairs of Satisfied clicks) is observed most frequently, whereas decreased satisfaction (i.e., transitions from Satisfied to Quickback clicks) occurs least often.
Given the raw counts that are used in computing the percentages reported in Table 1, we can also compute the extent to which they dev iate from expected given independence between the first and second click. The numbers in brackets present the percentage de-viations from the expected values given independence between the dwell time groups (i.e., for each cell, expected = (sum of row  X  sum of column) / overall total). It is clear that there is a significant deviation from the expected dwell time in the distribution of dwell times on the second (target) click for a query given the dwell time of the first (anchor) click (Chi-squared test over the frequency counts for the cells in Table 1 produces  X  2 (1) = 3139222 . 15 , p &lt; . 0001 ). This suggests that there is a strong association between the nature of the dwell time on an anchor click and the dwell time on the tar-get (anchored) click that follows. However, this analysis is only for queries with multiple clicks, and there may be limitations in consid-ering such situations alone. For instance, low quality documents X  that tend to have high Quickback rate X  X re more likely to appear in low quality search results. That is, if there is a low-relevance doc-ument returned in the top results, there might be a higher chance that the other top results are also low quality. To help address this concern, we identified a set of unanchored clicks comprising those with no preceding clicks (i.e., the first click for queries) from the same six weeks of data used in our log-based analysis. We then investigated the dwell times of the same query-URL pairs when Table 1: Distribution of dwell times across all combinations of t he two dwell time groups (Quickback (  X  15s), Satisfied (  X  30s)). Values in brackets denote percentage deviations from ex-pected given independence between dwell time bucket groups of first and second clicks. Arrows denote direction of the devia-tion (up=higher than expected, down=lower than expected). Figure 2: Cumulative dwell time distributions for anchored and u nanchored clicks, per anchor type (Quickback and Satisfied). they appear in our anchored sets. For each of the two anchor types ( Quickback and Satisfied ) we then computed the distribution of me-dian dwell times for the anchored clicks, and compared it against the distribution of median dwell times from the unanchored clicks.
Figure 2 shows the cumulative distribution of median dwell times acr oss the 60 seconds immediately following the page load (includ-ing the region from 16-29s that excluded in the earlier analysis), grouped by anchor type. At any point on the x -axis of the figure, the value on the y -axis reflects P(Dwell)  X  t , where t denotes the dwell time in seconds.

From Figure 2 we can observe clear differences in dwell time dis-tri butions between the documents in the anchored and unanchored sets. These differences are significant according to the Kolmogorov Smirnov test within each anchor type (Quickback Anchor: D = 0 . 6333 , p &lt; 0 . 001 ; SAT Anchor: D = 0 . 2833 , p = 0 . 0162 ). This demonstrates that the dwell time on the anchor document is related to the dwell time on the target page. Research on dwell time typi-cally considers the time spent on each document independently for applications such as satisfaction modeling, e.g., [ 6]. Our findings in thi s section suggest that models that interpret dwell times of a doc-ument from median dwell time across all searchers may be affected by anchoring biases. These models also need to consider the dwell time of any anchor that may be present when making inferences about the target (anchored) click, for applications such as relevance and satisfaction estimation.

The trends in our findings are clear and we analyze the aggre-gate behavior of millions of searchers, improving our confidence in the robustness of our conclusions. However, we should also ac-knowledge that there are other factors that may influence the dwell time beyond the nature of the anchor, such as those associated with searcher traits (e.g., people with a tendency to review pages quickly) or task constraints (e.g., pressing deadlines leading to the rapid review of content). Further work is required to understand the significance of these additional factors on the generalizability of our conclusions about anchoring effects and dwell times.
Anchoring is a cognitive bias that explains the human tendency to rely heavily on first presented information ( anchor ) when mak-ing decisions. In this paper, we studied how anchoring may af-fect human perception of relevance during relevance judging and in the examination of search results in naturalistic search settings. In particular, we focused on (1) relevance labels obtained from rele-vance assessors by obtaining explicit judgments, and (2) relevance labels inferred from dwell time, the time spent on a document. We showed that relevance of the last labeled document can have a sig-nificant effect on the relevance label assigned to the current doc-ument during relevance assessment. Our results demonstrate that judges tend to assign different labels depending on the relevance of the previously labeled document. We showed that the impact of short-term anchoring based on preceding documents could be different to those reported previously based on the longer term an-choring [ 9]. Determining the point where one effect diminishes and t he other becomes dominant is an interesting future direction. We further demonstrated that the time searchers spend dwelling on a document is highly related to the amount of time they have spent on the last document that they clicked on, which can lead to signif-icant biases on relevance labels inferred from dwell time.
Our findings can significantly impact the evaluation of retrieval systems and the training of learning-to-rank algorithms. Our results suggest that when human generated or implicit relevance labels are used, labels assigned to previous documents need to be considered.
