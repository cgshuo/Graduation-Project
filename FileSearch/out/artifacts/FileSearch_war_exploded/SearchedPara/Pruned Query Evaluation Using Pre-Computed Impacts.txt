 Exhaustive evaluation of ranked queries can be expensive, partic-ularly when only a small subset of the overall ranking is required, or when queries contain common terms. This concern gives rise to techniques for dynamic query pruning, that is, methods for elimi-nating redundant parts of the usual exhaustive evaluation, yet still generating a demonstrably  X  X ood enough X  set of answers to the query. In this work we propose new pruning methods that make use of impact-sorted indexes. Compared to exhaustive evaluation, the new methods reduce the amount of computation performed, re-duce the amount of memory required for accumulators, reduce the amount of data transferred from disk, and at the same time allow performance guarantees in terms of precision and mean average precision. These strong claims are backed by experiments using the TREC Terabyte collection and queries.
 H.3.1 [Information Storage and Retrieval]: Content analysis and indexing  X  indexing methods ; H.3.2 [Information Storage and Re-trieval]: Information storage  X  file organization ; H.3.3 [Information Storage and Retrieval]: Information search and retrieval  X  search process ; H.3.4 [Information Storage and Retrieval]: Systems and software  X  performance evaluation .
 Experimentation, performance, algorithms.
Exhaustive evaluation of ranked queries can be expensive, par-ticularly when only a small subset of the overall ranking is required, or when queries contain common terms. For example, typical two or three word web-style queries tend to contain at least one word that is not rare, meaning that a non-trivial fraction of the docu-ments in the collection might need to be scored before the query is resolved. But if only a small number of highly scoring documents are required for presentation to the user, exhaustive query process-ing might be an extravagant expense.
 Copyright 2006 ACM 1-59593-369-7/06/0008 ... $ 5.00.

The concern for speedy query evaluation gives rise to techniques for dynamic query pruning, that is, methods for eliminating redun-dant parts of the usual exhaustive evaluation, yet still generating a demonstrably  X  X ood enough X  set of answers to the query. Given the objective of reducing the time spent on each query, only a subset of the index pointers associated with the query term can be processed. The first goal of any pruning strategy is thus to identify how best to isolate a suitable subset of each term X  X  pointers, while still retain-ing confidence in the quality of the resultant ranking. Consequent to the goal of identification is a second goal of application  X  when only a minority subset of the pointers are to be used, appropriate data structures and index organizations need to be employed so as to capitalize on the possibility of saving execution time.
In this work we propose new pruning methods that make use of impact-sorted indexes. Compared to exhaustive evaluation, the new methods reduce the amount of computation performed, reduce the amount of memory required for accumulators, reduce the amount of data transferred from disk, and at the same time allow perfor-mance guarantees in terms of precision and mean average preci-sion. These strong claims are backed by experiments using the TREC Terabyte collection and queries.
Ranked query evaluation is based on the notion of a similarity heuristic , a function that combines observed statistical properties of a document (in the context of a collection) and a query, and com-putes a numeric score indicating the likelihood that the document is an answer to the query. There have been many such functions devised over more than forty years of research (see, for example, Voorhees and Harman [2005]). In this paper we focus on the im-plementation of the term-frequency based impact-oriented compu-tation described by Anh and Moffat [2005]. This section briefly summarizes that computation, and explains why it is of interest. The key idea of the impact-based scoring regime described by Anh and Moffat is that documents are treated as independent self-modelling entities, and used holistically to assign impact scores to the terms that appear within them. To do this, the queryable terms that appear within each document are sorted by decreasing within-document frequency f d,t . Based on that ordering, each term is as-signedanimpactvaryingfrom k down to 1 , with exponentially growing numbers of terms assigned to the lower-valued buckets. The exception is a set of 600 common stop words, which, when-ever they occur, are always assigned an impact score of 1 [Anh and Moffat, 2005]. The document-term impact associated with term t in document d is denoted  X  d,t . The upper limit k is decided at index construction time, and is typically a value such as k =8 .
For example, consider a document in which n d =55 distinct terms appear, of which n s =10 are stop words. The base B of the exponentially growing set of bucket sizes is calculated via B =( n d  X  n s +1) 1 /k , and the impact buckets are allocated to con-tain ( B  X  1) B i items, where i  X  0 ... 7 , items. When rounded off to integers, these values correspond to 1 , 1 , 1 , 3 , 4 , 7 , 11 , 17 items. (The Fibonacci-ish sequence of impact bucket sizes that results from this computation is, of course, no coincidence.) That is, the most frequent non-stop word in the document is assigned an impact of 8 (in this document); the next most frequent non-stop word an impact of 7 ; and so on; until the 17 most frequent non-stop words are all assigned an impact of 1 , along with the 10 stop words.
There is a slight complication that arises when terms have equal frequency within the document. In order to avoid discriminating between terms of equal frequency, and so as to avoid any require-ment for a secondary sort key, groups of terms with equal f treated identically, and are all assigned the impact that would ac-crue to the middle term in the cluster. In the example, if there were 20 non-stop word terms with f d,t =1 then all 20 would be assigned to the impact-1 bucket, even though nominally only 17 terms should have that impact score.

In a classical TF  X  IDF interpretation, these document-term im-pacts account for both the TF component, and, because they are relative to the number of terms in the document, document length normalization. The IDF component is supplied when the corre-sponding query-term impacts are computed. Following Anh and Moffat [2005], we compute query-term weights as where f m is the maximum value of f t over the collection, and f is the number of times term t appears in the query. The set of query-term weights is then transformed to a set of integer query-term impacts by linear scaling and integer quantization so that the maximum query-term impact is exactly k . The query-term impact associated with term t in query q is denoted  X  q,t .

Finally, the similarity of document d and query q is calculated as the inner-product of the two integer impact vectors: and is a number between 0 and k 2 | q | .

Anh and Moffat [2005] give retrieval effectiveness results that show that this formulation of similarity provides highly competi-tive retrieval performance when evaluated with reference to human relevance judgments on standard sets of queries. Because it is based on integer computations, it also allows for fast implementation; but at the cost of non-trivial amounts of memory. A key objective of our investigation was to preserve the retrieval effectiveness of the impact-based approach, while reducing the resources required to implement it.
The conventional arrangement for storing the pointer lists in an inverted index is in document order , see, for example, Witten et al. [1999]. This ordering is simple, and offers compact storage, be-cause the ascending set of document numbers is amenable to a range of compression techniques. Document-sorted indexes can be processed in either document-at-a-time mode or term-at-a-time mode. In document-at-a-time operations, the index lists for all terms are processed in tandem, and a merge/intersection operation performed. Each document is assigned a final score before the next document is considered, and a running set R of the top r highest scoring documents is maintained at all times. If other query fea-tures require checking while documents are being selected  X  for example, Boolean constraints, phrase or proximity restrictions, or structural constraints  X  that checking can be included in the same document-at-a-time pass.

The alternative, term-at-a-time processing, requires an accumu-lator variable per document, and an operational regime in which each term X  X  inverted list is processed, and partial score contribu-tions are added to the accumulators of the corresponding docu-ments. Once all the terms X  lists have been processed and any nec-essary normalizations undertaken (to account for document length, for example), the r documents with the largest accumulators are identified, and returned to the user as answers.

The advantage of term-at-a-time processing is that the merge is implicit, and is based on radix-type operations rather than the more expensive (particularly if there are many query terms) comparison-based merging. The disadvantage of term-at-a-time processing is the need for the accumulators, in an array (or some other data struc-ture) indexed by document number. In a collection of 25 million documents, an array-based accumulator structure requires 100 MB for a simple ranked query; and potentially even more if complex constraints such as phrase or proximity restrictions must be checked.
A third processing mode that is described in more detail in the next section is the score-at-a-time approach, in which a suitably structured index is created, and pointers are applied in an order that is neither strictly term-based nor strictly document-based.
Finally in this section, we note that another way in which in-dexes can be categorized is by the type of information contained in each pointer. In a document-level index each pointer contains a sin-gle document number, and a weighting factor (such as an impact score  X  d,t ,oran f d,t count) to allow ranked queries to be evalu-ated. The alternative is a word-level index, in which each pointer also includes a list of ordinal offsets at which that term appears in the document. In the experiments in this paper the focus is on the evaluation of ranked queries using a document-level index; Anh and Moffat [2006] examine how best to structure the word-level indexes necessary for more complex query modalities.
To reduce similarity computation effort, the notion of pruning was introduced by Buckley and Lewit [1985] for term-at-a-time processing, and by Turtle and Flood [1995] for document-at-a-time processing. These authors reasoned that a system that correctly identifies the top r documents is no less useful than one that com-pletely scores the whole collection. In the case of the term-at-a-time approach of Buckley and Lewit, whole query terms might be dropped as a result of pruning, and when they are, both processing time and disk transfer time can be saved. On the other hand, in the case of the document-at-a-time approach of Turtle and Flood, some pointers might be dispensed with after just a cursory amount of processing, saving overall processing costs; but all inverted lists must be fetched.

Moffat and Zobel [1996] describe a mechanism for inserting additional information called  X  X kips X  into document-sorted com-pressed inverted lists in order to support a term-at-a-time process-ing strategy that they called CONTINUE . Skips are forward pointers within a compressed inverted list, and allow unnecessary sections to be passed over with minimal effort, and then decoding resumed. The other key aspect of the CONTINUE approach is the notion of -mode and AND -mode processing of index pointers. If a pointer to some document is processed in OR -mode, then it has the au-thority to nominate this document as being a potential answer, and have it considered by subsequent processing steps, even if no other query terms appear in it. Every document that is eventually scored and ranked must have been nominated by a pointer processed in -mode. On the other hand, pointers processed in AND -mode are permitted to boost the sc ores of previously nominated documents, but are not allowed of themselves to nominate documents. If all of the index pointers corresponding to some document are processed in
AND -mode, then that document will not be scored, and will not be considered as a candidate answer.

Moffat and Zobel showed that term-at-a-time processing of se-lected query terms in OR -mode, and of the remaining terms in mode, resulted in considerable reduction in the space required by accumulators compared to an exhaustive evaluation; faster query processing; and no significant effect on retrieval effectiveness.
A third mode of processing is IGNORE -mode, in which point-ers are simply not considered. With these three modes available, the term-at-a-time pruning methods explored by Moffat and Zobel [1996] can be described as follows:
The benefit of having a significant number of pointers ignored is clear  X  there will be a saving in execution cost. In particular, if the IGNORE -mode pointers in each inverted list form a contigu-ous group, they can be discarded en masse . For example, the strategy of Moffat and Zobel (see also Lester et al. [2005]) poten-tially ignores whole inverted lists, as does the pruning technique of Buckley and Lewit [1985]. Fast query processing is the result.
There is also a tangible benefit in terms of memory consumption if the number of pointers processed in OR -mode can be kept small. As was noted above, the memory space required for accumulators can be significant, and minimizing the number of OR -mode pointers allows that cost to be controlled.

Even with skips inserted, it is, however, still necessary for the whole of each term X  X  document-sorted inverted list to be read from disk. Persin et al. [1996] recognized that the transfer costs were also an issue (and one that would become more important as the relative costs of disk access and processing time continued to shift) and suggested storing the pointers in the inverted lists in decreasing within-document frequency order, rather than in increasing docu-ment number. Their frequency-sorted indexes naturally group an attractive set of OR -mode pointers close the front of each inverted list, and, equally importantly, group the unattractive IGNORE pointers together in the tail section of each inverted list; where  X  X t-tractive X  is defined as  X  X aving a high within-document frequency compared to other pointers for that term X .

Anh et al. [2001] (see also Hawking [1998]) further extended the frequency-sorted approach, and suggested that the index be stored in impact-sorted form, where the index pointers in each list are or-dered according to their overall contribution to the similarity score, as described in Section 2. Impact-sorted indexes can be used for term-at-a-time processing or for s core-at-a-time processing, in both cases making use of a set of accumulators. In the latter case, all of the inverted lists are simultaneously open, with the merge by de-creasing contribution  X  d,t  X   X  q,t , so that pointers that make big contributions to document scores are processed first. This process-ing mode provides good support for dynamic pruning  X  the front part of each inverted list can be processed in OR -mode; then a sec-tion processed in AND -mode; and then the rest of the list ignored. Provided that long lists are fetched in fixed-length blocks, rather than all at once, disk transfer savings are likely.

Table 1 summarizes these various options for index organiza-tion, processing mode, and pruning. The italicized statement in the last cell shows the main purpose of this paper  X  we describe a pruning technique suitable for score-at-a-time processing using an impact-sorted index, and show that it is possible to obtain guaran-teed performance in terms of precision and mean average precision. The new method has low memory consumption, and is also signifi-cantly faster in execution than exhaustive evaluation, because only the required part of each inverted list need be transferred from disk.
To set a baseline for those experiments, the following pseudo-code shows the use of a score-at-a-time evaluation strategy, without any pruning. The set of accumulators is denoted A ;and R is a set of r highly-scoring documents, all of which have a similarity score greater than or equal to R min . When document-term impacts are in the range k... 1 and query-term impacts are in the range k... 1 , the maximum similarity contribution from any single pointer is k The main loop or the algorithm counts through exactly these k possible values. Note also that to aid the explanation, each pointer is presumed to be of the form t, d,  X  d,t , but that the actual index has common t and  X  d,t values factored out so as to save space. [ Baseline  X  Exhaustive computation ] set A  X  X } and R  X  X } for I  X  k 2 downto 1 do end for sort set R andreturnittotheuser
Other researchers have also considered techniques for acceler-ating query processing by suppressing  X  at either index construc-tion time or query evaluation time  X  some subset of the pointers associated with each term [Harman and Candela, 1990, Brown, 1995, Soffer et al., 2001, Broder et al., 2005, Theobold et al., 2005, Strohman et al., 2005, de Moura et al., 2005], but space restrictions preclude a detailed discussion of that work. However it is worth noting that the  X  X op docs X  approach of Brown [1995] and Strohman et al. [2005] is in some ways a rudimentary form of a frequency-or impact-ordered index, with a subset of the pointers identified as being  X  X mportant X  and extracted into a first block of pointers at the start of the list. But where Brown and Strohman et al. use two levels of pointer importance, we are able to use k .
We first introduce yet another mode of pointer processing, which we denote REFINE -mode. Pointers encountered in REFINE -mode are processed only if they relate to a document that is a member of R , the set of documents that have the highest scores. This stage of processing is done once the set of answers has been determined, but before their final ordering is fixed. If we seek only to guarantee precision at r documents retrieved, we can skip any REFINE processing; but if we seek to guarantee mean average precision, or some other metric that factors in document ranks, then the top r documents must be ordered correctly as well as selected correctly.
Figure 1 shows the relationship between REFINE -mode and the other modes, and the way that the size of the set A of interesting documents varies. The most influential pointers  X  those that should be processed in OR -mode  X  appear at the front of each inverted list. Partway through the processing of each inverted list, processing changes to AND -mode, only using each pointer if it augments the score of a candidate that has already been nominated. The transi-tion is made once it is certain that no document that has not already been nominated can be a member of the final answer set.

At a second transition point, also discussed below, a set of r an-swers has been generated, and all that remains is to place them into their final rank order for presentation to the user. At this transition, all other non-zero accumulators can be dispensed with, and processing restricted to the set R of documents d for which A d  X  R min ,where R min is the score of the r th highest document. That is, | R | X  r , and ties in scores are properly accounted for. Figure 1: Modes used when processing each impact-ordered in-verted list. Each inverted list contains k blocks of equal-impact pointers. Each block is processed in one of four modes, with only the highest impact pointers handled in OR -mode. The graph un-derneath shows the growth, stabilization, and then reduction in the number of accumulators that are active as the query is processed.
Finally, at some third transition point, it can be known that the ordering of the top r documents is final, even if not their exact scores, and pointer processing is halted. The remainder of each of the inverted lists is ignored, and r of the documents currently in R (or all of them, if | R | = r ) are presented to the user.
To facilitate computation of the three transition points, each ac-cumulator A d becomes a more complex structure. As well as record-ing the current score of the document d , it also tracks (as the quan-tity T d ) which terms contributed to that score. In addition, the quantity next t is maintained for each term t , to indicate the I value (starting at k 2 andcountingdownto 1 )atwhichterm t will supply its next block of pointers. If all blocks in the impact-sorted list for term t have been processed, then next t is set to zero. Each accumu-lator A d can then also have associated with it an inferred quantity M d , the maximum score that document d can aspire to based on the evidence encountered so far. This set of observations leads to the following processing logic: [ Method A  X  Optimized exact computation ] set A  X  X } and T  X  X } and R  X  X } set mode  X  OR -mode initialize next t for each term t for I  X  k 2 downto 1 do | q | # Exhaustive Method A, r =20 Method A, r =1 , 000
REFINE | A | ( X 000) OR (%) AND (%) REFINE | A | ( X 000) 1 11,410 277.3 277.3 38.7 0.0 0.0 107.2 39.0 0.0 0.0 108.0 2 15,200 1,859.9 1,732.9 3.3 19.5 37.1 56.1 6.9 36.6 41.7 122.0 3 10,258 5,866.8 4,856.8 1.6 47.2 38.0 88.6 3.8 57.5 29.8 219.2 4 6,248 12,003.4 7,459.1 1.2 53.1 21.2 136.8 2.9 61.5 12.0 331.9 5 3,361 17,960.8 9,217.3 1.0 56.8 10.7 171.0 2.4 63.5 2.8 407.4 6+ 3,513 33,488.3 14,263.6 0.7 64.5 3.9 226.4 1.7 66.9 0.6 519.0 All 49,990 6,893.9 4,141.2 1.6 53.4 17.5 104.2 3.2 60.4 11.9 212.1 Table 2: Average number of pointers processed per query in -mode, and REFINE -mode, and number of non-zero accumulators end for sort set R andreturnittotheuser Now consider the three conditional expressions that govern the tim-ing of the mode transitions. The first, condition1 ,mustbefalse until the r th highest similarity score among the documents that do have accumulators is at least as large as any score M d that can be attained by a document d which still has not yet been nominated by any of the query terms. But if document d has not yet been struck by any query terms, then M d is simply the sum of the next quantities, and condition1 can be expressed as: where q is the set of terms that comprise the query. That is, no document that has not yet been scored can enter the top r once the sum of the next values is less than the score already assigned to the r th largest accumulator.

Once AND -mode has been commenced, accumulator values A d continue to be non-decreasing, while at the same time, the M ues are non-increasing. The transition to REFINE -mode can thus be triggered when all M d values for documents that are in A but not in R are less than R min .Thatis, condition2 can be expressed as: For a document d and an accumulator A d , M d is calculated as
The final transition is governed by condition3 , a constraint that is established if But situations in which not all of the top r answers contain all of the query terms can also arise. Suppose that R is ordered by decreasing accumulator score, so that A R 1  X  A R 2 , and so on; then processing can be stopped when This latter expression is the condition3 used in Method A. Table 3: Average number of accumulator updates during mode and REFINE -mode processing in Method A, as a percentage of the total number of pointers processed while operating in each mode. Experimental details are as for Table 2.

Table 2 shows the effect of this method compared to the exhaus-tive evaluation strategy used by Anh and Moffat [2005]. A set of approximately 50 , 000 queries are broken into groups according to their length | q | . Two different retrieval depths are used, r =20 and r =1 , 000 , and applied to the 25 million documents in the 426 GB TREC GOV2 test collection (the cost of exhaustive evaluation does not depend on r ).

As can be seen from the table, the number of pointers needing to be processed in OR -mode is a very small fraction of the number of pointers indicated by the query terms, and averages just 1 . 6 %when r =20 ,and 3 . 2 %when r =1 , 000 . This translates into a dramatic saving in the amount of space required for accumulators, and even when r =1 , 000 answers are required, the number of accumulators averages around 200 , 000 , or approximately 1 % of the size of the collection. We reiterate that with this number of accumulators the ranking of the top r documents can be guaranteed to match the ranking that is delivered by an exhaustive computation.

The number of pointers processed in AND -mode is also bounded in Method A. Table 2 shows that for typical queries a further 50  X  60 % of pointers need to be processed if precision is to be guaran-teed, and then another 10  X  20 % if the exact rank ordering (and thus mean average precision) must be pr otected. Conversely, approxi-mately 25  X  30 % of all pointers can be ignored without there being any effect on the ranking. In addition, if the effort required to pro-cess a pointer in AND -mode (and hence REFINE -mode) is less that the effort required to process an OR -mode pointer, then a non-trivial saving in execution cost arises.
Q Topics 701 X 750 Topics 751 X 800 P@20 MAP P@20 MAP 10 0.4888 0.2332 0.5396 0.2813 30 0.5122 0.2542 0.5648 0.3107 50 0.5173 0.2571 0.5635 0.3145 100 0.5163 0.2620 0.5626 0.3199 Table 4: Retrieval effectiveness obtained on two different sets of recent TREC topics using the GOV2 collection and Method B pro-cessing. The two columns  X  X @20 X  note precision at r =20 doc-uments retrieved, while the two columns  X  X AP X  list mean average precision at r =1 , 000 documents retrieved. The variable Q cor-responds to the target percentage of the available post-pointers that are processed in AND -mode.

Table 3 shows further detail of the Method A computation. It lists, for the same set of queries, the fraction of REFINE -mode pointers that actually result in an increase to an ac-cumulator value A d . What is quite remarkable is that only a tiny fraction  X  around 2 % or so  X  of the pointers processed in mode actually strike candidates, and almost no pointers at all of the ones processed in REFINE -mode make tangible contributions. A clear conclusion is that the key component of processing dur-ing AND -mode and REFINE -mode is testing document numbers d for membership of (respectively) the sets A and R . The actual cost of changing A or R is not critical, since it happens so infre-quently. Our implementation exploits this observation, and once the OR -mode pointers have been exhausted, it flattens the accumu-lator search structure (a hash table) into a dense sorted array so that document-ordered merge operations against document-sorted equal-impact inverted list blocks are efficient.

Even though accumulator updates are rare, we still wish to avoid the CPU cost associated with computing condition2 and condi-tion3 , and the space associated with the sets T d . (Computing con-dition1 is easy.) Table 3 also suggests that while condition2 and condition3 give suitable guarantees, they may be unnecessarily pes-simistic. To address these two c oncerns, we introduce a  X  X idelity control knob X  Q , which controls the percentage of the post-mode pointers that are to be processed for any given query. We call this heuristic approach Method B ; space limits preclude the inclu-sion of full pseudo-code. In Method B, pointers are processed in the same manner as described in Method A in OR -mode until con-dition1 is satisfied. Then the total number of remaining pointers is multiplied by Q , and processing resumed in AND -mode until that many more pointers have been processed. That is, when Q =0 %, the final ranking is based on the OR -mode pointers alone; and when Q = 100 %, the all pointers are considered and the ranking is guar-anteed. Method B has the same space requirement as Method A.
Table 4 gives effectiveness results for Method B for two differ-ent sets of TREC Terabyte Track topics, those used in 2004 and 2005, and for two different retrieval effectiveness metrics. When Q =0 retrieval effectiveness is relatively poor. But by the time Q = 30% , both effectiveness metrics indicate good retrieval per-formance, on both sets of queries, and there seems little value in processing further pointers. Note that even when the scores of the top r documents are guaranteed by using a large value of Q ,the need for tie-breaking in the similarity scoring regime, and in the effectiveness evaluation metric that then gets fed those scores, can lead to small variations in effectiveness.

Q Exhaustive Pruned MB queries/sec MB queries/sec 30  X   X  1.5 12.1 100 95.0 4.2 1.5 7.5 Table 5: Average memory footprint per query, in megabytes, and query throughput rates, in queries per second, using the TREC Ter-abyte Efficiency Track protocols. For each of 49 , 990 queries, the TREC document identifiers for the top-ranked 20 documents are output, with all processing on each query completed before the next query is initiated. The hardware used is a 2 . 8 GHz Intel Pentium IV with 1 GB of RAM and 250 GB local SATA disk. Figure 2: Tradeoff as Q is varied in Method B, using collection GOV2 : (a) with effectiveness measured using precision at 20 ,and a retrieval depth of r =20 ; and (b) with effectiveness measured using mean average precision at a retrieval depth of r =1 , 000 .
Table 5 shows query throughput rates and memory requirements for Method B using three different values of Q , on a stream of nearly 50 , 000 queries. The reference point for this table is the Ex-haustive approach; because it processes all pointers in the only sensible accumulator data structure is an array, and the memory space required is high. Method B has much more compact memory requirements, and in part because of this, executes more rapidly, even when Q = 100 %. The other reason for the increase in throughput is that the revised implementation processes AND pointers faster than OR -mode pointers, and in the column  X  X xhaus-tive X  all pointers are regarded as being OR -mode.

Figure 2 combines Tables 4 and 5, to show how Q serves as a tradeoff knob controlling execu tion time. Note that the throughput rates are derived from the 49 , 990 Efficiency Track queries ( 10 of the original 50 , 000 TREC queries have no matching documents in our system); whereas the effectiveness results are from the TREC topics 701 X 750, and 751 X 800, respectively, so in this sense the graph does not represent a single set of data. It does, however, show in a convincing manner that use of Q =30 %, and only processing around a third of the post-OR -mode pointers (and thus completely ignoring around two thirds of the pointers for any given query) gives excellent retrieval effectiveness, at close to triple the query throughput rates of the exhaustive computation.
Section 6 showed that the new impact-sorted pruning regime greatly speeds query processing. This section draws on two sets of comparable results to further illustrate the gains achieved.
The first reference point we use was established by Strohman et al. [2005], who implemented and tested a document-at-a-time pruning mechanism based on the max score mechanism of Turtle and Flood [1995]. Strohman et al. did not have access to the 50 , 000 TREC queries at the time they carried out their experiments. Nev-ertheless, their results represent a useful yardstick. Compared to the requirements of the 2005 TREC Efficiency Track, Strohman et al. have retrieved r =10 rather than r =20 documents; have worked with 50 rather than 50 , 000 queries; and have carried out a  X  X arm-up X  run that was not part of the timing. On the other hand, their index is a word-level one that includes word positional information. These pointers are unused during ranked querying, and if stored with the document pointers in a fully interleaved man-ner, can markedly slow ranked query evaluation [Anh and Moffat, 2006]. Strohman et al. have also designed their implementation to allow future support of complex que ries involving positional and/or structural constraints, and the extent to which this flexibility affects operational speed on simple ranked queries is unknown.

Nevertheless, the fundamental comparison remains: on a 2 . 6 GHz machine, and with pruning enabled, Strohman et al. require 1 . 73 seconds to evaluate each query. Even taking into account the dif-ference in processor clock speeds, this is more than 10 times slower than the speeds attained in this paper.

As a second reference point, we draw on the set of data submitted by the participants of the 2005 TREC Terabyte Efficiency Track. Figure 3 shows the tradeoff between effectiveness and economy (measured in units of  X  X ueries per second per thousand American dollars X ), using data from Table 4 of Clarke and Scholer [2005]. Hardware costs are listed in that table in terms of 2005-value US dollars; the single processor system used in our experiments was purchased mid-2004 and has a 2005-equivalent hardware cost of approximately $US 750 . In each case the top r =20 documents for each of 50 , 000 web queries are identified, and their TREC doc-ument identifiers retrieved. Figure 3 shows in no uncertain terms the magnitude of the gain in performance that has been achieved by our pruning approach, and provided that Q  X  30 %, that gain is not in any way at the expense of retrieval effectiveness.

To allay possible concerns about the validity of the comparison made in Figure 3, we note that the same hardware was also used to build the index (with the compressed source data and all temporary and permanent index files stored on the local 250 GB disk), in a construction time of 6.5 hours.
The evaluation of the previous two sections has presumed that only pure ranked queries are being handled. But there are also sit-uations in which Boolean filters  X  perhaps expressed as term prox-imity or document structure relationships  X  must be checked prior to any similarity evaluation, and only documents that pass the filter Figure 3: Performance of retrieval systems, comparing effective-ness (P@20) and economy, the latter defined in units of queries per second per thousand dollars of hardware. In all cases a retrieval depth of r =20 is assumed. Data for the 2005 TREC systems is from Table 4 of Clarke and Scholer [2005], and covers two submit-ted runs from eight retrieval systems. The runs for the new system use the same methodology as in Table 5 and Figure 2(a). Figure 4: Distribution of impact values within inverted lists. The three bar graphs represent the average impact distribution over terms in the GOV2 test collection with f t =1 ( 10 , 725 , 138 terms); 1 , 000  X  f t &lt; 2 , 000 ( 22 , 390 terms); and 1 , 000 , 000 2 , 000 , 000 ( 641 terms). should be considered for ranking. When complex queries of this sort are being handled, document-at-a-time processing is the most practical approach [Strohman et al., 2005].

Processing in document-at-a-time order does not, however, mean that the inverted lists in the index must be document-sorted. All that needs to be done is that the document-based merge operation take place at the impact block level rather than the term level. For example, if the query has | q | terms, then the merge operates on k input cursors, rather than | q | . In addition, the difference is not as great as it might seem, because of the highly variable lengths of the impact blocks, and of the fact that to a certain extent they are mutually exclusive  X  factors that mean that a heap-based merge is not the best document-at-a-time strategy for impact-sorted indexes.
As evidence of these features of impact blocks, Figure 4 shows the distributions of impact values within inverted lists, averaged over three different groups of f t values, for the collection Rare terms (those with f t =1 ) tend to only occur sparingly in documents in which they do appear, and are assigned impact scores of 3 or 2 . Common terms, shown in the rightmost bargraph of the figure, are more likely to be assigned high impacts. (Note that this counter-intuitive bias is reversed when the query impacts are introduced, since they include an IDF factor.) But even so, within any given inverted list, the majority of the pointers are in the  X  3 block, and the impact blocks for other values are much shorter.
Index organization and Throughput processing mode (q/sec) Document-sorted, document-at-a-time 2.4 Impact-sorted, document-at-a-time 1.7 Impact-sorted, document-at-a-time (improved) 3.2 Table 6: Ranked query throughput rates for document-at-a-time processing using an impact-sorted index, assuming exhaustive pro-cessing and no pruning, using 50 , 000 TREC Efficiency Track queries, and identifying r =20 answers for each query.

Table 6 gives query throughput rates for exhaustive document-at-a-time processing. The first row shows the speed obtained using a document-sorted index. Relative to the speed shown in Table 5 for  X  X xhaustive X  processing, use of a document-sorted index in-volves more decoding steps out of the index, and a more complex processing logic.

The second row of Table 6 shows the throughput of the  X  X erging all the impact blocks X  approach; and the final row shows through-put for an improved mechanism that is sensitive to the lengths of the impact blocks, and uses a length-biased merge rather than an egal-itarian heap. These processing times may also be compared with those shown in the last row of Table 5; pure impact-sorted process-ing is faster, but document-at-a-time processing using an impact-sorted index is also perfectly sensible, and is faster than using a document-sorted index for these typical queries. That is, there is no reason to prefer a document-sorted index, even if a filtering step is required as a precursor to ranked querying. The impact-sorted in-dex also provides flexible support for the  X  X op docs X  approach to the max score pruning regime for complex queries that is proposed by Strohman et al. [2005].

Our final comment in this regard concerns index size  X  the impact-sorted index is slightly more compact than the document-sorted one, and (including all associated vocabulary information) occu-pies 6 . 12 GB for the GOV2 collection, compared to 6 . 77 GB for a document-ordered index.
We have described a new method for dynamic query pruning using an impact-sorted index, including compelling experimental evidence of the versatility and speed of the new system. The same index arrangement can also be used for fast document-at-a-time querying in support of complex queries.
 Acknowledgment . This work was supported by the Australian Re-search Council, by the ARC Center for Perceptive an d Intelligent Machines in Complex Environments, and by the NICTA Victoria Laboratory. We also thank the referees for their careful comments.
