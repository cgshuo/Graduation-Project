 Making revisions is central to improving a student X  X  writings, especially when there is a helpful instruc-tor to offer detailed feedback between drafts. How-ever, it is not practical for instructors to provide feedback on every change every time. While mul-tiple intelligent writing assistants have been devel-oped (Writelab, 2015; Draft, 2015; Turnitin, 2016), they typically focus on the quality of the current es-say instead of the revisions that have been made. For example, Turnitin identifies weak points of the es-say and gives suggestions on how to improve them; it also assigns an overall score to the essay so stu-dents can get a coarse-grained feedback on whether they are making progress in their revisions. How-ever, without explicit feedback on each change, stu-dents may inefficiently search for a way to optimize the automatic score rather than actively making the existing revisions  X  X etter X . Moreover, because stu-dents are the target users of these systems, instruc-tors typically can neither correct the errors made by the automatic analysis nor observe/assess the stu-dents X  revision efforts.

We argue that an intelligent writing assistant ought to be aware of the revision process; it should: 1) identify all significant changes made by a writer between the essay drafts, 2) automatically determine the purposes of these changes, 3) provide the writer the means to compare between drafts in an easy to understand visualization, and 4) support instructor monitoring and corrections in the revision process as well. In our previous work (Zhang and Litman, 2014; Zhang and Litman, 2015), we focused on 1) and 2), the automatic extraction and classifica-tion of revisions for argumentative writings. In this work, we extend our framework to integrate the au-tomatic analyzer with a web-based interface to sup-port student argumentative writings . The purpose of each change between revisions is demonstrated to the writer as a kind of feedback. If the author X  X  revi-sion purpose is not correctly recognized, it indicates that the effect of the writer X  X  change might have not met the writer X  X  expectation, which suggests that the writer should revise their revisions. The framework also connects the automatic analyzer with an inter-face for the instructor to manually correct the analy-sis results. As a side benefit, it also sets up an anno-tation pipeline to collect further data to improve the underlying automatic analyzer. The design of ArgRewrite aims to encourage stu-dents to concentrate on revision improvement: to iteratively refine the essay based on the feedback of the automatic system or the writing instructor. Our framework consists of three components, ar-ranged in a server client model. On the server side, the automatic analysis component extracts revision changes by aligning sentences across drafts and in-fers the purposes of the extracted revisions; this may reduce the writing instructor X  X  workload. On the client side, a web-based rewriting assistant inter-their revisions from the server, make changes to the essay and submit the modified essay to the server for another round of analysis. The interface is also accessible to the writing instructor and allows the instructor to have a quick overview of the students X  revision efforts. Another client side interface is a Java-based revision correction component 2 , which allows the writing instructors to override the results of the automatic analysis and upload the corrected feedback to the server.

As demonstrated in Figure 1, the complete pro-cess of the student X  X  writing using our system starts with the student X  X  rewriting and submission of the essay. The student writes the first draft of the essay before using our system and then modifies the orig-inal draft in our rewriting assistant interface. The submitted writings are automatically analyzed im-mediately after the receipt of the student X  X  submis-sion. Afterwards the instructor can manually cor-rect the analysis results if necessary. The student can choose to view the analysis results immediately after the completion of automatic revision analysis or wait until the analysis results were corrected by the instructor. After receiving the analysis feedback, the student can choose to continue with the cycle of essay revising until the revisions are satisfactory. 3.1 Automatic analysis Revision extraction. Following our prior work, we extracted revisions at the level of sentences by align-ing sentences across drafts. An added sentence or a deleted sentence is treated as aligned to null. The aligned pairs where the sentences in the pair are not identical are extracted as revisions. We first use the Stanford Parser (Klein and Manning, 2003) to break the original text into sentences and then align the sentences using the algorithm in our prior work (Zhang and Litman, 2014) which considers both sentence similarity (calculated using TF*IDF score) and the global context of sentences.

Revision classification. Following the argumen-tative revision definition in our prior work (Zhang and Litman, 2015), revisions are first categorized to Content ( Text-based ) and Surface 3 according to whether the revision changed the meaning of the es-say or not. The Text-based revisions include The-sis/Ideas ( Claim ), Rebuttal , Reasoning ( Warrant ), Evidence , and Other content changes ( General Con-tent ). The Surface revisions include Fluency ( Word-usage/Clarity ), Reordering ( Organization ) and Er-rors ( Conventions/Grammar/Spelling ). On the basis of later work, the system includes the two new cat-egories Precision 4 and Unknown 5 . Using the cor-pora and features defined in our prior work, a multi-class Random Forest classifier was trained to auto-matically predict the revision purpose type for each extracted revision. 3.2 Rewriting assistant interface Our rewriting assistant interface is designed with several principles in mind. 1) Because the revision classification taxonomy goes beyond the binary tex-tual versus surface distinction, we want to make sure that users don X  X  get lost distinguishing different cate-gories; 2) We want to encourage users to think about their revisions holistically, not always just focusing on low-level details; 3) We want to encourage users to continuously re-evaluate whether they succeeded in making changes between drafts (rather than fo-cusing on generating new contents). Thus, we have designed an interface that offers multiple views of the revision changes. As demonstrated in Figure 2, the interface includes a revision overview interface for the overview of the authors X  revisions and a revi-sion detail interface that allows the author to access the details of their essays and revisions.

Inspired by works on learning analytics (Liu et al., 2013; Verbert et al., 2013), we design the re-vision overview interface which displays the statis-tics of the revisions. Following design principle #1, the revision purposes are color coded and each pur-pose corresponds to a specific color. Our prior work (Zhang and Litman, 2015) demonstrates that only Text-based revisions are significantly correlated with the writing improvement. To inspire the writers to focus more on the important Text-based revisions, cold colors are chosen for the Surface revisions and warm colors are chosen for the Text-based revisions. The statistics and the pie chart provide a quantitative summary of the writer X  X  revision efforts. For exam-ple, in Figure 2, the writer makes many changes on the Fluency (15) of sentences but makes no change on the Thesis/Ideas (0). To allow the users to con-centrate on improving one revision type at a time, the interface allows the user to click on a single re-vision purpose type and view only the specified re-visions.

Following our design principle #2, the revision map in both interfaces presents an at-a-glance vi-sual representation of the revision. This design is inspired by (Southavilay et al., 2013). Each sen-tence is represented as a square in the map. The left column of the map represents the sentences in the first draft and the right column represents the sen-tences in the second draft. The paragraphs within one draft are segmented by blanks in the map. The aligned sentences appear in the same row. The added/deleted sentences would be aligned to blank in the map. The revision map allows a user (either an instructor or a student) to view the structure of the essay and identify the locations of all the changes at once. For example, in Figure 2, the user can quickly identify that the writer aims at improving the clar-ity and soundness of the third paragraph by making a Rebuttal modification on the second sentence and Fluency modifications on all other sentences. The user can also click on the square to view the details of the revision in the revision text area region of the revision detail interface .

To encourage students to make revisions (design principle #3), in the revision detail interface the revi-sion text area region highlights the revisions (color-coded by the revision categories) in the essay and allows the writer to modify it directly. The writer clicks on the text to read the revision and examine whether the revision purpose is recognized by the the aligned sentences to help the writer identify the differences between two drafts. In the example the writer can see that their  X  X vidence X  change is recog-nized, indicating that the revision effort is clear and effective. If the writer finds out that their real revi-sion purpose is not recognized, they can modify the essay in the textbox directly and submit the essay to the server when all the edits are done. 3.3 Revision correction The revision correction tool is developed for instruc-tors only. The instructor loads the revision annota-tion files from the server, corrects the analysis re-sults and uploads the corrections to the server. As demonstrated in Figure 3, the tool includes a sen-tence alignment correction interface and a revision purpose correction interface. The instructor first corrects the sentence alignment errors and then se-lects the revision purposes for the re-aligned or mis-labeled sentence pairs. The correction actions of the instructors will be recorded and used to improve the analysis accuracy of the automatic analysis module. In this work we demonstrate a novel revision assis-tant for argumentative writings. Comparing to other assistants, the system focuses on inspiring writers to improve existing revisions instead of making new revisions. The system takes the writer X  X  drafts as the input and presents the revision purposes (ana-lyzed manually or automatically) as the feedback. The writer revises iteratively until the purposes of the revisions are clear enough to be recognized.
In the future we plan to develop and incorporate the function of revision quality analysis, which not only recognizes the revision purpose, but also evalu-ates the quality of the revision (whether the revision weakly/strongly improves the essay). We are also about to begin a user study to evaluate the system. We want to thank the members of the SWoRD and ITSPOKE groups for their helpful feedback and all the anonymous reviewers for their suggestions.
This research is funded by NSF Award #1550635 and the Learning Research and Development Center of the University of Pittsburgh.
