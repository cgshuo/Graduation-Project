 Users nowadays are overwhelmed by the vast amount of information on the Web. Al-though they can find information for a specific topic easily using search engines, they still have di ffi culty in finding more detailed aspects of a topic before reading dozens of Web documents returned. For example, it is a non-trivial task to make a comprehensive survey of a topic such as  X 9 / 11 attacks X . Related reports may cover various aspects (i.e., sub-topics) including  X  X ttackers and their motivation X ,  X  X he rescue attempts X ,  X 9 / 11 in-vestigations X , etc. Each sub-topic may furt her contain a set of relat ed incidents, e.g.,  X 9 / 11 investigations X  has a series of related incidents along the timeline, such as  X  X he NSA intercepted communications that pointed to bin Laden on Sep.11, 2001 X ,  X  X BI released photos of the 19 hijackers on Sep.27, 2001 X , etc. Thus, discovering sub-topics and related incidents for a s pecific topic in a text corpus and summarizing them will greatly facilitate user X  X  navigation in the corpus space.

The above problems can be partially solved by topic decomposition and text summa-rization, which was first proposed systematically by Chen and Chen[1]. Their solution is called TSCAN (Topic Summarization and Content ANatomy). TSCAN equals to la-tent semantic analysis (LSA) based on th e singular value decomposition (SVD)[2]. We also study this problem in this paper. However, our solution is based on Non-negative Matrix Factorization (NMF)[3]. NMF has b een demonstrated advantages over SVD in latent semantic analysis, document clustering [4]. In our work, we model the documents of a specific topic as a terms-by-sentences matrix. NMF is used to factorize the matrix into a non-negative sub-topic matrix and a non-negative encoding matrix. Each row of the encoding matrix is examined to extract incidents and their summaries. Summary for each sub-topic is generated by composing its incidents X  summaries. We rank sentences by analyzing the encoding matrix, and th e top ranked sentences of each sub-topic are selected as the summary for the text corpus. For a given temporal documents of a specific topic, TSCAN has following steps: Firstly, the documents are decomposed into a set of blocks. Then, a m  X  n terms-by-blocks matrix A is constructed. A i , j is the weight of term i in block j , which is computed by TF-IDF weighting scheme. The b lock association matrix B = A T A , it is factorized as follow: where D r is a r  X  r diagonal matrix where the diagonal entries are the top r eigenvalues of B .And T r is a n  X  r matrix in which each of the r columns represents a sub-topic. By examining the constitution of each columns of T r , the significant incidents of each topic aspect are detected and their summaries are generated. Then, the summary of the topic documents is obtained by combining all detected incident X  X  summary. We assume the SVD of the terms-by-blocks matrix A as follow: where both U and V are orthogonal matrices, and  X  is a diagonal matrix. The diagonal entries of  X  are the singular values of the matrix A . Each column of matrices U and V are called left-singular and right-singular vectors. Then, The squares of singular values of the matrix A (i.e.,  X  T  X  ) are equal to the eigenvalues of the matrix A T A (i.e., B )[5].InLSA,the r largest singular values with corresponding singular vectors from U and V are used to approximation the matrix A [2], i.e., Then, B can be approximated as follow: Because  X  T r  X  r are the top r eigenvalues of the matrix B , the matrix V r is equal to the sub-topic matrix T r derived by TSCAN. That is, the sub-topics derived by TSCAN corresponds to the right singular vectors with most significant singular values of A . In this paper, we focus on extractive multi-document summarization which are widely studied[6,7,8,9]. It extracts top significant s entences calculated by a set of ranking meth-ods from the documents set. is a document at time point i . We call various topic asp ects as sub-topics and define composition is to find out sub-topics ST , the incidents STI k related to sub-topic st k . Besides, we generate summaries for the text corpus D , sub-topics ST and incidents STI . Each document in D is decomposed into a sequence of sentences using sentence separation software provided by DUC[10]. 425 Rijsbergen X  X  stopwords are removed and stemming is performed. The documents in D is represented by a m  X  n terms-by-sentences matrix M ,where m is the number of terms and n is the number of sentences respectively. M i , j is computed by TF-IDF weighting s cheme. The terms set is defined 3.1 Topic Decomposition Based on NMF Non-negative Matrix Factorization (NMF) is a matrix factorization method that gener-ates positive factorization of a given positive matrix[3]. It represents object as a non-negative linear combination of part information extracted from plenty of objects and is able to learn parts of semantic features from text. Given the matrix M , NMF decom-poses M into a non-negative matrix B and a non-negative matrix E so that
We can find out B and E by minimizing the following cost function: where  X  F denotes the Frobenius norm. The above constrained optimization problem can be solved by continuously updating B and E until the cost function converges under the predefined threshold or exceeds the numbe r of repetitions[3,4]. The update rules are as follows (1  X  i  X  m ,1  X  j  X  n and 1  X  k  X  r ):
The r columns of B embed the so called sub-topics and each column of E is the en-coding. We refer B as the sub-topic matrix and E as the encoding matrix. Each sentence s can be represented by a linear comb ination of sub-topics. i.e., where m j is j -th sentence (i.e., j -th column of M )and e j represents the j -th column of matrix E .Theentry B i , k indicates that the degree of term t i belongs to sub-topic k , while E , j represents that the degree of sentence s j associates with sub-topic k .
Because the sentences set S = { s 1 , s 2 ,..., s j ,..., s n } is indexed chronologically, the row k of encoding matrix E (i.e., e k , j ,1  X  j  X  n , we refer it as sub-topic encoding vector) also denotes the relative strength of sub-topic st k along the timeline. Herein, a list of continuous elements of e k , j (1  X  j  X  n ) with high bursty values can be regarded as an incident related to sub-topic k . Fig. 1 shows a sub-topic encoding vector of document cluster  X  X 30001t X  in DUC 2004[10] after applying NMF with r = 10. In Fig. 1, the encoding value is bursty around the sentence 170. It means that the sub-topic has a significant development around the sentence 170 (i.e., an incident breaks out).
The bursty detection problem is well studied in stream mining community [11]. For-mally, given an aggregate function G (here is sum), a sliding window of size w and corresponding thresholds  X  , the problem is to discover all these sub-sequences such check if where mean () and std () are the average and standard deviation function respectively. We s e t as 3 and w as 7 in our experiments. Finally, the detected bursty sequences are recognized as incidents. 3.2 Topic, Sub-topics and Incidents Summarization An interesting by-product of topic decompos ition is that the produced information can also be used to generate summary. Lee et al. [9] also use NMF to do generic document summarization. In their work, the rank of each sentence is calculated as follows: nally, the top-x sentences with highest rank are chosen as summaries. We refer this method as NMF in our experiments. As point out by [8], a good summary should con-tain as few redundant sentences as possible while contain every important aspects of the documents. However, the solution of Lee et al. [9] doesn X  X  satisfy above requirements sometimes. The top-x sentences with highest rank may belong to the same sub-topics and contain some overlapping information. In fact, most of the traditional summariza-tion methods select sentences from di ff erent sub-topics [6,7]. We design a generic multi-document summarization method based on NMF (We refer it as INMF). Before going on, we give the definition of a sentence X  X  main topic:
That is, the main topic of sentence s j is the sub-topic with the maximum encoding value in column j of encoding matrix E [4]. The function topic () returns the union of each sentence X  X  main topic of a sentences set, e.g.,
The proposed multi-document summariza tion method INMF is described in Algo-rithm 1. Most of the summarization evaluations require the generated summaries in limited size or limited sentences number. In Algorithm 1, we limit the number of sen-tences of the summary. It can be easily revised to control the size of final summary. Di ff erent from [9], the INMF algorithm selects s entences with most significant ranking scores from di ff erent sub-topics in order to ensure the coverage and diversity of the summary. For each incident sti k , i , we can straightforwardly choose the sentences with the largest or top-x encoding values of sti k , i as the summary. Then, the summary for sub-topic st k can be generated by composing all the summaries of STI k . In the following, we first evaluate the proposed topic summarization method. And then, we give a case study of topic decomposition. The dataset of multi-document summa-rization task in DUC 2004[10] is used to evaluate the proposed methods.
 4.1 Summarization Evaluations We implement four baseline summarization systems: FORWARD(extracts the initial sentences of all documents of a topic); BACKWARD(generates summaries by selecting the end sentences of a topic); TSCAN; NMF(method of Lee et al., [9]). The number of sentences of summary generated by TSCAN is indeterminate. To ensure the comparison is fair, the evaluation procedure is as follows [1]: For each r , we firstly apply TSCAN to each document cluster to select a set of sentences as summary. Then, we use other methods to extract the same number of sentences for each r and document cluster. Both ROUGE-1 [12] and summary-to-document content coverage[1] metrics are used.
The overall performance comparison of ROUGE-1 on DUC 2004 is listed in Ta-ble 1. It shows that the two NMF based summarization methods get much better results than other methods for all r . This is because both the two NMF based methods try to cover all content as much as possible. However, TSCAN may not consider sub-topics successfully, FORWARD extracts beginning sentences and BACKWARD takes the end sentences. As r increase, TSCAN extracts more s entences as the summary. Because ROUGE is recall-oriented, the ROUGE-1 scores of all methods increase with the in-creasing of summary size as showed in Table 1. The proposed INMF method increase summary coverage by selecting sentences from di ff erent sub-topics explicitly. As a re-sult, INMF outperforms NMF in most cases except r = 6.

A good summary should contain important aspects of original topic documents as much as possible [8]. Herein, we apply summary-to-document content similarity to measure the coverage of summary according to [1]. That is, given a document cluster of a specific topic and its summary which are represented by TF-IDF term vectors. It computes the average cosine similarity between each of the document clusters and its summary. The higher the similarity, the better the summary represents document cluster. We show the summary-to-documents similarity corresponding to table 1 in table 2. In table 2, both INMF and NMF achieve much better results than TSCAN, FOR-WARD, BACKWARD for all r . It is easy to understand that all the three latter methods lose some information and the coverage is poor. However, Non-negative Matrix Factor-ization decomposes all of topic X  X  information into r sub-topics, and the two NMF based summarization method extract the sentences with as much information as possible. Besides, the proposed INMF summarization method explicitly tries to select sentences belong to di ff erent topic aspects. That X  X  why INMF outperforms NMF in all cases. 4.2 Topic Decomposition The documents set  X  X 30001t X  of DUC 2004 is used as a case study for topic decompo-sition. It includes 10 documents and 179 se ntences about  X  X olitical crisis in Cambodia in October 1998 X . The detailed descriptio n about each sub-topic and sentence id of its summary is showed in table 3. We manually compared each sub-topics X  summaries with reference summaries( X  X 30001.M.100.T.A X  ,  X  X 30001.M.100.T.B X ,  X  X 30001.M.100.T.C X  and  X  X 30001.M.100.T.D X  with size 658, 661, 647 and 656 bytes respectively) created by DUC assessors. For  X  X 30001.M.100.T.C X  and  X  X 30001.M.100.T.D X , the information coverage is 100%. The text  X  X he opposition tried to cut o ff his access to loans X  (total 51 bytes) in  X  X 30001.M.100.T.A X  and  X  X pposition parties ask the Asian Development Bank to stop loans to Hun Sen X  X  government X (total 87 bytes) in  X  X 30001.M.100.T.B X  are lost in the generated summaries. Then, the average information coverage of the generated sub-topics X  summary to the reference summaries is ((658  X  51) / 658 + (661  X  87) / 661 + 100 + 100) / 4 = 94 . 75%.

Some sub-topics contain a series of incidents while others contain only one. For ex-ample, sub-topic 6 is about the process of f orming a coalition gov ernment which con-tains several incidents. Sub-topic 8 has only one incident, which is about King Norodom Sihanouk X  X  praise about the agreements by Cambodia X  X  top two political parties. We also compare our results with TSCAN for topic decomposition with r = 10. TSCAN detects total 23 incidents. 5 incidents are the same (some incidents duplicate more than 2 times), with only 15 sentences left as the i ncidents X  summary. The 15 sentences are from 7 documents while the incidents X  summaries of our method are from all 10 doc-uments. Besides, our method covers more aspects than TSCAN, e.g. sub-topic 5 and sub-topic 10 are not included in the result of TSCAN. In this paper, we study the problem of topic decomposition and summarization for a temporal-sequenced text corpus of a specific topic. We represent the text corpus as a terms-by-sentences matrix and derive sub-topics by factorize the matrix using Non-negative Matrix Factorization. By analyzing the encoding matrix, we can detect incidents of each sub-topic and generate su mmaries for both sub-topics and their re-lated incidents. The summary for the text corpus is generated by firstly ranking each sentences based on the encoding matrix, and then selecting most significant sentences from each sub-topics. Experimental results show that our method can e ff ectively find out di ff erent topic aspects of a documents set and generate promising results in summarization.

