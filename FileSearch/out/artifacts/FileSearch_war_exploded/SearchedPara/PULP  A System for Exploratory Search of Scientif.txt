 Despite the growing importance of exploratory search, infor-mation retrieval (IR) systems tend to focus on lookup search. Lookup searches are well served by optimising the precision and recall of search results, however, for exploratory search this may be counterproductive if users are unable to for-mulate an appropriate search query. We present a system called PULP that supports exploratory search for scientific literature, though the system can be easily adapted to other types of literature. PULP uses reinforcement learning (RL) to avert the user from context traps resulting from poorly chosen search queries, trading o  X  between exploration (pre-senting the user with diverse topics) and exploitation (mov-ing towards more specific topics). Where other RL-based systems su  X  er from the  X  X old start X  problem, requiring suf-ficient time to adjust to a user X  X  information needs, PULP initially presents the user with an overview of the dataset using temporal topic models. Topic models are displayed in an interactive alluvial diagram, where topics are shown as ribbons that change thickness with a given topics relative prevalence over time. Interactive, exploratory search ses-sions can be initiated by selecting topics as a starting point. Exploratory search; topic models; bandit algorithms; scien-tific literature search; query formulation
Exploratory search is defined as a class of search activities performed to learn or discover new information [16]. Unlike lookup search, where a discrete set of results achieves a well-defined objective, exploratory search can involve unfamiliar subject areas and uncertainty regarding search goals. In ex-ploratory tasks users are often uncertain how to formulate search queries [8] either because they are unfamiliar with the search topic or they have no clear search goals in mind. For these reasons exploratory search is considered to be chal-lenging for the user and requires additional support from information retrieval (IR) systems.

Our work addresses the observation that IR systems pri-marily target lookup search, despite the increasing impor-tance of exploratory search [5]. Most IR systems present a ranked list of documents in descending order of their rel-evance to the issued search query. This approach works very well for lookup tasks where the user has a discrete and well defined information need. However, retrieving the best matching results for the search query might trap the user in their initial query context and contribute to user perceptions that exploratory search is challenging. Moreover, for users attempting to acquire information in an unfamiliar domain where they lack the specific knowledge to formulate search queries, retrieving the best matching results to those queries may be counterproductive.

We address the issue of initial query formulation by visu-alizing what topics are covered by a given dataset and what keywords are associated with each topic, allowing the user to directly select a topic as their initial search query. In order to prevent the user from getting stuck in the context trap of the initial search query, we use reinforcement learn-ing (RL) to trade o  X  between exploration and exploitation throughout a search session.
Below, we briefly review the existing approaches to shed light on some of the open problems in exploratory search.
RelevanceFeedback. The first key approach developed by the IR community was relevance feedback: users mark documents as relevant or non-relevant and the query model is updated accordingly. Initial studies showed that inter-active IR systems benefited from relevance feedback [15], though later studies showed these features to be rarely used. The reasons are two-fold: (1) relevance feedback often leads to context traps, and (2) the cognitive load of selecting rel-evant documents is high compared to typing a new query [15]. Combining relevance feedback with user modelling techniques, such as reinforcement learning, would allow an IR system to suggest more relevant results and avoid getting stuck in context traps.

FacetedSearch. Faceted search aims to avoid the con-text trap by using global features instead of contextual ones [21]. It organizes information according to a faceted clas-sification system, allowing users to explore collections of documents by applying multiple filters. Such systems clas-sify information elements along explicit global dimensions, enabling the classifications to be accessed and ordered in dance. multiple ways. As the number of global features is often very large, the process, however, can quickly become overly demanding as users have to assess a large number of op-tions [21]. Assisting the user in formulating the initial search query in combination with improved modeling of user needs would allow a reduction in the number of facets and thus enhance user experience.

Reinforcement Learning. In exploratory search, the information-needs of a user are constantly evolving, and hence modeling is crucial for acquiring information on search intent and query reformulation [20]. Recently, RL tech-niques have been applied to modeling exploratory search [14, 10]. RL allows a system to balance exploitation (mov-ing towards more specific topics) and exploration (present-ing alternative topics). The main disadvantage is the  X  X old start X  problem: these systems need a number of sessions to adjust to a user X  X  information needs. A recent study showed that systems employing RL techniques can optimally bal-ance exploitation and exploration for a given set of users [2]. Assisting the user in formulating the initial query in an RL-based IR system would accelerate the user modeling aspect of such systems.

Exploratory Search Interfaces. The lack of success of relevance feedback is often attributed to user interface design failing to conveniently provide feedback at suitable levels of granularity. In response, a variety of systems were designed to support user feedback, including intelligent user interfaces that assist the user in comprehending an infor-mation space [7], visualizations that summarize results for faster relevance judgement [17], as well as interactive visu-alizations that allow the user to indicate the direction of exploration [10]. These systems are very useful but they usually do not take into consideration the evolving nature of user knowledge in exploratory search tasks and thus ei-ther fail to provide a good starting point for the search or lead to a context trap as the search progresses.

Models of Exploratory Search. Models of user be-haviour in information search help us to provide personal-ized support [1]. Information Foraging Theory is a promising model of exploration that predicts the information seeker X  X  decision from the expectation of information gain [18]. There are also models that assist the search systems in predicting user perception of the results from implicit interactions, in-cluding search satisfaction, disambiguating exploration and struggling scenarios [13], and predicting the user X  X  domain knowledge [9]. We draw inspiration for the design of our sys-tem from models of user behaviour in exploratory search.
Search and Topic Models. While there has been ex-tensive research on topic model visualisation, there has been less on their use for exploratory search. While there has been prior work using hierarchical topic models for docu-ment summarising and navigation [12], it did not present the dynamic aspect of time that is critical in scientific lit-erature. Another study presented the graph of connections (institutions, subject areas, authors, etc. ) [11]. In both of these examples, however, the intrinsic use of topics as an aid to IR was less developed.
PULP can be broadly divided into three parts: visual-ization of search topics, search engine interface, and search engine backend containing the document ranking algorithm. The current version of the system operates on  X  1million documents obtained from the arXiv repository, although the system can be easily adapted to other domains.

The search starts with the alluvial diagram of broad top-ics found in the arXiv dataset over the years (Figure 1). Each year is represented by a vertical list of research topics covered by papers added to arXiv in that particular year. Each research topic is labelled with the five most prominent keywords from that topic. The user can specify which years of the arXiv data should be visualised by using the slider in the upper left hand corner of the visualisation. The ribbons going between di  X  erent years indicate how topics in di  X  erent years are related to one another. When the search starts, all the ribbons representing the flow of the topics from year to year are light grey. The user can click on any of the existing topics to see its development over the years, at which point the ribbon representing that topic model will change colour. The user can click on as many topics as he likes. By clicking on the search button next to the slider, the top five keywords from the topics that the user clicked on will be sent to the search engine as a query. If none of the topics was selected, then by clicking on the search button, the user is directed to the query page of the search engine.

The search engine interface is presented in Figure 2. The query bar is at the top of the page. The keywords from the topics that the user clicked on in the alluvial diagram are shown in the search bar. The user can amend this query by deleting or adding keywords. If no topics in the alluvial diagram were selected, then the search bar is empty and the user can formulate their own query. After typing in the search query, the user is presented with 10 documents. The initial set of documents is ranked based on the Okapi BM25 algorithm [19]. Next, to see more documents the user can indicate which document interests them by clicking on the circle next to it and thus providing relevance score of 1 to that document. After clicking the  X  X ext X  button in the top right corner of the page, a new set of documents is displayed based on the feedback provided by the user so far. The user can provide feedback to as many documents as he wishes.
The right margin of the page contains the topic model dis-play with the distribution of topics in the presented search results and the rest of the dataset. The top most prominent topics for each document are shown. Each topic is repre-sented with a consistent colour across all documents. The length of the bar representing a given topic corresponds to the relative prominence of this topic in a given document. By hovering the mouse cursor above a given topic, a tooltip appears with top five keywords associated with that topic. The topics representing the currently displayed search re-sults are marked with a blue vertical line on the left side of the topic display bar. The remaining part of the topic model display shows how many potentially similar docu-ments in terms of their topic distribution there are in the dataset. The topic model display can be scrolled down to cover the entire dataset. Below, we describe how the topic model visualisation was constructed and how the documents are ranked.
All documents in the dataset were preprocessed to remove stop-words and made lower case. They were then grouped by year of last update, with all documents prior to 1993 placed together. The standard, non-bursty, non-parametric topic model of Buntine and Mishra [6] is run on each group to produce 20 topics per group. For these topics, words are then ranked according to their  X  X ift X  over the background (the model estimates a  X  X on-topical X  background distribu-tion as well [6]) and labelled with the top 5 ranked words.
Getting an estimate of topic flow is more challenging. The documents in year Y +1 are fit to the topics from year Y . The most likely topic for each word is then estimated using the topics from years Y and Y +1. This produces topic-to-topic mapping data from years Y to Y +1 for each word in year Y +1. We estimate the topic flows from this mapping data. We found this to be less noisy than heuristic estimates based on topic word-vector di  X  erences.
In order to help the user explore the document space, we use LinRel [4]. Suppose we have a matrix D , where each row d is a tf-idf (term frequency-inverse document frequency) feature vector representation of documents presented so far. Let r =( r 1 ,r 2 ...r t ) &gt; be the column vector of relevance scores received from the user up to time t .Weestimate the expected relevance r i of a document d i as E [ r i ]= d where the vector w is estimated from user feedback. LinRel estimates  X  w by solving r = D  X  w and estimates relevance score for each d i as  X  r i = d i  X   X  w
In order to deal with the exploration-exploitation trade-o  X  , we present documents not with the highest score  X  r with the largest upper confidence bound for the relevance score. Thus, if i is an upper bound on standard devia-tion of relevance estimate  X  r i , the upper confidence bound of document d i is calculated as r i + i , where &gt; 0is a constant used to adjust the confidence level of the up-per confidence bound. In each iteration, LinRel calculates s = d i  X  ( D &gt;  X  D + I ) 1 D &gt; , where is the regularization parameter which is set to 1 if each of the feature vectors sums up to 1 (following [4]) and the documents that max-imize s i  X  r + 2 k s i k are selected for presentation. The first term s i  X  r e  X  ectively ranks all the documents based on their similarity to the documents the user has selected so far and thus it narrows the area of the search space (exploitation). The second term 2 k s i k ensures that the user is presented with a more diverse set of results. The exploration rate is controlled by the parameter. The higher the value of , the more diverse, or exploratory, the results are. The explo-ration rate in the current system is = 1, based on results from a previous experimental study [2].
The same document preparation and topic model system discussed in Section 3.1 was used for the global topic repre-sentation. However, for the global model, more topics were used  X  300 in the current version of the system. For this, we needed to estimate topic proportions per document, which was done using 500 cycles of the Gibbs sampler after an ini-tial 1000 cycles. In the topic model visualisation in Figure 2 we used the top five topics proportions per document.
The search engine components of PULP (search interface and RL-based backend) were tested in a series of user ex-periments [2, 3]. The participants were researchers in vari-ous areas of computer science. Search results include more diverse results when users are exploring. Users found more useful results in exploratory tasks when compared to a base-line system, which was specifically tuned for lookup tasks. Overall, users gave higher ratings for documents retrieved during exploratory tasks using PULP compared to the base-line system. Additionally, in exploratory search tasks, there is a higher number of interactions, i.e. the number of search results users clicked or bookmarked in our system than in the baseline system. Currently, we are in the process of as-sessing exploratory searches initiated via our topic model visualisation versus free text search queries.
 Acknowledgments. The work was supported by The Finnish Funding Agency for Innovation (projects Re:Know and D2I) and by Academy of Finland (project COIN). [1] K. Athukorala, D. G lowacka, A. Oulasvirta, [2] K. Athukorala, A. Medlar, K. Ilves, and D. G lowacka. [3] K. Athukorala, A. Medlar, A. Oulasvirta, G. Jacucci, [4] P. Auer. Using confidence bounds for exploitation  X  [5] N. J. Belkin. Some (what) grand challenges for [6] W. Buntine and S. Mishra. Experiments with [7] D. H. Chau, A. Kittur, J. I. Hong, and C. Faloutsos. [8] S. Chowdhury, F. Gibb, and M. Landoni. Uncertainty [9] M. J. Cole, J. Gwizdka, C. Liu, N. J. Belkin, and [10] D. G lowacka, T. Ruotsalo, K. Konyushkova, [11] B. Gretarsson, J. O X  X onovan, S. Bostandjiev, [12] A. Haghighi and L. Vanderwende. Exploring content [13] A. Hassan, R. W. White, S. T. Dumais, and Y. Wang. [14] S. Hore, L. Tyrvainen, J. Pyykko, and D. Glowacka. A [15] D. Kelly and X. Fu. Elicitation of term relevance [16] G. Marchionini. Exploratory search: from finding to [17] J. Matejka, T. Grossman, and G. Fitzmaurice. [18] P. Pirolli and S. Card. Information foraging. Psych. [19] K. Sparck Jones, S. Walker, and S. E. Robertson. A [20] R. W. White, P. N. Bennett, and S. T. Dumais. [21] K.-P. Yee, K. Swearingen, K. Li, and M. Hearst.
