 Robert C. Grande RGRANDE @ MIT . EDU Jonathan P. How JHOW @ MIT . EDU In Reinforcement Learning (RL) (Sutton &amp; Barto, 1998), several new algorithms for efficient exploration in continu-ous state spaces have been proposed, including GP-Rmax (Jung &amp; Stone, 2010) and C-PACE (Pazis &amp; Parr, 2013). In particular, C-PACE was shown to be PAC-MDP, an im-portant class of RL algorithms that obtain an optimal pol-icy in a polynomial number of exploration steps. However, these approaches require a costly fixed-point computation on each experience, making them ill-suited for real-time control of physical systems, such as aircraft. This paper presents a series of sample complexity (PAC-MDP) results for algorithms that use Gaussian Processes (GPs) (Ras-mussen &amp; Williams, 2006) in RL, culminating with the introduction of a PAC-MDP model-free algorithm which does not require this fixed-point computation and is better suited for real-time learning and control.
 First, using the KWIK learning framework (Li et al., 2011), we provide the first-ever sample complexity analysis of GP learning under the conditions necessary for RL. The re-sult of this analysis actually proves that the previously de-scribed model-based GP-Rmax is indeed PAC-MDP. How-ever, it still cannot be used in real time, as mentioned above. Our second contribution is in model-free RL, in which a GP is used to directly model the Q -function. We show that existing model-free algorithms (Engel et al., 2005; Chung et al., 2013) that use a single GP may require an exponen-tial (in the discount factor) number of samples to reach a near-optimal policy.
 The third, and primary, contribution of this work is the in-troduction of the first model-free continuous state space PAC-MDP algorithm using GPs: Delayed-GPQ (DGPQ).
 DGPQ represents the current value function as a GP, and updates a separately stored value function only when suffi-cient outlier data has been detected. This operation  X  X ver-writes X  a portion of the stored value function and resets the GP confidence bounds, avoiding the slowed convergence rate of the na  X   X ve model-free approach.
 The underlying analogy is that while GP-Rmax and C-PACE are generalizations of model-based Rmax (Brafman &amp; Tennenholtz, 2002), DGPQ is a generalization of model-free Delayed Q-learning (DQL) (Strehl et al., 2006; 2009) to general continuous spaces. That is, while early PAC-MDP RL algorithms were model-based and ran a planner after each experience (Li et al., 2011), DQL is a PAC-MDP algorithm that performs at most a single Bellman backup on each step. In DGPQ, the delayed updates of DQL are adapted to continuous state spaces using a GP.
 We prove that DGPQ is PAC-MDP, and show that using a sparse online GP implementation (Csat  X  o &amp; Opper, 2002) DGPQ can perform in real-time. Our empirical results, in-cluding those on an F-16 simulator, show DGPQ is both sample efficient and orders of magnitude faster in per-sample computation than other PAC-MDP continuous-state learners.
 We now describe background material on Reinforcement Learning (RL) and Gaussian Processes (GPs). 2.1. Reinforcement Learning We model the RL environment as a Markov Decision Pro-cess (Puterman, 1994) M =  X  S,A,R,T, X   X  with a poten-tially infinite set of states S , finite actions A , and 0  X   X  &lt; 1 . Each step elicits a reward R ( s,a ) 7 X  [0 ,R and a stochastic transition to state s 0  X  T ( s,a ) . Every MDP has an optimal value function Q  X  ( s,a ) = R ( s,a ) +  X 
R corresponding optimal policy  X   X  : S 7 X  A . Note the bounded reward function means V  X   X  [0 ,V max ] .
 In RL, an agent is given S , A , and  X  and then acts in M with the goal of enacting  X   X  . For value-based methods (as op-posed to policy search (Kalyanakrishnan &amp; Stone, 2009)), there are roughly two classes of RL algorithms: model-based and model-free. Model-based algorithms, such as KWIK-Rmax (Li et al., 2011), build models of T and R and then use a planner to find Q  X  . Many model-based ap-proaches have sample efficiency guarantees, that is bounds on the amount of exploration they perform. We use the PAC-MDP definition of sample complexity (Strehl et al., 2009). The definition uses definitions of the covering num-ber of a set (adapted from (Pazis &amp; Parr, 2013)). Definition 1 The Covering Number N U ( r ) of a compact domain U  X  R n is the cardinality of the minimal set C = { c i ,...,c N c } s.t.  X  x  X  U ,  X  c j  X  C s.t. d ( x,c j )  X  r , where d (  X  ,  X  ) is some distance metric.
 Definition 2 Algorithm A (non-stationary policy A t ) with accuracy parameters and  X  in an MDP of size N SA ( r ) is said to be Probably Approximately Correct for MDPs (PAC-MDP) if, with probability 1  X   X  , it takes no more than where V A t ( s t ) &lt; V  X  ( s t )  X  .
 By contrast, model-free methods such as Q-Learning (Watkins, 1992) build Q  X  directly from experience without explicitly representing T and R . Generally model-based methods are more sample efficient but require more com-putation time for the planner. Model-free methods are gen-erally computationally light and can be applied without a planner, but need (sometimes exponentially) more samples, and are usually not PAC-MDP. There are also methods that are not easily classified in these categories, such as C-PACE (Pazis &amp; Parr, 2013), which does not explicitly model T and R but performs a fixed-point operation for planning. 2.2. Gaussian Processes This paper uses GPs as function approximators both in model-based RL (where GPs represent T and R ) and model-free RL (where GPs represent Q  X  ), while showing how to maintain PAC-MDP sample efficiency in both cases. A GP is defined as a collection of random variables, any fi-nite subset of which has a joint Gaussian distribution (Ras-mussen &amp; Williams, 2006) with prior mean  X  ( x ) and co-variance kernel k ( x,x 0 ) . In this paper, we use the radial basis function (RBF), k ( x,x 0 ) = exp (  X  k x  X  x 0 k 2 The elements of the GP kernel matrix K ( X,X ) are defined as K i,j = k ( x i ,x j ) , and k ( X,x 0 ) denotes the kernel vec-tor. The conditional probability distribution of the GP at new data point, x 0 , can then be calculated as a normal vari-able (Rasmussen &amp; Williams, 2006) with mean  X   X  ( x 0 k ( X,x 0 ) T [ K ( X,X ) +  X  2 n I ]  X  1 y , and covariance  X  ( x k ( x 0 ,x 0 )  X  k ( X,x 0 ) T [ K ( X,X ) +  X  2 n I ]  X  1 y is the set of observations, X is the set of observation in-put locations, and  X  2 n is the measurement noise variance. For computational and memory efficiency, we employ an online sparse GP approximation (Csat  X  o &amp; Opper, 2002) in the experiments. 2.3. Related Work GPs have been used for both model-free and model-based RL. In model-free RL, GP-Sarsa (Engel et al., 2005) has been used to model a value function and extended to off-policy learning (Chowdhary et al., 2014), and for (heuristi-cally) better exploration in iGP-Sarsa (Chung et al., 2013). However, we prove in Section 4.1 that these algorithms may require an exponential (in 1 1  X   X  ) number of samples to reach optimal behavior since they only use a single GP to model the value function.
 In model-based RL, the PILCO algorithm trained GPs to represent T , and then derived policies using policy search (Deisenroth &amp; Rasmussen, 2011). However, PILCO does not include a provably efficient (PAC-MDP) exploration strategy. GP-Rmax (Jung &amp; Stone, 2010) does include an exploration strategy, specifically replacing areas of low confidence in the ( T and R ) GPs with high valued states, but no theoretical results are given in that paper. In Section 3, we show that GP-Rmax actually is PAC-MDP, but the al-gorithm X  X  planning phase (comparable to the C-PACE plan-ning in our experiments) after each update makes it com-putationally infeasible for real-time control.
 REKWIRE (Li &amp; Littman, 2010) uses KWIK linear re-gression for a model-free PAC-MDP algorithm. However, REKWIRE needs H separate approximators for finite hori-zon H , and assumes Q can be modeled as a linear function, in contrast to the general continuous functions considered in our work. The C-PACE algorithm (Pazis &amp; Parr, 2013) has already been shown to be PAC-MDP in the continu-ous setting, though it does not use a GP representation. C-PACE stores data points that do not have close-enough neighbors to be considered  X  X nown X . When it adds a new data point, the Q -values of each point are calculated by a value-iteration like operation. The computation for this operation grows with the number of datapoints and so (as shown in our experiments) the algorithm may not be able to act in real time. In model-based RL (MBRL), models of T and R are cre-ated from data and a planner derives  X   X  . Here we review the KWIK-Rmax MBRL architecture, which is PAC-MDP. We then show that GPs are a KWIK-learnable representa-tion of T and R , thereby proving that GP-Rmax (Jung &amp; Stone, 2010) is PAC-MDP given an exact planner. 3.1. KWIK Learning and Exploration The  X  X nows What it Knows X  (KWIK) framework (Li et al., 2011) is a supervised learning framework for measuring the number of times a learner will admit uncertainty. A KWIK learning agent is given a hypothesis class H : X 7 X  Y for inputs X and outputs Y and parameters and  X  . With probability 1  X   X  over a run, when given an adversari-ally chosen input x t , the agent must, either (1) predict  X  y if || y t  X   X  y t ||  X  where y t is the true expected output ( E [ h  X  ( x )] = y t ) or (2) admit  X  X  don X  X  know X  (denoted  X  ). A representation is KWIK learnable if an agent can KWIK learn any h  X   X  H with only a polynomial (in  X  X  H | , 1 , number of  X  predictions. In RL, the KWIK-Rmax algo-rithm (Li et al., 2011), uses KWIK learners to model T and R and replaces the value of any Q  X  ( s,a ) where T ( s,a ) or R ( s,a ) is  X  with a value of V max = R max 1  X   X  . If T and R are KWIK-learnable, then the resulting KWIK-Rmax RL algorithm will be PAC-MDP under Definition 2. We now show that a GP is KWIK learnable. 3.2. KWIK Learning a GP First, we derive a metric for determining if a GP X  X  mean prediction at input x is -accurate with high probability, based on the variance estimate at x .
 Lemma 1 Consider a GP trained on samples ~y = [ y 1 ,...,y t ] which are drawn from p ( y | x ) at input lo-cations X = [ x 1 ,...,x t ] , with E [ y | x ] = f ( x ) and y  X  [0 ,V m ] . If the predictive variance of the GP at x i is then the prediction error at x i is bounded in probability: Pr {|  X   X  ( x i )  X  f ( x i ) | X  1 } X   X  1 .
 Proof sketch Here we ignore the effect of the GP-prior, which is considered in depth in the Supplementary ma-terial. Using McDiarmid X  X  inequality we have that Pr {|  X  ( x )  X  f 1 ( x ) |  X  1 }  X  2 exp  X  2 2 1 P c 2 the maximum amount that y i can alter the prediction value  X  ( x ) . Using algebraic manipulation, Bayes law, and not-ing that the max singular value  X   X  ( K ( X,X )( K ( X,X ) +  X 
I )  X  1 ) &lt; 1 , it can be shown that P c 2 i  X   X  2 ( x Substituting  X  2 ( x i ) to McDiarmid X  X  inequality and rear-ranging yields (1). See supplemental material.
 Lemma 1 gives a sufficient condition to ensure that, with high probability, the mean of the GP is within 1 of E [ h  X  ( x )] . A KWIK agent can now be constructed that pre-dicts  X  if the variance is greater than  X  2 tol and otherwise predicts  X  y t =  X  ( x ) , the mean prediction of the GP. The-orem 1 bounds the number of  X  predictions by such an agent, and when  X  1 =  X  N learnability of GPs. For ease of exposition, we define the equivalent distance as follows: Definition 3 The equivalent distance r ( tol ) is the max-imal distance s.t.  X  x,c  X  U , if d ( x,c )  X  r ( tol ) , then the linear independence test  X  ( x,c ) = k ( x,x )  X  k ( x,c ) 2 /k ( c,c )  X  tol .
 Theorem 1 Consider a GP model trained over a compact domain U  X  R n with covering number N U ( r ( 1 2  X  2 tol the observations ~y = [ y 1 ,...,y n ] be drawn from p ( y | x ) , with E [ y | x ] = f ( x ) , and x drawn adversarially. Then, the worst case bound on the number of samples for which and the GP is forced to return  X  is at most Furthermore, N U r 1 2  X  2 tol grows polynomially with 1 and V m for the RBF kernel.
 Proof sketch If  X  2 ( x )  X   X  2 tol ,  X  x  X  U , then (2) fol-lows from Lemma 1. By partitioning the domain into the Voronoi regions around the covering set C , it can be shown using the covariance equation (see supplemen-tal material), that given n V samples in a Voronoi region c i  X  C ,  X  2 ( x )  X  Solving for n V , we find that n V  X  2  X  2  X  2 drive  X  2 ( x )  X   X  2 tol . Plugging in n V into (1), we have c below  X  2 tol . Therefore, the total number of points we can sample anywhere in U before reducing the variance below  X  tol everywhere is equal to the sum of points n V over all regions, N U . The total probability of an incorrect predic-tion is given by the union bound of an incorrect prediction in any region, P c terial for proof that N U grows polynomially with 1 V Intuitively, the KWIK learnability of GPs can be explained as follows. By knowing the value at a certain point within some tolerance 2 , the Lipschitz smoothness assumption means there is a nonempty region around this point where values are known within a larger tolerance . Therefore, given sufficient observations in a neighborhood, a GP is able to generalize its learned values to other nearby points. Lemma 1 relates the error at any of these points to the pre-dictive variance of the GP, so a KWIK agent using a GP can use the variance prediction to choose whether to predict  X  or not. As a function becomes less smooth, the size of these neighborhoods shrinks, increasing the covering num-ber, and the number of points required to learn over the entire input space increases.
 Theorem 1, combined with the KWIK-Rmax Theorem (Theorem 3 of (Li et al., 2011)) establishes that the previ-ously proposed GP-Rmax algorithm (Jung &amp; Stone, 2010), which learns GP representations of T and R and replaces uncertainty with V max values, is indeed PAC-MDP. GP-Rmax was empirically validated in many domains in this previous work but the PAC-MDP property was not formally proven. However, GP-Rmax relies on a planner that can de-rive Q  X  from the learned T and R , which may be infeasible in continuous domains, especially for real-time control. Model-free RL algorithms, such as Q-learning (Watkins, 1992), are often used when planning is infeasible due to time constraints or the size of the state space. These al-gorithms do not store T and R but instead try to model Q  X  directly through incremental updates of the form: Q t +1 ( s,a ) = (1  X   X  ) Q t ( s,a ) +  X  ( r t +  X V  X  t ( s fortunately, most Q-learning variants have provably expo-nential sample complexity, including optimistic/greedy Q-learning with a linearly decaying learning rate (Even-Dar &amp; Mansour, 2004), due to the incremental updates to Q combined with the decaying  X  term.
 However, the more recent Delayed Q-learning (DQL) (Strehl et al., 2006) is PAC-MDP. This algorithm works by initializing Q ( s,a ) to V max and then overwriting Q ( s,a ) = been seen. In section 4.1, we show that using a single GP results in exponential sample complexity, like Q-learning; in section 4.2, we show that using a similar overwriting ap-proach to DQL achieves sample efficiency. 4.1. Na  X   X ve Model-free Learning using GPs We now show that a na  X   X ve use of a single GP to model Q  X  will result in exponentially slow convergence similar to greedy Q-learning with a linearly decaying  X  . Consider the following model-free algorithm using a GP to store values of  X 
Q = Q t . A GP for each action ( GP a ) is initialized opti-mistically using the prior. At each step, an action is chosen greedily and GP a is updated with an input/output sample:  X  x t ,r t +  X  max b GP b ( s t +1 )  X  . We analyze the worst-case performance through a toy example and show that this ap-proach requires an exponential number of samples to learn Q  X  . This slow convergence is intrinsic to GPs due to the variance reduction rate and the non-stationarity of the estimate (as opposed to T and R , whose sampled values do not depend on the learner X  X  current estimates). So, any model-free algorithm using a GP that does not reinitialize the variance will have the same worst-case complexity as in this toy example.
 Consider an MDP with one state s and one action a that transitions deterministically back to s with reward r = 0 , and discount factor  X  . The Q function is initialized opti-mistically to  X  Q 0 ( s ) = 1 using the GP X  X  prior mean. Con-sider the na  X   X ve GP learning algorithm described above, ker-nel k ( s,s ) = 1 , and measurement noise  X  2 to predict the Q-function using the GP regression equations. We can an-alyze the behavior of the algorithm using induction. Consider the first iteration:  X  Q 0 = 1 , and the first measure-ment is  X  . In this case, the GP prediction is equivalent to the MAP estimate of a random variable with Gaussian prior and linear measurements subject to Gaussian noise.  X  2 + i X  2 0 is the GP variance at iteration i . Substituting for  X  and rearranging yields a recursion for the prediction of  X  at each iteration, From (Even-Dar &amp; Mansour, 2004), we have that a series slowly in terms of 1 and 1 1  X   X  . However, for each term in our series, we have The modulus of contraction is always at least as large as the RHS, so the series convergence to (since the true value is 0 ) is at least as slow as the example in (Even-Dar &amp; Mansour, 2004). Therefore, the na  X   X ve GP implementa-tion X  X  convergence to is also exponentially slow, and, in fact, has the same learning speed as Q -learning with a lin-ear learning rate. This is because the variance of a GP de-cays linearly with number of observed data points, and the magnitude of GP updates is proportional to this variance. Additionally, by adding a second action with reward r = 1  X   X  , a greedy na  X   X ve agent would oscillate between the two actions for an exponential (in 1 1  X   X  ) number of steps, as shown in Figure 1 (blue line), meaning the algorithm is not PAC-MDP, since the other action is not near optimal. Randomness in the policy with -greedy exploration (green line) will not improve the slowness, which is due to the non-stationarity of the Q-function and the decaying update magnitudes of the GP. Many existing model-free GP algo-rithms, including GP-Sarsa (Engel et al., 2005), iGP-Sarsa (Chung et al., 2013), and (non-delayed) GPQ (Chowdhary et al., 2014) perform exactly such GP updates without vari-ance reinitialization, and therefore have the same worst-case exponential sample complexity. 4.2. Delayed GPQ for model-free RL We now propose a new algorithm, inspired by Delayed Q-learning, that guarantees polynomial sample efficiency by consistently reinitializing the variance of the GP when many observations differ significantly from the current  X  estimate.
 Delayed GPQ-Learning (DGPQ: Algorithm 1) maintains two representations of the value function. The first is a set of GPs, GP a for each action, that is updated after each step but not used for action selection. The second representation of the value function, which stores values from previously converged GPs is denoted  X  Q ( s,a ) . Intuitively, the algo-rithm uses  X  Q as a  X  X afe X  version of the value function for choosing actions and retrieving backup values for the GP Algorithm 1 Delayed GPQ (DGPQ) 1: Input: GP kernel k (  X  ,  X  ) , Lipschitz Constant L Q , En-2: for a  X  A do 3:  X  Q a =  X  4: GP a = GP. init (  X  = R max 1  X   X  ,k (  X  ,  X  )) 5: for each timestep t do 6: a t = arg max a  X  Q a ( s t ) by Eq 7 7:  X  r t ,s t +1  X  = Env. takeAct ( a t ) 8: q t = r t +  X  max a  X  Q a ( s t +1 ) 9:  X  2 1 = GP a t . variance ( s t ) 10: if  X  2 1 &gt;  X  2 tol then 11: GP a t . update ( s t ,q t ) 12:  X  2 2 = GP a t . variance ( s t ) 14:  X  Q a . update ( s t ,GP a t . mean ( s t ) + 1 ) 15:  X  a  X  A,GP a = GP. init (  X  =  X  Q a ,k (  X  ,  X  )) updates, and only updates  X  Q ( s,a ) when GP a converges to a significantly different value at s .
 The algorithm chooses actions greedily based on  X  Q (line 6) and updates the corresponding GP a based on the ob-served rewards and  X  Q at next the state (line 8). Note, in practice one creates a prior of  X  Q a ( s t ) (line 15) by updating the GP with points z t = q t  X   X  Q a ( s t ) (line 11), and adding back  X  Q a ( s t ) in the update on line 14. If the GP has just crossed the convergence threshold at point s and learned a value significantly lower ( 2 1 ) than the current value of  X  Q (line 13), the representation of  X  Q is partially overwritten with this new value plus a bonus term using an operation described below. Crucially, the GPs are then reset , with all data erased. This reinitializes the variance of the GPs so that updates will have a large magnitude, avoiding the slowed convergence seen in the previous section. Guide-lines for setting the sensitivity of the two tests (  X  2 tol are given in Section 5.
 There are significant parallels between DGPQ and the discrete-state DQL algorithm (Strehl et al., 2009), but the advancements are non-trivial. First, both algorithms main-tain two Q functions, one for choosing actions (  X  Q ), and a temporary function that is updated on each step ( GP a ), but in DGPQ we have chosen specific representations to handle continuous states and still maintain optimism and sample complexity guarantees. DQL X  X  counting of (up to m ) discrete state visits for each state cannot be used in continuous spaces, so DGPQ instead checks the immediate convergence of GP a at s t to determine if it should com-pare  X  Q ( s t ,a ) and GP a ( s t ) . Lastly, DQL X  X  discrete learn-ing flags are not applicable in continuous spaces, so DGPQ only compares the two functions as the GP variance crosses a threshold, thereby partitioning the continuous MDP into areas that are known (w.r.t.  X  Q ) and unknown, a property that will be vital for proving the algorithm X  X  convergence. One might be tempted to use yet another GP to model  X  Q ( s,a ) . However, it is difficult to guarantee the optimism of  X 
Q (  X  Q  X  Q  X   X  ) using a GP, which is a crucial property of most sample-efficient algorithms. In particular, if one at-tempts to update/overwrite the local prediction values of a GP, unless the kernel has finite support (a point X  X  value only influences a finite region), the overwrite will affect the val-ues of all points in the GP and possibly cause a point that was previously correct to fall below Q  X  ( s,a )  X  . In order to address this issue, we use an alternative function approximator for  X  Q which includes an optimism bonus as used in C-PACE (Pazis &amp; Parr, 2013). Specifically,  X  Q ( s,a ) is stored using a set of values that have been updated from the set of GP s, (  X  s i ,a i  X  ,  X   X  i ) and a Lipschitz constant L that is used to find an optimistic upper bound for the Q function for  X  s,a  X  : We refer to the set ( s i ,a ) as the set of basis vectors ( BV ). Intuitively, the basis vectors store values from the previ-ously learned GPs. Around these points, Q  X  cannot be predict optimistically at points not in BV , we search over BV for the point with the lowest prediction including the weighted distance bonus. If no point in BV is sufficiently close, then V max is used instead. This representation is also used in C-PACE (Pazis &amp; Parr, 2013) but here it sim-ply stores the optimistic Q-function; we do not perform a fixed point operation. Note the GP still plays a crucial role in Algorithm 1 because its confidence bounds, which are not captured by  X  Q , partition the space into known/unknown areas that are critical for controlling updates to  X  Q . In order to perform an update (partial overwrite) of  X  Q , we dundant constraints are eliminated by checking if the new constraint results in a lower prediction value at other basis vector locations. Thus, the pseudocode for updating  X  Q is as any j ,  X  i + L Q d (( s i ,a ) , ( s j ,a ))  X   X  j , delete  X  ( s from the set.
 Figure 1 shows the advantage of this technique over the na  X   X ve GP training discussed earlier. DGPQ learns the op-timal action within 50 steps, while the na  X   X ve implementa-tion as well as an -greedy variant both oscillate between the two actions. This example illustrates that the targeted exploration of DGPQ is of significant benefit compared to untargeted approaches, including the -greedy approach of GP-SARSA. In order to prove that DGPQ is PAC-MDP we adopt a proof structure similar to that of DQL (Strehl et al., 2009) and refer throughout to corresponding theorems and lemmas. First we extend the DQL definition of a  X  X nown state X  MDP M K t , containing state/actions from  X  Q that have low Bellman residuals.
 Definition 4 During timestep t of DGPQ X  X  execution with  X  Q as specified and  X  V ( s ) = max a  X  Q ( s,a ) , the set of known states is given by K t = { X  s,a  X  X   X  Q ( s,a )  X  ( R ( s,a ) +  X 
R MDP parameters as M for  X  s,a  X   X  K and values of V max for  X  s,a  X  /  X  K .
 We now recap (from Theorem 10 of (Strehl et al., 2009)) three sufficient conditions for proving an algorithm with greedy policy values V t is PAC-MDP. (1) Optimism:  X  V ( s )  X  V  X  ( s )  X  for all timesteps t . (2) Accuracy with respect to M K t  X  X  values:  X  V t ( s )  X  V  X  t M (3) The number of updates to  X  Q and the number of times a state outside M K is reached is bounded by a polynomial function of  X  X  S ( r ) , 1 , 1  X  , 1 1  X   X   X  .
 The proof structure is to develop lemmas that prove these three properties. After defining relevant structures and concepts, Lemmas 2 and 3 bound the number of possible changes and possible attempts to change  X  Q . We then de-fine the  X  X ypical X  behavior of the algorithm in Definition 6 and show this behavior occurs with high probability in Lemma 4. These conditions help ensure property 2 above. After that, Lemma 5 shows that the function stored in  X  Q is always optimistic, fulfilling property 1. Finally, property 3 is shown by combining Theorem 1 (number of steps before the GP converges) with the number of updates to  X  Q from Lemma 2, as formalized in Lemma 7.
 We now give the definition of an  X  X pdate X  (as well as  X  X uc-cessful update X ) to  X  Q , which extends definitions from the original DQL analysis.
 Definition 5 An update (or successful update ) of state-action pair ( s,a ) is a timestep t for which a change to overwrite) occurs such that  X  Q t ( s,a )  X   X  Q t +1 ( s,a ) &gt; attempted update of state-action pair (s,a) is a timestep t for which  X  s,a  X  is experienced and GP a,t . Var ( s ) &gt;  X  GP a,t +1 . Var ( s ) . An attempted update that is not successful (does not change  X  Q ) is an unsuccessful update . We bound the total number of updates to  X  Q by a polynomial term  X  based on the concepts defined above.
 Lemma 2 The total number of successful updates (over-writes) during any execution of DGPQ with 1 = 1 3 (1  X   X  ) is Proof The proof proceeds by showing that the bound on the number of updates in the single-state case from Sec-tion from V max to V min . This quantity is then multiplied by the covering number and the number of actions. See the Supplementary Material.
 The next lemma bounds the number of attempted updates. The proof is similar to the DQL analysis and shown in the Supplementary Material.
 Lemma 3 The total number of attempted up-dates (overwrites) during any execution of GPQ is We now define the following event, called A2 to link back to the DQL proof. A2 describes the situation where a state/action that currently has an inaccurate value (high Bellman residual) with respect to  X  Q is observed m times and this causes a successful update.
 Definition 6 Define Event A2 to be the event that for all timesteps t , if  X  s,a  X  /  X  K k 1 and an attempted update of  X  s,a  X  occurs during timestep t , the update will be success-ful, where k 1 &lt; k 2 &lt; ... &lt; k m = t are the timesteps where GP a k . Var ( s k ) &gt;  X  2 tol since the last update to affected  X  s,a  X  .
 We can set an upper bound on m , the number of experi-ences required to make GP a t . Var ( s t ) &lt;  X  2 tol , based on m in Theorem 1 from earlier. In practice m will be much smaller but unlike discrete DQL, DGPQ does not need to be given m , since it uses the GP variance estimate to decide if sufficient data has been collected. The following lemma shows that with high probability, a failed update will not occur under the conditions of event A2.
 Lemma 4 By setting we ensure that the probability of event A2 occurring is  X  1  X   X / 3 .
 Proof The maximum number of attempted updates is The next lemma shows the optimism of  X  Q for all timesteps with high probability  X  3 . The proof structure is similar to Lemma 3.10 of (Pazis &amp; Parr, 2013). See supplemental material.
 Lemma 5 During execution of DGPQ, Q  X  ( s,a )  X  Q ( s,a ) + 2 1 1  X   X  holds for all  X  s,a  X  with probability The next Lemma connects an unsuccessful update to a state/action X  X  presence in the known MDP M K .
 Lemma 6 If event A2 occurs, then if an unsuccessful up-date occurs at time t and GP a . Var ( s ) &lt;  X  2 tol at time t + 1 then  X  s,a  X  X  X  K t +1 .
 Proof The proof is by contradiction and shows that an un-successful update in this case implies a previously success-ful update that would have left GP a . Var ( s )  X   X  2 tol Supplemental material.
 The final lemma bounds the number of encounters with state/actions not in K t , which is intuitively the number of points that make updates to the GP from Theorem 1 times the number of changes to  X  Q from Lemma 2. The proof is in the Supplemental material.
  X  Q ( s,a )  X  Q  X  ( s,a )  X  2 1 1  X   X  holds for all t and  X  s,a  X  then the number of timesteps  X  where  X  s t ,a t  X  /  X  K t is at most where Finally, we state the PAC-MDP result for DGPQ, which is an instantiation of the General PAC-MDP Theorem (Theo-rem 10) from (Strehl et al., 2009).
 Theorem 2 Given real numbers 0 &lt; &lt; 1 1  X   X  and 0 &lt;  X  &lt; 1 and a continuous MDP M there exist inputs  X  2 tol (see (9)) and 1 = 1 3 (1  X   X  ) such that DGPQ executed on M will be PAC-MDP by Definition 2 with only The proof of the theorem is the same as Theorem 16 by (Strehl et al., 2009), but with the updated lemmas from above. The three crucial properties hold when A2 occurs, which Lemma 4 guarantees with probability  X  3 . Property 1 (optimism) holds from Lemma 5. Property 2 (accuracy) holds from the Definition 4 and analysis of the Bellman Equation as in Theorem 16 by (Strehl et al., 2009). Fi-nally, property 3, the bounded number of updates and es-cape events, is proven by Lemmas 2 and 7. Our first experiment is in a 2 -dimensional square over [0 , 1] 2 designed to show the computational disparity be-tween C-PACE and DGPQ. The agent starts at [0 , 0] with a goal of reaching within a distance of 0 . 15 of [1 , 1] . Move-ments are 0 . 1 in the four compass directions with additive uniform noise of  X  0 . 01 . We used an L 1 distance metric, L
Q = 9 , and an RBF kernel with  X  = 0 . 05 ,  X  2 n = 0 . 1 for the GP. Figure 2 shows the number of steps needed per episode (capped at 200 ) and computational time per step used by C-PACE and DGPQ. C-PACE reaches the optimal policy in fewer episodes but requires orders of magnitude more computation during steps with fixed point computa-tions. Such planning times of over 10 s are unacceptable in time sensitive domains, while DGPQ only takes 0 . 003 s per step.
 In the second domain, we use DGPQ to stabilize a sim-ulator of the longitudinal dynamics of an unstable F16 with linearized dynamics, which motivates the need for a real-time computable policy. The five dimensional state space contains height, angle of attack, pitch angle, pitch rate, and airspeed. Details of the simulator can be found in (Stevens &amp; Lewis, 2003). We used the reward r = h , desired height h d and elevator angle (degrees)  X  e . The control input was discretized as  X  e  X  { X  1 , 0 , 1 } and the elevator was used to control the aircraft. The thrust input to the engine was fixed as the reference thrust command at the (unstable) equilibrium. The simulation time step size was 0 . 05 s and at each step, the air speed was perturbed with Gaussian noise N (0 , 1) and the angle of attack was perturbed with Gaussian noise N (0 , 0 . 01 2 ) . A RBF kernel with  X  = 0 . 05 ,  X  2 n = 0 . 1 was used. The initial height was h , and if | h  X  h d |  X  200 , then the vertical velocity and angle of attack were set to zero to act as boundaries. DGPQ learns to stabilize the aircraft using less than 100 episodes for L Q = 5 and about 1000 episodes for L Q = 10 . The disparity in learning speed is due to the curse of dimensionality. As the Lipschitz constant doubles, N c in-creases in each dimension by 2 , resulting in a 2 5 increase in N c . DGPQ requires on average 0 . 04 s to compute its policy at each step, which is within the 20 Hz command frequency required by the simulator. While the maximum computa-tion time for DGPQ was 0 . 11 s, the simulations were run in MATLAB so further optimization should be possible. Fig-ure 3 shows the average reward of DGPQ in this domain using L Q = { 5 , 10 } . We also ran C-PACE in this domain but its computation time reached over 60 s per step in the first episode, well beyond the desired 20 Hz command rate. This paper provides sample efficiency results for using GPs in RL. In section 3, GPs are proven to be usable in the KWIK-Rmax MBRL architecture, establishing the previ-ously proposed algorithm GP-Rmax as PAC-MDP. In sec-tion 4.1, we prove that existing model-free algorithms using a single GP have exponential sample complexity, connect-ing to seemingly unrelated negative results on Q-learning learning speeds. Finally, the development of DGPQ pro-vides the first provably sample efficient model-free (with-out a planner or fixed-point computation) RL algorithm for general continuous spaces.
 We thank Girish Chowdhary for helpful discussions, Has-san Kingravi for his GP implementation, and Aerojet-Rocketdyne and ONR #N000141110688 for funding.
 Brafman, Ronen I. and Tennenholtz, Moshe. R-MAX -a general polynomial time algorithm for near-optimal re-inforcement learning. Journal of Machine Learning Re-search , 3:213 X 231, 2002.
 Chowdhary, Girish, Liu, Miao, Grande, Robert C., Walsh,
Thomas J., How, Jonathan P., and Carin, Lawrence. Off-policy reinforcement learning with gaussian processes. Acta Automatica Sinica , To appear, 2014.
 Chung, Jen Jen, Lawrance, Nicholas R. J., and Sukkarieh,
Salah. Gaussian processes for informative exploration in reinforcement learning. In Proceedings of the IEEE
International Conference on Robotics and Automation , pp. 2633 X 2639, 2013.
 Csat  X  o, L. and Opper, M. Sparse on-line gaussian processes. Neural Computation , 14(3):641 X 668, 2002.
 Deisenroth, Marc Peter and Rasmussen, Carl Edward.
Pilco: A model-based and data-efficient approach to pol-icy search. In Proceedings of the International Confer-ence on Machine Learning (ICML) , pp. 465 X 472, 2011. Engel, Y., Mannor, S., and Meir, R. Reinforcement learning with Gaussian processes. In Proceedings of the Interna-tional Conference on Machine Learning (ICML) , 2005. Even-Dar, Eyal and Mansour, Yishay. Learning rates for
Q-learning. Journal of Machine Learning Research , 5: 1 X 25, 2004.
 Jung, Tobias and Stone, Peter. Gaussian processes for sam-ple efficient reinforcement learning with RMAX-like ex-ploration. In Proceedings of the European Conference on Machine Learning (ECML) , 2010.
 Kalyanakrishnan, Shivaram and Stone, Peter. An empirical analysis of value function-based and policy search rein-forcement learning. In Proceedings of the International
Conference on Autonomous Agents and Multiagent Sys-tems , pp. 749 X 756, 2009.
 Li, Lihong and Littman, Miachael L. Reducing reinforce-ment learning to KWIK online regression. Annals of
Mathematics and Artificial Intelligence , 58(3-4):217 X  237, 2010.
 Li, Lihong, Littman, Michael L., Walsh, Thomas J., and
Strehl, Alexander L. Knows what it knows: a framework for self-aware learning. Machine Learning , 82(3):399 X  443, 2011.
 Pazis, Jason and Parr, Ronald. PAC optimal exploration in continuous space markov decision processes. In Pro-ceedings of the AAAI Conference on Artificial Intelli-gence , 2013.
 Puterman, Martin L. Markov Decision Processes: Discrete
Stochastic Dynamic Programming . Wiley, New York, 1994.
 Rasmussen, C. and Williams, C. Gaussian Processes for Machine Learning . MIT Press, Cambridge, MA, 2006. Stevens, Brian L. and Lewis, Frank L. Aircraft control and simulation . Wiley-Interscience, 2003.
 Strehl, Alexander L., Li, Lihong, Wiewiora, Eric, Lang-ford, John, and Littman, Michael L. PAC model-free reinforcement learning. In Proceedings of the Inter-national Conference on Machine Learning (ICML) , pp. 881 X 888, 2006.
 Strehl, Alexander L., Li, Lihong, and Littman, Michael L. Reinforcement learning in finite MDPs: PAC analysis.
Journal of Machine Learning Research , 10:2413 X 2444, 2009.
 Sutton, R. and Barto, A. Reinforcement Learning, an In-troduction . MIT Press, Cambridge, MA, 1998.
 Watkins, C. J. Q-learning. Machine Learning , 8(3):279 X 
