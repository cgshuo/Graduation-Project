 Many data sets contain rich information about objects, as well as pairwise relations between them. For instance, in networks of websites, scienti c papers, and other documents, each node has content consisting of a collection of words, as well as hyperlinks or citations to other nodes. In order to perform inference on such data sets, and make predictions and recommendations, it is useful to have models that are able to capture the processes which generate the text at each node and the links between them. In this paper, we combine classic ideas in topic modeling with a variant of the mixed-membership block model recently developed in the statistical physics community. The resulting model has the advantage that its parameters, including the mixture of topics of each document and the resulting overlapping communities, can be inferred with a simple and scalable expectation-maximization algorithm. We test our model on three data sets, performing unsupervised topic classi cation and link prediction. For both tasks, our model outperforms several existing state-of-the-art methods, achieving higher accuracy with signi cantly less computation, analyzing a data set with 1.3 million words and 44 thousand links in a few minutes.
 H.4 [ Information Systems Applications ]: Miscellaneous; I.2 [ ARTIFICIAL INTELLIGENCE ]: Learning Document classi cation; Topic modeling; Link prediction; Stochastic block model
Many modern data sets contain not only rich information about each object, but also pairwise relationships between them, forming networks where each object is a node and links represent the relationships. In document networks, for example, each node is a document containing a sequence of words, and the links between nodes are citations or hyper-links. Both the content of the documents and the topology of the links between them are meaningful.

Over the past few years, two disparate communities have been approaching these data sets from di erent points of view. In the data mining community, the goal has been to augment traditional approaches to learning and data min-ing by including relations between objects [15, 23, 33]: for instance, using the links between documents to help us la-bel them by topic. In the network community, including its subset in statistical physics, the goal has been to aug-ment traditional community structure algorithms such as the stochastic block model [14, 19, 30] by taking node at-tributes into account: for instance, to use the content of documents, rather than just the topological links between them, to help us understand their community structure.
In the original stochastic block model, each node has a dis-crete label, assigning it to one of k communities. These la-bels, and the k k matrix of probabilities with which a given pair of nodes with a given pair of labels have a link between them, can be inferred using Monte Carlo algorithms (e.g. [26]) or, more eciently, with belief propagation [12, 11] or pseudolikelihood approaches [7]. However, in real networks communities often overlap, and a given node can belong to multiple communities. This led to the mixed-membership block model [1], where the goal is to infer, for each node v , a distribution or mixture of labels v describing to what ex-tent it belongs to each community. If we assume that links are assortative, i.e., that nodes are more likely to link to others in the same community, then the probability of a link between two nodes v and v 0 depends on some measure of similarity (say, the inner product) of v and v 0 .
These mixed-membership block models t nicely with clas-sic ideas in topic modeling. In models such as Probabilistic Latent Semantic Analysis ( plsa ) [18] and Latent Dirichlet Allocation ( lda ) [4], each document d has a mixture d of topics. Each topic corresponds in turn to a probability dis-tribution over words, and each word in d is generated in-dependently from the resulting mixture of distributions. If we think of d as both the mixture of topics for generating words and the mixture of communities for generating links, then we can infer f d g jointly from the documents' content and the presence or absence of links between them.
There are many possible such models, and we are far from the rst to think along these lines. Our innovation is to take as our starting point a particular mixed-membership block model recently developed in the physics community [2], which we call the bkn model. It di ers from the mixed-membership stochastic block model ( mmsb ) of [1] in several ways: 1. The bkn model treats the community membership mix-2. The mmsb model generates each link according to a These two factors make it possible to t the bkn model using an ecient and exact expectation-maximization (EM) algorithm, making its inference highly scalable. The bkn model has another advantage as well: 3. The bkn model is degree-corrected , in that it takes the
In our work, we use a slight variant of the bkn model to generate the links, and we use plsa to generate the text. We present an EM algorithm for inferring the topic mixtures and other parameters. (While we do not impose a Dirichlet prior on the topic mixtures, it is easy to add a corresponding term to the update equations.) Our algorithm is scalable in the sense that each iteration takes O ( K ( N + M + R )) time for networks with K topics, N documents, and M links, where R is the sum over documents of the number of distinct words appearing in each one. In practice, our EM algorithm converges within a small number of iterations, making the total running time linear in the size of the corpus.
Our model can be used for a variety of learning and gen-eralization tasks, including document classi cation or link prediction. For document classi cation, we can obtain hard labels for each document by taking its most-likely topic with respect to d , and optionally improve these labels further with local search. For link prediction, we train the model using a subset of the links, and then ask it to rank the re-maining pairs of documents according to the probability of a link between them. For each task we determine the optimal relative weight of the content vs. the link information.
We performed experiments on three real-world data sets, with thousands of documents and millions of words. Our results show that our algorithm is more accurate, and con-siderably faster, than previous techniques for both document classi cation and link prediction.

The rest of the paper is organized as follows. Section 2 de-scribes our generative model, and compares it with related models in the literature. Section 3 gives our EM algorithm and analyzes its running time. Section 4 contains our exper-imental results for document classi cation and link predic-tion, comparing our accuracy and running time with other techniques. In Section 5, we conclude, and o er some direc-tions for further work.
In this section, we give our proposed model, which we call the Poisson mixed-topic link model ( pmtlm ) and its degree-corrected variant pmtlm-dc .
Consider a network of N documents. Each document d has a xed length L d , and consists of a string of words w for 1 ` L d , where 1 w d` W where W is the number of distinct words. In addition, each pair of documents d;d has an integer number of links connecting them, giving an adjacency matrix A dd 0 . There are K topics, which play the dual role of the overlapping communities in the network.
Our model generates both the content f w d` g and the links f
A model [18]. Each topic z is associated with a probabil-ity distribution z over words, and each document has a probability distribution d over topics. For each document 1 d N and each 1 ` L d , we independently choose a topic z = z d` Multi( d ), and choose the word w d` Multi( z ). Thus the total probability that w d` is a given word w is We assume that the number of topics K is xed. The dis-tributions z and d are parameters to be inferred. We generate the links using a version of the Ball-Karrer-Newman ( bkn ) model [2]. Each topic z is associated with a link density z . For each pair of documents d;d 0 and each topic z , we independently generate a number of links which is Poisson-distributed with mean dz d 0 z z . Since the sum of independent Poisson variables is Poisson, the total number of links between d and d 0 is distributed as In the data sets we study below, A dd 0 is 1 or 0 depending on whether d cites d 0 , giving a simple graph. On the other hand, in the sparse case the event that A dd 0 &gt; 1 has low probability in our model. Moreover, the fact that A dd 0 is Poisson-distributed rather than Bernoulli makes the deriva-tives of the likelihood with respect to the parameters dz and z very simple, allowing us to write down an ecient EM algorithm for inferring them.

This version of the model assumes that links are assor-tative, i.e., that links between documents only form to the extent that they belong to the same topic. One can easily generalize the model to include disassortative links as well, distinct topics z;z 0 to link [2].

We also consider degree-corrected versions of this model, where in addition to its topic mixture d , each document
Figure 1: Graphical models for link generation. has a propensity S d of forming links. In that case, We call this variant the Poisson Mixed-Topic Link Model with Degree Correction ( pmtlm-dc ).
Most models for document networks generate content us-ing either plsa [18], as we do, or lda [4]. The distinction is that plsa treats the document mixtures d as parame-ters, while in lda they are hidden variables, integrated over a Dirichlet distribution. As we show in Section 3, our ap-proach gives a simple, exact EM algorithm, avoiding the need for sampling or variational methods. While we do not impose a Dirichlet prior on d in this paper, it is easy to add a corresponding term to the update equations for the EM algorithm, with no loss of eciency.

There are a variety of methods in the literature to gener-ate links between documents. phits-plsa [10], link-lda [13] and link-plsa-lda [27] use the phits [9] model for link gen-eration. phits treats each document as an additional term in the vocabulary, so two documents are similar if they link to the same documents. This is analogous to a mixture model for networks studied in [28]. In contrast, block mod-els like ours treat documents as similar if they link to similar documents, as opposed to literally the same ones.
The pairwise link-lda model [27], like ours, generates the links with a mixed-topic block model, although as in mmsb [1] and lda [4] it treats the d as hidden variables integrated over a Dirichlet prior. They t their model with a variational method that requires N 2 parameters, making it less scalable than our approach.

In the c-pldc model [32], the link probability from d to d is determined by their topic mixtures d ; d 0 and the pop-ularity t d 0 of d 0 , which is drawn from a Gamma distribution with hyperparameters a and b . Thus t d 0 plays a role sim-ilar to the degree-correcting parameter S d 0 in our model, although we correct for the degree of d as well. However, c-pldc does not generate the content, but takes it as given.
The Relational Topic Model ( rtm ) [5, 6] assumes that the link probability between d and d 0 depends on the topics of the words appearing in their text. In contrast, our model uses the underlying topic mixtures d to generate both the content and the links. Like our model, rtm de nes the sim-ilarity of two topics as a weighted inner product of their topic mixtures: however, in rtm the probability of a link is a nonlinear function of this similarity, which can be logistic, exponential or normal, of this similarity.

Although it deals with a slightly di erent kind of dataset, our model is closest in spirit to the Latent Topic Hypertext Model ( lthm ) [17]. This is a generative model for hypertext networks, where each link from d to d 0 is associated with a speci c word w in d . If we sum over all words in d , the total number of links A dd 0 from d to d 0 that lthm would generate follows a binomial distribution When L d is large this becomes a Poisson distribution with ways: our parameters z give a link density associated with each topic z , and our degree correction S d does not assume that the number of links from d is proportional to its length.
We brie y mention several other approaches. The au-thors of [15] extend the probabilistic relational model ( framework and proposed a uni ed generative model for both content and links in a relational structure. In [24], the au-thors proposed a link-based model that describes both node attributes and links. The htm model [31] treats links as xed rather than generating them, and only generates the text. Finally, the lmmg model [21] treats the appearance or absence of a word as a binary attribute of each document, and uses a logistic or exponential function of these attributes to determine the link probabilities.

In Section 4 below, we compare our model to phits-plsa , link-lda , c-pldc , and rtm . Graphical models for the link generation components of these models, and ours, are shown in Figure 1.
Here we describe an ecient Expectation-Maximization algorithm to nd the maximum-likelihood estimates of the parameters of our model. Each update takes O ( K ( N + M + R )) time for a document network with K topics, N docu-ments, and M links, where R is the sum over the documents of the number of distinct words in each one. Thus the run-ning time per iteration is linear in the size of the corpus.
For simplicity we describe the algorithm for the simpler version of our model, pmtlm . The algorithm for the degree-corrected version, pmtlm-dc , is similar.
Let C dw denote the number of times a word w appears in document d . From (1), the log-likelihood of d 's content is S imilarly, from (2), the log-likelihood for the links A dd We ignore the constant term nominator of the Poisson distribution, since it has no bearing on the parameters. While we can use the total likelihood directly, in practice we can improve our performance signi -cantly by better balancing the information in the content vs. that in the links. In particular, the log-likelihood L content each document is proportional to its length, while its contri-bution to L links is proportional to its degree. Since a typical document has many more words than links, L content tends to be much larger than L links .
 Following [18], we can provide this balance in two ways. One is to normalize L content by the length L d , and another is to add a parameter that reweights the relative contribu-tions of the two terms L content and L links . We then maximize Varying from 0 to 1 lets us interpolate between two ex-tremes: studying the document network purely in terms of its topology, or purely in terms of the documents' content. Indeed, we will see in Section 4 that the optimal value of depends on which task we are performing: closer to 0 for link prediction, and closer to 1 for topic classi cation.
We maximize L as a function of f ; ; g using an EM al-gorithm, very similar to the one introduced by [2] for over-lapping community detection. We start with a standard trick to change the log of a sum into a sum of logs, writing Here h dw ( z ) is the probability that a given appearance of w in d is due to topic z , and q dd 0 ( z ) is the probability that a given link from d and d 0 is due to topic z . This lower bound holds with equality when giving us the E step of the algorithm.

For the M step, we derive update equations for the param-eters f ; ; g . By taking derivatives of the log-likelihood (7) (see the online version for details) we obtain Here d =
To analyze the running time, let R d denote the number of distinct words in document d , and let R = only KR of the parameters h dw ( z ) are nonzero. Similarly, q 0 ( z ) only appears if A dd 0 6 = 0, so in a network with M links only KM of the q dd 0 ( z ) are nonzero. The total num-ber of nonzero terms appearing in (9){(12), and hence the running time of the E and M steps, is thus O ( K ( N + M + R )).
As in [2], we can speed up the algorithm if is sparse, i.e. if many documents belong to fewer than K topics, so that many of the dz are zero. According to (9), if dz = 0 then h for all future iterations. If we choose a threshold below which dz is e ectively zero, then as becomes sparser we can maintain just those h d` ( z ) and q dd 0 ( z ) where dz 6 in turn simpli es the updates for and in (10) and (11).
We note that the simplicity of our update equations comes multilinear function of the parameters. Models where A dd is Bernoulli-distributed with a more complicated link prob-ability, such as a logistic function, have more complicated derivatives of the likelihood, and therefore more complicated update equations.

Note also that this EM algorithm is exact, in the sense that the maximum-likelihood estimators f b ; b ; b g are xed points of the update equations. This is because the E step (9) is exact, since the conditional distribution of topics associ-ated with each word occurrence and each link is a product distribution, which we can describe exactly with h dw and q we run our algorithm with many di erent initial conditions, and take the xed point with the highest likelihood.)
This exactness is due to the fact that the topic mixtures d are parameters to be inferred. In models such as lda and mmsb where d is a hidden variable integrated over a Dirichlet prior, the topics associated with each word and link have a complicated joint distribution that can only be ap-proximated using sampling or variational methods. (To be fair, recent advances such as stochastic optimization based on network subsampling [16] have shown that approximate inference in these models can be carried out quite eciently.)
On the other hand, in the context of nding communities in networks, models with Dirichlet priors have been observed to generalize more successfully than Poisson models such as bkn [16]. Happily, we can impose a Dirichlet prior on d with no loss of eciency, simply by including pseudocounts in the update equations|in essence adding additional words and links that are known to come from each topic. This lets us obtain a maximum a posteriori (MAP) estimate of an lda -like model. We leave this as a direction for future work.
Our model, like plsa and the bkn model, lets us infer a soft classi cation|a mixture of topic labels or community memberships for each document. However, we often want to infer categorical labels, where each document d is assigned to a single topic 1 z d K . A natural way to do this is to let z d be the most-likely label in the inferred mixture, ^ z d = argmax z dz . This is equivalent to rounding d to a delta function, dz = 1 for z = ^ z d and 0 for z 6 = ^ z d
If we wish, we can improve these discrete labels further using local search. If each document has just a single topic, the log-likelihood of our model is Note that here is a matrix, with o -diagonal entries that al-low documents with di erent topics z d ;z d 0 to be linked. Oth-erwise, these discrete labels would cause the network to split into K separate components.
 Let n z denote the number of documents of topic z , let L P d : z d = z C dw be the total number of times w appears in documents of topics z and z 0 , counting each link twice if z = z 0 . Then the MLEs for and are Applying these MLEs in (13) and (14) gives us a point es-timate of the likelihood of a discrete topic assignment z which we can normalize or reweight as discussed in Sec-tion 3.2 if we like. We can then maximize this likelihood using local search: for instance, using the Kernighan-Lin heuristic as in [20] or a Monte Carlo algorithm to nd a lo-cal maximum of the likelihood in the vicinity of ^ z . Each step of these algorithms changes the label of a single document d , so we can update the values of n z , L z , C zw , and m compute the new likelihood in O ( K + R d + d ) time. In our experiments we used the KL heuristic, and found that for some data sets it noticeably improved the accuracy of our algorithm for the document classi cation task.
In this section we present empirical results on our model and our algorithm for unsupervised document classi cation and link prediction. We compare its accuracy and running time with those of several other methods, testing it on three real-world document citation networks.
The top portion of Table 1 lists the basic statistics for three real-world corpora [29]: Cora, Citeseer, and PubMed Cora and Citeseer contain papers in machine learning, with K = 7 topics for Cora and K = 6 for Citeseer. PubMed consists of medical research papers on K = 3 topics, namely three types of diabetes. All three corpora have ground-truth topic labels provided by human curators.
Th ese data sets are available for download at http://www. cs.umd.edu/projects/linqs/projects/lbc/ The data sets for the three corpora are slightly di erent. PubMed contains the number of times C dw each word ap-peared in each document, while Cora and Citeseer record whether or not a word occurred at least once in the docu-ment. For Cora and Citeseer, we treat C dw as 0 or 1.
We compare the Poisson Mixed-Topic Link Model ( pmtlm ) and its degree-corrected variant, denoted pmtlm-dc , with phits-plsa , link-lda , c-pldc , and rtm (see Section 2.2). We used our own implementation of both phits-plsa and rtm . For rtm , we implemented the variational EM algo-rithm given in [6]. The implementation is based on the lda code available from the authors 2 . We also tried the code provided by J. Chang 3 , which uses a Monte Carlo algorithm for the E step, but we found the variational algorithm works better on our data sets. While rtm includes a variety of link probability functions, we only used the sigmoid function. We also assume a symmetric Dirichlet prior. The results for link-lda and c-pldc are taken from [32].

Each E and M step of the variational algorithm for rtm performs multiple iterations until they converge on estimates for the posterior and the parameters [6]. This is quite dif-ferent from our EM algorithm: since our E step is exact, we update the parameters only once in each iteration. Our con-vergence condition for the E step and for the entire EM al-gorithm are that the fractional increase of the log-likelihood between iterations is less than 10 6 ; we performed a maxi-mum of 50 iterations in each E step and a maximum of 500 EM iterations for the entire algorithm. To optimize the parameters (see the graphical model in Section 2.2) rtm uses a tunable regularization parameter , which can be thought of as the number of observed non-links. We tried various set-tings for , namely 0 : 1 M; 0 : 2 M; 0 : 5 M;M; 2 M; 5 M and 10 M where M is the number of observed links, and tuned sep-arately for each data set and each task. We used gradient descent to optimize the parameters in each M step.
As described in Section 3.2, for pmtlm , pmtlm-dc and phits-plsa we vary the relative weight of the likelihood of the content vs. the links, tuning to its best possible value for each data set and each task. For the PubMed data set, we also normalized the content likelihood by the length of the documents.
For pmtlm , pmtlm-dc and phits-plsa , we performed 500 independent runs of the EM algorithm, each with random initial values of the parameters and topic mixtures. For each run we iterated the EM algorithm up to 5000 times; we found that it typically converges in fewer iterations, with the crite-rion that the fractional increase of the log-likelihood for two successive iterations is less than 10 7 . Figure 2 shows that the log-likelihood as a function of the number of iterations are quite similar for all three data sets, even though these corpora have very di erent sizes. This indicates that even for large data sets, our algorithm converges within a small number of iterations, making its total running time linear in the size of the corpus.
S ee http://www.cs.princeton.edu/~blei/lda-c/
See http://www.cs.princeton.edu/~blei/lda/ Fi gure 2: The average log-likelihood of the PMTLM and PMTLM-DC models as a function of the num-ber of EM iterations, normalized so that 0 and 1 are the initial and nal log-likelihood for 5000 EM iterations. Each points is the average over 100 in-dependent runs. In both models and all three data sets, we approach 1 after just 1000 iterations, show-ing that the convergence time is roughly constant as a function of the size of the corpus.

For pmtlm and pmtlm-dc , we obtain discrete topic la-bels by running our EM algorithm and rounding the topic mixtures as described in Section 3.4. We also tested improv-ing these labels with local search, using the Kernighan-Lin heuristic to change the label of one document at a time until we reach a local optimum of the likelihood. More precisely, of those 500 runs, we took the T best xed points of the EM algorithm (i.e., with the highest likelihood) and attempted to improve them further with the KL heuristic. We used T = 50 for Cora and Citeseer and T = 5 for PubMed.
For rtm , in each E step, we initialize the variational pa-rameters randomly, and in each M step we initialize the hy-perparameters randomly. We execute 500 independent runs for each setting of the tunable parameter .
For each algorithm, we used several measures of the accu-racy of the inferred labels as compared to the human-curated ones. The Normalized Mutual Information (NMI) between two labelings C 1 and C 2 is de ned as Here MI( C 1 ;C 2 ) is the mutual information between C 1 C , and H( C 1 ) and H( C 2 ) are the entropies of C 1 and C respectively. Thus the NMI is a measure of how much infor-mation the inferred labels give us about the true ones. We also used the Pairwise F-measure (PWF) [3] and the Varia-tion of Information (VI) [25] (which we wish to minimize).
The best NMI, VI, and PWF we observed for each algo-rithm are given in Table 2, where for link-lda and c-pldc we quote results from [32]. The metrics of NMI and PWF used in [32] are identical to ours. For algorithms with tun-able parameters, including ours, phits-plsa and rtm , we tuned them based on the entire data set in order to measure its best possible performance. Of course, in practice one would tune these parameters based on partial knowledge, T able 1: The statistics of the three data sets, and the mean running time, for the EM algorithms in our model PMTLM, its degree-corrected variant PMTLM-DC, and PLSA, PHITS-PLSA, and RTM.
 Each corpus has K topics, N documents, M links, a vocabulary of size W , and a total size R . Running times for our algorithm, PLSA, and PHITS-PLSA are given for one run of 5000 EM iterations. Run-ning times for RTM consist of up to 500 EM itera-tions, or until the convergence criteria are reached. Our EM algorithm is highly scalable, with a running time that grows linearly with the size of the corpus. In particular, it is much faster than the variational algorithm for RTM. Improving discrete labels with the Kernighan-Lin heuristic (KL) increases our al-gorithm's running time, but improves its accuracy for document classi cation in Cora and Citeseer. such as the topics of a validation set of documents, and then use those parameter values to generalize to the test set.
We see that even without the additional step of local search, our algorithm does very well, outperforming all other methods we tried on Citeseer and PubMed and all but c-pldc on Cora. (Note that we did not test link-lda or c-pldc on PubMed.) Degree correction ( pmtlm-dc ) improves accu-racy signi cantly for PubMed.

Re ning our labeling with the KL heuristic improved the performance of our algorithm signi cantly for Cora and Cite-seer, giving us a higher accuracy than all the other methods we tested. For PubMed, local search did not increase accu-racy in a statistically signi cant way. In fact, on some runs it decreased the accuracy slightly compared to the initial labeling ^ z obtained from our EM algorithm; this is coun-terintuitive, but it shows that increasing the likelihood of a labeling in the model can decrease its accuracy.
In Figure 3, we show how the performance of pmtlm , pmtlm-dc , and phits-plsa varies as a function of , the relative weight of content vs. links. Recall that at = 0 these algorithms label documents solely on the basis of their links, while at = 1 they only pay attention to the content. Each point consists of the top 20 runs with that value of .
Figure 3 also shows that the optimal and its sensitiv-ity to performance di ers between data sets. For Cora and Citeseer, there is an intermediate value of at which pmtlm and pmtlm-dc have the best accuracy. However, this peak is fairly broad, showing that we do not have to tune very carefully. For PubMed, where we also normalized the con-tent information by document length, pmtlm-dc performs best at a particular value of . measure (PWF) achieved by each algorithm. Values marked by Kernighan-Lin heuristic is indicated by (KL).

We compare the running time of these algorithms, includ-ing pmtlm and pmtlm-dc with and without the kl heuris-tic, in Table 1. For algorithms with tunable parameters, we show the running time for a single value of that parameter. For our algorithms and phits-plsa , we show the running time for = 0 : 5, giving the content and the links equal weight. We see that our EM algorithm is much faster than the variational EM algorithm for rtm , and is scalable in that it grows linearly with the size of the corpus.
Link prediction (e.g. [8, 22, 34]) is a natural generalization task in networks, and another way to measure the quality of our model and our EM algorithm. Based on a training set consisting of a subset of the links, our goal is to rank all pairs without an observed link according to the probability of a link between them. For our models, we rank pairs according to the expected number of links A dd 0 in the Poisson distri-bution, (2) and (3), which is monotonic in the probability that at least one link exists.

We can then predict links between those pairs where this probability exceeds some threshold. Since we are agnostic about this threshold and about the cost of Type I vs. Type II errors, we follow other work in this area by de ning the accuracy of our model as the AUC, i.e. the probability that a random true positive link is ranked above a random true non-link. Equivalently, this is the area under the receiver op-erating characteristic curve (ROC). Our goal is to do better than the baseline AUC of 1 = 2, corresponding to a random ranking of the pairs.

We carried out 10-fold cross-validation, in which the links in the original graph are partitioned into 10 subsets with equal size. For each fold, we use one subset as the test links, and train the model using the links in the other 9 folds. We evaluated the AUC on the held-out links and the non-links. For Cora and Citeseer, all the non-links are used. For PubMed, we randomly chose 10% of the non-links for comparison. We trained the models with the same settings as those for document classi cation in Section 4.3; we executed 100 independent runs for each test. Note that unlike the document classi cation task, here we used the full topic mixtures to predict links, not just the discrete labels consisting of the most-likely topic for each document.
Note that pmtlm-dc assigns S d to be zero if the degree of d is zero. This makes it impossible for d to have any test link with others if its observed degree is zero in the training data. One way to solve this is to assign a small positive value to S d even if d 's degree is zero. Our approach assigns S d to be the smallest value among those S d 0 that are non-zero.
Figure 4(a) gives the AUC values for pmtlm and pmtlm-dc as a function of the relative weight of content vs. links. The green horizontal line in each of those subplots represent the highest AUC value achieved by the rtm model for each data set, using the best value of among those speci ed in Section 4.3. Note that the optimal value of the tunable parameters is task-dependent: the optimal value in rtm , or in our algorithms and phits-plsa , is not necessarily the same for link prediction as it is for document classi cation. Interestingly, for Cora and Citeseer the optimal value of is smaller than in Figure 3, showing that content is less im-portant for link prediction than for document classi cation. Thus, according to our experiments on both document clas-si cation and link prediction, the best choice of depends not only on the data set, but also on the task.

We also plot the receiver operating characteristic (ROC) curves and precision-recall curves that achieve the highest AUC values in Figure 4(b) and Figure 4(c) respectively. We see that, for all three data sets, our models outperform rtm and that the degree-corrected model pmtlm-dc is signi -cantly more accurate than the uncorrected one.
We have introduced a new generative model for document networks. It is a marriage between Probabilistic Latent Se-mantic Analysis [18] and the Ball-Karrer-Newman mixed membership block model [2]. Because of its mathematical simplicity, its parameters can be inferred with a particu-larly simple and scalable EM algorithm. Our experiments on document classi cation and link prediction show that it achieves high accuracy and eciency for a variety of data sets, outperforming other methods. In future work, we plan to apply it to other tasks including semisupervised learn-ing and content prediction, i.e., predicting the presence or absence of words in a document based on its links to other documents and/or a subset of its text. degree-corrected model PMTLM-DC performs best at a particular value of . uncorrected version (PMTLM).
We are grateful to Brian Ball, Brian Karrer, Mark New-man and David M. Blei for helpful conversations. Y.Z., X.Y., and C.M. are supported by AFOSR and DARPA un-der grant FA9550-12-1-0432. [1] E. Airoldi, D. Blei, S. Fienberg, and E. Xing. Mixed [2] B. Ball, B. Karrer, and M. E. J. Newman. Ecient [3] S. Basu. Semi-supervised Clustering: Probabilistic [4] D. M. Blei, A. Y. Ng, and M. I. Jordan. Latent [5] J. Chang and D. M. Blei. Relational topic models for [6] J. Chang and D. M. Blei. Hierarchical relational [7] A. Chen, A. A. Amini, P. J. Bickel, and E. Levina. [8] A. Clauset, C. Moore, and M. E. Newman.
 [9] D. Cohn and H. Chang. Learning to probabilistically [10] D. Cohn and T. Hofmann. The missing link-a [11] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborova. [12] A. Decelle, F. Krzakala, C. Moore, and L. Zdeborova. [13] E. Erosheva, S. Fienberg, and J. La erty.
 [14] S. E. Fienberg and S. S. Wasserman. Categorical data [15] L. Getoor, N. Friedman, D. Koller, and B. Taskar. [16] P. Gopalan, D. Mimno, S. Gerrish, M. Freedman, and [17] A. Gruber, M. Rosen-Zvi, and Y. Weiss. Latent topic [18] T. Hofmann. Probabilistic latent semantic indexing. In [19] P. Holland, K. Laskey, and S. Leinhardt. Stochastic [20] B. Karrer and M. E. J. Newman. Stochastic [21] M. Kim and J. Leskovec. Latent multi-group [22] L. L  X  u and T. Zhou. Link prediction in complex [23] Q. Lu and L. Getoor. Link-based classi cation. In [24] Q. Lu and L. Getoor. Link-based classi cation using [25] M. Meila. Comparing clusterings by the variation of [26] C. Moore, X. Yan, Y. Zhu, J. Rouquier, and T. Lane. [27] R. M. Nallapati, A. Ahmed, E. P. Xing, and W. W. [28] M. E. J. Newman and E. A. Leicht. Mixture models [29] P. Sen, G. Namata, M. Bilgic, and L. Getoor. [30] T. Snijders and K. Nowicki. Estimation and prediction [31] C. Sun, B. Gao, Z. Cao, and H. Li. HTM: A topic [32] T. Yang, R. Jin, Y. Chi, and S. Zhu. A Bayesian [33] P. Yu, J. Han, and C. Faloutsos. Link Mining: Models, [34] Y. Zhao, E. Levina, and J. Zhu. Link prediction for
