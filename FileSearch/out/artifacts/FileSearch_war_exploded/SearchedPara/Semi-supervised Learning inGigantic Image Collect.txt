
Courant Institute, NYU, Gigantic quantities of visual imagery are present on the web and in off-line databases. Effective techniques for searching and labeling this ocean of images a nd video must address two conflicting problems: (i) the techniques to understand the visual conte nt of an image and (ii) the ability to scale to millions of billions of images or video frames. Both aspec ts have received significant attention from researchers, the former being addressed by recent work on object and scene recognition, while the latter is the focus of the content-based image retrieval community (CBIR) [7]. A key issue pertaining to both aspects of the problem is the diversity of label information accompanying real world image data. A variety of collaborative and online anno tation efforts have attempted to build large collections of human labeled images, ranging from sim ple image classifications, to bounding-boxes and precise pixel-level segmentation [16, 21, 24]. Whi le impressive, these manual efforts have no hope of scaling to the many billions of images on the In ternet. However, even though most images on the web lack human annotation, they often have some kind of noisy label gleaned from nearby text or from the image filename and often this give s a strong cue about the content of the image. Finally, there are images where we have no informa tion beyond the pixels themselves. Semi-supervised learning (SSL) methods are designed to han dle this spectrum of label information [26, 28]. They rely on the density structure of the data itsel f to propagate known labels to areas lacking annotations, and provide a natural way to incorpora te labeling uncertainty. However, to model the density of the data, each point must measure its pro ximity to every other. This requires polynomial time  X  prohibitive for large-scale problems.
 In this paper, we introduce a semi-supervised learning sche me that is linear in the number of im-ages, enabling us to tackle very large scale problems. Build ing on recent results in spectral graph theory, we efficiently construct accurate numerical approx imations to the eigenvectors of the nor-malized graph Laplacian. Using these approximations, we ca n easily propagate labels through huge collections of images. 1.1 Related Work Cleaning up Internet image data has been explored by several authors: Berg et al. [4], Fergus et al. [8], Li et al. [13], Vijayanarasimhan et al. [22], amongst others. Unlike our approach, these methods operate independently on each class and would be pro blematic to scale to millions or bil-lions of images. A related group of techniques use active lab eling, e.g. [10]. Semi-supervised learn-ing is a rapidly growing sub-field of machine learning, deali ng with datasets that have a large number of unlabeled points and a much smaller number of labeled poin ts (see [5] for a recent overview). The most popular approaches are based on the graph Laplacian (e. g. [26, 28] and there has been much theoretical work devoted to the asymptotics of these Laplac ians [3, 6, 14]. However, these methods require the explicit manipulation of an n  X  n Laplacian matrix ( n being the number of data points), for example [2] notes:  X  X ur algorithms compute the inverse o f a dense Gram matrix which leads to O( n 3 ) complexity. This may be impractical for large datasets. X  The large computational complexity of standard graph Lapla cian methods has lead to a number of recent papers on efficient semi-supervised learning (see [27] for an overview). Many of these methods (e.g. [18, 12, 29, 25] are based on calculating the La placian only for a smaller, backbone, graph which reduces complexity to be cubic in the size of the s mall graph. In most cases [18, 12] the smaller graph is built simply by randomly subsampling a s ubset of the points, while in [29] a mixture model is learned on the original dataset and each mi xture component defines a node in the backbone graph. In [25] the backbone graph is found using non-negative matrix factorization. In [9] the backbone graph is a uniform grid over the high dimen sional space (so the number of nodes grows exponentially with dimension). In [20] the number of d atapoints is not reduced but rather the number of edges. This allows the use of sparse numerical alge bra techniques.
 The problem with approaches based on backbone graphs is that the spectrum of the graph Laplacian can change dramatically with different backbone construct ion methods [12]. This can also be seen visually (see Fig. 3) by examining the clusterings suggeste d by the full data and a small subsample. Even in cases where the  X  X orrect X  clustering is obvious when the full data is considered, the smaller subset may suggest erroneous clusterings (e.g. Fig. 3(left )). In our approach, we take an alternative route. Rather than trying to reduce the number of points, we t ake the limit as the number of points goes to infinity. We start by introducing semi-supervised learning in a graph setting and then describe an approxi-mation that reduces the learning time from polynomial to lin ear in the number of images. Fig. 1 illustrates the semi supervised learning problem. Followi ng the notations of Zhu et al. [28], we dataset X u = { x l +1 , ..., x n } . Thus in Fig. 1(a) we are given two labeled points and 500 unla beled points. Fig. 1(b) shows the output of a nearest neighbor clas sifier on the unlabeled points. The purely supervised solution ignores the apparent clusterin g suggested by the data.
 In order to use the unlabeled data, we form a graph G = ( V, E ) where the vertices V are the datapoints x 1 , ..., x n , and the edges E are represented by an n  X  n matrix W . Entry W ij is the edge weight between nodes i, j and a common practice is to set W ij = exp(  X  X  x i  X  x j k 2 / 2  X  2 ) . Let D be a diagonal matrix whose diagonal elements are given by D ii = Laplacian is defined as L = D  X  W , which is also called the unnormalized Laplacian.
 In graph-based semi-supervised learning, the graph Laplac ian L is used to define a smoothness operator that takes into account the unlabeled data. The mai n idea is to find functions f which agree with the labeled data but are also smooth with respect to the graph. The smoothness is measured by the graph Laplacian: Of course simply minimizing smoothness can be achieved by th e trivial solution f = 1 , but in semi-supervised learning, we minimize a combination of the smoothness and the training loss. For squared error training loss, this is simply: Figure 1: Comparison of supervised and semi-supervised lea rning on toy data. Semi-supervised learning seeks functions that are smooth with respect to the input density. Figure 2: Left: The three generalized eigenvectors of the graph Laplacian, for the toy data. Note that the semi-supervised solution can be written as a linear combination of these eigenvectors (in this case, the second eigenvector is enough). Using generalized eigenvectors (or equivalently normalized Laplacians) increases robustness of the first eigenvectors , compared to using the un-normalized eigenvectors. Right: The 2D density of the toy data, and the associated smoothness eigenfunctions defined by that density. The plots use the Matlab jet colormap . where  X  is a diagonal matrix whose diagonal elements are  X  ii =  X  if i is a labeled point and  X  ii = 0 for unlabeled points. The minimizer is of course a solution t o ( L +  X ) f =  X  y . Fig. 1(c) shows the semi-supervised solution.
 Although the solution can be given in closed form for the squa red error loss, note that it requires solving an n  X  n system of linear equations. For large n this poses serious problems with computation time and robustness. But as suggested in [5, 17, 28], the dime nsion of the problem can be reduced dramatically by only working with a small number of eigenvec tors of the Laplacian.
 Let  X  i ,  X  i be the generalized eigenvectors and eigenvalues of the grap h Laplacian L (solutions to L X  i =  X  i D X  i ). Note that the smoothness of an eigenvector  X  i is simply  X  T i L  X  i =  X  i so that eigen-vectors with smaller eigenvalues are smoother. Since any ve ctor in R n can be written f = the smoothness of a vector is simply the eigenvectors with small eigenvalues 1 .
 Fig. 2(left) shows the three generalized eigenvectors of th e Laplacian for the toy data shown in Fig. 1(a). Note that the semi-supervised solution (Fig. 1(c )) is a linear combination of these three eigenvectors (in fact just one eigenvector is enough). In ge neral, we can significantly reduce the dimension of f by requiring it to be of the form f = U  X  where U is a n  X  k matrix whose columns are the k eigenvectors with smallest eigenvalue. We now have: The minimizing  X  is now a solution to the k  X  k system of equations: 2.1 From Eigenvectors to Eigenfunctions Given the eigenvectors of the graph Laplacian, we can now sol ve the semi-supervised problem in a reduced dimensional space. But to find the eigenvectors in th e first place, we need to diagonalize a n  X  n matrix. How can we efficiently calculate the eigenvectors as the number of unlabeled points increases? We follow [23, 14] in assuming the data x i  X  R d are samples from a distribution p ( x ) and analyzing the eigenfunctions of the smoothness operator defined by p ( x ) . Fig. 2(right) shows the density in two dimensions for the toy data. This density defines a weighted s moothness operator on any function F ( x ) defined on R d which we will denote by L p ( F ) : creasing smoothness, the smoothness operator will define ei genfunctions of increasing smoothness. We define the first eigenfunction of L P ( f ) by a minimization problem: where D ( x ) = function  X ( x ) = 1 since it has maximal smoothness L P (1) = 0 . The second eigenfunction of L ( f ) minimizes the same problem, with the additional constraint that it be orthogonal to the first eigenfunction L ( f ) under additional constraints that value of an eigenfunction  X  k is simply its smoothness  X  k = L p ( X  k ) . Fig. 2(right) shows the first three eigenfunctions corresponding to the density of the to y data. Similar to the eigenvectors of the graph Laplacian, the second eigenfunction reveals the natu ral clustering of the data. Note that the eigenvalue of the eigenfunctions is similar to the eigenval ue of the discrete generalized eigenvector. How are these eigenfunctions related to the generalized eig envectors of the Laplacian? It is easy
P fine the eigenvectors approach the problems that define the ei genfunctions as n  X   X  . Thus under suitable convergence conditions, the eigenfunctions can b e seen as the limit of the eigenvectors as the number of points goes to infinity [1, 3, 6, 14]. For certain parametric probability functions (e.g. uniform, Gaussian) the eigenfunctions can be calculated an alytically [14, 23]. Thus for these cases, there is a tremendous advantage in estimating p ( x ) and calculating the eigenfunctions from p ( x ) rather than attempting to estimate the eigenvectors direct ly. For example, consider a problem with 80 million datapoints sampled from a 32 dimensional Gaussia n. Instead of diagonalizing an 80 mil-lion by 80 million matrix, we can simply estimate a 32  X  32 covariance matrix and get analytical eigenfunctions. In low dimensions, we can calculate the eig enfunction numerically by discretizing the density. Let g be the eigenfunction values at a set of discrete points, then g satisfies: where  X  W is the affinity between the discrete points, P is a diagonal matrix whose diagonal elements give the density at the discrete points, and  X  D is a diagonal matrix whose diagonal elements are the sum of the columns of P  X  W P , and  X  D is a diagonal matrix whose diagonal elements are the sum of the columns of P  X  W . This method was used to calculate the eigenfunctions in Fig . 2(right). Instead of assuming that p ( x ) has a simple, parametric form, we will use a more modest assum ption, that p ( x ) has a product form. Specifically, we assume that if we rotate t he data s = Rx then only the marginal distributions p ( s i ) .
 also an eigenfunction of L p with the same eigenvalue  X  i .
 Proof: This follows from the observation in [14, 23] that for separa ble distributions, the eigenfunc-tions are also separable.
 This observation motivates the following algorithm: The need to add a small constant to the histogram comes from th e fact that the smoothness operator L ( F ) ignores the value of F wherever the density vanishes, p ( x ) = 0 . Thus the eigenfunctions can oscillate wildly in regions with zero density. By adding a sm all constant to the density we enforce an additional smoothness regularizer, even in regions of ze ro density. Similar regularizers are used in [2, 9].
 This algorithm will recover eigenfunctions of L p , which depend only on a single coordinate. As discussed in [23], products of these eigenfunctions for dif ferent coordinates are also eigenfunctions, but we will assume the semi-supervised solution is a linear c ombination of only the single-coordinate eigenfunctions. By choosing the k eigenfunctions with smallest eigenvalue we now have k functions  X  k ( x ) whose value is given at a set of discrete points for each coord inate. We then use linear interpolation in 1D to interpolate  X ( x ) at each of the labeled points x l . This allows us to solve Eqn. 1 in time that is independent of the number of unlabeled points .
 Although this algorithm has a number of approximate steps, i t should be noted that if the  X  X nde-pendent X  components are indeed independent, and if the semi -supervised solution is only a linear combination of the single-coordinate eigenfunctions, the n this algorithm will exactly recover the semi-supervised solution as n  X  X  X  . Consider again a dataset of 80 million points in 32 dimensio ns and assume 100 bins per dimension. If the independent compon ents s = Rx are indeed indepen-dent, then this algorithm will exactly recover the semi-sup ervised solution by solving 32 100  X  100 generalized eigenvector problems and a single k  X  k least squares problem. In contrast, directly estimating the eigenvectors of the graph Laplacian will req uire diagonalizing an 80 million by 80 million matrix. In this section we describe experiments to illustrate the pe rformance and scalability of our approach. The results will be reported on the Tiny Images database [19] , in combination with the CIFAR-10 label set [11]. This data is diverse and highly variable, hav ing been collected directly from Internet search engines. The set of labels allows us to accurately mea sure the performance of our algorithm, while using data typical of the large-scale Internet settin gs for which our algorithm is designed. We start with a toy example that illustrates our eigenfuncti on approach, compared to the Nystrom method of Talwalker et al. [18], another approximate semi-supervised learning schem e that can scale to large datasets. In Fig. 3 we show two different 2D datasets , designed to reveal the failure modes of the two methods. 3.1 Features For the experiments in this paper we use global image descrip tors to represent the entire image (there is no attempt to localize the objects within the image s). Each image is thus represented by a single Gist descriptor [15], which we then project down t o 64 dimensions using PCA. As Figure 3: A comparison of the separable eigenfunction appro ach and the Nystrom method. Both methods have comparable computational cost. The Nystrom me thod is based on computing the graph Laplacian on a set of sparse landmark points and fails i n cases where the landmarks do not ad-equately summarize the density (left). The separable eigen function approach fails when the density is far from a product form (right). illustrated in Fig. 3, the eigenfunction approach assumes t hat the input distribution is separable over dimension. In Fig. 4 we show that while the raw gist descr iptors exhibit strong dependencies between dimensions, this is no longer the case after the PCA p rojection. Note that PCA is one of the few types of projection permitted: since distances between points must be preserved only rotations of the data are allowed.
 Figure 4: 2D log histograms formed from 1 million Gist descri ptors. Red and blue correspond to high and low densities respectively. Left: three pairs of dimensions in the raw Gist descriptor, along with their mutual information score (MI), showing str ong dependencies between dimensions. Right: the dimensions in the Gist descriptors after a PCA projectio n, as used in our experiments. The dependencies between dimensions are now much weaker, as the MI scores show. Hence the separability assumption made by our approach is not an unrea sonable one for this type of data. 3.2 Experiments with CIFAR label set The CIFAR dataset [11] was constructed by asking human subje cts to label a subset of classes of the Tiny Images dataset. For a given keyword and image, the su bjects determined whether the given image was indeed an image of that keyword. The resulting labe ls span 386 distinct keywords in the Tiny Images dataset. Our experiments use the sub-set of 126 c lasses which had at least 200 positive labels and 300 negative labels, giving a total of 63,000 imag es.
 Our experimental protocol is as follows: we take a random sub set of C classes from the set of 126. For each class c , we randomly choose a fixed test-set of 100 positive and 200 ne gative examples, reflecting the typical signal-to-noise ratio found in image s from Internet search engines. The training examples consist of t positive/negative pairs drawn from the remaining pool of 10 0 positive/negative images for each keyword.
 For each class in turn, we use our scheme to propagate labels f rom the training examples to the test examples. By assigning higher probability (values in f ) to the genuine positive images of each class, we are able to re-rank the images. We also make use of the the tr aining examples from keywords other than c by treating them as additional negative examples. For examp le, if we have C = 16 keywords and t = 5 training pairs per keyword, then we have 5 positive training examples and (5+(16-1)*10)=155 negative training examples for each cla ss. We use these to re-rank the 300 test images of that particular class. Note that the propagation f rom labeled images to test images may go through the unlabeled images that are not even in the same cla ss. Our use of examples from other classes as negative examples is motivated by real problems, where training labels are spread over many keywords but relatively few labels are available per cl ass.
 In experiments using our eigenfunction approach, we comput e a fixed set of k =256 eigenfunctions on the entire 63,000 datapoints in the 64D space with  X  = 0 . 2 and used these for all runs. For approaches that require explicit formation of the affinity m atrix, we calculate the distance between the 64D image descriptors using  X  = 0 . 125 . All approaches use  X  = 50 . To evaluate performance, we choose to measure the precision at a low recall rate of 15%, this being a sensible operating point as it corresponds to the first webpage or so in an Internet retr ieval setting. Given the split of +ve/-ve examples in the test data, chance level performance corresp onds to a precision of 33%. All results were generated by averaging over 10 different runs, each wit h different random train/test draws, and with different subsets of classes.
 In our first set of experiments, shown in Fig. 5(left), we comp are our eigenfunction approach to a variety of alternative learning schemes. We use C = 16 different classes drawn randomly from the 126, and vary the number of training pairs t from 0 up to 100 (thus the total number of labeled points, positive and negative, varied from 0 to 3200). Our ei genfunction approach outperforms other methods, particularly where relatively few training examp les are available. We use two baseline classifiers: (i) Nearest-Neighbor and (ii) RBF kernel SVM, w ith kernel width  X  . The SVM approach badly over-fits the data for small numbers of training exampl es, but catches up with the eigenfunction approach once 64+ve/1984-ve labeled examples are used.
 We also test a range of SSL approaches. The exact least-squar es approach ( f = ( L +  X )  X  1  X  Y ) achieves comparable results to the eigenfunction method, a lthough it is far more expensive. The eigenvector approach (Eqn. 1) performs less well, being lim ited by the k = 256 eigenvectors used (as k is increased, the performance converges to the exact least-squares solution). Neither of these methods scale to large image collections as the affinity matr ix W becomes too big and cannot be inverted or have its eigenvectors computed. Fig. 5(left) al so shows the efficient Nystrom method [18], using 1000 landmark points, which has a somewhat disap pointing performance. Evidently, as in Fig. 3, the landmark points do not adequately summarize th e density. As the number of landmarks is increased, the performance approaches that of the least s quares solution. Figure 5: Left: Performance (precision at 15% recall) on the Tiny Image CIFA R label set for differ-ent learning schemes as the number of training pairs is incre ased, averaged over 16 different classes. -Inf corresponds to the unsupervised case (0 examples). Our eigenfunction scheme (solid red) out-performs standard supervised methods (nearest-neighbors (green) and a Gaussian SVM (blue)) for small numbers of training pairs. Compared to other semi-sup ervised schemes, ours matches the exact least squares solution (which is too expensive to run o n a large number of images), while out-performing approximate schemes, such as Nystrom [18]. By us ing noisy labels in addition to the training pairs, the performance is boosted when few trainin g examples are available (dashed red). Right: (a) : The performance of our eigenfunction approach as the numbe r of training pairs per class and number of classes is varied. Increasing the number of classes also aids performance since labeled examples from other classes can be used as negative e xamples. (b) : As for (a) but now us-ing noisy label information (Section 3.3). Note the improve ment in performance when few training pairs are available. (c) : The performance of our approach (using no noisy labels) as t he number of eigenfunctions is varied.
 In Fig. 5(right)(a) we explore how our eigenfunction approa ch performs as the number of classes C is varied, for different numbers of training pairs t per class. For a fixed t , as C increases, the number of negative examples available increases thus aiding perfo rmance. Fig. 5(right)(c) shows the effect of varying the number of eigenfunctions k for C = 16 classes. The performance is fairly stable above k = 128 eigenfunctions (i.e. on average 2 per dimension), although some mild over-fitting seems to occur for small numbers of training examples when a v ery large number is used. 3.3 Leveraging noisy labels In the experiments above, only two types of data are used: lab eled training examples and unlabeled test examples. However, an additional source is the noisy la bels from the Tiny Image dataset (the keyword used to query the image search engine). These labels can easily be utilized by our frame-work: all 300 test examples for a class c are given a positive label with a small weight (  X / 10 ), while the 300( C  X  1) test examples from other classes are given negative label wi th the same small weight. Note that these labels do not reveal any information about which of the 300 test images are true positives. These noisy labels can provide a signific ant performance gain when few train-ing (clean) labels are available, as shown in Fig. 5(left) (c .f. solid and dashed red lines). Indeed, when no training labels are available, just the noisy labels , our eigenfunction scheme still performs very well. The performance gain is explored in more detail in Fig. 5(right)(b). In summary, using the eigenfunction approach with noisy labels, the performa nce obtained with a total of 32 labeled examples is comparable to the SVM trained with 64*16=512 lab eled examples. 3.4 Experiments on Tiny Images dataset Our final experiment applies the eigenfunction approach to t he whole of the Tiny Images dataset (79,302,017 images). We map the gist descriptor for each ima ge down to a 32D space using PCA and precompute k = 64 eigenfunctions over the entire dataset. The 445,954 CIFAR l abels (64,185 of which are +ve) cover 386 keywords, any of which can be re-ra nked by solving Eqn. 1, which takes around 1ms on a fast PC. In Fig. 6 we show our scheme on fou r different keywords, each using 3 labeled training pairs, resulting in a significant improve ment in quality over the original ordering. A nearest-neighbor classifier which is not regularized by th e data density performs worse than our approach.
 Ranking from search engine Nearest Neighbor re-r ank ing Eige nfunct ion re-ra nking Figure 6: Re-ranking images from 4 keywords in an 80 million i mage dataset, using 3 labeled pairs for each keyword. Rows from top:  X  X apanese spaniel X ,  X  X irbu s X ,  X  X strich X ,  X  X uto X . From L to R, the columns show the original image order, results of near est-neighbors and the results of our eigenfunction approach. By regularizing the solution usin g eigenfunctions computed from all 80 million images, our semi-supervised scheme outperforms th e purely supervised method. We have proposed a novel semi-supervised learning scheme th at is linear in the number of images, and then demonstrated it on challenging datasets, includin g one of 80 million images. The approach can easily be parallelized making it practical for Internet -scale image collections. It can also incor-porate a variety of label types, including noisy labels, in o ne consistent framework. The authors would like to thank H  X  ector Bernal and the anonymous reviewers and area chairs for their constructive comments. We also thank Alex Krizhevsky and Ge off Hinton for providing the CIFAR label set. Funding support came from: NSF Career award (ISI 0 747120), ISF and a Microsoft Research gift. [1] M. Belkin and P. Niyogi. Towards a theoretical foundatio n for laplacian based manifold meth-[2] M. Belkin, P. Niyogi, and V. Sindhwani. Manifold regular ization: A geometric framework for [3] Y. Bengio, O. Delalleau, N. L. Roux, J.-F. Paiement, P. Vi ncent, and M. Ouimet. Learning [4] T. Berg and D. Forsyth. Animals on the web. In CVPR , pages 1463 X 1470, 2006. [5] O. Chapelle, B. Sch  X  olkopf, and A. Zien. Semi-Supervised Learning. MIT Press, 2006. [6] R. R. Coifman, S. Lafon, A. Lee, M. Maggioni, B. Nadler, F. Warner, and S. Zucker. Geometric [7] R. Datta, D. Joshi, J. Li, and J. Z. Wang. Image retrieval: Ideas, influences, and trends of the [8] R. Fergus, L. Fei-Fei, P. Perona, and A. Zisserman. Learn ing object categories from google X  X  [9] J. Garcke and M. Griebel. Semi-supervised learning with sparse grids. In ICML workshop on [10] A. Kapoor, K. Grauman, R. Urtasun, and T. Darrell. Activ e learning with gaussian processes [11] A. Krizhevsky and G. E. Hinton. Learning multiple layer s of features from tiny images. Tech-[12] S. Kumar, M. Mohri, and A. Talwalkar. Sampling techniqu es for the Nystrom method. In [13] L. J. Li, G. Wang, and L. Fei-Fei. Optimol: automatic obj ect picture collection via incremental [14] B. Nadler, S. Lafon, R. R. Coifman, and I. G. Kevrekidis. Diffusion maps, spectral cluster-[15] A. Oliva and A. Torralba. Modeling the shape of the scene : a holistic representation of the [16] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freema n. Labelme: a database and web-[17] B. Schoelkopf and A. Smola. Learning with Kernels Support Vector Machines, Regulariza tion, [18] A. Talwalkar, S. Kumar, and H. Rowley. Large-scale mani fold learning. In CVPR , 2008. [19] A. Torralba, R. Fergus, and W. T. Freeman. 80 million tin y images: a large database for non-[20] I. Tsang and J. Kwok. Large-scale sparsified manifold re gularization. In NIPS , 2006. [21] L. van Ahn. The ESP game, 2006. [22] S. Vijayanarasimhan and K. Grauman. Keywords to visual categories: Multiple-instance learn-[23] Y. Weiss, A. Torralba, and R. Fergus. Spectral hashing. In NIPS , 2008. [24] B. Yao, X. Yang, and S. C. Zhu. Introduction to a large sca le general purpose ground truth [25] K. Yu, S. Yu, and V. Tresp. Blockwise supervised inferen ce on large graphs. In ICML workshop [26] D. Zhou, O. Bousquet, T. N. Lal, J. Weston, and B. Sch  X  olkopf. Learning with local and global [27] X. Zhu. Semi-supervised learning literature survey. T echnical Report 1530, University of [28] X. Zhu, Z. Ghahramani, and J. Lafferty. Semi-supervise d learning using gaussian fields and [29] X. Zhu and J. Lafferty. Harmonic mixtures: combining mi xture models and graph-based meth-
