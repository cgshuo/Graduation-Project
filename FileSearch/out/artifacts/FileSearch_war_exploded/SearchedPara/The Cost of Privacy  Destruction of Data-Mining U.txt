 Re-identification is a major privacy threat to public datasets containing individual records. Many privacy protection al-gorithms rely on generalization and suppression of  X  X uasi-identifier X  attributes such as ZIP code and birthdate. Their objective is usually syntactic sanitization: for example, anonymity requires that each  X  X uasi-identifier X  tuple appear in at least k records, while -diversity requires that the dis-tribution of sensitive attributes for each quasi-identifier have high entropy. The utility of sanitized data is also measured syntactically, by the number of generalization steps applied or the number of records with the same quasi-identifier.
In this paper, we ask whether generalization and sup-pression of quasi-identifiers offer any benefits over triv-ial sanitization which simply separates quasi-identifiers from sensitive attributes. Previous work showed that anonymous databases can be useful for data mining, but k -anonymization does not guarantee any privacy. By con-trast, we measure the tradeoff between privacy (how much can the adversary learn from the sanitized records?) and utility, measured as accuracy of data-mining algorithms executed on the same sanitized records.

For our experimental evaluation, we use the same datasets from the UCI machine learning repository as were used in previous research on generalization and suppression. Our results demonstrate that even modest privacy gains require almost complete destruction of the data-mining utility. In most cases, trivial sanitization provides equivalent utility and better privacy than k -anonymity, -diversity, and similar methods based on generalization and suppression.
 H.2.7 [ Database Management ]: Database Administra-tion X  Security, integrity, and protection ; H.2.8 [ Database Management ]: Database Applications X  Data mining Algorithms, Security
Microdata records contain information about specific in-dividuals. Examples include medical records used in public-health research, individual transactions or preferences re-leased to support the development of new data-mining algo-rithms, and records published to satisfy legal requirements.
In contrast to statistical databases and randomized re-sponse methods, the records in question contain actual, unperturbed data associated with individuals. Some of the attributes may be sensitive, e.g. , health-related attributes in medical records. Therefore, identifying attributes such as names and Social Security numbers are typically removed from microdata records prior to release. The published records may still contain  X  X uasi-identifiers, X  e.g. ,demo-graphic attributes such as ZIP code, age, or sex. Even though the quasi-identifier attributes do not directly reveal a person X  X  identity, they may appear together with the identity in another public database, or it may be easy to reconstruct their values for any given individual. Microdata records may also contain  X  X e utral X  attributes which are neither quasi-identifying, nor sensitive.

The association of quasi-identifiers with sensitive at-tributes in public records has long been recognized as a privacy risk [17,32]. This type of privacy breach is known as sensitive attribute disclosure , and is different from member-ship disclosure , i.e. , learning whether a certain individual is included in the database [12, 26, 30].

It is very easy to prevent sensitive attribute disclosure by simply not publishing quasi-identifiers and sensitive at-tributes together. Trivial sanitization that removes either all quasi-identifiers, or all sensitive attributes in each data re-lease provides the maximum privacy possible against an ad-versary whose knowledge about specific individuals is limited to their quasi-identifiers (this adversary is very weak, yet standard in the microdata sanitization literature [10,22,34]).
There is large body of research on techniques such as k -anonymity and -diversity that apply domain-specific gener-alization and suppression to quasi-identifier attributes and then publish them together with unmodified sensitive at-tributes. In this paper, we ask a basic question: what benefit do these algorithms provide over trivial san-itization? The only reason to publish generalized quasi-identifiers and sensitive attributes together is to support data-mining tasks that consider both types of attributes in the sanitized database. Our goal in this paper is to evaluate the tradeoff between this incremental gain in data-mining utility and the degradation in privacy caused by publishing quasi-identifiers together with sensitive attributes. Our contributions. First, we give a semantic definition of sensitive attribute disclosure. It captures the gain in the adversary X  X  knowledge due to his observations of the sani-tized dataset. This definition is somewhat similar to privacy definitions used in random-perturbation databases [13], but is adapted to the generalization and suppression framework.
Second, we give a methodology for measuring the tradeoff between the loss of privacy and the gain of utility. Pri-vacy loss is the increase in the adversary X  X  ability to learn sensitive attributes corresponding to a given identity. Util-ity gain is the increase in the accuracy of machine-learning tasks evaluated on the sanitized dataset. The baseline for both is the trivially sanitized dataset, which simply omits either all quasi-identifiers, or all sensitive attributes, thus providing maximum privacy and minimum utility.

Third, we evaluate our methodology on the same datasets from the UCI machine learning repository as used in pre-vious research on sanitized microdata utility [20 X 22]. We show that non-trivial generalization and suppression either results in large privacy breaches, or provides little incremen-tal utility vs. a trivially sanitized dataset. Therefore, even if the adversary X  X  knowledge is limited to quasi-identifiers, the data-mining utility must be destroyed to achieve only marginal privacy. To protect against an adversary with aux-iliary knowledge, the loss of utility must be even greater.
Privacy in statistical databases has been a topic of much research [2, 35]. Techniques include adding random noise to the data while preserving certain statistical aggregates [4, 8, 13] and interactive output perturbation [6, 11].

By contrast, microdata publishing involves releasing un-perturbed records containing information about individu-als. k -anonymity is a popular interpretation of privacy [10, 31, 34]. Many methods have been proposed for achieving it [5, 14, 15, 18, 19, 27, 29, 33, 41]; most apply generalization and suppression to quasi-identifiers only. In Section 6, we compare our experimental methodology to previous work.
Limitations of k -anonymity are: (1) it does not hide whether a given individual is in the database [26, 30], (2) it reveals individuals X  sensitive attributes [21, 22], (3) it does not protect against attacks based on background knowl-edge [22, 23], (4) mere knowledge of the k -anonymization algorithm can violate privacy [43], (5) it cannot be applied to high-dimensional data without complete loss of util-ity [3], and (6) special methods are required if a dataset is anonymized and published more than once [7, 37, 40].
In [28],  X hrn and Ohno-Machado proposed that the sen-sitive attributes associated with each quasi-identifier be  X  X i-verse. X  This is similar to p -sensitivity [36], -diversity [22], and others [39, 42]. Diversity of sensitive attributes, how-ever, is neither necessary, nor sufficient to prevent sensitive attribute disclosure (see [21] and Section 4). A stronger def-inition appears in [24], but it is unclear whether it can be achieved in the data access model considered in the gener-alization and suppression framework.

In the k -anonymity literature, the adversary X  X  knowledge is limited to quasi-identifiers such as age and ZIP code. Stronger adversaries with background knowledge are con-sidered in [9, 23]. Our results show that generalization and suppression do not p rotect privacy even against very weak adversaries who only know the quasi-identifiers; privacy ob-viously fails against stronger adversaries as well.
This paper is about sensitive attribute disclosure. Mem-bership disclosure, i.e. , learning whether a given individual is present in the sanitized database, is a different, incompa-rable privacy property. Methods for preventing membership disclosure such as [12,26,30] are complementary to our work.
Let T = { t 1 ,...t n } be a data table. Each t i is a tu-ple of attribute values representing some individual X  X  record. Let A = { a 1 ,...a m } be the set of attributes; t [ a i the value of attribute a i for tuple t . Weusethefollow-ing notation for subsets of attributes and tuples. If C = { c 1 ,c 2 ,...c p } X  A ,then t [ C ] denotes ( t [ c 1 ] ,...t U = { u 1 ,u 2 ,...u p } X  T ,then U [ a ] denotes ( u 1 [ a
Let S X  X  be the sensitive attribute . This is an attribute whose value the adversary should not be able to associate with an individual ( e.g. , medical information). Let { s 1 ,...s l } be the set of possible attribute values for the sensitive attribute S . All of the concepts in this paper are easily explained in the single sensitive attribute setting, but can also be generalized to multiple sensitive attributes.
Let Q X  X \S be the quasi-identifier , i.e. ,thesetofnon-sensitive ( e.g. , demographic) attributes whose values may be known to the adversary for a given individual.

Two tuples t i and t j are Q -equivalent (denoted t i Q  X  t if t i [ Q ]= t j [ Q ]. This equivalence relation partitions quasi-identifier equivalence classes, denoted as t j ,where t  X  t j iff t i Q  X  t j .Let E Q  X  T be a set of representative records for each equivalence class imposed by Q  X  .
We make the standard assumption that the adversary knows only the quasi-identifiers [10, 18, 34, 41]. This weak adversary model makes our results stronger because if pri-vacy fails against the weak adversary, it will also fail against adversaries who have additional knowledge [22, 23].
T may also contain attributes in A\ ( Q X  X  ), which are neither sensitive, nor quasi-identifying. For example, a user may wish to construct a classifier that predicts the values of these  X  X eutral X  attributes ( e.g. , length of hospital stay) on the basis of both quasi-identifiers ( e.g. , age) and sensitive attributes ( e.g. , diagnosis).

Consider a subset of tuples U = { u 1 ,u 2 ,...u p } X  T ,and the distribution of sensitive attribute values within U .For any sensitive attribute value s ,denoteby U s the set { u  X  U | u [
S ]= s } of tuples in U whose sensitive attribute value is equal to s , and denote by p ( U, s ) the corresponding fraction of tuples in U , computed as | U s | | U | . The notation be understood as  X  X he probab ility that a randomly chosen member of U has sensitive attribute value s . X 
We assume that whenever an adversary is provided with a sanitized table T , the record rows appear in a random order to prevent  X  X nsorted matching attacks X  [34].
Sensitive attribute disclosure occurs when the adver-sary learns information about an individual X  X  sensitive attribute(s). This form of privacy breach is different and in-comparable to learning whether an individual is included in the database, which is the focus of differential privacy [12].
To obtain a meaningful definition of data privacy, it is necessary to quantify the knowledge about sensitive attributes that the adversary gains from observing the san-itized database. We call our definitions semantic because they capture this shift in the adversary X  X  knowledge. The need for semantic definitions of privacy is well-understood for random-perturbation databases ( e.g. , [13]). By contrast, research on microdata privacy has focused on purely syntac-tic privacy definitions such as k -anonymity and -diversity (surveyed below), which only consider the distribution of attribute values in the sanitized database, without directly measuring what the adversary may learn. We use the standard model from the literature [10, 22]. The adversary is given a sanitized table T generated from an original table T , and the quasi-identifier t [ Q ]forsome target individual t knowntobeinthetable T ( i.e. ,weare not considering membership disclosure). We re-emphasize that giving the adversary more background knowledge will result in even worse disclosure than we demonstrate.
To keep the sanitized database  X  X ruthful X  [31,34], general-ization and suppression are applied only to quasi-identifiers, with sensitive attributes left intact. Therefore, the most  X  X rivate X  sanitized table possible with this approach is the trivial sanitization in which all Q are suppressed. Equally effective is the trivial sanitization in which all S are sup-pressed (and released in a separate, unlinked table).
The adversary X  X  baseline knowledge A base is the minimum information about sensitive attributes that he can learn after any sanitization, including trivial sanitization which releases quasi-identifiers and sensitive attributes separately. A base the distribution of sensitive attributes in the original table, which is revealed by any generalization and suppression al-gorithm because sensitive attributes are left untouched to keep them  X  X ruthful. X  We are concerned about privacy leaks in excess of this baseline knowledge; for example, if 90% of the individuals in T have cancer, then it should not be considered an attribute disclosure if the adversary concludes that t has cancer with probability 90%, since this baseline distribution is always revealed to the adversary. We for-mally define A base as the vector of probabilities representing the distribution of sensitive attribute values in the entire table T : A base = p ( T,s 1 ) ,p ( T,s 2 ) ,...,p ( T,s l
The adversary X  X  posterior knowledge A san is what he learns from the sanitized table T about the sensitive attributes of his target individual t  X  T . Unlike A base , A san takes quasi-identifiers into account, because the records in T contain a mixture of generalized and suppressed quasi-identifiers. Be-cause the generalization hierarchy on quasi-identifiers is re-quired to be totally ordered [10], the adversary can uniquely identify the quasi-identifier equivalence class t containing the sanitized record of t in T . A san is the distribution of sensitive attribute values within this class t : A san ( t p ( t ,s 1 ) ,p ( t ,s 2 ) ,...,p ( t ,s l ) .

Sensitive attribute disclosure is the difference between the adversary X  X  posterior knowledge A san and his baseline knowl-edge A base . It can measured additively or multiplicatively. Informally, it captures how much more the adversary learns by observing sanitized quasi-identifiers than he would have learned from a  X  X aximally private X  database where sensitive attributes are separated from the quasi-identifiers.
To capture the incremental gain in the adversary X  X  knowl-edge caused by the sanitized table T ,wefirstconsiderhis baseline knowledge A base as defined above. Recall that it consists of the distribution of sensitive attributes in the ta-ble T  X  , where all quasi-identifiers have been suppressed (any sanitization that does not touch sensitive attributes neces-sarily reveals T  X  ). Furthermore, the adversary knows t for all t  X  T , i.e. , the quasi-identifier attribute values for all individuals in the database. The adversary can easily learn these values from external databases and other resources.
Definition 1 (  X  -disclosure privacy). We say that an equivalence class t is  X  -disclosure-private with regard to the sensitive attribute S if, for all s  X  S Atable T is  X  -disclosure-private if for every t  X  X  Q , t  X  -disclosure private.

Intuitively, a table is  X  -disclosure private if the distribu-tion of sensitive attribute values within each quasi-identifier class is roughly the same as their distribution in the entire table. In contrast to [21], we use a multiplicative definition. It correctly models disclosures when some value of the sen-sitive attribute occurs in certain quasi-identifier classes, but not in others. It also allows us to derive a bound on the gain in adversarial knowledge, by relating the  X  parameter to in-formation gain used by decision tree classifiers such as ID3 and C4.5. Gain ( S , Q ) is defined as the difference between the entropy of S and the conditional entropy H ( S|Q ). Lemma 1. If T satisfies  X  -disclosure privacy, then Gain ( S , Q ) &lt; X  .Let  X  s = p ( T,s ) and let  X  t,s = Note that  X  s = Proof: = = = = &lt; =
Lemma 1 shows that when a database satisfies  X  -disclosure privacy, the ability to build a predictor for sensitive at-tributes S based on the quasi-identifier Q is bounded by  X  . Note that definition 1 is stronger than the bound given by lemma 1, because it requires that the distributions A base and A san be similar, rather than just have similar entropies. k -anonymity. k -anonymity is based on the observation that identity disclosure canleadto sensitive attribute disclo-sure : if an adversary can determine which database record corresponds to the target individual, then he can determine this individual X  X  sensitive attribute value.

Definition 2 ( k -anonymity [31,34]). Table T is k -anonymous if and only if for each t j  X  X  Q , | t j | X  k k -anonymity does not prevent sensitive attribute disclo-sure [22, 36], because an individual may belong to an equiv-alence class in which sensitive attributes have a low-entropy distribution. The attacker then learns the sensitive attribute without learning which record belongs to the individual. -diversity. To prevent homogeneity attacks, sensitive at-tribute values within each equivalence class should be  X  X i-verse. X  This was observed in [28] and in [22].

Definition 3 (Recursive ( c, ) -diversity [22]). Let r denote the number of times the i th most frequent sensi-tive value appears in t i . Given a constant c , t i satisfies recursive ( c, ) -diversity if r 1 &lt;c ( r + r +1 + ... Atable T satisfies recursive ( c, ) -diversity if for every t  X  X  -groups ( T ) , t i satisfies recursive ( c, ) -diversity. We say that ( c, 1) -diversity is always satisfied.

Unfortunately, -diversity is neither necessary, nor suffi-cient to prevent sensitive attribute disclosure [21]. While it prevents an attacker from learning a sensitive attribute exactly , it can still reveal a lot of probabilistic information.
For example, consider a database in which 1% of indi-viduals have a rare form of cancer and the quasi-identifier equivalence class t i in which, say, 30% have this form of cancer (or any high percentage, as required by the diversity criterion). If the adversary X  X  target individual t  X  t i ,then the adversary can immediately infer that his target is far more likely to have this form of cancer than a random in-dividual in the database. In general, probabilistic sensitive attribute disclosure occurs whenever an attribute which is not diverse in the overall sensitive attribute distribution ap-pears diverse in some quasi-identifier equivalence class. On the other hand, if only 1% of individuals in the equivalence class have this form of cancer, then there is no sensitive at-tribute disclosure, even though the class is not diverse. t -closeness. In [21], Li et al. assume that the adversary knows the distribution A base of sensitive attribute values over the entire table, which is a reasonable assumption because this information would have been revealed even if the quasi-identifiers had been completely suppressed.

If the adversary determines that the target individual is in an equivalence class with a sensitive attribute value distri-bution significantly different from A base ,thenhelearnsalot of information about the individual. The goal of t -closeness is to ensure that these distributions are never too different.
Definition 4 ( t -closeness [21]). An equivalence class t i has t -closeness if the distance between the distri-bution of a sensitive attribute in this class A san ( the distribution of the attribute in the whole table A base no more than a threshold t . A table has t -closeness if all equivalence classes have t -closeness.

The critical question is how to measure the distance be-tween distributions. In [21], the Earth Mover X  X  distance (EMD) is used, which for nominal attributes is equivalent to
A diff . This is an additive (as opposed to multiplicative) measure, and does not translate directly into a bound on the adversary X  X  ability to learn sensitive attributes associ-ated with a given quasi-identifier. By contrast, Lemma 1 gives such a bound for our semantic privacy definition.
Even though t -closeness does not directly bound the gain in adversary X  X  knowledge, it is similar in its spirit to se-mantic privacy; it, too, attempts to capture the difference between the adversary X  X  baseline knowledge and the knowl-edge he gains from the quasi-identifier equivalence classes in the sanitized table. As parameters ( t and  X  , respectively) approach 0, both t -closeness and our definition 1 converge to statistical independence of quasi-identifiers and sensitive attributes within the sanitized database.
Semantic privacy definitions, such as our definition 1, bound sensitive attribute disclosure, but an actual database instance may have less sensitive attribute disclosure (and thus more privacy) than permitted by the definition.
Conventional privacy metrics rely on syntactic properties of the sanitized dataset: number of records with the same quasi-identifier ( k -anonymity) or frequency of sensitive at-tributes within each quasi-identifier class ( -diversity). Un-fortunately, the two metrics are incomparable. In [22], k are compared directly, even though the two have different domains: k can vary from 1 to the total number of records, while can vary from 1 to the number of different sensi-tive attribute values. For example, a 1000-record database with a binary sensitive attribute can never be more than 2-diverse, but it can be anywhere up to 1000-anonymous.
We propose two different metrics to quantify attribute dis-closure allowed by a sanitized database T as opposed to T  X  where all quasi-identifiers have been trivially suppressed. The first is based on the attribute disclosure distance A
A know stands for  X  X dversarial knowledge gain. X  It is the average amount of information about the sensitive attributes of individual t that the adversary learns because he is able to identify the class t based on t  X  X  quasi-identifier.
One may also consider a metric based on A quot , but only semantically private databases achieve a finite privacy score. Other privacy definitions allow sensitive attribute values to be absent from some quasi-identifier classes, enabling the adversary to learn with certai nty that the corresponding in-dividual does not have this value.

The second metric quantifies the adversary X  X  ability to predict his target t  X  X  sensitive attribute using his best strat-egy, which is to guess the most common sensitive attribute in t . For a quasi-identifier class t ,let s max ( t )bethe most common sensitive attribute value found in t . Then, A acc stands for  X  X dversarial accuracy gain X  and measures the increase in the adversary X  X  accuracy after he observes the sanitized database T compared to his baseline accuracy from observing T  X  , which is the most private database that can be obtained by generalization and suppression.
A acc underestimates the amount of information leaked by the sanitized table T , because it does not consider shifts in the probabilities of non-majority sensitive attributes. It is still a useful metric because it can be directly compared to our metrics of data-mining utility, described in Section 5.
Utility of any dataset, whether sanitized or not, is innately tied to the computations that one may perform on it. For example, a census dataset may support an extremely ac-curate classification of income based on education, but not enable clustering based on household size. Without a work-load context, it is meaningless to say whether a dataset is  X  X seful X  or  X  X ot useful, X  let alone to quantify its utility.
Nevertheless, the stated goal of privacy-preserving micro-data publishing is to produce sanitized datasets that have  X  X ood X  utility for a large variety of workloads. The un-known workload is an essential premise X  X f the workloads were known in advance, the data publisher could simply ex-ecute them on the original data and publish just the results instead of releasing a sanitized version of the data.
The need for a workload-independent measure of utility has led to the use of syntactic properties as a proxy for utility. One approach is to minimize the amount of gener-alization and suppression applied to the quasi-identifier at-tributes to achieve a given level of privacy [10]. This  X  X ini-mization X  is done with respect to absolute difference, relative distance, maximum distribution, or minimum suppression. Other syntactic metrics include the number of generaliza-tion steps, average size of quasi-identifier equivalence classes, the sum of squares of class sizes [22], and preservation of marginals [16].

Workload-independent metrics quantify the  X  X amage X  caused by sanitization, but they do not measure how much utility remains. For example, small quasi-identifier equiva-lence classes do not imply anything about the accuracy of classifiers that one may compute on the sanitized data [25].
It has been recognized that utility of sanitized databases must be measured empirically, in terms of specific workloads such as classification algorithms [15, 20, 38]. This does not necessarily contradict the  X  X nknown workload X  premise of sanitization. It simply acknowledges that even when sani-tization satisfies a syntactic damage minimization require-ment, it may still destroy the utility of a dataset for certain tasks; it is thus essential to measure the latter when evalu-ating effectiveness of various sanitization methods.
We can assume that users of the sanitized database are in-terested in workloads that take advantage of attribute corre-lations within the database, e.g. , construction of classifiers. For workloads which consider attributes in isolation, the data publisher can achieve maximum privacy by simply pub-lishing two tables, one with the permuted quasi-identifiers, the other with the remaining attributes since they cannot be linked to the quasi-identifiers. Intuitively, utility of a sanitized database should be measured by how well cross-attribute correlations are preserved after sanitization.
It is critically important to measure both privacy and util-ity using the same methodology. Otherwise, maximizing utility may lead to privacy violations. For example, if utility is measured as the ability to predict sensitive attributes from the quasi-identifiers, then it is exactly the same as adversar-ial sensitive attribute disclosure! Iyengar [15] concludes that classification accuracy is maximized when attributes are ho-mogeneous within each quasi-identifier group: this directly contradicts the diversity requirement [22,36]. Similarly, [41] says that the data publishing process should preserve cor-relation between quasi-identifiers and sensitive attributes. This contradicts both diversity and semantic privacy, and immediately leads to sensitive attribute disclosure.
We aim to measure the tradeoffs between privacy and util-ity in a single framework, using semantic definitions for both: privacy in terms of adversarial sensitive attribute disclosure, utility in terms of concrete machine-learning tasks.
First, for a given workload w , we measure workload-specific utility of trivially sanitized datasets, i.e. , datasets from which either all quasi-identifiers Q , or all sensitive at-tributes S have been removed. Both provide the maximum privacy achievable using generalization and suppression. Let U ( w ) base be the corresponding empirical utility (to com-pute U ( w ) base , we pick the trivial sanitization with the largest utility). We give specific workloads and utility metrics in Section 6; for example, when the workload w involves
Then, we consider several non-trivially sanitized tables T one for each value of the sanitization parameter. For each table, we compute its workload-specific utility U ( w ) san utility gain provided by the release of non-trivially sanitized sanitization is pointless for this specific workload. In this case, a trivial sanitization which suppresses all Q or removes all S provides as much utility as any sophisticated sanitiza-tion algorithm while providing as much privacy as possible.
Another metric we X  X l employ is U ( w ) max , the utility of workload w as measured on the original, pre-sanitization database. If U ( w ) max is low ( e.g. , the corresponding classifier has low accuracy), this means that the workload is inappro-priate for the data regardless of sanitization. It does not make sense to measure utility in terms of this workload, because even if the users had been given the entire original database, the utility would have been low.
Our experiments demonstrate that a trivial sanitizer which simply suppresses all quasi-identifiers or all sensitive attributes produces datasets with equivalent utility and better privacy (or equivalent privacy and better utility) than non-trivial generalization and suppression.

This appears to contradict previous work. For example, it was shown that useful machine-learning workloads can be evaluated on k -anonymous datasets [15, 20]. Of course, k -anonymity is neither necessary, nor sufficient for privacy. The  X  X seful X  datasets in question simply don X  X  prevent sen-sitive attribute disclosure.

At the other end of the spectrum, -diversity [22] and t -closeness [21] do limit sensitive attribute disclosure. Utility, however, is measured syntactically, by the number of gen-eralization steps applied to quasi-identifiers, average size of quasi-identifier equivalence classes, sum of squares of class sizes, or preservation of marginals. In contrast to this paper, the actual data-mining utility is not measured.

Wang et al. [38] give a sanitization which ensures a strong privacy definition and better data-mining utility on the UCI Table 1: Summary of the UCI  X  X dult X  dataset.
 Adult dataset than simple removal of all sensitive attributes. They do not consider the other trivial sanitization, which is to remove all quasi-identifiers. We repeated their experi-ments and observed that their sanitization does not provide significantly better utility than the trivially sanitized dataset consisting of sensitive attributes only.
Semantic privacy, as defined in Section 4.2, is easily incor-porated into k -anonymity frameworks such as Incognito [18]. Like -diversity [22] and t -closeness [21], semantic privacy has the monotonicity property : a generalization of a seman-tically private table is itself semantically private.
We used the implementation of generalization and sup-pression from LeFevre et al. [20], and modified the con-straint checking portion of the code to support recursive ( c, )-diversity ( c =3 in all of our tests), t -closeness for nom-inal sensitive attributes, and semantic privacy from this pa-per (in the figures, s stands for  X  from definition 1). This im-plementation is  X  X orkload-aware, X  i.e. , when choosing quasi-identifiers in Q to generalize, it attempts to maximize infor-mation gain for some target attribute.
To enable direct comparison with previous microdata san-itization work [21, 22], we used the same data for our ex-periments: the 45,222-record Adult database from the UCI Machine Learning Repository [1], described in table 1. Our classifier learning used Weka with the default settings for C4.5 (J48), Random Forests, and Naive Bayes. For all clas-sification experiments, we used 10-fold cross-validation. Choosing the quasi-identifier. In a real database, the set of quasi-identifier attributes Q is domain-specific, and includes the attributes to which the adversary is most likely to have access via an external database ( e.g. , demographic information). For our experiments, we examined several dif-ferent sets of attributes for Q . All were picked to maximize the likelihood that sanitization will produce a useful table.
It is common in the literature to choose large quasi-identifiers, sometimes consisting of all non-sensitive at-tributes. A larger quasi-identifier, however, gives more prior information to the adversary and requires heavier generalization and suppression during sanitization. Large quasi-identifiers thus underestimate utility of the dataset and increase the risk of a privacy breach. Our most impor-tant criterion for choosing Q was to keep it small ,tomake the adversary X  X  task as hard as possible.

Furthermore, if a legitimate user (whom we will call  X  X e-searcher X ) is to get more utility out of the sanitized database Table 2: The effect of including age, sex, and race on decision tree learning accuracy. than the adversary, his task(s) must be different from the adversary X  X . If the sensitive attribute is also the researcher X  X  target attribute and all other attributes are quasi-identifiers, then both the researcher and the attacker are trying to use Q to predict S ! This is why in our measurements of utility, we consider utility of classification on  X  X eutral X  attributes which are neither quasi-identifiers, nor sensitive. Choosing the workload and the sensitive attribute. We must also choose a workload for the legitimate re-searcher. As discussed in Section 5, classification is a good workload because quality of classification depends on the correlations between attributes in the database, and the entire purpose of  X  X ruthfully X  publishing quasi-identifiers and sensitive attributes together is to preserve these cross-attribute correlations.

We will look at classification of both sensitive and neutral attributes. It is important to choose a workload (target) attribute v for which the presence of the quasi-identifier at-tributes Q in the sanitized table actually matters. If v be learned equally well with or without Q , then the data publisher can simply suppress all quasi-identifiers.
Table 2 shows the difference in decision tree learning accu-racy depending on whether or not the quasi-identifier (age, sex, race) is included. Only marital status shows a signif-icant drop when the quasi-identifier is entirely suppressed, thus we choose it as the workload attribute for the  X  X ccu-pation X  dataset. Even though salary is intuitively the sen-sitive attribute in the  X  X dult X  dataset, when the workload w is  X  X earning salary, X  then U ( w ) max  X  X  ( w ) base .Sincewearein-terested in measuring utility of non-trivial sanitization ( i.e. , how much utility it provides over the table in which all quasi-identifiers have been suppressed), we are only interested in sion of Q provides better privacy and same utility as any the sensitive attribute, and use marital status instead. Datasets used. In the  X  X arital X  dataset, Q =(age, occupa-tion, education), S =marital status, the workload attribute is salary. In the  X  X ccupation X  dataset, Q =(age, sex, race), S =occupation, the workload attribute is marital status. Learning the sensitive attribute S . The researcher may wish to build a classifier for the sensitive attribute S using both the quasi-identifiers and the neutral attributes as pre-dictors. Of course, if sanitization has been correctly per-formed, it is impossible to build a good classifier for S only on Q , because good sanitization must destroy any cor-relation between S and Q (otherwise, the adversary will eas-Figure 1: Gain in classification accuracy for the sen-sitive attribute (marital) in the  X  X arital X  dataset. With trivial sanitization, accuracy is 13.31% for the adversary and 30.18% for the researcher. ily learn the sensitive attributes associated with any quasi-identifier). Our results demonstrate that the researcher can build a classifier for S without using the attributes in Q as well as when using sanitized versions of Q .

Figure 1 shows the loss of privacy, measured as the gain in the accuracy of adversarial classification A acc for different sanitizations of the  X  X arital X  dataset, and compares it with w is building a decision tree classifier for the  X  X arital sta-tus X  attribute. As explained in Section 4.4, accuracy of ad-versarial classification underestimates the actual amount of sensitive attribute disclosure. Figure 1 shows that releasing a sanitized table instead of simply suppressing all Q helps the adversary associate sensitive attributes with individuals much more than it helps the researcher to build legitimate classifiers. Figure 2 shows the same result for the  X  X ccupa-tion X  dataset, where the workload w is building a decision tree classifier for the  X  X ccupation X  attribute.
 Learning a non-sensitive workload attribute. Perhaps it is not surprising that sanitization makes it difficult to build an accurate classifier for the sensitive attribute. We now consider the case when the researcher wishes to build a classifier for a non-sensitive attribute v .

If both Q and S are correlated with v , then a classifier based on both Q and S may have higher accuracy than one that considers only Q ,only S , or neither. Our results show that sanitization which removes the correlation between S and Q also destroys the correlation between S and v .
In these experiments, we compute U ( w ) base by running dif-ferent machine learning algorithms on both trivially sani-tized versions of the database; U ( w ) base is the accuracy of the sanitizations and different machine learning algorithms, and compare this to the increase in adversarial accuracy A acc
Figure 3 compares gains in adversary X  X  and researcher X  X  respective accuracies for the  X  X arital X  dataset (workload is learning the  X  X alary X  attribute). Classification accuracies Figure 2: Gain in classification accuracy for the sensitive attribute (occupation) in the  X  X ccupa-tion X  dataset. With trivial sanitization, accuracy is 46.56% for the adversary and 58.30% for the re-searcher. with all quasi-identifiers suppressed were 80.73% for J48, 77.12% for Random Forests, and 79.45% for Naive Bayes. Thus, the baseline for utility was set to 80.73%.

Figure 4 compares gains in adversary X  X  and researcher X  X  respective accuracies for the  X  X ccupation X  dataset (work-load w is learning the  X  X arital status X  attribute). Here we line comes from J48 learning with the sensitive attribute removed. With such a small gap between U ( w ) max and U ( w ) is not surprising that classification accuracies for sanitized datasets are below those of trivial sanitizations, where the sensitive attribute was simply removed.
 Privacy of the sanitized database. Table 3 shows the A acc and A know scores for different sanitizations of the  X  X c-cupation X  dataset (the accuracies are low even for the in-tact database because the quasi-identifiers do not identify a unique individual). Even for large k , k -anonymity barely changes the value of A know compared to the intact database. In other words, k -anonymity provides no privacy im-provement whatsoever on this dataset. Furthermore, -diversity is no better than trivial sanitization because it requires complete suppression of the quasi-identifiers to substantially limit the gain in adversary X  X  knowledge.
Our experimental results in Section 6 indicate that, em-pirically, it is difficult to find a database table on which san-itization permits both privacy and utility. Any incremental utility gained by non-trivial sanitization (as opposed to sim-ply removing quasi-identifiers or sensitive attributes) is more than offset by a decrease in privacy, measured as the adver-sarial sensitive attribute disclosure. It is possible, however, to construct an artificial database, for which sanitization provides both complete utility and complete privacy, even for the strongest definition of privacy (semantic privacy).
Consider table T , in which each tuple t has five attributes Figure 3: Gain in the adversary X  X  ability to learn the sensitive attribute (marital) and the researcher X  X  ability to learn the workload attribute (salary) for the  X  X arital X  dataset. With the trivial sanitization, accuracy is 13.31% for the adversary, and 80.73% for the researcher. a ,a 2 ,a 3 ,a 4 ,a 5 . Their values are defined by three coin flips r ,r 2 ,r 3 , which are generated independently at random for each tuple. The attributes are as follows: a
Now consider the case where Q = { a 1 ,a 2 } , S = a 3 .This database is a candidate for sanitization, since Q provides a lot of information about S (half of the sensitive attribute can be predicted perfectly from the quasi-identifier). If we sani-tize by suppressing a 2 , then we are left with a database which is perfectly private, since a 1 reveals nothing about But this database also has perfect utility, since a researcher can learn a 4 exactly from a 1 and a 3 , and he can learn actly from a 1 and a 3 , and he can learn a 3 exactly from and a 5 .Furthermore,if Q were completely suppressed, the Table 3: A acc and A know scores for different sanitiza-tions of the  X  X ccupation X  dataset. Figure 4: Gain in the adversary X  X  ability to learn the sensitive attribute (occupation) and the researcher X  X  ability to learn the workload attribute (marital) for the  X  X ccupation X  dataset. With the trivial saniti-zation, accuracy is 46.56% for the adversary, and 69.30% for the researcher. researcher could learn nothing about a 4 or a 5 and he could only learn half the information about a 3 ( r 2  X  r 3 ). If omitted, the researcher could learn nothing about a 4 and only half of the information about a 5 .

This artificial dataset is very unusual, and it is unclear whether any real datasets exhibit similar properties. For instance, sensitive attributes S can be split into two parts, one of which is 100% correlated with the quasi-identifiers and the other is completely independent of Q . Sanitization can thus suppress the dependent part of Q entirely, while leaving the independent part intact. Furthermore, a 4 and a are both completely determined by the joint distribution of
S and Q , but independent of either one taken alone. It is unclear how often attributes which are pairwise independent but jointly dependent arise in real data.
Microdata privacy can be understood as prevention of membership disclosure (the adversary should not learn whether a particular individual is included in the database) or sensitive attribute disclosure (the sanitized database should not reveal very much information about any in-dividual X  X  sensitive attributes). It is known that gen-eralization and suppression cannot prevent membership disclosure [12, 26]. For sensitive attribute disclosure, perfect privacy can be achieved X  X gainst a very weak adversary who knows just the quasi-identifiers X  X y simply removing the sensitive attributes or the quasi-identifiers from the published data. Of course, these trivial sanitizations also destroy any utility that depended on the removed attributes.
Algorithms such as k -anonymity and -diversity leave all sensitive attributes intact and apply generalization and sup-pression to the quasi-identifiers. The goal is to keep the data  X  X ruthful X  and thus provide good utility for data-mining applications, while achieving less than perfect privacy. We argue that utility is best measured by the success of data-mining algorithms such as decision tree learning which take advantage of relationships between attributes. Algorithms that need only aggregate statistical information can be executed on perturbed or randomized data, with much stronger privacy guarantees against stronger adversaries than achieved by k -anonymity, -diversity, and so on.
Our experiments, carried out on the same UCI data as was used to validate existing microdata sanitization algorithms, show that the privacy vs. utility tradeoff for these algorithms is very poor. Depending on the sanitization parameter, san-itized datasets either provide no additional utility vs. trivial sanitization, or the adversary X  X  ability to compute the sen-sitive attributes of any individual increases much more than the accuracy of legitimate machine-learning workloads.
An important question for future research is whether there exists any real-world dataset on which quasi-identifier gen-eralization supports meaningfully better data-mining accu-racy than trivial sanitization without severely compromising privacy via sensitive attribute disclosure.

Another important question is how to design microdata sanitization algorithms that provide both privacy and util-ity. Sensitive attribute disclosure results, in part, from the fact that each individual t can only belong to a unique quasi-identifier equivalence class t in the sanitized table T .This is a consequence of the requirement that the generalization hierarchy be totally ordered [10]. This requirement helps the adversary, but does not improve utility. If we consider G ( t ), the set of records in T whose quasi-identifier values are generalizations of t [ Q ], there is no privacy reason why each record of G ( t ) must have the same quasi-identifier val-ues. It is possible that a generalization strategy that uses, e.g. , DAGs instead of totally ordered hierarchies may pro-vide better privacy than the existing algorithms.
