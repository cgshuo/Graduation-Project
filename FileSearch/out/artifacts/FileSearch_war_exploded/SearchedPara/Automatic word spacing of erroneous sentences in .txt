 1. Introduction
With the rapid evolution of the mobile environment, many people access various multimedia content using mobile de-provide users with menu-driven navigation. However, menu-driven navigation has tremendous limitations, as it tends to frustrate users with lengthy and rigid interactions. One of the solutions for overcoming these limitations is to develop new user-interface systems ( e.g ., an information retrieval system and an information extraction system) based on natural language processing techniques. In many languages that have their own word spacing rules for better readability, the first step for realizing this solution may be to implement an automatic word spacing module with high performance. Unfortu-nately, this implementation is not easy because the following constraints must be considered in mobile environments. Uniqueness of mobile languages: The input channels of mobile devices are much smaller than those of personal computers.
These limitations cause users to intentionally replace formal sentences with mobile-style colloquial sentences. For exam-ple, mobile device users often ignore word spaces and punctuation marks in order to avoid the inconvenience of pressing special buttons. In addition, they often input abbreviated or idiomatic words that are intentionally generated to reduce the length of sentences or to express familiarity between friends. This uniqueness of mobile languages makes it difficult to use conventional word spacing models that perform well for ordinary sentences.  X 
Hardwarelimitations: Onthe positiveside, mobile devicesare portable, convenient,and providean anytime-anywhere com-munication capability. However, they have limited input and output capabilities, limited bandwidth, limited memory, and restricted processing power. In particular, many cellular phones have a very small volume of main memory. To make mat-cult to use statistical word spacing models that necessitate complex computations on a large amount of training data.
To overcome these constraints, we propose a robust and compact system to perform automatic word spacing by using character unigram statistics and correction rules in a hybrid manner, as shown in Fig. 1 .

To robustly correct broken word spaces in erroneous sentences, the proposed system uses a modified hidden Markov model (HMM) based on character unigrams that are extracted from colloquial sentences simulated with mobile styles. Such simulated colloquial sentences are used as a training corpus for the following reason: It is impossible to collect a large amount of real mobile-style colloquial sentences because most countries prohibit the unannounced collection of text mes-sages for the protection of communication privacy. The modified HMM is a new model to replace observation probabilities ness against erroneous sentences. To increase the accuracy, the proposed system re-corrects miscorrected word spaces by using lexical rules based on character bigrams or character trigrams. The current version of the proposed system operates in Korean, but we believe that changing the language will not be a difficult task because the system uses shallow natural language processing (NLP) techniques.

This paper is organized as follows. In Section 2 , we review the previous work on automatic word spacing systems. In Sec-we draw conclusions in Section 5 . 2. Previous works The previous approaches on automatic word spacing can be classified into two groups: rule-based models ( Choi, 1997; sentence from left to right or from right to left by using morphological information, such as functional word lists and language-dependent word patterns. Rule-based models then insert spaces between words by using heuristic rules, such as longest-match-preference rules, shortest-match-preference rules, and error patterns. Although rule-based models are simple and clear, they have some drawbacks. First, they need handcrafted linguistic knowledge, which is considerably costly to construct and maintain. Second, they cannot effectively handle unknown word patterns because they use lexical levels of predefined patterns.
 tain the necessary information, they do not require the construction and maintenance of linguistic knowledge. In addition, they are generally more robust against unknown word patterns than rule-based models. However, in mobile devices with a small main memory, it is impractical to use the previous statistical models based on character bigrams or longer combina-acter-based languages ( e.g. Korean, Chinese, and Japanese) require a larger memory than those in alphabet-based languages 24 Korean alphabets, with leading 14 consonants, intermediate 10 vowels, and optionally, trailing seven consonants. The number of characters that are often used in modern Korean is about 10 reaches 10 8 . Assuming that the frequency of each character bigram is represented by 2 bytes, about 200 MB (megabytes) of memory is required ( Park, Lee, &amp; Tae, 2005 ).

To resolve these problems with rule-based models and statistical models, we propose a hybrid model that sequentially combines a statistical model with a rule-based model. The proposed model adopts a modified HMM using character uni-grams as input features for improving the robustness against unknown word patterns. Then, it adopts a rule-based model using character bigrams or longer combinations as rule templates to reduce the human efforts and increase the accuracy for well-known word patterns. 3. A hybrid model for automatic word spacing 3.1. The first stage: preliminary correction based on a modified HMM
Let S 1, n denote a sentence that consists of a sequence of n characters c space between them. The label  X  X 1 X  X  means that there should be a space;  X  X 0 X  X  means that there should not be a space. For finding L 1, n when the sentence S 1, n without spacing is given, which results in:
In Eq. (1) , P ( S 1, n ) is dropped since it is constant for all of the L the statistics for each one, as shown in:
Now, Eq. (2) is simplified by making two Markov assumptions: that the current word spacing label is affected only by the niak, 1993; Ephraim &amp; Merhav, 2002 ).

Markov assumption makes it difficult to use a character level of contextual information to obtain the more reliable obser-vation probability. Many previous models have used character bigrams or longer combinations to resolve this problem. How-ever, these models are not suitable for mobile devices with small main memories because they require a large memory for Bayesian classification ( Zheng, 1998 ), as shown in:
In Eq. (4) , m is the number of contextual features, and f pendent of each other. By using these unigram features instead of character bigrams or longer combinations, the proposed system reduces the memory usage for loading the statistical information.

Then, to remove the floating-point calculations, the proposed system computes the conditional probabilities of the mod-ified HMM by using a logarithmic function with five significant figures, as shown in: per, we call the modified HMM a na X ve Bayesian X  X idden Markov model (NB X  X MM). Conditional random fields (CRFs) and maximumentropyMarkovmodels(MEMMs)aregoodframeworksthatusecontextual featuresforbuildingprobabilisticmod-that the proposed model will be useful for mobile devices with restricted processing power because CRFs and MEMMs gener-ally require more complex computations than an HMM for parameter estimations and probability calculations. 3.2. The second stage: re-correction based on a rule-based model
By using correction rules based on character bigrams or longer combinations, the proposed system corrects any word spaces that were miscorrected by the NB X  X MM. Fig. 2 shows how to automatically construct the correction rules.
As shown in Fig. 2 , the correction rules are generated from five rule templates: three bigram rule templates ( X  X  c 0 X  X ,  X  X  c i : c i +1 ? 1/0 X  X , and  X  X  c i +1 : c i +2 ? 1/0 X  X ) and two trigram rule templates ( X  X  c ( Kang &amp; Woo, 2001 ) showed that the combinations of three bigrams ( c ing problem. Based on this report, we designed the three bigram rule templates. In these rule templates, c indicates whether or not a space should be inserted at the position between the i th character and the i + 1th character.
For example, if the NB X  X MM corrected the input sentence  X  X  nael9sieboja (see you tomorrow at 9) X  X  into  X  X  nael 9siebo ja  X  X  symbol for the beginning of a sentence. After generating all of the correction rules by comparing the preliminary corrected spacing labels that are incorrectly modified by r . MaxW is the maximum rule score, and is used as a normalizing factor to culated by dividing the frequency of r by the total frequency of the rules. The information quantity log c : c selects n correction rules that are highly ranked in descending order of the confidence scores.

When a preliminary corrected sentence is input, the proposed system chooses correction rules (among the top n correc-shown in:
In Eq. (7) , l k is the k th word spacing label at the target positions. tri by some calibration). If bi k is a correction rule that is generated by the second rule template  X  X  c
Otherwise ( i.e ., if bi k is a correction rule that is generated by the first rule template  X  X  c  X  X  c i +1 : c i +2 ? 1/0 X  X ), we set a k to 0.25. Finally, the proposed system updates l if ClassScore ( l k ) is greater than zero, the proposed system assigns a value of  X  X 1 X  X  to l fidence rule model (CRM). 4. Evaluation 4.1. Data sets To experiment with the proposed system from the various viewpoints, we use six kinds of evaluation data; SEJONG,
CLMET, ETRI-POS, HANTEC-HKIB94, MOBILE-SEJONG, and REAL-COLL. In order to independently evaluate various perfor-mances ( e.g. memory usage, average accuracy, accuracy variation, and so on), we use SEJONG, CLMET, MOBILE-SEJONG, and REAL-COLL. In order to compare the accuracy of the proposed system with those of the previous systems, we use ETRI-POS and HANTEC-HKIB94 which were the evaluation data of the previous systems. SEJONG is the 21st Century Sejong Project X  X  colloquial corpus (6300,328 words; http://www.sejong.or.kr/eindex.php ). CLMET is a corpus of Late Modern British
English (9818,326 words; https://perswww.kuleuven.be/~u0044428 ). We use CLMET in order to evaluate the performance variations in languages with quite different structures. ETRI-POS is the part-of-speech (POS) tagged corpus (288,269 words)
HANTEC-HKIB94 is a set of news articles (3224,198 words) distributed by Korea Institute of Science and Technology Infor-(51,269 words) that we downloaded from the mobile phones of two undergraduate students and six graduate students.
We know that REAL-COLL is the best evaluation data. However, it is time and cost consuming to collect a large amount of such real mobile-style colloquial sentences because we should individually download and refine them (it is impossible to automatically collect them because of the protection of communication privacy). As the second best choice, we automatically corpus MOBILE-SEJONG.

To construct MOBILE-SEJONG, we used a conventional morphological analyzer, a correction dictionary, and conversion rules. The correction dictionary was manually constructed by six graduate students majoring in NLP. As shown in Table 2 , the correction dictionary included 1775 entries that consisted of mobile-style strings and their original forms.
In Korean, functional words such as Josa and Eomi follow content words, and the surface form of a functional word is changed based on the last consonant form of the preceding content word. The conversion rules are simple lexical patterns for these transformations of Korean functional words. Such conversion rules may be unnecessary in many languages because way of an example.

In the first step, the MOBILE-SEJONG construction system performs a morphological analysis of a normal colloquial sen-tence. Then, the system checks whether or not functional words exist in each Eojeol (a sequence of one or more morphemes, separated by spaces). If functional words exist, the system removes the functional words with 0.5 probabilities by matching the last consonants of the preceding content words against the conversion rules. We set the removal probability to 0.5 be-cause we assume that mobile users intentionally omit functional words to reduce the length of sentences in a random man-ner, regardless of age or knowledge. Although the removal probability was set without theoretical backgrounds, the proposed system showed better performances in the removal probability of 0.5 than in the removal probability of 0.3 or places the content words in mobile-style strings with 0.77 probabilities by matching to the longest entry in the correction erates a mobile-style colloquial sentence by simply combining each updated Eojel . 4.2. Experimental settings
To experiment with the proposed system, we divided some evaluation data into training and test data at a ratio of nine to one. Then, we removed any word boundary information in the test data in order to evaluate the automatic word spacing. Finally, we performed a 10-fold cross validation by using three kinds of evaluation measures, as shown in:
In Eq. (8) , A char is the character X  X nit accuracy which is used as the main evaluation measure. P precision and the space X  X nit recall rate which are used as supplementary evaluation measures, respectively. In order to evaluate the usefulness of the proposed system in a real mobile phone environment, the system was evaluated in a commer-cial mobile phone with a XSCALE PXA270 CPU, 51.26 MB memory, and Windows Mobile 5.0, using the C language. 5. Experimental results
The first experiment performed was to evaluate the amount of main memory used. We computed the average perfor-mances of the proposed system at various confidence score cutoff points, as shown in Table 3 .

In Table 3 , 100,000, 200,000, and 300,000 mean the number of correction rules that were selected by the proposed sys-tem. To simplify the explanation, we will call each integrated model Model-100K, Model-200K and Model-300K, respec-tively. As shown in Table 3 , Model-300K showed the best accuracy and the best F
However, we think that Model-100K and Model-200K are more suitable for mobile devices because Model-300K requires a large volume of working memory that may go beyond the general hardware capacities of mobile devices.
In the second experiment, we compared the performances of NB X  X MM and CRM with those of representative machine learning models by using the same input features, as shown in Table 4 .

In Table 4 , Model-TBL means a rule-based model for word spacing that we implemented using transformation-based learning (TBL). TBL is a symbolic error-driven machine learning model (one of well-known rule-based models in NLP) ( Brill, 1995 ). Model-CRFs means a statistical model for word spacing that we implemented using CRFs. CRFs is an undirected prob-each model in Table 4 use two times longer n -grams ( i.e. character bigrams for statistical models; character 4-grams and 6-grams for rule-based models) than Korean versions because a Korean character is represented by 2 bytes which are equal to the size of memory necessary for saving two English characters. As shown in Table 4 , the performances of NB X  X MM and
CRM were not seriously affected by language changes. The average accuracies of NB X  X MM and CRM were generally a little lower than those of Model-CRFs and Model-TBL, respectively. In addition, CRM used a larger working memory than Model-
TBL. However, in their response time, NB X  X MM and CRM greatly outperformed Model-CRFs and Model-TBL. Based on these experimental results, if computational cost is an important factor, we think that the integration of NB X  X MM and CRM may be one of the best solutions because Model-CRFs is computationally very expensive and Model-TBL sometimes requires many iterative processes to apply rewrite rules. In this experiment, we did not compare NB X  X MM with MEMMs because CRFs generally has outperformed MEMMs in many NLP applications ( Lafferty et al., 2001; Sha &amp; Pereira, 2003 ).
In the third experiment, we compared the average accuracies of Model-100K and Model-200K with those of the previous in Table 5 .

Kang-2001 ( Kang &amp; Woo, 2001 ) is a traditional n -gram model that uses the statistics of character bigrams as context information. Lee-2007-1 ( Lee et al., 2007 ) is the 1st order HMM that uses character unigrams as observation information.
Lee-2002-2 ( Lee et al., 2002 ) is the 2nd order HMM that looks at two previous labels and two previous characters. Park-2006 ( Park et al., 2006 ) is a self-organizing HMM that automatically adapts the context size from a unigram to a bigram.
As shown in Table 5 , Model-100K and Model-200K greatly outperformed the unigram-based model, Lee-2007-1. In addition, they showed better accuracies than all the bigram-based models ( i.e ., Kang-2001, Lee-2002-2, and Park-2006). Although the differences compared to the accuracies of the bigram-based models were not large, we think that Model-100K and Model-200K would be more useful for mobile devices because the bigram-based models require, at the least, larger memory allo-cations for saving bigram statistics.
 In the last experiment, we evaluated the effectiveness of using the mobile-style colloquial corpus, as shown in Table 6 .
In Table 6 , Model-CRFs&amp;TBL is a competing model in which Model-CRFs is integrated with Model-TBL. As shown in Table standard variation of 6.06). The proposed system showed an average accuracy of 93.52% (near to the average accuracy when the proposed system used SEJONG as training and test data) when it used REAL-COLL as training and test data. This fact re-veals that the proposed system will show good performances in real mobile-style sentences if the amount of training data is enough. The proposed system trained using MOBILE-SEJONG showed 0.03 X 1.43% higher accuracies regardless of the test data than that trained using SEJONG. Bases on this experimental result, we think that the simulated mobile-style corpus might be a helpful resource for training an automatic word spacing system suitable for a mobile environment, even though it is not a real mobile-style colloquial corpus. 5.1. Failure analysis
We categorized the cases where the proposed system incorrectly inserted word spaces and found that the main reasons why it returned incorrect results were as follows:
There were many unknown words in the test data, and low-frequency characters raised many errors. We think that these sparse data problems are very critical because newly-coined words are continuously being created, especially in mobile environments. To alleviate these sparse data problems at a basic level, we should increase the amount of training data or apply good smoothing methods to the proposed system.

There were many cases where even a person could not determine whether or not a space needed to be inserted by just looking at character 5-g as contextual information. To resolve this problem, we should study methods to select and use long distance characters ( i.e . characters that are over five characters away from a target position) as contextual information.

There were many differences between a real mobile-style corpus and a simulated mobile-style corpus. In the real mobile-style corpus, we found many sentences in which entire word phrases, as well as some functional words, were intention-ally omitted in order to shorten the length. These omissions contributed to the generation of many unknown word sequences. As a result, sparse data problems were increased. To resolve this problem, we should improve the method for automatically constructing a mobile-style colloquial corpus. 6. Conclusion
We proposed an automatic word spacing system for a mobile device. In addition, we constructed a simulated mobile-style colloquial corpus in order to experiment with the proposed system in a realistic mobile environment. The proposed system spaces words out using character unigram statistics and correction rules in a hybrid manner. To improve the robustness against unknown word patterns while reducing the memory usage, the proposed system first corrects word spacing errors by using a modified hidden Markov model based on character unigrams. Then, to increase the accuracy, it re-corrects any miscorrected word spaces by using lexical rules based on character bigrams or longer combinations. In the experiments with the simulated corpus, the proposed system showed good performances for the various evaluation measures, such as re-sponse time, memory usage, and accuracy. On the basis of these experiments, we think that the proposed system is suitable for mobile devices with limited computing resources.
 Acknowledgments
This research was supported by Basic Science Research Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education, Science and Technology (No. 2010-0009875).
 References
