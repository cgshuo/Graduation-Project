 Wikipedia has emerged as an important source of structured in-formation on the Web. But while the success of Wikipedia can be attributed in part to the simplicity of adding and modifying content, this has also created challenges when it comes to using, querying, and integrating the information. Even though authors are encour-aged to select appropriate categories and provide infoboxes that follow pre-defined templates, many do not follow the guidelines or follow them loosely. This leads to undesirable effects, such as template duplication, heterogeneity, and schema drift. As a step towards addressing this problem, we propose a new unsupervised approach for clustering Wikipedia infoboxes. Instead of relying on manually assigned categories and template labels, we use the structured information available in infoboxes to group them and infer their entity types . Experiments using over 48,000 infoboxes indicate that our clustering approach is effective and produces high-quality clusters.
 [ H ]: .2.5 [Heterogeneous Databases] Algorithms Experimentation Wikipedia infobox, clustering
Wikipedia infoboxes have emerged as an important source of structured information on the Web. An infobox is associated with an entity type and consists of a set of attribute-value pairs which summarize important information about a specific entity. The avail-ability of infoboxes has opened up new opportunities for posing complex queries over a large collection of data created through a mass collaboration effort. For example, one can search for titles and years of movies directed by James Cameron that grossed over 100 million dollars, whose stars were born in England .

To support these queries, systems have been proposed that model each Wikipedia document as an entity with a type, a set of associ-ated attributes (or schema), and relationships connecting it to other entities [3, 6, 7]. For example, the document Avatar 1 ,whichsat-isfies the query above, is associated with the entity type Film and contains attributes such as title , year ,and gross revenue . The entity Avatar is also connected to the director James Cameron through a directed by relationship and to the actor Sam Worthington ,whowas born in England , through a starring relationship.

The same mass collaboration approach that makes these queries possible also creates significant challenges in correctly formulat-ing and answering them. Although authors are encouraged to use infobox templates and select appropriate categories, these guide-lines are not always followed. This leads to inconsistencies like the duplication of templates and categories, schema drift, and data heterogeneity [12]. As a result, to formulate (or answer) a query, knowledge about how to deal with these inconsistencies is neces-sary. Approaches have been proposed to deal with this problem [3, 10, 12]. In DBpedia, the 350 most commonly used infobox tem-plates were manually mapped to an ontology consisting of 170 classes [3]. YAGO uses language-dependent heuristics to extract concept names from Wikipedia categories and maps them to Word-Net [10] 2 . KOG also uses WordNet and unifies entities that have the same canonical template names [12]. However, due to the het-erogeneity in categories and in template names, these approaches are error prone and require substantial human intervention.
While an infobox can be associated with a template name that corresponds to a pre-defined entity type , template names are not al-ways a reliable source for determining entity types. Since there is no central authority, several names can be used for the same entity type. For example, the entity type Film is associated with template names Infobox Film , Infobox Movie , Television infobox Korean film , etc . Editors also use templates with generic names e.g., wikitable , infobox ,orvery specific ones e.g., SinCityCharacter instead of Character . Furthermore, while some templates have descriptive names, others contain only abbreviations ( e.g., VG , MFL ).

Because Wikipedia category names are folksonomic, i.e., they are created by a group of people without the control of a central authority, they are also an unreliable source for inferring the con-ceptual entity type. Consider, for example, Figure 1 which shows an infobox for a movie and its associated categories. Given these categories, a system like Yago would assign to the infobox concepts like film , winner , olympics , culture , university ,and sport .However, only film corresponds to the entity described in the infobox.
To get a better sense for the heterogeneity in Wikipedia infoboxes, consider the template and category names for movie-related infor-mation in the Portuguese Wikipedia. There are 12 distinct tem-plate names for actor ; the template Info ator covers only 52% of actor entities, while the category atores covers 85% of the ac-tors. Therefore, relying on either category or template name to select actors will result in a significant number of infoboxes being missed. Given the dynamic nature of Wikipedia, with new enti-ties, categories and templates being created constantly, a system-atic approach is needed to discover the correspondences among the template names and categories. While manual curation can be ef-fective, it is not only costly, but as this example shows, it may lead to limited coverage.
 Our Approach and Contributions. We posit that the infobox structure, i.e., its attributes and values, provides a reliable means to identify the entity type, and frame the problem of entity type discovery as one of clustering infobox schemata. However, de-riving high-quality clusters is challenging. First, the number of clusters that need to be generated is not known a priori. Second, the undisciplined process of infobox creation leads to a wide vari-ation in the schemata. Besides the use of different attribute names, even within a single entity type, there are many optional as well as rare attributes, which occur in few instances of a given type [12]. Wikipedia infoboxes also present a long-tail distribution for types: a large number of types have few associated instances, and few types are associated with many instances. This makes it difficult to cluster the corresponding entities appropriately, in particular, for low-frequency templates. Last, but not least, ambiguous attributes, i.e., attributes that belong to multiple entity types, can mislead the clustering algorithm.

To address these problems, we have designed a new cluster-ing strategy: WIClust (Wikipedia Infobox Clustering). WIClust is an unsupervised method that complements existing human-assisted approaches to help organize and enrich Wikipedia content. It re-ceives as input a set of infoboxes and outputs a set of entity types. It uses the correlation among infobox attributes to identify impor-tant attribute sets which, in turn, are used as the basis to cluster (structurally) similar infobox schemata. As we describe below, identifying these important sets as a first step allows WIClust to derive types accurately, handle ambiguous attributes and incremen-tally add rare attributes, increasing the coverage of the schemata without significant loss in accuracy.

WIClust does not require the number of types (clusters) to be known in advance, it is resilient to the high skew present in the data, and it is robust in the presence of rare and optional attributes. Because the clustering process requires no manual intervention, it gracefully supports the dynamic nature of Wikipedia, and because it relies only on the structure of the infoboxes, it can be applied to infoboxes in different languages.

We report the results of a preliminary experimental evaluation using 48,000 infoboxes spanning 862 infobox templates which show that our clustering algorithm discovers meaningful entity types and derives high-quality clusters, outperforming other clustering tech-niques in term of F-measure.
Given a set of infoboxes as input, our goal is to identify a set of entity types and corresponding schemata that represent these in-foboxes and the relationships between the types.
 Terminology and Definitions An infobox ib = { &lt;a i ,v i 1 ..m } consists of a set of attribute-value pairs that describe an en-tity. The schema of an infobox ib is the set of its attributes and we denote it by S ib = { a i | i =1 ..m } . An infobox is associated with an entity type . For example, the schema of the infobox in Figure 1(a) corresponds to an entity of type Movie and includes attributes like Directed by , Box office , etc. The schema S T of an entity type T is the union of all attributes in infoboxes (instances) associated with T .

In the entity discovery process, we create attribute and entity clusters. An attribute cluster , corresponding to an entity type T and denoted by A T , consists of a set of important attributes of entity type T discovered by our algorithm. A entity cluster , denoted by E T , is a set of entities (infoboxes) that belong to entity type T . Solution Overview. Intuitively, infoboxes of the same type have similar structure, while infoboxes from different types have dif-ferent sets of attributes. Based on this intuition, we propose to discover the types for Wikipedia infoboxes by grouping together infoboxes with similar schemata.

A possible solution to cluster infoboxes would be to compute the similarity between all pairs of infoboxes. However, this is problem-atic since the distribution of infoboxes and of infobox attributes is highly skewed: few infobox types and attributes are very frequent, while most of them have low frequency. In addition, not only there is a large number of optional and ambiguous attributes, but also there is a high variability in how infobox attributes are represented. Consequently, pairwise comparisons can be misleading and result in heterogeneous clusters.

WIClust employs a two-pronged approach to address these chal-lenges: (i) it takes advantage of the large number of available in-foboxes and uses attribute correlation as a source of similarity (and dissimilarity ) information; and (ii) before clustering the infoboxes, it discovers sets of attributes that are highly correlated and yet dis-tinct from each other and thus, are good candidates for describing an entity. Hence, WIClust does not require the number of desired clusters to be known a priori: this is obtained naturally from the set of correlated attributes.

As illustrated in Figure 2, our approach consists of two main steps. Given a set of infoboxes, the Attribute Clustering step lever-ages the correlation between attributes to return a set of attribute clusters (section 2.1). Each attribute cluster comprises important attributes of an entity type T , i.e., attributes that are common to most entities of type T . For example, the attribute cluster discov-ered for the entity type Movie includes the attributes { Directed , Produced by , Written by , Starring }. In the Grouping step (section 2.2), each infobox ib is then assigned to an entity type T , whose attribute set is the most similar to its schema.
The goal of this step is to find, for each entity type, a group of representative attributes, which are used as the basis for the In-fobox Grouping step. Based on the observation that the important attributes for an entity type T co-occur often in the schema of in-foboxes of type T , our algorithm uses correlation to group these attributes together. Given two attributes a p and a q , we consider the negative correlation ( NC ) and positive correlation ( PC ) between them [9]:
NC ( a p ,a q )= where C p , C q ,and C pq correspond to the number of infoboxes that contain attribute a p , a q , and both of them, respectively. These cor-relations indicate the strength and direction of the relationship be-tween two attributes ( e.g., negative X  X ush away, or positive X  X ull together). Intuitively, NC is high when a p and a q rarely co-occur in the same schema ( C pq is small compared to C p and C q is zero when a p and a q co-occur in the schema of an infobox ib In contrast, PC is high when a p and a q often co-occur in the same schema ( C pq is close to C p and C q ). Attributes that have a high positive correlation are likely to belong to the same entity type. For example, PC( Directed by, Starring ) is high because these attributes usually co-occur in infoboxes of the type Movie . In con-trast, NC( Directed by, Instrument ) is high since these two attributes rarely co-occur in the same schema: Occupation pears in the schema of Artist while Directed by appears in the schemata of Movie .
 Clustering Constraints. Once NC and PC are computed for all attribute pairs, the decision on which attributes should be grouped together is based on the intuition that pairs of attributes with high NC scores should be assigned to different clusters, and all pairs of attributes within each cluster should have high PC scores. We introduce three constraints ( C 1 ,C 2 ,C 3 ), shown in Table 1, to cap-ture this intuition. Given a set of n attribute clusters  X  1 ..n } , constraint C 1 checks whether an attribute a has NC score greater than a threshold T s when compared against every attribute a ij in each cluster A i . If so, attribute a should not be grouped with any existing cluster. Instead, it should be separated into a new cluster. Constraint C 2 checks whether the PC scores between an attribute a and every attribute a ij of each cluster A than a threshold T g . If that is the case, a isaddedtocluster A Finally, constraint C 3 determines if an attribute a can be merged # Action Constraints
Table 1: Constraints used in the Attribute Clustering step into a cluster A k  X   X  by checking if the grouping constraint C between a and A k and the separating constraint C 2 between a and each remaining cluster A i  X   X  ,i = k are both satisfied.
Although these constraints reflect our intuition, they have a lim-itation. Consider for example the merging constraint ( C 3 given attribute, the grouping constraints must be satisfied by every attribute in a cluster and the separation constraints must be satis-fied by all the clusters. As a result, attributes that co-occur with many attributes in a cluster, but not with all of them, could be as-signed to a new cluster; and infrequent attributes or attributes of infrequent entity types, which have low NC scores with all other attributes, could be missing from the cluster. The following ex-ample illustrates this problem. Let { born , occupation , domestic partner , website } be the candidate attributes for the type Actor . Applying strict constraints C 1 and C 2 over these at-tributes will result in the cluster { born , occupation , because they co-occur often. Even though domestic partner often co-occurs with born and occupation , it rarely co-occurs with spouse , leading to a low positive correlation between them and thus, domestic partner will not be present in the resulting cluster. To address this problem, we relax the constraints by requir-ing only a subset of the attributes within a cluster to satisfy the cor-relation conditions for a given attribute. The relaxed constraints, C 1 , C 2 ,and C 3 , are given in Table 1. We define relaxation degrees D s and D g for these constraints to determine the lower bound for the percentage of attributes within one cluster that must satisfy the separating and grouping conditions, respectively. By ap-plying the relaxed constraints in the previous example, the derived cluster would contain { born , occupation , spouse , domestic partner }.

Another limitation of these constraints is that they do not allow multiple clusters to share common attributes: according to the sep-aration constraint, a newly-formed cluster must have high negative correlation scores with all other clusters. This condition is not ef-fective because an attribute may appear in multiple schemata asso-ciated with different types. We call such an attribute ambiguous . For example, given the schema of Movie ({ starring , directed , produced by , written by , website , language , country runtime }), and Show ({ starring , number of episodes , run-time , country , language , website }), the attributes website starring , running time , language ,and country are ambigu-ous. A conventional hard clustering algorithm would either sepa-rate the ambiguous attributes across different clusters or coalesce the clusters that share the ambiguous attributes, i.e., the ambigu-ous attributes would either be split across the two clusters or all attributes of Movie and Show will be coalesced into a single cluster if we relax the constraints. Algorithm 1 Attribute Clustering 1: Input: Set of unambiguous attributes U = { a i } 2: Output: Set of attribute clusters  X  = { A i | i =1 , .., n } 4: while P =  X  6: if  X  = { X  } 11: else if ME ( a p ,  X  ,k ) 13: end while Handling Ambiguous Attributes. To avoid this problem, we iden-tify and isolate potentially ambiguous attributes before deriving the attribute clusters, and restore them after the entity clusters are de-rived. To identify ambiguous attributes, we model the set of at-tributes as a graph G A = ( V A ,E A ) , where vertices are attributes, and there is an edge between two vertices if they co-occur often. Since an ambiguous attribute belongs to the infobox schemata of multiple entity types, its vertex will have higher centrality than the others. We use betweenness [4], a measure of vertex centrality, to distinguish attributes with respect to their ambiguity. The between-ness B ( v ) for vertex v is defined as: where  X  st is the number of shortest paths from s to t ,  X  number of shortest paths from s to t that pass through a vertex v . The intuition here is that vertices that occur on many shortest paths between other vertices have higher betweenness than those that do not. Thus, ambiguous attributes which are shared by different en-tity types will have higher betweenness. We then isolate these am-biguous attributes by choosing the top K attributes with the highest betweenness. Although there are other centrality measures, such as e.g., in-degree , unlike betweenness, these cannot simultaneously distinguish ambiguous attributes within large and small schemata. The Clustering Algorithm. Algorithm 1 uses the relaxed con-straints to generate the attribute clusters. Given the set of unam-biguous attribute pairs P present in the infoboxes of all entity types, we iteratively choose from P the attribute pair with the highest neg-ative correlation, integrate the pair into an existing cluster in  X  (line 12) or create a new cluster (line 10). The algorithm can be tuned using four parameters: T s , T g , D s ,and D g . The intuition behind them is simple. T s and T g are the correlation thresholds for separa-tion and grouping, respectively X  X igher values lead to higher pre-cision at the cost of lower coverage. D s and D g are the degrees of separation and grouping used in the relaxed constraints to deal with optional attributes X  X igh values for these mean more relaxation. The Attribute Clustering step produces a set of attribute clusters  X  ={ A i | i =1 ..n }, where A i corresponds to the schema for an en-tity type T i . 3 These are given as input to the Infobox Grouping step which produces a set of entity clusters  X  ={ E i | i =1 ..n }, where each E i is the set of infoboxes of entity type T i . We group in-foboxes of the same entity type together by assigning each infobox ib to the attribute cluster that is most similar to the schema of ib .
Different approaches are possible for assigning labels for types, e.g., labels can be manually selected or set to the most common terms in the category names or template names in the cluster.
Let S ib be the schema of infobox ib and S T be the schema of en-tity type T , which initially contains the attributes of A approach would be to choose T such that S ib is the most similar to S . However, the initial similarity between S ib and A T may be low because a given attribute cluster may not contain the complete set of attributes, notably, it may be missing both optional and ambiguous attributes. To repair potential omissions in the attribute clustering step and improve the similarity between the infobox and the cluster it may belong, we apply a bootstrapping approach which gradually updates the set of attribute clusters with attributes from the schema of their instances. In particular, we assign ib to type T whose at-tribute cluster A T has the highest similarity to S ib and that is also above a threshold  X  . These infoboxes form the initial set of entity clusters and we use them to build a probabilistic attribute model for each entity type T , where each attribute a i  X  S T is assigned a weight w i using TFIDF [2], to give higher weights to important attributes. We then compute the similarity between an unassigned infobox schema S ib and every attribute set S T using Equation 4 and then assign to ib the type T which has the highest similarity. Data set. We extracted 48,000 infoboxes (HTML tables) from Wikipedia pages that are related to movies. To create our gold data, we randomly selected 400 infoboxes and manually labeled the selected infoboxes with a class name. The choice of class was guided by our goal of obtaining an intuitive query interface. For the Movie domain we considered classes such as Movie, Show, Album, Artist, Actor, Writer, Country, State, City, etc. We should note, however, that this choice is subjective: there are different ways of classifying the infoboxes. For example, one can use fine-grained classes ( e.g., Eurovision Song Contest Entry, in addition to Song) or coarse-grained ones ( e.g., Person instead of Artist and Actor).
Table 2 lists some statistics for our infobox collection which pro-vide an idea of the heterogeneity in the data.
 Evaluation Metrics. To evaluate the entity clusters, we use F-measure. Given an entity type T ,let E T be the entity cluster dis-covered by our algorithm and E T be the gold entity cluster of T . The precision and recall for type T are defined as follows: We compute the weighted average of precision and recall according to the number of elements for each entity cluster. F-measure is the harmonic mean of precision and recall.
 Comparison against different clustering methods. We compare the results of our clustering algorithm against a series of state-of-the-art clustering methods: K-means, K-means++ [1], Normal-ized Cut (NCut) [8], and Markov Clustering (MCL) [11]. We also present a comparison against: MDHAC, a new categorical clus-tering algorithm which has been shown to outperform many cate-gorical clustering algorithms like ROCK, COOLCAT, CATUS that have been used to cluster Web-form schemata [5]. We use Jac-card as the similarity measure for all methods, except for MDHAC, Figure 3: Comparing with different clustering strategies in the Movie domain which uses the chi-squared test. We use the number of clusters in the gold dataset as the target number of clusters for these clustering methods. Note that WIClust does not require the number of clusters and it still outperforms the other clustering techniques .
As the results in Figure 3 show, for the Movie domain, our ap-proach obtains both high precision and recall: the F-measure values for WIClust are the highest  X  X he marginal gains vary from 4% to 20%.

K-means has the lowest performance likely because it heavily depends on the choice of initial seeds. NCut maximizes the group association inside each cluster and minimizes the disassociation be-tween different clusters, which leads to high precision but low re-call because it tends to split big clusters into smaller clusters with similar sizes. K-means++ outperforms K-means and NCut. This can be attributed to the fact that it chooses the furthest points as ini-tial seeds. However, because there is a large variation in the sizes of the clusters, some of the initial points fall into the same cluster. This results in fragmentation and a lower recall.

MCL simulates stochastic flows by performing random walks on a similarity graph: the random walks strengthen the flow where the similarity connections are dense and make the underlying clus-ter structure become visible through iterations. 5 But it tends to merge small clusters together, resulting in high recall but at the cost of lower precision. MDHAC assumes that homogeneous sources share the same generative attribute model. It uses the  X  -square hy-pothesis test to maximize the statistical heterogeneity among clus-ters through the clustering process. However, in this scenario, it tends to merge together unrelated schemata which contain ambigu-ous attributes. These mistakes are propagated through the iterative clustering process, leading to a low precision.
Similar to WIClust, YAGO [10] and DBpedia [3] also extract entity types in Wikipedia, but they do so in different ways. Auer and Lehmann [3] apply manually constructed mappings to assign template names and attributes to a manually built ontology. Not only this requires considerable human intervention, but as we have shown, it also misses many template names from the long tail of infrequent template names. YAGO [10], on the other hand, extracts concept names from the Wikipedia categories and maps them to WordNet. As discussed in Section 1, it can be difficult to accu-rately infer entity types since category names are heterogeneous, folksonomy oriented, and sometimes inconsistent due to incorrect assignments by editors.

Wu et al. [12] attempt to normalize infobox names and then group together infoboxes with the same canonical names. How-ever, using only infobox names is insufficient because they are usually terse, ambiguous, and contain acronyms. To identify class
In the experiment, we set T s =5 and T g =0.4.
We varied the inflation parameter in MCL to tune the coarseness and quality of the clustering. subsumption, they use YAGO data for training and employ differ-ent features, including class-name inclusion, bag-of-word attribute similarity score, and external information from the article edit his-tory. In contrast, WIClust is unsupervised and relies solely on the structured information from the infoboxes to infer the entity types. As a result, it supports the dynamic nature of Wikipedia and it is also effective at identifying infrequent types.
With the goal of organizing Wikipedia infoboxes, we have pro-posed a new clustering algorithm which is resilient to the skew in the entity distribution and robust in the presence of optional and ambiguous infobox attributes. We have also presented an experi-mental evaluation which shows that the derived clusters are accu-rate. Not only is our approach completely automatic, but because it relies only on the structure of the infoboxes it is also language independent.

While our preliminary experiments are promising, there are a number of directions we intend to pursue in future work. Besides experimenting with other domains, it would also be interesting to compare how our derived clusters with the types assigned by DB-Pedia. Manual inspection of the WIClust output has shown that it can derive clusters whose templates are not present in DBPe-dia as well as detect potentially incorrect mappings. For exam-ple, WIClust discovered that the templates Infobox Movie , film , Japanese film , Chinese film ,and Korean film be-long to the same group as Infobox Film . But some of these tem-plates are not covered by current mapping rules in DBpedia [3], e.g., infoboxes with template name Bond film are not recognized as Film in DBpedia. Thus, another aspect we would like to explore is the use of our approach to improve the coverage and quality of DBPedia. [1] D. Arthur and S. Vassilvitskii. k-means++: The advantages [2] R. A. Baeza-Yates and B. A. Ribeiro-Neto. Modern [3] C. Bizer, J. Lehmann, G. Kobilarov, S. Auer, C. Becker, [4] U. Brandes. A faster algorithm for betweenness centrality. [5] B. He, T. Tao, and K. C.-C. Chang. Organizing structured [6] G. Kasneci, M. Ramanath, F. Suchanek, and G. Weikum. The [7] H. Nguyen, T. Nguyen, H. Nguyen, and J. Freire. Querying [8] J. Shi and J. Malik. Normalized cuts and image [9] W. Su, J. Wang, and F. Lochovsky. Holistic query interface [10] F. M. Suchanek, G. Kasneci, and G. Weikum. Yago: A Core [11] S. M. van Dongen. Graph Clustering by Flow Simulation . [12] F. Wu and D. S. Weld. Automatically refining the wikipedia
