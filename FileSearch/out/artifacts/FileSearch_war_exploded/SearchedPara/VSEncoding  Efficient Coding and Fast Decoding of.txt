 Encoding lists of integers efficiently is important for many applications in different fields. Adjacency lists of large graphs are usually encoded to save space and to improve decoding speed. Inverted indexes of Information Retrieval systems keep the lists of postings compressed in order to exploit the memory hierarchy. Secondary indexes of DBMSs are stored similarly to inverted indexes in IR systems. In this paper we propose Vector of Splits Encoding ( VSEncoding ), a novel class of encoders that work by optimally partitioning a list of integers into blocks which are efficiently compressed by using simple encoders. In previous works heuristics were applied during the partitioning step. Instead, we find the optimal solution by using a dynamic programming approach. Ex-periments show that our class of encoders outperform all the existing methods in literature by more than 10% (with the exception of Binary Interpolative Coding with which they, roughly, tie) still retaining a very fast decompression algorithm.
 H.3.4 [ INFORMATION STORAGE AND RETRIEVAL ]: Systems and Software X  Performance evaluation (efficiency and effectiveness) ; E.4 [ DATA ]: CODING AND INFOR-MATION THEORY X  Data compaction and compression Algorithms,Performance,Experimentation d-gap encoding, inverted index encoding, adaptive encoding, index compression Data management systems such as: DBMSs, Information Retrieval Systems, Search Engines and the alike, are contin-uously facing the problem of the so called data deluge 1 . At the petabyte scale we cannot think of data as something to be viewed but, instead, something that need to be first ab-stracted and then use this abstraction to extract knowledge from new (possibly fresher) data. To compute any result from (real) data it is necessary to store them on some sort of storage and this does not come for free, especially at the petabyte scale. Encoding data to save space is, therefore, of utmost importance to enable the effective exploitation of the very large datasets managed by today X  X  systems. Con-sider that, even by changing scale, storage might be an is-sue. Personal devices size is constantly shrinking. Increas-ing data density in storage devices it is just not enough. We need to design technique enabling the efficient store of (rel-atively) large datasets in these devices. In these scenarios, data compression seems mandatory because it may induce a twofold advantage. On one hand, the obvious reduction in space occupancy allows more data to be stuffed within a sin-gle store unit. On the other hand, by fitting more data into faster memory levels, compression reduces the size of data to be transferred from the slower levels. This is a classic ex-ample of trading CPU cycles for decreased I/O latency and bandwidth. For example, given the amount of computing power on a modern multi-core CPUs, transferring a com-pressed payload from the disk and decompress its content into memory is still far cheaper than just transferring the uncompressed data. Not only data transfers from disk to memory benefit from compression, also data transfers from memory to CPU is also positively affected by compression as it is shown by IBM Memory Expansion Technology [1]. This is a very well know fact also in IR where many scientific results show hot to exploit this trade-off [19, 18, 20, 13].
In this paper we present Vector of Split Encoding , here-inafter VSEncoding , a novel class of encoders designed to efficiently represent lists of integers. Our encoders work by splitting the list in blocks and by encoding any integer in each block by using a fixed number of bits (namely, the number of bits required to represent the largest integer of the block). Obviously, we could have chosen to represent in-tegers in a block by using codewords of variable length, this would have, perhaps, increased the compression efficiency but also the decoding time. The simpler choice is adopted to allow a very fast decoding algorithm. Thus, VSEncoding http://glinden.blogspot.com/2006/10/ advantages-of-big-data-and-big.html are, basically, block-based compression schemes where the critical difference with respect to previous block-based meth-ods, e.g., P4D [21] and Simple9 [2], is the strategy used to choose the blocks. Differently from previous works, where heuristics were applied during the partitioning step [2, 21, 3], we optimally partition the lists in blocks via dynamic programming. The main contribution of our paper is the use of a global optimization technique that is able to dis-cover the best possible block allocation given the encoding method. Our encoders are able to achieve very good com-pression performance and outperform all the existing meth-ods in literature by more than 10% (with the exception of Binary Interpolative Coding with which they, roughly, tie). In particular, we report that our encoders are able to  X  beat the entropy  X  of the distribution of values in the lists. We observe that this is possible due to the fact that our en-coders are able to exploit regularities in the lists that are not captured by the entropy. Regarding decoding speed, our encoders are faster than the state-of-the-art PForDelta-like encoders [21, 20], VBytes [19], Simple9 [2] and Simple16 [20].

The paper is organized as follows. Section 2 is used to fix useful notation. In Section 3 we describe some of the most popular encoding methods proposed in the literature. We present known techniques that are either suitable for en-coding single integers, or specifically designed to compress lists of integers. In Section 4 we present our new class of integer list encoders. We also propose two of its instan-tiations that better exploit the skewness of the list to be encoded. Moreover, we show how to organize in memory the compressed representation of a list in order to achieve a very fast decompression algorithm. Section 5 show empiri-cal comparison among our solutions and the most popular ones on three real datasets representing the posting lists of inverted indexes of three different document collections. We conclude the paper in Section 6 by presenting our plans for future work.
Let L denote a list of n strictly positive integers. For any list L , L [ i ] denotes the i -th element, and L [ i : j ] is the contiguous sublist of L ranging from position i to j , 0 &lt; i  X  j  X  n . Therefore, L [1 : n ] denotes the entire list L . We say that L is sorted iff L [ i ] &lt; L [ i + 1], for any 0 &lt; i &lt; n . Given an integer L [ i ] we denote with bin( L [ i ]) its binary representation, and with | bin( L [ i ]) | its length in bits (namely, | bin( L [ i ]) | = b log 2 ( L [ i ]) c + 1).
Even if our encoders are able to compress any list of in-tegers, in the experimental part of this paper we apply our solutions to lists of d -gaps [19] coming from inverted indexes. Given a sorted list of integers L , a list of d -gaps D is defined as follows: D [1] = L [1], D [ i ] = L [ i ]  X  L [ i  X  1], i &gt; 1. As an example, consider the list L =  X  1 , 2 , 12 , 30 , 32  X  we have the corresponding d -gap list D =  X  1 , 1 , 10 , 18 , 2  X  . D-gap lists are be made up of smaller values than the original list Therefore, codes that represent small values with shorter codewords will result in more compact encodings for L .
For our purposes we are particularly interested in the dis-tribution of the integers in the (d-gap) lists. The skewness of a list can be (informally) defined as the measure of the
Obviously, to recover L from D requires a second pass to  X  X refix-sum X  up the values to have the original list back. asymmetry of the distribution of its elements. In particular, a  X  X ositively skewed X  distribution is a distribution where the mass of the distribution is concentrated on the left, i.e., an element of the list is rarely a large integer. It is worth notic-ing that the list distributions in which we are interested in practice are highly skewed. In particular, it has been shown that the distribution of d -gaps follows a power-law [19, 15, 20], which is an extremely positively skewed distribution, i.e., a large fraction of values are equal to  X 1 X .
The aim of section is that of introducing the most pop-ular encoders that are going to be compared in the exper-iments section (Section 5). We divide known methods in two classes: Integer encoder s and Integer List encoder s. The former codes assign a distinct codeword to each possible in-teger. Thus, a list is compressed by replacing each integer with its corresponding codeword. Encoders in the second class, instead, are specifically designed to compress lists of integers and may encode any of them considering also its neighbors in the list. These methods are much more power-ful than integer encoders since they can exploit regularities (e.g., clusters of almost equal integers) on the underlying list either to achieve higher compression or to provide faster decompression. As a consequence of this, methods in the second class may potentially be able to beat the entropy of the distribution of values in the underlying lists. Indeed, it is well-known that the compress size achievable by any of the former methods is lower bounded by entropy. Our methods belong to the class of Integer List encoders and are able to beat the entropy on the three tested datasets. Thus, we are sure that they achieve better compression than any integer encoder even without the need of an explicit experimental comparison.
In modern computer architectures, integers are usually represented (uncompressed) using 32 bits per integer. How-ever, whenever the largest possible integer to be encoded, say m = max using only d log 2 m e bits 3 . This representation may result in a net saving of 32  X  X  log 2 m e bits per integer with respect to the plain representation. This is the best compression we can hope to achieve whenever the underlying distribution of integers is uniform and m is an exact power of two. If m is not a power of two we can resort to minimal binary code s. Notice that, by assigning codewords of d log 2 m e bits, the fixed representation above wastes 2 d log 2 m e  X  m codewords. This implies that 2 d log 2 m e  X  m codewords can be shortened by one bit without loss of unique  X  X ecodability X . This is done by using in the code all the prefixes of numbers in a given interval. If we use the regular binary numbers to en-code the first six integers as (000 , 001 , 010 , 011 , 100 , 101), we miss  X 11 X  as a prefix. On the other hand the first six integers can be coded using a code (00 , 01 , 100 , 101 , 110 , 111). Note all possible prefixes of one bit (0 , 1) and all possible prefixes of two bits (00 , 01 , 10 , 11) appear in the code allowing the saving of one bit when encoding 0 and 1.

Fixed representation and minimal binary codes could be very inefficient for skewed distributions. This is the main
We recall that L values are strictly positive and, thus, d log 2 m e bits suffices to represent a value from 0 to m  X  1. motivation for integer encoders which assign to each integer a variable length codeword. The strategy adopted to assign codewords is crucial. Usually, each method is tuned to work (almost) perfectly on its  X  X deal X  distribution of values. How-ever, whenever the real distribution differs from the ideal one, the codewords lengths of various integers could be not suitable and the encoder could waste space. It is impor-tant to notice that each encoding is a prefix code: no valid codeword is prefix of another codeword and thus can be in-stantly decoded as it is read [14]. As a consequence, none of these kind of codes can beat the entropy of the underlying distribution of integers.
 Unary ( Unary ). In the unary representation each integer value x is represented using x  X  1 bits equal to  X 1 X  followed by a  X 0 X  that acts as a terminator [19]. Therefore, the length of the encoding of an integer x is | Unary ( x ) | = x . As an example, if x = 5 we have UN(5) = 11110.
 Elias X  Gamma (  X  ). In  X  , an integer x &gt; 0 is encoded by representing | bin( x ) | (i.e. b log 2 ( x ) c + 1) in unary followed by bin ( x ) without its most significant bit [9]. Therefore, |  X  ( x ) | is equal to 2 b log 2 ( x ) c + 1. As an example, if x = 5 we have bin ( x ) = 101 and thus  X  (5) is equal to 11001. Elias X  Delta (  X  ). In  X  , an integer x is encoded by rep-resenting | bin( x ) | by using  X  followed by bin( x ) without b log 2 x c + 2 b log 2 log 2 ( x ) c + 1. For instance,  X  (5) = 10101. Boldi&amp;Vigna X  X  Zeta (  X  k ). In a recent paper, Boldi and Vi-gna [4] propose a class of integer encoders that are suitable for lists of numbers drawn from a power-law distribution 4 Given an positive integer parameter k ,  X  k encodes a posi-tive integer x in the interval h 2 hk , 2 ( h +1) k  X  1 UN ( h + 1) followed by a minimal binary code of x  X  2 hk the interval h 0 , 2 ( h +1) k  X  2 hk  X  1 i . Note that  X  lent to  X  . As an example  X  2 (5) = 10001,  X  3 (5) = 0101, and  X  (5) = 00101.
 Others . In literature are known many other integer en-coders [13]. Among them we recall Golomb [11] and its vari-ation Rice . We do not enter into details of these two methods since it is well known that they are slower [20] than the pre-vious encoders and. Also, since their space occupancy is bounded by the entropy, they cannot beat our encoders in compression.

The previous encoders are said to be bit-oriented encod-ings since their codewords may cross the boundary of a com-puter word. During decoding, this requires additional bit-wise OR, mask, and shift operations that slows down the decoding phase. Other encoders are said to be byte/word-aligned codings since they try to find a workaround to this by aligning each codeword to byte (or word) boundary. Thus, usually they are faster but much less space efficient with respect to bit-oriented encodings.
 Variable-Bytes ( VBytes ). A non negative integer x is represented in VBytes as a list of 7-bit entries. Each ele-ment of the list is prefixed with 0 except for the last one, which is prefixed with a 1 [13, 19]. The length in bits of VBytes ( x ) is given by | VBytes ( x ) | = 8 d ( b log 2 (or alternatively d ( b log 2 x c + 1) / 7 e bytes). As examples, VBytes (5) = 10000101, VBytes (129) = 00000001 10000001.
Recall that a discrete random variable Z is distributed as a power-law with parameter  X  whenever the probability of the event Z = x is P ( { Z = x } ) = 1  X  (  X  ) x  X  .
The main limitation of integer encoders is that they en-code each integer in the list separately, without taking into consideration its neighbors. Instead, Integers list encoders may improve compression by, for example, exploiting clus-ters of almost equal integers in the underlying list. Binary Interpolative Coding ( Interpolative ). A more so-phisticated way of encoding a list of sorted integers is using the Binary Interpolative Coding of Moffat and Stuiver [12]. Starting from the assumption that in highly-skewed distri-butions integers usually appear clustered [5] within a list, Interpolative works by recursively splitting the interval of integers contained within a list and encoding the central el-ement via minimal binary code. By doing this, whenever a (sub)list of consecutive numbers is found it is encoded us-ing  X  zero  X  bits. Experiments performed throughout these years have shown that Interpolative is still the best encod-ing method for highly skewed lists of integers [19, 15, 20]. The major drawback of Interpolative is the poor performance exhibited at decoding time.
 Simple9 ( Simple9 ). It encodes groups of integers within a single 32-bit word. Basically, in Simple9 there are nine possible ways of encoding a list of positive integers: 28 1-bit integers, 14 2-bit integers, 9 3-bit integers (one bit unused), 7 4-bit integers, 5 5-bit integers (three bits unused), 4 7-bit integers, 3 9-bit integers (one bit unused), 2 14-bit integers, or 1 28-bit integer. The remaining four bits to complete the 32-bit word are used as status bits to represent which of the nine cases is used. Decompression is done by reading the status bits and, depending on their value, by applying a specific function that efficiently extracts all the integers in the word [2]. Simple9 wastes bits when encoding some combinations of integers. For instance, in encoding 5 5-bit integers we have three unused bits. To overcome this issue, Yan et al. have designed Simple16 [20], a different encoding schema for fitting sixteen different combinations of integers within a word. Experiments showed that Simple16 is more compact than Simple9 (from which it is inspired). Another variant of Simple9 , that reduces the wastage of bits of Simple9 is slide [3]. Since it incurs in a higher decoding complexity, we do not include slide in our experiments. PForDelta ( P4D ). P4D encodes blocks of k consecutive in-tegers (e.g. k = 128 integers). The method firstly finds the smallest b such that most (e.g. 90%) of the integers in the block are non greater than 2 b . Then, it performs the encod-ing by storing each integer as a b -bit entry. Each entry is then packed within a list of d k  X  b e bits. The parameter k is usually chosen to be a multiple of 32. This implies that the k  X  b bits list is always word aligned regardless of the value of b . Those integers that do not fit within b bits are treated as exceptions and stored differently [21]. We actually refer to a different representation of P4D by Yan et al. [20] (called OPT-P4D ). In this variant the number of exceptions is not forced to be smaller that 10% of the block length. Instead, it is chosen to minimize the space occupancy. Moreover, ex-ceptions are stored in a separate array that is merged to the original sequence of codewords during the decoding phase. According to Yan et al. [20], this representation is more compact and not significantly slower than the original P4D .
State-of-the-art integer list encoders use predefined schemes for partitioning a list into blocks and encoding each block separately. For example, P4D and its variations divide the list in to blocks of fixed length, and, then encode each block with b -bit codewords possibly generating exceptions for in-tegers greater than 2 b . Instead, Simple9 and its variations greedily partition the list into blocks of variable length and encode each of them accordingly to predefined possibilities. Finally, Interpolative represents the the middle value of the list encodes the remaining part recursively by dividing the list in two almost equal parts. These methods have inef-ficiencies either in achieved compression or decompression speed. By fixing the block length, P4D -based encoders are not allowed to adapt themselves to regularities present in the lists. For example, the block length should be smaller for some portion of the list and larger for the others. Ex-ceptions serve to attenuate the effect of misplacing integers of different magnitude in the same block. However, we pay their effort at a cost of introducing significant complications in the decompression algorithm that affect decompression speed. Simple9 , instead, is too limited in possible choices which inevitably led itself to miss some regularities in the list. For example, grouping a run of 1s into a single block and encoding each of them with one bit, is possible only if the run has length at least 28. Finally, the Interpolative strategy is very effective in term of compression but slow due to its recursive compression/decompression algorithms.
In what follows we present our class of integer list en-coders that overcome the above limitations. Our class of encoders is similar in the spirit to P4D and Simple9 but par-titioning and encoding steps are done in a more principled way in order to maximize the achieved compression still re-taining very simple and fast decompression algorithm. Our encoders (called, VSEncoding ) are parametric with respect to two given integer encoders M 1 and M 2 . Informally, the general scheme works as follow. We partition each list into blocks of variable length, and we encode the integers inside of each block with the number of bits, say b , required to en-code the largest one. Finally, we encode the above value of b with M 1 and the length of the block with M 2 . Obviously, the partition step is crucial for achieving high compression. On one hand, if a block is too large, we may waste a lot of space by encoding all its elements with b bits. On the other hand, if the block is too small, we may waste too much space in writing the value of b and the block length. Our solution uses a Dynamic Programming approach to find the optimal partition (i.e., the one that maximizes compression) with respect to M 1 and M 2 . The partition step is discussed in Subsection 4.1, for the moment, let us define more formally our class of encoders assuming that any partition is given. Let L be the list of n positive integers to compress and let S be the list of m &lt; n integers (called Vector of Splits ), with S [1] = 1, and S [ m ] = n + 1, that induces the given partition of L : each two consecutive elements S [ i ] and S [ i +1] induce a block, namely s i = L [ S [ i ] : S [ i + 1]  X  1]. For any block s b be the minimum number of bits required to represent any integer in the block s i , namely d (log 2 max( a  X  s i ) e ), and let k be the number of elements in s i , i.e. k i = S [ i + 1]  X  S [ i ]. Given the two integer encoders M 1 and M 2 , VSEncoding 5 encodes each block s i by encoding 1. value b i + 1 with M 1 ; 2. value k i with M 2 ; 3. the k i elements of s i using b i bits each.
 Let us make an example to show how VSEncoding works. be the given vectors of splits, M 1 be  X  , M 2 be Unary . From S we can devise the following partition: s 1 = L [1 : 2] = three blocks are encoded as: 1.  X  ( b 1 + 1 = 4) = 11000, Unary ( k 1 = 2) = 10, 101 000; 2.  X  ( b 2 + 1 = 4) = 11000, Unary ( k 2 = 2) = 10, 000 101; 3.  X  ( b 3 + 1 = 1) = 1, Unary ( k 3 = 2) = 10.

Notice that the encoding of elements of third block re-quires no bits, since we can infer that they are all 1s by knowing the value of b 3 .

Given a list L and a vector of splits S , we can easily com-pute the number of bits required by VSEncoding to encode L using the partition induced by S (which is denoted by | VSEncoding ( L,S ) | ). This quantity can be computed by summing up the costs of encoding all the blocks as follows: where c ( S [ i ] ,S [ i + 1]  X  1) = |M 1 ( b i + 1) | + |M is the cost (in bits) required to encode the i -th block 6
In the previous example we have that | VSEncoding ( L,S ) | = | Unary (2) | + 2  X  3) + |  X  (1) | + | Unary (2) | + 2  X  0 = 29 bits.
As we said before, the choice of correct partition is crucial to achieve high compression. To make a concrete example consider the partition induced by S 0 =  X  1 , 2 , 4 , 5 , 7  X  on the same list. The compress obtained with the same choices of M 1 and M 2 has size | VSEncoding ( L,S 0 ) | = 22 bits, which is more than 30% better than the previous one. In the next subsection we show how to efficiently compute the optimal vector of splits for a list L fixed M 1 and M 2 , that is, among all the possible vector of splits, we select one that achieves the best compression.
The problem of finding the optimal encoding for a list L is formulated as the problem of finding the vector of splits S  X  that minimizes | VSEncoding ( L,S ) | defined in Equation 1 among all the possible 2 n vectors of splits S . More formally, S  X  is such that
Since it is useful for the choices of M 2 s used in the exper-iments, we consider the case in which one can also fix the maximum length of the blocks by specifying a value maxK . Notice that this is actually a generalization of the problem
Actually, since VSEncoding is a class of encoders parametric in M 1 and M 2 , it should be denoted as VSEncoding M 1 , M to make more explicit this dependence. Since in the follow-ing the role of M 1 and M 2 is unambiguous, we decide to drop this more precise notation in favor of legibility.
Notice that the cost depends on the choices of M 1 and M 2 Figure 1: The algorithm to find the optimal parti-tion of a list L [1 ,n ] using encoders M 1 and M 2 to encode values of b and block length respectively and allowing only blocks of length at most maxK . above: to have no limits on blocks lengths, it is enough to set maxK equal to the length of the list.
 It is easy to prove that this problem can be solved via Dynamic Programming paradigm using the following recur-rence: where
To start the recurrence we set E [1] = 0, since it corre-sponds to the cost of encoding an empty list. Once we have solved Recurrence 2, the value of E [ n + 1] tells us the cost of the optimal partition of L .

The above recurrence can be solved in O ( n  X  maxK ) by re-sorting to the classic algorithm for this type of recurrences [7] (see Algorithm 1). In this algorithm we start by setting E [1] = 1, then we compute entries of E from left to right (Steps 2 X 9 in Algorithm 1). At the generic step, we compute E [ i ] by identifying an index j  X  &lt; i among the ones having the minimum value of E [ j  X  ] + c ( j  X  ,i ). This index j tified by simply trying all indexes j between i  X  maxK and i  X  1 (Steps 4 X 9) with the only wariness of doing this from the largest index to the smallest one. In this way, we are able to compute the value b of sublist L [ j : i  X  1] knowing the value of b of sublist L [ j  X  1 : i  X  1] in constant time (Step 5). During the execution of the algorithm, we also keep track of above index j  X  in the array P (Step 9), so that, at the end of the computation, we are able to reconstruct the vector of splits inducing the optimal partitioning by jump-ing back from n + 1 through values of P (namely, P [ n + 1], P [ P [ n + 1]], P [ P [ P [ n + 1]]], and so on). In our tools we implemented this simple algorithm mainly due to the fact that experimental evidences show that good values of maxK are small constants between 16 and 64 for our choices of en-coders M 1 and M 2 . For completeness, we point out that faster algorithms are possible by adapting known solutions (see [10] and references therein). For example, by resorting to the result in [10], we can compute an (1 + ) approximate solution of Recurrence 2 in time O ( n log 1+ n ), where is an arbitrary positive value. Moreover, we are able to extend this result to compute the exact solution of Recurrence 2 in time O ( n log 2 maxK ) whenever the encoders M 1 and M 2 chosen among most of integer encoders described in Section 3. As a final remark, we point out that in terms of encod-ing time VSEncoding is not less efficient than OPT-P4D [20]. OPT-P4D computes, for all the possible values of b , both space and time taken to encode/decode each list of integers. Therefore, encoding using OPT-P4D costs O ( bn ) where b is 20, whereas VSEncoding costs O ( n log 2 maxK ), where in practical implementation maxK ranges between 16 and 64. Moreover, since OPT-P4D does two passes over data (one pass to encode and verify the occupied space, the second pass to verify the decoding speed), the two methods have comparable performances.
We can obtain a valid instantiation of our encoders by choosing any possible combination of integer encoders among the ones described in Section 3.1 or the myriad introduced in literature [13]. We tried many of them in our experimental investigation but we report here only the two most promis-ing in terms of space achieve and decompression speed. It should not surprise that, since we particularly care about decompression speed, they are quite simple.

In the first instantiation (referred to as VSE in the ex-periments) we use two simple encoders. Given the list L to be encoded, we firstly compute the maximum value M of its elements, then M 1 simply encodes possible values of b using fixed codewords of length b log 2 d log 2 M ec + 1 bits. As far as M 2 is concerned, we still use a fixed representation which encodes values among { 1 , 2 , 4 , 6 , 8 , 12 , 16 , 32 } using 3 bits each. Any other value is considered non valid for the length of a block.

The second instantiation (referred as VSE-R in the exper-iments) uses similar encoders for M 1 and M 2 but performs a further, recursive, step. Firstly, from the original list L we produce a new list L 0 such that L 0 [ i ] = b log 2 L [ i ] c + 1 (i.e., L [ i ] is equal to the number of bits needed to represent value L [ i ]). Then, we encode each value L [ i ] by writing bin( L [ i ]) without its most significant bit. Notice that if L [ i ] = 1, no bit is emitted. Finally, we apply a variant of VSE to encode the list L 0 7 . Clearly, the value of L [ i ] can be reconstructed once we know the value of L 0 [ i ]. VSE-R is designed to re-duce the space wasted by encoding a sub-block of integers using a fixed amount of bits. In fact, of the kb bits used to encode k integers within b bits, a certain number of bits are left unused (in particular those wasted in encoding numbers smaller than 2 b ).

For a running example consider again the list in the ex-ample above (i.e., L =  X  8 , 1 , 1 , 8 , 1 , 1  X  ). The list L L 0 =  X  4 , 1 , 1 , 4 , 1 , 1  X  . We use VSEncoding on L same S as before obtaining the following 8 1.  X  ( b 1 + 1 = 3) = 101, Unary ( k 1 = 2) = 10, 11 00; 2.  X  ( b 2 + 1 = 3) = 101, Unary ( k 2 = 2) = 10, 00 11; 3.  X  ( b 3 + 1 = 1) = 1, Unary ( k 3 = 2) = 10;
We change M 2 so that it encodes using 3 bits only values among { 1 , 2 , 4 , 8 , 12 , 16 , 32 , 64 }
Here we use again  X  and Unary Figure 2: The Figure shows a running example of our layout. Assume that the list L has been parti-tioned in three blocks. First of all, we group inte-gers of L into two groups G 1 and G 2 that contains, respectively, integers whose corresponding b are 1 and 2 . Integers in the same group are written con-secutively in the compress. The arrows show how to permute the integers in the groups to obtain back the original list L . Notice that this permutation can be easily derived since values of b and k are written in the correct order.

Finally, we emit bits corresponding to L  X  X  elements as 000 000, notice that 1s in L are not encoded at all in this final step. The final compress of the method on this list has size 26 bits.

As we shall see in Section 5, VSE is faster in decompres-sion than VSE-R since it does not require the two steps of decoding while it is worse in compression. The better com-pression achieved by VSE-R is intuitively given by the fact that we encode a list of logarithmic values instead of plain values as in VSE . It is easy to show that in the case of highly skewed integer distributions, e.g. a power-law distributions with parameters  X  &gt; 1, with high probability the number of bits wasted by VSE-R is less than those wasted by VSE on the same vector of splits S . Roughly, it suffices to compute the number of bits wasted by the two methods with respect to the ideal case where each integer x requires b log 2 ( x ) c + 1 bits.

Finally, since we compute the optimal partitioning, we do not compare neither VSE nor VSE-R with any simpler heuristics for data splitting (e.g., taking simple fixed-length blocks of k elements at a time): either the simpler heuris-tics or the most sophisticated ones cannot outperform our optimal partitioning.
In order to achieve a very fast decompression algorithm for VSE and VSE-R we have to carefully organize informa-tion on the compress file. A trivial layout in memory for VSE and VSE-R has been briefly described in Section 4.2: We encode each block separately by simply writing its val-ues of b and k followed by the k integers encoded by using b bits each. In this way, the decompressor is very simple but, unfortunately, slower than the fastest known methods like Simple9 , Simple16 and P4D . The reason is mainly given by the fact that block representations are not word aligned. This forces us to perform at least a conditional jump for ev-ery decompressed value 9 . It is well-known that conditional jumps are very expensive, and an efficient algorithm should avoid them as much as possible. For example, in Simple9 or Simple16 a single conditional jump followed by a call to an appropriate ad hoc function suffices to decode each encoded word. The parameter k in P4D is chosen so that the encoded representation of a block is word aligned. This implies that the k integers in a block can be decoded by resorting to very effective ad hoc functions that completely avoid conditional jumps. To be more precise, we have a function for each pos-sible value of b that simply perform the correct operations required to decode k integers encoded with b bits each. For example, Figure 3 shows the function used in P4D to decode a block of k = 32 integers encoded by using b = 8 bits each. The decompression with these kind of functions is very fast. However, we recall that the decompression of blocks P4D has also to manage exceptions. This second step, in turn, significantly reduces its speed.

The layout we use for VSE and VSE-R is more involved with respect to the trivial one but allows a faster decom-pression algorithm. The idea is to organize the information so that the number of conditional jumps is considerably re-duced. In the explanation we concentrate on VSE , since the layout for VSE-R is similar. Assume that the list we have to compress has been partitioned into l blocks by the partitioning step and that the obtained values of b and k are b 1 ,b 2 ,...,b l and k 1 ,k 2 ,...,k l respectively. Firstly, we group the integers of the list accordingly to the number of bits that we have to use to represent them. Then, we write separately the values in each group: first the values that have to be represented with 1 bit, then with 2 bits, and so on. If necessary, we pad the representation of each group so that it becomes word aligned. Finally, we write values of b and k in their order (i.e., b 1 k 1 ,b 2 k 2 ,...,b l k l sion is done in the following way. We decompress each group by resorting to the same fast functions of P4D (e.g., the one in Figure 3). This is possible since groups representations are word aligned. At this point we obtained groups of orig-inal integers that are out of order. In order to reconstruct the original list we appropriately permute these integers by exploiting the fact that values of b and k has been stored in the correct order. See Figure 2 for a simple example.
This algorithm, combined with the fact that we do not have to perform any conditional branch, allows for fast de-compression speed as experiments in the next Section show.
In our experiments we use three collections to cover dif-ferent possible sizes: gov2 , wbr and wt10g . gov2 and wt10g are TREC test collections for use in the Terabyte Track. The former is a crawl of 25 , 205 , 170 .gov sites (as they were in early 2004) with documents truncated to 256 kb. wt10g is made up of 1 , 692 , 096 documents crawled in early 2000. wbr is made up of 5 , 939 , 061 web pages, representing a snap-shot of the Brazilian web (domains .br) as spidered by the
Notice that, in order to read values from a non-word aligned sequence of bits, we have to keep in memory a buffer of bits and check if it contains a sufficient number of bits before any read. Figure 3: The ad-hoc C function used in P4D to decode k = 32 integers represented by using b = 8 bits each. crawler of the TodoBR search engine in 1999. More infor-mation about these three collections are shown in Table 1 which reports basic statistics such as the size of plain col-lection in Mbytes, the number of documents, the number of terms (i.e., the number of lists), the number of encoded integers, the length of the longest list, and the average lists length.
 We tested the different methods on a PC with an Intel Xeon Quad-Core Processor equipped with 8GBytes RAM and SATA hard disks. The operating system is a 64-bit version of Linux 2.6.31-20. All our code is written in C and is available at http://hpc.isti.cnr.it/~integerencoding .
In the experiments we restricted our attention on com-pressing lists larger than 16 elements. The reason of this choice is given by the fact that we want to limit the over-head of function calls when we measure the decompression speed of the different methods. We experimentally observed that this choice does not affect the comparison among the different methods with respect to achieved compression. We also restrict our attention on lists in which document Ids are assigned by sorting the corresponding URI lexico-graphically. In this way we obtain lists that are much more compressible as well documented in many preceding works Table 1: The table reports some basic statistics on the collections we use in our experiments. Figure 4: Distribution of the first 10 d -gaps for our collections before ( .orig ) and after ( .sort ) the reassig-ment of document Ids. (see for example [17, 15, 20] and references therein). This phenomenon finds its explanation in the fact that documents in the same domain are likely to be similar (i.e., they con-tain almost the same set of terms). Thus, the reassignment above assigns close Ids to documents that belong to the same domain, so that it is likely to obtain very small d -gaps in the lists. By this reason, the resulting collections are much more compressible. Figure 4 shows the distribution of the first 10 smallest d -gaps in our collections after and before the above reassignment.
 Table 2 shows the gain in compression achievable with Interpolative and  X  on our datasets. The gain is impressive: the compress is from 21 to 110 more compact than the best performing method, i.e. P4D . Notice that the gain of the reordering largely compensates the negligible cost (few Mbs) of storing in an array the inverse assignments which may be necessary for some reason. Thus, the reassignment is a very profitable choice even when a different document Ids assignment is necessary. In the following, we restrict our attention to compress our datasets in which document Ids are sorted in this way.
 Compression performance. In our experiment we tried different compressors as reported in Table 3. In particular, OPT-P4D refers to the OPT-PforDelta described in [20] with blocks of size 128 values. We choose this parameter after experimental evaluations. The smaller the block length, the better the achieved compression, but slower is the decom-pression. With blocks larger than 128 we obtain compression performance which are significantly worse while the decom-Table 2: The table compares the compression achieved on original and sorted version of our datasets with Interpolative and  X  . The compression is expressed in bits per integer. The Gain factor tells the improvement in compression obtainable by reassigning document Ids. pression is just slightly faster. With smaller blocks, i.e., 32 or 64, the decompression speed is up to four times slower. We remark that we tested our implementation of OPT-P4D whose performance has been validated against the original implementation kindly provided by the authors of [20].
We point out that only our methods, together with In-terpolative , are able to beat the entropy of the lists on the datasets. This quasi-paradoxical effect is, indeed, present because entropy does not consider context information. En-tropy, or to use a notation commonly used in text compres-sion, zeroth-order entropy, does not take into account pat-terns (i.e. the context) that can be present in lists of blocks of integers. By grouping together blocks of integers, in fact, we are able to assign codewords to more than a single value at a time. Therefore, it appears obvious that we can beat the entropy in the case of VSE , VSE-R and Interpolative . Es-sentially, this is possible since we exploit regularities on the lists on these very skewed d -gaps lists (e.g., small values close to each other or quite long runs of 1s). We remark that beat the entropy is not possible with any prefix code (e.g., sta-tistical compressors like Arithmetic and Huffman or integer encoders like  X  ,  X  ,  X   X  X , Golomb, and so on). Therefore, our methods is certainly better in compression than any of these kind of methods without the need of any comparison. Any-way, for the sake of completeness, we report those results as well in Table 3.

To resume, our experiments show incontrovertibly that our methods achieve compression performance comparable (and in the case of wbr better) to those achieved by the state-of-the-art (in terms of space) compression method, i.e. Interpolative . As we are going to show, decoding speed is an issue in the case of Interpolative while our methods are instead faster than the state-of-the-art P4D .
 Decompression speed. Table 4 reports results on the decoding speed, in terms of millions of integers per second, of the different methods we tested. We report the performance computed over different postings lists and we indicate the average decoding speed along with its standard deviation. All the values have been rounded to the nearest ten.
As expected, Interpolative is the slowest as opposed to VSE which tops others with more than 800 millions of integers per second. Our methods, VSE and VSE-R , are among the fastest in decoding with a number of mis (millions of integers Table 3: Compression achieved by the various en-coders on our datasets expressed in bits per inte-gers (bpi). In bold we report the best compressor. For each compressor we also report its increase (in percentage) with respect to the best compressor.
 Table 4: Average decompression speed on the vari-ous compression methods on our datasets expressed in millions of integers per second (mis). The value after  X  indicates how much the speed of various ex-ecutions are different from the reported value. per second) decoded ranging from 450 of VSE-R to 835 of VSE both of them measured using the gov2 collection. It is interesting to observe the better performance in terms of decoding speed of VSE with respect to others, and in particular with respect to OPT-P4D , Simple9 and Simple16 which are considered state-of-the-art as far as decompression speed is concerned.

We would like to point the attention on the quite good decompression performance of  X  ,  X  and  X  3 . In our imple-mentations their decoders have been particularly optimized for decoding speed using table lookups to quickly decode se-quences of bits. We have measured the effect of such a table and we observe that, by only using 2 16 = 65 , 536 entries, a single table lookup suffices to decode the codeword for about 90% of the integers, so that only remaining integers are decoded with the classic and slow algorithm.
 From the experiments, Interpolative , VSE , and VSE-R , as Figure 5 shows, dominate all the others we tested. In par-ticular, what can be highlighted from the plot in Figure 5 is that our two methods optimize both decoding speed and compression space at the same time. Obviously, in envi-ronments like those typical of web search engines, where one should aim at being both fast and space efficient, our methods VSE and VSE-R result to be those of choice with a preference for VSE if one care more about speed than space. Encoders statistics. We report in this paragraph some statistics on our encoders, VSE and VSE-R , that help in understanding the correlation between the skewness of a Smaller Figure 5: A graphical comparison of the different en-coders showing the trade-offs in time and space. On the x -axis is represented the compress size (normal-ized between  X  1 and 1 ), on the y -axis is represented the decoding speed (normalized between  X  1 and 1 , as well.) dataset and, the number of bits and length of blocks pro-duced by the two methods.

As it can be observed in Figure 6 (above), using VSE we do not have a large variation in terms of block lengths. This means that VSE is able to adapt, correctly, to the underly-ing distribution of integers. In addition, another important aspect to point out is the large fraction of blocks encod-ing their members using 0 bits. This can be seen in Figure 6 (below), where it is shown the distribution of number of bits used to encode elements in each block using VSE . Interesting to notice that still a large fraction of elements needs more than 8 bits to be encoded, this is due, again, to the high skewness of our datasets characterized by long runs of 1s.
The two bar charts in Figure 7, instead show the empirical explanation for the reason why VSE-R appears to perform better, in practice, than VSE for skewed datasets. First of all, as in the previous case runs of  X 1 X  X  are frequent and from this we have a large fraction of blocks encoded using 0 bits. The main difference, though, is observed in the case of the number of blocks having a relatively large size. Blocks of length 8 and 16 are the most frequent (with a total frequency that is around the 40%). It is quite likely, then, that a large fraction of long blocks can be encoded using 0 bits. This is, again empirically, confirmed by the experiments shown above.
We have described VSEncoding , a class of encoders that through a dynamic programming algorithm are able to en-code lists of integers beating the entropy of the gaps dis-tribution. The assignment of codewords is done with the goal of optimizing both the space taken by the codewords themselves and the time needed to decode. We have shown, through extensive experiments, that our methods constantly outperform the others in terms of both space and time and, in our opinion, should be the methods of choice for data Figure 6: Distribution of lengths of the blocks (above) and the number of bits used to encode ele-ments in the blocks (below) in VSE over our datasets. management systems (e.g. web search engines) aiming at very high performance and low space consumption.

Even if our methods are already among the fastest state-of-the-art fast-encoders (e.g. those of the PForDelta family or Simple9 like), we would like to more extensively experi-ment other variations of our methods that could be obtained by varying encoders M 1 and M 2 in order to further improve either compression rate or decompression speed. Ideally, one would like to have a scheme that has decompression speed of VSE achieving compression rate of VSE-R .

We defer to a future work the study of the impact of list skipping [6] on the effectiveness of our method. Apart from the straightforward approach consisting in partitioning each list according to the strategy by Chierichetti et al. [6]. The challenge, anyway, is to find an optimal way of partitioning the lists of integers also in light of how skips are placed.
As it has been shown in the discussion of the data lay-out, the impact of the architecture is of fundamental im-portance to the efficiency of the decoding method. We are currently developing a very fast, and ad-hoc, VSE encod-ing like method for GPUs [8]. Preliminary experiments are Figure 7: Distribution of lengths of the blocks (above) and the number of bits used to encode elements in the blocks (below) in VSE-R over our datasets. very encouraging showing a sharp improvement in decoding speed.

Finally, we are aware that in Web Search Engines not all the lists are accessed with the same frequency. We are cur-rently studying strategies for the optimal encoding of post-ing lists also considering access patterns. We are using infor-mation available from query logs [16] to extract lists access patterns. [1] B. Abali, H. Franke, X. Shen, D. E. Poff, and T. B. [2] V. N. Anh and A. Moffat. Inverted index compression [3] V. N. Anh and A. Moffat. Improved word-aligned [4] P. Boldi and Sebastiano V. Codes for the world wide [5] A. Bookstein, S. T. Klein, and T. Raita. Modeling [6] F. Chierichetti, S. Lattanzi, F. Mari, and A.
 [7] S. Dasgupta, C. Papadimitriou, and U. Vazirani. [8] F. Dehne and K. Yogaratnam. Exploring the limits of [9] P. Elias. Universal codeword sets and representations [10] P. Ferragina, I. Nitto, and R. Venturini. On optimally [11] S. Golomb. Run-length encodings. Information [12] A. Moffat and L. Stuiver. Binary interpolative coding [13] D. Salomon. Variable-length Codes for Data [14] E. S. Schwartz and B. Kallick. Generating a canonical [15] F. Silvestri. Sorting out the document identifier [16] F. Silvestri. Mining query logs: Turning search usage [17] F. Silvestri, Salvatore Orlando, and R. Perego. [18] A. Trotman. Compressing inverted files. Inf. Retr. , [19] I. H. Witten, A. Moffat, and T. C. Bell. Managing [20] H. Yan, S. Ding, and T. Suel. Inverted index [21] M. Zukowski, S. Heman, N. Nes, and P. Boncz.

