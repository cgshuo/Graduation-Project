 1. Introduction
Travel time is the most intuitive measure of effectiveness to road users, transportation agency operators, and decision makers.
Even though the travel time measure is very well received by public, it has not yet been widely used by transportation agencies, in part due to the lack of reliable methods to estimate/measure travel times consistently. While some promising techniques, such as toll tag readers, probe vehicles, vehicle infrastructure integra-tion, etc., can accurately measure travel times, these methods rely solely on appropriately equipped vehicles and would take time to achieve significant market penetration levels.

The majority of urban freeway systems already deployed sensors spaced approximately every 0.8 km (0.5 mile) and travel times are estimated using speed data obtained from these sensors.
However, it is generally understood that the estimated travel times based on such fixed point sensors often contain large errors. This is mostly due to the sensor locations that are purposely selected to avoid excessive lane changing (the idea is to obtain accurate speeds from dual loop detectors by avoiding lane change) where weaving, diverging, and merging occur. Consequently, the speeds obtained from these sensors often do not directly capture congestion occurring in weaving, diverging, and merging areas. The other reason for such error is that the density of sensors is inadequate to capture dynamic queuing behaviors. Therefore, it is important to investigate whether a method for optimizing placement and density of point sensors can produce more accurate travel times.
Unlike loop detectors, non-intrusive sensors such as video, radar, and acoustic sensors can be easily relocated. Thus, if the findings of this research are significant, it is envisioned that the location of these non-intrusive sensors can be repositioned to better capture speeds for improving travel time estimations.

Previous studies investigated the relationship between the number of sensors and the traffic demand. For example, Chan and Lam (2002) found that as O X  X  demand increases, the speed detector density should also be increased, as the larger O X  X  demand produces larger mean travel times and travel time variances. Yang et al. (2005) also found that denser detector locations provided more accurate travel time estimations. In addition, Lu and Coifman (2007) pointed out that the density of the point-measurement stations is one of the key factors affecting estimation error of mean speed or link travel time. Fujito et al. (2006) found that the location of detectors plays an important role in estimating congestion measures, regardless of the average detector spacing. They also found that the unsystematic trend of under-or over-estimation resulted from the existence of bottle-necks around interchange and congestion areas. Edara et al. (2008) found that the placement of detectors for achieving accurate travel time estimates varies by location based on specific traffic and geometric conditions. They also found that the detector density needs to be higher in congested areas and that detectors are required at merge areas near entrance ramps, especially when the acceleration lanes are short.

These studies clearly indicated that the number of sensors and location of detectors do impact the estimation of travel times.
However, none of these studies incorporated an optimization approach to determine optimal sensor locations and the number of sensors. This paper presents a genetic algorithm (GA) based optimizationapproachto investigatetheimpactof sensorlocations and the number of sensors in the estimation of travel times (Holland, 1975) . It is noted that the proposed approach relies on a microscopic traffic simulation model. The basic premise is that once a microscopic traffic simulation model is well calibrated and validated to field conditions, the optimization results (i.e., findings of sensor locations and number of sensors) can be directly applicable to field implementation. Thus, the purpose of this research is twofold. One is to investigate optimal location of sensors, assuming typically deployed 0.8 km spacing is acceptable, and the other is to evaluate various densities of sensors to see if travel time estimation can be improved.

The remainder of this paper is organized as follows. The methodology section begins with a description of the VISSIM based freeway network and data reduction followed by the GA-based optimization program development and its implementation pro-cedure (Planung Transport Verkehr, 2007) . The optimal sensor locations obtained from the proposed approach are evaluated and compared with constantly spaced sensors typically implemented in the field. Finally, conclusions and recommendations are presented. 2. Methodology
As noted, the proposed approach used a microscopic traffic simulation model and a genetic algorithm. A typical approach of using the GA and a microscopic simulation model would select sensor locations (or number of sensors) at random and run the simulation model to obtain the performance of the potential GA solution. The performance would be computed comparing the estimatedtravel timefrom the speeds from the deployed sensors in the simulation program and the simulated travel times. This approach would require an independent microscopic simulation run for any given set of sensor locations and/or number of sensors.
The proposed method, however, makes the simulation runs with the maximum number of sensors (i.e., minimum detector spacing), and the estimated travel time is calculated from the speed data selectedfromthesesensorlocations.Thisallowssignificantsavings in the simulation run time. That is, once the simulation runs are completed, no additional simulation runs are needed during the GA optimization.
 The proposed methodology used the VISSIM (Planung Transport
Verkehr, 2007) microscopic traffic simulation model and the GA optimization method. It is noted that any microscopic simulation models including CORSIM (2003) , PARAMICS (Quadstone Ltd., 2005) , and AIMSUN-NG (TSS  X  Transport Simulation Systems, 2010) can be used. The proposed approach started from building aVISSIMfreewaynetworkwithspeedsensorsplacedataminimum spacing of 76.2 m (250 ft). The 76.2 m is deemed to be the minimum spacing. In order to consider peak and non-peak traffic conditions, two traffic volume sets, representingpeak and off-peak, were derived from the average annual daily traffic data. A small number of multiple simulation runs were made to incorporate day-to-day variations, and speed data obtained from sensors were kept for estimating travel times based on a set of sensors to be chosen during the GA optimization. As noted, using these speed data from all the sensors deployed in the network, no additional simulation runs were needed during the optimizations of sensor locations and number of sensors. Once the GA determines optimal sensor locations, additional VISSIM evaluation runs under varying traffic conditions, including increased demand and minor incident cases, were executed to generalize the findings. 3. VISSIM-based freeway network
Given that implementing the proposed approach in the real world would be impractical, this study relies on a microscopic traffic simulation model. As noted, if the proposed approach was found tobe useful, a reliable field implementation could be realized using a well calibrated and validated simulation model in the optimization. This study used the VISSIM simulation model to take advantage of the existing freeway network already coded in.
AVISSIM-basedfreewaynetworkwas developedwitha45.0 km long Interstate Highway 66 segment between US highway 15 (exit number 40) and Interstate Highway 495 (exit number 64), located in Prince William County and Fairfax County in Northern Virginia. The network geometry and lane configuration data were obtained from aerial photos available at the geospatial and statistical data center at the University of Virginia Library. Traffic volumes were derived from average daily traffic volumes, k -factor, directional factor, peak and off-peak volumes, and truck percentage data published in the Virginia Department Transportation Report (Commonwealth of Virginia Department of Transportation, 2010) . The VISSIM network consists of 11 interchanges along Interstate Highway 66. The number of lanes on the eastbound varied between two and five. 4. Data reduction 4.1. Types of data
The VISSIM model produced speed data from sensors and travel times experienced by the simulated vehicles (i.e., simulated  X  X  X urrogate ground truth X  X  for comparison purpose). Thus, the performance of travel time estimation was evaluated comparing the estimatedtraveltimesusing speeds and simulatedtraveltimes. The speed and travel times were aggregated at 5 min intervals for each 1 h period of peak and off-peak traffic conditions. Even though more hours or periods could have been used to better reflect real field conditions, it is assumed that 2 h with peak and off-peak are adequate for demonstration purposes. In the VISSIM network, a total of 594 sensors were placed at 76.2 m spacing throughout the entire network. The simulated travel times were measured at each ending point of the travel time section and speeds were collected at each data collection point. In order to consider day-to-day traffic flowvariations, twentyfiverandom seeded VISSIM simulationruns were made for each time period. Thus, a total of fifty simulation runs were made. 4.2. Characteristics of traffic condition
Themeans and standard deviationsof simulatedtraveltimes for both peak hour and off-peak hour are summarized in Table 1 .As expected, peak hour traffic shows much higher variations than that of off-peak.

Fig. 1 shows the trend of average travel times for each 76.2 m section under peak and off-peak traffic conditions. Each travel time is an arithmetic average from 5 min aggregated travel time using twelve 5 min intervals and 25 replications. While no apparent congestion is identified at the off-peak hour, severe congestions were observed at the peak hour. This clearly presents challenges in locating sensors. 5. Genetic algorithm (GA)-based optimization program
A GA-based optimization program is developed to find optimal sensor locations, minimizing discrepancies between simulated travel times (i.e., surrogate ground truth travel times) and esti-matedtraveltimes.AGAisawidelyusedoptimizationtoolinspired by natural selection and evolution and has been applied to mathematical optimization, where a population of candidate solutions  X  X  X volves X  X  toward an optimum (Holland, 1975) .
The GA-based optimization program was built with a MATLAB 7.0 (Mathworks, 2006) . General assumptions of genetic algorithm are commonly applied to the GA optimization program: The initial population is randomly generated.

Individual chromosomes are evaluated according to the fitness function.

Individual chromosomes are selected for reproduction on the basis of fitness; the higher an individual chromosome, the more likely it is to be selected.

Reproduction of chromosomes to create the next generation is achieved by  X  X  X reeding X  X  between pairs of chromosomes using thecrossoveroperatorandthenapplyingamutationoperatorto each of the offspring.

Fig. 2 shows a flow chart of the GA-based optimization program and the procedure is discussed in the following section. It is noted thatthe  X  X  X dditional numberof loop detectors X  X  determines if the GA completed all cases of the loop detector numbers. 5.1. Initial generation
An individual chromosome consists of 594 binary digits corre-sponding to 594 speed data collection points (or sensor locations) in the VISSIM network. In other words, a binary value of each digit in a chromosome represents whether a corresponding sensor is active ( X  X 1 X  X ) or inactive ( X  X 0 X  X ). For instance, if the GA program optimizes for sensor locations with 60 sensors, every chromosome is destined to have 60 active digits randomly arranged across 594 digits at the beginning. 5.2. Estimation of travel time
The GA based optimization program calculates estimated travel times using speed data measured at each active sensor. The binary value of individual chromosomes defines activation of a sensor by havingeither X  X 1 X  X  X oractiveor X  X 0 X  X  X orinactive.Toestimatetraveltime usingspeeddatafromactivesensors,itisassumedthatspeeddataat the current active sensor can represent adjacent freeway segments about a half-distance between the current sensor and the next available active upstream and downstream sensors. Fig. 3 illustrates howthe travel timeestimationworks.First,the programfindsspeed fromthenearestactivesensors(i.e.,bothupstreamanddownstream sensors) and estimates travel time using Eq. (1). In fact, the program uses the half-distance approach  X  one of the commonly used extra-polation approaches in the estimation of travel time (Smith, 2004) .
Binary tournament selection &amp; Elitism
Uniform crossover with mask randomly
Replace old population with new one
It is noted that 76.2 is converted in meters from 250 ft, the sensor spacing, and 0.2778 (  X  1000/3600) is a conversion factor from kilometers per hour to meters per second.

TT where i is the index for active sensor number; n the number of activesensors; ko i thenumberofinactivesensorsrepresentedbyan active sensor i ; v i the 5 min average speed at active sensor i (kph); and TT est the total estimated travel time over the entire network (s). 5.3. Calculation of fitness
The GA requires a fitness function to evaluate the performance of an individual chromosome. As shown in Eq. (2), the fitness function is based on the mean absolute relative error (MARE). The larger the fitness of an individual chromosome, the more accurate the travel time estimated with the individual chromosome. fitness  X  1 TT real TT est TT where TT real is the total simulated travel time (s); TT est estimated travel time (s); and 9 TT real TT est 9 / TT real relative error (MARE). 5.4. Tournament selection and elitism
A binary tournament selection method was adopted where two individuals from a previous generation were randomly selected and compared by their fitness values, and the individual with the larger fitness was selected for crossover operation. This selection continues until a predetermined number of winner chromosomes are prepared for next generation reproduction. Furthermore, elitism was applied so that the best individual, namely elitist, from a previous generation is preserved within next generation without being changed. 5.5. Uniform crossover and mutation
A uniform crossover method was used to reproduce offspring chromosomes of next generation. A pair of offspring chromosomes is produced by swapping partial genes between a pair of selected chromosomes based on the binary masks generated at random. The mutation flips a digit of a chromosome randomly with a prede-termined mutation rate. Once the mutation is done, the procedure of creating new population is completed. It is noted that an elitist chosen by the elitism skips mutation to preserve the best solution to next generation. 6. GA optimization
This section describes the GA parameters used in the optimiza-tion of sensor locations and the number of sensors using the peak and off-peak traffic conditions. 6.1. Parameters of genetic algorithm
It is generally understood that the GA parameters are largely determined on the basis of trial and error. The population size should not be too small such that GA can adequately cover the search space, while the number of generations can be determined by monitoring the convergence of the GA. In addition, the mutation rate should be relatively small as large mutation rates could become random searches. The following parameters were initially determined for the proposed GA optimization runs.
 Population size  X  200 Generation number  X  20 Mutation rate  X  0.05
Uniform crossover 6.2. GA optimization results
Fig. 4 illustrates that solutions from the GA based optimization program converge to optimum through 20 generations. This convergence plot was established making three independent GA optimization runs for sixty sensor locations. The three GA runs were made to ensure that GA converges consistently. The three curves show that the GA based optimization program finds an optimum solution quickly within seven or so generations. As expected, best fitness value never decreases due to elitism.
By running the GA program in the sensor ranging from 40 to 90 by 10 detector increments, optimal solution (OS) per the number of sensors was attained and the corresponding travel time estimation performance was plotted. Additionally, for the sake of comparison, constant spacing (CS) cases where sensors were deployed roughly at constant spacing as practiced in the field were evaluated using the same number of sensors ranging from 40 (i.e., about 4.5 km spacing) to 90 (i.e., about 0.5 km spacing). Fig. 5 (a) shows the optimal performance obtained by the GA optimization program and the performance of using CS. The GA performed very well showing better than 94% accuracy (or 6% or less error). Given that these performances were based on two representative traffic ko = 5 Max. fitness value conditions, it is important to evaluate the performance of optimal sensorlocationsundervarioustrafficconditionsincludingdifferent demand levels and with non-recurrent congestion. 7. Evaluations and results
The optimal sensor locations determined by the proposed approach was evaluated in several traffic conditions such as increased demand and non-recurrent congestion. 7.1. Extra data sets and traffic conditions
Extra data sets were prepared under various traffic conditions characterized by traffic volume, time of day, and incident duration (see Table 2 ). These include midday traffic, 10% and 20% increased traffic conditions for peak hour and off-peak hour, respectively, and 10 and 30 min incident at peak hour and off-peak hour, respectively.

To consider variationsinday-to-daytraffic,25replications were made for each traffic condition. Traffic volumes at midday hour were made up by taking an average between peak volume and off-peak volume. Increased volume condition was created simply by increasing all traffic volumes by 10 and 20%. Two incident cases with durations of 10 and 30 min were designed for both peak and off-peak hours.

The hypothetical incident was modeled with two features provided by the VISSIM simulation model: signal head and reduced speedzone.Asignalheadfeaturebasicallyprovidedforthepurpose of traffic control at signalized intersection is used to mimic the location of incident such that vehicles are forced to stop in front of the signal head located in the middle of link when it turns on red light during pre-scheduled incident duration. Given that incidents generaterubberneckingeffectintherealworld,itismodeledwitha reduced speed zone that makes vehicles within the zone travel at speedsrandomlyselectedamongthepredefinedspeeddistribution (15 X 20 kph) instead of at normal desired speeds or any speeds adaptive to the existing traffic conditions around the zone. The red light and reduced speed zone were activated during the duration of incidents representing incident detection time, response time, and clearance time. To this end, both 10 and 30 min were selected to replicate non-recurrent incidents in evaluating the performance of the proposed approach. 7.2. Descriptive statistics of evaluation data set
Table 3 presents descriptive statistics of simulated travel times from the evaluation data sets. Increased traffic volume produced higher mean and standard deviation for simulated travel times.
There is no significant difference in simulated travel times between 10 min incident and 30 min incident at peak hour. Travel times with 10 min incident at peak hour are much longer than the travel time at the peak hour of the base condition. At off-peak hour, travel time increases and speed decreases with traffic volume. Simulated travel times with incident at off-peak hour are significantly different between 10 min incident and 30 min incident. 10 min incident does not appear to have a significant impact on travel time in the base condition at off-peak hour. 7.3. Evaluation of optimal solution and constant spacing
Travel time estimation performance of each optimal sensor location for the ranges of sensors for eleven different traffic conditions is presented in Table 4 . Optimal solutions produced 90% or better accuracy for almost all traffic conditions. Especially, 50.0 60.0 70.0 80.0 90.0 100.0 Performance 50.0 60.0 70.0 80.0 90.0 100.0 Performance 60 sensor optimal solutions produced performance above 90 under all traffic conditions and the highest performance over all traffic conditions.

Theconstantlyspacedsensorsforthenumberofsensorsranging from 40 to 90 were evaluated with the same eleven traffic conditions to compare with the optimal solutions. Table 5 shows that in most cases travel time estimation performance of the OS is better than that of CS under all traffic conditions. 7.4. Statistical hypothesis test between optimal solution (OS) and constant spacing (CS)
A statistical hypothesis test was performed to compare travel time estimation performance of both OS and CS. Both null and alternative hypotheses are defined as follows:
H : x 1  X  x 2
H : x 1 o x 2  X  3  X  where x 1 is the mean fitness valueof CS for the givendensity, and x the mean fitness value of OS for the given density.

The t -test result is presented with p -value in Table 6 and shows that there is statistical significance suggesting that OS has better performance in estimating travel timethan CS does at each number of detectors except for the detector case with forty sensors.
Thus, the results clearly indicated that travel time estimations could be significantly improved if an optimization approach was adopted. However, one question that needs to be answered is  X  X  X ow much better is acceptable and practically meaningful? X  X  Obviously it depends on the applications  X  real time route guidance would require more accuracy than off-line performance evaluation for transportation planning purposes. In the case of 60 sensor cases (roughly 0.8 km spacing), the mean absolute relative error over 12.5 min time intervals and 25 replications is less than 10% for all the traffic conditions considered as shown in Fig. 5 (b). Its standard deviation across these conditions was calculated to range from 0.018 to 0.026, which indicates the errors do not spread widely and the optimal solution of the case of 60 sensors performs consistently reliable estimations of travel time. Interestingly, the performances under 10 and 30 min incident cases were still less than 10%, while constant spacing case showed quite inferior performance for most traffic conditions. 8. Conclusions and recommendations
This paper proposed an optimization approach to determine sensor locations for better estimating the travel times. The proposed approach was applied using a microscopic traffic simula-tion model with a 45 km-long freeway segment. The optimal sensor locations determined from two representative traffic con-ditions were compared with those of constant spacing case by evaluating the travel time estimation performance using 11 different traffic conditions including increased demand and non-recurrent congestions.

The study results clearly indicated that the travel time estima-tion performance can be improved such that average mean absolute relative error was not worse than 10% compared to around 15% for the constant spacing case. In addition, it was found that the best travel time estimation performance was achieved at 60sensors (i.e., roughlyequivalent to0.8 km ora half milespacing), which corresponds to current state of practice in the sensor spacing.
 It is noted that the sensor locations determined by the proposed GA optimization were solely based on the microscopic traffic simulation model. Thus, to ensure successful field implementation, is envisioned that the existing non-intrusive sensors such as radar, video, and acoustic sensors can be relatively easily relocated, while loop detectors should be newly deployed if additional coverages are needed.

Among the recommendations for future research, another case study could be performed to model stochastic location and dura-tion of both minor and major incidents and evaluate more cases accommodating irregularity of traffic patterns caused by non-recurrent incidents. Additional research efforts could be made to apply the GA optimization program to find optimal solutions combined with certain strategies of interest, such as the require-ment of loop detectors near merge/diverge areas, recurrent con-gestion areas, etc. It is recommended that the GA optimization program be evaluated with extensive real data from freeways for the validation of its performance. In addition, a cost-benefit analysis along with the GA based optimization program could help decision makers determine the most cost-efficient number and location of loop detectors in planning a new freeway or assist thosewhowanttoknowwheretoaddmoredetectorstoanexisting freeway as cost efficiently as possible. As a potentially promising technique for the future, a similar approach could be applied to the development of an optimization program that is capable of finding optimal locations of toll tag readers, provided that their market penetration level is adequate.
 References
