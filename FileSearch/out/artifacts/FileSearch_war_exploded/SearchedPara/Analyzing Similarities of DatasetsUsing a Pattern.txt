 One of the reasons for the effectiveness of frequent pattern methods is that some of the frequently occurring patterns can capture crucial aspects of the underlying semantics of the data. There exist many techniques to characterize transaction as An interesting question that follows is the quantification of the similarity between two sets of patterns. Such a similarity measure can, in many ways, allow us to compare different data sets by comparing the characterizing subsets of patterns representing them. This would be useful, e.g., in change detection and classifica-tion. In this work, we address this problem of quantifying similarity between two sets of patterns, where patterns could be itemsets or serial episodes. as the Bag of Words (BoW) representation [ 3 , 9 ]. For example, in a classification application, we can first discover a good representative subset of patterns (called a dictionary) from the training data. Then any given data instance can be repre-sented as a feature vector whose dimension is same as the size of the dictionary. Each component of the feature vector specifies presence or absence (or number of occurrences) of the corresponding dictionary pattern in the data instance. The feature vectors can then be used to find similarities between different data instances.
 The BoW representation treats different patterns in the dictionary as indepen-dent features and it cannot take into account any similarities between different patterns in the dictionary. When comparing datasets, patterns representing dif-ferent datasets may be quite similar without being exactly the same. Hence, what is desirable is a measure that gives weightage to structural similarity between pat-terns such as the number of shared subpatterns. In addition to structural similar-ity between patterns, we also need to consider their relative importance to data. A pattern occurring in a dataset with a high frequency may be more important as opposed to one with low frequency.
 There exists a host of methods for comparing different types of data. Most of the methods are based on (probability) model comparison or comparison of sets of patterns representing the data. [ 8 ] maps vectors belonging to a set, to a Hilbert space and fits a Gaussian distribution to the set using Kernel PCA. The kernel between two sets of vectors is then defined as the Bhattacharya X  X  measure of affinity between Gaussians. A similar kernel, where a Gaussian Mixture is fitted over a Hilbert space is proposed in [ 12 ]. Similarity measures for structured data like graphs, consider the structural similarity of patterns [ 4 ]. Even though subgraph isomorphism is NP-Hard, there exists efficient graph kernels which look at simpler substructures [ 4 ]. Two such polynomially computable graph kernels areproposedin[ 12 ]. For string comparison, string kernels based on the counts of shared substrings have been proposed in [ 11 ].
 However, in the context of episodes or itemset patterns, similarity measures based on comparison of patterns, usually do not take into account structural similarity of patterns; rather they look at the collective similarity such as the amount of data the collection of patterns share [ 17 ]. Similarity measures con-sidering the amount of compression achieved by different sets of patterns in representing the data have also been considered for transaction data [ 14 , 15 ]. In this paper, we propose a kernel, called the Pattern Set Kernel ing two sets of patterns . We define this kernel for serial episodes and itemsets. We first define what we call a Pattern Kernel , which is a measure of similarity between two patterns. The measure depends on the extent of subpatterns shared by the two patterns along with frequencies of the patterns. Pattern Set Kernel is then defined using the Pattern Kernel. Even though, our pattern kernel is based on the number of common subpatterns, which could be exponential, we present efficient algorithms for calculating the kernel; complexity of which grows only as the product of lengths of the two patterns. We demonstrate the effectiveness of this new measure of similarity through extensive empirical studies. The rest of the paper is organized as follows. Section 2 presents the Pattern Kernel for serial episodes and itemsets. We present the Pattern Set Kernel in Sect. 3 . Section 4 gives the simulation results showing the effectiveness of our kernel and we conclude the paper in Sect. 5 .
 In this section, we present the kernel for a pair of patterns. We first define the pattern kernel for injective serial episodes, and then extend it for itemsets. 2.1 Episode Kernel: Pattern Kernel for Injective Serial Episodes In the episodes framework, the data, referred to as an event sequence tuple ( E i ,t i ) (called an event ), E i denotes the event-type and t of occurrence of the event. E i  X  X  , a finite alphabet, and t  X  , is denoted as ( e 1  X  e 2  X   X  X  X   X  e k ) where e i  X  X  , subepisode of  X  , denoted as  X   X  , if there exists integers i m  X  k and 1  X  i An episode  X  =( e 1  X  e 2  X   X  X  X   X  e k ) is called an injective serial episode e ,  X  i, j, i = j . In this paper, we call injective serial episodes as serial episodes (or just episodes). We denote the frequency of  X  (in a given data sequence) as fr (  X  ) 2 . E + denotes the set of all serial episodes of size 1 or more. episode to a vector in the feature space |E + | .Weuse  X   X  X  components of  X  . The mapping  X  for an episode  X  is given by Note that since K EK (  X ,  X  )=  X  (  X  ) T  X  (  X  ), those coordinates  X  for which either  X  (  X  )or  X   X  (  X  ) are zeros do not contribute to the kernel. Thus we obtain
K EK (  X ,  X  )= Also note that the size of the set, {  X  :  X   X ,  X   X  } in Eq. ( 1 ) could be exponential in the size of alphabet. For instance, if  X  =  X  , with  X  , being an M -node episode, then |{  X  :  X   X ,  X   X  }| =2 M  X  1. However, the size of this set can be efficiently calculated in O ( MN ) steps, where M and N are lengths of  X  and  X  respectively, as described below.
 Efficient Computation of K EK . We rewrite the episode kernel as K fr (  X  )  X  fr (  X  )  X  K 1 (  X ,  X  ) where K 1 (  X ,  X  )= |{ For an n node serial episode  X  =(  X  [1]  X   X  [2]  X   X  X  X   X   X  [ n ]), the i prefix subepisode of  X  is defined as the serial episode (  X  [1] for i  X  n . We denote the i -node prefix subepisode of  X  by  X  Let  X  and  X  be two serial episodes of length M and N respectively. We assign K ( X, Y ) = 0, if X =  X  or Y =  X  , where  X  denotes empty or null episode whose size is zero. The iterative algorithm for computing K 1 (  X ,  X  ) is as follows for i =1 ..M do for j =1 ..N do In the algorithm above, both  X  [1 .. 0] and  X  [1 .. 0] K (  X  plexity of the above algorithm is O ( MN ).
 Proof of Correctness for Calculating K 1 . We first note that the episodes we consider are injective episodes. Thus, each event-type occurs in an episode at most once. Suppose we want to find K 1 (  X  [1 ..i ] , X  [1 ..j ] based on the values of  X  [ i ]and  X  [ j ]. 1. Let  X  [ i ]=  X  [ j ]= A .Let A be the set of all subepisodes common to  X  and  X  [1 .. ( j  X  1)] . Episodes in A are also common subepisodes to  X   X  [1 ..j ] . For every episode  X  and  X  [1 ..j ] . The one-node episode A is an obvious common subepisode for  X  [1 ..i ] and  X  [1 ..j ] and no other subepisode is common to both  X  2. Let  X  [ i ] =  X  [ j ]. Any subepisode  X  common to  X  [1 ..i ] to any one of the three mutually exclusive sets (a) A = {  X   X  X  + :  X   X  [1 ..i ] , X   X  [1 .. ( j  X  1)] , X  [ i ] is the last event of  X  (b) B = {  X   X  X  + :  X   X  [1 .. ( i  X  1)] , X   X  [1 ..j ] , X  [ j ] is the last event of  X  It is easy to see that neither  X  [ i ]nor  X  [ j ] is the last event for  X  in
A are formed by aligning  X  [ i ] (which forms the last event of the episodes) and that A , B , C are mutually exclusive, we have This completes the proof of correctness of the algorithm.
 2.2 Itemset Kernel: Pattern Kernel for Itemsets In this section, we define Pattern Kernel for itemsets. Itemsets are patterns obtained from transaction data. Let I = { i 1 ,i 2 ,...,i where t i  X  X  ,  X  i . Suppose we give an ordering for the items in items in the itemsets based on that order. Then an itemset correspond to the unique injective 3 serial episode ( i j 1 itemset kernel is exactly episode kernel on these injective serial episodes. for  X  and  X  ,whichis  X   X   X  . Then any common subset of  X  and  X  is a subset of  X   X   X  and hence the number of shared itemsets between  X  and  X  is 2 Hence, itemset kernel, denoted as K IK for two itemsets  X  and  X  is K fr (  X  )  X  fr (  X  )  X |{  X  :  X   X ,  X   X  }| = fr (  X  )  X  fr (  X  ) Now, using the pattern kernel, we define the Pattern Set Kernel for sets of patterns. We denote the pattern kernel (for both the pattern types) as K Definition 1. The Pattern Set Kernel, denoted as K PSK , between two sets of patterns F function that maps each set of patterns F where E + denotes the set of all patterns of size one or more.
 Based on the context, E + could represent the set of serial episodes or itemsets of size one or more. For each F ,the  X  coordinate of  X  is given by We now show how K PSK can be computed using the underlying pattern kernel. Proposition 1. K PSK ( F 1 , F 2 )=  X   X  X  Proof.
 Pattern Set Kernel between two sets of patterns is thus the summation of Pattern Kernels of pairs of patterns from F 1 and F 2 . 3.1 Complexity for Finding the Pattern Set Kernel Let |F | denote the number of patterns in set F . Then, based on Proposition 1 , K of episodes, assuming that the maximum size of the episodes in and N respectively, each Episode Kernel ( K EK ) calculation would cost O ( MN ). For itemsets, the itemset kernel ( K IK ) computation would cost only O ( M + N ). Hence the total cost of computing K PSK ( F 1 , F 2 )is O ( and O ( |F 1 ||F 2 | ( M + N )) for itemsets. In this section, we show the effectiveness of Pattern Set Kernel (PSK), where the patterns are either itemsets or episodes. We show the effectiveness of PSK, by comparing data from different sources. We also show the effectiveness of PSK for change detection and classification. All algorithms are implemented in Matlab and experiments were executed single threaded on an Intel i7 4-core processor with 16 GB memory running over linux OS.
 We define a measure of similarity between two sets of patterns as which is the usual cosine similarity measure between two vectors, normally used with kernels. We also define a corresponding distance metric between two sets of patterns as dist ( F 1 , F 2 )=1  X  ( sim -score ( F 1 4.1 Measuring Similarity Between Sequences We first consider sequence data, where patterns are injective serial episodes. Of various techniques for summarizing sequences using serial episodes [ 7 , 9 , 13 ], we use the method CSC-2 [ 7 ], which retrieves a representative set of injective ser-ial episodes from any sequence data. The algorithm does not need any user spec-ified parameter such as frequency threshold. We consider the Coupled Conveyor Systems (CCS) [ 2 ] for our first set of sequence data. CCS are reconfigurable con-veyor systems for moving material/packages from input sources to output desti-nations. They are built with units called Segments and Turns that each operate autonomously. A Segment moves packages over a predetermined length over its belt. A Turn is a unit that can serve as merger or splitter for package flow. The system can be configured to different topologies that determine different paths over which packages move. There are sensors at each turn and segment that record events of packages moving through them with time stamps. Data mining over such data streams is useful for remote monitoring and visualization of such systems. The data we consider here is obtained through a detailed discrete event simulator of CCS [ 2 ]. (See [ 7 ] for more details on this application).
 and b. The topologies share the same subpaths, but the actual paths are different in different topologies. For our simulation, for each topology, we generate two sets of sequences, corresponding to two different input rates (assumed Poisson) of 0.2 and 0.8, by running the simulator for a period of 5000 s each. Each such stream is cut into 5 disjoint chunks of time span 1000 s.
 cies normalized to the chunk size) for each chunk using CSC-2 algorithm [ 7 ] and compare these sets using the Pattern Set Kernel. Three sets of experiments were conducted, comparing ( a ) chunks from the same sequences (same topology and same input rates) ( b ) chunks from different sequences generated using the same topology, but different input rates and ( c ) chunks from different sequences generated from different topologies, but with the same input rate. Some repre-sentative samples of these results are shown in Tables 1 , 2 and 3 . Each chunk is referred in the tables using the notation, T i -rate -chunknumber .Ascanbe seen, chunks from the same topology and same rate have very high similarity scores (greater than 0.9), chunks from same topology but different rates have somewhat lower scores (greater than 0.55) and chunks from different topologies have much lower similarity scores (less than 0.38). The results show the PSK based similarity measure is able to capture the difference in characteristics of the datasets.
 4.2 Pattern Set Kernels for Classification Next we present results to show the effectiveness of PSK as a distance metric for classification. We show results on sequences from the CCS problem and a few benchmark sequence datasets in the domain of sign languages [ 9 ]. We compare performance of three classifiers. Our first classifier is a PSK based Neighbors (KNN) classifier, where distance metric used is dist , introduced earlier. We denote this classifier as PSK-KNN. The second classifier is also a KNN classifier, but has a Bag of Word (BoW) representation of data sequences using selected episodes of all classes as dictionary. The third classifier we use is linear Support Vector Machine (SVM) classifier, where again each sample is represented using BoW representation.
 For data sequences in CCS problem, we compare performance of the three classifiers on predicting which topology a given data chunk comes from. The training data consists of data chunks generated from different topologies and we use CSC-2 algorithm for selecting a representative set of serial episodes for each class. Test data chunks are also characterized by episodes discovered from them. For this classification problem, we generated three datasets. The first dataset consisted of sequences from Topologies T 1and T 2 (Fig. 1 ). The data samples for the two classes are sequences of chunk size 50 s generated from the respective topologies. The second data consisted of sequences from the same topology T 1, but with different input rates, 0.5 and 0.8. The two classes correspond to the two rates. The third data is similar to the first, except that chunk sizes have SVM classifier, we varied the error cost parameter  X  C  X  over a range of values. The results we present are the best among all parameter values. We show the classification results on CCS data in the upper part of Table 4 . We see that PSK based KNN classifier outperforms other classifiers for all three datasets. The other datasets we consider contain labeled small sequences, correspond-ing to different actions in sign language and have been used earlier as benchmark sequence datasets [ 9 ]. Extracting characterizing serial episodes from very small sequences is not generally possible. Hence, we generate new datasets, where indi-vidual sequences corresponding to a class are generated by randomly selecting 10 sequences (of the original dataset) of the same class and concatenating them. We experimented with five such benchmark datasets all of which are multi-class. (The SVM classifier for the multi-class scenario is implemented using the one versus rest approach). The results are shown in the lower part of Table 4 .Wesee that, except for  X  X slbu X  dataset (where it achieves slightly lower accuracy), the PSK-KNN achieves higher accuracy as compared to other classifiers. 4.3 Change Detection in Streaming Data from Conveyor Systems In this section, we use the PSK based distance metric to detect changes in streams obtained from different topologies of the conveyor system. We generate sequences from three topologies, T 1, T 2and T 3 (T3 is another topology, very similar to T 1and T 2 such that they share many subpaths, yet have different sets of actual paths). For the change detection experiment, we generate data streams consisting of a random mixture of data chunks from two different topologies. For this, we adopt the following scheme. We randomly choose a number r between 5 and 10, and select the next r chunks, alternatively from one of the two topologies. We continue this process until we get 100 chunks which forms the data stream for change detection. Each chunk is compared with the previous one using the metric, dist , to see how dissimilar they are. Results are shown in Fig. 2 . Vertical lines correspond the actual points of change in the stream. It is easy to see that the dist metric using PSK is very effective in detecting points of change. using suitable thresholds on distances. We generated 10 instances of random streams for T 1  X  T 2, T 1  X  T 3and T 2  X  T 3. The thresholds are calculated from training data as follows. We take the 20th percentile of dist values corresponding to change points in the training data as the distance threshold the sequence of  X  X erivatives X , which are changes in successive distances. We then calculate derivative ( i )  X  derivative ( i + 1). As is easy to deduce, at points of change, these correspond to a fall from a high value of derivative (at the point of change) to a low value of derivative (at the subsequent point). We again take the 20th percentile value of the fall (in training data) as prediction, we report a change, when distance and fall of a point is above 90 % of both thresholds. We report average precision and recall values in Table 5 .We see that PSK based distance measure achieves high precision and recall values. 4.4 Measuring Similarity Between Transaction Data In this section, we show the effectiveness of Pattern Set Kernel, when patterns are itemsets. We use the Krimp algorithm [ 16 ] for mining itemsets from transaction datasets. Krimp is an MDL based lossless compression algorithm, which outputs a set of itemsets, that summarizes transaction data well. The dataset we consider is 2-class mushroom transaction dataset [ 10 ]. Mushroom dataset consists of 8124 transactions, each having 22 categorical attributes. Transactions belonging to each class are equally divided into 5 chunks of transaction data. The classes are denoted by c 1and c 2 respectively. Since Krimp initially mines the set of frequent itemsets before finding the best summarizing subset of itemsets, we specify the frequency threshold as 12 % (for mining) of the total number of transactions in each chunk. Each chunk is referred using the format class -chunknumber . Tables 6 , 7 and 8 show the results on the different chunks. We see that sim -score s are higher between chunks belonging to the same class. The sim-ilarity scores between the chunks of different classes are extremely low, thus showing the effectiveness of our kernel based similarity score.
 Remark 1. We would like to point out that we have not compared our similarity measure with any other known similarity measures for comparing datasets. Even though there is a sequence kernel for comparing sequences [ 11 ], it is computa-tionally inefficient for long sequences. The rest of the similarity measures are not directly comparable with the measures we proposed here. Thus, although methods for comparing sequences such as strings are known, as per our knowl-edge, this is the first work, wherein structural comparison of sets of patterns is considered for analyzing similarities between sequential or transaction data. The goal of frequent pattern mining is to gain useful information about data. Recently, many algorithms have been proposed for characterizing or representing when data is represented by a set of  X  X haracteristic X  patterns, a natural question is that of comparing sets of patterns representing the datasets in order to gain insights into the similarity of different datasets. In this paper, we have looked at this problem of pattern set comparison. We proposed a Pattern Kernel for quantifying similarity between pairs of patterns and then used it to define a Pattern Set Kernel for comparing sets of patterns. Pattern Kernel was defined for injective serial episodes and itemsets. The Pattern Kernel value depends on number of common subpatterns shared by the two patterns. We also presented efficient algorithms for calculating these kernels.
 datasets as well as for a 2-class transaction dataset. We defined a similarity score and a distance measure using Pattern Set Kernel and used it for scenar-ios involving direct comparison of datasets, change detection and classification. On many sequence datasets as well as on the transaction dataset, it is seen that our kernel-based similarity measure is very effective in capturing similari-ties/differences between data.
 analyzing sequence data [ 18 ]. While we did not consider these in this paper, our pattern kernel can be defined for sequential patterns also [ 5 ]. In the case of sequential patterns we need to assume a form of injectiveness that is somewhat restricted, to be able to compute the kernel efficiently. We would be further exploring this and other issues of extending our pattern kernel to all types of frequent patterns in our future work. The field of pattern mining has opened-up a new view of data through significant local patterns that occur in data. Similarity measure such as Pattern Set Kernel proposed here would prove to be very useful in utilizing such a view of data in many applications.
