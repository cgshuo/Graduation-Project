 Relation extraction is the task of extracting semantic relations X  such as synonymy or hypernymy X  X etween word pairs from corpus data. Past work in relation extraction has concen-trated on manually creating templates to use in directly ex-tracting word pairs for a given semantic relation from cor-pus text. Recently, there has been a move towards using machine learning to automatically learn these patterns. We build on this research by running experiments investigating the impact of corpus type, corpus size and different param-eter settings on learning a range of lexical relations. Categories and Subject Descriptors: I.2.6 [Learning]: Knowledge acquisition General Terms: Algorithms, Experimentation Keywords: relation extraction, information extraction
Information Extraction (IE) is the task of abstracting away from surface linguistic realisation in a text to expose its underlying informational content, usually relative to a predefined set of semantic predicates. Examples of typical IE tasks are fact discovery of corporate mergers and acqui-sitions from news articles (e.g. Company X was acquired by Company Y at time Z ), and identifying that animal is a hypernym (= super-type) of dog from patterns of corpus co-occurrence. In this paper we focus on the latter task of relation extraction, over a range of binary lexical relations. That is, for a given word 2-tuple ( x;y ) and a lexical rela-tion rel , we predict whether rel ( x;y ) holds or not, based on analysis of corpus co-occurrences of x and y .

There are several motivations for relation extraction. The first is to provide knowledge for use in applications such as question answering or text summarisation, where relational data can enhance performance. The second is to support the (semi-)automatic construction of semantic taxonomies, e.g. for specific domains or under-resourced languages [9]. Se-mantic taxonomies are widely used across a range of natural language processing tasks. Despite their usefulness, they tend to have low coverage due to the need for manual con-struction. With high-accuracy customisable relation extrac-tion, we can hope to greatly reduce the manual overhead associated with constructing semantic taxonomies.
The main contribution of this work is to build on the work of [8] on supervised relation extraction, over a larger range of relation types (hypernyms, synonyms and antonyms), ex-ploring the impact of the choice of corpus, size of training data, and various parameter settings on extraction perfor-mance. For a given noun pair and relation type, the method performs automatic analysis of the patterns of sentential co-occurrence of the given nouns, and learns a classifier based on a set of training instances. Our results over the three lexical relations are some of the best achieved to date.
This paper is a shortened version of [11], which we refer the reader to for full methodological and experiment details.
There are several approaches to relation extraction. One is to take co-occurrence feature vectors representing the con-text of usage of each word, and use clustering to induce sets of words with similar co-occurrence properties. Finally, the clusters are labelled in some way. For example, there is a good chance that dog , cat , and bird will be grouped in the same cluster. If the cluster were then labelled as animal , it could be inferred that bird is a hyponym of animal [7].
The other main approach is to use lexico-syntactic pat-terns, as popularised by [4] for hypernym learning. The patterns are selected to be strong indicators of a given lexi-cal relation between a pair of words, e.g. X, such as Y which strongly indicates that X is a hypernym of Y . We adopt this approach in this paper, with the key difference that we do not use a predefined set of high-precision patterns, instead relying on our classifier to identify them for a given relation; the learned patterns provide both positive and negative ev-idence for a given noun pair belonging to that relation. The major drawbacks with this approach are: (1) patterns are expensive to generate  X  they require manual labour and do-main expertise; (2) not all relations can be identified by a set of reliable patterns; and (3) while the patterns generally have high precision, they tend to suffer from low recall. Re-cent research has attempted to address the drawbacks men-tioned above by iteratively bootstrapping these patterns [10] or automatically identifying them from text [8].

The research mentioned above exemplifies supervised re-lation extraction, because it requires hand-labelled data or seed examples to learn a given relation type. If a new rela-tion type is to be targeted or a new domain explored (e.g. extracting the book  X  author relation), effort is required to source relevant training instances in sufficient quantities. In an attempt to relax this requirement, there has been increas-ing growth in semi-supervised IE, which leverages a handful of seed instances and large amounts of unannotated data [2].
More recently, there has been work on a new style of IE termed OpenIE [1]. Unlike traditional IE systems that re-quire a specific relation to be predefined, OpenIE offers a means of extracting word pairs that are associated with an unspecified relation type.
Our proposed approach to relation extraction builds di-rectly off the work of [8]. Specifically, we tackle the task of noun relation extraction over a range of relation types by implicitly learning patterns which are positively and nega-tively correlated with a given relation type, in the form of a supervised classifier. To demonstrate the generalisability of the method, we experiment with three noun semantic re-lations: antonymy, synonymy and hypernymy. Further, to investigate the impact of the type and size of the corpus on the performance of the system, we experiment with two different corpora.

As discussed in Section 2, patterns play a vital role in the relation extraction task. We thus require some way of representing the different lexico-syntactic configurations in which noun pairs occur. Ultimately we are interested in exploring a wide range of possibilities of preprocessing and removing the assumption of a parser, but for the purposes of this paper we use a dependency parser to generate these patterns in the form of dependency paths. In this, we take the parse for each sentence, identify all of the nouns, and generate the shortest dependency path between each pairing of nouns. We collect together all instances of a given noun pair across all sentences in our corpus, and calculate how many times it occurs with different patterns. A subset of these noun pairs is then selected for annotation according to a given semantic relation.

Our next step is to build a classifier to learn which pat-terns are positively and negatively correlated with the rela-tion of interest. Tackling this problem as a machine learning task, we treat the noun pairs as our instances and the pat-terns of co-occurrence for a given noun pair as its features. We represent the frequency of occurrence of a given noun pair with a particular pattern as binary overlapping thresh-old buckets features, with thresholds defined by a power series of degree 2, i.e. f 1 ; 2 ; 4 ; 8 ;:::; g , up to the maximum frequency of occurrence for any noun pair for a given pat-tern.

We next select a subset of noun pairs which are known to occur with the given relation as positive instances, and a subset of noun pairs which are known not to occur with the relation, and use these to train our classifier. As this process will typically be carried out relative to a set of seed instances or semi-developed lexical resource, in practical applications we expect to have ready access to some number of positive instances. Negative instances are more of an issue, in terms of both distinguishing unannotated from known negative in-stances, and also determining the ideal ratio of positive to negative instances in terms of optimising classifier perfor-mance. In their research, [8] addressed this issue by taking a random sample of noun pairs and hand-annotating them, to get a feel for the relative proportion of positive to negative instances. This is a luxury that we may not be able to afford, however, and clearly slows down the development cycle. As part of this research, therefore, we investigate the impact of differing ratios of negative/positive training instances on our classifier performance.

In their work, [8] applied two filters: (1) noun pairs had to occur across at least 5 distinct patterns; and (2) patterns had to appear across at least 5 distinct noun pairs. In our work, we investigate how great an influence these parameters have on classifier performance across different relation types. We use two different corpora in our experiments: (1) the English Gigaword corpus, containing around 84 million sen-tences; and (2) the English Wikipedia July 2008 XML dump, containing roughly 38 million sentences after preprocessing. Note that Wikipedia is less than half the size of Gigaword.
We use MINIPAR in our experiments to parse sentences from the corpus. MINIPAR is a fast and efficient broad-coverage dependency parser for English [6]. It produces a dependency graph for each parsed sentence. Every word in a sentence is POS-tagged and represented as a node in the dependency graph; dependency relations between word pairs ( w 1 ;w 2 ) are represented by directed edges of the form: w ,POS w 1 : relation :POS w 2 : w 2 .

Since we are only interested in nouns, we first identify all nouns, and from this form the set of noun pairs. The pattern between a noun pair in our experiments is defined as the path of length four or less edges linking that noun pair. We generalise the patterns by removing w 1 and w 2 . Furthermore, we follow [8] in post-processing the output of MINIPAR to: (a) include  X  X atellite links X ; and (b) distribute patterns across all noun members of a conjunction.
WordNet v3.0 [3] is used as the source of instance la-bels in our experiments. This is done by first identifying all noun entries that appear in the semantic concordance (SemCor) file of WordNet. We then exhaustively generate all (directed) pairings of the 12,003 unique nouns obtained. For all noun pairings, we use WordNet to identify whether the pairing is in an antonym, synonym, or hypernym rela-tion (recursively up the WordNet hierarchy), based on the first sense of each noun. If this is found to be the case, we classify that pairing as a positive instance relative to the given relation. We classify a pairing as a negative instance iff the given relation does not hold between any of the senses (first or otherwise) of the two nouns. This leaves two sets of noun pairs, which we ignore in the experimentation de-scribed in this paper: (1) those where one or both nouns do not occur in SemCor; and (2) those where both nouns occur in SemCor but the given relation holds for a non-first sense.
All of our experiments are based on the BSVM machine learner [5], with a linear kernel and default settings otherwise ( C = 1 ; = 0 : 001).
As discussed in Section 3, [8] used three parameters in their original work. The first parameter is a threshold over the number of patterns a given noun pair occurs with: noun pairs are included iff they occur with at least n patterns. Table 1: Patterns used by the baseline system (for hypernymy, Y is a hypernym of X ) We denote this parameter by j np j n , and test three set-tings ( j np jf 5 ; 10 ; 20 g ). We expect noun pairs which occur across less patterns to be harder to classifier, and the clas-sifier performance to thus increase as the threshold value increases.

The second parameter determines which patterns are to be used as features in our data set, and acts as a means of feature selection. Only patterns which occur across at least m noun pairs are used in classification, which we denote as j pat j m . Here, we experiment with the following settings: j pat jf 5 ; 10 ; 20 ; 50 g . As this value rises, the feature space is thinned out but also becomes less sparse (as a given pat-tern will occur for more noun pairs).

The final parameter is the ratio of the negative to positive instances in our data. The ratio j N j = j P j = r denotes that there are r times as many negative as positive instances in our data (based on the post-filtered count of positive noun pairs). We always use all available positive instances when evaluating over a given parameter selection and re-lation type. The number of negative instances for a given relation depends on the j N j = j P j threshold and the number of positive instances for that relation. Given pos positive instances and the ratio j N j = j P j = r , we pick the top r pos most frequently appearing negative pairs in the data. We performed experiments with j N j = j P j = f 1 ; 10 ; 25 ; 50 ; 100
In the original work, the parameters were set to j np j 5, j pat j 5, and j N j = j P j = 50.

Evaluation in all cases is based on 10-fold stratified cross validation, and the performance statistics reported here are the average of the performance scores across the 10 folds. Throughout evaluation, we measure relation extraction per-formance in terms of precision, recall and F-score ( = 1).
The baseline for our experiments is a simple rule-based system that classifies a noun pair as having a given relation iff that noun pair occurs at least once in any one of the hand-crafted patterns associated with that relation. These patterns are listed in Table 1.

We use the result from [8] as our benchmark. Since they only evaluate their system on hypernym relation, we can only compare the results of the two systems for hypernyms. Their best system (using logistic regression algorithm) per-forms at an F-score of 0.348 for j np j 5, j pat j 5, and j
N j = j P j = 50.
In our experiments, our interest is in: (1) the performance across different relation types, corpora types and sizes; (2) the performance relative to a standardised baseline; and (3) the effect of the parameter settings on performance. Table 2: Performance for hypernym, synonym and antonym learning over Gigaword and Wikipedia (R = recall, P = precision, F = F-score; numbers in brackets are the ERR relative to the corresponding baseline)
First, the overall results across the three lexical relations for Gigaword and Wikipedia are presented in Table 2. In order to track the relative change in these values in a nor-malised manner, we calculate the ERR over the baseline, based on the following calculation: These numbers are presented in brackets underneath the corresponding precision, recall or F-score value.
In all cases, the precision and F-score are both well above the baseline, but recall actually falls below baseline for syn-onyms in particular, largely due to the overly-permissive baseline pattern (i.e. any pair of nouns which occurs in a coordinate structure is considered to be a synonym pair). Comparing the different lexical relations, hypernyms are harder to learn than synonyms or antonyms, largely because of the ancestor-based interpretation of hypernyms (meaning that organism is a hypernym of aardvark , for example) vs. the more conventional interpretation of the other two lexi-cal relations. Comparing Gigaword and Wikipedia, we see the Gigaword is superior as a source of training data for hy-pernym learning (esp. in terms of recall), but that otherwise there is relatively little separating the two resources (despite Wikipedia being less than half the size of Gigaword). We further investigate this effect in our next set of experiments.
We observe that the corpora have an interesting effect on the system performance. A consistent effect across all three lexical relations is that the number of positive in-stances observed in the data increases much more quickly for Wikipedia than Gigaword, because of its greater domain coverage and hence higher heterogeneity. In this sense, our original results have to be taken with a grain of salt: while we directly compare the recall, precision and F-score for the two corpora, the number of positive instances they are eval-uated over (and implicitly the number of negative instances, based on the j N j = j P j ratio value) is not consistent. As such, Wikipedia leads to a larger set of predictions with compara-ble precision, recall and F-score for synonyms and antonyms. Closer analysis of the results over hypernyms reveals that the recall is lower because the system is having to classify a larger set of noun pairs, including a higher proportion of lower-frequency, hard-to-classify pairs. Direct comparison is thus not fair, and further research is required to ascertain how the method is performing over noun pairs of different types. brackets are the ERR relative to the corresponding baseline)
Finally, we investigate the effects of the system param-eters in Table 3. With j N j = j P j (the ratio of negative to positive instances), we expect there to be an overall drop in the numbers as the ratio increases, as the negative instances progressively overshadow the positive instances.

Unlike the effects of changing the values of the other two parameters, the effect of changing the value of the j N j ratio parameter is substantial, with recall being particularly hard hit as the ratio increases. In fact, the recall actually drops below that of the baseline for j N j = j P j = 100 over hypernyms and synonyms, although the F-score is still com-fortably above the baseline. This situation can be explained by the fact that the output is increasingly biased towards the negative instances. The performance of hypernyms suffers the most out of all three relations. This can be explained by the large number of hypernym positive instances that we have in our gold standard compared to the number of positive pairs for the other two relations, meaning that the raw number of negative instances swamps the classifier more noticeably.

We experimented with different settings for j np j and j pat and observed that they tended not to affect the system per-formance. We also observed that the (marginally) best per-formance was achieved with the settings of j np j 5 and j pat j 5, as used by [8]. For full details, see [11].
As seen in the ERR figures in Table 2, our system outper-formed the baseline in terms of F-score in all cases, across all relations.

With the same parameter settings, our system outper-formed the system of [8] at hypernym extraction (0.654 (Gi-gaword) and 0.445 (Wikipedia) vs. 0.348 F-score). However, this is not a strictly fair comparison as we used almost 15 times (Gigaword) and 7 times (Wikipedia) the amount of the data as they used in their experiment, with different machine learning algorithms, and the membership of posi-tive and negative instances differed due to us enforcing the requirement that both nouns occur in SemCor.

In future work, we plan to investigate the effect of includ-ing noun pairs where both nouns occur in SemCor but the given relation holds over a second or lower sense for one or more of the nouns (a class which is currently excluded from evaluation somewhat artificially). We noticed that there are quite a number of noun pairs that possess this relation when we build our gold standard. Also, we plan to perform the experiment on other relations, such as meronymy, as well as going beyond nouns and WordNet.
We experimented with a great number of experiment set-tings for the pattern-based relational learning from corpus data. Building on the method of [8], we have established that the method can be applied successfully across different semantic relations, and gained insights into the effects of different parameterisations on classifier performance. The experiments produced highly encouraging results, and sug-gested a number of promising directions for future research. NICTA is funded by the Australian Government as repre-sented by the Department of Broadband, Communications and the Digital Economy and the Australian Research Coun-cil through the ICT Centre of Excellence program.
