 class is severely under-represented and overwhelmed by the majority class. In this case, the distribution of the classes is skewed. Subsequently, usual classification methods will generate poor results because the distribution is one of the most important factors that affect the performance [ 7 , 15 , 19 ].
 is balanced, and the misclassification cost is equal for both classes. However, there exists some cases that the class distribution is skewed and the misclassifi-cation cost is extremely unequal. Further, sometimes people focus more on the minority class because it usually contains more information and interest than the majority class. Let us take cancer diagnosis as an example, the number of patients who have cancer is much less than the number of healthy people in reg-ular checkups. It is obvious that the cost for misdiagnosing a healthy person to be sick, which only brings the person mental stress and more payment to further diagnosis, is much less than diagnosing a patient to be health, which may lead to the loss of the patient X  X  life. Therefore, when dealing with imbalanced data, the misclassification cost is one of the most significant factors that affect the process of learning. In addition to the algorithms, evaluation metrics also play important roles in imbalanced learning. Suppose there are 100 cancer patients out of 10,000 people, the normal classifier will tend to predict  X  X ealthy X , because even all predictions are  X  X ealthy X , the accuracy of this classifier is still as high as 99 %. Therefore, simply using the accuracy or error rate is not comprehen-sive enough to measure the performance of a classifier dealing with imbalanced data. Usually, three evaluation metrics for class imbalance problem, i.e. AUC, F1 and G-mean, will be used. In this paper, we focus on the binary classification problem, and following the convention, we treat samples in the minority class as positive and samples in the majority class as negative.
 ods have been proved to be effective. Several studies have shown that training on the balanced data set by sampling methods can achieve better overall classifica-tion performance than the original imbalanced one [ 11 , 21 ]. Usually, the sampling methods, such as random undersampling or oversampling, are integrated with ensemble methods, such as bagging or boosting, in order to overcome their draw-backs and provide more diversity to the boosted classifier [ 12 ].
 form each other, e.g. see a recent survey [ 12 ], in which RUSBoost [ 17 ] (under-sampling based) wins SMOTEBoost [ 9 ] (oversampling based) 22 times, draws 4 times and loses 18 times and UnderBagging [ 1 ] (undersampling based) wins SMOTEBagging [ 20 ] (oversampling based) 18 times, draws 1 time and loses 25 times (shown in Tables XX and XXI in [ 12 ]). It can be seen that the results generated by ensemble-based undersampling or oversampling highly depend on the data. In other words, some data has better performance with undersam-pling, while the other ones with oversampling. Therefore in terms of the sam-pling process, it is expected that the hybrid of undersampling and oversampling can take advantage of their properties. That is, the hybrid sampling generally outperforms each individual sampling method because undersampling and over-sampling are complementary to each other and cure the skewed distribution of class imbalanced data in different extents. In addition, no matter undersampling, oversampling or even hybrid sampling In this paper, we therefore propose a novel method for the class imbalance undersampling and oversampling. the data to achieve best performance. application requirement.
 To validate the effectiveness of the hybrid sampling scheme and sampling rate methods to cure imbalance, by randomly replicating data in minority class and discarding data in majority class, respectively. The drawbacks of them are that oversampling will easily cause overfitting and undersampling may discard useful data that leads to information loss. As an improvement to random oversam-pling, Synthetic Minority Over-sampling TEchnique (SMOTE) [ 8 ] synthesizes artificial data in the minority class instead of replication. Borderline-SMOTE [ 13 ] and ADASYN [ 14 ] improve SMOTE by assuming that the samples close to the borderline are more important, thus synthesize more data there. The idea of combining undersampling and oversampling has been mentioned in [ 20 ]. It com-bines undersampling with oversampling to create a training set with the same number of positive and negative samples. The number of samples in each class after sampling is determined by a predefined re-sampling rate a %.
 imbalanced problem themselves. Usually, they are combined with sampling meth-ods to utilize the diversity provided by sampling to enhance the ensemble classifier. A comprehensive review of ensemble methods for class imbalance problem can be found in [ 12 ]. OverBagging [ 20 ] and UnderBagging [ 1 ] com-bine random undersampling and oversampling with bagging, respectively. They adopt oversampling or undersampling after bootstrapping the training data to create a balanced training set. As an improvement of OverBagging, SMOTE-Bagging [ 20 ] combines SMOTE with random oversampling and the sampling rate of SMOTE increases in every iteration to provide more diversity. As the counterpart of bagging-based methods, SMOTEBoost [ 8 ] and RUSBoost [ 17 ] are boosting-based. They created balanced training set by SMOTE and random undersampling in each boosting iteration. After sampling applied, the sample weights are normalized. The following steps are the same as Adaboost [ 16 ]. IIV-otes [ 3 ] combines IVotes ensemble [ 5 ] and SPIDER [ 18 ] data preprocessing to obtain improved balance between the sensitivity and specificity for the minority class. Since training in the balanced data set is not guaranteed to produce the best result [ 11 ], the proposed HSBagging does not aim to create the balanced training set, but depending on a specified sampling rate p , which is different from the hybrid scheme in [ 20 ]. In HSBagging, the minority class is enlarged by p and meanwhile the majority class is shrank by p . Conducting undersampling and oversampling at sampling rate p at the same time can explore the best sam-pling rate from severe imbalance, slight imbalance to balance or even reversed imbalance (i.e. the minority class becomes majority after sampling). Since each data set tends to have different best sampling rate, it is necessary to estimate the sampling rate during bagging. HSBagging estimates the best sampling rate by Out-Of-Bag (OOB) estimate, which is used to estimate parameters in the bootstrapped set by leaving the samples not selected by bootstrapping as val-idation set. There are two advantages of using OOB estimate: (1) it acts as The proposed HSBagging is shown in Algorithm 1 . In each iteration, the original minority class, where n maj and n min represents the number of samples in the majority class and minority class. Therefore, when p = 0, the data set B after sampling is as same as the original data set B and when p =1,the number of samples in the majority class and the minority class gets reversed after sampling. Thus, undersampling and SMOTE are effectively combined. By learning B by the learner L ,aclassifier h p can be built for the sampling rate p . After that, f m ( h p ,B o ) estimates the performance of h p on the OOB set B o and metric f m . The sampling rate p  X  t is set to the p associated with best performance on f m and h p  X  sampling rate selection, the following iterations simply use the averaged value of the first k selected sampling rates to do sampling and train the classifier h t .At last, each individual classifier is combined into the final boosted classifier H ( x ). where | X | is the cardinality of a set and L ( n ) is the computational cost of the weak learning L with n training samples. Compared with SMOTEBagging [ 20 ], although HSBagging costs ( k  X  1) | I | more iterations to select the sampling rate, it trains only on n samples in each iteration, while SMOTEBagging trains 2 n maj samples. If the number of iterations T is relatively large and the imbalance problem of the data set is severe, HSBagging will be computational cheaper than SMOTEBagging. In this section, we conducted four experiments. Experiment 1 shows the times of best performance on each sampling rate for each data set. It verifies that the sampling rate corresponding to the best performance varies from data to data. Experiment 2 compares the proposed HSBagging with bagging on original imbalanced data set, UnderBagging [ 1 ], SMOTEBagging [ 20 ] and IIVotes [ 3 ]. We denote SMOTE with bagging by full sampling rate ( p = 1) as SMOTEBagging-1, and SMOTE with bagging by increasing sampling rate in each iteration, which is proposed in [ 20 ], as SMOTEBagging-2. Experiment 3 compares HSBagging with those methods on different sampling rates to verify that the superior performance is not only caused by sampling rate selection, but also effected by the hybrid sampling scheme. Experiment 2 and 3 verify that hybrid sampling is significantly better than individual sampling. Finally, Experiment 4 shows the performance of HSBagging on different number k of iterations for sampling rate estimation. [ 2 ] summarized in Table 1 , which cover a wide range of applications and imbal-ance ratios. The imbalance ratio (IR) is calculated by the number of data in the majority class divided by the number of data in the minority class. All experi-ments adopted 5-fold cross validation, where 80 % of the samples in each data were used for training and the rest for testing in each fold. The final results were aver-aged by 10 runs of experiments. The number of iterations T in bagging was set at 10 for all methods, except IIVotes, whose iteration was automatically deter-mined. CART [ 6 ] was adopted as the base learner for all bagging-based methods. minority class. HSBagging conducted both undersampling and SMOTE at sam-pling rate p instead of OOB estimate as described in Algorithm 1 .FromFig. 1 , it can be observed that the best results of all three methods almost appear on all sampling rates on each evaluation metric. Especially, some best sampling rates of HSBagging occur at sampling rate 0 or 1, which means that the original imbalanced data or the reversed imbalanced data may also be able to generate good results. Furthermore, the sampling rate corresponding to the best perfor-mance on different evaluation metrics may also be different, e.g. higher sampling rates generate relatively better results on G-mean. Thus, we can argue that, no matter which sampling method is adopted, selecting a proper sampling rate on a specific metric for each data set is effective and necessary. 4.2 Experiment 2: Comparative Studies Since CART generates discrete outputs, AUC can only be calculated by the ensemble of CART classifiers and is not available for individual CART classifier. Therefore, we use F1 and G-mean as the metric f m to select the best sampling rate for HSBagging, denoted as HSBagging-F1 and HSBagging-Gmean, respec-tively. The number of iterations k for sampling rate estimate is set to 3 and sampling rate selection set I = { 0 , 0 . 2 , 0 . 4 , 0 . 6 , 0 . 8 , 1 } . to show the statistical significance of the compared methods. It measures the difference between two methods and rank their magnitude among data sets. Greater difference will count more in this evaluation. The sum of ranks of R result of the i th data set. If the significance value N with a certain significance level  X  is greater than T = min { R + ,R  X  } , the null hypothesis is rejected which indicates one method significantly outperforming the other one. In the following Tables 2 , 3 and 4 ,aswellasTable 5 in Sect. 4.4 , the method shown in the left upper corner is marked as + and the compared methods are marked as  X  .The sign (+,  X  )inthe T column indicates which method wins more ranks and the symbol  X  indicates the significance with significance level  X  =0 . 05. Tables 2 and 3 show the Wilcoxon signed-rank test results of HSBagging-F1 HSBagging -Gmean significantly outperforms bagging, SMOTEBagging-1,
SMOTEBagging-2 and IIVotes on G-mean.
F1 and HSBagging-Gmean also achieve comparable or better performance on AUC. Especially, the performance of HSBagging-Gmean on AUC shows similar significance as its performance on G-mean. otes, and on F1, HSBagging-Gmean outperform Bagging, UnderBagging and IIVotes.
 Table 4 shows the comparison between HSBagging-F1 and HSBagging-4.3 Experiment 3: Sampling Rate Comparison In addition to the sampling rate selection, the new hybrid sampling scheme also plays an important role in terms of the superiority of HSBagging. In this sub-section, we show that the effectiveness of the proposed HSBagging depends on not only sampling rate selection, but also the hybrid scheme. The comparison of HSBagging with UnderBagging and SMOTEBagging-1 at different sampling rate is shown in Fig. 2 with the same setting as the experiment in Sect. 4.1 .The figures are generated by averaging all 26 UCI data sets. On most of the sam-pling rates, HSBagging can achieve better results on average than UnderBag-ging and SMOTEBagging-1. Besides, the best results of HSBagging are better than the best results of UnderBagging and SMOTEBagging among all sam-pling rates. Figure 2 illustrates that, even sampling rate selection is adopted for UnderBagging and SMOTEBagging, the overall performance cannot be as good as HSBagging. That implies that HSBagging outperforming UnderBagging and SMOTEBagging benefits from not only the choice of a proper sampling rate, but also the hybrid scheme. A novel method called HSBagging has been proposed to solve the discovered
