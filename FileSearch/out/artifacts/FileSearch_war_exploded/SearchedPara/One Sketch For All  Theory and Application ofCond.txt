 Dept. of Statistical Science pingli@cornell.edu Learning algorithms often assume a data matrix A  X  R n  X  D with n observations and D attributes and operate on the data matrix A through pairwise distances. The task of computing and maintaining distances becomes non-trivial, when the data (both n and D ) are large and possibly dynamic. For example, if A denotes a term-doc matrix at Web scale with each row representing one Web page, then n  X  O (10 10 ) (which may be verified by querying  X  X  X  or  X  X he X  in a search engine). Assuming model can boost the dimension to D  X  O (10 10 ) . Google book search program currently provides meaning that one can treat either documents or terms as features, depending on applications. Another example is the image data. The Caltech 256 benchmark contains n = 30 , 608 images, provided by two commercial firms. Using pixels as features, a 1024  X  1024 color image can be Text data are large and sparse , as most terms appear only in a small fraction of documents. For example, a search engine reports 10 7 pagehits for the query  X  X IPS, X  which is not common to the histograms; they are, however, dense when pixel-based features are used. 1.1 Pairwise Distances Used in Machine Learning The l p distance and  X  2 distance are both popular. Denote by u 1 and u 2 the leading two rows in A  X  R n  X  D . The l p distance (raised to the p th power), and the  X  2 distance, are, respectively,
The  X  2 distance is only a special case of Helbertian metrics, defined as,
Helbertian metrics are defined over probability space[7] and hence suitable for data generated from histograms, e.g., the  X  X ag-of-words X  model. For applications in text and images using SVM, empir-ical studies have demonstrated the superiority of Helbertian metrics over l p distances[3, 7, 9]. for any generic function g . An efficient method for computing (1) for any g would be desirable. 1.2 Bottleneck in Distance/Kernel-based Learning Algorithms For popular kernel SVM solvers including the SMO algorithm[16], storing and computing kernels is the major bottleneck[2], because computing kernels is expensive, and more seriously, storing the full kernel matrix in memory is infeasible when the number of observations n &gt; 10 5 . We should emphasize that this challenge is a universal issue in distance-based methods, not limited to SVMs. For example, popular clustering algorithms and multi-dimensional scaling algorithms require frequently accessing a (di)similarity matrix, which is usually distance-based. In addition to computing and storing distances, another general issue is that, for many real-world applications, entries of the data matrix may be frequently updated, for example, data streams[15]. There have been considerable studies on learning from dynamic data, e.g., [5, 1]. Since streaming data are often not stored (even on disks), computing and updating distances becomes challenging. 1.3 Contributions and Paper Organization Section 2 reviews the original CRS. Section 3 extends CRS to dynamic/streaming data. Section 4 focuses on using CRS to estimate the Hamming norm of a single vector, based on which Section 5 provides a generic estimation procedure for CRS, for estimating any linear summary statistics, with the focus on the Hamming distance and the  X  2 distance. Finally, Section 6 concludes the paper. Conditional Random Sampling (CRS) [12, 13] is a local sampling strategy. Since distances are local (i.e., one pair at a time), there is no need to consider the whole matrix at one time. provides an example of a column-permuted data matrix. The next step of CRS is to construct a corresponding to the three rows of the (column) permuted data matrix in Figure 1(a). Figure 1: (a): A data matrix with three rows and D = 16 columns. We assume the columns are  X  X D X  is the column ID after the permutation and  X  { val }  X  is the value of that entry. Had we directly taken the first D s = 8 columns from the permuted data matrix, we would obtain the in this example, the element 10 { 8 } in sketch K 1 is excluded.
 On the other hand, since the columns are already permuted, any D s columns constitute a random [13] viewed this as a random sample conditioning on D s .
 suggest their D s = min(max(ID(K 1 )), max(ID(K 3 ))) = min(10,12) = 10.
 (good) heuristic. There are two ways to understand why this argument is not strictly correct. permuted data matrix. Assuming sparse data, elements at the D s th column should be most likely zero. However, in the  X  X onditional random sample X  obtained from CRS, at least one element at the D s th column is non-zero. Thus, the estimates of the original CRS are, strictly speaking, biased . For a more obvious example, we can consider two rows with exactly one non-zero entry in each row at the same column. The original CRS can not obtain an unbiased estimate unless D s = D . Thus, a one-pass algorithm is needed to compute and update distances for training. Learning with dynamic (or incremental) data has become an active topic of research, e.g., [5, 1]. 3.1 Dynamic/Streaming Data model [15] is extremely popular and assumes a linear updating function H, i.e., user i orders (i.e., I t &gt; 0) or cancels (i.e., I t &lt; 0 ) at time t .
 In terms of the data matrix A  X  R n  X  D , we can view it to be a collection of n data streams. 3.2 CRS for Streaming Data Once sketches are constructed, the estimation procedure will be the same regardless whether the 3.3 (Symmetric) Stable Random Projections (SRP) Since the method of (symmetric) stable random projections (SRP) [8, 10] has become a standard algorithm for data stream computations, we very briefly introduce SRP for the sake of comparisons. v 1 = R T u 1 and v 2 = R T u 2 have i.i.d. stable entries, i.e., for j = 1 to k ,
Thus, one can estimate an individual norm or distance from k samples. SRP is applicable to dy-namic/streaming data, provided the data follow the Turnstile model in (2). Because the Turnstile model is linear and matrix multiplication is also linear, one can conduct A  X  R incrementally. Compared with Conditional Random Sampling (CRS) , SRP has an elegant mathematical deriva-determined in fully rigorous fashion. The accuracy of SRP is not affected by heavy-tailed data. CRS, however, exhibits certain advantages over SRP: Counting the Hamming norm (i.e., number of non-zeros) in an exceptionally long, dynamic vector has important applications[4, 15]. For example, if a vector u t records the numbers of items users have ordered, one meaningful question to ask may be  X  how many distinct users are there? X  The purpose of this section is three-fold. (1) This is the case we can rigorously analyze CRS and propose a truly unbiased estimator. (2) This analysis brings better insights and more reasonable similar accuracy as stable random projections (SRP). Empirically, CRS (slightly) outperforms SRP. 4.1 The Proposed (Unbiased) Estimator and Variance Suppose we have obtained the sketch K. For example, consider the first row in Figure 1: D = 16 , k = 4 and the number of non-zeros f = 7 . Lemma 1 (whose proof is omitted) proposes an unbiased estimator of f , denoted by  X  f , and a biased estimator based on the maximum likelihood, f mle . Lemma 1
Assume f/D is small and k/f is also small, then Var Note that, since Var 4.2 The Approximation Using the Conditioning Argument out replacement) from a pool of D balls and we observe that k 0 balls are red; then a natural (and unbiased) estimator for the total number of red balls would be D D This seems to imply that the  X  X onditioning X  argument in the original CRS in Section 2 is  X  X orrect X  if we make a simple modification by using the D s which is the original D s minus 1. While this is what we will recommend as the modified CRS, it is only a close approximation.
 4.3 Comparisons with Stable Random Projections (SRP) l norm with very small p , as an approximation to f . For p  X  0+ , the recent work for SRP [10] proposed the harmonic mean estimator. Recall that after projections v = R T u  X  R k consists of i.i.d. stable samples with scale parameter F p =
Denote this estimator by  X  f srp (using p as small as possible), whose variance is Var which is roughly equivalent to the variance of  X  f , the unbiased estimator for CRS. We empirically compared CRS with SRP. Four word vectors were selected; entries of each vector tailed. The percentage of zero elements (i.e., sparsity) varies from 58% to 95% . Figure 2 presents the comparisons. (1): It is possible that CRS may outperform SRP non-negligibly. (2): The variance (3) based on the approximate  X  X onditioning X  argument is very accurate. (3): The unbiased estimator  X  f is more accurate than  X  f mle ; the latter actually uses one more sample. Figure 2: Comparing CRS with SRP for approximating Hamming norms in Web crawl data (four  X  X RS+mle X  respectively correspond to  X  f and  X  f mle , derived in Lemma 1.  X  X RP X  corresponds to the both CRS and SRP. The curve labeled  X  X pprox. Var X  is the approximate variance in (3). The modified CRS estimation procedure is based on the theoretical analysis for using CRS to ap-proximate Hamming norms. Suppose we are interested in the distance between rows u 1 and u 2 and we have access to sketches K 1 and K 2 . Our suggested  X  X quivalent X  sample size D s would be We should not include elements in K 1 and K 2 whose IDs are larger than D s Consider K 1 and K 2 in Figure 1, the modified CRS adopts D s = min(10  X  1 , 8  X  1) = min(9 , 7) = 7 . Removing 10 { 8 } from K 1 and 8 { 7 } from K 2 , we obtain a sample for u 1 and u 2 : 5.1 A Generic Estimator and Approximate Variance P assumption, an  X  X nbiased X  estimator of d g ( u 1 , u 2 ) (and two special cases) would be A generic (approximate) variance formula can be obtained as follows: statistical approximation: E (max( x, y ))  X  max ( E ( x ) , E ( y )) .
 max it in general does not have the worst-case performance guarantees.
 distance or the  X  2 distance for kernel SVMs achieved good performance. 5.2 Estimating the Hamming Distance mate h using the modified CRS procedure, denoted by  X  h . The approximate variance (5) becomes We also apply SRP using small p and its most accurate harmonic mean estimator[10]. The empirical comparisons in Figure 3 verify two points. (1): CRS can be considerably more accurate than SRP for estimating Hamming distances in [4]. (2): The approximate variance formula (6) is very accurate. Figure 3: Approximating Hamming distances ( h ) using two pairs of words. The results are presented approximate variance of CRS in (6).
 In this example, the seemingly impressive improvement of CRS over SRP is actually due to that we used the definition of Hamming distance in [4]. An alternative definition of Hamming distance is h ( u 1 , u 2 ) = l distance after a binary term-weighting. As we have commented, if using SRP in dynamic data, term-weighting is not possible; thus we only experimented with the definition in [4]. 5.3 Estimating the  X  2 Distance According to (5), the estimation variance should be approximately which is affected only by the second moments, because P D imating the  X  2 distances. CRS does not provide any worst-case guarantees; its performance relies reasonably bounded in machine learning applications.
 Figure 4 presents some empirical study, using the same four words, plus the UCI Dexter data. Even though the four words are fairly common (i.e., not very sparse) and they are heavy-tailed (no term-weighting was applied), CRS still achieved good performance in terms of the normalized MSE (e.g.,  X  0 . 1 ) at reasonably small k . And again, the approximate variance formula (7) is accurate. Results in the Dexter data set (which is more realistic for machine learning) are encouraging. Only about k = 10 is needed to achieve small MSE. Figure 4: Left two panels: CRS for approximating the  X  2 distance using two pairs of words ( D = 44850 pairs)  X  2 distances using CRS. The three curves report the quantiles of normalized MSEs. The ubiquitous phenomenon of massive, high-dimensional, and possibly dynamic data, has brought computing and retrieving summary statistics, in particular, various types of distances. Conditional Random Sampling (CRS) provides a simple and effective mechanism to achieve this goal. Compared with other  X  X ain stream X  sketching algorithms such as stable random projections (SRP) , can approximate any linear summary statistics. This would be very convenient in practice. Originally based on a heuristic argument, the preliminary version of CRS, was proposed as a tool justification of CRS and various modifications, to make the algorithm more rigorous and to extend CRS for handling dynamic/streaming data. We demonstrate, empirically and theoretically, the ef-fectiveness of CRS in approximating the Hamming norms/distances and the  X  2 distances. Ping Li is partially supported by grant DMS-0808864 from the National Science Foundation, and National Science Foundation, and grant 2R01 CA 72028-07 from the National Institutes of Health.
