 Tensor decomposition operation is the basis for many data analysis tasks from clustering, trend detection, anomaly de-tection, to correlation analysis. One key problem with ten-sor decomposition, however, is its computational complexity  X  especially for dense data sets, the decomposition process takes exponential time in the number of tensor modes; the process is relatively faster for sparse tensors, but decomposi-tion is still a major bottleneck in many applications. While it is possible to reduce the decomposition time by trading performance with decomposition accuracy, a drop in accu-racy may not always be acceptable. In this paper, we first recognize that in many applications, the user may have a fo-cus of interest  X  i.e., part of the data for which the user needs high accuracy X  and beyond this area focus, accuracy may not be as critical. Relying on this observation, we propose a novel Personalized Tensor Decomposition (PTD) mecha-nism for accounting for the user X  X  focus: PTD takes as input one or more areas of focus and performs the decomposition in such a way that, when reconstructed, the accuracy of the tensor is boosted for these areas of focus. We discuss alter-native ways PTD can be implemented. Experiments show that PTD helps boost accuracy at the foci of interest, while reducing the overall tensor decomposition time.
Tensors are multi-dimensional arrays and are commonly used for representing multi-dimensional data, such as Web graphs, sensor streams, and social networks. Specific uage examples in the literature include representations of RDF  X  This work is funded by NSF Grant #1016921, X  X ne Size Does Not Fit All: Empowering the User with User-Driven Integration X . The symbol  X  indicates student authors (with equal contributions).
 Figure 1: Personalization through user X  X  focus of in-terest: in this example, the user has expressed spe-cial interest on a subrange of values along the first and second modes of the tensor. Consequently, any processing on the tensor (including tensor decom-positions) should preserve the accuracy along the corresponding slices and especially the region con-sisting of the intersection of foci of interest triples of the form (subject-predicate-object) in knowledge bases (venue-author-keywords) relationships in scientific digital libraries [12], and (movie-user-rating) relationships in movie recommendation [28]. As a consequence, tensor de-composition operations (such as CP [8] and Tucker [26]) be-gan to form the basis for many data analysis and knowledge discovery tasks, from clustering, trend detection, anomaly detection [12], to correlation analysis [23].

One key problem with tensor decomposition, however, is its computational complexity  X  especially for dense data sets, the decomposition process takes exponential time in the number of tensor modes. While, the process is rela-tively faster for sparse tensors, decomposition is still a ma-jor bottleneck in many applications: decomposition algo-rithms have high computational costs and incur large mem-ory overheads and, thus, are not suitable for large problems. Recent improvements (including scalable implementations, such as TensorDB [14, 15], Grid PARAFAC [17], and Gi-gaTensor [10]) also suffer from high computational costs.
Due to the approximate nature of the tensors decomposi-tion process, one way to reduce computational requirements might be to trade performance with accuracy. However, nat-urally, a drop in accuracy may not be acceptable in many applications. Therefore, this is not a feasible solution to tackle the computational cost.

In this paper, we first recognize that in many applications, the user may have a focus of interest  X  i.e., part of the data for which the user needs high accuracy X  and beyond this area focus, the user may not be interested in maintaining high accuracy. For example, in a clustering application, the user might be interested in ensuring the clustering accuracy (i.e., differentiating power) for a high priority subset of the objects in the database. As a specific example, consider a movie recommendation application, where tensor decompo-sition of a movie-user-ratings tensor is used for generating recommendations: in this example, the user may want to decompose this tensor in a way that maximizes accuracies for recent movies and the most active users of the system.
Relying on this observation, we propose a novel Personal-ized Tensor Decomposition (PTD) mechanism for account-ing for the user X  X  focus and interests during tensor decompo-sition: PTD takes as input (a) an input tensor and (b) user X  X  interest in the form of one or more area of focus (Figure 1). Given this input, PTD performs the tensor decomposition operation in such a way that, when reconstructed, the accu-racy of the decomposition is boosted within the high-priority areas of focus.

Intuitively, the proposed Personalized Tensor Decomposi-tion (PTD) algorithm partitions the tensor into multiple regions and then assigns different ranks to different sub-tensors: naturally, the higher the target rank is, the more accurate the decomposition of the sub-tensor. However, we note that preserving accuracy for foci of interest, while re-laxing accuracy requirements for the rest of the input tensor is not a trivial task, especially because loss of accuracy at one region of the tensor may impact accuracies at other ten-sor regions : for example, the basic alternating least squares (ALS) based decomposition algorithms [13] improve the ac-curacy of the whole tensor iteratively and it is not possible to separate accuracy of one part of the tensor from accuracies of other parts. As we show in Section 4.2, even when using block based decomposition approaches [17] (which partition the tensor into multiple blocks, decompose the blocks inde-pendently, and combine these initial decompositions) initial decomposition accuracy of one tensor partition may impact final decomposition accuracies of other partitions.
Therefore, we present alternative ways to account for the impact of the accuracy of one region of the tensor to the accuracies of the other regions of the tensor, each based on a different assumption about how the impact of inaccuracies propagates along the tensor. Given a model of impact, PTD (a) first partitions the input tensor in a way that reflects user X  X  interest, (b) constructs a sub-tensor impact graph re-flecting the tensor content and its partitions, and then (c) analyzes this sub-tensor impact graph (in the light of the user X  X  interest) to identify initial decomposition ranks for the sub-tensors in a way that will boost the final decompo-sition accuracies for partitions of interest.
This paper is organized as follows: We conclude the paper in Section 6.
Tensor based representations of data and tensor decompo-sitions (especially the two widely used decompositions CP [8] and Tucker [26]) are proven to be effective in multi-aspect data analysis for capturing high-order structures in multi-dimensional data. In [11], for example, the authors incor-porate contextual information to the traditional HITS al-gorithm, formulating the task as tensor decomposition. [4] analyzes the ENRON email network using tensor decompo-sition. In [1], authors introduce a tensor-based framework to identify epileptic seizures and, in [24], authors use tensors to incorporate user click information to improve web search. [7] shows that tensor decomposition of fMRI data can help in differentiating healthy and Alzheimer affected individuals.
There are two widely used toolboxes for tensor manipula-tion: the Tensor Toolbox for Matlab [3] (for sparse tensors) and N-way Toolbox for Matlab [2] (for dense tensors). Yet, due to the significant cost [12] of tensor decompositions, var-ious parallel algorithms and systems have been developed. In [18], authors propose a two stage (partition and merge) scheme for implementing the CP decomposition in a paral-lizable manner: the data tensor is divided into a grid and the resulting sub-tensors are factorized in parallel. Then the factors for the complete tensor are estimated from these sub-factors, in parallel, using special update rules. [27] in-troduces various parallelization strategies for speeding up factor matrix update step in alternating least squares (ALS) based decomposition. Authors propose techniques for dis-tributing a large tensor onto the servers in a cluster, mini-mizing data exchange, and limiting the memory needed for storing matrices or tensors. [25] proposes MACH, a ran-domized algorithm that speedups the Tucker decomposition while providing accuracy guarantees. More recently, [10] proposed GigaTensor, a massively distributed Map-Reduce based implementation of PARAFAC. Authors introduce a series of optimizations, which (in combination with the use of the Map-Reduce environment) lead to a highly scalable PARAFAC decomposition platform. In [19], authors pro-pose PARCUBE, a sampling based, parallel and sparsity promoting, approximate PARAFAC decomposition scheme. Scalability is achieved through sketching of the tensor (using biased sampling) and parallelization of the decomposition operations onto the resulting sketches.

In this paper, we focus on personalized decomposition, where users provide foci of interest for accurate decomposi-tion. A related, but orthogonal, research area is supervised and semi-supervised decomposition or factorization, where external metadata [9, 16, 21, 22] are incorporated in the de-composition/clustering process in the form additional matri-ces (for joint factorization) or domain hierarchies that drive the decomposition process.
In this section, we present the relevant background and notations and formalize the PTD problem.
Tensors are generalizations of matrices: while a matrix is essentially a 2-mode array, a tensor is an array of possibly larger number of modes. Intuitively, the tensor model maps a relational schema with N attributes to an N -modal array (where each potential tuple is a tensor cell).

The two most popular tensor decomposition algorithms are the Tucker [26] and the CANDECOMP/PARAFAC (or CP) [8] decompositions. Intuitively, both decompositions generalize singular value matrix decomposition (SVD) to tensor. CP decomposition, for example, decomposes the in-put tensor into a sum of component rank-one tensors.
More specifically, given a tensor X , the CP decomposition factorizes the tensor into F component matrices (where F is a user supplied non-zero integer value also referred to as the rank of the decomposition). For the simplicity of the discussion, let us consider a 3-mode tensor X  X  R I  X  J  X  K CP would decompose X into three matrices A , B , and C , such that where a f  X  R I , b f  X  R J and c f  X  R K . The factor matrices A , B , C are the combinations of the rank-one component vectors into matrices; e.g., A = [ a 1 a 2  X  X  X  a F ].
Many algorithms for decomposing tensors are based on an iterative process that tries to improve the approximation until a convergence condition is reached through an alter-nating least squares (ALS) method: at its most basic form, ALS estimates, at each iteration, one factor matrix while maintaining other matrices fixed; this process is repeated for each factor matrix associated to the modes of the input tensor. Note that due to the approximate nature of tensor decomposition operation, given a decomposition [ A , B , C ] of X , the tensor  X  X that one would obtain by re-composing the tensor by combining the factor matrices A , B , and C is often different from the input tensor, X . The accuracy of the decomposition is often measured by considering the Frobenius norm of the difference tensor: accuracy ( X ,  X  X ) = 1  X  error ( X ,  X  X ) = 1  X 
As we discussed in the introduction, when analyzing a dataset, the user may require high accuracy for a specific subset of the data rather than the whole dataset. For exam-ple, given a three mode tensor where the modes represent users, movies, and ratings, a recommendation application may focus on the recommendation accuracy for more recent movies or the ratings with highest and lowest scores. In this paper, we argue that it is possible to perform the ten-sor decomposition operation in such a way that we speed up the process significantly by trading accuracy with speed, but still preserve (and boost) the accuracy for the subset of the data that is critical for the analysis task.
 into a grid, X = { X ~ k | ~ k  X  X } , of sub-tensors, such that The number, k X k , of partitions (and thus also the number, kKk , of partition indexes) is Q N i =1 K i .

In addition, let K P  X  K be the set of sub-tensor indexes that indicate those sub-tensors for which the user requires higher accuracy. Note that, without loss of generality, we as-sume that the tensor X is re-ordered before it is partitioned n such a way that the number, K i , of resulting partitions along each mode-i is minimal  X  i.e., along each mode, the entries of interest are clustered together to minimize the number of partitions 1 .

Example 1 (PTD Specification). Figure 2 shows a 3-mode tensor, partitioned into 27 sub-tensors: 12 tensor-blocks (sub-tensors 1,3,7,9,10,12,16,18,19,21,15,27), 12 slices (sub-tensors 2,8,11,17,20,26,4,6,13,15,22,24), and 3 fibers (sub-tensors 5,14,23). The figure also highlights the sub-tensors that the user marked as important. 3
Given the above, let us use X P as a shorthand to denote the cells of X collectively covered by the sub-tensors indexed by K P . The goal of the Personalized CP Tensor Decompo-sition (PTD-CP) is to obtain a personalized (or preference sensitive) decomposition  X  X of X , which maximizes the ac-curacy gain defined as where
Naturally, we also aim that the time to obtain the person-alized decomposition  X  X will be lesser than the time needed Figure 2: A sample 3-mode tensor, partitioned into 27 sub-tensors (consisting of blocks, slices, and fibers); the figure also highlights a sample user focus in the form of high-priority partitions to obtain preference insensitive decomposition  X  X and also that the personalized decomposition minimally impacts the rest of the tensor; i.e.,
As formalized in the previous section, the proposed Per-sonalized Tensor Decomposition (PTD) algorithm partitions the tensor into multiple regions (indicated with the index set K ) and aims to maximize the accuracy of a subset, K P  X  X  , of partitions specified by the user.

To achieve this, we rely on a block-based CP decomposi-tion approach [17], which initially decomposes different parts of the tensor independently and then iteratively combines these decompositions into a final composition (we provide an overview of this block-based decomposition scheme in Section 4.1). Given this block-based decomposition process, we argue that we can improve the accuracy of the high prior-ity sub-tensors (indicated by K P ) by assigning them higher initial decomposition ranks than the rest of the partitions.
Based on this observation, we reformulate the personal-ized tensor decomposition (PTD) problem as choosing the appropriate initial decomposition ranks for the partitions of the given tensor. The key challenge , however, is that one cannot arbitrarily reduce the decomposition ranks of low priority partitions, because (as we see in Section 4.1), the accuracy in one partition may impact final decomposi-tion accuracies of other tensor partitions. Consequently, in this paper, we propose alternative ways to account for the impact of the accuracy of one region of the tensor to the accuracies of the other regions, each based on a different assumption about how inaccuracies propagate among the partitions of the tensor. But, first, we provide a brief review of the block-based CP decomposition scheme, which forms one of the building blocks of our approach.
 Figure 3: Each sub-tensor can be described in terms of the corresponding sub-factors As before, let us consider an N -mode tensor X  X  R X = { X ~ k | ~ k  X  X } where K is the set of sub-tensor indexes. Let us also assume that we are given a target decomposition rank, F , for the tensor X . Let us further assume that each sub-tensor in X has already been decomposed with target F -rank sub-factors 2 corresponding to the sub-tensors in X along mode i . In other words, for each X ~ k , we have where I is the N -mode F  X  F  X  ...  X  F identity tensor, where the diagonal entries are all 1s and the rest are all 0s.
Given this, [17] presents an ALS based algorithm for com-posing these initial sub-factors into the full F -rank factors, A ( i ) (each one along one mode), for the input tensor, X . The outline of this block based process is as follows:
Let us partition each factor A ( i ) into K i parts correspond-ing to the block boundaries along mode i : Given this partitioning, each sub-tensor X ~ k , [ k ,...,k i ,...,k N ]  X  K can be described in terms of these sub-factors (Figure 3): [17] shows that the sub-factor A ( i ) ( k where such that, given ~ l = [ l 1 ,l 2 ,...,l N ], we have Figure 4: The block-based update rule maintains A ( k i ) incrementally by using the current estimates Above, ~ denotes the Hadamart product and denotes the element-wise division operation.
While the precise derivation of the above update rule is not critical for our discussion (and is beyond the scope of this paper), it is important to note that, as visualized in for all 1  X  j  X  N , Moreover, and most importantly for our work, this update On the other hand, while it is true that sub-tensor ranks are flexible and the above formulation enables us to possi-bly reduce the initial decomposition ranks for low-priority sub-tensors if we choose to do so, it is important note that such an action would not be free of impact on the final de-composition accuracies of high priority sub-tensors. Figure 5: The sub-tensors whose initial decompo-sition accuracies directly impact given sub-tensors are aligned (i.e., share the same slices) with that sub-tensor along the different modes of the tensor. In other words, for each sub-tensor X ~ k , there is a set, direct impact ( X ~ k )  X  X , of sub-tensors that consists of those sub-tensors whose initial decomposition accuracies di-rectly impact the final decomposition accuracy of X ~ k . More-over, as visualized in Figure 5, direct impact ( X ~ k ) consists of those sub-tensors that are aligned (i.e., share the same slices) with X ~ k , along the different modes of the tensor.
These observations provide us with a way to assign ranks to different sub-tensors in a way that maximizes the final ac-curacies of the regions of the tensor marked as high-priority.
As visualized in Algorithm 1, the proposed PTD algo-rithm first constructs a sub-tensor impact (SI) graph , G , that accounts for the propagation of inaccuracies along the tensor during a block-based decomposition process. Intu-itively, in this graph, for each sub-tensor, there are incoming edges from other sub-tensors which impact its accuracy. The PTD algorithm then leverages this graph to account for the impact of the initial decomposition inaccuracy of one sub-tensor on the final decomposition accuracy of X P ; i.e., the cells of X collectively covered by the user X  X  declaration of interest (i.e., K P ).

In the following, we first formally define the sub-tensor impact (SI) graph , G , of a tensor X , partitioned into sub-tensors X . We then describe how to compute decomposition Algorithm 1 Overview of the PTD-CP process ranks of sub-tensors using G (Section 4.5). Then, in Sec-tion 4.6, we discuss alternative ways the edges in G can be weighted to capture how inaccuracies propagate among the sub-tensors of X .
Let X be a tensor partitioned into a set (or grid) of sub-tensors X = { X ~ k | ~ k  X  K} as specified in Section 3.3. The corresponding sub-tensor impact (SI) graph (which accounts for propagation of inaccuracies along the matrix during a block-based decomposition process) is a directed, weighted graph, G ( V,E,w ()), where
We discuss alternative ways to construct the edge weight function, w (), in Section 4.6. For now, let us assume that we are given a tensor, X , partitioned into a set of sub-tensors, X , and let G ( V,E,w ()) be the corresponding di-rected, weighted sub-tensor impact (SI) graph . Let us fur-ther assume that the user has specified a decomposition rank F and provided K P  X  K , indicating the high priority sub-tensors for which the s/he would like to preserve accuracy.
As stated in Section 3.3, our goal is to assign an ini-tial decomposition rank F ~ k  X  F to each ~ k  X  K , such that in the resulting decomposition,  X  X , of X , we will have accuracy ( X P ,  X  X P ) &gt; accuracy ( X P ,  X  X P ) , where reconstruction of the user selected region from the personal-ized decomposition  X  X and  X  X P is the reconstruction of the same region from a decomposition of X insensitive to the user preference. We also aim that the time to obtain per-sonalized decomposition will be lesser than the time needed to obtain preference insensitive decomposition and that the personalized decomposition has a small impact on the rest of the tensor.
Intuitively, the initial decomposition rank, F ~ k , of sub-tensor X ~ k , will need to reflect the impact of the initial de-composition of the sub-tensor X ~ k on the final decomposition of the high-priority sub-tensors, X ~ X  , ~ X   X  X  P .

The edges on the sub-tensor impact (SI) graph, G , de-scribe the direct accuracy impact between the sub-tensors. However, a second look at Equations 2 and 3 shows that the final accuracy of the sub-tensor, X ~ X  , also depends ( in-directly ) on sub-tensors outside of direct impact ( X ~ X  is because, during the alternating least-square (ALS) com-putation, based on the update rule in Equation 3, inaccura-cies in a given sub-factor A ( i ) ( k errors among sub-tensors even though they are not directly tied with direct impact () relationships. This implies that, when picking the decomposition ranks, F ~ k , we need to look beyond the immediate neighborhood relationships on G .
While the immediate neighborhood relationships on the sub-tensor impact graph, G , are not sufficient for setting F , it is important to note that the indirect propagation of inaccuracies occur also over the same graph G : We therefore need to measure how inaccuracies propagate within G over a large number of iterations of the alternating least squares (ALS) process. For this purpose we rely on a random-walk based measure of node relatedness on the given graph. More specifically, we rely on personalized PageRank (PPR [5, 6]) to measure sub-tensor relatedness. Like all random-walk based techniques, PPR encodes the structure of the graph in the form of a transition matrix of a stochastic process and complements this with a seed node set, S  X  V , which serves as the personalization context: each node, v the graph is associated with a score based on its positions in the graph relative to this seed set (i.e., how many paths there are between v i and the seed set and how short these paths are). In particular, the PPR score ~p [ i ], of v i is obtained by solving the following equation: where T G denotes the transition matrix corresponding to the graph G (and the underlying edge weights) and ~s is a re-seeding vector such that if v i  X  S , then ~s [ i ] = 1 bution of a random walk on G which follows graph edges (according to the transition probabilities T G ) with proba-bility (1  X   X  ) and jumps to one of the seeds with probabil-ity  X  . Correspondingly, those nodes that are close to the seed nodes over a large number of paths obtain large scores, whereas those that are poorly connected to the nodes in S receive small PPR scores.
We note that the iterative nature of the random-walk pro-cess underlying PPR fits well with how inaccuracies prop-agate during the iterative ALS process. Based on this ob-servation, given a directed, weighted sub-tensor impact (SI) graph , G ( V,E,w ()), we construct a transition matrix and obtain the PPR score vector ~p by solving the above equation 3 . Then, we compute the initial decomposition rank, F ~ k of X ~ k as where ~p [ k ] denotes the PPR score of the node that corre-sponds to the sub-tensor X ~ k in G . Intuitively, this formula sets the initial decomposition rank of the sub-tensor with the highest PPR score (i.e., highest accuracy impact on the set of sub-tensors chosen by the user) to F , whereas other sub-tensors are assigned progressively smaller ranks (poten-tially all the way down to 1) 4 based on how far they are from the seed set in the sub-tensor impact graph, G .
In Section 4.4, we discussed how to set the nodes and edges of the sub-tensor impact (SI) graph , G ( V,E,w ()), given a tensor X , partitioned into a set of sub-tensors X = { X ~ k | ~ k  X  X } as specified in Section 3.3. In this subsec-tion, we consider alternative ways in which the edge weight function, w (), can be constructed to account for the propaga-tion of inaccuracies within the tensor during the block-based decomposition process.
The most straightforward way to set the weights of the edges in E is to assume that the propagation of the inaccu-racies over the sub-tensor impact graph is uniform. In other words, in this case, for all e  X  E , we set w uni ( e ) = 1.
While being simple, the uniform edge weight alternative may not properly account for the impact of the varying di-mensions of the sub-tensors on the error propagation. As we see in Figure 2, in general, the neighbors of a given sub-tensor can be of varying shape and dimensions and we may need to account for this diversity in order to properly as-sess how inaccuracies propagate in the tensor. In particular, in this subsection, we argue that the surface of interaction between two sub-tensors X ~ j and X ~ l , defined as below, may need to be considered to account for inaccuracy propagation:
Definition 1 (Surface of Interaction). Let X be a tensor partitioned into a set (or grid) of sub-tensors X = { X ~ k | ~ k  X  X } . Let also X ~ j and X ~ l be two sub-tensors in X , such that We define the surface of interaction , surf ( X ~ j , X ~ X j and X ~ l as follows: Here, as formalized in Section 3.3, I j h ,h is the size of the j th partition along mode h .

Principle 1. Let G ( V,E,w ()) be a sub-tensor impact graph and let ( v ~ j  X  v ~ l )  X  E be an edge in the graph. The weight of this edge from v ~ j to v ~ l should reflect the area of the surface of interaction between the sub-tensors X ~ j and X Intuitively, this principle verbalizes the observation that er-rors are likely to propagate more easily if two sub-tensors share large dimensions along the modes on which their par-titions coincide. Under this principle, we can set the weight of the edge ( v ~ j  X  v ~ l )  X  E as follows:
Although surface of interaction based edge weights ac-count for the varying shapes and sizes of the sub-tensors of X , they fail to take into account for how similar these sub-tensors are  X  more specifically, they ignore how the val-ues within the sub-tensors are distributed and whether these distributions are aligned across them.

Intuitively, if the value distributions are aligned (or simi-lar) along the modes that two sub-tensors share, then they are likely to have high impacts on each other X  X  decomposi-tion during the application of the update rule 3. If they are dissimilar, on the other hand, their impacts on each other will be minimal. Therefore, considering only the area of the surface of interaction may not be sufficient to properly account for the inaccuracy propagation within the tensor. More specifically, we need to measure the value alignment between sub-tensors as well:
Definition 2 (Value Alignment). Let X be a ten-sor partitioned into a set (or grid) of sub-tensors X = { X ~ k | ~ k  X  K} . Let also X ~ j and X ~ l be two sub-tensors in X , such that Let, A = { h | k j h = k l h } be the set of modes along which the two sub-tensors are aligned and let R be the remaining modes. We define the value alignment , align ( X ~ j between X ~ j and X ~ l as where the vector ~ c ~ j ( A ) is constructed from the sub-tensor X j as follows and the tensor M ~ j ( A ) is constructed from X ~ j by fixing the values along the modes in A :  X  1  X  i h  X  I j h ,h , Here, norm () is the standard Frobenius norm and X A take values i 1 , i 2 , through i | A | .
 Intuitively, ~ c ~ j ( A ) captures the value distribution of the ten-sor X ~ j along the modes in A .

Principle 2. Let G ( V,E,w ()) be a sub-tensor impact graph and let ( v ~ j  X  v ~ l )  X  E be an edge in the graph. The weight of this edge from v ~ j to v ~ l should reflect the structural alignment between the sub-tensors X ~ j and X ~ l . This principle verbalizes the observation that errors are likely to propagate more easily if two given sub-tensors are structurally aligned along the modes on which their parti-tions coincide. As before, under this principle, we can set the edge weights of the edge ( v ~ j  X  v ~ l )  X  E in the sub-tensor impact graph as follows:
As we have seen above, the surface of interaction based edge weights account for the shapes of the sub-tensors, but do not account for their value alignments. In contrast, value alignment based edge weights consider the structural sim-ilarities of the sub-tensors, but how big the surfaces they share are.

Therefore, a potentially more effective alternative would be to combine these surface of interaction and value align-ment based edge weights into a single weight that takes into account both aspects of the sub-tensor interaction: where comb ( Y , Z ) = align ( Y , Z )  X  surf ( Y , Z ).
In this section, we report experiments that aim to as-sess the effectiveness of the proposed personalized tensor decomposition (PTD) approach in helping preserve the ten-sor decomposition accuracy at parts of the tensor that are high-priority for the user. We also evaluate different strate-gies for accounting the propagation of inaccuracies within the tensor during the PTD process. Data Sets. In these experiments, we used three real datasets: Epinions [29], Ciao [29], and Enron [20]. The first two of these are comparable in terms of their sizes and semantics: they are represented in the form of 170  X  1000  X  18 (density 2 . 4  X  10  X  4 ) and 167  X  967  X  18 (density 2 . 2  X  10  X  4 ) tensors, respectively, and both have the schema  X  user,item,category  X  . The Enron email data set, however, is much larger (5632  X  184  X  184, density 1 . 8  X  10  X  4 ) and has a different schema,  X  time,from,to  X  .
 User Focus. In order to observe the impacts of different types of user feedback, we considered (a) different numbers Table 1: Various tensor partitioning scenarios con-sidered: percentages are the sizes of the partitions (relative to the size of the mode) along each mode of partitions, (b) different ratios of partition sizes, and (c) different numbers of partitions included in the user X  X  focus. Table 1 lists the various partitioning scenarios considered in these experiments. For each partitioning scenario, ran-dom partitions of the input tensors have been created and we report averages of the runs for different focus selections on these partitions. In the experiments, we also considered scenarios where the user X  X  interest (d) is focused on different numbers (1 , 2 , 3 , 4) of sub-tensors.
 Decomposition Strategies. We considered five block-based tensor decomposition strategies: not personalized block-based ( NP 6 ), uniform edge weights ( UNI ), surface of in-teraction based edge weights ( SURF ), value alignment based edge weights ( VAL ), and the combined edge weights ( COMB ). These were implemented by modifying the grid PARAFAC technique [17]. The decomposition rank, F , is set to 10. Evaluation Criteria. We use the measure reported in Sec-tion 3.2 to assess decomposition accuracy. In particular, we report accuracies for both user X  X  area of focus as well as the whole tensor (see Section 3.3). We also report the decom-position time for different decomposition schemes 7 . Hardware and Software. We used a quad-core Intel(R) Core(TM)i5-2400 CPU @ 3.10GHz machine with 8.00GB RAM. All codes were implemented in Matlab and run using Matlab 7.11.0 (2010b) and Tensor Toolbox Version 2.5. [3]. General Overview. Figure 6 shows accuracies (for the focus area as well as for the whole tensor) and decomposition times for the configuration with 2  X  2  X  2 partitioning of the input tensor and 2 partitions highlighted in the user focus.
As we see in this figure, as expected, for all data sets, PTD algorithms boost accuracy for the high-priority partitions in the user focus, especially where the partitions are of hetero-geneous sizes (as is likely to be the case in real situations).
While, as would be expected, the PTD algorithms impact the overall decomposition accuracy for the whole tensor, this is more than compensated by gains in accuracies in high-priority areas 8 . Moreover, the figure also shows that the gains in accuracy in high-priority partitions within the user X  X  focus comes also with significant gains in execution times for the decomposition process.

As a control, we have also considered a naive approach where we simply zero-out out-of-focus areas of the input ten-sor: As would be expected, this had drastic impacts on the out-of-focus areas. For example, for the Epinions data sets, the out-of-of-focus accuracies for this zeroing-out method were 1.82E-05 (for the most balanced partitioning), 2.20E-05, 2.44E-05, 2.40E-05, 1.54E-05, 3.99E-08 (for the least bal-anced partitioning); i.e., several orders worse than COMB .
The figure also establishes that, both in terms of accu-racy and execution time gains, we can order the various edge weighting strategies as follows: UNI (least effective), VAL , SURF , and COMB (most effective). In other words, as we argued in Section 4.6.4, the most effective way to account for the propagation of inaccuracies is to combine the sur-face of interaction and value alignment based edge weights into a single weight which accounts for both shapes of the sub-tensors and their value alignments.
 Varying the Number of Partitions. Figure 7 verifies whether the observations also hold when the number of par-titions of the tensor is larger (i.e., 4  X  4  X  4 vs. 2  X  2  X  2). Due to the large number of alternatives, for the 4  X  4  X  4 partitioning scenario, the two partitions in the user focus are generated as follows: for each of the 64 partitions, 10 other partitions are paired with it randomly, leading to 640 pairs (a) Epinions accuracy (b) Enron accuracy Figure 7: Experiment results for 2  X  2  X  2 vs. 4  X  4  X  4 partitioning with 2 partitions in focus (least balanced partitioning configuration; also Ciao results are omitted due to ) of partitions used as user focus (consequently, each partition is guaranteed to be in user focus at least 10 times).
As we see in Figure 7, PTD based algorithms boost the accuracy for the high priority regions of the tensor. More-over, the decomposition time gain table shows that the time number of partitions of the input tensor increases. Varying the Number of High-Priority Partitions. Figure 8 studies the impact of the number of partitions that are in the user focus. Due to the large number of alterna-tives, the k partitions (for 1  X  k  X  4) in the user focus are generated as follows: each of the 64 partitions for the 4  X  4  X  4 Figure 8: Results for varying number of partitions in focus (least balanced partitioning configuration for 4  X  4  X  4 ; also Ciao results and Epinions accuracy results are omitted due to space constraints) partitioning scenario has been paired with 10 ( k  X  1) parti-tions selected randomly, leading to a total of 640 partition k -tuples that are used as high-priority partitions in the user focus (consequently, each partition of the tensor is guaran-teed to be in user focus at least 10 times).

As we see, the PTD based algorithms boost the accu-racy in the high priority region for all user foci considered: the relative gains in accuracy improves from 8% for a single high-priority partition to 29% for 4 high-priority partitions in focus. Naturally, the gains vary depending on the dis-tributions of the values in the tensors at different scales. The decomposition time gain remains high as the number of high-priority partitions increase.
In this paper, we argued that in many data analysis appli-cations, the user may have a focus of interest for which s/he needs high accuracy and s/he may willing to trade accuracy for performance outside of this area focus. Based on this observation, we proposed a Personalized Tensor Decompo-sition (PTD) algorithm for accounting for the user X  X  focus in tensor decomposition. The proposed algorithm partitions the given tensor based on the focus of the user and assigns different initial decomposition ranks for different partitions. (a) PTD first constructs a sub-tensor impact (SI) graph and (b) analyzes this graph in the light of the user X  X  interest to calculate initial decomposition ranks for the partitions of the tensor. Experimental results showed that PTD is very effective in boosting accuracy for high priority regions of the tensor, while reducing the decomposition time.
