 Users on an online social network site generate a large number of heterogeneous activities, ranging from connecting with other users, to sharing content, to updating their profiles. The set of activities within a user X  X  network neighborhood forms a stream of updates for the user X  X  consumption. In this paper, we report our experience with the problem of ranking activities in the LinkedIn homepage feed. In particular, we provide a taxonomy of social network ac-tivities, describe a system architecture (with a number of key com-ponents open-sourced) that supports fast iteration in model devel-opment, demonstrate a number of key factors for effective ranking, and report experimental results from extensive online bucket tests. H.4 [ Information Systems Applications ]: Miscellaneous Design; Experimentation Activity Ranking; Relevance; Large Scale Learning
Professional networking site LinkedIn and other social network-ing sites provide users an easy way to connect with other users and keep track of various updates from them. An important mechanism that helps each user keep track of such updates is through a feed of activities that is centered around her egocentric networks (her con-nections, people she follows, companies she follows, etc). Some examples of activities generated by a user X  X  first degree network include job changes or profile updates, sharing of content, likes or comments on a piece of information, joining a group or connecting with another user. Since the volume of social network activities is usually large and each user has limited time to consume informa-tion in his/her feed, it is important to rank activities according their  X  X elevance X  to the user, while keeping the feed fresh and diverse.
The primary goal of the feed is to maximize long-term user en-gagement, manifested by a user visiting the site more often. How-ever, optimizing the visit frequency of a user is difficult in practice due to long feedback loops. Proxies that are readily measurable with little feedback delay are often optimized in practice. While there are several such proxies, the most popular approach is based on the probability of a user clicking on an activity. Such a click probability is also called click-through rate (CTR).
 Challenges: Ranking activities in social network feeds is challeng-ing because of the following reasons.  X  Scalability : The volume of activities to be ranked is high. The  X  Personalization : There is wide variation in egocentric net-This paper describes our experience working with the production system at LinkedIn. We describe the approaches we took to ad-dress these challenges and the lessons learned. We would like to point out at the very outset that this paper is not about presenta-tion of fundamentally new machine learning methods, but rather about practical challenges faced when deploying machine learning methods in a production environment and our attempts to address them. When studying new data mining and machine learning meth-ods, testing out-of-sample predictive accuracy of machine-learned models on retrospective data is usually the main focus . Although useful, it does not provide the whole story of the challenges in-volved in deploying models in production.

Before we discuss our ranking system, we first describe a cou-ple of potential straw man approaches that are practiced in various industrial settings when dealing with feed recommendation prob-lems. We did not find these to be satisfactory in our problem set-ting.
 Approach 1  X  Reverse chronological ordering of feed activities: Ranking based on reverse chronological ordering (recency) of ac-tivities leads to a fresh but not necessarily a relevant feed. We conducted an online bucket test for three weeks in 2013 compar-ing pure recency ordering with relevance-based ranking (using our model that served the majority of users at that time), and found the CTR of relevance-based ranking to be 43% higher than that of or-dering by recency. 1 By the end of 2013, we further improved our relevance model and obtained an additional 15% gain. We believe that a well-designed relevance-based ranking system can signifi-cantly outperform recency-based ranking. In our scenario, even after achieving 60% lift (in CTR), there is still more room left for improvement.
 Approach 2  X  Ranking by social popularity: On LinkedIn, users can like (by clicking on the  X  X ike button X ) or comment on activi-ties. Social popularity can be defined as the number of likes (i.e.,  X  X ike button X  clicks) or comments. It may seem at the first glance that ranking activities according to their social popularity is good enough. This is not true in our scenario. Firstly we note that a non-negligible fraction of activities do not receive any like or com-ment on LinkedIn, despite being shown to a non-negligible set of users. For those that do receive likes or comments, the CTR is not a monotonically increasing function of social popularity as shown in Figure 1. This does not imply that social popularity is not a use-ful signal, but that it is extremely sparse and requires systematic modeling for leverage.
 Contributions: In this paper, we make the following contributions:
This experiment only considers each user X  X  first feed visit in a session. Subsequent visits in a session are served through other methods. http://www.project-voldemort.com/voldemort/ http://www.senseidb.com/ Terminology: In the rest of this paper, the terms activity and item are used interchangeably both referring to a professional network activity. The term impression is used to refer to an item X  X  appear-ance in a user X  X  LinkedIn feed. Because users primarily respond to an activity in the feed through clicks (including clicks on links to see more information and clicks on buttons to share, like or submit a comment on the activity), we combine all these responses and denote it as the click-through rate (CTR) and use it as our primary measure to optimize in this paper.
Ranking activities from a user X  X  interpersonal network has al-ways been important in social networks. One line of work is to rank homogeneous tweet feeds in Twitter. Chen et. al. conducted a set of user studies for information stream recommendation on Twitter. Their purpose was to manually study how various features result in various user preferences to the Tweet feeds. They studied content sources, topic interest of users, and social voting on items in [8]; and thread length, topic relevance and tie-strength in [7], respectively. Their work paved the way for the future automated studies. For example, [1, 11] enriched user profiles with their ac-tivities on Twitter for a personalized news recommendation system; and [6] produced different rankers for different sub-communities of a user X  X  social network, where these rankers are described as topics so that the user can rank his/her social feeds by specific topics. As this line of work mainly focused on homogeneous feeds, they are essentially close to the earlier personalized news feed recommen-dation work [10], except that they used additional social network features like social voting, social tie strength etc. to model the ac-tivity relevance. Furthermore, this line of work did not conduct large-scale study.

More recently, researchers started to pay attention to the hetero-geneous nature of the network activity ranking problem. Starting from small-scale data, [18] identified different types of user ac-tivities from a few users X  tweet feeds in Twitter; and [9] investi-gated the importance of diversity on the Twitter item recommen-dation and found that content was more relevant to users when it was highly homogeneous or highly heterogeneous. These works inferred heterogeneous activity types from Twitter X  X  text feeds.
Subsequently, researchers started to directly study the hetero-geneous data. Berkovsky et. al. computed the relevance of the feed items using user-user relationship strengths and user-action interest scores for the social feed personalization in the online To-tal Wellbeing Diet (TWD) portal [3] and an experimental eHealth portal [2] respectively. These personalized streams attracted more user attention compared to the chronologically ordered feed lists. Paek et. al. [19] attempted to understand how people judge the importance of their newsfeed in Facebook by asking some Face-book users to rate the importance of their newsfeed posts as well as their friends X . Classifiers were learned to identify predictive fea-tures for the newsfeed and friend relevance. Bourke et. al. [4] improved social stream relevance by leveraging features from mes-sages, content source, and the users for Facebook. Soh et. al. [20] recommended Facebook social feeds to users by exploiting their re-sponse behavior in the past. Based on SocialBlue X  X  news feeds, [12] computed the relevance of network activity feeds from the observed interactions of the individuals in the past. They diversified features into four types: user-user, user-action, user-object and age of activi-ties. Some researchers also investigated the heterogeneous network activity stream personalization problem for enterprise network ac-tivity streams [13, 14]. They concluded that the activity stream based user profile is more productive than entity-based user profile for personalizing the stream. Despite the boom of this line in re-search, all the above work on heterogeneous activity feed ranking has not been evaluated on top of large-scale production systems.
The closest related work is [15], where various machine learn-ing techniques including linear models on features and latent factor models (matrix factorization and tensor factorization) were applied to activity ranking in LinkedIn feed. Their focus was on offline modeling. Here, we present an overview of the end-to-end system and online bucket test results.
In this section, we study several important aspects of activity ranking based on randomized data. In particular, we assigned a small fraction of users to the random bucket . For each user in the random bucket, we shuffle (uniformly at random) the top-100 ac-tivities that could appear in the user X  X  feed and present the random-ized feed to the user. For details, see Section 4.2. Such randomized data helps to reduce the serving bias caused by the ranking algo-rithm used during the data collection period and the positional bias caused by potential high likelihood of placing certain kinds of ac-tivities at certain positions in the feed.
 We start by introducing different types of activities in the feed. We then show some interesting characteristics of our data. They motivate our relevance modeling described later.
In this section, we present a taxonomy of different types of ac-tivities in a social network. In general, each activity can be repre-sented as a triple: (actor, verb, object). For example, member A connects to member B , member A shares article C , and member A updates her profile picture D . Each user has two roles in a so-cial network. On the one hand, a user is an actor who generates activities for other users to consume. On the other hand, a user is also a viewer , who receives a stream of activities generated by the actors in her network. The problem we study is how to rank this activity stream for each viewer. We note that, in addition to users, there may be other kinds of actors in the network. For example, in the LinkedIn network, companies, schools and content channels are actors as well, which can share different kinds of information.
The type of an activity, also called an activity type , is a triple of (actor type, verb type, object type), e.g., (member, connect, mem-ber), (member, share, article) or (member, profile-update, picture). We broadly classify activities in social networks into the following five categories and also show examples of activities on LinkedIn in Table 1. 1. Connection activities: These are the activities that add new edges in the social network. We can further classify these activities into:  X  Symmetric connection : For example, the event of a member  X  Asymmetric connection : For example, the event of a member 2. Informational activities: These are the activities where entities in the network pass information to others. In the LinkedIn network, members and companies can share messages, articles, pictures, dis-cussions (in groups) or jobs. Figure 2: Relative CTRs of different activity types over a 10-week period. The y -axis is log-scaled. 3. Profile activities: These are the activities generated due to users X  profile changes. Examples of such profile changes on LinkedIn include updates of profile pictures, contact information, job posi-tions, etc. 4. Opinion activities: These are activities where users express their opinions on different kinds of objects (e.g., articles, pictures, discussions, etc.). Two kinds of opinions are common on social network sites: like and comment , where the former takes a user little effort to express his/her opinion, while the latter requires a user to put his/her opinions into words. 5. Site-specific activities: The above four categories of activities generally appear on many social network websites. However, each website may also have some activities specific to its purposes or needs. For example, LinkedIn is a professional social network and has job-related activities: job anniversaries, endorsements of users X  skills and job recommendations.
 Characteristics of CTR of activity types: Users X  response to dif-ferent activity types vary. In Figure 2, we show the CTRs of differ-ent activity types relative to the average CTR of all types for users in the random bucket. Each curve represents the CTR of an activity type over a 10-week period. Due to confidentiality reasons, we do not label the curves. However, the key message from this plot is that a few activity types have much higher CTRs than others. Also note that the CTRs of some activity types do not change much over time, while the CTRs of some other types can have large temporal variations.
In this section, we show how freshness of activities affects click-through rate. We consider two aspects of freshness. First, the age of an activity since its creation. Second, the number of times a user has seen a particular item in the past. Note that these two notions of freshness can differ significantly for different users. For example, a heavy user might see an activity several times within the first hour of its lifetime. In contrast, a light user who did not visit the feed for a week would not have seen any activity less than 7 days old.
To measure the effect of age of activity on CTR, we analyzed the impression and click data collected from the random bucket (de-scribed in Section 4.2) over a period of six weeks. Even though the random shuffling mechanism used in the random bucket helps to Figure 3: Effect of activity age on CTR for information sharing (left) and for profile picture updates (right). The CTRs have been normalized so that the CTR at age 0 is one. remove serving and positional biases, the number of times an item has been seen by a user in the past can still confound the analysis. To account for any potential bias due to user fatigue from repeated impressions, we only considered the first impression of an item for each user. The age of an item was coarsely divided into a few buck-ets. For example, if the age of an item was between zero and twelve hours, it was placed in the zero bucket. If the age was between 12 and 24, it was placed in the bucket corresponding to age 12 and so on. For each of these buckets, we estimated the CTR based on the items that were presented to users.

The CTR curves of two different activity types are shown in Fig-ure 3. Due to proprietary reasons, for each activity type, the y -axis have been normalized based on the CTR of the first bucket. The plots show that the decay of CTR with age can look different for different types of activities. In the case of profile picture updates, there is a spike in CTR a few hours after the activity is created which then drops down subsequently. In contrast, in the case of information sharing there is a sharp drop in CTR initially followed by a much slower decay from there on.
To determine the variation in CTR with repeated impressions, we also use the random bucket data. For each activity, we counted the number of past impressions of the activity for each user. If an item never appeared in a user X  X  feed before, the number of past impressions for that (user, item) pair would be zero.

The behavior of CTR with the number of previous impressions of the same activity is shown in Figure 4. In this plot, the x-axis shows the number of times an activity was seen before and the y-axis shows the corresponding CTR. The plot has been normalized by the CTR of the first impression. It is obvious from the plot that the CTR of an activity decreases as it is seen multiple times. This curve suggests the need for discounting the predicted CTR based on the number of previous impressions of an activity.
LinkedIn feed consists of diverse types of activities as introduced in Section 3.1. In this section, we analyze how CTR changes with respect to the diversity in the feed using the random bucket data. For each feed that was presented to a user, we consider the ID of the actor of the activity at position 10 and count the number of times activities from the same actor appeared in positions 1 to 9. When this count is high, this corresponds to a scenario where most activ-ities in the feed have the same actor. In other words, a high count reflects lack of actor diversity in the feed. Similarly, we considered the counts of the same verb type or same object type in positions 1 to 9. To avoid any bias cased by users with very few connections (thus, it is unlikely to have diverse feeds for them), we restricted this analysis to only those users who had at least 50 connections. The decay of CTR with respect to repeated actor ID, verb type and object type is shown in Figure 5. In all the plots, the y -axis have been normalized so that the CTR in the case of no repetition (i.e., #repetitions = 0) is one. It can be seen that for all the three different types of repetitions, there is a rapid initial decay in CTR.
Since members on LinkedIn are connected via a professional network, we can study how the connection relationship between a viewer and an actor affects whether the viewer would click on an activity of the actor. A few examples of connection relationship between a viewer and an actor are in the following. Table 2: CTR lift when the viewer and the actor have the same characteristic compared to the case where they do not.
 Effect of connection relationship: We now present some observa-tions from our data with respect to a couple of these similarity mea-sures by looking at whether the viewer and the actor belong to the same company, have the same job function, are in the same industry or in the same geo-region. Table 2 shows that when the viewer and the actor have similar characteristics, there is a higher chance that the viewer would click on an activity of the actor. However, such behavior also depends on the type of activity. For example, con-sider whether the viewer and actor are in the same company or not. Averaged over all activity types, the CTR of  X  X ame company X  is 66% higher than the CTR of  X  X ifferent companies X . However, Ta-ble 3 shows that it is not the case for job-change type of activities. A viewer clicks more often on a job-change of an actor in a differ-ent company than a job-change of an actor in the same company, perhaps due to the high likelihood that the viewer already knew her connections in her company changed jobs.
From our analysis, we observe that users X  interaction with activ-ities differ significantly across the mobile and desktop platforms. We first looked at the impression distribution across different age Table 3: CTR lift when the viewer and the actor are in the same company for different types of activities.
 Figure 6: Behavior of different age groups of viewers on mobile (red) vs. desktop (blue) groups of the viewer. It is evident from Figure 6a that the distri-bution of impressions is similar across age groups on the two plat-forms. We then looked at the CTR by age groups on these two plat-forms. This is shown in Figure 6b. Due to proprietary reasons, the plot only shows the deviation of CTR for an age group compared to the overall CTR on that platform. Surprisingly, although the im-pressions distribution is similar on the two platforms, different age groups respond differently on mobile and desktop. This example shows differences in user behavior across different age groups on different platforms. It is important to analyze users X  behavioral dif-ferences on different platforms and build models that capture the behavior specific to each platform. As another example, we will show in Section 5 that a mobile specific model performs signifi-cantly better than simply using a desktop model to rank activities on our mobile feed.
In this section, we first give an overview of our system as of 2013 and then outline our model training process which includes data collection, large-scale logistic regression and an offline replay methodology for model evaluation. The primary goal of relevance modeling is to accurately predict the CTR of a given user-item pair, i.e., the probability of the user clicking on the item. In additional to CTR prediction, it is also important to incorporate mechanisms to ensure freshness and diversity. These will be discussed in Sec-tion 4.5.
At a high level our system consists of online components for serving users X  feed requests, and also offline components for gen-erating features and training models on Hadoop. Figure 7 shows the main components of our system and their interaction with one another. We would like to mention that two of these important components, the feature store (Voldemort) which provides large-scale feature storage and retrieval, and the item index (SenseiDB) which indexes items and ranks them according to a scoring model, have already been open-sourced and we are in the process of open-sourcing the model training component.

When a user logs in to LinkedIn, the web-server sends a re-quest for the feed to the feed service. Activities generated on the LinkedIn professional network are stored in SenseiDB. When the feed service receives the ID of the viewer, it retrieves the appropri-ate model for the viewer (depending on the online bucket test run-ning at that time) and sends the model to SenseiDB to score the can-didate activities for the viewer stored in SenseiDB. The SenseiDB stores item specific features (for example: activity type, creation time etc). Some of our models incorporate additional features that are either not item specific or are derived through additional com-putation. For example, we use a score for the affinity of the viewer to each activity type in our models. Such features are generated by separate feature pipelines on Hadoop, and pushed to a Voldemort store which is a distributed key-value store. The feed service can then access these features efficiently from the Voldemort store. The feature pipelines generate features from the tracking data which is a log of impressions (with basic features of activities) and clicks. The features produced by the feature pipelines are also used in the training process which generates models for use in SenseiDB. More details on model training, features used and evaluation are provided next.
Collection of training data for our application can be a non-trivial exercise. In the absence of a good ranking model, a seemingly natural option is to rank activities in a reverse chronological order and present to the viewers. The response information (click/no-click) thus collected could then comprise the training dataset. How-ever, this method of training data collection can severely suffer from serving bias in the following way. Usually viewers X  atten-tion is drawn primarily to the first few results in the feed. So if the serving scheme never serves activities of certain types to a viewer, the viewer cannot give his/her feedback on such activities. At LinkedIn, the frequencies at where different types of activities are generated are quite different. For example there are many more member connect member updates in the candidate set of activities than some other of the other types. This implies that a reverse chronologically sorted stream of a typical viewer would comprise of a large number of connection activities and other infrequent ac-tivities, like job changes, will rarely, if ever, get impressed on the viewer although they might be more relevant. This leads to data sparsity problem for a number of activity types.

The most common approach for eliminating serving bias is ran-dom serving, where activities in the candidate pool are ranked ran-domly. However, this scheme is likely to cause very poor user ex-perience due to the following two reasons. First, many irrelevant and stale activities might get promoted to the top positions. Sec-ond, the set of activities presented at the first few positions would change dramatically upon each page reload. Moreover, the feed would still be dominated by a few types of activities that are more common than the others.
 We overcome the aforementioned problems in the following way. We first use a ranking algorithm to rank activities. Then, the top-k results are permuted uniformly at random to prepare the final feed for the viewer. This scheme removes the serving bias from the training data to a large extent, without completely sacrificing user experience. Our training data consists of activities that were presented using this mechanism. The activities on which a viewer clicked are considered positive examples and those with which the viewer did not interact are considered negative examples. Such training data comes at the cost of reduced user experience due to randomization. Since model training/re-training is a continuous process which entails persistent availability of fresh training data, this cost must to be minimized. This is done by serving this ran-domized feed to only a small fraction of our viewers, and frequently changing this bucket of viewers. Also, we keep updating the under-lying ranking model (which produces the top-k results for random-ization) to the current best model. This ensures an improving user experience even for those viewers who happen to fall in the random serving bucket.
A score is required to rank the activities in the candidate pool. As discussed earlier, we use the predicted CTR as the score. To predict the CTR of each activity, we learn a logistic regression model with ` regularization. Let y denote the click on the activity described by feature vector x then the corresponding Bernoulli random variable Y is modeled as where  X  is the parameter vector which we want to learn. Once  X  has been estimated from the training data, for any new example x new we simply use the mean of the Bernoulli distribution as the predicted CTR, i.e.,
CTR predict = P ( { Y = 1 } ) = E [ Y ] = 1 1 + exp(  X   X  &gt; x
We train a logistic regression model with a number of features, such as features extracted from the viewer X  X  and actor X  X  profile, fea-tures qualifying the connection strength and similarity between the viewer and the actor, features representing the age of an activity, features quantifying the affinity between the viewer and the type of an activity and the affinity between the viewer and the actor. Note that some of the features are generated using other models  X  lo-gistic regression can be thought of as a tool to combine such base models. We also include interactions among the aforementioned features.
We train the logistic regression model using a Hadoop based dis-tributed logistic regression algorithm. Our implementation of lo-gistic regression is based on the Alternating Direction Method of Multipliers (ADMM) [5] which is a popular method for solving a convex optimization problem in a distributed fashion. We are in the process of making our implementation open source so that others facing similar challenges can benefit from it. The ADMM algo-rithm partitions the set of training examples into several blocks. Each block is assigned to a machine which solves (in parallel) a convex optimization problem independently on the training exam-ples in that block. Solutions from different partitions are collected and a consensus solution is formed. The consensus solution is then sent to individual machines which update their solutions again. This process is repeated until convergence. More details can be found in [5]. This method is scalable and it allows us to train our models with a large number of training examples collected from our random bucket.

After we train a model, we need to evaluate the model before de-ploying it in production. For this, we use a replay methodology for unbiased offline evaluation of online serving schemes [17, 16]. We use the offline replay methodology since it is a theoretically sound way of evaluation. Typically we consider a few weeks of data from the random bucket (described in Section 4.2), starting from a point in time after the period over which the training data was collected. For replay, the activities that were presented at serve time in the ran-dom bucket are reordered using the candidate model obtained via training described above. After reordering, we only consider im-pressions that occur at exactly the same position as they appeared at serve time. We call these matched impressions and count the to-tal number of clicks on such matched impressions. We call the total number of clicks on matched impressions as reward. We typically consider reward at the first feed position and the reward at the first three feed positions when comparing our models. Although the absolute value of reward may not be very meaningful, the relative rewards obtained from two different models are typically indicative of which model would perform better in production.
Our model training described above focused only on accurately modeling the CTR of each user-activity pair. However, we pointed out in Section 3 that the click-through rate can also be influenced by aspects such as diversity and freshness.

Although relevance and diversity can be modeled jointly, such approaches typically require greedily picking the next activity based on the current set of items, which can be computationally pro-hibitive in a real online system serving millions of users. Hence we use a simple mechanism to enforce diversity in the feed. In this mechanism, we first score and sort all the candidate activities using the ranking model. We then go through the ranked list and count the number of times a particular actor or object appeared on the list before each item. The score of each item is discounted based on this count in a way that more prior occurrences of the same actor or object leads to a stronger discounting. In our experiments, we simply fix this reranking mechanism for all our models.

It is also important to keep the feed fresh. However, simply learning age based and impression count based coefficients from the data may not provide desired level of feed freshness. Since we also do not want to give a completely time sorted feed, we perform an additional exponential decay on the score based on the age of activities. The half lives of this decay are set in accordance with product requirements (which is influenced by user experience stud-ies). Similarly, repeated impressions of the same activity is also discouraged by introducing an additional decay factor that takes into account past impression counts of the activity. These rerank-ing modules allow an easy way of tuning the feed to match required characteristics.
In this section we describe several online bucket tests (A/B ex-periments) on the LinkedIn feed and a few offline replay experi-ment results. Most of our experiments are based on a large number of real users who visited LinkedIn. LinkedIn currently has more than 300 million members geographically spread across the world. A large fraction of visitors see the LinkedIn feed.

We continuously test new features and new models in our pro-duction system. We typically launch a new feature to a small frac-tion of users and if the feature works well compared to existing models, we slowly ramp the feature to more users. At any point in time, we are testing several models in production. It is impossible to present results from all the models that were in bucket tests. In this section, we present several interesting experiments focusing on just one aspect at a time. We also present some results where we combined a number of useful features in our production system. We described our offline replay evaluation metric in Section 4.4. In this section, we show the offline replay results and the corre-sponding online replay results for two models. We considered a baseline model in production that was constructed from the fea-tures mentioned in Section 4.3. We refer to this as Model A. We then added features derived from networks of viewer and actor and learned a new model using our training pipeline. We denote this model as Model B. In offline replay analysis, Model B showed a lift of 4.96% at the first feed position over Model A. When we ran these models in production for two weeks, the lifts that we ob-served for model B in those two weeks were 3.6% and 5.0% com-pared to model A. Although the absolute value of the performance improvement in the first week is slightly different from the offline lift for most of the models we tried in production, we have found that replay provides a sound way for deciding whether a model is a promising candidate for deployment. In the rest of this paper, we provide bucket test results from models deployed online rather than focusing on offline analyses.
In this section, we show the online bucket test results using sev-eral new features that we incorporated into our modeling. First, we show the effect of adding them independently and finally we show the effect when we combined all the features.
As we discussed in Section 4.5, we control the freshness of the feed using an exponential decay on the score based on the age of the activity. To show how different segments of users behave differ-ently to different levels of freshness, we consider two models that we had in our bucket tests. The two models were exactly the same except for the half life period on the exponential decay. Model A had an exponential decay with a half life of four days (which means the score predicted by the model halves every four days), while model B had an exponential decay with a half life of two days. Al-though the overall CTR for model B was slightly higher than that of model A, the results are more interesting when we segment users and look at the lifts on the individual segments. Users were divided into four segments (light, moderate, regular, heavy) based on the number of visits to LinkedIn feed. For heavy users, model B had a lift of about 25%, while for regular users model B had a smaller lift of about 7%. However, for moderate and light users, model A had a lift of 2.3 % and 6.6% respectively over model B. These results intuitively make sense. Visitors who come to LinkedIn more often expect to see fresher content, whereas visitors who visit LinkedIn less often are happy to consume older content.

In the previous paragraph, we described bucket tests with age based freshness as the criterion. We now consider the effect based on repeated impressions of items. As we described in Section 3.2.2, we first estimated how the CTR decays with repeated impressions of activities. One challenge for impression discounting is that we have to store the number of past impressions to every activity that is served to a user. This requires a large key value store, where key is the ID of an activity and value is the number of past impressions of that activity for each viewer. We make use of a Voldemort store which was described in Section 4.1. We store the number of past impressions for several millions of activities such that these counts are available at runtime on our production systems. We then ap-plied impression discounting to our best model at that time. By ap-plying impression discounting, there was an improvement in CTR of about 2.2% during a period of one week. However, for our heav-iest user segment (who visit our feed more than 10 times on an average per day), there was an improvement of over 5%.

In this section, we show results from couple of model improve-ments that we achieved through personalization. Our first approach was to develop an affinity score between viewers and activity types. As we described in Section 3.1, there are several activity types on the LinkedIn feed. Since different LinkedIn members can have dif-ferent affinity towards different types of activities, we model the affinity between users and activity types. This affinity score was devised based on historic data of interaction between viewers and activity types. The basic idea of this score is to segment users based on their features (such as industry, job seniority, company size, language etc.). For each segment, we look at those user X  X  his-toric data to collect how often they interacted with different activ-ity types. From all this information, we compute an affinity score which is then used as a feature in modeling the click-through rate prediction via logistic regression. A second approach was to use a personalized age based decay for every user depending on time based features. This involved utilizing the last visit time of a user to LinkedIn. This feature effectively personalizes the feed such that heavy users tend to get fresher content. We note that the model with the time based feature also included impression discounting when we tested in production.

We trained models that consisted of each of the above feature in-dividually combined with the features in our baseline model. The improvement from adding viewer activity type affinity features and personalized decay (with impression discounting) were 6.5% and 11.5% respectively over a period of one week. The results from our bucket tests over the days of the week are also shown in Fig-ure 8a. While there are some variations over days, the overall trend clearly indicates performance lifts from personalization and im-pression discounting.
Previous paragraphs showed how adding each feature individu-ally improved user engagement. We then trained a model jointly with all the features described in the previous paragraphs (i.e., im-pression discounting, affinity feature and time based features) com-bined with the features in our baseline model. Since we usually discontinue inferior models in our production system, the result in Figure 8b only compares the all-inclusive model against the best performing models from Figure 8a which were active at the same time. As evident from Figure 8b, over a period of one week, the combined model ("With All Features" in figure) achieved a no-table improvement of 7.3% over the model including only viewer activity type affinity ("Viewer-type affinity"), and 5.1% over the model including only personalized decay and impression discount-ing ("With Time Features").
To understand the effectiveness of using the mobile specific mod-els, we performed a bucket test for a period of 7 days. The results are shown in Figure 9. We first partitioned our mobile user base into 3 buckets. We then use the CTR performance of the best desk-top model (trained on data from our desktop feeds) to serve traffic on mobile as the baseline (the curve labeled  X  X esktop X  in Figure 9). We also had a model which was trained using data collected from the random bucket. The mobile specific model (labeled  X  X obile X ) consistently outperformed the desktop model. A mobile model with impression discounting (labeled  X  X obile+imp_discount X ) had a significant lift over the 7 day X  X  period. We introduced the problem of ranking activities in LinkedIn feed. We showed the diverse nature of our feed through a taxonomy of activities. We analyzed various characteristics of our data and ar-gued that a relevance system should take into account many of these characteristics to be successful in production. We then described our system architecture and showed how we collect training data, generate features, continuously train models using a distributed al-gorithm and evaluate those models offline before launching them in production. Finally, we showed several bucket test results which show how personalization and other modeling efforts help to im-prove the system.

Here are some lessons that we learned during the course of this work: [1] F. Abel, Q. Gao, G.-J. Houben, and K. Tao. Analyzing user [2] S. Berkovsky, J. Freyne, S. Kimani, and G. Smith. Selecting [3] S. Berkovsky, J. Freyne, and G. Smith. Personalized network [4] S. Bourke, M. P. O X  X ahony, R. Rafter, and B. Smyth.
 [5] S. Boyd, N. Parikh, E. Chu, B. Peleato, and J. Eckstein. [6] M. Burgess, A. Mazzia, E. Adar, and M. Cafarella.
 [7] J. Chen, R. Nairn, and E. H. Chi. Speak little and well: [8] J. Chen, R. Nairn, L. Nelson, M. Bernstein, and E. H. Chi. [9] M. D. Choudhury, S. Counts, and M. Czerwinski. Identifying [10] W. Chu and S.-T. Park. Personalized recommendation on [11] G. D. Francisci, A. Gionis, and C. Lucchese. From chatter to [12] J. Freyne, S. Berkovsky, E. M. Daly, and W. Geyer. Social [13] I. Guy, I. Ronen, and A. Raviv. Personalized activity streams: [14] I. Guy, T. Steier, M. Barnea, I. Ronen, and T. Daniel. [15] L. Hong, R. Bekkerman, J. Adler, and B. D. Davison. [16] J. Langford, A. Strehl, and J. Wortman. Exploration [17] L. Li, W. Chu, J. Langford, and X. Wang. Unbiased offline [18] M. Naaman, J. Boase, and C.-H. Lai. Is it really about me? [19] T. Paek, M. Gamon, S. Counts, D. M. Chickering, and [20] P.-H. Soh, Y.-C. Lin, and M.-S. Chen. Recommendation for
