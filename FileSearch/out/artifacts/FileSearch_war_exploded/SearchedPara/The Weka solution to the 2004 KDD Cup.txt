 This short communication describes the Weka solution for the 2004 KDD cup problems, mostly focusing on the bioin-formatics problem, where this approach performed best among all submissions. Differences were not significant for the best three submissions, though. The identical setup trained for the physics problem achieved rank nineteen, which is still reasonable. The same final setup achieved overall first place on the pro-tein homology prediction problem, and ranked nineteen on the quantum physics problem. Both problems are described in a lot more detail in the introductory paper in this journal written by the KDD cup organizers. The homology predic-tion problem was slightly non-standard, as the attributes actually describe the outcomes of various methods trying to estimate the similarity between pairs of protein sequences. Unique identifiers for both sequences were available, and the scoring was done in a one-to-many fashion, collecting all re-sults for one sequence into a bag. This approach can be viewed as a variation of multi-instance learning [1]. Even though an extension for multi-instance learning is available for Weka [2], it was not used as it currently only supports standard multi-instance learning and not this slightly differ-ent setting. Bearing in mind that the homology problem is an extremely skewed two-class prediction problem with an apriori proba-bility of 0 . 88% for the minority class, it may seem amazing that such good results as reported below are possible at all. A possible explanation is the fact that this is almost a meta-learning problem, as the inputs being used for prediction are already outputs of other sophisticated predictors including learning approaches like hidden Markov models. Therefore these attributes are of rather high quality, as can also be seen from Figures 1, 2, and 3 which depict plots of all at-tributes. One can clearly see some attributes with almost bell-shaped blue bumps representing the majority class, and well-separated long red tails representing the minority class. Attributes 3, 10, and 23 in Figure 3 are typical examples for that pattern. The remaining two figures show similar shapes for many of the other attributes.
 The size of the datasets is just about within the range of datasets that can reasonably be handled by Weka on a cur-rent PC with about half a gigabyte of memory. Therefore we decided to employ only two-fold cross-validation instead of the more commonly used ten folds. Given the size of the datasets one can expect reasonable estimates from half the data already. For homology prediction, evaluation was to use the following four performance criteria: Note that only RMSE is actually sensitive to the absolute magnitude of each prediction, whereas for all other criteria these absolute magnitudes have no influence on the outcome, only the relative ranking of examples that they generate is important.
 Unfortunately for this experimental setup Weka currently only supports RMSE directly, and RMSE did not seem to work well as a proxy for the other measures, so standard predictive accuracy was used as a proxy in all the cross-validation experiments.
 As Weka supports a large number of different classification algorithms out of the box, all the standard ones were applied and compared in a first round. Not surprisingly, the follow-ing three algorithms enjoyed a slight edge over the rest: Rank 1 2 3 4 5 6 7 SVM a53 a58 a3 a60 a59 a8 a45 RR a53 a63 a55 a58 a3 a34 a54 DS a53 a55 a58 a59 a60 a54 a3 cross-validation was employed, but instead randomized train-test splits with only 10% of the data being used for train-ing were used. This allowed for more extensive parameter tuning which in turn revealed that a slightly higher-order polynomial kernel of degree  X  1 . 5 as well as a Gaussian kernel of appropriate non-default bandwidth (for Weka this is achieved with the option setting  X  G 0 . 44) actually work slightly better than a linear kernel. Also, larger ensemble sizes, i.e. voting more than three classifiers, seems to be more robust, but the differences are rather small on an ab-solute scale. But given the edge a non-linear kernel enjoys, some attribute engineering might help to further improve the performance of classifiers other than a support vector machine.
 From an application point of view, it is always interesting to try to extract some knowledge from the induced classifiers. For both the linear support vector machine as well as the random rules it is straightforward to at least generate at-tribute rankings, thus revealing the relative importance and merit of each single attribute. Boosted ensembles of trees are harder to inspect and understand, therefore an boosted ensemble of simple decision stumps was generated as a kind of proxy for the full-trees ensemble. Again, the ensemble of stumps provides an attribute ranking. Table 2 lists the top seven attributes for each of the three classifiers. There is considerable agreement on what are the top attributes be-tween the three classifiers. This might be useful information for further data engineering in the domain. As already mentioned above, the exact same setup applied to the Physics task achieved only rank nineteen. Looking at what the winners for that task have done to be significantly better than all other submissions, it is clear that some kind of data preprocessing was essential for success. This data has a lot of missing values, and for some attributes strong pat-terns of correlated missing values are present. This suggests that these values are not actually randomly missing, but more like measurements that are not applicable in certain settings, and which can only be represented as  X  X issing X  in a rigid attribute-value pair representation. Transforming these attributes, splitting up the data into disjoint subsets, and more extensive tuning of especially the support vector machines might have given better results. There are a few potential directions to approach this prob-lem using Weka which have not been tried so far:
