 The goal of this paper is to show that generalizing the notion of support can be useful in extending association analysis t o non-traditional types of patterns and non-binary data. To that end, we describe a framework for generalizing support that is based on the simple, but useful observation that sup-port can be viewed as the composition of two functions: a function that evaluates the strength or presence of a patter n in each object (transaction) and a function that summarizes these evaluations with a single number. A key goal of any framework is to allow people to more easily express, explore , and communicate ideas, and hence, we illustrate how our support framework can be used to describe support for a variety of commonly used association patterns, such as fre-quent itemsets, general Boolean patterns, and error-toler ant itemsets. We also present two examples of the practical use-fulness of generalized support. One example shows the use-fulness of support functions for continuous data. Another example shows how the hyperclique pattern X  X n association pattern originally defined for binary data X  X an be extended to continuous data by generalizing a support function. Categories and Subject Descriptors: H.2.8 [Database Management]: Database Applications -Data Mining General Terms: Algorithms, Theory Keywords: association analysis, support, hyperclique
For binary transaction data, the support of a set of binary attributes (items) X is the number of objects (transactions) for which all the attributes of X have a value of 1. While simple, this notion of support is central to the definition of frequent and maximal itemsets, association rules, sequent ial patterns, and other ideas in the area of data mining known as association analysis [1, 2, 6, 7, 14]. Nonetheless, few ef -forts to extend association analysis to handle non-traditi onal types of patterns and non-binary data do so by modifying the notion of support, and those efforts that do have been specific to the work at hand. Thus, an overall framework for understanding and extending support is still lacking.
The goal of this paper is to provide such a framework and show its usefulness. Towards that end, this paper makes the following contributions: We introduce a framework for support based on a view of support as the composition of two func-tions: a pattern evaluation function that evaluates the strength or presence of a pattern in each ob-ject (transaction) and a summarization function that summarizes these evaluations with a single number. Since a key goal of any framework is to allow people to more easily express, explore, and communicate ideas, we il-lustrate how our framework can be used to describe support for a variety of association patterns. This includes suppor t for traditional frequent itemsets, as well as support for as -sociation patterns such as those based on general Boolean formulas [3, 10] and error-tolerant itemsets (ETIs) [13]. We extend traditional support measures to data sets with continuous attributes. Traditional support mea-sures were designed for binary data, and although a con-tinuous attribute can be mapped to binary attributes, this technique has some well known limitations, e.g., informati on is lost. We illustrate this fact and the usefulness of suppor t functions for continuous data through an example based on Min-Apriori [5]. Also, because the anti-monotone property of support is important for the efficient generation of asso-ciation patterns, we investigate the conditions under whic h support measures for continuous data possess this property . We show how an association pattern defined for bi-nary data, the hyperclique pattern [12], can be ex-tended to continuous data by using a generalized notion of support. The key step is to choose pattern evaluation and summarization functions to construct a ver-sion of support that preserves both the anti-monotone and high affinity properties of the hyperclique pattern. The high affinity property guarantees that the attributes in a hyper-clique are pairwise similar to one another at some minimum level, e.g., have a pairwise cosine similarity of 0.5.
In this section, we review the definitions of support-based concepts used in traditional transaction analysis. An over view of the notation used in this and later sections is provided in Table 1. Also, throughout this document, the terms  X  X ow, X   X  X ransaction, X  and  X  X bject X  are used interchangeably, as a re the terms  X  X olumn, X   X  X tem, X   X  X ariable, X  and  X  X ttribute. X 
Given binary transaction data, the support of a set of binary attributes (items) X is the number of objects (trans-actions) for which all the attributes of X have a value of 1. More formally, for a binary data matrix D , the support of an itemset X  X  I is given by  X  ( X ) = |{ t  X  T : D ( t, i ) = 1 ,  X  i  X  X } where |{ X }| denotes the number of elements that belong to a given set. An itemset is frequent if  X  ( X ) &gt; minsup , where minsup is a specified minimum support threshold.
An association rule, X  X  Y , describes a relationship be-tween two itemsets X and Y such that the items of Y occur in a transaction whenever the items of X occur. We measure the strength of such a relationship by the support of an as-sociation rule ,  X  ( X  X  Y ) =  X  ( X  X  Y ), which is the number of transactions in which the relationship holds, and by the confidence of an association rule , conf ( X  X  Y ) =  X  ( X  X  Y )  X  ( X ) , which is the fraction of transactions containing the items of X that also contain the items of Y .

An important property of support is the anti-monotone property : If X and Y are two itemsets where X  X  Y , then  X  ( Y )  X   X  ( X ). The downward closure or anti-monotone property [14] of standard support can be used to efficiently find frequent itemsets and is the foundation of the well-known Apriori algorithm [1]. If a new support measure also possesses the anti-monotone property, then we may also be able to find its associated patterns efficiently, and thus, in what follows, we shall often focus on this issue.
In the next three subsections, we describe the three con-cepts that are fundamental to our support framework: pat-tern evaluation ( eval ) functions, summarization ( norm ) func-tions, and the support functions that can be created from eval and norm functions. We then show how this support framework can be used to express support for frequent item-sets, general Boolean patterns, and Error Tolerant Itemset s (ETIs) [13].
The evaluation of the strength of a pattern can take var-ious forms. Most commonly, and this is the case for tra-ditional association analysis, the pattern is either prese nt, i.e., the pattern strength is 1, or it is absent, i.e., the pat tern strength is 0. An example of such a pattern is the elemen-twise  X  X nd X  as defined in Table 2. In other situations, such as continuous or count data, a binary evaluation of pattern strength may not be as interesting. For example, suppose that we are interested in sets of values that are relatively homogenous within an object. Then, for non-binary data, the range of the attribute values might be a useful mea-sure of pattern strength X  X ne that gives a wider variation in strength than 0 and 1. This might be useful for count data, such as that in Table 4, which shows the number of times that a term occurs in a document. However, we may want to combine both of the preceding approaches, by measur-ing the strength of the pattern using a continuous measure, such as the range, but then evaluating whether this measure meets a specified condition, such as whether the range of the values is less than a specified threshold.

Thus, an evaluation function, eval , is a function that takes a set of of attributes X  X  I as an argument, and returns a pattern evaluation vector , v , whose i th component is the strength of the target pattern in the i th object. More for-mally, we can write If there are several sets of attributes under consideration , e.g., X and Y , then we will distinguish between their pat-tern evaluation vectors by using subscripts, e.g., v X and v Notice that an eval function may be applied either to a sin-gle object, in which case, it returns a single value, or to a set of objects, in which case, it returns a vector of values. Various eval functions are shown in Table 2.
 Table 3: norm functions. M is the length of the vector, k is a parameter, and w is a vector of weights.
It is useful to summarize the pattern evaluation vector v by a single number, e.g., by using a vector norm [4]. The most common vector norm is the L k norm which is defined in Table 3, along with two of its most useful specific versions , the L 1 and L 2 norms. We also use the squared L 2 norm, L which is the sum of the squares of the components of v . We use the notation L k or norm L k to refer to these functions.
We can also consider norm functions which are weighted sums, where the weights are associated with objects. We identify the following special cases: the weights sum to 1 (t he weighted average norm, norm wavg ); the weights are equal and sum to 1 (the average norm, norm avg ); (the weights are all 1 (the sum norm, norm P ). It is also possible to define the weighted L k norm. For completeness, these norm functions are also shown in Table 3, but for simplicity, we restrict our discussion to the L 1 , L 2 , and L 2 2 norms.
The support of a pattern among a set of attributes X is a function,  X  ( X ), that is the composition of a pattern evalu-ation function, eval , and a summarization function, norm , which summarizes these evaluations with a single number.
Given a support function, the goal is to use it to find sets of attributes that meet some support criterion. If, our sup-port function has the anti-monotone property, as is typical ly the case, then we proceed by setting a minimum support threshold minsup and using an algorithm such as Apriori. The result is a collection of strong pattern sets , 1 i.e., a col-lection of sets of attributes that have support greater than minsup .
We present different choices of eval and norm that re-produce the standard definition of support for binary data. Consider the following three support functions from Table 2: the logical and of the attribute values, eval  X  , the prod-uct of the values, eval Q , and the minimum of the values, binary attributes). For a specific binary transaction, any o f these pattern evaluation functions will produce a 1 exactly when all the attributes of X have attribute values of 1; if any attribute value is 0, then these functions return a 0.
If we use any of these three eval functions to produce the pattern evaluation vector, v , then the L 1 , and L 2 2 norms X  X ee Table 3 X  X ill yield a value that is the count of the number of transactions that have all the items of X .

We adopt the following notation to refer to the different types of support functions that we have created: For example, the support function that is based on the
A Boolean support function,  X  b, L 1 , is any support func-tion that uses a Boolean pattern evaluation function eval and the L 1 norm. (A Boolean pattern evaluation function returns either 0 or 1.) An example of a Boolean support function is the traditional support of an itemset X , which is equivalent to measuring the size of the set of transaction s for which a conjunction of of the items (binary attributes) in X is true. This approach can be generalized X  X ee for ex-ample [3, 10] X  X o more general Boolean formulas that use the logical connectives  X  ( and ),  X  ( or ), and  X  ( not ). Even more generally, we can consider a Boolean pattern evalua-tion function such as eval range &lt;constant [9], where the eval function is not a Boolean formula and where the data may not be binary. To illustrate, consider the data of Table 4. Se t constant = 3 and let X = { term 1 , term 2 , term 3 } . Then the pattern evaluation vector is given by v = eval range &lt; 3 i.e., only 3 documents of Table 4 support the pattern. While  X  eral, Boolean support functions may not be either monotone or anti-monotone.
The name frequent itemset is not appropriate in the general case.
Error tolerant itemsets [13] relax the requirement that every transaction supporting the itemset must contain ever y item. Instead, it is enough that each transaction contain most of the items in the specified itemset. The definition of a strong ETI given below is taken from [13], but modified to make the notation and terminology consistent with that of this paper. For example, we might specify a strong ETI by requiring that each supporting transaction have at least 4 of the 5 specified items ( = 0 . 2), and that at least 2% of the transactions (  X  = 0 . 02) support the strong ETI. Definition 1. Strong Error Tolerant Itemset A strong ETI consists of a set of items X  X  I , such that there exists a subset of transactions R  X  T consisting of at least  X   X  M transactions and, for each t  X  R , the fraction of items in X which are present in t is at least 1  X  . M is the number of transactions,  X  is the minimum support expressed as fraction of M , and is the fraction of items that can be missing in a transaction.

Given a parameter , we can define a Boolean evaluation function eval eti, to detect a strong ETI pattern:
This eval function, together with the L 1 norm, can be used to define a support function for strong ETIs.
The traditional approach to dealing with continuous data in association analysis is to convert each continuous attri bute into a set of binary attributes. This is typically a two step process. First the continuous attribute is discretized, i. e., we find a set of thresholds that can be used to convert the attribute into a categorical variable. Then, each value of t he categorical variable is mapped to a binary variable. How-ever, converting continuous data to binary transaction dat a loses information with respect to both the magnitude of the data and the ordering between values. The motivation for considering continuous support measures is to allow asso-ciation analysis for continuous data without such a loss of information.
We begin our investigation of continuous support mea-sures with an example based on the Min-Apriori algorithm [5] and the data of Table 4. Min-Apriori corresponds to the use of the support function  X  min , L 1 . However, Min-Apriori first normalizes the data in each column by dividing each col-umn entry by the sum of the column entries. The normalized data is shown in Table 5. One reason for using normalization is to make sure that the resulting support value is a number between 0 and 1. Another, perhaps more important reason is to ensure that all data is on the same scale so that sets of items that vary in the same way have similar support val-ues. For example, suppose we have three items i 1 , i 2 , and i and that i 2 = 2 i 3 , while i 3 = 3 i 1 . Without normalization,  X  normalization is often desirable in many domains, e.g., tex t documents.

However, a side-effect of normalization is that individual items can no longer be pruned using a support threshold since all items have a support of 1. In Section 6.3, we discuss normalization in the context of the hyperclique pattern. The computation of the support of the set of attributes X = { term 3 , term 4 } is shown in Table 6, where the first two columns show the normalized values for term 3 and term 4, while the third column shows the minimum of these two val-ues for each row (object), i.e., column 3 is the pattern eval-uation vector v = eval min ( { term 3 , term 4 } ). The support of { term 3 , term 4 } is computed by taking the sum of column 3. Notice that term 3 and term 4 have individual supports of 1, as do all individual terms after normalization. The sup-port of { term 3 , term 4 } is 0.33, which indicates a moderate relationship. By contrast, the support of { term 2 , term 4 } is 0 since these two terms do not co-occur in any document.
An alternative would be to convert the original data to a binary matrix 2 and then compute support. If we express support as a fraction, this yields a support of 0.1 for { term 3, term 4 } . The reason for the discrepancy between the two versions of support is that these two terms do not co-occur much, but both have about a third of their weight in the last document. As an example of a case, where both versions of support are close, the traditional support for { term 1, term 2 } is 0.5, which is similar to the value of 0.53 computed using normalized data and  X  min , L 1 .
 Table 5: Table of document-term frequencies nor-malized to have an L 1 norm of 1. Table 6: Computation of support for the set of at-tributes containing term 3 and term 4 .

We convert entries to a 1 only if they are greater than 0.
The situation with respect to the anti-monotone prop-erty of support depends on the norm and eval functions, as well as the data. We start by defining the concept of an anti-monotone eval function and the conditions under which selected norm functions are monotonic. We then prove a general theorem that relates the anti-monotone property of an eval function and the monotonicity of a norm function to the anti-monotone nature of a support function based on them. (This is important, of course, because an anti-monotone support function can yield efficient algorithms for discovering support based patterns.) Using these results a nd the anti-monotone property of eval min and eval Q , we can then show that support functions based on eval Q or eval and the L k and L 2 2 norms, also have the anti-monotone prop-erty for continuous data. We will also use this result later.
Simply put, an eval function is anti-monotone if its values is guaranteed to be non-increasing as the number of items increases. More formally, we have the following definition: Property 4.1. Anti-monotone Property for Pattern Evaluation Functions A pattern evaluation function, eval , is anti-monotone if, for any two sets of attributes X and Y where X  X  Y , eval ( t, Y )  X  eval ( t, X ) ,  X  t  X  T .

Before proving the main theorem of this section, we need a lemma about norm functions.

Lemma 4.1. For any two vectors u and v of length M , if the L k and L 2 2 norms.

Proof. The L k and L 2 2 norms (and their weighted ver-sions with non-negative weights) are monotonic functions o f the absolute values of the components of u and v .
The following key theorem connects the anti-monotone property of an eval function with the anti-monotone prop-erty of a support function based on it.

Theorem 4.1. Let eval be an anti-monotone, non-negative pattern evaluation function. Then the support functions,  X  Proof. We assume that X and Y are sets of attributes, X = { i 1 , . . . , i k } and Y = X  X  { i k +1 } , where i Let eval ( X ) = v X and eval ( Y ) = v Y . Since eval is anti-monotone, v Y ( t )  X  v X ( t ). Because eval is non-negative, v
X and v Y are as well, and Lemma 4.1 can then be applied to yield norm ( v Y )  X  norm ( v X ) for the L k and L 2 2 property for non-negative data.

The eval functions, eval min and eval Q , have the anti-monotone property, i.e., eval min is anti-monotone for non-negative data and eval Q is anti-monotone for non-negative data between 0 and 1. (These proofs are straightforward and are omitted to save space.) Thus, we can prove the fol-lowing two theorems about the anti-monotone property of support functions based on these two eval functions.
Theorem 4.2. For non-negative data, support functions,  X 
Proof. This follows directly from the anti-monotone prop-erty of eval min and Theorem 4.1.
Theorem 4.3. For non-negative data between 0 and 1, i.e., 0  X  D ( t, i )  X  1 , t  X  T , i  X  I , the support functions,  X 
Proof. This follows directly from the anti-monotone prop-erty of eval Q and Theorem 4.1
A hyperclique pattern [12] is a frequent itemset with the additional requirement that every item in the itemset impli es the presence of the remaining items with a minimum level of confidence known as the h-confidence (or all-confidence [8]). More formally we have the following definition:
Definition 2. Hyperclique A set of attributes, X  X  I , forms a hyperclique with a particular level of h-confidence, where h-confidence is defined as
The following properites of h-confidence are proved in [12]. h-confidence is in the interval [0 , 1] and has the anti-monotone property. The cross support property, which is useful for efficiently finding hypercliques, states that the only possible attributes that can be in a hyperclique with an attribute i for a given level of h-confidence h c are those at-This feature of the hyperclique pattern implies that attrib utes that are too different in terms of their support cannot belong to the same hyperclique pattern. Finally, hypercliques als o have the high affinity property, i.e., items in a hyperclique with a high h-confidence have a high pairwise similarity.
In Section 6, we will show that we can extend the hy-perclique pattern to continuous data. However, even before that, we can show an important relationship between hy-percliques in binary transaction data and the support func-alent to standard support for binary data, we can substi-Equation 10. If we normalize all attributes to have an L 2 Equation 10, hconf( X ) =  X  min , L 2 framework provides a simple interpretation of h-confidence as support for normalized data.

To illustrate this point, we provide an example. In tables 7 and table 8 we show, respectively, some sample data and the same data after it has been normalized to have an L 2 norm of 1. Let X be the itemset consisting of all five items. Then, from Table 7, we see that the (standard) support of X is 3, while the maximum support of any item is 5. Thus, the h-confidence of X is 3/5 = 0.6. Using Table 8, we can compute  X  min , L 2 2 ( X ) by taking the min of each row, squaring it, and then summing, i.e.,  X  min , L 2
In this section, we extend the hyperclique pattern to con-tinuous data by using the  X  min , L 2 straightforward to show that all the properties of h-confide nce Table 7: Example to illustrate h-confidence as support X  X riginal data.
 Table 8: Example to illustrate h-confidence as support X  X ormalized data.
 for binary data that were discussed in section 5.1, also hold for continuous data. However, because of space limitations , we only prove results for the high-affinity property of hyper-cliques with normalized data. Further details are in [11].
As with binary data, the high-affinity property for hyper-cliques with continuous data guarantees that the attribute s are pairwise similar to one another at some minimum level. Specifically, a lower bound for the minimum pairwise cosine similarity is given by the h-confidence of the hyperclique. We formally prove this in Theorem 6.1.

In the following, i and j are attributes i and j interpreted as vectors and they have an L 2 norm of 1.

Theorem 6.1. Cosine high-affinity property. Assume that the data is non-negative and all attributes have an L norm of 1. Let X be a set of attributes with an h-confidence of h c . Then, for any two attributes of X , i and j , cos ( i, j )  X  h , where cos ( i, j ) is the cosine similarity between i and j .
Proof. Line 2 follows from line 1 because i and j are elementwise  X  v for i  X  X or j  X  X . Line 3 follows from the definition  X  min , L 2 2 ( X ) when attributes have an L 2 norm of 1.
To illustrate the high-affinity property for continuous hy-percliques, we use an example based on the data of Table 4. The computation of the support of the set of attributes X = { term 1, term 2, term 3 } is shown in Table 9, where the first three columns are normalized versions of term 1, term 2, and term 3 from Table 4. (Here, we use the L 2 norm, not the L norm as in the Min-Apriori example.) The fourth column shows the minimum of these three attributes for a particular row (object). The support of the three terms is computed by taking the sum of squares of column 4, and that value, 0.38, is also the h-confidence. This is indeed a lower bound for the pairwise cosine similarity, since the lowest pairwi se similarity of these items is 0.6. Table 9: Computation of support for the set of at-tributes containing term 1 , term 2 , and term 3 .
Normalization is not required for extending the hyper-clique pattern to continuous data X  X ee [11]. However, as with Min-Apriori, normalization adjusts for attributes wi th different measurement scales and produces a support value that is between 0 and 1. On the negative side, after normal-ization, all single items have a support of 1 and thus, cannot be pruned by using a support threshold or the cross support property.

To more fully understand the pluses and minuses of nor-malization, we consider two additional facts. First, conti n-uous attributes can have widely different support and still be very similar to one another. This is not true for bi-nary attributes. 3 Second, for continuous hypercliques, the cross support property still dictates that two attributes w ith widely different levels of support cannot be together in a hyperclique with high h-confidence X  X ee [11]. Thus, contin-uous attributes, which are highly similar, but which have widely different support, can only appear in hypercliques with low h-confidence. However, many attributes in such low h-confidence hypercliques will not be very similar to one another.

To summarize, without normalization, we can effectively find continuous hypercliques with highly similar attribute s only if they have similar support. This is exactly the same as with binary attributes. However, to effectively find highl y similar continuous attributes with widely differing suppor t, normalization is necessary.
To save space, we limit our discussion of prior work to that already present in the body of the paper and refer the reader to our technical report [11] for more details.
We have described a framework for generalizing the no-tion of support and have shown that this framework can be used to express support for several existing association patterns: frequent itemsets, general Boolean patterns, an d error tolerant itemsets. We also showed how this framework can be used to extend binary association patterns, e.g., the hyperclique pattern, to continuous data.

There are many possibilities for future work. On the prac-tical side, we plan to explore applications of the continuou s hyperclique pattern. On the theoretical side, we plan to in-vestigate new types of support for non-binary data and non-traditional association patterns, and to explore how confi-
It is straightforward to show that for two binary at-tributes i and j , with  X  ( { i } )  X   X  ( { j } ), that cos ( i, j )  X  p  X  ( { i } ) / X  ( { j } ), where  X  is standard support. dence should be extended for non-standard support mea-sures. Preliminary work in both areas is presented in [11]. Finally, a key benefit of a framework is that it allows re-searchers to more easily express, explore, and communicate ideas. We hope that our framework will prove useful and will motivate additional research in this area. This work was partially supported by NASA grant # NCC 2 1231, by DOE/LLNL grant W-7045-ENG-48, by NSF grant IIS-0308264, and by the Army High Performance Computing Research Center under the auspices of the De-partment of the Army, Army Research Laboratory coopera-tive agreement number DAAD19-01-2-0014. The content of this work does not necessarily reflect the position or policy of the government and no official endorsement should be in-ferred. Access to computing facilities was provided by the AHPCRC and Minnesota Supercomputing Institute. [1] R. Agrawal and R. Srikant. Fast algorithms for mining [2] R. Agrawal and R. Srikant. Mining sequential [3] P. Bollmann-Sdorra, A. Hafez, and V. V. Raghavan. A [4] J. W. Demmel. Applied Numerical Linear Algebra . [5] E.-H. Han, G. Karypis, and V. Kumar. Tr# 97-068: [6] J. Han and M. Kamber. Data Mining: Concepts and [7] J. Hipp, U. G  X untzer, and G. Nakhaeizadeh.
 [8] E. R. Omiecinski. Alternative interest measures for [9] J. Pei and J. Han. Can we push more constraints into [10] R. Srikant, Q. Vu, and R. Agrawal. Mining association [11] M. Steinbach, P.-N. Tan, H. Xiong, and V. Kumar. [12] H. Xiong, P. Tan, and V. Kumar. Mining strong [13] C. Yang, U. M. Fayyad, and P. S. Bradley. Efficient [14] M. J. Zaki and M. Ogihara. Theoretical foundations of
