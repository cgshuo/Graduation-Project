 Apache Spark is an open-source cluster computing framework for big data processing. It has emerge d as the next generation big data processing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce X  X  linear scalability and fault tolerance, but extends it in a few important ways: it is much faster (100 times faster for certain applications), much easier to progr am in due to its rich APIs in Python, Java, Scala (and shortly R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a variet y of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing. This tutorial will provide an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices. D.3.3 [ Concurrent Programming ]:  X  Distributed programming . G.3 [ PROBABILITY AND STATISTICS ]  X  Statistical computing . Algorithms, Measurement, Performance, Design, Reliability, Experimentation, Security, Languages. Distributed Systems, Hadoop, HD FS, Map Reduce, Spark, Large Scale Machine Learning, Data Science. Big data is a broad term for data sets so large or complex that Concretely, imagine an average size laptop (4-8 gig of memory, dual-core, with one terabyte (TB) of disk space) that gets overwhelmed with a machine learning task involving a 3-4 gig dataset; imagine the same laptop taking 3 hours to read one TB of data. Clearly, this type of single node sequential computing is inadequate for processing terabyte s/petabytes of data, which are commonplace in modern day society where both machines (internet of Things (IoT)) and hum ans generate petabytes of data every day (Figure 1). Until recently  X  X ig data X  was very much the purvey of database management and summa ry statistics systems such as Hadoop (HDFS and MapReduce) and was largely underleveraged by machine learning. These systems though useful suffered from their limited utility (challenging to code up; lack of purpose built tools and libraries). This cour se builds on and goes beyond this collect-and-analyze phase of big data by focusing on how machine learning algorithms can be rewritten and in some cases extended to scale to work on petabytes of data, both structured and unstructured, to generate sophisticated models that can be used for real-time predictions. Predictive modeling at this scale can lead to huge boosts in perform ance (typically in the order of 10-20%) over small scale models running on standalone computers that require one to significantly down-sample, and necessarily simplify big data. Concretely, this tutorial focuses on how the Map-Reduce design pattern from parallel computi ng can be extended and more faithfully leveraged to tackle the somewhat  X  X mbarrassingly parallel X  task of machine learning (a lot of machine learning algorithms fit this mold). In this tutorial, this is accomplished via the Apache Spark project and it many related sub projects. Spark has emerged as the next generation big data proce ssing engine, overtaking Hadoop MapReduce which helped ignite the big data revolution. Spark maintains MapReduce X  X  linear scalability and fault tolerance, but extends it in a few important ways : it is much faster (100 times faster for certain applications), much easier to program in due to its rich APIs in Python, Java, Scala (and R), and its core data abstraction, the distributed data frame, and it goes far beyond batch applications to support a vari ety of compute-intensive tasks, including interactive queries, streaming, machine learning, and graph processing This tutorial provides an accessible introduction to Spark and its potential to revolutionize academic and commercial data science practices. It is divided into two parts: Industrial applications and depl oyments of Spark will also be presented. Example code will be made available in python (pySpark) notebooks. We also review some of the limitations of Spark in its current form. See Table 2 for a list of the subject matter that will be covered in this tutorial. [1] Karau, H., Konwinski. A., Wendell, and P., Zaharia, M.. [2] Owen, S., Ryza, S., Lasers on, U., and Wills, J.. 2015. [3] Zaharia, M.. 2011. Spark: In-Memory Cluster Computing for [4] Apache Foundation. 2014. Cluster Mode Overview -Spark [5] Databricks. 2015. Databricks Spark Reference Applications . [6] Shanahan, J. G., and Kurra, G., 2010. Web Advertising: [7] Panda, B., Herbach, J. S., Basu, S., and Bayardo, R. J.. 2009. [8] Tak X cs, G. et al. 2008. Matrix factorization and neighbor [9] Ott, P. 2008. Incremental Matrix Factorization for 
