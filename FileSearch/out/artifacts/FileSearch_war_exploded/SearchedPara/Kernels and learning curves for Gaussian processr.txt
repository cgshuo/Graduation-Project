 Gaussian processes (GPs) have become a standard part of the machine learning toolbox [1]. Learning curves are a convenient way of characterizing their capabilities: they give the generalization error as a function of the number of training examples n , averaged over all datasets of size n under appropriate assumptions about the process generating the data. We focus here on the case of GP regression, where a real-valued output function f ( x ) is to be learned. The general behaviour of GP learning curves is then relatively well understood for the scenario where the inputs x come from a continuous space, typically R n [2, 3, 4, 5, 6, 7, 8, 9, 10]. For large n , the learning curves then typically decay as a power law  X  n  X   X  with an exponent  X   X  1 that depends on the dimensionality n of the space as well as the smoothness properties of the function f ( x ) as encoded in the covariance function.
 But there are many interesting application domains that involve discrete input spaces, where x could be a string, an amino acid sequence (with f ( x ) some measure of secondary structure or biological function), a research paper (with f ( x ) related to impact), a web page (with f ( x ) giving a score used to rank pages), etc. In many such situations, similarity between different inputs  X  which will govern our prior beliefs about how closely related the corresponding function values are  X  can be represented by edges in a graph . One would then like to know how well GP regression can work in such problem domains; see also [11] for a related online regression algorithm. We study this problem here theoretically by focussing on the paradigmatic example of random regular graphs, where every node has the same connectivity.
 Sec. 2 discusses the properties of random-walk inspired kernels [12] on such random graphs. These they have surprising properties on large graphs. In particular, while loops in large random graphs are long and can be neglected for many purposes, by approximating the graph structure as locally tree-like, here this leads to a non-trivial limiting form of the kernel for  X   X   X  that is not constant. The fully correlated limit, where the kernel is constant, is obtained only because of the presence of loops, and we estimate when the crossover to this regime takes place.
 In Sec. 3 we move on to the learning curves themselves. A simple approximation based on the graph eigenvalues, using only the known spectrum of a large tree as input, works well qualitatively and predicts the exact asymptotics for large numbers of training examples. When the kernel lengthscale is not too large, below the crossover discussed in Sec. 2 for the covariance kernel, the learning curves depend on the number of examples per vertex. We also explore how this behaviour changes as the kernel lengthscale is made larger. Sec. 4 summarizes the results and discusses some open questions. We assume that we are trying to learn a function defined on the vertices of a graph. Vertices are labelled by i = 1 ...V , instead of the generic input label x we used in the introduction, and the associated function values are denoted f i  X  R . By taking the prior P ( f ) over these functions f = ( f 1 ,...,f V ) as a (zero mean) Gaussian process we are saying that P ( f )  X  exp(  X  1 2 f T C  X  1 f ) . The covariance function or kernel C is then, in our graph setting, just a positive definite V  X  V matrix.
 The graph structure is characterized by a V  X  V adjacency matrix, with A ij = 1 if nodes i and j are connected by an edge, and 0 otherwise. All links are assumed to be undirected, so that A ij = A ji , and there are no self-loops ( A ii = 0 ). The degree of each node is then defined as d i = P V j =1 A ij . The covariance kernels we discuss in this paper are the natural generalizations of the squared-exponential kernel in Euclidean space [12]. They can be expressed in terms of the normalized graph Laplacian, defined as L = 1  X  D  X  1 / 2 AD  X  1 / 2 , where D is a diagonal matrix with entries d ,...,d V and 1 is the V  X  V identity matrix. An advantage of L over the unnormalized Laplacian D  X  A , which was used in the earlier paper [13], is that the eigenvalues of L (again a V  X  V matrix) lie in the interval [0,2] (see e.g. [14]).
 From the graph Laplacian, the covariance kernels we consider here are constructed as follows. The p -step random walk kernel is (for a  X  2 ) while the diffusion kernel is given by We will always normalize these so that (1 /V ) P i C ii = 1 , which corresponds to setting the average (over vertices) prior variance of the function to be learned to unity.
 To see the connection of the above kernels to random walks, assume we have a walker on the graph who at each time step selects randomly one of the neighbouring vertices and moves to it. The probability for a move from vertex j to i is then A ij /d j . The transition matrix after s steps follows as ( AD  X  1 ) s : its ij -element gives the probability of being on vertex i , having started at j . We can now compare this with the p -step kernel by expanding the p -th power in (1): C  X  Thus C is essentially a random walk transition matrix, averaged over the number of steps s with Figure 1: (Left) Random walk kernel C `,p plotted vs distance ` along graph, for increasing number of steps p and a = 2 , d = 3 . Note the convergence to a limiting shape for large p that is not the naive fully correlated limit C `,p  X  X  X  = 1 . (Right) Numerical results for average covariance K 1 between neighbouring nodes, averaged over neighbours and over randomly generated regular graphs.
  X  X ttempts X . To obtain the actual C the resulting averaged transition matrix is premultiplied by D  X  1 / 2 and postmultiplied by D 1 / 2 , which ensures that the kernel C is symmetric. For the diffusion kernel, one finds an analogous result but the number of random walk steps is now distributed as s  X  Poisson (  X  2 / 2) . This implies in particular that the diffusion kernel is the limit of the p -step kernel for p,a  X   X  at constant p/a =  X  2 / 2 . Accordingly, we discuss mainly the p -step kernel in this paper because results for the diffusion kernel can be retrieved as limiting cases.
 In the limit of a large number of steps s , the random walk on a graph will reach its stationary distribu-tion p  X   X  De where e = (1 ,..., 1) . (This form of p  X  can be verified by checking that it remains unchanged after multiplication with the transition matrix AD  X  1 .) The s -step transition matrix for large s is then p  X  e T = Dee T because we converge from any starting vertex to the stationary dis-tribution. It follows that for large p or  X  2 the covariance kernel becomes C  X  D 1 / 2 ee T D 1 / 2 , i.e. C which the random walk can diffuse along the graph: once this lengthscale becomes large, the covari-ance kernel C ij is essentially independent of the distance (along the graph) between the vertices i and j , and the function f becomes fully correlated across the graph. (Explicitly f = v D 1 / 2 e under the prior, with v a single Gaussian random variable.) As we next show, however, the approach to this fully correlated limit as p or  X  are increased is non-trivial.
 We focus in this paper on kernels on random regular graphs. This means we consider adjacency matrices A which are regular in the sense that they give for each vertex the same degree, d i = d . A uniform probability distribution is then taken across all A that obey this constraint [15]. What will the above kernels look like on typical samples drawn from this distribution? Such random regular graphs will have long loops, of length of order ln( V ) or larger if V is large. Their local structure is then that of a regular tree of degree d , which suggests that it should be possible to calculate the kernel accurately within a tree approximation. In a regular tree all nodes are equivalent, so the kernel can only depend on the distance ` between two nodes i and j . Denoting this kernel value C `,p for a p -step random walk kernel, one has then C `,p =0 =  X  `, 0 and where  X  p is chosen to achieve the desired normalization C 0 ,p = 1 of the prior variance for every p . Fig. 1(left) shows results obtained by iterating this recursion numerically, for a regular graph (in the tree approximation) with degree d = 3 , and a = 2 . As expected the kernel becomes more long-ranged initially as p increases, but eventually it is seen to approach a non-trivial limiting form. This can be calculated as and is also plotted in the figure, showing good agreement with the numerical iteration. There are (at least) two ways of obtaining the result (7). One is to take the limit  X   X   X  of the integral representation of the diffusion kernel on regular trees given in [16] (which is also quoted in [13] but with a typographical error that effectively removes the factor ( d  X  1)  X  `/ 2 ). Another route is to find the steady state of the recursion for C `,p . This is easy to do but requires as input the unknown steady state value of  X  p . To determine this, one can map from C `,p to the total random walk probability S `,p in each  X  X hell X  of vertices at distance ` from the starting vertex, changing variables to S 0 ,p = C 0 ,p that simply describes a biased random walk on ` = 0 , 1 , 2 ,... , with a probability of 1  X  1 /a of remaining at the current ` , probability 1 / ( ad ) of moving to the left and probability ( d  X  1) / ( ad ) of moving to the right. The point ` = 0 is a reflecting barrier where only moves to the right are allowed, with probability 1 /a . The time evolution of this random walk starting from ` = 0 can now be analysed as in [17]. As expected from the balance of moves to the left and right, S `,p for large p is peaked around the average position of the walk, ` = p ( d  X  2) / ( ad ) . For ` smaller than this S `,p has a tail behaving as  X  ( d  X  1) `/ 2 , and converting back to C `,p gives the large-` scaling of C `,p  X  X  X   X  ( d  X  1)  X  `/ 2 ; this in turn fixes the value of  X  p  X  X  X  and so eventually gives (7). The above analysis shows that for large p the random walk kernel, calculated in the absence of loops, does not approach the expected fully correlated limit; given that all vertices have the same degree, the latter would correspond to C `,p  X  X  X  = 1 . This implies, conversely, that the fully correlated limit is reached only because of the presence of loops in the graph. It is then interesting to ask at what point, as p is increased, the tree approximation for the kernel breaks down. To estimate this, we note that a regular tree of depth ` has V = 1 + d ( d  X  1) `  X  1 nodes. So a regular graph can be tree-like at most out to `  X  ln( V ) / ln( d  X  1) . Comparing with the typical number of steps our random walk takes, which is p/a from (4), we then expect loop effects to appear in the covariance kernel when To check this prediction, we measure the analogue of C 1 ,p on randomly generated [15] regular graphs. Because of the presence of loops, the local kernel values are not all identical, so the appro-priate estimate of what would be C 1 ,p on a tree is K 1 = C ij / p C ii C jj for neighbouring nodes i and j . Averaging over all pairs of such neighbours, and then over a number of randomly generated graphs we find the results in Fig. 1(right). The results for K 1 (symbols) accurately track the tree pre-dictions (lines) for small p/a , and start to deviate just around the values of p/a expected from (8), as marked by the arrow. The deviations manifest themselves in larger values of K 1 , which eventually  X  now that p/a is large enough for the kernel to  X  X otice X  the loops -approach the fully correlated limit K 1 = 1 . We now turn to the analysis of learning curves for GP regression on random regular graphs. We assume that the target function f  X  is drawn from a GP prior with a p -step random walk covariance kernel C . Training examples are input-output pairs ( i  X  ,f  X  i of variance  X  2 ; the distribution of training inputs i  X  is taken to be uniform across vertices. Inference from a data set D of n such examples  X  = 1 ,...,n takes place using the prior defined by C and a Gaussian likelihood with noise variance  X  2 . We thus assume an inference model that is matched to the data generating process. This is obviously an over-simplification but is appropriate for the present first exploration of learning curves on random graphs. We emphasize that as n is increased we see more and more function values from the same graph, which is fixed by the problem domain; the graph does not grow.
 The generalization error is the squared difference between the estimated function  X  f i and the target f , averaged across the (uniform) input distribution, the posterior distribution of f  X  given D , the distribution of datasets D , and finally  X  in our non-Euclidean setting  X  the random graph ensemble. Given the assumption of a matched inference model, this is just the average Bayes error, or the average posterior variance, which can be expressed explicitly as [1] where the average is over data sets and over graphs, K is an n  X  n matrix with elements K  X  X  0 = C depends, in addition to n , on the graph structure as determined by V and d , and the kernel and noise level as specified by p , a and  X  2 . We fix d = 3 throughout to avoid having too many parameters to vary, although similar results are obtained for larger d .
 Exact prediction of learning curves by analytical calculation is very difficult due to the complicated way in which the random selection of training inputs enters the matrix K and vector k in (9). However, by first expressing these quantities in terms of kernel eigenvalues (see below) and then approximating the average over datasets, one can derive the approximation [3, 6] This equation for has to be solved self-consistently because also appears on the r.h.s. In the Euclidean case the resulting predictions approximate the true learning curves quite reliably. The derivation of (10) for inputs on a fixed graph is unchanged from [3], provided the kernel eigen-values  X   X  appearing in the function g ( h ) are defined appropriately, by the eigenfunction condition  X  C ij  X  j  X  =  X  X  i ; the average here is over the input distribution, i.e.  X  ...  X  = V  X  1 P j ... From the definition (1) of the p -step kernel, we see that then  X   X  =  X V  X  1 (1  X   X  L  X  /a ) p in terms of the cor-responding eigenvalue of the graph Laplacian L . The constant  X  has to be chosen to enforce our normalization convention P  X   X   X  =  X  C jj  X  = 1 .
 Fortunately, for large V the spectrum of the Laplacian of a random regular graph can be approxi-mated by that of the corresponding large regular tree, which has spectral density [14] positive. (There are also two isolated eigenvalues  X  L = 0 , 2 but these have weight 1 /V each and so can be ignored for large V .) Rewriting (10) as = V  X  1 P  X  [( V  X   X  )  X  1 + ( n/V )( +  X  2 )  X  1 ]  X  1 and then replacing the average over kernel eigenvalues by an integral over the spectral density leads to the following prediction for the learning curve: with  X  determined from  X  R d X  L  X  (  X  L )(1  X   X  L /a ) p = 1 . A general consequence of the form of this result is that the learning curve depends on n and V only through the ratio  X  = n/V , i.e. the number of training examples per vertex. The approximation (12) also predicts that the learning curve will have two regimes, one for small  X  where  X  2 and the generalization error will be essentially independent of  X  2 ; and another for large  X  where  X  2 so that can be neglected on the r.h.s. and one has a fully explicit expression for .
 We compare the above prediction in Fig. 2(left) to the results of numerical simulations of the learn-ing curves, averaged over datasets and random regular graphs. The two regimes predicted by the approximation are clearly visible; the approximation works well inside each regime but less well in the crossover between the two. One striking observation is that the approximation seems to predict the asymptotic large-n behaviour exactly; this is distinct to the Euclidean case, where generally only the power-law of the n -dependence but not its prefactor come out accurately. To see why, we exploit that for large n (where  X  2 ) the approximation (9) effectively neglects fluctuations in the training input  X  X ensity X  of a randomly drawn set of training inputs [3, 6]. This is justified in the graph case for large  X  = n/V , because the number of training inputs each vertex receives, Binomial ( n, 1 /V ) , has negligible relative fluctuations away from its mean  X  . In the Euclidean case there is no similar result, because all training inputs are different with probability one even for large n . Fig. 2(right) illustrates that for larger a the difference in the crossover region between the true (nu-merically simulated) learning curves and our approximation becomes larger. This is because the average number of steps p/a of the random walk kernel then decreases: we get closer to the limit of uncorrelated function values ( a  X   X  , C ij =  X  ij ). In that limit and for low  X  2 and large V the Figure 2: (Left) Learning curves for GP regression on random regular graphs with degree d = 3 and V = 500 (small filled circles) and V = 1000 (empty circles) vertices. Plotting generalization error versus  X  = n/V superimposes the results for both values of V , as expected from the approximation (12). The lines are the quantitative predictions of this approximation. Noise level as shown, kernel parameters a = 2 , p = 10 . (Right) As on the left but with V = 500 only and for larger a = 4 . Figure 3: (Left) Learning curves for GP regression on random regular graphs with degree d = 3 and V = 500 , and kernel parameters a = 2 , p = 20 ; noise level  X  2 as shown. Circles: numerical simulations; lines: approximation (12). (Right) As on the left but for much larger p = 200 and for a single random graph, with  X  2 = 0 . 1 . Dotted line: naive estimate = 1 / (1 + n/ X  2 ) . Dashed line: approximation (10) using the tree spectrum and the large p -limit, see (17). Solid line: (10) with numerically determined graph eigenvalues  X  L  X  as input. true learning curve is = exp(  X   X  ) , reflecting the probability of a training input set not containing a particular vertex, while the approximation can be shown to predict = max { 1  X   X , 0 } , i.e. a decay of the error to zero at  X  = 1 . Plotting these two curves (not displayed here) indeed shows the same  X  X hape X  of disagreement as in Fig. 2(right), with the approximation underestimating the true generalization error.
 Increasing p has the effect of making the kernel longer ranged, giving an effect opposite to that of increasing a . In line with this, larger values of p improve the accuracy of the approximation (12): see Fig. 3(left).
 One may ask about the shape of the learning curves for large number of training examples (per vertex)  X  . The roughly straight lines on the right of the log-log plots discussed so far suggest that  X  1 / X  in this regime. This is correct in the mathematical limit  X   X   X  because the graph kernel has a nonzero minimal eigenvalue  X   X  =  X V  X  1 (1  X   X  L + /a ) p : for  X   X  2 / ( V  X   X  ) , the square bracket in (12) can then be approximated by  X / ( +  X  2 ) and one gets (because also  X  2 in the asymptotic regime)  X   X  2 / X  .
 However, once p becomes reasonably large, V  X   X  can be shown  X  by analysing the scaling of  X  , see Appendix  X  to be extremely (exponentially in p ) small; for the parameter values in Fig. 3(left) it is around 4  X  10  X  30 . The  X  X erminal X  asymptotic regime  X   X  2 / X  is then essentially unreachable. A more detailed analysis of (12) for large p and large (but not exponentially large)  X  , as sketched in the Appendix, yields This shows that there are logarithmic corrections to the naive  X  2 / X  scaling that would apply in the true terminal regime. More intriguing is the scaling of the coefficient c with p , which implies that to reach a specified (low) generalization error one needs a number of training examples per vertex of order  X   X  c X  2  X  p  X  3 / 2  X  2 . Even though the covariance kernel C `,p  X  in the same tree approximation that also went into (12)  X  approaches a limiting form for large p as discussed in Sec. 2, generalization performance thus continues to improve with increasing p . The explanation for this must presumably be that C `,p converges to the limit (7) only at fixed ` , while in the tail `  X  p , it continues to change. For finite graph sizes V we know of course that loops will eventually become important as p in-creases, around the crossover point estimated in (8). The approximation for the learning curve in (12) should then break down. The most naive estimate beyond this point would be to say that the kernel becomes nearly fully correlated, C ij  X  ( d i d j ) 1 / 2 which in the regular case simplifies to C ij = 1 . With only one function value to learn, and correspondingly only one nonzero kernel eigen-value  X   X  =1 = 1 , one would predict = 1 / (1 + n/ X  2 ) . Fig. 3(right) shows, however, that this sig-nificantly underestimates the actual generalization error, even though for this graph  X   X  =1 = 0 . 994 is very close to unity so that the other eigenvalues sum to no more than 0.006. An almost perfect prediction is obtained, on the other hand, from the approximation (10) with the numerically calcu-lated values of the Laplacian  X  and hence kernel  X  eigenvalues. The presence of the small kernel eigenvalues is again seen to cause logarithmic corrections to the naive  X  1 /n scaling. Using the tree spectrum as an approximation and exploiting the large-p limit, one finds indeed (see Appendix, Eq. (17)) that  X  ( c 0  X  2 /n ) ln 3 / 2 ( n/c 0  X  2 ) where now n enters rather than  X  = n/V , c 0 being a constant dependent only on p and a : informally, the function to be learned only has a finite (rather than  X  V ) number of degrees of freedom. The approximation (17) in fact provides a qualitatively accurate description of the data Fig. 3(right), as the dashed line in the figure shows. We thus have the somewhat unusual situation that the tree spectrum is enough to give a good description of the learn-ing curves even when loops are important, while (see Sec. 2) this is not so as far as the evaluation of the covariance kernel itself is concerned. We have studied theoretically the generalization performance of GP regression on graphs, focussing on the paradigmatic case of random regular graphs where every vertex has the same degree d . Our initial concern was with the behaviour of p -step random walk kernels on such graphs. If these are calculated within the usual approximation of a locally tree-like structure, then they converge to a non-trivial limiting form (7) when p  X  or the corresponding lengthscale  X  in the closely related diffusion kernel  X  becomes large. The limit of full correlation between all function values on the graph is only reached because of the presence of loops, and we have estimated in (8) the values of p around which the crossover to this loop-dominated regime occurs; numerical data for correlations of function values on neighbouring vertices support this result.
 In the second part of the paper we concentrated on the learning curves themselves. We assumed that inference is performed with the correct parameters describing the data generating process; the generalization error is then just the Bayes error. The approximation (12) gives a good qualitative description of the learning curve using only the known spectrum of a large regular tree as input. It predicts in particular that the key parameter that determines the generalization error is  X  = n/V , the number of training examples per vertex. We demonstrated also that the approximation is in fact more useful than in the Euclidean case because it gives exact asymptotics for the limit  X  1 . Quantitatively, we found that the learning curves decay as  X   X  2 / X  with non-trivial logarithmic correction terms. Slower power laws  X   X   X   X  with  X  &lt; 1 , as in the Euclidean case, do not appear. We attribute this to the fact that on a graph there is no analogue of the local roughness of a target function because there is a minimum distance (one step along the graph) between different input points. Finally we looked at the learning curves for larger p , where loops become important. These can still be predicted quite accurately by using the tree eigenvalue spectrum as an approximation, if one keeps track of the zero graph Laplacian eigenvalue which we were able to ignore previously; the approximation shows that the generalization error scales as  X  2 /n with again logarithmic corrections. In future work we plan to extend our analysis to graphs that are not regular, including ones from application domains as well as artificial ones with power-law tails in the distribution of degrees d , where qualitatively new effects are to be expected. It would also be desirable to improve the predictions for the learning curve in the crossover region  X   X  2 , which should be achievable using iterative approaches based on belief propagation that have already been shown to give accurate approximations for graph eigenvalue spectra [18]. These tools could then be further extended to study e.g. the effects of model mismatch in GP regression on random graphs, and how these are mitigated by tuning appropriate hyperparameters.
 We sketch here how to derive (13) from (12) for large p . Eq. (12) writes = g (  X V/ ( +  X  2 )) with and  X  determined from the condition g (0) = 1 . (This g ( h ) is the tree spectrum approximation to the g ( h ) of (10).) Turning first to g (0) , the factor (1  X   X  L /a ) p decays quickly to zero as  X  L increases 1) extending the integration limit to  X  gives and this fixes  X  . Proceeding similarly for h &gt; 0 gives g ( h ) =  X r (1  X   X  L  X  /a ) p [ p/ ( a  X   X  L  X  )]  X  3 / 2 F ( h X V  X  1 (1  X   X  L  X  /a ) p ) , F ( z ) = Dividing by g (0) = 1 shows that simply g ( h ) = F ( hV  X  1 c  X  1 ) /F (0) , where c = 1 / [  X  (1  X   X   X  /a ) p ] = rF (0)[ p/ ( a  X   X  L  X  )] we then have = g (  X V/ X  2 ) = F (  X / ( c X  2 )) /F (0) and the desired result (13) follows from the large-z behaviour of F ( z )  X  z  X  1 ln 3 / 2 ( z ) .
 One can proceed similarly for the regime where loops become important. Clearly the zero Laplacian eigenvalue with weight 1 /V then has to be taken into account. If we assume that the remainder of the Laplacian spectrum can still be approximated by that of a tree [18], we get The denominator here is  X   X  1 and the two terms are proportional respectively to the covariance kernel eigenvalue  X  1 , corresponding to  X  L 1 = 0 and the constant eigenfunction, and to 1  X   X  1 . Dropping the first terms in the numerator and denominator of (17) by taking V  X   X  leads back to the previous analysis as it should. For a situation as in Fig. 3(right), on the other hand, where  X  1 is close to unity, we have  X   X  V and so The second term, coming from the small kernel eigenvalues, is the more slowly decaying because it corresponds to fine detail of the target function that needs many training examples to learn accu-rately. It will therefore dominate the asymptotic behaviour of the learning curve: = g ( n/ X  2 )  X  Fig. 3(right) is consistent with this form.
