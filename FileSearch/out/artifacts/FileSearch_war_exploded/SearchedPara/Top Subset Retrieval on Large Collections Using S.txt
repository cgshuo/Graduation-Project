 In this poster we describe alternative inverted index struc-tures that reduce the time required to process queries, pro-duce a higher query throughput and still return high qual-ity results to the end user. We give results based upon the TREC Terabyte dataset showing improvements that these indices give in terms of effectiveness and efficiency. Categories and Subject Descriptors: H.3.3 Informa-tion Storage, Information Search and Retrieval General Terms: Performance, Experimentation Keywords: Sorted Inverted Index, Large-Scale Retrieval
The task of developing a fast and effective search engine to deal with many billions of web pages is very difficult. While indexing billions of web pages is beyond the needs of most organisations, indexing large test collections (such as the TREC GOV2 collection of over 25,000,000 pages) is often necessary. In this poster we examine alternative indexing techniques to make this possible with even very limited resources.

In our previous work [2], we presented a search engine architecture for an efficient Terabyte search engine. We dis-tributed GOV2 across four leaf search engines and used an aggregate engine to combine search results. In an effort to produce an even more cost-effective means to search the col-lection we indexed the entire collection on a single Pentium 4, 2.6GHz machine, with 1.5GB of RAM. However when reducing the cost of indexing a collection by reducing the number of machines used, problems with index structures become more acute. It was with this in mind that we exam-ined alternative index structures.
The main challenge as we see it, is not in the indexing of the collection on a single machine but in performing re-trieval. This can be seen when using the TREC Terabyte topics titles from 2004 as queries, after removing stopwords there remains an average of over a million documents asso-ciated with each term, and with an average of 3.1 terms per topic that amounts to over 3 million similarity calculations per query. In queries with more popular terms there are over 12 million similarity calculations. Working on a stan-dard desktop machine, this is difficult to achieve efficiently.
A possible solution to this is to choose to process only a certain number of documents associated with each term. As described in [2] one of the index structures supported by our search engine is similar to a conventional inverted index. Fundamentally, for each term in a collection-wide lexicon, there is an object that contains the list of docu-ments where that term occurs, and it X  X  corresponding term frequency (TF). This structure allows easy sorting of docu-ments associated with each term, which provides a mecha-nism to allow the retrieval of only a top subset of documents associated with each term and still attain relatively high ac-curacy, with only a fraction of overall memory requirements.
If the documents are sorted naively we cannot attempt to take only a top subset of documents for each term and hope to find a large portion of relevant documents. By re-ranking documents in the postings lists in descending order based on their term frequency, similar to Persin et al in [3] and first introduced in [4], the most influential documents for each term are processed first and so processing only a limited number of documents for each term can produce high qual-ity results with only a fraction of the computational costs. Persin also describes methods to compress this type of in-verted index and presents means by which their frequency sorted indices can be stored in less space than the conven-tional document-order inverted index.

In addition to this term frequency sorted index we also created an alternative index structure: we sorted the index by the normalised TF (NTF = TF/document length( dl t )) to rank based on the term X  X  overall influence on that doc-ument rather than purely on the number of occurrences of the term. It would seem intuitive to think that this in-dex, sorted based on the NTF would perform better than one sorted based on TFs. However we found the opposite to be the case, as was the case for Anh et al in [1]. This poor performance in comparison with TF sorting may be explained by the high ranking of small documents in the inverted index which would not be normalised greatly due to their small length. In order to promote important doc-uments with not just overly long TFs (as in the case of TF sorting) and not overly short documents (as can be the case in NTF sorting), we devised the following alternative method of sorting, (calculated based on divergence from the average document length) which would penalise both long and short documents, and then combine this with their TF to give an overall ranking as to the importance for each doc-ument associated with each term.
 where e is the base of natural logarithms, avg dl is the average and max dl is the maximum document length.

This sorting scheme works as follows: BiDist avg is a mea-sure of the distance of the document length from the aver-age document length, and becomes smaller the further the document length deviates from the average. When this is combined with the TF to calculate W eight tf , it gives a good measure of the term X  X  overall influence on the document. A variant on this is to combine the NTF: with the BiDis avg measure to produce a new weighting ( W eight ntf ): An alternative method of sorting is to combine the NTF with a measure ( Dist avg ), which only penalises short documents, as the longer documents would have been penalised enough already by being divided by their large dl t :
We conducted our experiments using the TREC Terabyte 2004 data, and running automatic queries from the title field of the topics. We present performance details using different forms of sorted indices and show how utilising only the top subset of each of these indices can effect system performance as measured in terms of MAP and Precision at 10. Overall the WeightTFSorted index performed best in terms of Preci-sion at 10(P@10) and Mean Average Precision(MAP). The WeightNTFSorted and SWeightNTFSorted indices achieved a clear and consistent improvement over all other index for-mats in terms of P@10, however with WeightNTFSorted this gain is at the expense of MAP in which it performs poorly. It is interesting to note that for TFSorted, WeightTF-Sorted and SWeightNTFSorted there is a degradation in P@10 the more documents that are evaluated. While Weight-edNTF managed to maintain a regular level of consistency.
Perhaps the best improvement to be seen from these new indices is clear increase in precision at 10 (P@10) that the Figure 2: Mean Average Precision Comparisons.
 WeightedNTF and SWeightNTF indices gives. At only 100,000 documents processed they give P@10 values of 0.4673 and 0.4612 respectively, which none of the other indices can achieve, even after processing as much as 700,000 documents for each term. These type of indices could clearly be useful in a web search engine(or similar) where the typical user wants a fast response time with high precision results at the top of the ranked list and generally will not browse past the top 10-20 results.
We have presented additional techniques for sorting in-verted indices which give substantial improvements over con-ventional document sorted indices, and moderate improve-ments over TF sorted indices. These experiments have been carried out on a single machine, but we have achieved sim-ilar improvements using a distributed search engine archi-tecture, allowing these approaches to scale up to work with larger collections.
 Acknowledgement: This work was supported by Science Foundation Ireland, under grant number 03/IN.3/I361. [1] V. N. Anh, O. de Kretser, and A. Moffat. Vector-space [2] P. Ferguson, C. Gurrin, P. Wilkins, and A. F. Smeaton. [3] M. Persin, J. Zobel, and Ron-Sacks-Davis. Filtered [4] A. F. Smeaton and C. J. van Rijsbergen. The nearest
