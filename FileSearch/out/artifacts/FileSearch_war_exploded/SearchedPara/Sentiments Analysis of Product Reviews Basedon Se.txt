 Since first Internet transaction completed in 1998, electricity business in China has been booming, and its huge consumption figures are not only the evolution of Chinese market, but also the changes in the way of life in China. In this era, more and more people are willing to shop online and publish their own com-ments. Therefore, the amount of feelings for people to goods and the process of shop-ping spread not just by word of mouth but also by the online reviews. Ex-amples include the goods reviews and the hotel comments, greatly affect peoples consumption decisions.
 ing filed. There are a number of useful practices in sentiment classification, such as the analysis of consumer feedback on products. Most of previous studies focus on machine learning based methods [1, 2] and lexicon based method. The perfor-mance of machine learning methods such as Support Vector Machine [2], Na  X  X ve Bayes, Maximum Entropy model, Random Walk model strongly depends on the quality of the extracted features, and the performance of lexicon based method relied heavily on the quality of emotion lexicon [3]. The calculation of these methods is simple and easy to implement, but they are limited to the expression ability of classification problems and limited by the generalization ability in some ways. Convolutional neural network belongs to the deep neural net-work, and it trains a deep nonlinear network to compensate the restriction [4 X 7]. Whats more, it can focus on essential characteristics and capture dates from a small amount of samples, and the feature extraction and pattern classification could be done at the same time. So, some researchers use it to do classification of the text and image. Fig.1 shows basic process of our sentiment classification. cess is to do segment of the comments, followed by matching the emotional dictionary and model the text. And the main model to model the text is: the bow model, sequence model and structure model. Taking into account the in-fluence of the order of words and the semantic information between words on sentiment analysis, we proposed a model to model the sentence inspired by the PV-DM.
 hensiveness of text extraction, we use a mixed vector, Sentence Vector which combines the sentence vector on the basis of the word embedding model [21]. Such a representation can project rich context information into the vector s-pace, and subsequently be used to infer similarity measures among sentences, and documents. Then, the vector of a single text is used as the input of CNN in the form of a square, so that the local and global features could be excavated at the same time. A multi-layer convolutional neural network is trained to dig the connection of semantic links. For the reviews, the emotional orientation of the text is not necessarily manifested by the positive or negative vocabulary, which requires the mining of the semantic relations of the text. We have evaluated our proposed method with benchmark experiments. Results reveal in Sec.4 and show that it is effective to use the convolution neural network to analyze the sentiment tendency for the Chinese corpus.
 re-views the related work. Next, the method for sentiment analysis of reviews is described in Section 3. And Section 4 reports the results of our experiments. Finally, Section 5 draws a conclusion. This section gives an overview of the existing research on statistical machine learning and deep neural networks for sentiment classification briefly. 2.1 Sentiment analysis with Statistical Machine Learning Sentiment analysis of word level has been a hot research direction in the field of affective computing. Most of the previous works were used lexicons based method or machine learning based methods, utilizing the data to train model and using model to analyze the sentiment of the text.
 HowNet, Zhu et al. [11] calculated the similarity between words and tenden-tiousness of words according to certain rules, so as to determine the positive or negative tendency. Determining the emotional tendency towards the target word by words and benchmark words does not consider the relationship between the target word and synonym, Wang et al. [12] presented a method for word sentiment orientation based on synonyms, not only consider the correlation be-tween target words and benchmark words, but also consider the correlation of synonyms and the target word of the reference words. Pang et al. [2] utilized SVM, Naive Bayesian, Maximum entropy and other machine learning methods to classify the sentiment polarities of movie reviews. They found that standard machine learning methods are better than basic method, but slightly worse than topic-based approach. Mullen et al. [13] proposed a SVM approach that com-bines the unigram and grammatical features to construct a classification model. Read J et al. [14] utilized emoticons to train semantic libraries. But the train cost is huge and has poor generalization ability. Wen et al. [15] classified its emotion to six classes and use class sequential rules to classify the microblog text. They firstly extract emotional tags of each sentence by using an emotion lexicon and a ma-chine learning approach respectively. Then, they mined class sequential rules as new features for emotion classification of microblog texts. Maria et al. [16] leveraged lexicon-based feature and word embedding-based feature to build hybrid vectors to classify the polarity of document. Most of the above researches use external semantic resources to obtain the semantic relations between words. However, for the discovery of new emotional words, it is difficult to obtain the semantic information of the new words if there is no corresponding update and expansion of the external semantic resources. to sentiment analysis. 2.2 Sentiment Classi cation with Deep Neural Networks The neural network is introduced to sentiment analysis and the adaptive neural network model for depth of short text vector learning can dig out the hidden features of short text vector deeper, and get better classification results. ments in many aspects, such a parsing, search word retrieval, sentence modeling and other traditional natural language work. CNN uses the local spatial corre-lation between layer and layer, and the neurons of each adjacent layer is only connected with the closer previous neuron node, which greatly reduces the ar-chitecture parameters size of the Neural Network [21].
 as the input of model. Liang et al. [17] extended LSTM to recursive neural network to capture the deeper level of semantic syntax information. Sun et al. [18] pro-posed a DNM model, which is composed by several layers of RBM stacked together is implemented to extract some higher level feature for short text. Yung et al. [19] proposed a novel approach, named DKV, for recognizing polarity and direction of sentiment using distributed real-valued vector representation of keywords learned from neural networks. Du et al. [20] used the Convolution Neural Network model of deep learning to analyze the emotional tendency of text and used the method of segmented pool to consider the sentence structure, while dropout algorithm was introduced to avoid the over fitting model and enhance the generalization ability. We describe the proposed method, named SeCNN, for sentiment analysis in this section. We first give an overview of the approach, followed by the method-s described for parts of the model. Afterwards, the model training process is described. 3.1 An Overview of the Approach Fig.2 describes the architecture of the SeCNN that we use for sentiment analysis. As is shown, the model takes an input sentence and discovers multiple levels of feature extraction, where higher levels represent more abstract aspects of the inputs. It mainly consists of three parts: text semantic expression, CNN learn-ing and classified output. The system does not need any complicated syntactic or semantic preprocessing, and the input of the system is a sentence with the segmentation and removal of the stop word. Then the word and sentence are transformed into vectors and next is fed into CNN model to train the deep fea-tures. Finally, to compute the confidence of each category, the feature vector is fed into a softmax classifier.
 3.2 Deep semantic expression with Word Embedding To solve the sentiment analysis problem, an important step is to convert the text and feature into a digital sequence. There are two ways to express the text, one-hot representation [8] and distributed representation [9, 10]. The biggest problem of one-hot representation is that it cant analyze the semantic relation between words, in addition, its dimension is particularly large. And the distributed rep-resentation method is an appropriate way to overcome above shortcomings, of which Word2Vec [24] is a typical representative of distributed representation. Although Word2Vec is a good analysis of the semantic relations between words and solves the problem of dimension disaster, but it doesnt take into account the order between words, so different sentences may have the same representation, which leads to high error rate.
 model [22] is proposed to express the text. It firstly remove the stop words and map the other words in the sentence into word vectors, and use the only id of them to express the only sentence. And then put sentence vector into the matrix D by column, and the whole word vector in matrix W by column. The sentence vector and the word vector are accumulated or connected as input of softmax layer. The output layer softmax built Huffman tree by taking the entries in the sentence as the leaf nodes and the number of entries in the text as weights. Actually, the model is to maximize the average log probability: U and b are parameters, h is connected by the word vector extracted from the D and W matrix.
 sentence vector of global feature, and then combine the word vector and the sentence vector to construct the mix vector Sentence Vector to represent the text. 3.3 CNN and Classi cation In the prediction stage, the parameters of the word vector and the output layer softmax are kept unchanged in the training, and the text is trained by using the Stochastic Gradient Ascent method. And then manage Sentence Vector into the data type which applied to the CNN model.
 mapped to the text vector layer as an image, U 2 R n  X  n . Then the U is fed into the convolutional layer to extract deeper level features. The convolutional layer has multiple convolution filter and each filter shares the same parameters, including the same weight matrix and bias, and different filter generate different local features (feature map). The advantage of share weight is that it does not take into account the location of the local features in the feature extraction, and greatly reduces the number of free parameters to learn. Given a window size w , the bias is seen as b . A feature map can be obtained by an activation function f . Each output map could be a value combining by multiple filters, and it can be calculate as follow: where z i and u i is an element in the output and input image, respectively. Because there are m filters, so the formula can also be written as follow,where M j the represents a collection of selected input maps: layer is to further abstract the features generated from the convolutional layer. In this paper, we use the max as the aggregation function. For each filter, its feature map Z is passed through the max function to produce a new map. After the pooling operation,we repeat the convolutional layer and pooling layer again. And the last layer is a full-connection softmax layer which gets the output of each category. We use the p ( i j x,  X  ) to denote the probability of the review being class i , where  X  is the parameters of model: 3.4 Training model The essence of the sentiment analysis about reviews is a classification problem, dividing the text reviews into two categories and ultimately summed up as the positive and negative emotions.
 and backward propagation. In the forward stage, given a sample ( x, y ) (where x represents the sentence, and y represents the real category of sentiment), and calculate the output y X  after putting it into the network. The information is transferred from the input layer to the output layer. In this process, the actual implementation of the network is that the input multiply the weight matrix of each layer to get the final output: the error between the y and y . Secondly, propagate the error to the previous level and adjust the weight parameters based on mining error: where W l is the weight parameters of layer L, and  X  is the learning rate. After the two stages of training, a classified model is training completed. The structure of the model is showed as Fig.2. Overall, the parameters for the model are: the word embedding matrix, the weight and bias of filters and the weight matrix for full-connection softmax layer. 4.1 Dataset and preprocessing In this section, experiments are conducted to evaluate the performance of the proposed algorithm. We crawl data from Chinese largest online shopping plat-form Taobao(http://www.taobao.com) and select 4000 reviews with marking the data into two categories. For example, the positive emotions of the text label is 1, the negative emotions of the text label is 2. Data sample showed as Table 1. Emotional dictionary is also important, it is the basic resource of the text sentiment analysis and usually be divided into two parts: the positive and negative. In a general sense, emotional dictionary refers to the phrase or sentence that contains the tendency of emotion. In a narrow sense, it refers to the collection of words containing the tendency of emotion. The emotion dictionary is constructed by two parts: the core dictionary and the temporary dictionary. The authoritative entry corpus are calculated, and two level hash structure is used to store the core dictionary. The affective lexicon is chosen as the corpus of the temporary dictionary.
 the noise and make the features in the text easier to extract. In this paper, the first step is to split the text of content description. The word in the updated core dictionary is used as the basis of the word segmentation, and the Reverse Maximum Matching algorithm is used to segment the emotional text.
 separated by spaces. Then, we collect the stop words List, and delete the useful words in the list, and then remove the stop words in the corpus according to the list. Remove stop words is to save storage space and improve efficiency of the system.After the word segmentation and removing stop words, each word is used as the basic unit of the sentence to train word embedding. In the process of word embedding, the word vector of the words which do not appear in the emotional dictionary will be initialized randomly. And we set the window size 5 and filter words with frequency less than 5. 4.2 Results and evaluation In this paper, we compare our method with baseline models: 1. Word Vector: use the Word Vector and classification with SVM 2. Sentence Vector: use the features combined of Word Vector and sentence 3. WordCNN: use the Word Vector and classification with CNN.
 4. SeCNN: classification using the features combined of Sentence Vector with commodity reviews, we use the following indicators as evaluate metric: { Accuracy: The accuracy reflects the ability of the classifier system to judge { Precision: Precision reflects the proportion of the true positive samples in { Recall: Recall also known as True Positive Rate, it reflects the proportion of { F-measure: F-measure is comprehensive index which can be calculate as where the T P is the number of reviews that are correctly divided into positive review, i.e. the number of the reviews that are classified as positive and its real category is positive too. The T N is the number of reviews that belong to negative category and assigned with negative.
 racy of Word2vec(83.5%) with Word2vec+Sent(90.2%), it is found that using the features combined of word vector and sentence vector as input leading to higher accuracy than only use the features of word vector. The reason is that the combination vector contains the context semantic information, so it can get better results in the sentence sentiment analysis. By comparing the accuracy of Word2vec+Sent(90.2%) with SeCNN(91.5%), it is found that adding CNN for feature extraction and classification from the combination of word vector and sentence vector can improve accuracy than only use the features of the combi-nation vector. The reason is that CNN can capture the essential features of data from samples even if it is small size. The Fig. 3. shows the results of precision, recall and F-measure. Likewise, it indicates that the SeCNN performs best, and its F-measure reaches 0.909.
 miss some local features, we select some commonly used size to train. Table 3. shows the results.
 In this paper, we propose a sentiment classification method SeCNN, which makes the combination of Sentence Vector and Convolutional Neural Network algo-rithms very effective. This method not only takes into account the semantic relations between words and global features, but also solves the curse of dimen-sionality. CNN can compensate the lack of shallow features learning methods through learning a kind of deep nonlinear the network structure, using the dis-tributed representation characterization of input data that shows a strong ability of features learning. Feature extraction and pattern classification can be done at the same time. Sparse connection and weight sharing in CNN model can reduce the training parameters in the network, which make the neural network structure more simple and adaptable. Unlike word2vec and bag of words to express the word, our method considers the global semantic feature. And using our method to deal with emotion classification can improve the accuracy of sentiment clas-sification.
 This work is supported by the cyberspace security Major Program in National Key Research and Development Plann can significangrant no. 2016YFB0800201, the Natural Science Foundation of China under grant no.61070212 and 61572165, the State Key Program of Zhejiang Province Natural Science Foundation of Chi-na under grant no. LZ15F020003, the Key Lab of Information Network Security, Ministry of Public Security under grant no. C16603.

