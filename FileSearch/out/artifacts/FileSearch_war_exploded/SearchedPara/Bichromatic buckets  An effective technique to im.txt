 1. Introduction
In databases, estimating the selectivity of queries is an essential part of query optimization [1 problem of selectivity estimation has been intensively investigated. Several selectivity estimation approaches have been most popular and effective ways to obtain accurate estimates of selectivity [22,11,13] .
Let D be a data set of our interest and S be the data space of D . A histogram H for D consists of a set of m buckets B where m is usually a system parameter. Each bucket B i has a data space S that a query Q on D is given by the user to retrieve data objects within a range S number of data objects in S Q ) by using the histogram H , denoted by F under the intra -bucket uniform distribution assumption . Here, || denotes the size of data space and ( S intersecting area of S i and S Q . Note, however, that the details of F organization of the buckets is computationally intractable [23].
 correct number 1.

However, since a much higher amount of memory must be used to describe the polygons than the rectangles in general, much partitioning the data space and deciding the specific shapes of the polygons.
 stand-alone histogram construction method. Instead, it serves as a useful component to be added into existing histogram simplicity of the originally rectangular ones.
 difference of density levels among the sub-regions of the bucket.
 As we mentioned above, the proposed technique takes the role of an additional component into existing histogram methods.
Section 9 . Finally, we conclude the paper in Section 10 . 2. Sketch of the proposed technique this line the separating line of B and we call these two parts P of these objects that lie inside one of the two parts. For simplicity, let P parameter, and x , y are the variables. Thus, without loss of generality, we define P coordinates ( x i , y i )(s) where y i  X  ax i + b ,while P estimate for the overlapping region between Q and P  X  and the estimate for the overlapping region between Q and P such as rivers, mountains, parks, and boundaries between geographic regions. 3. Bichromatic bucket construction based on potential skewness gain index points and a potential separating line, created by connecting an index point to another.
Specifically, Algorithm BBC -PotentialSkewnessGain proceeds as follows.  X 
We first compute the positions of n index points and store these points in a list P =  X 
For each point p i where i is from 0 to ( n -2 ), we examine all points p of i and j , the combination of two points p i and p j defines a potential separating line. Let L 5). Fig. 4 (b) shows all potential separating lines defined by p
In general, we will consider all these separating lines, starting from the ones defined by p defined by p 1 and other index points, and so on. Among the potential lines which started from a point p in Fig. 4 (c)), we illustrate the order of examination by the lines' increasing color intensity.  X 
Consider a specific line L ij . This line divides the data space of B into two disjoint regions, which we called P follows. Note that, this definition is the same as the one in [14].

De fi nition 1 (Skewness of a data region). Consider a data region D . Skew ( D ), which denotes the skewness of the data distribution in D , is computed as
For a bucket B , we will use Skew ( B ) to denote  X  skewness of the data region of bucket B and Skew ij ( P  X  ) denote the skewness of the two regions P
Skew ij ( P  X  ) and Skew ij ( P  X  ). Then, we compute the potential skewness gain of using L  X 
While we examine potential separating lines L ij ( i = 0.. ( n
L of the part P  X  and the object frequency of B , and store this ratio in B (Line 13). 3.1. Running time complexity
Let N be the total number of objects in a bucket B and n be the number of index points. Note that, n is a user-specified time complexity of Algorithm BBC -PotentialSkewnessGain is O( n + N ( n 4. Bichromatic bucket construction based on difference of density levels
In Algorithm BBC -PotentialSkewnessGain presented in the previous section, we examine several combinations of the index main steps of Algorithm BBC -DifferenceOfDensityLevels . In detail, these main steps proceed as follows.  X 
First, the data objects in B are quantized by a uniform grid G (Line 1). Each grid cell has a size grid cells and store these frequencies in a set CellFreqs . Let Freq ( g the median value, denoted by F med , of all frequency values in CellFreqs .  X 
For each grid cell g i in G (1  X  i  X  k ), let C i be the center (i.e., center point) of the data space of g denote the set of all centers of all cells in G . For each center C (Lines 2 to 4). Let Class ( C i ) be the class that C i belongs to. If the object density of g (i.e., Freq ( g i )  X  F med ), C i is assigned into class H (i.e., Class ( C (i.e., Class ( C i )  X  L ). Note that, to remove noise, if a center C represented by white points in the figure.  X 
Our aim now is to find a line L  X  that can separate all the grid cell centers C into the same group. To have the line L  X  , we use a linear regression model as follows.  X 
For each grid cell center C i of class H , we check whether g centers. That is, whether C i has 2 to 5 neighboring cell centers belonging to the opposite class (i.e., class L ). If g cell, let L ( i , j ) denote the line segment that connects the center C neighboring grid cell of g i and C j belongs to the opposite class of C grid cells g i and g j . Let m denote the total number of bridges we found.  X 
For each bridge L ( i , j ), we compute its center point from the coordinates of C center points of these bridges are represented by squares (for clarity).  X 
We now apply a linear regression model to these bridge centers to obtain the line L model for a set of data points {( x 1 , y 1 ), ... ,( x m where the observed value of the dependent variable y i is composed of a linear function error term i is commonly assumed to follow a Gaussian distribution N (0, the values y i (s) (1  X  i  X  m ) are assumed to be observations from the random variables where N denotes a Gaussian distribution.  X 
Using least squares fitting techniques, we can obtain the estimates of the parameters respectively, as follows.
Here, ^  X  0 and ^  X  1 represent the separating line y  X  ^  X 
For the linear regression model to be robust to outliers, after obtaining the estimates each data points in the set BridgeCenters as which are the differences between the observed values of the dependent variable y we compute the standardized residuals as where ^  X   X  construct the bichromatic version of the input bucket B .  X 
Now, we approximate the fitted regression line y  X  ^  X  1 x  X  line can be reconstructed easily if the index values i  X  and j frequency of the part P  X  and the object frequency of B , and store this ratio in B (Line 15). 4.1. Running time complexity time complexity of Algorithm BBC -DifferenceOfDensityLevels is O( N + n + k ). 5. Adaptive bichromatic bucket construction the center points of the bridges (stored in the set BridgeCenters ={( x we compute the sample correlation coefficient , denoted by coordinates, respectively, of these centers. The sample correlation coefficient
The absolute value of  X  , denoted by |  X  |, ranges from 0 to 1. When | to approximate the centers of the bridges. In contrast, when | on potential skewness gain examination is used. Conversely, when | using the linear regression model as in Algorithm BBC -DifferenceOfDensityLevels . will use this comprehensively adaptive algorithm in our experiments to construct bichromatic buckets for the histograms. 6. Estimating the selectivity using a bichromatic bucket difference is in the specific way we use a bichromatic bucket to estimate the selectivity of Q . bucket, Est ( Q , B ) would be computed as where S Q , S B , and F B are the data space of Q , the data space of B , and the object frequency of B , respectively.
However, B is a bichromatic bucket with two disjoint parts P
More specifically, let F  X  and F  X  denote the object frequencies of P and P  X  , respectively. Since the object frequency of B and the ratio of objects that lie in P
We can also compute S  X  and S  X  from the position of B and the values of the index points p Then, the selectivity of Q is where and 7. Integration into existing histogram methods and show how the proposed technique can be integrated to improve these methods further. 7.1. The MinSkew method done without any additional cost of loading the data objects into the memory.
 (e.g., the RkHist method [13]). 7.2. The STHist method improved root bucket and converts these hotspots into child buckets. For each of these child buckets, we apply Algorithm
During this process, no additional loading of the data objects is required. 8. Performance evaluation 8.1. Experimental setup bichromatic buckets based versions, called Bi-MinSkew, Bi-RkHist, and Bi-STHist, respectively. [26] that contains 4077 populated places in Netherlands.

Q is
For a set of K queries { Q 1 , ... , Q K }, the average relative error E where i rel is the relative error of query Q i .
 entire data region. Recall that each bucket will be quantized by a uniform grid G and each grid cell has a size to 0.2 % the perimeter of the input data set. 8.1.1. Bucket quota adjustment
Let m be the number of buckets that can be constructed for a histogram of normal buckets. Let m clear that a bichromatic bucket uses more memory than a normal bucket. Thus, m for a normal rectangular bucket B , we need to store the coordinates ( x to store both i and j . For the object ratio, 2 bytes is enough for an approximation of high precision. Consequently, B 6  X  4 bytes while B needs 5  X  4 bytes for storage. Therefore, given a specific value for m , we will set m 6)  X  m ) for a fair comparison. 8.2. Result analysis
Fig(s). 10 and 11 show the performance of the histogram methods. regions with more uniform data distribution in each region.
 1000 words. That is, the number of buckets is 200 for each normal histogram and 166 for each histogram with bichromatic to be useful in various real-life applications.

In Table 1, we show the average amounts of time used to construct a bichromatic bucket by the proposed algorithm. We observe that the bichromatic bucket construction time gradually decreases when the storage space (also, the number of buckets) increases. This is because the average size of the data space of a bucket becomes smaller when there are more bucket range from less than a second to more than ten seconds depending on the specific data set and the number of low. In this work, since our target is geographic data points where updates do not frequently occur, the histogram construction is not a frequent operation. Thus, these amounts of time for constructing bichromatic buckets may be considered negligible.
 separating line directly based on the difference of density levels when the absolute value of the sample correlation a number of index points, then choose the best line based on potential skewness gains. Here, instead of using 0.5 as the selection threshold for switching between the two strategies, we can use any other threshold between 0 and 1 because the value of |  X  |rangesfrom0to1.Let  X  denote this selection threshold. By default, we set both strategies. In practice, varying the value of  X  can give a trade-off between accuracy and speed of the proposed algorithm.

Fig. 14 (a) and (b) respectively demonstrates the effectiveness and efficiency of the Bi-STHist method when we vary the histograms for selectivity estimation and the efficiency is measured as the amounts of time to construct completely the histograms with bichromatic buckets. The storage space for each histogram was set at 1000 words. Note that, when the strategy based on difference of density levels is used. In contrast, when gains is used. We can see from the results in both Fig. 14 (a) and (b) that, as the value of should decrease (or increase) the value of  X  . Note that, even when is still considerably higher than that of the histograms constructed by STHist. 9. Related work
Histograms have a rather long research history. A relatively full record of their history can be found in [22].Inthe following, we review some important milestones and concentrate on the studies that are related to ours. Histograms were each bucket is equivalent. In the EquiHeight histogram, buckets are constructed to represent equivalent frequency summations. The EquiHeight method was shown to provide significantly higher accuracy in selectivity estimation than the
EquiWidth method. Nevertheless, the performance of these methods when generalized for higher dimensional data, such as geographic data, is not satisfactory [7].

For multi-dimensional data, beside the MinSkew [7] and STHist [14] methods which have been reviewed in Section 7 , there are several other methods. EquiDepth [4] is the first histogram construction method for data having multiple constructed histogram, each bucket contains the same number of data objects. Then, in [5],amethodnamedMHIST-2was area) between adjacent values and places a bucket boundary between those values. Consequently, when frequency is used as a source parameter, the resulting MaxDiff histogram appro ximately minimizes the variance of value frequencies within each bucket. In [11], the authors proposed a histogram method named GenHist. The main difference between GenHist and the previously proposed methods is that GenHist allows buckets to overlap. This new method exploits the fact that, with the same number of bucket quota, the overlapping between buckets permits the data space to be partitioned into a higher are converted into buckets. More recently, a method named RkHist was introduced in [13]. RkHist builds histograms based a sliding window method, coupled with a new uniformity mea sure, to further improve the quality of the selectivity adopted lossless compression techniques to represent the histograms. Compression was obtained by exploiting the hierarchical partition scheme underlying the histograms. Several heuristics guiding the histogram construction were proposed and the best accuracy turned out from combining a g rid-constrained partitioning scheme with one of the new heuristics.

In addition to statically computed histograms, such as those created by MinSkew [7] ,GenHist[11],andRkHist[13] ,there are dynamically generated histograms. For example, the Self-Tuning histogram [6] incrementally maintains buckets in response to feedback from the query execution engine about the actual selectivity of range selection operators. This are not very good when the data skewness is high [6]. The StHoles method [8] also dynamically analyzes query results, indicated by the query workload. The ISOMER algorithm [12] extends the basic StHoles model by using the information-theoretic principle of maximum entropy to refine the histogram based on query feedback. It automatically detects and eliminates inconsistent feedback information in an efficient manner and uses only the consistent ones. related to those queries can be updated. Thus, updates in the other regions may not be reflected. Note that, we focus on statically computed histograms in this paper.
 data which is recently becoming more popular [34,35] .
 robustness across a wide variety of application domains [22,11,13] . 10. Conclusion
Histograms have been widely used for selectivity estimation and constructing highly accurate histograms is an important rectangular buckets are preserved while additionally new advantages of polygonal buckets are incorporated. 2 times on average and more than 4 times in several cases.
 higher dimensional data.
 Acknowledgments
This work was supported by the National Research Foundation of Korea (NRF) grant funded by the Korean government (MEST) (No. 2012R1A2A2A01046694) and the Brain Korea 21 Project, the Department of Computer Science, KAIST in 2012.
References
