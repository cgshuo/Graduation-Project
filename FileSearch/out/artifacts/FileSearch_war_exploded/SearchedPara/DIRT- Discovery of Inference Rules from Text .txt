 inference rule because we also want to include relationships that are not exactly paraphrases, but are nonetheless related and are potentially useful to information retrieval systems. For example, the two phrases do not mean exactly the same thing. Traditionally, knowledge bases containing such inference rules are created manually. This knowledge engineering task is extremely laborious. More importantly, building such a knowledge base is inherently difficult since humans are not good at generating a complete list of rules. For example, while it is quite trivial to come up with the rule "'Xwrote YzXis the author of Y", it seems hard to dream up a rule like "X manufactures Y X's Y factory", which can be used to infer that "Chr~tien visited Peugot's newly renovated car factory in the afternoon" contains an answer to the query "What does Peugot manufacture?" Most previous efforts on knowledge engineering have focused on creating tools for helping knowledge engineers transfer their knowledge to machines [6]. Our goal is to automatically discover such rules. In this paper, we present an unsupervised algorithm, DIRT, for Discovery of Inference Rules from Text. Our algorithm is a generalization of previous algorithms for finding similar words [10][15][19]. Algorithms for finding similar words assume the Distributional Hypothesis, which states that words that occurred in the same contexts tend to have similar meanings [7]. Instead of applying the Distributional Hypothesis to words, we apply it to paths in dependency trees. Essentially, if two paths tend to link the same sets of words, we hypothesize that their meanings are similar. Since a path represents a binary relationship, we generate an inference rule for each pair of similar paths. The remainder of this paper is organized as follows. In the next section, we review previous work. In Section 3, we define paths in dependency trees and describe their extraction from a parsed corpus. Section 4 presents the DIRT system and a comparison of our system's output with manually generated paraphrase expressions is shown in Section 5. Finally, we conclude with a discussion of future work. Most previous work on variant recognition and paraphrase has been done in the fields of natural language generation, text summarization, and information retrieval. The generation community has focused mainly on rule-based text transformations in order to meet external constraints such as length and readability [11][18][22]. Dras [4] described syntactic Table 1. A subset of dependency relations in Minipar outputs. 
RELATION DESCRIPTION EXAMPLE appo appositive of a noun the CEO, John det determiner of a noun the dog gen genitive modifier of a noun John's dog mod adjunct modifier of any head tiny hole nn prenominal modifier of a noun station manager subj subject of a verb John loves Mary. Minipar parses newspaper text at about 500 words per second on a Pentium-III 700Mhz with 500MB memory. Evaluation with the manually parsed SUSANNE corpus [23] shows that about 89% of the dependency relationships in Minipar outputs are correct. In the dependency trees generated by Minipar, each link between two words in a dependency tree represents a direct semantic relationship. A path allows us to represent indirect semantic relationships between two content words. We name a path by concatenating dependency relationships and words along the path, the path between John and problem is named: N:subj:V&lt;-find--)V:obj:N--)solution--&gt;N:to:N (meaning "Xfinds solution to Y"). The reverse path can be written as: N:to:N(-solution(--N:obj:V(--find--&gt; V:subj:N. The root of both paths is find. A path begins and ends with two dependency hand side and SlotY on the right-hand side. The words connected SlotX of N:subj:V&lt;--find-&gt;V:obj:N-&gt;solution-&gt;N:to:N and problem fills the SlotY. The reverse is true for N:to:N('-solution &lt;-N:obj :V (-find --) V:subj :N. In a path, dependency relations that are not slots are called internal relations. For example, find--)V:obj:N--)solution is an internal relation in the previous path. We impose a set of constraints on the paths to be extracted from text for the following reasons:  X  most meaningful inference rules involve only paths that  X  the constraints significantly reduce the number of distinct  X  the constraints alleviate the sparse data problem because The constraints we impose are: 
Table 2. Sample slot fillers for two paths extracted from a newspaper corpus. commission strike committee problem committee civil war clout crisis committee crisis government problem government crisis he mystery government problem she problem Based on these common contexts, one can statistically determine that duty and responsibility have similar meanings. In the algorithms for finding word similarity, dependency links are treated as contexts of words. In contrast, our algorithm for as a context for the path. We make an assumption that this is an extension to the Distributional Hypothesis: For example, Table 2 lists a set of example pairs of words connected by the paths N:subj:V(--find--)V:obj:N'-)solution---) N:to:N ("X finds a solution to F9 and N:subj:V(--solve--) many overlaps between the corresponding slot fillers of the two paths. By the Extended Distributional Hypothesis, we can then claim that the two paths have similar meaning. To compute the path similarity using the Extended Distributional path p that connects two words w~ and w2, we increase the frequency counts of the two triples (p, Slot)(, wl) and (p, SlotY, w2). We call (SlotX, wl) and (SlotY, w2) features of path p. Intuitively, the more features two paths share, the more similar they are. We use a triple database (a hash table) to accumulate the frequency counts of all features of all paths extracted from a parsed corpus. An example entry in the triple database for the path 
N:subj:V(--pull--)V:obj:N--)body--)N:from:N is shown in Figure 2. The first column of numbers in Figure 2 and the second column of numbers is the mutual information Y is solved by X Xresolves Y X finds a solution to Y Xtries to solve Y Xdeals with Y Y is resolved by X X addresses Y X seeks a solution to Y X do something about Y Xsolution to Y that fill in the s slot ofpathpi. geometric average of the similarities of their SloO( and SlotY slots: where Slot~i and SlotYi are path i's SlotX and SlotY slots. The discovery of inference rules is made by finding the most large number of paths in the triple database. The database used in our experiments contains over 200,000 distinct paths. Computing the similarity between every pair of paths is obviously impractical. Given a pathp, our algorithm for finding the most similar paths of p takes three steps: (a) Retrieve all the paths that share at least one feature with p (b) For each candidate path c, count the number of features (c) Compute the similarity between p and the candidates that Table 3 lists the Top-50 most similar paths to "X solves Y" generated by DIRT. Most of the paths can be considered as paraphrases of the original expression. We performed an evaluation of our algorithm by comparing the inference rules it generates with a set of human-generated paraphrases of the first six questions in the TREC-8 Question-Q# PATHS MAN. DIRT INT. ACC. There is very little overlap between the automatically generated paths and the paraphrases, even though the percentage of correct paths in DIRT outputs can be quite high. This suggests that finding potentially useful inference rules is very difficult for humans as well as machines. Table 6 shows some of the correct paths among the Top-40 extracted by our system for two of the TREC-8 questions. Many of the variations generated by DIRT that are correct paraphrases are missing from the manually paraphrases. However, given the output of our system, humans our system would greatly ease the manual construction of inference rules for an information retrieval system. The performance of DIRT varies a great deal for different paths. Usually, the performance for paths with verb roots is much better than for paths with noun roots. A verb phrase typically has more than one modifier, whereas nouns usually take a smaller number of modifiers. When a word takes less than two modifiers, it will not be the root of any path. As a result, paths with noun roots occur less often than paths with verb roots, which explains the lower performance with respect to paths with noun roots. In Table 5, DIRT found no correct inference rules for Q2. This is due to the fact that Q2 does not have any entries in the triple database. Better tools are necessary to tap into the vast amount of textual data that is growing at an astronomical pace. Knowledge about inference relationships in natural language expressions would be is the first attempt to discover such knowledge automatically from a large corpus of text. We introduced the Extended Distributional Hypothesis, which states that paths in dependency trees have similar meanings if they tend to connect similar sets of words. Treating paths as binary relations, our algorithm is able to generate inference rules by searching for similar paths. Our experimental results show that the Extended Distributional Hypothesis can indeed be used to discover very useful inference 
