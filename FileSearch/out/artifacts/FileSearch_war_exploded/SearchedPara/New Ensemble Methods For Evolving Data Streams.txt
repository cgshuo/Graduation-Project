 Advanced analysis of data streams is quickly becoming a key area of data mining research as the number of applica-tions demanding such processing increases. Online mining when such data streams evolve over time, that is when con-cepts drift or change completely, is becoming one of the core issues. When tackling non-stationary concepts, ensembles of classifiers have several advantages over single classifier methods: they are easy to scale and parallelize, they can adapt to change quickly by pruning under-performing parts of the ensemble, and they therefore usually also generate more accurate concept descriptions. This paper proposes a new experimental data stream framework for studying con-cept drift, and two new variants of Bagging: ADWIN Bagging and Adaptive-Size Hoeffding Tree (ASHT) Bagging. Using the new experimental framework, an evaluation study on synthetic and real-world datasets comprising up to ten mil-lion examples shows that the new ensemble methods perform very well compared to several known methods.
 H.2.8 [ Database applications ]: Database Applications X  Data Mining Algorithms Data streams, ensemble methods, concept drift, decision trees
Conventional knowledge discovery tools assume that the volume of data is such that we can store all data in memory or local secondary storage, and there is no limitation on pro-cessing time. In the Data Stream model, we have space and time restrictions. Examples of data streams are sensoring video streams, network event logs, telephone call records, credit card transactional flows, etc. An important fact is that data may be evolving over time, so we need methods that adapt automatically. Under the constraints of the Data Stream model, the main properties of an ideal classification method are the following: high accuracy and fast adaption to change, low computational cost in both space and time, theoretical performance guarantees, and minimal number of parameters.

These properties may be interdependent: adjusting the time and space used by an algorithm can influence accuracy. By storing more pre-computed information, such as look up tables, an algorithm can run faster at the expense of space. An algorithm can also run faster by processing less infor-mation, either by stopping early or storing less, thus having less data to process. The more time an algorithm has, the more likely it is that accuracy can be increased.

Ensemble methods are combinations of several models whose individual predictions are combined in some manner (e.g., averaging or voting) to form a final prediction. En-semble learning classifiers often have better accuracy and they are easier to scale and parallelize than single classifier methods.

A majority of concept drift research in data streams min-ing is done using traditional data mining frameworks such as WEKA [26]. As the data stream setting has constraints that a traditional data mining environment does not, we be-lieve that a new framework is needed to help to improve the empirical evaluation of these methods.

We present in Section 2 a novel framework for evaluation of concept drift. Sections 3 and 4 present two novel ensem-ble methods for handling concept drift, and Section 5 shows a first comprehensive cross-method comparison. We present conclusions in Section 6. Source code and datasets will be made available at http://sourceforge.net/projects/moa-datastream .
A data stream environment has different requirements from the traditional setting [15]. The most significant are the following: Requirement 1 Process an example at a time, and inspect it only once (at most) Requirement 2 Use a limited amount of memory Requirement 3 Work in a limited amount of time Requirement 4 Be ready to predict at any time We have to consider these requirements in order to design a new experimental framework for data streams. Figure 1 illustrates the typical use of a data stream classification al-gorithm, and how the requirements fit in a repeating cycle: 1. The algorithm is passed the next available example 2. The algorithm processes the example, updating its data 3. The algorithm is ready to accept the next example. In traditional batch learning the problem of limited data is overcome by analyzing and averaging multiple models pro-duced with different random arrangements of training and test data. In the stream setting the problem of (effectively) unlimited data poses different challenges. One solution in-volves taking snapshots at different times during the induc-tion of a model to see how much the model improves.
The evaluation procedure of a learning algorithm deter-mines which examples are used for training the algorithm, and which are used to test the model output by the algo-rithm. The procedure used historically in batch learning has partly depended on data size. As data sizes increase, practi-cal time limitations prevent procedures that repeat training too many times. It is commonly accepted with considerably larger data sources that it is necessary to reduce the num-bers of repetitions or folds to allow experiments to complete in reasonable time. When considering what procedure to use in the data stream setting, one of the unique concerns is how to build a picture of accuracy over time. Two main approaches arise: As data stream classification is a relatively new field, such evaluation practices are not nearly as well researched and established as they are in the traditional batch setting. The majority of experimental evaluations use less than one mil-lion training examples. Some papers use more than this, up to ten million examples, and only very rarely is there any study like Domingos and Hulten [8, 14] that is in the or-der of tens of millions of examples. In the context of data streams this is disappointing, because to be truly useful at data stream classification the algorithms need to be capable of handling very large (potentially infinite) streams of ex-amples. Demonstrating systems only on small amounts of data does not build a convincing case for capacity to solve more demanding data stream applications.

A claim of this paper is that in order to adequately eval-uate data stream classification algorithms they need to be tested on large streams, in the order of tens of millions of examples where possible, and under explicit memory limits. Any less than this does not actually test algorithms in a realistically challenging setting.
We present a new experimental framework for concept drift. Our goal is to introduce artificial drift to data stream generators in a straightforward way.

The framework approach most similar to the one pre-sented in this paper is the one proposed by Narasimhamurthy et al. [18]. They proposed a general framework to generate data simulating changing environments. Their framework accommodates the STAGGER and Moving Hyperplane gen-eration strategies. They consider a set of k data sources with known distributions. As these distributions at the sources are fixed, the data distribution at time t , D ( t ) is specified through v i ( t ), where v i ( t )  X  [0 , 1] specify the extent of the influence of data source i at time t : Their framework covers gradual and abrupt changes. Our approach is more concrete, we begin by dealing with a simple scenario: a data stream and two different concepts. Later, we will consider the general case with more than one concept drift events.

Considering data streams as data generated from pure dis-tributions, we can model a concept drift event as a weighted Figure 2: A sigmoid function f ( t ) = 1 / (1 + e  X  s ( t  X  t combination of two pure distributions that characterizes the target concepts before and after the drift. In our framework, we need to define the probability that every new instance of the stream belongs to the new concept after the drift. We will use the sigmoid function, as an elegant and practical solution.

We see from Figure 2 that the sigmoid function has a derivative at the point t 0 equal to f 0 ( t 0 ) = s/ 4. The tangent of angle  X  is equal to this derivative, tan  X  = s/ 4. We observe that tan  X  = 1 /W , and as s = 4 tan  X  then s = 4 /W . So the parameter s in the sigmoid gives the length of W and the angle  X  . In this sigmoid model we only need to specify two parameters : t 0 the point of change, and W the length of change. Note that for any positive real number  X  and that f ( t 0 +  X   X  W ) and f ( t 0  X   X   X  W ) are constant values that don X  X  depend on t 0 and W : f ( t 0 + W/ 2) = 1  X  f ( t 0  X  W/ 2) = 1 / (1 + e  X  2 )  X  88 . 08% f ( t 0 + 2 W ) = 1  X  f ( t 0  X  2 W ) = 1 / (1 + e  X  8 )  X  99 . 97%
Definition 1. Given two data streams a , b , we define c = a  X  W t 0 b as the data stream built joining the two data streams a and b , where t 0 is the point of change, W is the length of change and We observe the following properties, if a 6 = b : In order to create a data stream with multiple concept changes, we can build new data streams joining different concept drifts:
Synthetic data has several advantages  X  it is easier to re-produce and there is little cost in terms of storage and trans-mission. For this paper and framework, the data generators most commonly found in the literature have been collected. SEA Concepts Generator This artificial dataset contains STAGGER Concepts Generator They were introduced Rotating Hyperplane It was used as testbed for CVFDT Random RBF Generator This generator was devised to LED Generator This data source originates from the CART Waveform Generator It shares its origins with LED, and Function Generator It was introduced by Agrawal et al. Data streams may be considered infinite sequences of ( x,y ) where x is the feature vector and y the class label. Zhang et concept drift in two types:
Note that the Random RBF Generator has RCD drift, and the rest of the dataset generators have LCD drift.
It is not easy to find large real-world datasets for public benchmarking, especially with substantial concept change. The UCI machine learning repository [3] contains some real-world benchmark data for evaluating machine learning tech-niques. We will consider three : Forest Covertype, Poker-Hand, and Electricity.
 Forest Covertype dataset It contains the forest cover type Poker-Hand dataset It consists of 1 , 000 , 000 instances and Electricity dataset Another widely used dataset is the
The size of these datasets is small, compared to tens of millions of training examples of synthetic datasets: 45 , 312 for ELEC2 dataset, 581 , 012 for CoverType, and 1 , 000 , 000 for Poker-Hand. Another important fact is that we do not know when drift occurs or if there is any drift. We may sim-ulate RCD concept drift, joining the three datasets, merging attributes, and supposing that each dataset corresponds to a different concept.

As all examples need to have the same number of at-tributes, we simple concatenate all the attributes, and we set a number of classes that is the maximum number of classes of all the datasets. In this section, we present a new method of bagging using Hoeffding Trees of different sizes.

A Hoeffding tree [8] is an incremental, anytime decision tree induction algorithm that is capable of learning from massive data streams, assuming that the distribution gener-ating examples does not change over time. Hoeffding trees exploit the fact that a small sample can often be enough to choose an optimal splitting attribute. This idea is supported mathematically by the Hoeffding bound, which quantifies the number of observations (in our case, examples) needed to estimate some statistics within a prescribed precision (in our case, the goodness of an attribute). More precisely, the Hoeffding bound states that with probability 1  X   X  , the true mean of a random variable of range R will not differ from the estimated mean after n independent observations by more Figure 3: Kappa-Error diagrams for ASHT bagging (top) and bagging (bottom) on dataset RandomRBF with drift, plotting 90 pairs of classifiers. than: A theoretically appealing feature of Hoeffding Trees not shared by other incremental decision tree learners is that it has sound guarantees of performance. Using the Hoeffding bound one can show that its output is asymptotically nearly identi-cal to that of a non-incremental learner using infinitely many examples. See [8] for details.
 In this paper, we introduce the Adaptive-Size Hoeffding Tree (ASHT). It is derived from the Hoeffding Tree algo-rithm with the following differences:
The intuition behind this method is as follows: smaller trees adapt more quickly to changes, and larger trees do better during periods with no or little change, simply be-cause they were built on more data. Trees limited to size s will be reset about twice as often as trees with a size limit of 2 s . This creates a set of different reset-speeds for an en-semble of such trees, and therefore a subset of trees that are a good approximation for the current rate of change. It is important to note that resets will happen all the time, even for stationary datasets, but this behaviour should not have a negative impact on the ensemble X  X  predictive performance.
When the tree size exceeds the maximun size value, there are two different delete options: We present a new bagging method that uses these Adaptive-Size Hoeffding Trees and that sets the size for each tree. The maximum allowed size for the n -th ASHT tree is twice the maximum allowed size for the ( n  X  1)-th tree. Moreover, each tree has a weight proportional to the inverse of the square of its error, and it monitors its error with an exponential weighted moving average (EWMA) with  X  = . 01. The size of the first tree is 2.

With this new method, we attempt to improve bagging performance by increasing tree diversity. It has been ob-served that boosting tends to produce a more diverse set of classifiers than bagging, and this has been cited as a factor in increased performance [16].

We use the Kappa statistic  X  to show how using trees of different size, we increase the diversity of the ensemble. Let X  X  consider two classifiers h a and h b , a data set containing m examples, and a contingency table where cell C ij contains the number of examples for which h a ( x ) = i and h b ( x ) = j . If h a and h b are identical on the data set, then all non-zero counts will appear along the diagonal. If h a and h b are very different, then there should be a large number of counts off the diagonal. We define We could use  X  1 as a measure of agreement, but in prob-lems where one class is much more common than others, all classifiers will agree by chance, so all pair of classifiers will obtain high values for  X  1 . To correct this, the  X  statistic is defined as follows:  X  uses  X  2 , the probability that two classifiers agree by chance, given the observed counts in the table. If two classifiers agree on every example then  X  = 1, and if their predictions coin-cide purely by chance, then  X  = 0.

We use the Kappa-Error diagram to compare the diversity of normal bagging with bagging using trees of different size. The Kappa-Error diagram is a scatterplot where each point corresponds to a pair of classifiers. The x coordinate of the pair is the  X  value for the two classifiers. The y coordinate is the average of the error rates of the two classifiers. Figure 3 shows the Kappa-Error diagram for the Random RBF dataset with drift parameter or change speed equal to 0.001.We observe that bagging classifiers are very similar to one another and that the decision tree classifiers of different size are very diferent from one another.
ADWIN [5] is a change detector and estimator that solves in a well-specified way the problem of tracking the average of a stream of bits or real-valued numbers. ADWIN keeps a variable-length window of recently seen items, with the property that the window has the maximal length statis-tically consistent with the hypothesis  X  X here has been no change in the average value inside the window X .

ADWIN is parameter-and assumption-free in the sense that it automatically detects and adapts to the current rate of change. Its only parameter is a confidence bound  X  , indicat-ing how confident we want to be in the algorithm X  X  output, inherent to all algorithms dealing with random processes.
Also important for our purposes, ADWIN does not maintain the window explicitly, but compresses it using a variant of the exponential histogram technique. This means that it keeps a window of length W using only O (log W ) memory and O (log W ) processing time per item.
 ADWIN Bagging is the online bagging method of Oza and Rusell [19] with the addition of the ADWIN algorithm as a change detector and as an estimator for the weights of the boosting method. When a change is detected, the worst classifier of the ensemble of classifiers is removed and a new classifier is added to the ensemble.
M assive O nline A nalysis (MOA) [13] is a software envi-ronment for implementing algorithms and running exper-iments for online learning from data streams. The data stream evaluation framework and all algorithms evaluated in this paper were implemented in the Java programming language extending the MOA software. MOA includes a collection of offline and online methods as well as tools for evaluation. In particular, it implements boosting, bagging, and Hoeffding Trees, all with and without Na  X   X ve Bayes clas-sifiers at the leaves.

One of the key data structures used in MOA is the de-scription of an example from a data stream. This structure borrows from WEKA, where an example is represented by an array of double precision floating point values. This pro-vides freedom to store all necessary types of value  X  numeric attribute values can be stored directly, and discrete attribute values and class labels are represented by integer index val-ues that are stored as floating point values in the array. Double precision floating point values require storage space of 64 bits, or 8 bytes. This detail can have implications for memory usage.
 We compare the following methods: Hoeffding Option Trees, bagging and boosting, and DDM. We review them and their main properties briefly.
Bagging and Boosting are two of the best known ensem-ble learning algorithms. In [19] Oza and Russell developed online versions of bagging and boosting for Data Streams. They show how the process of sampling bootstrap replicates from training data can be simulated in a data stream con-text. They observe that the probability that any individual example will be chosen for a replicate tends to a Poisson(1) distribution.

For the boosting method, Oza and Russell note that the weighting procedure of AdaBoost actually divides the to-tal example weight into two halves  X  half of the weight is assigned to the correctly classified examples, and the other half goes to the misclassified examples. They use the Pois-son distribution for deciding the random probability that an example is used for training, only this time the parameter changes according to the boosting weight of the example as it is passed through each model in sequence.

Pelossof et al. presented in [21] Online Coordinate Boost-ing, a new online boosting algorithm for adapting the weights of a boosted classifier, which yields a closer approximation to Freund and Schapire X  X  AdaBoost algorithm. The weight update procedure is derived by minimizing AdaBoost X  X  loss when viewed in an incremental form. This boosting method may be reduced to a form similar to Oza and Russell X  X  algo-rithm.

Chu and Zaniolo proposed in [7] Fast and Light Boosting for adaptive mining of data streams. It is based on a dy-namic sample-weight assignment scheme that is extended to handle concept drift via change detection. The change de-tection approach aims at significant data changes that could cause serious deterioration of the ensemble performance, and replaces the obsolete ensemble with one built from scratch.
Hoeffding Option Trees [22] are regular Hoeffding trees containing additional option nodes that allow several tests to be applied, leading to multiple Hoeffding trees as separate paths. They consist of a single structure that efficiently represents multiple trees. A particular example can travel down multiple paths of the tree, contributing, in different ways, to different options.
 An Adaptive Hoeffding Option Tree is a Hoeffding Option Tree with the following improvement: each leaf stores an estimation of the current error. It uses an EWMA estimator with  X  = . 2. The weight of each node in the voting process is proportional to the square of the inverse of the error.
The drift detection method (DDM) proposed by Gama et al. [9] controls the number of errors produced by the learn-ing model during prediction. It compares the statistics of two windows: the first one contains all the data, and the second one contains only the data from the beginning until the number of errors increases. Their method doesn X  X  store these windows in memory. It keeps only statistics and a window of recent errors.

They consider that the number of errors in a sample of examples is modeled by a binomial distribution. A signif-icant increase in the error of the algorithm, suggests that the class distribution is changing and, hence, the actual de-cision model is supposed to be inappropriate. They check for a warning level and a drift level. Beyond these levels, change of context is considered.

Baena-Garc  X  X a et al. proposed a new method EDDM [4] in order to improve DDM. It is based on the estimated dis-tribution of the distances between classification errors. The window resize procedure is governed by the same heuristics.
We use a variety of datasets for evaluation, as explained in Section 2.2. The experiments were performed on a 2.0 GHz Intel Core Duo PC machine with 2 Gigabyte main memory, running Ubuntu 8.10. The evaluation methodol-ogy used was Interleaved Test-Then-Train: every example was used for testing the model before using it to train. This interleaved test followed by train procedure was carried out on 10 million examples from the hyperplane and Random-RBF datasets, and one million examples from the LED and SEA datasets. Tables 1 and 2 reports the final accuracy, and speed of the classification models induced on synthetic data. Table 3 shows the results for real datasets: Forest CoverType, Poker Hand, Electricity and CovPokElec. Ad-ditionally, the learning curves and model growth curves for LED dataset are plotted (Figure 4). For some datasets the differences in accuracy, as seen in Tables 1, 2 and 3, are marginal.

The first, and baseline, algorithm (HT) is a single Hoeffd-ing tree, enhanced with adaptive Naive Bayes leaf predic-tions. Parameter settings are n min = 1000,  X  = 10  X  8 and  X  = 0 . 05, used in [8]. The HT DDM and HT EDDM are Hoeffding Trees with drift detection methods as explained in Section 5.3. HOT, is the Hoeffding option tree algorithm, restricted to a maximum of five option paths (HOT5) or fifty option paths (HOT50). AdaHOT is explained in Section 5.2.
Bag10 is Oza and Russell online bagging using ten classi-fiers and Bag5 only five. Bag ADWIN is the online bagging ver-sion using ADWIN explained in Section 4. We implemented the following variants of bagging with Hoeffding trees of dif-ferent size (ASHT): And finally, we tested three methods of boosting: Oza Boost-ing, Online Coordinate Boosting, and Fast and Light Boost-ing. The parameters used in the experimental evaluation were found to work well across a range of problems during the PhD of the first author.
 Bagging is clearly the best method in terms of accuracy. This superior position is, however, achieved at high cost in terms of memory and time. ADWIN Bagging and ASHT Bag-ging are the most accurate methods for most datasets, but they are slow. ADWIN Bagging is slower than ASHT Bagging and for some datasets it needs more memory. ASHT Bag-ging using weighted classifiers and replacing oversized trees with new ones seems to be the most accurate ASHT bagging method. We observe that bagging using 5 trees of different size may be sufficient, as its error is not much higher than for 10 trees, but it is nearly twice as fast. Also Hoeffding trees using drift detection methods are faster but less accurate methods.

In [22], a range of option limits were tested and averaged across all datasets without concept drift to determine the optimal number of paths. This optimal number of options was five. Dealing with concept drift, we observe that in-creasing the number of options to 50, we obtain a significant improvement in accuracy for some datasets.

A summary of the best results from the synthetic and real datasets in Tables 1-3 show that of the two novel methods presented here Bag10 ASHT W+R wins five times, Bag ADWIN 10 HT wins four times, and Bag10 HT, OzaBoost, and OC-Boost win once each. This confirms that the variants pro-posed in this paper are superior across this collection of datasets.
Our goal is to build an experimental framework for data streams similar to the WEKA framework, so that it will be easy for researchers to run experimental data stream bench-marks. New bagging methods were presented: ASHT Bag-ging using trees of different sizes, and ADWIN Bagging using a change detector to decide when to discard underperforming ensemble members. These methods compared favorably in a comprehensive cross-method comparison. Data stream eval-uation is fundamentally three-dimensional. These compar-isons, given your specific resource limitations, indicate the method of preference. For example, on the SEA Concepts and Forest Covertype datasets the best performing method across all three dimensions are arguably HT DDM and HT EDDM, as they are almost the fastest, and almost the most accurate and, by at least an order of magnitude, easily the most memory-efficient methods. On the other hand, if both runtime and memory consumption are less of a concern, then variants of bagging usually produce excellent accuracies.
Partially supported by the EU PASCAL2 Network of Ex-cellence (FP7-ICT-216886), and by projects SESAAME-BAR (TIN2008-06582-C03-01), MOISES-BAR (TIN2005-08832-C03-03). Albert Bifet is supported by a FI grant through the SGR program of Generalitat de Catalunya. [1] R. Agrawal, S. P. Ghosh, T. Imielinski, B. R. Iyer, and [2] R. Agrawal, T. Imielinski, and A. Swami. Database [3] A. Asuncion and D. Newman. UCI machine learning [4] M. Baena-Garc  X  X a, J. D. Campo- X  Avila, R. Fidalgo, [5] A. Bifet and R. Gavald`a. Learning from time-changing [6] L. Breiman et al. Classification and Regression Trees . [7] F. Chu and C. Zaniolo. Fast and light boosting for [8] P. Domingos and G. Hulten. Mining high-speed data [9] J. Gama, P. Medas, G. Castillo, and P. Rodrigues. [10] J. Gama, R. Rocha, and P. Medas. Accurate decision [11] J. Gehrke, R. Ramakrishnan, and V. Ganti.
 [12] M. Harries. Splice-2 comparative evaluation: [13] G. Holmes, R. Kirkby, and B. Pfahringer. MOA: [14] G. Hulten, L. Spencer, and P. Domingos. Mining [15] R. Kirkby. Improving Hoeffding Trees . PhD thesis, [16] D. D. Margineantu and T. G. Dietterich. Pruning [17] M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: A fast [18] A. Narasimhamurthy and L. I. Kuncheva. A [19] N. Oza and S. Russell. Online bagging and boosting. [20] N. C. Oza and S. Russell. Experimental comparisons [21] R. Pelossof, M. Jones, I. Vovsha, and C. Rudin. [22] B. Pfahringer, G. Holmes, and R. Kirkby. New options [23] J. C. Schlimmer and R. H. Granger. Incremental [24] J. C. Shafer, R. Agrawal, and M. Mehta. SPRINT: A [25] W. N. Street and Y. Kim. A streaming ensemble [26] I. H. Witten and E. Frank. Data Mining: Practical [27] P. Zhang, X. Zhu, and Y. Shi. Categorizing and
