 Department of Information Technology, Sri Krishna College of Engineering and Technology, Coimbatore, India Department of Computer Science and Engineering, Government College of Technology, Coimbatore, India 1. Introduction
Data mining, also called as Knowledge Discovery in Databases, is one of the most significant areas in the 21 st century  X  the information age [1]. It is defined as  X  X he nontrivial extraction of implicit, previously unknown, and potentially useful information from data X  [2]. It is utilized to extract struc-tured knowledge automatically from huge data sets [3]. Its applications are extensive, comprising of medicine, finance, commerce and engineering, to name a few [1]. There are two general classes of data mining namely, descriptive and predictive. The main objective of descriptive data mining is to find out patterns, e.g., product configurations formed in mass customization applications. The predictive data mining aspires at building models to find out (predict) an outcome, e.g., a stock level. Due to the un-limited width of data analysis by the data-mining algorithms, the patterns identified are generally not data mining applications has become increasingly popular [11].

In data mining, an innovative research area known as utility-based data mining is concerned with all types of utility factors in data mining processes [12]. The main objective of utility-based data mining is to combine utility considerations in both predictive and descriptive data mining tasks. Clearly, economic utility greatly influences the assessment of the decisions made on the basis of the learned knowledge. Simple assessment measures like predictive accuracy have left way to economic measures, for instance profitability and return on investment. Utility-based data mining has been exploited in almost all walks mining techniques to extract patterns of high utility or productivity.

Utility based data mining is expected to be helpful in a wide variety of practical applications [21], where the most important objective is productive economy (higher return on investments) as it is in most finance and accounting organizations. An essential financial sector where utility-based data mining could be efficiently applied is the stock markets. A significant extent of the recent economic boom could be accounted to the substantial surge in the world stock markets and the emergence of new markets [14]. The Stock markets are complex, nonlinear, and dynamic [16]. Stock Market trading encapsulates two elemental trading philosophies; Fundamental and Technical analysis (Technical-Analysis 2005) [17]. However, it is very complicated for a normal trader or researcher to apply data mining techniques (Tech-nical analysis) on his own owing to the complexity involved in the entire data mining process [13]. An  X  X ntelligent X  prediction model for stock market prediction would be intensely desirable and would of extensive interest [15]. 1.1. Problem description
Most algorithms associated with the prediction of stock markets suffer from inconsistency as they rely heavily on the concept of frequent access pattern and the prominence offered to non-utility based param-eters. But in stock trading, trading rules are to be derived with the perspective of achieving maximum But in the literature, utility-based data mining is applied only in super market kind of applications. The utility-based data mining has not been applied in analyzing stock market data i.e. in analyzing the change of stock prices. Only general data mining algorithm, especially, classical apriori algorithm has been ap-plied in analyzing the stock price changes. The obtained rules from the stock market data are capable of informing the rate of movement of stock price only on frequency basis. Decision based on this rule possibly leads to loss in market as it is frequency based. Also, utility-based data mining based, which is working out on other applications, on stock price (as utility) will not work out practically. Moreover, the data mining algorithms has been applied over predicting the stock market movement, however, lack error. Hence, a cornerstone for effective stock market analysis is a strong need in the area.
In this article, we devise a novel and efficient approach to generate optimal and utility based trading rules to aid in stock market prediction, by employing ARM and GA. Here, we have based our work on datasets that are required for analysis are created. Subsequently, ARM is used to formulate a set of association rules based on pattern trends of stock movement. The mined association rules are given as input to GA for generating preliminary optimal and utility based rules. The obtained GA rules are that aids in cost effective prediction of stock market.

The following sub-sections comprises of a concise description of the Technical Indicators utilized in the paper, Association Rule Mining: a classic data mining technique and the GA: a popular Evolutionary Computation algorithm. The Section 2 details the recent research works and Section 3 is constituted by the proposed u tility analysis for generating trading rules w ith required illustr ation, exemplary de-scriptions and mathematical formulations. Section 4 discusses the implementation results and Section 5 concludes the paper. 1.2. Technical indicators
There are numerous technical indicators available to support in stock market prediction. Moving av-erage, exponential moving average, weighted moving average, moving average difference oscillator, relative strength index, volume, volume change, moving average convergence-divergence and more. In our work, we make use of the following technical indexes in order to achieve effective trading rules for real time stock trading.

Pivot Point (PP): There are numerous ways to calculate PP, including the averaging of the High, Low, and Close (H, L, C) of the preceding day X  X  trading, or including the Open trading price in the average. Pivot point estimation affords traders with objective visual bench marks, which are sometimes used to that are likely to cause price movement.

Exponential Moving Average (EMA): An exponential moving average gives more weight to recent prices and is calculated by applying a percentage of today X  X  closing price to yesterday X  X  moving average. The primary benefit of an exponential average is its more potential to adapt to price variations.
Relative Strength Index (RSI): An oscillator, which was introduced by J. Welles Wilder, Jr., is on the basis of the difference between average gain versus average loss over a given period. The RSI compares the magnitude of a stock X  X  recent gains to the magnitude of its recent losses. The RSI has the benefit of being a very elegant indicator, in that its movements are even, and it can fit into a neat package between 0 and 100. It has the added advantage of being utilized by several traders out there, which is not only a testament to its abilities, but it also makes its signals self-fulfilling at times. When used to specify divergences, it can be moderately influential.

Rate of change: The Rate of Change (ROC) indicator is a very simple, yet an efficient momentum change in particular fixed time duration. The Rate of Change symbolizes the momentum and therefore, the acceleration or slowing down of a trend. Higher Rate of Change of Price (ROC) denotes that stocks are overbought and Lower Rate of Change of Price (ROC) indicates oversold stock position. 1.3. Association Rule Mining (ARM)
One of the renowned descriptive data mining techniques is Association Rule Mining (ARM) [5] due to its widespread use in marketing and retail communities in addition to many other diverse fields. ARM algorithms determine high-level prediction rules in the order: IF the condition of the values of the predicting attributes is true, THEN predict values for some goal attributes [7]. It is a method of determining relationships of the form X = &gt; Y amongst itemsets that occur jointly in a database where X and Y are disjoint itemsets [3]. The association rule, mentioned above, points out that the transactions that contain X will also contain Y [6]. The association rule mining process includes two steps, namely, Generation of Frequent Itemset and Generation of Association Rule. The former groups all sets of items having support greater than a specific threshold, known as minsupport. The latter, on the basis of the frequent itemsets, formulates association rules that have confidence better than a particular threshold called minconfidence [8].

Every association rule has two metrics to measure its interestingness, support and confidence. The support of an association rule is the support of the itemset that encloses all items in the rule, that is, the itemset including the union of the items of X and Y. The confidence of the rule (X = &gt; Y) is the percentage of tuples that comprise Y from those that comprise X. In other words, confidence of the rule (X = &gt; Y) is the conditional probability P(X | Y) [18]. As per the user interest, the support and confidence thresholds can be defined. 1.4. Genetic Algorithm (GA)
GA is one search heuristic developed by Holland in 1975 [19]. It can be described as a randomized search methodology appropriate for determining best possible solutions having its roots in the natu-ral selection process [10]. Goldberg [20] based his work on genetic selection by randomly choosing a suitable string. Those strings are then passed to the appropriate processes to obtain the best machine allocation during the processing itself. The simple GA composes of 5 steps: 1) chromosome encoding, mutation [19].

Primarily, Standard GA applies genetic operators namely selection, crossover and mutation on an initial random population with the aim of comput ing a whole generation of new strings.  X  Selection deals with the probabilistic survival of the fittest in the more fit chromosomes, where  X  Crossover takes individual chromosomes from Parent chromosomes and merges them to form new  X  Mutation modifies the new solutions so as to add stochasticity in the search for better solutions.
GA X  X  have served in the successful implementation of optimal solutions for a broad variety of combi-natorial problems [10]. On the whole, the main motivation for using GAs in the discovery of high-level greedy rule induction algorithms often utilized in data mining [9]. 2. Related works
In this Section, we review the research works that are related to our area. Firstly, we review the works that employ data mining techniques in stock market data and financial data. Secondly, we review the works that introduce utility in data mining techniques. 2.1. Review on data mining techniques in stock market data and financial data
Hochachka et al. [24] have offered the distinctions among data mining analyses and parametric sta-tistical analyses. Besides they have demonstrated three strengths of data-mining tools for producing hypotheses from data. They have recommended useful ways in which data mining and statistical anal-yses can be combined into a thorough analysis of data to smooth the progress of speedy creation of precise models.

Zhang et al. [25] have offered a study on uniting game theory along with data mining by launching the concept of domination game analysis. They have offered a multidimensional market model, where each dimension denotes one attribute of a commodity. Each product or customer was denoted by a point in the the necessities of the customer. The anticipated market share of a product was measured by the expected number of the buyers in the customers, all of which are equally likely to buy any product dominating him. A Nash Equilibrium was a configuration of the products attainin g stable expected market shares for all products.

They have shown that Nash Equilibrium in such a model can be computed in polynomial time if all manufacturers attempt to alter its product in a round robin manner. In additional enhance the effective-ness of the computation, they have also designed two algorithms for the manufacturers to capably and their best response to the remaining products in the market.

Kusiak [26] has launched the basic concepts of machine learning as well as data mining. Machine learning algorithms extract knowledge from diverse databases that can be utilized to construct decision-making systems. For instance, on the basis of the operational engineering data, equipment faults can be identified, the number of items to be ordered can be predicted, best possible control parameters can be found. A framework for organizing and applying knowledge for decision-making in manufacturing and service applications was offered. His framework makes use of decision-making constructs such decision tables, decision maps, and atlases. It presents a new data-driven paradigm of significance to modern manufacturing and service organizations. Instances of data mining applications in industrial, medical, and pharmaceutical domains were also offered. It was envisioned that the data-driven framework offered improves these applications.
 Venkatesh and Thangaraj [27] have dealt with the application of SOM based clustering as well as Artificial Intelligence techniques, to examine the patterns of soils distributed across huge geographical area and recognized the appropriate types of crops for the particular soil. Estimation of exact crop(s) appropriate for a specific region can help stave off redundant maintenance and the inherent expenditures growing country like India, where the vast majority of the population depends mainly on agriculture for livelihood. 2.2. Review on utility based data mining techniques
Xu et al. [23] have studied the problem of utility-b ased anonymization. At first, they have proposed a simple framework to indicate the utility of attributes. Their framework deals with both numeric and categorical data. Next, they have built-up two simple however effective heuristic local recoding methods for utility-based anonymization. Their widespread performance studies by means of both real data sets and synthetic data sets have revealed that their methods outperform the state-of-the-art multidimensional global recoding methods in both discernability and query answering accuracy. In addition, their utility-based method can enhance the quality of analysis by means of the anonymized data.
 Chu et al. [28] have developed two novel algorithms, namely TP-RUI (Two-Phase Rare Utility Itemsets)-Mine and TRUI (Temporal Rare Utility Itemsets)-Mine, for mining temporal rare utility item-sets from temporal databases. To the best of their knowledge, this is considered to be the first work on mining temporal rare utility itemsets from temporal databases. The novel contribution of TRUI-Mine was mainly that it can efficiently recognize the temporal rare utility itemsets by producing fewer tem-poral high transaction weighted utilization 2-itemsets in temporal databases. In this fashion, the process under all time windows of temporal databases can be attained efficiently with restricted memory space, less candidate itemsets and CPU I/O time. Their experimental results have revealed that TRUI-Mine can find out the temporal rare utility itemsets with higher performance and candidate itemsets judge against to the other algorithm TP-RUI-Mine that was also proposed under a variety of experimental conditions.
Shankar and Purusothaman [12] have performed a comprehensive survey of the algorithms and meth-ods in existence for the mining of frequent item sets and association rules with utility considerations. A brief discussion of a number of algorithms was also presented along with a comparative study of a few significant ones based on their performance and memory usage.

Shankar et al. [30] presented set of algorithms to determine the active customers from retail business transaction database. They mined based on rate of recurrence of itemsets (frequency) and the business yield (utility).

Pillai [31] addressed the problem of data mining based on frequency for business data. He promoted that the utility based data mining is the preferred knowledge extraction technique for business data. The work focused on utilizing the utility-based data mining techniques for solving market-basket data.
Saravanabhavan and Parvathi [32] have presented a tree structure for utility mining itemsets. They worked out for mining high utility mining sets efficiently by compressing a large database into a smaller data structure and then utilizing their pattern growth method for mining utility itemsets.
As described in Section 1.1, it can be seen that the works that deal with the incorporation of utility in stock market data are very less. Also, the current utility based data mining cannot be utilized for stock market data as it is. These drawbacks can be overcome by our analysis and so we compare the results with conventional frequency based trading rules and utility-emphasized rules (can be obtained from the works reviewed in Section 2.2). Please note that all the frequency and utility based association rules mining algorithms show their performance only in terms of mining efficiency (i.e. time). 3. The proposed utility based analysis to formulate the utility emphasized rules
Utility based data mining is an extensive area that encompasses of all the aspects of economic utility in data mining. The works in utility based data mining is aimed at the discovery of trends of high utility mining into the stock exchange application, utility based trading rules would be generated which will aid us to make cost effective trading in stock market. Hereby, we propose an analysis for generating better, optimal and utility based trading rules, which concentrates largely in selling of stocks. The pictorial representation of the proposed analysis is given in the Fig. 1.
 The proposed analysis, as illustrated in the Fig. 1, is composed of a pre-analysis and a core analysis. The pre-analysis includes two phases of operation, namely, data interpretation based on four technical indicators and Association Rule Mining. The pre-analysis lays corner stone for the core analysis to generate cost effective (or) utility dominating trading rules. The core analysis is responsible for mining utility emphasized trading rules by means of two phases of operation, namely GA analysis for utility based preliminary rules and weightage based utility analysis. The proposed work analyzes the historical Hence, the analysis is developed in such a manner with the aid of classical apriori as well as GA. 3.1. Pre-analysis for generation of utility emphasized trading rules
The raw historical data contain closing value, opening value, highest value and lowest value of the stock market index for n periods. These raw historical data have to be normalized so that it is adaptable for the further interpretations. The interpretation that is utilized in our proposed analysis expose the movement of stock market irrespective of its closing price.

Interpretation of raw data based on technical indicators: Interpretation of the raw historical data, the first phase of the pre-analysis, is performed to make the data adaptable for further analysis. Here, the interpretation is performed based on the four technical indicators, namely, Pivot Point, EMA, ROC and RSI.
 The Pivot Point based data interpretation is given as where
From Eq. (1), an interpreted data for each period can be obtained and this interpreted dataset { D p } is based on the comparison of difference between the current day X  X  closing price and its Pivot point value with the maximum and minimum threshold value. In Eq. (2), h i is the highest stock value of i th day, c i is the close price of i th day and l i is the lowest stock value of i th day.
 In EMA based data interpretation, we utilize Simpl e moving average (SMA) along with Exponential Moving average (EMA) among various moving average means. The SMA is determined for first m days of the records (termed as SMA m ), which can be calculated as With the aid of SMA m , the EMA can be determined for each period of the interval ( m +1 ,n ) as follow where
Then a decision making process based on the resulted EMA values is performed as follows where | D EMA | = n  X  m and m&lt;i n .
 EMA max th is the maximum EMA threshold.
 EMA min th is the minimum EMA threshold.
 Thus the data set { D EMA } possesses the interpreted data based on EMA.
 The ROC based interpretation is done by means of calculating the rate of change of each day as With this dataset the decision making process will be performed as in EMA based data interpretation.
Similarly, the Relative Strength Index (RSI), the final technical indicator of our proposed analysis, is used for the interpretation of raw historical data as In Eq. (8), RS j ( m ) is the Relative Strength of the j th day on consideration with previous m days.
Thus, we can get a set of RSI based interpreted data alike the number of elements in ROC and EMA based data.
 Example 1: Consider a stock movement of a company X. As stated earlier, the data is comprised of low price, high price, opening price and closing price of the company in a particular stock market. A sample stock movement data for 9 days is given in Table 1.
 The obtained interpreted data based on the four different technical are tabulated in Table 2. In Table 2a, the first record has the PP values based on the closing price of the previous 3 days i.e. Day 1 of first record considers the closing price from 3rd June, 2000 to 6 th June, 2000. In other words, (in our case, it is selected as 3) and so the PP value starts from the fourth day (mentioned as day 1 in Table 2a), which is based on previous 3 days. Following the steps, day 2 X  X  PP value in the second record is occupied by day 2 price of the first record and so on. Only, the day 7 price is new to the record 2. The similar process if followed for calculating all the subsequent records. But the remaining technical indicators calculate the value based on only the previous day value. Hence, for manipulation purpose, the first day values are omitted and organized as given in Table 2.

ARM for trading rules generation: In the second phase of the pre-analysis, Association Rule Mining is of deriving relationships of the form X  X  Y among itemsets that occur together in a database where X and Y are disjoint itemsets [29].

Apriori, a classic algorithm for learning association rules is used here for mining association rules. As we have four different interpreted datasets, ARM is performed over each data set separately by apriori. Each mined rule set has the maximum index values of n P , n EMA , n ROC and n RSI for the mined datasets, [ (Day3 = PP Down) with a support of 2 from Table 2a. This resultant mined data will be fed to the GA for the purpose of generating trading rules. 3.2. Core analysis for mining the utility emphasized trading rules Generally, the utility based data mining analysis focus on the price of a product or itemsets. Example 2: Consider an itemset I 1 with contributing transactions of 5 (i.e. frequency = 5) and price $10 (utility value = $10) and another itemset I 2 with the contributing transactions of 7 (i.e. frequency = 7) and price $12 (utility value = $12).

In the above example, I 2 can be taken as an interesting itemset (because the utility of I 2 is $84 = 7*$12 whereas I 1 is just $50) if we deploy the conventional utility-based data mining as it is, however, for stock market data this is not the way to determine the interesting days for high return of investment. between the current day and previous days as utility significance.

GA based analysis for the generation of better optimal trading rules: In the first phase of the core analysis, GA is applied for generating utility based preliminary trading rules from the generated set of trading rules, which are obtained from pre-analysis. In general, GA is a search technique utilized for discovering nearly precise or approximate solutions to optimization and search problems. In our work, we use GA to search for preliminary trading rules based on utility by considering the combination of the four sets of mined technical indicators based rules.
 Chromosome initialization: Initially, the chromosomes, constituted by genes, are generated randomly. Here, the chromosome length L is  X 4 X , which means each chromosome is constituted by four genes with a population size of n c . Fitness evaluation: The fitness value decides the survival of each chromosome and it is determined as
In Eq. (9), the fitness for each technical indicator based rules are determined, where T represents Pivot point, EMA, ROC and RSI. To make the objective as a minimization function, we use Eq. (9) by sub-technical indicator can be directly considered, but the objective would be a maximization function. The as follows Here, in Eq. (10), p a c confidence of a Pivot point based rule respectively, p a c confidence and consequent-confidence of an EMA based rule respectively, p a c parameter of antecedent-confidence and consequent-confidence of a ROC based rule respectively and p rule respectively.

The fitness of each technical indicator based rule is nothing but the product of parameter of antecedent-confidence and consequent-confidence of a rule. The parameter of antecedent-confidence and consequent-confidence for Pivot point based rule is given respectively as and
The antecedent-confidence and consequent-confidence are equivalent to the confidence calculation in association rule mining. However, antecedent-confidence can be defined as the frequency of rules with both antecedent and consequent among the rules only with the associated antecedent. Subsequently, consequent-confidence can be defined as the frequency of rules with both antecedent and consequent among the rules only with the associated consequent In the above mentioned equations, n A represents the number of occurrences of the Pivot point based rule, which has A as antecedent and B as consequent, in the dataset of association rules, n A the Pivot point based rule, which has A as antecedent and other than B as consequent, in the dataset of association rules and n not A which has any patterns other than A as antecedent and B as its consequent. Similarly, the parameter of antecedent-confidence and consequent-confidence for EMA based rule, ROC based rule and RSI based rule is determined and hence the corresponding fitness values is evaluated. Finally, the mean fitness will be calculated using the Eq. (9) and thus each chromosome gets evaluated. For convenient calculation, here we use mean fitness. The same problem can be solved as general or customized multi-objective problem so that the best rules can be obtained with more concentration on specified technical indictors. Moreover, the usage of GA can increase the efficiency of obtaining the near best rules.
Crossover and mutation: Here, we use single point crossover for crossover operation in which the crossover point C p is chosen as unity (i.e. C p =1 )asthereare L  X  1 points. Hence after crossover, we obtain another n c number of children chromosomes. Then, Mutation is performed over the children of each parent chromosome. In general, mutation is achieved by randomly changing two of the gene values of child chromosomes.

Chromosome selection: After crossover and mutation, an entirely new population of 2 n c chromo-somes is obtained. Fitness is calculated for the newly obtained population using Eq. (9). Among these 2 n c chromosomes, better n c chromosomes which have minimum fitness are selected as the new parent chromosomes. The same process of crossover, mutation, fitness evaluation and selection are repeated until the iteration value reaches its maximum iteration limit.

After the process gets terminated, we have n c chromosomes which are the better chromosomes. Each chromosome represents the combination of the trading rules we have generated already by ARM algo-rithm. The obtained trading rules by means of GA is given by resultant trading rules are utility based preliminary rules as it takes the combination of the rules based on the four technical indicators into account. For example, a sample of final best population pool rules is given as in Table 3.

These rules are just preliminary rules and they can be called as utility-based preliminary rules because the rules are based on technical indicators, which consider the stock movement based on return on invest-ment. Such obtained rules are applied for weightage based utility analysis to obtain utility emphasized trading rules.

Weightage based utility analysis: This weightage based utility, phase II of the core analysis, concen-trates greatly in mining utility dominating trading rules. In any uncertain environment, the frequency of occurrence of a state and the confidence level of its occurrence are considered to be the primary and the economic significance (if any) as secondary. The same hypothesis is followed in our case also. Firstly, we have focused on preliminary rules by considering the confidence factors and then we consider the closing prices to generate utility-emphasized rules. Moreover, incorporating both the objectives in GA the processing with GA. These utility emphasized trading rules aid to perform a cost proficient trading in stock market. A pre-arranged format of closing values will be generated from the raw historical data to perform the utility based analysis. The pre-arranged format of the closing values can be obtained as [
D RSI ] which individually depends upon the technical indicators. Hence, we can obtain the corresponding index values of each rule from the interpreted dataset. For instance, if a rule from GA is given as (Day1 =
PPLevel Day2 = PPLevel) == &gt; (Day3 = PPLevel); (Day1 = EMALevel Day3 = EMALevel) == &gt; (Day2 = EMALevel Day4 = EMALevel); (Day3 = ROCLevel Day4 = ROCLevel Day7 = ROCLevel) == &gt; (Day5 = ROCLevel Day6 = ROCLevel); (Day2 = RSILevel Day5 = RSILevel Day6 = RSILevel) == &gt; (Day1 = RSILevel Day4 = RSILevel), then this will be queried into the interpreted data to get the index values of the locations of where the rule has occurred.

The GA rule based on PP is queried by representing the day value as column into the interpreted dataset, so that we can get the row values of the dataset as queried index values I q p . Example 3: If (Day6 = PP UP) == &gt; (Day7 = PP UP) is queried in Table 2a, then I q boldly marked records in Table 2a) is obtained.

As the rule will exist in the dataset more than once, the I q p will be a vector of one or more integers within the limit of n  X  m +1 . The same querying process will be performed for the other indicator based rules from GA so that we can get the queried index vector I q vectors thus formed have a size which may be or may not be equal to each other. With these index values, weightage will be calculated for each GA based trading rules which will decide the utility of each rule generated so far.

Weightage calculation: The weightage of each gene of every GA based rule which is pointed by the index values of the final best chromosomes are determined by
In Eq. (15), the weightage for pivot point gene of the GA based preliminary rule is calculated and so r  X  I chosen in such a way that the deviation of the closing price between the chosen days should be higher than that of the others. Similar calculations will be performed for the other genes of each rule so that r  X  I variances of closing price values are taken into account while calculating the Eq. (15), as here we are concentrating only on the sales of the stocks. Hence we will obtain [ w R q =1 , 2 , 3 ,...,n increase of the stock price when compared to the previous day stock price of the rule. The significance f ( q ) represents the significance level and the second term 1 resents the utility value. The weightage vector will be sorted in such a way that we are making a com-bination of the technical indicators based rule, which is the gene of each GA rule, on the weightage basis. So, the resultant rule will have a combination of the technical indicators based rule in which each technical indicator based rule have the highest weightage.
 Example 4: Consider Table 2d for easy understanding of weightage based utility analysis and the process of Example 3. Querying the rules (Day6 = RSI UP) == &gt; (Day7 = RSI UP) (marked as bold) and (Day5 = RSI UP) == &gt; (Day6 = RSI DOWN) (marked as italic) in Table 2d produce | I q 1 respectively. The further steps of the example interpret the aforesaid weightage calculation.  X  Obtain the indices (Day6 = RSI UP) == &gt; (Day7 = RSI UP) and (Day5 = RSI UP) == &gt; (Day6  X  The corresponding RSI values are taken from the following days. The closing prices of the mentioned days are considered the weightage calculation.

Thus obtained rules are highly utility emphasized which depicts us an exact stock market status and and arranged on the basis of weight. The rules that are obtained after weightage based utility analysis are given in Table 4.

Eventually, all the four rules that are present in the record of first chromosome are obtained as the utility-emphasized rules. 4. Implementation results
The proposed analysis for generating utility emphasized trading rules has been implemented in JAVA platform (version 1.6). The performance of the technique has been evaluated using different historical stock market data. 4.1. Dataset description
In analyzing the technique, we have used the day-by-day stock price of Bombay dyeing and Man-ufacturing Company Ltd, Reliance India (Communication) Ltd., Wipro (Infotech) Company Ltd. For Reliance and Wipro, we have the stock movement price for 5 years, 2004 to 2008. So, we have gener-ated five stock price datasets for both the companies. Each dataset has opening price, closing price, high and low price of the 273 days (for 12 months). i.e. n = 273 days. Ultimately, we have evaluated and analyzed the technique with five datasets of the Reliance communications, five datasets of the Wipro and a dataset of the Bombay dyeing. 4.2. Results and discussion
In our technique, we have set m = 7. The interpretation of the historical data has been performed and then ARM is applied in the pre-analysis. In core analysis, we have performed utility based GA analysis for preliminary trading rules and then a weightage calculation.
 4.2.1. Threshold selection
In the interpretation of raw data, the selection of threshold values is significant. In our experiments, trial and error method is performed to select the threshold values.

Criterion: A selection of minimal and maximal threshold value has to produce a reasonable number of interpreted data levels.

Theoretical proof: When selecting these values, we got 90 Ups, 90 DOWNs and 90 LEVELs (approx-imate values). This is because that 273 3  X  270 3 =90 . When other than these thresholds are selected, any of the three levels (either UPs or DOWNs or LEVELs) has reached higher frequency. The reason having equal status throughout the analytical periods. In other words, a dataset should be interpreted in such a way that it must have all the three levels of data at equal frequency in approximate. Otherwise, the impact of the data level which has maximum frequency will reside in the utility emphasized rules only based on the frequency pattern. 4.2.2. GA parameters selection
The chosen GA parameters are: population size of 50, crossover rate of 0.5, mutation rate of 0.3 and maximum number of iterations of 100. Generally, the crossover rate would be selected to a maximum is because of the fact that (i) the offspring obtained after crossover should have maximum property of space with index values, the genes have to be mutated strongly i.e. the new offspring should be deviated at least 30% from its parents. Hence, the mutation rate is selected as 0.3. 4.2.3. Empirical results
After all these analyses and executing the steps of the technique, we obtain  X 50 X  trading rules each having a weightage calculated from the historical data. The rules thus obtained from the analyses that support better trading are given in the Table 1.

Here in Table 1, only four best rules among the  X 50 X  rules are given. It is to be noted that the rules are presented in the apriori format, where the antecedent and the consequent part of the rule need not be values of the days Day 1, Day 2 and Day 3 are 631.24, 632.71 and 635.28 respectively, then the closing price of Day 4 and Day 5 would be 646.42 and 646.43 respectively. This rules means that if any of the days has a closing value of 631.24, then the subsequent/mentioned days from the first day will follow the given closing value. The frequency of such rules is given in the table. For example, the days that follows aforesaid rule can be given as {March 31st (if day 1), April 1st (then Day 2), April 2nd (then Day 3), April 3rd (then Day 4) and April 4th (then Day 5)}, {January 15th (if Day 1), January 16th (then Day 2), January 17th (then Day 3), January 18th (then Day 4) and January 19th (then Day 5)}, etc. In this manner, rules based on the other technical indicators have also been presented in the Table 1 along with the corresponding weightage. The days with the most difference in closing values from the previous day are mentioned as best days in the Table 1. This weightage decides the combination of the technical indicators based trading rules obtained from the GA preliminary rules. 4.2.4. Analytical results
The rules obtained from the technique have been analyzed in terms of utility and weightage. Among the obtained utility emphasized rule s and the non-utility emphasized rules from ARM are illustrated in Fig. 2.

In Fig. 2, some five best trading rules obtained a s from our proposed utility analysis are compared separately with the rules obtained from the classical apriori based on the weightage. The weightage for apriori rules has been determined as similar to the weightage calculation given in Eq. (15).
Secondly, the mean utility an d weightage value of the 5 trading rules have been compared against the mean utility and weightage of all the rules obtained from apriori and finally, the calculated mean utility and weightage value of the proposed 5 trading rules are compared with the mean utility and weightage values of the rules that can be obtained from conventional utility mining algorithms. Please note that all the association rule mining algorithms based on frequency and utility only focus on improving mining efficiency (i.e. time). Here we don X  X  consider the efficiency instead we focus on effectiveness of the obtained rules. Hence, we made a comparison over the frequency based rules (it may be from any frequency based ARM algorithms but here we consider Apriori) and conventional utility-emphasized rules (as described in Example 2).

In Figs 3b(i), 4e(i) and 5(i), the PP, RSI and ROC values of utility emphasized rules are less than the conventional utility-emphasized rules, respectively. However, when comparing with other indicators, this indicators. Moreover, the major concern is that, this happened only in considering the utility. But our rules are based on utility as well as frequency. So, when analyzing the weightage based utility analysis, utility weightage of all our rules are greater than the conventional utility-emphasized and frequency-based rules. In analyzing the frequency of proposed utility-emphasized, conventional utility-emphasized and frequency-based rules, our rules may have lesser frequency rather than the conventional utility-emphasized and frequency-based rules. Even though the proposed rules have lesser frequency value measures calculation determines the percentage of increment/decrement of utility value or frequency or weightage over the frequency-based rules and p roposed utility-emphasized rules respectively. Similarly, D proposed and D conventional are determined for depicting the deviation between the proposed utility-emphasized rules and conventional utility-emphasized rules. Hence, de viation between proposed u tility-emphasized rules and frequency based rules is plotted in Fig. 9 and deviation between proposed utility-emphasized rules and conventional utility-emphasized rules are based rules is plotted in Fig. 10 by determining the mean deviation of the different technical indicators for the different datasets. Please note (in Figs 9 and 10) that the positive bar indicates higher performance over the other whereas negative bar indicates lower performance over the other.

We can see the performance variation of the proposed, conventional utility-emphasized rules and frequency-based rules in terms of frequency of occurrence of the rules, utility and weightage values of rules for Reliance Communications occurred 10% (approximated average value) more than that of frequency-based rules in the five year historical dataset, whereas frequency-based rules are just 5% (hence it is shown as negative in Fig. 9a) of the pr oposed rules. Similarly, the utility and weightage value of the proposed utility-emphasized rules a re 10% and 23% more than the trading rules that are mined based on frequency. When analyzing the average performance (in terms of the three parame-ters), proposed utility-emphasized rules are 15.29% be tter than the frequency-based rules whereas the frequency-based rules performs only 9.32% of the performance of proposed utility-emphasized rules. Similar deviation analysis is performed between the proposed utility-emphasized rules and conventional utility-emphasized rules. As an extr eme extent, the proposed analysis offered 82.4% more better trading rules for Bombay dyeing and Manufacturing Ltd over the rather than the conventional utility-emphasized rules in terms of frequency, utility and weightage. Thus obtained utility emphasized rules will be very supportive for stock se lling as well as the prediction of the sto ck market. Nowhere the proposed rules under perform over the others, which is the essential pre-requisite when getting into the business world. 5. Conclusion
Utility mining problem is at the heart of several domains, including retailing business, web log tech-niques, etc. The incorporation of utility based data mining into the economic oriented domains leads to high returns, the best suited instance may be the stock exchange. The proposed utility based analysis for generating trading rules has succeeded in generating trading rules which are utility dominating. As, powerful technical indicators were used in our analysis, utility based processes were also measured to be more effective. Rather than, generating trading rules based on raw historical data or only technical indicators, our proposed analysis generates more effective trading rules based on the combination of ity emphasized rules. Thus obtained utility emphasized rules were compared with the classically mined frequency-based and utility-emphasized rules. The comparison proves that the former rules have more utility rather than the later. Thus generated utility emphasized ru les would also be v ery suppor tive in taking decisions in stocks sales and thereby in prediction.
 References
