 Non-guaranteed display advertising (NGD) is a multi-billion dollar business that has been growing rapidly in recent years. Advertisers in NGD sell a large portion of their ad cam-paigns using performance dependent pricing models such as cost-per-click ( CPC ) and cost-per-action ( CPA ). An accu-rate prediction of the probability that users click on ads is a crucial task in NGD advertising because this value is re-quired to compute the expected revenue. State-of-the-art prediction algorithms rely heavily on historical information collected for advertisers, users and publishers. Click predic-tion of new ads in the system is a challenging task due to the lack of such historical data. The objective of this paper is to mitigate this problem by integrating multimedia features extracted from display ads into the click prediction models. Multimedia features can help us capture the attractiveness of the ads with similar contents or aesthetics. In this paper we evaluate the use of numerous multimedia features (in addition to commonly used user, advertiser and publisher features) for the purposes of improving click prediction in ads with no history. We provide analytical results gener-ated over billions of samples and demonstrate that adding multimedia features can significantly improve the accuracy of click prediction for new ads, compared to a state-of-the-art baseline model.
 H.4 [ Information Systems Applications ]: Miscellaneous; D.2.8 [ Software Engineering ]: Metrics X  complexity mea-sures, performance measures Multimedia Features, Image, Flash, Click Prediction, New Ads, Display Advertising, GMM
Display advertising generates revenue by showing graphi-cal ads on web pages. It is traditionally sold as a guaranteed delivery (GD) contract, which constitutes a deal between a publisher and an advertiser to deliver a pre-specified num-ber of impressions over a certain period of time at a fixed price per impression (CPM). An alternative mechanism for delivery that has been growing in the recent years is spot markets, where the impressions are sold one at a time. Spot markets, such as Right Media Exchange (Yahoo), lets the advertisers bid differently for each impression, allowing them to use highly granular targeting methods. Most spot mar-kets also provide advertisers with a wider range of pricing models. Similar to GD, advertisers can choose to pay per impression (CPM). However many advertisers would pre-fer to pay only if the ad attracted the user X  X  attention. To address this concern, many NGD exchanges provide perfor-mance based pricing models such as pay-per click (CPC) and pay-per-conversion (CPA), which can be further categorized as post-view or post-click, depending on there being a click before the conversion event. In a marketplace where ads with different payment models are competing for the same opportunity, the auction mechanism needs to convert the bids to a common currency. This is often done by comput-ing expected revenue (eCPM). For a CPM ad the expected revenue is obviously going to be the same as the bid. For a CPC ad, however, the expected revenue depends on the probability that the user will click on that ad. Similarly, the expected revenue of a post-view CPA ad depends on the probability of conversion after the user views the ad; and for post-click CPA the expected revenue calculation needs to take into account both the click and the conversion prob-ability. As a result, accurate prediction of click probability plays an important role in NGD advertising.

A state-of-the-art NGD system typically relies on machine learning models to estimate the click probability of eligible CPC/CPA ads. These models are trained using data col-lected from live systems. The identity of the users, publish-ers and ads are typically used as features in such models, together with some high level category information. For ads that have been in the system for a long period of time, the es-timation of click probability using identifier based features is generally reliable, however it becomes a challenging problem for new ads. Besides, identifier based features for advertis-ers fail to provide any information about the aesthetics or the content of the ads, which is the key factor that the user responds to.
This cold-start problem is often addressed by the use of content based features. However, most literature in this area focuses on textual representation of content. What makes this a unique problem for display advertising is the fact that the ads are not represented as text; display ads are graphical ads. In this paper we propose the use of multimedia features to represent the content (and aesthetics) of such ads. The contributions of our paper are as follows: We develop fea-tures that can be extracted from both static images as well as animated flash ads. We explore the use of global features to capture the visual characteristics of the entire image, local features to describe the sub sections in an image, and high level features to capture the user X  X  perception. For flash ads, we develop an additional set of features that are extracted from the meta data. We also propose a clustering model based approach to capture the shared visual content of these images, and investigate the use of the group id as features. A feature selection algorithm is also developed to remove fea-tures with low click relevancy and high redundancy. Finally, we show experimental results on data sets collected from a commercial NGD exchange demonstrating multimedia fea-tures significantly improve the click prediction for new ads, compared to a state-of-the-art baseline model.

The paper is organized as follows. In Section 2, we de-scribe the baseline NGD click prediction model. The multi-media features used in our work are introduced in Section 3 followed by the introduction of our feature selection method in Section 4. We present the feature analysis and modeling results in Section 5. Related work is discussed in Section 6.
In NGD advertising a spot auction is run for every ad slot on the publisher X  X  page, in which all advertisers with matching target profiles participate. The ads are ranked based on their expected revenue and the winning ad is dis-played. Estimating the expected revenue for pay-per click and post-click conversion payment models requires knowing the probability that the user will click on the candidate ad if shown in that ad slot on the publisher X  X  page.
The click prediction problem in NGD can be formulated as a classification problem, where each data point represents a publisher-ad pair presented to the user. Assume there is a set of n training samples, D = { ( f ( p j ,a j ,u j where f ( p j ,a j ,u j )  X  d represents the d -dimensional feature space for publisher-ad-user tuple j and c j  X  X  X  1 , +1 } is the corresponding class label (+1 : click or  X  1 : no-click). Given a publisher p ,ad a and user u , the problem is to calculate the probability of click p ( c | p, a, u ). We use a maximum entropy algorithm for this supervised learning task because of its simplicity and strength in combining diverse features and large scale learning [3]. The maximum-entropy model, also known as logistic regression, has the following form: where f i ( p, a, u )isthe i -th feature derived from the publisher-ad-user tuple ( p, a, u )and w i  X  w is the weight associated with it. Given the training set D , the model learns the weight vector w by minimizing the total losses in the data formulated as: where L () is a logistic loss function used in this paper and  X  controls the degree of L 2 regularization to smooth the objective function. L-BFGS [5] is well suited to solve this kind of large scale convex optimization problem.
Designing informative features is very important for su-pervised learning algorithms. Many features are derived from the publisher-ad-user tuple. On the user side, demo-graphic information such as age and gender are common features used for click prediction. On the publisher and ad-vertiser side there are hierarchies of entities. The entity identifiers are typically used as features in click models to capture the click behavior at different levels of abstraction. Publishers use site id to label their sites and section id to tag different parts of their pages. The url and host of the page are also informative features. An advertiser can set up multiple campaigns and creatives and the same creative can be used in multiple campaigns. And finally, publishers and advertisers connect to ad exchanges via networks, which constitute the root of the hierarchies.

The identifier features are all binary indicators that take the value 1 when present and 0 otherwise. Other ad features that are useful include the size of the ad, the topical category and the format (e.g. pop-up, floating or static banner ads).
Conjunctions are used to capture the interaction between different feature groups, such as user and publisher, pub-lisher and ad, and user and ad conjunctions The number of features will grow exponentially after feature conjunction. Given a large set of identifiers, the final number of param-eters in the model will be very large. Feature hashing [29] is a simple and effective dimension reduction technique to limit the feature space as well as maintaining the model per-formance by hashing the feature to a predefined number of bins.
Display ads are mainly created in two formats, image and flash, from which we derive four sets of features. The first set is composed of features generated from images and im-age elements in flash ads. They are designed to capture the visual aspects of the images that may strongly affect users X  response to the ads. The second set of features consists of meta information extracted from flash ads. In addition, we extract the latent mixture components from the images to capture their shared visual content as a separate set of features for click prediction. Finally, the image and flash features extracted from the ads are conjoined with user and publisher side features. For instance, the user age and ad color can be used as an additional feature to capture the variations in different age groups X  responses to different col-ors.
A digital image with resolution X  X  Y is typically treated as a grid of pixels with X rows and Y columns. The in-tensity of each pixel at location ( x, y ) can be represented in various color spaces including but not limited to RGB, Grayscale, HSV, HSL and YUV. RGB stores individual val-ues for red ( R ), green ( G ) and blue ( B ) for each pixel at ( x, y ). RGB can be easily converted to grayscale and con-sequently to binary, black or white, by setting a threshold value on grayscale value. HSV (hue, saturation, value) is another color space that takes human perception into ac-count in the color encoding. In HSV, the brightness of a pure color is equal to the brightness of white. HSL (hue, saturation, lightness/luminance) is similar to HSV, except that the lightness of a pure color is equal to the lightness of a medium gray. The YUV model defines a color space in terms of one luma (Y) and two chrominance (UV) com-ponents. Different color spaces characterize an image from different perspective, based on which we can extract various features to describe the content of the image.

The features extracted from the image are divided into three categories, global features, local features and high level features. Global features are utilized to describe the content of the entire image using a small number of values while local features represent the characteristics of the local regions of the image. Both global and local features are computed directly from the image. The high-level features attempt to capture the human visual perception of the image. They involve more complex processing of the underlying image data, that typically requires applying a model trained on an additional image corpus. A subset of these high-level visual features has been used in the context of predicting a user X  X  photo preferences in social media sharing sites [28].
Global features capture the visual effect of the entire im-age as a whole and are generally easy to calculate. The global features used in our work are introduced as following:
Users often pay more attention to certain regions in an image. Features that are generated from local regions can therefore be useful. To generate local features, we first divide the image into many segments using a connected component algorithm [25]. Let S = S 1 ,S 2 , ...S g be the final g set of seg-ments created from the image (note that segments smaller than 5% of the image are dropped). Some of the global fea-tures introduced in Section 3.1.1 can be extended to local features by calculating them in local segments similar to [2]. The local features used in our work are described as follows:
Most of the global and local features introduced above describe the content of the image at low level of visual per-ception. In this section, we introduce a set of more advanced features that is able to capture high level interpretation of an image.
Flash is commonly used in display advertising, because it enriches the user experience by supporting streamlining of video, audio, animation of text, drawings and images. Compared with static image ads, flash ads are more attrac-tive and therefore more likely to be clicked by users. Hence features extracted from flash ads can provide additional in-formation to click models. In our work, a flash ad is de-composed into many elements including image, sound, font, text, button, shape, frame, and action. Obviously the image features introduced in Section 3.1 can be extracted from the image element of a flash ad. A list of flash specific features are introduced as following:
When users look at a display ad they do not perceive it as a matrix of pixels, but rather they process the content of the ad. Images with similar content may receive similar responses from users. One way to capture this is to clus-ter images based on content similarity and use the cluster membership as a feature. We use a Gaussian Mixture Com-ponent (GMM) model for this purpose, instead of more ad-vanced models such as Probabilistic Latent Semantic Anal-ysis (PLSA) [13], because of its scalability. The weight of mixtures as well as the mean vectors and covariance matrices for each mixture are learned through a maximum likelihood process from the images in the training set, which are de-scribed as vectors of image features introduced in Section 3.1. For every image in the test data we estimate the prob-ability of component membership from the learned model. The component id with the maximum posterior probability is then used as the mixture component feature in the click model.
The  X  X ttractiveness X  of a rich media ad to different users will vary because users may have different interests and taste. For example, male and female users will certainly react differently when seeing an ad with a beautiful human face; young users may be more attracted to ads with car-toons than older users. The ad performance on different publishers also varies. As an example, ads with cars in the image are more likely to be clicked when shown on a au-tomobile related site than when shown on a fashion site. These factors are not taken into account by the multimedia features introduced above since they are extracted from the ad content only. To solve this problem we use conjunction features, generated by taking the cross product of multime-dia features with user features (such as age, genderetc.) or publisher features (such as publisher id, url, etc.).
Some of the multimedia features introduced above may not be relevant to the click prediction task or can be redun-dant because of the intrinsic correlations with other features. Including all of them in the model may not only increase the Table 1: Feature set in baseline model for click pre-diction.
 Feature Group Feature Name
Publisher publisher id, publisher network id, User age,gender
Ad ID advertiser id, campaign id, creative id,
Ad Type ad size, offer type id, pop type id model complexity but also degrade the model performance. In this paper, a clustering based feature selection algorithm is developed to remove irrelevant and redundant features. First, features are ranked based on their relevance to the tar-get in the training data, measured using a standard mutual information method [32]. Irrelevant features are removed by thresholding the relevance score.

We use a spectral clustering method to group similar fea-tures together. A fully connected similarity graph is con-structed, in which nodes represent features and edge weights are defined by the similarity (in terms of mutual informa-tion) of the two features they connect. A normalized cut method [9] is then applied to the graph to obtain clusters of features that are strongly correlated with each other. Finally features within each cluster are ranked using the relevance score and only the top k features in each cluster are selected to build the model. Clearly, there is a trade-off between rel-evance and redundancy that can be tuned by varying the k according to the size of the cluster.
In this section, we first describe the features, data sets and models used in the experimental evaluations. Then we present a click-through rate (CTR) analysis of the image fea-tures. Finally, we compare the click prediction model with multimedia features to a baseline model on click prediction of new ads.
We conducted our experiments on data sampled from Ya-hoo X  X  NGD advertising system, from a period of 5 weeks. Each sample is an impression of an ad and is labeled as  X  X licked X  or  X  X ot clicked X , as derived from the user click logs. The data from first 4 weeks is used as training set and the last week is treated as test set. There are approximately 2 . 3 billion samples in the training data. The data contains about 1 . 4 million unique display ads. 54% of the ads are images and the rest 46% are flash objects. The first image of the flash ad is extracted as its image representation. In order to evaluate the performance of multimedia features on new ads, we further create subsets of test data, which in-clude only impressions with new ads that never appeared in the training period.

The baseline model used for comparison is trained based on numerous features extracted from users, publishers and advertisers. Table 1 summarizes all the basic features used in the baseline model. The baseline model also includes all the conjunction features conjoining every pair of feature groups in Table 1. A 24-bit hash function is used to hash all the features into 16 million bins. With 16 million features, the baseline model is considered strong. To evaluate the performance of multimedia features in click prediction, we retrain a model by adding multimedia features to each train-ing sample as additional features. Events in the test set are ordered by the predicted clickability score. Precision-recall ( P-R ) curves as well as area under the ROC curve ( AUC ) are calculated to measure the accuracy of the models.
We evaluated three click prediction models with multi-media features. The first model uses all the multimedia features introduced in Section 3. The second model adds only the mixture component feature to the baseline model. We set the number of components to 150, largest we could use without increasing the cost of feature generation beyond our latency budget. The third model utilizes a subset of the multimedia features selected using the method presented in Section 4. Note that all the models use the same 24-bit hash function to hash all the features. As a result, adding multimedia features does not increase the model size.
Here we conduct an initial study on various multimedia features to obtain an intuitive understanding of their impact on user X  X  ad click behavior. One week of test data is used for this analysis. We quantize each multimedia feature into multiple bins using a k-means clustering algorithm and the CTR for each bin is calculated. Figure 1 shows the CTR dis-tribution for 9 representative multimedia features selected in our study. The Y -axis of each figure shows the CTR and the X -axis represents the bin index of features. The actual CTR values in the figures are omitted to protect proprietary infor-mation. Our observations from Figure 1 are listed as follows (note that images referred below include flash images): It is clear that all these features are highly correlated with CTR and therefore have a great potential to improve the performance of click models. In fact, many of these observa-tions are consistent with each other. For example,  X  X imple X  images often have a small number of interest points, a few connected components, or a high ratio of dominant color. These observations can also be used to guide the creative design process for the purpose of increasing their CTR. Figure 2: Percentage of new unique advertiser iden-tifiers, creatives and campaigns emerging for each day in one month (relative to those existing in the training period).
Modeling multimedia features allows us to improve the click prediction for new ads by enabling the use of histori-cal performance of pre-existing ads with similar content. In order to evaluate the scope of the problem we first mea-sure the burst rate of new ads in the data. For this anal-ysis we treat the first 4 weeks of data as the reference set, and calculate the ratio of new ads per day for the follow-ing 2 weeks. Advertiser campaigns often have an inherent structure: one advertiser can have multiple campaigns, and one campaign might have multiple creatives. In the case of Yahoo X  X  exchange the same creative can actually appear in multiple campaigns. To understand the problem space better we measure the burst rate at each of these levels sep-arately. As shown in Figure 2, it is clear that the burst rate is increasing steadily day by day for all three levels, however creative has the most significant change. 31 . 7% of unique ads are new creatives after a period of 13 days. The burst rate of campaigns is slower, with 23 . 0% of new unique ads after a period of 13 days. The advertiser is the most stable of the three levels, which still has 11 . 9% new unique ads. This underlines the impact of our work on the performance of the NGD advertising system.
 Table 2: Performance improvement in terms of AUC of click prediction model with multimedia features on new ads with different level of newness.

Next we evaluate the performance of the three click mod-els with multimedia features as introduced in Section 5.1 on new ads. Specifically we create multiple slices of test data which contain new ads represented at four different levels: advertiser, creative, campaign and campaign  X  creative. The campaign  X  creative slice includes ads with either new cam-paigns or new creatives. The P-R curve and corresponding AUC for each slice of test data are shown in Figure 3. Look-ing at the test data with new advertiser id, we observe a significant improvement by adding the multimedia features. The model with all multimedia features gains 6 . 03% in terms of AUC. Using feature selection method introduced in Sec-tion 4 improves the performance even more, with a 6 . 50% gain over baseline. This is not surprising since a lot of fea-tures introduced in Section 3 seem strongly correlated with each other. It is also worth mentioning that the mixture component feature is so informative that adding it alone brings 1 . 5% gain over the baseline. We also observe notable improvements by adding multimedia features on the other new ad slices. The improvements are relatively smaller com-pared to the result on the new advertiser slice. This is ex-pected because advertisers use the same advertiser identifier for new creatives and new campaigns thus providing some historical information even for new ads. Table 2 summarizes the AUC gains compared to the baseline for the model with selected multimedia features using the method introduced in Section 4. All the improvements are statistically significant ( p&lt; 0 . 01) based on a simple paired t  X  test on the predic-tion results generated by thresholding the prediction score of the compared models using the equal error point (EER). In summary, multimedia features play an important role on advertisers with limited historical information. Note that all these improvements are achieved without adding model complexity in terms of the total number of features due to the feature hashing strategy.
Click prediction in on-line advertising has received in-creasing attention from the research community in recent years. Most of the work in the literature can be categorized broadly as either new feature or new model development. In terms of feature development, Liu et al. [20] propose to use syntactic features for sponsored search to model the rel-evance between query and ad by treating the ad X  X  text as a short document and building a language model as in classic information retrieval; Chakrabarti et al. [6] developed click feedback features [6] based on aggregated historical click data, which has proven to be very effective in click predic-tion; Cheng et al. [7] developed a personalized click model by including user specific features and demographic features which dramatically improve the performance of click predic-tion. In terms of model development, maximum entropy [7, 3] or decision trees [10] are common models used for click prediction in on-line advertising. Specifically for sponsored search we see the use of generative graphical models [30] that focus on identifying the factors that affect the user X  X  response to ads on a search page; Graepel et al. [11] pro-pose a Bayesian on-line learning algorithm used for CTR prediction in Bing X  X  sponsored search product.

Most published work in the area of click prediction for new ads is focused on identifying new ads either by using categories [6] or identifying a set of similar ads by using the textual content of the ads [10], which cannot be ap-plied to display advertising directly. In our case, some level of ad and page category was already being utilized by the baseline models. An alternative approach was proposed by Agarwal et al. [1] that utilizes the hierarchy in the advertis-ers campaigns to improve the prediction for new ads. While we did not use the exact same model, we did use features that capture part of this information in our baseline mod-els. Therefore the improvements we present in this paper are complementary to their work.

Image features have been widely researched for the task of content based image retrieval (CBIR)[31, 24] including both global features and local features. Global features capture global characteristics of images such as color histogram, tex-ture values, shape parameters. For example, Yoo et al. [31] developed a set of global visual features for the task of large scale image searching. Global features fail to work when the retrieval task is targeted to specific local objects, e.g. hu-man face recognition. Local features are typically used in such tasks [24]. CBIR mainly focuses on finding similar im-ages in a database, whereas our task requires measuring the  X  X ttractiveness X  of the image particularly in terms of CTR. There is some work in literature developing features to esti-mate the beauty of an image using many features [33, 19], however little work is found on exploring their correlation with CTR. In addition to the features being studied in this paper, there are other derivatives from the computer vision domain that could have a strong impact on user X  X  response to the ads, such as company logos. We would like to extend our analysis to include these features.

To the best of our knowledge Azimi et al. X  X  [2] proposal is the first use of multimedia features for the purposes of esti-mating click probability of display ads. Some of the image features described in Section 3.1 were introduced in their work. However their proposal was limited to static image ads in the absence of publisher and user information, and the experimental results provided in their work used a sim-ple weighted sampling based CTR estimation as baseline on asmallsetofsamples.
In this paper, we propose to use multimedia features to improve the accuracy of click prediction for new ads in a NGD advertising system. Specifically, we developed image and flash features that describe the visual perception of the content of display ads. We analyzed the click distributions for the different multimedia features and observed a strong correlation between the CTR and features defined. We fur-ther tested the models with multimedia features on large scale data collected from real traffic. Compared to a state-of-the-art baseline model without multimedia features, we observed significant improvement in terms of prediction ac-curacy on test data with new ads. We also developed a fea-ture selection algorithm to remove the redundant and highly correlated features, which was shown to improve the accu-racy of the final prediction task. We thank our colleagues Kannan Achan, Lihong Li, Olivier Chapelle and R  X  omer Rosales for their assistance with data collection and model discussions. [1] D. Agarwal, R. Agrawal, R. Khanna, and N. Kota. [2] J. Azimi, R. Zhang, Y. Zhou, V. Navalpakkam, [3] A. L. Berger and V. J. D. Pietra. A maximum entropy [4] G. Bradski and A. Kaehler. Learning OpenCV: [5] R. H. Byrd, J. Nocedal, R. B. Schnabel, R. H. B. J. [6] D. Chakrabarti, D. Agarwal, and V. Josifovski. [7] H. Cheng and E. Cant  X  u-Paz. Personalized click [8] D. Cohen-Or, O. Sorkine, R. Gal, T. Leyvand, and [9] T. Cour, F. Benezit, and J. Shi. Spectral segmentation [10] K. S. Dave and V. Varma. Learning the click-through [11] T. Graepel, J. Q. Candel, T. Borchert, and [12] D. Hasler and S. E. Suesstrunk. Measuring [13] E. Horster, R. Lienhart, and M. Slaney. Continuous [14] X. Hou and L. Zhang. Saliency detection: A spectral [15] K.-Q. Huang, Q. Wang, and Z.-Y. Wu. Natural color [16] J. Itten. The art of color. New York: Van Nostrand [17] J. Itten. Color design. Asakura Shoten , 1995. [18] G. H. Joblove and D. Greenberg. Color spaces for [19] N. Kalidindi, A. Le, J. Picone, H. Y. L. Zheng, and [20] C. Liu, H. Wang, S. Mcclean, J. Liu, and S. Wu. [21] D. G. Lowe. Object recognition from local [22] Y. Luo and X. Tang. Photo and video quality [23] C. Poynton. Yuv and luminance considered harmful: [24] Z. R. Srihari and A. Rao. Image background search: [25] L. G. Shapiro and G. C. Stockman. Computer Vision . [26] C. Y. Suen, M. Berthod, and S. Mori. Automatic [27] H. Tamura, S. Mori, and T. Yamawaki. Textural [28] R. van Zwol, A. Rae, and L. G. Pueyo. Prediction of [29] K. Weinberger, A. Dasgupta, J. Langford, A. Smola, [30] W. Xu, E. Manavoglu, and E. Cant  X  u-Paz. Temporal [31] H.-W. Yoo, D.-S. Jang, S.-H. Jung, J.-H. Park, and [32] M. Zaffalon and M. Hutter. Robust feature selection [33] X. Zhang, V. Ramani, Z. Long, Y. Zeng,
