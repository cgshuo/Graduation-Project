 Novel and diverse document ranking is an effective strategy that involves reducing redundancy in a ranked list to max-imize the amount of novel and relevant information avail-able to users. Evaluation for novelty and diversity typically involves an assessor judging each document for relevance against a set of pre-identified subtopics, which may be dis-ambiguations of the query, facets of an information need, or nuggets of information. Alternately, when expressing a pref-erence for document A or document B, users may implicitly take subtopics into account, but may also take into account other factors such as recency, readability, length, and so on, each of which may have more or less importance depend-ing on user. A user profile contains information about the extent to which each factor, including subtopic relevance, plays a role in the user X  X  preference for one document over another. A preference-based evaluation can then take this user profile information into account to better model utility to the space of users.

In this work, we propose an evaluation framework that not only can consider implicit factors but also handles dif-ferences in user preference due to varying underlying infor-mation need. Our proposed framework is based on the idea that a user scanning a ranked list from top to bottom and stopping at rank k gains some utility from every document that is relevant their information need. Thus, we model the expected utility of a ranked list by estimating the utility of a document at a given rank using preference judgments and define evaluation measures based on the same. We validate our framework by comparing it to existing measures such as  X  -nDCG, ERR-IA, and subtopic recall that require explicit subtopic judgments We show that our proposed measures correlate well with existing measures while having the poten-tial to capture various other factors when real data is used. We also show that the proposed measures can easily han-dle relevance assessments against multiple user profiles, and that they are robust to noisy and incomplete judgments. Categories and Subject Descriptors : H.3 [Informa-tion Storage and Retrieval] ; H.3.4 [Systems and Soft-ware] : Performance Evaluation Keywords: Novelty and Diversity, Evaluation
The concept of relevance is the probably the most critical aspect of theoretical and practical information retrieval (IR) models. But which documents are relevant can differ from user to user depending on their exact information need, even if they start with the same keyword query. Queries can be ambiguous and/or underspecified, and the retrieval systems are required to handle these diverse information needs while providing novel information. Traditional IR evaluation also works under the assumption that documents are indepen-dently relevant separate from any user context The major drawback with this approach is that it does not penalize re-dundancy in rankings, potentially reducing the amount of novel information available to the user. Recently a subtopic based approach was introduced, to handle the redundancy problem and account for diverse information needs. The underlying information need for a query is decomposed into set of subtopics, and the number of novel subtopics that a document is relevant to ( i.e. not seen earlier in the ranking) provides a measure of novelty. Various evaluation measures have been defined based on this approach [1, 9, 23, 27].
While subtopics are used to account for the diverse infor-mation needs of a query, the relation between them varies from user to user. For example, consider the query living in India . A person planning to visit India could be interested in information for visitors and immigrants &amp; how people live in India whereas a student writing an essay would be more interested in the history about life and culture in India . Even though all of these subtopics seem relevant to the query, the importance of a subtopic is dependent on the user and the scenario in which the search was performed. It is well-known that user preferences are influenced not only by topical rele-vance but also by other factors such as readability, subtopic importance, completeness, etc. User profiles can be used to represent the combination of relevant subtopics and the above mentioned factors that precisely reflects the user X  X  in-formation need. Currently, there is no evaluation measure that (a) takes into account various factors affecting user pref-erence, (b) handles multiple user profiles for a given query.
In this work, we propose an evaluation framework and metrics based on user preference for the novelty and diversity task. The framework revolves around the idea of assigning utility scores that reflect each set of user X  X  preference towa rds each document. The document utilities are estimated using a series of preference judgments collected conditional on pre-viously ranked documents. Document utility at a given rank implicitly accounts for the subtopic coverage, novelty, topi-cal relevance and the other factors as well. As pointed out earlier, the utility of document could differ for each user, thus user preference are obtained across a pool of users to account for diverse information need of a query. Evaluation metrics defined based on this framework directly models a user traversing a ranking from top to bottom seeking rele-vant and novel information for the issued query. Therefore, our proposed measures estimate the total utility of a ranked list available to the user for a given query.

The rest of the paper is organized as follows: a detail ex-planation of the existing evaluation framework and the exist-ing metrics for novelty and diversity is provided in Section 2. We point out issues with the current method and propose preference-based evaluation measures in Section 3. A de-scription of the datasets along with the experimental design employed in our work can be found in Section 4. We ana-lyze in detail the performance of our metrics and compare it to various existing ones in Section 5. Finally, Section 6 summarizes our findings and sketches our future directions.
Search result diversification is an effective strategy to deal with the diverse information needs of the user while reducing redundancy in the ranked list [19, 28, 25]. Several methods have been proposed to produce a ranking that maximizes rel-evance with respect to multiple information needs for a given query, starting with the maximum marginal relevance model of Carbonell et al. [4]. In addition to new models, the task demands new evaluation metrics, as traditional IR measures are focused on relevance with respect to a single user and do not penalize redundancy in results. Zhai et al. studied the subtopic retrieval task in the context of the TREC Interac-tive track [17], and defined simple evaluation measures such as subtopic recall and subtopic precision based on the rele-vance of documents to pre-defined subtopics. Clarke et al. proposed an evaluation strategy that decomposes underlying information needs of a query into information nuggets; doc-ument utility is determined by the number of novel nuggets covered by the document. NRBP, also introduced by Clarke et al. combines ideas from  X  -nDCG and Rank-Biased Pre-cision [12]. Agarwal et al. focused on the diversity problem in the web domain by taking into account the importance of user intents via a probability distribution. Each of these measures will be described in more detail below.

Almost all of the existing measures are based on the idea of explicit subtopics : decompositions of a given query into sev-eral pieces of information (such as facets, intents, or nuggets) that account for various underlying information needs. In this framework, novelty is solely dependent on the docu-ment X  X  relevance to a subtopic. System effectiveness is esti-mated by iterating over the ranked list, penalizing relevant documents relevant to subtopic(s) seen earlier in the rank-ing, and rewarding documents relevant to unseen subtopic(s). Test collections such as those produced for the TREC Interactive tracks [17] and the TREC Question Answering tracks [26] consist of subtopic-level judgments in documents. The TREC Web track diversity datasets created to study the problem of novelty and diversity are most suitable to our work. These datasets comprise a set of topics, and for each topic a set of subtopics that were identified semi-automatically with the help of a tool that clusters reformula-tions of the given query. The tool combined evidences from clicks and reformulations to obtain clusters of queries; the track organizers used these clusters to manually pick the set of subtopics for a given target query.

Binary judgments of relevance were made by NIST asses-sors for each subtopic to each document. Note that the use of this method means that only subtopics evidenced by a large number of users will be present in the data; interpre-tations that are equally  X  X eal X  yet less popular will not be represented when this method is used.
Evaluation measures for novelty and diversity must ac-count for both relevance and novelty in rankings. It is im-portant that redundancy caused by documents containing previously retrieved information are penalized while doc-uments containing novel information are rewarded; as de-scribed above, this is achieved using subtopic relevance judg-ments. A brief description of the commonly used metrics that employ a subtopic based approach is given below: Subtopic recall measures the proportion of unique subtopics retrieved at a given rank [27]. Given that a query q has m subtopics, the subtopic recall at rank k is given by the ratio of number of unique subtopics contained by the subset of document up to rank k to the total number of subtopics m .  X  -nDCG scores a result set by rewarding newly found subtopics and penalizing redundant subtopics [13]. Computation of the gain vector and a rank discount are key to  X  -nDCG. The gain vector is computed by summing over subtopics ap-pearing in the document at rank i : where c j,i is the number of times subtopic j has appeared in documents up to (and including) rank i .

The most commonly used discount function is log 2 (1 + i ), although other discount functions are possible. Summing gains over discounts gives discounted cumulative gain :  X  -DCG must be normalized to compare the scores against various topics. This is done by finding an  X  X deal X  rank-ing that maximizes  X  -DCG, which can be done using a greedy algorithm. The ideal ranking computation is an NP-Complete problem [5]. The ratio of  X  -DCG to that ideal gives  X  -nDCG.
 Intent-aware family Agrawal et al. [1] studied the prob-lem of evaluating ambiguous web queries. They proposed evaluating a ranking against each subtopic (or  X  X ntent X ) by any traditional IR measure, and then combining the re-s ults based on importance of subtopic. This gave rise to a family of measures that are known as intent-aware . Most traditional measures such as precision@ k , average precision (AP), nDCG, etc. can be cast as intent-aware versions; for instance, intent-aware AP would be expressed as: where m is the number of intents/subtopics, P ( i | q ) is the probability that the user is interested in intent i for query q , and AP i is average precision computed only with the docu-ments relevant to intent i .
 ERR-IA Expected Reciprocal Rank (ERR) is a measure based on  X  X iminishing returns X  for relevant documents [10]. According to this measure, the contribution of each docu-ment is based on the relevance of documents ranked above it. The discount function is therefore not just dependent on the rank but also on relevance of previously ranked documents. where R i is a function of the relevance grade of the doc-ument at rank i (typically defined to be (2 g  X  1) / 2 g m ERR-IA is defined exactly as other intent-aware measures: a weighted average of ERR computed separately for each subtopic/intent [9]. We mention it separately because it has some appealing mathematical properties and it is one of the official measures of the TREC Web track [9].
 D-Measure The D and the D# measures described by Sakai et al. [22] aim to combine two properties into a single evalu-ation measure. The first property is to retrieval documents covering as many intents as possible and second is to rank documents relevant to more popular intents higher than doc-uments relevant to less popular intents.
The subtopic-based evaluation framework focuses on es-timating the effectiveness of a system based on topical and sub-topical relevance. In practice, there may be many other factors such as reading level, presentation, completeness, etc. that influence user preferences for one document over an-other in the context of novelty and diversity [8]. We could describe the information needs of a user that consists of var-ious details, including specifics of pieces of information the user is interested in, reading level of the user, and so on in a user profile . Then we could view the goal of an evaluation measure as determining how well a ranking of documents satisfies a variety of user profiles.

In order to understand the concept of user profiles, let us consider an example query from the TREC Web track: air travel information . Table 1 shows the subtopics defined for the Web track X  X  diversity task and provides the information needs of three different possible users for the given query (assuming we restrict ourselves to the TREC paradigm and represent the user X  X  information need using only subtopics). We can think of user A as a first time air traveler looking for information on air travel tips and guidelines, user B as a journalist writing an article on the current quality of air travel and looking for statistics and reports to accomplish the task, and user C as an infrequent traveler looking restric-tions and rules for check-in and carry-on luggages. There-fore, user A  X  X  profile for the above example query consists of subtopics d and e , user B  X  X  of c , and user C  X  X  of a and b . (In practice, the profiles would typically take into account other factors such as presentation, readability, and other factors as well, but none of this need be made explicit.)
Even if we restrict ourselves to modeling only subtopics, there are some issues with existing measures based on subtopics: (a) subtopic identification is challenging and tricky as it is (b) measures often require many parameters to be set be-(c) measures assume subtopics to be independent of each Let us refer to Table 1 to consider these issues. First, given the granularity of these subtopics, it would not be difficult to come up with additional subtopics that are not in the data. Top-ranked results from a major search engine sug-gest subtopics such as  X  X re airports currently experiencing a high level of delays and cancellations? X ,  X  X  am disabled and require special consideration for air travel; help me find tips. X , and  X  X y children are flying alone, I am looking for tips on how to help them feel comfortable and safe. X  Are users with these needs going to be satisfied by a system that optimizes for the limited set provided?
Second, measures like  X  -nDCG and ERR-IA have a sub-stantial number of parameters that must be decided on. Some are explicit, such as  X  (the penalization for redun-dancy) [15] or P ( i | q ) (the probability of an intent/subtopic given a query 1 ). Others are implicit, hidden in plain sight because they have X  X tandard X  X ettings: the log discount of  X  -nDCG or the grade value R i of ERR-IA, for instance. Each of these parameters requires some value; it is all too easy to fall back on defaults even when they are not appropriate.
Third, some subtopics are clearly more related to each other than others (in fact, we used this similarity to create the profiles). Documents that are relevant to subtopic c are highly unlikely to also be relevant to any of the other subtopics, but it is more likely that there are pages relevant to both subtopics a and b .

In this work, we sidestep these issues by proposing an evaluation framework that simply allows users to express preferences between documents. Their preferences may be based on topical or subtopic relevance, but they may also be based on any other factors that are important to them. Preferences can be obtained over many users to capture the varying importance of topics and factors, and when a suffi-ciently large set of preferences has been obtained, systems can be evaluated according to how well they satisfy those users. Preference judgments have only scantly been used in IR evaluation, having been introduced by Rorvig [20] but not subject to empirical study until recently [7, 2]. Com-parison studies between absolute and preference judgments show that preference judgments can often be made faster than graded judgments, with better agreement between as-sessors (and more consistency with individual assessors) [7] while making much finer distinctions between documents.
T he original definition of  X  -nDCG has parameters for subtopic weights as well.
Chandar and Carterette [8] introduced a preference-based framework similar to ours, but there exists no evaluation measure that incorporates preference judgments directly for novelty and diversity. Moreover, that work focused only on ranking novel documents, without considering the more general question of diversity X  X hat different users will have different preferences depending on their profile.
Chandar and Carterette X  X  preference-based framework is based on so-called levels of preference judgments. We use a similar idea; in this work, a test collection of preferences for novelty and diversity consists of two different types of preference judgments: 1. simple pairwise preference judgments, in which a user 2. conditional preference judgments, in which a user is
Simple pairwise preferences produce a relevance ranking: given a pair of documents, assessors select the preferred doc-ument based on some criteria. We expect topical relevance to be the primary criteria, although many criteria (such as ease of reading, completeness of information, salience of arti-cle, etc.) could factor into an assessor X  X  choice. Since differ-ent users may have different needs and different preferences for the same query, pairs can be shown to multiple assessors to get multiple preferences. Over a large space of assessors, we would expect that documents are preferred proportion-ally according to the relative importance of the subtopics they are relevant to, with various other factors influencing finer-grained orderings.

Simple pairwise preferences cannot capture novelty; in fact, two identical documents should be equally preferred in all pairs in which they appear and therefore end up tied in the final ordering. Conditional preference judgments at-tempt to resolve this by asking for a preference for a given pair of document conditional on the information in other documents shown to the assessor at the same time. The as-sessor is asked to read those documents, then select which of the remaining two they would like to see next .
Figure 1 illustrates conditional preferences with a triplet of documents: the assessor would read document X , then select which of A or B they would like to see next 2 We
N ote that any document may be placed at the top of a triplet; it need not be the most preferred document among the simple pairwise preferences.
 Figure 1: Left: a simple pairwise preference for w hich an assessor chooses A or B . Right: a triplet of documents for conditional preference judgments. An assessor would be asked to choose A or B condi-tional on having read X . expect the assessor X  X  choice to be based not only on topical relevance, but also on the amount of new information given what is provided in the top document. Again, they can use other factors in their preferences, but novelty should be a primary consideration: if X is identical to A , we expect them to choose B , and then a system that ranks X and A adjacent would be penalized for failing to rank B after X .
Similarly, we could obtain preferences with quadruplets of documents, quintuplets of documents, and so on. In practice it becomes increasingly difficult for assessors to make such fine distinctions, so we limit to only obtaining judgments on triplets. A triplet in our framework corresponds to Chandar and Carterette X  X   X  X evel 2 X  judgments; as they showed, these judgments capture most of the necessary information about novelty. Preferences conditional on greater numbers of other documents contribute less and less [8].
We propose a model-based measure using preferences to assess the effectiveness of systems for the novelty and di-versity task. Model based measures can be composed from three underlying models: browsing model , document utility , and utility accumulation [6]. The way users interact with the ranked list is defined by the browsing model; we rely on the most accepted model in which the user scans docu-ments down a ranked list one-by-one and stops at some rank k . The document utility model defines the amount of util-ity provided by a single document, and utility accumulation models the total utility derived during browsing.
We define our utility based model for novel and diversity ranking task as follows: a user scanning documents down a ranked list derives some utility U ( d ) from each document and stops at some rank k . We hypothesize that the utility of a document at rank i is dependent on previously ranked document ( i.e. d 1 to d i  X  1 ). Given a probability distribution for a user stoping at rank k , the utility accumulation model can be defined as: where P ( k ) is the probability that a user stops at rank k and U ( d 1 , ..., d k ) is the total utility of the documents from ranks 1 through k .

We simplify this by formulating U ( d 1 , ..., d k ) as a sum of individual document utilities conditional on documents ranked before: where P ( k ) is the probability that a user stops at rank k , U ( d i | S ) gives the utility of the document at rank i condi-tional on a set of previously ranked document S , and the sum from i = 1 to k gives the total utility of all documents from ranks 1 through k . There are two main components in the above equation: the probability that a user stops at a given rank ( P ( k )) and the utility of a document conditioned of previously ranked documents ( U ( d i | S )). Carterette demon-strated different ways to model the stopping rank from the various ad-hoc measure such as Rank Biased Precision [16], nDCG, and Reciprocal Rank [6]. 1. P RBP ( k ) = (1  X   X  ) k  X  1  X 
Finally, we define the document utility model in which the document utility at a given rank is conditioned on previously ranked documents. The utility of the document at rank i is given by U ( d i ) for i = 1 since at rank 1 the user would not have seen any other documents and therefore would not be conditioning on any other documents. For subsequent depends on documents already viewed.

Now our goal is to estimate these utilities using prefer-ence judgments. Since we have simple pairwise preferences and conditional preferences in triplets, we decompose the document utility model as follows: where the function F () takes an array of conditional utilities ( U ( d i | d j )).

The utility U ( d i ) can be directly obtained using the pair-wise judgments; we simply compute it as the ratio of number of times a document was preferred to the number of times it appeared in a pair. The utilities U ( d i | d i  X  1 ) can similarly be obtained from the conditional preferences, computed as the ratio of the number of times d i was preferred conditional on d i  X  1 appearing as the  X  X iven X  document to the number of times it appear with d i  X  1 as the  X  X iven X  document. Note that these utilities can be computed regardless of how many times a document has been seen, how many different asses-sors have seen it, how much disagreement there is between assessors, and so on. Although, a document must be shown at least few time in order to determine its relevance esti-mate. An estimate of the document X  X  utility is obtain using the ratio of number of times the document was preferred to the number of time it was shown.

We experiment with two functions for F (): average and minimum . The intuition behind these functions can be ex-plained with the help of an example. Consider a ranking R = { d 1 , d 2 , d 3 } . According to equation 8 the utility of d depends on U ( d 3 | d 1 ) and U ( d 3 | d 2 ). The minimum function assumes that d 3 cannot be any more useful conditional on both d 1 and d 2 than it is on either one separately, thus giving a sort of worst-case scenario. The average function assumes that the utility of d 3 conditional on both d 1 and d 2 is some-where in between its utility conditioned on each separately, giving d 3 some benefit of the doubt that it may contribute something more when appearing after both d 1 and d 2 than it does when appearing after either one on its own.
Our measure as defined is computed over the entire ranked list. In practice, measures are often computed only to rank 5, 10, or 20 (partially because relevance judgments may not be available deeper than that). When we compute the mea-sure to a shallower depth, we must normalize it so that it will average over a set of queries. As a final step in the com-putation of nP rf , we normalize equation 7 cut off at rank K by the ideal utility score. where I -P rf [ K ] is the ideal utility score that could be ob-tained at rank K . This can be obtained by selecting the document with the highest utility value conditioned on pre-viously ranked documents. Document ( d 1 ) with the highest utility value takes rank 1 and the document with highest utility when conditioned on d 1 takes rank 2 and so on.
Table 2 provides an example showing the distinction be-tween our preference based measure and  X  -nDCG based on the user profiles in Table 1. The document utilities are es-timated by obtaining the preference judgements for all doc-uments from all three users. We would expect the users X  preferences to be consistent with their information need, for example user A would prefer d 1 and d 2 consistently to other documents that are not relevant to their needs (but relevant to other needs). Notice that  X  -nDCG weighs all subtopics equally but the preference measure takes into account the dependency between the subtopics.
In Section 3.2, we proposed various evaluation measures based on a user model for novelty and diversity. Evalua-tion of the proposed metrics is challenging since there is no ground truth to compare to; there are only other measures. Approaches used in the past to validate newly introduced metrics include comparing the proposed measure to exist-ing measures or click metrics [18, 11]; using user preferences to compare the metrics [24]; and evaluating the metric on various properties such as discriminative power [21]. While each of these approaches have their own advantages, we ar-gue that comparison of existing measures to our measures using simulated data is suitable for this work.

Remember, our goal is to build evaluation measures for our preference based framework that assigns utility scores to a document based on user preferences. In reality, user preferences are based on various implicit factors that include metrics are able to distinguish the difference. subtopic relevance as well as many other properties. Since prior work [8] has suggested that presence of subtopics in a document plays a major role in user preferences, we believe it is important to validate our measures when user preferences are based solely on subtopic information. We therefore rely on the existing data with subtopic information to simulate user preferences.
In our experiments, we used the ClueWeb09 dataset 3 con-sisting of one billion web pages (5 TB compressed, 25 TB uncompressed), in ten languages, crawled in January and February 2009. A subset of this collection with only En-glish documents was used for the diversity task at TREC in 2009/10/11 [14]. A total of 150 queries have been devel-oped and judged for the TREC Web track; the number of subtopics for each ranges from 3 to 8. For the diversity task, subtopic level judgments are available for each subtopic in-dicating the relevance of a document to each subtopic along with the general topical relevance. We also acquired the experimental runs submitted to TREC each year by Web track participants. A total of 48 systems were submitted by 18 groups in 2009, 32 system by 12 groups in 2010, and 62 systems by 16 groups in 2011.
In order to verify and compare our metrics against ex-isting measures, we acquire preferences by simulating them from subtopic relevance information. These will be based on the preferences of simulated users that are modeled by groupings of subtopics (as in Table 1). In this way we use only data that is provided as part of the TREC collection, and therefore achieve the fairest and most reproducible pos-sible comparison between evaluation measures. In reality, our measure is well-suited for crowd-sourced assessments in a way that other measures are not, but we save that exper-iment for future work.

We created our user profiles by generating search scenar-ios for each query and marking subtopics relevant to the scenario. In Section 3, we explained our reasoning behind the user profiles in Table 1 for the query air travel informa-tion ; we use the same approach to obtain the user profiles for all TREC queries. The user profiles were created by the authors of this paper and have been made available for public download at http://ir.cis.udel.edu/~ravichan/ data/profiles.tar . In addition, there is a mega-user that we refer to as the  X  X REC profile X ; this user is equally inter-ested in all subtopics. h ttp://lemurproject.org/clueweb09.php
These profiles are used to determine the outcome of pref-erences. For simple pairwise preferences, we always prefer the document with greater number of subtopics relevant to the user profile. In the case of a tie, we make a random choice between the left or right document. For conditional preferences, we have three documents (left, right, and top); between the left and the right, we prefer the document that contains the greater number of subtopics relevant to the user profile and not present in the top document. Prefer-ence judgments obtained this way are used to compute our preference measure. Finally, using the  X  X REC profile X  to simulate preferences for our measure offers the most direct comparison to other measures.
We have presented a family of preference-based measures for evaluating systems based on novelty and diversity, and outlined the advantages of our metrics over existing subtopic-based measures. In this section, we demonstrate how our metrics take into account the presence of subtopics implic-itly by comparing them with  X  -nDCG, ERR-IA, and s-recall.
We evaluated all experimental runs submitted to TREC in 2009, 2010, and 2011 using our proposed measure with three different stopping probabilities P ( k ) and two different utility aggregation functions F (). Figure 2 shows the performance of systems with respect to both  X  -nDCG and our preference measure computed with P RBP ( k ) and F avg () functions and preferences simulated using the  X  X REC profile X . Each point represents a TREC participant system; they are ordered on the x-axis by  X  -nDCG. Black circles give  X  -nDCG values as computed by the ndeval utility used for the Web track; blue x X  X  indicate the preference measure score for the same sys-tem. In these figures we can see that the preference measure is roughly on the same scale as  X  -nDCG, though typically 0 . 1  X  0 . 2 lower in an absolute sense.

Each increase or drop in the position of x X  X  indicates dis-agreement with  X  -nDCG. The increasing trend of the curves in Figure 2 indicates that the correlation between the pref-erence measure and  X  -nDCG is high. A similar trend was observed while using different P ( k ) and F () functions as well (not shown). Both  X  -nDCG and our preference mea-sure agree on the top ranked system in 2009 and 2010.
We analyzed the reason behind disagreement by carefully looking at the actual ranked lists. We investigated how  X  -nDCG and our proposed measures reward diversified sys-T able 3: Kendall X  X   X  correlation values between the existing evaluation measures. Values were computed using 48 submitted runs in TREC 2009 dataset. tems on a per topic basis. Based on our analysis, the ma-jor reason for disagreement is that  X  -nDCG penalizes sys-tems that miss documents containing many unique subtopics more harshly than the preference measure does. Much of the variance in  X  -nDCG scores is due to differences in rank po-sition of the documents with the greatest number of unique subtopics. In practice, this explains the lower scores re-turned by the preference measure as well.
We measure the stability of our metrics using Kendall X  X   X  by ranking the experimental runs under different effective-ness measures. Kendall X  X   X  ranges from -1 (lists are reversed) to 1 (lists are exactly the same), with 0 indicating essentially a random reordering. Prior work suggest that a  X  value of 0.9 or higher between a pair of rankings indicates high simi-larity between rankings while a value of 0.8 or lower indicates significant difference [3].

Figure 3 summarizes the rank correlations between ex-isting subtopic-based metrics and our proposed preference metric using all three P ( k ) (plus using no P ( k ) at all X  equivalent to a uniform stopping probability) and both F () functions, simulating preferences with the  X  X REC profile X . The correlations are fairly high across TREC datasets, P ( k ) functions, and F () functions. The P DCG ( k ) rank function fares worst, with correlations dipping quite a bit for the 2010 data in particular. Subtopic recall is a very simple non-rank based metric for diversity and thus the Kendall X  X   X  values are expected to be slightly lower.

For comparison, Table 3 shows the Kendall X  X   X  correlation values between  X  -nDCG, ERR-IA and s-recall. These corre-lations are similar to those in Figure 3, suggesting that the ranking of systems given by our preference measure varies no more than the rankings of systems given by any two stan-dard measures.
 There is almost no difference between the correlations for F avg () and F min () functions for aggregating utility. In fact, the correlation between preference measures computed with those two is nearly 1. Thus we can conclude that the choice of F () (between those two options) does not matter. There is a great deal of difference depending on choice of P ( k ), however, and thus this is a decision that should be made carefully based on the observed behavior of users.
The experiments above are based on the  X  X REC profile X , a user profile that considers every subtopic to be equally relevant. In this experiment, we demonstrate the ability of our methods to handle multiple, more realistic user profiles and show the stability of our metrics. Measures based on absolute subtopic judgments cannot naturally incorporate multiply-judged documents. One must average judgments, or take a majority vote, or use some other scheme. In con-trast, judgments from multiple users can be incorporated easily into our preference framework in the estimation of document utilities, as the document utility is simply the ratio of number of times a document was preferred to the number of times it appeared in a pair, regardless of which user or assessor happened to see it.

We simulate preferences for each of our user profiles for each topic in the TREC set. We compute the preference measure using each profile X  X  preferences separately (giving at least three separate values for each system: one for each user profile), and then use the full set of preferences ob-tained to compute a single value of the measure. Note that the latter case is not the same as computing the preference measure with the  X  X REC profile X : the TREC profile user uses all subtopics to determine the outcome of a preference, while individual users would never use a subtopic that is not relevant to them to determine the outcome of a preference.
We can also compute subtopic-based measures such as  X  -nDCG against our profiles. To do this, we simply as-sume that only the subtopics that are relevant to the profile  X  X ount X  X n the measure computation. We will compare values of measures computed this way to our preference measures.
Our hypothesis for this experiment is twofold: 1) that the preference measure computed for a single profile will correlate well to subtopic-based measures computed against the same profile; 2) that the preference measure computed with preferences from all profiles will not be the same as an average of the individual profile measures, and also not the same as subtopic-based measures computed as usual. In other words, that the preference measure based on pref-erences from many different users is measuring something different than the preference measure based on preferences from one user, and also different from the subtopic measures.
Figure 4 shows the results of evaluating systems using user profile 1, 2, and 3 for each topic and averaging over topics (note that the user profile number is arbitrary; there is nothing connecting user profile 1 for topic 100 to user profile 1 for topic 110). We can see that the system ranking changes for both  X  -nDCG and the preference measure, as expected. The correlation between the two remains high: 0 . 83 , 0 . 88, and 0 . 82 for user profile 1, 2, and 3 respectively. This is in the same range of correlation values that we saw in Figure 3, and supports the first part of our hypothesis.
Figure 5 shows the results of evaluating systems with all user profiles, comparing to the evaluation with the TREC profile and with  X  -nDCG computed with all subtopics. Note here that all three rankings are different, as evidenced by the  X  correlations reported in the inset tables. This supports the second part of our hypothesis: that allowing many different users the opportunity to express their preferences can result in a different ranking of systems than treating all assessors as equivalent, as the TREC profile and  X  -nDCG do.
The test collection procedure discussed in Section 3.1 re-quires two sets of judgments: pairwise and conditional pref-erences. The number of pairwise judgments increases quadrat-ically with increase in number of documents in the pool; it is not feasible to collect a complete set of preferences. We envision that our measure would always be computed with incomplete judgments. For this experiment we test the sta-bility of our measures by comparing the system rankings obtained by using all preference judgments against a set of incomplete judgments. with P RBP and F Average . Compare to  X  -nDCG scores. P ( k ) and F() are shown.

To do this, we randomly select N triplets of documents for each query. For each triplet, one document is randomly selected to be the  X  X op X  document that the other two would be judged conditional on. Though we do not explicitly ob-tain simple pairwise preferences, we expect that there will be enough cases in which the top document is not relevant to the user profile that they must fall back on a simple pairwise comparison. We then sample 5 user profiles (with replace-ment) from those defined for the topic and simulate their preferences for the triplet. In this way we obtain 5 N prefer-substantial differences as well. Figure 4: Comparison between  X  -nDCG and our preference measure computed against user profiles 1 (top), 2 (middle), and 3 (bottom) for TREC 2009 systems. ences for each topic in a similar way as would be done in a real crowd-sourced assessment. We use those preferences to compute our measure, then compute the correlation to the measure computed with all available preferences. We repeat this 10 times for each topic, measure the correlation each time, and average the correlations.

Figure 6 shows the correlation between the system rank-ings when evaluated using complete judgements and increas-ing numbers of preferences. Correlation tends to increase as the number of preferences increases, though it does not reach 0.9. This may be partly because user profiles are not evenly represented in the preferences (which is in fact more realis-tic than when they are, as in the full-preference case), and Figure 6: TREC 09/10/11 diversity runs evalu-a ted with our preference based metric at rank 20 (nPrf@20) with P RR and F Minimum using single asses-sor with complete judgments and multiple assessor with incomplete judgments. partly because our max number of preferences is still a fairly small fraction of the total number possible: even selecting triplets from only 100 documents, there are over 161,000 possible triplets, of which we have only obtained less than 5%! Thus we expect that continuing to increase the number of triplets would continue to push the correlations higher, even though we see dips in the trend (due to variance).
In this work, we proposed a novel evaluation framework and a family of measures for IR evaluation. Our measure incorporates novelty and diversity, but can also incorporate any property that influences user preferences for one docu-ment over another. Our measure is motivated directly by a user model and has several advantage over the existing m easures based on explicit subtopic judgments: it captures subtopics implicitly and at finer-grained levels, it accounts for subtopic importance and dependence as expressed by user preferences, and it requires few parameters X  X nly a stopping probability function, for which there are several well-accepted options that can be chosen from by compar-ing to user log data. It correlates well with existing mea-sures, but also clearly measures something different (which is a positive for a new measure).

This framework and measure is most well-suited for as-sessments done by crowd-sourcing. In a crowd-sourced as-sessment, we would naturally have a large user base with a wide range of preferences. Over a large number of pref-erences, the most important subtopics and intents would naturally emerge; documents relevant to those would be-come the documents with the highest utility scores. Yet the conditional judgments would prevent too many documents with those subtopics from reaching the top of the ranking. The measure is designed to handle multiple judgments, dis-agreements in preferences, and novelty of information, and as such it is novel to the information retrieval literature.
The clearest direction for future work is to perform an actual crowd-sourced assessment and determine whether our preference measure correlates better with human judgments of system performance than other measures. We plan to start this immediately. Another direction for future work is using triplets in a learning-to-rank algorithm to learn a novelty ranker. Since many learning algorithms are based on pairwise preferences, it seems a natural extension to triplets. Acknowledgments: This work was supported in part by the National Science Foundation (NSF) under grant num-ber IIS-1017026. Any opinions, findings and conclusions or recommendations expressed in this material are the authors X  and do not necessarily reflect those of the sponsor.
