
Department of Computer and Systems Sciences, Stockholm University, Kista, Sweden
AstraZeneca Research and Development, S X dert X lje, Sweden
Department of Pharmacy, Uppsala University, Uppsala, Sweden Department of Computational Chemistry, H. Lundbeck A/S, Valby, Denmark 1. Introduction
Modeling absorption, distribution, metabolism, excretion and toxicity (ADMET) using predictive models is important for the pharmaceutical industry in order to decrease development time and costs of new drugs [16]. The term quantitative structure-activity relationship (QSAR) is used to denote a rela-tionship between the chemical structure of compounds and their effects, such as solubility, permeability, protein binding, mutagenicity, carcinogenicity, metabolic stability and so on. Understanding such rela-tionships are of high importance within the drug discovery process, and screening for potential issues using computers ( in silico ) early on in the drug discovery process helps researchers to decide whether or not the compounds should be synthesized and tested in laboratories ( in vitro) , something which is both costly and time-consuming. QSAR models that accurately help avoiding synthesizing unsuccessful leads without losing truly successful leads are hence crucial [42].

Broadly speaking, the generation of a QSAR model comprises two parts [19]; (1) extracting rele-vant structural information of compounds, and (2) building the predictive model using the structural information encoded as attributes. The structural information can be represented by various structure-based theoretical estimates of properties of compounds, which are commonly referred to as (chemical) descriptors. These descriptors are twofold; (experimentally derived) properties-based and (topological) structure-based. The former include global properties of molecules, such as their weight, number of atoms and bonds, various atomic measures, number of p re-identified substructures, e.g., carbons rings, and so forth. Several such descriptor sets have been developed within the field of chemoinformatics, capturing various properties of molecules [32]. The SELMA descriptor set [41] is an example of such a collection of descriptors. Descriptors that represent the topological structure of a molecule usually transform the 2D or 3D structure of the molecule into a set of single-valued descriptors, e.g., ECFI [7], characterizing structures with respect to their size, degree of branching, overall shape, etc. Some such descriptors are derived from 2D representations of molecules in the form of strings, such as SMILES and SMARTS [9]. Further details of this type of domain-specific representations of chemical compounds, which in this study are referred to as chemoinformatics-based descriptors , can be found in [34].
Chemical compounds can be represented by graphs, where the vertices of a graph represent atoms of a compound and the edges corresponds to bonds. This allows graph mining methods to be used for discovering descriptors (or molecular fragments) in the form of subgraphs. Unlike the chemoinformatics based descriptors, the graph mining methods discover descriptors that are local to the dataset, and there-fore different datasets may result in different numbers of descriptors. Mining subgraphs from chemical databases is gaining increased attention, since the resulting molecular fragments allow for interpreta-tion and hence may provide insights into what parts of the molecules cause certain specific behaviors. Graph mining methods discover such subgraphs by using various topological features of the graphs of molecules, such as frequent subgraphs and their representative subsets [2,18,26,35,40,43], interesting subgraphs [13,27,29 X 31], significant subgraphs [37], using methods such as sampling [44], pattern sum-marization [24], iterative feature selection [15] and so forth. Such approaches to descriptor discovery, which in contrast to the chemoinformatics-based approaches, are purely data-driven. These are in this study referred to as substructure-based approaches .

Chemoinformatics-based and substructure-based QSAR modeling methods have been developed quite independently, each type resulting in reportedly powerful QSAR models. However, comparisons be-tween the two types of models are rarely seen. Such evaluations could provide insights into the relative quality of the descriptors generated by the two different strategies. A few such comparisons can be found in early studies of inductive logic programming methods, e.g., [33]. However, these studies have only concerned a few datasets, hence not allowing conclusions to be drawn regarding what relative perfor-mance can be expected in general. Other studies, such as [25], use primitive descriptors that are far from state-of-the-art when comparing to substructure-based methods. This paper aims to bridge this gap by comparing the performance of state-of-the-art chemoinformatics-based and substructure-based QSAR models. We aim to seek answers to two questions;
The rest of the paper is organized as follows. The next section gives a brief introduction to the em-ployed chemoinformatics-based and substructure-based methods. In Section 3, the setup of the empirical investigation is presented. Section 4 reports the outcome of the experiments. Finally, in Section 5, the main conclusions of the study are summarized and directions for future work are outlined. 2. Generation of descriptors for QSAR modeling
In this study, we consider seven approaches to generate descriptors for QSAR modeling, two of which can be considered state-of-the-art in the field of chemoinformatics, namely ECFI [7], which is based on the (topological) structure of the molecules, and SELMA [41], which is based on (experimentally derived) properties of molecules. The other five methods discover substructures using graph and itemset mining algorithms. The m ethods MoFa (frequent molecular fragments miner) [2], SUBDUE (interesting subgraphs discovery) [30] and GraphSig (Mining Significant graphs) [37] discover subgraphs using various measures, which are described below. MFI (mining maximal frequent itemsets) [39], and CP (mining significant itemsets using constraint programming) [36] are two methods that are commonly employed for finding either frequent or significant subsets of itemsets. By transforming the graph mining task into an itemset mining task, the latter methods can also be employed in conjunction with graph data. 2.1. Chemoinformatics-based approaches
As described in the previous section, the chemoinformatics based methods use a pre-defined set of descriptors for building QSAR models. A molecular descriptor, as defined in [34], is the final result of a logical and mathematical procedure that transforms chemical information encoded within a sym-bolic representation of a molecule into a useful number or the result of some standardized experiment. Therefore, defining molecular descriptors involve interdisciplinary theories from algebra, graph theory, physical and organic chemistry, information theory, computational chemistry, etc, at different levels and domain expert X  X  knowledge on the molecules in general. However, the performance of different descrip-tor sets depends on how strongly the properties used for discovering the descriptors correlate with the target variables. The strengths and potentials of various descriptors have been discussed in detail in [34]. Common for these methods is that the values of the descriptors are computed from compound datasets in some standard format, e.g., the SD format [1] or SMILES strings [9]. The two descriptor sets we have used in this study, ECFI and SELMA, which are described below, employ SMILES strings when computing the descriptors.

ECFI  X  The Extended Connectivity Fingerprints [7] represent structural fragments of various sizes and they encode 2D and 3D features of a molecule as an array of binary values or counts. These struc-tural fragments are computed using an iterative updating procedure on the non-hydrogen atoms of the structure, which is based on the number of atoms in the neighborhood, atomic number, attached hydro-gen count, etc. In the initial assignment stage , each non-hydrogen atom of the structure is assigned an integer identifier. During each iteration, larger and larger circular neighborhoods (of atoms) around each atom are covered and the respective identifiers are updated. After removing the duplicate identifiers ,the remaining set of identifiers are collected into a list. This integer list could be used directly for model building or converted into a fixed length vector containing binary values. A binary vector of length 1024 of ECFI descriptors are used in this study.
SELMA  X  The SELMA descriptors [41] is a collection of commonly used 2D molecular descriptors related to molecular size, flexibility, ring structure, connectivity, polarity, charge, lipophilicity and hy-drogen bonding. This collection includes 94 such descriptors. 2.2. Substructure-based approaches
The substructure-based methods use graph theory and other mathematical concepts in discovering useful molecular fragments from compounds sets when considering the 2D structure of a molecule as a graph, where the atoms of the molecule are represented by the vertices of the graph and the bonds are represented by (undirected) edges. In this general framework, a graph G is defined as a quintuple ( V, E,  X ,  X  ) ,where V is the set of vertices, E  X  ( V  X  V ) is the set of edges and  X  : V  X  L v and of possible labels for vertices and edges, respectively. Further, two vertices u  X  V and v  X  V , connected by an edge e , is denoted as e =( u, v ) ,if ( u, v )  X  E . For undirected graphs,  X  (( u, v ))=  X  (( v,u )) . A subgraph G s of G , denoted by G s  X  G ,isagraph G s =( V s ,E s , X  s , X  s ) ,where V s  X  V,E s  X  isomorphic to G , if there exists an injective mapping f : V s  X  V such that,  X  u  X  V s ,  X  s ( u )=  X  ( f ( u )) say G s occurs in G if G s is subgraph isomorphic to G . Let the database D contain a collection of graphs G , then, the support of a subgraph G s in D is the number of graphs G  X  D to which G s is subgraph isomorphic. If the support exceeds a pre-defined value, G s is a frequent subgraph in D .

A graph may be transformed into a lexicographically ordered list of edges, edge list , in order to allow for applying itemset mining algorithms. An edge list L of a graph G =( V, E,  X ,  X  ) is defined as L = ((  X  ( u ) , X  ( v ) ,e ) ,where u, v  X  V,e =  X  (( u, v )) . Within this framework, a graph mining problem can be viewed as a frequent itemset mining problem such that; L is the set of items and X  X  L is an itemset. Let the transaction database D be a multiset of subsets of L .Foritemset X , a transaction including X is an occurrence of X and the support(X) is the occurrences of X in the transaction database. The problem of frequent item set mining is: given a minimum support , determine all item sets X such that support(X) minimum support . Further, a frequent itemset X which is included in no other frequent itemset is a maximal frequent itemset .

The transformation of a graph in to its corresponding edge list loses the structure of the graph up to a certain extent, due to the fact that labels may be mapped to identical elements in the edge list, which prevents reconstructing the graph from the list. Nevertheless, if the labeling function of vertices,  X  ,is injective (i.e., there are no repeated vertex labels), the graph structure is preserved. This property is illustrated using the two structures in Fig. 1. The graph in Fig. 1(left) represents a benzene ring that could be found in a compounds database. The corresponding edge list of this graph is (assuming that edges corresponding to single-bonds are labeled by 1 while edges corresponding to double-bonds are ( C,H,1 ), ( C,H,1 ), ( C,H,1 )). The  X  function in this graph is not injective due to the repetition of vertex labels. Therefore, the structure of the graph is not guaranteed to be exactly determined by the edge list L (i.e., the reverse transformation from an edge list into a graph is not possible). In contrast, the graph in Fig. 1(right) has unique vertex labels (  X  is injective) and the structure of the graph could therefore be determined by its corresponding edge list L = (( A,B,1 ), ( A,F,1 ), ( B,C,1 ), ( C,D,1 ), ( D,E,1 ), ( E,F,1 )).
A brief description of the substructure-based methods that have been considered in this study are given below.

GraphSig [37] mines significant patterns from large databases of graphs. The significance of the graphs discovered by GraphSig is tested using a probability measure (p-value) related to the support of each graph in the graph database. If this p-value is below a user defined threshold, the graph is considered significant. The mining process includes clustering the dataset using domain knowledge (class labels) and finding frequent sub-graphs in those clusters. Graphs are converted into a feature space and signifi-cant graphs are discovered from the feature space. The conversion of the graphs into feature space loses the structural information of the graphs, but avoids generating large sets of random graphs and calculat-ing the frequency of the query graph. The clustering approach allows GraphSig to successfully overcome the scalability issue of discovering subgraphs with low frequencies, which is a common problem for the frequent graph mining methods [37].

Molecular Fragment miner (MoFa) [2] is an algorithm that can be used for discovering frequent molec-ular fragments in chemoinformatics databases. Although being domain-specific, the approach is here grouped with the other graph mining approaches, since it uses data mining techniques for fragment search and support calculation. Application of this method is confined to the chemical domains since it considers ring structures in the molecules as single units and uses  X  X ildcard atoms X  (atom types that are chemically equivalent), during the search of significant fragments. The MoFa algorithm searches for arbitrarily connected sub-graphs, avoiding frequent embeddings of previously discovered sub-graphs by using a specific search strategy. The algorithm maintains parallel embeddings of a fragment into all molecules throughout the growth process and exploits a local order of the atoms and bonds of a fragment to effectively prune the search, which allows for a restricted depth-first search, similar to the Eclat [26] association rule mining algorithm. MoFa selects substructures that have a certain minimum support in a given set of molecules, i.e., they are part of at least a certain percentage of the molecules. However, in order to restrict the search space, the algorithm considers only connected substructures, i.e., subgraphs for which all vertices are (directly or indirectly) connected by edges.

SUBDUE [30] is a graph-based knowledge discovery system that finds structural and relational pat-terns in data representing entities and relationships. SUBDUE represents relational data using labeled, directed graphs, where the entities are represented by labeled vertices or subgraphs, and relationships are represented by labeled edges between the entities. It uses the minimum description length (MDL) principle to measure the interestingness of the subgraphs discovered. SUBDUE employs a step-by-step procedure, which starts from single vertices and performs a computationally constrained beam search in order to expand the considered subgraphs by other vertices or edges. It aims for generating small sets of subgraphs that optimally compress the dataset.

Maximal Frequent Itemset (MFI) [39] is an approach that uses maximal frequent itemset mining meth-ods for discovering frequent subgraphs (which are not necessarily connected) from graph databases. The MFI algorithm requires that graph data are transformed into edge lists as described above. The MAFIA algorithm [5] is used on the edge lists to discover the maximal frequent itemsets. MAFIA computes frequent itemsets using a simple depth-first search strategy over the lexicographic tree and a dynamic ordering on the candidate list in order to remove or retain candidates, along with an Apriori based stop-ping condition. Maximality of the discovered frequent itemsets is guaranteed by enumeration of super-sets. The input data to the MAFIA algorithm should contain sets of items that do not contain duplicate elements. However, edge lists of graphs of molecules often contain repeated items, since the vertex label-ing function employed in the chemical compounds domain is not injective. Therefore, we have slightly modified the MAFIA algorithm in order to to handle duplicate items. Also, a lexicographical order is maintained within the vertex labels  X  ( u ) and  X  ( v ) in elements of L as well, since the compound graphs are equivalent. For example, the benzene ring in Fig. 1(left) has an edge list L = (( C,C,1 ), ( C,C,1 ), icographical order within the elements of L prevents including elements such as ( H,C,1 ). Further, the modification to the MAFIA algorithm consider L as it is, i.e., with duplicate elements, which otherwise would be reduced to the itemset {( C,C,1 ), ( C,C,2 ), ( C,H,1 )}.

Constraint programming based Itemset Mining (CP) [36], uses a correlation measure to discover sig-nificant patterns. A rescaling of the ROC space, which is called the bounded PN space, is used together with two constraints, namely, coverage and support, to find correlated patterns. This approach differs from the traditional branch and bound correlated itemset miners in terms of propagation through cover-age and support, and thereby discovering the number of transactions that exceed a pre-defined support with respect to the class label. In tailoring this method for graph data, we have applied a similar modifi-cation to the method as for MFI concerning the distinction between identical items in the edge list.
Two types of graph can be found in graph data repositories. One concerns datasets with very large graphs with (almost) unique vertex labels, such as web data and social networks. These graphs contain a large number of vertices (and vertex labels), which as a result constructs a huge search tree during sub-graph search. The graph mining methods tackle these types of graphs requires techniques for efficient traversing and pruning of the search tree. Nevertheless, determining whether or not the labels and the structure (or part of the structure) of two graphs are identical does not involve the NP-complete subgraph isomorphism test [13], due to the fact that these graphs can be transformed into a canonical form (usually a string), that makes the same computation less costly [6]. In contrast, small graphs, such as molecules (which are considered in this study), which contain repeated labels, require the subgraph isomorphism test [13] or a suitable transformation to avoid the subgraph isomorphism test during the subgraph discov-ery and support count. This type of graph typically require the search tree to be grown in several ways by adding vertices or edges due to the repetition of vertex and edge labels [23]. Furthermore, determin-ing whether a graph is a subgraph in another graph essentially involves a subgraph isomorphism test, because the vertices and edges of these types of graphs cannot be ordered in general [6]. Any canonical transformation employed to avoid the isomorphism test either has to suffer from loss of structural infor-mation or high computational cost [6]. The quality of a graph mining method tailored for this type of data is therefore measured by the ability to discover substructures effectively and efficiently.
In contrast to mining subgraphs, the frequent itemset mining algorithms generate their itemsets only once during the mining process, due to the fact that the transaction database is already ordered lexico-graphically, prior to mining. Furthermore, determining whether an item set (or a part of it) is identical to another itemset is computationally inexpensive. These features allow frequent itemset mining to be more efficient than frequent subgraph mining, as demonstrated in [39,40]. The loss of the structural informa-tion when transforming graphs into itemsets is not necessarily important if the predictive performances of the models using the itemset mining approach are comparable with the graph mining methods [40], which is exactly the purpose of using itemset mining in this study.

Although the seven descriptor discovery methods we consider in this study use the 2D structures of compounds as the input to the methods, the amount of structural information that can be extracted by the methods differ. GraphSig and SUBDUE use only the atom bond relations (and the respective class labels) for descriptor discovery, while MoFa identifies the ring structures and chemically equivalent atoms (wild cards) from the atom bond relations. SELMA and ECFI extract descriptors from SMILES fingerprints of atom bond structures. However, one cannot neglect the fact that more efficient graph mining implementations, which allow extracting additional background knowledge, may lead to more accurate QSAR models [33]. 3. Experimental setup
In this section, we describe the datasets, methods and experimental procedure used in the empirical investigation. 3.1. Datasets
We have collected eighteen datasets from the chemoinformatics domain, which are publicly avail-able [8] and concern modeling tasks of various end points, such as biological activity, absorption, dis-tribution, metabolism, excretion and toxicity (ADMET) [42]. In addition to a binary class label, each compound was also labeled by the activity level, represented by a continuous variable, allowing the same datasets to be used for evaluating predictive performance with respect to both classification and regression tasks. In Table 1, a summary of the datasets is provided. Further details of these benchmark datasets for QSAR modeling can be found in [8] and in the papers referenced therein. 3.2. Methods
As stated in the previous section, ECFI [7] and SELMA [41] are pre-defined chemical descriptors and the methods of calculating them are publically available [34]. We have implemented the two methods at AstraZeneca R&amp;D. Publicly available original implementations of the methods MoFa, SUBDUE, GraphSig, MFI, and CP were used for substructure-based methods in this experiment.

For classification, three well-known machine learning algorithms, random forests [21], support vector machines (SVM) [20] and the k-nearest neighbor algorithm [4], as implemented in the WEKA data mining toolkit [17], were used to generate the QSAR models. The number of trees generated by the random forest algorithm was set to 50. Two kernels for the SVM algorithm were investigated; the RBF kernel with complexity 2, and the polynomial kernel with complexity 2. The IBk algorithm with the number of nearest neighbors k = 3 was used as the nearest neighbor classifier.

For classification, accuracy was chosen as the performance metric, which was estimated using 10-fold cross-validation. Since the intention of this study was to draw conclusions of the relative performance of the descriptor sets without reference to a specific learning algorithm, the parameters of the learning algorithms were not tuned for optimal performance. For each dataset, one of the three learning algorithms together with the parameter settings were randomly selected, and one conclusion was then drawn for all descriptor sets independently of the algorithms. The alternative would have been to apply all learning algorithms on all descriptor sets, and either draw one conclusion for each learning algorithm or look for the best combination of descriptor set and learning algorithm, something which could be expected to require a very large number of datasets in order to allow for any statistically significant differences to be detected.

For regression, we have used the SVM algorithm for regression problems (SMOReg) as implemented in WEKA [17]. Two different parameter settings for SMOReg algorithm were chosen arbitrarily in the experiments, namely, the nonlinear polynomial kernel with complexity 2 and the RBF kernel with complexity 2. The root mean squared error (RMSE) was chosen as performance metric for the regression tasks, again using 10-fold cross validation. The class labels were used for feature construction in MoFa, GraphSig, SUBDUE, and CP. For MFI and CP, the minimum support was optimized by cross-validation on the training sets, and the optimized parameter was used on the test set. The same training and test folds were used for all methods. Again, one learning algorithm and parameter setting were randomly chosen for each dataset.

The experiments were carried out using a HP EliteBook, with two Core2 Duo Intel processors 2.40 GHz each, and 2.9 GB main memory, running under Ubuntu 10.4. An upper limit of 24 hours was set for each method on each dataset in order to penalize methods that were computationally (too) costly. 4. Results
In this section, we present and analyze the experimental results in relation to the two raised re-search questions; (Q1) Do the state-of-the-art chemoinformatics-based descriptors result in more ac-curate QSAR models than the substructure-based descriptors? and (Q2) Can QSAR models be improved by combining the two types of descriptors?
As described in Section 2, the graph mining and item set mining methods were compared in terms of efficiency and predictive performance. It should be first noted that some of the graph mining algo-rithms required excessive computational resources. GraphSig exhausted the memory for the datasets ache and bzr. MoFa did not complete within the 24 hour limit for the AMPH1dataset, consumed more than 22 hours to discover substructures for the ache dataset and exhausted memory after discovering two substructures for the thr dataset. SUBDUE was relatively efficient, yet spent a few hours to discover features for the datasets AI, ache and thr. As expected, the itemset mining methods MFI and CP were efficient, being able to find substructures for all the 18 datasets within the range of seconds.
The accuracies of the classification models and the root mean squared errors of the regression models built using both the chemical descriptors and the discovered substructures were compared. Figures 2 and 3 show the RMSE of the regressions models and Figs 4 and 5 depict the model accuracies of the classifier models respectively, for the seven descriptor sets, where the abbreviation within parentheses after each dataset name shows what learning algorithm and parameter setting were randomly selected. P and RBF stand for the support vector regression method with polynomial kernel and RBF kernel, respectively, while RF, KNN, SVMP and SVMR stand for the four classification methods Random Forests, K-Nearest Neighbor, and support vector machine with polynomial and RBF kernel, respectively. Regression results
To investigate the results statistically, a null hypothesis was formed stating that there is no differ-ence between the RMSE values yielded by different feature construction methods. The significance of the differences of the regression errors was tested by comparing the ranks (relative performance of the methods) using the Friedman test [11]. The Friedman test rejected the null hypothesis for this experi-ment. Therefore, further tests were conducted to identify pairs of methods for which the difference in performance was significant. The average ranks were used for pair-wise significance tests, based on the Nemenyi test [11]. When comparing the methods, a method that failed to produce a feature set was assigned the highest rank (corresponding to the worst performance).

Table 2 gives the differences of ranks for all the pair-wise tests. The pairs corresponding to dark cells in the table show the methods that performed significantly different, where a positive value indicates that the method in the column label outperformed the method in the row label, and vice versa for negative values. According to Table 2, the regression models built using ECFI descriptors outperformed all the other models by achieving a difference of ranks larger than the critical difference, which is 2.12 in this case. None of the other methods outperformed each other.
 Classification results
Similar to when the performances of regressions models were compared, a Friedman test of the dif-ferences in classifier accuracy also rejected the null hypothesis, i.e., stating that there was no difference between the classifier accuracies obtained by using the different descriptor sets. The result from applying the post-hoc Nemenyi test is shown in Table 3, where dark cells indicate pairs with significantly different performance, where again, a positive value indicates that the method in the column label outperformed the method in the row label, and vice versa for negative values.
 The statistical tests showed that models generated using ECFI outperformed the graph mining method GraphSig. The difference of the ranks of SUBDUE and ECFI were quite close to, but not greater than, the critical difference. The itemset mining algorithm and CP and the SELMA descriptor based models were competitive with ECFI. Nevertheless, except for CP and SELMA outperforming GraphSig, all the other methods performed comparably.
 In order to examine the effect of combined sets of different descriptors on the performance of the QSAR models, combinations of the chemoinformatics-based and substructure-based descriptors were investigated. The combined sets were used as features for the classification and regression models. For example, ECFI descriptors were combined with SELMA, SUBDUE, graphSig, MoFa, MFI and CP descriptors respectively, and so on. The same randomly selected classification and regression methods as used in the first experiment were considered for this analysis as well.
 Regression results
The models generated from combined feature sets were compared to the models generated from the constituent feature sets. For example, a model generated from the combination of CP and ECFI features was compared to a model generated from ECFI alone and a model generated from CP alone. Table 4 shows for each combined model and a model generated from one of its constituent feature sets, for how many datasets the combination resulted in an improvement in predictive performance. 1 For example, the number 14 in the cell on row 2 and column 1 means that models using SELMA + ECFI obtained a lower RMSE (the performance was improved) than models generated from SELMA alone for 14 out of the 18 data sets.

Adding ECFI descriptors to the other descriptor sets helped increasing the predictive performance of the resulting models for almost all the datasets. On the other hand, the performance of models gener-ated with ECFI could not be improved significantly by combination with other descriptors, other than SELMA. Thus, the ECFI descriptors appeared to represent a good starting point for deriving predictive regression models.
 Classification results
Table 5 shows the results from comparing models generated from combined feature sets to models generated from their constituent feature sets. 2
Again, the ECFI descriptors showed to be a strong candidate for being combined with, in order to improve predictive performance. Furthermore, the addition of SELMA to ECFI, improved the accuracy of ECFI based models in 10 out of 18 datasets, while adding SUBDUE to ECFI improved the accuracy in 8 out of 18 cases, and CP in 9 cases, which showed that ECFI based models could also be improved by being combined with other descriptor sets. 4.1. Why do ECFI descriptors outperform the other descriptors?
The question of why the ECFI descriptors performed better than the other descriptors was addressed by analyzing differences in the structure discovery approach of ECFI compared to those of the other methods that also use the 2D structure of molecules to discover features. ECFI, as described in Sec-tion 2, iteratively captures neighborhoods of the non-hydrogen atoms in the molecules. In doing so, it employs an identifier schema, which reflects the neighborhoods. The circular expansion of the neighbor-hoods around a particular atom of concern, during each updating step, allows it to consider rather large substructures quite rapidly. This contrasts to the generation of sub-graphs by the graph mining methods, which frequently extend the search tree by either a single vertex or an edge [3]. Therefore graph mining methods often substantially restrict pattern growth in order to allow for efficient mining, such as maxi-mum depth of the search tree traversal, and threshold values for limiting the number discovered patterns. As a consequence, some important substructures can be missed.

Furthermore, ECFI accumulates all the non-similar fragments generated around the non-hydrogen atoms after each identifier updating, and therefore includes fragments that contain a mixture of substruc-tures of different sizes. As a result, both large and quite specific substructures and small and relatively common substructures could be included in the same descriptor set [7]. In contrast, the substructures dis-covered by the graph mining methods limit the discovered patterns to above or below certain threshold values on the length of the discovered patterns and/or their support.

Also, ECFI uses wildcard atoms [7], which allow representing all the molecules in the data set in terms of a substructure plus the wildcard. This approach leads to a convenient molecular similarity checking. SUBDUE [30] uses a somewhat similar method of replacing small substructures by single atoms, which helps obtaining larger structures efficiently. MoFa [2] also uses a similar method to replace ring structures by single atoms prior to the mining step.

The approach of initialization and updating identifie rs in ECFI, while considering influences of the neighborhoods of the atoms, and which is based on a variant of the classical Morgan algorithm [14], is the key factor of the ECFI descriptor discovery method. An application of the Morgan algorithm in graph mining in [32] shows the benefits of using such an approach for enhanced performance. Nevertheless, the approach in [32] is quite primitive compared to the identifier assigning procedure of ECFI and a more detailed study of the usage of neighborhood information on atoms for graph mining methods is required prior to estimating how influential such information is, in order to discover important substructures. 5. Concluding remarks
Two main approaches to descriptor discovery, namely, chemoinformatics-based and substructure-based approaches, were compared when generating both regression and classification QSAR models. The study investigated whether the substructures discovered using data driven methods resulted in QSAR models with better or worse predictive performance than when using state-of-the-art chemical descrip-tors. The study further investigated whether it was possible to improve QSAR models by combining de-scriptor sets. The experimental results showed that chemoinformatics-based ECFI descriptors obtained significantly higher performance than the other considered descriptor sets. The experimental results fur-ther showed that the addition of ECFI descriptors to the other descriptor sets lead to improved predictive performance. Conversely, for about half of the considered datasets, the ECFI descriptors could be further improved by adding chemoinformatics-based SELMA or substructure-based SUBDUE descriptors.

There are several lines of research along which this study could be extended. Similar studies could be carried out for particular types of chemical compound datasets, e.g., very large or imbalanced datasets. As an alternative to combining feature sets prior to generating the QSAR models, one could also consider combining the output predictions of each model, c.f., [12]. Graph similarity based methods, such as graph kernels [10,39], including the Tanimoto kernel, Min-Max, Hybrid, etc. [22], are also commonly used for classification of chemical compounds. Although these methods do not generate explicit feature sets, and hence cannot be directly compared with the descriptor sets that we have considered in this study, the resulting models can at least be compared to the models generated from the descriptor sets.

Domain-specific background knowledge is rarely considered by current graph mining methods. Yet, the importance of background knowledge has been demonstrated in several studies, including [2,32, 33]. As an alternative way of incorporating the background knowledge encoded in the ECFI descriptors, rather than just merging feature sets, one could consider using the features, which correspond to 2D substructures, as a basis for compressing molecules, similar to replacing ring structures in MoFa, so that the graph mining methods may discover larger substructures in fewer number of iterations under mild constraints.
 Acknowledgments This work was supported by the Swedish Foundation for Strategic Research through the project High-Performance Data Mining for Drug Effect Detection (ref. no. IIS11-0053) at Stockholm University, Sweden.
 References
