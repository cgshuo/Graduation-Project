 In linguistics, it is often necessary to align words or phonetic sequences. Covington (1996) uses align-ments of cognate pairs for the historical linguis-tics task of comparative reconstruction and Ner-bonne and Heeringa (1997) use alignments to com-pute relative distances between words from various Dutch dialects. Algorithms for aligning pairs of words have been proposed by Covington (1996) and Kondrak (2000). However, it is often necessary to align multiple words. Covington (1998) proposed a method to align multiple words based on a hand-crafted scale of similarity between various classes of phonemes, again for the purpose of comparative reconstruction of languages.

Profile hidden Markov models (Profile HMMs) are specific types of hidden Markov models used in biological sequence analysis, where they have yielded success for the matching of given sequences to sequence families as well as to multiple sequence alignment (Durbin et al., 1998). In this paper, we show that Profile HMMs can be adapted to the task of aligning multiple words. We apply them to sets of multilingual cognates and show that they pro-duce good alignments. We also use them for the re-lated task of matching words to established cognate sets, useful for a situation where it is not immedi-ately obvious to which cognate set a word should be matched. The accuracy on the latter task exceeds the accuracy of a method based on edit distance.
Profile HMMs could also potentially be used for the computation of word similarity when a word must be compared not to another word but to an-other set of words, taking into account properties of all constituent words. The use of Profile HMMs for multiple sequence alignment also presents ap-plications to the acquisition of mapping dictionaries (Barzilay and Lee, 2002) and sentence-level para-phrasing (Barzilay and Lee, 2003).

This paper is organized as follows: we first de-scribe the uses of Profile HMMs in computational biology, their structure, and then discuss their appli-cations to word-related tasks. We then discuss our data set and describe the tasks that we test and their experimental setups and results. We conclude with a summary of the results and a brief discussion of potential future work. In computational biology, it is often necessary to deal with multiple sequences, including DNA and protein sequences. For such biological sequence analysis, Profile HMMs are applied to the common tasks of simultaneously aligning multiple related se-quences to each other, aligning a new sequence to an already-aligned family of sequences, and evalu-ating a new sequence for membership in a family of sequences.

Profile HMMs consist of several types of states: match states, insert states, delete states, as well as a begin and end state. For each position in a Pro-file HMM, there is one match state, one insert state, and one delete state. A Profile HMM can thus be vi-sualized as a series of columns, where each column represents a position in the sequence (see Figure 1). Any arbitrary sequence can then be represented as a traversal of states from column to column.

Match states form the core of the model; each match state is represented by a set of emission prob-abilities for each symbol in the output alphabet. These probabilities indicate the distribution of val-ues for a given position in a sequence. Each match state can probabilistically transition to the next (i.e. next-column) match and delete states as well as the current (i.e. current-column) insert state.

Insert states represent possible values that can be inserted at a given position in a sequence (before a match emission or deletion). They are represented in the same manner as match states, with each out-put symbol having an associated probability. Insert states are used to account for symbols that have been inserted to a given position that might not other-wise have occurred  X  X aturally X  via a match state. In-sert states can probabilistically transition to the next match and delete states as well as the current insert state (i.e. itself). Allowing insert states to transition to themselves enables the consideration of multiple-symbol inserts.

Similarly, delete states represent symbols that have been removed from a given position. For a se-quence to use a delete state for a given position indi-cates that a given character position in the model has no corresponding characters in the given sequence. Hence, delete states are by nature silent and thus have no emission probabilities for the output sym-bols. This is an important distinction from match states and insert states. Each delete state can prob-abilistically transition to the next match and delete states as well as the current insert state.

Figure 2 shows a small example of a set of DNA sequences. The match columns and insert columns are marked with the letters M and I respectively in the first line. Where a word has a character in a match column, it is a match state emission; when there is instead a gap, it is a delete state occur-rence. Any characters in insert columns are insert state emissions, and gaps in insert columns repre-sent simply that the particular insert state was not used for the sequence in question.

Durbin et al. (1998) describe the uses of Pro-file HMMs for tasks in biological sequence analy-sis. Firstly, a Profile HMM must be constructed. If a Profile HMM is to be constructed from a set of aligned sequences, it is necessary to designate cer-tain columns as match columns and others as insert column. The simple heuristic that we adopt is to label those columns match states for which half or more of the sequences have a symbol present (rather than a gap). Other columns are labelled insert states. Then the probability a kl of state k transitioning to state l can be estimated by counting the number of times A kl that the transition is used in the alignment: Similarly, the probability e k ( a ) of state k emitting symbol a is estimated by counting the number of times E k ( a ) that the emission is used in the align-ment: There is the danger that some probabilities may be set to zero, so it is essential to add pseudocounts. The pseudocount methods that we explore are de-scribed in section 3.

If a Profile HMM is to be constructed from a set of unaligned sequences, an initial model is gener-ated after which it can be trained to the sequences using the Baum-Welch algorithm. The length of the model must be chosen, and is usually set to the av-erage length of the unaligned sequences. To gener-ate the initial model, which amounts to setting the transition and emission probabilities to some initial values, the probabilities are sampled from Dirichlet distributions.

Once a Profile HMM has been constructed, it can be used to evaluate a given sequence for member-ship in the family. This is done via a straightforward application of the forward algorithm (to get the full probability of the given sequence) or the Viterbi al-gorithm (to get the alignment of the sequence to the family). For the alignment of multiple unaligned se-quences, a Profile HMM is constructed and trained as described above and then each sequence can be aligned using the Viterbi algorithm.

It should also be noted that Profile HMMs are generalizations of Pair HMMs, which have been used for cognate identification and word similar-ity (Mackay and Kondrak, 2005) between pairs of words. Unlike Pair HMMs, Profile HMMs are position-specific; this is what allows their applica-tion to multiple sequences but also means that each Profile HMM must be trained to a given set of se-quences, whereas Pair HMMs can be trained over a very large data set of pairs of words. Using Profile HMMs for biological sequences in-volves defining an alphabet and working with related sequences consisting of symbols from that alphabet. One could perform tasks with cognates sets in a sim-ilar manner; cognates are, after all, related words, and words are nothing more than sequences of sym-bols from an alphabet. Thus Profile HMMs present potential applications to similar tasks for cognate sets. We apply Profile HMMs to the multiple align-ment of cognate sets, which is done in the same manner as multiple sequence alignment for biolog-ical sequences described above. We also test Pro-file HMMs for determining the correct cognate set to which a word belongs when given a variety of cognate sets for the same meaning; this is done in a similar manner to the sequence membership evalua-tion task described above.

Although there are a number of Profile HMM packages available (e.g. HMMER), we decided to develop an implementation from scratch in order to achieve greater control over various adjustable pa-rameters. 1 We investigated the following parame-ters: Favouring match states When constructing a Pro-Pseudocount method We implemented three pseu-Pseudocount weight The weight that the pseudo-Smoothing during Baum-Welch The problem has Our data come from the Comparative Indoeuropean Data Corpus (Dyen et al., 1992). The data consist of words in 95 languages in the Indoeuropean fam-ily organized into word lists corresponding to one of 200 meanings. Each word is represented in the English alphabet. Figure 3 shows a sample from the original corpus data. We manually converted the data into disjoint sets of cognate words, where each cognate set contains only one word from each lan-guage. We also removed words that were not cog-nate with any other words.

On average, there were 4.37 words per cognate set. The smallest cognate set had two words (since we excluded those words that were not cognate with any other words), and the largest had 84 words. There were on average 10.92 cognate sets in a mean-ing. The lowest number of cognate sets in a meaning was 1, and the largest number was 22. Similar to their use for multiple sequence alignment of sequences in a family, we test Profile HMMs for the task of aligning cognates. As described above, an initial model is generated. We use the aforemen-tioned heuristic of setting the initial model length to the average length of the sequences. The transition probabilities are sampled from a uniform-parameter Dirichlet distribution, with each parameter having a value of 5.0. The insert-state emission probabil-ities are set to the background frequencies and the match-state emission probabilities are sampled from a Dirichlet distribution with parameters set in pro-portion to the background frequency. The model is trained to the cognate set via the Baum-Welch algo-rithm, and then each word in the set is aligned to the model using the Viterbi algorithm. The words are added to the training via a summation; therefore, the order in which the words are considered has no effect, in contrast to iterative pairwise methods.
The setting of the parameter values is discussed in section 6. 5.1 Results To evaluate Profile HMMs for multiple cognate alignment, we analyzed the alignments generated for a number of cognate sets. We found that increasing the pseudocount weight to 100 improved the quality of the alignments by effectively biasing the model towards similar characters according to the substitu-tion matrix.

Figure 4 shows the Profile HMM alignment for a cognate set of words with the meaning  X  X ay. X  As with Figure 2, the alignment X  X  first line is a guide label used to indicate which columns are match columns and which are insert columns; note that consecutive insert columns represent the same insert state and so are not aligned by the Profile HMM. While there were some duplicate words (i.e. words that had identical English orthographic representa-tions but came from different languages), we do not show them here for brevity.

In this example, we see that the Profile HMM manages to identify those columns that are more highly conserved as match states. The ability to identify characters that are similar and align them correctly can be attributed to the provided substitu-tion matrix.

Note that the characters in the insert columns should not be treated as aligned even though they represent emissions from the same insert state (this highlights the difference between match and insert states). For example, Y, A, Z, D, R, and O are all placed in a single insert column even though they cannot be traced to a single phoneme in a protoform of the cognate set. Particularly infrequent charac-ters are more likely to be put together than separated even if they are phonetically dissimilar.

There is some difficulty, also evident from other alignments we generated, in isolating phonemes rep-resented by pairs of characters (digraphs) as singular entities. In the given example, this means that the dz in dzien was modelled as a match state and then an insert state. This is, however, an inherent difficulty in using data represented only with the English al-phabet, which could potentially be addressed if the data were instead represented in a standard phonetic notation such as IPA. Evaluating alignments in a principled way is diffi-cult because of the lack of a gold standard. To adjust for this, we also evaluate Profile HMMs for the task of matching a word to the correct cognate set from a list of cognate sets with the same meaning as the given word, similar to the evaluation of a biologi-cal sequence for membership in a family. This is realized by removing one word at a time from each word list and then using the resulting cognate sets within the meaning as possible targets. A model is generated from each possible target and a log-odds score is computed for the word using the forward algorithm. The scores are then sorted and the high-est score is taken to be the cognate set to which the given word belongs. The accuracy is then the frac-tion of times the correct cognate set is identified.
To determine the best parameter values, we used a development set of 10 meanings (roughly 5% of the data). For the substitution matrix pseudo-count method, we used a log-odds similarity ma-trix derived from Pair HMM training (Mackay and Kondrak, 2005). The best results were achieved with favouring of match states enabled, substitution-matrix-based pseudocount, pseudocount weight of 0.5, and pseudocounts added during Baum-Welch. 6.1 Results We employed two baselines to generate scores be-tween a given word and cognate set. The first base-line uses the average edit distance of the test word and the words in the given cognate set as the score of the word against the set. The second baseline is similar but uses the minimum edit distance between the test word and any word in the given cognate set as the score of the word against the entire set. For ex-ample, in the example set given in Figure 4, the aver-age edit distance between zen and all other words in the set is 2.58 (including the hidden duplicate words) and the minimum edit distance is 1. All other can-didate sets are similarly scored and the one with the lowest score is considered to be the correct cluster with ties broken randomly.

With the parameter settings described in the pre-vious section, the Profile HMM method correctly identifies the corresponding cognate set with an ac-curacy of 93.2%, a substantial improvement over the average edit distance baseline, which obtains an ac-curacy of 77.0%.

Although the minimum edit distance baseline also yields an impressive accuracy of 91.0%, its score is based on a single word in the candidate set, and so would not be appropriate for cases where consider-ation of the entire set is necessary. Furthermore, the baseline benefits from the frequent presence of du-plicate words in the cognate sets. Profile HMMs are more robust, thanks to the presence of identical or similar characters in corresponding positions. Profile HMMs present an approach for working with sets of words. We tested their use for two cognate-related tasks. The method produced good-quality multiple cognate alignments, and we believe that they could be further improved with phonetically transcribed data. For the task of matching words to correct cognate sets, we achieved an improvement over the average edit distance and minimum edit dis-tance baselines.

Since Profile HMM training is highly sensitive to the choice of initial model, we would like to ex-plore more informed methods of constructing the initial model. Similarly, for building models from unaligned sequences, the addition of domain knowl-edge would likely prove beneficial. We also plan to investigate better pseudocount methods, as well as the possibility of using n-grams as output symbols.
By simultaneously considering an entire set of re-lated words, Profile HMMs provide a distinct ad-vantage over iterative pairwise methods. The suc-cess on our tasks of multiple alignment and cognate set matching suggests applicability to similar tasks involving words, such as named entity recognition across potentially multi-lingual corpora.
 We thank Qing Dou for organizing the cognate sets from the original data. We are also grateful to the anonymous reviewers for their valuable comments. This research was funded in part by the Natural Sci-ences and Engineering Research Council of Canada.
