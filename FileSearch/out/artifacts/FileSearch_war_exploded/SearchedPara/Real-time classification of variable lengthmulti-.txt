 REGULAR PAPER Chuanjun Li  X  Latifur Khan  X  Balakrishnan Prabhakaran Abstract Multi-attribute motion data can be generated in many applications/ devices, such as motion capture devices and animations. It can have dozens of attributes, thousands of rows, and even similar motions can have different durations and different speeds at corresponding parts. There are no row-to-row correspondences between data matrices of two motions. To be classified and recognized, multi-attribute motion data of different lengths are reduced to feature vectors by using the properties of singular value decomposition (SVD) of motion data in this paper. The reduced feature vectors of similar motions are close to each other, while reduced feature vectors are different from each other if their motions are different. By applying support vector machines (SVM) to the feature vectors, we efficiently classify and recognize real-world multi-attribute motion data. With our data set of more than 300 motions with different lengths and variations, SVM outperforms classification by related similarity measures, in terms of accuracy and CPU time. The performance of our approach shows its feasibility of real-time applications to real-world data.
 Keywords Classification  X  Pattern recognition  X  Support vector machines  X  Singular value decomposition  X  Multi-attribute motion 1 Introduction Multi-attribute motion data is encountered in several applications/devices. For in-stance, a gesture sensing device such as CyberGlove has multiple sensors that transmit values to indicate motion of a hand, and a motion capture system gener-ates multiple degrees of freedom (DOF) data for human motions. If we consider 3D animations, motion of a model typically involves translation and rotation for different nodes of the model. In many of these cases, more than one value is gen-erated at each time, rather than only one value at each time as in a time series data sequence. As a result, a multi-attribute motion yields a matrix, rather than a mul-tidimensional vector as for a time series sequence. For example, multi-attribute motion data used in this paper are matrices, each of which is for one motion and has one column from each of the 22 sensors of a CyberGlove and has one row for each data collection.
  X  Matrices of data sequences of multiple variables or attributes are involved be- X  Motions can be carried out with different speeds at different time, they can  X  There might be hundreds or thousands of classes of motions used to classify using the singular vector decomposition (SVD) properties of the motion matrix. SVD optimally exposes the geometric structure of a matrix and this geometric structure can be exploited to obtain a feature vector for a motion matrix. We then for classification by using a training set of vectors each of which has a unique class label. After training, the machines can determine the label of a new vector and thus classify the new vector during a testing phase. We show by experiments that motions of different durations with different variations can be recognized effectively and quickly.
 the most powerful classification techniques that have been successfully applied to many real-world problems [2, 17, 18]. SVM has been proven to be computation-ally efficient especially when dealing with relatively large data sets [4]. SVM is relatively insensitive to the number of data points and the classification complex-ity does not depend on the dimensionality of the feature space. Hence, SVM can be potentially used to learn a larger set of patterns and thus be able to scale better than other techniques such as neural networks [4].
 larities of motion data in the testing datasets with motion data in the training data-sets. The similarity measures used are MAS [13] and the weighted-sum SVD [ 16 ], two similarity measures for variable-length multi-attribute motion data which at-tempts to address the above challenges.
  X  Obtaining a feature vector for each motion matrix by using SVD. This fea- X  Applying SVM to the feature vectors of the multi-attribute motion data and  X  Using the MAS and weighted-sum SVD similarity measures to compute  X  Demonstrating that testing data classification can be done in real time with of related work. Section 3 contains the background knowledge of SVM, SVD and dynamic time warping (DTW). Section 4 proposes a new approach to classifying multi-attribute motion data using SVD and SVM, and the classification is further verified by using DTW for motion directions. Section 5 experimentally evalu-ates the accuracy and CPU time of our proposed approach, followed by Section 6 which concludes the paper. 2 Related work Recognition of multi-attribute sequences has obtained increasing attentions in re-cent years. Mostly, distance measures are defined for multi-attribute data to re-equal lengths are considered. Scaling and shifting transformations are considered when defining sequence distances and an index structure is proposed for shift and scale transformations. Similarity search of multi-attribute sequences with differ-ent lengths cannot be solved by the distance definitions and the index as proposed in [9]. of the partitioned subsequences is contained in a minimum bounding rectangle (MBR). Every MBR is indexed and stored into a database by using an R-tree or any of its variants. Estimated MBR distances are used to speed up the searching of similar motions. If two sequences are of different lengths, the shorter sequence is compared with the other by sliding from the beginning to the end of the longer sequence. This makes it impossible to recognize two similar sequences with dif-ferent durations or with local accelerations and decelerations.
 ity measures of multi-attribute data in [ 19 ]. Before the exact LCSS or DTW is performed, sequences are segmented into MBRs to be stored in an R-tree. Based on the MBR intersections, similarity estimates are computed to prune ir-relevant sequences. Both DTW and LCSS have a computational complexity of O (w d ( m + n )) ,where w is a matching window size, d is the number of attributes, and m , n are the lengths of two data sequences. When w is a significant portion of m or n , the computation can be even quadratic in the length of the sequences, making it non-scalable to large databases with long multi-attribute sequences. It has been shown in [19] that the index performance significantly degrades when the warping length increases. Even for a small number of 20 MBRs per long size.
 writing recognition [8, 11] as well as American Sign Language (ASL) recognition problems [17]. Different states should be specified for each sign or motion unit when HMMs are involved, the number of words in a sentence is required to be known beforehand, and grammar constraints should also be known beforehand for using HMMs. When the specified states are not followed, or motion variations are relatively large, recognition accuracy would decrease dramatically. This is true even when legitimate or meaningful motions are generated for HMM-based recog-nitions. Our paper addresses the classification of individual motions, no states or grammar constraints are involved for individual motions. Thus, HMMs are not suitable for our classification purpose.
 Bayesian classifiers and Neural Networks to recognize static signs for a 10-sign vocabulary, and achieved 84.66% accuracy. In [16], a weighted-sum SVD is de-fined for measuring the similarity of two multi-attribute motion sequences. The similarity definition takes the minimum of two weighted sum of the inner prod-ucts of right singular vectors.
 follows. where u 1 and v 1 are the first singular vectors of Q and P , respectively,  X  =  X / |  X  | ,  X  =  X / |  X  | ,and  X  and  X  are the vectors of the singular values of Q T Q and P
T P , respectively. Weight parameter  X  is to ensure that the normalized singular value vectors  X  and  X  and the first right singular vectors u 1 and v 1 have similar contributions to the similarity measure and is determined by experiments.  X  can be set to 0.9 for the multi-attribute motion data. This similarity measure captures the most important information revealed by the first right singular vectors and the singular values, and can be applied to prune most of the irrelevant motion data, and inner products of equal length re-interpolated first left singular vectors have been used as the first attempt to consider motions with different directions or with repetitions.
 angular similarity of two motions as MAS hereafter. A more detailed comparison of MAS and weighted-sum SVD is addressed in Section 5.3 . 3 Background In this section, we give some background knowledge of support vector machines and singular value decomposition, as well as dynamic time warping, for our pro-posed classification approach. 3.1 Support vector machines (SVM) SVM are a class of learning machines that aim at finding optimal hyperplanes, the boundaries with the maximal margin of separation between every two classes, among different classes of input data or training data in a high dimensional feature space F , and new test data can be classified using the separating hyperplanes. The optimal hyperplanes, found during a training phase, make the smallest number of training errors. Fig. 2 illustrates an optimal hyperplane for two classes of training data.
 y ,and y i  X  X  X  1 , + 1 } for binary classification. Given an input vector x ,anSVM constructs a classifier of the form where {  X  i } are non-negative Lagrange multipliers each of which corresponds to an example from the training data, b is a bias constant, and K (  X  ,  X  ) is a kernel sat-isfying the conditions of Mercer X  X  theorem [ 18 ]. Frequently used kernel functions are the polynomial kernel K ( x i , x j ) = ( x i  X  x j + 1 ) d and Gaussian radial basis quadratic programming problem: where 0  X   X  i  X  C with parameter C allowing for trading off training error against model complexity.
 timization techniques. The vectors for which  X  i &gt; 0 after optimization are called support vectors . Support vectors lie closest to the optimal hyperplane. After train-ing, only the support vectors of the training data are used to represent the classi-fiers, and other training vectors have no influences.
 and one-versus-one approaches. The one-versus-rest method constructs k classi-fiers for k classes, each of which separates that class from the rest of the data, and a test data point will be classified in the class that maximizes the decision func-for each pair of classes. A test data vector is classified by all the k ( k  X  1 )/ 2clas-sifiers, and belongs to the class with the largest number of positive outputs from these k ( k  X  1 )/ 2 pair classifiers. The one-versus-one method will be used for this work due to its simplicity and high classification accuracy [3].
 data. To classify multi-attribute data, which are matrices rather than vectors, we need to transform or reduce multi-attribute data into uni-attribute data or vectors. We propose to use SVD to reduce multi-attribute motion data to feature vectors. Before showing how to generate feature vectors, we present a brief introduction of SVD in the following subsection. 3.2 Singular value decomposition (SVD) As proved in [6], for any real m  X  n matrix A , there exist orthogonal matrices such that The  X  i is the i th singular value of A in non-increasing order and the vectors u i and v are the i th left and right singular vectors of A for i = min ( m , n ) , respectively. The singular values of a matrix A are unique, and the singular vectors correspond-ing to distinct singular values are uniquely determined up to the sign [14]. Fig. 3 shows an SVD example for a 6  X  4 matrix A .
 vector v 1 gives the direction along which the multi-dimensional row vectors or points contained in A have the largest variations, and the second right singular vector v 2 is the direction with the second largest variations, and so on. The singular values  X  i reflect the variations along the corresponding i th singular vectors. Fig. 4 shows the data in an 18  X  2 matrix and its first singular vector v 1 and its second singular vector v 2 . Along the first singular vector v 1 , data points have the largest variation as shown in Fig. 4 .
 in A in the new coordinate system spanned by the column vectors of V [ 10 ]. Let x be a non-zero row vector, the multiplication of x and A , i.e., xA ,givesa corresponding vector of x in the system spanned by the column vectors of V . responding eigenvectors of M = A T A , and the singular values of A are the square roots of the corresponding eigenvalues of M . Hence, the computations of right singular vectors and singular values can be done by computing the eigenvectors and eigenvalues of the symmetric matrix M = A T A . For symbolic convenience, we will use u 1 and  X  to represent the respective first singular vector and singular value vector of M = A T A , and use v 1 and  X  to represent those of a similar motion M = B T B . 3.3 Dynamic time warping DTW is used to align two data sequences of length m and n , respectively. Let d ( i , j ) be the local distance between the two sequences at i and j ,and D ( i , j ) be the global accumulative distance up to the sequence locations at i and j , respec-tively. Then warping window [1]. 4 Multi-attribute motion data classification In this section, we propose an efficient classification approach using both SVD and SVM as shown in Fig. 5 . We use CyberGlove [5], a fully instrumented glove that provides up to 22 high-accuracy joint-angle measurements, to generate hand motion data, and compute the SVD of the motion matrices. A feature vector is reduced from each motion matrix SVD, and all the feature vectors of training motion datasets are used as inputs to SVM for training. This training phase can be done offline (see the top portion of Fig. 5 ). Similarly, we generate testing data sets and these testing data can be classified by the SVM that has already been trained offline with training data sets. We would like to classify the testing data in real time (see the bottom portion of Fig. 5 ).
 classify feature vectors r k  X  X  using SVM as shown in Fig. 5 . Motions following the similar trajectories but in different directions are further recognized by using DTW distances of the first left singular vectors.
 4.1 Feature vector generation When two motions are similar, the geometric structures of the motion data ma-variations should be close to each other, that is, their first right singular vectors u 1 and negated due to the sign non-uniqueness of SVD as shown in Fig. 6 . The singular values can be very small in the other directions, making the directions of other sin-gular vectors vary even if two motions are similar to each other as shown in Fig. 7 . For this reason, the first singular vectors are dominating and reveal the essential geometric structure information of the data matrices, and are considered for the feature vectors. should also be proportional to the corresponding variations of the other similar motion. Let us assume normalized singular value vectors  X  =  X / |  X  | and  X  =  X / |  X  | ,then  X  should be close to  X  if the two motions are similar to each other. not impossible, to tell if two vectors are different just by their signs by comparing just a few elements of the two vectors as Fig. 8 shows.
 x in the system spanned by the column vectors of V . The first few components of the projection vector xV can vary in the range [ X  1 , 1 ] , and the last few compo-nents approach zero as shown in Fig. 9 . If the first component of the vector xV is negative, we negate all components of xV , then similar motions will have similar vectors after negation. If the first component of the projection vector xV is close to zero, say in the range [ X  0 . 05 , 0 ] , small motion variations can cause the first component of a similar motion to be positive while the two vectors are close to each other and no negation is needed for them. Hence, we consider each projection vector and its negation for classification when the first component of a projection vector xV is close to zero, say in [ X  0 . 05 , 0 . 05 ] .
 the singular values by concatenating the projection vector (negated if necessary) and the normalized singular value vector of a motion matrix.
 P k in the training data sets as follows.  X  Compute the SVD for each matrix M k = P T k P k . Let its first right singular  X  X et S be the matrix with v k being its k th row. Compute the SVD of S T S :  X  Compute SV , negate its rows in which the first components are negative, and  X  Generate r k as the concatenation of t k and  X  k ,where t k is the k th row of T . as follows:  X  Compute the SVD for matrix M = Q T Q . Let its first right singular vector be  X  Compute u 1 V ,where V is the SVD component of S as computed above.  X  Generate r q as the concatenation of t and  X  k .
 otherwise their feature vectors would be different. This is true even when one part of a motion is slower than the corresponding part in a similar motion, while the rest of the two motions are of similar speeds as shown in Fig. 10 . The reason is that the portion corresponding to slower motion will have more points (rows) than the corresponding part with fast speed as illustrated in Fig. 4 , yet the directions of the first singular vectors of similar motions should be close to each other, and the directions of the normalized singular value vectors should also be close to each other. In other words, if two motions are similar, but one is slower than the other, the slower one might have multiple lines where the faster one has a single line in the matrix. Furthermore, since the motion is smooth, data collections at adjacent times would be nearly the same, and they are nearly the same as the corresponding row in the matrix for the fast motion, resulting in virtually the same matrix ranks for two motions. As SVD will reveal the true rank and eliminate the redundancy caused by the extra nearly identical rows, the SVDs would be virtually the same. As a matter of fact, although all the experimental data of similar motions used in Sect. 5 are of different speeds at corresponding portions, the classification results show that their feature vectors are similar if the motions are similar. 4.2 Classification of feature vectors using SVM Classification of feature vectors includes two phases: training and testing. Support vectors and optimal hyperplanes are obtained for the feature vectors during the offline training phase, and the subsequent testing phase classifies new test data in real time based on the offline training results. SVM software package [3] is used for this work, and the RBF kernel function is used for training. The type of kernel utilized by the SVM is inconsequential as long as the capacity is appropriate for the amount of training data and complexity of the classification boundary [18]. Both the training vectors and the testing vectors have the following format: the i th component of r k .
 versus-one multi-class SVM is employed for classification. The class label l in the is close to zero, t k in r k is negated and a second r k with negated t k concatenated with  X  k is also used during testing to classify r k . 4.3 Recognition of motions in different directions The following theorem implies that motions following the same trajectories in different directions will have the same feature vectors.
 Theorem 1 Let A be any real m  X  n matrix, and A = U 1 V T .If Bisanm  X  n matrix, and any row in A is in B and any row in B is also in A, then B = U 2 V T . Proof Let A T be the transpose of A , = T and M = A T A ,then difference to M . The only difference between A and B is that the order of rows in A might be different from the order of rows in B , hence B T B = M .Since and V in M are decided by rows of A and are independent of the order of rows in A , they must also appear in the SVD of B . It follows that B = U 2 V T ,which completes the proof of the theorem.
 to be different, the above obtained feature vectors do not suffice. Since the left the left singular vector information to determine the different motion direction issue.
 tions of row vectors in A onto v 1 , the direction in which A has the largest variation.  X  u 1 can also be understood as the sum of row vector components weighted by So  X  u 1 actually contains the motion path information of A , and different directions can be reflected in  X  u 1 . Motions following similar trajectories have similar v 1 as implied by Theorem 1 , and if they have different directions, their projections onto v 1 would be different; i.e., the corresponding other. This makes it possible for us to consider motion directions by taking into account only  X  u 1 , a uni-attribute sequence, rather than the original multi-attribute matrix A .  X  u 1 can be computed by multiplying A with v 1 , which can be obtained by computing the SVD of M = A T A . Since motions can have different durations and their lengths can be different, and different rates cause local scaling or dis-alignments, Euclidean distance cannot be used to measure the distance of vectors  X  u . Since DTW can warp the time dimension, we propose to use DTW distance to measure distance of vectors  X  u 1 .
 Without loss of generality, assume that the mean of v 1 components is positive, or equivalently, negate  X  u 1 if the mean of v 1 components is negative.
 showninFig. 12 , and their DTW distance is just 0.04, while the DTW distance of  X  u 1 of motions (a) and (c) as shown in Fig. 13 is 0.40. If two motions follow similar trajectories, we can assign the same class label to them. If a testing motion is classified as this class label, left singular vector information is used to find the motion which in the training class has the smallest DTW distance to the testing motion. 5 Performance evaluation In this section, we evaluate the proposed classification approach by classifying real-world data and compare the classification accuracy and CPU time with those obtained by similarity computation using the MAS [13] and the weighted-sum SVD similarity measure [16]. Finally, DTW distances of the left singular vectors are utilized to further recognize different motions following similar trajectories but in different directions. 5.1 Motion data generation We generated motion data for different hand motions using CyberGlove. Motions of a hand are captured by 22 sensors located at different positions of the glove, and one sensor generates one angular value at about 120 times per second. One hundred and ten different motions were carried out, and three similar motions were generated for each of them. Each motion has a different duration from all the others, and all the resultant motion data matrices have different lengths ranging from about 200 to about 1500 rows. The data matrix of one motion can have more than 2 times the length of the data matrix of a similar motion. Each data matrix of different motions is given a unique class label, and similar motions have the same class label. 5.2 Performance evaluation We divide the data into three data sets. Each data set has data for 110 different motions, and includes data of one of the three similar motions. Sequentially, we use two data sets for training, and the third data set for testing. Three test cases have been run, with one different data set used for testing in each test case. All the experiments were performed on a 2.70 GHz Intel processor of a Genuine Intel Linux box, and the code was implemented in C/C ++ .
 Fig. 14 illustrates the partitioning of the data sets for the K -fold validation (with K = 3) and (a), (b) and (c) correspond to the cases with the data sets 1, 2 and 3 being the respective testing data sets. We define the accuracy as the percentage of motions classified or recognized correctly.
 comparison, the two similar motions in each training class are used as patterns for computation of similarity with motions in the testing data set using MAS and weighted-sum SVD. Motions in the testing data sets were recognized as the corresponding patterns with the highest similarities. As shown in Fig. 15 ,when both similar motions are used as patterns, SVM classification outperforms MAS and weighted-sum SVD in terms of classification/recognition accuracy for the first two testing data sets, and SVM classification and MAS both achieve 100% accuracy for testing data set 3, compared with the 90% accuracy of weighted-sum SVD for testing data set 3.
 tation using MAS [13] and weighted-sum SVD [16], while multiple similar mo-tions are assumed for each class in the training of the SVMs. Fig. 16 shows the recognition accuracies using MAS and weighted-sum SVD when only one pat-tern is used for each different motion. The proposed SVM classification approach clearly outperforms MAS and weighted-sum SVD for all testing data sets. SVD is shown in Fig. 17 . The proposed SVM classification takes CPU time com-parable to that needed by MAS, and takes less time than weighted-sum SVD. 5.3 Discussion After classification, DTW distance can be used to further determine motion direc-tions if any class has motions following similar trajectories in different directions. This has already been shown using CyberGlove motion data in Sect. 4.3 . All the data used in this paper were generated by using CyberGlove, and the data genera-tion rates are different for different motions and are different for different motion components.
 the corresponding singular vectors weighted by their singular values, and takes the minimum of two sums of the inner products as the similarity measure of two matrices. This definition includes some noise components since we observed that not all corresponding singular vectors of similar motions are close to each other as shown in Fig. 7 , hence not all the inner products of singular vectors should be considered in the definition of the distance measure. Since the inner products of singular vectors can be both positive and negative, and the weights can be the singular values of either matrix, it is very likely that the weighted sum can drop or jump sharply even if a testing matrix approximately matches some training motion. In contrast, MAS does not consider all singular vectors for the similarity definition. It considers only the dominating first singular vectors and the singular values. By generating a new vector from the normalized singular values, MAS takes into consideration both the dominating direction and the shape of the mo-tion data hyper-ellipsoid, while noises caused by variations of similar motions are reduced by not considering other singular vectors. The proposed SVM classifi-cation achieves performances better than MAS and weighted-sum SVD due to the optimal hyperplanes between different classes and due to its considering only the dominating factors, i.e., the first singular vectors and the normalized singular value vectors. 6Conclusion We have shown that by reducing multi-attribute motion data into feature vectors, SVM can be used to efficiently classify multi-attribute data. Feature vectors are as shown by the high accuracies of SVM classification we have achieved. RBF function has been used as the kernel function in this paper, although other kernel functions can also be provided to the SVMs during the training process, which se-lects a small number of support vectors for the hyperplanes. The high accuracy and low CPU testing time make SVMs a feasible technique to classify and recognize multi-attribute data in real time.
 allows for less variations in similar motion as shown in Fig. 16 . By reducing multi-attribute motion data into feature vectors, and using a group of feature vectors for a class, a new motion has higher expected probability of being recognized by SVMs as optimal hyperplanes are obtained during the training phase.
 by using CyberGlove. Data generation rates may be different for each motion and may be different for different motion components. Motions are classified irrespec-tive of their directions at first, and taking into consideration the first left singular vectors by using DTW can further distinguish motions following similar trajecto-ries in different directions. To reduce the time required for computing SVD and to explore the characteristics of SVM further in order to reduce the offline train-ing time are among the future work. We have addressed the problem of real time recognition of individual motions accurately and efficiently. We will investigate the feasibility of our approach in segmenting and recognizing individual motions in continuous streaming data.
 References Author Biographies
