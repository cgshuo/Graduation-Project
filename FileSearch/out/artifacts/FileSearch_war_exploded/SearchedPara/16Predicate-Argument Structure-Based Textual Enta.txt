 TOMOHIDE SHIBATA and SADAO KUROHASHI, Kyoto University RTE (Recognizing Textual Entailment) is the task to detect whether a hypothesis ( H ) can be inferred/entailed by a text ( T ) [Dagan et al. 2006]. RTE is important in ba-sic analysis such as parsing and anaphora resolution as well as applications such as Question Answering (QA) [Harabagiu and Hickl 2006], Information Retrieval (IR), and Machine Translation (MT).

Several methods for RTE have been proposed so far. Most of those methods take a machine learning approach where a text and hypothesis are represented by a set of features obtained from word overlapping ratio, lexical/syntactic clues, and linguistic 2007].

Different from the conventional machine learning approaches, we take a structural matching approach to Japanese Recognizing Textual Entailment task. Let us consider the following simple example, whose correct answer is  X  X O X . The conventional ma-chine learning approaches would wrongly judge it as  X  X ES X  since all the words in H are matched with a word in T .
 (1) T:  X  X  X  X   X  X  X  X  X  X  X   X  X  X  X 
By structural analysis, the following structure can be obtained: while in T ,the  X  ( acc ) 1 case of the predicate  X   X  X  X   X  X  X   X  (release) is  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X   X  (clear clean plus whitening), in H ,the  X  ( acc ) case of the predicate  X   X  X  X   X  X  X   X  (release) is  X   X  X  X  X   X   X  (malic-acid). By perform-ing predicate-argument matching, the system can correctly judge it as  X  X O X , since the  X  ( acc ) case of the same predicate is different between T and H . The utilization of predicate-argument-based matching is expected to achieve high precision compared to the conventional machine learning approaches.

Our proposed method regards predicate-argument structure, which consists of a text/hypothesis, and performs the matching between a text and hypothesis. Both the text and hypothesis are divided into predicate-argument structures based on predicate-argument structure analysis, and if all the predicate-argument structures in the hypothesis are matched to predicate-argument structures in the text, the hy-pothesis is judged to be entailed from the text.

To perform precise matching, wide-coverage lexical knowledge between words/ phrases, such as synonym, is-a and antonym, is indispensable. To recognize the follow-ing entailment relation, the synonym between  X   X  X  X  X  X  X  X   X  (atomic power generation) and  X   X  X  X   X  (the abbr. of  X   X  X  X  X   X  X  X   X ), and the synonym between  X   X  X  X   X (emit)and  X   X  X  X   X  (emit) are required. (2) T:  X  X  X  X  X  X  X   X   X  X  X  X  X  X  X  X  X   X  X  X   X  X  X  X   X  X  X  X  X  X  X  X  X 
Our proposed method acquires such relations from a dictionary and Wikipedia, and calculates distributional similarity using a Web corpus. Then, they are utilized when matching a text and hypothesis. The synonym between  X   X  X  X  X   X  X  X   X  (atomic power generation) and  X   X  X  X   X  (the abbr. of  X   X  X  X  X   X  X  X   X ) is acquired from a dictionary, and the synonym between  X   X  X  X   X (emit)and X   X  X  X   X  (emit) can be recognized based on the distributional similarity using a large Web corpus. Figure 1 describes our proposed method.
We also take a machine learning approach to consider relatively shallow clues such as the overlap ratio of characters and morphemes as well as the result of predicate-matching method. This section describes resources utilized for the matching between a text and hypoth-esis, which include relations between words/phrases extracted from a dictionary and Wikipedia, and distributional similarity calculated using a large Web corpus. First, synonym, is-a, and antonym relations are automatically extracted from an ordi-nary dictionary and Wikipedia. Next, by assigning an ID to each synonymous group, synonymy database is constructed. Then, the SynGraph data structure we proposed earlier [Shibata et al. 2008] is introduced to pack expressive divergence. 2.1.1. Synonym/is-a/antonym Extraction from an Ordinary Dictionary. We extract synonym, is-a, and antonym relations from definitions of an ordinary dictionary. The last word of the first definition sentence is usually the hypernym of an entry word since Japanese is a head-final language. Some definition sentences in a Japanese dictionary are shown below (the word to the left of  X : X  is an entry word, the right sentence is a definition, and words in bold font are the extracted words). For example, the last word  X  X  X  (meal) can be extracted as the hypernym of  X  X   X  (din-ner). In some cases, however, a word other than the last word can be a hypernym or synonym. These cases can be detected by sentence-final patterns as follows (the underlined expressions represent the patterns).

Hypernyms.
Synonyms / Synonymous Phrases. yyy
In a definition sentence, if there is an antonym, it is represented in a special for-mat. Therefore, we can acquire antonyms automatically. For example,  X   X  X  X   X (hot)is extracted as the antonym of  X   X  X  X   X  (cold). 2.1.2. Synonym/is-a Extraction from Wikipedia. From Wikipedia, synonym/is-a relations including named entities, technical terms, neologisms, etc., can be extracted, which are hard to extract from a dictionary. By applying the same method as the one introduced in the previous section to Wikipedia articles, synonym/is-a relations are extracted from the first sentence in Wikipedia articles (the same as Sumida and Torisawa [2008]). For example,  X   X  X  X  X   X  (probe) can be extracted as the hypernym of Genesis from the following sentence.

Furthermore, synonyms can be extracted from redirection pages. For example, since  X   X  X  X  X  X  X  X  X  X  X  X   X  (touch screen) is redirected to the article on  X   X  X  X  X  X  X  X  X  X   X  (touch panel),  X   X  X  X  X  X  X  X  X  X  X  X   X  is extracted as the synonym of  X   X  X  X  X  X  X  X  X  X   X . 2.1.3. Synonymy Database Construction. With the extracted binomial relations, a syn-onymy database can be constructed. Here, polysemic words should be treated care-fully 3 . When the relations A=B and B=C are extracted, and B is not polysemic, they can be merged into A=B=C. However, if B is polysemic, the synonym relations are not merged through a polysemic word. In the same way, as for is-a relations, A  X  Band B  X  C, and A  X  BandC  X  B are not merged if B is polysemic. By merging binomial synonym relations with the exception of polysemic words, synonymous groups are con-structed first. They are given IDs, hereafter called SYNID, which is surrounded by  X   X . Then, is-a and antonym relations are established between synonymous groups. We call this resulting data a synonymy database. Figure 2 shows examples of synony-mous groups in the synonymy database. 2.1.4. SynGraph Data Structure. SynGraph data structure is an acyclic directed graph, and the basis of SynGraph is the dependency structure of an original sentence. In the dependency structure, each node consists of one content word and zero or more function words, which is called a basic node . If the content word of a basic node belongs to a synonymous group, a new node with the SYNID is attached to it, and it is called a SYN node . For example, in Figure 3, the shaded nodes are basic nodes and the other nodes are SYN nodes.
Then, if the expression conjoining two or more nodes corresponds to one synonymous group, a SYN node is added there. In Figure 3,  X  X  X  X  (nearest) is such a SYN node. Furthermore, if one SYN node has a hyper synonymous group in the synonymy database, the SYN node with the hyper SYNID is also added.

In this SynGraph data structure, each node has a score, NS (Node Score), which reflects how much the expression of the node is shifted from the original expression.
By converting both a text and hypothesis to SynGraph data structure, the synonym/is-a relation between words/phrases in the text and a hypothesis are added, and the matching between the text and hypothesis considering the synonym/is-a rela-tion can be performed.
 Although synonym/is-a relation can be acquired from a dictionary/Wikipedia in the way introduced in Section 2.1, some near-synonymous relations cannot be acquired. For example, near-synonymous predicate relations such as  X   X  X   X  (abolish) and  X   X  X   X  (stop) cannot be acquired.

Therefore, distributional similarity [Firth 1957; Harris 1968], which is calculated based on the notion that  X  X ords that occur in similar contexts tend to be semantically similarities are high are utilized when matching predicates/arguments in a text and hypothesis.

In this article, the following types of distributional similarity are calculated. (1) Between predicates: (2) Between predicate-arguments where the argument is identical: (3) A predicate and an idiom consisting of a predicate and an argument: (4) Noun and noun that has a redundant suffix:
The distributional similarities for each type are calculated in the same framework, is extracted from a corpus. Then, distributional similarity between units is calculated. 2.2.1. Feature Extraction. A set of features f for the unit u is extracted from a large follows. (1) Between predicates: (2) Between predicate-arguments where the argument is identical: (3) Between a predicate and an idiom consisting of a predicate and an argument: (4) Between noun and noun that has a redundant suffix:
Features are extracted from a corpus, and then each type is represented by a feature vector. Table I shows examples of feature of the predicate  X   X  X   X  (abolish). ity calculation into weight and measure functions [Curran 2004]. The weight function transforms the raw counts for each feature instance into more comparable values. The measure function calculates the similarity between the two weighted vectors.
Several weight/measure functions have been utilized thus far. In this article, the most suitable combination of weight and measure functions was determined using the evaluation set [Aizawa 2008] among weight functions listed in Table II and measure functions listed in Table III. In this evaluation set, the similarity measure is eval-uated in terms whether similar/dissimilar nouns can be identified. The combination of Binary for a weight function and JACCARD -SIMPSON for a measure function per-formed best, and thus it is adopted for all the types (predicate, predicate-argument, and noun phrase) since we assume that the suitable weight/measure function does not depend on the type.
 In both a text and a hypothesis, we perform morphological analysis using the Japanese Morphological Analyzer JUMAN 4 and syntactic/case analysis and zero anaphora reso-lution [Sasano and Kurohashi 2011] using the Japanese parser KNP 5 [Kawahara and Kurohashi 2006b]. Then, they are converted to SynGraph data structure. Based on the syntactic/case analysis, a text and a hypothesis are divided into predicate-argument structures. An example of predicate-argument structure is shown in Figure 4. Each predicate-argument structure consists of a predicate and zero or more arguments. For example, the sentence (3) in Figure 4 is decomposed to two predicate-argument structures (1 X  1) and (2 X 1) based on predicate-argument structure analysis, and predicate-argument structure (1 X 1) consists of the predicate  X   X  X  X  X   X  (enjoy) and the two arguments: case component  X   X   X ( acc )and X   X   X ( loc ).

Basically, a verb, adjective, and noun+copula are regarded as a predicate, and case ment has a modified phrase, the attribute [mod] is assigned.

Predicate-argument structure can have another predicate-argument structure that has the same meaning but has the different case structure. For example, predicate-argument structure (2 X 1) is an original one, and predicate-argument structure (2 X 2) is another predicate-argument structure that has the same meaning as (2 X 1). (In this case, while (2 X 1) has a passive form, (2 X 2) has an active form.)
KNP makes a case analysis using case frames, which are automatically acquired from a large Web corpus [Kawahara and Kurohashi 2006a]. In the case frames, the case alignment of two case frames, such as active and passive, is performed. For exam-and the  X   X   X ( acc ) of the case frame  X   X  X  X   X  (hold) is aligned. By using this alignment, predicate-argument structure as (2 X 2) in Figure 4 can be generated.

In addition, the following are regarded as a predicate-argument structure.  X  Deverbative noun: (4)  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X   X   X  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X 
The deverbative noun  X   X  X   X  (suicide) is regarded as a predicate, and the following predicate-argument structure is generated.  X  Apposition: (5)  X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X  X   X  X  X  X  X   X  X  X 
By KNP,  X   X  X  X   X  (priest) and  X   X  X  X  X  X  X  X  X  X  X   X  (St. Valentine) are recognized as an apposition relation, and the following predicate-argument structure is generated.
The representation for both predicate and argument is handled by a surface form. If a word has a synonym, the attribute [syn] whose value is its SYNID is added. For ex-ample, the word  X  structure (1-1) has the attribute [syn] whose value is  X  X  X  (season). Similarly, if a word has a hypernym, the attribute [is-a] whose value is its SYNID is added.

If a verb has negation expression, the negation flag is attached to the verb. For example, the verb  X   X  X  X  X  X   X  (don X  X  write) has the negation flag.

As for verbs which represent affirmation/negation for the depending clause, no predicate-argument structure is generated for the verb. Verbs which represent affir-mation/negation are manually prepared as follows.  X  Affirmation:  X  Negation:
As for verbs representing negation, the negation flag of the verb in the depending clause is reversed. In the following example, since  X   X  X  X  X   X  (misunderstanding) corre-sponds to a verb representing negation, the negation flag is attached to the verb  X   X  X   X  (fall down). (6)  X  X  X   X  X  X  X  X   X  X  X  X  X  X  X  X  Some entailment relations are caused by numerical expressions. Numerical expres-sions are handled in a different way from ordinal predicate-argument structure intro-duced in the previous section. A predicate is represented by a tuple: number, counter, and class, and a class corresponds to exact , over ,or less . An argument takes only the  X  ( nom ) case component. (7)  X  X  X  X   X  X  X  X  100  X  X  X  X  X  In the preceding sentence, the following predicate-argument structure is generated. (8)  X  X  X  X   X  X  X  X  130  X  X  X  X  X  X  X  X 
In the preceding sentence, the following predicate-argument structure is generated.
As for predicates that take numerical expressions as an argument, they are treated in the same way as the above example. (9)  X  X  X  X  X  X  X  3  X  X  X  X   X  X  X  X  X 
In the preceding sentence, the following predicate-argument structure is generated. Such predicates are shown in Table IV.

Numerical expressions are made some normalizations as follows.  X 2  X  (ten thousand) 5000  X  25000  X 5  X  (ten percent)  X  50%. Based on predicate-argument structures of T and H , the entailment judgement is performed by matching T and H considering a predicate-argument structure as a ba-sic unit. If all the predicate-argument structures in an H are matched to predicate-argument structures in a T , H is judged to be entailed from T , as shown in Figure 5. If a predicate-argument in H is equal or  X  X eneral X  compared to a predicate-argument T , the entailment relation is identified. If there is something referred only in H ,the entailment relation is not identified.
 The entailment of predicate-argument structures is defined as follows (Figure 6). argument structure in T (as shown in the top of Figure 6), that is, the predicate and all the arguments in H are matched to those in T , where this match includes the cor-respondence of surface form, the correspondence of SYNID (which means synonymous relation), and the distributional similarity is greater than a threshold 6 .
When arguments or predicates in H have is-a relation compared to those in T ,the entailment relation is also identified (as shown in the bottom of Figure 6). The is-a relation of predicates/arguments is defined as follows. is-a relation of predicates is-a relation of arguments
If all the arguments are identical and the negation flag in a predicate is not identi-cal, this is judged as  X  X  X  (Contradiction). Entailment relation between numerical expressions is handled in a different way. The entailment relation is defined as shown in Table V, which is classified in terms of the class of number ( exact , over , less )in T and H , and the magnitude relation between number of T ( num T ) and number of H ( num H ). For example, when the sentence 3.2 is a text and the sentence 3.2 is a hypothesis, the relation is judged as  X  X  X  (corresponds to (4) in Table V). Although the method of entailment judgement introduced in the previous section is that precise matching cannot be achieved due to the gaps in the structure, parsing To consider relatively shallow clues such as the overlap ratio of characters and mor-phemes, we take a machine learning approach where these clues as well as the result of PA-matching method are considered as a feature. SVM (Support Vector Machine) is adopted as a machine learning method, and the following features are considered.  X  The overlap ratio of morphemes between T and H .  X  The overlap ratio of characters between T and H .  X  1-gram, 2-gram, 3-gram, 4-gram.  X  The result of PA-matching method.  X  There is a correspondence of predicate.  X  There is an inconsistent of negation flag in a predicate of T and H .

In the MC task, the SVM handles the 3-class classification problem (Y,C,N), and the one-vs-rest method is adopted. In the classification step, the trained SVM model is applied to the pair of T and H and the pair of H and T , and then the result is classified into five class (B, F, R, C, I) based on the SVM results, as shown in Table VI. Our proposed method was evaluated using Japanese BC, MC, EXAM, and RITE4QA subtasks of RITE in NTCIR-9 [Shima et al. 2011]. While the development set was prepared for BC, MC, and EXAM, it was not prepared for RITE4QA, and thus our method for BC was applied to RITE4QA test set. For the acquisition of relations between words/phrases described in Section 2.1, REIKAI-SHOGAKU dictionary (a dictionary for children), which consists of approxi-mately 30,000 entries, and Japanese Wikipedia were utilized. As Japanese Wikipedia, the XML dump of Japanese Wikipedia 7 at the version dated December 2011 was uti-lized, which contains approximately two million articles. From the dictionary, approx-imately 10,000 synonyms, 20,000 is-a relations, and 2,000 antonyms were extracted, and from Wikipedia, approximately 200,000 synonyms and 600,000 is-a relations were extracted.

In the distributional similarity calculation described in Section 2.2, approximately 100 million Japanese Web pages were used.

For the implementation of SVM, svm light 8 was adopted, and the linear kernel was used, where the default parameters were chosen. For the development set, the SVM-based method were evaluated on the five cross validation fold, and for the test set, SVM models were trained using all the development set data, and were applied to the test set.
 In EXAM and RITE4QA, the confidence score for each text pair was required in NTCIR-9 RITE task. In PA-matching method, it is 1.0 in the case of exact matching; otherwise 0.8. In SVM-based method, it is obtained by transforming SVM score x with the sigmoid function as follows: Table VII shows the accuracy of BC dev, BC test, MC dev, MC test, EXAM dev, EXAM test, and RITE4QA. This table includes the accuracy of our current system, the accu-racy of our system submitted to NTCIR-9 RITE task [Shibata and Kurohashi 2011], the best system in NTCIR-9 RITE (JAIST [Pham et al. 2011] for BC test, IBM [Tsuboi et al. 2011] for MC test and EXAM test, and KYOTO for RITE4QA), and the accu-racy of baseline (char overlap), which was provided by the organizers [Shima et al. 2011]. The current system and our system submitted to NTCIR-9 RITE task differ in that the current system performed (i) distributional similarity calculation for a vari-ety of types, (ii) modifications and scale-up in the acquisition of lexical resources, and (iii) small modifications in handling predicate-argument structure, etc.
In BC, the accuracy of PA-matching method and SVM-based method were almost the same. In MC, SVM-based method performed better than PA-matching method both in development set and test set. This is due to the effectiveness of the shallow features such as the overlap ratio of characters and morphemes. The coverage for label  X  X  X  is especially low in PA-matching method and SVM-based method, which depressed the overall accuracy (see also in the latter half of Section 6.3.2). In Exam, SVM-based method performed best in both development and test set. The number of dataset whose label is  X  X  X / X  X  X  are different (the number of dataset labeled as  X  X  X  is larger than la-beled as  X  X  X ). Since shallow features utilized in SVM-based method could capture this tendency, SVM-based method performed better than the PA-matching method. In RITE4QA, PA-matching method performed best, and also performed best in Table XV). Therefore, this just indicates that the recall is very low. Table VIII shows an evaluation result where other metrics (Top1 and MRR 9 ) are utilized. The score of PA-matching Method and SVM-based Method was lower compared to the best system (LTI) in NTCIR-9 RITE.

Although the best accuracy of our system for BC test, MC test, and EXAM took the different values, these are almost the same or performed slightly better compared to the baseline method (simply using char overlap). This implies that our performance was not different for each subtask.

Tables IX, X, XI, XII, XIII, XIV, and XV show a confusion matrix of BC dev, BC test, MC dev, and MC test, EXAM dev, EXAM test, RITE4QA, respectively. Confu-sion matrices show that the coverage (the ratio of our output labeled as  X  X  X  in BC, or  X  X  X / X  X  X / X  X  X  in MC) of PA-matching method was low, but the precision was relatively high (except for the BC test). In SVM-based method, the coverage was improved in all the cases due to the effectiveness of shallow features.

Table XVI shows the result of one of the leave out experiments. For the PA-matching method, the method that does not use Wikipedia in SynGraph, the method that does not use SynGraph (which means this method does not utilize synonyms and is-a re-lations extracted in a dictionary and Wikipedia), and the method that does not use distributional similarity were tested. While for BC dev, MC dev, EXAM dev, EXAM test, these two resources were effective, for the others, they were not effective. Since the evaluation sets often contain several gaping, if one or more of the gaping were matched, the accuracy would not be improved, and if all the gaping were matched, the accuracy would be improved. Table XVII shows the number of PAs in H matched with one of PA in T . This table demonstrates the number of PAs in H matched with one of PA in T was improved by using SynGraph and distributional similarity, and especially Wikipedia and distributional similarity contribute to the improvement of the coverage (of course, the matched PA contain both correct and incorrect ones). Although the accu-racy in one leave out experiment remains almost the same, the coverage (the number of matched PA) was improved using SynGraph and distributional similarity.
In Table XVI, for SVM-based method, the method that does not use the PA-matching method feature was tested. While for BC dev, MC dev, MC test, EXAM dev and RITE4QA, it is effective, for the others, it was not effective. This is because the ac-curacy of PA-matching method is not so good.
 the synonymous relation  X   X  X  X   X  (difference) and  X   X   X  (difference), which was acquired from a dictionary. (10) T:  X  X  X  X  X   X  X  X  X   X  X  X  X   X  X  X   X  distributional similarity between  X   X  X  X   X  (open) and  X   X  X  X  X  X   X  (open) is high. (11) T:  X  X  X  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X   X  X  X  distributional similarity between  X   X  X  X   X  (space) and  X   X  X  X  X  X  X  (outer space) is high. (12) T:  X  X  X  X  X  X   X  X  X   X  X  X   X  X  X  X  X  X   X  X  X  X  X  X  X  6.3.1. False Positives. The following is an example in which the PA-matching method incorrectly judged as  X  X  X . Although with the is-a relation between  X   X  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X   X  (six-and-over-story-gen newly built condominium) and  X   X  X  X  X  X  X  X  X  X  X   X  (newly built condominium), the system judged  X  X  X ,  X  6  X   X   X   X   X  (six-and-over-story-gen ) works as a condition for  X   X  X  X   X  X  X  X  X  X  X   X  (newly built condominium) X , and thus the correct answer was  X  X  X . To recognize such conditions is our future work. (13) T:  X  X  X  X  X  X  X  X   X  X  X  X  X  X   X  X  X  X  X  X  X  X  X  X  98  X  X  X 
The following is also an example in which the PA-matching method incorrectly judged as  X  X  X . Handling quotations and metaphorical expressions is our future work. (14) T:  X  X  X  X  X  X  X  X  X  X  X  X  X  X   X  X  X  X   X  X  X  X  40  X  X  X   X  X  X  X   X  X  X  X  X  6.3.2. False Negatives. The followings are examples where our system could not rec-ognize the synonymous/entailment/implication relations that cause false negatives.  X  Synonym:  X  Between noun/noun phrases:  X  Between predicate-arguments:  X  Entailment/implication:  X  Between noun/noun phrases:  X  Between predicate-arguments:  X  Require world knowledge: current Natural Language Processing technique, the acquisition of knowledge about synonymous/entailment relations on noun/predicate/predicate-argument is our future work.

In the MC task, the accuracy of contradiction recognition was relatively low in both in the PA-matching method and the SVM-based Method. This is due to the fact we can recognize contradictions only when the mismatch of negation flag or numerical expressions occurred. Although the coverage of SVM-based method for  X  X  X / X  X  X / X  X  X  was improved compared to PA-matching Method, the coverage of SVM-based method for  X  X  X  was 0. This shows that the contradiction relation could not be detected by using the shallow features. The acquisition of knowledge about other kinds of expressions representing a contradiction is our future work. Most work for Recognizing Textual Entailment has taken a machine learning ap-proach. Ferres and Redriguez take a machine learning approach with support vector machines and AdaBoost [Ferr  X  es and Rodr  X   X guez 2007]. They first perform a lexical, syntactic, and semantic analysis, and compute a set of semantic-based distances be-tween sentences. Then they extract a set of features and train a classifier using several machine learning techniques. Hickl et al. take a classification-based approach [Hickl et al. 2006]. They perform several linguistic processing, including syntactic/semantic parsing and conference resolution, and lexical alignment. They also acquire a large col-lection of paraphrases from the Web and extract features from them. Then, they per-form the entailment classification using obtained features using decision trees. Wang and Neumann [2007] exploit a structure-oriented representation. The structural fea-tures are automatically acquired from tree skeletons, which are extracted from de-pendency trees. Although these research approaches utilize structural features, our proposed method differs from them in that we perform the matching between a text and a hypothesis based on predicate-argument structures.

The work presented by Wang and Zhang [2009] is the most similar to our proposed method, who propose an alignment model based on the predicate-argument structures. They first obtain predicate-argument structures for a text and hypothesis using a se-mantic role labeler. They calculate semantic relatedness between predicate-argument structures. In this calculation, string matching and lexical resources such as Word-Net and VerbOcean are utilized. Our proposed method differs from them in that they calculate the relatedness (score) between a text and hypothesis, but we perform the alignment between the text and hypothesis. Furthermore, they are different in that they do not perform predicate-argument transformation such as active/passive and the special handling for numerical expressions.
 WordNet, DIRT, VerbOcean, etc. DIRT is a database of paraphrases, which are ob-tained by calculating two paths in a dependency tree [Lin and Pantel 2001]. VerbOcean is a broad-coverage semantic network of verbs [Chklovski and Pantel 2004], where the relation between verbs includes similarity, strength, antonymy, enablement, and tem-poral happens-before. In Japanese, Matsuyoshi et al. [2008] construct a database of re-lations between predicate argument structures. They first collect argument structures and logical relations, such as near synonym and antonym, from definition sentences in a dictionary. Then they augment the relations with a thesaurus. This article presented a predicate-argument structure based Textual Entailment Recognition method. Our system regarded predicate-argument structure as a basic ing between the text and hypothesis. Wide-coverage relations between words/phrases pus, and Wikipedia, and were utilized when matching the text and hypothesis. We also took a machine learning approach to consider relatively shallow clues such as the overlap ratio of characters and morphemes as well as the result of the PA-matching method.

Experimental results on NTCIR-9 RITE demonstrated that while the PA-matching method achieved the relatively high precision, the SVM-based method achieved the highest coverage. Furthermore, wide-coverage lexical knowledge extracted from a dic-tionary, Wikipedia, and Web corpus could contribute to the improvement of coverage.
Our future work includes the further acquisition of linguistic knowledge concern-ing predicate/argument/predicate-arguments and expressions representing a contra-diction, and the flexible matching between structures of a text and hypothesis. We are also planning to compare our proposed method with the existing machine learning approaches using this dataset.

