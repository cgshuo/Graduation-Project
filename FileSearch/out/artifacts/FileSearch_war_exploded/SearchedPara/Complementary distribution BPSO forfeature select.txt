
Institute of Biotechnology and Chemical Engin eering, I-Shou University, Kaohsiung, Taiwan
Department of Network Systems, Toko University, Chiayi, Taiwan Department of Electronic Engineering, National Kaohsiung University of Applied Sciences, Kaohsiung, Taiwan 1. Introduction
The feature selection process constitutes a commonl y encountered problem of global combinatorial selection improves the predictive accuracy of algorithms by re ducing the dimensi onality, removing irrelevant features, and reducing the amount of data needed for the learning process.

Currently, there are many algorithms for feature selection, such as pattern recognition [13], data mining [26,29], multimedia information retrieval [23,40], structure-activity correlation [1], and other areas where feature selection can be applied. Consequently, feature selection has to be necessarily used 15]. Two general approaches of feature selection are the fi lter methods and wrapper methods. Filter wrapper methods incorporate classi fi cation algorithms to search for and select relevant features [20]. The wrapper methods generally outperform fi lter methods in terms of classi fi cation accuracy [43]. The CD-BPSO introduced in this paper belongs to the wrapper method. A high number of selected features In some cases, the performance of algorithms devoted to speed and predictive accuracy of the data characterization can even decrease because features may be irrelevant or misleading, or due to spurious ef fi ciency and classi fi cation accuracy.

The most basic solution to fi nding minimal reductions is to generate all possible reductions and choose the ones with minimal cardinality. However, Skowron et al. have shown that fi nding minimal reductions or all reductions are both NP-hard problems [35]. Therefore, evolutional approaches have to be considered for feature selection. Several methods have previously been used to perform feature selection of training and testing data, such as genetic algorithms [32], branch and bound algorithms [4, 41], sequential search algorithms [31], mutual information [2], neural networks [5], tabu search [42], genetic algorithms, hybrid genetic algorithms [28] and binary particle swarm optimization [3,36,38]. GAs demonstrate the ability to r each near-optimal solutions for lar ge problems. However, they may require a rather long processing time to reach a near-optimal solution [11]. Similarly to GAs, BPSO is an optimizer based on a population. BPSO has a memory, so knowledge of good solutions is retained by all the particles. BPSO arrived at an optimal solution by the swarm following the best particle. In contrast to GAs, BPSO does not incorporate crossover and mutation processes. It has a much more profound intelligent background and can be performed more easily. Based on these advantages, BPSO is not only of interest in scienti fi c research, but also for practical engineering applications [34]. method [8,10], backpropagation neural networks [27], and support vector machines [37]. Methods successfully applied to multiclass classi fi cation include support vector machines: (1) one-versus-rest and one-versus-one [21], (2) DAGSVM [30], (3) the method by Weston and Watkins [6,39], and (4) the method by Crammer and Singer [6,9]. Generally speaking, solving multiclass classi fi cation problems is not as trivial as solving binary ones.

The present study proposes a novel optimization algorithm for feature selection called complementary distribution binary particle swarm optimization (CD-BPSO). In CD-BPSO, we employ the strategy of Complementary particles are introduced into the search space if the fi tness of gbest has not improved (i.e., is unchanged) after a number of consecutive iterations. In this study, the K-nearest neighbor method (K-NN) with leave-one-out cross-validation (LOOCV) based on Euclidean distance calculations it either obtains a higher classi fi cation accuracy or uses fewer features than BPSO and other feature selection methods.
 2. Methods 2.1. Binary particle swarm optimization ( BPSO )
PSO is a population basedoptimization tool that was originally proposed by Kennedyand Eberhart [17] as an optimization technique for real-number spaces. In PSO, each particle is analogous to an individual The process of PSO is initialized w ith a population of random particles. Then the algorithm searches for optimal solutions by continuously updating generations. Each particle makes use of its own memory and the knowledge gained by the swarm as a whole to fi nd the best solution. The position of the ith particle can be represented by x v = ( v its individual best position p individual pbest each generation, the position and velocity of the ith particle are updated by pbest swarm.

However, many optimization problems can occur in a space featuring discrete, qualitative distinctions between variables and between levels of variables. For this reason, Kennedy and Eberhart introduced binary PSO (BPSO), which can be applied to discrete binary variables [19]. In a binary space, a particle may move to near corners of a hypercube by fl ipping various numbers of bits; thus, the overall particle velocity may be described by the number of bits changed per iteration [19]. In BPSO, each particle is updated based on the following equations: constants, which control how far a particle will move in a single generation. Velocities v new denote the velocities of the new and old particle, respectively. x old x to a maximum velocity V max . If the sum of accelerations causes the velocity of that dimension to the function S ( v new (meaning this position is selected for the next update). If S ( v new value is represented by { 0 } (meaning this position is not sel ected for the next update) [19]. 2.2. Complementary distribution binary particle swarm optimization ( CD-BPSO )
The principal task of the complementary strategy is to assist particles trapped in a local optimum to move on to a new region of the search space, and thus to ensure global exploration. Like the particle in original BPSO, the complementary particles of CD-BPSO can be represented in binary string form. BPSO is naturally applicable to feature selection since the problem has an exponential search space. The pioneering work by Wang et al. demonstrated evidence for the superiority of BPSO compared to representative classical algorithms [38]. Subsequently, many literature reports were published that have shown advantages of PSOs for feature selection [3,36,38]. In a particle, each dimension represents which the trapped particles is changed based on the following complementary rules: In Eq. (5), P is the population size, and D is the dimension of the particle. During the complementary process, every dimension of the particle i ( x the dimension of the particle i ( x
In the original BPSO, particles can get trapped in a local optimum; premature convergence is the principal reason for this entrapment. In CD-B PSO, a particle swarm is r andomly initia lized in a fi rst step with the particles being distributed over a D-dimensional search space. Each particle is updated in the population. The pos ition and velocity of each particle ar e updated by Eqs (1) X (4). However, according to Eqs (1) X (4), each particle clusters around gbest and only moves a very small distance in the next generation if the distance between gbest and the surrounding particles is small. The complementary particles are introduced in order to avoid premature convergence. These new particles replace 50% of the original particles randomly selected from the swarm. The complementary particles are essential for the success of a given optimization task.

Figure 1 plots a simple instance of the complementary strategy. In (Fig. 1-a), we assume that gbest is not updated for three generation, and subsequently half of the population is selected as S .The c) and (Fig. 1-d) show how the complementary particles continue the search in new regions of the search space and how they may fi nd better solutions. The population in its entirety may arrive at more optimal dif fi cult to achieve during the initialization process of BPSO.

The complementary BPSO strategy in this paper is easy to implement and can be executed without increasing the computational complexity of the process. Complementary particles overcome the inherent weakness (premature convergence) of BPSO by initializing a new global search over the entire search space from the complementary positions at which they are introduced. 2.3. K-nearest neighbor
The K-nearest neighbor (K-NN) method is one of the most popular nonparametric methods [8,12] used K-nearest neighbor category. Classi fi ers do not use any model for K-nearest neighbors and results are determined solely based on the minimum distance from the query instance to the training samples. Any tied results are solved by a random procedure.

The advantage of the K-NN method is that it is simple and easy to implement. K-NN is not negatively affected when training data are large and is furthermore indifferent to noisy training data [8]. In this study, the feature subset was measured by the leave-one-out cross-validation (LOOCV) of one nearest neighbor (1-NN). Neighbors are calculated using their Euclidean distance. The 1-NN classi fi er does not the LOOCV method, a single observation from the original sample is selected as the validation data and validation where K is equal to the number of observations in the original sample. The pseudo-codes for CD-BPSO and 1-NN are shown below.
 2.4. Parameter settings In our experiment, identical parameters were used to compare the performance of SGA [28], HGAs [28], BPSO and CD-BPSO algorithms, i.e., population size = 20 [28], and K = 1 [28] in the K-Nearest Neighbor part. The other parameters of CD-BPSO were set as follows: number of generations = 100 (maximum number of iterations), w = 1.0 [18], and c1 = c2 = 2 [18].
 3. Experimental results and discussion
In order to investigate the effectiveness and performance of the CD-BPSO algorithm for classi fi cation problems, ten classi fi cation problems obtained fromthe UCIRepository were tested in ourexperiment[4]. sized groups, were tested. If the number of features was between 10 and 19, the sample group was be considered small; the Glass, Vowel, Wine, Letter, Vehicle, and Segmentation problems constitute such small sample groups. If the number of features was b etween 20 and 49, the sample group test problems were of medium size. Groups in this category incl ude the WDBC, Ionosphere , and Satellite problems. If the number of features was greater than 50, the test problems were large sample group problems; the Sonar problem falls into this group. Furthermore, in order to compare our result to the results published in HGAs [28] under identical condition, the 1-NN method with leave-one-out cross-validation (LOOCV) was used to evaluate all data sets. Employing the 1-NN method with LOOCV as a fi tness function for CD-BPSO has two distinct advantages; fi rstly, the calculation time can be decreased, and secondly, higher classi fi cation accuracy can be obtained. 4. Experimental results
In Table 2, the classi fi cation accuracy and the number of selected features is shown for the data sets tested with BPSO, CD-BPSO and other feature selec tion methods from the literature [28]. In this paper, that Oh et al. used in their study [28]. In Table 2, the total number of features (db) and A(%) (average accuracyvalue)for BPSO and CD-BPSO is given. CD-BPSO obtained the highest classi fi cationaccuracy for all classi fi cation problems. The classi fi cation accuracies for the Wine, Segmentation and WDBC classi fi cation problems obtained with CD-BPSO are 99.44%, 97.92% and 98.24%, respectively. As write the exception of BPSO. This increase in accuracy was achieved despite the fact that the number of with CD-BPSO for the Glass and Vowel classi fi cation problems were the same as the accuracies obtained by the other feature selection methods. Yet, CD-BPSO in the Glass classi fi cation problem achieved this accuracy with fewer features selected. The overall results indicate that CD-BPSO can serve as a valuable preprocessing tool, which optimizes the feature selection process and improves classi fi cation accuracy.
Figure 2 plots the number of generations versus the accuracy and the number of features selected for the ten data sets. For the Glass data set (Fig. 2-a), the proposed method obtained 100% classi fi cation accuracy before reaching the maximum number of iterations. The fi gure shows that the number of features selected converges at an early stage; however classi fi cation accuracy keeps improving and if the total number of selected features stays unchanged. This means that a good feature selection method not only decreases the number of features, but also selects features relevant for improving while the number of iterations keeps increasing, until the purposed method obtained 100% classi fi cation accuracy. Figures (2-a) to (2-j) demonstrate the reciprocal relationship between the achieved accuracy fact that proves that classi fi cation accuracy does not have to be negatively affected by a lower number of selected features. As long as the chosen features contain enough feature classi fi cation information, higher classi fi cation accuracy can be achieved. 5. Discussion
Figure 3 plots the number of generations versus the accuracy of BPSO and CD-BPSO for the ten data sets. The classi fi cation accuracy curve of BPSO usually remains unchanged after three consecutive iterations. This means that BPSO is stuck in a local optimal. However, the classi fi cation accuracy achieved by BPSO can further be improved when complementary distributed particles are introduced (CD-BPSO). Furthermore, it can be observed in Fig. 3 that the curve of CD-BPSO displays step shape. This shape indicates that CD-BPSO has the potential of breaking through a local optimum.

The classi fi cation accuracies were measured by the four values D/5, 2D/5, 3D/5, and 4D/5 of the total number of features D in Oh et al. [28]. The optimal number of features for each test problem is however unknown. The method of Oh et al. is time-consuming compared to the method we propose here, in which an optimal number of features is determined by CD-BPSO. Even if the accuracy values obtained by CD-BPSO cannot be directly compared to the method of Oh et al., it seems safe to imply that they proposed method was the same as the one obtained in Oh et al. while the number of features selected was lower. For the Vowel test problem, the classi fi cation accuracy obtained by the proposed method was the same as the one obtained in Oh et al. while the number of features selected was also same. For the test problems Wine, Letter, Vehicle, Segmentation, WDBC, Ionosphere, Sate llite, and Sonar, classi fi cation accuracies obtained by the proposed method were higher than the classi fi cation accuracies obtained in Oh et al. [28], even though the number of features selected was actually lower.

The methods used for feature selection in Table 2 are described below. The sequential forward selection method (SFS) begins with a feature subset and sequentially adds or removes features until some termination criterion is met. SFS suffers from a nesting effect. Since the algorithms do not examine all possible feature subsets, there is no guarantee that they can produce an optimal result [42]. Furthermore, features that have been discarded cannot be re-selected, and the selected features cannot be removed at a later stage. The sequential backward selection method (SBS) is the backward analogue. backward r stages (by deleting r features via SBS) and then repeats this process [22]. PTA was proposed subset than the feature subset obtained so far at the same size [22]. However, SFFS suffers from getting trapped in local optimal solutions when applied to large-number features problems [42]. Simple genetic algorithms (SGA) have rather poor search ability in the near local optimum region, and the phenomenon of premature convergence in SGAs seems to be dif fi cult to avoid. Furthermore, parameters in SGA have to be carefully selected and tested [42].
 Table 3 showsthe time complexity of CD-BPSO compar ed to the other methods from the literature [22]. for a multiplicative constant), and O( ) denotes an estimate of complexity for which only an upper bound is known. Time complexities under a typical setting of parameters are shown in parentheses in the Table 3. These time complexities are only an indication when used in these algorithms [22]. Search types can be divided into sequential and parallel searches.
 In Oh et al. [28], the computa tional complexity for SGA and HGAs can be derived as O(PG) and O(PG+LS) respectively, where P is the population size, G is the number of generations, and LS is the local search operation in HGAs. The computational complexity of th e CD-BPSO method can be derived as O(PG). Consequently, CD-BPSO is more ef fi cient than either SGA or HGAs. 6. Statistical analysis
To investigate the statistical robustness of CD-BPSO, its classi fi cation accuracies were compared to the Friedman test and the multiple comparison approach [7]. The Friedman test was used to test whether 7. Friedman test
The Friedman test is a nonparametric counterpart of the parametric two-way analysis of variance test of the underlying population was not speci fi ed. The hypothesis being tested was that all the methods had equal classi fi cation accuracy. The alternative hypothesis was that all methods did not have equal classi fi cation accuracy.

Let R following equations: where methods had the same classi fi cation accuracy at a signi fi cance level of  X  = 0.05. 8. Multiple comparison approach
The multiple comparison approach was used to determine which method had a signi fi cantly different where R ( n were ordered in an array, and a rank was assigned to each corresponding value according to its order. The rank sums of CD-BPSO, BPSO, HGA(4), HGA(3), HGA(2), HGA(1), SGA, SFFS, PTA, and SFS two methods are more than 16.91 units apart (  X  = 0.05), they may be regarded as having an unequal prediction accuracy. It can be concluded that the CD-BPSO method is statistically superior to the BPSO, HGA(4), HGA(3), HGA(2), HGA(1), SGA, SFFS, PTA, and SFS methods for the data sets tested. 9. Conclusions
In this study, complementary distribution binary particle swarm optimization (CD-BPSO) is applied to perform feature selection and improve the performance of BPSO. The K-NN method with LOOCV served as an evaluator of the CD-BPSO fi tness functions. In CD-BPSO, complementary particles initialize a new search from the complementary positio n of the 50% of particles randomly selected from the swarm when the gbest value stays unchanged for three numbers of iterations. Better solutions can be found via global exploration by guiding the entire particle swarm to more promising regions in the search space. The experimental results show that CD-BPSO simpli fi es feature selection by effectively reducing the total number of necessarily selected features, and by obtaining the highest accuracy values in nine of the ten data set test problems compared to BPSO and other feature selection methods from the literature. CD-BPSO can be used as a preprocessing tool to optimize the feature selection process. From a standpoint of computational complexity and classi fi cation accuracy, it can be concluded that the proposed CD-BPSO method is more ef fi cient than BPSO and the other methods it was compared to. CD-BPSO could conceivably be applied to problems in other areas in the future.
 References
