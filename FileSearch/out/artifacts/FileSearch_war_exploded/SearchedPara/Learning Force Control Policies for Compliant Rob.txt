 Mrinal Kalakrishnan  X  kalakris@usc.edu Ludovic Righetti  X  X  ludovic.righetti@a3.epfl.ch Peter Pastor  X  pastorsa@usc.edu Stefan Schaal  X  X  sschaal@usc.edu Developing robots capable of fine manipulation skills is of major importance in order to build truly assistive robots. These robots need to be compliant in their actuation and control in order to operate safely in hu-man environments. Manipulation tasks imply com-plex contact interactions with an unstructured envi-ronment, and involve reasoning about the forces and torques to be applied. In order for robots to co-exist in an environment with humans, safety is a prime con-sideration. Therefore, touching and manipulating an unstructured world requires a certain level of compli-ance while achieving the intended tasks accurately. Methods for planning kinematic trajectories for ma-nipulators are well-studied and widely used. Rigid body dynamics models even allow us to plan trajecto-ries that take the robot dynamics into account. How-ever, once the robot comes into contact with the en-vironment, planning algorithms would require precise dynamics models of the resulting contact interactions. These models are usually unavailable, or so imprecise that the generated plans are unusable. This seems to suggest alternate solutions that can learn these manip-ulation skills through trial and error.
 In this abstract, we present an approach to learning manipulation tasks on compliant robots through re-inforcement learning. We demonstrate our approach on two different manipulation tasks: opening a door with a lever door handle, and picking up a pen off the table (Fig. 1). We show that our approach can learn the force control policies required to achieve both tasks successfully. The contributions of this work are two-fold: (1) we demonstrate that learning force con-trol policies enables compliant execution of manipu-lation tasks with increased robustness as opposed to stiff position control, and (2) we introduce a policy pa-required forces and torques cannot be observed dur-ing this process, and are initialized to zero. The poli-cies are then optimized using the P olicy I mprovement with P ath I ntegrals ( PI 2 ) reinforcement learning al-gorithm (Theodorou et al., 2010). This allows acquisi-tion of a suitable force/torque control policy through trial and error. Policy performance is measured by a cost function that measures task success and penal-izes squared accelerations of the trajectory. The latter part of the cost function is designed to be quadratic in the trajectory parameters. This specific choice of con-trol cost in conjunction with the PI 2 algorithm allows for generation of smooth trajectories for exploration that do not deviate from the start or goal points, and ensures smoothness of the trajectory after every iter-ation. The algorithm samples trajectories around the current policy, measures their cost by executing them on the robot, and subsequently updates the policy as a weighted average of the samples. This process is it-erated until convergence. More details may be found in the full paper (Kalakrishnan et al., 2011). The combined position/force trajectory needs to be controlled by the robot in a suitable way during policy execution. PI 2 , being a model-free reinforcement learn-ing algorithm, is indeed agnostic to the type of con-troller used. It simply optimizes the policy parameters to improve the resulting cost function, treating the in-termediate controllers and unmodeled system dynam-ics as a black box. Our approach was verified using two different manipu-lation tasks: opening a door and picking up a pen lying on a table. These tasks were chosen because each one involves significant contact with the environment, and are thus appropriate test-beds for the use of force con-trol policies. The learning process and final executions of both tasks are shown in the cited video (vid). Both tasks were performed on the 7 degree of freedom Bar-rett WAM arm, equipped with a three-fingered Barrett Hand and a 6-DOF force-torque sensor at the wrist. Success in the door-opening task was measured by at-taching an inertial measurement unit to the door han-dle to verify that the handle was turned and the door was indeed opened. For the pen-grasping task, suc-cess was quantified by measuring the amount of time the pen stayed within the fingers of the robot without slipping out. Both tasks were learnt in around 100 trials, and their final policies achieved 100% success. Fig. 3 shows the evolution of cost functions for each task during learning. The force control policy learnt for the pen-grasping task is shown in Fig. 4. This pol-icy was also found to be robust to errors in the position and orientation of the pen.
