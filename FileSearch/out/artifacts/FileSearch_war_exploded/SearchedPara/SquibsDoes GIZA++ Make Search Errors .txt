 University of Southern California University of Southern California been implemented in the GIZA (Al-Onaizan et al. 1999) and GIZA++ (Och and Ney 2003) alignments that are sub-optimal according to a trained model. 1. Background Word alignment is the problem of annotating a bilingual text with links connecting words that have the same meanings. Brown et al. (1993) align an English/French sen-and word alignment a : e :NULL 0 Mary 1 did 2 not 3 slap 4 the 5 green 6 witch 7 f :Mary 1 no 2 di  X  o 3 una 4 bofetada 5 a 6 la 7 bruja 8 verde 9 a : [134550576] to-one, meaning that each English word can produce several French children, but each 2. IBM Model 3 positing this generative story (IBM Model 3):
Given an English sentence e 1 ... e l : 3. Choose a number  X  0 of  X  X purious X  French words, by doing the following 4. Let m = m +  X  0 . 5. Choose a French translation  X  ik for each English word (including e 0 )and 7. Place each of the  X  0 spuriously generated words, one by one, into vacant 9. Output alignment a 1 ... a m , where a j is the English position i from that mind, Brown et al. provide the following formula: mined by the alignment a 1 ... a m . 296 3. Finding the Viterbi Alignment in Equation 1. Brown et al. were unable to discover a polynomial time algorithm for this problem, which was in fact subsequently shown to be NP-complete (Udupa and after which it greedily executes small changes to the alignment structure, gradually a is changed, and swaps , in which a pair a j and a k exchange values. At each step in the greedy search, all possible moves and swaps are considered, and the one which search halts. 2 this, we built a slow but optimal IBM Model 3 aligner, by casting the problem in the set up the following variables: one. This means each French token has exactly one alignment link, as required by IBM 1  X  fert the Viterbi IBM Model 3 alignment. Figure 1 presents the components of the objective ity tables n , d , t ,and p .
 demonstrates this for a simple example. The reader can extract the optimal alignment from the alignment variables chosen in the ILP solution. 4. Experiments For Chinese/English experiments, we run GIZA++ training on 101,880 sentence pairs.
In tests, we compare GIZA++ Viterbi alignments (based on greedy hill-climbing) with optimal ILP alignments. We use CPLEX to solve our ILP problems. 298 alignments. We measure the rate at which GIZA++ makes search errors, and we com-to an overall loss in alignment task accuracy, as measured by F-score. erage difference in log model scores between GIZA++ and ILP Viterbi alignments in 300 gap between GIZA++ and ILP scores. As we move to longer sentences, the alignment procedure becomes harder and GIZA++ makes more errors. Finally, Figure 5 plots the as well. 5. Discussion We have determined that GIZA++ makes few search errors, despite the heuristic nature aspect of SMT systems.
 corpus given the English side. For Model 3 and above, Brown et al. collect parameter alignments would yield better counts and better overall parameter values. Of course, optimum, which provides further opportunity for search errors. We leave the problem of search errors in alignment training to future study.
 Acknowledgments References
