 Non-negative Matrix Factorization (NMF, [5]) and Prob-abilistic Latent Semantic Analysis (PLSA, [4]) have been successfully applied to a number of text analysis tasks such as document clustering. Despite their different inspirations, both methods are instances of multinomial PCA [1]. We further explore this relationship and first show that PLSA solves the problem of NMF with KL divergence, and then explore the implications of this relationship.
 H.3.3 [ Information Search and Retrieval ]: Clustering; I.5.3 [ Clustering ]: Algorithms Algorithms Document clustering, probabilistic models, PLSA, NMF
Non-negative Matrix Factorization (NMF, [5]) decomposes a (positive) matrix V into a product of non-negative factors: It has been shown that NMF offers a number of advantages in various contexts, in particular when data must be decom-posed into a sum of additive components. NMF has been used for example to cluster (textual) documents [5, 8].
Probabilistic Latent Semantic Analysis (PLSA, [4]) is a model-based document clustering technique. In a collection of J documents indexed with I words, PLSA models the I  X  J term-document co-occurrence matrix M (where M ij is the number of occurrences of word w i in document d j ) as arising from a mixture model with K components:
Parameters are estimated by maximizing the likelihood of the observed data M .

Earlier studies [1] showed that NMF and PLSA are both instances of a more general model, and are therefore linked. In this short note, we show a formal equivalence between these methods, down to almost identical update rules, and discuss its implications.
Introducing the I  X  K matrix W 0 s.t. W 0 ic = P ( w i | c ) P ( c ), and the K  X  J matrix H 0 s.t. H 0 cj = P ( d j | c ), we can rewrite eq. 2 as [ P ( w i , d j )] = W 0 . H 0 . As probability matrices are obviously non-negative, PLSA corresponds to factorizing the joint probability matrix in non-negative factors.
Conversely, given the NMF formulation in eq. 1, assume without loss of generality that P ij V ij = 1 (otherwise V , W and H may be appropriately scaled). Let us introduce A and B , two K  X  K diagonal scaling matrices such that A where ( W . A  X  1 ) (resp. ( B  X  1 . H )) has all the formal prop-erties of the conditional probability matrix [ P ( w i | c )] (resp. In passing, we note that the NMF is invariant if we multiply W by a diagonal matrix and H by its inverse. The follow-ing property expresses a stricter relationship between PLSA and NMF.
 Property Any (local) maximum likelihood solution of PLSA is a solution of NMF with KL divergence.
 Proof. Without loss of generality, we assume that the posi-tive matrix to be decomposed is a co-occurrence matrix 1 M . Let V be the scaled matrix V ij = M ij /N , N = P ij M ij lutions of NMF with KL divergence are fixed points of the following update rules [5, 6]: The EM algorithm used to maximize the likelihood in PLSA iterates two steps, and solutions are fixed points of the fol-lowing equations [4]: Any positive matrix can be approximated, wrt the L 1 or L 2 norm, arbitrarily well as the product of a co-occurrence matrix and a scalar, the latter being just a scale factor in the NMF or PLSA decomposition. with P ( c | w i , d j ) ( t ) = P ( w i , c ) ( t ) P ( d Joining the equations for P ( c ) and P ( d j | c ), we obtain ing P ( c | w i , d j ) ( t ) and using the notation introduced above: Similarly, one obtains: W At any fixed point of the above equations, W 0 ( t +1) ic Thus, any fixed point of EM is also a fixed point of the update rules 3. 2
Using a similar fixed point argument, as well as the formal equivalence above, it is possible to show that the converse is true, that is: Any solution of NMF with KL divergence yields a (local) maximum likelihood solution of PLSA.
The relation we demonstrated in the previous section has a number of consequences. The main implication is that whenever a problem may be formulated with NMF (for ex-ample word alignment, [3]), it may be efficiently solved using PLSA. There are in fact many advantages to do so. First, PLSA is a probabilistic model which offers the convenience of the highly consistent probabilistic framework. Whereas the NMF factors are a set of values (with scale invariance issues, cf. below), the PLSA parameters may be interpreted as probabilities. We may for example evaluate the impor-tance of the factors ( P ( c )) and interpret each factor as a probabilistic profile ( P ( w | c ) or P ( d | c )). The probabilistic framework also comes in handy in some situations such as orthogonalizing the non-negative factors (for example [3]).
Regarding parameter estimation, the above development shows that the NMF update rules are essentially an EM al-gorithm. This means that they must be prone to the usual problems associated with EM such as sensitivity to local minima 2 , even though, to our knowledge, this is not widely acknowledged in the NMF literature. On the other hand, with probabilistic models, it is possible to benefit from the considerable body of work dedicated to addressing various shortcomings of EM. Such advances include the use of  X  X em-pered X  (or deterministic annealing) EM [4] in order to sta-bilize the parameter estimation, reduce the sensitivity to initial conditions, and even, to some extent, optimize the number of components in the mixture. A related aspect is the selection of a proper model structure. Many techniques have been proposed in the literature for assessing the num-ber of components in a mixture model [7]. In comparison,
Indeed, the property established before implies that NMF has at least as many local optima as PLSA. the NMF literature puts little emphasis on the choice of the correct number of factors.
 We noted in passing that a NMF is invariant if we multiply W and H by a square matrix and its inverse, respectively. This shows that there are infinitely many equivalent fac-torizations (in addition to the permutations of factors). In the vocabulary of mixture models, the NMF factors are not identifiable, whereas the PLSA model is. This has implica-tions on theoretical properties of the Maximum Likelihood estimator of PLSA, such as strong consistency [7]. In ad-dition, let us note that PLSA has hierarchical extensions [2]. These are especially convenient for document clustering (and categorization), because it is quite common to organize documents in hierarchies of classes. NMF offers no such ex-tension and is therefore limited to  X  X lat X  factors.
However, as NMF is expressed in terms of matrix approxi-mation, different results may be obtained by optimizing dif-ferent measures of the approximation. In fact, [5] derives update rules corresponding to a squared (Euclidean) approx-imation loss. We are not aware of any direct equivalence to this in terms of Maximum Likelihood estimation with PLSA, and we are currently working on adapting the PLSA estima-tion model to different losses (squared loss, absolute value, etc.). Finally, the relation we have exhibited also supports the results reported in document clustering using NMF (for example [8]). In particular, as PLSA has been designed to overcome some limitations of LSI, it is not surprising that NMF, being essentially equivalent, also outperforms LSI. This short note exhibits a remarkable relation between two common document clustering techniques, NMF and PLSA, and shows that PLSA solves NMF with KL divergence. This relation allows us to solve the NMF problem with a proba-bilistic mixture model for which various extensions and tech-nical advances have been proposed.
 We thank Fran  X cois Yvon for discussions related to this topic. [1] W. Buntine. Variational extensions to EM and [2] E. Gaussier, C. Goutte, K. Popat, and F. Chen. A [3] C. Goutte, K. Yamada, and E. Gaussier. Aligning [4] T. Hofmann. Probabilistic latent semantic analysis. In [5] D. D. Lee and H. S. Seung. Learning the parts of [6] D. D. Lee and H. S. Seung. Algorithms for non-negative [7] G. McLachlan and D. Peel. Finite Mixture Models . [8] W. Xu, X. Liu, and Y. Gong. Document clustering
