 OCR errors hurt retrieval performance to a great extent. Re-search has been done on modelling and correction of OCR errors. However, most of the existing systems use language dependent resources or training texts for studying the nature of errors. Not much research has been reported on improving retrieval performance from erroneous text when no training data is available. We propose a novel algorithm for detect-ing OCR errors and improving retrieval performance on an E-Discovery corpus. Our contribution is two-fold : (1) iden-tifying erroneous variants of query terms for improvement in retrieval performance, and (2) presenting a scope for a pos-sible error-modelling in the erroneous corpus where clean ground truth text is not available for comparison. Our algo-rithm does not use any training data or any language speci c resources like thesaurus. It also does not use any knowledge about the language except that the word delimiter is blank space. The proposed approach obtained statistically signi -cant improvements in recall over state-of-the-art baselines. H.3.3 [ Information Search and Retrieval ]: Clustering; Query formulation Algorithms, Legal Aspects Noisy Data, Co-occurrence, E-Discovery
Erroneous text collections have posed challenges to the researchers. Many such collections have been created and researchers have tried several error modelling and correcting techniques on them. The techniques involve training models on sample pairs of correct and erroneous variants. But such c  X 
The rest of the paper is organized as follows: we describe the proposed approach in Section 2, provide the results in Section 3, and conclude in Section 4. We rst describe two key terms used in our algorithm. Then we describe our algorithm.
We say that two words w 1 and w 2 co-occur if they appear in a window of size s ( s &gt; 0) words in the same document d . In our experiment we take the whole document as the window. Word co-occurrence gives a reliable measure of association between words as it re ects the degree of context match between the words. This association measure gets more strength when it is used in conjunction with a string matching measure. For example, two words with high string similarity are likely to be variants of each other if they share the same context as indicated by a high co-occurrence value between them. The word industrious is highly similar to industrial . But, they are not variants of each other. They can be easily segregated by examining their context match as they are unlikely to have a high co-occurrence frequency.
Given a sequence X =  X  x 1 , x 2 ,...., x m  X  , another sequence Z = all j = 1,2,..., k , we have x i j = z j . Now, given two sequences X and Y , we say that Z is a common subsequence of X and Y if Z is a subsequence of both X and Y . A common subsequence of X and Y that has the longest possible length is called a longest common subsequence or LCS of X and Y . For example, let X =  X  A; B; C; B; D; A; B  X  and Y =  X  B; D; C; A; B; A  X  . Then, the sequence  X  B; D; A; B  X  is an LCS of X and Y . Note that LCS of X and Y is not in general unique.

In our problem, we consider sequences of characters or strings. For strings industry and industrial , an LCS is in-dustr . Now, we de ne a similarity measure as follows :
LCS simil arity ( w 1 ; w 2 )
So, LCS simil arity ( industry ; industrial ) = = 0 : 7
Note that the value of LCS simi larity lies in the interval [0,1].
Our algorithm has two major parts : 1. Grouping, and 2. Binding
Now, given a query word w i , we need to nd the erroneous variants from the OCRed corpus. Let C = f Cl 1 , Cl 2 ,..., Cl k g be the set of all clusters formed from L w i (lexicon of the er-roneous corpus) by the clustering algorithm discussed in the last subsection. So, each cluster Cl j is of the form f ( w j 1 , wt word-weight pair in cluster Cl j . In the clusters of C , we look for the word that has the maximum LCS simil arity with w i . Let w closest 2 L w i be the word such that ter containing w closest . Then, we choose all the words in Cl closest as the erroneous variants of w i . If there are more than one such w closest having maximum similarity with w i , we do not choose any cluster.

A pictorial view of the algorithm is provided in Figure 2. For a Query Word , we get a Subset of Lexicon after ltering out from Lexicon based on an threshold. Co-occurrence Pairs' Block represents the repository of all the co-occurrence information between all the word pairs in the document collection. For the Subset of Lexicon , the Co-occurrence Pairs' Block is used to read the co-occurrence values for this subset of words and form the Graph . Next, the Graph is trimmed using the threshold and we get Trimmed Graph . Then, Trimmed Graph is clustered to get Clusters . After weight assignment , we get Clusters (weighted nodes) . Now, for the Query Word , we choose the appropriate cluster from Clusters (weighted nodes) . We call this process Bind-ing . The chosen cluster (if any cluster is chosen), along with the Query Word , forms the Expanded Query . The Expanded Query is then used for retrieval.
We run our experiments on the IIT CDIP 1.0 dataset cre-ated for TREC Legal track. The collection statistics are shown in Table 1. We use the 43 topics of TREC Legal Ad Hoc 2007. We use only the FinalQuery eld for our exper-iments. This query was prepared for Boolean retrieval and hence it contains Boolean operators like OR and AND . But, since we use it for ranked retrieval, we ignore the Boolean operators. We also convert the words in wildcard format like fertiliz!, phosphat!, agricultur! etc. to the shortest correct words like fertilize, phosphate, agriculture etc. respectively. The proximity operators like \w/15" are also dropped. We have compared our approach with two baselines:
T able 1: Collection statistics for IIT CDIP 1.0
In this paper we have proposed a new paradigm which has not been well explored -improving IR performance from erroneous text without the availability of training data or language-speci c resources. We have also proposed a novel algorithm to solve the problem on a large and extremely noisy E-Discovery corpus. The results show that we have achieved statistically signi cant improvements over the base-lines in recall. Also, we have shown that the use of context information is extremely bene cial and reliable in agglomer-ating semantically related erroneous variants. In addition, the automatic identi cation of error variants lays a sound platform for the modelling of error patterns for the text cor-pora where the clean text sample is not available for com-parison. We have seen that it is feasible to obtain useful error variants of the terms in the noisy corpus. This may be extremely useful in automatic cleaning of the IIT CDIP 1.0 corpus which, in its cleaned version, will be very important to the E-Discovery community. [1] A. Chakraborty, K. Ghosh, and U. Roy. A word
