 A standard approach to cross-language information retrieval (CLIR) uses Latent Semantic An alysis (LSA) in conjunction with a multilingual parallel aligned corpus. This approach has been shown to be successful in identifying similar documents across languages -or more precisely, retrieving the most similar document in one language to a query in another language. However, the approach has severe drawbacks when applied to a related task, that of clus tering documents  X  X anguage-independently X , so that documen ts about similar topics end up closest to one another in the semantic space regardless of their language. The problem is that documents are generally more similar to other documents in the same language than they are to documents in a different language , but on the same topic. As a result, when using multilingual LS A, documents will in practice cluster by language, not by topic. We propose a novel application of PARAFAC2 (which is a variant of PARAFAC, a multi-way generalization of the singular value decomposition [SVD]) to overc ome this problem. Instead of forming a single multilingual term-by-document matrix which, under LSA, is subjected to SVD, we form an irregular three-way array, each slice of which is a separate term-by-document matrix for a single language in the parallel corpus. The goal is to compute an SVD for each language such that V (the matrix of right singular vectors) is th e same across all languages. Effectively, PARAFAC2 imposes the constraint, not present in standard LSA, that the  X  X oncepts X  in all documents in the parallel corpus are the same regardless of language. Intuitively, this constraint makes sense, since the whole purpose of using a parallel corpus is that exactly the same concepts are expressed in the translations. We tested this approach by comparing the performance of PARAFAC2 with standard LSA in solving a particular CLIR problem. From our results, we conc lude that PARAFAC2 offers a very promising alternative to LSA not only for multilingual document clustering, but also for solving other problems in cross-language information retrieval. H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval  X  clustering, retrieval models. H.3.4 [ Information Storage and Retrieval ]: Systems and Software  X  performance evaluation (efficiency and effectiveness). Algorithms, Measurement, Design, Experimentation, Languages, Theory, Verification. Latent Semantic Analysis (LSA), information retrieval, multilingual, clustering, PARAFAC2. As the World Wide Web (WWW) has developed, content has become readily available in a multitude of languages, and interest has grown in the problem of cro ss-language information retrieval (CLIR) (see for example [21]). Ba sed on our own fairly informal survey (using Google and limiting results of a variety of queries by language), we believe that Fi gure 1 is a reasonable estimate of the distribution of internet content by language. Figure 1. Estimated WWW content, distribution by language Moves in various parts of the wo rld towards political integration are another significant driver for the interest in CLIR. Nowhere is this more evident than in the Eu ropean Union (EU), where official documents are created, and must be managed, in an ever-increasing number of languages. Indeed, the EU has funded a significant amount of research in recent years into CLIR; the Cross-Language Evaluation Forum (CLEF) [21] is one example. Our own interest in CLIR is as a means to cluster documents from the WWW. Clearly, these documents could be in any language, but we would like to cluster the documents by topic, factoring language out, so that documents on the same topic appear close to one another irrespective of their language. In section 2, we outline a standard approach to CLIR, and in section 3, we describe our impl ementation of that approach. As described in section 4, we found that we were able to confirm that this approach worked well for certain CLIR problems, but that it had severe drawbacks when we attempted to use it for cross-language clustering. The reasons fo r this are discussed, and we propose a novel alternative appro ach using PARAFAC2 instead of standard SVD in section 5. We compare how PARAFAC2 measures up to standard LSA in practice in section 6, and conclude on our results in section 7. A standard approach to cross-la nguage information retrieval uses Latent Semantic Analysis (LSA ) [11] in conjunction with a multilingual parallel aligned training corpus. This application of LSA to multilingual data is described in [5] and used in [23]. A term-by-document matrix of wei ghted frequencies is formed from the corpus; each  X  X ocument X  consists of the concatenation of all the languages, so terms from all languages will appear in any given document. A variety of wei ghting schemes can be used, but the log-entropy weighting scheme is generally believed to be one of the most effective for this purpose [10]. In this scheme, the weighted frequency x t,n of a particular term t in a particular document n is given by: where F t is the raw frequency of t in n , H entropy of the term across all doc uments, and N is the number of documents in the cor pus. (Accordingly, log 2 (N) is the maximum entropy that any term can have in the corpus; (1 + H t / log 1 for the most distinctive terms in the corpus, 0 for those which are least distinctive.) In the standard approach, th e term-by-document matrix of weighted frequencies X is s ubjected to SVD: X = USV output is a term-by-concept matrix (U, or the matrix of left singular vectors), a set of singular values (S, a diagonal matrix), and a document-by-concept matrix (V, or the matrix of right singular vectors). The number of columns computed for U and V is referred to as the number of LSA dimensions. Vectors for new documents (those not in the origin al parallel corpus) are computed by multiplying the vectors of weighted frequencies of terms in the new documents by US -1 . The cosine between any two such vectors is a measure of the similarity between those two documents. There are a number of well-unders tood practical advantages to using an approach like LSA for CLIR. Essentially, the parallel corpus used for training acts like a  X  X osetta Stone X ; it is the key which unlocks the door to comparing documents across language boundaries, while the underlying algorithms remain constant regardless of which languages are being compared. This becomes particularly advantageous when language-specific expertise is in short supply. An alternative approach to CLIR which is commonly employed, for example, is to translate documents: before computing a similarity, the source document is translated into the language of the target document. However, even if a machine translation (MT) system is used to automate this procedure, it is usually the case that a separate MT system must be put in place for each language pair, and that some familiarity with each language in the pair is required to build each such system. For any significant number of languages, the cost of building the required  X  X ystem of systems X  is likely to be prohibitive, even if the expertis e and resources required to do so are available. Another alternative approach (exemplified in [18]) is to use bilingual dictionaries, but these may not be available in all languages. In light of this, it is easy to see the attractiveness of a generic approach like LSA which relies only on the ability to tokenize text at the boundaries betw een words, or more generally semantic units  X  a procedure which can be generalized to virtually all languages, even logographic languages like Chinese. In implementing multilingual LSA, perhaps the major decision to be made is which parallel aligned corpus to use in training. For the work described here, we used the Bible. Although it is hard to come by reliable statistics which allow direct comparison, the Bible is generally believed to be the world X  X  most widely translated book ([8], [9], [22]) with at least partial translations into at least 2,426 languages and full translations into at least 429 languages [6]. A single website [7] has at least 80 parallel translations in over 50 languages (Table 1 lists most of these); almost all of the translations available for download are public-domain, and all are in a tab-delim ited format which can easily be aligned by verse (see Figure 2 for an example). Figure 2. Sample data from pub licly-available parallel corpus Table 1. Languages potentially available for multilingual LSA Armenian (Eastern) 1 Armenian (Western) 1 Chinese (Traditional) 2 Chinese (Simplified) 2 Greek (Modern) 1 Greek (New Testament) 6 Hebrew (Modern) 1 Hebrew (Old Testament) 3 Manx Gaelic 1 Scots Gaelic 1 TOTAL 80 Conveniently for our purposes, all of the languages represented most frequently in the WWW (see Figure 1) are also represented in [7]. The list of represented languages is less biased towards European languages (or at least la nguages of a particular language group) than is commonly the case with purpose-built parallel corpora, a reflection of the reasons that the translations of the even when sections of the parallel corpus are defective (for example, when only a portion of the Bible exists in a particular language), the defective sections can still be used without overall detriment [8]. We estimate, therefore, that using the Bible (in the dozens of translations that we have already downloaded) as a parallel corpus for training LSA, we would achieve about 99.75% coverage of internet content, a coverage which would have been hard to match using parallel te xt from any other single source. A question which is commonly rais ed is how representative the vocabulary of the Bible is of modern vocabulary, and therefore how suitable it is as training data. One answer to this is that it depends on which translations are used; many languages have multiple translations of the Bible (among our downloads, for example, there are 8 English translations ranging from the King James Version, dati ng from 1604, to the World English Bible, dating from 2006). Clearly, the more modern the translation, the better will be the coverage of the modern language. According to [22], the Bible X  X  coverage may be somewhere between 75%-85%, the vocabulary which is not covered consisting mostly of technical terms and proper name s. Our own informal tests confirmed that this estimate is pr obably not too far off; based on a sample of 602,995 web pages we collected, and after removing items which were treated as wo rds by our tokenizer but cannot reasonably be considered words (s uch as  X  X  X F X ,  X  X G X ), we believe that our coverage of vocabulary (as opposed to languages ) from the WWW would be around 70%. In any case, there is no reason to suppose that coverage has to approach 100% to allow for effective CLIR: in fact, we shall present evidence in this paper that vocabulary coverage of even less than 60% is sufficient to allow a high level of precision in solving certain CLIR problems. And although we have used the Bibl e as the training data, there is no reason that the approach could not be extended to the Bible plus additional parallel corpora. Since the Bible is alignable by verse, and there are more than 30,000 verses in the Bible, each averaging about a sentence or two in length, an extremely fine-grained term-by-document matrix can be created. Generally, we have found that the finer the granularity, the better CLIR results we obtain. With 77 parallel versions and using our alignment scheme 2 , our term-by-document By contrast, parallel corpor a developed with government funding, for example, are unders tandably more restricted in scope. The corpora developed with EU funding, for example, naturally consist mostly of ma terial in European languages. 77 is the greatest number of para llel versions we have used so far in multilingual LSA. The alignment of the raw data in [7] is not always perfect owing to minor differences in versification between translations. We addre ssed this by spending some time cleaning the raw data to improve the alignment. As a result, the number of verses in our alignm ent scheme (31,226) is greater than the number of verses in most of the raw downloads (31,102). matrix was 1,454,289 by 31,226. As is typical in natural language processing, this matrix is extremely sparse; the number of nonzeros in this case was 21,759, 766, representing a density of around 0.048%. We stored the parallel text in a relational SQL Server database to allow for eas y aggregation of the statistics required to form different te rm-by-document matrices for different language combinations and use by different CLIR algorithms. To compute the SVD, we used either SVDPACK [4] or a library called Anasazi [3], which is part of the Trilinos framework [14]. In each case, we computed a truncated SVD corresponding to the 300 highest singular values. We found, however, that SVDPACK was unable to cope with the size of term-by-document matrices necessary to process more than around two dozen languages in paralle l, and thus we resorted in these cases to using Trilinos (whi ch is designed to run on a Linux cluster and is consequently considerably more scalable). The results of SVD were then imported back into SQL Server and we used SQL scripts to compute the vectors for new documents or queries, for example those in the test set. The test data we used were the 114 suras (chapters) of the Quran, which has also been translated into a number of languages. Clearly, test data of this sort are a prerequisite in order to be able to measure effectiveness in multilingual clustering. For most of the work described in this pape r, we limited the selection of languages to Arabic, English, French, Russian and Spanish (the respective abbreviations AR, EN, FR, RU and ES are used hereafter), in both the training and the test data. With this data, the initial term-by-document matrix was 160,396 by 31,226 with 2,684,938 nonzeros. With the five languages, the test data amounted to 570 documents: a rela tively small set, but large enough to achieve statistically si gnificant results for our purposes, as will be shown. Note also th at although the test documents all come from a single topic domain, it is reasonable to assume that the comparative results we will report in this paper are valid in general, because in all tests we describe, we are using the same test set. Perhaps surprisingly, the Bibl e X  X  coverage of the Quran X  X  vocabulary appears to be lower than the Bible X  X  coverage of general WWW vocabulary. Of 58,015 distinct terms in the Quran, only 33,423 (or about 58%) appear in the Bible. We tokenized each of the 570 test documents, applying the weighting scheme descri bed above to obtain a vector of weighted frequencies of each term in the document, then multiplying that vector by U  X  S -1 , also as described above. The result was a set of projected document vectors in the 300-dimensional LSA space. We used four separate measures to evaluate the effectiveness of CLIR given this data. These m easures are listed in Table 2. # Measure 1 Precision at 1 document (for a given source and target 2 Precision at 0 (for a given source and target language) 3 Multilingual precision at 5 documents (for 5 languages) 4 Multilingual precision at 0 (for 5 languages) There is sometimes confusion a bout the different measures of precision, so for the avoidance of all doubt, we shall spell out how exactly these measures are calculated. The first of these, precision at 1 document, is the proportion of cases, on average, where the translation was retrieved first. Fo r example, if French sura number 5 was the most similar sura among all the French suras to English sura number 5, then precision at 1 document in this case would be 1, and 0 otherwise. This is a strict measure, since no more credit is given if the translation is ranked second than if it is ranked bottom. The second measure, precisi on at 0, is less strict. This represents the maximum precision at any level of recall. Since we are dealing with translations, only one document is considered relevant, and precision at 0 is therefore the inverse of the ranking of the translation. These firs t two measures relate to the effectiveness of our CLIR techni que in finding similar documents given the language of the query and the language of the results , and for convenience we will re fer to these two measures collectively as  X  X anguage-specific X  precision metrics. Measures 3 and 4, on the other hand, relate to the effectiveness of our technique in finding similar documents regardless of source or target language . These measures give an indication of how well multilingual clustering is likely to work. Since we have 5 languages, the best result we c ould achieve for clustering would be to have all five translations ranked in the top 5 in similarity to the query.  X  X ultilingual precision at 5 documents X , therefore, represents the proportion of the top 5 retrieved results which are translations of the query, and  X  X ultilingual precision at 0 X  (again, the less strict measure) represents the maximum precision at any level of recall after the fifth document. We refer to these two measures as  X  X ultilingual X  precision metrics. Using the standard approach and measures 1 and 2 as the evaluation metric, we obtained our best results using LSA with the given languages with 280 dimens ions. These results are shown in Table 3 and Table 4. AR 1.000 0.500 0.491 0.570 0.474 EN 0.684 1.000 0.912 0.974 0.833 ES 0.500 0.860 1.000 0.930 0.605 FR 0.605 0.930 0.947 1.000 0.789 RU 0.474 0.825 0.798 0.789 1.000 AR 1.000 0.656 0.653 0.695 0.645 EN 0.765 1.000 0.935 0.983 0.899 ES 0.630 0.897 1.000 0.953 0.731 FR 0.711 0.961 0.964 1.000 0.869 RU 0.608 0.877 0.866 0.854 1.000 On average, precision at 1 docum ent here is 0.780, and precision at 0 is 0.846. (As more parallel tr anslations are added, both these precisions rise further, to ar ound 0.81 and 0.87 with 52 languages and 77 parallel translations [8].) These averages include the diagonal values of 1.000. These reflect very favorably on the ability of the standard approach to identify translations, providing the search space is limited in each case to a single language: here, almost 80% of the time, the translation is retrieved first. The results also compare favorably with published results which use different methodologies for CLIR (using a different data set, McNamee and Mayfield report mean average precision of no more than 0.45 for English-to-Spa nish CLIR using 5-grams [19]). Recall that these results were achieved despite the Bible X  X  coverage of the Quran X  X  vocabulary being less than 60%; proof, it would seem, that even with only partial coverage of the target vocabulary, CLIR can be very effective. Under measures 3 and 4, however , a different picture emerges. The relevant results are presented in Table 5 (not broken down by language pair, because the different languages are now mixed together in the test set). Multilingual precision at 5 documents 0.259 Multilingual precision at 0 0.265 It is worth noting that under standard LSA, while language-specific precision tends to increase as more LSA dimensions are used (at least up to 300 dimensions , which is as far as we have tested), the opposite seems to be true for multilingual precision, at least to a certain point. Above 5 dimensions, it appears that multilingual precision generally decreases (see Figure 3). Following the findings in [8], we attempted to boost either language-specific or multilingual precision by increasing the number of parallel translations us ed in training LSA. Our results did confirm the premise in [8] th at more linguistic parallelism is beneficial to LSA (both for language-specific and multilingual precision). However, even with 77 parallel translations, multilingual precision rises no higher than 0.300 (at 5 documents) and 0.307 (at 0); and from Figure 3 it will be seen that in the best case, we were unable to achieve multilingual precision at 0 of above around 0.35 using LSA. Cons idering that these measures can never be lower than 0.2 with 5 languages (since each document is always most similar to itself, and therefore ranks top in the results), these results are all the more unimpressive: on average, hardly any of the second-to-fifth ranked results are translations of the query. How can this be, when the first two measures produced much more encouraging results? In part, this can be answered by considering what happens when we attempt to use the LSA document vectors to  X  X ap X  the documents in a graphical representation, such that similar documents are located close to one another. When we attempted this, we found that the documents essentially cluster by language, not by topic. To understand how this can happen, consider the hypothetical example of some ra nked results shown in Table 6. 
Ranking Language of retrieved document Relevant? In this example, while the first document retrieved in each language was the relevant one , many non-relevant English documents were returned before the relevant documents in the other languages. In this example, measures 1 and 2 would each have been 1, but measure 3 w ould have been only 0.2. This occurs, we believe, because each language has its own distinctive statistical  X  X ignature X , as is re flected in the differing counts for  X  X ypes X  (unique terms) versus  X  X  okens X  (instantiations of those terms in the text) in the parallel Bible text we used in training. These counts are shown in Table 7. TOTAL 163,745 3,307,654 Assuming that the translations of the Bible in our parallel corpus are accurate and complete, this tabl e would appear to suggest that Arabic takes just over half the number of terms to express the same amount of information as E nglish, that English and French take similar numbers of terms, and so on. Intuitively, this seems right given that Arabic and Russian rely much more than than English, French and Spanish on the use of morphology (endings, and so on) to add to or modify the meanings of words. The same phenomenon can also be illustrated well on a small scale by considering the first  X  X ocument X  in the parallel corpus (the first verse), shown in Table 8. It is lik ely to be no coincidence that the best cross-language prediction resu lts we achieved were for pairs of languages with similar statis tics (for example, English and French), and the worst results were for those with dissimilar statistics (such as Arabic and E nglish) (see Table 3 and Table 4). Text word AR  X  X  X  X  X  X   X  X  X  X  X  X  X   X  X   X  X  X   X  X  X  X  X   X  X  . 6 14 
EN In the beginning God created the 
FR Au commencement Dieu cr X a les 
RU  X   X  X  X  X  X  X   X  X  X  X  X  X  X  X   X  X  X   X  X  X  X   X 
ES En el principio cri X  Dios los cielos y TOTAL 42 100 The statistical differences can, in fact, be shown to have a detrimental effect on LSA  X  not just empirically, but theoretically as well. Under the standard l og-entropy weighting scheme, we can verify whether, according to this scheme, the contribution of each of the 5 languages in our multilingual aligned parallel corpus is equal  X  which it should be, if the translations are complete and accurate. The computed entropy is a measure of information content: because the same information is being conveyed in the translations of any given document in the training corpus, the total entropy per language (the sum of te rm entropies of terms in that language) should be constant for any given document. Upon examination, we found that with the standard log-entropy weighting the computed informati on content varies quite widely by language, which is perhaps unsurprising. If it takes 560,524 Russian words to express what English says in 789,744 words, then on average Russian words must contain more information (or meaning) than English words (aga in, a notion which is consistent with what we know about the way words are formed in Russian and English). However, since entropy (or information content) in the log-entropy scheme is simply the entropy of a particular term across all documents, and since the scheme takes no account of the specific properties of different languages, there is no guarantee that the contributions of different languages in the parallel corpus will be equal as they should be. In fact, in our parallel corpus, where under LSA all languages are  X  X ixed together X  in the bag-of-words approach, languages which have more terms overall (such as E nglish and French) generally account for a higher percentage of the  X  X nformation X  in each document. This points to a flaw in standard multilingual LSA, or at least in the log-entropy weighti ng scheme as applied within that approach. One other point to note is the difference between the total of 163,745 shown in Table 7 above, and the figure of 160,396 mentioned in section 4.1. The di fference of 3,349 represents those terms that occur in more than one language, such as  X  X e X  ( X  X f X  in French and Spanish), English  X  X oin X  versus French  X  X oin X  ( X  X orner X ). The relatively small num ber of such terms is unlikely to affect the cross-language precision results significantly, but it is worth pointing out that standard LSA has no way to distinguish between homographs from different languages, and in some cases this could be problematic, especi ally when the homographs have very different meanings in the different languages. 3 Given all this, the statistical explanation seems to be a reasonable one for why, when we attempted to map the documents graphically such that similar documents were close to one another, the documents clustere d by language rather than by topic. Precisely the same issue has been identified elsewhere in the literature: Mathieu et al [18] report that  X  X ven if the cross-lingual similarity measure is desi gned to behave the same when comparing documents written in the same language and documents written in different one s, our evaluation shows that it still tends to gather in a cluster documents of same language prior to different language ones X . As discussed in the previous s ection, it is a drawback of the standard approach to LSA that there is no delineation between different languages in the training data. All languages are concatenated together in training, so that each  X  X ocument X  is multilingual. Within the LSA fra mework, however, this is unavoidable, since without the c oncatenation, LSA is unable to make the associations between wo rds in different languages when they co-occur. To overcome this problem, th erefore, we propose a novel application of PARAFAC2 [13] as an alternative to LSA. PARAFAC2 is a variant of PARAFAC [12], a multi-way generalization of the SVD. Th e PARAFAC model is based on a  X  X arallel proportional profile principle X  that applies the same factors across a parallel set of matrices to minimize a least-squares objective. Let the M  X  N matrix X k , k = 1, ..., K, denote the k th slice of a three-way data array X, and let R be the number of dimensions of the LSA conceptual space. Then the standard PARAFAC model is The difference between the total of 3,307,654 and the 2,684,938 nonzeros mentioned in section 4. 1 can also be explained: the figure in Table 6 is higher because some terms occur more than once in the same verse. For example, if  X  X he X  occurs three times in a particular verse, this w ould account for three tokens, but only one nonzero entry in th e term-by-document matrix. where U is an M  X  R factor matrix for the terms, S diagonal matrix of weights for the kth slice of X, and V is an N  X  R factor matrix for the documents. In this form, it is easy to see PARAFAC X  X  similarity to the S VD. Here, though, we find factor matrices U and V that are the same for every matrix X k . However, the factors U and V are not orthogonal as they are for the SVD. In our application, we can let X k be the term-by-document matrix for the k th language in the parallel corpus. It has M k documents; however, since the number of rows in each slice differs, the PARAFAC model is not appropriate. PARAFAC2 is a related model that is appropriate because it relaxes the constraint that the U matrix is the same acr oss all slices. Thus, we form an irregular three-way array, each slice of which is a separate term-by-document matrix for a single language in the parallel corpus. The number of documents in each s lice will be the same, since the corpus is parallel, but the number of terms will vary by language. The K=5 slices of X for our application are shown in Figure 4. Figure 4. Term-by-document matrices by language as a three-The PARAFAC2 has the following form: Here, there is an orthonormal M k  X  R factor matrix U for each slice of X, and an H matrix of size R  X  R. Because this model lacks certain uniqueness properties associated with the standard PARAFAC model, an invariance constraint is needed on the left factor matrices (i.e., the product U k H). To gain uniqueness, Harshman [12], [13] imposed the constraint that the cross product (U k H) T (U k H) is constant over k, which in this formulation is accomplished with the constraint that H is nonsingular. The PARAFAC2 model is shown in Figure 5. Conceptually, the goal is to compute something like an SVD for each language such that V (analogous to a matrix of right singular  X  X ocument X  vectors, though not orthonormal) is the same across all languages, although for each language k there will be a separate U k (analogous to a matrix of left singular  X  X erm X  vectors for language k) and S k (analogous to singular values). A benefit of PARAFAC2 is that it has a separate mapping for each language into the LSA conceptual space; in particular, each mapping is orthogonal for each language rather than the one large orthogonal mapping for all languages at once. In other words, PARAFAC2 imposes the constraint , not present in standard LSA, that the  X  X oncepts X  (i.e., columns of U k ) of any given language in the parallel corpus taken on its own map to those of any other language. Intuitively, this constrai nt makes sense, since the whole purpose of using a parallel corpus is that translations are supposed to render the same concepts in different languages. To compute the PARAFAC2 mode l of X, we implemented a variant of the algorithm outlined in [15] that is adapted to handle very large and sparse data. The complete procedure is summarized as follows. 
Step 0. Initialize V as the R principal eigenvectors of  X 
Step 1a. Compute the SVD of Z k = HS k V T X k T = P k  X 
Step 1b. Update H, V, and S 1 ,...,S k by one iteration of an 
Step 2. Repeat step 1 until a maximum number of iterations This algorithm was written in MATLAB using the Tensor Toolbox [1], [2], and the PARAFAC2 model was computed on a dual 3GHz Pentium Xeon desktop computer with 2GB of RAM. Once the PARAFAC2 model has b een computed for all languages according to these constraints, the manner in which new documents are projected into the semantic space is similar to that used in LSA. A vector of weight ed term frequencies (the term-by-document vector) is formed as described in 4.1 above. The difference is that this v ector is multiplied by the U k S the language of the document, rather than the general US -1 languages which is the artifact of LSA. This relies, of course, on knowing the language of the new document, but there are a variety of machine learning met hods for reliably determining the language of an unseen document; one such method (which achieves an accuracy of over 99%) is mentioned in [20], and we have achieved similar results by training a neural network on the LSA vectors. Thus, it can be seen that the additional step necessitated by PARAFAC2 could easily be automated and is not a significant obstacle to wider deployment. The main disadvantage of PARAFAC2 compared to LSA is that more computation is required to obtain the decomposition. In fact, since there is currently no parallel implementation of PARAFAC2, we can compute at most 240 dimensions using PARAFAC2. However, as with LSA, the PARAFAC2 decomposition need only be comput ed once, and the results are then available for use multiple times, so the one-time cost of using PARAFAC2 is essentially one which can be highly leveraged. This disadvantage in performance is also offset by an advantage which applies at run-time: since the language-specific U k are considerably smaller than the general U matrix, the process of matrix multiplication can be considerably faster than it is under LSA. There is another linguistic/theoretical advantage to PARAFAC2, and this has to do with the  X  X omographs X  issue identified in section 4.4 above. Since, under PARAFAC2, we are now delineating between the input of different languages in training, English  X  X oin X  is diffe rentiated from French  X  X oin X   X  which, one would assume, can onl y be advantageous in CLIR since the homographs in this partic ular pair are, as far as we know, unrelated in meaning. With the same training and test data as described in section 4 above, and using PARAFAC2, we obtained the results shown in Table 9, Table 10, and Table 11. Since we were limited to 240 dimensions, for a fair comparison we also recalculated precision under LSA using only the top 240 dimensions. The relevant results are shown in Table 12, Table 13, and Table 14. AR 1.000 0.667 0.693 0.746 0.693 EN 0.632 1.000 0.947 0.982 0.833 ES 0.605 0.947 1.000 0.974 0.886 FR 0.728 0.974 0.956 1.000 0.895 RU 0.728 0.921 0.895 0.939 1.000 AR 1.000 0.785 0.793 0.827 0.793 EN 0.738 1.000 0.968 0.990 0.887 ES 0.705 0.967 1.000 0.981 0.918 FR 0.791 0.989 0.972 1.000 0.930 RU 0.807 0.947 0.935 0.958 1.000 Multilingual precision at 5 documents 0.402 Multilingual precision at 0 0.415 Table 12. Precision at 1 document -LSA, 240 dimensions AR 1.000 0.447 0.456 0.579 0.561 EN 0.649 1.000 0.904 0.965 0.746 ES 0.465 0.798 1.000 0.921 0.596 FR 0.518 0.939 0.956 1.000 0.734 RU 0.439 0.754 0.798 0.763 1.000 AR 1.000 0.600 0.607 0.704 0.678 EN 0.736 1.000 0.937 0.978 0.842 ES 0.591 0.859 1.000 0.946 0.727 FR 0.652 0.966 0.967 1.000 0.832 RU 0.585 0.845 0.856 0.845 1.000 Multilingual precision at 5 documents 0.261 Multilingual precision at 0 0.268 From these results it can be seen that PARAFAC2 outperforms standard LSA by a significant margin on the multilingual precision metrics  X  0.402 compared to 0.261, or 0.415 compared to 0.268 depending on which measure is used. This is empirical confirmation that PARAFAC2 lives up to its promise, which is to ensure that the  X  X oncepts X  of the different languages are aligned differences between languages th at caused problems for LSA. PARAFAC2 also appears to outperform LSA (by a narrower but still highly significant margin) in the language-specific metrics. The average precision at 1 doc ument is 0.866 for PARAFAC2 compared with 0.760 for LSA, and for precision at 0 the averages are 0.907 and 0.830 respectively. Moreover, it will be seen by comparing Table 9 with Table 12, and Table 10 with Table 13, that the results using PARAFAC2 are superior almost across the board. The only exceptions are in precision at 1 document: English-to-Arabic was slightly lower for PARAFAC2, and French-to-Spanish was a tie. In all cases, precision at 0 is better under PARAFAC2. Since the aver age precisions represent the averages across 2,850 (114  X  5  X  5) query submissions, the differences between the results for PARAFAC2 and LSA are highly significant (p  X  5.22  X  10 -40 for overall average precision at 1 document, using a chi-squared test). We repeated the same comparisons at various different numbers of dimensions and found that PARAFAC2 consisten tly outperformed LSA, no matter how many dimensions the deco mposition was computed in, and usually the difference was highly statistically significant. In fact, even our best results using standard LSA 4 still could not compare with the PARAFAC2 resu lts in Table 9 above. With standard LSA, precision at 1 document averaged around 0.82 with 300 dimensions and 45 or more parallel translations used in training. For reference and comparison with Figure 3, the effect of the number of dimensions on precision under PARAFAC2 (to the extent we have run tests, and with lines to interpolate for numbers of dimensions not tested) is shown in Figure 6. Figure 6. Effect of number of dimensions on PARAFAC2 It seems, therefore, that the e ffect of number of dimensions upon precision under PARAFAC2 follows a pattern similar to that for LSA. In summary, the evidence appears to be highly compelling that PARAFAC2 is a superior alte rnative to standard LSA for multilingual information retrieval, at least for the two CLIR problems we want to solve. In line with our expectations, we found that this was particularly true for multilingual document clustering. However, since we had achieved respectable  X  X anguage-specific X  results using LSA and thus already found it an effective tool for identification of translations, it was more unexpected for us to find that PARAFAC2 essentially beats LSA  X  X t its own game X . Even by th e language-specific metrics which portray LSA in a good light, PARAFAC2 is a more effective tool than standard LSA. In section 2, we outlined some of the qualitative features which make LSA attractive as a vehicle for CLIR: essentially, its extensibility to virtually all languages, particularly when used in conjunction with a widely-translate d parallel corpus such as the advantages apply just as much to PARAFAC2 as they do to LSA. Although PARAFAC2 has a greater lead over standard LSA in the metrics which relate to multilingual clustering than it does in those that relate to language-specific CLIR, it has to be said that the initial baseline set by LSA was much lower (0.27 for multilingual precision at 0, compared with 0.83 for language-specific precision at 0). Further, even with the boost that PARAFAC2 provides for multilingual precision, the highest multilingual precision that we were able to attain (scarcely over 0.4) is not as high as we had hoped, and we are still doubtful that this level of precision will overcome the problem that we had hoped to solve, that of preventing documents from simply clustering by language in a graph-based analysis. Nevertheless, PARAFAC2 repres ents a good step forward from LSA in addressing this problem. We intend to carry out further experiments to determine whethe r further adaptations can be made to PARAFAC2 to allow for multilingual document clustering to be carried out successfully. It remains to be seen what these adaptations might look like and to what extent we can streamline the method to maximize multilingual precision, but given the fact that our research with PARAFAC2 is still in a PARAFAC2 offers a promising way forward for truly language-independent clustering of documents by topic. We are grateful to Steve Verzi, Stephen Helmreich, and Brad Mancke for the many constructiv e comments they have given us as we have worked on the material for this paper. Sandia is a multiprogram laboratory operated by Sandia Corporation, a Lockheed Martin Company, for the United States Department of Energy X  X  National Nuclear Security Administration under cont ract DE-AC04-94AL85000. [1] Bader, B. W., and Kolda, T. G. Efficient MATLAB [2] Bader, B. W., and Kolda, T. G. MATLAB Tensor Toolbox, [3] Baker, C. G., Hetmaniuk, U. L., Lehoucq, R. B., and [4] Berry, M. W., Do, T., O X  X rien, G. Krishna, V., and [5] Berry, M. W., Dumais, S. T., and O X  X rien, G. W. Using [6] Bible Society. A Statistical Summary of Languages with the [7] Biola University. The Unbound Bible , 2005-2006. Accessed [8] Chew, P. A., and Abdelali, A. Benefits of the  X  X assively [9] Chew, P. A., Verzi, S. J., Bauer, T. L., and McClain, J. T. [10] Dumais, S. T. Improving the Retrieval of Information from [11] Dumais, S. T., Furnas, G. W., Landauer, T. K., Deerwester, [12] Harshman, R. A. Foundations of the PARAFAC Procedure: [13] Harshman, R. A. PARAFAC2: Mathematical and Technical [14] Heroux, M., Bartlett, R., Howl e, V., Hoekstra, R., Hu, J., [15] Kiers, H. A. L., Ten Berge, J. M. F., and Bro, R. [16] Kolda, T. G. and Bader, B. W. The TOPHITS model for web [17] Landauer, T. An Introduction to Latent Semantic Analysis. [18] Mathieu, B., Besan X on, R. and Fluhr, C. Multilingual [19] McNamee, P. and Mayfield, J. Character N-Gram [20] Nie, J-Y. and Jin, F. A Multilingual Approach to [21] Peters, C. (ed.). Cross-Language Information Retrieval and [22] Resnik, P., Olsen, M. B., and Diab, M. The Bible as a [23] Young, P. G. Cross Language Information Retrieval Using 
