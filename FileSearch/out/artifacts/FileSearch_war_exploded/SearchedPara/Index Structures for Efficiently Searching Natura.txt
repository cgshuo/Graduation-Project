 Many existing indexes on text work at the document granu-larity and are not effective in answering the class of queries where the desired answer is only a term or a phrase. In this paper, we study some of the index structures that are capa-ble of answering the class of queries referred to here as wild card queries and perform an analysis of their performance. Our experimental results on a large class of queries from different sources (including query logs and parse trees) and with various datasets reveal some of the performance bar-riers of these indexes. We then present Word Permuterm Index (WPI) which is an adaptation of the permuterm in-dex for natural language text applications and show that thi s index supports a wide range of wild card queries, is quick to construct and is highly scalable. Our experimental results comparing WPI to alternative methods on a wide range of wild card queries show a few orders of magnitude perfor-mance improvements for WPI while the memory usage is kept the same for all compared systems.
 I.7.3 [ Document and Text Processing ]: Index Gener-ation; H.2.4 [ Database Management ]: Systems X  Query Processing Performance Indexing natural language text, Querying performance, Wil d card queries
Natural language text is pervasive; emails, news-group messages, web pages, research papers, books, news, etc. are almost entirely authored in a human readable natural language. Such huge volumes of data provide the feed for many applications including search engines, Question An-swering (QA) systems, and many analytical tools built on more ad-hoc basis. QA systems in particular are important in that they extract relevant facts and meaningful answers to questions, but mostly deal with relatively small and clea n corpora and involve deep and time-consuming natural lan-guage processing. Hence, there has been little focus on the performance of such systems on large text collections. This is the topic we study in this paper. In particular, we intro-duce and evaluate access methods that greatly improve upon the performance of keyword searching for wild card queries over large natural language text collections. The work has a significant impact on many applications where answers to natural language questions are sought or the query includes some wild cards.
Question answering on a large corpus is a challenging task mainly because it is difficult to analyze the whole (or a good portion of) data and to retrieve candidate answers. On open-domain QA applications, such as question answering over the web 1 , it would be very difficult to build a general QA system with high accuracy. In those cases, a reason-able approach is to convert natural language questions into queries and benefit from available querying engines to en-hance the performance of the search. The choice of queries can further affect the efficiency of searching, ease of trans-lating questions to queries and the relevance of the results . Table 1 gives a list of query types and some of the contexts where each query type is used.

Of those listed, wild card queries, where keywords match-ing a wild card are sought, are particularly important for several reasons. First, a large number of natural language questions can easily be translated into one or more wild card queries. As an example who and what questions and a large number of which and where questions can be translated into wild card queries (See Table 2). Second, the results of such queries can easily be joined with data that may reside in a database. For example, candidate answers for the which question in Table 2 can be further refined by looking up the values returned by the first query in a database populated with a list of city names (second query). Finally, question answering systems often rely on NLP components that may directly or indirectly use wild card queries. Examples are taxonomy construction, fact extraction, named entity reco g-
See OpenEphyra [7] for an example nition and query expansion. Rafiei and Li [28] present wild card querying over web text and discuss several techniques such as query expansion and relevance ranking to increase the precision and recall of extractions.
 Table 2: Samples of natural language questions and their corresponding wild card queries
There has been a great deal of activity around increasing the efficiency of keyword-based queries. However, the same structures and algorithms would not necessarily be useful or efficient for evaluating wild card queries. Assume we are given an inverted index structure, such as the one depicted in Figure 1 with four terms and three documents. Each term t in the index has a list of postings, each posting in the form id, f t,d is the frequency of t in d and o 1 o f t,d are offsets in d where t appears.

Given a keyword query Q1: X  X orld population X  and a wild card query Q2: X  X orld population is % X  , the algo-rithm for evaluating Q1 involves only intersecting the posting lists of terms  X  X orld X  and  X  X opulation X  , and finding the list of matching documents. However, for Q2 , each match-ing document has to further be scanned in order to find the keywords that match the wild card. In the above example, Q2 matches  X 6706993152 X  which is located on offset 10 of document 3. Although Q2 matches its answer in fewer docu-ments than Q1 , the query response time for Q2 using inverted indexes in one of our experiments was 12 times larger. This indicates that inverted indexes are not appropriate for eva l-uating wild card queries.
Solutions on multi-keyword queries such as phrase and nextword indexes [11, 30] can help reduce the time it takes to intersect the posting lists, but won X  X  help in the keyword matching step, which is in most cases the dominant process. Therefore, development of solutions for efficient retrieval of keyword matches from text seems essential.
In this paper we introduce Word Permuterm Index (WPI), as an efficient index structure for evaluating wild card queri es over natural language text. WPI extends Permuterm Index (PI) [22] in several aspects. (1) By construction, WPI sup-ports pattern matching over keywords rather than charac-ters, (2) WPI supports a wider range of queries than PI, adding support for queries that are more frequently used over natural language text, and (3) WPI returns the ac-tual keywords that match a wild card query whereas PI is mostly used to find the range of elements that match a pat-tern. Thus, WPI goes one more step toward matching the keywords after finding the range of matching elements. Our other contribution is a broad set of experiments and analy-ses comparing the performance of WPI to alternative state-of-the-art methods. Our performance comparison includes cases where WPI is given a limited memory and is forced to do paging. To the best of our knowledge this is the first work that experimentally compares traditional inverted fil e indexes with more recent compressed full-text indexes (See the survey in [26]).

The rest of this paper is structured as follows. In Section 2 we describe the different index structures in the literature and how they can be utilized to solve the wild card querying problem. In Section 3 we detail our solution and give a the-oretical comparison with the previous approaches. In Sec-tion 4 we provide comprehensive comparison among these structures and their performance. A review of the literatur e on different query types, index structures and querying al-gorithms are given in Section 5. Finally we conclude and provide remarks for future work in Section 6.
Without loss of generality, we consider phrase queries that have exactly one wild card and any number of non-wild card terms, referred to as literals. For queries with multiple wi ld cards, one can find the matches for query sub-sequences that have only one wild card, substitute the wild card with its matches and look for further matches.

Next, we introduce a few baseline access methods that are used within natural language applications and study their performance. Regardless of the access strategy, a wil d card query evaluation can be often divided into two phases: (1) Binding phase , where the indexed elements (e.g. sen-tences, paragraphs or documents) are filtered based on the query literals that are present and maybe their order, and (2) Matching phase in which filtered elements are scanned and the keywords that match the wild card are retrieved.
A straightforward approach for answering wild card queries is to scan the full dataset and check every unit for possible query matches. If the dataset fits in main memory, a full scan may not be a bad idea given that the initial cost of loading is negligible when amortized over a reasonable-siz ed set of queries.
As illustrated in Figure 1, our implementation of an in-verted index stores a linear vector of posting triplets &lt; d, f t,d , [ o 1 , , o f t,d ] &gt; . Wild card query evaluation over inverted index can be easily adapted from the standard im-plementations of keyword queries. Keyword queries are eval -uated by intersecting the posting lists of query literals an d finding the matching documents and corresponding offsets. The key idea behind wild card query evaluation is to sequen-tially scan these documents and to find and extract the wild card matches. Thus, in order to do the wild card matching we need to store and access the text dataset as well.
The complexity of wild card matching over an inverted index is O ( the number of bindings of a pattern P 2 . Since we have to go through all the matching elements in order to find the wild card matches, the cost is k Q k | d avg | , where d average size of an index element.
Neighbor index, as proposed by Cafarella and Etzioni [14], is an inverted index that is more suitable for queries over natural language text data. The index stores for each term both its left and right neighbors. As shown in Figure 2 for our running example (given in Figure 1), the inverted lists have grown significantly larger, but the answer to wild card matches are stored within the index and can be found by looking at the appropriate neighbors of a query literal. For example, to find the matches for Q2 in the neighbor index, the search is conducted in the inverted index until offset o = 10 in document d = 1 is identified as an answer. To obtain the actual answer, it is sufficient to look at the right neighbor of the term at offset 9 in the index without retrieving the document. This can speed up the evaluation of wild card queries by 1-2 orders of magnitude compared to inverted index, as reported by the authors and confirmed in some of our experiments.

The original implementation of the neighbor index stores for each neighbor in addition to the term, both its part of speech (e.g. noun-phrase) and its role (e.g. term). Since th e tags are not explicitly used in our queries, we implemented a simplified version of the neighbor index, where for each off-set, only one left neighbor and one right neighbor were store d with no further information. Therefore, the structure of a posting in our implementation of the neighbor index looked like &lt; d, f t,d , [( o 1 , l 1 , r 1 ) , , ( o f t,d and r i are the left and right neighbors of the i  X  X h occurrence of t in d , respectively.

Given that neighbor index is an inverted index, the algo-
The number of documents matching P rithm for evaluating wild card queries over neighbor index follows the same bind-and-match process of any inverted in-dex, except that the matching phase is much less costly. Once the matching documents and offsets are found, the wild card matches can be extracted in constant time. Thus, the running time of wild card query evaluation over a neigh-bor index will be O (
This section presents our Word Permuterm Index (WPI) as an efficient access method that supports wild card query-ing over natural language text. WPI is an adaptation of the permuterm index [22, 21] and as such it has the following three components: (1) A word level Burrows-Wheeler (BW) transformation of text [13], (2) an efficient mechanism to store and access the alphabet, and (3) an efficient mecha-nism to access the ranks. Next, we discuss these components in more detail.
Burrows-Wheeler transformation (BWT) is a reversible transformation that is used in well-known compression algo -rithms such as bzip2 and is believed to give a permutation that is more amenable to compression. The transformation, when applied to a character string, can change the ordering of the characters in the string but not their values. Our work applies BWT to words instead of characters; a word-level transformation has some interesting properties especiall y in answering wild card queries.
 Assume we are given a dataset containing three sentences S1:  X  X ome is a city X  , S2:  X  X ountries such as Italy X  and S3:  X  X ome is the capital of Italy X  , and we would like to index them using WPI. Adapting the ideas proposed by Manning et al. [25] and Ferragina and Venturini [21], we sort this dataset lexicographically 3 and use the $ symbol, to mark the sentence boundaries and the e symbol, to mark the end of the dataset. This results in our dataset to look like T:  X $ such as Italy $ e  X  .

A word-level BWT is obtained by (1) computing all the cyclic rotations of the words, (2) sorting the rotations, an d (3) finding the vector that contains the last word in the ro-tations in the same order after the sorting. Figure 3 depicts the result of applying these three steps to T in the given example. Note that the set of sentences are rotated by one word at each level. We denote the vector of last words, BW-transformation, by L and the sorted vector of first words, by F .

BWT has some very interesting properties. First, for any word in T , the j  X  X h occurrence of the word in L and the j  X  X h occurrence of the word in F correspond to the same word of the sequence. For instance, the second occurrence of the word  X  X taly X  in L is preceded by  X  X s X  , and so is the second  X  X taly X  in F ; hence, L (4) = F (6). Second, for every row, L ( i ) precedes F ( i ) in T . Given these two properties, Ferragina and Manzini [20] propose the following function for traversing L in backward order:
Sorting guarantees nice properties on BWT, See Section 3.4 where C [ L [ i ]] is the number of words smaller than L [ i ] and Rank L [ i ] ( L, i ) is the number of times L [ i ] appears in the sub-sequence L [1 ..i ]. LF ( i ) tells where the element pre-ceding L [ i ] in T is located in L . E.g. LF (6) = C [  X  Rank  X  as  X  ( L, 6) = 9 + 1 = 10 and L (10) is  X  X uch X  and is the word preceding L (6) =  X  X s X  in T . Since T is sorted, one can start from L (1) = F ( n ) and repeatedly call LF to find L ( n ) = F (1), traversing the whole text in backward order. Therefore, L is reversible, meaning that given L , any sub-sequence of words in T can be re-constructed. We can use this property to turn L into an index that can support searches over word sequences. The challenges would be to support a wide range of wild card queries and to efficiently support access to C and Rank , required for traversing L in backward order. Next, we discuss these challenges and the proposed solutions. A major difference between the permuterm index and WPI is in the size of their alphabets. The alphabet in permuterm index typically consists of ascii characters and symbols which are small in size and are not required to be explicitly stored. However, the alphabet size in WPI grows with the size of text dataset almost linearly. When |  X  | is in the order of millions, efficient access to alphabet elements, their ordering and their frequency is crucial.

In order to provide efficient access to  X , we built one ar-ray and one hash table. The array stores the elements of  X  in ascending order, therefore the first element is always $ and the last is e . The array helps to find which alphabet element is represented by which numerical code, which is its index in the array. Coding the alphabet is essential for effi-cient implementation of algorithms such as backwardSearch and Rank . Without coding, we will not be able to achieve the time complexities we later report for these algorithms. Moreover, coding reduces the index size, replacing a key-word and a delimiter by a code which uses smaller number of bits.

The hash table stores the same information in the reverse order; given an alphabet element, the hash returns the code of the element, together with its frequency and cumulative frequency, C . Thus, C [ t ] counts the number of alphabet elements in the whole dataset that are smaller than t . In the above example, |  X  | = 13 and the hash table provides constant-time access to C values for all the alphabet ele-ments.
Rank c ( L, i ) returns the number of occurrences of c  X   X  in the prefix 1 i of array L . In order to evaluate queries over WPI, we make frequent accesses to Rank and therefore, quick access would be required. Naive baseline solutions to the rank problem are as follows. (1) Start from the first ele-ment on L and compute rank by counting, which has space complexity and average search complexity of O ( n ). (2) Keep a matrix of all the alphabet elements and all the locations on L and pre-compute all the values. This approach has the optimal constant search time but a space requirement of O ( n |  X  | ), which is too much given the fact that |  X  | grows relative to the size of the dataset. Given the large size of our alphabet, we chose a combination of a wavelet tree [23] and a three level architecture to support constant time rank operation over a bit sequence [26].

A wavelet tree is a perfect binary tree, with a bit sequence at each node representing the occurrences of a sequence of alphabet elements. The root represents  X  over L and each leaf represents one of the alphabet elements. A non-leaf node v represents alphabet elements  X  v = { e i e j } and contains a bit sequence B v = b i b j . For each i  X  k  X  j we have b k = 0, if L [ k ]  X  { e i e ( i + j ) / 2 } and b of v will represent elements of  X  in { e i e ( i + j ) / 2 right child represents alphabet elements { e ( i + j ) / 2+1 recursively. Thus, the algorithm for computing rank of an al -phabet element e  X   X  in prefix 1 . . . i of L , using the wavelet tree, would be as shown in Figure 4.
 Figure 4: Rank function computes the occurrences of e
In Figure 4, nodeRank ( Node, e, i ) counts the number of 1 X  X  in the prefix 1 . . . i at node Node . The count of 0 X  X  can be obtained by i  X  nodeRank ( Node, e, i ). Counting the number of 1 X  X  in each node by sequential scanning is very in-efficient . There are a few solutions, that provide constant-time acces s to binary rank values over a bit sequence [26]. In our work, we used a solution which uses n + o ( n ) bits of additional stor-age at each node, where n is the length of the bit sequence in the node. Figure 5 depicts our wavelet tree solution over L for the example of Section 3.1. For the nodeRank to operate in constant time, two arrays are maintained at each node, namely sbr and br 4 . For each node, sbr [ i ] stores the count of 1 X  X  in the range [ b 0 . . . b i  X  S 2 i  X  { 0 . . .  X  n S 2 [ b (sr) is pre-populated, which stores the binary rank values for bit sequences of size t =  X  S b / 2  X  + 1.

Recall that nodeRank function returns the rank of a prefix of the bit string stored at a given node. As depicted in Figure 6, nodeRank uses sbr , br and sr arrays to compute the rank in constant time. nodeRank is computed as shown in Figure 6. In this figure, b 2 d ( bs, p, len ) returns the decimal equivalent of the bit sub-sequence bs p . . . bs p + len .
Ferragina and Manzini in [20] benefit from the proper-ties of the Burrows-Wheeler transformation discussed in Se c-
These stand for super block rank and block rank , re-spectively Figure 6: A constant-time nodeRank , returning binary tion 3.1 and propose backwardSearch algorithm, which searches for a pattern over PI in backward order and returns the range of matching strings. The term-level adaptation of backwardSearch over WPI is depicted in Figure 7. Given a sequence of natural language words P = p 1 p q , back-wardSearch finds the range [ first, last ] of the sorted cyclic rotations prefixed by P . For the example provided in Fig-ure 3, backwardSearch returns the range [7 , 8] for the pat-tern P =  X  X ome is X  , which is the range of cyclic rotations prefixed by P . backwardSearch makes O ( | P | ) accesses to C and Rank . We adjust the hash table size so that it provides constant time access to hash elements. The wavelet tree access for Rank requires traversing from the root to one of the leaves which requires O (log |  X  | ) accesses to the tree nodes. Thus, backwardSearch has a complexity of O ( | P | log |  X  | ). Figure 7: backwardSearch algorithm for traversing L in backward order
Adding delimiters and sorting stings as discussed in Sec-tion 3.1, permuterm index supports wild-card pattern match -ing over dictionary strings. More specifically, it supports (1) Prefix ($  X  %), (2) Suffix (%  X  $), (3) Substring (  X  ) and (4) PrefixSuffix ($  X  %  X  $) queries where  X  ,  X  and  X  are ar-bitrary sequences of characters [21]. We adapted the above four queries to wild-card keyword matching over natural lan -guage text. Thus in our queries  X  ,  X  and  X  are sequences of natural language text words. Moreover, we have added support for queries such as (5)  X  %, (6) %  X  , (7)  X  %  X  , (8)  X  %  X  $ and (9) $  X  %  X  where  X  or  X  could be in arbitrary places in the document. The set of queries supported by PI are very limited and we often need to search for natural language patterns that are neither a prefix nor a suffix in a document.

The key idea behind supporting wild card queries us-ing backwardSearch is to convert them into prefix searches over rotations. Table 3 gives a summary of how to evalu-ate wild card queries using backwardSearch . In this table, the columns from left to right display the different types of are stored. queries supported by WPI, the pattern(s) to invoke back-wardSearch with, the range of wild card keyword matches, and the time complexity of the query evaluations, respec-tively. As displayed in this table, the first six queries coul d be matched with only one call to backwardSearch , while the last three require two invocations of backwardSearch as the sequence of words are separated by a wild card. For these queries, first  X  and last  X  are the beginning and end of the range returned by backwardSearch when invoked by  X  . Re-call that backwardSearch returns only a range of matching rotations, prefixed by a given pattern. Therefore, it does no t provide any efficient support for extracting keyword matches for a wild card. We solved this problem by storing two ad-ditional lists, T and I F , where I F is the list of locations of elements of F over T ; hence T [ I F [ i ]] = F [ i ]. These lists require O ( n ) extra space. However, since the overall space consumption of the index is O ( n log |  X  | ), storing these addi-tional lists will not change the space complexity of WPI.
For the experiments we used all or parts of the following two text collections. (1) News Dataset is the AQUAINT corpus of English News Text [3], which we processed and extracted the sentences to be indexed. It contains around 18 million sentences and its size is more than 2 GBs. (2) Web Dataset is our crawl of the web done on May 2008, which contains around 2 million documents and is around 8 GBs in size.

We created three sets of wild card queries. (1) WHQ query-set was created by replacing the wh keywords in who and what questions from AOL query log [27] with a wild card. (2) SVO query-set was generated by randomly replac-ing the subject or the object of a Subject-Verb-Object rela-tion with a wild card. We obtained the SVO relationships using the minipar dependency parser [6]. Finally, (3) n-gram query-set was generated by randomly replacing a keyword with a wild card in an n-gram, with n = 1 .. 5. These n-grams were selected according to their number of bindings in our datasets, in an attempt to cover a wide range of bindings.
WPI is a memory-based index, hence to be fair to other indexes we assigned in our experiments as much cache to the inverted and neighbor indexes as the memory used by WPI. We ran each query multiple times and only considered the last running time, in order to make sure cache is being uti-lized by the querying engine. Neighbor and inverted indexes were implemented over Berkeley DB, with the terms as the keys and posting lists as values.
Our first set of experiments compared the performance of the indexes under different settings, in terms of the average running time of queries in seconds. Table 4 gives a summary of the performance of each index over 10 million sentences of news data and 1 million documents of the web data and all the query sets.

As Table 4 suggests, WPI performs the best among all in-dexes on any combination of data and query sets. The third row of the table shows the average number of bindings per query for each query and data set used. Neighbor index per-forms relatively good when the number of bindings are high. Inverted index performs very poorly on queries that match a large number of documents. MemScan performs relatively slow regardless of what type of query is given. The statisti-cal correlation of the running time of queries over indexes i s largest for inverted index and smallest for WPI. These cor-relations reflect how the indexes perform when the number of bindings grow. Figures 8 and 9 depict the behavior of these four methods with respect to the number of bindings [last]+ | $  X  | ] O ( | $  X  | log |  X  | ) 1 O ( |  X  | log |  X  | ) [last]+ |  X  | ] O ( |  X  | log |  X  | ) ]+ |  X  | ], k  X  k  X  k  X  k O [( |  X  | + |  X  | ) log |  X  | ] + ]-1], k  X  k &gt; k  X  k O [ min ( k  X  k|  X  | , k  X  k|  X  | )] ]+ |  X  | ], k  X  k  X  k  X  $ k O [( |  X  | + |  X  $ | ) log |  X  | ] + ]-1], k  X  k &gt; k  X  $ k O [ min ( k  X  k|  X  $ | , k  X  $ k|  X  | )] ]+ | $  X  | ], k $  X  k  X  k  X  k O [( | $  X  | + |  X  | ) log |  X  | ] + ]-1], k $  X  k &gt; k  X  k O [ min ( k $  X  k|  X  | , k  X  k| $  X  | )] Table 4: Summary of the performance of the indexes in terms of the running time in seconds of a query, plotted over 100 n-gram queries over 10 million sentences of news data and 1 million documents of web data, respectively. As these figures show, the running time of WPI is almost entirely independent of the number of bindings of the query. For the data presented in these figures, on aver-age WPI is 5 orders of magnitude faster than the neighbor index. The worst case performance of WPI is still an order of magnitude faster than the neighbor index whereas in its best case, WPI is 6-7 orders of magnitude faster. The worst case, observed as a spike in Figures 8 and 9 for the run-ning time of WPI, belongs to the query  X  X he % of X  . The running time of WPI on this particular query is relatively higher because the query is of type  X  %  X  whose running time complexity is decided by k  X  k and k  X  k according to Table 3. Since  X  = X  X he X  and  X  = X  X f X  are the two highest selective words in the alphabet, we observe the spike in these two figures.
In order to compare the scalability of the indexes we con-ducted another experiment to compare how the indexes per-form as the dataset size grows. Figure 10 shows the total querying time of the four indexes over 1000 SVO queries computed over web datasets of size 0.4, 0.8, 1.2, 1.6 and 2 million documents. As this figure shows, the running time of WPI stays almost constant. Starting as low as 0 . 095 sec-onds for 0 . 4 million documents and going up to at most 0 . 118 seconds for 2 million documents, WPI shows only 24% growth in the overall querying time. The running times of neighbor and inverted indexes grow almost linearly with the dataset size. The minimum (maximum) running times Figure 8: The performance of the indexes based on the number of bindings of queries over 10 million sentences of news data are 370 (1693) and 600 (2211) for neighbor and inverted in-dexes, respectively. Finally, MemScan shows an exponentia l growth with respect to the dataset size. The maximum run-ning time (for 2 million documents) shows almost two orders of magnitude growth with respect to the minimum running time of MemScan.
Given that WPI is a memory-based index, it is impor-tant to evaluate its performance in settings where the space consumption of WPI exceeds the available system physical memory. This is a worst-case scenario for WPI whereas in-verted and neighbor indexes are not expected to be affected much by limitations on the size of memory. A straight-forward solution would be to use disk as a supplementary storage and allocate more memory than available and let the operating system do the paging 5 (i.e. decide which memory blocks to swap with disk). In an attempt to push WPI to the terms swapping and paging are used interchangeably in this paper Figure 9: The performance of the indexes based on the number of bindings of queries over 1 million doc-uments of web data do paging, we ran a set of experiments on the news data of sizes 4, 6, 8, 10, 12, 14, 16 and 18 million sentences and web data of sizes 0.4, 0.8, 1.2, 1.6 and 2 million documents. We used a machine with 4 GBs of physical memory, around 0.8 GB of which was reserved by a distribution of the Linux operating system for kernel and other system processes. We report here the amount of memory that was required for storing all data structures required by WPI, as a percentage of the available system physical memory. These memory re-quirements are depicted on the horizontal axis of Figures 11 and 12 for different sizes of data. The reported values are not the peak memory usage of operating system for WPI process as the process needed additional memory for code, stack and other static and dynamic data items. Hence, the amount of memory the process required exceeded the above figures, and paging could happen for the smaller datasets as well.
 Figures 11 and 12 show the total running time of 1000 SVO queries over WPI and neighbor index as the datasets vary in size. As Figure 11 shows, WPI X  X  running time grows dramatically as its size grows to 80% of the memory size. This shows the effect of paging on the WPI process. More-over, as the figures show, even when paging happens, the running time of WPI is still much lower than the neighbor index. By increasing the swap size, we were able to run WPI over datasets that required memory equal to approximately 10 times that of the available system memory. For large datasets, a major part of the index resides over disk and increasing the dataset size, as our results suggest, does no t dramatically change the running time of the queries. Even with such a naive disk-based solution to WPI, it performs pretty well and can scale up well with limited available mem-ory.

The total running times of queries for the inverted index and MemScan exceed those of the neighbor index in Fig-ures 11 and 12 and have been omitted for brevity.
Table 5 shows the time required to construct WPI com-pared to neighbor index for our experiment in Section 4.3. Figure 10: Scalability of the indexes over web data of growing sizes.
 As this table suggests, the construction time of WPI is smaller than neighbor index for the given sets of data. In most cases, inverted index has a slightly lower constructio n time than WPI and memory scan can be considered as hav-ing no construction time except loading the dataset once into the main memory.
 Table 5: Index construction time of WPI compared to the neighbor index in seconds
Querying over natural language text is often addressed in the literature by indexes that are based on inverted lists. For large text corpora, these indexes run into the problem of high costs of intersecting long posting lists. As a result , solutions for multiple keywords have been proposed that ma-terialize posting lists for more than one keyword. Examples are the works on phrase index and nextword index [11, 30]. Phrase index extracts natural language phrases from a query log and stores inverted lists for such phrases. A nextword in -dex, for each term, keeps a list of high frequency terms that follow it in the text and the pair X  X  corresponding inverted list. Chaudhuri et al. [17] propose breaking long posting lists into smaller ones by storing lists for multiple keywor ds. As a result they can guarantee an upper bound for the worst case running time of the queries. The above works have no support for wild card queries. However, it would be interest -ing to compare the performance of these systems on keyword queries with WPI.

Recently, there has been an evolving trend in develop-ing indexes supporting fast sub-string searches over large Figure 11: The performance of WPI vs. neighbor index using paging on News Data of sizes 4, 6, 8, 10,12, 14, 16 and 18 million sentences text corpora. As a result, self-indexes 6 have been developed and many interesting problems associated with them have been proposed or solved [26]. One of the key ideas that led to the development of such indexes have been the idea of the Permuterm Index by Garfield [22]. Burrows-Wheeler transformation [13] discussed in Section 3.1 uses Garfield X  s permuterm index to build a self-index that is highly com-pressible. Ferragina and Manzini [20] propose algorithms f or searching patterns over BWT in time proportional to the length of the pattern. In order to do that, they benefit from constant time access to structures such as Count and Rank . As discussed by Navarro and Makinen in a survey on com-pressed full-text indexes [26], different structures could be used to provide constant-time access to Rank over a fixed size alphabet. For large variable size alphabets, as is the case for WPI, wavelet trees [23] are proposed. WPI benefits from the above ideas for solving full-text search over strin gs to improve querying over natural language text. More space efficient implementations of nodeRank function have been reported in [29]. Similar to WPI, keyword-based generaliza -tions of text index structures such as suffix arrays [19] and suffix trees [10] have been developed. Finally, Manning et al. [25] propose solutions for wild card queries with more than one wild card over Permuterm Index. They use the similar concept of materializing the range of matching ro-tations for one substring of query literals and intersectin g with the results obtained from the prefix range returned by the rest of the query literals. Storing T and I F , we would be able to answer queries with arbitrary number of wild card over WPI.

There X  X  a great deal of recent activity around compressing indexes that support full-text search. This emergence has been started since the introduction of the Burrows-Wheeler transformation in 1994 which has a high compressibility and is reversible, making it a good candidate for building a self -index. The state of the art research in this area suggests indexes that are nearly optimal in size and search time. Fer-ragina and Venturini [21] proposed Compressed Permuterm an index which could replace the text Figure 12: The performance of WPI vs. neighbor index using paging on Web Data of sizes 0.4, 0.8, 1.2, 1.6 and 2 million documents Index (CPI). CPI benefits from the high compressibility of the Burrows-Wheeler transformation. They propose the full indexing algorithms and asymptotic analysis and study the performance of CPI under different compression techniques and how it compares with other indexes such as a trie. There is also work on compressing natural language text databases in [12], suggesting high compression ratios are achievable . We did not dig into the subject of compression over our WPI and focused more on efficiency of indexing. However, since we are using the same concept of BW-transformation, our index could also benefit from the results achieved in this domain.
We discussed the development of Word Permuterm In-dex (WPI) which supports single wild card natural language queries. WPI fills in the gaps for a time-efficient index sup-porting a wide range of wild card queries over natural lan-guage text. In this paper we presented our data structures and algorithms. Our asymptotic analysis of the complexity bounds of querying over different indexes shows the better time complexity of WPI over other approaches. Our wide range of experiments show the large gap in the performance of WPI with neighbor and inverted indexes over all combi-nations of data and query sets and number of bindings. Our results also show that WPI performs better than neighbor index even in the lack of sufficient physical memory, result-ing in paging memory pages in and out of the disk which greatly reduces its performance.

Allowing the operating system to swap memory pages in and out of the disk is a naive approach for solving the high memory consumption of WPI. One future extension would be to benefit from the localities available in natural langua ge text to store WPI structures over disk in such a way to opti-mize the number of disk block accesses; hence, increasing th e efficiency. WPI X  X  high space consumption is currently one of its main drawbacks. As another improvement, Compression techniques can be used to reduce the size of WPI. Finally, as memory is getting cheaper and the indexing of natural language text is a data parallel task, one idea would be to generously allocate memory to WPI structures and build a distributed WPI. This research was supported by the Natural Sciences and Engineering Research Council and the BIN network. [1] Altavista. http://www.altavista.com . [2] Apache lucene. http://lucene.apache.org/java/2_ [3] The aquaint corpus of english news text. http: [4] Google. http://www.google.com . [5] Indri -language modeling meets inference networks. [6] Minipar home page. http: [7] Openephyra -ephyra question answering system. [8] Yahoo! search -web search. [9] Oracle text, an oracle technical white paper, 2005. [10] A. Andersson, N.J. Larsson, and K. Swanson. Suffix [11] D. Bahle, H.E. Williams, and J. Zobel. Efficient [12] N.R. Brisaboa, A. Fari  X na, G. Navarro, and J.R. [13] M. Burrows and D.J. Wheeler. A block-sorting lossless [14] M.J. Cafarella and O. Etzioni. A search engine for [15] M.J. Cafarella, C. Re, D. Suciu, and O. Etzioni. [16] S. Chakrabarti, K. Puniyani, and S. Das. Optimizing [17] S. Chaudhuri, K. Church, A.C. Konig, and L. Sui. [18] O. Etzioni, M.J. Cafarella, D. Downey, S. Kok, [19] P. Ferragina and J. Fischer. Suffix arrays on words. In [20] P. Ferragina and G. Manzini. Indexing compressed [21] P. Ferragina and R. Venturini. Compressed [22] E. Garfield. The permuterm subject index: An [23] R. Grossi, A. Gupta, and J.S. Vitter. High-order [24] A. Maier and H.J. Novak. Db2 X  X  full-text search [25] C.D. Manning, P. Raghavan, and H. Schutze. An [26] G. Navarro and V. Makinen. Compressed full-text [27] G. Pass, A. Chowdhury, and C. Torgeson. A picture of [28] D. Rafiei and H. Li. Data extraction from the web [29] R. Raman, V. Raman, and S.S. Rao. Succinct [30] H.E. Williams, J. Zobel, and D. Bahle. Fast phrase
