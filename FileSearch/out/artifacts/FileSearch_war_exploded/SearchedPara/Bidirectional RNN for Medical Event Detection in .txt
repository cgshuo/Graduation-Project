 EHRs report patient X  X  health, medical history and treatments compiled by medical staff at hospitals. It is well known that EHR notes contain information about medical events including medication, diagno-sis (or Indication), and adverse drug events (ADEs) etc. A medical event in this context can be described as a change in patient X  X  medical status. Identifying these events in a structured manner has many im-portant clinical applications such as discovery of ab-normally high rate of adverse reaction events to a particular drug, surveillance of drug efficacy, etc. In this paper we treat EHR clinical event detection as a task of sequence labeling.

Sequence labeling in the context of machine learning refers to the task of learning to predict a la-bel for each data-point in a sequence of data-points. This learning framework has wide applications in many disciplines such as genomics, intrusion detec-tion, natural language processing, speech recogni-tion etc. However, sequence labeling in EHRs is a challenging task. Unlike text in the open domain, EHR notes are frequently noisy, containing incom-plete sentences, phrases and irregular use of lan-guage. In addition, EHR notes incorporate abundant abbreviations, rich medical jargons, and their varia-tions, which make recognizing semantically similar patterns in EHR notes difficult. Additionally, dif-ferent events exhibit different patterns and possess different prevalences. For example, while a medi-cation comprises of at most a few words of a noun, an ADE (e.g.,  X  X as not felt back to his normal self X ) may vary to comprise of a significant part of a sen-tence. While medication information is frequently described in EHRs, ADEs are typically rare events.
Rule-based and learning-based approaches have been developed to identify and extract informa-tion from EHR notes (Haerian et al., 2012), (Xu et al., 2010), (Friedman et al., 1994), (Aronson, 2001), (Polepalli Ramesh et al., 2014). Learning-based approaches use sequence labeling algorithms like Conditional Random Fields (Lafferty et al., 2001), Hidden Markov Models (Collier et al., 2000), and Max-entropy Markov Models (McCal-lum et al., 2000). One major drawback of these graphical models is that the label prediction at any time point only depends on its data instance and the immediate neighboring labels.

While this approach performs well in learning the distribution of the output labels, it has some limi-tations. One major limitation is that it is not de-signed to learn from dependencies which lie in the surrounding but not quite immediate neighborhood. Therefore, the feature vectors have to be explicitly modeled to include the surrounding contextual in-formation. Traditionally, bag of words representa-tion of surrounding context has shown reasonably good performance. However, the information con-tained in the bag of words vector is very sensitive to context window size. If the context window is too short, it will not include all the information. On the other hand if the context window is too large, it will compress the vital information with other irrel-evant words. Usually a way to tackle this problem is to try different context window sizes and use the one that gives the highest validation performance. However, this method cannot be easily applied to our task, because different medical events like med-ication, diagnosis or adverse drug reaction require different context window sizes. For example, while a medication can be determined by a context of two or three words containing the drug name, an adverse drug reaction would require the context of the entire sentence. As an example, this is a sentence from one of the EHRs,  X  X he follow-up needle biopsy results were consistent with bronchiolitis obliterans , which was likely due to the Bleomycin component of his ABVD chemo  X . In this sentence, the true labels are Adverse Drug Event(ADE) for  X  bronchiolitis oblit-erans  X  and Drugname for  X  ABVD chemo  X . However the ADE ,  X  bronchiolitis obliterans  X  could be miss-labeled as just another disease or symptom, if the entire sentence is not taken into context.

Recent advancements in Recurrent Neural Net-works (RNNs) have opened up new avenues of research in sequence labeling. Traditionally, re-current neural networks have been hard to train through Back-Propagation, because learning long term dependencies using simple recurrent neurons lead to problems like exploding or vanishing gra-dients (Bengio et al., 1994), (Hochreiter et al., 2001). Recent approaches have modified the sim-ple neuron structure in order to learn dependencies over longer intervals more efficiently. In this study, we evaluate the performance of two such neural net-works, namely, Long Short Term Memory (LSTM) and Gated Recurrent Units (GRU).

Timely identification of new drug toxicities is an unresolved clinical and public health problem, cost-ing people X  X  lives and billions of US dollars. In this study, we empirically evaluated LSTM and GRU on EHR notes, focusing on the clinically important task of detecting medication, diagnosis, and adverse drug event. To our knowledge, we are the first group re-porting the uses of RNN frameworks for information extraction in EHR notes. Medication and ADE detection is an important NLP task in biomedicine. Related existing NLP ap-proaches can be grouped into knowledge or rule-based, supervised machine learning, and hybrid ap-proaches. For example, Hazlehurst et al. (2005) de-veloped MediClass, a knowledge-based system that deploys a set of domain-specific logical rules for medical concept extraction. Wang et al. (2015) , Humphreys et.al. (1993) and others map EHR notes to medical concepts to an external knowledge resource using hybrid rule-based and syntactic pars-ing approaches. Gurulingappa et al. (2010) detect two medical entities ( disease and adverse events ) in a corpus of annotated Medline abstracts. In contrast, our work uses a corpus of actual medical notes and detects additional events and attributes.

Rochefort et al. (2015) developed document clas-sifiers to classify whether a clinical note contains deep venous thromboembolisms and pulmonary em-bolism. Haerian et al. (2012) applied distance supervision to identify terms (e.g., including  X  X ui-cidal X ,  X  X elf harm X , and  X  X iphenhydramine over-dose X ) associated with suicide events. Zuofeng Li et al. (2010) extracted medication information us-ing CRFs.

Many named entity recognition systems in the biomedical domain have been driven by the Shared tasks of BioNLP (Kim et al., 2009), BioCreAtivE (Hirschman et al., 2005) i2b2 shared NLP tasks (Li et al., 2009) and ShARe/CLEF evaluation tasks (Pradhan et al., 2014). The best performing clinical NLP systems for named entity recognition includes Tang et al (2013) which applied CRF and structured SVM.

Neural Network models like Convolutional Neu-ral Networks and Recurrent Neural Networks (LSTM, GRU) have recently been been success-fully used to tackle various sequence labeling prob-lems in NLP. Collobert (2011) used Convolutional Neural Network for sequence labeling problems like POS tagging, NER etc. . Later, Huang et al. (2015) achieved comparable or better scores using bi-directional LSTM based models. The annotated corpus contains 780 English EHR notes or 613,593 word tokens (an average of 786 words per note) from cancer patients who have been diagnosed with hematological malignancy. Each note was annotated by at least two annotators with inter-annotator agreement of 0.93 kappa. The anno-tated events and attributes and their instances in the annotated corpus are shown in Table 1.

The annotated events can be broadly divided into two groups, Medication , and Disease . The Medica-tion group contains Drugname , Dosage , Frequency , Duration and Route . It corresponds to information about medication events and their attributes. The at-tributes (Route, Frequency, Dosage, and Duration) of a medication (Drug name) occur less frequently than the Drugname tag itself, because few EHRs re-port complete attributes of an event.

The Disease group contains events related to dis-eases ( ADE , Indication , Other SSD ) and their at-tributes ( Severity ). An injury or disease can be la-beled as ADE , Indication , or Other SSD depending on the semantic context. It is marked as ADE if it is the side effect of a drug. It is marked as Indication if it is being diagnosed currently by the doctor and a medication has been prescribed for it. Any sign, symptom or disease that does not fall into the afore-mentioned two categories is labeled as Other SSD . Other SSD is the most common label in our corpus, because it is frequently used to label conditions in the past history of the patient.

For each note, we removed special characters that do not serve as punctuation and then split the note into sentences using regular expressions. 4.1 Long Short Term Memory Long Short Term Memory Networks (Hochreiter and Schmidhuber, 1997) are a type of Recurrent Neural Networks (RNNs). RNNs are modifications of feed-forward neural networks with recurrent con-nections. In a typical NN, the neuron output at time t is given by: Where W i is the weight matrix, b i is the bias term and  X  is the sigmoid activation function. In an RNN, the output of the neuron at time t  X  1 is fed back into the neuron. The new activation function now becomes: Since these RNNs use the previous outputs as recur-rent connections, their current output depends on the previous states. This property remembers previous information about the sequence, making them use-ful for sequence labeling tasks. RNNs can be trained through back-propagation through time. Bengio et al. (1994) showed that learning long term depen-dencies in recurrent neural networks through gradi-ent decent is difficult. This is mainly because the back-propagating error can frequently  X  X low-up X  or explode which makes convergence infeasible, or it can vanish which renders the network incapable of learning long term dependencies (Hochreiter et al., 2001).

In contrast, LSTM networks were proposed as so-lutions for the vanishing gradient problem and were designed to efficiently learn long term dependencies. LSTMs accomplish this by keeping an internal state that represents the memory cell of the LSTM neu-ron. This internal state can only be read and written through gates which control the information flowing through the cell state. The updates of various gates can be computed as: Here i t , f t and o t denote input, forget and output gate respectively. The forget and input gate deter-mine the contributions of the previous output and the current input, in the new cell state c t . The output gate controls how much of c t is exposed as the out-put. The new cell state c t and the output h t can be calculated as follows: c
The cell state stores relevant information from the previous time-steps. It can only be modified in an additive fashion via the input and forget gates. Sim-plistically, this can be viewed as allowing the error to flow back through the cell state unchecked till it back propagates to the time-step that added the rele-vant information. This nature allows LSTM to learn long term dependencies.

We use LSTM cells in the Neural Network setup shown in figure 1. Here x k , y k are the input word, and the predicted label for the k th word in the sen-tence. The embedding layer contains the word vec-tor mapping from words to dense n-dimensional vector representations. We initialize the embedding layer at the start of the training with word vectors calculated on the larger data corpus described in sec-tion 4.4. This ensures that words which are not seen frequently in the labeled data corpus still have a rea-sonable vector representation. This step is necessary because our unlabeled corpus is much larger than the labeled one.

The words are mapped into their corresponding vector representations and fed into the LSTM layer. The LSTM layer consists of two LSTM chains, one propagating in the forward direction and other in the backward direction. We concatenate the output from the two chains to form a combined representation of the word and its context. This concatenated vector is then fed into a feed-forward neuron with Softmax activation function. The Softmax activation function normalizes the outputs to produce probability like outputs for each label type j as follows: Here l t and u t are the label and the concatenated vector for each time step t . The most likely label at each word position is selected. The entire net-work is trained through back-propagation. The em-bedding vectors are also updated based on the back-propagated errors. 4.2 Gated Recurrent Units Gated Recurrent Unit (GRU) is another type of re-current neural network which was recently proposed for the purposes of Machine Translation by Cho et. al. (2014). Similar to LSTMs, Gated Recurrent Units also have an additive mechanism to update the cell state, with the current update. However, GRUs have a different mechanism to create the update. The candidate activation e h t is computed based on the previous cell state and the current input . Here r t is the reset gate and it controls the use of previous cell state while calculating the input acti-vation. The reset gate itself is also computed based on the previous cell activation h t  X  1 and the current candidate activation . The current cell state or activation is a linear combi-nation of previous cell activation and the candidate activation. Here, z t is the update gate which decides how much contribution the candidate activation and the previ-ous cell state should have in the cell activation. The update gate is computed using the following equa-tion: Gated recurrent units have some fundamental differ-ences with LSTM. For example, there is no mech-anism like the output gate which controls the expo-sure of the cell activation, instead the entire current cell activation is used as output. The mechanisms for using the previous output for the calculation of the current activation are also very different. Re-cent experiments (Chung et al., 2014), (Jozefow-icz et al., 2015) comparing both these architectures have shown GRUs to have comparable or sometimes better performance than LSTM in several tasks with long term dependencies.

We use GRU with the same Neural Network struc-ture as shown in Figure 1 by replacing the LSTM nodes with GRU. The embedding layer used here is also initialized in a similar fashion as the LSTM net-work. 4.3 The Baseline System CRFs have been widely used for sequence labeling tasks in NLP. CRFs model the complex dependence of the outputs in a sequence using Probabilistic Graphical Models. Probabilistic Graphical Models represent relationships between variables through a product of factors where each factor is only influ-enced by a smaller subset of the variables. A par-ticular factorization of the variables provides a spe-cific set of independence relations enforced on the data. Unlike Hidden Markov Models which model the joint p ( x,y ) , CRFs model the posterior probabil-ity p ( y | x ) directly. The conditional can be written as a product of factors as follows: Here Z is the partition function used for normaliza-tion,  X  t are the local factor functions.

CRFs are fed the word inputs and their corre-sponding skip-gram word embedding ( section 4.4). To compare CRFs with RNN, we add extra context feature for each word. This is done because our aim is to show that RNNs perform better than CRFs us-ing context windows. This extra feature consists of two vectors that are bag of words representation of the sentence sections before and after the word re-spectively. We add this feature to explicitly provide a mechanism that is somewhat similar to the sur-rounding context that is generated in a Bi-directional RNN as shown in Figure 1. This CRF model is re-ferred to as CRF-context in our paper. We also eval-uate a CRF-nocontext model, which trains a CRF without the context features.

The tagging scheme used with both CRF models is BIO (Begin, Inside and Outside). We did not use the more detailed BILOU scheme (Begin, Inside, Last, Outside, Unit) due to data sparsity in some of the rarer labels. 4.4 Skip-Gram Word Embeddings We use skip-gram word embeddings trained through a shallow neural network as shown by Mikolov et al., (2013) to initialize the embedding layer of the RNNs. This embedding is also used in the baseline CRF model as a feature. The embeddings are trained on a large unlabeled biomedical dataset, compiled from three sources, the English Wikipedia, an un-labeled EHR corpus, and PubMed Open Access ar-ticles. The English Wikipedia consists of text ex-tracted from all the articles of English Wikipedia 2015. The unlabeled EHR corpus contains 99,700 electronic health record notes. PubMed Open Ac-cess articles are obtained by extracting the raw text from all openly available PubMed articles. This combined raw text corpus contains more than 3 bil-lion word tokens. We convert all words to lowercase and use a context window of 10 words to train a 200 dimensional skip gram word embedding. For each word, the models were trained to predict either one of the nine medically relevant tags de-scribed in section 3, or the Outside label. The CRF tagger was run in two modes. The first mode (CRF X  nocontext) used only the current word and its cor-responding skip-gram representation. The second mode (CRF X  context) used the extra context feature described in section 4.3. The extra features are ba-sically the bag of words representation of the pre-ceding and following sections of the sentence. The first mode was used to compare the performance of CRF and RNN models when using the same input data. It also serves as a method of contrasting with CRF X  X  performance when context features are ex-plicitly added. CRF Tagger uses L-BFGS optimizer with L2-regularization.

The RNN frameworks are trained on sentence level and document level. The sentence level neural networks are fed only one sentence at a time. This means that the LSTM and GRU states are only pre-served and propagated within a sentence. The net-works cell states are re-initialized before each sen-tence. The document level neural networks are fed one document at a time, so they can learn context cues that reside outside of the sentence boundary. We use 100 dimensional hidden layer for each di-rectional RNN chain. Since we use bi-directional LSTMs and GRUs, this essentially amounts to a 200 dimensional recurrent hidden layer. The hidden layer activation functions for both RNN models are tanh . Output of this hidden layer is fed into a Soft-max output layer which emits probabilities for each of the nine medical labels and the Outside label. We use categorical cross entropy as the objective func-tion. Similar to the CRF implementation, the Neural Net cost function also contains an L2-regularization component. We also use dropout (Srivastava et al., 2014) as an additional measure to avoid over-fitting. Fifty percent dropout is used to manipulate the in-puts to the RNN and the Softmax layer. We use AdaGrad (Duchi et al., 2011) to optimize the net-work cost.

We use ten-fold cross validation to calculate the performance metric for each model. The dataset is divided at the note level. We separate out 10 % of the training set to form the validation set. This vali-dation set is used to evaluate the different parameter combinations for CRF and RNN models. We em-ploy early stopping to terminate the training run if the validation error increases consistently. We use a maximum of 40 epochs to train each network. The batch sizes used were kept constant at 128 for sen-tence level RNNs and 16 for document level RNNs.
We report micro-averaged recall, precision and f-score. We use exact phrase matching to calculate the evaluation score for our experiments. Each phrase labeled by the learned models is considered a true positive only if it matches the exact true boundary of the phrase and correctly labels all the words in the phrase.

We use CRFsuite (Okazaki, 2007) for implement-ing the CRF tagger. We use Lasagne to setup the ing library focused towards neural networks that is build on top of Theano (Bergstra et al., 2010). Table 2 shows the micro averaged scores for each method. All RNN models significantly outperform the baseline (CRF-context). Compared to the base-line system, our best system (GRU-document) im-proved the recall (0.8126), precision (0.7938) and F-score (0.8031) by 19% , 2% and 11 % respectively. Clearly the improvement in recall contributes more to the overall increase in system performance. The performance of different RNN models is almost sim-ilar, except for the GRU model which exhibits an F-score improvement of at least one percentage point over the rest.

The changes (gain or loss) in label wise F-score for each RNN model relative to the baseline CRF-context method are plotted in Figure 2. GRU-document exhibits the highest gain overall in six of the nine tags: indication or diagnosis, route , du-ration , severity , drug name , and other SSD . For indication, its gain is about 0.19, a near 50% in-crease over the baseline. While the overall sys-tem performance of GRU-sentence, LSTM-sentence and LSTM-document are very similar, they do ex-hibit somewhat varied performance for different la-bels. The sentence level models clearly outperform the document level RNNs (both GRU and LSTM) for ADE and Dosage . Additionally, GRU sentence model shows the highest gain in ADE f-score.
Figure 3 shows the word level confusion matrix of different models for each label. Each cell shows the percentage of word tokens in row label i that were classified as column label j . The consistent increase of diagonal entries of RNN models for all ten labels, indicates an increase in the overall system accuracy when compared to the baseline. The most densely populated column in this figure is the Outside col-umn, which denotes percentage of words that were erroneously labeled as Outside .

Figure 4 shows the change in average F-scores for each method with changing percentage of training data used. The setup for training, development and test data is kept the same as the ten-fold cross valida-tion setup mentioned in Section 5. Only the training data is randomly down-sampled to achieve the re-duced training data size. The figure shows that Re-current Neural Network models perform better than traditional CRF models even with smaller training data sizes. We already discussed in the previous section how improved recall seems to be the major reason behind improvements in the RNN F-score. This trend can be observed in Figure 3 where RNN models lead to significant decreases in confusion values present in Outside column.
Further examination of Figure 3 shows two ma-jor sources of error in the CRF systems. The largest source of error is caused by confusing the relevant medical words as Outside (false negatives) and vice versa (false positives). The extent of false positives is not clear from Figure 3, but can be estimated if one takes into account that even a 1 % confusion in the Outside row represents about 5000 words. The sec-ond largest source of error is the confusion among ADE , Indication and Other SSD labels. As we dis-cuss in the following paragraphs, RNNs manage to significantly reduce both these type of errors. The large improvement in recall of all labels for RNN models seems to suggest that RNNs are able to recognize a larger set of relevant patterns than CRF baselines. This supports our hypothesis that learning dependencies with variable context ranges is crucial for our task of medical information ex-traction from EHR notes. This is also evident from the reduced confusion among ADE , Indication and Other SSD . Since these tags share a common vocab-ulary of Sign, Symptom and Disease Names, identi-fying the underlying word or phrase is not enough to distinguish between the three. Use of relevant patterns from surrounding context is often needed as a discriminative cue. Consequently, ADE , Indi-cation confusion values in the Other SSD column for RNNs exhibit significant decreases when com-pared to CRF-nocontext and CRF-context. We also see large improvements in detecting Duration , Fre-quency and Severity . The vocabulary of these la-bels often lack specific medical jargon terms. Ex-amples of these labels include  X  X even days X ,  X  X ne week X  for duration ,  X  X ome X ,  X  X mall X ,  X  X o signifi-cant X  for severity and  X  X s needed X ,  X  X wice daily X  for frequency . Therefore, they are most likely to be con-fused with Outside label. This is indeed the case, as they have the highest confusion values in the Out-side column of CRF-nocontext. Including context in CRF improves the performance, but not as much as RNN models which decrease the confusion by al-most half or more in all cases. For example, GRU-document only confuses Frequency as an unlabeled word about 6.1 % of the time as opposed to 31 % and 19 % for CRF-nocontext and CRF-context re-spectively.

Document level models benefit by using context from outside the sentence. Since the label Indica-tion requires the most use of surrounding context, it is clear that its performance would improve by us-ing information from several sentences. Indications are diseases that are diagnosed by the medical staff, and the entire picture of the diagnosis is usually dis-tributed across multiple sentences. Analysis of ADE is more complicated. Several ADE instances in a sentence also contain explicit cues similar to  X  X ec-ondary to X  and  X  X aused by X . When coupled with Drugnames this is enough to classify the ADE. Sen-tence level models might depend more on these local cues which leads to improved performance. Docu-ment models, on the other hand, have to recognize patterns from a larger context, using a very small dataset (total ADE annotations are just 905) which is quite difficult.

The LSTM-document model does not show the same improvement over the sentence models as GRU-document. One possible reason for this might be the simpler recurrence structure of GRU neuron as compared to LSTM. Since there are only 780 document sequences in the dataset, the GRU model with a smaller number of trainable parameters might learn faster than LSTM. It is possible that with a larger dataset, LSTM might perform comparable to or better than GRU. However, our experiments with reducing the hidden layer size of LSTM-document model to control for the number of trainable param-eters did not produce any significant improvements.
Moreover, figure 4 seems to indicate that there is not much difference between the performances of LSTM and GRU with different data sizes. However it is clearly surprising that RNN models with a larger number of parameters can still perform better than CRF models on smaller dataset sizes. This might be because the embedding layer, which contributes to a very large section of the trainable parameters, is initialized with a suitably good estimate using skip-gram word embeddings described in section 4.4. We have shown that RNNs models like LSTM and GRU are valuable tools for extracting medical events and attributes from noisy natural language text of EHR notes. We believe that the significant im-provement provided by gated RNN models is due to their ability to remember information across dif-ferent range of dependencies as and when required. As mentioned previously in the introduction, this is very important for our task because different labels have different contextual dependencies. CRF mod-els with hand crafted features like bag of words rep-resentation, use fixed context windows and lose a lot of information in the process.

RNNs are excellent in extracting relevant patterns from sequence data. However, they do not explic-itly enforce constraints or dependencies over the output labels. We believe that adding a probabilis-tic graphical model framework for structured output prediction would further improve the performance of our system. This experiment remains as our fu-ture work.
 We thank the UMassMed annotation team, including Elaine Freund, Wiesong Liu and Steve Belknap, for creating the gold standard evaluation set used in this work. We also thank the anonymous reviewers for their comments and suggestions.

This work was supported in part by the grant 5U01CA180975 from the National Institutes of Health (NIH). We also acknowledge the support from the United States Department of Veterans Af-fairs (VA) through Award 1I01HX001457. The con-tents of this paper do not represent the views of the NIH, VA or United States Government.

