 Regular expressions are omnipresent in database applica-tions. They form the structural core of schema languages for XML, they are a fundamental ingredient for navigational queries in graph databases, and are being considered in lan-guages for upcoming technologies such as schema-and trans-formation languages for tabular data on the Web. In this paper we study the usage and effectiveness of the counting operator (or: limited repetition) in regular expressions. The counting operator is a popular extension which is part of the POSIX standard and therefore also present in regular expressions in grep, Java, Python, Perl, and Ruby. In a database context, expressions with counting appear in XML Schema and languages for querying graphs such as SPARQL 1.1 and Cypher.

We first present a practical study that suggests that coun-ters are extensively used in practice. We then investigate evaluation methods for such expressions and develop a new algorithm for efficient incremental evaluation. Finally, we conduct an extensive benchmark study that shows that ex-ploiting counting operators can lead to speed-ups of several orders of magnitude in a wide range of settings: normal and incremental evaluation on synthetic and real expressions. H.2.3 [ Database Management ]: Languages X  Data De-scription Languages (DDL) ; H.2.3 [ Database Management ]: Languages X  Query Languages) Languages, Algorithms XML; Schema; Regular Expressions  X  Supported by grant number MA 4938/2 X 1 from the Deutsche Forschungsgemeinschaft (Emmy Noether Nach-wuchsgruppe).

Regular expressions are omnipresent in programming lan-guages, shell tools, and database query-and schema lan-guages. In the context of XML, DTDs and XML Schema def-initions use regular expressions for defining content models and XPath uses regular-expression-like constructs for navi-gation in trees. In the context of graph-structured data and RDF, regular expressions have been studied under the name regular path queries (RPQs) for decades and have recently been integrated in popular graph query languages such as SPARQL [16] and Cypher [24]. Currently, they are being considered as a basic building block for processing tabular data on the Web [22].

We investigate the usage and effectiveness of regular ex-pressions with the counter operator, also known as the lim-ited repetition operator. This operator makes some expres-sions exponentially more succinct and permits a subexpres-sion to be matched anywhere between a given minimum and maximum number of times. For example, the expression (ab) { 9,20 } (in POSIX syntax) matches all words that con-sist of at least nine and at most twenty repetitions of ab . Expressions with counting are relevant to several authorita-tive languages for processing XML and RDF. In particular, they form the structural core of XML Schema Definitions, 1 where they are used to define content models [10]. In the context of RDF and graph query languages, counting op-erators are used in Cypher X  X  path patterns [24] and they have been considered in SPARQL 1.1 property paths [16]. However, the effect of counters in regular expressions is not yet fully understood, which is why they were removed from SPARQL 1.1. For this reason we want to contribute to the understanding of such succinct regular expressions and show that (1) counting operators are widely used in practice and therefore seem desirable to practitioners and (2) can be ex-ploited to develop highly efficient evaluation algorithms.
We start with a practical study that investigates how counters are used in regular expressions in practice. We in-vestigate RegExLib , the main regular expression repository available on the Web [26]; Snort , a system for detecting net-work intrusion [28], and a large corpus of XML Schema defi-nitions which we harvested from the Web. The data suggests that counters are used often and that counter values can be-come rather large. For example, in XML Schema we found values up to ten million. We also discovered that schemas of companies such as Amazon, Ebay, Microsoft, Oracle, and Paypal use expressions with counting.
The counting operators are called minOccurs/maxOccurs in XML Schema.
We then turn to efficient evaluation of expressions. The  X  X tandard X  evaluation problem for expressions is defined as: Given an expression e and a word w , can e be matched onto w ? However, many settings in databases call for a more re-fined version of evaluation. For example, the word w is usu-ally not static and is subject to updates. Examples of such settings are (a) schema constraints and integrity constraints that safeguard the validity of the data, and (b) trigger condi-tions that continuously monitor whether some action needs to be performed. In settings like these, one would like to perform incremental evaluation . Here, we start with a word w and an expression e that matches w , and then the word w is updated (see, e.g., [2, 3, 7]). We want to find out quickly if the expression still matches the updated word u ( w ). In order to do so, incremental evaluation algorithms may store auxiliary data . Naturally, we are interested in striking a good balance between the speed of handling the update and the size of the auxiliary data.

We develop an incremental evaluation algorithm for regu-lar expressions with counting and show that it is possible to avoid the usual exponential translation of such expressions to (standard) finite automata which is used by e.g., current implementations of tools like grep.

We then perform an extensive experimental study that evaluates the efficiency of our evaluation algorithms on syn-thetic and real-world expressions. As expected, avoiding the exponential blow-up to automata leads to significant speed-ups. When expressions have large counter values, we see speed-ups of several orders of magnitude in every setting we investigate. We look into both normal and incremental evaluation, on both synthetic and real-world expressions. Related work. Although incremental evaluation of queries and automata has attracted much attention, this is the first work on incremental evaluation of regular expressions with counters, to the best of our knowledge. In the context of databases, the most well studied problem in this context is incremental view maintenance . However, in contrast to that problem, where one aims at maintaining the (possibly large) output of a query, we only want to decide whether a query or a pattern can be matched or not. In this sense, the present work is much closer to incremental evaluation of XML schemas [2, 3] and the matching of XPath patterns [7]; a line of work going back to [25].

Efficient evaluation of regular expressions is also heavily investigated in deep packet inspection in networking (see, e.g., [4, 5]). In this context, fast evaluation of regular ex-pressions with counting has been investigated by Smith et al. [27]. While this line of work bears similarities to ours, the focus is quite different. Most prominently, the focus is on evaluating the expressions on a data stream . Our setting is more general because we allow insertions and deletions in arbitrary positions of the word, while a data stream is es-sentially a word which only receives insertions at the back.
Ghelli et al. [13] study fast evaluation for a class of reg-ular expression with counting and interleaving. In the ex-pressions in their study, each alphabet symbol can appear at most once and iteration (counters and Kleene star) can only be applied directly to alphabet symbols. They develop a linear-time algorithm but it does not work for all regular languages, whereas ours does. It would be interesting to in-vestigate if the advantages of both settings can be combined. Another setting where regular expressions are augmented with intervals was studied by Nakayama et al. [23], who also provide an efficient evaluation algorithm. There, however, the intervals do not signify repetition, but rather duration and the strings considered are ones where each letter has an assigned duration. Our incremental evaluation algorithm is designed for automata with counters , see, e.g., [18, 9].
The regular expressions over a set of symbols  X , denoted by RE, are defined as follows:  X  ,  X  , and every  X -symbol is in RE; and whenever r and s are in RE, then so are ( rs ), ( r + s ), and ( r  X  ). For readability, we omit parentheses where appropriate. The language defined by an expression r is de-noted by L ( r ) and defined as usual. We denote the regular expressions with counters as RE # and define them as fol-lows. Every RE-expression is an RE # -expression. Further-more, when r is an RE # -expression then so is r k,` for k  X  and `  X  N +  X  X  X  X  with k  X  ` . Here, N + denotes N \{ 0 } . Furthermore, L ( r k,` ) = S ` i = k ( L ( r )) i , where ( L ( r )) the i -fold concatenation (or repetition) of L ( r ). Notice that counters are just syntactic sugar. An expression r  X  RE # can always be converted to an equivalent RE by  X  X nfolding X  the counters. The blow-up is worst-case exponential.
We conducted an extensive practical study to get an idea of how counters are used in regular expressions. We investi-gated expressions in the RegExLib repository [26], in Snort rules [28], and conducted a thorough search for expressions with counters in XML Schemas on the Web.

In the remainder of this section, we say that an expression uses non-trivial counters if it has a subexpression of the form r k,` in which one of k or ` is at least two, i.e., in N  X  X  0 , 1 } .
In a nutshell, we discovered that RegExLib and Snort rules have a surprisingly large fraction ( &gt; 50%) of expres-sions with non-trivial counters. RegExLib and Snort have expressions with fairly large counter values (between 1K and 10K), but the largest counters we found occurred in XML Schemas and were up to ten million.
The RegExLib.com library describes itself as the Inter-net X  X  first Regular Expression Library [26]. It has a corpus of expressions for recognising URIs, markup code, pieces of Java code, SQL queries, spam, etc. We crawled the expres-sions of the library and obtained a set of 3024 expressions after removing duplicates and expressions that could not be parsed by Bart Kiers X  PCREParser available at Github. 2
Of these 3024 expressions, 1705 (about 56.3%) use non-trivial counters. This seems to indicate that counters are widely used in practice. We investigated the size of the counters and the nesting of non-trivial counters so that we could get an idea of how large equivalent expressions without counters would become. This estimates the internal blow-up that happens in tools like grep. For subexpressions of the form r k,` in the library, we found values of k ranging from 0 to 255 and values of ` ranging from 1 to 1500.

However, some expressions use nesting of counters, which lead to even larger minimal equivalent REs. We discovered that 86 expressions in the corpus use nesting of non-trivial counters. Several expressions in the corpus contain nested https://github.com/bkiers/PCREParser counters of the form ( r 3 ( r 2 ( r k 1 ,` 1 1 )) k 2 ,` ` = 255, and ` 3 = 255. The smallest equivalent RE would require at least 780,000 symbols. Another example uses a four-fold nesting of a counter 9, leading to equivalent REs of at least 10,000 symbols.

In this corpus, we see that expressions with non-trivial counters are used quite often, but often the values of the counters are also rather small. It is unclear whether this is because users do not need large counters or because there are no adequate tools for dealing with expressions involving large counters. (On our own computers, tools such as grep complain quickly when counters grow beyond 1000.)
The majority of the expressions in the corpus have simple parse trees. Most expressions are of the form  X  1 where each  X  i is of the form ( a 1 +  X  X  X  + a n ) k,`  X  possibly with n = 1, ( k,` ) = (1 , 1) or ( k,` ) = (0 ,  X  ). We refer to such expressions as CHAINs . Therefore, abc is a CHAIN and so is ( A + B + C ) 3 , 4 (0 + 1)  X  . However, ( abc ) because it has a concatenation nested within the counter. We classify 2217 expressions (73.3%) from this corpus as CHAINs. Notice that the remaining expressions also contain those that use regex-specific features (e.g., lookahead) or tests beyond regular languages (e.g., backreferences), etc.
Snort [28] is an open source system for preventing network intrusion and, according to its web site, the most widely de-ployed IDS/IPS technology worldwide. It has a set of freely available community rules , some of which contain regular ex-pressions. After distilling the expressions from the rules and removing duplicates, we obtained a set of 458 expressions, of which 270 (about 58.9%) use non-trivial counters. For subexpressions of the form r k,` we found values for k rang-ing from 0 to 1024 and values for ` ranging from 1 to 1075. We found only two expressions with nesting of non-trivial counters.

Equivalent REs for freely available Snort rules therefore do not become as large as for RegExLib expressions, but Snort has a larger fraction of expressions with non-trivial counters. For example, we found 100 expressions with a three-digit number and 4 with a four-digit number for k . For ` , 102 expressions have a three-digit value and 62 have a four-digit number. From a structural point of view, the expressions for Snort seemed to be similar to the ones from RegExLib. A quick analysis showed that at least 85.1% are CHAINs. Those that weren X  X  CHAINs typically used a feature that we don X  X  consider here (lookahead in regexes, for example) or a disjunction of words such as (http+ftp). We conducted a deep and labour-intense search for XML Schema Definitions (XSDs) on the Web. There have been studies of regular expressions in schema definitions in the past [6] but, as far as we know, this is the first study that looks at counters. We harvested XSDs from the Web by crawling the maven.org Central Repository and by using the API of Google X  X  Custom Search Engine (CSE) [15]. We chose the Maven Central Repository because it contains high-quality source code and meta-data for many open source projects 4 and because it showed up very often when we
Chain regular expressions were also studied in [13, 21].
See http://search.maven.org/#browse were manually googling for XSDs. We obtained XSDs from Maven by recursively crawling its entire directory, down-loading all .jar files from projects, and extracting the XSDs from those.
 We used Google X  X  CSE to find results on the entire Web. We experimented with several search engines (Bing and Ya-hoo) but Google returned a superset of the results we found using the alternatives and allowed us to search more pre-cisely. We used queries of the form (and variations thereof) to explicitly search for schemas with a maxoccurs value of X and to search for schemas with a maxoccurs value Z such that X  X  Z  X  Y . (Similarly for minoccurs.) Since Google CSE only allows to process around 100 queries per day, it was infeasible within our time constraints to conduct a query of type (1) for each possible counter value. Furthermore, a single query to Google X  X  CSE only returns at most 100 links. We therefore used queries of type (2) mainly for discovering new values of X that return non-empty results and then use type (1) to find all results with this new value.
 Data Cleaning. We extracted 4808 XSDs from the Maven Central Repository. Among them, 285 (about 6%) use a non-trivial counter value. Additionally, through Google we found 12,211 unique URLs outside Maven Central that match at least one of the queries of type (1) or (2) above. About 1400 of these URLs did not directly point to an XSD but to an HTML file that, somewhere, contains a link or path of links to an XSD. By resolving these links recursively and by also downloading XSDs that are referenced in other XSDs, we found 8944 more URLs claiming to point to a file with .xsd extension. From this data set, we removed everything that is not a valid XSD. This operation resulted in a total of 3259 unique URLs containing XSDs with non-trivial counter values that we obtained through Google.

We then removed duplicate files. More precisely, we parsed all XSDs we found, normalized the whitespace, and removed files that occurred more than once after this operation. In total, we obtained 1191 unique, well formed XSD files that contain non-trivial counter values; 906 through Google and 285 from the Maven Central Repository. This set of XSDs forms the input of the study that follows.
 Description of the Corpus. The 1191 schemas from our repository contain 9389 regular expressions with non-trivial counter values. Our repository (raw data and the clean cor-pus) can be found at http://regx.github.io/.

The distribution of counter values in our corpus is sum-marized in Fig. 1. Here we present, per possible finite max-occurs value (Fig. 1a) and minoccurs value (Fig. 1b), the number of regular expressions in which it was found. The largest value we found was 9,999,999 and occurred in two different versions of an XSD from IBM, related to electronic data interchange for administration, commerce, and trans-port (EDIFACT). 5
Notice that the total sum of count values in Fig. 1 is larger than 9389, because a single expression can contain multi-ple occurrences of counters. One example is the expression https://github.com/DFDLSchemas/EDIFACT/raw/master/ EDIFACT-SupplyChain-D03B/EDIFACT-SupplyChain-Messages-D.03B.xsd. This schema contains other large values too. val #exp val #exp val #exp val #exp 10 995 38 1 120 6 1536 3 11 27 39 6 127 14 2000 10 12 79 40 16 128 23 3000 1 13 31 45 5 136 2 5000 16 14 15 48 2 146 1 9999 87 15 162 50 101 150 1 20000 3 16 83 51 2 176 1 65025 5 17 9 52 3 192 1 65535 25 18 15 54 3 198 2 65536 17 19 17 59 1 200 26 99999 14 20 303 60 20 250 2 200000 6 21 7 62 6 255 3 999999 24 22 4 63 2 256 19 9999999 2 23 4 64 13 299 15 24 11 66 1 300 4 25 125 67 1 350 6 val #exp val #exp val #exp val #exp 2 1099 9 3 19 1 54 3 3 284 10 4 20 24 60 4 4 73 11 9 22 1 85 3 5 24 12 10 23 2 93 1 6 11 13 1 24 3 127 1 7 11 15 8 31 2 365 4 8 9 16 8 51 1 Figure 1: Non-trivial counter values occurring in our corpus. be related to the MPEG-7 standard. A brief inspection of our corpus exposes that several major companies use regu-lar expressions with non-trivial counters, for example: the Amazon Simple Storage Service (S3) schema 6 (maxoccurs 100), FedEx X  X  web services 7 (maxoccurs 12, 99, 999), Pay-pal X  X  WSDL interface 8 (maxoccurs 100, 1000), and several schemas for the MPEG-7 standard 9 use maxoccurs values up to 65636. We note that our corpus by no means contains all available XSDs on the Web with non-trivial counter val-ues. Some prominent examples that we did not find auto-matically can be found in Microsoft X  X  MSDN library, which http://doc.s3.amazonaws.com/2006-03-01/AmazonS3.xsd http://www.fedex.com/templates/components/apps/wpor/ secure/downloads/xml/Aug09/Advanced/ShipService v7.xsd http://www.paypalobjects.com/wsdl/eBLBaseComponents.xsd http://standards.iso.org/ittf/PubliclyAvailableStandards/ MPEG-7 schema files/mpeg7-v2.xsd Figure 2: Star-, counter-, and iteration depth for expressions in our corpus. contains schemas with maxoccurs values of, e.g., 50, 100, 256, 1000, and 100,000. 10 Structural Analysis. We analyzed the structure of regular expressions in our corpus, in the same spirit as was done by Bex et al. [6]. This allows us to compare results with [6], which was performed a decade ago. Bex et al. observed that a very large percentage of expressions in practice only use each alphabet symbol at most once. This observation still holds: in our corpus, 98% of all regular expressions use alphabet symbols at most once. A little bit of care is needed when comparing this number to [6]: In the latter study, non-trivial counter values in expressions were first rewritten to trivial ones, e.g., b 4 , 5 was rewritten to bbbbb ?. As such, the expression b 4 , 5 would not be counted as having a single b . The highest numbers of occurrences for the same symbol in one expression we found was 200.

When looking at the structural complexity of the expres-sions in our corpus, we see that they have shallow parse trees. For computing parse tree depth, we first normalize expressions by exploiting associativity of disjunction and concatenation. That is, when we see an expression of the form (( a + b ) + c ) + d , we normalize it to a + b + c + d and say that its parse tree has depth two, rather than four. After this preprocessing step, 8132 expressions have depth 3; 620 have depth 4; 542 have depth 5; 36 have depth 6; and 59 have depth ranging from 7 to 9.
 Next, we looked at the nesting of repetition operators. Fig. 2 contains the star depth of the expressions in our cor-pus, i.e., the number of nestings of operators that allow an unbounded number of repetitions of their associated subex-pression, i.e., operators of the form k,  X  for some k  X  N both expressions ( a 0 ,  X  ) 2 , 100 and a 2 ,  X  have star depth one.) Not surprisingly, most expressions do not use any nesting of such expressions at all. Only nine expressions in our entire corpus use nesting depth 2 or 3.

Also contained in Fig. 2 is the counter depth of regular expressions, which is the nesting depth of counters of the form ( k,` ) where at least one of k or ` is a natural number larger than 1. (So, both expressions ( a 0 ,  X  ) 2 , 100 have counter depth one.)
Finally, we define the iteration depth to be the nesting depth of any form of iteration, that is stars and counters. The former of the two aforementioned expressions has it-eration depth two and the latter has one. Fig. 2 suggests that expressions with non-trivial counters in XSDs are struc-turally rather simple. We suspect that this is because highly complex regular expression in an XSD may indicate that the design of the underlying XML database is not very elegant. A closer look at the expressions showed that 8330 expres-sions from our corpus with non-trivial counter values are http://msdn.microsoft.com/en-us/library/cc233001.aspx, http://msdn.microsoft.com/en-us/library/jj583348.aspx, http://msdn.microsoft.com/en-us/library/gg309601.aspx b ; { c b &lt; 12 } ; { c b ++ } CHAINs, which is about 88.7%. (Within the pool of all regular expressions we found in schemas, about 86% were CHAINs.)
Most algorithms that process regular expressions with count-ing (notably, grep and all XML Schema validation tools that we are aware of) first convert them to ordinary regular ex-pressions by expanding the counters. Subsequently, these (already exponentially large) expressions are converted to finite automata. This exponential blow-up can be avoided by directly converting the expressions into a much more memory-efficient representation: finite automata with coun-ters. Such automata have been used and implemented be-fore, for example in the context of deep packet inspection in networks [27], where significant improvements in speed and memory consumption were reported.

We provide a gentle introduction to such automata here and we compare them to other evaluation algorithms in Sec-tion 6.1. The performance gain is clear: compared to the above mentioned naive method for evaluation, we see im-provements of several orders of magnitude. The differences for incremental evaluation become even more drastic. Counter Automata by Example. Counter automata are a natural representation of expressions with counters that is easier to deal with in algorithms. We first discuss a counter automaton by example and explain them more for-mally next. Figure 3 depicts a counter automaton for the study in a schema related to the MPEG-7 standard. An ordinary non-deterministic finite state automaton for this expression would require 851955 states. The counter au-tomaton only has three: a start state q 0 and states for each occurrence of a symbol in the expression  X  q a for a and q b . (Given a regular expression with counters that has n oc-currences of symbols, we can always construct an equivalent counter automaton with n + 1 states.)
The automaton has two counters : c b , for counting the number of b  X  X  and c ab for counting how many times it has seen a word that matches ab 2 , 12 . Initially, all counters are set to 1.

The state transitions in the automaton consist of three ingredients: a symbol , a set of guards , and a set of counter updates . For example, the transition from q 0 to q symbol a , has guards c b = 1 and c ab = 1, and an empty set of counter updates. So, the transition reads an a -symbol and can be made when c b = c ab = 1. The next interesting transition is the b -loop on q b . It allows to read a b when c &lt; 12, and it increases c b by one. Finally, the transition from q b to q a reads an a , can be performed if c b is between 2 and 12 and c ab &lt; 65535. It increases c ab by one and overwrites c b with 1.

The automaton accepts if it is in an accepting state (dou-ble circles) and if the acceptance condition holds, that is, c ab is at most 65535. It is easy to see that the automaton accepts precisely the words that match ( ab 2 , 12 ) 0 , 65535 Formal Definition. We now define counter automata for-mally. Their definition is rather technical but essentially follow the lines of the example above. Readers who already understand the main idea can safely skip this definition.
Formally, counter automata are an extension of non-de-terministic finite automata, which we briefly recall here for the sake of clarifying notation. A non-deterministic finite automaton (NFA) is a tuple N = ( Q,  X  , X ,I,F ), where Q denotes its set of states,  X  its alphabet, I its set of initial states, and F its set of final or accepting states. The transi-tion rules  X  are of the form q 1 a  X  q 2 , indicating that reading an a  X   X  in state q 1 can bring the automaton to state q 2 Acceptance is defined in the standard manner. We denote by L ( N ) the set of words accepted by N .

Counter automata extend NFAs with counter variables or counters . We follow [11] in their definition. Let C be a finite set of counter variables and  X  : C  X  N be a function assign-ing a value to each counter variable. A guard over C is a function  X  : C  X  ( N  X  N  X  ), where N  X  denotes N  X  X  X  X  . Fur-thermore, we will require that, if  X  ( c ) = ( k,` ), then k  X  ` . The semantics of a guard is defined as follows. We say that a counter assignment  X  satisfies guard  X  if, for every c  X  C , whenever  X  ( c ) = ( k,` ), then k  X   X  ( c )  X  ` . Intuitively, this means that a guard defines, for each counter c , a minimum allowed value k and a maximum allowed value ` . As such, guards are used in counter automata to model the upper and lower bounds of counters in regular expressions. By  X  | =  X  we denote that the counter assignment  X  satisfies guard  X  . By Guards( C ) we denote the set of guards over C .
A basic update over C is a partial function  X  : C  X  X  reset , increment } . When  X  ( c ) = reset , this means that c should be reset to one and when  X  ( c ) = increment it means that c should be incremented by one. (If  X  is undefined on c , we leave c unchanged.) By Basic-Up( C ) we denote the set of all basic updates over C .

Definition 4.1. A (non-deterministic) automaton with counters (NFA # ) is a 6-tuple A = ( Q,q 0 ,C, X ,F, X  ) where
Intuitively, an NFA # A can make a transition ( q,a, X , X ,q  X  whenever it is in state q , reads label a , and the guard  X  is satisfied by the current values of the counter variables. It then updates the counter variables according to the update  X  (in a way which we explain next) and moves into state q . To explain the update mechanism formally, we use con-figurations . A configuration is a pair ( q, X  ) where q  X  Q is the current state and  X  : C  X  N is the function map-ping counter variables to their current value. An update  X  transforms  X  into  X  0 by setting  X  0 ( c ) := 1 whenever  X  ( c ) = reset and  X  0 ( c ) :=  X  ( c ) + 1 whenever  X  ( c ) = increment . We sometimes abuse notation and denote  X  0 by  X  (  X  ).
Let  X  init be the function mapping every counter vari-able to 1. The initial configuration  X  0 is ( q 0 , X  configuration ( q, X  ) is final if  X  | = F ( q ). A configuration  X  = ( q 0 , X  0 ) immediately follows a configuration  X  = ( q, X  ) by reading a  X   X , denoted  X   X  a  X  0 , if there exists ( q,a, X , X , q )  X   X  with  X  | =  X  and  X  0 =  X  (  X  ).

For a word w = a 1  X  X  X  a n and two configurations  X  and  X  , we denote by  X   X  w  X  0 that  X   X  a 1  X  X  X   X  a n  X  0 . A configuration  X  is reachable if there exists a word w such that  X  0  X  w  X  . A word w is accepted by A if  X  0  X  w  X  where  X  f is a final configuration. We denote by L ( A ) the set of words accepted by A .

The size of a transition  X  or acceptance condition F ( q ) is the total number of occurrences of alphabet symbols, states, counter variables, and Boolean connectives which occur in it, plus the size of the binary representation of each integer occcurring in it. In the same spirit, the size of A , denoted by | A | , is | Q | + P q  X  Q log  X  ( q ) + | F ( q ) | + P
It is known that RE # expressions can be efficiently trans-lated into equivalent NFA # s by applying a natural extension of the known Glushkov construction [11, 29].
We present a new incremental evaluation algorithm for regular expressions with counters. To the best of our knowl-edge, this is the first such algorithm that avoids an exponen-tial translation of the expressions. In Section 6 we see that incremental evaluation outperforms evaluation from scratch by factors up to three orders of magnitude and, shifting to automata with counters improves up to another three or-ders of magnitude. In fact, such speed-ups can be expected: incremental evaluation is an exponential improvement over evaluation from scratch; and doing it with counter automata is an yet another exponential improvement over doing it with ordinary automata.

As a warm-up, we recall how to incrementally evaluate or-dinary regular expressions on words. The technique was first described by Patnaik and Immerman [25] and was studied in more detail and implemented by Balmin et al. [2]. Incremental Evaluation of REs and NFAs. Assume that we have a word w = a 1  X  X  X  a n  X   X   X  and a regular ex-pression r and we want to incrementally maintain whether w  X  L ( r ). The incremental evaluation problem consists of two phases: a one-time preprocessing phase in which we con-struct an auxiliary data structure that we will maintain dur-ing updates; and an evaluation phase in which updates of i  X  X  1 ,...,n } is a position in the word and a  X   X  is a sym-bol. The updates do the obvious: relabel changes a i to a , delete deletes the symbol a i and results in a word of length n  X  1, and insert ( i,a ) inserts an a before position i , resulting in a word of length n + 1. We denote the newly obtained word by w 0 . In the evaluation phase, the task is to be able to say whether w 0  X  L ( N ) quickly after the update arrives. So, after a one-time preprocessing phase, we want to be able to deal with (multiple) updates quickly. Here, we describe a method for incremental evaluation that uses O ( n  X | r | 2 for preprocessing and then, in the update phase, can answer in time O ( | r | 3  X  log n ) per update whether the new word w is still in L ( r ) or not. Therefore, when n is large, the proce-dure is much faster than re-evaluating the expression from scratch, which would require at least linear time in n .
The incremental update algorithm works on the NFA for r , which can be constructed in linear time and which we denote by N = ( Q,  X  , X ,I,F ). We first describe the auxiliary data structure we will maintain during the updates. For each i,j , 1  X  i &lt; j  X  n , let T ij be the transition relation { ( p,q ) | p,q  X  Q,p a i  X  X  X  a j  X  X  X  X  X  X  X  q } , where p a i that N can reach state q when it starts in state p and reads a where ./ denotes the natural join on binary relations, that is, T ij contains all ( x,z ) such that there is a y with ( x,y )  X  T
For simplicity, assume first that n is a power of 2, say n = 2 . The main idea is to keep as auxiliary information just the T ij for intervals [ i,j ] obtained by recursively splitting [1 ,n ] into halves, until i = j . More precisely, consider the transition relation tree T n whose nodes are sets T ij , defined inductively as follows: Note that T n has n + ( n/ 2) +  X  X  X  + 2 + 1 = 2 n  X  1 nodes and has depth log n . Thus, the size of the auxiliary structure is O ( n  X | Q | 2 ).

The preprocessing phase consists of building this auxiliary data structure. Notice that, once we have T n , it is easy to decide whether w  X  L ( N ). Indeed, w  X  L ( N ) if and only if ( q,f )  X  T 1 n for some q  X  I and f  X  F . Therefore, we only have to show that this auxiliary data structure can be updated efficiently.

We now describe the evaluation phase . For simplicity, consider the case when one update occurs, changing the label of the symbol in w at position k to b . That is, the new word is w 0 = a 1  X  X  X  a k  X  1 ba k +1  X  X  X  a n . The relations T are affected by the updates are those lying on the path from the leaf T kk to the root of T n . Denote this set of relations by I and notice that it contains at most log n relations. The tree T n is updated by recomputing the relations in I bottom-up as follows: First, the leaf relation T kk is set according to  X  and b . Then each T ij  X  I with children T 0 and T 00 which one has been recomputed, is replaced by T 0 ./ T Thus, at most log n relations have been recomputed, each in time O ( | Q | 3 ), yielding a total time of O ( | Q | we arrive at the root, we know that w 0  X  L ( N ) if and only if ( q,f )  X  T 1 n for some q  X  I and f  X  F .

The above approach can easily be adapted to words whose length is not a power of 2. Further, the auxiliary data struc-ture has size O ( n  X | Q | 2 ). Finally, handling updates in which elements are inserted or deleted is also done in [2], but then some precautions have to be taken in order to make sure that the tree T n remains properly balanced.

Theorem 5.1 ([25]). Incremental evaluation of an RE r on a word w is possible with preprocessing time O ( n  X  | r | 3 ) , an auxiliary data structure of size O ( n  X | r | 2 ) , and time O (log n  X | r | 3 ) per update.
Using fast matrix multiplication algorithms, this time can be improved to O ( | Q |  X   X  log n ) (where  X  denotes the best known bound for matrix multiplication). The Incremental Evaluation Algorithm for RE # s. The incremental evaluation algorithm for RE # s extends the one for REs. It is based on NFA # s rather than NFAs and it does not translate RE # s to exponentially larger automata.
Intuitively, the algorithm for RE # s follows the same lines as the algoritm for REs, but it (1) stores different informa-tion at the nodes of the auxiliary tree and, subsequently also (2) uses a different algorithm for joining the information in neighboring nodes. We now describe these two changes. New information in the auxiliary tree. In the algorithm for REs, a tuple ( p,q ) is in T ij if and only if the automaton N can read a i  X  X  X  a j when going from state p to state q . When we want to do something similar for NFA # s, we need to take the counters in account. To this end, a (general) update over C is a function  X  : C  X  ( { inc , assign } X  N readability we often consider  X  as a set of statements of the form inc ( c,k ) or assign ( c,k ). (Hence,  X  only contains one of inc ( c,k ) or assign ( c,k ) for each counter c .) Intuitively, when  X  contains inc ( c,k ), then counter c should be incremented with k . If it contains assign ( c,k ), then c is assigned the value k , so it is overwritten. We need assign because if we perform some sequence of transitions in which a counter c is reset, then c should be assigned a value k , which will be the number of increments that were done to c after the last reset plus 1. 12 To this end, we use a rule assign ( c,k ). If we model a sequence of updates without a reset, then we use a rule inc ( c,k ), where k is the number of increments to c in the sequence. We write Updates( C ) for the set of all general updates over C .

Formally, an update  X   X  Updates( C ) transforms counter assignment  X  into  X  0 by setting  X  0 ( c ) = k if  X  contains We sometimes denote  X  0 by  X  (  X  ). Thus, if  X  is transformed into  X  0 by  X  , then  X  (  X  )( c ) =  X  0 ( c ). For the remaining defi-nitions of this section, we fix a NFA # A = ( Q,q 0 ,C, X ,F, X  ). In the incremental evaluation algorithm for RE # s, the sets T ij contain transformation tuples :
Definition 5.2. A transformation tuple of A is a quadru-ple t  X  Q  X  Guards( C )  X  Updates( C )  X  Q . Tuple t = ( p, X , X ,q ) is consistent with a string w if, for every config-uration  X  = ( p, X  ) such that  X  | =  X  , there is a configuration  X  = ( q, X  0 ) such that  X   X  w  X  0 in A and  X  0 =  X  (  X  ). That is, a transformation tuple is consistent with w if it captures the effect of w on the NFA # , i.e., if it describes how configurations that match it change by reading w . Example 5.3. Consider the NFA # ( Q,q 0 ,C, X ,F, X  ) from Figure 3. The tuple t 1 = ( q 0 , X  1 , X  1 ,q b ) where  X  w = abbb . (Intuitively, if the NFA # starts in q 0 and reads w , it ends up in state q b and increases c b by two.) If  X  1 only require 1  X  c b  X  2, the tuple would not be consistent with w , because we can only read a in q 0 if c b = 1.
The tuple t 2 = ( q b , X  2 , X  2 ,q b ) where  X  2 expresses and  X  2 contains assign ( c b , 3) and inc ( c ab , 1) is consis-tent with babbb and also with bbbabbb . It is not consistent with abbb (because we cannot go from q b to q a if c b = 1) or with bbbbabbb (because we cannot read b in q b if c b = 12).
The plus one term comes from the fact that the counters are reset to 1.
 So, the auxiliary data structure for incremental evaluation of RE # s is a binary tree T n whose nodes are sets T ij that con-tain precisely the transformation tuples that are consistent with a i  X  X  X  a j .
 Joining of transformation tuples. It remains to explain how the transformation tuples can be computed and updated. To this end, we merely have to redefine the join operation ./ we used in the algorithm for REs. In other words, we must define when transformation tuples t 1 = ( p 1 , X  1 , X  and t 2 = ( p 2 , X  2 , X  2 ,q 2 ) can be joined. To this end, we say that t 1 is compatible with t 2 if q 1 = p 2 and there is a counter assignment  X  such that  X  | =  X  1 and  X  1 (  X  ) | =  X  Intuitively, if t 1 is consistent with word w 1 , t 2 is consistent with word w 2 , and t 1 is compatible with t 2 , then there is a counter assignment  X  such that ( p 1 , X  )  X  w 1 w 2 ( q 2  X  =  X  2 (  X  1 (  X  )).

Example 5.4. For the NFA # of Figure 3, the tuple t from Example 5.3 is compatible with t 2 . For example, for the assignment  X  0 with c b = 1 and c ab = 1 we have that  X  1 :=  X  1 (  X  0 ) maps c b to 3 and c ab to 1. Therefore,  X  As such, when in configuration ( q 0 , X  0 ) the NFA # can read strings abbbbabbb or abbbbbbabbb .
 We now define how transformation tuples are joined.
Definition 5.5. Let t 1 = ( p, X  1 , X  1 ,r ) be compatible to t 2 = ( r, X  2 , X  2 ,q ). Then the join of t 1 with t 2 t ./ c t 2 , is the tuple ( p, X , X ,q ) such that, for every counter assignment  X  , we have (i)  X  | =  X   X  (  X  | =  X  1  X   X  1 (  X  ) | =  X  and (ii)  X  (  X  ) =  X  2 (  X  1 (  X  )).

Example 5.6. The join of t 1 with t 2 from Example 5.3 is ( q 0 , X , X ,q b ), where  X  expresses and  X  contains assign ( c b , 3) and inc ( c ab , 1). Consider tu-ple t 0 1 = ( q b , X  0 1 , X  0 1 ,q b ) where  X  0 1 expresses and  X  0 1 contains inc ( c b , 2) and inc ( c ab , 0), then t patible with t 2 and t 0 1 ./ c t 2 is ( q b , X  0 , X  0 ,q presses and  X  0 contains assign ( c b , 3) and inc ( c ab , 1). Intuitively, we have 3  X  c b  X  7 in the result because  X  0 1 requires 3  X  c and  X  2 requires  X  0 1 ( c b )  X  9. 2
When regular expressions contain few alphabet symbols and large counter values, as is typically the case in the CHAINs we found in the practical study (Section 3), the new algorithm performs much better. This is seen most clearly when considering RE # s of the form r = a k,k suming unit cost for basic arithmetic on numbers up to k , incremental evaluation for their NFA # s on a word of length n costs preprocessing time O ( n ) and update time O (log n ), whereas the algorithm of Section 5 would cost preprocessing speed-ups we obtain in Section 6 are therefore due to the fact that expressions with large counters we found in prac-tice seem to have a rather simple structure, which exploits the potential of NFA # s close to optimal.
We performed an extensive experimental study on reg-ular expression evaluation algorithms; incremental or oth-erwise. We implemented all algorithms and benchmarks in Java, mainly because it is very portable and wide-spread. To ensure comparability between measurements, we also reim-plemented all other algorithms in Java (and on the same underlying data structures). Since Java uses garbage collec-tion, there are several caveats (which we X  X l discuss later) for measuring memory consumption as well as execution time. All our experiments are conducted on a machine with an In-tel Core i7-2600K. We allocated a heap of at least 3 GiB for all JVM instances during our tests. All tests were executed on a 64-bit JVM.

We perform two kinds of experiments: evaluation from scratch (i.e., not dealing with updates) and incremental eval-uation (dealing with updates). In both settings, we compare algorithms based on NFAs to algorithms based on NFA # s. The NFA # variants are several orders of magnitude faster than the algorithms based on NFAs, scale much better, and consume much less memory. The experimental task in this section is the following: We compare the following algorithms: (RE#): Fast squaring algoritm for RE # (NFAsim): Building and simulating an NFA (NFA # sim): Building and simulating a NFA # (NFApre): Preprocessing for incremental NFA evaluation (NFA # pre): Preprocessing for incremental NFA # eval We discuss the five algorithms next. The benchmark (RE#) is an optimization of an algorithm by Kilpel  X  ainen and Tuhka-nen [18] which is the only polynomial time algorithm for evaluating RE # we are aware of that does not translate to automata. 13 It considers the parse tree of the expression r and computes bottom-up, for each node u in the parse tree, all pairs ( i,j ) of positions in w = a 1  X  X  X  a n such that the sub-expression rooted at u matches the subword a i  X  X  X  a j . For example, when node u is labeled by a disjunction, its cor-responding relation can be obtained by taking the union of the relations of u  X  X  children. If u is a concatenation, its rela-tion is the composition (or natural join) of the relation of its children. If u is a  X  , we need to take the transitive-reflexive closure of the relation of its child. Lastly, if u is a count-ing operator (say, a k -fold iteration), its relation is the k -fold composition of the relation of its child. In this latter case lies the optimization. Instead of performing O ( k ) compositions as in [18], we comput the k -fold composition by fast squar-ing, which only costs O (log k ) compositions. This algorithm is known to have worst-case complexity O ( | w | m  X | r | ), where m is the best bound for multiplying two | w | X | w | zero-one matrices. We refer the reader to [18, 20] for further details of the algorithms and their complexity.

The (NFAsim) algorithm translates r into an NFA, which is then evaluated on w . The bottleneck of this approach lies at dealing with the counters of r . A counter value of k typ-ically results in  X ( k ) states in the NFA. Since the counter
Notice that rewriting an RE # into an equivalent RE al-ready costs exponential time. value is represented by log k bits in r , this constitutes expo-nential cost in | r | .

In (NFA # sim), we avoid this blow-up by compiling r into a NFA # of size only O ( | r | ), which we then evaluate on w .
The algorithms (NFApre) and (NFA # pre) are the prepro-cessing phases of the incremental evaluation algorithms of Section 5. Once these preprocessing phases are finished, one can decide in constant time whether w  X  L ( r ) by inspecting the root of the auxiliary data structure. By comparing the cost of the ( X  X re) with the ( X  X im) variants we can there-fore gauge the cost of the one-time overhead for building an auxiliary data structure that allows incremental evaluation for future updates.

We present three benchmarks: a sanity check , a worst-case synthetic benchmark with much non-determinism in expressions, and a benchmark that shows how large counter values behave in real-world and synthetic expressions. The benchmarks use expressions that are structurally very sim-ple, because (1) simplicity in the setup allows for a better understanding of the results and (2) we did not see differ-ent results on more complex setups; all behaviour we saw in more complex (real-world or synthetic) settings could be explained by these three experiments.

In order to get reliable measurements in Java we repeated each experiment about five hundred times, discarded the best and worst 10% to get rid of garbage cleaning artefacts, and took the average of the remaining ones. (We observed that this way we consistently obtained the same measure-ments when experiments were repeated.) Since this proce-dure makes experiments lengthy, we only measured up to 30 seconds. The charts we present are optimized for readability and drastically summarise the data points we measured. Sanity Check Benchmark. First we compared the five candidates for r = a  X  and w being words containing only a  X  X  (Fig. 4, left). The rationale behind this test is to see how the five methods compare if the NFA # has no advantage over the other methods and to gauge the overhead of NFA # versus NFAs in our implementation. Since a  X  can be con-verted to an NFA or NFA # within microseconds, we only see the time required to process the word. Here, fast squaring (RE#) is by far the slowest of all. The reason is its high com-plexity in terms of | w | . The incremental variants are about an order of magnitude slower than the non-incremental vari-ants. This factor can be explained by the extra cost of building the external data structure for the incremental al-gorithms. In this setting where the NFA # -based algorithms do not have any advantage over the NFA-based algorithms, the NFA # -based algorithms are marginally slower.
 Real-world and large counters. Next we compared the candidates on real-world and synthetic expressions that use very large counter values. The underlying idea of this test is to see how the candidates compare if the NFA # s can exploit counters to their maximum advantage. In this experiment we noticed that the behavior for real-world expressions was the exactly same than on synthetic ones. In Fig. 4, middle, we present for increasing values of n , the expression r = a n/ 2. We then perform membership tests of words of length n , n  X  n/ 4, n  X  n/ 4  X  1, n + n/ 4, and n + n/ 4 + 1 and take the average of the evaluation times we measured. It is striking that, throughout this experiment, the preprocessing phase for incremental NFA # evaluation is faster than conversion to an NFA and evaluating it. Synthetic worst-case. Finally we investigated synthetic expressions with counting, but with a high amount of non-determinism (Fig. 4, right). We consider expressions r for which the minimal deterministic finite automaton is dou-ble exponentially larger than r . We perform this test be-cause the two previous experiments concerned very sim-ple deterministic expressions and it is well-known that non-determinism complicates fast evaluation.

The most striking and perhaps counter-intuitive fact about this experiment is that (NFA # pre) is the fastest evaluation algorithm in this case. We believe that this is so because the evaluation algorithms for NFAs (resp., NFA # s) need to store and maintain a large number of possible states (resp., con-figurations) while reading the word. Since (NFA # pre) works completely differently and can summarize configurations, it even outperforms the standard evaluation algorithms.
A second striking fact is that non-determinism combined with large counter values can bring most evaluation algo-rithms to their knees quickly. This may be the reason why the XML Schema specification only allows very limited non-determinism in regular expressions (cfr. [11]).
We compare the two variants of incremental evaluation: (incNFA) Incremental NFA evaluation (incNFA # ) Incremental NFA # evaluation For the other evaluation algorithms of Section 6.1, the cost of incremental evaluation will be the same as for evaluation from scratch so we don X  X  include them again here. Our im-plementations of (incNFA) and (incNFA # ) closely follows Section 5. Auxiliary data is stored as AA trees [1].
We use the following methodology to produce our mea-surements (and to compensate for the side-effects of garbage collection). Given a regular expression and a word, we first construct the auxiliary data structures to prepare for incre-mental evaluation. We then perform a large (and equal) number of insertions and deletions in the word; equally dis-tributed at the beginning, middle, and end of the word. We then discard the 10% largest and 10% smallest measure-ments and take the average of the remaining ones.

We perform experiments of two kinds for increasing values of n : one lets the expression and the word grow simultane-ously with n and the other keeps the expression constant and only lets the word grow. As such we obtain a detailed insight in how the scalability depends on the word and on the expression.
 Expressions with Large Counter Values. The leftmost graph in Fig. 5 contains our measurements for (incNFA) and (incNFA # ) for expressions with large counter values as al-ready described in Section 6.1: For a given value of n , we with the word w = a n . We then perform a large number of insertions and deletions and measure the average time of such an operation (as described before). Throughout the experiment, (incNFA # ) outperforms (incNFA) and scales much better. Even for n = 1 , 000 , 000, (incNFA # ) is faster than (incNFA) for n = 50; so, here, (incNFA # ) is deal-ing with a word about five orders of magnitude larger than (incNFA). Lastly, recall that we do not have time measure-ments for (incNFA) for n &gt; 1000 since preprocessing timed out. We also measured memory consumption (not in a sep-arate chart for reasons of space) and noticed that, even for n = 100K, (incNFA # ) consumes less memory than (incNFA) with n = 1K.
 Synthetic worst-case. The second graph in Fig 5 de-picts the incremental version of the corresponding experi-ment in Section 6.1. This test is designed to be hard on (incNFA # ) even though expressions have large counter val-ues. In this experiment the transformation tuples cannot summarize large sets of configurations of NFA # s as well as in the previous experiment, due to the non-determinism in the expressions. In terms of time, (incNFA # ) consistently outperforms (incNFA) about one order of magnitude. In terms of space, (incNFA # ) still scales much better than (in-cNFA); about two orders of magnitude.
 Real-world, MPEG7 Expression. In this experiment and the next we consider a (fixed) expression that we found in our practical study and only let the length of the word grow as n increases. This gives a clearer picture of the scal-ability of both evaluation methods when only the size of the data becomes very large. Here, we consider the expres-sion ( ab 2 , 12 ) 0 , 65535 that we found in XML Schemas related to the MPEG-7 standard 14 and refer to as MPEG7 expres-sion . From all the expressions we found in practice, this one seemed to be among the more challenging ones for state-of-the-art evaluation algorithms because it contains both nested counters and large counter values.
 The MPEG7 expression is extremely challenging for (inc-NFA) because it translates into an NFA with about 850K states. As we can see from the third graph in Fig 5, incre-mental NFA evaluation for very short words (10 symbols) is already extremely slow. This is strongly contrasted by incre-mental NFA # evaluation, which still evaluates incrementally in 0.08 milliseconds for words of length one million. We also say a huge difference in memory consumption: 5 orders of magnitude already for words of length 50.
 Real-world, CHAIN Expressions. This final experi-ment is designed to capture the behavior of CHAIN-like ex-http://standards.iso.org/ittf/PubliclyAvailableStandards/ MPEG-7 schema files/mpeg7-v2.xsd 10 pressions which we abundantly found in our practical study (Section 3). The experiment is extremely simple: we con-sider a fixed regular expression a 0 , 1000 (which we also found in our practical study) and incrementally evaluate with re-spect to words of increasing length n . We experimented with various and more complex forms of CHAIN expressions from our corpus, containing more disjunctions, more concatena-tions of disjunctions, larger alphabets, but we observed the same behavior in all cases. All experiments for CHAIN-like expressions boil down to this case. Again, as we can see the rightmost graph in Fig. 5, the evaluation algorithms based on NFA # s scale much better than those based on NFAs.
This paper shows that large counter values in regular ex-pressions do not necessarily imply slow processing. Indeed, while regular expressions with counting often imply high complexity for static analysis problems due to their suc-cinctness [12], it seems that the same succinctness can be ex-ploited for designing highly efficient evaluation algorithms.
In the future, we want to extend the algorithm so that it works for full-fledged XML Schemas, make a more extensive study of regular expressions in Schemas and perhaps even in more general contexts, and look more closely at how to adapt the algorithms towards path expressions on graphs. [1] A. Andersson. Balanced search trees made simple. In [2] A. Balmin, Y. Papakonstantinou, and V. Vianu.
 [3] D. Barbosa, A.O. Mendelzon, L. Libkin, L. Mignet, [4] M. Becchi and P. Crowley. Efficient regular expression [5] M. Becchi and P. Crowley. A-DFA: A time-and [6] G.J. Bex, F. Neven, and J. Van den Bussche. DTDs [7] H. Bj  X  orklund, W. Gelade, and W. Martens.
 [8] R. Book, S. Even, S. Greibach, and G. Ott. Ambiguity [9] S. Dal-Zilio and D. Lugiez. XML schema, tree logic [10] S. Gao, C.M. Sperberg-McQueen, and H. S.
 [11] W. Gelade, M. Gyssens, and W. Martens. Regular [12] W. Gelade, W. Martens, and F. Neven. Optimizing [13] G. Ghelli, D. Colazzo, and C. Sartiani. Linear Time [14] V.M. Glushkov. The abstract theory of automata. [15] Google custom search. www.google.com/cse. [16] S. Harris and A. Seaborne. SPARQL 1.1 query [17] D. Hovland. Regular expressions with numerical [18] P. Kilpel  X  ainen and R. Tuhkanen. Regular expressions [19] P. Kilpel  X  ainen and R. Tuhkanen. Towards efficient [20] K. Losemann and W. Martens. The complexity of [21] W. Martens, F. Neven, and T. Schwentick.
 [22] W. Martens, F. Neven, and S. Vansummeren.
 [23] K. Nakayama, K. Yamaguchi, and S. Kawai. I -regular [24] Neo4J. Cypher patterns. http://docs.neo4j.org/ [25] S. Patnaik and N. Immerman. Dyn-FO: A parallel, [26] Regexlib. www.regexlib.com. [27] R. Smith, C. Estan, S. Jha, and S. Kong. Deflating [28] Snort. www.snort.org. [29] C.M. Sperberg-McQueen. Notes on finite state
