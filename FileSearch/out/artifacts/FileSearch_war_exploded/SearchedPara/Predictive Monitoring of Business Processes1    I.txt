 The execution of business processes is generally subject to internal policies, norms, best practices, regulations, and laws. For example, a doctor may only perform a certain type of surgery if this is preceded by a pre-opera tional screening, while in a sales process, an order can be archived only after that th e customer has confirmed receipt of all or-dered items. We use the term business constraint to refer a requirement imposed on the execution of a process that separates compliant from non-compliant behavior [20].
Compliance monitoring is an everyday imper ative in many organizations. Accord-ingly, a range of research proposals have addressed the problem of monitoring business processes with respect to business constraints [15,14,16,26,13,19,4,10,5,28]. Given a process model and a set of constraints  X  expressed, e.g., in temporal logic  X  these tech-niques provide a basis to monitor ongoing executions of a process (a.k.a. cases )inorder to assess whether they comply with the constraints in question. However, these moni-toring approaches are reactive , in that they allow users to identify a violation only after it has occurred rather than supporting them in preventing such violations in the first place.
 In this setting, this paper presents a novel monitoring framework, namely Predictive Business Process Monitoring , based on the continuous generation of predictions and recommendations on what activities to perform and what input data values to provide, so that the likelihood of violation of business constraints is minimized. At any point during the execution of a business process, the use r can specify a business constraint using Linear Temporal Logic (LTL). Based on an analysis of execution traces, the framework continuously provides the user with estim ations of the likelihood of achieving each business constraint for a given ongoing process execution. The proposed framework takes into account the fact that predictions often depend both on: (i) the sequence of activities executed in a given case; and (ii) the values of data attributes after each activity execution in a case. For example, for some diseases, doctors may decide whether to perform a surgery or not, based on the age of the patient, while in a sales process, a discount may be applied only for premium customers.

The core of the proposed framework is a method to generate predictions of business constraint fulfillment. Specifically, the t echnique estimates for each enabled activity in an ongoing case, and for every data input that can be given to this activity, the prob-ability that the execu tion of the activity with the corresponding data input will lead to the fulfillment of the business constraint. In line with the principle of considering both control-flow and data, the proposed tec hnique proceeds according to a two-phased approach. Given an ongoing case in which certain activities are enabled, we first se-lect from the set of completed execution traces, those that have a prefix  X  X imilar X  to the (uncompleted) trace of the ongoing case ( control-flow matching). Next, for each selected trace, we produce a data snapshot consisting of a value assignment for each data attribute up to its matched prefix. Given a business constraint, we classify a data snapshot as a positive or a negative example based on whether the constraint was even-tually fulfilled in the completed trace or not. In this way, we map the prediction task to a classification task, wherein the goal is t o determine if a given data snapshot leads to a business constraint fulfillment and with what probability. Finally, we solve the re-sulting classification task using decision tr ee learning, i.e., we produce a decision tree to discriminate between fulfillments and violations. The decision tree is then used to estimate the probability that the business constraint will be achieved, for each possible combination of input attribute values.
 The proposed framework can be applied both for prediction and recommendation. For prediction, the decision tree is used to evaluate the probability for the business con-straint to be satisfied for a given combination of attribute values. For recommendation, the decision tree is used to select combinations of attribute values that maximize the probability of the business constraint being satisfied. The predictive monitoring frame-work has been implemented in the ProM toolset for process mining. The framework has been validated using a real-life log (provided for the 2011 BPI challenge [1]) pertaining to the treatment of cancer patients in a large Dutch academic hospital.

The remainder of the paper is structured as follows. Section 2 introduces a run-ning example. Section 3 intr oduces concepts pertaining to LTL and decision trees. Sec-tion 4 presents the predictive monitoring framework and its implementation. Section 5 discusses the validation on a real-life log. Finally, Section 6 discusses related work and Section 7 draws conclusions and perspectives. During the execution of a business process, process participants cooperate to satisfy certain business constraints. At any stage of the process enactment, decisions are taken aimed at achieving the satisfaction of these c onstraints. Therefore, it becomes crucial for process participants to be provided with predictions on whether the business constraints will be achieved or not and, even more, to receive recommendations about the choices that maximize the probability of satisfying the business constraints.
 Fig. 1 shows a BPMN model of a business process we will use as running example. It describes how a patient is nursed according t o the instructions of a doctor. During the process execution, the doctor has to make decisions on therapies and on the doses of medicines to be administered to the patient. The process starts when the patient provides the doctor with lab test results. Based on the tests, the doctor formulates a diagnosis. Then, the doctor has to decide the therapy to prescribe. The therapy can be a surgery, a pharmacological therapy or a manipulati on. In case of a pharmacological therapy, the doctor has also to prescribe the quantity of medicine the patient has to assume.
In this scenario, historical information about past executions of the process could be used to support the doctor in making decisions by providing him or her with predictions about the (most likely) iter of the disease and recommendations about the best choices to be made in order to guarantee the patient recovery. The approach presented in this paper aims at supporting process participants in their decisions by providing them with predictions about the satisfaction of their constraints and, in case they can influence the process with their decisions, by recommending them the best choices to be made to satisfy their business constraint.

In our example, the constraint the doctor wants to satisfy could be that every diag-nosis is eventually followed by the patient recovery. By exploiting data related to the clinical history of other patients with similar characteristics, our technique aims at pro-viding the process participants with predictions about whether the patient will recover or not. In addition, whenever the doctor has to make decisions (e.g., prescribe the type of therapy or choose the dose of a medicine), recommendations are provided about the options for which it is more likely that the patient will recover.
 In this section, we first introduce the language used for the business constraint definition (LTL), and we then provide an overview on decision tree learning. 3.1 LTL In our proposed approach, a business constraint can be formulated in terms of LTL rules, as LTL (and its variations) is classically used in the literature for expressing business constraints on procedural knowledge [21]. LTL [23] is a modal logic with modalities devoted to describe time aspects. Classica lly, LTL is defined for infinite traces. How-ever, when focusing on the compliance of business processes, we use a variant of LTL defined for finite traces (since business pro cess are supposed to complete eventually).
We assume that events occurring during the pr ocess execution fall in the set of atomic propositions. LTL rules are constructed from these atoms by applying the temporal operators X (next), F (future), G (globally), and U (until) in addition to the usual boolean connectives. Given a formula  X  , X  X  means that the next time instant exists and  X  is true in the next time instant (strong next). F  X  indicates that  X  is true sometimes in the future. G  X  means that  X  is true always in the future.  X  U  X  indicates that  X  has to hold at least until  X  holds and  X  must hold in the current or in a future time instant.
In the context of the running example, examples of relevant business constraints formulated in terms of LTL rules include: 3.2 Decision Tree Learning Decision tree learning uses a decision tr ee as a model to predict the value of a tar-get variable based on input variables (features). Decision trees are built from a set of training dataset. Each internal node of the tree is labeled with an input feature. Arcs stemming from a node labeled with a feature are labeled with possible values or value ranges of the feature. Each leaf of the decision tree is labeled with a class, i.e., a value of the target variable given the values of the input variables represented by the path from the root to the leaf.

Each leaf of the decision tree is associated with a class support ( class support )and a probability distribution ( class probability ). Class support represents the number of examples in the training set, that follow the path from the root to the leaf and that are correctly classified; class probability ( prob ) is the percentage of examples correctly classified with respect to all the examples fo llowing that specific path, as shown in the formula reported in (1). One of the most used decision tree learning algorithms is the C4.5 algorithm [24]. C4.5 relies on the normalized information gain to choose, for each node of the tree, the feature to be used for splitting the set of examples. The feature with the highest normalized information gain is chosen to make the decision. In this section, we present the details of the proposed approach, which combines dif-ferent existing techniques ranging from clustering approaches to decision tree learning, to provide predictions, at runtime, about the fulfillments of business constraints in an execution trace. In the following sections, w e provide an overview of the approach and of the more specific implementation. 4.1 General Approach Before presenting the approach proposed in this paper, some assumptions should be made. First, we assume that a set of histori cal execution traces of the process is available from which we can extract information about how the process was executed in the past. Based on the information extracted from the historical traces, we can provide predictions and recommendations for a r unning execution trace. Second, we assume that the underlying business process should be in some way non-deterministic or, at least, the mechanisms that guide the decisions taken during the process execution should not be known by the user. Any recommendation or prediction would be useless if the process participant already knows how the process develops given the input data values provided (we can think to a doctor who may not know about new therapies, or to a company providing services that does not know about the behaviors of its customers). Third, we assume that data used in the process are globally visible throughout the whole process. Fig. 2 sketches the proposed Predictive Business Process Monitoring Framework . It relies on two main modules: a Trace Processor module to filter and classify (past) execution traces and a Predictor module, which uses the Trace Processor output as training data to provide predictions an d recommendations (when an input is requested to the user).

The Trace prefix-based Filterer submodule of the Trace Processor module extracts from the set of historical traces only those traces having a prefix control flow similar to the one of the current execution trace (up to the current event). The filtering is needed since data values are usually strongly dependent on the control flow path followed by the specific execution. In addition, traces with similar prefixes are more likely to have, eventually in the future, a s imilar behavior. The similar ity between two traces is evalu-ated based on their edit distance. We use thi s abstraction (instead of considering traces with a prefix that perfectly matches the current partial trace) to guarantee a sufficient number of examples to be used for the decision tree learning. In particular, a similarity threshold can be specified to include more traces in th e training set (by considering also the ones that are less similar to the current trace).

Each (historical) trace is identified with a data snapshot containing the assignment of values for each attribute in the corres ponding selected prefix. The traces (the data snapshots) of the training set are classified by the Trace Classifier submodule based on whether, in each of them, the desired business constraint is satisfied or not. The constraint is expressed in terms of a set of LTL formulas. In the case of our running example, the constraint  X  X henever a diagnosis is performed, then the patient will even-tually recover X  can be represented in LTL through formula  X  0 reported in Section 3.1.
Formulas have to be satisfied along the whole execution trace. Four possible cases can occur at evaluation time:  X  the formula is permanently violated: the prediction is trivial (non-satisfied);  X  the formula is permanently satisfied: the prediction is trivial (satisfied);  X  the formula is temporary violated/satisfied: the prediction should be able to indicate
Once the relevant traces and, therefore, th e corresponding data snapshots, are clas-sified, they are passed to the Decision tree learning module, in charge to derive the learned decision tree with the associated class support and probability. Fig. 3 shows a decision tree related to our running example: the number of data training examples (with values of the input variables followi ng the path from the root to each leaf) re-spectively correctly and non-correctly classified is reported on the corresponding leaf of the tree. For example, for values  X  X oint di slocation X  and  X  X harm acological therapy X , the resulting class is the formula satisfaction ( X  X es X ), with 2 examples of the training set following the same path correctly classified and 1 non-correctly classified, i.e., with a class probability prob = 2 2+1 =0 . 66 .

All the data values assigned in the past, are supposed to be known by the predictor system at the current execution point of the trace. The tree can hence be pruned by re-moving all the branches corresponding to known values. The pruning algorithm returns either a unique path (and a unique class) or a subtree of the original tree, according to whether the system is used as predictor (the values of all the tree attributes are known) or as a recommender (there are attributes in the tree that are still unknown), respectively. In the latter case, leaves are ranked according to the associated class probability. The conditions on the values of the unknown attributes corresponding to the leaves with the highest rankings are returned to the user as recommendations.

For example, consider the case in which a diagnosis ( X  X oint dislocation X ) and a ther-apy ( X  X harmacological therapy X ) have been given by the doctor. The Predictor will consider only the path from the root to leaf l 1 (pruning all the other branches) and will predict the satisfaction of the formula with a probability class prob =0 . 66 (see Fig. 3). We can also consider the case in which a diagnosis has already been made (e.g.,  X  X oint dislocation X ), but no therapy has been prescribed yet. Then, all the branches cor-responding to other values of the diagnosis a ttribute (i.e.,  X  X rthrosis X ,  X  X upuytren X  X  contracture X ,  X  X steoarthritis X ,  X  X lipped disc X ) can be pruned. Only the subtree corre-sponding to the branch  X  X oint dislocation X  is analyzed and, since no other attribute is known, the class probability of each leaf computed. As shown in Fig. 3, the three leaves have the following classes and class probabilities:  X  l 1 : satisfied with prob l 1 = 2 2+1 =0 . 66  X  l 2 : non-satisfied with prob l 2 = 1 1 =1  X  l 3 : satisfied with prob l 3 = 3 3+0 =1 The system will hence recommend  X  X anipulation X  ( prob l 3 =1 ).

Note that, if we consider as a feature of t he decision tree the next activity to be executed, our framework is also able to recommend which activity should be performed next to maximize the probability of satisfying a business constraint. 4.2 Implementation The approach has been implemented in the ProM process mining toolset. ProM provides a generic Operational Support (OS) environment [2,29] that allows the tool to interact with external workflow man agement systems at runtime. A stream of events coming from a workflow management system is r eceived by an OS service. The OS service is connected to a set of OS providers implementing different types of analysis that can be performed online on the stream. Our Predictive Business Process Monitoring Framework has been implemented as an OS provider.

Fig. 4 shows the entire architecture. The O S service receives a stream of events (in-cluding the current execution trace) from a wo rkflow management system and forwards it to the Predictive Business Process Monitoring Framework that returns back predic-tions and recommendations. The OS service sends these results back to the workflow management system.

For the implementation of the Predictor , we rely on the WeKa J48 implementation of the C4.5 algorithm, which takes as input a .arff file and builds a decision tree. The .arff file contains a list of typed variables (i ncluding the target variable) and, for each trace prefix (i.e., for each data snapshot), the corresponding values. This file is created by the Trace Processor andpassedtothe Predictor . The resulting decision tree is then analyzed to generate predictions and recommendations. We have conducted a set of experiments by using the BPI challenge 2011 [1] event log. This log pertains to a healthcare process and, in particular, contains the executions of a process related to the treatment of patients diagnosed with cancer in a large Dutch academic hospital. The whole event log contains 1 , 143 cases and 150 , 291 events dis-tributed across 623 event classes (activities). Each case refers to the treatment of a different patient. The event log contains dom ain specific attributes that are both case attributes and event attributes in addition to the standard XES attributes. 1 For example, Age , Diagnosis ,and Treatment code are case attributes and Activity code , Number of executions , Specialism code ,and Group are event attributes.

In our experimentation, first, we have ordered the traces in the log based on the time at which the first event of each trace has occurred. Then, we have splitted the log in two parts. We have used the first part (80% of the traces) as training set, i.e., we have used these traces as historical data to derive pred ictions. We have implemented a log replayer to simulate the execution of the remainin g traces (remaining 20%) and send them as an event stream to the OS service in ProM (test set). We defined 5 business constraints corresponding to a subset (from  X  1 to  X  5 )ofthe LTL rules reported in Section 3.1. This set of rules, indeed, allows us to exercise all the LTL constructs while investigating possibly real business constraints. We have asked for a prediction about each of the defined business constraints in different evaluation points during the replay of each trace in our test set. In particular, we have considered as evaluation points the initial event (start event) of each trace, an early event (i.e., an event located at about 1/4 of each trace), and an intermediate event (i.e., an event located in the middle of each trace).

As well as a similarity threshold (see Section 4.1), the implemented OS provider al-lows the user to specify a minimum number o f traces to be used in the training set. In this way, if the threshold does not guarantee a sufficient number of examples, further traces are considered from the set of histori cal traces with a similarity with the cur-rent execution trace lower than the specified threshold. In a first experiment, we have considered a similarity threshold of 0 . 8 and a minimum number of traces of 30 .
For evaluating the effectiveness of our a pproach, we have used the ROC space anal-ysis. In particular, we have classified predictions in four categories, i.e., i ) true-positive ( T
P : positive outcomes correctly predicted); ii ) false-positive ( F P : negative outcomes predicted as positive); iii ) true-negative ( T N : negative outcomes correctly predicted); iv ) false-negative ( F N : positive outcomes predicted as negative). The gold standard used as reference is the set of all true positive instances. In our experiments, we can easily identify the true positive instances. Indeed, if we are asking for a prediction at a certain point in time during the replay of a trace, we can understand if the prediction is correct by replaying the trace until the end.
To draw a ROC space, we need two metrics, i.e., the true positive rate (TPR) ,repre-sented on the y axis, and the false positive rate (FPR) , represented on the x axis. The TPR (or recall) defines how many positive outcomes are correctly predicted among all positive examples available: On the other hand, the FPR defines how many negative outcomes are predicted as pos-itive among all negative examples available:
We have classified predictions for each LTL rule  X  i , and, therefore, each of them is represented as one point in the ROC space. In Fig. 5, we show four spaces drawn by classifying the evaluation points by position (start, early, intermediate). In the figure, we also show the results obtained by considering all the evaluation points together. Note that the best possible prediction method would yield a point in the upper left corner of the ROC space, representing 100% sensitivity (no false negatives) and 100% specificity (no false positives). A completely random guess would give a point along a diagonal line from the left bottom to the top right corners. Points above the diagonal represent good classification results, points below the line poor results.

The ROC space analysis highlights that for  X  1 ,  X  2 ,  X  4 ,and  X  5 our OS provider was able to discriminate well between positive and negative outcomes. 2 The results for  X  3 are less good since the number of positive examples for this formula is extremely low and the discovered decision tree overfits.

In general, the position in a trace in which we ask for a prediction does not affect sig-nificantly its reliability. In the presented scenario, in which case attributes are available since before the initial event occurs, this is true also for the initial event. Nevertheless, in case of overfitting, there is more variability . Table 1 shows that our results are good also in terms of positive predictive value (PPV) , or precision, indicating how many positive outcomes are correctly predicted among all the outcomes predicted as positive: in terms of harmonic mean of precision and recall: and in terms of accuracy. Accuracy is partic ularly important in our context since it indicates how many times a prediction was correct: Note that the accuracy value is good also in case of overfitting (formula  X  3 ).
In a second experiment, we used a lower similarity threshold ( 0 . 5 ) and, again, a minimum number of traces equal to 30 . The results for this experiment (for all the evaluation points together) are reported in Table 2 and in Fig. 6. This experiment shows that generating predictions based on a hi gher number of historical traces not always improves the quality of the results. This is due to the fact that, even if we are considering a larger training set, this set also includes t races that are quite dissimilar from the current trace, thus producing misleading results.

One way of assessing the reliability or  X  X oodness X  of a prediction is to use its class probability . In Table 2 and in Fig. 6, we show the results obtained by filtering out predictions with a class probability that is lower than the average. Table 2 also reports the prediction loss (LOSS), i.e., the percentage of predictions lost when filtering out predictions with a low class probability. This experiment shows that considering only predictions with a high class probability not always improves the quality of the results, though the percentage of predictions lost is not high (about 20%).

Another way of evaluating the reliability of a prediction is to consider its class sup-port. In Table 2 and in Fig. 6, we show the results obtained by filtering out predictions with support lower than the median of the supports. In this case, although the cut of predictions is high (more than half of the predictions are filtered out), there is a clear improvement in all the considered metrics: in the ROC dimensions, in the F-measure as well as in the average accuracy of the predictions.

In summary, the evaluation shows that the proposed approach is feasible and pro-vides accurate predictions (and hence reco mmendations). Results seem overall not to be affected by the position of the evaluation point, thus demonstrating that the approach works well even when few variables are known. Support seems to be an important fac-tor influencing the results, i.e., the more evidences we have in the training set, the more accurate are the produced predictions. If on t he one hand this highlights the need to have adequate training sets, on the other it also shows that sacrificing outlier predictions, it is possible to obtain very accurate results (accuracy around 0.9). In the literature, there are some works that provide approaches for generating predic-tions and recommendations during process execution and are focused on the time per-spective. In [3,2], the authors present a set of approaches based on annotated transition systems containing time information extract ed from event logs. The annotated transition systems are used to check time conformance w hile cases are being executed, predict the remaining processing time of incomplete cases, and recommend appropriate activities to end users working on these cases. In [9], an ad-hoc predictive clustering approach is presented, in which context-related execution scenarios are discovered and modeled through state-aware performance predictors. In [25], the authors introduce a method for predicting the remaining execution time of a process based on stochastic Petri nets.
There are several works focusing on ge nerating predictions and recommendations to reduce risks. For example, in [7], the authors present a technique to support process participants in making risk-informed decisions, with the aim of reducing the process risks. Risks are predicted by traversing decision trees generated from the logs of past process executions. In [22], the authors propose an approach for predicting of time-related process risks by identifying (using statistical principles) indicators observable in event logs that highlight the possibility of transgressing deadlines. In [27], the authors propose an approach for Root Cause Analysis based on classification algorithms. After enriching a log with information like workload, occurrence of delay and involvement of resources, they use decision trees to identify the causes of overtime faults.
An approach for prediction of abnormal termination of business processes has been presented in [11]. Here, a fault detection algorithm (local outlier factor) is used to es-timate the probability of a fault to occur. Alarms are provided to early notify probable abnormal terminations to prevent risks rath er than simply react to them. In [6], Castel-lanos et al. present a business operations management platform equipped with time series forecasting functionalities. This platform allows for predictions of metric values on running process instances as well as for predictions of aggregated metric values of future instances (e.g., the number of order s that will be placed next Monday). Predictive monitoring focusing on failures and quality has also been applied to real case studies (e.g., in transportation contexts [18,8]).

A key difference between these approaches and our technique is that they rely ei-ther on the control-flow or on the data perspective for making predictions at runtime, whereas we take both perspectives into consideration. In addition, the purpose of our recommendations is different. We provide recommendations neither to reduce risks nor to satisfy/discover timing constraints. We aim instead at maximizing the likelihood of satisfying business constraints expressed in the form of LTL rules. This paper presented a framework for predictive business process monitoring based on the estimation of probabilities of fulfillment of LTL rules at different points during the execution of a case. The framework takes into account both the sequencing of activities as well as data associated to the executio n of each activity. A validation of the frame-work using a real-life log demonstrates that recommendations generated based on the framework have a promising level of accuracy when sufficient support is available.
Increased accuracy could be achieved by e xtending the technique along two direc-tions. First, the proposed technique matches the trace of an ongoing case against pre-fixes of completed traces based on edit dista nce. While this is a well-known measure of similarity and suitable as a first step in this study, other approaches could be considered, including trace similarity measures based on occurrences of n-grams, counts of activi-ties and activity pairs, and other relevant feat ures that have been studied in the context of trace clustering [17]. In a similar vein, discriminative sequence mining techniques [12] could be applied in order to extract prefix patterns that are associated with fulfillment of a given business constraint. These patterns can also be taken as input in the predic-tion. Secondly, we have considered the use of decision trees to build the classifier. With larger number of attributes, which might be encountered in richer logs, decision trees are likely to exhibit lower accuracy due to their inherent weaknesses when dealing with large feature sets. In this context, other classification techniques, such as random forests or sparse logistic regression are possible alternatives.
 Acknowledgments. This work is partly funded by ERDF via the Estonian Centre of Excellence in Computer Science and by the European Union Seventh Framework Pro-gramme FP7-2013-NMP-ICT-FOF (RTD) under grant agreement 609190 - X  X ubject-Orientation for People-Centred Production X .

