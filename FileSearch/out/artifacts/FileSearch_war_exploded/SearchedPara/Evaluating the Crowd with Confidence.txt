 Worker quality control is a crucial aspect of crowdsourcing sys-tems; typically occupying a large fraction of the time and money invested on crowdsourcing. In this work, we devise techniques to generate confidence intervals for worker error rate estimates, thereby enabling a better evaluation of worker quality. We show that our techniques generate correct confidence intervals on a range of real-world datasets, and demonstrate wide applicability by using them to evict poorly performing workers, and provide confidence intervals on the accuracy of the answers.
 H.1.0 [ Information Systems Applications ]: Models and Princi-ples X  General Algorithms, Human Factors, Reliability crowdsourcing, confidence
A crowdsourcing system employs human workers to perform tasks , including data processing tasks such as classification and clustering. A major issue in any crowdsourcing system is worker quality: workers can naturally perform some tasks incorrectly, but there are often workers that incorrectly perform more than their share. Some of the low quality workers may not have the necessary abilities for the tasks, some may not have adequate training, and some may simply be  X  X pammers X  that want to make money with-out attempting tasks diligently. Anecdotal evidence indicates that the spammer category is especially problematic, since these work-maximize their income.

To correct or compensate for poor worker quality, a crowdsourc-ing system implements some type of worker quality control WQC. Typically workers have known identities, so that WQC can iden-tify the poor workers and then possibly take action against them or against their results.

In this paper we focus on one of the most important aspects of WQC: estimating the quality (specifically, error probability) of workers based on their past work. The estimates can then be used either to take action against bad workers (e.g., preventing them from doing future work, paying them less) or for adjusting results (e.g.,  X  X owngrading X  results from bad workers).

Although there is substantial work on WQC in crowdsourcing systems (see related work section), as far as we know we are the first to estimate worker quality with confidence intervals for these estimates. To illustrate the importance of confidence intervals, con-sider two scenarios. In the first scenario, a particular worker W has performed 3 tasks, one of them incorrectly. (Assume that in this case the correct answers are known in advance, i.e., the worker is performing tasks for evaluation purposes only.) Hence, we can estimate the probability that W 1 makes a mistake is 1 / 3 . In Sce-nario 2, worker W 2 has performed 30 tasks, 10 of them incorrectly. In this case we get the same error estimate for W 2 as for W 10 / 30 = 1 / 3 . However, in the second scenario we are much more confident that worker W 2 is making quite a few mistakes. In Sce-nario 1, perhaps W 1 was unlucky or was just distracted, and his/her one incorrect answer is not representative. If we are going to fire workers based on these estimates, it is important that we have  X  X uf-ficient confidence X  in our estimates before we take actions. Thus, we may demand that our 1 / 3 error estimate has, say, a 90% confi-dence interval of size 0.05. Given a requirement like this, we will show how many tasks we must require before making a decision.
Instead of comparing worker results against known correct an-swers, we focus on schemes that do not require known answers. These schemes are easier to set up, and require no supervision. These well known schemes [7, 8] rely on the frequency of disagree-ment among workers to estimate error probabilities. In this paper we present a novel way to estimate errors based on disagreements, and we show that our method is as accurate as the previous meth-ods and in addition yields the confidence intervals associated with its estimates.

Determining confidence intervals for a single estimate is a well-studied problem in statistics, with well-known solutions [32]. How-ever, in our scenario, the standard solutions do not apply because we are simultaneously generating confidence intervals for multiple estimates (in this case, the worker error probabilities for each of the workers working on the same task) that are dependent on each other in complex ways.

In this paper, we present a technique that is guaranteed to provide accurate confidence intervals for worker error estimates under some simplifying assumptions (e.g., fixed difficulty tasks, fixed worker error rate during one evaluation period). Even with these assump-tions, we will show that our technique provides accurate confidence intervals in practice over real-world datasets that cover a range of crowdsourcing applications, such as peer evaluation, image com-parisons, and predicate evaluation. In Section 7, we describe gen-eralizations to our method for other scenarios not captured by our basic technique, such as multiple task types, varying difficulty and categorization.
 In summary, we make the following contributions in this paper:
The crowdsourcing system operates in phases . During one phase, a set of workers W perform a set of tasks T . Each worker in W performs all n tasks in T .

In our base case, we consider tasks that have two possible results, which we will call  X  X es X  (Y) and  X  X o X  (N). The correct outcome is unknown to the evaluation system and the workers. For now we assume we have no a-priori knowledge of the fraction of tasks that have Y as the correct output. (We consider this case in Section 7.)
We assume that a given worker i has an error probability p is, with probability p i a worker will produce N for a task whose correct result is Y, or will give Y for an N task. Our goal at the end of a phase is to provide an estimate  X  p i for the p i worker. Our estimate  X  p i will have a confidence interval of half-size and confidence level c i . The interpretation is that if we consider say 100 phases with similar workers and tasks, in c i percent of them the true error rate p i will be between  X  p i  X  i and  X  p clear what worker we are referring to, we will omit the i subscript and simply use p , c and .

We know that in practice some of these assumptions may not hold. For example, workers may collude so their error rates may not be independent. Worker errors may depend of task  X  X ifficulty X  or other environmental factors. As mentioned earlier, our assump-tions can be relaxed, see Section 7. But it is important to study the base-case solution (with strong assumptions) as it forms the basis of the generalizations. And as also mentioned earlier, the base-case solution can still give quite accurate results in many cases (Sec-tion 3.1, 4.1), even if we do not know whether the assumptions hold.
 At the end of a phase, we compute the error estimates for our W workers and we take appropriate action, e.g., replacing some of the workers for the next phase. The focus of this paper is on the worker evaluations within a single phase. However, in Section 6.3 we discuss how our estimates can be used to make decisions at the
With a small number of tasks,  X  p i may not be exactly at the center of the interval. end of a phase, and we show the impact of the decisions across multiple phases.
In this scenario we have three workers with unknown error rates p , p 2 and p 3 . We are given that these error rates are less than 1 / 2 . (An error rate larger than 1 / 2 is unlikely as it implies that a worker is making worse than random choices.) All three workers do a sequence of n tasks, for which we do not know the correct result, and our goal is to estimate the worker error rates based on the differences in their responses. Intuitively, when a worker disagrees with the majority, it is a sign that the worker may have made a mistake. (In the following section we consider the case of more than 3 workers.)
For each i from 1 to n , we define three random variables, X X i, 23 and X i, 13 , that track the differences among workers in the i task. For instance, variable X i, 12 is 1 when workers 1 and 2 agree on the i th task, and 0 otherwise. Let the means of these random variables be auxiliary variables q 12 , q 23 and q 13 . Since X 1 when workers 1 and 2 are both right or both wrong, we have q 12 = p 1 p 2 + (1  X  p 1 )(1  X  p 2 ) . We can get similar equations for q 13 and q 23 . So we have : Note that since p 1 ,p 2 ,p 3 &lt; 1 2 , we have p 1 &lt; (1  X  p (1  X  p 2 ) and p 3 &lt; (1  X  p 3 ) . Hence by the rearrangement inequality, q ,q 13 ,q 23 are all greater than 1 / 2 and are decreasing in p and p 3 .
 each of X 12 ,X 23 ,X 13 represents an outcome of a Binomial Ex-periment, which is a sum of n statistically independent Bernoulli Trials. Suppose a Binomial Experiment consists of n Bernoulli tri-als with mean p each, and the outcome of the experiment is a , then it is known [32] that p can be estimated as  X  p = a n . And for any con-fidence level c , we can find the c -confidence interval for p using the so-called Wilson Score Interval [34].
 where t = 1  X  c 2 , and z t gives the t th percentile of the standard normal distribution. The values of z t are readily available in table form [4, 32].
 The half-size of the interval is:
Bin ( X  p,c,n ) = (Interval Half-Size) =
For large n , the Binomial distribution starts to resemble the Nor-mal distribution, and the terms 1 2 n z 2 t , z 2 t 4 n 2 gible compared to the terms they are being added to. In this case, the end points of our c confidence interval are: The half-size of the interval is:
So if b 12 is the number of times workers 1 and 2 disagree (out of n tasks), then we can estimate  X  q 12 = b 12 /n . Using Equation 5 (or 7), we can estimate the half-size of the c confidence interval for  X  q 12 as 12 = Bin ( X  q,c,n ) (or q  X  Bin 0 ( X  q,c,n ) ). The estimates  X  q 23 and  X  q 13 , and the half-sizes of their intervals 23 and found similarly.

The q estimates and our desired p estimates are related via Equa-tions 1, 2 and 3, where all variables have hats. We can solve the three equations for  X  p 1 ,  X  p 2 and  X  p 3 : Lemma 1. The value for  X  p 1 is The values for  X  p 2 and  X  p 3 are computed analogously. 2
The proofs for this and the other lemmas in this paper can be found in the extended technical report [16].

Our next task is to compute confidence intervals for  X  p 1  X  p . This is more complex than getting confidence intervals for the q s because each confidence interval depends on the intervals of the three inter-related variables X 12 , X 23 and X 13 .

To illustrate our solution, first consider a simpler case. Say ran-dom variable Z = f ( X,Y ) for some function f and random vari-ables X and Y . Say f is monotonically increasing in both X and Y . Furthermore, say  X  x is our estimate for the mean of X , with c confidence interval with half-size x . Similarly for Y we have  X  y and y (same c ). Now consider the interval I from If X and Y were independent variables, then we could say that with probability c 2 the true mean of X would be in its interval and the true mean of Y would be in its interval and thus the true mean of Z is within I . (Remember that f is increasing.) Hence, we can use I as the confidence interval for Z , with confidence c 2 and half-size ( max  X  min ) / 2 .

Unfortunately, X and Y may not be independent. Hence, we can only provide a bound for the confidence of I . In particular, with probability 1  X  c the mean of X is not in its interval, and with probability 1  X  c the same is true for Y . The probability of the union of these two events is at most (1  X  c )+(1  X  c ) . Hence the converse occurs with at least probability 1  X  ((1  X  c )+(1  X  c )) = (2 c  X  1) . That is, the mean of Z will be in I with probability (2 c  X  1) or larger. (We assume that 2 c is larger than 1.) If c is close to 1, then I will still have a confidence close to 1. However, if c has a relatively small value, then our bound is less useful. For instance, if c = 0 . 80 , our confidence in I is only 0 . 6 .
In Lemma 2 below we show how to compute a confidence bound for our p estimates for the differences scheme (3 workers). The derivation is similar to what we have illustrated, except that there are three variables involved, and composite random variable Z is increasing on two of its parameters and decreasing on the third. In Lemma 3 we then show how this bound can be significantly improved by making two reasonable assumptions.
 Lemma 2. Define function f ( a,b,c ) for parameters a , b and c to be vals for  X  q 12 ,  X  q 23 and  X  q 13 all have confidence c and c  X  confidence interval for  X  p 1 ranges from where 12 , 23 and 13 are the half-sizes of the confidence intervals for the q variables (computed using the large n approximation). The half-size of the  X  p 1 interval, 1 is ( max 1  X  min 1 dence associated with this interval is at least 3 c  X  2 . The intervals for  X  p 2  X  p 3 are computed in an analogous fashion. 2 Lemma 3. Consider our three random variables X 12 , X X 13 with mean estimates  X  q 12 ,  X  q 23 and  X  q 13 , and c confidence inter-val half-sizes of 12 , 23 13 respectively. Also consider a general function f ( a,b,c ) . Let us assume that Then, the interval with endpoints has confidence c or higher. 2
In our case, we are using the f functions given in Lemma 1 to entiable and we are only interested in values near  X  q 12 the linearity assumption is reasonable.
 Summary: Evaluation with 3 Differences Scheme. Consider three workers that perform n tasks. First compute auxiliary esti-mates  X  q 12 ,  X  q 23 and  X  q 13 using the number of times each worker pair disagrees. For instance,  X  q 12 = b 12 /n where b 12 is the number of times workers 1 and 2 disagree. Second, compute estimates for the three worker error rates  X  p 1 ,  X  p 2 and  X  p 3 using Lemma 1. Third, com-pute the half-sizes of the three confidence intervals 1 , the p estimates using Lemma 3. 2
In this section we experimentally evaluate our confidence inter-vals, checking if they are valid, even if we do not know if the as-sumptions made hold. We use three real datasets to test our tech-niques : image comparison, predicate evaluation, and peer evalu-ation. In all these datasets, to estimate the accuracy of our con-fidence intervals, we use or estimate true answers for each of the tasks in the dataset. Note that the number of tasks is relatively small on purpose; one typically wants to evaluate workers after a small number of tasks, to save time and money.
 Data and Setting: In the first scenario, which we call  X  X mage Com-parison X , denoted IC, a worker is asked to compare a pair of sports photos (each photo showing one athlete) and to indicate if the pho-tos show the same person (Yes/No answer). The correct response for each task was known to us in advance. We evaluated the set of 48 tasks using 19 workers on Amazon X  X  Mechanical Turk [1], ac-cumulating a total of 48  X  19 responses. The data is available at [6]. The second dataset, which we call  X  X chools of Thought X , denoted SOT, is from [29]. A total of 402 workers were given 5 sets of 12 binary tasks each, making 60 tasks total per worker. In each task, workers were given an image, and asked to filter the image based on some question, such as  X  X s it an image of the Sky? X , or  X  X s it an image of a building? X , or  X  X s the image beautiful? X . The correct response of each task is assumed to be the majority response for the task.

The third dataset is a MOOC (Massive Open Online Course) [5] grading dataset from the HCI Course at Stanford in Fall 2012, de-noted MOOC. Students were asked to grade assignments of their Figure 1: Accuracy of 3 worker differences method in estimat-ing confidence peers, providing a rating from 0 to 5 . Prior this peer grading pro-cess, each student was required to grade a fixed set of  X  X est X  tasks. There were 388 such  X  X est X  tasks that were evaluated by all workers (students). Since these  X  X est X  tasks were also graded by staff mem-bers, the correct grade for each of these tasks is known to us. Since the grade provided is not binary, we interpret a grade from 0 to 2 as a no (or  X  X ail X ) and 3 to 5 as a yes (or  X  X ass X ).

In each of the datasets, we find that some tasks that had high agreement ( 100% of the workers returned the same answer), and some had low agreement ( 52% of the workers returned the same answer). Therefore, error probabilities of workers are positively correlated with each other (as opposed to zero correlation as re-quired by our model), with all workers committing more errors on hard tasks. Moreover, we find that the assumption of false posi-tive and false negative error rate both being equal is also not true in these datasets. We now want to see how good our 3-differences confidence interval estimates are for these data sets.
 Experiment: We first pick a confidence level c . We then pick a random set of m = 3 workers, and run the general differences scheme on their responses (on tasks which all of them have at-tempted), to get a c -confidence interval for each of their error rates. We want to check if the worker X  X  true error rate lies in the our c -confidence interval. Since we do not have access to the true worker error rate, we compute the fraction of errors made by each worker (using the correct responses, known to us), and use that as a proxy for the true error rate. We repeat this process for 100 random combinations of m = 3 workers, for every value of confidence we count the number of times the true error rate lay within our c -confidence interval, and divide by the number of confidence in-tervals we computed, and plot this value against c in Figure 1. (The figure also contains two lines named  X 3 worker MOOC easy X  and  X 3 worker MOOC hard X . They can be ignored for now.)
Note that the solid line y = x in Figure 1 represents the ideal case. If an outcome is above the line, the predicted error rate was inside the computed interval more than c percent of the time. Thus, these points are safe but conservative estimates. If an outcome is below the y = x line, the confidence interval was too small. The closer an outcome is to the y = x line, the less imprecise it is.
Our results show that in all cases the outcomes are close to the y = x line. For the IC and the SOT data sets, our estimates are always above the y = x line and hence always conservative. For the MOOC data set, the confidence interval is a bit imprecise but only for confidences less than 0 . 7 . Since one would expect the desired confidence level to be above 0.7 to be useful, we believe this imprecision is not critical.

Of course, if we know more about the tasks and workers, and can model dependencies, task difficulties, and so on, one can do better. To illustrate, we jump ahead and use one of the generalizations of Section 7. In the MOOC scenario, say we postulate that there are two types of tasks, hard and easy, and that workers perform differ-ently on each type of task. Say we call tasks  X  X asy X  if more than 90% of the workers that attempted them agree on the outcome, and  X  X ard X  otherwise. (There are much more sophisticated ways to label tasks, but this is just an illustration.) We can then evaluate the error rates of workers for each type of task, using our same 3-differences approach. The results are shown in Figure 1. The new plots are labeled  X  3 worker MOOC easy X  and  X  3 worker MOOC hard X . As the plot shows, the accuracy is now above the y = x line, which suggests that our technique for handling difficulty works well in this case.

In summary, in scenarios where we do not have detailed infor-mation on correlations, biases, task difficulty, etc., our base-case 3-differences scheme seems to provide reasonable confidence in-tervals. If more information is available or learned, one can extend our method to take the information into account.
 Although we have not shown the actual confidence intervals in Figure 1, it turns out some of them can be relatively large when one evaluates them over a small number of tasks. We have addressed this in Section 6.2.1
Our solution for the differences scheme with 3 workers does not easily generalize to more than 3 workers. We could define q ues for every possible pair of workers (analogous to Equations 1 through 3). From these values we can formulate equations for the worker error rates. If we have j workers, we get j ( j  X  1) / 2 equa-tions for j desired rates  X  p 1 through  X  p j . However, it is not clear how we may use all these equations to get the error rates.

Instead, for the j -worker case we use the following strategy. Say we are evaluating worker 1. Let us call the remaining j  X  1 workers the peers . From the peers we form two disjoint sets of workers, S and T , and treat each of these sets as a  X  X uper-worker X . That is, the result produced by the S super-worker is the majority result of the S workers, and similarly for the T workers. We then apply the 3-worker solution to workers 1, S and T to get the desired error estimate  X  p 1 .

The key question is what workers should go into sets S and T . In the 3-worker solution, the accuracy of  X  p 1 will improve as the error rate of S and T decrease (Please refer to the technical report [16] for additional details on this). Thus, the question is how to obtain low-error rate S and T super-workers.

To illustrate the issues, say we have j = 7 workers and are evaluating worker 1. Assume the true error rates for the peers are p 2 = p 3 = 0 . 1 and p 4 = p 5 = p 6 = p 7 = 0 . 4 . That is, peers 2 and 3 are good, and the rest are not very good. Say we form S out of peers 2, 4 and 5. It is straightforward to compute the error rate of S (the probability that the majority choice is incorrect) to be 0.208. In this case, we would have been better off simply having worker 2 alone in S ! On the other hand, it is easy to construct cases where the super-worker has a lower rate than its individual members.
Hence, we need a strategy for selecting good S and T sets, i.e., ones that have the lowest (or close to lowest) possible error rates. Keep in mind that the selection procedure will not have access to the true error rates, as in our example.

We suggest three possible strategies for the S , T selection. (Other variations are possible.) Figure 2: Accuracy of multiple worker differences method in estimating confidence Summary: Evaluation with General Differences Scheme. Con-sider j workers that perform n tasks. To evaluate a worker w , divide the remaining workers into disjoint sets S and T , and treat each set as a  X  X uper-worker X  whose response equals the majority response of workers from that set. Then proceed as in the 3 worker differences scheme, using worker w and the two super workers. 2
We use the same three datasets we used to test the 3 worker dif-ferences scheme.
 Experiment: Figure 2 shows the results for our evaluation of the general differences scheme. The data sets are as before, and the axis of the figure are the same as for Figure 1. In this case we use m = 7 workers instead of three, and we use a greedy heuristic to find super-workers. In this case we see that all outcomes are above the y = x line, meaning that all our intervals are correct but conservative, even though some of our assumptions do not hold.
In this section, we compare our basic worker error estimates against the popular Expectation Maximization algorithm [8, 7], or EM, for short, and against a simple Majority Heuristic. The EM algorithm is used to provide estimates of hidden parameters, given values of observed parameters, such that the estimates are Maximum-Likelihood. In our setting, the hidden parameters are the worker error rates, which are unknown, while the observed pa-rameters are the answers of workers in a given phase. Note, how-ever, that the EM algorithm provides no confidence intervals for the hidden parameter estimates, unlike our method. So, for this comparison, we ignore our confidence intervals.

The EM algorithm uses two steps, computing the expectation (E), and maximization (M), until convergence. Specifically, the al-gorithm begins by initializing a random error rate for each worker and a random probability for Boolean answers for each task. Then, in alternate steps, it sets the maximum likelihood error rates for each worker based on the current answer probabilities, and sets the maximum likelihood answer probabilities given the worker re-sponses and current error rates. The algorithm is guaranteed to converge to a local optimum of Maximum-Likelihood; however, in most practical settings, the algorithm does converge to the global optimum.

The Simple Majority Heuristic [11], assumes that the majority response of workers is always correct, and hence evaluates worker X  X  by counting the fraction of times they agreed/disagreed with the majority.

We tabulate the comparison between our algorithm (without con-fidence intervals), the EM algorithm, and the Simple Majority Heuris-tic in Table 1: For this comparison, we focused on a single phase, while varying the number of tasks (between 200-500), and while varying the number of workers (3 or 5). We generate task out-comes by assuming each worker has an error rate equal to 0 . 2 or 0 . 3 with probability 1 2 each, independently of other workers. We then estimated the error rates  X  p for each of the workers for a single phase using our technique, EM and Simple Majority, (ignoring con-fidence intervals), and recorded the error | p  X   X  p | , i.e., how far away the error rate estimates are from the actual error rate. We recorded this value across 500 iterations, and then took the average.
As can be seen in the table, the error for EM and our technique is usually very close, while the error for simple majority is somewhat larger. For example, for 400 tasks and 3 workers, the average error for our technique is 0.034, that for EM is 0.035, while average error for the simple majority heuristic is 0.062. Thus, for all practical purposes, our technique and EM are equivalent to each other, and better than simple majority, in providing  X  p estimates. Note that our technique, unlike the others, provides a wealth of more information in our confidence interval estimates, allowing us to better judge if workers are indeed of high quality.
In this section we present case studies of how our confidence interval estimates may be used in various ways; while we have selected some especially important applications, we believe there may be many other ways this information may be used, some of which we haven X  X  even envisioned yet. So far we have focused on estimating the error rates of workers. However, once we have worker error estimates, they can be used to improve the accuracy of the task results we produce.

For instance, say we have five workers, W 1 through W 5 , that have executed a particular task. Say workers W 1 , W 2 and W give the result Y, while workers W 4 and W 5 say N. Typically we would take a majority vote of the results, to mask out incorrect answers. In this case, our final task result would be Y, the majority vote. However, say that in the current phase we have evaluated the workers and we obtained that  X  p 1 =  X  p 2 =  X  p 3 = 0 . 4 and that  X  p  X  p 5 = 0 . 1 . Intuitively, it seems that we should weight the results of W 4 and W 5 more heavily since they are much more accurate, giving a final result of N.
We start by formally defining the variables used. Let random variable X represent the correct result of a given task, either 1 if the result is Y or -1 if the result is N. We have j workers and let random variable X i represent worker i and x i (lowercase) be the specific result (1 or -1) given by worker i ( 1  X  i  X  j ) . Say we know exactly the worker error rates p i . (Below we discuss what happens when we only have estimates  X  p i .) Assume further that the task selectivity is s .

We wish to give a specific result  X  X (either  X  X = 1 or  X  1 ) based on the results given by the workers. The accuracy of our result is the probability that X =  X  X given the evidence (given the each X i = x i ). For instance, if we say  X  X = 1 then the accuracy is the probability that X = 1 given the evidence. Our next lemma tells us how to compute the accuracy.
 Lemma 4. To compute accuracy, we first compute two probabili-ties: Then, the accuracy of the task is: If we use a majority vote to compute the final task result can now say how accurate the answer is. However, our next lemma tells us how to select  X  X by appropriately weighting the individual results, to maximize accuracy.
 Lemma 5. We wish to give a specific result  X  X (either or  X  1 ) based on the results given by the workers, such that we maximize accuracy, i.e., the probability that X =  X  X . To compute this maximum likelihood result we first compute Then, if  X  +  X  &gt; 0 the maximum likelihood result  X  X is Y (1), else it is N (0).

Thus to provide a result with maximum accuracy, we need to give sum, and add the initial bias factor log ( s 1  X  s ) , and return the sign of the result as our guess  X  X . 2
To illustrate the potential improvements that may be achieved by using Lemma 5, we consider a simple scenario with 9 workers. In this scenario we have two types of workers: good workers have a true error rate of 0.1, while bad workers have a rate of 0.3. Figure 3 shows the fraction of incorrect results (1 minus accuracy) for both a simple majority and a weighted majority (Lemma 5), as the num-ber of bad workers varies. For instance, if we have 6 bad workers, the error rate for the combined result drops by about half when the optimal weighted majority is used (from about 0.02 to about 0.01). We see that the use of a weighted majority improves accuracy sig-nificantly, unless most workers are equally good, or unless every single worker is equally bad.

In practice we do not have the exact worker errors p i but only their estimates  X  p i . To compute accuracy, we can use the estimates Figure 3: Fraction of incorrect results using a simple vs. a weighted majority in place of the p i values in Lemma 4, to obtain an estimate of the accuracy . Thus, for each task we have  X  A and not the true accuracy A . Is our estimate for  X  A any good? If we tell whoever is consuming the results of our tasks that  X  X e think that X  the answer we provide is correct 95% of the time, how will they know if they can trust that 95% accuracy estimate?
Fortunately, not only do we have error estimates for the workers, but we also have confidence intervals for those estimates! Thus we can perform a  X  X orst-case X  analysis: For each  X  p i we know with c confidence level that at most the true p i is  X  p i + i . We now estimate  X  A , which is the accuracy we obtain when we use  X  p i + i instead of p i in Lemma 4. Since  X  A 0 is a differentiable function, and since larger error rates make accuracy worse (smaller), we can say that with c confidence, the true accuracy A will be larger than that once again, we are using the linearity principle of Lemma 3, i.e., that the majority function for  X  A 0 is locally linear around the estimates of the values of  X  p i .

Returning to our simple example, say our (worst case) accuracy bound is 95% with 90% confidence. Now we can tell our customer that over many tasks we will give an incorrect answer in no more than 14.5% of the cases, since we do not lie within the desired accuracy with probability 0 . 1 , and if we do lie within the desired accuracy bound (with probability 0 . 9 ), then with probability 0 . 05 we end up making an error, giving us overall: (1  X  0 . 9) + 0 . 9  X  (1  X  0 . 95) = 0 . 145 .
In the previous subsection, we provided a technique for provid-ing a confidence value along with an accuracy bound for the answer for any given task: we used the larger extreme of the worker error probabilities  X  p i + i to provide an accuracy bound  X  A 0 we know that the worker error probabilities p i are all smaller than  X  p + i with confidence c . Thus, overall, we get an accuracy bound of  X  A 0 with confidence value c .

In this section, we first set a lower bound on the desired accuracy, and we study the impact on the confidence value of varying the number of tasks or number of workers in a single phase. Note that increasing the number of tasks will increase the confidence value, as will increasing the number of workers.

In Figure 4(a), we plot the number of workers required to get answer accuracy of at least 90% at different confidence levels on fixing the number of tasks to 500. As predicted, as the confidence levels increase, the number of workers required increases. In Fig-ure 4(b), we plot the number of tasks required to get answer ac-curacy of 90% at different confidence levels on fixing the number of workers to 3. Once again, as predicted, as the confidence lev-els increase, the number of tasks required increases. In a sense, these graphs illustrate the  X  X ost of confidence/accuracy X . That is, the graphs show us how much we need to  X  X ay X , in terms of more workers or more tasks to achieve our goal.

Lastly, in Figure 4(c), we plot the dependence of the number of workers on the number of tasks when we fix the confidence level to 90% , and the answer accuracy of at least 90% . In other words, we fix the number of workers, and increase the number of tasks until we get the desired confidence and accuracy levels. We may use this plot to find the optimal tradeoff between the number of tasks ( n and number of workers ( w ). Let us say we wanted to ensure the confidence level and answer accuracy of a single task to be both greater than 90% . Then, to ensure this accuracy level, we can use various combinations of n d and w with the total cost for a phase being n d  X  w . For w = 11 in the figure, we find that n while for w = 15 , n d  X  250 . We find that the optimal combination of n d and w occurs around w = 25 , where n d  X  130 , for a total number of task instances of n d  X  w = 3250 .
Figures 4(a), 4(b) seem to indicate that getting small confidence intervals, for high levels of confidence, requires a large number of tasks and workers. For smaller amounts of data, the interval sizes can be relatively large. This is partly because schemes based on worker differences (as opposed to gold standard tasks) usually require more data to get the same-sized confidence intervals. So the reader may wonder, how useful are these intervals if they can be large for a modest number of tasks or workers?
First, simply ignoring the intervals (as is commonly done) is not a good idea even if the intervals are large. (Recall the proverbial os-trich hiding its head underground.) Large intervals give us a warn-ing that our estimates may be incorrect, and it is best to take that into account. For instance, in Section 6.3 we will see that it is good to take into account uncertainty when evicting workers. (When we evict workers with uncertain error estimates, it is better to be con-servative than overly aggressive.) We can also heed the warning by evaluating workers over more tasks, changing our evaluation scheme (to say using a gold standard), or by having more workers repeat each task. Whether we can afford more tasks or workers is an orthogonal issue that can only be discussed rationally with knowledge of confidence intervals.

Second, note that computing confidence intervals is  X  X ree. X  That is, knowing the confidence intervals does not require additional work beyond what is already being done to evaluate workers and to perform tasks.

Third, also note that even wide confidence intervals can be useful if we are trying to distinguish between classes of workers that have quite different rates. For instance, if we are trying to detect spam-mers (rates close to 0.5) from good workers (say with rates below 0.05), then coarse intervals can easily differentiate the workers.
As discussed in the introduction, we may also use our error esti-mates to periodically evict poorly-performing workers. Prior work [24, 11] has used heuristics to eliminate poorly performing work-ers, but here, armed with our toolbox of evaluation techniques, we may use precise estimates of worker errors to truly judge if a worker is good or bad.

Specifically, at the end of every phase, we choose to evict some workers based on how well they performed in that phase, and re-place them with other workers. A straightforward technique is to simply use our estimate of the error rate of the worker (  X  p ), and reject workers whose error rates fall above an appropriately set threshold t . We call this technique the normal eviction technique.
Instead, we may use our confidence interval estimates to only evict workers who are truly poor. That is, we may use  X  p an estimate of the lowerbound of the error rate of a worker. If the lowerbound is still higher than an appropriately set threshold t with confidence c , we can be confident that the worker is indeed poor and can then evict him/her. We call this technique the conservative eviction technique. Note that varies for each worker, i.e., for some workers we can be more precise about their error, and for other less so. Thus, conservative eviction sets an error threshold that varies by worker, taking into account our precision.

Since some workers may be retained across phases, we estimate error rates across multiple phases. That is, we compute the average rate over all phases a worker has been involved in, and then apply the threshold. (There are other options for averaging but are not considered here.)
To illustrate our techniques, consider a scenario where a newly hired worker will have error rate 0 . 3 with probability 0 . 3 , 0 . 2 with probability 0 . 4 , and 0 . 1 with probability 0 . 3 . Note that identifying and evicting poor workers in such a scenario is especially important since the error rates of the good and bad workers are very different.
Our goal is to compare normal and conservative eviction on two metrics: (a) how poor our workers are at the end of each phase, and (b) how many good workers do we mistakenly evict. Our first metric prefers techniques that select for good workers, while the second metric prefers techniques that are not overly aggressive. Note that mistakenly evicting good workers can result in a poor reputation for the requester in the crowdsourcing marketplace, and can have detrimental effect on quality in the long run (since good workers will refuse to work with the requester). Note also that the two aspects are interrelated, e.g., aggressive eviction may improve quality, but may evict many good workers.
 To evaluate these aspects, we use a simple accuracy cost function C = c 1 +  X c 2 : The value c 1 evaluates how bad the workers are at the end X  X e assign a cost of 1 for ending up with a worker with error rate 0 . 2 , and a cost of 3 for a worker with error rate 0 . 3 (and a cost of 0 for a worker with error rate 0 . 1 ). The value c how many good workers do we mistakenly evict X  X e assign a cost of 5 for each worker with error rate 0 . 1 evicted at the end of a phase, and a cost of 0 if we evict workers with error rates 0 . 2 or 0 . 3 . The parameter  X  &gt; 0 is a multiplier that allows us to weight c relative to each other. Note that these costs are merely illustrative: in a real application, we may use different functions for c and different  X  s.

In our experiment, we evaluate C for both techniques at the end of every phase, and then take the average across phases. We fix the number of phases k at 30 , with the number of tasks in each phase n being 25 . (Similar results were obtained for k = 50 , 75 , 100 , with nk fixed.)
In Figure 5(a), we depict the average accuracy cost C (where  X  = 1 ) in log scale as a function of threshold t , for both eviction techniques, and for 35% confidence intervals (Similar behavior is observed for 50% and 75% intervals.) Note that we consider neg-ative threshold values, even though error probabilities can only be positive. Our techniques can yield negative estimates, e.g.,  X  p can be negative. We find that it is more effective not to cut off these values at 0, since the magnitude still conveys some useful infor-mation (e.g., we would rather evict a worker with lowerbound -0.2 than one with a bound of -0.1).

As can be seen in the figure, the average cost of conservative eviction is typically lower than normal eviction across all thresh-olds (except for a small region between 0 . 2 and 0 . 3 where the dif-ference is not much). Moreover, we find that on both sides of 0 , i.e., from 0  X  0 . 4 and from 0  X   X  0 . 4 , the accuracy cost grows much slower for the conservative technique than the normal tech-nique. The slower growth implies that conservative is less sensitive to the threshold choice than normal. Thus, even though at their op-timal thresholds both techniques perform roughly equally, if we are unable to set the threshold precisely, conservative will do better.
To study the impact of  X  on performance, we repeated the ex-periment with  X  = 5 and  X  = 1 5 , depicted in Figures 5(b) and 5(c) respectively. We note similar behavior for both plots. For large  X  , the curve is almost monotonically decreasing X  X he trough is al-most not visible, unlike  X  = 1 or 1 5  X  X his behavior is not surprising given that if we put too large a penalty on evicting good workers, then the best strategy is to simply not evict any workers. Also, for both figures, we find that conservative eviction has lower cost than normal eviction for almost every threshold (except for  X  = tween 0.17 and 0.35, where the difference is not very large). Just as in Figure 5(a), the accuracy cost growth of conservative eviction is slower than normal eviction on both sides of the origin X  X hus even if we happen to not choose the optimal threshold, conserva-tive eviction will ensure that we end up not paying as much of a price in terms of accuracy cost.)
To summarize, our confidence interval estimates provide a useful basis to evaluate and maintain worker quality across phases, while simultaneously ensuring good quality and few evictions of good workers.
Our base methods assume a relatively simple model, where task outputs are binary, and worker error rates are not dependent on task difficulty or type. We also assume that task selectivity is unknown. If we have more information, we can obtain more refined confi-dence intervals by extending our base mechanism. Here we briefly discuss four such extensions. Other more sophisticated extensions are possible, but not covered here due to space limitations. Selectivity: In some cases, the task selectivity s may be known be-forehand. That is, we may know in advance that with probability s the result of given task is Y, and with probability (1  X  s ) it is N. In this case, we can construct a  X  X  X orker X  who always responds with  X  X es X  if s &gt; 0 . 5 and no if s &lt; 0 . 5 . This worker will have an error rate of min ( s, 1  X  s ) . By considering this worker along with two or more additional workers, we can use the 3 worker or mul-tiple worker differences scheme, to get error rates for the workers. The differences method gives more precise results when more data is available [16], and so using selectivity can improve our worker evaluation by providing us with the responses of an extra  X  X orker X . Non-binary tasks: Some tasks may have non-binary outcomes. For example, if the task is to identify the background color in a photograph, we may have outcomes red, blue, green, and yellow. However, non-binary tasks can be reduced to multiple binary tasks, which can then be analyzed by our methods. To illustrate, say the output of a task is one of k categories. If k is a power of two, then we can express the category as a log ( k ) length binary string, and convert the task to log ( k ) binary tasks of the form : Is the i th bit of the category 1 ? For example, for the four colors red, blue, green and yellow, we can map (for our analysis) each worker answer to two answers for the questions  X  X s the background either red or green? X  and  X  X s the background either yellow or green? X . If k is not a power of two, we can add more categories until it becomes a power of two. In our example, if the picture backgrounds were all red, green or blue, we can still add another category yellow. The workers will never reply yellow, but our methods can still be applied to the results.
 Multiple Task Types: Typical crowdsourcing marketplaces have tasks of various types, for example, translation (language centric), identifying people in images (knowledge centric), or debugging code (programming centric). A worker may have a different ap-titude level for each task type, and hence a different error rate, vio-lating our assumption of a single error rate. To account for multiple task types, we can apply our method to tasks of only one type at a time, and find a worker X  X  error rate separately for each task type. Varying Task Difficulty: Tasks in a set may have varying difficul-ties, and the error rate of a worker may be higher for more difficult tasks. If we can identify difficulty level of tasks beforehand, then we can treat tasks of different difficulty level the way we treated tasks of different type. If we don X  X  know difficulty levels before-hand, we can try to deduce them by looking at the strength of the majority of worker responses. For instance, a majority close to 100% would indicate an easy task, while majority close to 50% would indicate a hard one. We illustrated in Figure 1 the use of this technique, and the improvements it yields.
The prior work related to ours can be placed in four categories; we describe each of them in turn: Crowd Algorithms: There has been a lot of recent work on find-ing crowdsourcing analogs of standard data processing algorithms, such as filtering [21], sorting and joins [14, 19], deduplication and clustering [3, 23, 31] and categorization [22, 26]. Most of these algorithms assume a simple model of human errors (i.e., that all human beings are alike.) All these algorithms would benefit from a  X  X refiltering X  or  X  X valuation X  phase where humans with low accu-racy not employed for tasks.
 Statistics and EM: Expectation Maximization, or EM [7, 8] has been studied and used in the statistics and machine learning com-munity for several decades now, with many textbooks and surveys on the topic [15, 20, 30]. Expectation Maximization provides max-imum likelihood estimates for hidden model parameters based on a sequence of steps that converge to a locally optimal solution. We compare our approach against Expectation Maximization in Sec-tion 5. However, EM, unlike our technique, does not provide con-fidence intervals for error rate estimates.

There is a variant of the EM algorithm called multiple impu-tation [27], which can be used to derive a distribution of hidden model parameters. In our case, we derive confidence intervals for hidden model parameters. Unlike our method which provide cor-rect guarantees, multiple imputation is heuristic in nature.
We use many fundamental concepts from statistics and probabil-ity [32, 4], including the Wilson score interval.
 Worker Error Estimation: The work most closely related to ours is that of simultaneous estimation of answers to tasks and errors of workers (typically using the EM algorithm). There have been a number of papers studying increasingly expressive models for this problem, including difficulty of tasks and worker expertise [9, 13], adversarial behavior [25], and online evaluation of workers [33, 24, 18]. While our worker error model and task model are simpler, we provide confidence interval guarantees along with error rates, allowing users of our technique to have more fine-grained informa-tion to evaluate workers.

There has also been some work on selecting which items to get evaluated by which workers in order to reduce overall error rate [17, 28, 11]. While [11] uses heuristic confidence intervals in their algorithm, but does not provide confidence guarantees, the other papers do not provide confidence intervals of any kind.
 Applications: There are a number of applications that crowdsourc-ing has been successfully used for, including sentiment analysis [12], identifying spam [10], determining search relevance [2], transla-tion [35]. All these applications would benefit from a prefilter-ing phase where the workers are evaluated and the poor workers (judged based on average behavior as well as confidence intervals) can be barred from further work.
In this paper, we presented a technique for determining confi-dence intervals in addition to worker error rates. Confidence inter-vals provide the equivalent of  X  X uarantees X  in the setting where we need to determine worker errors, and thus are useful when we need to precisely determine the range in which worker errors may lie. We showed that our confidence intervals are relatively accurate even in scenarios where our assumptions do not hold, and we discussed extensions for scenarios where the assumptions are markedly dif-ferent.
