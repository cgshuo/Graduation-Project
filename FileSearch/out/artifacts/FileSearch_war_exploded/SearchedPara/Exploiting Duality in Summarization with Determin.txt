 Summarization is an important task in data mining. A major chal-lenge over the past years has been the efficient construction of fixed-space synopses that provide a deterministic quality guaran-tee, often expressed in terms of a maximum-error metric. His-tograms and several hierarchical techniques have been proposed for this problem. However, their time and/or space complexities remain impractically high and depend not only on the data set size n , but also on the space budget B . These handicaps stem from a requirement to tabulate all allocations of synopsis space to different regions of the data. In this paper we develop an alternative method-ology that dispels these deficiencies, thanks to a fruitful application of the solution to the dual problem: given a maximum allowed er-ror, determine the minimum-space synopsis that achieves it. Com-pared to the state-of-the-art, our histogram construction algorithm reduces time complexity by (at least) a B log 2 n log  X  factor and our hier-archical synopsis algorithm reduces the complexity by (at least) a the optimal error. These complexity advantages offer both a space-efficiency and a scalability that previous approaches lacked. We verify the benefits of our approach in practice by experimentation. F.2 [ Analysis of Algorithms and Complexity ]: Miscellaneous; H.3 [ Information Storage and Retrieval ]: Miscellaneous; H.2.4 [ Database Management ]: Systems X  Query processing Algorithms, Experimentation, Theory, Performance efficiency, histograms, synopses, wavelets
Work supported by grant 7160/05E from Hong Kong RGC and by project PENED 2003 in the 3rd Community Support Programme. Copyright 2007 ACM 978-1-59593-609-7/07/0008 ... $ 5.00. The need to reduce a very large data set into a compact representa-tion or synopsis that captures its basic characteristics arises often; it finds application in OLAP/DSS systems [28], approximate query answering [26, 3], cost-based query optimization [24], time-series indexing [4], data mining [23], data stream approximation [2, 5] and the efficient handling of multi-measure [6] and multidimen-sional data sets [18]. Diverse synopsis data structures have been proposed [9]; the goal with all of them is to minimize an appropri-ate error metric over the original data in a given space budget. Past research has led the way from conventional synopsis techniques such as histograms [15, 17, 26, 14, 11, 2] and Haar wavelets [24, 28, 3, 7, 8, 20, 11, 12, 13, 6] to more sophisticated ones such as compact hierarchical histograms [27] and the Haar + tree [21]. A general optimal-histogram construction algorithm was presented by Jagadish et al. [17] and later specialized by Guha et al. [14] for the case of maximum-error metrics. The practical usefulness of this class of error metrics has spawned focused attention to them in recent studies, all based on hierarchical synopsis structures. A dy-namic programming algorithm that derives the optimal Haar wavelet synopsis for a maximum-error metric (as opposed to the computa-tionally easier Euclidean error) was developed by Garofalakis and Kumar [8] and optimized in terms of space and time by Guha [11]. Later, Guha and Harb [12, 13] improved on the robustness of the re-stricted Haar wavelet synopsis model of [8, 11]. In their approach, wavelet coefficient values in the synopsis are arbitrary, and differ from those in the wavelet transform of the data; their approxima-tion scheme for unrestricted Haar wavelet synopses achieves both higher accuracy of approximation and better asymptotic behavior in time than the restricted model. Recent research has created less restrictive hierarchical synopsis data structures in two independent routes [27, 21]. The Haar + tree [21] goes further, in terms of flex-ibility, than the unrestricted Haar wavelet model, by enhancing the structure itself. The Compact Hierarchical Histogram (CHH) was independently introduced in [27]; as we observe in this paper, it is a special case of a Haar + tree. In other words, a Haar + merges the unrestricted Haar wavelet and the CHH models. Both [27] and [21] experimentally demonstrate that the structures they propose can, in certain circumstances, achieve higher quality of ap-proximation than the optimal histogram of [17, 14].
 Despite this progress, the complexities of all summarization algo-rithms for maximum-error metrics are still inefficiently high. The main reason for this defect is their dependence on the given synop-sis space budget B , due to a requirement to tabulate possible allo-cations of space to different data intervals; the problem is gravest in the models of [12, 13, 21], due to their two-dimensional tab-ulation over both space and candidate approximation values. An effort to tame these space complexities [11] did not manage to erad-icate their dependence on B ; besides, as we show, it creates an un-wieldy tradeoff between time-and space-efficiency in the case of maximum-error metrics. In this paper we eliminate these shortcom-ings with an alternative approach, based on a lucrative application of the solution to the dual , error-bounded problem: detect a space-optimal synopsis under an error bound. Our solutions do not tab-ulate over B and do not present performance tradeoffs. Compared to the state of the art, in histogram construction we reduce the time complexity from (at least) O ( nB log 2 n ) to O ( n log  X  archical synopsis construction, we reduce the complexity from (at least) O ( R 2 n log 2 B ) to O ( R 2 n (log  X  +log n )) in time and from (at least) O ( RB log n B ) to O ( R log n + n ) in space, where optimal error and R the cardinality of an examined value set. We experimentally verify the practical implications of this reduction. In this section we briefly present previous approaches to offline data reduction with a maximum-error deterministic guarantee. We con-sider the principal synopsis structures employed, namely plain his-tograms and hierarchical representations. Under both approaches, given an n -size data vector D =  X  d 0 , d 1 , . . . , d n  X  1 is to devise an approximate representation  X  D of D using at most B space, so that a given error metric in the approximation is min-imized. Maximum-error metrics are most generally expressed in their weighted version: where  X  d i denotes the reconstructed value for d i and w weight for the corresponding error value; in the case of the maxi-mum relative error (MRE), it is w i = 1 max {| d a sanity bound that prevents small values from unnaturally domi-nating the error result [8]. In the case that  X  i, w i = 1 , the error metric at hand is the maximum absolute error (MAE). Previous studies [17, 8, 12, 13, 27, 21] have generalized their results into wider classes of distributive and Minkowski-distance metrics. Still, the sub-class of maximum-error metrics remains more practically interesting than the esoteric metrics of those classes [7]. A histogram synopsis (also called segmentation or partitioning) di-vides D into B n successive disjoint intervals [ b i , e i i  X  B called buckets or segments , and attributes a single value v to each of them that approximates all consecutive values therein, d , j  X  [ b i , e i ] . A single bucket (segment) can be expressed by the triad s i = { b i , e i , v i } . Given a target error metric, the best value for v i is defined as a function of the data values in [ b 2 B  X  1 numbers suffice to represent a B -bucket histogram (since  X  i, 1 &lt; i  X  B, b i = e i  X  1 +1 and the edges are fixed). Initial work on histograms focused on heuristics [16]. An O ( n 2 B ) dynamic programming algorithm that assigns optimal bucket boundaries for the Euclidean ( L 2 ) error metric ( O ( n 3 B ) for other metrics) was presented 2 in [17]. The basic idea behind it is that the b -optimal histogram for D can be recursively derived from the space of ( b  X  1) -optimal partitionings of prefix vectors of D . For a maximum-error metric, the minimal error E ( i, b ) of a b -bucket histogram of the prefix vector  X  d 0 , d 1 , . . . , d i  X  is recursively expressed as:
For the Euclidean error, the optimal v i is the mean of the values in [ b i , e i ] [17]; for MAE it is the mean of maximum and minimum values in [ b i , e i ] , while for MRE a case analysis is given in [14].
Since this problem is a special case of the problem of approximat-ing a curve by line segments, the solution of [17] is a special case of the algorithm introduced in [1]. where E ( j +1 , i ) measures the minimal maximum error for a bucket that contains the items  X  d j +1 , . . . , d i  X  . The resulting algorithm requires an O ( nB ) tabulation of minimized error values E ( i, b ) and chosen last-bucket boundaries j corresponding to those opti-mal error values. Guha et al. [14] proposed a specialization of the general-purpose algorithm of [17] for (among others) MRE (ap-plicable to any maximum-error metric). The crucial observation is that, in order to determine the j that minimizes the max function in Equation 1, it suffices to perform a binary search, since E ( j, b  X  1) and E ( j + 1 , i ) are monotonic functions of j . [14] employs an in-terval tree to determine the minimum error for a bucket in logarith-mic time. Hence this algorithm requires O ( nB log 2 n ) time and O ( nB ) space. Table 1 summarizes the complexity results of pre-vious work on the offline one-dimensional histogram construction problem for maximum-error metrics and introduces the complexity of the solution 3 we propose; B is the space-bound expressed as the number of buckets and  X  is the optimal error. The space-efficient variant of the algorithm in [14] is discussed in Section 3. Table 1: Summary of results for optimal offline one-dimensional histogram construction (maximum-error metrics) Another stream of research has been based on index structures that represent the data in consecutive hierarchical levels of detail. This approach started with the application of the Haar wavelet decom-position, long used in signal processing [19]. Most recently, two independent, yet interrelated structures employing a hierarchy have been introduced [21, 27]. We now review this research.
 The Haar wavelet hierarchy can be visualized through a complete binary tree, the Haar tree . The coefficient in the Haar tree root node contains the overall average value and each other coefficient value c i contributes the value + c i to all data values (leaves) in its left sub-tree and  X  c i to those in its right sub-tree. Hence each original data value is reconstructed by adding/subtracting the coef-ficients in the path towards its position. Figure 1a depicts the Haar decomposition of an example data vector D of 8 values (shown at the leaves of the tree). Value d 3 =  X  6 can be reconstructed as + c 0 + c 1  X  c 2 + c 5 . A Haar wavelet synopsis of D is a vector B n non-zero  X  i, c i  X  terms, such that its inverse wavelet trans-form  X  D = W  X  1 (  X  Z ) approximates the data vector D . Figure 1b of Figure 1a, with maximum absolute error 4 . This is the optimal MAE synopsis with B = 4 . For the Euclidean error ( L 2 ), the opti-mal Haar wavelet synopsis consists of the top-B normalized coeffi-cients of the complete Haar wavelet transform [19]; the normalized value of a coefficient c is | c |  X  sides in the Haar tree. For example, the L 2 -optimal synopsis, with B = 2 , for the data vector in Figure 1a is { X  0 , 4  X  ,  X  5 ,  X  7  X  X  . This After this work was submitted for publication, [2] proposed an where U is the size of the domain for data values; as U can be arbitrarily large, our solution retains its competitiveness towards that algorithm too.
 computational convenience has allowed for the extension of the L synopsis methodology to various settings [10, 18, 5, 6]. On the other hand, the problem is computationally harder for maximum-error metrics.
 Restricted Synopses for Maximum-Error Metrics. After its iden-tification in [24], the first systematic treatment of the space-bounded Haar wavelet synopsis problem for maximum-error metrics was based on a randomized rounding scheme [7]. However, as shown in [14] and [8], this scheme does not produce results of high qual-ity. Garofalakis and Kumar [8] suggested a dynamic program-ming (DP) scheme that deterministically retains the optimal coeffi-cient subset of a dataset X  X  Haar wavelet transform. [20] proposed a streaming-capable and reliable greedy counterpart to this solution. Muthukrishnan [25] suggested that an algorithm solving the dual, error-bounded problem 4 can provide a shortcut to the solution of the space-bounded problem, gaining a log n log  X  -factor time complex-ity advantage. Still, these solutions are all confined to the restricted variant of problem, in which a coefficient may be only assigned a fixed value in the complete Haar tree (candidate assigned values are also fixed in advance in the low-quality probabilistic model). Unrestricted Synopses for Maximum-Error Metrics. Guha and Harb [12, 13] discerned that the values assigned to the coefficients retained in a wavelet synopsis can be arbitrary and provided a fully polynomial-time approximation scheme (FPAS) for the resulting unrestricted space-bounded Haar wavelet synopsis problem. The solution of [12, 13] is a DP algorithm guided by a two-dimensional tabulation per Haar tree node. Each node c i calculates the mini-mum attainable error E ( i, v, b ) over both every possible incoming value 5 v and every possible amount of space b allocated to the sub-tree rooted at c i ; possible incoming values are discretized by a reso-lution step  X  . For each E ( i, v, b ) entry, both the  X  -optimal assigned value z (also quantized as a multiple of  X  ) and the  X  -optimal distri-bution of b units of space among the left i L and right i of c i are detected. This DP recursion can be summarized as:
E ( i, v, b ) = min Computing E (0 , 0 , B ) determines the best B nodes to keep in the synopsis and the best values z to be assigned to each of these nodes for a given value of  X  . The ranges of incoming values v and as-signed values z to be tested per node can be restricted using the maximum absolute value M in D [12], or, more efficiently, by a guessed upper-bound E for the target minimized error [13]. In both cases, the resulting cardinality R = O ( M  X  ) or R = O ( set of examined values enters the complexity expressions.
That is, find a minimal-space synopsis achieving error bound .
The incoming value of a node c i is the value constructed by the path from the root of the sparse Haar tree up to c i . For example, the incoming value of node c 7 in the tree of Figure 1b is c 0 The Haar + Tree The Haar + tree [21] extends the Haar wavelet hi-erarchy by allowing extra coefficient values which contribute their (signed) value to a single dyadic interval alone. In the example Haar + tree of Figure 2, node c 0 (root coefficient) contributes its value to all approximated data values { d 0 , d 1 , d 2 , d followed by a binary tree of triads ( C 1 , C 2 and C 3 ), which substi-tute the single non-root coefficients of the classical Haar tree. In each such triad (e.g., C 1 ), the head coefficient (e.g., c its value positively to its left sub-tree and the same value negatively to its right sub-tree. The left (e.g., c 2 ) and right (e.g., c mentary coefficients contribute their values positively only in the single subinterval that they affect (e.g., c 2 contributes positively to d and d 1 only). An optimal synopsis of space budget B for a given error metric E places B non-zero coefficient values at any posi-tions in the Haar + tree so that E is minimized. For example, for the four-element data set { 5 , 3 , 12 , 4 } the 2 -term Haar minimizes the MAE consists of the coefficients { c 0 = 4 , c The Haar + structure outperforms its predecessors in both accuracy of approximation and synopsis construction time [21]. The Compact Hierarchical Histogram (CHH) [27] is a related data approximation structure, which defines a (binary by default) hierar-chy of (dyadic) intervals and selects an optimal subset of nodes to represent the approximated data set. In fact, the CHH structure is equivalent to a Haar + tree, in which only supplementary coefficients are allowed. With the benefit of hindsight, a Haar + tree can be seen as a merging of a CHH and a Haar tree. [27] proposed heuristic CHH construction techniques, after observing that the calculation of the optimal value to retain on a node is computationally hard, due to the interdependence between nodes in the hierarchy. On the other hand, [21] eschews this problem by an approximation tech-nique, similar to that in [12, 13], which provably approximates the theoretically optimal solution by a small margin of error. Hence, the Haar + technique can achieve at least as high accuracy as an heuristically derived CHH over a binary hierarchy due to both its structural and algorithmic advantages. Guha [11] identified space as the most significant resource for an offline summarization problem and furnished a space-efficiency paradigm for synopsis construction. His main idea is to avoid stor-ing all tabulated results throughout the DP; part of them can be dropped and re-computed later. In histogram construction, the tabu-lation (Equation 1) on { i, b } should progress with increasing b , 1  X  b  X  B (i.e., the loop of b is the outer loop). Since the values E (  X  , b ) are fully determined by E (  X  , b  X  1) , after a b -column has been used to calculate the ( b +1) -column, it is dropped. Hence the space is O ( n ) . Besides, the tabulation also detects and stores the sin-gle bucket M ( i, b ) in the optimal b -partitioning of  X  d that contains the middle data item b n 2 c of the summarized vector. After the optimal error E ( n, B ) and middle-item bucket M = M ( n, B ) have been established, the two O ( n 2 ) independent sub-n ) probabilistic restricted Haar problems for the intervals on the left and right of M are re-solved recursively. Hence, the total time for the general-error histogram construction algorithm [17] becomes O  X  P log n ` =1 2 ` ` n O ( n 2 B ) , i.e., the re-computation cost is amortized. [11] applies the same methodology to the restricted Haar wavelet synopsis al-gorithm of [8]. In this case, the required tabulation progresses in a bottom-up fashion in the Haar tree; all table entries on a parent node are computed from the tables of its children nodes, which can then be dropped. Accordingly, at most log n + 1 tables need be concurrently stored, covering one path through the Haar tree. Af-ter the solution is established at the top level of the Haar tree, the two half-size sub-problems in the two sub-trees of c 1 are re-solved [11]. Restricted Haar wavelet synopsis construction requires time quadratic to n , because each of n Haar tree nodes has to consider O (2 log n ) = O ( n ) possible choices of values in its ancestor-set [8]. Hence, the re-computation cost is amortized in this case as well. Table 2 summarizes the complexity results of previous work on the offline one-dimensional space-bounded hierarchical synopsis con-struction problem for maximum-error metrics and introduces the complexity of the solution we propose; q is a probability quantiza-tion parameter, R is the cardinality of the examined set of incoming or assigned values per coefficient, and  X  is the optimal error. We explain the space-and time-efficient variants in the sequel. The state-of-the-art for all examined methods features a demanding tabulation over space allocations [14, 12, 13, 27, 21]. Guha strived to tame these demands [11]; the result was good, but not sufficient: the burden of space tabulation remains. This burden is heaviest for the unrestricted Haar and Haar + methods: their two-dimensional tabulation renders their memory requirement impractical for large data sizes. Besides, the amortization achieved by the paradigm of [11] does not hold for the algorithms of time linear (or near-linear) to n reviewed in Section 2. Applied on them, the paradigm creates a tradeoff between space-and time-efficiency, as [21] presented for the Haar + case. Hence, applied on the MRE algorithm of Sec-tion 2.1, this technique decreases its space requirements to O ( n ) , but increases its time complexity to O ( nB log 3 n ) (Table 1). The same holds for the unrestricted Haar and the Haar + cases of Sec-tion 2.2 (Table 2). In the space-efficient variant, after the arrays of E ( i,  X  ,  X  ) , they are dropped. Again, at most log n + 1 arrays need to be concurrently stored. The price for this space-efficiency is an extra log n time complexity factor due to re-computation. For the time-efficient variant two different approaches are possible: If B memory. The size of the array at node c i , residing in level ` tree, is O ` R min { B, 2 ` i }  X  , which, after summation, gives a space complexity of O ( Rn log B ) . Still, if B to keep only the at most log n +1 necessary arrays, with the full solutions corresponding to each of their entries appended on them as lists, as suggested in [13]. The size of a solution maintained with each entry of an array at level ` i is at most min { B, 2 the space required for an array at level ` i is O ` R (min { B, 2 This sums up to a space complexity of O ` RB 2 log n B  X  expressions are equal when n log B = B 2 log( n B )  X  B = ues of B both higher and lower than the preferable method depends on the application at hand. Table 2 shows both. A similar performance tradeoff applies to the win-ner greedy heuristic of [27] (Table 2). Overall, the complexity question on summarization with deterministic guarantees remains unsatisfactorily resolved. In this paper, we provide an alternative methodology that addresses 6 these shortcomings. We show how the space-bounded summarization problems can be solved more efficiently by exploiting their duality to the corresponding error-bounded problems through binary search; in those dual problems, the goal is to minimize the space of a synopsis that achieves error no larger than an error bound . In the sequel, we formulate and solve the error-bounded histogram and hierarchical synopses prob-lems. Then we define and analyze Indirect synopsis construction algorithms for the corresponding space-bounded problems. This section introduces our solution to the space-bounded histogram construction problem for weighted maximum-error metrics. Our technique utilizes the solution to the complementary error-bounded problem. Section 4.1 presents a linear algorithm 7 for this auxil-iary problem, which achieves the minimal space B  X  under an error bound ; we discuss how the solution can be tested on whether it achieves, secondarily, the minimal error  X  in the required space B , providing a strong optimization. In Section 4.2 we exploit this solution in order to efficiently solve the space-bounded histogram construction problem, which is our main interest and contribution. We formulate the L w  X  -bounded histogram construction problem: Problem 1 Given a data vector D and an L w  X  -error bound , con-struct a histogram H of D with the minimum number of buckets B , such that L w  X  ( D , H )  X  .
The basic idea behind this methodology was applied for the re-stricted Haar wavelet synopsis problem in [25], but yielded only a marginal benefit (see Table 2) that did not reveal its full potential.
A similar algorithm was proposed in [22] for the effective sum-marization of data streams, albeit it treated the MAE metric only. Our algorithm for this problem establishes a minimal-space his-togram H of B  X  buckets that satisfies in one linear pass, drawing from the following Lemma.

L EMMA 1. Let B  X  be the minimum number of buckets required to satisfy the bound for data vector D and H = {{ b i , e 1  X  i  X  B  X  be a B  X  -bucket histogram such that the achieved error is L w  X  ( H , D )  X  . Furthermore, let L w  X  ,i be the error in bucket (segment) s i = { b i , e i , v i }  X  H , i &lt; B advance the right bucket boundary e i by one position, so that the remains  X  L w  X  ,i  X  , then the new segmentation  X  H as a whole also achieves the error bound L w  X   X   X  H , D  X   X  .
 Based on Lemma 1, the MinHistSpace algorithm of Figure 3 per-forms a linear scan of the data. During this scan, it extends the right boundary of the running segment s i as long as L w  X  ,i encountered data item d i with error weight w i defines a tolerance interval [ d i  X  w isfy for d i . The algorithm only needs to calculate the intersection I of such intervals for arriving data items. When I becomes null, s cannot be magnified any more. Then a new bucket boundary is inserted before the last read data item. The value assigned to the formed bucket is defined as v = w j d j + w k d k w data items responsible for the limits 8 a , b of the last non-null value of I = [ a, b ] . Hence MinHistSpace needs O ( n ) time and space. As an example, assume that we want to find a histogram with L error at most = 5 approximating the data vector D = { 11 ,  X  1 ,  X  6 , 8 ,  X  2 , 6 , 6 , 10 } . MinHistSpace scans D and com-putes H incrementally. d 0 = 11 combined with d 1 =  X  1 violate the bound (the absolute error of such a bucket is 11  X  (  X  1) Thus, the first bucket has b 1 = e 1 = 0 and value v 1 = 11 . The algorithm continues by putting d 1 in the next bucket which is terminated when d 3 = 8 is found ( d 3 violates ). Continu-ing this way, MinHistSpace eventually computes the histogram H = {{ 0 , 0 , 11 } , { 1 , 2 ,  X  3 . 5 } , { 3 , 6 , 3 } , { 7 , 7 , 10 }} . Figure 3: Minimum Space Histogram construction algorithm We now prove that MinHistSpace achieves space-optimality.
T HEOREM 1. The histogram H returned by MinHistSpace has achieved the minimal space B  X  subject to the L w  X  -error bound .
P ROOF . Let B be the number of buckets in H . Assume there exists a histogram segmentation of D in B 0 &lt; B segments H {{ b 0 i , e 0 i , v 0 i }} , 1  X  i  X  B 0 , such that L w will be at least one segment { b 0 i , e 0 i , v 0 i }  X  H where e 1 i is the right boundary of the i -th segment in H , otherwise H 0 would not have less segments than H . Let { s 0 i , e 0
The item d i most distant from the limit at hand, hence of smallest weight w i , is chosen in case more than one items are responsible for the same limit. be the first such segment encountered from left; then s 0 hence [ s 1 i , e 1 i ]  X  [ s 0 i , e 0 i ] . Since [ s 0 [ s , e 1 i + 1] can also satisfy this bound as a bucket. However, if al-gorithm MinHistSpace has fixed the i -th segment as [ s 1 the interval [ s 1 i , e 1 i + 1] could not make a segment satisfying . By reductio ad absurdum, it follows that there is no histogram seg-mentation H 0 as we assumed. Hence the B is the -optimal space B .
 The following lemma defines an error-optimality test for the his-togram returned by MinHistSpace . Given the result H of an exe-cution of MinHistSpace , the objective of the test is to determine, with one more call of MinHistSpace (i.e., in linear time), whether the actual L w  X  -error of H is the minimum possible for the space B  X  occupied by H .
 L EMMA 2. Let H be the B  X  -bucket histogram segmentation of D for error bound returned by MinHistSpace and  X   X  be the actual L w  X  -error of H . Let  X  H be the  X  B -bucket histogram segmenta-tion of D returned by MinHistSpace running under the constraint L  X  ,r &lt;  X  , allowing error values less than but not equal to  X  . Let be the minimum L w  X  -error of a histogram segmentation of D in B  X  buckets. Then  X  =  X  if and only if  X  B &gt; B  X  .

P ROOF . B  X  is the least number of buckets required to satisfy error bound  X   X  , hence  X  B  X  B  X  . If  X  B = B  X  , then there exists a B  X  -bucket histogram partitioning of D with L w  X  -error less than  X  , hence H has not achieved the optimal error  X  in B  X  buckets. Therefore  X  =  X   X   X  B &gt; B  X  . In reverse, if  X  B &gt; B togram partitioning of D with L w  X  -error less than  X  requires more than B  X  buckets, hence H has achieved error optimality. Thus,  X  B &gt; B  X   X   X  =  X  . In conclusion,  X  =  X   X   X  B &gt; B We now define an efficient algorithm for space-bounded histogram construction under a maximum-error metric that exploits the so-lution to the dual error-bounded problem. Formally, given a data vector D and a space bound B , we seek a histogram with at most B buckets that has minimal L w  X  -error. The crucial observation is that the L w  X  -error of the optimal B -histogram is monotonically non-decreasing with B . Therefore we can apply binary search with guesses of in the space of error-bounded problems. This idea is materialized by our IndirectHist algorithm shown in Fig-ure 4. In our implementation, the seed value of is obtained by linearly measuring the L w  X  -error of an equi-width B -bucket his-togram of D , which provides an upper bound for the B -optimal L  X  -error. Thereafter, the MinHistSpace procedure is repeatedly invoked with binary search on the error bound value ; it performs an optimality test , as defined in Lemma 2, for each guessed er-ror bound value that does not require more than B space. The search terminates when the guessed error bound reaches a value that requires a histogram of  X  B  X  B space and actual error  X  , while the optimality test indicates than any error bound &lt;  X  re-quires  X  B &gt; B space; then an optimal histogram of minimum error =  X  in the space budget B has been created. At line 8 of Fig-ure 4, the call MinHistSpace ( &lt;  X  ) corresponds to a variation of MinHistSpace (  X  ) , in which the condition at line 5 of Figure 3 is replaced by ( L w  X  ,r  X   X  ) . This search process brings an O (log runtime factor 9 , hence the time complexity of the Indirect algo-rithm is O ( n log  X  ) . Section 6.2 verifies the time advantage of this algorithm in practice. In this section we introduce our solution to the space-bounded hi-erarchical synopsis problem for maximum-error metrics. We study both the unrestricted Haar and Haar + models. Again our technique exploits the solution to the dual error-bounded problem. We formulate a strong version of the L w  X  -bounded hierarchical syn-opsis problem as follows: Problem 2 Given a data vector D and an error bound , construct a representation  X  Z of D , producing a reconstruction  X  D , such that L minimized. Of all representations with s  X  non-zero terms satisfying , select the one with the minimal actual error  X   X  .
 An incoming value at node c i of the Haar tree (or triad C Haar + structure) is a value reconstructed in the path of ancestor co-efficients from the root node up to c i in the sparse representation of D . In a wavelet decomposition W ( D ) , this is the average value in the interval I under the scope of c i , henceforward called real incoming value at c i . Similarly, an assigned value at node c coefficient value retained at that node in  X  Z ; in W ( D ) , this is the ac-tual semi-difference of the average values in the two sub-intervals I , I R under the scope of c i , henceforward called real assigned value. For example, in Figure 1a, the real incoming value of node c is 2 , while the incoming value constructed for this node in the synopsis of Figure 1b is also 2 . On the other hand, the incoming value of node c 3 in Figure 1b is 4 , whereas the corresponding real incoming value is 5 (see Figure 1a). Similarly, the real assigned value to node c 3 is  X  3 , whereas the value assigned to this node in the synopsis  X  2 . These concepts are directly extended to the Haar tree [21]. In order to construct our solution, we need to explore the space of possible retained coefficients and values assigned to them. We use a dynamic-programming (DP) framework, as in previous hierarchical synopsis algorithms [6, 7, 8, 11, 25, 12, 13, 21]. In a bottom-up process, this algorithm considers all possible incoming values v and, for each v , all possible assigned values z node c i of the Haar tree and determines the optimal value to assign at c i for v ; in a Haar + tree, possible head and left/right supplemen-tary coefficients, z h , z l , z r , on a triad C i are all examined. We quantize the (real-valued) domains of v and z v i into multiples of a small resolution step  X  . The next section outlines some lemmata that establish upper and lower bounds for these domains.
The log function expresses the dependence of running time on the derived error value; it is to be understood as a growth function, as in [25]; the case  X   X  1 does not imply non-positive time. We study the simple Haar wavelet case first. As we will see, despite its disadvantage in accuracy and, for non-maximum error metrics, complexity, in relation to the Haar + tree, the classical Haar wavelet structure has an advantage in its potential for delimitation of search space for maximum error metrics.

L EMMA 3. Let v i be the real incoming value at node c i . Let v be an incoming value to c i for which the error bound under the L  X  metric can be satisfied, and  X  = min j  X  I {| w j |} , where I is the interval under the scope of node c i ; then | v i  X  v |  X   X  . Lemma 3 implies that the finite set S i  X  IR of possible incoming values we have to examine at node c i consists of the multiples of  X  in the interval [ v i  X   X  , v i +  X  ] ; thus, |S i |  X  b 2  X  We now demarcate the assigned values.

L EMMA 4. Let v i be the real incoming value to node c i , z real assigned value at c i , v  X  S i be a possible incoming value to c for which the maximum error bound can be satisfied, and z value that can be assigned at c i for incoming value v , satisfying ; then | z i  X  z v i |  X   X   X  | v i  X  v | .
 Lemma 4 implies that the finite set S v i  X  IR of possible assigned values we have to examine at node c i , for a given incoming value v  X  S i , consists of the multiples of  X  in the interval [ z v | ) , z i +(  X   X  | v i  X  v | )] ; hence, |S v i |  X  b 2(  X   X  X  v Lemmata 3 and 4 are most simple in the case of the maximum absolute error metric, when  X  i, w i = 1 ; in the case of the maximum relative error metric,  X  =  X  max { S, max j  X  I {| d j |}} , where S is the sanity bound. Naturally, the same lemmata hold with any upper bound E for the optimal L w  X  error of a synopsis, even when that error is not known in advance. This observation will be useful in our implementation of the direct solution to the space-bounded problem (Section 6.3).
 Haar + Synopses.
 Delimitation lemmata analogous to Lemmata 3 and 4 also apply to the Haar + structure. [21] shows how the flexibility of this structure enables an equally robust delimitation of the search space, based on minimum and maximum data values, for any target error met-ric; this comes in contrast to the classical Haar tree, where such target-generic delimitation is not possible. However, due to the same flexibility, the delimitation that exploits a given error bound, particular to the case of a maximum error metric, is less tight with the Haar + structure. In this case, the delimitation lemmata take the following forms.

L EMMA 5. Let m i be the minimum and M i the maximum in-dividual data value under the scope of triad C i and v  X  S possible incoming value at C i for which the maximum error bound is satisfied, and  X  = min the scope of C i ; then v  X  [ m i  X   X  , M i +  X  ] .
 Lemma 5 implies that the set S i of incoming values we have to examine for triad C i consists of the multiples of  X  in the interval [ m i  X   X  , M i +  X  ] ; thus, |S i |  X  b M i  X  m i +2  X   X  c + 1 = O (  X   X  is the difference of the minimum from the maximum value in D . We now demarcate the values assigned to the head coefficient.
L EMMA 6. Let v  X  S i be a possible incoming value at C i z  X  S v i,H be a value that can be assigned at the head coefficient of C for incoming value v , satisfying the individual-data error bound ; then | z h |  X  min { M i  X  v, v  X  m i } +  X  .
The inequality  X  accommodates for the variation in the number of integers in a fixed interval.
 Lemma 6 implies that the finite set of possible assigned values we have to examine for the head coefficient at C i is S v |S i,H | = O (  X   X  ) . The possible assigned values at the left and right supplementary coefficients of triad C i can be delimited in a sim-ilar fashion. Based on this delimitation, we devise our dynamic programming solution. Its essence is the same in both the Haar wavelet and the Haar + case. We use the former model as our illus-trative example. The extension to the latter is straightforward by incorporating provisions for the supplementary coefficients. In a nutshell, our recursive MinHaarSpace procedure works in a bottom-up left-to-right scan over the Haar (or Haar + ) tree. At each visited node c i it calculates an array A of size |S i | from the pre-calculated arrays L and R of its children nodes c i L , c array C for the child i C of the root node). A holds an entry A [ v ] for each possible incoming value v at c i (a single element A for the root node). Such an entry contains: (i) the minimum number A [ v ] .s = S ( i, v ) of non-zero coefficients that need to be retained in the sub-tree rooted at c i with incoming value v , so that the resulting synopsis satisfies the error bound ; (ii) the  X  -optimal value A [ v ] .z to assign at c i , for incoming value v ; and (iii) the actual minimized maximum error A [ v ] .e thus obtained in the scope of c i recursively expressed as: The above equations compute the least of (i) the minimum required space if a non-zero coefficient value z is assigned at node c (ii) the required space if a zero value is assigned at it. The latter case applies only if 0  X  S v i . For economy in presentation, the +1 term that appears in the former case is uniformly expressed by the boolean integer ( z 6 = 0) . This convention is used throughout this coefficient at c i can be omitted with incoming value v , or 0  X  S 1 otherwise. The former case occurs if and only if the maximum error yielded by v at the affected data values below c i satisfies . The s entry of array A at node c i for each allowed incoming value v , A [ v ] .s , is computed from those of arrays L and R of children nodes c i L and c i R (array C for the child c i C of the root node). Let  X  S v i  X  IR denote the set of those assigned values at node c for incoming value v that require the minimum space in order to achieve the error bound : The  X  -optimal value to select is the one among these candidates that also minimizes, in a secondary priority, the obtained L in the scope of c i . Let E ( i, v ) be the minimum L w  X  in the scope of c i with incoming value v and an assigned value z , with S ( i, v ) coefficients retained in the sub-tree rooted at c This error value is assigned to A [ v ] .e ; the value A [ v ] .z is the as-signed value that minimizes the error expression above. For a last level node ( i  X  n 2 ) , if 0 /  X  S v i , then the best non-zero value z to assign at c i is the one that minimizes the L w  X  error yielded at the two affected data values: w i L | d i L  X  ( v + z  X  ) | and w ( v  X  z  X  ) | . This maximum error is minimized when the two are equal: w i L | d i L  X  ( v + z  X  ) | = w i R | d i R  X  ( v  X  z lute error, this is the actual Haar wavelet decomposition value at node c i . Hence, for last-level nodes, we do not need to consider multiples of  X  ; the value of E ( i, v ) for a last-level node is:
E ( i, v ) = This error value has to be assigned to A [ v ] .e in this case; A [ v ] .z is either 0 or z  X  , respectively. A pseudo-code for the proposed recur-sive MinHaarSpace DP procedure is shown in Figure 5. Following the generic space-efficiency paradigm of [11], the memory occu-pied by the arrays C , L and R needs to be reserved only when their entries are first computed and is freed after they have in turn been used for the creation of A . Therefore, for a data set of size n , the maximum number of arrays that need to be concurrently stored is log n + 1 : one array for each level of resolution plus the currently computed ones. This maximum is necessitated when the right-bound post-order recursion reaches the right-most Haar tree node. This basic bottom-up process computes the wavelet trans-form X  X  incoming and assigned values on-the-fly in order to define the sets S i and S v i as it needs. Hence a recursive procedure that derives the  X  -optimal space result answer without constructing the synopsis itself is defined.
 Complexity Analysis The result array A on each node c i holds |S | entries, one for each possible incoming value, hence its size is O (  X  ) ; besides, at each node c i and for each v  X  S through all |S v i | possible assigned values needs O (  X  the runtime of MinHaarSpace (0 , ) is O ` (  X  ) 2 n  X  . Besides, since at most log n + 1 arrays need to be concurrently stored, the space complexity is O `  X  log n + n  X  , where n stands for the storage of the data. The construction of the synopsis after the  X  -optimal answer has been established by a run of MinHaarSpace (0 , ) presents us with a time-space tradeoff. We outline both alternatives.
 The Space-Efficient Solution. After MinHaarSpace returns from the topmost level, so that the values of c 0 and c 1 have been estab-lished, we can call a process that reenters the problem in the two branches of c 1 and recomputes the respective solutions thereafter, recursively. The total running time is the sum of the basic running time for all re-entered sub-problems. Setting ` as the Haar tree On the other hand, the space complexity remains O `  X  log n + n as we need to maintain the stored data set (or its wavelet transform) throughout the computation.
 The Time-Efficient Solution. Alternatively, we may choose to maintain all necessary computed information throughout the re-cursion of MinHaarSpace . This maintenance allows us to con-struct the final solution as soon as the minimum space has been derived. Consider a DP array entry A [ v ] at node c i ; this entry de-scribes the local part of a candidate solution, for incoming value v , which has already been calculated in the sub-tree rooted at c The rest of this candidate solution is maintained by annexing to entry A [ v ] the set of all coefficient values retained in it. There-with the sub-problem re-entry is avoided. The total running time remains only O ` (  X  ) 2 n  X  . For each DP array entry A [ v ] of node c at level ` , a set of at most min { B M , 2 ` } coefficients is retained, where B M is the maximum size of a candidate solution stored throughout the computation. Thus, the space complexity becomes O  X  storage of the decomposition is not required. MinHaarSpace approximates the optimal solution in IR . In the space-bounded problem, if E B is the optimal maximum absolute ( L  X  ) error for a B -term synopsis in IR , then rounding its val-ues to the closest multiples of  X  can increase that error by at most min { B, log n } [12, 21]. For the error-bounded problem, we can formulate the conditions under which the minimum space under resolution  X  is the optimal in IR , as follows:
T HEOREM 2. Let B be the minimum space, under resolution  X  , that satisfies the L  X  error bound , and E be the minimum L error that can be achieved within space B  X  1 , under resolution satisfy error bound in IR .

P ROOF . If E  X  , then the approximation algorithm for the error-bounded problem with bound would find the solution with B  X  1 space; hence E &gt; . Let E B  X  1 be the error achieved by the optimal ( B  X  1) -term representation in IR . B is the optimal space for bound if and only if cannot be satisfied with less than B non-zero terms; hence it should be E B  X  1 &gt; . Since E  X  E
B  X  1 +  X  2 min { B  X  1 , log n } , a sufficient condition for optimality is E  X   X  2 min { B  X  1 , log n } &gt; , or  X  &lt; 2( E X  ) According to Theorem 2, in order to ascertain that the answer B , derived for an L  X  -error bound under resolution  X  , is optimal in IR , we need to derive the error result E for the space-bounded prob-lem with space bound B  X  1 under  X  . If  X  and E satisfy the condition value of  X  and repeat the process until we reach space-optimality. As discussed in Section 2.2, the state-of-the-art solution [12, 13, 21] for space-bounded hierarchical synopsis construction is bur-dened by a two-dimensional tabulation of E ( i, v, b ) entries per node. We infer that, as in the histogram case, the space-bounded problem can be more efficiently solved through a binary search in-vocation of the algorithm for the error-bounded that shuns the tab-ulation over b . In our implementation, the upper bound of in the search is the E corresponding to the synopsis of B largest Haar decomposition coefficients by absolute value, easily computed in O ( n log B ) time; the lower bound of is the ( B + 1) -th high-est absolute coefficient value | z k | . Given that the solution to the strong error-bounded problem minimizes the error within the  X  -optimal space, its application to the space-bounded problem yields the  X  -optimal error when the binary search converges to the space budget B . Still, in order to ensure the termination of the search, MinHaarSpace also performs an optimality test (as in Section 4.2) for guessed error bound values that require less than B space. Hence, the search terminates when it reaches an error bound that either requires a synopsis of exactly B space, or requires a synop-sis of  X  B &lt; B space and actual error  X  , while any error bound &lt;  X  requires  X  B &gt; B space. When the tested error bound is decreased during the binary search, the minimum error derived for the previ-ous bound is used for determining the new bound. Figure 6 shows a pseudocode for this IndirectHaar algorithm.
 Complexity Analysis. As in the histogram case, the binary search increases the time requirements of the error-bounded problem by an O (log  X  ) worst-case factor. We present a space-efficient solution without dependence on B . The advantage of the alternative time-efficient solution in time is negligible, since the O (log comparable to the log n factor which is paid only once for synopsis construction. Since the highest value of the changing bound is E , the runtime of this Indirect algorithm is O ` ( E  X  ) 2 n (log The former log term expresses the cost of the binary search, while the latter expresses the cost of constructing the B -term synopsis in a space-efficient manner after the optimal error value been established. This complexity absorbs the O ( n log B ) term for determining the seeds of the search. The log  X  factor does not grow with n , hence this runtime is decisively lower than the O ` ( E rithm and, unless 11 n B log B , lower than its O ` ( E  X  basic runtime too. Besides, the Indirect algorithm requires O space of the space-efficient Direct algorithm in cases where log n marization scenaria, assuming B  X  n 2 . In addition, the respective O ( n 2 ) -time restricted algorithm uses O ( B log n B + n ) = O ( n ) space [11], which becomes larger than O ` E  X  log n + n  X  B log n B E  X  log n . This inequality holds when B E  X  and, additionally, ( n B ) B n E  X   X  n B large enough summarization problems. In conclusion, this Indirect algorithm has better asymptotic behavior than both direct counter-parts in time and space. Section 6.3 verifies the runtime benefit of this Indirect algorithm in practice.
The constraint is verified for reasonable B n ratios; e.g. for B = 16 , B log B = 65536 .
 Our focus is the Indirect solution to the space-bounded problem. We have devised an algorithm for the error-bounded problem in order to serve this goal. Still, this algorithm can present an in-dependent interest of its own. In this context, it is comparable to the O ( n 2 log n ) -time restricted Haar algorithm for the maximum-error-bounded problem that was proposed in [25]. This algorithm tries the incoming values yielded by all 2 ` ancestor subsets of a node c i at level ` in the Haar tree; it stops recursing and resorts to local search within each of the d n log n e sub-trees in the bottom b log log n c Haar tree levels; the examined assigned values for c are z i and 0 . The application of Lemmata 3 and 4 prunes ancestor subsets that add up to prohibited incoming values in this algorithm, yet does not annul its near-quadratic time complexity. This com-plexity is due to the fact that the restricted strategy explicitly enu-merates and examines different ancestor-subsets whose coefficients may add up to nearby incoming values. In contrast, the unrestricted Haar and Haar + strategies precalculate a set of equally-spaced al-lowed incoming values that anticipate all possible contributions an-cestor coefficients can add up to. The algorithm in [25] determines the minimum space without constructing the synopsis itself, us-ing O ( n ) space for storing the decomposition. A sub-problem re-computation in this case costs O ting k = log n 2 ` , the complexity becomes O  X  n P log n O ( n 2 log n ) . Hence, constructing the synopsis does not present a time-space tradeoff. On the other hand, the algorithm of Section 5.1 is linear to n ; hence, for sufficiently large n , it outpaces the near-quadratic restricted Haar algorithm. For an appropriate value of  X  , it produces better synopses too (as in [12, 13, 21]). Hence, our al-gorithm for the error-bounded problem not only provides the basis for a more efficient solution to the dual space-bounded problem, but treats the error-bounded problem itself more efficiently and ac-curately than previous approaches too. In this section we present experimental results demonstrating the advantage of our Indirect solutions vs. the respective Direct for all considered summarization methods. Both solutions compute syn-opses of equal error (in the hierarchical cases, for equal resolution  X  ); hence our comparison pertains to runtime (with MAE as the tar-get metric); besides, the space advantage for hierarchical synopses is clear, due to its connection with B . All algorithms were imple-mented with the g++ 3.4.3 compiler and run on a 4 CPU Opteron 2.2GHz machine with 4GB of main memory running Solaris. We used two real data sets. The first data set (TM) is a sequence of 178,080 sea surface temperature measures extracted from drifting buoys positioned throughout the equatorial Pacific. The average value in TM is 26.75 and the set has a standard deviation of 1.91. The second data set (FC) is extracted from a relation of 581,012 tuples describing the forest cover type for 30 x 30 meter cells, ob-tained from US Forest Service. FC contains the frequencies of the distinct values of attribute aspect in the relation. The frequen-cies average at 1613 (standard deviation: 730) and feature spikes of large values (min value: 499, max value: 6308). FC and TM were downloaded from the UCI KDD Archive. 12
Available at http://kdd.ics.uci.edu/ In this experiment we measure the runtime for histogram construc-tion. We compare the Direct solution of [14], in its space-efficient variant, to our Indirect method (Section 4.2). For the Direct method, we measured the basic runtime required to derive the optimal error result only. The Indirect algorithm computes the optimal histogram segmentation per se, which is the same for both. Figure 7a shows their performance as a function of n with a constant summarization ratio B = n/ 64 for various-sized subsets of the TM data set. As expected from our theoretical analysis, Indirect vastly outperforms Direct . Our second experiment measures the running time with re-spect to the bucket space B for a constant data size n . Figure 7b shows the results for the FC data set with constant n = 360 . As ex-pected, the Indirect method exhibits its independence of B in both cases; it always terminated after a few repetitions (mean 12 . 3 ). In contrast, the runtime of Direct grows with B . In this experiment we measure the runtime of hierarchical sum-marization algorithms, starting with the classical Haar case. We first compare the Indirect method of Section 5.2 to two versions of the maximum-error unrestricted Haar synopsis algorithm of [12, 13]. The former, Direct , first calculates, in O ( n log B ) time, the target error for the synopsis consisting of the top-B Haar wavelet terms by absolute value; then it employs it for bounding the search space. The latter, OracleDirect , is an infeasible algorithm, which represents a conceptual limit for the best-case performance of the guess-based solution of [13]. In OracleDirect , the value of the fi-nal optimal error is assumed to be provided in advance by an or-acle, hence its search space is optimally delimited. Both direct algorithms compute the same error result as Indirect . After ex-perimentation, we settled for a reasonable, in the given data set, constant value of  X  = 0 . 1 (i.e. the resolution step for delimiting the domains of incoming and assigned values) for all three algorithms. Smaller values burdened the running time without significant qual-ity increase; larger values were undermining the quality of the syn-opses. We also ran an enhanced version of the restricted algorithm of [8, 11] ( Restricted ), which also prunes its search space using a precalculated error bound. For all algorithms, we measured the ba-sic time required to derive (for OracleDirect , to verify) the optimal error result. Figure 8a shows (on a log-log graph) their performance as a function of n with a constant summarization ratio B = n/ 64 for various-sized subsets of the TM data set. As expected, Indirect presents the most affordable runtime growth; not only it outper-forms Direct , but it outpaces the OracleDirect too; hence it invari-ably produces identical quality in shorter time and smaller space. Besides, Restricted , which achieves lower synopsis quality, under-goes the fastest growth, eventually becoming the slowest, due to its quadratic time complexity; this result reconfirms the finding of [12] in the realm of these pruning-intensive enhanced variants and with larger data sizes that reveal the disadvantage more clearly. Figure 8b plots (on a log-lin graph) the runtime for the FC data set with respect to B (for constant n = 512 , obtained after zero-padding the wavelet decomposition), setting  X  at 5 and 10 . The results for Direct exhibit the interplay between two factors affecting the run-ning time: One the one hand, the increase of B results into tighter delimitation of the search space based on a smaller pre-calculated error upper bound, with a significant impact on running time. How-ever, B affects the time complexity itself as well. Hence the run-time of Direct presents unstable behavior, with a maximum at the intermediary position B = 15 . On the other hand, the runtime of Indirect tends to decrease as B (hence the tightness of the er-ror bound) grows; this algorithm terminated after a few repetitions, even fewer than in the histogram case: it converges more robustly thanks to its exploitation of the strong version of the error-bounded problem. Haar + Synopsis Construction.
 We repeated, in the Haar + case, with the same data sets and config-urations, the comparison between the Indirect method of Section 5.2 and its Direct counterpart [21]; this also prunes its search space as much as possible, using a precalculated error bound and Lem-mata 5 and 6. Figure 9 shows the results. The runtime of Direct is larger in this case, due to the less intensive pruning that the Haar structure allows. However, the performance of Indirect is equally satisfactory as in the classical Haar case (compare Figures 8a and 9a). The difference is more conspicuous for runtime versus syn-opsis size B (Figure 9b). In this case, the increasing tightness of the pre-calculated error bound cannot overcome the effect of the in-creasing B itself on the runtime of Direct ; hence, it grows with B . Still, the runtime of Indirect shows a decreasing trend as B grows in this case too. The plots of Figure 9 have identical x-axes to those of Figure 8, but their logarithmic y-axes are scaled differently for the sake of readability. In this paper we have examined the problem of summarization with deterministic guarantees from a new perspective, applied on state-of-the-art histogram and hierarchical methods. We demon-strated the advantage gained by solving the computationally heavy and memory-hungry space-bounded problems through their lighter error-bounded counterparts; this advantage consists of complexities which are lower, independent of synopsis space and free of perfor-mance tradeoffs; it stems from the removal of a tabulation that hin-dered previous solutions and, in the hierarchical case, the tight de-limitation of the search space. In conclusion, our solutions provide the most recommendable option for the time-and space-efficient offline summarization of very large data sets with a maximum-error guarantee. In the future, we plan to extend our techniques to the summarization of multi-measure and multidimensional data.
