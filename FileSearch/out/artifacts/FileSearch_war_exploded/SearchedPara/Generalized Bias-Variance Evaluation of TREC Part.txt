 Recent research has shown that the improvement of mean re-trieval effectiveness (e.g., MAP) may sacrifice the retrieval stability across queries, implying a tradeoff between effec-tiveness and stability. The evaluation of both effectiveness and stability are often based on a baseline model, which could be weak or biased. In addition, the effectiveness-stability tradeoff has not been systematically or quantita-tively evaluated over TREC participated systems. The above two problems, to some extent, limit our awareness of such tradeoff and its impact on developing future IR models. In this paper, motivated by a recently proposed bias-variance based evaluation, we adopt a strong and unbiased  X  X ase-line X , which is a virtual target model constructed by the best performance (for each query) among all the participat-ed systems in a retrieval task. We also propose general-ized bias-variance metrics, based on which a systematic and quantitative evaluation of the effectiveness-stability tradeoff is carried out over the participated systems in the TREC Ad-hoc Track (1993-1999) and Web Track (2010-2012). We observe a clear effectiveness-stability tradeoff, with a trend of becoming more obvious in more recent years. This implies that when we pursue more effective IR systems over years, the stability has become problematic and could have been largely overlooked.
 Category and Subject Descriptors: H.3.3 [Information Search and Retrieval] General Terms: Theory, Measurement, Performance Keywords: Evaluation, Effectiveness-stability tradeoff, Bias-variance tradeoff, Virtual target model
While IR research is often focused on improving the re-trieval effectiveness (e.g., mean average precision), the per-formance could become instable across queries. Such an effectiveness-stability tradeoff has been evidenced by some recent experiments [1, 3, 8, 9], but the tradeoff has not been systematically and quantitatively evaluated over the systems participated in TREC tasks.

The effectiveness evaluation has a long history in IR. One of the most commonly used effectiveness metrics is the mean average precision (MAP). The evaluation often involves the comparison with a baseline. Armstrong et al. [2] argued that the effectiveness evaluation based on a weak baseline is not sufficient to prove the effectiveness of a test method. On the other hand, the retrieval stability issue is an emerging topic. To address this issue, some risk metrics (e.g., RI (reliability of improvement) and U risk ) [7, 4] have been proposed. These risk metrics usually rely on the performance comparison a-gainst a single baseline method. However, Dincer et al. [5] described that the evaluation based on a single baseline sys-tem is biased since different baselines can yield different risk values. It is suggested that a less biased approach is to con-struct the baseline from a set of different systems/runs [5]. To sum up, we need a strong and unbiased baseline to eval-uate the effectiveness-stability tradeoff.

In this paper, our X  X aseline X  X s a virtual target model made from the best performance (for each query) among all the IR systems in a TREC task. The concept of the virtual target model was mentioned in [8]. Indeed, to evaluate the retrieval effectiveness and stability in an integrated manner, the bias-variance metrics of average precision (AP) were proposed in [8]. However, the experiments in [8] are limited to query expansion using a single target model, and have nothing to do with the systems that participated in a real TREC task.
We will propose a generalized formulation of the bias-variance analysis with respect to IR performance metrics (such as MAP and ERR) and construct a systematic evalu-ation of TREC participated systems. Specifically, we gener-alize the original bias-variance framework by 1) making the bias-variance definition more consistent with the definition in estimation theory, 2) extending from the bias-variance of AP to MAP and any other mean effectiveness metrics, and 3) explicitly formulate the target model in the evaluation metrics. We extend AP to MAP for avoiding the adverse influence of some too big/small AP values in the computa-tion of variance. In addition, we propose to quantify the tradeoff degree by two standard correlation coefficient mea-sures. Furthermore, we carry out systematic evaluation of the systems that participated in the TREC Ad-hoc Track (1993-1999) and Web Track (2010-2012).
In [8], the performance difference between a current model under evaluation and the target model is viewed as a kind of error. By assuming the target AP is 1, which is AP X  X  maximal value, this error can be formulated as E ( AP 1) 2 It can be decomposed into bias and variance: where the expectation E ( ) is computed over all queries and E ( AP ) computes the mean of AP, i.e., the MAP of the cur-rent model. A smaller E ( AP 1) 2 means that the current model is closer to the target model. Since E ( AP ) computes MAP, the smaller bias (i.e., [ E ( AP ) 1] 2 ) can reflect the better retrieval effectiveness. The smaller variance of AP suggests that the current model is more stable.

Since the maximal AP (i.e., 1) is an ideal case, a practical version of Bias 2 ( AP ) is also defined in [8] as [ E ( AP ) M AP T ] 2 , where M AP T is the upper-bound MAP that is achieved by a single model in the reported case study [8]. In [8], it states that this practical bias has the similar trend as the original bias in Eq. 1. Using the practical bias, the sum of bias and variance can be: In [8], the retrieval effectiveness-stability tradeoff has been observed based on the above bias-variance in a case study.
Now, we introduce a generalized bias-variance formula-tion. Our motivation is that in the estimation theory [6], bias and variance are defined to evaluate the estimation quality of the distribution parameter (i.e., mean) of a vari-able. If AP is treated as a variable like in [8], it is more gen-eral to define bias-variance on the mean of AP X  X  distribution across queries, i.e., Mean of Average Precision (MAP). We first introduce an expected squared error based on MAP as: where f is a current model under evaluation, and f T is the target model which has the best performance for each query; Q
S is a query sample of the query population Q which con-tains all the possible queries. This expected squared error considers the MAP X  X  difference between the current model f on the query samples Q S and the target model f T on the query population Q .
 To facilitate the bias-variance decomposition, we can rewrite Eq. 3 and decompose it as: where the expectation E ( ) is computed over all query sam-ples. If we consider each query as a query sample, the bias-variance of MAP in Eq. 4 is equivalent to the bias-variance of AP in Eq. 2 . Therefore, the bias-variance of AP can be considered as a special case of the bias-variance of MAP.
In addition to the single query sampling, we can parti-tion all the queries (denoted as Q ) in a test collection into several subsets Q S , and treat each query subset as a query sample. In the experiments, we adopt two partition meth-ods: one is random partitioning and the other is based on query difficulty (detailed in Section 3.2.2).

Under the above query sample configurations, the term of bias in Eq. 4 can be derived as: It computes the derivation of M AP ( f; Q ) w:r:t: M AP ( f which explicitly formulates a virtual target model f T . f is constructed by assigning the best performance (for each query among all the considered systems/runs in a retrieval task) to it. Dincer et al. [5] described that the evaluation based on a single baseline system is biased since different baselines can yield different risk values. The virtual tar-get model f T designed in this paper is unbiased, since it is constructed by all various considered systems.

From the formulation of the bias in Eq. 5, we can know that the better the retrieval effectiveness (measured by MAP of the current model f ) is, the smaller the bias will be. How-ever, the better retrieval effectiveness does not guarantee a better retrieval stability, which will be measured by the vari-ance term (i.e., V ar ( M AP ( f; Q S )) in Eq. 4).
In Eq. 4, the variance term is a direct way to measure the variance of MAP of the current model. However, in this manner, even the target model may have a big variance value because of the variability of the query difficulty across dif-ferent query subsets. Therefore, we normalize M AP ( f; Q as: The target MAP, i.e., M AP ( f T ; Q S ) will be 1 for all the query samples after the normalization. Then, the bias-variance formulation on the normalized MAP can be Table 1: Datasets and topics used for Ad-hoc Track. where th e variance considers the variance of the normalized MAP. We refer this variance as the normalized variance. In this manner, the normalized variance of the target model becomes zero. Now, we can focus on the performance vari-ability caused by the model/system rather than the query difficulty across different query samples. Note that if we use the normalized MAP, the smaller normalized bias can still imply a better retrieval effectiveness.
In addition to MAP, we can use other metrics (e.g., ERR or NDCG) in Eq. 3. Denoting a mean performance metric as M , we have a more general expected squared error: Correspondingly, we can have the bias-variance decompo-sition as we did for MAP and the normalized MAP. The retrieval effectiveness-stability tradeoff reflected by the bias-variance formulation of the metric ERR will also evaluated in our experiments.
We carry out bias-variance evaluation on Ad-hoc Track and Web Track. For each task, we evaluated all the submit-ted systems/runs for several years. Table 1 shows the doc-ument collections and query topics used for Ad-hoc Track. For Web Track from 2010 to 2012, ClueWeb09 dataset and the query topics (provided by organizers) are used, based on one task (i.e., adhoc task) on Web Track. The Web Track 2013 data are not available to us.

MAP is the effectiveness measure for Ad-hoc Track, and the main effectiveness measure for Web Track is expected reciprocal rank at 20 documents (denoted as ERR@20). For a better distinguishability, we adopt (M)ERR@20 to repre-sent the mean value of ERR@20 on all test queries.
We analyze the tradeoff between the squared bias and vari-ance of AP on Ad-hoc Track from 1993 to 1999 and ERR@20 on Web Track from 2010 to 2012. In Table 2(a), we quan-tify the tradeoff by the Pearson and Spearman correlation coefficient of Bias 2 and V ar . It shows that the correlation coefficients are often strongly negative (i.e., r &lt; 0 : 7) on Ad-hoc Track and Web Track, indicating quite significant tradeoff between effectiveness and stability of runs/systems. Figure 1: Results of Bias 2 , V ar and Bias 2 + V ar of MAP and (M)ERR@20 based on query difficulty partition, where x -axis represents the runs partic-ipated in Ad hoc 1993 and Web Track 2012.
The partition strategy of Q can affect the bias-variance results. We first randomly partition the whole query set and each partitioned subset includes 10 queries. The process is repeated for ten times, based on which the average value of these bias and variance is computed.

We can obverse Bias 2 and V ar of MAP and (M)ERR@20 in Table 2(b). On both tracks, it shows a clear bias-variance tradeoff, evidenced by the strongly negative correlation co-efficients. The absolute values of correlation coefficients of MAP and (M)ERR@20 are often smaller than those of AP. This indicates that the variance is more smooth which is consistent with our motivation in designing the variance of MAP (see the last paragraph in Introduction).

There is a problem of random partition: the bias-variance results are different for different random partitioning pro-cesses. Therefore, we partition all the queries into several subsets on the basis of query difficulty. The query difficulty is measured by the best performance (i.e., the best AP) of a given query. The lower the best AP is, the more difficult the corresponding query is. We rank all the queries based on the query difficulty degree and group them into several subsets based on the rank, with each subset including 10 queries. Note that we have similar results when we set each subset as different sizes, or measure the query difficulty based on the average performance (among systems) of a query.
Table 2(c) shows bias-variance results based on the query difficulty partition. We still see a clear bias-variance trade-MAP/(M)ERR@20 based on query difficulty partition. off. Fi gure 1 visualizes the tradeoff on the Ad-hoc 1993 and Web track 2012. In Figure 1, all the systems are sorted in an ascending order of Bias 2 . In addition to bias and variance, it also plots the sum of them ( Bias 2 + V ar ). We can observe that the V ar of most systems/runs increases along with the decrease of Bias 2 . This stresses a research question: how to get a better retrieval stability (a lower variance) when we pursue a high effectiveness (a lower bias).

In [8], it is stated that the smaller Bias 2 + V ar reflects the better overall retrieval performance (considering both effectiveness and stability). In Figure 1, we observe that the lowest Bias 2 + V ar is not corresponding to the lowest squared bias. This verifies that the best overall performance is not only determined by the best effectiveness.
We now report the evaluation results when the bias and variance are all obtained by a normalization process imposed on M AP ( f; Q S ). As discussed in Section 2.2.2, the normal-ization is expected to help us focus more on the performance variability caused by the model/system rather than the vari-ability of the query difficulty.

In Table 2(d), there still exists a tradeoff between Bias and V ar of regularization MAP and (M)ERR@20 (because of the negative correlation coefficients), but it is less serious compared with the former results. This is because that we have normalized the performance variability caused by the variability of query difficulty.

One interesting point is that if we look at the correlation coefficients along the TREC years, we can observe that the coefficients often become closer to 1 (a more obvious bias-variance tradeoff) along with the increasing  X  X ecentness X  of years. This indicates a more obvious effectiveness-stability tradeoff, in more recent years.
In this paper, we have proposed a generalized bias-variance evaluation strategy and evaluated the IR systems which par-ticipated in Ad-hoc track(1993-1999) and Web track (2010-2012). The bias-variance evaluation results can show the effectiveness-stability trends with respect to different sys-tems, tasks, and years. We observe clear bias-variance trade-off, which indicates retrieval effectiveness-stability tradeoff of the participated systems. In addition, the experimen-tal results (in Figure 1) show that the improvement of ef-fectiveness does not always mean the improvement of the overall retrieval performance (considering both effectiveness and stability). Moreover, the effectiveness-stability tradeoff could become more obvious in more recent TREC years (see Table 2(d)). Since participated systems (especially before 2013) are mainly designed to achieve better performance based on traditional effectiveness metrics, we can speculate that the stability of IR may become more problematic yet could have been overlooked in TREC contests over years.
Currently, we do not know the detailed algorithms/methods behind those tested systems, so our evaluation has a limita-tion that we do not fully understand why some systems have good effectiveness but bad stability, or are both effective and stable. In the future we will carry out a systematical eval-uation of a large number of typical IR models, and analyze such questions in depth, for more insights in how to improve both retrieval effectiveness and stability.
The authors would like to thank Mr. Qian Yu for the help on the experiments. This work is supported in part by the Chinese National Program on Key Basic Research Project (973 Program, grant No.2013CB329304, 2014CB744604), and the Natural Science Foundation of China(grant No. 61402324, 61272265, 61105072).
