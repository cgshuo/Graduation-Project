 The goal of Brain-Computer Interface (BCI) research [1, 2, 3, 4, 5, 6, 7] is to provide a direct control pathway from human intentions reflected in brain signals to computers. Such a system will not only provide disabled people more direct and natural control over a neuro-prosthesis or over a computer application (e.g. [2]) but also opens up a further channel of man machine interaction for healthy people to communicate solely by their intentions. Machine learning approaches to BCI have proven to be effective by requiring less subject training and by compensating for the high inter-subject variability. In this field, a number of studies have focused on constructing better low dimensional representations that combine various features of brain activities [3, 4], because the problem of classifying EEG signals is intrinsically high dimensional. In particular, efforts have been made to reduce the number of electrodes by eliminating electrodes recursively [8] or by decomposition techniques e.g., ICA, which only uses the marginal distribution, or Common Spatial Patterns (CSP) [9] which additionally takes the labels into account. In practice, often a BCI system has been constructed by combining a feature extraction step and a classification step.
 Our contribution is a logistic regression classifier that integrates both steps under the roof of a single minimization problem and uses well controlled regularization. Moreover, the classifier output has a probabilistic interpretation. We study a BCI based on the motor imagination paradigm. Motor imagination can be captured through spatially localized band-power modulation in the  X  -(10-15Hz) or  X  -(20-30Hz) band characterized by the second-order statistics of the signal; the underlying neuro-physiology is well known as Event Related Desynchronization (ERD) [10]. 1.1 Problem setting ment 1 , where d is the number of electrodes and T is the number of sampled time-points in a trial. We consider a binary classification problem where each class, e.g. right or left hand imaginary movement, is called positive (+) or negative (  X  ) class. Let y  X  X  +1 ,  X  1 } be the y for an unobserved trial X . 1.2 Conventional method: classifying with CSP features In the motor-imagery EEG signal classification, Common Spatial Pattern (CSP) based clas-sifiers have proven to be powerful [11, 3, 6]. CSP is a decomposition method proposed by Koles [9] that finds a set of projections that simultaneously diagonalize the covariance as: Then, the simultaneous diagonalization is achieved by solving the following generalized eigenvalue problem: Note that for each pair of eigenvector and eigenvalue ( w j ,  X  j ), the equality  X  j = holds. Therefore, the eigenvector with the largest eigenvalue corresponds to the projection with the maximum ratio of power for the  X + X  class and the  X   X   X  class, and the other-way-around for the eigenvector with the smallest eigenvalue. In this paper, we call these eigenvectors filters 3 ; we call the eigenvector of an eigenvalue smaller (or larger) than one a filter for the  X  +  X  class (or the  X   X   X  class) , respectively, because the signal projected with them optimally (in the spirit of eigenvalues) captures the task related de -synchronization in each class. It is common practice that only the first n of largest eigenvectors and the last n of smallest eigenvectors are used to construct a low dimensional feature representation. The feature vector consists of logarithms of the projected signal powers and a Linear Discriminant Analysis (LDA) classifier is trained on the resulting feature vector. To summarize, the conventional CSP based classifier can be constructed as follows: How to build a CSP based classifier : 2.1 The model We consider the following discriminative model; we model the symmetric logit transform of the posterior class probability to be a linear function with respect to the second order statistics of the EEG signal: where  X  := ( W, b )  X  Sym( d )  X  R , W is a symmetric d  X  d matrix and b is the bias term. The model (3) can be derived by assuming a zero-mean Gaussian distribution with no temporal correlation with a covariance matrix  X   X  for each class as follows: However training of a discriminative model is robust to misspecification of the marginal distribution P ( X ) [13]. In another words, the marginal distribution P ( X ) is a nuisance parameter; we maximize the joint log-likelihood, which is decomposed as log P ( y, X |  X  ) = log P ( y | X,  X  ) + log P ( X ), only with respect to  X  [14]. Therefore, no assumption about the generative model is necessary. Note that from Eq. (4) normally the optimal W has both positive and negative eigenvalues. 2.2 Logistic regression 2.2.1 Linear logistic regression We minimize the negative log-likelihood of Eq. (3) with an additional regularization term, which is written as follows: Here, the pooled covariance matrix  X  P := 1 n term in order to make the regularization invariant to linear transformation of the data; if simply the Frobenius norm of a symmetric matrix  X  W ; the transformation corresponds to the is the negative logarithm of the conditional likelihood words the probability of observing head ( y i = +1) or tail ( y i =  X  1) by tossing n coins with the loss term of Eq. (5) converges asymptotically to the true loss where the empirical average is replaced by the expectation over X and y , whose minimum over functions in L 2 ( P X ) is achieved by the symmetric logit transform of P ( y = +1 | X ) [15].
 Note that the problem Eq. (5) is convex . The problem of classifying motor imagery EEG signals is now addressed under a single loss function. Based on the criterion (Eq. (5)) we can say how good a solution is and we know how to properly regularize it. 2.2.2 Rank=2 approximation of the linear logistic regression Here we present a rank=2 approximation of the regression function (3). Using this approx-imation we can greatly reduce the number of parameters to be estimated from a symmetric matrix coefficient to a pair of projection coefficients and additionally gain insight into the relevant feature the classifier has found.
 The rank=2 approximation of the regression function (3) is written as follows: function is that the Bayes optimal regression coefficients in Eq. (4) is the difference of two positive definite matrices; therefore two bases with opposite signs are at least necessary in capturing the nature of Eq. (4) (incorporating more bases goes beyond the scope of this contribution).
 The rank=2 parameterized logistic regression can be obtained by minimizing the sum of the logistic regression loss and regularization terms similarly to Eq. (5): Here, again the pooled covariance matrix  X  P is used as a metric in order to ensure the invariance to linear transformations. Note that the bases { w 1 , w 2 } give projections of the signal into a two dimensional feature space in a similar manner as CSP (see Sec. 1.2). We call w 1 and w 2 filters corresponding to  X + X  and  X   X   X  classes, respectively, similarly to CSP. The filters can be topographically mapped onto the scalp, from which insight into the classifier can be obtained. However, the major difference between CSP and the rank=2 parameterized logistic regression (Eq. (7)) is that in our new approach, there is no distinction between the feature extraction step and the classifier training step. The coefficient that linearly combines the features (i.e., the norm of w 1 and w 2 ) is optimized in the same optimization problem (Eq. (7)). 3.1 Experimental settings We compare the logistic regression classifiers (Eqs. (3) and (6)) against CSP based classifiers with n of = 1 (total 2 filters) and n of = 3 (total 6 filters). The comparison is a chronological validation. All methods are trained on the first half of the samples and applied on the second half. We use 60 BCI experiments [6] from 29 subjects where the subjects performed three imaginary movements, namely  X  X ight hand X  (R),  X  X eft hand X  (L) and  X  X oot X  (F) according to the visual cue presented on the screen, except 9 experiments where only two classes were performed. Since we focus on binary classification, all the pairwise combination of the performed classes produced 162 (= 51  X  3 + 9) datasets. Each dataset contains 70 to 600 trials (at median 280) of imaginary movements. All the recordings come from the calibration measurements, i.e. no feedback was presented to the subjects. The signal was recorded from the scalp with multi-channel EEG amplifiers using 32, 64 or 128 channels. The signal was sampled at 1000Hz and down-sampled to 100Hz before the processing.
 The signal is band-pass filtered at 7-30Hz and the interval 500-3500ms after the appearance of visual cue is cut out from the continuous EEG signal as a trial X . The training data is whitened before minimizing Eqs. (5) and (7) because both problems become considerably simpler when  X  P is an identity matrix. For the prediction of test data, coefficients including Eq. (6) are used, where  X  W and  X  w j denote the minimizer of Eqs. (5) and (7) for the whitened data. Note that we did not whitened the training and test data jointly , which could have improved the performance. The regularization constant C for the proposed method is chosen by 5  X  10 cross-validation on the training set. 3.2 Classification performance In Fig. 1, logistic regression (LR) classifiers with the full rank parameterization (Eq. (3); left column) and the rank=2 parameterization (Eq. (6); right column) are compared against CSP based classifiers with 6 filters (top row) and 2 filters (bottom row). Each plot shows the bit-rates achieved by CSP (horizontal) and LR (vertical) for each dataset as a circle. Here the bit-rate (per decision) is defined based on the classification test er-ror p err as the capacity of a binary symmetric channel with the same error probability: Figure 1: Comparison of bit-rates achieved by the CSP based classifiers and the logistic regression (LR) classifiers. The bit-rates achieved by the conventional CSP based classifier and the proposed LR classifier are shown as a circle for each dataset. The proportion of datasets lying above/below the diagonal is shown at top-left/bottom-right corners of each plot, respectively. Only the difference between CSP with 2 filters and rank=2 approximated LR (lower right) is significant based on Fisher sign test at 5% level. 1  X  ventional method for datasets lying above the diagonal. Note that our proposed logistic regression ansatz is significantly better only in the lower right plot.
 Figure 2 shows examples of spatial filter coefficients obtained by CSP (6 filters) and rank=2 parameterized logistic regression. The CSP filters for subject A (see Fig. 2(a)) include hand X  class) of filters corrupted by artifacts, e.g., muscle movements. The CSP filters for the  X  X oot X  class in subject B (see Fig. 2(b)) are corrupted by strong occipital  X  -activity, which might have been weakly correlated to the labels by chance. Note that CSP with 2 and 2(b). On the other hand the filter coefficients obtained by the logistic regression are clearly focused on the area physiologically corresponding to ERD in the motor cortex (see Figs. 2(c) and (d)). 4.1 Relation to CSP Here, we show that at the optimum of Eq. (7) the regression coefficients w 1 and w 2 are generalized eigenvectors of two uncertainty weighted covariance matrices corresponding to two motor imagery classes, which are weighted by the uncertainty of the decision 1  X  P ( y = y | X = X i ) for each sample. Samples that are easily explained by the regression function are weighed low whereas those lying close to the decision boundary or those lying on the wrong side of the boundary are highly weighted. Although, both CSP and the rank=2 approximated logistic regression can be understood as generalized eigenvalue decomposition, the classification-optimized weighting in the logistic regression yields filters that focus on the task related modulation of rhythmic activities more clearly when compared to CSP, as shown in Fig. 2.
 Differentiating Eq. (7) with either w 1 or w 2 , we obtain the following equality which holds at the optimum. respectively. Moreover, Eq. (8) can be rewritten as follows: where we define the uncertainty weighted covariance matrix as: Note that increasing the regularization constant C biases the uncertainty weighted covari-ance matrix to the pooled covariance matrix  X  P ; the regularization only affects the right-generalized eigenvectors of Eqs. (9) and (10), respectively. 4.2 CSP is not optimal When first proposed, CSP was rather a decomposition technique than a classification tech-nique (see [9]). After being introduced to the BCI community by [11], it has proved to be also powerful in classifying imaginary motor movements [3, 6]. However, since it is not op-timized for the classification problem, there are two major drawbacks. Firstly, the selection of  X  X ood X  CSP components is usually done somewhat arbitrarily. A widely used heuristic is to choose several generalized eigenvectors from both ends of the eigenvalue spectrum. How-ever, as in subject B in Fig. 2, it is often observed that filters corresponding to overwhelming strong power come to the top of the spectrum though they are not correlated to the label so strongly. In practice, an experienced investigator can choose good filters by looking at them, however the validity of the selection cannot be assessed because the manual selection cannot be done inside the cross-validation. Secondly, simultaneous diagonalization of co-variance matrices can suffer greatly from a few outlier trials as seen in subject A in Fig. 2. Again, in practice one can inspect the EEG signals to detect outliers, however a manual outlier detection is also a somewhat arbitrary, non-reproducible process, which cannot be validated. In this paper, we have proposed an unified framework for single trial classification of motor-imagery EEG signals. The problem is addressed as a single minimization problem without any prior feature extraction or outlier removal steps. The task is to minimize a logistic regression loss with a regularization term. The regression function is a linear function with respect to the second order statistics of the EEG signal.
 We have tested the proposed method on 162 BCI datasets. By parameterizing the whole regression coefficients directly, we have obtained comparable classification accuracy with CSP based classifiers. By parameterizing the regression coefficients as the difference of two rank-one matrices, improvement against CSP based classifiers was obtained.
 We have shown that in the rank=2 parameterization of the logistic regression function, the optimal filter coefficients has an interpretation as a solution to a generalized eigenvalue problem similarly to CSP. However, the difference is that in the case of logistic regression every sample is weighted according to the importance to the overall classification problem whereas in CSP all the samples have uniform importance.
 The proposed framework provides a basis for various future directions. For example, in-corporating more than two filters will connect the two parameterizations of the regression function shown in this paper and it may allow us to investigate how many filters are suf-ficient for good classification. Since the classifier output is the logit transform of the class probability, it is straightforward to generalize the method to multi-class problems. Also non-stationarities, e.g. caused by a covariate shift (see [16, 17]) in the density P ( X ) from one session to another, could be corrected by adapting the likelihood model.
 Acknowledgments : This research was partially supported by MEXT, Grant-in-Aid for JSPS fellows, 17-11866 and Grant-in-Aid for Scientific Research on Priority Areas, 17022012, by BMBF-grant FKZ 01IBE01A, and by the IST Programme of the European Community, under the PASCAL Network of Excellence, IST-2002-506778. This publication only reflects the authors X  views.
 Figure 2: Examples of spatial filter coefficients obtained by CSP and the rank=2 parame-terized logistic regression. (a) Subject A. Some CSP filters are corrupted by artifacts. (b) Subject B. Some CSP filters are corrupted by strong occipital  X  -activity. (c) Subject A. Logistic regression coefficients are focusing on the physiologically expected  X  X eft hand X  and  X  X ight hand X  areas. (d) Subject B. Logistic regression coefficients are focusing on the  X  X eft hand X  and  X  X oot X  areas. Electrode positions are marked with crosses in every plot. For CSP
