 Large scale of short text records are now prevalent, such as news highlights, scientific paper citations, and posted mes-sages in a discussion forum, which are often stored as set records in (hidden) databases. Many interesting informa-tion retrieval tasks are correspondingly raised on the cor-relation query over these short text records, such as find-ing hot topics over news highlights and searching related scientific papers on a certain topic. However, current rela-tional database management systems (RDBMS) do not di-rectly provide support on set correlation query. Thus, in this paper, we address both the effectiveness and efficiency issues of set correlation query over set records in databases. First, we present a framework of set correlation query inside databases. To our best knowledge, only the Pearson X  X  cor-relation can be implemented to construct token correlations by using RDBMS facilities. Thereby, we propose a novel correlation coefficient to extend Pearson X  X  correlation, and provide a pure-SQL implementation inside databases. We further propose optimal strategies to set up correlation fil-tering threshold, which can greatly reduce the query time. Our theoretical analysis proves that, with a proper setting of filtering threshold, we can improve the query efficiency with a little effectiveness loss. Finally, we conduct exten-sive experiments to show the effectiveness and efficiency of proposed correlation query and optimization strategies. H.2.8 [ Database Management ]: Database Applications Measurement, Performance
Short text records become more and more popular, which are often stored as set records in (hidden) databases. For example, in many news agencies web sites, instead of offer-ing full text documents, often, news highlights are shown on the front page to attract users X  interests. These news short highlights are examples of set records. Given another example, in many scientific paper citation web sites, such as Citeseer and ACM Portal, paper citations are given in the form of short texts. Other examples of short texts as set records include users X  text inputs in the logs of a search engine and reply messages in a discussion forum. Efficient set similarity operators over such set records have been well studied [2, 15, 22].

With the emerging of these short text records, a very use-ful yet interesting query, correlation query , has been intro-duced in many applications to find the correlated records (e.g., with similar topics [36]). For example, in order to detect hot topics among the news highlights from different news agencies (e.g. BBC, CNN, and AOL), we can conduct a correlation query for each highlight (a short set record) to find a number of other highlights in the database that are correlated to the query on similar topics. The set record that has the most number of correlated records can be treated as the hot topic in the current database. Given another exam-ple application on managing scientific literature data, liter-ature records can be collected from different personal web pages, official publication web sites (IEEE or ACM), and conference proceedings. As shown in Table 1, we usually store paper citations as set records in databases. Again, it is promising to conduct a query of correlated records over set records in this database. For example, suppose that a fresh postgraduate student want to search papers related to  X  X ecord linkage X  . A comprehensive correlation query result is expected to return the literatures in  X  X uplicate record detec-tion X  ,  X  X eference reconciliation X  and  X  X ntity resolution X  as well. Motivated by these interesting applications, we study the correlation query over large scale set records in databases.
Unfortunately, current relational database management sys-tems (RDBMS) do not directly provide support on correla-tion query. A novel framework should be studied by using current RDBMS facilities to support the correlation query on set records effectively and efficiently inside databases. We emphasize the importance of implementing correlation query inside RDBMS, since it can be naturally extended to correla-tion join over set records as well. With the implementation in RDBMS, correlation query can benefit in the following aspects. First, we can utilize the optimization facilities pro-vided by RDBMS engine, such as join order selection, to automatically achieve the optimal performance [9, 5, 18]. Second, with the help of RDBMS, we can handle very large Table 1: Set records in a scientific literature database
ID Paper (Set records) 1 Duplicate Record Detection: A Survey; Ahmed 2 Record Linkage: Similarity Measures and Algo-3 A latent dirichlet model for unsupervised entity res-4 Reference reconciliation in complex information size of correlations for real-world vocabularies, for examp le, consider the vocabulary from the Web. Therefore, in this study, we mainly focus on seeking and querying correlations inside databases.
 Contributions. In this paper, motivated by the huge amount of set records stored in databases, we address both the effec-tiveness and efficiency issues in the correlation query over set records in databases. To our best knowledge, this is a first attempt to support correlation query over set records inside databases. Our contributions in this paper are sum-marized as follows:
The rest of this paper is organized as follows: In Section 3, we introduce a query framework with respect to correlations on the set record, and the corresponding implementation in RDBMS. Section 4 presents the definition of token cor-relations, as well as the construction of correlations insi de databases. Section 5 addresses how to answer correlation queries efficiently, and provides theoretical analysis for opti -mization issues. Section 6 demonstrates the performance of our approach in the experiments. Finally, we conclude this paper in Section 7.
Techniques on set similarity operators over set records have been well studied [2, 15, 22], where the matching sim-ilarity is mainly considered between set records. Instead o f set similarity, in this paper, we mainly focus on operators of set correlation. In order to evaluate the correlated set records, most of the correlation query approaches [7, 19, 38] use the Pearson X  X  correlation coefficient [37] to measure cor -relations, which can also be implemented in databases.
Recently, Sahami and Heilman [30] measure the corre-lation between short set records by leveraging web search results to provide greater context for short texts. Thus, the correlation relies heavily on the quantity of search re-sults. Chirita et al. [10] propose the query expansion by us-ing users X  personalized information. Several techniques fo r determining expansion terms from personal documents are introduced, including co-occurrence statistics and extern al thesauri. Theobald et al. [34] also study the query expan-sion in a dynamical and incremental way. A priority queue is used for maintaining result candidates and pruning of can -didates together with probabilistic estimators of candida te scores. Liu et al. [26] utilized the WordNet to formulate the correlation between word tokens in documents. However, these techniques require knowledge outside databases and can hardly be directly applicable by using current RDBMS facilities. In this paper, instead of using the general doma in knowledge outside databases, we are interested in conduct-ing correlation quires inside databases.

The retrieval of small text records as sentence level is also studied in [27, 1, 4, 25, 24]. Li and Croft [25, 24] learn sentence level information patterns from the training data to identify potential answers. Murdock and Croft [28] con-duct the sentence retrieval as translations from the query to the results. A parallel corpus has to be exploited for train-ing the translation model. Fung and Lee [13] also develop the correlations of new words for translation, by consider-ing the co-occurrence of other (seed) words in the sentences . Cao et al. [8] develop a dependency model to incorporate the co-occurrence information in language modeling. Un-fortunately, the proposed techniques with language models are too complicated to be supported by using the current RDBMS facilities, and again, not directly applicable in our correlation query (as well as join) problem inside databases .
Furthermore, learnable similarity metrics on set records have been investigated in recent studies. For example, Jin et al. [20] propose a supervised term weighting scheme by considering the correlation between word frequency and cat-egory information of documents. Bilenko and Mooney [6] compute the comparison similarity vector of two records and classify the vector as similar or not with a similarity value output. Sarawagi et.al. [32] propose an active leaning ap-proach by picking up the most uncertain data which will be labeled manually. These approaches are learning tech-niques, which need training data sets. Different from the learning approach, in this paper, we mainly focus on the ap-proaches that explore correlations inside the dataset and do not require pre-labeled training data.

Hofmann [16, 17] proposes the probabilistic latent seman-tic analysis (PLSA), which addresses the problem of dif-ferent words with a similar meaning or the same concept. However, the probabilistic latent semantic analysis as an ex-tension of latent semantic analysis LSA [12] is a type of dimension reduction techniques. Rather than removing the
Figure 1: Storage scenario of correlation query tokens, our correlation-based query enriches the correlati ons between tokens and finds the correlations between tokens without any class knowledge.
In this work, we consider set records stored in relational databases. This section provides a framework to support correlation query inside databases. Formally, we define the correlation query problem over set records as follows:
Problem 3.1 (Correlation Query). Given a query q (a set record with several tokens as well), the correlation query returns the records r ranked by cor ( q, r ) , where cor is the correlation between q and r .
Due to the sparse features of text data, we extend a verti-cal schema to store set records [5]. The data storage scenari o in query framework is illustrated in Figure 1. Specifically, we segment each record into a set of tokens (e.g. words or q-grams ) and store all the pairs of token tid and record rid in the table records ( tid, rid ). As presented in Section 4, we can compute a correlation weight of each token pair in the database. These token correlations corresponding with the ir weights are represented in the view, correlation ( t i , t where w ij denotes the correlation weight cor ( t i , t j tokens t i and t j .

Similarity matching-based query of short text strings in databases has been studied by [11, 14, 9, 3]. The query is directly performed on records based on matching tokens be-tween the query and records. As shown in Figure 2(a), we can evaluate the similarity matching query in our framework by conducting a Join operation on the query and records ta-bles with query.tid = records.tid , i.e., the similarity match-ing query: The similarity matching query generates all candidate recor ds which have common matching tokens with the query, when the results of T s are grouped by the record rid . For each group of a record, we can compute an aggregation score as the matching similarity, e.g., sim ( q, r ) = | q  X  r | .
To answer correlation queries, we have to consider all the pairs of tokens ( t i , t j ) between the query q and record r . Based on the pairs of tokens with correlations, we can de-termine the correlation of the query q and r . Therefore, as shown in Figure 2(b), we consider the correlations be-tween set records in the query and records directly one by one. Specifically, for each pair of query and record ( q, r ), we have to search the correlation data frequently, since the correlations between tokens in the query and records are represented in the correlation view. In terms of storage in Figure 1, we conduct a Cartesian product on the tables of query and records to generate a large number of results as candidates. Then, token correlations are performed on these pairs of candidates to select the records with highest correlations, i.e., the q  X  r  X  c correlation query order: The correlation query with q  X  r  X  c order considers all the possible combination of token pairs across the query and each record, and then checks whether the correlations be-tween those token pairs exist. Again, after finding all the candidates of query results in T c , the efficient facilities in RDBMS, such as GROUP BY and ORDER BY , can be utilized directly for aggregating and ranking the results.

Moreover, since the data are stored in a relational database , we can automatically optimize the correlation query by uti-lizing the efficient facilities provided by RDBMS, such as join order selection. Note that the results of Cartesian pro d-uct are large in size according to the q  X  r  X  c correlation query order, where most pairs of tokens might not have correlations. Thus, rather than enumerating all the pairs of tokens between the query and records, we select the to-kens with correlations first. In terms of storage in Figure 1, we first perform the Join operation on the table of query and correlation , rather than the connecting on query and records . Since the query size is small, the result of the first join step will be small as well. Then, we query the small size output in the records like the similarity matching ap-proaches. Consequently, the correlation query is rewritten in the q  X  c  X  r order, i.e., first generating all the tokens correlated with the tok ens in the query, and then performing the efficient matching with records based on common (matched) tokens. We use SQL to implement correlation queries, thus, the query optimizer of RDBMS can automatically be used to select the above correct join order based on the join sizes and available in-dexes.
Note that this framework can be naturally extended to correlation join over set records as well. That is, we replace query with set records on attribute A 1 in Figure 1, and records denote the set records on attribute A 2 . Then, the correlation join over A 1 and A 2 can be evaluated, which is out of the scope of this paper. We now define the correlation between two set records. There are many distance functions designed for measuring string similarity, including matching coefficient , Jaccard co-efficient , Euclidean distance , Cosine similarity and so on [31]. These distance functions consider pairs of matched tokens between two strings, where the relationship between matche d tokens are one-to-one. In our correlation query case, instea d of matched token pairs, we study correlated token pairs be-tween query q and record r . Note that the correlations be-tween tokens are many-to-many relationship, that is, one token may be correlated with several tokens in the other set record.

Given a query q and a record r , C ( q, r ) is used to represent the set of all the pairs of tokens between q and r whose token correlations are greater than 0.
 where cor ( t i , t j ) denotes the token correlation between t and t j , i.e., w ij in Figure 1. In the following Section 4, we introduce the construction of such token correlations, i.e ., ei-ther the Pearson X  X  correlation in formula 3 or our proposed inverted correlation in formula 9. We evaluate both token correlation measures through extensive experiments and re-port the results Section 6.

Consequently, we aggregate all the token correlations cor ( t ) in the set of C ( q, r ) as the record correlation As mentioned, such record correlation score can be naturall y computed by an aggregation query provided by RDBMS, once token correlations cor ( t i , t j ) are obtained.
As presented in the query framework in Section 3.1, we store the query in the table query ( qid ). The following algo-rithm aggregates all the correlation weights of each record . The returned results of rid are ordered in descending or-der according to the scores. Though the algorithm is im-plemented with q  X  c  X  r order, we can rely on the query optimizer of RDBMS to select the optimal join order, most probably the q  X  c  X  r order.
 SELECT SUM(T.Cweight) AS score, D.rid
FROM records AS D, WHERE D.tid = T.Ctid GROUP BY D.rid ORDER BY score DESC
In order to evaluate and compare the correlation query and similarity matching query, we also present the imple-mentation of the inverted index-based similarity matching approach. The following algorithm counts all the matching tokens between the query q and the record r as the similarity score and rank the records in the descending order. SELECT COUNT(*) AS score, r.rid FROM records AS r, query AS q WHERE r.tid = q.tid GROUP BY r.rid
ORDER BY score DESC
As the framework illustrated in Figure 1 and 2, the key issue of supporting correlation query is to construct token correlations inside databases. Many sophisticated work on measuring token correlations have been proposed in informa -tion retrieval literature. Unfortunately, due to the limit ation of facilities provided by current RDBMS, most of such ad-vanced techniques cannot be supported by databases. To our best knowledge, only the Pearson X  X  correlation coeffi-cient [37] can be implemented inside databases. Therefore, in this section, we extend the Pearson X  X  correlation, namely inverted correlation, and provide corresponding database implementation.
Assume that we have segmented an unstructured set record into a set of tokens, e.g. words or q-grams . The Pearson X  X  correlation coefficient [37] can be used to compute the cor-relations between tokens t i and t j . cor ( t i , t j ) = Pr( t i , t j )  X  Pr( t i ) Pr( t j ) where Pr( t i ) is the support of t i , i.e. Pr( t i ) = f ( t the number of records containing token t i and N is the total number of records. In other words, Pr( t i ) also denotes the probability that token t i appears in a record in the database. However, according to the information property of texts, a token that appears frequently in the records in a dataset, i.e., high f ( t i ) value, might not be important in evaluating the correlations of records [33, 29]. For example, the word token  X  X nd X  may appear frequently in set records. However, the correlation between token t i and token  X  X nd X  does not help greatly in finding correlated records.

Thus, to evaluate the correlations between tokens, we first introduce the concept of inverted probability which ha s been successfully adopted in the inverse document frequency (idf) [33, 29]. The idf is based on the essential intuition that a token appears frequently in different documents (records) is not a good discriminator and should be associated with a low feature weight; while a token with a low document fre-quency means that it is more relevant to those documents where it appears. The basic formula of idf is: where N denotes the total number of documents (records), and f ( t i ) is the number of documents (records) that contain token t i . Similar to the case of documents in idf , we replace the documents by our set records in this study.

Specifically, motivated by successes of the inverted doc-ument frequency in retrieval of correlated text documents, we propose the inverted probability of a token t i , which is defined as: where P i ( t i ) denotes the inverted probability of token t higher frequency f ( t i ) of token t i appearing in records in-dicates a lower inverted probability value P i ( t i ). Moreover, considering the case that two tokens t i and t j appear to-gether, we define the joint inverted probability of t i and t as: where f ( t i , t j ) denotes the number of records where both token t i and t j appear, i.e. the joint frequency of ( t
Note that we can model the correlation relationship of to-ken co-occurrence using a conditional probability. Given t wo tokens t i and t j , the conditional probability of t j appearing in a record when t i has already appeared in the same record is: where Pr( t i , t j ) denotes the probability of t i and t ing together in a record.

Now, similar to the conditional probability in Equation 7, we can compute the token correlations according to the inverted probability with token frequency. Note that the conditional probability describes an asymmetric relations hip from t i to t j . With the inverted probability, we define the asymmetric correlation from token t i to t j as: where we have Cor ( t j | t i ) 6 = Cor ( t i | t j ). However, in the real world, we prefer to describing the correlations betwee n two tokens in a symmetric way, i.e. if t i is correlated with t then t j is also correlated with t i . Therefore, we define the token correlations formally in a symmetric style by consid-ering both Cor ( t j | t i ) and Cor ( t i | t j ).
Definition 4.1 (Token Inverted Correlation). Given two tokens t i and t j , the token correlation between t i can be represented by: where f ( t i ) and f ( t j ) denote the number of records contain-ing t i and t j respectively, f ( t i , t j ) is the number of records with both t i and t j , and N is the total number of records in the database.

The semantic meaning of the correlation between tokens t i and t j is described as the probabilistic relationship of these two tokens appearing together in the same records within the whole database. Specifically, the larger the value of f ( t is, the higher the correlation between t i and t j . Thus, two tokens appearing together frequently have high correlation . On the other hand, for a fixed f ( t i , t j ) observation, if the frequency values f ( t i ) and f ( t j ) of tokens t i and t the correlation is low. Thereby, if two tokens t i and t j appear together in the same records, i.e. f ( t i , t j ) = f ( t f ( t j ), we have the correlation cor ( t i , t j ) = 1. The intuition can also be extended to the correlation between the token t and itself, i.e. cor ( t i , t i ) = 1. If two tokens t i never appear together in the same records, i.e. f ( t i , t then there is no correlation between these two tokens, i.e., cor ( t i , t j ) = 0. In fact, according to the definition of token correlation, we have the following property:
Property 4.1. If the correlation cor ( t i , t j ) between two tokens t i and t j exists, we have the correlation value in the range of 0 &lt; cor ( t i , t j )  X  1 .
 Proof. At first, in the real application, we have f ( t ) N , i.e. N f ( t ) &gt; 1. If the correlation between two tokens t t exists, then these two tokens should appear together in some records, i.e., 0 &lt; f ( t i , t j ) N . Since each log N fied. Moreover, intuitively, t i and t j appearing together in the records implicates that t i must exist in these records. Therefore, we have f ( t i , t j )  X  f ( t i ) and f ( t i It is the same for the Cor ( t i | t j )  X  1. Consequently, we also have cor ( t i , t j )  X  1. To sum up the above arguments, the correlation with 0 &lt; cor ( t i , t j )  X  1 is satisfied.
We start from the dataset that has already been repre-sented in the table records ( tid, rid ). First, we count the times of each token pair appearing in the records, i.e., f ( t in equation 9, and store the generated tuples ( t i , t j in the correlation view with schema correlation ( tid 1 , tid Note that f ( t i ) of each token is also computed by this step and represented as ( t i , t i , f ( t i )). Then, we count all the num-ber of records in the table of records , namely @N . Next, we perform the correlation computation according to Equa-tion 9. Consider a tuple of correlation c3 in the algorithm with (tid1, tid2, weight) . Let c1.weight be the fre-quency of token c3.tid1 and c2.weight be the frequency of token c3.tid2 , according to the definition in Equation 9 the correlation weight of c3 can be computed as: Finally, we update the correlation weight in correlation . The SQL statement for token correlation construction is de-scribed as follows.
 INSERT INTO correlation DECLARE @N INT SET @N = (SELECT COUNT(DISTINCT rid) UPDATE correlation SET weight = temp.w
FROM
WHERE tid1 = temp.id1 AND tid2 = temp.id2
According to the correlation query framework, the num-ber of token correlations affects performance of correlatio n queries. Thus, we try to improve the query efficiency by reducing the size of token correlations. Specifically, we in -vestigate the interaction between the number of token corre -lations (correlation count) and the importance of token cor -relations (correlation weight). Intuitively, two tokens w ith low correlation weights have small probability of appear-ing together, in other words, they are less important in our correlation query. In fact, noisy data often exist in real applications, and some false correlations with low weights might be generated by isolated noise. Therefore, we can set a minimum correlation threshold  X  to filter out those token correlations with low correlation weights. If the threshol d  X  is small, noisy correlations are taken into account in rank-ing the results. Consequently, the noisy correlations affect the results largely. On the other hand, with the increase of threshold  X  , only those correlations with high weights are reserved, which are close to 1. Thereby, the difference be-tween the correlation query and similarity matching query also becomes small, and two approaches tend to get similar query results. From the above discussion, we can find out that setting a proper filtering threshold is essential to im-prove both the efficiency and effectiveness of a correlation query.
A good filter should remove non-important correlations and reserve the important correlations as many as possible. Here, the importance of correlations is represented by the correlation weight. In the rest of this section, we provide an estimated range of correlations as a guideline of threshold selection, and prove that we can use less number of cor-relations to represent more important correlations in that correlation range.

In order to evaluate the balance of reserving more im-portant correlation information and reducing more number of correlations, we compare the distributions of correlati ons on correlation numbers and correlation weights. We show the statistics of correlation distributions of RCV1 and NSF datasets in Figure 3. For a correlation weight x , the fre-quency distribution bar f ( x ) denotes the normalized num-ber of correlations with the weight x , i.e., correlation weight distributions. From Figure 3, we find that the correlations are not distributed uniformly in the whole range of correla-tion value (0 , 1]. A curve of the nominal distribution is also plotted to compare with the frequency distribution, and we observe similar patterns in both two datasets.
 Based on the frequency distribution observation on the RCV1 and NSF datasets, we model the correlation distri-bution as the nominal distribution . Let random variable X be the token correlation weight, we consider the following two distributions.

Distribution 5.1. N c (  X  c ,  X  2 c ) : The distribution of X in terms of the number of correlations. Each correlation is treated as a unit of statistic sample. The probability den-sity function f c ( x ) describes the probability distribution of X on the number of correlations. The cumulative distribu-tion function F c ( x ) denotes the probability of X less than x , P ( X  X  x ) , in terms of correlation count. The estimation of  X  c and  X  2 c can be described as: where n is the total number of correlations. The probability density function of N c represents the distribution of X on correlation counts, e.g. how many number of correlations with the correlation weight in the range of 0 . 1 &lt; X &lt; 0 . 2 .
Distribution 5.2. N s (  X  s ,  X  2 s ) : The distribution of X in terms of the weight of correlations. Each unit of correlatio n weight is treated as a statistic sample, while each correlat ion with weight x i is treated as a number of x i statistic samples. The probability density function f s ( x ) describes the proba-bility distribution of X on the weight of correlations. The cumulative distribution function F s ( x ) denotes the probabil-ity of X less than x , P ( X  X  x ) , in terms of correlation weight. The estimation of  X  s and  X  2 s is given by where P n i =1 x i denotes the total units of correlation weights. The variable X is the same with the distribution 1, but the distributions of the variable are different. In the distribu-tion 1, one correlation with weight x i is counted as 1 time, however, for distribution 2, we treat the correlation as x units and count one correlation with weight x i for x i times. The probability density function of distribution 2 represe nts the distribution of X on correlation weights, e.g. how many weight units of correlations with the correlation weights i n the range of 0.1 &lt; X &lt; 0.2. Recall that our record correlation function also aggregates the correlation weights.

Now we discuss the selection of the minimum threshold  X  , in order to efficiently and effectively achieve the optimal query results. First, we present the relationship between th e estimate values  X  c and  X  s of distribution 1 N c and distribu-tion 2 N s . Then, for a given value x between  X  c and  X  s can exploit the bounds of the cumulative distribution func-tion F c ( x ) and F s ( x ). Finally, we prove that by setting a certain filtering threshold, we can remove more less impor-tant correlations without losing much query effectiveness.
Theorem 5.1. Let  X  c be the estimate value of distribu-tion 1 N c and  X  s be the estimate value of distribution 2 N and then we have  X  c  X   X  s .
Proof. According to the definition of correlation, all the correlation weights have 0 &lt; x i &lt; 1. Therefore, i.e. we have proved  X  c  X   X  s .

Theorem 5.2. Let F c ( x ) be the cumulative distribution function of distribution N c , F s ( x ) be the cumulative distri-bution function of distribution N s . If the value x  X  [  X  and then we have F s ( x )  X  0 . 5  X  F c ( x )
Proof. If a variable X is normally distributed with mean  X  and variance  X  2 , the definition of cumulative distribution function F ( x ) can be described as: According to the property of cumulative distribution func-tion, we have F (  X  ) = 0 . 5; for x  X   X  , then F ( x )  X  0 . 5; and vice versa. Consider the value x  X  [  X  c ,  X  s ], in the distribu-tion N c , we have x  X   X  c thus F c ( x )  X  0 . 5; and also in the distribution N s , x  X   X  s indicates F s ( x )  X  0 . 5.
Theorem 5.3. By setting the minimum threshold  X  of correlation weight in the range of  X  c  X   X   X   X  s , we can use less than 50% number of correlations to represent more than 50% correlation weights (importance).

Proof. Recall that F c ( x ) denotes the proportion of the number of correlations with weights less than x , while F denotes the proportion of the total weights (importance) of correlations with weights less than x . For a certain thresh-old  X  , the value of cumulative distribution function F (  X  ) indicates the proportion that are filtered out and (1  X  F (  X  )) represents the reserved parts. According to Theorem 5.2, by setting the minimum threshold  X  of correlation weights in the range of  X  c  X   X   X   X  s , the lower bound of the number of correlations that are filtered out is 50%; and the upper bound of the total weight of correlations that are filtered ou t is also 50%.

Therefore, by setting a minimum correlation threshold  X   X  [  X  c ,  X  s ], we can remove more than 50% of less important correlations, but lose less than 50% correlation weights. I n Figure 4, we present an example of the cumulative distri-bution function F ( x ). The  X  c of N c is about 0.5 and the  X  s of N s is about 0.6. In the range of x  X  [0 . 5 , 0 . 6], we have F c ( x )  X  50% and F s ( x )  X  50%. With a threshold of  X  (0 . 5  X   X   X  0 . 6), for example  X  = 0 . 5, we can filter out about 50% of correlations but only lose 30% of total correla-tion weights. In other words, by the filter, we reduce the size of correlations largely but with little loss on the correlat ion weights.

So far, we have proved that by using a filter with a thresh-old  X  around the range of [  X  c ,  X  s ], we can remove more correlations (counts) and lose less correlation importanc e (weights). In practice, given a set records database, we can compute token correlation distribution first, and then set u p the filtering threshold according to the above derived opti-mal threshold range.
The following algorithm estimates the parameters of  X  c and  X  s . The first query returns the estimation of  X  c accord-ing to the definition in equation 12, and the second query result is the estimation of  X  s according to the definition in equation 14. Note that the correlation weight between to-ken t i to itself is constant, i.e. cor ( t i , t i ) = 1. Therefore, we do not take those tuples of the self-correlations into ac-count and set a condition of tid1 &lt;&gt; tid2 in the algorithm. Finally, we can implement the filter with threshold  X  and re-move all the correlations with weights less than the specifie d threshold  X  .
 SELECT SUM(weight) / COUNT(*) FROM correlation WHERE tid1 &lt;&gt; tid2 SELECT SUM(weight*weight) / SUM(weight) FROM correlation WHERE tid1 &lt;&gt; tid2 DELETE FROM correlation
WHERE weight &lt; $\eta$
We now present our extensive experimental evaluation, in terms of both effectiveness and efficiency. We compare the correlation query with the similarity matching query. Since the edit operation based approaches like edit distance
Figure 5: Effectiveness on RCV1 can only capture limited similarity and fail in many cases such as various word orders [14], we adopt the vector space based approach as the similarity matching technique, i.e., the matching coefficient sim ( q, r ) = | q  X  r | of text [31]. We also compare our inverted correlation in formula 9 with the Pearson X  X  correlation coefficient [37] in formula 3. The experiments are conducted on a machine with Intel Core 2 CPU (2.13 GHz) and 2 GB of memory. RDBMS uses SQL Server 2005.
 Datasets. We run the experiments on Reuters Corpus Vol-ume I (RCV1) [23], which is also a widely used benchmark dataset with category labels in text retrieval area. We ab-stract m ( m = 10 in this experiment) words with the highest tf*idf value from each article as the summary, and merge these m words to an unstructured set record. The category label of each article is also utilized as the label of corresp ond-ing record. The second dataset we used in the effectiveness experiments is the 20 Newsgroups (20NG) [21]. The 20NG data are also text entries with data category labels. Simi-lar to the RCV1 , we use the 20NG dataset to evaluate the effectiveness in the same way 1 .
 Evaluation Criteria. The evaluation criteria address two aspects in the experiments, i.e. the effectiveness and the effi-ciency. Specifically, we select 100 records from the dataset as queries. For the time performance evaluation, we conduct all the queries and compute the response time as runtime costs. In addition, we also report the number of token correlations left after a minimum correlation threshold applied, since i t is directly related to the running time. For the effectivenes s, we evaluate the accuracy in the top-k query results. accuracy = number of results with the same label of q The accuracy value denotes the correctness of query results, i.e., the query and the results belong to the same labeled cat-egory. Higher accuracy of a query means better effective-ness. Here, we do not adopt the f-measure [35] with recall to evaluate the completeness of the top-k results. It is mean-ingful to tell the high correlation between  X  X BA X  and  X  X as-ketball X  , but unreasonable (in real applications) to achieve a complete answer set in top-k results, i.e., to enumerate and return all possible records that are correlated with the quer y  X  X asketball X  . Therefore, rather than evaluating the recall, we mainly concern the accuracy of correlation query.
Similar results on 20NG are not reported in some experi-ments, due to the limitation of space.
The first experiment is focused on the effectiveness com-parison of correlation query and similarity matching-based query. In this experiment, we do not conduct the filter on token correlations. The program runs on 50,000 records in the RCV 1 dataset and 10,000 records in the 20 N G dataset. Figure 5 and 6 shows the accuracy of top-k results by three different approaches, including the similarity matching, t he inverted correlation proposed in this paper, and the Pear-son X  X  correlation coefficient. As shown in the results, these three approaches achieve high accuracy in the top-20 re-sults. These highly correlated result records have consid-erable common tokens with each other, thus the similar-ity matching-based query can also identify the correlation relationship. However, when larger sizes of query results are needed, for example top-100 or top-200, the relation-ships between the query and records can hardly be detected by only using the overlaps of matching tokens. Therefore, with the increase of result sizes, the accuracy of similarit y matching-based query drops quickly, while our correlation query achieves comparatively high accuracy. Most impor-tantly, our token inverted correlation shows a higher accu-racy comparing with the Pearson X  X  correlation coefficient, which also verifies the derivation in Section 4.1. To summa-rize, the experiment results demonstrate the effectiveness o f our correlation queries.
In this experiment, we study the improvement of effec-tiveness by applying the correlation filter. Recall that nois y correlations may affect the query results. The filtering strat -egy is to set a minimum correlation weight threshold  X  . We evaluate the effect of correlation filter by reporting the re-sults of accuracy (Figure 7).

First, we can improve the effectiveness by introducing a filter in the correlation query. As shown in the Figure 7, the accuracy of query results improves by using the minimum threshold filter  X  = 0 . 4. By setting this threshold, we filter out the correlations with weights less than 0.4, which are less important (such as noisy data). On the other hand, if the threshold is set too large, the accuracy drops as shown in Figure 7 with  X  = 0 . 6. As we increase the threshold  X  , more correlations are filtered out, and the query can only rely on the matching tokens and few correlations still left. Thus, the results of correlation query are closer to the similarity matching ones, when the threshold is brought near 1.
To estimate the filtering threshold, we observe the dis-tribution of correlations at first, and compute the estima-tion value of  X  c = 0 . 505 and  X  s = 0 . 566 according to the definition in Section 4.2. According to Lemma 5.3, by set-Figure 8: Scalability on effectiveness ting  X  c  X   X   X   X  s , we can reduce more than 50% num-ber of correlations while reserve more than 50% correlation information. Therefore, in order to improve the efficiency without losing too much accuracy, the threshold  X  is pre-ferred around the range of [  X  c ,  X  s ] to guarantee the tradeoff between the correlation count (size) and correlation weigh t (importance). The results in Figure 9 also confirm that by setting  X  = 0 . 55  X  [0 . 505 , 0 . 566], we can reduce the number of correlations significantly and with a little loss on effec-tiveness (Figure 8) compared with  X  = 0 . 4 or 0 . 5.
In fact, the estimation of  X  c and  X  s is a guideline strategy of choosing threshold. As the threshold increases, the valu es of F c ( x ) and F s ( x ) become closer in the cumulative distri-bution function. In other words, the difference between the number of correlations ( N c ) and the importance of corre-lations ( N s ) is smaller. Since we could no longer keep all highly important correlations by a smaller number of cor-relations, the accuracy drops quickly as shown in Figure 7 with  X  = 0 . 6.
This experiment demonstrates the scalability of our ap-proach in different data sizes. We perform a number of 100 queries on the data sizes from 10,000 to 50,000 respectively. For the effectiveness, we observe the number of results re-turned in different data scales and the accuracy of the top-200 results for each data size. For the efficiency, we study the correlation size under several filters and compare the runtime costs with the similarity matching-based query.
In Figure 8, we compare the effectiveness of different ap-proaches. Our correlation query achieves a higher accuracy under different data sizes. Moreover, the results of corre-lation query with different filters are quite similar. By in-creasing the threshold, the accuracy arises at  X  = 0 . 4 and  X  = 0 . 5, then drops back to the original level when  X  = 0 . 6, but still without losing too much accuracy.

In Figure 10, we illustrate the time performance of our approach. The runtime costs largely depend on the size of correlations in the computation. As shown in Figure 9, the number of correlations increases approximately in linea r with the increasing of dataset size under different threshol ds. By applying a filter, we can reduce the runtime costs largely. When the threshold  X  = 0 . 6, the time costs of correlation query are even quite similar to the similarity matching-based approach, under all of the data sizes. Meanwhile, the effec-tiveness of correlation query (Figure 8) is much higher than the similarity matching query.

We also evaluate the scalability on the NSF dataset. Since the records in the NSF dataset are not labeled with cate-gories, we only conduct the experiments in terms of time performance. The results are presented in Figure 11. We get similar curves of time costs as the results shown in Fig-ure 8 on RCV1 dataset. The time costs increase linearly as the increase of data size. When the threshold is set to be  X  = 0 . 6, the time costs of correlation query are compa-rable to the similarity matching-based query. On the other hand, as the same results as described in the previous ex-periments, the correlation query is more effective than the similarity matching approach.
In this paper, motivated by the significance of supporting correlation query over set record in databases, we propose a novel query framework for set correlation query by using RDBMS facilities. To our best knowledge, this is the first work on supporting set correlation query inside databases, and only the Pearson X  X  correlation can be implemented to construct token correlations by using current RDBMS fa-cilities. Thereby, we study a novel correlation coefficient to extend Pearson X  X  correlation, and provide a pure-SQL implementation inside databases. We emphasize this pure-SQL implementation of set correlation query inside RDBMS, which can be naturally extended to set correlation join over set records as well. Moreover, our theoretical analysis prov es that, with a proper setting of filtering on token correlation s, we can improve the query efficiency with a little effectiveness loss. Finally, our experiments demonstrate the superiority of our approach in terms of both effectiveness and efficiency. This work is supported by HONG KONG RGC GRF grant under project no. 611608. [1] J. Allan, C. Wade, and A. Bolivar. Retrieval and [2] A. Arasu, V. Ganti, and R. Kaushik. Efficient exact [3] A. Arasu, V. Ganti, and R. Kaushik. Efficient exact [4] N. Balasubramanian, J. Allan, and W. B. Croft. A [5] J. L. Beckmann, A. Halverson, R. Krishnamurthy, and [6] M. Bilenko and R. J. Mooney. Adaptive duplicate [7] S. Brin, R. Motwani, and C. Silverstein. Beyond [8] G. Cao, J.-Y. Nie, and J. Bai. Integrating word [9] S. Chaudhuri, V. Ganti, and R. Kaushik. A primitive [10] P.-A. Chirita, C. S. Firan, and W. Nejdl. Personalized [11] W. W. Cohen. Integration of heterogeneous databases [12] S. C. Deerwester, S. T. Dumais, T. K. Landauer, [13] P. Fung and L. Y. Yee. An ir approach for translating [14] L. Gravano, P. G. Ipeirotis, N. Koudas, and [15] M. Hadjieleftheriou, X. Yu, N. Koudas, and [16] T. Hofmann. Probabilistic latent semantic analysis. In [17] T. Hofmann. Probabilistic latent semantic indexing. In [18] A. Jain, A. Doan, and L. Gravano. SQL queries over [19] C. Jermaine. The computational complexity of high [20] R. Jin, J. Y. Chai, and L. Si. Learn to weight terms in [21] K. Lang. Newsweeder: Learning to filter netnews. In [22] H. Lee, R. T. Ng, and K. Shim. Power-law based [23] D. D. Lewis, Y. Yang, T. G. Rose, and F. Li. Rcv1: A [24] X. Li and W. B. Croft. Novelty detection based on [25] X. Li and W. B. Croft. Improving novelty detection [26] S. Liu, F. Liu, C. Yu, and W. Meng. An effective [27] D. Metzler, S. T. Dumais, and C. Meek. Similarity [28] V. Murdock and W. B. Croft. A translation model for [29] S. Robertson. Understanding inverse document [30] M. Sahami and T. D. Heilman. A web-based kernel [31] G. Salton. Automatic Text Processing: The [32] S. Sarawagi and A. Bhamidipaty. Interactive [33] K. Sparck Jones. Index term weighting. Information [34] M. Theobald, R. Schenkel, and G. Weikum. Efficient [35] C. J. Van Rijsbergen. Information Retrieval . [36] R. W. White and J. M. Jose. A study of topic [37] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar. [38] H. Xiong, S. Shekhar, P.-N. Tan, and V. Kumar.
