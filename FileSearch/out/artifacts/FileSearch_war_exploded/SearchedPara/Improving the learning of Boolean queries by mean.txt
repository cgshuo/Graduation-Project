 1. Introduction
Information retrieval (IR) may be defined, in general, as the problem of the selection of documentary information from storage in response to search questions provided by a user ( Baeza-Yates &amp; Ribeiro-Neto, 1999;Salton&amp;McGill,1983 ).Informationretrievalsystems(IRSs)areakindofinformationsystemsthatdeal withdatabasescomposedofinformationitems X  X ocumentsthatmayconsistoftextual,pictorialorvocalinfor-mation X  X nd process user queries trying to allow the user to access to relevant information in anappropriate timeinterval.Nowadays,thedevelopmentofthe WWW hasincreasedtheinterestonthestudyofIRSs.

Many IRSs still consider the Boolean IR model ( Van Rijsbergen, 1979 ), based on the use of Boolean queries where the query terms are joined by the logical operators AND and OR. This way, the user needs to have aclear knowledge on how to connect the query terms together using the Boolean operators in order to build a query defining his information needs. The difficulty found by nonexpert users to formulate these kindsofqueriessometimesmakesnecessarythedesignofautomatic methods forthis task.Theparadigmof Inductive Query by Example (IQBE) ( Chen, Shankaranarayanan, She, &amp; lyer, 1998 ), where a query describing the information contents of a set of documents provided by a user is automatically derived, can be useful to assist the user in the query formulation process. Focusing on the Boolean IR model, the most known existing approach is that of Smith and Smith (1997) , which is based on a kind of evolu-tionary algorithm (EA) ( Ba  X  ck, Fogel, &amp; Michalewicz, 1997 ), genetic programming (GP) ( Koza, 1992 ). guided by a weighted fitness function combining two retrieval accuracy criteria, precision and recall. The main characteristic of this approach is that it provides a single query in each run.

GiventheretrievalperformanceofanIRSisusuallymeasuredintermsofthesetwocriteria,precisionand recall( VanRijsbergen,1979 ),theoptimizationofanyofitscomponents,andconcretelytheautomaticlearn-ing of Boolean queries, is thus a clear example of a multiobjective problem. EAs have been commonly used for IQBE purposes and their application in the area has been usually based on combining both criteria in a singlescalarfitnessfunctionbymeansofaweightingscheme( Cordo  X  n,Herrera-Viedma,Lo  X  pez-Pujalte,etal., 2003 ).However,thereisakindofEAspeciallydesignedformultiobjectiveproblems, multiobjective evolution-ary algorithms , which are able to obtain different nondominated solutions to the problem in a single run ( Coello,VanVeldhuizen,&amp;Lamant,2002;Deb,2001 ).InIR,specificallyintheIQBEparadigm,theywould allow us to derive a number of queries with a different precision X  X ecall trade-off in a single run of the IQBE algorithm, andin such a way to improve the aid possibilities to the users in the formulation of their queries. In this paper, we present a new evolutionary tool to learn Boolean queries that improves the Smith and Smith s (1997) approach, called multiobjective IQBE EA. We define it by extending the Smith and Smith s approach incorporating Pareto-based evolutionary multiobjective components into GP. To do so, we con-sideroneofthemostknownandwellperformingPareto-basedmultiobjectiveEAs,SPEA( Zitzler&amp;Thiele, 1999 ). The main feature of this EA is the maintenance of the elitism concept in a multiobjective evolution-ary algorithm. This improves the performance of our multiobjective GP algorithm. In order to represent a real-world text retrieval IQBE environment where a user provides a relatively small number of relevant and irrelevantdocuments,theexperimentaltestbedwillbebasedontwoofthemostknownsmallsizeIRbench-marks, the Cranfield and CACM document collections ( Baeza-Yates &amp; Ribeiro-Neto, 1999; Salton &amp; McGill, 1983 ). With our proposal we improve and increase the user assistance possibilities in the formula-tion of queries by means of evolutionary computation tools.

With this aim, this contribution is structured as follows. Section 2 is devoted to introduce the prelimi-naries, including the basis of Boolean IRSs, the definition of both precision and recall criteria, the main aspects of IQBE techniques, a review on EAs and on their application to IR, tasks, arid finally, the main aspects of multiobjective EAs. Section 3 is devoted to introduce the main aspects of the Smith and Smith s proposal and to extend the latter algorithm to deal with the multiobjective problem of simultaneously optimizing both precision and recall by means of the SPEA Pareto-based approach while the experiments developed to test the new proposal and the results obtained are shown in Sections 4 and 5 , respectively.
Finally, several concluding remarks are pointed out in Section 6 . 2. Preliminaries 2.1. Boolean IRS An IRS is basically constituted by three main components, as shown in Fig. 1 .

The documentary data base . This component stores the documents and the representation of their infor-mationcontents.Itisassociatedwiththe indexer module ,whichautomaticallygeneratesarepresentationfor each document by extractingthe document contents. Textual document representation is typically based on indexterms(thatcanbeeithersingletermsorsequences)whicharethecontentidentifiers ofthedocuments.
Inthe Boolean retrieval model, the indexer moduleperformsabinary indexinginthe sensethatatermin all).Let D beasetofdocumentsand T beasetofuniqueandsignificanttermsexistinginthem.Theindexer module of the Boolean IRS defines an indexing function: F : D  X  T ! {0,1}, where F ( d , t ) takes value 1 if term t appears in document d and 0 otherwise.

The query subsystem . It allows the users to formulate their queries and presents the relevant documents retrieved by the system to them. To do so, it includes a query language , that collects the rules to generate legitimate queries and procedures to select the relevant documents.

Boolean queries are expressed using a query language that is based on query terms and permits combi-nations of simple user requirements with logical operators AND, OR and NOT ( Van Rijsbergen, 1979 ).
The result obtained from the processing of a query is a set of documents that totally match with it, i.e., only two possibilities are considered for each document: to be or not to be relevant for the user s needs, repre-sented by his query.

The matching mechanism . It evaluates the degree to which the document representations satisfy the requirements expressed in the query, the retrieval status value (RSV), and retrieves those documents that are judged to be relevant to it.

As said, the RSV has only two values associated, 0 and 1, in Boolean IRSs. In order to match a query, a document has to fulfill it completely, i.e., it has to include the positive query terms specified in the search expression and not to include those that have been specifically given in a negative way. In order to obtain the set of relevant documents for a query, it is represented as a parse tree and is evaluated from the leaves to the root. Each leaf is associated to the set of documents including (or not including) the corresponding (negative) query term. Then, the retrieved document sets in the inner nodes are computed by applying set arithmetic(withtheANDoperatorbeingthesetintersectionandtheORoperatorstandingforthesetunion). The final set of retrieved documents is that associated to the root when finishing the evaluation of the tree. 2.2. Evaluation criteria of IRSs
There are several ways to measure the quality of an IRS, such as the system efficiency and effectiveness, and several subjective aspects related to the user satisfaction (see, for example, Baeza-Yates &amp; Ribeiro-Neto, 1999, Chapter 3 ). Traditionally, the retrieval effectiveness X  X sually based on the document relevance with respect to the user s needs X  X s the most considered. There are different criteria to measure this aspect, with the precision arid the recall being the most used.

Precision is the rate between the relevant documents retrieved by the IRS in response to a query and the total number of documents retrieved, whilst recall is the rate between the relevant documents retrieved and the total number of relevant documents to the query existing in the data base ( Van Rijsbergen, 1979 ). The mathematical expression of each of them is shown as follows: with r d 2 {0,1} being the relevance of document d for the user and f d 2 {0,1} being the retrieval of docu-ment d in the processing of the current query. Notice that both measures are defined in [0,1], with 1 being the optimal value.

We should also notice that the only way to know all the relevant documents for a query existing in a documentary base (needed to compute the recall measure) is to evaluate all of them one by one. Due to this and to the relevance subjectivity, there are several classical documentary test collections available, each of them with a set of queries with known relevance judgments, that can be used to test the different new tion, we will deal with the well-known Cranfield and CACM collections.

Assaid,uptoourknowledge,allthepreviousapplicationsofmachinelearningtechniquestoanyoftheIRS componentstryingtooptimizebothcriteriahaveconsideredaweightedcombinationofthesaidtwocriteria. 2.3. The IQBE paradigm and its application to IR
IQBE was proposed in Chen et al. (1998) as  X  X  X  process in which searchers provide sample documents (examples) and the algorithms induce (or learn) the key concepts in order to find other relevant docu-ments X  X . This way, IQBE is a process for assisting the users in the query formulation process performed by machine learning methods. It works by taking a set of relevant (and optionally, non relevant docu-ments) provided by a user X  X hat can be obtained from a preliminary query or from a browsing process in the documentary base X  X nd applying an off-line learning process to automatically generate a query describing the user s needs (as represented by the document set provided by him). The obtained query can then be run in other IRSs to obtain more relevant documents. This way, there is no need that the user interacts with the process as in other query refinement techniques such as relevance feedback ( Salton &amp; McGill, 1983 ).

There have been designed several IQBE algorithms for the different existing IR models. As said, Smith and Smith (1997) proposed the GP algorithm to derive Boolean queries that will be considered in this paper. On the other hand, all of the machine learning methods considered in Chen et al. s (1998) paper (regression trees, genetic algorithms and simulated annealing) dealt with the vector space model ( Salton &amp; McGill, 1983 ). Moreover, there are several approaches for the derivation of weighted Boolean queries for fuzzy IRSs ( Bordogna, Carrara, &amp; Pasi, 1995 ), such as the GP algorithm of Kraft, Petry, Buckles, and Sadasivan (1997) , the niching GA-P method ( Cordo  X  n, Moya, &amp; Zarco, 2000 ) and the simulated annealing-GP hybrid ( Cordo  X  n, Moya, &amp; Zarco, 2002 ). For descriptions of some of the previous techniques based on EAs, the interested reader can refer to Cordo  X  n, Herrera-Viedma, Lo  X  pez-Pujalte, et al. (2003) . 2.4. EAs and their application to IR
Evolutionary computation ( Ba  X  ck et al., 1997 ) uses computational models of evolutionary processes as key elements in the design and implementation of computer-based problem solving systems. There is a variety of evolutionary computational models that have been proposed and studied, which are referred as EAs.
There have been four well-defined EAs which have served as the basis for much of the activity in the field: genetic algorithms (GAs) ( Michalewicz, 1996 ), evolution strategies ( Schwefel, 1995 ), GP ( Koza, 1992 ) and evolutionary programming ( Fogel, 1991 ).

An EA maintains a population of trial solutions, imposes random changes to these solutions, and incor-porates selection to determine which ones are going to be maintained in future generations and which will be removed from the pool of trials.

GP is based on evolving structures encoding programs such as expression trees. As Boolean and extended Booleanqueriescan beeasilyrepresented intheformofexpressiontrees,GPhasbeen widelyused in the IR query learning topic.

EAs are not specifically learning algorithms but they offer a powerful and domain independent search ability that can be used in many learning tasks, since learning and self organization can be considered as optimization problems in many cases. Due to this reason, the application of EAs to IR has increased in the last decade. Among others, EAs have been applied to solve the following problems: (1) Automatic document indexing , either by learning the relevant terms to describe them ( Gordon, 1988 ) (2) Clustering of documents ( Gordon, 1991 ) and terms ( Robertson &amp; Willet, 1994 ). In both cases, a GA is (3) Query definition , by means of an on-line relevance feedback procedure in vector space ( Horng &amp; Yeh, (4) Design of user profiles for IR in the Internet . IRSs are limited by the lack of personalization in the rep-
For a review of several of the previous approaches, see Cordo  X  n, Herrera-Viedma, Lo  X  pez-Pujalte, et al. (2003) . 2.5. Multiobjective EAs and IR
Most of the IQBE approaches in IR evaluate the performance of the derived queries using the two usual criteria, precision and recall (see Section 2.2 ). Therefore, the optimization of the components of an IRS becomes a clear example of a multiobjective problem.

EAsareveryappropriatetosolvemultiobjectiveproblems.Thesekindsofproblemsarecharacterizedby the factthat several objectives have to be simultaneously optimized. Hence, there isnot usually a single best solution solving the problem, i.e. being better than the remainder with respect to every objective, as in single-objective optimization. Instead, in a typical multiobjective optimization framework, there is a set of solutions that are superior to the remainder when all the objectives are considered, the Pareto set. These solutions are known as nondominated solutions ( Chankong &amp; Haimes, 1983 ), while the remainder are known as dominated solutions . Sincenoneof theParetosetsolutions isabsolutely betterthan theother non-dominated solutions, all of them are equally acceptable as regards the satisfaction of all the objectives.
This way, thanks to the use of a population of solutions, EAs can search many Pareto-optimal solutions in the same run, specifically, many queries with different precision X  X ecall trade-offs in our case.
Evolutionary approaches in multiobjective optimization can be classified into three groups: plain aggre-gating approaches , population-based nonPareto approaches , arid Pareto-based approaches ( Coello et al., 2002; Deb, 2001 ).

The first group constitutes the extension of classical methods to EAs. The objectives are artificially com-bined, or aggregated, into a scalar function according to some understanding of the problem, and then the EA is applied in the usual way. 1
Population-based nonPareto approaches allow us to exploit the special characteristics of EAs. A nondom-inated individual set is obtained instead of generating only one solution. In order to do so, the selection mechanism is changed. Generally, the best individuals according to each of the objectives are selected, and then these partial results are combined to obtain the new population. An example of a multiobjective GA belonging to this group is Vector Evaluated Genetic Algorithm (VEGA) ( Schaffer, 1985 ).
Finally, Pareto-based approaches seem to be the most active research area on multiobjective EAs now-adays. In fact, algorithms included within this family are divided into two different groups: first and second generation( Coelloetal.,2002 ).Theyallattempt topromotethegenerationofmultiple nondominatedsolu-tions, as the former group, but directly making use of the Pareto-optimality definition.

The difference between the first and the second generation of Pareto-based approaches arises on the use of elitism. Algorithms included within the first generation group, such as Niched Pareto Genetic Algorithm (NPGA), Non-dominated Sorting Genetic Algorithm (NSGA) and Multiple-Objective Genetic Algorithm (MOGA), do not consider this characteristic. On the other hand, second generation Pareto-based multiob-jective EAs are based on the consideration of an auxiliary population where the nondominated solutions generated among the different iterations are stored. Examples of the latter family are Strength Pareto EA (SPEA) ( Zitzler &amp; Thiele, 1999 ) (the one considered in this contribution) and SPEA2, NSGA2 and NPGA2, among others. For the description of all of these algorithms, the interested reader can refer to Deb (2001) and Coello et al. (2002) .

When multiobjective optimization is tackled, the definition of the quality is substantially more complex than for single-objective optimization problems, since the optimization process itself involves several objectives: (1) The distance of the resulting nondominated set to the Pareto-optimal front should be minimized. (2) A good (in most cases uniform) distribution of the solutions found is desirable. The assessment of this (3) The extent of the obtained nondominated front should be maximized.

Several quantitative metrics have been proposed in the literature to formalize the above definition (or
Given a set of pairwise nondominated decision vectors X 0  X  X , a neighborhood parameter r &gt; 0 (to be chosen appropriately), and a distance metric i  X  i : (1) The function M 1 gives the average distance to the Pareto-optimal set X X : (2) The function M 2 takes the distribution in combination with the number of nondominated solutions (3) The function M 3 considers the extent of the front described by X 0 : Analogously, Zitzler et al. (2000) define three metrics. M 1 , M 2 , and M 3 on the objective space. Let Y 0 ,
Y Y be the sets of objective vectors that correspond to X 0 and X , respectively, and r * &gt; 0 and i  X  i * as before: 3. A multiobjective IQBE EA to learn multiple Boolean queries
Our main objective is to improve Smith and Smith s results obtaining several queries instead of just one in a single run. To do so, we will use a multiobjective focus, incorporating Pareto-based evolutionary mul-tiobjective components intoGP,whose goodbehaviourwasdemonstratedin Rodr X   X  guez-Vazquez,Fonseca, and Fleming (1997) .

Firstly we will review Smith and Smith s approach and then introduce our proposal. 3.1. The Smith and Smith X  X  approach to learn Boolean queries
Smith and Smith (1997) proposed an IQBE process to derive Boolean queries based on GP which we extend inthe followingsectiontoimprove theaidpossibilities to theusers inthe query formulation process. Its components are described as follows:
Coding scheme : The Boolean queries are encoded in expression trees, whose terminal nodes are query terms and whose inner nodes are the Boolean operators AND , OR or NOT , as shown in Fig. 2 .
Selection scheme : Each generation is based on selecting two parents, with the best fitted one having a greater chance to be chosen, and generating two offspring from them. Both offspring are added to the cur-rent population. 2
Genetic operators : The usual GP crossover is considered ( Koza, 1992 ), which is based on randomly selecting one edge in each parent and exchanging both subtrees from these edges between the both parents. No mutation operator is considered. 3
Generation of the initial population : All the individuals in the first population are randomly generated. A pool is created with all the terms included in the set of relevant documents provided by the user, having those present in more documents a higher probability of being selected.
 Fitness function : The following function is maximized: where precision P and recall R are computed as shown in Section 2.2 , while a and b defined in R are the weighting factors. 3.2. A new approach to learn Boolean queries by means of multiobjective evolutionary algorithms
AsisshowninSection 2.5 ,thereare severalkindsofmultiobjective EAs. Infirstgeneration Pareto-based algorithms the elitism concept is lost. There is no way to assure the presence of the best solution since there is not an only best solution, but a set of them. To solve this, new multiobjective evolutionary models were designed. These models use an external population, where the nondominated solutions found are progres-sively stored.

With the aim of maintaining the elitism concept, we have considered SPEA ( Zitzler &amp; Thiele, 1999 )as the multiobjective EA to be incorporated into the basic GP algorithm. 4 This algorithm introduces the elit-ism concept, explicitly maintaining an external population P e . This population stores a fixed number of nondominated solutions which have been found since the start of the run.

Fig. 3 shows the scheme of the SPEA algorithm. In each generation, the new nondominated solutions found are compared with the solutions in the existing external population, storing the resulting nondom-inated solutions on the latter. Furthermore, SPEA uses these elitist solutions, together with those in the current population, in the genetic operations, in the hope to lead the population to good areas in the search space.

Hence, the intermediate population is created from both the current population and the external popu-lation by means of tournament selection. This selection process involves randomly choosing a number of individuals of the population, the so called tournament size, with or without replacement, selecting the best individual of this group, and repeating the process until the number of selected individuals matches up with the population size. To perform the selection, there is a need to assign a fitness value to each individual of both populations. The fitness functions considered are: Elements of the elitist population: where n i isthenumberofsolutionsinthecurrentpopulationdominatedbythe i thindividualoftheelitist population, and N is the size of the current population.
 Elements of the current population:
When the intermediate population is created, the genetic operators are used over the new individuals to getanewpopulationofsize N .Then, thenondominatedsolutionsexisting inthenewpopulationarecopied to the elitist population P e , removing dominated and duplicated solutions. Therefore, the new elitist popu-lation is composed of the best nondominated solutions found so far, including new and old elitist solutions.
To limit the growth of the elitist population, the size is restricted to N e solutions using clustering tech-niques, selecting the solutions closer to the center of each cluster by means of the clustering algorithm shown in Fig. 4 . 4. Experiments developed
As said, the experimental study has been developed using the Cranfield and CACM collections. Cranfield is composed of 1398 documents about Aeronautics while CACM contains 3204 documents published in the journal Communications of the ACM between 1958 and 1979. In both collections, the textual documents have been automatically indexed in the usual way 5 by first extracting the nonstop words and performing a stemming process, thus obtaining a total number of 3857 and 7562 different indexing terms, respectively, and then considering the binary indexing to generate the term weights in the document representations. Both collections have associated a large number of queries (225 in the Cranfield collection and 64 in the CACM collection).Inour problem,eachquerygeneratesadifferent experimentandourgoalinvolvesauto-maticallyderivingasetofqueriesthatdescribestheinformationcontentsofthesetofdocumentsassociated with it. Instead of working with the complete query set, we have selected a representative sample that allow us to study the behavior of our proposal.

The experimental environment considered is graphically shown in Fig. 5 . The role of the user who pro-vides documents will be played by the queries associated with the considered collection, and more exactly, by the relevance judgments associated with them. In this way, for example, if there are 29 relevant docu-ments for query 1 in the Cranfield collection, this query will mimic a situation in which the user provides 29documentsrelatedto his information need. Besides, the remaining,nonrelevant 1369 documents(1398 X  29) will be considered as nonrelevant documents provided to the IQBE process as well. We should remark that, opposite to relevance feedback techniques in which the collection query structures are considered and processed in the same way that documents, we only use the relevance judgments of the existing queries, as the IQBE process learns the query structures starting from scratch.

So, among the 225 queries associated to the Cranfield collection, we have selected a representative sub-set: onthe onehand,those queries presenting 20or more relevant documents have been taken into account; on the other hand, 10 queries with 15 or less relevant documents have also been chosen to test the perfor-mance of our approach with queries presenting a lesser number of relevant documents (representing a situ-ation where the user provides a small number of documents to the IQBE process). The resulting 17 queries (numbers1,2,3,7,8,11,19,23,26,38,39,40,47,73,157,220and225)have29,25,9,6,12,8,10,33,7,11, 14,13,15,21,40,20and25relevantdocuments associated, respectively.Ontheother hand,18queries have been selected from the 64 associated to the CACM collection (numbers 4, 7, 9, 10, 14, 19, 24, 25, 26, 27, 40, 42, 43, 45, 58, 59, 60 and 61), those 13 presenting more than 20 relevant documents and five with less than 15 relevant documents (12, 28, 9, 35. 44, 11, 13, 51, 30, 29, 10, 21, 41, 26, 30, 43, 27 and 31 relevant documents, respectively). We have selected these queries in order to have enough chances to show the performance advantages of our multiobjective algorithm.

The experiments developed involve to run our multiobjective proposal as well as the Smith and Smith s one as comparison algorithm. Each algorithm has been run 10 times with different initializations for each selected query during the same fixed number of fitness function evaluations (100,000) in a 2.4 GHz Pentium IV computer with 1Gb of RAM. 6 The common parameter values considered are a maximum of 20 nodes for the trees, 7 0.8 arid 0.2 for the crossover and mutation probabilities, respectively, 5 for the tournament size arid a population size of M = 1600 queries. The high value of the latter parameter is because it is well known that GP requires large population sizes to achieve good performance. Apart from these parameters. and the size of the elitist population has been fixed to 100 in SPEA.

In Section 2.5 , a set of metrics usually considered to measure the quality of the Pareto sets has been shown. Specifically,wehaveusedthreedifferentmetrics: M 2 arid M 3 ,definedintheaforementionedsection, and the number of nondominated solutions in the Pareto set.

The metric M 1 has been given up since it cannot be used as we do not know the optimal Pareto fronts; furthermore, this metric does not consider the Pareto set distribution. Therefore, we have used M 2 (that measures the distribution of nondominated solutions) and M 3 (which measures the size of the area that contains the nondominated solutions). The reason of using M 2 and M 3 instead of M 2 and M 3 comes from the fact that we are interested in that Boolean queries learned are well distributed in the objective space, to be able to obtain several queries with different precision X  X ecall trade-offs.

Notice that, since our problem is composed of just two objectives, M 3 is equal to the distance among the objective vectors of the two outer solutions (hence, the maximum possible value is
Although the main aim of this paper is to get an IQBE algorithm generating several queries with a different precision X  X ecall trade-off in a single run, we are going to establish a procedure to compare the performance of the proposed technique with that of the original Smith and Smith s proposal.
To do so, the best average solution in precision and recall is selected from the Pareto set derived by our multiobjective IQBE algorithm. This solution is obtained as shown below: (1) 1000 pairs of random numbers are generated ( w i 1 , w i 2 ), w i 1 2 [0,1], w i 2 =1 w i 1 . (2) For each Boolean query S j included in the Pareto set, with recall R j and precision P j , the next index is (3) The solution that maximizes the Average value is selected.
 5. Results and analysis of results 5.1. Analysis of the Pareto sets derived
Tables 1 and 2 show several statistics corresponding to our multiobjective proposal. These tables collects several data, about the composition of the 10 Pareto sets generated for each query, always showing the averaged value and its standard deviation. From left to right, the columns contain the number of nondom-inated solutions obtained (# p ), equal to the number of different objective vectors (i.e., precision X  X ecall pairs) existing among them, and the values of the two multiobjective EA metrics selected, M 2 and M 3 all of them followed by their respective standard deviation values.

The main aim of this paper has been clearly fulfilled since the Pareto fronts obtained are very well dis-tributed, as demonstrated by the high values in the M 2 and M 3 metrics. So, we can see that all runs gen-erate a proportional number of Boolean queries with different precision X  X ecall trade-offs according to the number of relevant documents associated with them (for those cases where a larger number of relevant documents are provided, alarger number of different queries are obtained in the Pareto sets); and that stan-dard deviation values are around 0.4 and 0.6 in the Cranfield and CACM collections, respectively. The val-ues of the M 2 and M 3 metrics are very appropriate as well, emphasizing the values of the latter, very closer to 1.4142, the maximum possible value. This shows us how the Pareto fronts generated cover a wide area in the space.
 More specifically, the experiments generated from queries 157 of the Cranfield collection and 25 of the CACM collection are those deriving the Pareto sets with the best average values. So, query 157 generates a Pareto front with around 24 different solutions, and obtains a value of 10.56 for the distribution of these solutions over it  X  M 2  X  and a value of 1.29 for the  X  M 3  X  metric. Similarly, query 25 of CACM has the fol-lowing values associated: 29.7 for the number of nondominated solutions in the Pareto front, and 12.9 and 1.316 for M 2 and M 3 metrics, respectively.

As an example, Figs. 6 and 7 graphically show the Pareto fronts obtained for queries 157 of Cranfield and 25 of CACM, respectively, representing the recall values in the X -axis and the precision ones on the
Y -axis. As done in Zitzler et al. (2000) , the Pareto sets obtained in the 10 runs performed for each query were put together, and the dominated solutions where removed from the unified set before plotting the curves.

The problem found is that the number of solutions presenting different precision X  X ecall values (different son of this behavior is found in the way we measure the similarity between a pair of solutions (queries).
Two solutions can be equal in the objective space or in the decision space. In IR, if we work in the objec-tive space, two solutions will be equal when their precision and recall values are the same, regardless of its structure. However, if we work in the decision space, two solutions will be equal when their structures (i.e., query compositions) coincide.

In this proposal, we have decided to work in the objective space, supported by the recommendation of the SPEA algorithm s authors ( Zitzler &amp; Thiele, 1999 ) of using the clustering algorithm in this space. Nevertheless, we notice that, with this criterion, several optimal solutions were eliminated. The only differ-ence between these solutions and those of the elitist population is the query composition. This considerably reduces the final number of individuals in the elitist population.

To solve this, we decided to work in the decision space, utilizing the edit or Levenshtein distance ( Levenshtein, 1996 ) to measure the similarity between expression trees. Although this measure increased the number of solutions, the run time also increased. 8 Finally, we decided to choose the original option, leaving the search for new similarity functions between query expressions for future works. 5.2. Analysis of the  X  X  X est X  X  queries derived
Before developing this new analysis, we should again remark that, though the obtained results can be used for comparing our algorithm with the basic one, the fundamental SPEA-GP aim is not to obtain The results obtained by the basic algorithm on the Cranfield and the CACM collections are shown in Tables 3 and 4 , respectively, In both tables, # q stands for the corresponding query number, Sz for the aver-age of the generated queries size and r Sz for its standard deviation, F and r F for the average and standard deviation of the fitness value, respectively, P and R for the average of the precision and recall values (respectively, r R and r P for their standard deviations), # rt for the number of documents retrieved by the query, and # rr for the number of relevant documents retrieved, both with their standard deviations, r # rt
Tables 5 and 6 show the results obtained by our multiobjective proposal. These results come from the best average queries derived, chosen by means of the procedure described in Section 4 . Notice that, in both tables, the best results are shown in boldface, fn view of these results, the performance of our proposal is verysignificant. Onthe onehand, thefitness value provided byour multiobjective proposal improvesSmith and Smith s results in all the queries of the Cranfield collection, 9 and in 16 of the 18 queries of the CACM collection. In both collections, the average precision value is slightly reduced, while the average recall value is significantly increased in the most of the cases, being these variations more pronounced in the CACM collection. In the same way, the number of relevant documents retrieved is also significantly increased. It seems that the diversity induced by the Pareto-based selection and the use of an elitist population make SPEA-GP converge to better space zones.

This way, we can conclude that our multiobjective IQBE approach is not only a good way to derive dif-ferent queries with several precision X  X ecall trade-offs but also to obtain individual queries with high retrie-val accuracy.
 6. Concluding remarks
The automatic derivation of Boolean queries has been considered by incorporating a second generation multiobjective evolutionary approach, SPEA, to an existing GP-based IQBE proposal. The proposed approach has performed appropriately in 35 queries, 17 of the well known Cranfield collection, and 18 of the CACM collection, in terms of absolute retrieval performance arid of the quality of the obtained Pareto sets, allowing us to derive a set of queries with different precision X  X ecall trade-offs.
In our opinion, many different future works arise from this preliminary study. Firstly, we will search for new functions to measure the similarity between expression trees with the purpose of being able to work in the decision space. On the other hand, preference information of the user on the kind of queries to be de-rived can be included in the Pareto-based selection scheme in the form of a goal vector whose values are adapted during the evolutionary process ( Fonseca &amp; Fleming, 1993 ). On the other hand, the proposed
IQBE algorithm can be extended to other kinds of IRSs based on complex query languages such as ex-2003 ) (several preliminaries works on this topic can be found in Cordo  X  n, Herrera-Viedma, Luque, et al., application of the proposed IQBE algorithm.
 Acknowledgement This research has been supported by CICYT under projects TIC2003-07977 and TIC2003-00877 with FEDER fundings.
 References
