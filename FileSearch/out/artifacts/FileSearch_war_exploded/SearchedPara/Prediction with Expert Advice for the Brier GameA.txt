 Vladimir Vovk vovk@cs.rhul.ac.uk Fedor Zhdanov fedor@cs.rhul.ac.uk Egham, Surrey TW20 0EX, UK The paradigm of prediction with expert advice was introduced in the late 1980s (see, e.g., Littlestone &amp; Warmuth, 1994, Cesa-Bianchi et al., 1997) and has been applied to various loss functions; see Cesa-Bianchi and Lugosi (2006) for a recent book-length review. An especially important class of loss functions is that of  X  X ixable X  ones, for which the learner X  X  loss can be made as small as the best expert X  X  loss plus a constant (depending on the number of experts). It is known (Haussler et al., 1998; Vovk, 1998) that the optimal additive constant is attained by the  X  X trong aggregating algorithm X  proposed in Vovk (1990) (we use the adjective  X  X trong X  to distinguish it from the  X  X eak aggregating algorithm X  of Kalnishkan &amp; Vyu-gin, 2005).
 There are several important loss functions that have been shown to be mixable and for which the optimal additive constant has been found. The prime examples in the case of binary observations are the log loss func-tion and the square loss function. The log loss func-tion, whose mixability is obvious, has been explored extensively, along with its important generalizations, the Kullback X  X eibler divergence and Cover X  X  loss func-tion.
 In this paper we concentrate on the square loss func-tion. In the binary case, its mixability was demon-strated in Vovk (1990). There are two natural direc-tions in which this result could be generalized: Regression: observations are real numbers (square-Classification: observations take values in a finite set The mixability of the square loss function in the case of observations belonging to a bounded interval of real numbers was demonstrated in Haussler et al. (1998); Haussler et al. X  X  algorithm was simplified in Vovk (2001). Surprisingly, the case of square-loss non-binary classification has never been analysed in the framework of prediction with expert advice. The purpose of this paper is to fill this gap. The full ver-sion (Vovk &amp; Zhdanov, 2008) of this paper is available on arXiv. A game of prediction consists of three components: the observation space  X , the decision space  X , and the loss function  X  :  X   X   X   X  R . In this paper we are interested in the following Brier game (Brier, 1950):  X  is a finite and non-empty set,  X  := P ( X ) is the set of all probability measures on  X , and where  X   X   X  P ( X ) is the probability measure concen-trated at  X  :  X   X  {  X  } = 1 and  X   X  { o } = 0 for o 6 =  X  . (For example, if  X  = { 1 , 2 , 3 } ,  X  = 1,  X  { 1 } = 1 / 2, (1 / 4  X  0) 2 + (1 / 4  X  0) 2 = 3 / 8.) The game of prediction is being played repeatedly by a learner having access to decisions made by a pool of experts, which leads to the following prediction proto-col: Protocol 1 Prediction with expert advice L 0 := 0.

L k 0 := 0, k = 1 , . . . , K . for N = 1 , 2 , . . . do end for At each step of Protocol 1 Learner is given K experts X  advice and is required to come up with his own deci-sion; L N is his cumulative loss over the first N steps, and L k N is the k th expert X  X  cumulative loss over the first N steps. In the case of the Brier game, the deci-sions are probability forecasts for the next observation. An optimal (in the sense of Theorem 1 below) strat-egy for Learner in prediction with expert advice for the Brier game is given by the strong aggregating al-gorithm. For each expert k , the algorithm maintains its weight w k , constantly slashing the weights of less successful experts.
 Algorithm 1 Strong aggregating algorithm for the Brier game w k 0 := 1, k = 1 , . . . , K . for N = 1 , 2 , . . . do end for The algorithm will be derived in Section 5. The fol-lowing result (to be proved in Section 4) gives a per-formance guarantee for it that cannot be improved by any other prediction algorithm.
 Theorem 1. Using Algorithm 1 as Learner X  X  strategy in Protocol 1 for the Brier game guarantees that for all N = 1 , 2 , . . . . If A &lt; ln K , Learner does not have a strategy guaranteeing for all N = 1 , 2 , . . . . In our first empirical study of Algorithm 1 we use his-torical data about 6416 matches in various English football league competitions, namely: the Premier League (the pinnacle of the English football system), the Football League Championship, Football League One, Football League Two, the Football Conference. Our data, provided by Football-Data, cover two full seasons, 2005/2006 and 2006/2007, and part of the 2007/2008 season (which ends in May shortly after the paper submission deadline). The matches are sorted first by date and then by league. In the terminology of our prediction protocol, the outcome of each match is the observation, taking one of three possible values,  X  X ome win X ,  X  X raw X , or  X  X way win X ; we will encode the possible values as 1, 2, and 3.
 For each match we have forecasts made by a range of bookmakers. We chose eight bookmakers for which we have enough data over a long period of time, namely Bet365, Bet&amp;Win, Gamebookers, Interwetten, Lad-brokes, Sportingbet, Stan James, and VC Bet. (And the seasons mentioned above were chosen because the forecasts of these bookmakers are available for them.) A probability forecast for the next observation is essen-bers summing to 1. The bookmakers do not announce these numbers directly; instead, they quote three bet-ting odds, a 1 , a 2 , and a 3 . Each number a i is the amount which the bookmaker undertakes to pay out to a client betting on outcome i per unit stake in the event that i happens (the stake itself is never returned to the bettor, which makes all betting odds greater than 1; i.e., the odds are announced according to the  X  X ontinental X  rather than  X  X raditional X  system). The inverse value 1 /a i , i  X  { 1 , 2 , 3 } , can be interpreted as the bookmaker X  X  quoted probability for the obser-vation i . The bookmaker X  X  quoted probabilities are usually slightly (because of the competition with other bookmakers) in his favour: the sum 1 /a 1 +1 /a 2 +1 /a 3 exceeds 1 by the amount called the overround (at most 0 . 15 in the vast majority of cases). We used as the bookmaker X  X  forecasts; it is clear that p 1 + p 2 + p 3 = 1.
 The results of applying Algorithm 1 to the football data, with 8 experts and 3 possible observations, are shown in Figure 1. Let L k N be the cumulative loss of Expert k , k = 1 , . . . , 8, over the first N matches and L
N be the corresponding number for Algorithm 1 (i.e., we essentially continue to use the notation of Theorem 1). The dashed line corresponding to Expert k shows the excess loss N 7 X  L k N  X  L N of Expert k over Al-gorithm 1. The excess loss can be negative, but from Theorem 1 we know that it cannot be less than  X  ln 8; this lower bound is also shown in Figure 1. Finally, the thick line (the positive part of the x axis) is drawn for comparison: this is the excess loss of Algorithm 1 over itself. We can see that at each moment in time the algorithm X  X  cumulative loss is fairly close to the cumulative loss of the best expert (at that time; the best expert keeps changing over the time). Figure 2 shows the results of another empirical study, involving data about a large number of tennis tour-naments in 2004, 2005, 2006, and 2007, with the to-tal number of matches 10,087. The tournaments in-clude, e.g., Australian Open, French Open, Wimble-don, and US Open; the data is provided by Tennis-Data. The matches are sorted by date, then by tourna-ment. The data contain information about the winner of each match and the betting odds of 4 bookmakers for his/her win and for the opponent X  X  win. There-fore, now there are two possible observations (player 1 X  X  win and player 2 X  X  win). There are four bookmak-ers: Bet365, Centrebet, Expekt, and Pinnacle Sports. The results in Figure 2 are presented in the same way as in Figure 1. Typical values of the overround are below 0 . 1. In both Figure 1 and Figure 2 the cumulative loss of Algorithm 1 is close to the cumulative loss of the best expert, despite the fact that some of the experts perform poorly. The theoretical bound is not hope-lessly loose for the football data and is rather tight for the tennis data. The pictures look exactly the same when Algorithm 1 is applied in the more realistic man-ner where the weights w k are not updated over the matches that are played simultaneously.
 Our second empirical study (Figure 2) is about binary prediction, and so the algorithm of Vovk (1990) could have also been used (and would have given similar re-sults). We included it since we are not aware of any empirical studies even for the binary case.
 Other popular algorithms for prediction with expert advice that could be used instead of Algorithm 1 in our empirical studies are Kivinen and Warmuth X  X  (1999) Weighted Average Algorithm (WAA) and Freund and Schapire X  X  (1997) Hedge algorithm (HA). The perfor-mance guarantees for these two algorithms are much weaker than the optimal (1), especially in the case of the HA (even if the loss bound given in Freund &amp; Schapire, 1997, is replaced by the stronger bound given in Vovk, 1998, Example 7). The weak perfor-mance guarantees show in the empirical performance of the algorithms. For the football data the maxi-mal difference between the cumulative loss of both the WAA and the HA and the cumulative loss of the best expert is about twice as large as that for Algorithm 1 (and so is approximately equal to the optimal bound ln K given by (1)). For the tennis data the maximal difference for the WAA is about three times as large as for Algorithm 1, and for the HA it is about twice as large; therefore, both algorithms violate the opti-mal bound ln K . For further details, see Vovk and Zhdanov (2008).
 The data used for producing Figures 1 and 2 can be downloaded from http://vovk.net/ICML2008 . This proof will use some basic notions of elementary differential geometry, especially those connected with the Gauss X  X ronecker curvature of surfaces. (The use of curvature in this kind of results is standard: see, e.g., Vovk, 1990, and Haussler et al., 1998.) All defini-tions that we will need can be found in, e.g., Thorpe, 1979.
 A vector f  X  R  X  (understood to be a function f :  X   X  R ) is a superprediction if there is  X   X   X  such that, for all  X   X   X ,  X  (  X ,  X  )  X  f (  X  ); the set  X  of all superpredictions is the superprediction set . For each learning rate  X  &gt; 0, let  X   X  : R  X   X  (0 ,  X  )  X  be the homeomorphism defined by The image  X   X  ( X ) of the superprediction set will be called the  X  -exponential superprediction set . It is known that can be guaranteed if and only if the  X  -exponential su-perprediction set is convex (part  X  X f X  for all K and part  X  X nly if X  for K  X   X  are proved in Vovk, 1998; part  X  X nly if X  for all K is proved by Chris Watkins, and the details can be found in, e.g., Vovk, 2007, Ap-pendix). Comparing this with (1) and (2) we can see that we are required to prove that  X   X   X  ( X ) is convex when  X   X  1;  X   X   X  ( X ) is not convex when  X  &gt; 1.
 Define the  X  -exponential superprediction surface to be the part of the boundary of the  X  -exponential super-prediction set  X   X  ( X ) lying inside (0 ,  X  )  X  . The idea of the proof is to check that, for all  X  &lt; 1, the Gauss X  Kronecker curvature of this surface is nowhere vanish-ing. Even when this is done, however, there is still un-certainty as to in which direction the surface is bulging (towards the origin or away from it). The standard ar-gument (as in Thorpe, 1979, Chapter 12, Theorem 6) based on the continuity of the smallest principal cur-vature shows that the  X  -exponential superprediction set is bulging away from the origin for small enough  X  : indeed, since it is true at some point, it is true ev-erywhere on the surface. By the continuity in  X  this is also true for all  X  &lt; 1. Now, since the  X  -exponential superprediction set is convex for all  X  &lt; 1, it is also convex for  X  = 1.
 Let us now check that the Gauss X  X ronecker curvature of the  X  -exponential superprediction surface is always positive when  X  &lt; 1 and is sometimes negative when  X  &gt; 1 (the rest of the proof, an elaboration of the above argument, will be easy). Set n := |  X  | ; without loss of generality we assume  X  = { 1 , . . . , n } . A convenient parametric representation of the  X  -exponential superprediction surface is  X   X   X   X   X   X   X  of (4) in u 1 is  X   X   X   X   X   X   X   X  the derivative in u 2 is and so on, up to all coefficients of proportionality being equal and pos-itive.
 for purely typographical reasons. A normal vector to the surface can be found as Z :=  X   X   X   X   X   X   X   X   X 
 X  The coefficient in front of e 1 is the ( n  X  1)  X  ( n  X  1) determinant  X   X   X   X   X   X   X   X   X 
 X  + (  X  1) n +1 ( u 1  X  u 3 ) +  X  X  X  + (  X  1) n +1 ( u 1  X  u the first  X  ; the third equality follows from the expan-sion of the determinant along the last column and then along the first row).
 Similarly, the coefficient in front of e i is propor-tional (with the same coefficient of proportionality) to determinant representing the coefficient in front of e i can be reduced to the form analogous to (5) by moving the i th row to the top.
 The coefficient in front of e n is proportional to The Gauss X  X ronecker curvature at the point with co-tive coefficient of proportionality, possibly depending on the point) to  X  ing for transposition).
 for typographical reasons. A straightforward calcula-tion allows us to rewrite determinant (6) (ignoring the  X   X   X   X   X   X   X   X   X   X 
 X   X   X   X   X   X   X   X   X   X   X 
 X  1  X  2  X u 1 0  X  X  X  0  X  1 + 2  X u n (with a positive coefficient of proportionality; to avoid calculation of the parities of various permutations, the reader might prefer to prove the last equality by in-duction in n , expanding the last determinant along the first column). Our goal is to show that the last expression in (7) is positive when  X  &lt; 1 but can be negative when  X  &gt; 1.
 If  X  &gt; 1, set u 1 = u 2 := 1 / 2 and u 3 =  X  X  X  = u n := 0. The last expression in (7) becomes negative. There-fore, the  X  -exponential superprediction set is not con-vex (Thorpe, 1979, Chapter 13, Theorem 1).
 It remains to consider the case  X  &lt; 1. Set t i := 1  X  2  X u i , i = 1 , . . . , n ; the constraints on the t i are Our goal is to prove (1  X  t 1 ) t 2 t 3  X  X  X  t n +  X  X  X  + (1  X  t n ) t 1 t 2  X  X  X  t i.e., This reduces to if t 1  X  X  X  t n &gt; 0, and to if t 1  X  X  X  t n &lt; 0. The remaining case is where some of the t i are zero; for concreteness, let t n = 0. By (8) we are positive; this shows that (9) is indeed true. are positive (if two of them were negative, the sum t +  X  X  X  + t n would be less than n  X  2; cf. (8)). Therefore, To establish (9) it remains to prove (11). Suppose, t 1 /t is convex, we can also assume, without loss of gen-so therefore, Finally, let us check that the positivity of the Gauss X  Kronecker curvature implies the convexity of the  X  -exponential superprediction set, for  X   X  1. Because of the continuity of the  X  -exponential superprediction surface in  X  we can and will assume, without loss of generality, that  X  &lt; 1. The  X  -exponential superpredic-tion surface will be oriented by choosing the normal vector field directed towards the origin; this can be done since with the first coefficient of proportionality positive (cf. (4) and the bottom row of the first determinant in (7)), and the scalar product of the two vectors in (12) is always negative.
 Let us first check that the smallest principal curvature of the  X  -exponential superprediction surface is always positive (among the arguments of k 1 we list not only (4) but also the learning rate  X   X  (0 , 1)). At least at is positive: take a sufficiently small  X  and the point on the surface (4) at which the maximum of x 1 +  X  X  X  + x n is attained (the point of the  X  -exponential superpre-diction set at which the maximum is attained will lie on the surface since the maximum is attained at positive: if k 1 had different signs at two points in the set  X  we could connect these points by a continuous curve ly-ing completely inside (13); at some point on the curve, k 1 would be zero, in contradiction to the positivity of the Gauss X  X ronecker curvature k 1  X  X  X  k n  X  1 . Now it is easy to show that the  X  -exponential super-prediction set is convex. Suppose there are two points A and B on the  X  -exponential superprediction surface such that the interval [ A, B ] contains points outside the  X  -exponential superprediction set. The intersec-tion of the plane OAB , where O is the origin, with the  X  -exponential superprediction surface is a planar curve; the curvature of this curve at the point between A and B closest to the origin will be negative (with the curve oriented by directing the normal vector field towards the origin), contradicting the positivity of k 1 at that point and Meusnier X  X  theorem (cf. (12)). To achieve the loss bound (1) in Theorem 1 Learner can use, as discussed earlier, the strong aggregating al-gorithm (see, e.g., Vovk, 2001, Section 2.1, (15)) with  X  = 1. In this section we will find a substitution func-tion for the strong aggregating algorithm for the Brier game with  X   X  1, which is the only component of the algorithm not described explicitly in Vovk (2001). Our substitution function will not require that its in-put, the generalized prediction, should be computed perts; this is a valuable feature for generalizations to an infinite number of experts (as demonstrated in, e.g., Vovk, 2001, Appendix A.1).
 Suppose that we are given a generalized prediction ( l , . . . , l n ) T computed by the aggregating pseudo-algorithm from a normalized distribution on the member that we are assuming  X   X  1), we are only required to find a permitted prediction  X   X   X   X   X  (cf. (4)) satisfying Now suppose we are given a generalized prediction ( L 1 , . . . , L n ) T computed by the APA from an unnor-malized distribution on the experts; in other words, we are given  X  for some c  X  R . To find (14) satisfying (15) we can first is still a superprediction and then find (14) satisfying satisfy the required (15).
 Proposition 1. Define s  X  R by the requirement The unique solution to the optimization problem t  X  max under the constraints (16) with  X  1 , . . . ,  X  n as in (14) will be There exists a unique s satisfying (17) since the left-hand side of (17) is a continuous, increasing (strictly increasing when positive) and unbounded above func-tion of s . The substitution function is given by (18). Proof of Proposition 1. Let us denote the u i and t de-fined by (18) and (19) as u i and t , respectively. To see that they satisfy the constraints (16), notice that the i th constraint can be spelt out as which immediately follows from (18) and (19). As a by-product, we can see that the inequality becomes an equality, i.e., for all i with u i &gt; 0.
 We can rewrite (16) as and our goal is to prove that these inequalities imply t &lt; t (unless u 1 = u 1 , . . . , u n = u n ). Choose u case, however, we can, and will, also choose u i &gt; 0) for which  X  i := u i  X  u i is maximal. Then every value of t satisfying (21) will also satisfy t  X  L i  X  1 + 2 u i  X  = L i  X  1 + 2 u i  X  2  X  i  X   X  L i  X  1 + 2 u i  X  with the last  X  following from (20) and becoming &lt; when not all u j coincide with u j .
 The detailed description of the resulting prediction al-gorithm was given as Algorithm 1 in Section 2. As discussed, that algorithm uses the generalized predic-tion G N (  X  ) computed from unnormalized weights. In this paper we only considered the simplest predic-tion problem for the Brier game: competing with a finite pool of experts. In the case of square-loss regres-sion, it is possible to find efficient closed-form predic-tion algorithms competitive with linear functions (see, e.g., Cesa-Bianchi &amp; Lugosi, 2006, Chapter 11). Such algorithms can often be  X  X ernelized X  to obtain predic-tion algorithms competitive with reproducing kernel Hilbert spaces of prediction rules. This would be an appealing research programme in the case of the Brier game as well.
 Acknowledgments We are grateful to Football-Data and Tennis-Data for providing access to the data used in this paper. This work was partly supported by EPSRC (grant EP/F002998/1). Comments by Alexey Chernov, Alex Gammerman, Yuri Kalnishkan, and anonymous refer-ees have helped us improve the presentation.
 Brier, G. W. (1950). Verification of forecasts expressed in terms of probability. Monthly Weather Review , 78 , 1 X 3.
 Cesa-Bianchi, N., Freund, Y., Haussler, D., Helmbold, D. P., Schapire, R. E., &amp; Warmuth, M. K. (1997).
How to use expert advice. Journal of the Association for Computing Machinery , 44 , 427 X 485.
 Cesa-Bianchi, N., &amp; Lugosi, G. (2006). Prediction, learning, and games . Cambridge, England: Cam-bridge University Press.
 Dawid, A. P. (1986). Probability forecasting. In
S. Kotz, N. L. Johnson and C. B. Read (Eds.), Ency-clopedia of statistical sciences , vol. 7, 210 X 218. New York: Wiley.
 Freund, Y., &amp; Schapire, R. E. (1997). A decision-theoretic generalization of on-line learning and an application to boosting. Journal of Computer and System Sciences , 55 , 119 X 139.
 Haussler, D., Kivinen, J., &amp; Warmuth, M. K. (1998).
Sequential prediction of individual sequences under general loss functions. IEEE Transactions on Infor-mation Theory , 44 , 1906 X 1925.
 Kalnishkan, Y., &amp; Vyugin, M. V. (2005). The Weak
Aggregating Algorithm and weak mixability. Pro-ceedings of the Eighteenth Annual Conference on Learning Theory (pp. 188 X 203). Berlin: Springer. Kivinen, J., &amp; Warmuth, M. K. (1999). Averaging ex-pert predictions. Proceedings of the Fourth European
Conference on Computational Learning Theory (pp. 153 X 167). Berlin: Springer.
 Littlestone, N., &amp; Warmuth, M. K. (1994). The Weighted Majority Algorithm. Information and Computation , 108 , 212 X 261.
 Thorpe, J. A. (1979). Elementary topics in differential geometry . New York: Springer.
 Vovk, V. (1990). Aggregating strategies. Proceedings of the Third Annual Workshop on Computational Learning Theory (pp. 371 X 383). San Mateo, CA: Morgan Kaufmann.
 Vovk, V. (1998). A game of prediction with expert advice. Journal of Computer and System Sciences , 56 , 153 X 173.
 Vovk, V. (2001). Competitive on-line statistics. Inter-national Statistical Review , 69 , 213 X 248.
 Vovk, V. (2007). Defensive forecasting for opti-mal prediction with expert advice (Technical Re-port arXiv:0708.1503 [cs.LG]). arXiv.org e-Print archive.
 Vovk, V., &amp; Zhdanov, F. (2008). Prediction with ex-pert advice for the Brier game (Technical Report arXiv:0708.2502v2 [cs.LG]). arXiv.org e-Print
