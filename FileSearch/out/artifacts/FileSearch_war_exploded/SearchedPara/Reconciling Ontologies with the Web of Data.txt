 To integrate Linked Open Data, which originates from var-ious and heterogeneous sources, the use of well-defined on-tologies is essential. However, oftentimes the utilization of these ontologies by data publishers differs from the intended application envisioned by ontology engineers. This may lead to unspecified properties being used ad-hoc as predicates in RDF triples or it may result in infrequent usage of spec-ified properties. These mismatches impede the goals and propagation of the Web of Data as data consumers face difficulties when trying to discover and integrate domain-specific information. In this work, we identify and classify common misusage patterns by employing frequency analy-sis and rule mining. Based on this analysis, we introduce an algorithm to propose suggestions for a data-driven on-tology re-engineering workflow, which we evaluate on two large-scale RDF datasets.
 H.2.8 [ Database Management ]: Database applications ontology engineering, Linked Data, data mining
The concept of Linked Open Data (LOD) enables infor-mation providers to create and share linkable data across the Web. Besides the increasing number of LOD sources, there also exists a set of rules for publishing Linked Data [6]. Still, consuming and integrating LOD necessitates a thor-ough analysis and study of the data sources, as individual data providers have different understandings or knowledge of useful vocabulary definitions. Here, reusable knowledge bases and ontologies facilitate comprehending and integrat-ing multiple data sources. These ontologies provide meta-data to define the domains and ranges of properties of re-sources or taxonomical relationship between these resources. In our work, we analyze the differences between specification and usage of such vocabularies and offer a data-driven ap-proach to refine existing ontologies.

We observed a significant distinction between well-defined ontologies and the common understanding of LOD. While LOD in general is designed to be easily extensible so that facts about resources in one dataset can be added ad-hoc and independently of other sources, ontologies usually de-fine structural information for a number of data sources and are therefore less flexible. Hence, integration of LOD, ontol-ogy discovery and matching pose major challenges for the pervasion of the Web of Data. While there are best prac-tices for publishing Linked Open Data using established on-tologies [6], our analysis shows that due to various reasons certain  X  X isusage X  patterns occur frequently. This misuse partly stems from the fact that ontology definitions may be either too specific or too generic. Thus, custom namespace-specific properties often are added to an ontology concept when need arises. It is likely that some of these proper-ties are redundant, as data providers are unaware of one another X  X  additions.

Addressing such quality issues of an ontology can be con-sidered a form of schema analysis. Here, a schema defines the set of properties whose domain is a specific class within the ontology. Hence, redesign of ontologies requires analy-sis and mining of the underlying data. The following real-world example illustrates the discrepancy between ontology specification and usage. For brevity and readability, we use a number of prefix abbreviations when denoting RDF re-sources. These abbreviations are defined in Listing 1. Listing 1: RDF prefix abbreviations used in this work Motivational Example. For a manifestation of the prob-lem addressed by this work consider Listing 2 which shows an excerpt of the :Settlement class definition. If a geo-location data provider decides to publish her data using this specification, she might be confused about how to set proper values for some of the properties. The properties in lines 2 and 3 are intuitively applicable as predicates 1 to all instances of :Settlement , whereas others seem only useful for a strict
In accordance with the well-known RDF triple model (subject-predicate-object), in this paper we refer to instance-level properties as predicates . subset of instances, such as :scottishName and :distance-ToEdinburgh in lines 4 and 5.
However, none of the properties of this class (or any of its parent classes :PopulatedPlace , :Place , and owl:Thing ) model the latitude and longitude degrees of a settlement al-though these are set for many instances of :Settlement in DBpedia, e.g., via the dbp:latd and dbp:longd predicates, respectively. As dbp:latd and dbp:longd are not explicitly specified for :Settlement , ambiguous predicates with simi-lar values also occur in the instance data, e.g., dbp:latDeg and dbp:longDeg .

Overall, for a more intuitive vocabulary definition some of the properties (e.g., :scottishName ) could be removed, del-egated to a more suitable subclass of :Settlement if avail-able, or marked as optional if supported by the ontology language. Additionally, some of the predicates that are al-ready set for a large number of instances of :Settlement (e.g., dbp:latd ) may be included in the class definition.
In general, the denoted discrepancies may cause confusion which ontology and which classes therein to adopt when pub-lishing RDF data. Using vocabularies that are not intended for certain resources or unwarranted extensions to existing ontologies limit machine readability and thus impedes the benefits of Linked Data. The goal of our work is to identify recurring misuse of ontology definitions and help overcome these problems by offering re-engineering suggestions. Contributions. To facilitate the process of such ontology re-engineering we contribute: 1. The definition of two general ontology misusage cases. 2. An automated data-driven approach that supports on-3. A thorough evaluation that shows the usefulness and
Based on an existing ontology, we identify two typical cases where the specification differs from usage patterns: overspecification and underspecification . We refer to a cer-tain class as being overspecified, if one or more properties are declared for this class by the ontology, but are rarely (if ever) used for real-world data, e.g., :scottishName for :Settlement . We deem a class to be underspecified, when in real-world data certain properties are used frequently even though they are not specified by the vocabulary, e.g., dbp:latd for :Settlement .

Note that a class can simultaneously be overspecified and underspecified (with regard to different properties). Both overspecification and underspecification may stem from data providers being unaware of the specifics of the ontology they employ for the information they publish. Thus, they may neglect certain properties (although suitable) or introduce new ones (even though these may be semantically equiva-lent to existing ones). However, in our work we focus on ontology engineers by offering usage information as well as re-engineering suggestions to them.
Over time, RDF ontologies may grow by introducing new class and property definitions. Especially cross-domain on-tologies, such as DBpedia and Yago, have evolved exten-sively since their first specification. However, revising ex-isting class definitions is sometimes neglected during this evolutionary process. This leads to several problems:
In most cases for which we identified overspecification, a solution is to remove properties from the class concerned. However, sometimes these properties are still valid for cer-tain subclasses of this class. Thus, before removing a prop-erty from the ontology it is essential to verify whether it is used in instances of any subclass. If so, the property needs to be pushed down to the appropriate subclass.
The flexible RDF data model allows the ad-hoc assign-ment of predicates to instances of a certain class. However, if this predicate is suitable for a large number of instances, it might prove beneficial to add it as a property to the corre-sponding class definition. Doing so aids data integration and ensures that other data providers are aware of this property. The reasons for underspecification include:
To overcome underspecification, properties may be added to a class definition. In our approach, we ensure that our suggestions do not create unnecessary redundancy. If both a class and several of its subclasses are underspecified with respect to a certain property, this property is not added to these subclasses, but pushed up to the common parent class instead. Finally, we combine both approaches for under-and overspecification so that the results are consistent and without loss of information. http://km.aifb.kit.edu/projects/btc-2011/
To determine data-driven ontology re-engineering options, one could align specific classes and associated instance data to identify predicates to be removed from or added to the class definition, respectively. However, this would have two limitations: (1) Some properties might be removed instead of being allocated to a more suitable subclass, and (2) Some properties might be added to a class and its subclasses inde-pendently without regarding inheritance relationships. There-fore, we propose a holistic approach that processes all classes of a given ontology by also considering class hierarchies.
Our approach to generate suggestions for predicate re-moval and inclusions consists of the following steps: 1. Identify typed entities in the data (declaring rdf:type ) 2. Generate removal suggestions by detecting rarely used 3. Generate inclusion suggestions by mining predicates
Depending on the dataset, the first task is more or less straightforward: For instance, in the case of DBpedia, ex-plicit instance mapping information as well as a well-defined ontology is available. The last two steps describe our prin-cipal approach for generating re-engineering suggestions. In the following, we present the intuition of our approach and the procedure of making ontology adjustment proposals.
After having identified the typed entities in a dataset, we construct frequent sets of predicates for instances of each type. A frequent set of predicates is a set of predicates that co-occur for a significant number of instances and hold min-imum support s [2]. A set of predicates X holds support s if s % of instances of a specific type in the dataset involve all the predicates of X . Moreover, we can detect depen-dencies between frequent sets of predicates as association rules: A positive association rule X  X  Y states that the predicates in Y ( consequent ) depend on the predicates in X ( condition ). The confidence conf of such an association rule is the conditional probability P ( Y | X ). Denoting the sup-port of a set X as supp( X ) , conf( X  X  Y ) is computed as supp( X  X  Y ) / supp( X ) . Relevant rules are those that hold some minimum confidence c .
Given a dataset with typed instances and a corresponding ontology, we apply frequency and association rule analysis to identify over-and underspecification. Next, we describe the two consecutive steps for the ontology adjustment: Gener-ating property removal and property inclusion suggestions.
Identifying cases of overspecification in a class definition is straightforward: Given a minimum support threshold of s , each property that does not hold s in the given data con-stitutes an overspecification of the current class and should therefore be suggested for removal from this class. Thus, for each property defined for the class its distinct occurrences as predicate for all entities of the given class are counted and the total is compared against the minimum support. The set of all properties that do not meet the minimum support are marked for removal from the specific class definition.
Having marked removal suggestions, we now determine predicates that are used frequently for instances of a spe-cific class but are not defined as properties for the class itself or any of its parent classes in the ontology. We also consider association rules and propose only those predicates to a class that are highly correlated with already (validly) defined properties of this class. This is the first constraint we define for our suggestions:
Constraint 1 If for two predicates p 0 and p there is a rule p  X  p with minimum support s and minimum confidence c , and domain ( p 0 ) = C and domain ( p ) 6 = C and domain ( p ) 6 = P be proposed as a property for C .

It may happen that under Constraint 1 a predicate is pro-posed for a class C as well as for one or more of the subclasses of C . For example, the predicate :anthem may hold enough support among the instances of type :Place and may also be associated with properties defined for :Place . However, it might be the case that most or all those instances of type :Place with :anthem are in fact instances of the more spe-cific type :Country (which is a subclass of :Place ). Then, the algorithm proposes the property to be added to the more specific class. On the other hand, when a predicate such as :populationDensity is proposed for multiple subclasses of :PopulatedPlace , such as :Country , :City , or :Continent , as well as for :PopulatedPlace itself on behalf of Const. 1 it is more appropriate to propose the predicate for the more general class :PopulatedPlace . To avoid redundant sugges-tions and specifying properties as fittingly as possible we define the following two constraints.
 Constraint 2 If Constraint 1 holds for predicate p and class C as well as for p and exactly one of C  X  X  strict subclasses S property p should be proposed for the subclass S i instead of C . Constraint 3 If Constraint 1 holds for predicate p and class C as well as for p and more than one of C  X  X  strict subclasses S , S 2 , . . . , where at least two of these subclasses are not sub-classes of one other, p should be proposed only for C .
Algorithm 1 illustrates the workflow for generating prop-erty inclusion suggestions after the identification of removal candidates. The input of the algorithm includes the com-plete set of instance triples (i.e., triples about resources for which the type is known) denoted as triples as well as the set of classes classes of the ontology to be adjusted. For each class c it is indicated whether properties have been defined specifically for the class or inherited from parent classes and whether the properties are removal candidates. The result of the algorithm are enriched class definitions containing in-clusion and removal suggestions for each class.

The algorithm traverses the classes in the given ontolog-ical hierarchy breadth-first, beginning with the leaves and moving up towards the root. It may happen that one predi-cate is an inclusion candidate for a class as well as for some of its subclasses. To decide for which class specifically to propose such a property it is necessary that all subclasses have been analyzed before the parent class. This is ensured by the reverse topological sorting of classes in line 1. After-wards, for each c in classes rule mining is executed on all triples in triples belonging to instances of type c based on given minimum support and confidence.

For each rule discovered in the process, Constraint 1 is checked in line 8. If it holds, the rule X  X  consequence is added to the set of inclusion suggestions . As we apply the same Algorithm 1: Property Inclusion Suggestion Algorithm
Data : classes : class definitions (including original and Data : triples : instance triples for all available classes
Data : minSupp , minConf : minimum support and 1 classes . topologicalSortAscending (); 2 foreach c  X  classes do 3 rules  X  genRules ( minSupp , minConf , T , c ); 4 schema  X  c . cleanProperties ; 5 inheritedSchema  X  c . cleanInheritedProperties ; 6 candidates  X  X  X  ; 7 foreach r  X  rules do 8 if r. condition  X  schema  X  9 r. consequence /  X  inheritedSchema then 10 suggestions . add (r . consequence ); 11 foreach s  X  suggestions do 12 subclasses = c . getSubclassesWith ( s ); 13 if | subclasses | = 1 then 14 suggestions . remove ( s ); 15 if | subclasses | &gt; 1 then 16 foreach subclass  X  subclasses do 17 subclass . removeProperty ( s ); 18 c . addSuggestions ( suggestions ); minimum support here as in the property removal suggestion step, no property is proposed for the same class for which it has been marked for removal earlier. Note that schema and inheritedSchema contain only those properties that have not been marked when identifying property removal suggestions, i.e., every property that has been marked for removal in the previous step may appear as an inclusion candidate for some of the subclasses of c .

Having scanned rules for appropriate candidates , the next step is to test the current class c and suggestions for the Con-straints 2 and 3. Thus, for each suggestions s it is checked whether there are subclasses of the current class c that in-clude s in their schema or inclusion suggestions list. Accord-ing to Constraints 2 and 3, the algorithm either removes s from all definitions in subclasses (either specified or pro-posed) of c or from the current suggestions of c .

Finally, the remaining suggestions are added to the cur-rent class c . These inclusion suggestions can be used to extend the original class definitions.
To assess the quality of our ontology re-engineering pro-posals, we applied our approach on the DBpedia dataset and corresponding ontology, and evaluated the resulting sugges-tions 3 . The DBpedia ontology is manually generated using Wikipedia infobox templates [3], and evolves over time as the infobox templates are changed.

We performed our evaluation on the DBpedia 3.6 and (at the time of writing most current) DBpedia 3.7 datasets along with the respective DBpedia ontology versions utilized for See [9] for a discussion of a second case study  X  the Billion Triple Challenge 2011 dataset. the data. In Tab. 1, we list the total amounts of (unique) triples and subjects in the individual datasets as well as the total amounts of unique classes and properties in the ontologies.
 Table 1: Total of unique values in DBpedia 3.6 and 3.7
We identified 503 removal suggestions in the DBpedia 3.6 ontology and 622 removal suggestions in the DBpedia 3.7 ontology, all with support  X  1%. Table 2 shows sample results of overspecification in DBpedia 3.7. Some of the removal suggestions can be moved to a more suitable sub-class, Tab. 3 presents such an alternative allocation of the bottom five properties in Tab. 2. Clearly, the proposed as-signment is more appropriate than the actual specification, as the suggestion support is in orders of magnitude higher than the original support. While for some properties the suggestion support still seems low (e.g., :countySeat ), in-stances of these classes still include more than 90% of the occurrences of the properties among the originally assigned parent class.

For underspecification, we applied Algorithm 1 ( minSupp : 1%, minConf : 70%) on DBpedia 3.6 and DBpedia 3.7. The choice of the thresholds corresponds to traditional associa-tion rule mining settings and proved reasonable in our sce-nario. A higher minSupp threshold results in more removal suggestions and less inclusion suggestions. A higher min-Conf threshold reduces only the number of inclusion sugges-tions. We evaluated all suggested properties for the classes of the ontology manually by labeling whether their assign-ment to a specific class was useful , not useful , or undecided . An assignment was undecided when labelers could not de-termine the appropriateness of the assignment.

Overall, the majority of the suggestions have been la-beled as useful, including the pushed properties mentioned in Tab. 3. Some of our proposed properties for the DBpe-dia 3.6 dataset have indeed been included in the DBpedia 3.7 ontology, such as :numberOfEpisodes and :numberOfSea-sons for the class :TelevisionShow , thus further validating our results. Table 4 illustrates the amount and quality of class property suggestions for DBpedia 3.6 and 3.7.
Suggestions marked as undecided are those for which we could not decide whether they enhance the class definition or not. This was often the case, when a similar or synonymous property had already been defined for a class in the ontology (e.g., for :Person , :Person/weight is specified, :weight is suggested). For DBpedia 3.6 we discovered that of the 283 suggested properties 206 had no specified domain, 17 were pushed up or down from a sub-or superclass, 12 had com-pletely different domains, and 48 had synonymous specified properties. Of the 317 suggested properties in DBpedia 3.7, 225 had no domain, 18 were pushed, 17 had different do-mains, and 57 had synonymous specified properties.

Note that we considered only properties from the DBpedia ontology namespace, but properties from other namespaces might also be valid suggestions. Overall, our evaluation shows that data-driven suggestions lead to useful results. The runtime of our algorithm is in the order of a few hours (including the time for creating the transaction database for association rule mining) even for large datasets and mostly depends on the parameters used for rule mining.
There are only a few approaches and projects that apply data mining on the Semantic Web. Mostly, they are in the fields of inductive logic programming and approaches that make use of the description logic of a knowledge base [7, 8]. Those approaches concentrate on mining answer-sets of queries towards a knowledge base. An association rules based approach for mining the Semantic Web is proposed by Nebot et al. [11], where a SPARQL endpoint allows the user to define targets of mining in any desired graph con-text. Their approach also focuses on entities and ignores predicates. Our approach corresponds to one of the six min-ing configurations outlined in [1] that also includes predicate mining. ProLOD is a tool for profiling Linked Open Data, which includes association rule mining on predicates for the purpose of schema analysis [4]. In this work, we present a concrete application exploiting the mined predicates.
Several works in the field on ontology engineering aim at establishing and enriching ontology specifications by using machine learning techniques [5]. The authors of [10] present a semi-automatic approach for cross-domain ontology learn-ing. Similarly, in [13] machine learning methods are em-ployed to refine the definition of the Wikipedia infobox-class ontology. In contrast to these works, our approach allows incremental re-engineering of an existing ontology based on provided instance data without any training data or third party data sources, such as WordNet.

The authors of [12] present a schema induction approach based on association rules to recreate axioms of the DBpedia ontology. As their approach generates an ontology axiom for every generated rule, redundant or conflicting axioms are created, too. Our approach, differs from their work in two major aspects: First, we do not want to recreate an ontology but to improve a given ontology.Second, we have defined constraints that allow our approach to create sound suggestions for improving an ontology definition by avoiding the generation of redundant and conflicting suggestions.
We presented an approach to evaluate the usage of an on-tology in real-world RDF data and suggest possible modifi-cations to the ontology X  X  definition based on this evaluation. An ontology engineer can use these suggestions to revise the specification, thereby improving the intuitiveness of the on-tology and aiding its propagation.

In this work, we identified and described two misconcep-tions of the vocabulary definition regarding its application to real-world data: over-and underspecification. To cope with these challenges, we presented an automated approach that facilitates ontology re-engineering by suggesting the removal or incorporation of properties for given ontology classes.
We evaluated our approach on the DBpedia dataset along with the DBpedia ontology. As illustrated in Sec. 4, there are significant mismatches between the intended use of cer-tain well-known ontology specifications and how they are employed. These might by caused by illegitimate use of the vocabulary by data publishers, ontology evolution, or gen-eral design flaws in the vocabulary, amongst other things. In future work we want to analyze negative correlations and association rules for ontology re-engineering.
