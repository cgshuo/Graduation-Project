 Prior research in resource selection for federated search mainly focused on selecting a small number of information sources that are most relevant to a user query. However, result nov-elty and diversification are largely unexplored, which does not reflect the various kinds of information needs of users in real world applications.

This paper proposes two general approaches to model both result relevance and diversification in selecting sources, in or-der to provide more comprehensive coverage of multiple as-pects of a user query. The first approach focuses on diversify-ing the document ranking on a centralized sample database before selecting information sources under the framework of Relevant Document Distribution Estimation (ReDDE). The second approach first evaluates the relevance of infor-mation sources with respect to each aspect of the query, and then ranks the sources based on the novelty and relevance that they offer. Both approaches can be applied with a wide range of existing resource selection algorithms such as ReDDE, CRCS, CORI and Big Document. Moreover, this paper proposes a learning based approach to combine mul-tiple resource selection algorithms for result diversification, which can further improve the performance. We propose a set of new metrics for resource selection in federated search to evaluate the diversification performance of different ap-proaches. To our best knowledge, this is the first piece of work that addresses the problem of search result diversifica-tion in federated search. The effectiveness of the proposed approaches has been demonstrated by an extensive set of experiments on the federated search testbed of the Clueweb dataset.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Federated Search, Resource Selection, Diversification
Federated search, also known as distributed information retrieval [28, 4, 14], focuses on searching information dis-tributed across multiple information sources such as local repositories or verticals. There are three major sub-problems in federated search: resource representation obtains infor-mation about contents and other key properties of each indi-vidual information source, resource selection selects a small number of most useful sources given a user query, and re-sult merging integrates individual ranked lists from selected sources into a single final list. A large body of research has been conducted for resource selection in federated search [4, 28]. However, little is known about selecting a set of sources that balances relevance and novelty. This substantially lim-its the usability of federated search in many applications.
On the other side, search result diversification has been studied extensively in ad hoc search in order to offer more coverage for ambiguous and multifaceted queries. In sev-eral occasions, users X  intents in their queries may not be expressed explicitly. For example, an ambiguous query such as  X  X aguar X  may refer to an animal or a car model; or a multifaceted query such as  X  X atman X  may refer to a name of a movie, a comic character, or the comic itself. Search result diversification circumvents this problem by explicitly or implicitly considering probable aspects of the query and presenting the search results in a way that is easier for users to find the needed information. Since 2009, the TREC Web track has incorporated diversification in the evaluation of the Web track [9]. Several evaluation metrics have been developed in ad hoc search to measure the effectiveness of different approaches of search result diversification.
Search result diversification in federated search may not be as simple as diversifying the final ranked list obtained from the selected sources. In a federated environment where documents of a same source cover similar topics, selecting a set of sources that balances relevance and novelty becomes crucial. As for the example of the query  X  X aguar X  above, if the sources related to  X  X aguar X  as a car dominate the re-source selection result, it will be much harder to obtain a diversified ranked list in the end.

This paper proposes new approaches in diversifying results of resource selection in federated search. To the best of our knowledge, this is the first study that tackles the issue. First, a set of new metrics is designed for measuring result diver-sification in resource selection. The metrics can incorporate any diversity measure that has been developed in ad hoc search, including the intent-aware expected reciprocal rank (ERR) [7],  X  -nDCG@k [13] and MAP-IA [1]. Second, two general approaches are proposed for diversifying resource se-lection. The first approach extends the ReDDE framework and utilizes a ranked list of documents on the centralized sample database. By reranking the sample documents with respect to result diversification, a better set of sources can be obtained in term of relevance and aspect coverage. The second approach offers a different view. Instead of reranking the sample documents based on their relevance to the query aspects, the information sources are reranked in a similar process. This can be done by estimating the relevance of each information source with respect to different aspects of the query by any existing resource selection algorithm. Fur-thermore, a learning based classification approach is pro-posed to combine multiple resource selection algorithms for a better estimation of source relevance with respect to query aspects. With some training data, the learning based clas-sification approach can improve the effectiveness of resource selection for search result diversification.

An extensive set of experiments has been conducted with the federated search testbed of the Clueweb dataset to eval-uate the proposed research. In many experimental settings, the new approaches can successfully improve result diversi-fication over traditional approaches that only consider doc-ument relevance. In particular, the new approaches provide superior performance on two test levels: source-level results of resource selection and document-level results of the fi-nal ranked list of federated document retrieval. Finally, the learning based approach, which combines results from mul-tiple resource selection algorithms, outperforms each indi-vidual algorithm in result diversification.

The rest of the paper is organized as follows. Section 2 offers the literature review on both resource selection in federated search and result diversification in ad hoc search. Section 3 discusses our proposed metric to measure the di-versity of selected sources. Section 4 proposes the two result diversification approaches for resource selection in federated search. Section 5 presents the learning based classification approach. The new proposed research is examined by an extensive set of experiments in Sections 6 and 7. Section 8 concludes our work and points out some potential research directions in the future.
Considerable research have been conducted for all three sub-problems of federated search as resource representation, resource selection and result merging [4, 28, 15]. This section provides a discussion of prior research on resource selection, as well as a brief review on resource representation and re-sult merging. We also discuss some popular ad hoc search algorithms for search result diversification.

Resource representation is the first step of federated search for obtaining important properties of distributed informa-tion sources such as content and size statistics. Query-based sampling [4] is a common approach as it obtains sample documents from available sources with randomly generated queries. The sample documents obtained in this process can be placed together in a centralized sample database .
Resource selection selects a small number of most relevant information sources for a user query. Some early resource selection algorithms such as CORI [4], CVV [37] and KL [36] treat each source as a big document and derive useful statistics to rank available sources with respect to a user query. However, these algorithms have limitation of los-ing the boundaries of individual documents, and thus may underestimate a big source with many relevant documents. Topic modeling has been proposed by recent work to over-come this limitation [3].
 Other resource selection algorithms such as ReDDE [32], DTF [17], CRCS [27] and SUSHI [33] step away from the big document assumption by modeling individual documents of a source. Several selection algorithms in this category rely on the centralized sample database to build a ranked list of sample documents for a user query, and then assign a rele-vance score to available sources based on the scores of their sample documents in the list. Different algorithms use differ-ent methods for aggregating document contribution to avail-able sources. Recent work by Markov et al. follows a sim-ilar approach, but attempts to minimize uncertainty in the centralized sample database by sampling different queries, retrieval systems, or rankings [22].

Learning based models have also been proposed for re-source selection. They treat resource selection in federated search [20], or vertical search [2] as a classification prob-lem. In particular, given a set of training queries and some relevance judgment, a classification model can learn to pre-dict the relevance of an information source. In some ex-periments, the classification approaches have been shown to provide more accurate resource selection results than tradi-tional algorithms without the training process.

Result merging is the last step in federated search, which merges documents returned by selected sources into a single ranked list. Modern methods such as SSL [31] and SAFE [30] both attempt to merge documents by approximating the centralize retrieval results in different ways.

Existing research in federated search have not explored an important issue of result novelty and diversification, which limits their abilities in representing the various information needs of users. The research work in [29] estimates the de-gree of document overlap among available sources, but its focus is only on duplicate documents and does not directly address the diversification problem related with multiple as-pects of user queries. Other research work in [25] and [39] ad-dress diversification in aggregated search, which is similar to federated search, but operates in cooperative environments. Most importantly, those work do not target diversification in selecting relevant verticals (or sources) directly.
On the other hand, result diversification has been a pop-ular research topic in ad hoc search. Its goal is to make a trade-off between relevance and novelty in ranking docu-ments [38, 5, 1, 6, 26]. In order to achieve the desired effect of covering sufficient aspects of a user query (so that user will likely find the sought information), diversification al-gorithms target on discovering novel aspects that have not been covered in the ranked list, and reducing the redundancy information shared between multiple documents.

The earlier generation of diversification algorithms do not explicitly consider multiple aspects of a query [5, 38]. In-stead, they build the ranked list from top to bottom, and make a choice of whether to include a document based on its similarity with existing documents in the list. More recent diversification algorithms directly incorporate query aspects into consideration. Agrawal et al. proposed the use of tax-onomy to classify query aspects, in order to discover novel and redundant information [1]. Carterette and Chandar di-rectly optimized the ranked list with respect to evaluation measures based on diversity [6]. Santos et al. proposed the eXplitcit Query Aspect Diversification (xQuAD) method, which estimates the surplus information that a document can add to a ranked list, using the probability of relevance with respect to all aspects of the query [26]. Recently, Dang and Croft proposed a different view on diversification by preserving the proportionality of document presence with respect to each aspect of the query [16]. Their proposed algo-rithm PM-2 has proven to achieve superior performance over several other algorithms of the same category. Other recent work on search result diversification include a combination of implicit and explicit topic representations [19], personal-ized diversification [34], and explicit relevance model [35].
There exists several standard diversification metrics for ad hoc search. However, no evaluation metric has been de-veloped to compare the diversification results of different re-source selection algorithms in federated search. This section proposes a new metric in order to fill that gap. Our metric is based on the popular R-metric [4] for resource selection in federated search. It is calculated as the ratio between the number of relevant documents contained in sources se-lected by a particular algorithm, over the number of rele-vance documents in sources selected by an ideal algorithm. In particular, the R-metric is defined as: where E i denotes the number of relevant documents of the i -th source according to the ranking E by a particular resource selection algorithm, and B i denotes the same quantity with respect to the optimal ranking B . In this case, the optimal ranking B should order sources by the true number of their relevant documents. We adopt the idea to our new generic metric, which is called R-based diversification metric, as fol-lows.
 R M ( S ) = where S is the set of selected sources for comparison and M is a diversity metric of a ranked list of documents such as ERR-IA,  X  -nDCG, Prec-IA, S-Recall and NRBP. The op-timal ranking of documents in S is the list that achieves the best score with respect to metric M . For most of the aforementioned metrics, finding the optimal ranked list is an NP-hard problem, but this can be acceptably approximated by a greedy algorithm (i.e., repeatedly select the next docu-ment that maximizes the metric given the current ranking; cf. [8, 13]). The intuition of the proposed metric is that, if we can select a minimal set of sources that contains enough diversified documents to reach the optimal measure, then the R-based diversification metric is maximized to 1. Otherwise, it gives us an estimation of how far our selected sources are from the optimal ones.

Like R-metric, the proposed R-based diversification met-ric is independent of the retrieval algorithm utilized by each source. The R-based diversification metric returns 1 if all available sources are selected. In general, for comparison between different resource selection algorithms, the maxi-mum number of sources is determined beforehand.
This section proposes two approaches for selecting diver-sified information sources. The first approach extends the ReDDE framework by ranking sample documents with con-sideration to diversity. The second approach estimates the source relevance to each aspect of the query and ranks the sources based on the estimations of their aspect relevance.
We first describe the Relevant Document Distribution Es-timation (ReDDE) framework [32] for ranking sources based on the centralized sample database. In this framework, a given query is issued to the centralized sample database to retrieve a ranked list of sample documents. ReDDE makes an assumption that each sample document in this list repre-sents a number of (unseen) documents from the source that it belongs to. Based on that, a source score is calculated by aggregating all contribution from its sample documents. The amount of contribution is scaled up depending on the source X  X  size. Original ReDDE assigns a constant score for all documents on the top part of the returned list, and mul-tiplies that constant with the ratio of the estimated source size over the sample size. The obtained quantity is then used for aggregating source scores. The CRCS resource se-lection algorithm [27] follows the same approach, but varies the amount of contribution of each document by an exponen-tial decay function, as documents further down the ranked list have less contribution to its source.

ReDDE and many other algorithms of the same family such as CRCS and SUSHI [33] utilize a ranking on the cen-tralized sample database to estimate the relevance of avail-able sources. For algorithms of this family, they mostly vary in the way of defining a utility function for each document in the list. Having said that, the original ranked list from the centralized sample database plays an important role. Non-diversification algorithms target on building a centralized ranked list that covers as many relevant documents to the query as possible. In many cases, this may not pay enough attention to sources that cover multiple query aspects. For diversification purpose, we should be careful when select-ing a source that mainly contains documents relevant to an aspect that has been covered before.

The above observation suggests a way to achieve good search diversification results by constructing a ranked list that covers several aspects of the query. We call this ap-proach Diversification approach based on sample Documents (DivD). Instead of building a centralized ranked list that fo-cuses only on relevance, we construct a ranked list that offers more diversity. The goal is to reduce the contribution of a document (on behalf of its source) that may be relevant to the query, but offers less novelty in the overall ranking.
This approach can combine a wide range of resource selec-tion algorithms with any diversification algorithm that has been developed before, for instance, PM-2 [16] and xQuAD [26], which were mentioned in Section 2. A typical example Algorithm 1 Diversification Approach based on Sample Documents using ReDDE and PM-2 1: Initialize scores of all sources to 0 2: Rank documents of the centralized sample database by 3: Rerank that list by PM-2 4: for each document on top of the ranked list do 5: Add a constant score c to the source containing the 6: end for 7: return the ranked list of all sources based on their of combining the standard ReDDE and PM-2 is shown in Algorithm 1.

Some resource selection algorithms utilize the relevance score of each document in the centralized sample ranked list for ranking sources (e.g. ReDDE.top [2]). In the diversifica-tion approach that has been discussed so far, this relevance score can be replaced by the diversity score given by a diver-sification algorithm for ad hoc search. There are different interpretations about those diversity scores, depending on the assumptions made by the diversification algorithms. In our experiments, diversity scores can be used with most re-source selection algorithms effectively.
The second approach follows a different strategy than the first one. As presented in the previous section, the diversifi-cation approach based on sample documents works directly on the ranked list of sample documents, which is not a nat-ural component for resource selection algorithms similar to CORI. Indeed, several resource selection algorithms utilize summary statistics of a source to estimate its relevance to a query. It is not straightforward to apply a diversifica-tion method based on individual sample documents for those kinds of algorithms.

This paper proposes another diversification approach for resource selection that operates at the source level. More specifically, many existing diversification algorithms for ad hoc search rank documents by estimating their relevance with respect to each aspect of the query, and then harvest-ing this information in order to produce a ranked list that balances multiple query aspects. It is possible to design a similar process that uses the estimated source relevance with respect to each aspect of the query. More specifically, instead of building a diversified list of documents for a given query as in ad hoc search, we can build a diversified list of available sources in a similar manner. All estimations with respect to the documents can be replaced by estima-tions with respect to the sources. This resource selection approach for results diversification is called Diversification approach based on Source-level estimation (DivS).

An important step in DivS is therefore to compute the probability of relevance of a source with respect to a query aspect. Many existing resource selection algorithms are able to provide such information. CORI can directly provide a relevance estimation based on the big document assump-tion. ReDDE and CRCS do not provide direct estimations, but it is possible to use their source scores aggregated from sample documents for such a purpose. An example of the Algorithm 2 Source-based Diversification using ReDDE and PM-2 1: Rank all sources using standard ReDDE 2: for each aspect q i of query q and each source s j do 3: Estimate P ( s j | q i ), the probability of relevance of 4: end for 5: Rerank the list obtained in step 1 using PM-2 algorithm 6: return the ranked list of all sources based on their diversification approach based on source-level estimation us-ing ReDDE and PM-2 algorithms is presented in Algorithm 2.

Compared with diversification approach based on sample documents, the diversification approach based on source-level estimation can work with a wider range of resource selection algorithms. On the other hand, it requires multiple runs of a resource selection algorithm for all different aspects of a query, which is more time consuming than the former approach. In real-world applications, it is possible to design a parallel solution for multiple resource selection runs to speed up the process.
The two diversification approaches based on sample doc-uments and source-level estimation both utilize a specific resource selection algorithm and a diversification algorithm for ad hoc search. It is possible to combine the results of multiple resource selection algorithms for search result di-versification, which may provide better results by modeling complementary results from different algorithms. This sec-tion proposes a learning based classification approach. With some training information, this method can learn how to combine evidence supporting available sources from differ-ent resource selection algorithms for result diversification.
In particular, we adapt the classification-based approach that has been used for both vertical search [2] and federated search [20]. For collecting training information in learning the classification model, this approach generates a pseudo-relevant judgment of a source given a query by counting the number of relevant documents that the source contains. If the number is higher than some threshold value  X  , the source is considered to be relevant. In this paper, the train-ing dataset for result diversification consists of multiple in-stances, each of them represents a pair of a source and a query aspect. A source is considered relevant to a query aspect if it contains at least one document relevant to that aspect of the query. For each pair of a source and a query aspect, we utilize the source score information from the fol-lowing resource selection algorithms as features:
More specifically, the learning based approach attempts to naturally integrate evidence from two different views of resource selection algorithms, one based on big document assumption (Big Document and CORI), and the other based on aggregated information of sample documents (ReDDE.top and CRCS). In the experimental section, we will provide more analysis and some examples about why this combina-tion strategy may outperform each individual method.
All the features provided by the above algorithms are nor-malized for each query in order to achieve more consistency. Given the training dataset, it is possible to design a learning method that estimates P ( s | q k ) for a new query aspect q We choose logistic regression model as it has been shown to be among the best in many practical applications such as text categorization [18].

Let s j i be the binary variable that indicates the relevance of the i -th source to the query q j , i.e. s j i = 1 indicates rele-vance, and s j i = 0 indicates otherwise. Let f j i be the vector of all features returned by the resource selection algorithms mentioned above. We can then represent the relevance prob-ability of source s i given f j i by a sigmoid function  X  : where w denotes the combination weight vector.

Learning the combination weight w can be conducted by maximizing the log-likelihood function using the iterative re-weighted least squares method. The learned parameter can be then used to estimate the relevance probability P ( s | q for any particular aspect of a new user query. This probabil-ity becomes inputs for the diversification approach based on source-level estimation to rank the sources. Our hypothesis is that, if the learning model can provide a more accurate estimation than those produced by a single resource selec-tion algorithm, we can expect the learning based approach to generate more accurate results. This approach is denoted as LR-DivS, as it applies logistic regression within the di-versification approach based on source-level estimation.
The experiments in the paper have been conducted on the federated search testbed of the Clueweb English dataset. Table 1: Document statistics of the federated search testbed based on Clueweb English sources of docs of docs of docs of docs 2,780 151,161,188 48 3,417,805 54,364.27 The Clueweb dataset 1 is a large collection of web pages that has been used in several official tasks in the Text REtrieval Conference (TREC) tracks. Furthermore, the available in-formation of queries with multiple aspects and the corre-sponding relevance judgment has enabled the evaluation of search result diversification. The federated search testbed derived from Clueweb is publicly available 2 as an attempt to offer a large and realistic testing environment for federated search. This collection contains 2 , 780 information sources and about 151 million documents, which is much larger than most other testbeds that have been previously used in fed-erated search. Information sources are created by collect-ing web documents of the same domains (e.g. blogspot.com, about.com ), and the Wikipedia documents are clustered into 100 collections using the K-means clustering algorithm with two passes/iterations. More statistics information of this dataset is given in Table 1. It is noticed that Clueweb has also been used in distributed environment, albeit in a differ-ent problem setting [21].

We are aware of a recent dataset that has been proposed for federated search in web search environment [24]. How-ever, this dataset does not provide relevance judgments on multiple aspects of a query, and thus does not fully support experiments of this research at the moment.

Each information source in the testbed has been assigned a retrieval algorithm, which was chosen from a set of al-gorithms such as Inquery, Language Model and tf.idf in a round-robin manner. This strategy simulates the behav-ior that information sources in real world applications may use different types of retrieval algorithms. In order to build the centralized sample database, 300 documents have been sampled from each source via the source X  X  specific retrieval algorithm. The Indri retrieval algorithm [23] was used in all retrieval processes on the centralized sample database.
The queries used in our experiments consist of 148 queries from the TREC Web Track 2009[9], 2010[10], and 2011[11]. Diversity by Proportionality [16] (PM-2) and eXplicit Query Aspect Diversification (xQuAD) [26], which are among the state-of-the-art diversification algorithms, have been exam-ined in our study. In particular, we notice that the perfor-mance of PM-2 tends to be better than xQuAD in most of the metrics, thus we only report our results based on PM-2. In our implementation of this algorithm, we have chosen to rerank the top K = 500 documents in the centralized sample database, as well as in the final step of diversifying results from the centralized complete database (i.e., all documents in available sources). The parameter  X  in PM-2 is set to be 0 . 5 in all settings. http://boston.lti.cs.cmu.edu/Data/clueweb09 http://www.cs.purdue.edu/homes/dthong/clueweb/
An important component of PM-2 and many other search result diversification algorithms is the estimation of P ( d | q which is the relevance score of a document d with respect to a particular aspect q i of a query. These aspects are usu-ally not available in real-world applications. We follow the work of [16, 26] to report our results on two scenarios: all the aspects of a query are provided; or we can retrieve the aspects from a commercial search engine such as Google or Bing. For the first scenario, we use subtopics that come with TREC queries as aspects. For the second scenario, we send the original query to the search engine (Google in our experiments), and adopt its suggestions as query aspects. The second set contains 144 queries, as 6 original queries do not have search engine X  X  suggestions at the moment of our work. We name the first scenario  X  X iven Subtopics X  and the second  X  X uggestions X  in the results presented in Section 7.
A set of commonly used resource selection algorithms as described in section 5 has been utilized in the new research of result diversification in resource selection. They consist of ReDDE.top, CRCS with exponential decay function, Big Document and CORI.

With CRCS, the top 500 sample documents returned by the centralized sample database are considered. The expo-nential decay function of CRCS makes it more stable on the Clueweb collection, which exhibits a highly skewed distri-bution of source sizes. On the other side, ReDDE.top is more sensitive to noise in such an environment, as a not-so-relevant sample document from a really big source may result in too much bias in favor of that source and affects the final ranked list. Therefore, for ReDDE.top, we set the num-ber of top sample documents for each query to be smaller than CRCS X  X , chosen from the set { 50 , 100 , 150 ,  X  X  X } . We report the results using top 50 documents for ReDDE.top as it provides the most consistent performance.

To name the different methods, we use a prefix  X  X  X  for a diversification resource selection algorithm to indicate an approach based on sample documents, and a prefix  X  X  X  to indicate an approach based on source-level estimation. As discussed before, the Big Document and CORI algorithms have only the S versions. For all methods reported in the next section, we select up to 10 sources for each query.
All the proposed resource selection algorithms for search result diversification do not need any training data except the final approach of combining multiple resource selection algorithms in a learning model (LR-DivS). Therefore, for the algorithms other than LR-DivS, we report the results on all 148 queries from TREC Web Track 2009-2011. For LR-DivS, since it requires a training dataset, we use queries with TREC id less than or equal to 75 for training, and the rest for testing. We also report the results on the testing set of all other algorithms for comparison. For the scenario with suggested aspects of user queries, since there is no cor-responding relevance judgment, we train our model using the provided query subtopics/aspects and their correspond-ing relevance judgments. However, in the testing phase, the model is applied with features derived from the suggested aspects, i.e., we estimate P ( s | q i ) where q i is a query as-pect suggested by a commercial search engine. Finally, we evaluate all approaches with respect to TREC X  X  provided subtopics/aspects. This strategy is consistent with the eval-uation process in TREC Web Track [11].

The proposed new research has been evaluated at two lev-els: source selection and federated document retrieval.
An extensive set of experiments has been conducted for evaluating several approaches proposed in this paper, which are: the approach based on sample documents (DivD), the approach based on source-level estimation (DivS) and the learning based approach (LR-DivS). We conduct experiments on two levels for different purposes: source-level results for resource selection and document-level results for federated document retrieval. More specifically, the first subsection compares the performance of a diversification approach with standard resource selection algorithms. The second set of experiments in 7.2 compares multiple resource selection al-gorithms adapting DivD and DivS approaches. The third set of experiments in 7.3 demonstrates the advantage of the learning based approach (LR-DivS) over approaches with a single resource selection algorithm. The last set of exper-iments in 7.4 compares the document-level diversification results across all proposed approaches.
This subsection compares the performance of two stan-dard resource selection algorithms ReDDE.top and CRCS with their diversification counterparts at source selection level. In particular, we choose to study the first diversifi-cation approach based on sample documents (DivD) in this subsection as they are more related with standard ReDDE.top and CRCS algorithms, while more results of both diversifi-cation approaches will be presented shortly.
Table 2 shows the performance using all the R-based diver-sification metrics described in the previous section. Without diversification, it can be observed that the standard CRCS significantly outperforms standard ReDDE.top in all met-rics. This may be attributed to the fact that CRCS gener-ally selects more relevant sources to the query, which leads to a wider range of aspects being covered. The advantage of CRCS may come from using the exponential decay func-tion for document utility, which tends to be better than using document score as utility in ReDDE.top. When the document-based diversification approach is applied, it fur-ther increases the performance of the standard algorithms: D-ReDDE.top significantly outperforms ReDDE.top in its capacity of selecting diversified sources. As for CRCS, its di-versification version (i.e., D-CRCS) is also consistently bet-ter than the standard CRCS algorithm. The same observa-tion can be seen in both scenarios when the query aspects are given, or suggested by Web search.
This subsection studies the performance of the two pro-posed resource selection approaches (i.e., DivD and DivS) using several resource selection algorithms, including ReDDE-top, CRCS, Big Document and CORI. The results are pre-sented in Table 3. In all settings, the standard Big Docu-ment and standard CORI algorithms are outperformed by the other methods. Furthermore, both S-Big Document and S-CORI, which are under the same assumption of collaps-ing sample documents within a source, are inferior to S-ReDDE.top and S-CRCS. These results indicate that ReDDE-top and CRCS tend to be more effective in resource se-lection for result diversification than Big Document and CORI, which is consistent with previous research in feder-ated search for resource selection without diversification.
Both DivD and DivS approaches produce comparable re-sults when applied to ReDDE.top and CRCS. The D-CRCS version based on sample documents is better than its coun-terpart based on source-level estimation (i.e., S-CRCS), whereas the contrary is observed for ReDDE.top. In case of ReDDE.top, the difference is significant, which may be explained by the fact that when the original ranked list is short (only 50 for ReDDE.top), it is more difficult for the diversification algo-rithm to find a document that covers many query aspects, rather than finding a source that covers many aspects.
The results using the provided query aspects and sug-gested ones reveal an interesting observation. We notice that the performance of the two settings are quite compara-ble with little difference. One possible explanation is that, in federated environment, it may not need a perfect set of query aspects for selecting a diversified set of information sources, as sources are already somehow divided by different types of semantic topics. Since our goal is to select sources that can cover as many query aspects as possible, the re-source selection algorithms can do a reasonably good work as long as the suggestions of query aspects provide some meaningful interpretations of different aspects of the query.
This subsection compares the performance of the learning based classification approach with all other diversification approaches mentioned above. Since the classification ap-proach requires a set of training queries and does the test-ing on another set, we also report the results of all previ-ous methods on the testing queries for comparison. Table 4 presents the results. The performance of the standard resource selection algorithms and their diversification coun-terparts are better on the set of testing queries than on the set of all queries, due to the particular division of training and testing queries. The comparison between standard algo-rithms X  performance and those of diversification approaches on the testing queries raises similar observations as men-tioned in the previous subsections 7.1 and 7.2.

It can be seen that the classification approach provides the best performance over all metrics. It can be attributed to the fact that the learning based classification approach can harness the advantage of different algorithms, and com-bine them in an effective way. A typical example from our training set is the query  X  X bama family tree X  with its provided subtopic  X  X ind the TIME magazine photo essay Barack Obama X  X  Family Tree X . For ReDDE.top and CRCS, it is almost impossible to find a sample document containing all the keywords of the subtopic/aspect, if such a document does not exist in the sample database. On the other hand, Big Document and CORI can provide some useful hints for selecting sources by looking at all sample documents from each source as whole. For example, several different sam-ple documents of a source may contain various parts of the query. This is particularly useful for sources that cover a wide range of topics, for instance, Wikipedia. Another ex-ample is the query  X  X olvo X  with its provided subtopic  X  X ind a Volvo dealer X . Big Document and CORI can give hint to the sources that contain the words  X  X olvo X  and  X  X ealer X  from different sample documents. For those sources, the classifi-cation approach can utilize the complementary results for improving search results diversification.
This subsection compares the search diversification results in document-level of federated document retrieval. Given the top ten sources selected by the proposed algorithms for each query, the final ranked list generated from the sources is evaluated. One set of results compares algorithms without training over all queries. The other set of results is included for the test queries to compare LR-DivS with other algo-rithms. We omit the results of Big Document and CORI on the first set due to space limitation.
 Table 5 provides the results on all queries. The standard ReDDE.top algorithm falls behind the other models in its diversification capacity. Among all the methods without training, D-CRCS consistently outperforms the other meth-ods, which is consistent with its performance on the R-based diversification metrics in the source-level.

Table 6 provides the results on the test queries. Again, the same trend with algorithms without training can be observed. When some training information is available to learn how to combine multiple evidence, the classification approach LR-DivS consistently provides the best document-level diversification performance among all models.
Resource selection is an important research problem in federated search for selecting a small number of relevance sources for a given query. Various algorithms have been pro-0.652  X  0.698  X  0.484  X  0.658  X  0.598  X  0.645  X  0.438  X  0.612  X  0.672  X  0.721  X  0.499  X  X  0.674  X  0.649  X  0.694  X  0.477  X  0.659  X  0.589  X  0.635  X  0.434  X  0.604  X  0.671  X  0.720  X  0.491  X  X  0.677  X  respectively ( p &lt; 0 . 05).
 BO 0.645 B 0.438 BO 0.612 B B 0.591 B 0.351 B 0.561 B B 0.635 B 0.434 BO 0.604 B B 0.587 B 0.347 B 0.562 B ReDDE.top, S-CRCS, S-BigDoc, and S-CORI respectively. ( p &lt; 0 . 05). ( p &lt; 0 . 05).
 0.406  X  r 0.365  X  rR 0.152 rR 0.530  X  r 0.345  X  0.304  X  0.123  X  0.477  X  0.387  X  r 0.340  X  r 0.141  X  r 0.526  X  r 0.395  X  r 0.346  X  r 0.142  X  r 0.534  X  r 0.378  X  r 0.315  X  0.157  X  rR 0.535  X  r 0.333  X  0.286  X  0.129  X  0.469  X  0.359  X  r 0.300  X  0.144  X  r 0.512  X  r 0.376  X  r 0.320  X  0.147  X  r 0.530  X  r S-BigDoc, and S-CORI respectively ( p &lt; 0 . 05).  X  0.400  X  0.178  X 
BO 0.410 posed for resource selection in federated search, but limited attention has been paid to result novelty and diversification, which affects the effectiveness of existing algorithms. As far as we know, this paper proposes the first piece of research for incorporating search result diversification in resource se-lection for federated search.

A family of new evaluation metrics is first proposed for measuring search result diversification in resource selection, which combines some popular diversification metrics in ad hoc search with the recall-based evaluation metric in re-source selection. Two general approaches are then proposed for diversification in selecting relevant sources. The first ap-proach is based on sample documents, which ranks sample documents with respect to result diversification, and then utilizes the ReDDE framework for ranking the sources. The second approach is based on source-level estimation, which directly ranks each information source as a whole for result diversification. Furthermore, a learning based classification approach is proposed to combine multiple resource selection algorithms for more accurate diversification results.
An intensive set of empirical studies has been conducted to evaluate the proposed research on the Clueweb feder-ated search dataset. Both the approach based on sample documents and on source-level estimation can outperform traditional resource selection algorithms in result diversi-fication in both source-level for resource selection and in the document-level for federated document retrieval. More-over, the learning based approach, which combines outputs of multiple resource selection algorithms for result diversi-fication, has been shown to generate the best results when some training data is available.

There are several possible directions to pursue in the fu-ture. The learning based method in this paper utilizes a simple model for combining outputs of multiple algorithms for result diversification, while a more sophisticated learning method may be more effective. Furthermore, it is an inter-esting topic to design new result merging algorithms with the focus on result diversification. This work is partially supported by NSF research grants IIS-0746830, CNS-1012208 and IIS-1017837. This work is also partially supported by the Vietnam Education Foun-dation, the Center for Science of Information (CSoI), an NSF Science and Technology Center, under grant agreement CCF-0939370, and a travel grant from the ACM Special In-terest Group on Information Retrieval. Any opinions, find-ings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily re-flect those of the sponsors.
