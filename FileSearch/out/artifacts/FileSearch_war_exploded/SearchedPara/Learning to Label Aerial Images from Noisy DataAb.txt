 Volodymyr Mnih vmnih@cs.toronto.edu Department of Computer Science, University of Toronto Geoffrey Hinton hinton@cs.toronto.edu Department of Computer Science, University of Toronto Information extracted from photographs of the earth X  X  surface that were taken by airborne sensors has found applications in a wide range of areas including ur-ban planning, crop and forest management, disaster relief, and climate modeling. Relying on human ex-perts for extracting information from aerial imagery is both slow and costly, so automatic aerial image inter-pretation has received much attention in the remote sensing community. So far, there are only a few, semi-automated systems that operate in limited domains (Mayer, 2008), but recent applications of large-scale machine learning to aerial image interpretation have produced object detectors with impressive levels of ac-curacy on challenging high-resolution data (Kluckner &amp; Bischof, 2009; Kluckner et al., 2009; Mnih &amp; Hinton, 2010).
 In machine learning applications, aerial image inter-pretation is usually formulated as a pixel labeling task. The goal is to produce either a complete semantic seg-mentation of an aerial image into classes such as build-ing, road, tree, grass, and water (Kluckner &amp; Bischof, 2009; Kluckner et al., 2009) or a binary classification of the image for a single object class (Dollar et al., 2006; Mnih &amp; Hinton, 2010). In both scenarios, the avail-ability of accurately labeled data for training tends to be the limiting factor. Hand-labeled data tends to be reasonably accurate, but the cost of hand-labeling and the lack of publicly available hand-labeled datasets strongly restricts the size of the training and test sets for aerial image labeling tasks.
 At present, maps of many major cities not only pro-vide the locations of most roads and parks, but also the locations of buildings. So one alternative to using hand-labeled data is to use maps from projects such as OpenStreetMap for constructing the labels. For ob-ject types covered by these maps, it is now possible to construct datasets that are much larger than the ones that have been hand-labeled. While the use of these larger datasets has improved the performance of machine learning methods on some aerial image recognition tasks (Mnih &amp; Hinton, 2010), datasets con-structed from maps suffer from two types of label noise:  X  Omission noise occurs when an object that ap- X  Registration noise occurs when the location of The presence of these kinds of errors in the training labels significantly reduces the accuracy of classifiers trained on this data.
 In this paper, we show how one can deal with the pres-ence of both kinds of noise in the training labels. We present two robust loss functions, one that reduces the effect of omission errors on the resulting classifier, and one that accounts for both omission and registration errors in the training data. After incorporating these improvements into a deep learning framework we ob-tain a substantial improvement in the state of the art on the largest and most challenging road detection dataset. We will consider aerial image labeling tasks with bi-nary labels, where the goal is to label all pixels be-longing to an object class of interest with 1 X  X  and all other pixels with 0 X  X . Road and building detection are two examples of such problems. We adopt a problem setup that closely resembles the patch-based approach used for road detection in (Mnih &amp; Hinton, 2010). Let S be an aerial/satellite image and  X  M be the corre-sponding map image of equal size produced from the given map 1 .  X  M i,j = 1 whenever the pixel at location ( i,j ) contains the object of interest and  X  M i,j = 0 oth-erwise. The goal is to learn to predict patches of map  X  M from patches of S , and following a probabilistic ap-proach, we model the distribution where n ( I i,j ,w ) is the w  X  w patch of image I centered at location ( i,j ). Typically w m is set to be smaller than w s because some context is required to predict the value of a map pixel. While w m can be set to 1 to predict one pixel at a time, it is generally more efficient to predict a small patch of labels from the same context. To simplify notation, we will use vectors s and  X  m to denote the aerial image patch n ( S i,j ,w s ) and the map patch n (  X  M i,j ,w m ) respectively. Following earlier work (Mnih &amp; Hinton, 2010) we assume conditional inde-pendence of the map pixels and model the map distri-bution using a neural network. We assume that each p (  X  m i | s ) is a Bernoulli distribution whose mean value is deter-mined by the i th output unit of the neural network. We will refer to this as the noise free model. 3.1. Network Architecture Unlike earlier work, we use a deep neural network to model the map distribution. The input to the neural network is a w s by w s patch of an aerial image encoded in the RGB color space, while the output is a w m by w m map patch. The input layer is followed by three hidden layers all of which make use of the rectified linear activation function (Nair &amp; Hinton, 2010), for which the output is defined as max(0 ,input ). Rectified linear units have been found to be better than logis-tic units on various image classification tasks and we find that this advantage also exists on image labeling tasks (Nair &amp; Hinton, 2010).
 The first two hidden layers in our network are locally connected layers, in which each hidden unit is con-nected to only a small subset of the input units. To precisely define the connectivity pattern, assume that the input units of a locally connected layer make up a w in  X  w in image, possibly consisting of multiple chan-nels. The input image is divided into evenly spaced filter sites by moving a w f  X  w f window over the im-age by a stride of w str vertically and horizontally, for ferent set of f filters of size w f  X  w f and consisting of the same number of channels as the input image is applied at each filter site. Hence, a single locally con-nected layer results in f  X  (( w in  X  w f ) /w str + 1) 2 den units. The hidden units of one locally connected layer can then act as the input to another locally con-nected layer by viewing the hidden units as a square image with f channels and width ( w in  X  w f ) /w str + 1. Unlike a convolutional or tiled net, there is no weight-sharing of any kind.
 The third hidden layer is fully connected, with each unit connected to every unit in the preceding hidden layer. The output layer consists of w 2 p logistic units for which the output is 1 / (1 + exp(  X  input )). Typically, w p = w m and each output unit models the probability that the corresponding pixel in the w m by w m output map patch belongs to the class of interest.
 The values of the parameters such as the number of filters f , their width w f , and stride w str should vary from problem to problem. The settings we use in our experiments are described in Section 6. 3.2. Preprocessing We preprocess each input patch by subtracting the mean value of the pixels in that patch from all pix-els and then dividing by the standard deviation found over all pixels in the dataset. This type of prepro-cessing achieves some contrast normalization between different patches. 3.3. Learning We learn the parameters of the neural network by min-imizing the negative log likelihood of the training data. For the model given in Equation 2 the negative log like-lihood takes the form of a cross entropy between the patch  X  m derived from the given map and the predicted patch  X  m We optimize this objective function using mini-batched stochastic gradient descent with momentum. 3.4. Discussion of Architecture Our general architecture has some similarity to various convolutional architectures that have recently become popular (Lee et al., 2009; Kavukcuoglu et al., 2010). In fact, the locally connected layers can be seen as convo-lutional layers with untied weights. Weight-sharing in convolutional architectures is advantageous on smaller datasets because it helps reduce overfitting by restrict-ing the number of parameters, but we do not need such a restriction because the abundance of labels, com-bined with random rotations, allows us to avoid over-fitting by training on millions of labeled aerial image patches. Like convolutional architectures, our locally connected architecture is computationally and statisti-cally more efficient than a fully connected architecture. 3.5. Unsupervised Pretraining Initializing neural networks using unsupervised learn-ing methods is known to improve performance on a va-riety of vision tasks (Hinton et al., 2006; Krizhevsky, 2011; Mnih &amp; Hinton, 2010). We use unsupervised pre-training to initialize the deep neural network following the approach described in (Nair &amp; Hinton, 2010) for training Restricted Boltzmann Machines with rectified linear units. Omission noise, as shown in Figure 1(a), occurs when some map pixels are labeled as not belonging to the object class of interest when they, in fact, do. When trained on data containing a substantial number of such pixels a classifier will be penalized for correctly predicting the value of 1 for pixels affected by omission noise. This will cause a classifier to be less confident and potentially increase the false negative rate. We propose using a robust loss function that explicitly models asymmetric omission noise in order to reduce its effect on the final classifier. The noise-free model of the data from Equation 2 assumes that the observed labels  X  m are generated directly from the aerial im-age s . In order to model label noise, we assume that a true, uncorrupted, and unobserved map patch m is first generated from the aerial image patch s according to some distribution p ( m | s ). The corrupted, observed map  X  m is then generated from the uncorrupted m ac-cording to a noise distribution p (  X  m | m ). For simplicity, our omission model assumes that conditioned on m , all components of  X  m are independent and that each  X  m i is independent of all m j for j 6 = i . The observed map distribution that corresponds to this model can then be obtained by marginalizing out m , leading to The noise distribution p (  X  m i | m i ) is assumed to be the same for all pixels i , and is determined by parameters For modeling omission noise we set  X  0  X  1 because the probability that the observed label  X  m i is 1 given that the true label m i is 0 should be very close to 0, while the probability that the observed  X  m i is 0 given that the true label m i is 1 should still be small but not as close to 0 as  X  0 .
 We refer to this model as the asymmetric Bernoulli noise model, or the ABN model for short. In the noise-free scenario we described in the previous section, the map distribution in Equation 2 was modelled directly by a deep neural network. In the noisy setting, we can use the neural network to model the true map distri-bution p ( m | s ). Learning can still be done efficiently by minimizing the negative log probability of the training data under the ABN model given in Equation 5. Since the ABN model factorizes over the pixels i and there is only a single Bernoulli latent variable m i for each pixel i , the derivative of the negative log probability can be found directly.
 In the noise-free scenario, the derivative of the nega-tive log probability with respect to the input to the i th output unit of the neural network takes the form  X  m i  X   X  m i . The learning procedure is trying to make the prediction  X  m i close to the observed label  X  m i der the ABN model, this derivative takes the form p ( m i = 1 |  X  m i , s )  X   X  m i . Hence, the learning procedure is trying to make the prediction  X  m i close to the pos-terior probability that the unobserved true label m is 1. This has the effect that the neural network gets penalized less for making a confident but incorrect pre-diction. Figure 2 demonstrates how the derivatives for the noise-free and the ABN models differ as a function of the prediction  X  m i . Registration noise occurs when an aerial image and the corresponding map are not perfectly aligned. As shown in Figure 1(b), the error in alignment between the map and the aerial image can vary over the dataset and cannot be corrected by a global translation. Our approach attempts to eliminate registration errors us-ing local translations of the labels.
 We extend the robust loss function we introduced in the previous section for dealing with omission noise to also handle local registration errors. As with the ABN model, we introduce a generative model of the observed map patches. On a high level, the generative model works by first generating an uncorrupted and perfectly registered map from the aerial image, then selecting a random subpatch of the true map, and fi-nally generating the observed map by corrupting the selected subpatch with asymmetric noise. More for-mally, the generative process is as follows: 1) An uncorrupted and perfectly registered true map patch m of size w m 0  X  w m 0 is generated from s accord-ing to p ( m | s ). We set w m 0 = w m + 2 t max where t is the maximum possible registration error/translation between the map and aerial image measured in pixels. 2) A translation variable t is sampled from some distri-bution p ( t ) over T + 1 possible values 0 ,...,T . In this paper, we use T = 8, where t = 0 corresponds to no translation while 1 ,...,T index 8 possible translations by t max pixels in the vertical and horizontal directions as well as their combinations (see Figure 3). 3) An observed map is sampled from the translational noise distribution where Crop ( m ,t ) selects a w m by w m subpatch from the w m 0 by w m 0 patch m according to the translation variable t as shown in Figure 3, and p ABN (  X  m i | m i the pixelwise asymmetric binary noise model defined in the previous section.
 For simplicity we assume that p ( t = i ) = (1  X  p ( t = 0)) /T for all i 6 = 0 and parameterize p ( t ) using only a single parameter  X  t = p ( t = 0). Hence, we use a to-tal of four parameters: t max ,  X  t , and two parameters needed to define p ABN (  X  m i | m i ). We refer to this gen-erative model as the translational asymmetric binary noise model, or the TABN model for short. 5.1. Learning The observed map distribution under the TABN model is given by We set the parameters of p ( t ) and p (  X  m | m ,t ) using a validation set and learn the parameters of p ( m | s ) by minimizing the negative log likelihood in Equation 8 using the EM-algorithm. The required EM updates can be performed efficiently.
 M-step: Since p ( m | s ) is modelled by a neural net-work, we cannot do a full M-step and instead do an approximate partial M-step by doing a single gradient descent update of the neural network parameters on a mini-batch of training cases. The required derivative of the expected log likelihood is  X  where  X  m i is value of the i th output unit of the neural network and x i is the input to the i th output unit. The updates for all weights of the neural network can be computed from the above equation using backprop-agation.
 E-step: The role of the E-step is to compute p ( m i |  X  m , s ) for use in the M-step, and as we will show, this computation can be done in time T  X  w 2 m by ex-ploiting the structure of the noise model.
 We first define C t to be the set of indices of pixels of m that are cropped for transformation t . Since this set will have w 2 m entries we will slightly abuse notation and also use it to index into  X  m . By defining the observed map distribution can be rewritten as p (  X  m | s ) = P t p ( t )  X  P t . Now using the identity p ( m i |  X  m , s ) = where m  X  i denotes all entries of m other than i , p ( m i |  X  m , s ) can be expressed as "
X We note that the width of patches to which the noise model is applied ( w m 0 ) can be different from the width of patches predicted by the neural network ( w p ). This allows us to decouple the size of patch for which regis-tration error is assumed to be constant from the size of predicted patch. In this paper we used w m 0 = 4 w p . In this case, we construct the w m 0  X  w m 0 patch  X  m out of 16 non-overlapping w p  X  w p patches predicted by the neural net. We then find the w m 0  X  w m 0 patch of pos-terior marginals p ( m i |  X  m , s ) as described above, break it up into 16 non-overlapping w p  X  w p subpatches, and backpropagate the derivatives from all the subpatches through the neural network. 5.2. Model Demonstration While we delay quantitative results until Section 6, Figure 4 shows some qualitative results that demon-strate the workings of the translational noise model. This example was constructed using a road detector trained on poorly registered road data with the trans-lational noise model. The model uses the target la-bels produced from the given map (top row) and the model X  X  predictions (middle row) to obtain realigned data (bottom row) through the posterior p ( m |  X  m , s ). 6.1. Datasets and Metrics We evaluate our deep learning framework along with the proposed noise models on the problem of road de-tection. We use the URBAN1 and URBAN2 datasets that were used in (Mnih &amp; Hinton, 2010) to evaluate a road detection system because these are the largest and arguably most challenging aerial image labeling datasets with published results. The URBAN1 dataset consists of over 500 square kilometers of training data and 48 square kilometers of test data at a resolution of 1.2m per pixel. This dataset contains both urban and suburban areas of a large city and has relatively few registration problems but does contain omission errors. The URBAN2 dataset consists of a 28 square kilome-ter subset of a different city than the one covered by URBAN1 and has significant registration problems in addition to containing omission errors. In earlier work, the URBAN2 dataset was only used for testing due to its registration problems. We created an additional training set out of roughly 250 square kilometers of imagery by using the entire city covered by URBAN2 in order to study the effects of registration noise on the training process.
 We follow the standard evaluation protocol for road detection problems which involves computing preci-sion/recall plots (Wiedemann et al., 1998). Since the ground truth road locations can suffer from registra-tion problems, it is common practice to use a buffer when computing precision and recall. If a buffer of  X  pixels is used, then a pixel predicted as road is con-sidered to be correctly classified if there is a true road pixel within  X  pixels. Similarly, a true road pixel is considered to be correctly classified for computing re-call if there is a predicted road pixel within  X  pixels. We use a buffer of 3 pixels in our evaluations. 6.2. Experimental Setup To make our results comparable to the best published results on URBAN1 and URBAN2 we used an experi-mental setup similar to the one used in (Mnih &amp; Hin-ton, 2010). We trained neural networks to predict 16 by 16 patches of map from 64 by 64 patches of aerial image. In most experiments we used the three hid-den layer neural net described in Section 3. The first hidden layer used filter width 12 with stride 4 and 64 filters at each site. The second hidden layer used filter width 4 with stride 2 and 256 filters at each site. The third hidden layer had 4096 hidden units.
 The best published results on this data (Mnih &amp; Hin-ton, 2010) make use of a postprocessing procedure that improves the predictions of a base model by training a new predictor that takes a patch of predictions of the base model as input instead of the aerial image. This procedure tends to fill in short gaps in the pre-dicted road network as well as remove spurious bits of road. The neural network we use for post-processing predicts a 16 by 16 map patch from a 64 by 64 patch of predictions. The network has two locally connected hidden layers with the same connectivity as the first two layers of our three layer network.
 We trained all models using mini-batch stochastic gra-dient descent with a fixed learning rate. We used mini-batches of size 64 and momentum of 0.9. Model pa-rameters were either tuned on the URBAN1 valida-tion set (filter sizes and strides) or set once (number of filters or hidden units) and held fixed for all ex-periments. All post-processing nets were trained for 8 epochs while base predictor networks were trained for 20 epochs. Our models were trained using con-sumer GPUs and took about a day to train. We used the CudaMat (Mnih, 2009) and Gnumpy (Tieleman, 2010) Python libraries to implement the algorithms. 6.3. Training on URBAN1 We train three different models on the URBAN1 train-ing set and evaluate them on the URBAN1 and UR-BAN2 test sets. Figure 5 shows the precision recall curves on the test sets. The model denoted by NF is the three layer network described in the previous sec-tion trained using the noise-free model of Equation 2. To investigate the effectiveness of the ABN loss func-tion we used it to train a model with the same archi-tecture as NF. For this model, shown as ABN, we used the parameter values  X  0 = 0 . 001 and  X  1 = 0 . 05. There is a clear improvement in both precision and recall from training with this robust loss function. Neural networks trained with the ABN loss tend to be much more confident in their predictions because they are not penalized as much on the noisy training cases. We also experimented with using the TABN loss function for training on the URBAN1 data but found that this led to nearly identical performance as with the ABN loss. One possible reason for this is that the ABN loss function offers some robustness to translation noise in addition to omission noise. Since the URBAN1 train-ing set has only minor registration problems the added robustness from using the TABN loss does not seem to help.
 The best published results for URBAN1 and URBAN2 (Mnih &amp; Hinton, 2010) are included in the plot as MH2010. This model uses a fully connected single hidden layer neural net as a base predictor, followed by a fully connected post processing net. For high re-call levels, our ABN model outperforms the MH2010 model by a wide margin on both datasets. When we also train a post-processing neural network (ABN-P) on the outputs of the ABN model we see a further im-provement in precision and recall. At low recall levels, MH2010 and NF outperform ABN and ABN-P on the URBAN2 test set, likely because the test set is poorly registered. The noise model allows ABN and ABN-P to make more confident predictions, which lowers pre-cision on parts where the ground truth is misaligned. 6.4. Training on URBAN2 While the URBAN1 dataset contains relatively few registration problems, with most road labels within one or two meters of the true locations, road cen-terlines in the URBAN2 dataset are often more than 5 meters away from their true locations. We train four models on the URBAN2 training set and evalu-ate them on the URBAN1 test set. We do not evaluate on the URBAN2 test set because we used the entire city for training due to it being quite small. Figure 6 shows precision/recall curves on the URBAN1 test set for four models trained on the URBAN2 train-ing set and demonstrates the clear advantage of using robust loss functions on this task. The neural network trained without a noise model, denoted NF, does very poorly, achieving recall of about 0.3 at 0.9 precision. The neural network trained with the ABN model, de-noted ABN, achieves double the recall at 0.9 precision when compared to NF. Unlike for models trained on URBAN1, we get a huge improvement in the precision recall curve from training a network with the TABN call of roughly 0.75, which is 2.5 times higher than that of the same network trained without a noise model (NF). Finally, we trained a post-processing network on the outputs of TABN, improving recall to 0.8 for precision 0.9. This last network, denoted TABN-P, was also trained using the translational noise model. While the performance of these models is worse than for models trained on URBAN1, a gap in performance is to be expected. In addition to its registration prob-lems, the URBAN2 training set is less than half the size of the URBAN1 training set and covers mostly suburban areas which do not resemble the more urban areas in the URBAN1 test set. We are not aware of any work related to image labeling that specifically addresses learning from noisy labels. (He &amp; Zemel, 2008) pointed out that the lack of ac-curately labeled data is a bottleneck in general image labeling and considered the related problem of learn-ing to label images from incomplete observations. The presence of label noise in the aerial image data was ad-dressed in (Kluckner &amp; Bischof, 2009), but it was only used to motivate the use of random forests which can tolerate some mislabeling of the data without having an explicit noise model.
 The general problem of learning from noisy labels has been considered in a variety of settings. In particular, the idea of modeling true unobserved labels as latent variables is widely used. For example, (Pal et al., 2007) also uses an asymmetric Bernoulli noise model, but in the context of information extraction.
 The idea of automatically aligning data has been ex-plored in unsupervised learning, where (Frey &amp; Jojic, 1999) incorporated latent variables encoding spatial transformations into a mixture modeling framework, allowing it to simultaneously align and cluster images. Maps provide a very rich source of labels for systems that learn to interpret aerial images and this makes it possible to use systems with a very large number of parameters such as deep neural networks trained on large image patches. However, the performance of these systems is significantly degraded by missing labels and poor registration. We have shown that it is possible to significantly improve performance by using robust loss functions that treat the target labels as noisy observations of true labels. During learning, a version of the EM algorithm is used to infer the true labels and these inferred labels are then used as the targets for training the parameters.
 Dollar, Piotr, Tu, Zhuowen, and Belongie, Serge. Su-pervised learning of edges and object boundaries. In Proceedings of the 2006 IEEE Computer Society
Conference on Computer Vision and Pattern Recog-nition , pp. 1964 X 1971, 2006.
 Frey, Brendan J. and Jojic, Nebojsa. Estimating mix-ture models of images and inferring spatial transfor-mations using the em algorithm. In In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pp. 416 X 422, 1999.
 He, Xuming and Zemel, Richard S. Learning hybrid models for image annotation with partially labeled data. In NIPS , pp. 625 X 632, 2008.
 Hinton, Geoffrey E., Osindero, Simon, and Teh, Yee-Whye. A fast learning algorithm for deep belief nets. Neural Comput. , 18:1527 X 1554, July 2006.
 Kavukcuoglu, Koray, Sermanet, Pierre, Boureau, Y-Lan, Gregor, Karol, Mathieu, Micha  X el, and LeCun,
Yann. Learning convolutional feature hierachies for visual recognition. In Advances in Neural Informa-tion Processing Systems (NIPS 2010) , 2010.
 Kluckner, Stefan and Bischof, Horst. Semantic classi-fication by covariance descriptors within a random-ized forest. In Computer Vision Workshops (ICCV) , pp. 665 X 672. IEEE, 2009.
 Kluckner, Stefan, Mauthner, Thomas, Roth, Peter M., and Bischof, Horst. Semantic classification in aerial imagery by integrating appearance and height infor-mation. In ACCV , volume 5995 of Lecture Notes in Computer Science , pp. 477 X 488. Springer, 2009. Krizhevsky, Alex. Convolutional deep belief networks on cifar-10. Technical report, University of Toronto, 2011.
 Lee, Honglak, Grosse, Roger, Ranganath, Rajesh, and
Ng, Andrew Y. Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations. In Proceedings of the 26th Inter-national Conference on Machine Learning , pp. 609 X  616, 2009.
 Mayer, Helmut. Object extraction in photogrammetric computer vision. ISPRS Journal of Photogrammetry and Remote Sensing , 63(2):213 X 222, March 2008. Mnih, Volodymyr. Cudamat: a CUDA-based matrix class for python. Technical Report UTML TR 2009-004, Department of Computer Science, University of Toronto, November 2009.
 Mnih, Volodymyr and Hinton, Geoffrey. Learning to detect roads in high-resolution aerial images. In Pro-ceedings of the 11th European Conference on Com-puter Vision (ECCV) , September 2010.
 Nair, Vinod and Hinton, Geoffrey E. Rectified linear units improve restricted boltzmann machines. In ICML , pp. 807 X 814, 2010.
 Pal, Chris, Mann, Gideon, and Minerich, Richard.
Putting semantic information extraction on the map. In Sixth International Workshop on Informa-tion Integration on the Web , 2007.
 Tieleman, T. Gnumpy: an easy way to use GPU boards in Python. Technical Report UTML TR 2010-002, University of Toronto, Department of Computer Science, 2010.
 Wiedemann, Christian, Heipke, Christian, Mayer, Hel-mut, and Jamet, Olivier. Empirical evaluation of au-tomatically extracted road axes. In Empirical Evalu-ation Techniques in Computer Vision , pp. 172 X 187,
