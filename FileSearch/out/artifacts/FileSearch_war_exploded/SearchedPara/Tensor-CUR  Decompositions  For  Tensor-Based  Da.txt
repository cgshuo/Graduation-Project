 Motivated by numerous applications in which the data may be modeled by a variable subscripted by three or more in-dices, we develop a tensor-based extension of the matrix CUR decomposition. The tensor-CUR decomposition is most relevant as a data analysis tool when the data consist of one mode that is qualitatively different than the others. In this case, the tensor-CUR decomposition approximately ex-presses the original data tensor in terms of a basis consist-ing of underlying subtensors that are actual data elements and thus that have natural interpretation in terms of the processes generating the data. In order to demonstrate the general applicability of this tensor decomposition, we ap-ply it to problems in two diverse domains of data analysis: hyperspectral medical image analysis and consumer recom-mendation system analysis. In the hyperspectral data appli-cation, the tensor-CUR decomposition is used to compress the data, and we show that classification quality is not sub-stantially reduced even after substantial data compression. In the recommendation system application, the tensor-CUR decomposition is used to reconstruct missing entries in a user-product-product preference tensor, and we show that high quality recommendations can be made on the basis of a small number of basis users and a small number of product-product comparisons from a new user.
 Categories and Subject Descriptors: E.m [Data] : Mis-cellaneous; H.m [Information Systems] : Miscellaneous General Terms: Algorithms, Experimentation Keywords: CUR Decomposition, Tensor CUR, Hyperspec-tral Image Analysis, Recommendation System Analysis
Novel algorithmic methods to structure large data sets are of continuing interest. A particular challenge is pre-sented by tensor-based data, i.e., data which are modeled  X  Part of this work was performed while at the Department of Mathematics, Yale University, New Haven, CT, USA 06520. Copyright 2006 ACM 1-59593-339-5/06/0008 ... $ 5.00. byavariabl esubscripte dbythre eormor eindice s[28,19, 30, 39, 6]. Numerous examples suggest themselves, but to guide the discussion consider the following three. First, in internet data applications, if one is studying the properties of a large time-evolving graph, the data may consist of a graph or its adjacency matrix sampled at a large number of sequential time steps, in which case A ijk may represent the weight of the edge between nodes i and j at time step k . Second, in biomedical data applications, if one is studying cancer diagnosis, the data may consist of a large number of hyperspectrally-resolved biopsy images, in which case A ijk may represent the absorbed or transmitted light intensity of a biopsy sample at pixel ij at frequency k .Third,incon-sumer data applications, if one is studying recommendation systems, the data may consist of product-product preference data for a large number of users, in which case A ijk may be  X  1, depending on whether product i or j is preferred by user k . Tensor-based data are particularly challenging due to their size and since many data analysis tools based on graph theory and linear algebra do not easily generalize.
When compared with algorithmic results for data mod-eled by either matrices or graphs, algorithmic results for data modeled by multi-mode tensors are modest. For exam-ple, even computing the rank of a general tensor A (defined as the minimum number of rank-one tensors into which A can be decomposed) is an NP-hard problem [20]. On the other hand, the model proposed by Tucker [39], as well as the related the  X  X anonical decomposition X  [6] or the  X  X aral-lel factors X  model [19], have a long history in applied data analysis [24, 25, 26, 28]. They provide exact or approximate decompositions for higher-order tensors. Recent research has focused on the relationship between these data tensor models and efforts to extend linear algebraic notions such as the Singular Value Decomposition to multi-mode data tensors [28, 29, 30, 31].
 A seemingly unrelated line of work has focused on matrix CUR decompositions [10, 13, 12]. As discussed in more de-tail in Section 2.2, a matrix CUR decomposition provides a low-rank approximation of the form A  X   X  A = CUR ,where C is a matrix consisting of a small number of columns of A , R is a matrix consisting of a small number of rows of A , and U is an appropriately-defined low-dimensional encoding matrix [10]. Thus, a CUR matrix decomposition provides a dimensionally-reduced low-rank approximation to the origi-nal data matrix A that is expressed in terms of a small num-ber of actual columns and a small number of actual rows of the original data matrix, rather than, e.g., orthogonal linear combinations of those columns and rows.
In this paper, we extend a recently-developed and prov-ably accurate matrix CUR decomposition to tensor-based data sets in which there is a  X  X istinguished X  mode, and we apply it to problems in two of the three data set domains mentioned previously. When applied to hyperspectral image data, we use tensor-CUR to perform compression in order to run a classification on a more concise input, and when applied to recommendation system data we use tensor-CUR to perform reconstruction in the absence of the full input.
By a  X  X istinguished X  mode, we mean a mode that is qual-itatively different than the other modes in an application-dependent manner. The most appropriate data structure for a data set consisting of, e.g., a time-evolving internet graph or a set of hyperspectrally-resolved biopsy images or user-product-product preference data for consumers, depends on the application and is a matter of debate. Nevertheless, we will view such a data set as a tensor, albeit one in which one of the modes is  X  X istinguished. X  For example, in these three applications, the distinguished mode would be the mode de-scribing, respectively, the temporal evolution of the graph, the frequency or spectral variation in the images, and the users. The tensor-CUR decomposition computes an approx-imation to the original data tensor that is expressed as a linear combination of subtensors of the original data tensor. As we shall see, since these subtensors are actual data el-ements, rather than, e.g., more complex functions of data elements, in many cases they lend themselves more readily to application-specific interpretation.
The following theorem is a fundamental result from linear algebra that is widely-used (often via the related Principal Components Analysis) in data analysis.

Theorem 1. If A  X  R m  X  n , then there exist orthogonal matrices U =[ u 1 u 2 ...u m ]  X  R m  X  m and V =[ v 1 v 2 where  X   X  R m  X  n ,  X  =min { m, n } and  X  1  X   X  2  X  ...  X  0 . Equivalently, The three matrices U , V , and  X  constitute the Singular Value Decomposition (SVD) of A . If we define r by  X  1  X   X  2  X  ...  X   X  r &gt; X  r +1 = ... =  X   X  =0,thenrank( A )= r .In addition, if k  X  r and we define then the distance (as measured, e.g., by the Frobenius norm  X  approximation to A is minimized by A k . More formally, we have the following theorem.

Theorem 2. If A  X  R m  X  n and A k  X  R m  X  n is defined by (3), then For more details about these results, see [16].
Recent work in theoretical computer science, numerical linear algebra, and statistical learning theory [10, 12, 37, 38, 3, 18, 17, 40, 13] has focused on low-rank matrix decompo-sitions with structural properties that satisfy the following definition:
Definition 1. Let A be an m  X  n matrix. In addition, let C be an m  X  c matrix whose columns consist of a small number c of columns of the matrix A , let R be an r  X  n matrix whose rows consist of a small number r of rows of the original matrix A , and let U be a c  X  r matrix. Then is a column-row-based low-rank approximation ,ora CUR approximation ,to A if it may be explicitly written as Several things should be noted about this definition. First, for data applications, we prefer not to provide too precise a characterization of what we mean by a  X  X mall X  number of columns and/or rows, but one should think of r, c m, n . For example, they could be constant, independent of m and n , logarithmic in the size of m and n , or simply a large con-stant factor less than m, n . Second, since the approximation is expressed in terms of a small number of columns and rows of the original data matrix, it will provide a low-rank approx-imation to the original matrix, although one with structural properties that are quite different than those provided by truncating the SVD. Third, a CUR approximation approxi-mately expresses all of the columns of A in terms of a linear combination of a small number of  X  X asis columns, X  and sim-ilarly for the rows.

Finally, and most relevant for the present paper, note that a CUR matrix decomposition has structural properties that are auspicious for its use as a tool in the analysis of large data sets. For example, if the data matrix A is large and sparse but well-approximated by a low-rank matrix, then C and R (consisting of actual columns and rows) are sparse, whereas the matrices consisting of the top left and right sin-gular vectors will not in general be sparse. In addition, in many applications, interpretability is important; practition-ers often have an intuition about the actual columns and rows that they fail to have about linear combinations of (up to) all the columns or rows.

The following algorithmic result regarding a matrix CUR approximation was recently proven [10].

Theorem 3. There exists a randomized algorithm (see the LinearTimeCUR algorithm of [10]) that takes as input an m  X  n matrix A and a fixed rank parameter k , and that returns as output an m  X  c matrix C consisting of c columns of A ,an r  X  n matrix R consisting of r rows of A ,andan c  X  r matrix U . The columns/rows are randomly sampled in c / r independent trials according to a judiciously-chosen probability distribution depending on the Euclidean norm of the corresponding column/row. If c = O ( k log(1 / X  ) / 4 r = O ( k/ X  2 2 ) ,then holds with probability at least 1  X   X  . The algorithm requires O ( m + n ) additional time and scratch space after reading the matrix A twice from external storage.
 Our two tensor-CUR algorithms are tensor-based extensions of this matrix algorithm. For more details about these re-sults, see [8, 9, 10, 13].
Tensors are a natural generalization of matrices. We shall use calligraphic letters to denote higher-order or multi-mode tensors with d&gt; 2 modes. For example, let A X  R n 1  X  n 2 be a d -mode tensor of size n 1  X  n 2  X  X  X  X  X  n d . In addition, let  X   X  X  1 ,...,d } beaparticularmodeandlet N  X  =
Consider the following definitions: Define the matrix A [ the  X  th coordinate of A while leaving the rest fixed. We refer to the (usually implicit) construction of A [  X  ] as matricizing or unfolding A along mode  X  and define the  X  -rank of the tensor A to be the rank of the matrix A [  X  ] . Given any n matrix B , define the  X  -mode tensor-matrix product to be the d -mode tensor of size n 1  X  X  X  X  X  n  X   X  1  X  c  X   X  n  X  +1  X  X  X  X  X  whose i 1  X  X  X  i d element is Denote the SVD of A [  X  ] by where, e.g., U [  X  ] is an n  X   X  rank( A [  X  ] )matrixand U is a n  X   X  k  X  matrix consisting of the left singular vectors corresponding to the top k  X  singular values of A [  X  ] . Define the (square of the) Frobenius norm to be Let us refer to as slabs each of the n  X  d  X  1-mode tensors of size n 1  X  X  X  X  X  n  X   X  1  X  n  X  +1  X  X  X  X  X  n d constructed by fixing the  X  th coordinate to some particular value i  X   X  X  1 ,...,n  X  } Similarly, let us refer to as fibers each of the N  X  vectors (mode-one tensors) of size n  X  constructed by fixing each of the other coordinates to a particular value.

For more details about these results, see [28, 29, 12].
In this subsection, for simplicity of exposition and in light of the two applications we will consider, we restrict ourselves to tensors that are subscripted by three indices, i.e., so-called three-mode tensors.

Consider an n 1  X  n 2  X  n 3 tensor A , defined as the collection of elements The elements may be thought of as a data cube, i.e, a three-dimensional block such that index i runs along the vertical axis, index j runs along the horizontal axis, and index k runs along the  X  X epth X  axis. Since by assumption there is a  X  X istinguished X  mode, we are considering the special case of a (2+1)-tensor , i.e., an n 1  X  n 2  X  n 3 tensor in which two modes (without loss of generality, we will assume they are the first two) are similar in some application-dependent manner and the third is qualitatively different. See Figure 1 for a pictorial description of a (2 + 1)-data tensor. In this Figure 1: Pictorial representation of a (2 + 1) -data tensor. case, we refer to each of the n 3 different n 1  X  n 2 matrices as  X  X labs X  and each of the n 1 n 2 different n 3 -vectors as  X  X ibers. X 
With this in mind, consider the (2 + 1) -Tensor-CUR algorithm, described in Figure 2. This algorithm takes as input an n 1  X  n 2  X  n 3 tensor A , a probability distribution { p i } n 3 i =1 over the slabs, a probability distribution { over the fibers, a number c of slabs to choose, and a num-ber r of fibers to choose. (Without loss of generality, we have assumed that the preferred mode  X   X  X  1 , 2 , 3 } is the third mode.) The tensor A is decomposed along this mode in a manner analogous to the original CUR matrix decom-position [10]. More precisely, this algorithm computes the approximation by performing the following: first, choose c slabs (2-mode subtensors, i.e., matrices) in independent ran-dom trials and choose r fibers (1-mode subtensors, i.e., vec-tors) in independent random trials according to the input probability distributions; second, define the n 1  X  n 2  X  sor C to consist of the c chosen slabs and also define the r  X  n 3 matrix R to consist of the chosen fibers; third, let be an an appropriately-defined and easily-computed (given C and R ) c  X  r matrix.

Clearly,  X  A = C X  3 UR ,where  X  3 is a tensor-matrix mul-tiplication, is a n 1  X  n 2  X  n 3 tensor. Thus, by using the (2 + 1) -Tensor-CUR algorithm, we make the following ap-proximation: Thus, in particular, if i  X  1 ,...,n 3 is one of the slabs that is not randomly selected, then by using the (2 + 1) -Tensor-CUR algorithm, we make the following approximation: where A (: , : ,i )isthe n 1  X  n 2 matrix formed from A by fixing the value of the third mode to be i , C is a set indicating which c indices were randomly chosen, and X (: ,i )isavector consisting of the i th column of the matrix UR .

See Figure 3 for a pictorial description of the action of the algorithm and this approximation. In particular, note that a small number of slabs are sampled, and every other slab is approximately reconstructed using the information in those slabs as a basis and the information in a small number of fibers (depicted as the dashed lines). The extent to which (10) or (11) is a good approximation has to do with the selection of slabs and fibers. In Sections 4 and 5, we show that (10) holds empirically for our two applications if the slabs and fibers are chosen uniformly and/or nonuniformly Input: An n 1  X  n 2  X  n 3 tensor A , a probability distribution { p tegers c and r .
 Output: An n 1  X  n 2  X  c tensor C ,a c  X  r matrix U ,and a r  X  n 3 matrix R . 1. Select c slabs in c i.i.d. trials according to { p i } n 3 2. Select r fibers in r i.i.d. trials according to { q i } n 1 n 2 3. Let the r  X  c matrix W be the matricized intersection 4. Define the c  X  r matrix U = D C ( D R WD C ) + D R . with probabilities that depend on the Frobenius norms of slabs and Euclidean norms of fibers, respectively. See the proof of Theorem 4 in Section 3.2 and also [8, 9, 10] for a discussion of the algorithmic justification for this sampling.
We emphasize that, as with the matrix CUR decompo-sition, when this tensor-CUR decomposition is applied to data there is a natural interpretation in terms of underlying data elements. For our imaging application, a  X  X lab X  corre-sponds to an image at a given frequency step and a  X  X iber X  corresponds to a time-or frequency-resolved pixel. Simi-larly, for our recommendation system application, a  X  X lab X  corresponds to a product-product preference matrix for a single user and a  X  X iber X  corresponds to preference informa-tion from every user about a single product-product pair.
In this subsection, to provide a theoretical justification for the tensor-CUR decomposition of Section 3.1, we present our main algorithmic result. Our main algorithmic result is a generalization the (2 + 1) -Tensor-CUR algorithm and an associated provable quality-of-approximation bound for the Frobenius norm of the error tensor A X  X  X  3 UR .

The Tensor-CUR algorithm, described in Figure 4, takes mode  X   X  X  1 ,...,d } , a rank parameter k  X  , an error param-eter &gt; 0, and a failure probability  X   X  (0 , 1). The algo-rithm returns as output three carefully constructed subten-sors that, when multiplied together, are an approximation  X  A to A . Both the number of slabs c  X  and the number of fibers r  X  that are randomly sampled depend on the rank parameter k  X  , an error parameter , and a failure proba-bility  X  . The subtensors C and R are chosen by sampling according to a carefully-constructed nonuniform probabil-Input: An n 1  X  n 2  X  ...n d tensor A ,amode  X   X  X  1 ,...,d a rank parameter k  X  , an error parameter &gt; 0, and a failure probability  X   X  (0 , 1).
 Output: An n 1  X  X  X  X  X  n  X   X  1  X  c  X   X  n  X  +1  X  ...  X  n d tensor C ,a c  X   X  r  X  matrix U ,anda r  X   X  n  X  matrix R . 1. Let c  X  =4 k  X  2. Define { p i } n  X  i =1 to be p i = 3. Define { q j } N  X  j =1 to be q j = 4. Select c  X  slabs in c  X  i.i.d. trials according to the prob-5. Select r  X  fibers in r  X  i.i.d. trials according to the prob-6. Let  X  be the best rank-k approximation 7. Define the c  X   X  r  X  matrix U = X ( D R  X ) T .
 ity distribution. In order to obtain the provable quality-of-approximation bounds of Theorem 4, the probability distri-bution depends on the Frobenius norms of the slabs and the Euclidean norms of the fibers, respectively. Intuitively, this biases the random sampling toward the subtensors that are of most interest; see [8, 9, 10] for details.

In more detail, the approximation  X  A is computed by per-forming the following: first, form (implicitly) each of the n  X  subtensors (slabs of mode d  X  1) defined by fixing i  X  { 1 ,...,n  X  } and also form (implicitly) each of the N  X  = Q =  X  n i subtensors (fibers of mode 1, i.e., vectors) defined by fixing a value for each of the modes i =  X  ; second, con-struct nonuniform probability distributions with respect to which to sample the slabs and the fibers; third, choose c the d  X  1-mode slabs in independent random trials and also choose r  X  of the 1-mode fibers in independent random trials; to consist of the c  X  chosen d  X  1-mode slabs, and also de-fine the tensor R X  R r  X   X  n  X  to consist of the r  X  chosen 1-defined and easily-computed (given C and R ) tensor of mode 2 (i.e., matrix). Then, we may define where C X   X  UR is the  X  -mode tensor-matrix product between C and UR to be an n 1  X  X  X  X  X  n  X   X  1  X  n  X   X  n  X  +1  X  X  X  X  X  n d sor that is an approximation to the original tensor A .(The awkward form of U is currently necessary for our provable results. Nevertheless, U is a subspace-perturbation of the Moore-Penrose generalized inverse of matricized intersection between C and R . Thus, for the (2 + 1) -Tensor-CUR algo-rithm and for the applications described in Sections 4 and 5 we have taken it to be exactly this quantity.) Our main quality-of-approximation bound for the Tensor-CUR algorithm is given by the following theorem, in which we bound the Frobenius norm of the error tensor A X   X  A . Note that in (13), the sure of the extent to which the  X  X nfolded X  matrix A [  X  ] not well-approximated by a rank-k  X  matrix, and the A F term is a measure of the loss in approximation quality due to the choice of slabs and fibers (rather than, e.g., the top k  X  eigen-slabs and eigen-fibers along the  X  mode).
Theorem 4. Let A be an n 1  X  n 2  X  ...n d tensor, and let  X   X  X  1 ,...,d } be a particular mode, k  X  be a rank parameter, &gt; 0 be an error parameter, and  X   X  (0 , 1) be a failure prob-ability. Construct a tensor-CUR approximate decomposition to
A with the output of the Tensor-CUR algorithm. Then, with probability at least 1  X   X  , A X  X  X   X  UR Proof: Since  X  X nfolding X  A along any mode does not change the value of its Frobenius norm (since it is simply a reorder-ing of indices in a summation) it follows that Note that the Frobenius norm on the left hand side of (14) is a tensor norm and that the Frobenius norm on the right hand side of (14) is a matrix norm. Due to the form of the sampling probabilities used in the Tensor-CUR algorithm, it is this latter quantity that Theorem 5 of [10] bounds. By applying this result [10], the theorem follows.

With regard to complexity considerations, assume, for simplicity, that the tensor A is stored externally and as-sume that k i = O (1) and that n i = n for every i =1 ,...,d . Then the matrices C [ i ] each occupy only O ( n ) additional scratch space. In general, O ( n d  X  1 ) additional scratch space will be needed to compute the probabilities of the form used by the Tensor-CUR Algorithm, and this will be compara-ble to the overall memory requirements if d is large. On the other hand, if the uniform probabilities are approximately optimal for each of the d nodes, then only O ( n ) additional scratch space and computation time are needed, resulting in a substantial scratch memory and time savings [8].
In hyperspectral imagery, an object or scene is imaged at a large number of contiguous wavelengths [33, 34]. Al-though hyperspectral imagery originated in astronomy and geosensing, it has been employed more recently in numerous other application areas, including agriculture, manufactur-ing, forensics, and medicine. In many of these applications, target resolution is limited by available spatial resolution. By considering the spectral variation of light intensity, one obtains rich information about the object or scene being im-aged that complements traditional spatial information. One also obtains data sets that are very large and contain much redundancy. For example, if a single scene is imaged at 200 frequency bands, where at each frequency a 256  X  256 image is generated, then the data cube generated for this single object consists of 13 million values.

When applied to medical samples, a variety of hyperspec-tral devices have been used to discriminate among, e.g., cell types, tissue patterns, and endogenous and exogenous pigments. Although the increasing power of these meth-ods holds the promise for developing automatic diagnostics, the increased volume and formal dimensionality of the data make the development of more efficient algorithms necessary in order to extract statistically useful and reliable informa-tion about the data.
The hyperspectral image data set we consider consists of 59 data cubes derived from 59 biopsies (20 normal, 19 be-nign adenoma, and 20 malignant carcinoma colon biopsies, one per patient). Each data cube consists of 128 grey-scale images at 400X magnification over the frequency range ca. 440nmto700nm,whereeachimageis495  X  656 pixels in size (for a total of ca. 40 million pixels). Each image is gen-erated using a prototype tuned light source by measuring the modulated light transmitted through the sample. For details about the data and its generation, see [33, 34].
Figure 5 illustrates one of the 128 images, i.e., a hyper-spectral image at a single frequency, in a typical (very ma-lignant) sample, and Figure 6 illustrates a typical frequency-Figure 5: A very malignant sample at a single fre-quency in one hyperspectral data cube. Figure 6: Average normalized spectrum and a single typical spectrum in one hyperspectral data cube. Vertical axis represents normalized energy-per-frequency in the spectra. Horizontal axis is the slab index. resolved pixel and the average spectrum of the ca. 324 , 000 frequency-resolved pixels in this data cube. Although not illustrated, both successive images and also pixels from dif-ferent spatial regions are strongly correlated with one an-other.

In this imaging application, the tensor C consists of a small number of dictionary or basis images (which are ac-tual and not eigen-images) with respect to which the re-maining images are expressed in an approximately-optimal least-squares manner. Similarly, the matrix R consists of the spectral variation of a small number of dictionary or ba-sis pixels with respect to which the spectral variation of the remaining pixels are expressed.
 In the next two subsections, we will see that the tensor-CUR decomposition can be applied to this hyperspectral image data in order to compress the data and to perform two classification tasks of interest on the data. Slabs will be chosen randomly with a probability proportional to the average normalized spectrum of Figure 6 and fibers will be chosen uniformly at random. The data-dependent motiva-tion for this is that the intensity of transmitted light cap-tures a meaningful notion of information as a function of varying frequency, but not as a function of varying spatial coordinates due to the particular staining technology. Figure 8: Reconstruction error. Caption indicates how many slabs (S) and fibers (F) were sampled.
 Vertical axis is the relative reconstruction error (for the Frobenius norm). Horizontal axis is the slab index. Average and standard deviation are over 4 slab draws and 3 fiber draws.
For each slab we did not randomly sample, we use the tensor-CUR decomposition to reconstruct that slab in an approximately-optimal least-squares sense in the basis pro-vided by the sampled slabs, and we do so using only a small number of pixels in that slab. In Figure 7 we present a representative example of the reconstruction of one spectral slice in a normal biopsy. The redundancy in the data is ev-ident by the quality of the reconstruction under very heavy downsampling. For example, it suffices to judiciously choose as few as 8 or even 2 of the original 128 slabs, and to recon-struct the remaining slabs it suffices to choose ca. 1000 (or fewer) of the original ca. 324 , 000 fibers.

In Figure 8, we present the approximation error as a func-tion of downsampling to different number of slabs and then to different number of fibers. Clearly, due to the form of the slab sampling probabilities, slabs between ca. 30 and ca. 60 tend to be reproduced much better than those toward the tails of the spectrum. Slabs below ca. 20 and above ca. 70 tend to have a lower signal-to-noise ratio and are less im-portant for the problem of approximate data reconstruction (but not necessarily for other problems).

A close examination of images such as those presented in Figure 7 reveals a subtle interplay between sampling-induced error and denoising due to the low-dimensionality of the sample. Note that by permitting our algorithm to sample different numbers of slabs and fibers, we can, e.g., sample slabs to a level appropriate for structure identifica-tion and sample more fibers for denoising purposes.
In medical applications, one is interested in the classifica-tion of an entire data cube, i.e., a medical sample, as normal or malignant. Since nuclei are the most discriminative struc-tures for this task, as an intermediate step, one is interested in classifying the pixels in a single data cube into different Table 1: Confusion matrix of predictions of normal and malignant nuclei (patches of size 64 by 64 ,with averaged 10 -fold cross-validated error). TN, TM stand for True Normal and True Malignant, and PN, PM stand for the corresponding predictions. Clas-sification is based on using all 128 slabs, 16 , 8 ,or 2 slabs, as indicated. tissue types, e.g., nuclei, cytoplasm, or lamina propria. For details on the classification procedures, see [33, 34]. We sim-ply note that for the normal versus malignant classification task, we have access to a label (assumed correct) provided by a pathologist [33, 34], while no such ground truth is avail-able for the tissue classification.

In Figure 9, we present typical results for the tissue clas-sification task in the exact data cube and in two downsam-pled and reconstructed data cubes. The two examples pre-sented involve sampling 16 and 8 slabs, respectively, and as with the reconstruction problem, in both cases there is little quality loss until the number of fibers samples is less than ca. 1000. As before, a careful analysis reveals a com-plex interplay between sampling-induced information loss and sampling-induced denoising. If the nuclei identified by this tissue classification are then used to classify data cubes as normal or malignant, the results can be compared with the true value. Results of the confusion matrix for this clas-sification task are presented in Table 1 [33, 34]. High qual-ity results are obtained using samples of 16 and 8 slabs, but quality degrades if only 2 slabs are used. Similar results are seen when we classify into normal, abnormal, and malignant.
In recommendation system analysis, one is typically in-terested in making purchase recommendations to a user at an electronic commerce web site. Collaborative methods (as opposed to content-based or hybrid) involve recommending to the user items that people with similar tastes or prefer-ences liked in the past. Probably the most well-known exam-ple of a collaborative filtering system is that of Amazon.com which is based on rules of the form  X  X sers who are inter-ested in item X are also likely to be interested in item Y X  [32]. Many collaborative filtering algorithms represent a user as an n dimensional vector, where n is the number of dis-tinct products, and where the components of the vector are a measure of the rating provided by that user for that product. Thus, for a set of m users, the user-product ratings matrix is an m  X  n matrix A ,where A ij is the rating by user i for product j (or is null if the rating is not provided). A recom-mendation algorithm generates recommendations for a new user based on a few user who are most similar to the user, after querying the new user about his (or her) rating on a small number of products. For details, see [35, 5, 1].
A matrix CUR decomposition has been used to obtain competitive recommendation performance by judiciously sam-pling O ( m + n ) entries of the user-product ratings matrix and reconstructing missing entries [11]. In more detail, assuming access to a matrix C consisting of the ratings of every user for a small number of products and a matrix R consisting of the ratings of a small number of users for every product, then under assumptions CUR is a provably good approxi-mation to the user-product matrix A [11]. Other theoretical work includes [27, 2, 23], and other applications of linear algebra have used the SVD for dimensionality reduction [4, 36, 15].

Although the ratings in the user-product matrix A are often interpreted in terms of the utility of product j for user i , utility in neoclassical economics is an ordinal and not a cardinal concept. This is since utility functions are constructs that encode preference information and since the same preferences are described when the utility function is subject to a wide class of monotonic transformations. This observation motivates the definition of an m  X  n  X  n user-product-product (2 + 1)-tensor A ,where A ijk is +1 or  X  1 depending on whether product j or product k is preferred by user i . Similar preference-based models have appeared [7, 14, 22, 21], and have been motivated by such observations as that two users with very similar preferences over items may have very different rating schemes. When faced with a new user, this preference model depends on obtaining pairwise preference information such as that the user bought product on compressed data ( 8 slabs and 1200 fibers).
 A when he could have bought product B , or that the user clicked on link A when he could have clicked on link B .
Under this preference model for recommendation system analysis, the tensor C consists of a small number of dictio-nary or basis elements from a small number of users, where each element corresponds to the full n  X  n pairwise preference matrix for a single user. Similarly, the matrix R consists of a dictionary or basis set of preference information from every user about a small number of product-product pairs.
In the next subsection, we will see that the tensor-CUR decomposition can be applied to recommendation system data under this model to reconstruct missing entries in the user-product-product preference tensor in order to make high-quality recommendations. Since most recommendation sys-tem databases do not provide data in this preference-based format, the data set we will consider will be derived from the ratings in the well-studied Jester data [15]. As an ini-tial application, we consider the m =14 , 116 (out of ca. 73 , 421) users who rated all of the n = 100 products (i.e., jokes). From this m  X  n user-product ratings matrix, we define an m  X  n  X  n user-product-product preference ten-sor by performing the following for each user: convert the n dimensional rating vector into an n  X  n preference matrix in which the ij entry is +1 or  X  1 depending on whether or not the user prefers product i to product j . (Although this results in ordered and fully-consistent preferences, this is not required by our decomposition.) In this application, in the absence of a better model, both slabs and fibers will be chosen uniformly at random.
For each slab (i.e., user) we did not randomly sample, we use the tensor-CUR decomposition to reconstruct that slab in an approximately-optimal least-squares sense in the ba-sis provided by the sampled slabs, and we do so using only a small number of product-product preference queries from that slab. Then we will use this reconstruction to make rec-ommendations by approximating the reconstructed matrix of preferences, and picking the 10 products with the largest row sums. We will make 10 recommendations, and we will evaluate the quality of our recommendation with the Top-N procedure [36], i.e., by the number of products out of the exact top 10 that we correctly recommend. Figure 10: Average number of successful recommen-dations out of the top 10 for a basis consisting of a varaible number of users but using complete pair-wise product-product preference information.

In order to determine an upper bound on the quality of recommendations based on using a small number of basis slabs, consider Figure 10, which shows the average number of recommendations out of the top 10 that can be captured using a small number of basis slabs. In this figure, we use full fiber information, and thus we are considering the exact least-squares fit of a new slab on the space spanned by the basis slabs. For example, using 128 basis slabs, we can hope to predict up to 4 . 5, 6, or 8 of the top 10 items by sampling 64, 128, or 256 fibers, respectively. As a lower bound on quality, we expect that we will make ca. 1 prediction cor-rectly since we are making 10 predictions and there are 100 products.

In Figure 11, we show that by using a basis of preference information from 128 users and performing a small num-ber of product-product preference queries on a new user, we can make a large number of high-quality recommendations. Similar results are seen with 64 and 256 basis slabs. Since we are sampling a small number of fibers in this case, we are performing an approximate least-squares fit using just the information about a new user contained in a small number of fibers. The number of top-10 recommendations is com-petitive with the best possible using the small basis and is well-above the random level. Note the nonmonotonicity near Figure 11: Average number of successful recommen-dations out of the top 10 for a basis consisting of 128 users versus the number of pairwise product-product preference queries. Figure 12: Distribution of number of users making a given number of successful top 10 recommendations for a basis consisting of 128 users. ca. 64 queries; this may be a fitting issue and is the subject of further exploration. Finally, in Figure 12, we present the distribution of correct predictions for the 14 , 116 users by using 128 slabs and a variable number of fiber queries.
In evaluating performance, we distinguish between pre-diction and reconstruction. In the former, we want to know how much user i will like product j (in a ratings model) or whether user i will prefer product j or product k (in a preference model). In the latter, which is of interest to us, we want to give a list of, e.g., the top 10 products for user i . We use tensor reconstruction as an intermediate step to making high-quality recommendations.
We conclude with several related extensions of the present work. First, it would be worth examining how these meth-ods can be coupled with more traditional methods of image analysis and recommendation system analysis. This could be performed either by choosing slabs and fibers and then analyzing each slab or fiber with more traditional methods, or by using structural insights from more traditional meth-ods to construct the sample of slabs and fibers. Second, it would be worth determining whether the sample of slabs and/or fibers could be chosen to preserve some interesting multilinear structure in the data tensors that is damaged by the sampling techniques we have used. Third, it would be worth determining the extent to which it would be possible to combine fibers from several data cubes into a  X  X ictio-nary X  that could be used, along with a few slabs in a new data cube, to describe the entire new data cube.

Acknowledgments We thank the authors of [33, 34], and in particular Gustave L. Davis of Yale University, for making available the hyperspectral data. M. Maggioni is partially supported by NSF-DMS grant 0512050. [1] G. Adomavicius and A. Tuzhilin. Toward the next [2] Y.Azar,A.Fiat,A.R.Karlin,F.McSherry,and [3] M.W. Berry, S.A. Pulatova, and G.W. Stewart.
 [4] D. Billsus and M.J. Pazzani. Learning collaborative [5] J. Breese, D. Heckerman, and C. Kadie. Empirical [6] J.D. Carroll and J.J. Chang. Analysis of individual [7] W.W. Cohen, R.E. Schapire, and Y. Singer. Learning [8] P. Drineas, R. Kannan, and M.W. Mahoney. Fast [9] P. Drineas, R. Kannan, and M.W. Mahoney. Fast [10] P. Drineas, R. Kannan, and M.W. Mahoney. Fast [11] P. Drineas, I. Kerenidis, and P. Raghavan.
 [12] P. Drineas and M.W. Mahoney. A randomized [13] P. Drineas and M.W. Mahoney. On the Nystr  X  om [14] Y. Freund, R. Iyer, R.E. Schapire, and Y Singer. An [15] K. Goldberg, T. Roeder, D. Gupta, and C. Perkins. [16] G.H. Golub and C.F. Van Loan. Matrix Computations . [17] S.A. Goreinov and E.E. Tyrtyshnikov. The [18] S.A. Goreinov, E.E. Tyrtyshnikov, and N.L.
 [19] R.A. Harshman and M.E. Lundy. The PARAFAC [20] J. H  X  astad. Tensor rank is NP -complete. Journal of [21] R. Jin, L. Si, and C.X. Zhai. Preference-based graphic [22] R. Jin, L. Si, C.X. Zhai, and J. Callan. Collaborative [23] J. Kleinberg and M. Sandler. Using mixture models [24] P.M. Kroonenberg and J. De Leeuw. Principal [25] J.B. Kruskal. Three-way arrays: Rank and uniqueness [26] J.B. Kruskal. Rank, decomposition, and uniqueness [27] R. Kumar, P. Raghavan, S. Rajagopalan, and [28] L. De Lathauwer, B. De Moor, and J. Vandewalle. A [29] L. De Lathauwer, B. De Moor, and J. Vandewalle. On [30] D. Leibovici and R. Sabatier. A singular value [31] L.-H. Lim and G.H. Golub. Tensors for numerical [32] G. Linden, B. Smith, and J. York. Amazon.com [33] M. Maggioni, G.L. Davis, F.J. Warner, F.B.
 [34] M. Maggioni, G.L. Davis, F.J. Warner, F.B.
 [35] P. Resnick and H.R. Varian. Recommender systems. [36] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. [37] G.W. Stewart. Four algorithms for the efficient [38] G.W. Stewart. Error analysis of the [39] L.R. Tucker. Some mathematical notes on three-mode [40] C.K.I. Williams and M. Seeger. Using the Nystr  X  om
