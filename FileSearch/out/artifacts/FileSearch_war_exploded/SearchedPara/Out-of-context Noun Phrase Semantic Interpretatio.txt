 The acquisition of semantic knowledge is paramount for any application that requires a deep understanding of natural language text. Motivated by the problem of building a noun phrase-level semantic parser and adapting it to var-ious applications, such as machine translation and multilin-gual question answering, in this paper we present a domain-independent model for noun phrase semantic interpretation. We investigate the problem based on cross-linguistic evi-dence from a set of four Romance languages: Spanish, Ital-ian, French, and Romanian. The focus on Romance lan-guages is well motivated. It is generally the case that En-glish noun phrases translate into constructions of the form  X  NPN  X  in Romance languages where, as we will show, the P (preposition) varies in ways that correlate with the se-mantics. Thus, based on a set of 22 semantic interpretation categories (such as part-whole, agent, possession )we present empirical observations regarding the distribution of these semantic categories in a cross-lingual corpus and their mapping to various syntactic constructions in English and Romance. Furthermore, given a training set of English noun phrases along with their translations in the four Romance languages, our algorithm automatically learns classification rules and applies them to unseen noun phrase instances for semantic interpretation. Experimental results are compared against a state-of-the-art model reported in the literature. H.3.1 [ Content Analysis and Indexing ]: Linguistic pro-cessing; I.2 [ ARTIFICIAL INTELLIGENCE ]: Natural Language Processing X  Language parsing and understanding; (I.2.7) ;I.2[ ARTIFICIAL INTELLIGENCE ]: Natural Language Processing X  Text analysis (I.2.7) Algorithms, Experimentation, Languages Copyright 2006 ACM 1-59593-433-2/06/0011 ... $ 5.00. computational semantics, semantic relations, classification, SVM
The acquisition of semantic knowledge is paramount for any application that requires a deep understanding of natu-ral language text. In the past few years at many workshops, tutorials, and competitions 1 this task has received consid-erable interest from the computational linguistics commu-nity. The automatic identification of semantic information in text has become increasingly important in ontology de-velopment, information extraction, question answering, and other advanced natural language processing applications.
In this paper we present a model for the automatic se-mantic interpretation of noun phrases (NPs), which is the task of determining the semantic relation among the noun constituents. For example, chocolate cake encodes a part-whole relation, while chocolate article refers to topic .These constructions are ambiguous due to their imprecise and in-complete syntactic and semantic information, and thus pose various challenges to the research community. Moreover, the extension of this task to other natural languages brings for-ward new issues and problems. For example, for applications such as second language acquisition, machine translation and multilingual question answering , appropriate understanding of noun phrases is essential for humans and machines to be able to avoid the generation of infelicitous or ungrammati-cal constructions in the target language. For instance, beer glass translates into tarro de cerveza in Spanish, bicchiere da birra in Italian, verre ` abi` ere in French, and pahar de bere in Romanian. Thus, translators should know what are the equivalent constructions in the target language that preserve the meaning in context.

In this paper we investigate noun phrases based on cross-linguistic evidence and present a domain independent model for their semantic interpretation. We aim at uncovering the general aspects that govern the semantics of NPs in En-glish based on a set of four Romance languages: Spanish, Italian, French, and Romanian. The focus on Romance lan-guages is well motivated. Most of the time, English noun
Workshop on Multiword Expressions at COLING/ACL 2006, 2004, 2003; Computational Lexical Semantics Work-shop at ACL 2004; Tutorial on Knowledge Discovery from Text at ACL 2003; Shared task on Semantic Role Labeling at CONLL 2005, 2004 and at SENSEVAL 2005. phrases translate into constructions of the form  X  NPN  X  in Romance languages where, as we will show below, the P (preposition) varies in ways that correlate with the seman-tics. Thus Romance languages will give us another source of evidence for disambiguating the semantic relations in En-glish NPs.

Moreover, the observations and results obtained from this research will provide new insights into important areas such as second language acquisition and pedagogy -wheretheus-age of prepositions, for example, is considered the second type of error that is almost exclusive to non-native writ-ers/speakers. According to [6] , some of these errors are made by both native speakers of English and foreign learners, but some of them, including articles and prepositions (in this order) are especially problematic for foreign language learn-ers.

The paper is organized as follows. Section 2 presents re-lated work. In Section 3 we describe the general approach for the interpretation of noun phrases and list the seman-tic interpretation categories used along with observations regarding the distribution of these semantic categories in a cross-lingual corpus. Sections 4 and 5 present a model and results for the interpretation of English noun phrases. Fi-nally, discussion and conclusions are offered in Section 6.
The automatic interpretation of noun phrases is a diffi-cult task for both unsupervised and supervised approaches. Currently, the best-performin g NP interpretation methods in computational linguistics focus only on two or three-word consecutive noun instances (noun compounds) and rely ei-ther on rather ad-hoc, domain-specific, hand-coded seman-tic taxonomies, or on statistical models on large collections of unlabeled data. Recent results have shown that sym-bolic noun compound interpretation systems using machine learning techniques coupled with a large lexical hierarchy perform with very good accuracy, but they are most of the time tailored to a specific domain [20], [21], or are general purpose but rely on semantic similarity metrics on Word-Net [22] and/or on semantic resources, such as word sense disambiguation tools [4]. On the other hand, the major-ity of corpus statistics approaches to noun compound in-terpretation collect statistics on the occurrence frequency of the noun constituents and use them in a probabilistic model [9]. The problem is that most noun compounds are rare and thus, statistics on such infrequent instances lead in general to unreliable estimates of probabilities. More re-cently, [8] showed that simple unsupervised models applied to the noun compound interpretation task perform signifi-cantly better when the n-gram frequencies are obtained from the web (accuracy of 55.71% on Altavista on a general and abstract set of eight prepositional paraphrases as seman-tic classification categories), rather than from a large stan-dard corpus. [16] improve over Lapata &amp; Keller X  X  method through the use of surface features and paraphrases only for the task of noun compound bracketing (syntactic parsing of three word noun compounds without their interpretation). [18, 17, 19] use clustering techniques coupled with syntac-tic dependency features to identify is-a relations in large text collections. However, although the web-based solution might overcome the data sparseness problem, the current probabilistic models are limited by the lack of linguistic in-formation.
In this paper we focus only on compositional noun phrases ( X  X  N X  and  X  X  P N X ), ie. those whose meaning can be derived from the meaning of the constituent nouns. Our method of inquiry is as follows: 1. We present a contrastive study of the distribution of vari-ous noun phrase-level constructions in English and Romance languages. 2. We show empirical observations on the distribution of a core set of 22 semantic interpretation categories (semantic tag sets) and aim at uncovering the general aspects of noun phrase interpretation. 3. We present a supervised learning algorithm that cap-tures and models syntactic and semantic regularities in En-glish noun phrase interpretation based on cross-linguistic evidence.
For this task we consider a core set of 22 semantic tags (22 SRs) which were identified by [15] 2 . These 22 SRs are presented and exemplified in Table 3. We show below the coverage of these semantic tag sets on a large corpus and present their mapping to various NP constructions.
For a better understanding of the semantic relations en-coded by  X  X  N X  and  X  X  P N X  instances, we analyzed the semantic behavior of these constructions on a large cross-linguistic corpus of examples. Our intention is to answer questions like: (1) What syntactic constructions are used to translate the English instances to the target Romance languages and vice-versa? (cross-linguistic syntactic mapping), (2) What semantic relations do these constructions encode? (cross-linguistic semantic mapping), (3) What is the corpus distribution of these mappings? ,and finally (4) How can the instances encoded by these constructions be semantically disambiguated?
For the questions (1) and (2), we extend over the work of [3] on the syntax of noun phrases in English and Ro-mance languages by providing cross-lingual empirical evi-dence for out-of-context instances for the set of 22 semantic tags. Based on a configurational approach, Giorgi &amp; Longo-bardi X  X  work focuses only on those noun phrases that have a theta-role structure (also called synthetic ), such as the cap-ture of the soldier ( theme ), where the noun capture is de-rived through nominalization from the verb to capture and thus, inherits its theta-grid (the set of semantic relations in predicate-argument structure). Besides these synthetic noun phrases we also consider root NPs -those whose con-stituent nouns are not derived from verbs -such as door knob ( part-whole ).
Moldovan &amp; Girju proved empirically that this set is en-coded by noun phrases and is a subset of their larger list of 35 semantic relations.
 There are also other lists of semantic relations used by the research community (eg, [1]), but they overlap considerably with that of [15]. The data.
 We collected the data from a multilingual dictionary The WordReference Dictionaries 3 , a free online translation dic-tionary site that contains over 200,000 translations for each of its target languages (English, Italian, Spanish, Portuguese). Besides words, the dictionary also provides translations for larger constructions, such as noun phrases. Thus, most of the time, for each word listed, the dictionary provides be-sides its definition, a list of general purpose noun phrases (mostly  X  X  N X  and  X  X  P N X ) containing the word. These instances and their translations were generated through the language forums X  discussions which are monitored by lan-guage experts. For our experiments we used only the Span-ish, French, and Italian translations for a list of 2,200 ran-domly selected English noun phrases after we eliminated the lexicalized instances. Since the dictionary does not cover Romanian, the Romanian translations were added by hand by one of our experienced annotators (a native speaker). For example, for travel agent the provided translations were agente de viajes (Spanish), agent de voyages (French), agente di viaggi (Italian), and agent de voiaj (Romanian). Some-times, different noun phrases can refer to the same concept with similar translations. For example, nail email and nail polish received different synonymous translations in the tar-get languages. Since the translations of the instances are provided out of context, most of them are either unambigu-ous or have a preferred meaning. For example, war book encodes most of the time a topic relation, but it can be easily overwritten by context if one is provided.
The corpus instances used in the corpus analysis phase have the following format: where NP is a noun phrase instance in English or in one of the Romance languages considered, and has the type  X  X  P N X  or  X  X  N X . The word  X  target  X  is one of the relations of the semantic categories considered, such as the list of 23 (22 + other ) semantic relations. For example, &lt; lead glass; cristal de plomo; vetro al piombo; verre au plomb; pahar de plumb; Part-Whole &gt; .
 Corpus Annotation and Inter-annotator Agreement.
 Two experienced annotators have tagged separately all the noun phrases accompanied by their translations in the cor-pus with the semantic classification relations. Whenever the annotators found an example encoding a semantic relation other than those provided or they didn X  X  know what inter-pretation to give, they had to tag it as  X  other-sr  X .
The annotators X  agreement was measured using Kappa statistics, one of the most frequently used measure of inter-annotator agreement for classification tasks: where Pr ( A ) is the proportion of times the annotators agree and Pr ( E ) is the probability of agreement by chance. The K coefficient is 1 if there is a total agreement among the annotators, and 0 if there is no agreement other than that expected to occur by chance.

After retaining only the  X  X  N X  and  X  X  P N X  instances and splitting the corpus into training X  X est sets, the Kappa www.wordreference.com values obtained were 0.67 for the training data, and 0.66 for the test data. We also computed the number of pairs that were tagged with other by both annotators for each semantic interpretation relation, over the number of exam-ples classified in that category by at least one of the judges (76% (training) and 78% (testing)).

The K coefficient shows a fair to good level of agreement for the corpus data on the set of 22 relations, taking into consideration the task difficulty. Syntactic mapping .
 Table 1 shows the distribution of various syntactic construc-tions used for the translation of the 2,200 English noun phrases in each of the four target languages considered. The data show that about 43% (with a higher percentage for Ro-manian) of the NPs are translated as instances of  X  NdeN  X  constructions (the equivalent form of  X  NofN  X  in English). Moreover, on average, 31% of the examples are translated into complex nominals for all four Romance languages. It is interesting to note here that many of these noun phrases are translated into both noun X  X oun (N N) and noun X  X djective (N A) compounds in the target languages. For example, love affair translates into enredo amoroso (Spanish), aventure amoureuse (French), and aventur X  aamoroas X  a (Romanian) while in Italian it can be translated either using the more common  X  X  di N X  pattern ( storia d X  X more ) or the noun-adjective compound one ( relazione amorosa ). 80 (French), 125 (Italian), 50 (Spanish), and 71 (Romanian) corpus in-stances have two translations. There are also instances that have just one word correspondent in the target language. For example, ankle boot is translated into bottine in French and stivaletto in Italian. The rest of the data is encoded by other prepositional patterns or other syntactic paraphrases. For example, bomb site is translated into Italian as luogo dove ` e esplosa la bomba 4 . Moreover, Table 2 shows the dis-tribution of the prepositions present in the  X  X  P N X  trans-lations.

The distribution data show that, besides the more com-mon pattern  X  X  de N X , the target languages considered use other syntactic constructions to encode the translations of English noun phrases. This shows one more time that the translation of English NPs into Romance languages and vice-versa is not a trivial task, neither for humans (eg, sec-ond language acquisition) nor for machines (automatic ma-chine translation). Subsections 3.3 and 4.2 show that the choice is influenced by the NP meaning.

For the purpose of this research, from the initial corpus we selected those English noun phrase instances that had all the translations encoded only by these constructions. Columns 2 and 3 in Table 1 show that there are 1,160 (French), 1,135 (Italian), 1,161 (Spanish), and 1,361 (Romanian) such in-stances. This indicates that there were some instances that had more than one translation in the target language. Out of these, only 936 instances were encoded by  X  X  N X  and  X  X  P N X  in all languages considered  X  and this is the corpus we consider for our further experiments.  X  the place where the bomb is exploded  X 
Syntactic NPN Total mapping
Spanish para (3.5%); por (1.6%); con (0.8%); 981
Romanian pentru (4.5%); despre (1.6%); Table 2: The distribution of  X  X  P N X  constructions used in the translation of the English noun com-pound instances.
 Semantic mapping.
 Table 3 shows the semantic distribution of the noun phrase instances. The most frequen tly occurring relations were purpose, part-whole, location, topic, temporal ,and theme . The 8.6% of other relation represents the ratio of those instances that did not encode any of the 22 semantic relations.

Thus, one of the research questions we are interested in is What governs the choice of syntactic constructions when translating into the target Romance languages? The data show that there are dependencies between the structure of the Romance language translations and the semantic cat-egory of the nominal. For example, part-whole ( 7 )is encodedinItalianby di , a , ad and noun-compounds; in contrast, location ( 12 )isencodedby di , su , a , in and noun-compounds. Of course, some prepositions are seman-tically more ambiguous then others (eg, su (encoding loca-tion and topic )vs. di (encoding almost all 22 SRs)). The many-to-many mappings of the prepositions to the semantic classes adds to the complexity of the interpretation task.
Given the NP syntactic constructions considered, the goal is to develop a procedure for the automatic labeling of the se-mantic relations they encode. The semantic relation derives from the lexical and semantic features of each NP instance.
Semantic classification of lexico-syntactic patterns in gen-eral can be formulated as a learning problem, and thus ben-efit from the theoretical foundation and experience gained with various learning paradigms. This is a multi-class classi-fication problem since the output can be one of the semantic relations in the set. We cast this as a supervised learning problem where input/ output pairs are available as training data.

An important first step is to map the characteristics of each NP construction (usually not numerical) into feature vectors. Let X  X  define with x i the feature vector of an instance i and let X be the space of all instances; ie x i  X  X .
The multi-class classification is performed by a function that maps the feature space X intoasemanticspace S , f : X  X  S ,where S is the set of semantic relations from Table3,ie, r j  X  S .

Let T be the training set of examples or instances T = ( x 1 r 1 .. x l r l )  X  ( XxS ) l where l is the number of examples x each accompanied by its semantic relation label r .The problem is to decide which semantic relation to assign to a new, unseen example x l +1 . In order to classify a given set of examples (members of X ), one needs some kind of measure of the similarity (or the difference) between any two given members of X . Most of the times it is difficult to explicitly define this function, since X can contain features with numerical as well as non-numerical values.

The interpretation task is defined as a semantic classifi-cation problem. The system receives as input English noun phrase instances along with their translations in the Ro-mance languages, plus a set of extra-linguistics features. The output is a set of learning rules that classify the data based on the set of 22 semantic target categories. The learn-ing procedure is supervised and takes into consideration the cross-linguistic lexico-syntactic information gathered for each noun phrase. The mapping was obtained on the 936 instance corpus.
A further study of the NP instances and their semantic distribution presented in Section 3 provided new insights into the semantic interpretation problem. Thus, we have identified and experimented with the following NP features. Features 1 through 3 are defined for the English NP in-stances only, while 4 and 5 also apply to their Romance translations. 1. Semantic class of head noun specifies the WordNet sense (synset) of the head noun and implicitly points to all its hypernyms. The NP semantics is heavily influenced by the meaning of the noun constituents. For example: GM car is a make/produce relation. In case the noun has multiple inheritance, the first semantic class is chosen. For example, the hypernyms of car#1 are: { motor vehicle } , { self-propelled vehicle } , { wheeled vehicle } , { vehicle } , { conveyance { instrumentality } , { artifact } , { object } , { entity } 2. Semantic class of modifier noun specifies the WordNet synset of the modifier noun. For the example family car it will be the hypernym hiearchy of the noun family#1 in WordNet. 3. Semantic class of the WordNet derivationally related form specifies the top semantic class of the verb the noun is re-lated to. WordNet contains information about nouns de-rived from verbs using ove rt morphology (e.g. noun state-ment derived from to state ); words that can be either verbs or nouns (e.g. cry ); or verbs derived from nouns (e.g. death from to die ). 4. Prepositional cues that link the two nouns. These can be either simple or complex prepositions such as of or in materia de (Spanish). In case of  X  X  N X  instances, the value of this feature is  X  X  X . 5. Type of nominalized noun indicates the specific class of nouns the head or modifier belongs to depending on the verb it derives from. First, we check if a noun is a nominalization or not. For English we used the NomLex-Plus dictionary of nominalizations [10] to map nouns to corresponding verbs. NomLex-Plus is a hand-coded database of 5,000 verb nomi-nalizations, de-adjectival, and de-adverbial nouns including the corresponding subcategorization frames (verb-argument structure information). Example:  X  destruction of the city  X , where destruction is a nominalization. For English, this fea-ture may overlap with feature#3 which is used in case the noun to be checked doesn X  X  have an entry in the NomLex-Plus dictionary. In the case of other Romance languages, such as Romanian, we used a set of morphological patterns that identify nominalizations [2].

This feature is of particular importance since it imposes some constraints on the possible set of relations that the instance can encode. The feature takes the following values: a. Active form nouns which have an intrinsic active voice predicate-argument structure. In English this is a necessary restriction, while in Italian the class is bipartite (there are some nouns that do take passive arguments) [3]. Such ex-amples are the love of children (*the love by the children), il desiderio di cioccolata (It.) 5 . Most of the time, they repre-sent states of emotion, such as fear, desire, etc. These nouns mark their internal argument through of and require most of the time prepositions like por and not de (for example in Spanish) and vice-verse. Thus, these nouns cannot encode the agent relation. b. Ergative nouns which are derived from ergative verbs that take only internal arguments (eg, not agentive ones). For example, the entrance of the queen ( * the entrance by the queen ) cannot encode an agent relation. c. Intransitive nouns which are derived from intransi-tive verbs. They can take only agent or only patient semantic relations. For example, la telefonata del ragazzo (It.) 6 d. Inherently passive nouns . For example la cattura del soldato ( the capture of the soldier ). These nouns, like the verbs they are derived from, assume a default agent (subject) and being transitive, associate to their internal argument (introduced by  X  X f X  in the example above) the theme relation.

We assembled a list of about 3,000 nouns that belong to classes a X  X . For this, we used the information on the verbs in VerbNet [7] and verb  X  nominalizations in NomLex-PLUS. VerbNet is a database which encodes rich lexical information for a large number of English verbs in the form of subcatego-rization information, selectional restrictions (thematic roles for each argument of the verb) and alternations (the syn-tactic construction(s) the verb participates in). Thus, for each of the transitive, intransitive, sentential complement, and resultative frames in VerbNet, we checked and selected those verbs that have nominalizations as described by classes a X  X  (cf. general definitions in [3]). We generated lists for the four Romance languages by searching wordreference.com for corresponding translations. For Romanian, the list was generated by hand.

Consider the example &lt; love of children; amor por los ni  X  nos; ammore per i bambini; l X  X mour pour le enfants; dragoste de copii; recipient &gt; . Its corresponding feature vector is &lt; entity#1, entity#1,  X , of, active-form;  X ,  X ,  X , por, active-form; X , X , X ,per,active-form; X , X , X ,pour,active-form;  X ,  X ,  X , de, active-form; recipient &gt; .  X  X he desire for chocolate X   X  X he boy X  X  phone call X .
Several learning models can be used to provide the dis-criminating function f . So far we have experimented with the support vector machines model and compared the re-sults against Semantic Scattering, a state-of-the-art model described in [13, 5, 12].
 Support Vector Machines (SVM) In order to achieve classification in n semantic classes, n&gt; 2, we built a binary classifier for each pair of classes (a total of C n classifiers), and then used a voting procedure to estab-lish the class of a new example. For the experiments with semantic relations, the simplest voting scheme has been cho-sen; each binary classifier has one vote which is assigned to the class it chooses when it is run. Then the class with the largest number of votes is considered to be the answer. The software used in these experiments is the package LIBSVM, ( http://www.csie.ntu.edu.tw/  X  cjlin/libsvm/ ) which implements an SVM algorithm. We tested with the radial-based kernel.
After the initial NP instances in the training and test-ing corpora were expanded with the corresponding features, we had to prepare them for SVM. The set up procedure is described below.
 Corpus set up for SVM: The processing method consists of a set of iterative pro-cedures of specialization of the examples on the WordNet is-a hierarchy. Thus, after a set of necessary specializa-tion iterations, the method produces specialized examples which through supervised machine learning are transformed into sets of semantic rules for the noun phrase interpretation task.

Initially, the training corpus consists of examples that fol-low the format exemplified at the end of the feature space subsection (Subsection 4.2). Note that for the English NP instances, each noun constituent was expanded with the corresponding WordNet top semantic class. At this point, the generalized training corpus contains two types of exam-ples: unambiguous and ambiguous. The second situation occurs when the training corpus classifies the same noun  X  noun pair into more than one semantic category. For ex-ample, both relationships  X  chocolate cake  X -part-whole and  X  chocolate article  X -topic are mapped into the more general type &lt; entity # 1, entity # 1, part-whole/topic &gt; 7 cursively specialize these examples to eliminate the ambigu-ity. By specialization, the semantic class is replaced with the corresponding hyponym for that particular sense, i.e. the concept immediately below in the hierarchy. These steps are repeated until there are no more ambiguous examples. For the example above, the specialization stops at the first hyponym of entity : physical entity (for cake )and abstract entity (for article ). For the unambiguous examples in the generalized training corpus (those that are classified with a single semantic relation), constraints are determined using cross validation on SVM.
The corresponding feature vectors are: chocolate cake : &lt; entity#1, entity#1,  X ,  X ,  X ;  X ,  X ,  X , de,  X ;  X ,  X ,  X , di,  X ;  X ,  X ,  X , au,  X ;  X ,  X ,  X , de,  X ; PART-WHOLE &gt; . chocolate article : &lt; entity#1, entity#1,  X ,  X ,  X ;  X ,  X ,  X , sobre,  X ;  X ,  X ,  X , su,  X ;  X ,  X ,  X , de,  X ;  X ,  X ,  X , despre,  X ; TOPIC &gt; . Semantic Scattering (SS) The SS model was initially designed to semantically classify genitive constructions, but is applicable to other noun X  X oun instances [14, 12]. Essentially, it consists of using a train-ing data set to establish a boundary G  X  on WordNet noun hierarchies such that each feature pair of noun X  X oun senses f fined list of semantic relations 8 , and any feature pair above the boundary maps into more than one semantic relation. Due to the specialization property on noun hierarchy, feature pairs below the boundary also map into only one semantic relation. For any new pair of noun X  X oun senses, the model finds the closest boundary pair, in semantic sense, using a procedure called semantic scattering.

The authors define with SC m = { f m i } and SC h = { f h the sets of semantic class features for modifier noun and, respectively head noun. A pair of &lt; modifier  X  head &gt; nouns maps uniquely into a semantic class feature pair &lt;f m i From now on we will replace &lt;f m i , f h j &gt; with f probability of a semantic relation r given feature pair f ij , ber of occurrences of a relation r inthepresenceoffeature in the corpus. The most probable semantic relation  X  ris
From the training corpus, one can measure the quantities of f ij two cases are possible: Case 1 . The feature pair f ij is specific enough such that there is only one semantic relation r for which P ( r | f and 0 for all the other semantic relations.
 Case 2 . The feature pair f ij is general enough such that there are at least two semantic relations for which P ( r 0. In this case equation (2) is used to find the most appro-priate  X  r .
 Definition . A boundary G  X  in the WordNet noun hierarchies is a set of synset pairs such that : a) for any feature pair on the boundary, denoted f G  X  ij f ij maps uniquely into only one relation r ,and b) for any f u ij f G  X  ij , f u ij maps into m ore than one relation r ,and lation r . Here relations and  X  mean  X  X emantically more general X  and  X  X em antically more specific X  respectively. As proven by observation, there are more concept pairs un-der the boundary G  X  than above, i.e. |{ f l ij }| |{ f u
An approximation to boundary G  X  is found using the training set through an iterative process called semantic scattering . The algorithm starts with the most general bound-ary corresponding to the nine noun WordNet hierarchies and then specializes it based on the training data until a good approximation is reached.
 Step 1. Create an initial boundary.
 The initial boundary denoted G 1 is formed from combi-nations of the nine WordNet hierarchies: abstraction #6, act #2, entity #1, event #1, group #1, possession #2, phenomenon #1, psychological feature #1, state #4. To each [13] used a list of 35 semantic relations -actually only 22 of them proved to be encoded by NP constructions. training example a corresponding feature f ij = &lt;f m i is first determined, after which is replaced with the most general corresponding feature consisting of top WordNet hi-erarchy concepts. For instance, to the example  X  apartment of the woman  X  it corresponds the general feature entity#1 X  entity#1 and possession relation, to  X  husband of the woman  X  it corresponds entity#1 X  X ntity#1 and kinship relation, and to  X  hand of the woman  X  it corresponds entity#1 X  X ntity#1 and part-whole relation. At this high level, to each fea-ture it corresponds a number of semantic relations. For each feature, one can determine the most probable relation using the argmax equation. For instance, to feature entity#1-entity#1 there correspond 13 relations and the most proba-ble one is the part-whole relation.
 The boundary specialize is done in the following steps: Step 1. Constructing a lower boundary This step consists of specializing the semantic classes of the ambiguous features. A feature f ij is ambiguous if it cor-responds to more then one relation and its most relevant relation has a conditional probability less then 0.9. To elim-inate non-important specializations, the algorithm special-izes only the ambiguous classes that occur in more then 1% of the training examples.

The specialization procedure consists of first identifying features f ij to which correspond more than one semantic relation, then replaces these features with their hyponym synsets. Thus one feature breaks into several new special-ized features. The net effect is that the semantic relations that were attached to f ij will be  X  X cattered X  across the new specialized features. This process continues until each fea-ture will have only one semantic relation attached. Each iteration creates a new boundary.
 Step 2. Testing the new boundary.
 The new boundary is more specific then the previous bound-ary and it is closer to the ideal boundary. However, one does not know how well it behaves on unseen examples and is looking for a boundary that classifies with a high accuracy the unseen examples. The boundary is tested on unseen ex-amples. For that, only 10% of the annotated examples (dif-ferent from the 10% of the examples used for testing) are used. Then, the accuracy (f-measure) of the new boundary is computed on them.

If the accuracy is larger than the previous boundary X  X  ac-curacy, the algorithm is converging toward the best approx-imation of the boundary and thus it repeats Step 2 for the new boundary. If the accuracy is lower than the previous boundary X  X  accuracy, the new boundary is too specific and the previous boundary is a better approximation of the ideal boundary.
We performed various experiments on the test corpus us-ing the SVM supervised model described above. Table 4 shows the results obtained compared against five baselines and SS. In each experiment we measured the contribution of the word sense disambiguation (WSD) used in determin-ing the WordNet classes: for  X  no WSD  X  the first sense in WordNet was used, and for  X  WSD  X  the sense was generated by a state of the art WSD tool [11]. Since our intuition is that the more translations are provided for an English noun phrase instance, the better the results, we wanted to see what is the impact of the number of translation lan-guages used on the overall performance. Thus, Baseline1 measures the system performance only for English instances with no translation. Moreover, here we wanted to see what is the difference between SS and SVM, and what is the con-tribution of the English prepositions. Baseline2 shows the results obtained for English and the best Romance language (English + French). Here we computed the performance on all four combinations and chose the Romance language that provided the best result. Baseline3 (English + French + Italian) adds another best language to Baseline2, while Baseline 4 (English + French + Italian + Spanish) improves over Baseline3.

The results obtained are presented using the standard measure of accuracy (number of correctly labeled instances over the number of instances in the test set).

The first two SVM models from Baseline1 were introduced to better compare with the the performance obtained by [13] and [5]. The table shows that: a. Automatic WSD improves the performance but not in a radical way. b. For our data, SVM [-PP] gives slightly better results then SS. The inclusion of the preposition (SVM [+PP]) doesn X  X  add much since most of the  X  X  P N X  instances were of the type  X  X  of N X . The cross-linguistic SVM Baseline1 model shows the contribution of our features for English only. c. The addition of the features for the Romance languages in Baselines 2 X 4 incrementally improves the results. This shows that cross-linguistic information is important for the task of semantic interpretation. The contribution of each language varies here due to the features X  ambiguity and to the examples in the training and test corpora. The last Romance language proved to add the least information. A quick error analysis showed that most of the Romanian in-stances contained ambiguous prepositions such as de .How-ever, for a better understanding of the problems we need a larger corpus.
The experiments described in this paper are defined in the framework of a larger project. This project is to inves-tigate various linguistic issues and develop specific language models for the interpretation of noun phrase constructions in various languages, such as Germanic and Romance.
Our approach to noun phrase interpretation is novel in several ways. We tackled the semantic interpretation prob-lem using a cross-linguistic approach. We provided empirical observations on the distribution of the syntax and meaning of noun phrases (along with their mapping) on a fairly large corpus by performing various experiments with human judg-ments on a state-of-the-art semantic classification tag set. For the semantic annotation tasks, the paper presents exper-imental results with a supervised learning model based on linguistic features and compares them against several base-lines and a state of the art learning model.

The linguistic implications are also important to mention here. We hope that the corpus investigations presented in this paper provide new insights for the machine transla-tion and multilingual question answering communities. Not all noun phrase syntactic constructions are appropriate to translate a compound instance from one language to an-other. For example, although a fairly large number of noun phrase instances are translated into Spanish following the pattern  X  NdeN  X , this is not always the case in the other di-rection. For instance, market analyst translates into analista de mercado , but causa de preocupaci  X  on cannot be translated into worry cause .

For future work we consider the following issues: a. In our experiments so far we used only the data pro-vided by an on-line dictionary where the noun phrase in-stances and their translations were provided out of context. However, an automatic cross-linguistic noun phrase inter-pretation tool has to rely on contextual information for an accurate translation. We would like to investigate various methods and see how context-dependent is this task and what exact contextual information is needed for it. b. A much larger corpus is needed to get accurate explana-tions of the various issues encountered, such as the contri-bution of each feature to the overall performance. The dis-tribution of the corpus used for this research was somehow skewed toward some semantic relations. Thus, we would like to test the model on other text genre as well. c. We need to study further the semantic (ir)regularities among English and Romance NPs in both directions. Al-though the English instances and their translations encode the same meaning, our observations on the data show that there are various differences among the instances in each language. d. Besides the  X  X  N X  and  X  X  P N X  patterns, another fre-quently occurring syntactic construction is  X  X dj. N X  (En-glish) and  X  X  Adj. X  (Romance). We would like to extend our procedure for their interpretation as well.
We would like to thank Prof. Richard Sproat for his feed-back and support of this project, and Dustin Parr and Matt Garley for their help with the corpus creation and annota-tion. Last, but not least, we would like to thank the anony-mous reviewers for their constructive comments. [1] Barker, K., and Szpakowicz, S. Semi-automatic [2] Cornilescu, A. Romanian nominalizations: Case [3] Giorgi, A., and Longobardi, G. The syntax of [4] Girju, R., Moldovan, D., Tatu, M., and Antohe, [5] Girju, R., Moldovan, D., Tatu, M., and Antohe, [6] Gocsik, K. English as a Second Language .
 [7] Kipper, K., Dong, H., and Palmer, M.
 [8] Lapata, M., and Keller, F. The Web as a baseline: [9] Lauer, M. Corpus statistics meet the noun [10] Meyers, A., Reeves, R., Macleod, C., Zielinska, [11] Mihalcea, R., and Faruque, E. Senselearner: [12] Moldovan, D., and Badulescu, A. Asemantic [13] Moldovan, D., Badulescu, A., Tatu, M., [14] Moldovan, D., Badulescu, A., Tatu, M., [15] Moldovan, D., and Girju, R. Knowledge discovery [16] Nakov, P., and Hearst, M. Search engine statistics [17] Pantel, P., and Pennacchiotti, M. Espresso: [18] Pantel, P., and Ravichandran, D. Automatically [19] Pennacchiotti, M., and Pantel, P. Ontologizing [20] Rosario, B., and Hearst, M. Classifying the [21] Rosario, B., Hearst, M., and Fillmore, C. The [22] Turney, P. Expressing implicit semantic relations
