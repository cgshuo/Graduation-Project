 1. Introduction
Data warehouses (DWs) are complex systems serving as a repository of an organization X  X  data. Apart from their role as and finally, business intelligence tools for performing analytical operations. Conceptually, data warehouses are used for the flow of data from operational systems to decision support environments. The process of gathering, cleaning, transform-source data stores) is assigned to the Extract-Transform-Load (ETL) processes.

The design of data warehouses through ETL processes constitutes a troublesome and time-consuming task with a signif-icant cost in human, system, and financial resources [44]. The main problems are due to the inherent amount of complexity forms them according to the target format, and finally loads them into the target warehouse (or target data store).
Meanwhile, the complexity of data warehouse environments is rising every day and data volumes are growing at a sig-nificant pace. The data warehouse administration and design group should manage enterprise information efficiently, and with high quality results. Approximately 30 X 50% of the data warehouse design process is spent on analysis, to ensure that source systems are understood and that there is alignment between the involved parties: business and administrators. Usu-continuously refined through a feedback process (Fig. 1-II). An integrated metadata strategy may reduce the time needed, lower risk, and produce an ongoing record of understanding, useful in the whole data warehouse lifecycle. Hence, it is of this early stage. Moreover, the proposed solution should be simple and comprehensive enough to be used by all the parties involved, which usually have different technical skills, knowledge background, and communication codes.
Several research approaches have been proposed for the ETL conceptual design e.g., [20,42,44] following either ad hoc ties should have good knowledge of the design method used. On the other hand, commercial ETL tools e.g., [10,11,23,24] erCenter, one may use the PowerCenter Data Stencil and the MS Visio to create conceptual designs. Additionally, in several real-world projects spreadsheets or text documents are used for the task of representing the conceptual solution. Although of precise metadata.

In a previous work, we demonstrated that an ontology-based approach is suitable for capturing the information needed tating the data stores, correspondences between the sources and the data warehouse are inferred automatically by a rea-language (specifically, in OWL-DL [39]), which does not favor the communication among the different parties and makes the validation of the result harder for the designer (Fig. 1 -II).

In this paper, we tackle this problem by exploiting the fact that the ontology contains all the appropriate metadata describing the ETL process and we translate the result of the reasoner into a narrative, which represents the most natural means of communication. In particular, we complement our previous effort, focusing on the establishment of a common application terminology and the use of the ontology, as a common language, to produce a textual description of meaningful reports concerning the outcomes of the ETL design phase, namely the data store annotations and the generated ETL scenario. reports, we devise a template-based technique, which is comprehensive, flexible, and customizable. It also allows to group together related information, so that the generated output is more concise, and thus, more comprehensive.
The usefulness and importance of this goal has already been stressed in the literature. Previous work emphasizes the ben-efits of the automatic technical document generation [2,29] . It is denoted that automatic document generation is probably dards are followed, and simplifying the process of updating documents to react to changes in the documented hardware X  [29].

Notice that by providing NL reporting for conceptual ETL design, we do not aim at substituting existing conceptual mod-els; rather, we offer a complimentary functionality to them. NL reports can be used without requiring particular technical of reports in multiple languages without human intervention [2]. 1.1. Contributions
The main contributions of our work are the following:  X  We point out the usefulness of representing the formal requirements and specifications of ETL processes in a comprehen-design, implementation, maintenance, and documentation.  X  We demonstrate that an ontology, apart from constituting a formal model for capturing application requirements and automatically inferring correspondences among the data stores, can also serve as a means for verbalizing these requirements.  X  We discuss how a common application terminology can be established semi-automatically, using linguistic techniques.  X  We introduce a template-based technique to represent the semantics and the metadata of ETL processes as a narrative, based on information stored in the ontology, which captures business requirements, documentation, and existing schemata. as the grouping of related information to produce more concise and comprehensive output.  X  We discuss implementation issues and the impact of ETL evolution to our approach. 1.2. Outline
The rest of the paper is structured as follows. Section 2 discusses related work. Section 3 presents our framework for ontology-based design of ETL processes. Section 4 focuses on the use of linguistic techniques to draw a common application terminology from the source and DW schemata. Section 5 provides an overview of our previous work on the generation of the ETL requirements and operations from a formal language to a narrative. Section 7 delves into meaningful report produc-on the design. Finally, Section 10 concludes our results. 2. Related work
This section presents efforts related to the conceptual design of ETL processes, ontology creation, ontology translation, and NL applications in databases. 2.1. Conceptual models for ETL cerns a well known  X  but, only to the technical people  X  modeling technique, such as UML e.g., [20] or Model Driven Archi-with easy to understand and review reporting capabilities.
 In addition, a semi-automated method exploiting ontologies for the design of multidimensional DW is presented in [31]. The method handles potentially heterogeneous data sources, and assumes an ontology that represents the business domain. dimensions and measures have been pointed out, potential facts can be identified: the more potential dimensions and mea-
Although, that work does not deal with the back stage of the data warehouse, we find that our work is complementary to any reporting issues, which is our main focus in this paper. 2.2. Ontology creation
In [18], the authors give an overview of the terminology extraction methods and they show how to detect inconsistencies in the requirements document and how to extract an ontology after eliminating these inconsistencies. The issue of the re-ontology extraction in the context of requirements engineering, in spite of the necessary manual intervention. Other ap-proaches have dealt with the issue of resolving differences in terminologies in distinct data sources [14]. Such solutions can be useful in the early phases of our method, namely term extraction and ontology construction (see Section 3). 2.3. Ontology translation
There have been efforts towards the generation of textual representations from ontologies e.g., [1,2,45,46] . These ap-ations used in our case for semantically describing the data stores and inferring correspondences among them. Hence, even though in principle it would be possible to use one of these approaches, the resulting output would be too verbose and ficult to customize the output and achieve different levels of granularity according to particular information needs. 2.4. NL and databases
Typically, works related to database and NL research have dealt with the task of the automatic extraction of semantics from NL, which is the inverse problem of the one we are dealing with e.g., [39,41] . More related to our context are works dealing with validation, formalism, and automatic documentation. 2.4.1. Validation
The research community has studied the problem of the validation of the results. [30] presents a Requirement Engineer-ments. Apart from the linguistic approach, it discusses the issue of conceptual schema validation based on text generation they deal with the view integration problem considering the issue of detection of synonyms and homonyms using NL techniques. 2.4.2. Formalism [19] discusses the automation of the conversion from a requirements document written in NL to a formal specification ism of NL and formal specification language. 2.4.3. Automatic documentation
Several approaches deal with the automatic generation of technical documentation and reports e.g., [2,12,29,34,40] .In [29] the authors deal with the automatic generation of technical documentation. In particular, they discuss technical is-sues, as well as the costs and the benefits, of using Natural Language generation techniques, in combination with ontol-ogies and linguistic and contextual models, to automatically produce technical documentation. In [2], the authors describe an approach for the automatic generation of reports from domain ontologies. [34] describes the translation of logical database subsets (i.e., a database schema following the same rules and constraints as the original one, which contains and problems for the translation of database content and elements (e.g., queries, views, and so on) into natural language description.
 ferent sources or the validation of the produced reports.) The results on automatic documentation and ontology creation using NL techniques, are not directly applicable to our environment, mostly due to the existence of complex and composite transformations (e.g., functions, aggregations, surrogate keys (SK) assignment, and slowly changing dimensions (SCD) Type for that goal.

This paper is an extended version of [35]. The original version [35] contained the full-fledged framework of our approach, the handling of terminology issues, the lexicalization process, and a detailed presentation of our template mechanism con-thorough discussion on related work, production of ETL design, and implementation. In addition, in this paper, we discuss 3. System description
In this section, we present our method for ontology-based ETL conceptual design and reporting. The underlying idea is the ously, then, we do not need to perform all the steps of our approach, as we describe next.
 Our approach comprises three main phases, as depicted in Fig. 2 .
 ETL design and reporting. We discuss this phase in Section 4.
 domain ontology does exist already, then the pre-processing phase and the ontology construction step will not be needed.) overview of this phase.
 the reports. The template mechanism uses predefined templates in order to represent the design constructs in a textual description, which is the output of this phase. The reporting phase is presented in Section 6. As we discuss in Section 7, the report production should take under consideration user requirements, skills, and needs. We present a method for pro-viding users with a set of alternative reporting styles that match user profiles.

Before proceeding with the details of our approach, we introduce a reference example for better illustrating the concepts introduced. 3.1. Example ETL scenario We consider a source data store DS _ SPart , and a target data store the parts were purchased and their price. DW _ SupPart keeps records only of parts purchased after the year from 2 to 5 suppliers. Prices in DS _ SPart and DW _ SupPart cords the quantity of stored parts for each individual storage location, while all locations. Lastly, DS _ SPart contains information for parts in the categories software, hardware or accessories ( while DW _ SupPart contains only software or hardware parts ( semi-automatically during the pre-processing phase. Next, the application ontology is created. The ontology can be repre-sented as a graph and an incomplete version of it for our reference example is depicted in Fig. 4 .
Using this ontology, and OWL-DL constructs, the two data stores are annotated, respectively, by the following formal expressions: DS _ SPart SuppliedPart u =1hasPartID u DW _ SupPart SuppliedPart u =1hasPartID u
The reasoner then identifies the appropriate conceptual operations that should take place. Table 1 contains the list of transformations identified for this example. At this point, the aforementioned expressions and the list of transformations [37].

The design process is complemented by our reporting functionality. Different kinds of reports can be produced covering the internals of the ETL process. An example textual description corresponding only to the information about
DS _ SPart to DW _ SupPart data stores requires the following operations: retrieve, convert, and store. In more detail: 4. Pre-processing phase
Usually ETL processes deal with highly heterogeneous sources, which sometimes follow very different formats; e.g., from common vocabulary among the elements of the source schemas to drive their inter-attribute mappings. This phase deals with the different terminologies of the data stores, aiming at establishing a common vocabulary among them, in order to overcome naming conflicts. This is performed in two steps: term extraction and term selection.

In the first step, the names of the elements of each individual data store are analyzed and mapped to concepts using a [27]. For example, in Fig. 3 b the attributes SPart.sCode whereas the names of the data stores are mapped to the concept that the designer has to verify the extracted terms and provide the correct term for elements that were mapped wrongly or not mapped at all. The amount of manual effort required depends on the quality of the naming scheme that was used during ing types of information: (a) the concepts of the domain, (b) the relationships between those concepts, (c) the attributes cuss this issue further in the next section.

In the second step, the concepts identified for each individual data store are merged to create a unified terminology, which will be used as the basis of the constructed application ontology. The merging process is based on identifying syn-onyms (e.g., price, value, cost) and hypernyms (e.g., address, location) among the terms. When two or more synonyms are onyms is the one that was extracted from the target schema, so that the derived terminology is biased towards the target terminology. For example, in Fig. 3 b for the two synonyms hypernym, the underlying mappings, i.e., the data store elements that were related to this term, are updated accordingly. less manual effort than the first.

In addition, during the design of a data store schema, it is a common practice to include a short textual description for cation terminology, significantly increasing the accuracy of the process, and consequently further reducing the required onyms, as well as distinguishing between homonyms. 5. ETL design phase
In this section, we provide an overview of the production of the conceptual ETL design. However, this material is not a contribution of the current work; we simply describe here only the basic ideas and corresponding terminology and notation, refer the interested reader to our previous work [37].

This phase aims at the construction of the conceptual ETL design, which comprises a set of data store annotations and a list of transformations needed for the inter-attribute mapping of the source and target schemata. For achieving that, an ontology is used as a common reference model and drives the mapping process. The use of the ontology is beneficial be-cause the schema of a data store describes the way the data are structured, but it does not provide any information for their intended semantics. In many cases, such ontology may already exist; e.g., as a supply accumulated by previous ETL projects of the same domain, like financial data or data related to human resources, product orders/sales, and so on. How-extracted terminology (see Section 4) and exploiting available knowledge and requirements regarding the data stores. The manual work required during the ontology creation phase is not an additional overhead that our approach imposes on the designer. The process of integrating the information from the available data sources and populating the data warehouse through the construction of an appropriate ETL workflow requires that the designer first clarifies and resolves the seman-tics of the involved sources, either this is done by means of a naming convention [36] and a reference vocabulary or a global schema, a UML model, and so on. Following an ontology-based approach presents some significant benefits over other alternatives, such as: (a) explicit and formal representation with well-defined semantics, which allow leveraging on existing reasoning techniques in order to automate several parts of the process (this cannot be done with the other approaches, as when using for example a UML model); (b) better support for reuse and evolution (see also the discussion in Section 9); and (c) better support for visual representation and documentation. In this work, we focus on the latter point.

In general, an ontology describes a knowledge domain by means of classes and properties. For our purposes, the ontology representation formats, ranges of values or aggregated attributes. Hence, the constructed ontology comprises the following elements:  X  a set of classes, C C , representing the concepts of interest in the application domain;  X  a set of properties, P P , representing attributes of these concepts or relationships between them;  X  a set of classes, C TP , denoting the ranges of properties in P  X  a property, convertsTo , relating classes in C TF , indicating that a conversion from one format to another is possible;  X  two properties, aggregates and groups , indicating, respectively, which aggregation function is used in a particular
The ontology is represented as a graph, which makes it easier for the designer to create, verify, and maintain the ontol-ogy, as well as to use it as a means of communication between the different parties involved in the project. Thus, classes correspond to nodes and properties correspond to edges, as shown in Fig. 4 . The data stores are then annotated by attach-ing to them logical expressions formed using the classes and the properties in the ontology. Essentially, these are con-structed by specifying mappings between each data store graph and the ontology graph. The produced formal denoted by 8 P : C , and specify that the range of the property P is restricted by the class C. The latter are formed as = example presented in Section 3).

Finally, based on the information provided by the formal annotations, a reasoner is employed to infer semantic correspon-dences and conflicts among the data stores and to propose a set of generic conceptual operations for transforming and cleansing source data to populate the DW data stores, ensuring that the target constraints, requirements, and formats are satisfied. We consider a set of generic conceptual operations that can capture a wide variety of transformations typically occurring in ETL processes, as shown in Table 2 . This set contains, among others, operators like filter, project ( mation. However, our work does not anticipate the formal determination of the functionality of each transformation, rather we aim at the identification of a conceptual transformation, whose functionality will be determined later by the designer through a template library similar to the one proposed in [43]. For example, a as an SQL query, an XPath query or even a Web Service call, depending on the underlying data source. A may require a simple comparison of values or a complex regular expression. A application operation, which may be implemented as e.g., a simple conversion between different units of measurements or may even be carried out by a separate, composite business process.

The applied reasoning process serves two main purposes: (a) to identify the relevant data sources, and, specifically, the priate transformations for data stemming from the identified source elements, so that the constraints and requirements of provider for a given target element, if their corresponding expressions have a common superclass and are not disjoint. The two elements do not contradict each other (e.g., different ranges of values). The second task deals with how the data re-corresponding elements. The result is a series of ETL operations; regarding our reference example, this result is shown in sented next. 6. Reporting phase
This section discusses the components of the reporting phase, namely entity lexicalization, template creation, and report production (see the system architecture in Fig. 2 ). 6.1. Lexicalization words or phrases to use for transforming an underlying message into a readable text. In our case, this issue arises when this task differs from the pre-processing phase (presented in Section 4), since the common terminology of that phase: (a) may not be ready for translation due to programming restrictions (e.g., no spaces are allowed in the names) and (b) is not necessarily used for the construction of a new ontology, but for the matching with an already existing ontology that may or may not be appropriately annotated for the translation process.

Therefore, we should attach to the ontology elements appropriate labels for rendering them in a textual format. In OWL it the entity X  X  name. First, the name is tokenized, based on common heuristic rules, such as capitalization, underscores, and dashes. Then a look up to a lexicon takes place, using common string matching techniques, such as the edit distance. If a plied by X , respectively.

In summary, due to the special characteristics of the ETL environment the following solutions fit in our case: 1. The ontology is already annotated and it contains the appropriate labels from a previous attempt to design the ETL 2. We use a naming convention based on the entities X  names; the assumption that the name of each entity is meaningful 3. We create appropriate labels during the construction and/or maintenance of the ontology. Since, our application ontol-4. We use string matching techniques [32] (e.g., edit distance and n-gram similarity) and a lexicon for annotating the 5. We use a hybrid-method based on different combinations of the above.

Both approaches (2) and (3) may be used in combination with (4). For instance, regarding the third case, it is not always entities, having probably different comments attached to each one of them, are mapped to the same ontology entity. In our
WordNet). 6.2. Templates
In the following, we present our mechanism for the derivation of reports describing the outcomes of the ETL design phase, without intermediate representations) to the linguistic surface structure, and arguments are given against the perception of providing a comprehensive and intuitive way to customize the produced output, without requiring that the DW designer should understand complex natural language processing techniques. 6.2.1. Template language
The translation process is realized by suitable templates, which are constructed using elements provided by our template built-in functions and macros suited to the ETL design task. Notice that the generated text may also contain HTML tags, so that highly formatted output can be produced. Next, we describe the aforementioned constructs in more detail. 6.2.1.1. Variables. A template may contain variables, which are denoted by their name preceded by the symbol $. When the template is instantiated, the template engine that processes it replaces each variable with a provided value.
Specifically, the directives #set, #if/ #elseif/ #else , and tors are also supported.

Table 3 . 6.2.1.4. Macros. Macros allow simpler templates to be reused and/or combined to define more complex ones. Thus, they sig-types of ETL operators, to specify the textual description of these elements. The designer may customize and extend the translation mechanism, by modifying these macros or defining new ones. An example general-purpose macro that renders the elements of a list follows:
Macro: LIST(L) #set ( $ size = #SIZE( $ L) ) #set ( $ counter = 0 ) #foreach( $ item in $ L) #end
In the result, a comma follows each list item, but the word  X  X nd X  comes before the last one. Observe that standard pro-gramming knowledge is enough for the macro creation. Apart from such macros that concern generic functionalities, we do provide as well default macros for translating data store annotations and conceptual ETL operations, as shown in Tables tive code.) Having constructed appropriate templates based on macros ensures the reusability and extensibility of the reporting mechanism. 6.2.2. Template instantiation instantiated by expanding any contained macros, evaluating any contained functions and directives, and assigning concrete values to its parameters. Thus, the template instantiation is realized in the following order:  X  First, the macro definitions are replaced by their expansions.  X  Then, the functions and variables appearing in #set directives and loop boundaries are calculated.  X  Next, any loop that may exist is evaluated.  X  Finally, the rest variables are instantiated.
 ables are instantiated. 6.2.3. Report production
The format of a report is highly dependent on each application. In our experience and understanding, for the early phases Case 1 . A template for rendering the annotation D of a data store S is structured as follows.

Template: PRINT _ ANNOT(S, D) #set ( $ head = HEAD( $ D) ) #HEADER( $ S, $ head ) #set ( $ res _ list = #PARSE _ ANNOT( $ D) ) #foreach( $ res in $ res _ list) #end Given the annotations of the source and target data stores ple of Section 3, the above template produces the reports depicted in Fig. 5 a and b.
 Template: PRINT _ ETL(W)
Transformations from $ source to $ target: #set ( $ op _ list = #PARSE _ FLOW( $ W) ) #foreach( $ op in $ op _ list) #end Given the ETL operations for the running example illustrated in Table 1 , the above template produces the report shown in Fig. 5 c.

Case 3 . Apart from rendering a complete description of the ETL flow, a template can focus on information regarding spe-of CONVERT operations.

Template: PRINT _ ETL _ STATS(W) #set ( $ n1=0) #set ( $ n2 = 0 ) #set ( $ op _ list = #PARSE _ FLOW( $ W) ) #foreach( $ op in $ op _ list) #end
This ETL flow contains a total of $ n1 operations. $ n2 of these are CONVERT operations.

As shown from the sample report of Fig. 5 c, the resulting output may often contain repeated information. Even though in other cases a more concise representation is preferable. This issue is addressed by grouping together related pieces of information, a process that is commonly referred to as sentence aggregation [4,28] in Natural Language Processing. In our grouping together restrictions on the same property, (c) grouping together operations of the same type, and (d) grouping together operations on the same attribute.

Although this can be done programmatically by specifying the appropriate conditions in the corresponding templates, to reduce coding effort and simplify the templates, we overload the built-in functions providing two new variations for each one: PARSE _ ANNOT(D,R), PARSE FLOW(W,P). The first returns only restrictions of type R , while the second returns only restrictions applied on property Similarly, the third and the fourth return only operations of a given type
Hence, using in addition the function SIZE(L) shown in Table 3 , the previous template can be significantly simplified as follows.
 Template: PRINT _ ETL _ STATS _ SHORT(W)
This ETL flow contains a total of #SIZE(#PARSE _ FLOW( $ W)) operations. #SIZE(#PARSE _ FLOW( $ W, X  X  X ONVERT X  X ) ) of these are CONVERT operations.
 concerning a specific property. The latter case is especially useful to track the transformations occurring on a specific administrator to design the execution order of an ETL workflow [33]. 7. Report presentation
In this section, we first discuss the need for customizing ETL reports according to user needs and managing the report sizes, and then we describe a method for achieving that. 7.1. Report customization once.

For example, one usage of the reports is as a way to communicate with the business users so that at this early stage they can catch any deviation from the business requirements. However, such users typically are neither satisfied nor able to understand an overwhelming amount of technical details. Therefore, we would prefer to avoid overloading a business user able to provide him/her with the portion of the design results that are more relevant for him/her at a given moment. results when he/she is provided with a very long list of such? How to spot the design results of a particular entity? How relevant or even useful is it to present certain details of the design to a business user?
To cope with these challenges, we consider a customization method that enables the definition of distinct kinds of reports the user can choose between a high-level view and a more detailed one and navigate through related views along the appro-priate network links associated to his profile.

For the business users (and for the initial communication with them) high-level views would be more useful. For exam-ple, a high-level view could provide information on the mappings at a higher level; e.g.,  X  X  relation R understand that a certain requirement he/she may have, requires for example more functions and that may increase the overall cost of the process.
 sition to the logical model when the conceptual model matures enough. For example, a given user according to his/her role together). 7.2. Navigation space and properties that are valid in the navigation space, like navigate , related , and report .
 user.business _ level, user.moderate _ level , and user.detailed in fact, each template is related to at most one level of detail per dimension. Consider an example template lated to the design.transformations _ high _ level, formatting.bullets Depending on his/her profile, a user may or may not see the template dimension and any level of detail of the formatting dimension can be used. Since the template trary, this user cannot use a template involving the level
Formally, the navigation space D is a multidimensional space that is composed of a finite set of dimensions d
Each dimension d has a name and a finite, partially ordered set of levels l
The levels are partially ordered following either an order of increasing detail or an order decided by the designer. For example, the first level of detail of the design dimension suggests that only high-level mappings among source and target
DS _ SPart  X  X  attribute price to the respective attribute of the target
The network N is defined as a graph comprising the levels of detail as nodes and the links among levels belonging to dif-ferent dimensions as edges: A profile P is defined as a subgraph of the network: A view W is defined as a path in the network consisting of one level from each dimension:
A view W k can be used by a specific user having a profile P the set of nodes of the profile too:
Of course, more than one view can be used by a certain profile. A user is allowed to navigate through these views and only these.
 ing to different dimensions of the navigation space:
Note that in some cases, a view may contain more levels than the respective template, since the view should contain one level per dimension, whereas a template might not be related to all dimensions. Hence, a view may be related to multiple templates. We have chosen to be more flexible with templates (and this is a reason for introducing views as well) for two dimensions after the creation of a template, by giving him/her the option to omit some levels that are not so relevant to the created template. Secondly, for ensuring the reusability of templates; e.g., an example template the design.transformations _ high _ level and formatting.bullets dimension.
 template is related to levels of detail that belong to the set of nodes of the profile:
Finally, let us comment on the extensibility of this approach. The concept of navigation space is generic enough to allow may add a new dimension called multi-language dim having as levels the different language that the system can support, such as English level, German level, Greek level, Italian level, Spanish level such dimension requires multi-lingual templates as well. In the same way, a new level can be added in an existing dimen-sion, as depicted in Fig. 7 for the formatting dimension. 8. Implementation issues
This section discusses implementation issues related to the pre-processing and reporting phases. We do not describe here [37].

Our prototype uses the Java programming language and a collection of open-source API X  X  and tools. WordNet [48] is used as a semantic lexicon for identifying synonyms and hypernyms, during the extraction of the application terminology. The application ontology is represented in OWL. Management of the ontology is performed by means of the OWL API provided by Jena, a Java-based, open-source framework for building Semantic Web applications [49]. Reasoning tasks, such as infer-ring subsumption relationships between the ontology classes, are performed by Pellet, an open-source OWL-DL reasoner engine, which is often used in applications that require separation between the business layer and the presentation layer.
The built-in functions described in Section 6.2 are implemented as a collection of methods, written in Java, which can be ule to the Prot X g X -OWL [52] editor.

In the following, we describe the report generation module in more detail. As mentioned above, this module is supported by the Velocity template engine, which provides a language for specifying templates that can reference Java objects. This plate layer. For example, considering the indicative template:
This ETL scenario contains $ n operations. stored in the file example.vm, the following simple Java code:
VelocityContext context = new VelocityContext(); context.put( X  X  X  X  X , 10); Template template = Velocity.getTemplate( X  X  X xample.vm X  X );
BufferedWriter writer = new BufferedWriter(new OutputStreamWriter(System.out)); template.merge(context, writer); will print the statement: This ETL scenario contains 10 operations.
 sider the case of rendering the annotation of a data store (see Fig. 8 ). For this purpose, we use two generic classes, tionParser and AnnotationObject . The first parses the formal annotation of the datastore (specified in OWL, as required by the functions in Table 3 ). Then, an instance of the providing the according set() and get() methods. This object is made available to the template author through the  X  X  X on-text X  mechanism described above. Thus, the template author can easily construct an appropriate template to create a report using any of the information provided by the functions of Table 3 . The same process is used for the case of reporting ETL flows, using the corresponding classes ETLParser and ETLObject
Finally, a Profiler module communicates with the template library and is responsible for filtering out templates that user chooses a predefined profile. Currently, the choice of a template for the translation is realized manually; the user prefer to lose in detail in order to get a more concise result.

The repository contains the following information: (a) the schema of the navigation space, as discussed in Section 7, (b) past activity, and (d) metadata information.

The mechanism for report production is abstractly depicted in Fig. 8 . 9. Handling ETL evolution events
As discussed, ETL processes comprise a set of individual operations that are interconnected with each other. As a simple example, assuming two subsequent operations o 1 and o 2 in an ETL workflow, the latter depends on at the data warehouse schema. In both cases, the source or target schemata change and that makes the produced design ob-requires human intervention.

A previously proposed approach identifies that the impact of changes affects the ETL related software in two ways: (a)
ETL environment. The crux of that work suggests to exploit a graph-based representation of ETL processes, in order to keep the change, block the change or prompt the designer/administrator to interactively decide on a specific event.
Although in our context we deal with a higher level of abstraction and we consider the ETL design at the conceptual level, proposed changes. At the conceptual level, a potential change may affect the ETL design mostly semantically. For example, and correction of an affected mapping, the resulted ETL design absorbs the change. Whereas the automation of both tasks templates constructed as described in Section 6.

In addition, once the inconsistency gets resolved, the ETL design should be rendered again. One obvious question involves the incremental flavor of this process. In general, to reason with changing knowledge bases is currently considered as an important challenge, namely incremental reasoning. The key idea of this challenge is to maintain as much state information from previous reasoning cycles. The current state of the art DL reasoners do not handle updates incrementally. However, we do not support incremental reasoning and consequently, we do not support incremental production of only the affected limitation does not impose any realistic setback, the incremental update of the design belongs to our immediate plans. 10. Conclusions Representing the formal requirements and specifications of ETL processes in a comprehensive textual format, resembling
NL, is useful for facilitating the communication among the involved parties and the overall process of design, implementa-production of ETL reports based on Semantic Web technology. More specifically, in this paper, we have focused on the con-ceptual design of ETL processes and we have provided a means for translating conceptual ETL designs into natural language.
We have considered an approach to the automation of conceptual design using Semantic Web technologies. First, the in-volved data stores are mapped to a domain ontology, and then, the ETL design is constructed using reasoning. We have exploited the richness in metadata that this approach has and we have shown how linguistic techniques can be used in syn-ergy with the ontology and the proposed template mechanism, for translating the semantic annotations of the data stores and the required ETL conceptual operations, from a formal syntax to a textual description. Additionally, we have presented a method for producing meaningful results in accordance with user needs and we have demonstrated how one can navigate through a multidimensional space of alternative reporting options. Finally, we have discussed implementation issues and how our approach can adapt issues related to the evolution of ETL processes.
 improve the naturalness of the generated textual reports. Another challenge is to improve our method for report presenta-in the presence of changes.
 Acknowledgments
We thank the anonymous referees for their careful reading of the paper and their valuable comments that significantly improved its quality. Also, we thank the participants of DOLAP X 08 whose feedback during the presentation of the original on the report presentation.

References
