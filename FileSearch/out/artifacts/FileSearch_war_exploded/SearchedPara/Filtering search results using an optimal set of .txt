 1. Introduction
When searching textual databases by keywords, a large number of the retrieved texts may not be relevant to the searcher/user. An automatic filtering system, which learns the user  X  s preferences and filters the search results accordingly, might be useful. Information filtering (IF) is a research area that provides tools for fil-tering out irrelevant information. It provides personalized assistance for continuous retrieval of informa-tion in situations of information-overflow in general, and on the Internet in particular. Information filtering combines tools from the field of artificial intelligence (AI), such as intelligent agents or software robots ( X  X  X oftbots X  X ), with information retrieval (IR) methods, geared to representing, indexing and retriev-&amp; Shoval, 2001 ). IF differs from traditional IR in that it deals with users who have long term interests (information needs), expressed by means of user profiles, rather than with casual users whose needs are ex-pressed using ad hoc queries ( Belkin &amp; Croft, 1992 ).

Artificial neural networks (ANN) have been used for modeling complex systems where no explicit func-tions correlating inputs and outputs are known ( Bishop, 1995 ), and to form predictive models from histor-ical data. ANN have already been applied in IF and IR systems ( Goren-Bar &amp; Kuflik, 2004; Pal, Talwar, &amp;
Mitra, 2002 ). Advanced algorithms that can train large ANN models, with hundreds or thousands inputs and outputs, are available ( Boger, 2002; Guterman, 1994 ). Analysis of the trained ANN may extract useful knowledge from it ( Boger &amp; Guterman, 1997 ). ANN modeling has been applied to a similar task of finding relevant words in e-mail messages ( Boger, Kuflik, Shoval, &amp; Shapira, 2001 ).

Keyword selection for the specification of user profiles or queries is sometimes a frustrating task. Many text analysis techniques can be utilized for this task. In this paper we describe a technique based on ANN modeling. We handle this task by training a large-scale ANN-based filter that uses all meaningful words in the document space (i.e. the data items) as inputs, and the user-given relevance rating as output. A novel feature of the proposed technique is the analysis of the trained ANN model that identifies the minimal set of words that still can classify data items correctly, according to their relevance to the user. To test this tech-nique we modeled and analyzed 12 sets of research papers abstracts, each consisting of 100 texts down-loaded and rated for relevancy by a user on a subject of his interest.
 The rest of this paper is structured as follows: Section 2 reviews some essential concepts in IR and IF.
Section 3 provides a brief introduction on ANN modeling techniques. Section 4 describes the ANN input reduction algorithm and Section 5 describes possible application of ANN to information filtering. Section 6 explains the ANN model building technique proposed and applied in this paper, and presents the results, and Section 7 compares them with the results of the classical Rocchio algorithm ( Rocchio, 1971 ) using data from previous research. Section 8 concludes and discusses further research issues. 2. Concepts in information retrieval and filtering
Information retrieval may be characterized as  X  X  X eading the user to those documents that will best enable him/her to satisfy his/her need for information X  X  ( Robertson, 1981 ). This definition (among many others) can be described in a model of information retrieval where the user expresses his/her information need using a query, and the system conducts a search for relevant information in some data space (e.g. a database of documents).

Interesting lessons learned from IR are in three main areas: text representation, retrieval techniques, and acquisition of user information needs. The vector-space model ( Salton &amp; McGill, 1983 ), according to which a document is represented by a (possibly weighted) vector of terms, is a very popular model in information retrieval. In this model, the user  X  s query can be represented as a vector of keywords in a similar way. The main task of the IR system is to match the two vectors of terms and provide the user with relevant data items that best match the query.
 Various IR models enable determination of the weights of terms in documents or in queries. The classic
Boolean model considers index terms to be present or absent in a document; hence the index term weights are all binary: 0 or 1. The Boolean model determines if a document is relevant or not-relevant ; there is no partial matching. Exact matching may lead to retrieval of too few or too many documents. In the vector-space model non-binary weights are given to index terms in queries and/or in documents. The weights reflect the importance of terms in the query or in a document. These weights are used to compute the degree of similarity between each document and the query; thus partial matching is achieved. One well-known method to determine term weights in documents is TF * IDF X  X  X erm Frequency quency ( Salton &amp; McGill, 1983 ). This method assigns a weight to a term in proportion to the number of its occurrences in the document and in inverse proportion to the number of documents in which it occurs. This method is based on the statistical observation that the more times a term appears in a text the more relevant it is for representing the document, and the more documents a term appears in, the more poorly it discrim-inates between documents in the collection of documents. Another model for determination of term weights is a probabilistic model, which uses the difference in the distribution behavior of words over all documents in a collection to guide the selection of index terms ( Robertson &amp; Sparck-Jones, 1976 ).
In IF systems, the users  X  long-term information needs are expressed as profiles. A content-based profile represents the user  X  s interest/needs by a set of terms. The profile can be defined  X  X  X anually X  X  (i.e. by the user), or generated automatically from a sample set of data items that are known to be of interest to the user.

Rocchio (1971) is a well-known algorithm for building a user profile using the vector-space represen-tation of documents. By using a training set of relevant and non-relevant documents, the user profile is generated as a linear combination of the vectors representing the training set. Hence, the user profile is an average, or a centroid, of the training set. One problem with this and with other content-based methods, such as the probabilistic model suggested by Robertson and Sparck-Jones (1976) , is the large number of terms that they generate to represent each document, as well as the large number of terms in the user profile.

Dimensionality-reduction is a well-known and widely researched topic in information retrieval and fil-tering, aimed at reducing computation time and storage space needed for document representation. Latent
Semantic Indexing (LSI) is one of those methods. It identifies semantic links between different terms, com-bines them and reduces the dimension of the original representation ( Deerwater, Dumais, Furnas, Landaur, &amp; Harshman, 1990; Furnas, Landauer, Gomez, &amp; Dumais, 1987 ). However, LSI is computationally com-plex and cumbersome to use ( Booker et al., 1999; Hull, 1994 ). Egecioglu and Ferhatosmanoglu (2000) sug-gest a new dynamic method for dimensionality-reduction. Karypis and Han (2000) suggest a fast-supervised dimensionality-reduction algorithm with applications to document categorization and retrieval.
Support Vector Machine (SVM) is an emerging technology used for text categorization and information filtering which tries to draw a separation line between two groups of examples (two categories of documents in our case). This is done by finding those examples,  X  X  X upport vectors X  X , whose attributes provide the max-imum separation between the groups, thus classifying correctly new examples. SVM has already been ap-plied in IF with good results ( Drucker, Shaharary, &amp; Gibbon, 2002; Sebastiani, 2002 ). The computational burden of the mathematical transformations needed for the SVM classification does not depend on the term vector length, which is a great advantage. A possible drawback of the SVM is that the computational bur-den increases with the third power of the number of examples, thus it may be too slow when the number of examples is large. For more information on SVM the reader is referred to ( Sebastiani, 2002 ), who reviews the field of text categorization, presenting the various technologies available to the researcher and developer wishing to take up these technologies for deploying real-world applications. 3. A brief introduction on artificial neural networks modeling
ANN modeling is done by learning from examples. ANN is a network of simple (sigmoidal, for example) mathematical  X  X  X eurons X  X  connected by adjustable weighted links. The most used ANN architecture is feed-forward two-layer ANN, in which neurons are placed in one hidden layer between the data inputs and the neurons of the output layer, and the information flows only from the inputs to the hidden neurons and from them to the output neurons. Training examples are presented as inputs to the ANN, which uses a  X  X  X eacher X  X  to train the model. An error is defined as the difference between the model outputs and the known  X  X  X eacher X  X  outputs. Error back-propagation algorithms adjust the initial random-valued model con-nection weights to decrease the error, by repeated presentations of input vectors ( Bishop, 1995; Rumelhart,
Hinton, &amp; Williams, 1986; Werbos, 1974, 1993 ). Once the ANN is trained and verified by presenting inputs not used in the training, the ANN is used to predict outputs of new inputs presented to it. Examples of many types of feed-forward ANN include ( Creput &amp; Caron, 1997; Koehn, 2002; Kurbel, Singh, &amp; Teuten-berg, 1998; Kwok, 1995; Lam &amp; Lee, 1999; Ruiz &amp; Srinivasan, 1999, 2002; Wermter, 2000; Yang &amp; Liu, 1999; Yang, 1999; Yu &amp; Liddy, 1999 ).

There are several obstacles in applying an ANN to systems containing a large number of inputs and outputs. Most ANN training algorithms need thousands of repeated presentations ( X  X  X pochs X  X ) of the in-puts to finally achieve small modeling errors. Large ANN tends to get stuck in local minima during the training. As most ANN training starts from an initial random connection weights sets, and the number of neurons in the hidden layer is usually determined by heuristic rules, many re-training trials with different random initial connection weights, or different number of hidden neurons, are needed to achieve a good model.
 An efficient training algorithm set, developed by Guterman and Boger ( Boger &amp; Guterman, 1997;
Guterman, 1994 ), can easily train large scale ANN models, as it pre-computes non-random initial connec-tion weights from the manipulation of training data sets, avoiding or escaping local minima during the train-ing. A useful added feature of the Guterman X  X oger algorithm set is that it recommends a small number of neurons in the hidden layer. Apart from these features, the ANN architecture used by the Guterman X 
Boger algorithm is the most common one X  X  X ully connected forward only, one hidden layer, and sigmoid activation function. The Guterman X  X oger algorithm was successfully used to train ANN models with hun-dreds to thousands of inputs and outputs ( Boger, 1992, 1997, 2002, 2003; Greenberg &amp; Guterman, 1996 ).
In real-life models, not all inputs are influencing the model outputs in the same degree. A knowledge extraction technique is the ranking of the inputs according to their relevance to the ANN prediction accu-racy. Calculating the relative contribution of each input to the variance in the hidden neurons inputs when the training set is presented to the trained ANN model does this.

This is a measure of the contribution of a specific input to the activity of all hidden neurons. A low relative contribution means that either the variance of the input is small, indicating almost constant value of this input in the presented examples, or that the ANN training has assigned low connection weights from that input to all hidden neurons, making it almost constant. In both cases a constant value can be added to the bias input of the ANN after deleting this input as less relevant ( Boger &amp; Guterman, 1997;
Boger, 2003 ). The detailed derivation of the input relevance calculation is given in Section 4. The least relevant inputs may be discarded and the ANN can be re-trained with the reduced input set that usually gives better prediction accuracy. The explanations for this possible improvement are: (a) Elimination of noise or conflicting data in the non-relevant inputs; (b) Reduction of the number of connection weights in the ANN that improves the ratio of the number of examples to the number of connection weights, thus reducing the chance of over-fitting small number of examples to a model with many parameters ( X  X  X ver-training X  X ).

The issue of over-training has troubled ANN model developers for a long time. It can be argued that the number of connection weights in the ANN has to be considerably smaller than the number of training examples. However, experience with real-world large systems indicates that this requirement is too conservative, as there are hidden relationships in the data that increase the generalization capacity of the trained ANN model ( Boger, 1997; Caruana, Laurence, &amp; Giles, 2000; Lawrence, Giles, &amp; Tsoi, 1997 ). 4. ANN input dimension reduction algorithm This section is based on Boger and Guterman (1997) .

Basic statistics ( Vandaele, 1983 ) provide that the variance of a linear combination of random variables x is given by: where R is an n  X  n covariance matrix R = cov( x ). If the variable set x is un-correlated, the calculation can be further simplified to:
However, in normal industrial plant data the variables are usually correlated in some way, so Eq. (2) is used in the present procedure. Defining the matrix W H to be the input-hidden layer weight set of the ori-ginal (trained but unreduced) ANN, and taking x p to represent the network inputs for the p th training example, then the input to the j th hidden layer node for the p th example is: where n is the number of network inputs. Following Eq. (2) the variance ( V node j over the full set of training examples is: where ( W H ) j represents the j th row of W H and R is the covariance matrix for the network inputs x , esti-mated from the training set. From Eq. (5) , the relative variance of the input to hidden node j can be cal-culated as: where n h is the number of hidden nodes. Eqs. (5) and (6) are the essential equations for determining the statistical relevance of a hidden node.
 Moving to the input relevance analysis, R i , the i th row of R , is associated with the i th input variable. From Eq. (5) , the contribution of input i to the variance ( V and the contribution of input i to the total variance of the hidden layer inputs is:
An equivalent, more concise form of Eq. (8) is: where ( W H  X  T i is the i th row of the transpose of W H relative contribution of input i to the variance of the hidden layer inputs can now be calculated:
Eqs. (9) and (10) are the equations used to determine the statistical relevance of network inputs. The V values serve as a check on the significance of particular hidden layer nodes. Nodes with low V good candidates for elimination. Calculation of the adjustment of the bias for an output node k compen-sating for the removal of the hidden node j is based on the expected value of o where W o represents the hidden-output layer weight set. A reasonable approximation of Eq. (11) is given by:
Since Eq. (12) is approximate, additional training by PCA-CG is used to re-tune the network after hidden nodes are removed. Input variables with low V rel I values do not contribute much information to the network and can be eliminated. Analogous to Eq. (12) the bias adjustment for hidden layer node j to compensate for removal of input i is:
Procedurally, one begins by training the ANN with the whole data set, with a reasonable value of the infor-mation content used for estimating of the number of the hidden nodes, for example, 70%. After training the
ANN with the PCA-CG algorithm, the best candidates for removal according to Eqs. (6) and (10) are iden-tified. The group of top candidates is removed, and the network is retrained using PCA-CG. On the retrain-ing step, different information content can be used in selection of the hidden layer architecture, according to the relative variance values of the hidden nodes. Our experience shows that optimum results are obtained when there is only one hidden node with a relative variance smaller than 10%, so a higher information con-tent is used when the least significant hidden node has relative variance higher than 10%, and a smaller information content is used when more than one hidden node have a relative variance smaller than 10%. 5. The application of ANN to information filtering
The idea to match the capabilities of ANN modeling to information retrieval is not new, and many pa-pers are dealing with it. Most of the papers use the unsupervised self-organized maps (SOM) technique for grouping similar examples into clusters ( Kohonen, 1997 ). Thus text clusters are formed based on the sim-ilarity of keywords in the texts. Once trained, the ANN will classify new documents as belonging to one of these clusters. Recent reviews discuss ANN along with other  X  X  X oft X  X  tools for Web mining application ( Pal et al., 2002 ) and text categorization ( Sebastiani, 2002 ).

The basic obstacle in training ANN models is the large dimension of the inputs (terms) representing the documents. As noted, large-scale ANN models tend to get stuck in local minima during the training, lead-ing to restart with different initial connection weights. Various techniques were used to reduce the number of terms to a manageable one. Ahmed, Bale, and Burford (1999) chose 50 terms with the highest  X  X  X eirdness coefficient X  X  from the top 5% of the most frequent terms. They defined this coefficient as the ratio of the relative term frequency in the specific corpus learned, to the relative frequency of the term in natural lan-guage. Dasigi, Mann, and Protopopescu (2001) used Latent Semantic Indexing as a method for dimension-ality-reduction in order to make ANN training feasible. They tried several approaches for features extrac-tion from a collection, all by applying LSI and even fusion of several LSI-based feature extraction approaches, in order to generate the input for training an ANN for text classification. Yu and Liddy (1999) employed a genetic algorithm, coupled with the  X  X  X aldwin effect X  X , to choose candidates for the re-duced term vector. Wermter (2000) reduced the number of terms according to their significance value, which is defined as the ratio of the frequency of word w in the semantic class c , to the sum of the frequency of this word in all classes. Other researchers used the well known TF terms ( Rauber, Schweighofer, &amp; Merkl, 2000; Tomsich, Rauber, &amp; Merkl, 2000 ) while Vlajic and Card (1999) deleted from the TF * IDF terms those with very high or very low correlation. As the Guterman X 
Boger algorithm can easily train ANN with thousands of inputs, no such term reduction is necessary for the initial ANN model ( Boger et al., 2001 ).

The ability of the ANN to model non-linear, non-obvious relationships can be applied to the matching of the textual features (inputs to the ANN) to the user relevance rating (ANN outputs). When applying statistical methods for the required modeling, subjective selections of the number of terms and the form of the model equations are made. No such assumptions are needed in ANN modeling.

Supervised ANN should be preferred over unsupervised approaches like SOM, as it is more adjustable to an individual  X  s user profile. SOM may classify texts according to their similarity, but eventually the user has to evaluate the number of clusters (too few or too many) and rank the clusters according to their degree of interest, as demonstrated by Goren-Bar and Kuflik (2004) .

The most important feature of ANN modeling is that the user need not specify which features (such as keywords) to extract from the text. If the ANN can use all the words in the text as inputs, the post-training
ANN analysis would reveal, according to the user profile, what are the more relevant words in the text. This would avoid the frustration of getting too many responses to a general query, or the suspicion of missing important results from a too narrow subjective selection of keywords. The trained ANN should then act as a sorter, evaluating each additional text according to the user profile requirements.

In Section 6 we demonstrate the feasibility of ANN modeling and keyword extraction, predicting the importance ranking of unseen texts based on ANN trained on texts ranked by the user. We employ a data-base used in a previous modeling of user profiles ( Kuflik, 2003 ). 6. Training an ANN as information finder
In order to use a categorization mechanism such as ANN for document filtering, an appropriate docu-ment representation is required. In our case we used a binary vector representation of terms to represent the documents. The dataset we used contained 12 sets of 100 documents each, which were returned as search query results issued at academic publications repositories (such as Econlyt that contains abstracts, indexing, and links to full-text articles in economics journals). It abstracts books and indexes articles in books, work-ing papers series, and dissertations ( EconLit ), Geobase, that is Worldwide literature on geography, geology, and ecology ( Geobase ), and INSPEC that provides scientific and technical literature in physics, electrical engineering, electronics, communications, control engineering, computers, computing, information tech-nology, manufacturing, production and mechanical engineering ( INSPEC ). These documents are abstracts of academic publications in specific areas (such as Software Engineering, Machine Learning, etc.). Such databases are used by researchers as sources for information in their daily research work. Several research-ers were asked to define search queries in their areas of expertise. The search queries (presented in Table 1 ) were used to query the repositories and the first 100 results of every set (abstracts of papers) were returned to the researchers who judged by them for relevancy on a 1 X 5 scale, 1 meaning least relevant and 5 X  X  X ost relevant.

Trivial words in the text were discarded and the rest were stemmed and counted. For ANN modeling, words in the bottom and top 5% count were discarded, and the rest were used to form a binary vector, where 1 signifies the presence of a word in the text. The average length of the resulting vector was 805 terms.
Thus, an ANN model was trained with the word presence vector as input, and with five hidden neurons and five binary outputs. The ANN target for a document is a 5-bit binary vector with 1 at the appropriate relevance ranking position and 0  X  s at the other positions.

The ANN was trained with the Guterman X  X oger set of algorithms described in the earlier sections. The trained ANN model was analyzed for identifying the more relevant inputs that were used to train another, smaller, ANN. The training and input reduction process was repeated until we noticed an increase of the prediction error of the training example set.

To evaluate the effect of the number of training examples, four different ANNs were trained for each data set, with 20, 40, 60 and 80 examples. The last 20 examples were always used as a validation set, i.e. they were not used in the training but presented at the end of the training as  X  X  X ew X  X  examples to asses the reliability of the current ANN model  X  s predictions.

The gathering and pre-processing of the training and testing data is the first phase (usually the most time-consuming phase) of ANN modeling. From a previous research ( Kuflik, 2003 ) we obtained 12 users  X  relevancy judgments of abstracts of research papers, using a 1 X 5 scale. We considered the values 1, 2 and 3 as non-relevant, and values 4 and 5 as relevant. The words in the abstracts were already stemmed and the common words were removed by a stop-list. We had also the filtering results of these sets for the Rocchio-based filtering. Rocchio (1971) is a classical, well-known algorithm used for building a user-profile in the vector-space model and thus we used these results as a reference in our research. These results are discussed and presented later on in Section 7 (For more details, see Kuflik, 2003 ).

In this research, the aim was twofold: (a) to predict the relevancy of abstracts, and (b) to evaluate the ability of the ANN to identify important keywords for a user profile. All the stemmed words in an abstracts group were combined into one  X  X  X eyword X  X  list whose length varied between 449 and 1092 terms X  X 805 terms on average. Each abstract was transformed into a binary vector of 1  X  s and 0  X  s, where 1  X  s signify presence of a word in the abstract. The data pre-processing to the form used in the ANN training consisted of changing the 1 and 0 binary inputs to +1 and 1 values, respectively, and adding a small random noise value to them.
This was done in order to avoid having an empty input column vector of constant 1 values. The [0,1] binary outputs were transformed into [0.1,0.9] values, avoiding the slow asymptotic approach to the target during the training.

The Guterman X  X oger algorithms set trained four fully connected ANN models with five neurons in the hidden layer, as this is the typical number needed for real-life data sufficient to achieve good prediction rates for each user. As said, the training was done with 20, 40, 60, and 80 examples, setting aside the last 20 examples as validation set. Each model was initially trained with the full word vectors, and then with the repeatedly reduced number of inputs, until no more reduction was possible.

The  X  X  X ptimal X  X  set of inputs was selected by observing the plots of the training and validation errors as a function of the decreasing number of words as inputs (see Fig. 1 ).

Each panel in Fig. 1 plots the change in the Root Mean Square errors of a particular (rank #) ANN output, of a data set (representing the error in predicting each level of relevancy between 1 and 5). The training example errors are marked by  X  X   X  X , and the validation example errors are marked by  X  X   X  X . The x -scale shows the inverse number of input terms (so the reduction process results are presented from left to right). The numbers on the plot are the number of terms in the current ANN model. As can be seen, the progressive reduction of the number of the original 692 terms does not have a significant effect on the training error until 12 terms (8  X  10 2 on the x -scale). Thus the ANN model with 12 terms should be se-lected. There is a beneficial effect on some of the validation classification errors. For example, the first point pair in the lower right panel shows the prediction error of the 4th ranking when the full 632 term set is used for training the ANN. The validation error seems to progressively decrease as the number of terms used for the ANN model training is automatically reduced, while the training error remains essentially the same low value. When the reduced term length is less than about 30 ( x -scale 3  X  10
It demonstrates our claim that the reduction of the terms used in the ANN model is both feasible and useful for text categorization. 7. ANN as predictor of term importance
As mentioned above, the trained ANN was used to filter the validation sets. The results of the ANN filtering are presented in Tables 2 and 3 , while Table 4 presents Rocchio results. Fig. 2 provides a graphical representation of the overall average performance using the  X  X  F  X  X  measure ( van Reijsbergen, 1979 ). Common measures for the performance of IF systems are  X  X  X recision X  X  and  X  X  X ecall X  X , where precision is the ratio of relevant documents retrieved (as judged by the user) out of the total number of document retrieved by the system, and recall is the ratio of the relevant document retrieved out of all relevant documents available in the collection. The  X  X  F  X  X  measure that we used combines precision and recall as follows: F = 2/(1/Precision + 1/Recall).

Table 2 presents the filtering results of the original (large) ANN . The Original Precision row presents the ratio of relevant documents in every set, giving an idea on the quality of that set in general. The next four rows present the filtering results after ANN training with 20, 40, 60 and 80 documents.

As it seems from Table 2 , increasing the number of training examples does not improve much the mod-els  X  performance. Paired T -tests were performed between every two consecutive sets-sizes, the results of the paired T -tests are: p = 0.12 between filtering based on 20 examples and filtering based on 40 examples; p = 0.38 between filtering based on 40 examples and filtering based on 60 examples; and p = 0.5 between filtering based on 60 examples and filtering based on 80 examples. These paired T -tests reveal that there is no significant difference between the results based on the different sizes of training sets. However, there is a significant difference between filtering based on 80 examples and filtering based on 20 examples ( p = 0.00016), and also between filtering based on 60 examples and filtering based on 20 examples ( p = 0.036). The difference between filtering based on 80 examples and filtering based on 40 examples is not significant though ( p = 0.084). This analysis shows that there is a slight improvement, as can be noticed in the table and in the graphical presentation, but in general the overall filtering performance of the original ANN seems to be poor: low values of  X  X  F  X  X , below an average of 0.2 for all training sets sizes.
Table 3 presents the results of filtering by the optimal ANN . The term reduction process described in Sec-tion 5 yielded a different ANN at every step, trained with vectors containing less and less terms for every set.
The optimal result achieved by a specific ANN is the best filtering result found for every case. (For every data set and for every size of training set, an optimal ANN with a different set of terms was found; this will be discussed in detail later on.) As can be seen from comparing Tables 2 and 3 , the optimal ANNs performed better than the original
ANNs for each and every case (with a few exceptions where the results were identical). Moreover, paired T -tests show that the differences between the optimal and the original ANNs are significant for every level of training (training sets sizes). The optimal ANN performed significantly better than the original ANN: paired T -test yielded p &lt; 0.00 for every size of training set in average (of all sets). Comparing the filtering performance of the individual sets, paired T -tests reveal that the differences are significant for almost every last set there are no results at the first steps).

Table 4 presents the results of Rocchio-based filtering (only 20, 40 and 60 examples were used for train-ing, while the next 20 were used for threshold adaptation and the last 20 for validation, as in our case). From looking at Tables 2 X 4 , we can see that it outperforms the full ANN in most cases, but the optimal
ANN outperforms it in average. Paired T -test reveals that this differences are significant ( p &lt; 0.00 between the full ANN and Rocchio and p = 0.033 between the optimal ANN and Rocchio).
 Looking at Fig. 2 , it is clear that the optimal ANN outperforms the initial ANN. Moreover, paired
T -tests confirm that the differences between the full and the optimal ANN are significant ( p &lt; 0.00). Fig. 2 presents also the filtering results of the classical Rocchio algorithm that were taken from Kuflik (2003) . (Only 20, 40 and 60 examples were used for training, while the next 20 were used for threshold adap-tation and the last 20 for validation, as in our case.) Note that Rocchio-based filtering outperforms the full
ANN (for the 20, 40 and 60 examples training sets used), but the optimal ANN outperforms both. Here too, in both cases paired T -tests revealed that the differences are significant ( p &lt; 0.00 between the full ANN and Rocchio and p = 0.033 between the optimal ANN and Rocchio).

Since the differences in performance between the two versions of the ANN are noticeable and significant, it is interesting to look more into the main cause for the differences, namely the terms vectors. Table 5 shows the sizes of the terms-vectors in each and every case. (As mentioned earlier, for each and every case an optimal ANN was selected, in most cases it had a different minimal number of terms.) It also shows the average size (Avg.) of the term-vectors and the original (Orig.) full vector used for training the  X  X  X ull X  X  ANN.
It can be seen that on average, the size of the optimal vector is 7.5% of the size of the original vector. In about 50% of the cases the optimal vector size is 20 terms or less. Not only that the optimal vector performs better, it also represents a significant reduction in storage space and computation effort. For example, the training cycle that starts with the full ANN and goes down to the least number of terms, for all four size sets of training examples, takes about an hour on a 2.4 MHz PC in an interpretive MATLAB environment, and would be faster in a compiled environment. Paired T -tests show that the differences in the sizes of the opti-mal vectors from the original vectors, in every case (every training set size) and on average, are significant ( p &lt; 0.00 in all four cases and for the average as well). 8. Conclusions and suggestions for further research
The results presented in Sections 6 and 7 show that the initial large ANN model that can be trained from non-trivial words in a text gives an inferior prediction relative to a classical statistically derived model (Roc-chio). However, the iterative term reduction process, resulting in reduced term-vectors and improved per-formance, outperforms significantly both the original ANN model and the classical statistical Rocchio model. The reduction process does not affect the training classification until a very low number of terms, typically between 10 and 30 terms, as discussed in Section 6 and demonstrated in Table 4 . The terms reduc-tion process sometimes improves the validation classification errors. The  X  X  X ptimal X  X  number of terms is, on average, less than 10% of the original number of terms.

The effect of the number of training examples on the filtering efficiency shows a different pattern, depend-ing on the example sets. For some of them the filtering is effective even with only 20 examples and this high efficiency does not increase much with additional number of training examples. This was typical for sets where more than 30% of the documents were relevant (sets 1 X 4, see first row of Table 1 ). On the other hand, in sets with less than 20% of relevant documents (sets 8 X 12, see first row of Table 2 ) there is an improvement with the number of examples.

Several important capabilities are made possible by the success of the  X  X  X ptimal X  X  ANN classification. The identified  X  X  X elevant X  X  words can be analyzed by the  X  X  X ausal index X  X  algorithm ( Baba, Enbutu, &amp; Yoda, 1990 ) to identify the effect of each of these words on the user preferences, and thus be used as  X  X  X ntelligent X  X  key-word set. It may be possible to use a small set of training example to generate a useful sorting ANN model that can be incrementally re-trained with the results of evaluation of more texts down-loaded by the intel-ligent key-word set.

Another interesting avenue for future research is the comparison of the efficiency of the reduced-ANN-based IR with the SVM algorithm. It is not clear yet if the SVM can efficiently classify more than two groups, or learn the minimal set of discriminating terms.
 References
