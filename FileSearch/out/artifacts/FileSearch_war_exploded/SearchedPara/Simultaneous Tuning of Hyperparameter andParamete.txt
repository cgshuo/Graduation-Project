 learning and applying Support Vector Machines (SVM) [1,2,3]. The common tuning issue of this gradient based strategy in [8].

Let be the hyperparameter vector, and  X  denote some estimation of SVM, such vious tuning methods could be summarized as follows: 1. Initialize , 2. Solve the optimization problem:g ( ) min  X  G (  X  ) , 3. Update hyperparameter with certain strategy, 4. Go to step 2 unless termination condition is satisfied. is, the convergence speed.

To address the problem, we propose a new framework that combines steps 2 and 3 into one: 1. Initialize X, 2. Compute f ( X ) , 3. Update X with certain strategy, 4. Go to step 2 unless terminating condition is satisfied, where X T T T , and are hyperparameter and Lagrange multiplier of soft optimal classifier and hyperparameter can be obtained simultaneously. end in Section 5 with conclusions and future works. 2.1 New Formula of Soft Margin SVM The soft margin SVM can be described as follows: Let  X  z x i T e T i get and let I The dual problem becomes to 2.2 Calculating Radius Margin Bound According to Burges [12], theorem bellow.
 Theorem 1. The generalization bound is smooth in . (Cristianini,1998. See [13]) sample points. So let D be the diameter 1 , D 2 x p x q 2 ,where p q argmax tone function of x i x j .
 Proof. Since Then R 2 r (0) K x p x q 2, if Gauss kernel is used, we have R 2 is determined only by kernel function, needing not to know the feature map : n m , m n . Obviously, formula 2.3 is more efficient than formula 2.2, as p , q on average.

In soft margin SVM,  X  K K I C ,then i.e. The radius margin bound changes to bound in [15]. 2.3 The New Tuning Approach From formula 2.5 we have: Combining C , 2 ,and into one vector X C 2 T T and constructing vector  X  Y 0 0 Y T T ,where Y y 1 y l T , we can describe the new tuning approach as follows:
The approach is derived from the transformed radius margin bound given by for-we can obtain the optimal classifier and the hyperparameter simultaneously. In this section we address the implementation issue of the new tuning approach. 3.1 A Synthetic Schema with a gradient descent based approach VMM (Variable Metric Method)[10]. Given a constrained minimization problem: SUMT reconstructs a new object function without constrain:
Accordingly formula 2.6 is converted to The solution of formula 3.1 viz. X is also the solution of formula 2.6 [9]. direction in [7](see Lemma 2). The details can be found in Appendix. 3.2 Choosing Start Points Here we also take Gauss kernel K x i x j exp x i x j 2 2 2 for discussion. Theorem 3. All training points become support vectors as 0 . Proof. When 0, y i 1or y i 1is l or l respectively. Assume the Lagrange multiplier i is Because i l i 0, all the sample points become support vectors. Below we want to find a solution to i . From the dual problem solution of SVM we have hence, From the KKT condition we get which can be transformed to Let 0 we get the simultaneous equations: Combining with formula 3.2, we can finally obtain the value of i and b : Let C max 2 l l 2 l l , the claim holds.
 Theorem 4. SVM ranks all the test samples to the same class when . Proof. When , lim K x i x j 1, the determine function is constant G : recommended region of is 0 x i x j max . We prefer X 0 C 0 2 0 0 r 0 starting at 1 1 0 100 . Algorithm 1 is the algorithm for formula 3.1.
Algorithm 1. SimulTuning tests.
  X  X oxed X  points. promising.

Although the tuning approach only deals with model selection problem on bench-A Computing Gradient B Computing Searching Direction then
