 Frequent itemsets derived from databases have been extensively used in associa-tion rule mining [1], clustering [2], classification [3], and so on. Therefore, mining efficiently frequent itemsets is ver y important in data mining area. 1.1 Problem Definition Let I = { i 1 ,i 2 ,i 3 ,...,i n } be a set of n distinct items. An itemset X is a subset of I , i.e., X  X  I ,and X is called a k -itemset if | X | = k . DB is a transaction database, where each transaction T is also a subset of I , i.e., T  X  I .Wesay that transaction T satisfies itemset X if X  X  T .Let S ( DB )bethenumber of transactions in DB and C ( X ) the number of transactions satisfying X ,and then C ( X ) /S ( DB )isthe support of X . Given a user specified minimum support threshold  X  ,anitemset X is called frequent if ( C ( X ) /S ( DB )) &gt; =  X  .Thefre-quent itemset mining problem [4] is to en umerate all the frequent itemsets with their supports given a DB and a  X  . For a database with n items, 2 n itemsets must be checked, and thus the problem is intractable.
 1.2 Previous Solutions Apriori [5] is one of the most well-known algorithms for frequent itemset mining, and it uses the downward closure property of itemset support: any superset of an infrequent itemset is infrequent and any subset of a frequent itemset is frequent. After scanning a database, Apriori find out all the frequent 1-itemsets from which it generates the candidate 2-itemsets. Afterwards, Apriori scans iteratively the database to find out all the frequent k -itemsets ( k&gt; = 2) from which it generates the candidate ( k + 1)-itemsets. The qualification as a candidate ( k + 1)-itemset is that all of its subsets containing k items, namely k +1 k -itemsets, are frequent. The Apriori-based approaches such as [6], [7] are called candidate generation-and-test approaches.

Pattern growth approaches such as FP-Growth [8] adopt the divide-and-conquer strategy to mine frequent items ets. They first identify all the frequent items in a database, and subsequently divide the database into the disjoint con-ditional databases according to the frequent items. After that, each conditional database is recursively processed. For a frequent k -itemset, each frequent item in its conditional database can be appended to the k -itemset, which makes it grow intoafrequent( k +1)-itemset. Many pattern growth approaches employ prefix-trees to represent (conditional) databases . Prefix-trees are highly compressed on which both database scan and support counting can be performed fast.
There are a number of other mining approaches. Using a vertical database lay-out, the Eclat algorithm [9] links each item up with a set of transaction identifiers and then intersects the sets to mine frequent itemsets. The TM algorithm [10] is a variant of Eclat, and the dEclat algorithm [11] incorporating the  X  X iffset X  technique significantly improves Eclat X  X  performance. The FIUT algorithm [12] mines frequent itemsets by g radually decomposing a length k transaction into k length ( k  X  1) transactions. Tiling [13] makes the best of CPU cache to speed previous algorithms up; CFP-growth [14] consumes less memory than other algo-rithms; LCM [15] integrates multiple optimization strategies and achieves good performance. 1.3 Contribution The two major costs for a mining algorithm are database scan (or prefix-tree traversal) and support counting. For the algorithms constructing conditional databases, the construction cost of conditional databases is also nontrivial.
For a large database and/or a small minimum support, the mining task usually becomes very intractable because numer ous transactions need to be scanned and a very large number of itemsets must be ch ecked. In this case, a high-performance algorithm is indispensable. To obtain better performance, a common method is to reduce the costs of a previous algorithm as much as possible. For example, the FPgrowth* algorithm [16] significantly improves FP-Growth X  X  performance by reducing half the traversal cost. The difficulty of the method is that the decrease in a cost can lead to the increase in another cost. Although FPgrowth* outper-forms FP-Growth, it is usually neglected that the counting cost of FPgrowth* is more than that of FP-Growth.

The paper presents a novel algorithm, ca lled BFP-growth, for frequent itemset mining. BFP-growth employs prefix-trees to represent (conditional) databases as most pattern growth algorithms do. We compare the traversal, counting, and construction costs among BFP-growth, FP-Growth, and FPgrowth* in details, and demonstrate that these costs in BFP-growth are greatly reduced. We con-duct extensive experiments in which several famous algorithms are tested besides the three algorithms aforementioned. Experimental data show that BFP-growth achieves significant performance impro vement over previous works. The rest of the paper is arranged as follows. Section 2 looks back on the classic pattern growth approach. Section 3 presents the BFP-growth algorithm. The three costs of BFP-growth, FP-Growth and FPgrowt h* are analyzed in Section 4. Section 5 gives experimental data. The paper ends in the conclusion of Section 6. The classic pattern growth approach, FP-Growth [8], employs extended prefix-trees called FP-trees to repres ent (conditional) databases. FP-Growth first iden-tifies all the frequent items by a scan over a (conditional) database. After that, it constructs an (conditional) FP-tree by processing each transaction as follows: (1) pick out the frequent items from the transaction; (2) sort the items in frequency-descending order to generate a branch; (3) insert the branch into the FP-tree. Fig. 1(a) and (b) show a transaction database and the corresponding FP-tree. A prefix-tree X  X  node contains two fields: an item and a count , and an FP-tree X  X  node holds two extra pointers: a parent-link pointing to its parent node and a node-link pointing to another node containing the same item. There is a header table for each FP-tree, in which an entry registers a frequent item, its support, and the head of the list that links all the nodes containing the item.
FP-Growth processes all the items in a header table one by one. For item i , the paths from all the nodes containing i to the root constitute the conditional database of i . FP-Growth traverses the paths along both node-links and parent-links to count for the items in the paths. After identifying the frequent items in the conditional database, FP-Growth traverses the paths again to construct the conditional FP-tree of i . Fig. 1(c) shows the conditional database and conditional FP-tree of item e .

FPgrowth* is an efficient variant of FP-Growth, and it counts for the items in an FP-tree when constructing the FP-tr ee. In this way, FPgrowth* reduces half the traversal cost of FP-Growth and thereby gains significant performance im-provement, although it increases the coun ting cost. FPgrowth* [16] is the fastest algorithm in IEEE ICDM Workshop on frequent itemset mining implementations (FIMI X 03). To obtain better performance, the questions are: (1) Can we further reduce the traversal cost? (2) Why does FPgrowth* increase the counting cost? Can that be avoided? (3) Can the FP-Growth-based methods mine frequent itemsets using plain prefix-trees (e.g., Fi g. 2) rather than extended prefix-trees? BFP-growth mines frequent itemsets by c onstructing recurs ively conditional prefix-trees, as most pattern growth approaches do. Different from previous ap-proaches, for a (conditional) prefix-tree, BFP-growth first builds the counting vectors for all the items in the tree, and subsequently constructs simultaneously all the conditional prefix-trees of next level. 3.1 Building Counting Vectors Given a prefix-tree, the paths from all the nodes containing item i to the root constitute the conditional database of i . To construct the conditional prefix-tree of i , all the frequent items in its conditional database should be first identified. For the purpose, BFP-growth will build the counting vector for i denoted as CV i . Each item in the conditional database of i corresponds to a component of CV i . For example, the counting vector for item d of the prefix-tree in Fig. 2 contains the three components co rresponding to items a , b ,and c in the conditional database of d . After initializing the counting vectors for all the items of a prefix-tree, BFP-growth starts to count for the items in all the conditional databases.
Using a work stack, BFP-growth continually updates the counting vectors for a prefix-tree in the process of traversing the prefix-tree in depth-first way. The stack stores the items in the path from the parent node of the current node to the root. Fig. 3 shows how the counting vectors for the prefix-tree in Fig. 2 are updated when BFP-growth processes each node numbered at its upper left corner according to the depth-first order. For example, when BFP-growth arrives at the node numbered 6, the path from the node to the root is a part of item e  X  X  conditional database. Therefore, items a , b ,and c in the path stored in the stack are counted and the corresponding components in CV e are increased by1(1isthe count of the node numbered 6 and indicates one occurrence of a , b , and c in e  X  X  conditional database). In this way, BFP-growth builds the counting vectors for all the items in a prefix-tree by one traversal of the prefix-tree. 3.2 Constructing Conditional Prefix-Trees After building the counting vectors for all the items in a prefix-tree, BFP-growth can identify the frequent items in any conditional database. Subsequently, BFP-growth will traverse the prefix-tree again to construct simultaneously all the conditional prefix-trees of next level.

When processing the node containing item i , BFP-growth picks first out the frequent items from the items in the path from the parent node of the node to the root according to CV i . These frequent items are sorted in frequency-descending order and subsequently inserted into CT i (the Conditional prefix-Tree of item i ). Fig. 4 demonstrates BFP-growth X  X  construction procedure for the prefix-tree in Fig. 2. Only when a conditional prefix-tree is updated is it depicted in the figure. For example, when BFP-growth arrives at the node numbered 4, there are items a , b ,and c stored in the stack. Accordin g to the counting results in CV d ,only b and c are frequent (the minimum support is 30%). They are sorted in frequency-descending order, and a branch { cb :2 } is generated (2 is the count of the node numbered 4 and indicate s two occurrences of transaction cb in the conditional database of item d .). Afterwards, the branch is inserted into CT d . 3.3 Pseudo-code of BFP-Growth Algorithm 1 shows the pseudo-code of BFP-growth.

BFP-growth first traverses prefix-tree T (the second parameter) to build the counting vectors for all the items in T (line 1). Component j in counting vector CV i denoted by CV i [j] stores the support of item j in the conditional database of item i .If CV i [j] exceeds minimum support threshold minsup (the third parame-ter), item j is frequent in the conditional database of item i . Then, the two items and prefix itemset F (the first parameter) constit ute a new frequent itemset, and the itemset with its support CV i [j] is outputted (lines 2-8). From another perspective, the counting vectors actually store the supports of all the 2-itemsets of T . After outputting the frequent itemsets, BFP-growth constructs simulta-neously all the conditional prefix-trees as stated in Section 3.2 (line 9). The counting vectors are released before th e algorithm enters the recursions of next level (line 10). At last, for each conditional prefix-tree CT i , BFP-growth gener-ates its prefix itemset ExF (line 12) and processes it recursively (line 13). Algorithm 1. BFP-growth
Given database DB and minimum support minsup , after prefix-tree T is con-structed from DB and all the frequent 1-itemsets are outputted, BFP-growth( X , T , minsup ) can generate all the frequent k -itemsets ( k&gt; =2). Most pattern growth algorithms derive from FP-Growth [8], in which FPgrowth* [16] is very efficient. Prefix-tree traversal, support counting, and prefix-tree con-struction are the major co sts for these algorithms. The section compares the costs among FP-Growth, FPgrowth*, and BFP-growth. 4.1 Less Traversal Cost Prefix-tree traversal is always necessa ry for both support counting and condi-tional prefix-tree construction, and the traversal cost takes a very large propor-tion in the whole cost for a mining task. FPgrowth* gains significant performance improvement over FP-Growth by reducing half the traversal cost.

A prefix-tree/FP-tree T with n items has 2 n nodes in the worst case. If the root is at level 0, the number of nodes at level i is combination number C i n .For a node at level i , FP-Growth counts for the items in the path from the node to the root, and thereby i nodes are accessed. Let and n is an even number for convenience of computation. Then, the number of accessed nodes in FP-Growth X  X  counting phase for T is: The same number of nodes are accessed in FP-Growth X  X  construction phase. Hence, the total number of nodes accessed by FP-Growth for T is: FPgrowth* merges the counting procedure into the construction procedure (see Section 4.2), and thus the a ccessed nodes in FPgrowth* for T are half those in FP-Growth, namely: BFP-growth traverses a whole prefix-tree in the counting phase and does it again in the construction phase, and hence the total number of nodes accessed by BFP-growth for T is: On the one hand, there are usually many items in a database, and namely n is very large (see Fig. 6); On the other hand, there are a large number of prefix-trees generated during a mining process [17]. Therefore, the traversal cost of BFP-growth is far less than that of FP-Growth and that of FPgrowth*. 4.2 Should the FP-Array Technique Be Incorporated? FPgrowth* counts for the items in the conditional databases of all the items in an FP-tree when constructing the FP-tree (the FP-array technique) and thereby reduces its traversal cost. Although the technique can also be applied to BFP-growth, we find out that the FP-array technique leads to the increase in counting cost. The following example explains this point.

Suppose the minimum support is 15% (rather than 30%), and then items a , b , c ,and d are all frequent in the conditional database of item e for the prefix-tree in Fig. 2 according to the counting results in CV e in Fig. 3. FPgrowth* constructs CT e and simultaneously counts for the items in all the conditional databases of next level, which is demonstrated in Fig. 5(a) (It is the FP-tree that FPgrowth* constructs, but both parent-links and node-links don X  X  relate to the analysis here.). FPgrowth* performs 13 times of counting labeled as shaded blocks when constructing CT e . The counting procedure of BFP-growth performed after CT e has been constructed is demonstrated in Fig. 5(b), and there are only 8 times of counting labeled with asterisks.

The fundamental reason why the times of counting in BFP-growth are fewer than those in FPgrowth* is that BFP-growth counts for the items in a com-pressed database (namely a prefix-tree) but FPgrowth* counts in an uncom-pressed database. Especially for a dense database, there are a relatively large number of transactions and a corresponding relatively highly-compressed prefix-tree, and thus counting on the prefix-tr ee is more efficient than counting on the transactions. In this case, the FP-array technique cannot significantly speed the algorithm up because the increase in the counting cost counteracts to a large extent the decrease in the traversal cost. It is also the reason why FPgrowth* gives up the FP-array technique when confronted with dense databases [16]. We observed by preparatory experiments that BFP-growth X  X  performance was not significantly improved and was even deteriorated a little in some databases when the counting procedure is merged int o the construction procedure. There-fore, BFP-growth doesn X  X  incorporate the technique. 4.3 Plain Prefix-Trees Another advantage of BFP-growth is that it employs plain prefix-trees, but FP-Growth and FPgrowth* employ extende d prefix-trees, namely FP-trees. The following lemma holds for BFP-growth and FP-Growth/FPgrowth*.
 Lemma 1. Given a database and a minimum support, there is a one-to-one correspondence between the FP-trees constructed by FP-Growth/FPgrowth* and the prefix-trees constructed by BFP-growth.
 Proof. (1) BFP-growth constructs the initial prefix-tree from the database as FP-Growth/FPgrowth* constructs the in itial FP-tree (see Section 2). Hence, the initial prefix-tree is the same as the initial FP-tree, except that the latter holds a parent-link and a node-link for each node. (2) Without regard to both parent-links and node-links, suppose that FP-tree FPT isthesameasprefix-tree PT .Foritem i in FPT , FP-Growth/FPgrowth* takes the paths from the nodes containing i to the root as the conditional database of i , and BFP-growth does so for item i in PT . Therefore, the conditional FP-tree of i constructed by FP-Growth/FPgrowth* is the same as the conditional prefix-tree of i constructed by BFP-growth. (3) FP-Growth/FPgrowth* processes all the items in FPT (one by one), and BFP-growth processes (simultaneously) all the items in PT as well. The Lemma can be deduced from (1), (2), and (3).
 Because of extra overheads for building p arent-links and node-links for FP-trees, we can conclude from Lemma 1 that the construction cost of BFP-growth is less than that of FP-Growth/FPgrowth* for a mining task. Our experiments include the algorithms: BFP-growth, FP-Growth, FPgrowth* (the fastest algorithm on FIMI X 03), AFOPT [18], dEclat [11], [19], and LCM [15] (the fastest on FIMI X 04). We implemented BFP-growth. To avoid imple-mentation bias, the implementation of FP-Growth was downloaded from [20], and the implementations of the other algorithms downloaded from [21]. All of the codes were written in C/C++, used the same libraries, and were compiled using gcc (version 4.3.2). Fig. 6 shows the statistical information about the ex-perimental databases from [21]. The experiments were performed on a 2.83GHz PC (Intel Core2 Q9500) with 4  X  10 9 bytes memory, running on a Debian (Linux 2.6.26) OS. Running time contains input time, CPU time, and output (directed to  X /dev/null X ) time.

The experimental results are depicted in Fig. 7 (we did not plot when an imple-mentation terminated abnormally.). For almost all the databases and minimum supports, BFP-growth performs the best. For example, in Fig. 7(b), the run-ning times of the algorithms are respectively: BFP-growth (10.975 seconds), FP-growth* (143.173s), FP-Growth (400.934s), AFOPT (139.097s), dEclat (23.875s), LCM (19.153s) when the minimum support is 25% for real dense database chess . For synthetic sparse database T 40 I 10 D 100 K in Fig. 7(e), their running times are respectively: BFP-growth (45.777 seconds), FPgrowth* (168.714s), FP-Growth (2890.735s), AFOPT (186.062s), dEclat (763.604s), LCM (114.614s) when the minimum support is 0.08%.

Fig. 8 shows BFP-growth X  X  performance improvement over the previous pat-tern growth algorithms, in which the execution speed of FP-Growth is normal-ized as 1. For dense databases, for example, in Fig. 8(b) and (c), FPgrowth* has a speedup of less than 5-fold compared with FP-Growth, and BFP-growth has a speedup of about 30-fold. A small-size and highly-compressed prefix-tree is usually constructed from a dense database, which means the relatively small traversal cost and the relatively large co unting cost in the mining task. Based on FP-Growth, FPgrowth* reduces half the traversal cost but increases the count-ing cost, whereas BFP-growth reduces the m ore traversal cost without increasing the counting cost. Therefore BFP-growth has a larger speedup than FPgrowth*. For sparse databases, the corresponding bushy prefix-trees mean the relatively large traversal cost and the relatively small counting cost. In this case, for exam-ple, in Fig. 8(d) and (e), the speedup of FPgrowth* is more than 5-fold, whereas BFP-growth is even over 50 times faster than FP-Growth for low minimum sup-ports. Compared with FPgrowth*, BFP-gr owth accesses fewer nodes, constructs simpler trees, and does not increase times of counting, thereby gaining more performance improvement. In this paper, we proposed the BFP-growth algorithm for frequent itemset min-ing. The advantages of BFP-growth over the previous pattern growth algorithms are as follows. (1) For any prefix-tree generated during a mining process, BFP-growth traverses it only twice and thus d ramatically reduces the traversal cost. (2) The counting cost of BFP-growth is less than that of FPgrowth*, one of the fastest algorithms. (3) BFP-growth employs plain prefix-trees to represent databases, and therefore the construction cost of BFP-growth is less than that of FP-Growth/FPgrowth* representing databases by extended prefix-trees. Ex-tensive experimental data show that BFP-growth outperforms several famous algorithms including FPgrowth*, dEclat, and LCM, ones of the fastest algo-rithms, for various databases.
