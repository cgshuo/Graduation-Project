 Traditional Web search engines do not use the images in the HTML pages to find relevant documents for a given query. Instead, they typically operate by computing a mea-sure of agreement between the keywords provided by the user and only the text portion of each page. In this pa-per we study whether the content of the pictures appearing in a Web page can be used to enrich the semantic descrip-tion of an HTML document and consequently boost the per-formance of a keyword-based search engine. We present a Web-scalable system that exploits a pure text-based search engine to find an initial set of candidate documents for a given query. Then, the candidate set is reranked using se-mantic information extracted from the images contained in the pages. The resulting system retains the computational efficiency of traditional text-based search engines with only a small additional storage cost needed to encode the visual in-formation. We test our approach on the TREC 2009 Million Query Track, where we show that our use of visual content yields improvement in accuracies for two distinct text-based search engines, including the system with the best reported performance on this benchmark.
 Categories and Subject Descriptors: H.3.3 [Informa-tion Storage and Retrieval]: Information Search and Re-trieval Keywords: Web Search, Ranking, Image Content  X  X  picture is worth a thousand words. X  Despite this old saying, modern Web search engines ignore the pictures in HTML pages and retrieve documents merely by comparing the query keywords with the text in the documents. Of course this text includes the words in image captions and  X 
This research was conducted while the first author was a visiting student at Dartmouth College.
 (a) (b) Figure 1: Method overview: the query q is issued (a) markup tags, but does not look at the pixels themselves. The exclusive reliance on text-based technology to search the Web is explained by the challenges posed by the handling of image data: automatic image understanding is still today computationally expensive and prone to mistakes.

In this paper we propose a novel document retrieval ap-proach that uses the content of the pictures in the Web pages to boost the accuracy of pure text-based search engines. At a high-level we expect that, for example, for the query  X  X er-rari Formula 1 X , users will judge documents containing pic-tures of Ferrari cars to be more relevant than pages with unrelated images. Consequently our hope is that a search system combining the textual information with the visual information extracted from the pictures will yield improved accuracy. While there is a large literature on combining text and image data for image search, we know of no work that attempts to improve document search using image content. The closest work to ours is the approach of Yu et al. [7] who demonstrate improved ranking by using simple image mea-sures such as aspect ratio, size, and high-level features such as blurriness. In contrast, we use a modern object recogni-tion system to provide rich data on the image content.
In order to design an image-based search engine that can scale to Web-size databases we are posed with two funda-mental challenges. First, the descriptor extracted from the pictures must be semantically rich but also very compact so that the overall size of the document is sufficiently small for fast search in billions of pages. Second, we must devise a way to efficiently translate the query keywords into a visual model (i.e., an image classifier) that can be used to measure the compatibility between the text query and the photos in a Web page. We address the first requirement by utilizing a compact attribute-based image descriptor X  X he classeme vector [6] X  X hich has been shown to yield accurate object recognition even with simple linear classifiers, which are ef-ficient to train and test. The second requirement is met by learning  X  X n the fly X  the visual model associated to the query keywords using as positive training examples the top image results of a text-based image search engine, such as Google Images or Bing Images. The visual classifier can then be used together with the text-based techniques of traditional Web search to measure the compatibility between the query and the page content, now both visual as well as textual . The architecture of our system is illustrated in Fig. 1. Let D be the database of Web pages. In order to pro-duce the list of relevant documents for an input query q , we use a reranking strategy combining traditional text-retrieval methods with the visual classifier learned for query q : (a) The query q is provided as input to a text-based search (b) In parallel, the query q is issued to a keyword-based im-(c) The list of pages r is reranked (Fig. 1(c)) by taking into The intuition is that when the query represents a concept that can be recognized in images, the learned visual classifier can be applied to increase or decrease the relevancy of a candidate page in the ranking list depending on whether the document contains pictures exhibiting that visual concept.
Our system can perform efficient query-time learning and testing of the visual classifier in large databases. This scala-bility stems from the small size of the classeme vector (only 333 bytes/image) and the use of a linear (i.e., fast to train and test) classification model. Here we use a linear Support Vector Machine (SVM) trained on M = 50 examples.
We now describe our image-based reranking model. We use a query-relative representation of the documents: let x ( q,i )  X  R d be the feature vector describing the i -th doc-ument in the database D relative to query q . Given an input query q , our approach enables real-time computation of the vector x ( q,i ) for each document i in the ranking list r produced by text-search engine S . The vector x ( q,i ) cludes several image-based features. In the next subsection we present our features. In subsection 3.2 we describe how these features are used to rerank the documents in r .
The vector x ( q,i ) for query-document pair ( q, i ) comprises the following 12 features.
 Text features ( x ( q,i ) 1 , 2 ):  X  X elevance score X  and  X  X anking po-sition X  of document i in the ranking list r produced by S for query q . The  X  X elevance score X  feature is a numerical value indicating the relevancy of the document as estimated by S , purely based on text. The  X  X anking position X  is the position of i in the ranking list r . By including these two features we leverage the high-accuracy of modern text-based search. Visual metadata features ( x ( q,i ) 3 , 4 ):  X # linked images X  and  X # valid images X  . These attributes are used to describe whether the document contains many images. Web pages often include many small images corresponding to clipart, icons and graphical separators. These images usually do not convey semantic information. To remove such images from consideration, we extract the classeme vector only from pic-tures having at least 100 pixels per side. The feature  X # valid images X  gives the total number of images in the page for which the classeme descriptor was computed.
 Query visualness features ( x ( q,i ) 5 , 6 ):  X  X isual classifier ac-curacy X  and  X  X isual concept frequency X  . These features are dependent only on the query (i.e., they are constant for all documents) and describe the ability of the visual clas-sifier learned for query q to recognize that concept in im-ages. In particular,  X  X isual classifier accuracy X  is the 5-fold cross-validation accuracy of the classifier trained on the ex-amples retrieved by Bing Images for query q . While this feature describes how reliably the classifier recognizes query q in images, it does not convey how frequently this visual concept is present in pictures of Web pages. This informa-tion is captured by  X  X isual concept frequency X  which is the fraction of times the visual classifier for query q returns a positive score on images of the database D .

Intuitively, these two query visualness features provide the reranker with an indication of the usefulness of employing the visual classifier for query q to find relevant pages. Visual content features ( x ( q,i ) 7  X  12 ):  X  X istogram of visual scores X  and  X  X ocument relevancy probability X  .
 The  X  X istogram of visual scores X  is a 5-bin histogram ( x representing the quantized distribution of the scores (i.e., the SVM outputs) produced by the visual classifier of query q on the images of document i .
 The  X  X ocument relevancy probability X  ( x ( q,i ) 12 ) is the posterior probability that the document i is relevant for query q given the observed classification scores of the images contained in the page, i.e., p ( i is relevant | s 1 , . . . , s n i ), where s are the binarized scores that the SVM for query q produces on the n i (valid) images of document i . This probability is computed via standard Bayes X  X  rule under the assumption of conditional independence (the Na  X   X ve Bayes assumption): p ( i is relevant | s 1 , . . . , s n i ) = where m i is the number of images of i having positive classi-fication score while TP denotes the true positive rate of the classifier, i.e., TP = p ( s u = 1 | i is relevant ). The denomi-nator in Eq. 1 can be evaluated via application of the sum and product rules in terms of the prior, TP , and the false positive rate ( FP ). We assume that the rates TP , FP are query-independent and we estimate them empirically over a large number of labeled training queries.
Our objective is to learn a reranking function f : R d  X  R such that f ( x ( q,i ) ) provides a numerical estimate of the final relevancy of document i for query q , where i is one of the pages in the list r retrieved by S . In order to avoid the com-putational cost of training the reranker at query-time, we learn a query-independent function f : this function is trained only once during an offline training stage, using a large collection of labeled training examples for many different queries. We denote with T = { ( q 1 , r 1 , y 1 ) , . . . , ( q the offline training set used to learn f , where r j is the sorted ranking list of K documents produced by the text-based search engine S for input query q j , i.e., r jk  X  X  denotes the ID of the document ranked in the k -th position; the vector y contains the corresponding ground-truth relevance labels. We use binary relevance labels with y jk = 1 denoting that document r jk is relevant for query q j , and value 0 indicating  X  X on-relevant X . We denote with  X  the learning parameters of we tested the following reranking models:  X  Ranking SVM. This algorithm [4] learns a linear model of the features, i.e., f ( x ( q,i ) ;  X  ) =  X  T x ( q,i ) ters  X  are optimized to produce a ranking that preserves as much as possible the ordering of the training examples, i.e., such that ideally  X  T x ( q j ,k ) &gt;  X  T x ( q j  X  Random Forest. This method learns a random for-est [2] with each tree greedily optimized to predict the rel-evance labels y jk of the training examples. The resulting hypothesis computes an average of the P independently trained regression trees f (1) , . . . , f ( P ) , i.e., f ( x at each split only d 0 &lt; d randomly chosen features (we set d 0 to 10% of the number of features). The value of P is selected via cross-validation.  X  Gradient Boosted Regression Trees (GBRT). This model also predicts by averaging the outputs of P regres-sion trees. However, unlike in case of the random for-est where the trees are independently learned, the GBRT trees are trained in sequence to correct the current regres-sion error (for further details see [9]).
Although our implementation requires downloading the images returned by the image search engine and then ex-tracting the classeme vectors from them, in a real appli-cation scenario the classeme descriptors (which are query-independent) would be precomputed at the time of the cre-ation of the index by the image-search service. Then the image and document search would be issued in parallel, and the image service would return only the classeme vectors for the image results (333 bytes per image). The compu-tational cost of learning the query-specific visual classifier on the classeme vectors is certainly of the same order as ranking in existing text-based systems. Finally, testing the visual classifier is also efficient: it takes less than one second to evaluate a linear SVM on 1M classeme vectors. Table 1: Precision @ 10 and 30 on the TREC MQ09
As for the storage cost, our system requires saving the classeme vectors of the valid images in each Web page. In the dataset used for our experiments, each page contains on average 1.44 valid images. Thus, the added storage cost due to the use of images is less than 500 bytes per document, which can be easily absorbed by modern retrieval systems.
We evaluate our system on the ad-hoc retrieval bench-mark of the TREC 2009 Million Query Track (MQ09) [3]. This benchmark is based on the  X  X ategory B X  ClueWeb09 dataset [1] which includes roughly 50 million English pages crawled from the Web. The publicly available distribution of this dataset includes the original HTML pages collected by the ClueWeb09 team in 2009, but not the images linked in them. In order to run our image-based system on this col-lection, in September 2011 we attempted to download the pictures linked in these documents. Unfortunately many of the pages and images were no longer available on the Web. Thus here we restrict our experiments only to the pages for which we successfully downloaded all images linked in the original document (this amounts to 41% of the pages).
To train and test our reranking system, we use the publicly available MQ09 queries and human relevance judgements.In all, judgements are available for 684 queries, with each query receiving either 32 or 64 document assessments. The rel-evance values are  X  X ot relevant X  ( y jk = 0) or  X  X elevant X  ( y jk = 1). In order to meet the conditions for reusability of the MQ09 topics and judgements [3], we chose as our text-search engines S the UDMQAxQEWeb system [8], which was one of the systems participating in the MQ09 competi-tion. We refer to this system as UDMQ. The ranking lists of UDMQ on the MQ09 queries are publicly available.
To test the ability of our method to work with differ-ent text-search systems S , we also present results with the popular Indri search engine [5]. We generated the ranking lists of Indri on the MQ09 queries by using its public batch query service. Unlike UDMQ, Indri did not participate to the MQ09 competition. Thus, while the estimate of the ab-solute accuracy of Indri on MQ09 may be unreliable, here we use it just as a baseline to judge the relative improvement produced by reranking its search results with our system.
For both engines, we generate the vector r by truncating the ranking list at K = 200. We employ 10-fold cross val-idation over the queries, thus using in each run 9 / 10th of the queries for training and the remaining 1 / 10-th for val-idation. Performance is measured as precision at 10 and 30 (denoted as statMPC@10 and statMPC@30) using the Figure 2: Precision @ 10 using different image features  X  X tatistical evaluation method X  [3] . We focus on these mea-sures as our main goal is to improve the relevancy of the documents in the top part of the ranking list.

In Table 1 we compare the accuracy of the text-based search engines (UDMQ and Indri) to the different image-based ranking models introduced in section 3.2. First, we see that all image-based rerankers yield higher values of statMPC@10 than the search engines using text only. The GBRT reranker is by far the best, improving by over 33% the precision of UDMQ, which achieved the highest accu-racy among all search engines participating in the MQ09 competition. This clearly indicates that our image-based features provide new and relevant information compared to that captured by traditional text-based engines. Instead, no significant gain is achieved in terms of statMPC@30. Empir-ically we found that our reranker tends to apply fairly small displacements to the positions of documents in the original ranking list. While these small rearrangements have a pos-itive impact on the top-10 lists examined by statMPC@10, they are too small to change sensibly the statMPC@30.
Next, we want to study which features contribute to the statMPC@10 improvement. For this purpose we retrain the GBRT model (our best performing model) using two differ-ent variants of our feature vector: 1)  X  X ext + visual meta-data X  (i.e., we use only the features x ( q,i ) 1  X  4 capture the content of the images); 2) the vector  X  X ll fea-tures except visualness X  (i.e., we exclude only features x which capture the document-independent visualness of the query). The results are presented in Figure 2 using UDMQ (red bars) and Indri (yellow bars) as text-retrieval models S . We see that, although GBRT with the  X  X ext + visual meta-data X  descriptor achieves accuracy slightly superior to the text-based search engines, the performance is not as good as when our approach uses all features, including the visual content. This suggests that despite the noisy nature of the Bing training images, our visual classifier does capture in-formation that is useful to predict whether a document is relevant with respect to the query. Excluding the query vi-sualness features from our descriptor also causes a drop in accuracy. Intuitively, this happens as these features allow Table 2: A comparison across queries between the the reranker to determine whether the query is visually rec-ognizable and to modulate accordingly the contribution of the visual content features in the reranking function.
In Table 2 we report the percentage of queries for which our image-based GBRT reranker provides a higher value of prec@10 than S , i.e.,  X  X ins X  over the text-based engine. Our method and S are tied for roughly 72% of the queries, while the number of times one wins over the other are fairly evenly divided. However, in the cases where our system wins, it gives a much higher gain in prec@10, compared to when S wins (+33.1% vs +20% when S =UDMQ; +29.5% vs +20% when S =Indri). It is also interesting to observe that the cross-validation error of the visual classifier is lower for the subset of queries where our system wins over S .
In this work we have studied the largely unexplored topic of how to improve Web-document search using images. We have demonstrated that by using modern object recognition systems it is possible to extract useful semantic content from the photos of a Web page and that this additional informa-tion improves the accuracy of state-of-the-art text-based re-trieval systems. All this is achieved at the small cost of a few additional hundred bytes of storage for each page.
This work was supported in part by Microsoft Research and by NSF CAREER award IIS-0952943. SRV X  X  visit to Dartmouth College was partially funded by the Basque Gov-ernment under grant number IE11-316.
