 The Multi-Label Classification (MLC) problem has aroused wide concern in these years since the multi-labeled data ap-pears in many applications, such as page categorization, tag recommendation, mining of semantic web data, social net-work analysis, and so forth. In this paper, we propose a novel MLC solution based on the random walk model, called MLRW. MLRW maps the multi-labeled instances to graphs, on which the random walk is applied. When an unlabeled data is fed, MLRW transforms the original multi-label prob-lem to some single-label subproblems. Experimental results on several real-world data sets demonstrate that MLRW is a better solution to the MLC problems than many other existing multi-label classification methods.
 H.3.2 [ Information Storage ]: Record classification; I.5.2 [ Design Methodology ]: Classifier design and evaluation; H.2.8 [ Database Applications ]: Data mining Algorithm, Design, Experimentation, Measurement multi-label classification, multi-label random walk graph, multi-label random walk graph collection, multi-label ran-dom walk model
The data classification problem aims to assign each data point to a set of categories. It is widely applied in many fields, such as web information retrieval, management and mining of web data, social system analysis, etc. In tradition-al data classification problems, each data point is assigned This work was partially supported by the National Natural Science Foundation of China (No. 60803016,No. 61170064), Tsinghua National Laboratory for Information Science and Technology (TNLIST) Cross-discipline Foundation, and the National HeGaoJi Key Project (No. 2010ZX01042-002-002-01).
 to only one category/class. It is expected to make a decision on whether each unclassified data point belongs to a given categoryornot. Thiskindofproblemsisalsocalledthe Single-Label Classification (SLC) problems.

In many situations, however, a data point may belong to more than one category simultaneously. For example, in ACM Computing Classification System , there are a total of 11 first-level and 81 second-level categories and subject de-scriptors. An article may be assigned to more than one cate-gory by the author(s), and thus can be retrieved from differ-ent categories in the ACM digital library as well. This kind of classification problems is called the Multi-Label Classifi-cation (MLC) problems. The relevance and co-occurrence between different labels make the MLC problems quite d-ifferent from traditional SLC problems. Thus, traditional SLC methods cannot be directly applied to the MLC prob-lems [5].

Recent MLC solutions can be summarized into two cate-gories, i.e., the Problem Transformation (PT) methods and the Algorithm Adaptation (AA) methods. The former aims at transforming an MLC problem to a set of SLC problems, to which the traditional SLC methods can be applied; the latter aims to adapt existing SLC algorithms as MLC solu-tions.

In this paper, we propose a novel M ulti-L abel classifica-tion method based on the R andom W alk model, called ML-RW . MLRW is a kind of PT method. In MLRW, the random walk model is extended as a reasonable representation of the multi-label data. Therefore, the original multi-label input s-pace is mapped to a multi-label random walk graph .The connectivity between vertices in this graph indicates the re-lationship between different data points which share one or more same labels.

The main contributions of this paper are as follows.  X  The concept of multi-label random walk graph is pro-posed. The multi-label training instances are mapped to a multi-label random walk graph.  X  MLRW, a novel multi-label classification method based on the random walk model, is proposed. It maps the multi-labeled instances and an unlabeled instance to a collection of multi-label random walk graphs. On these graphs, the random walk model is applied and the output vectors are utilized to build a binary classifier for each label. There-fore, MLRW transforms the original multi-label classifica-tion problem to some binary classification subproblems, which can be resolved by traditional classification methods.  X  We make extensive comparison experiments between MLRW and many other state-of-the-art multi-label classi-fication methods. Experimental results on the real data sets demonstrate that MLRW provides a better solution than these methods.
Given a d -dimensional space R d and a finite label set Y {  X  1 , X  2 ,..., X  q } , the training set D is
D = { ( x i ,L  X  ( x i )) | x i  X  R d ,L  X  ( x i )  X  X  , 1 where x i is a training instance in R d , L  X  ( x i )the actual la-bel set of x i and m the number of training instances in D . Similarly, the testing set H consisting of n instances is
H = { ( t j ,L  X  ( t j )) | t j  X  R d ,L  X  ( t j )  X  X  , 1 Definition 1 (Multi-Label Classification, MLC). Given a multi-label classification function f :R d  X  2 Y , when a test instance t  X  R d arrives, the predicted label set of t is obtained by applying f ( t ) , denoted as L ( t ) ( L ( t ) It is aimed to learn the function f on the training set D ,such between L ( t j ) and L  X  ( t j ) is as small as possible.
In this section, the overview of the MLRW method is in-troduced first. The rest subsections give a detailed descrip-tion of the method step by step.
As shown in Fig. 1, MLRW takes the following steps to deal with the MLC problems. (1) Step 1: Map the multi-labeled instances to multi-label random walk graphs. MLRW maps the multi-label training instances to a multi-label Random Walk Graph (RWG), denoted as G (See Section 3.2 for details). (2) Step 2 X 4: Random walks. When an unlabeled instance t arrives, a multi-label Random Walk Graph Col-lection (RWGC) consisting of q RWGs is built on t and G . And then, the random walk is carried out on each RWG of the RWGC to get the output vectors (See Section 3.3 for details). (3) Step 5 X 7: Transform to single-label problems. The output vectors mentioned above are utilized to build a classifier for each label. As a result, MLRW transforms the original multi-label problem to some single-label problems with the help of validation set D (SeeSection3.4forde-tails). Some traditional machine learning methods, such as SVM, Bayes and Decision Tree, can be utilized to build the classifiers. (4) Step 8: Make final predictions. The binary clas-sifiers built in Step 5 X 7 are utilized to make the final pre-diction.
In MLRW, the multi-label training instances are mapped to a multi-label random walk graph (RWG) first. The oper-ations in this subsection correspond to Step 1 in Fig. 1. Definition 2 (Multi-label Random Walk Graph).
 Let D be a training set. The multi-label random walk graph of D is an undirected graph G D =( V,E ) ,where V = { x i L ( x i ))  X  D } , E = { ( x i ,x j ) | X  ( x i ,L  X  ( x i )) , ( x D,L  X  ( x i )  X  L  X  ( x j ) =  X  X  .

In G D , each training instance is mapped to a vertex. The vertices carrying one or more same labels are connected.
At each vertex in G D , the walker has two choices, travel-ing to one of its neighbor vertices, or randomly teleporting to some vertex in the RWG. The probability of teleporting within the graph is called the teleporting probability ,denoted as  X  , and the probability of traveling to one of its neighbors is denoted as 1  X   X  . It can be proved that the walk pro-cessing in the MLRW method is ergodic, therefore the walk processing tends to be convergent, from which we obtain a stable output vector.
 Furthermore, the weight of each edge in G D is calculated. The weight matrix is denoted as W D ,inwhichtheelement Similar to [1], the Euclidean Distance is utilized.
Without loss of generality, we assume that G D is a con-nected graph. If not, we can split G D into a series of con-nected components . Each connected component corresponds to a subset of Y , and the subsets are disjoint from each other. Meanwhile, this situation indicates the existence of irrelevant label subsets in Y . As a result, the original MLC problem can be broken up into several independent MLC subproblems so that the MLRW method can be applied to each subproblem.
The operations in this subsection correspond to Step 2 X 4 in Fig. 1. When an unlabeled instance t is fed ( Step 2 ), the following two operations, which correspond to Step 3 and Step 4 , will be carried out, respectively. (1) For each la-bel  X  l  X  X  , we connect t with all the vertices carrying  X  i.e.  X  l  X  L  X  ( x i ), to form a new RWG. We denote this event as t  X  l .Sincethereare q labels in Y ,weget q RWGs to form a random walk graph collection (RWGC). (2) On each RWG in the RWGC, the random walk is carried out and a vector is obtained. In this paper, this vector is utilized to de-scribe the similarity between t and each label in the label set. And then, the conditional probability vector is calculated. Since there are q RWGs in the RWGC, we get q conditional probability vectors, denoted as d tl ( l =1 , 2 ,...,q ). We build a multi-label random walk graph collection (R-WGC) on the aforementioned RWG G D and the unlabeled instance t ( Step 3 ).
 Definition 3 (Random Walk Graph Collection).
 Let D be a training set, Y the label set, R d the d -dimensional input space, an instance t  X  R d . The multi-label random walk graph collection (RWGC) of D and t is where G D i is the RWG built on D { ( t, {  X  i } ) } .
After that, the random walk is carried out on each RWG in the RWGC. Random walks are assumed to be Markov processes [3]. The random walk is carried out on each RWG, where at each step the walker travels to its neighbor vertices or ran-domly teleports to some vertex in the RWG according to a probability distribution. In our random walk model, at each vertex the walker prefers to go to its nearer points. The less the distance is, the higher probability it travels to. We cannot say about the exact position of the walker after some steps. The position is random, but we can calculate the distribution. In this paper, this distribution is utilized to describe the similarity between an unlabeled instance t andeachlabelin Y .

In random walks, four input parameters are needed. They are the adjacency matrix M , the initial value of the vector s , i.e. s 0 , the teleporting vector d and the teleporting prob-ability  X  .

The initial vector s 0 represents how the walk starts. Each random walk consists of a number of iterations. At each iteration, the vector s u is updated as follows. The Equ. (5) is applied iteratively until s u = s u +1 .Inthis situation, we say the random walk is convergent and denote the vector of s u as  X  , such that
In actual applications, if || s u +1  X  s u || &lt; ( is a predefined threshold), let  X  = s u . In our experiments, the value of is set to 10  X  6 .

Suppose the random walk is carried out on the RWG G D l of
G D,t .Thevector  X  is a ( | D | +1)-dimensional vector and its first element  X  [1] corresponds the situation that the walker remains at the starting position of t . We define the Similar-ity Score between the instance t and the training instance x  X  D to be the ( i +1) th element of the vector  X  ,suchas
For each label  X  k , the training instances carrying label  X  are selected and their similarity scores are averaged as the similarity score between t and the label  X  k , such as Similarity( t,  X  k )=avg { Similarity( t, x i ) | x i  X  D, X  And then, the conditional probability vector P (  X  k  X  L ( t )  X  ) will be d tl [ k ]= P (  X  k  X  L ( t ) | t  X  l )= where k =1 , 2 ,...,q and d il [ k ]isthe k th element of d d [ k ] indicates the conditional probability that the predict-ed label set of t i includes  X  k when t i  X  l . We make use of a validation set to transform the original MLC problem to some SLC subproblems. The validation set, denoted as D ,isa labeled data set which is similar to the training set, but the two data sets have no common instance. It will be utilized to build a classifier for each label in Y .

For each instance z i  X  D , Step 5 X 6 are taken, which are similar to Step 3 X 4 . After that, we get a series of vectors as Equ. (9), denoted as d i 1 , d i 2 ,..., d iq as follows. After that, we make use of these vectors to build a new training set for each label ( Step 7 ).

Suppose S l is the new training set for the label  X  l . S is generated on the vectors of { d il | z i  X  D } .Thevector d il is marked as a positive training instance for the label  X  if the actual label set of z i contains  X  l , i.e.,  X  l  X  Otherwise, d il is marked as a negative training instance for  X  .Formally,
When an unlabeled instance arrives, after Step 1 X 4 is tak-en, the q output vectors will be applied as the inputs of the q classifiers respectively to get the final prediction result ( Step 8 ). The comparison of different classifiers is presented in Section 4.2, in which some popular classification model-s, such as SVM, ID3 decision tree, kNN, AdaBoost, Na  X   X ve Bayes, are involved.
MLRW provides much more accurate predictions than many other MLC methods. Experimental results and re-lated theoretical analysis will be involved in this section.
There are two types of measures for MLC methods, i.e. the ranking based measures and the classification based ones [5]. For a testing set H denoted as Equ. (2), the multi-label av-erage precision , one-error , coverage are utilized as the rank-ing based measurements. The traditional single-label mea-surements of precision , recall and F-measure have also been adapted as classification based multi-label measurements.
In this paper, the experiments were carried out on the yeast and scene data sets (all publicly available), which s-tood for the biology and image data 1 .

We implemented MLRW based on MuLan 2 ,whichisan open source Java library for multi-label learning tasks. Based on Weka 3 , MuLan implements many state-of-the-art multi-label classification algorithms. All the experiments were car-ried out on a PC with an Intel R Core TM 2.33 GHz CPU, 2GB memory and 1.0TB hard disk. The OS of the PC was Ubuntu Server 9.10 and the version of JDK was 1.6.0.
The multi-label learning methods of binary relevance (BR) [6], label powerset (LP) [4], calibrated label ranking (CLR) [2], random-k label set (RA k EL) [7], include labels (IL) [6] and multi-label k -nearest neighbor (ML k NN) [8] were involved in the comparison experiments. The base classifier of BR, LP, CLR and IL was the frequently used support vector machine http://mulan.sourceforge.net/datasets.html http://mulan.sourceforge.net/ http://www.cs.waikato.ac.nz/  X  ml/weka/ (SVM). As an implementation of the SVM model, the  X  X ib-SVM X  library was utilized 4 . As shown in Table 1, for each SVM in our experiments we had examined 4 most used k-ernel functions, i.e. the linear, polynomial, RBF(Gaussian) and sigmoid kernels. The one conducts the best result (F-measure) was selected. In ML k NN, we changed the value of k from 2 to 10 and selected the one making best result (F-measure), e.g. k was set to 3 for yeast .ForRA k EL, k was set to the recommended value, i.e. k =3 ,n =2 q .
Before each experiment, we mixed the original training and testing data sets. The experiments were then carried out with a 5-fold cross validation. The ratio between the training and the validation set was 4 to 1. Noticed that the training, validation and testing sets were disjoint from each other. Each experiment was repeated for 10 times with a random sampling and permutation as a preprocess. Table 2 and 3 showed the average results over these 50 runs of the experiment.

Taking the yeast data set for example, experimental re-sults showed that the one-error and coverage of MLRW were the best of all methods. Also, MLRW achieved a good out-put on the evaluation of average-precision .Thebestvalue of coverage indicates the smallest cost to find out all proper labels. The best one-error result implicates the top ranked output of MLRW is usually a credible result. In many appli-cations, such as web search, advertisement recommendation, personalized customization, the top ranked result(s) usually attracts more attention. MLRW will be an applicable solu-tion in these situations. On the aforementioned multi-label measurements of multi-label precision, recall and F-measure, the BR, LP, CLR, IL, RA k EL and ML k NN methods were al-so utilized to make comparisons. For MLRW, AdaBoost.M1 was utilized to build the classifiers in Step 7 Similarly, they were the average results of 10 runs of each approach with 5-fold cross validation. Among all MLC methods in the ex-periment, MLRW provided the most accurate outputs. In detail, on F-measure MLRW achieved a 10.08% improve-ment compared to ML k NN, which took the second place. In BR, the prediction of each label was considered as an in-dependent task and the label relationships were completely ignored. MLRW achieved a 10.50% improvement than BR. As another kind of MLC method, LP encoded each multi-label combination into a new label. It thus significantly increased the total number of labels and made the predic-tion much more complicated. In this experiment, MLRW achieved a 22.46% improvement than LP. http://www.csie.ntu.edu.tw/  X  cjlin/libsvm/
ML k NN, which made use of the statistical information to carry out predictions, did better than other MLC method-s except MLRW. But there were two shortcomings in the ML k NN method. One was that the value of the parame-ter k should be decided manually and it might rely on the actual data set. Another defect of ML k NN was that k n-earest neighbor(s) of each data point was (were) found di-rectly in the original input space. As the dimension of the input space grew, no matter which distance function was utilized (Euclidean distance, Manhattan distance, or oth-ers), the distances between any two data points tended to be similar. The phenomenon was the well-known curse of dimensionality. Both of the two aspects delivered negative effects on the prediction results. MLRW made use of the distance between two data points to build the random walk graph, but it just connected those data points sharing same label(s), leaving other points unconnected, i.e. the distances were infinite. Additionally, MLRW transformed the origi-nal input space into a q -dimensional one, making MLRW more applicable to high-dimensional data than ML k NN. In conclusion, MLRW is a more effective solution to the MLC problems than ML k NN and other MLC methods.
In this paper, we proposed a novel multi-label classifica-tion method based on the random walk model, called ML-RW. It maps the multi-label data to graphs, on which the random walk model is applied. And then, the original multi-label classification problem is transformed to some binary classification subproblems, which can be resolved by tradi-tional classification methods. Experimental results on real data sets show that MLRW is an effective solution to the multi-label classification problems. In future works, we will focus on the reduction of time consumption of MLRW to further improve its performance.
