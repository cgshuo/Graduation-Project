 We presen t a probabilistic mo del-based framew ork for dis-tributed learning that tak es into accoun t priv acy restrictions and is applicable to scenarios where the di eren t sites have div erse, possibly overlapping subsets of features. Our frame-work decouples data priv acy issues from kno wledge integra-tion issues by requiring the individual sites to share only privacy-safe probabilistic mo dels of the local data, whic h are then integrated to obtain a global probabilistic mo del based on the union of the features available at all the sites. We pro vide a mathematical form ulation of the mo del integration problem using the maxim um likeliho od and maxim um en-trop y principles and describ e iterativ e algorithms that are guaran teed to con verge to the optimal solution. For certain commonly occurring special cases involving hierarc hically or-dered feature sets or conditional indep endence, we obtain closed form solutions and use these to prop ose an ecien t alternativ e scheme by recursiv e decomp osition of the mo del integration problem. To address interpretabilit y concerns, we also presen t a mo di ed form ulation where the global mo del is assumed to belong to a speci ed parametric family . Finally , to highligh t the generalit y of our framew ork, we pro vide em-pirical results for various learning tasks suc h as clustering and classi cation on di eren t kinds of datasets consisting of con-tinuous vector, categorical and directional attributes. The results sho w that high qualit y global mo dels can be obtained without much loss of priv acy .
 H.4.8 [ Database Managemen t ]: Database Applications | Data Mining; I.2.6 [ Arti cial Intelligence ]: Mac hine Learning Algorithms Distributed learning, priv acy , heterogeneous data sources, probabilistic mo dels
Recen t adv ances in data acquisition technology have re-sulted in the creation of large distributed rep ositories. How-ever, extracting useful kno wledge from suc h rep ositories is of-ten challenging due to real-w orld constrain ts stemming from priv acy , proprietary , computational or comm unication issues. Suc h restrictions may prev ent one from directly integrating the distributed data into a single dataset at a cen tral site. This has led to the emergence of distributed data mining techniques [10, 19] that extract high qualit y in-formation from distributed sources with limited interactions among the data sites. In particular, rising concerns on in-formational priv acy have resulted in an increased focus on priv acy-preserving distributed data mining techniques [14, 6, 20]. Most of these techniques are applicable only to scenar-ios where the data is either vertic ally partitione d (di eren t sites con tain di eren t attributes/ features of a common set of records/ob jects) or horizontal ly partitione d (ob jects are distributed amongst di eren t sites, but have the same set of features). In real life, however, there are a num ber of more complex situations where the di eren t sites con tain overlap-ping sets of objects and features, i.e., the data is neither vertically nor horizon tally partitioned.

In this pap er, we focus on priv acy-preserving learning in a distributed setting 1 where the di eren t sites have div erse, possibly overlapping sets of features, and also need not share objects 2 . The protot ypical application scenario is one in whic h there are multiple collab orating parties with con den-tial databases of di eren t, possibly overlapping schemas. The objectiv e is to characterize the entire data using a suitable represen tation so that learning tasks suc h as clustering and classi cation can be readily performed. For example, the col-lab orating parties could be a group of medical researc hers, eac h owning a database of clinical results. The collectiv e goal in this case is to nd patterns and correlations among the var-ious clinical conditions, without compromising the priv acy of the patien ts. The symptoms (features) of the patien ts (ob-jects) need not be the same for all the researc hers. Figure 1 depicts a possible scenario with three collab orating parties, eac h with data on a di eren t pair of symptoms.

Our form ulation of the distributed learning problem as-sumes that there exists a (unkno wn) meaningful, underly-ing distribution that captures the information con ten t in the
Note that horizon tal partitioning is a special case of this setting.
Ob jects owned by di eren t sites are treated as di eren t ob-jects, whic h could result in loss of information in case of overlapping objects. di eren t data sources. The individual data sources pro vide only partial views that need to be e ectiv ely integrated in or-der to reconstruct the original underlying distribution. Our rst attempt [16] to address this problem involved dividing the learning pro cess into two sub-tasks:(a) learning privacy-safe mo dels from the local data, and (b) com bining the local mo dels e ectiv ely to obtain an appropriate \global" mo del. We also prop osed a de nition of priv acy based on informa-Figure 1: Distributed learning scenario with overlap-ping sets of features. tion theoretic ideas in order to formalize the rst sub-task. This de nition e ectiv ely captures the non-uniform priv acy requiremen ts in most real scenarios and is also applicable to the general setting where the distributed sites have varying schemas. However, the mo del integration approac h in [16] is applicable only for horizon tally partitioned data, wherein all the local and global mo dels corresp ond to probabilit y distri-butions on the same domain.

In this pap er, we substan tially extend our previous work and mak e three new con tributions. (i) Using the maxim um-lik eliho od principle and the relation (ii) We presen t algorithms to solv e the mo del integration (iii) For certain commonly occurring scenarios involving con-
Our mo del integration approac h extends the priv acy pre-serving framew ork prop osed in [16] to a large class of learn-ing tasks that include classi cation, clustering and semi su-pervised learning, and to more general scenarios involving distributed sites with div erse schemas. The framew ork is ap-plicable to a wide range of data types (both discrete and con tinuous) and only requires the individual sites to pro vide probabilistic mo dels of the local data.

The rest of the pap er is organized as follo ws. Section 2 con tains a formal de nition of the mo del integration prob-lem. Sections 3 and 4 describ e solutions to the unconstrained and the constrained mo del integration problems resp ectiv ely. Section 5 describ es a recursiv e scheme for mo del integration and presen ts closed form solutions for certain special sce-narios. Section 6 con tains exp erimen tal results. We presen t related work in Section 7 and conclude in Section 8.
Notation: Sets suc h as f x 1 ; ; x n g are enumerated as f x i g n i =1 and an index i running over the set f 1 ; ; n g is de-noted by [ i ] n 1 . Matrices are denoted using upp er case letters e.g., A , whereas the lower case letters a uv denote the matrix elemen ts, and the bold lower case letters, a v ; a T u denote the column and row vectors. Transp ose of a matrix A is denoted by A T . Feature sets are denoted by F with appropriate sub-scripts and feature vectors are denoted by the corresp onding bold lower case letters. Probabilit y distributions of mo dels are denoted by p with the mo del name as the subscript, e.g., p ( ) and the marginal densities corresp onding to the feature set F are iden ti ed with a sup erscript, e.g., p F .
Our main objectiv e is to address the distributed learning problem for scenarios where there are no restrictions on the features available at the various sites and the di eren t sites need not share objects. We rst consider priv acy issues.
There are two main notions of priv acy in the existing lit-erature that are applicable to distributed learning scenarios. The rst is based on a secure multi-part y computation [21] point of view where a computation is considered secure if eac h collab orating part y learns nothing beyond its own input and the nal result of the computation. The second notion characterizes the priv acy of a computation in terms of the uncertain ty [1, 16] in reco vering the input from the result.
In this pap er, we adopt an uncertain ty-based de nition of priv acy since it tak es into accoun t the information disclosed by the nal result. In particular, our framew ork is based on an information-theoretic de nition of priv acy [16] where the priv acy ( X ; ) of a dataset X with resp ect to a probabilistic mo del is quan ti ed in terms of the likeliho od of predicting X from the mo del, i.e., Using this notion of priv acy , it is possible to dev elop dis-tributed learning techniques based on com bining probabilis-tic mo dels that satisfy the desired priv acy constrain ts. The chief bene ts of this approac h are that it is scalable for large datasets, does not require the assumption of non-colluding parties, and permits the individual parties to have varying priv acy requiremen ts. Moreo ver, it allo ws the collab orating parties to use proprietary domain kno wledge and algorithms in the local learning pro cess.
We divide the distributed learning problem into two sub-problems -(i) learning probabilistic mo dels from the local data while adhering to information-theoretic priv acy con-strain ts, and (ii) integrating the local mo dels e ectiv ely to obtain an appropriate \global" mo del. This separation of priv acy and integration issues also allo ws the individual par-ties to use their own means of sanitizing the local mo dels during the local learning pro cess, e.g., stripping of unique ids. In the curren t work, we mainly focus on the mo del in-tegration problem assuming the availability of privacy-safe models .

Let fX i g n i =1 be n datasets with feature sets fF i g n corresp onding feature vectors f f i g n i =1 suc h that jX Let f i g n i =1 be the local mo dels obtained from these datasets suc h that the probabilit y distributions f p i g n i =1 closely ap-pro ximate the true distributions on the corresp onding datasets as well as satisfy the local priv acy constrain ts, i.e., ( X ; [ i ] n 1 where f i g n i =1 are the desired priv acy levels for the local mo dels 3 . The mo del integration problem involves com-bining the local mo dels f i g n i =1 to obtain a \go od" global complete mo del corresp onding to a join t probabilit y distri-bution on the union of all the features, i.e., F c = n i =1 For example, in Figure 1, there are three collab orating par-ties with feature sets F 1 = f A; B g , F 2 = f B; C g , and F f A; C g resp ectiv ely and the goal is to obtain a join t mo del on the feature set F c = f A; B; C g . The above form ulation encompasses a num ber of common distributed learning tasks. For example, when all the sites share a class attribute, the distributed classi cation problem can be posed in terms of learning a join t densit y on the class lab els and all the avail-able features. Similarly , the distributed clustering task can be form ulated in terms of learning a join t mixture densit y on all the features. Semi-sup ervised classi cation and Bayesian net work learning are other examples of learning problems that can be posed in terms of the above form ulation.
To concretely form ulate the mo del integration problem, we need to rst quan tify the qualit y of the global mo del. In the absence of any priv acy constrain ts, one could possibly pool all the distributed data and obtain a complete mo del using the maxim um likeliho od principle. The qualit y of any given complete mo del c can then be measured using the data likeliho od or log-lik eliho od, i.e., Since a complete mo del is de ned on the feature set F c , whic h is the union of the local feature sets fF i g n i =1 , the data like-liho ods p c ( X i ) ; [ i ] n 1 are in fact the incomplete likeliho ods obtained by assuming the una vailable feature values as miss-ing data. Now, using the well kno wn relation between log-likeliho od and cross entrop y [4], it can be sho wn that the incomplete data log-lik eliho od with resp ect to a complete mo del is linearly related to the KL-div ergence or the rela-tive entrop y of true distribution on X with resp ect to the corresp onding marginal distribution of the complete mo del. Lemma 1 Let X = f x j g m j =1 be a dataset with featur e set F such that p X corresponds to the distribution on X and let
The phrases 'probabilit y densit y' and 'probabilit y distribu-tions' are used interc hangeably to mean either the probabilit y densit y function (in case of con tinuous distributions) or the probabilit y mass function (in case of discrete distributions). p c be any probability distribution de ne d on a featur e set F such that F F c . Then, wher e p F In the above relation, the entrop y term is indep enden t of the probabilit y densit y p c . Therefore, maximizing the av-erage data likeliho od is equiv alen t to minimizing the KL-div ergence between the data distribution p X and the appro-priate marginal densit y, i.e., the maxim um likeliho od prin-ciple corresp onds to a minim um KL-div ergence principle. Since the local mo dels f i g n i =1 are obtained from the datasets fX i g n i =1 , the corresp onding probabilit y distributions can be assumed to be reasonable appro ximations of the true dis-tributions on the datasets. Therefore, using Lemma 1, we de ne the quality cost of the global mo del to be where a lower cost indicates a better mo del and the weigh ts to the priv acy constrain ts, the local mo dels will not exactly corresp ond to the true distribution on the datasets fX i but C KL is a good choice for the cost function since we only have access to the local mo dels and not the original data.
With C KL as the qualit y measure, the problem of nding the optimal global mo del essen tially involves minimizing the KL-div ergence between the local mo dels and the appropriate marginal densit y induced by the global mo del. Since KL-div ergence is strictly con vex in both its argumen ts [4], and the marginal densities are linear functions of the complete densit y function, the overall qualit y cost function, whic h is a comp osition of the two functions, is con vex in the complete densit y p c , but not strictly so. Therefore, the qualit y cost has a unique minim um value, but there could be multiple mo dels corresp onding to the same minimal cost. In order to choose one of these mo dels, we invoke the maxim um entrop y principle [5], whic h is equiv alen t to making \no extra assump-tions" about the global mo del. Putting together both the maxim um entrop y and the minim um KL-div ergence princi-ples, we can formally state the Mo del Integration problem as where and P ( F c ) is the set of all probabilit y distributions over the feature set F c . Figure 2 sho ws a pictorial represen tation of the problem form ulation for the distributed scenario of Figure 1.

Since the KL-div ergence cost minimization (5) is a con vex optimization problem, the set of minimizers M is also a con-vex set. Further, since entrop y is strictly con vex, the overall mo del integration problem has a unique minimizer.

We consider two mo del integration scenarios | the rst where we require the optimal estimates of the join t distribu-tion for eac h elemen t in the domain and the second where it is desirable to have an interpretable global mo del even if it is Figure 2: Mo del integration in a distributed learning scenario involving di eren t feature sets. less accurate than the optimal one. Unless the optimal solu-tion has a closed form, numerical solution to the exact mo del integration problem is feasible only for situations where the complete feature vector f c tak es a nite num ber of distinct values. Section 4 addresses the exact mo del integration prob-lem for discrete domains. To address the second scenario, in Section 5, we form ulate and solv e a mo di ed version of the original mo del integration problem ( Par ametric Model Inte-gration ) where the global mo del is sough t from a speci ed parametric family , e.g., mixture of 10 Gaussians.
In this section, we transform the mo del integration prob-lem for a discrete domain into a pro jection problem based on KL-div ergence. Then, we deriv e prop erties of this for-mulation and describ e ecien t iterativ e algorithms that are guaran teed to con verge to the globally optimal solution.
Let F be any feature set with corresp onding feature vector f . Let ( F ) = ( f ) denote the set of all distinct values tak en by the feature vector f . To solv e the mo del integration problem (4) for a nite discrete domain, we map eac h discrete distribution p 2 P ( F ), the set of all distributions over F , to a unique point in the probabilit y simplex of dimension d = j ( F ) j . The basic idea is to enumerate all the elemen ts in ( F ) and represen t eac h mo del with a non-negativ e vector p 2 d + consisting of the probabilities asso ciated with these elemen ts. More precisely , if the elemen ts in ( F ) are ordered suc h that ( F ) = f w k g d k =1 , then the mapping from the probabilistic mo del to p is given by Hence, k p k 1 = d k =1 p ( f = w k ) = 1.

Using the above represen tation, every complete distribu-tion p c 2P ( F c ) maps to a unique vector p in the d c -simplex where d c = j ( F c ) j . Further, the marginal densities induced on any subset of features F can be sho wn to be linear pro-jections of the original complete densit y on appropriate sub-spaces. Let p F plete densit y p c on the feature set F F c . Let f be the feature vector corresp onding to the complemen t feature set F = F c nF . Then, p F w ), or in other words, p F set ( F c ) into j ( F ) j groups, one for eac h value of f , and summing up the probabilities asso ciated with eac h group. The marginal densit y p F mensional vector A F p where A F is the d d c mem bership matrix consisting of entries in f 0 ; 1 g with d = j ( F ) j and d = j ( F c ) j resp ectiv ely. Since eac h elemen t in the original domain ( F c ) (represen ted by columns) is included within a single elemen t of ( F ) (represen ted by the rows), eac h col-umn in A F will have exactly one row entry equal to 1 and the rest equal to 0.
 mo dels p i in the new represen tation. Then, the qualit y cost in the KL-div ergence minimization problem (5) can be rewritten as Example. Consider the mo del integration example sho wn in Figure 1 where F 1 = f A; B g ; F 2 = f A; B g , F 3 = f A; C g and F c = 3 i =1 F i = f A; B; C g . Let us assume that the features A; B; C tak e two values eac h so that j ( F c ) j = 8. Then, any complete mo del c over the features ( A; B; C ) can be mapp ed to a unique p 2 8 + , where Similarly , the local mo dels 1 ; 2 ; 3 can be mapp ed to vec-tors q 1 ; q 2 ; and q 3 resp ectiv ely where The pro jection matrices for obtaining the marginal densit y over F 1 in this case is given by Similarly , we can obtain the matrices A F 2 and A F 3 . Using the linearit y prop erty of KL-div ergence, the cost func-tion in (7) can be simpli ed and written as the KL-div ergence between a single discrete distribution q and A p where q and A are uniquely determined by the follo wing result. Prop osition 1 Let p 2 d c + such that k p k 1 = 1 and let q ; ; q n be n vectors such that q i 2 d i + and jj q i jj 1 1 ; [ i ] n 1 . Let A F 1 ; ; A F n be non-ne gative matric es such that A
F i is a d i d matrix wher e d i = j ( F i ) j ; d c = j ( F the sum of each column in each matrix A F i equals 1, i.e., k a v k 1 = 1 ; [ v ] d c 1 ; [ i ] n 1 . Further, let f i g n weights such that n i =1 i = 1 . Then, wher e q = [ 1 q 1 T ; n q n T ] T , A = [ 1 A F 1 T ; n A Using the above result, the KL-div ergence minimization in the mo del integration problem can be restated as where q 2 d 0 + and A 2 d 0 d c + are determined by Prop o-sition 1. Since (8) involves minimizing a con vex function under the linear constrain ts p 2 d c + and k p k 1 = 1, there is a unique minim um value and the set of minimizers is con vex. The follo wing result sho ws that the solution set is in fact the intersection of an ane space with the d -simplex.
 Prop osition 2 The solution set for the KL projection prob-lem (8) is given by f p 2 d c + j A p = q g wher e q is a uniquely determine d non-ne gative vector such that k q k rank ([ A; q ]) , then q = q itself.
 Prop osition 2 pro vides a closed form for the solution of (8) for the special cases where rank ( A ) = rank ([ A; q ]) or when the null space of A can be easily characterized. It also simpli-es the entrop y maximization problem, since the set of min-imizers M in (5) is now characterized by linear constrain ts p 2 d c + , A p = q and k p k 1 = 1. When A is a full column rank matrix (i.e., rank ( A ) = d c ), then there is a unique min-imizer, i.e., jMj = 1. In general, the entrop y maximization problem is
The KL-div ergence minimization and the entrop y max-imization problems discussed above are both con vex opti-mization problems with linear constrain ts and can be solv ed ecien tly using iterativ e scaling algorithms.
 The KL-div ergence pro jection problem (8) has earlier been studied in the con text of positron emission tomograph y [3, 18]. It is usually solv ed using the Exp ectation Maximization Maxim um Lik eliho od (EMML) algorithm (see Algorithm 1), whic h is guaran teed to con verge to a global minimizer p whic h can then be used to iden tify the entire solution set f p 2 d c + j A p = q = A p KL g . Eac h iteration of the EMML algorithm requires a computation time that is linear in the size of p and A (i.e., num ber of non-zeros).

A more ecien t approac h involves reducing the KL div er-gence pro jection problem to a form solv able by the Iterativ e Re-w eigh ted Least Squares (IRLS) algorithm [9]. The basic idea is to re ne the solution using Newton-Raphson like ad-ditiv e updates, whic h for the KL-div ergence cost function is b minimization problem where only B and h change in eac h iteration, the problem can be solv ed ecien tly using the IRLS algorithm. The com-putational time of the algorithm is still linear in the size of p and A (i.e., num ber of non-zeros), but eac h iteration only requires computational time that is linear in p .
 The linear constrained entrop y maximization problem (9) has been studied in a num ber of con texts [3, 5]. It is of-ten solv ed using the Multiplicativ e Algebraic Reconstruction technique (MAR T), whic h is a cyclic minimization pro cess (see Algorithm 1) guaran teed to con verge to the optimal distribution for any A 2 d 0 d c + and q 2 d 0 + suc h that a the case in (9).
 Algorithm 1 Discrete Mo del Integration
In this section, we form ulate a constrained version of the mo del integration problem, whic h we call the Par ametric Model Inte gration problem. Then, we sho w how this prob-lem can be ecien tly addressed using sampling techniques and the Exp ectation-Maximization (EM) algorithm.

Our form ulation of the parametric mo del integration prob-lem is motiv ated by the fact that in real scenarios, it is often preferable to obtain a solution that is easy to understand and describ e even at the cost of optimalit y. To incorp orate this requiremen t, we constrain the global mo del to belong to a speci ed parametric family G P ( F c ), e.g., mixture of k Gaussians ( k &lt; 10). The KL-div ergence cost minimization problem for this case can be stated as where G P ( F c ) is the speci ed family of parametric distri-butions. However, unlik e the unconstrained case, (11) is not necessarily a con vex problem since the optimization is over the mo del parameters and the KL-div ergence cost is not al-ways con vex in these parameters. As a result, there are usu-ally multiple local minima for the KL-div ergence minimiza-tion. Moreo ver, the set of minimizers may not be a con vex set, whic h mak es it more dicult to solv e the entrop y max-imization problem. To mak e the constrained mo del integra-tion problem tractable, we assume that the parametric family G is chosen so that ther e is a unique minimizer for the KL-diver genc e optimization problem , whic h eliminates the need for solving the maxim um entrop y problem. However, in gen-eral, (11) is itself a dicult problem to solv e and it is only possible to obtain the local minimizers since C KL is not al-ways a con vex function of the parameters and G need not be a con vex set. A direct solution of (11) using regular optimization techniques suc h as gradien t descen t, Newton-Raphson's metho d, etc., is computationally infeasible when the local mo dels corresp ond to con tinuous probabilit y dis-tributions since only the parameters of the local mo dels are available at the integration. Therefore, we pose an appro xi-mate version of the mo del integration problem by generating arti cial samples from the local mo dels. The main idea is to appro ximate the KL-div ergence with resp ect to the local mo dels in terms of the likeliho od of the generated data.
Let the datasets f ~ X i g n i =1 be obtained by sampling from the local mo dels f i g n i =1 . When the dataset sizes ~ m i are large (tend to 1 ), then the distribution on the datasets f ~
Xg n i =1 is iden tical to the corresp onding local mo del distri-butions. Therefore, from Lemma 1, it follo ws that for large sample sizes, the average log-lik eliho od of eac h dataset with resp ect to any probabilistic mo del c 2G is linearly re-lated to the KL-div ergence between the corresp onding i and the appropriate marginal densit y of c . More speci cally , lim where H ( p i ) is the entrop y of the local mo del i , whic h is indep enden t of c . Hence, maximizing the log-lik eliho ods of the datasets f ~ X i g n i =1 is equiv alen t to minimizing the KL-div ergence with resp ect to the corresp onding local mo dels. This observ ation enables us to pose the constrained mo del integration problem (11) as a maxim um likeliho od parameter estimation problem, whic h can then be con venien tly solv ed using an Exp ectation-Maximization (EM) algorithm.

Algorithm 2 sho ws the main steps in the pro cess, i.e., gen-erating arti cial datasets from the local mo dels and learn-ing the maxim um likeliho od parametric mo del based on the com bination of these datasets. Since the local mo dels are based on a smaller set of features than the global mo del, the una vailable feature values are mo deled as missing and are re-estimated during the E-step of the algorithm.

The global mo del a c resulting from the EM algorithm is a local minimizer of the appro ximate problem and not neces-sarily the same as the optimal solution of (11). However, it is guaran teed to asymptotically con verge to a locally optimal solution as the size of ~ X c goes to 1 . In practice, one can use multiple runs of the EM algorithm and pick the best solution among these so that the obtained mo del is reasonably close to the globally optimal mo del.
In this section, we rst describ e an alternate scheme for solving the mo del integration problem by recursiv ely decom-posing it into smaller sub-problems. Then, we presen t closed form solutions for certain special distributed learning sce-narios, whic h can be incorp orated into the recursiv e scheme to ecien tly solv e the mo del integration problem for more general scenarios.
 Algorithm 2 Parametric Mo del Integration
Solving the mo del integration problem (8) can be compu-tationally exp ensiv e when j ( F c ) j is large since the size of A and p dep ends on j ( F c ) j . However, it is possible to re-duce these computational costs when some of the features can be assumed to be indep enden t or conditionally indep en-den t given some other features based on domain kno wledge or the available local mo dels. The key idea is that the original mo del integration problem (8) can be recursiv ely split into smaller mo del integration problems with closed form solu-tions. This recursiv e decomp osition pro cess is more ecien t than a direct solution since it only estimates the indep enden t parameters required to determine the complete mo del, whic h is usually much less than j ( F c )) j .

The recursiv e decomp osition pro cedure is based on the fol-lowing result that express the optimal solution to the mo del integration problem in terms of the solutions to smaller prob-lems on subsets of features.
 Prop osition 3 Let F a ; F a be complementary subsets of F i.e., F a = F c nF a . Let F ia = F i F a ; F ia = F i F Let p a ; p a be the optimal solutions for the model inte gra-tion problems based on the local mar ginal densities f p F Further, let p a j a ( f a j f a ) be the optimal solution to the model inte gration problem involving the conditional densities f p [ i ] 1 . The solution to the original model inte gration problem (4) is given by (a) p (b) p
The above result outlines a metho d for reducing the orig-inal mo del integration problem of size ( F c ) into smaller problems of sizes ( F a ) and ( F a ). When the desired global mo del exhibits conditional indep endence, a recursiv e scheme with judicious choice of F a and F a at eac h stage can result in an ecien t solution. For example, when the local mo d-els all corresp ond to naiv e Bayes classi ers with the same class attribute, but possibly di eren t data attributes, invok-ing Prop osition 3(b) with F a as the class attribute and F the remaining attributes enables us to decomp ose the overall mo del integration problem into that of integrating the class priors and the class conditional densities for eac h class. Fur-ther, using the naiv e Bayes assumption and Prop osition 3(a), the optimal conditional densit y for eac h class is iden tical to the pro duct of the optimal densities of the individual at-tribute given the class.

Algorithm 3 in [15] sho ws a recursiv e decomp osition scheme for mo del integration. If a given problem has a closed form solution, then the optimal mo del is computed directly . Oth-erwise, we look for subsets F a and F a that are indep enden t of eac h other and when suc h indep enden t splits are available, we invoke Prop osition 3(a) to split the mo del integration problem. When there are no indep enden t splits, we choose any split F a and F a , preferring those in whic h one of the subsets can be further split into indep enden t sets and invoke Prop osition 3(b).
We now describ e some special scenarios that admit closed form solutions for the mo del integration problem.
 First, we consider two simple base scenarios corresp onding to iden tical and disjoin t feature sets resp ectiv ely. In case of iden tical feature sets(i.e., horizon tally partitioned data), the complete feature set F c = F i ; [ i ] n 1 and the optimal solution to the mo del integration problem (4) is given by [16] In case of disjoin t feature sets, the complete feature set F is no overlap of features, none of the local mo dels sho w de-pendence between any pair of feature sets. Hence, a rep eated application of Prop osition 3(a) sho ws that the optimal solu-tion is just the pro duct distribution, i.e., We now look at scenarios where the feature sets of the collab-orating parties can be organized in a certain hierarc hical fash-ion. First, consider a at tree (depth =1) con guration where the feature sets of all the parties have a common overlapping feature set F 0 corresp onding to the root node. The feature sets of the individual parties are assigned to the leaf nodes of this tree. In order to solv e the mo del integration problem for this setting, we rst split it into sub-problems corresp onding to the complemen tary sets F 0 and F 0 = F c nF 0 by invok-ing Prop osition 3(b). The sub-problems corresp onding to F and F 0 can then be directly addressed using the closed form solutions for iden tical and disjoin t feature sets resp ectiv ely as sho wn in the follo wing lemma.
 Lemma 2 In problem (4), when every pair of collaborat-ing parties have a common set of overlapping featur es, i.e., F Then, the optimal solution to (4) is given by For the general case, we adopt the follo wing pro cedure to construct the tree. First, the feature set of eac h collab o-rating part y is assigned to a leaf node and for every pair of nodes, an interme diate node corresp onding to the features shared between the two nodes is created suc h that eac h in-termediate node corresp onds to a unique set of features (i.e., no rep etition). All the nodes are then arranged in the form of a tree suc h that for eac h node, all the nodes corresp onding to sup ersets of the node are con tained in the sub-tree under that node. In general, suc h an arrangemen t results in some nodes lying under multiple sub-trees as in Figure 3(a), i.e., there are cycles in the tree. However, for situations where the feature sets can be arranged as a tree without cycles as in Fig-ure 3(b), the feature sets are considered to be hierarc hically ordered and the unconstrained mo del integration problem (4) admits a closed form solution. Note that we have not made any indep endence assumptions in this setting. More Figure 3: Figures (a) and (b) sho w examples of sce-narios that can and cannot be arranged in a hierar-chical fashion. The shaded nodes are the leaf nodes and the rest are intermediate ones. formally , let F ( N ) represen t the feature set corresp onding to any node N . Let C ( N ) denotes the num ber of imme-diate children of N and f N c k g C ( N ) k =1 be the children. Then, the nodes in the tree satisfy F ( N ) = C ( N ) k =1 F ( N
To address this general scenario, we observ e that Lemma 2 enables us to com bine the child node distributions to obtain the optimal distribution over the union of the feature sets of all the children. Hence, the optimal distribution over all the feature sets in the tree can be obtained by computing the optimal solution at eac h sub-tree in a bottom-up fashion using rep eated invocations of Lemma 2.
In this section, we pro vide empirical evidence that high qualit y global mo dels can be obtained without much loss of priv acy using our mo del integration approac h for both dis-crete and con tinuous data. We presen t results on arti cial and real datasets that sho w how various factors suc h as fea-ture correlation and priv acy of local mo dels a ect the qualit y of the global mo del obtained through our metho d.
For our exp erimen ts, we used both arti cial and real datasets consisting of con tinuous vector, discrete and high dimen-sional directional attributes. Table 1 sho ws details of the datasets. The main purp ose of the arti cial datasets was to facilitate comparison of the global mo del qualit y with the true generativ e mo del and to study the e ect of correlation between features on the e ectiv eness of our approac h. In order to achiev e this, we generated ve Gaussian datasets from mixtures of 15 spherical Gaussians. To ensure that the datasets had increasing degrees of correlation between the features, the mixture cen ters for eac h dataset were ob-tained by sampling from a Gaussian with the same mean, but varying covariance matrices (increasing o -diagonal el-emen ts). The average absolute pairwise correlation values between features for the ve datasets are 0 : 0295, 0 : 0869, 0 : 1374, 0 : 2447, 0 : 4171 resp ectiv ely. We also used two real datasets | Mini-20Newsgr oup (subset of the 20 Newsgr oup dataset [13]) consisting of high dimensional directional text data, and Dermatolo gy (from UCI) consisting of clinical trial results. The datasets can be downloaded from http://lans.ece.utexas.edu/ srujana/mr/ .
For eac h exp erimen t, the relev ant dataset was partitioned at random among the various sites. The features were also partitioned so that eac h site has access to only some fea-tures. In case of classi cation, we hold out a test set and partition only the training set among the sites. Dep ending on the learning task, we built parametric mo dels from the local data using appropriate EM algorithms or direct maxi-mum likeliho od estimation (MLE) metho ds. For con tinuous data, the local mo dels were used to generate arti cial sam-ples, whic h were then used to learn the global mo del. For discrete data, the local mo dels were directly used to obtain the global mo del by solving the appropriate KL-pro jection problem (8). We also trained a cen tralized mo del by pooling data from all the sites.

For eac h exp erimen t, we computed the priv acy of the local mo dels as well as the qualit y of the global, cen tralized and the various local mo dels. For clustering tasks, the qualit y was quan ti ed using normalized mutual information (NMI) and for classi cation, it was quan ti ed using misclassi cation error (ME). In case of arti cial data, KL-div ergence (KL) with resp ect to the true mo del was also computed. Further, results were averaged over multiple runs of the exp erimen ts performed using either di eren t sets of features or di eren t datasets in the case of arti cial data. Table 2 sho ws details of the learning algorithms and performance metrics.
First, we performed con trolled exp erimen ts on the arti -cial datasets to analyze the beha vior of our algorithms for both discrete and parametric mo del integration. For a fair comparison, the local mo dels were assumed to have the same form as the global and cen tralized mo del and the num ber of arti cial samples used for learning the global mo del (global sample size) was chosen to equal the com bined size of all the local datasets. Keeping all other factors unc hanged, we studied how the qualit y of the global solution varies with re-spect to feature correlation, num ber of overlapping features, priv acy of the local mo dels and the global sample size. The results of our exp erimen ts are discussed below.
 of the various mo dels for the Gaussian data. In all the cases, the qualit y of the global mo del is better than that of the local mo dels and is closer to that of the cen tralized mo del. variation of the qualit y of the various mo dels with the correla-tion between the features and the num ber of overlapping fea-tures. We observ e that the di erence between the global and the local mo dels is more signi can t when the average feature correlation is low and there is less overlap between features since in this case, the com bined information in the local mo d-Figure 4: Qualit y vs. avg. feature correlation for A-Gaussian . #clusters = 15, global sample size = 5000. Columns corresp ond to the cases #features = 2,6 and 10. Figure 5: Qualit y vs. priv acy for A-Gaussian . #fea-tures/site=6 and global sample size = 5000. els is signi can tly greater than that in the individual mo dels. Further, the mo del qualit y sho ws an initial decrease with in-creasing feature correlation, but eventually impro ves. This non-monotonic beha vior probably arises due to the trade-o between the problem dicult y and the amoun t of informa-tion in a given num ber of samples. When the features are uncorrelated, the amoun t of information per sample is the highest, but the dicult y level of the problem is also high, whereas for highly correlated features, it is the opp osite.
Quality vs. privacy . We also studied the trade-o be-tween priv acy , and the qualit y of the global mo del by vary-ing the resolution of the local mo dels, i.e., the num ber of local clusters in this case. Figure 5 sho ws the variation of the qualit y measures and the average log-priv acy with the local mo del resolution. We note that as the local mo del res-olution increases, the qualit y of global mo del impro ves while the average log-priv acy goes down. In particular, the cen tral-ized mo del corresp onding to a complete loss of priv acy has the highest qualit y. However, due to the natural structure in data, comparable qualit y can be obtained with much less resolution and reasonably high priv acy .

Quality vs. sample size. The parametric mo del integra-tion problem is based on an appro ximation that is exact only when the num ber of arti cial samples used for learning the Figure 6: Qualit y vs. global sample size for A-Gaussian . #features/site=6, #lo cal clusters=15 global mo del tends to 1 . Hence, we exp ect the global mo del qualit y to impro ve with increasing sample size. The variation of the global mo del qualit y with resp ect to the global sample size sho wn in Figure 6 supp orts this hypothesis. In particu-lar, we observ e that the qualit y of the global mo del steadily increases and approac hes that of the cen tralized mo del when the global sample size is the same as the com bined size of the local data sources.
We also conducted exp erimen ts on some real-life datasets to demonstrate the e ectiv eness of our mo del integration techniques for various types of data and applications. the qualit y of the global mo del and the average log priv acy of the local mo dels on the mini-20Newsgr oup data for vary-ing num ber of local clusters. We observ e that the global mo del is better than even the best local mo del. Further, for a reasonable num ber of clusters, the global mo del is almost as good as the cen tralized mo del, while ensuring a high level of priv acy .
 dataset consists of discrete clinical attributes that are rele-vant for diagnosing skin diseases and hence, classi cation is the most relev ant learning task in this case. To accommo-date the large num ber of attributes, we assume conditional indep endence of features ( Naive Bayes ) or sets of features ( Partial Conditional Indep endenc e ) given the class. Table 3 sho ws the classi cation results. The ensem ble mo del is based on averaging the class posteriors of the local mo dels. When there is no compression, the cen tralized mo del is iden tical to the global mo del since both the mo dels are based on iden-tical conditional indep endence assumptions, whic h mak e the extra information available to the cen tralized mo del redun-dan t. As before, there is a trade-o between the priv acy of the local mo dels and the qualit y of the global mo del.
Our work is primarily related to four main areas: dis-tributed learning, priv acy preserving data mining, informa-tion theory and iterativ e algorithms. In particular, the prob-lem involves distributed learning in a priv acy-preserving set-ting while the mathematical form ulation is based on information-theoretic ideas suc h as maxim um likeliho od and maxim um entrop y and the prop osed solutions are based on iterativ e optimization techniques.

As men tioned earlier, there has been a lot of work on dis-tributed learning techniques. However, most of these tech-niques [20, 10, 19] focus only on horizon tally or vertically partitioned data. The work in [17] considers heterogeneous data sources, but is mainly focused on obtaining general-ization error bounds for the distributed classi cation task. In the curren t work, we prop ose a distributed mo del-based learning framew ork that can sim ultaneously address a num-ber of learning tasks suc h as classi cation, clustering, learn-ing Bayesian net works, etc., and is applicable to a wide range of distributed scenarios where there are no restrictions on the features available at eac h site. Further, unlik e some ear-lier parametric mo del com bining techniques [7] that are re-stricted to vector data, our framew ork is based on generativ e mo dels and applies to a wide range of complex data types encoun tered in data mining.
In the recen t years, there has been considerable work on priv acy-preserving distributed data mining techniques A sur-vey of these techniques can be found in [8]. Of these, ran-dom perturbation based techniques[2] are limited to vector data and there are no theoretical guaran tees on the achiev ed priv acy [11]. In con trast, secure multi-part y computation based techniques [14, 20] do not often capture the priv acy requiremen ts of real-life scenarios and also involve high com-putational and comm unication costs. Our curren t work ex-tends the framew ork prop osed in [16], whic h is based on an information-theoretic notion of priv acy and involves sharing only parametric mo dels that satisfy the priv acy requiremen ts at eac h site. The main bene t of our approac h is scalabilit y and mo dularit y, whic h mak es it amenable to other priv acy-preserving transformations suc h as data swapping, etc.
Our form ulation of the mo del integration problem is based on the maxim um likeliho od and the maxim um entrop y prin-ciples, whic h are kno wn to have applications in a wide range of domains [4, 5]. For discrete domains, the KL-pro jection problem is closely related to inverse problems in positron emission tomograph y [3, 18] highligh ting the fact that the mo del integration problem can also be view ed as reconstruct-ing the original distribution from noisy partial views. A sim-ilar pro jection problem arises in linear multi-v ariate logis-tic regression based on multinomial or Poisson mo dels [12]. However, the order of the argumen ts q and A p is rev ersed in this case, requiring a completely di eren t solution for this case. The solution to the maxim um entrop y problem is based on iterativ e pro jection metho ds [3] dev elop ed for optimiz-ing con vex objectiv e functions asso ciated with Bregman loss functions and in particular, KL-div ergence. [3] describ es a num ber of these techniques suc h as Bregman's row action metho d, MAR T, SMAR T, etc. For the KL-div ergence mini-mization, we adopt the IRLS algorithm [9], whic h is kno wn to be computationally more ecien t, and has in past been applied to problems suc h as multi-v ariate logistic regression.
We prop osed a distributed learning framew ork based on probabilistic mo dels that tak es into accoun t priv acy con-strain ts, and is applicable to a large class of learning tasks, and to general distributed settings involving div erse schema. In order to achiev e this, we form ulated the distributed mo del integration problem using maxim um likeliho od and maxi-mum entrop y principles. We also dev elop ed ecien t solutions for both discrete and con tinuous domains, and specialized algorithms for scenarios involving conditional indep endence assumptions and hierarc hically ordered sets. All our algo-rithms require a computation time that is linear in the size of the local mo dels, thus making our approac h scalable for large datasets. Exp erimen tal evaluation of our algorithms on various types of data (both con tinuous and discrete) indi-cates that high qualit y distributed learning can be performed without much loss of priv acy .
We would like to ackno wledge supp ort from the NSF under gran ts IIS-0307792 and IIS-0312471.
