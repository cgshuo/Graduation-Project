 Discretization is an effective technique to deal with continuous attributes for machine learning and data mining. In many algorithms such as rules selection and feature clas-sification, continuous (real value) tributes must be discretized, which is especially attributes with the symbolic attributes. 
The main essence of discretization is to keep the information as discrete as possible and reduce the information loss. Existing discretization methods can be generally and static versus dynamic [3]. Local methods are used for producing partitions that methods use the entire instance space to discretize. Unsupervised methods, such as equal width interval and equal frequency interval methods, do not utilize instance class labels in the discretization process. By contrast, supervised methods are referred to those discretization methods that utilize the class labels. Static methods, entropy-attributes, where m is the maximum number of intervals produced in discretizing an attribute. Dynamic methods conduct a search through the space of possible m values for all attributes simultaneously, thereby capturing interdependencies in attribute discretization. A number of entropy measure-based methods establish a strong group of works in the discretization domain. This co ncept uses class entropy as a criterion to algorithm of searching for an optimal set of separating hyper-planes by a genetic algorithm. Liu and Wang proposed a new measure of class heterogeneity of intervals from the view point of class probability itself in [7]. Other algorithms use class attrib-utes interdependency information as the crite rion [2],[4]. These methods try to maxi-mize the interdependence between the discretized attributes and class labels based on information theory. 
The ChiMerge algorithm introduced by Kerber in 1992 is a supervised global dis-cretization method [4]. The method uses 2  X  test to determine whether the current point is merged or not. Liu et al. [6] proposed a Chi2 algorithm in 1997 based on the ChiMerge algorithm. In this algorithm, the authors increase the value of the 2  X   X  threshold dynamically and decide the intervals X  merging order according to the value of D, where 2 2  X   X   X   X  = D and 2  X   X  is fractile decided by the significance level Francis et al. further improved the Chi2 algorithm and proposed the Modified Chi2 algorithm in [15]. The authors showed that it is unreasonable to decide the degree of freedom by the number of decision classes on the whole system in the Chi2 algorithm. Conversely, the degree of freedom should be determined by the number of decision classes of each two adjacent intervals. In [14], the authors pointed out that the method of calculating the freedom degrees in the modified Chi2 algorithm is not accurate and proposed the Extended Chi2 algorithm, which replaces D with D / v 2 . statistic and significance level to determine whether a node should be merged is ana-lyzed. Based on the study the meaning of 2  X  statistic, a new modified algorithm called Rectified Chi2 algorithm is proposed. The new algorithm regards a new merg-value attributes exactly and reasonably. We ran respectively C4.5 and SVM on the discreted data, and the experiment results show that the proposed lgorithm is effec-tive. Besides, a sequence method (DSM) is proposed to solve the problem that the algorithms of Modified Chi2 algorithm and Extended Chi2 have, i.e., the two algo-rithms only adopting maximal difference as standard of interval merger, and the new algorithm X  X  advantage performance is manifested by experiments. Compared with Modified Chi2 algorithm based on DSM and Extended Chi2 algorithm based on DSM, the Rectified Chi2 algorithm based on DSM still have better performance. Finally, the imprecise problem of ij E value in the 2  X  statistic are analyzed and two modified schemes are proposed. The experiment results show the effectiveness of the above presented algorithm. 
The rest of this paper is organized as follows. Section 2 introduces some basic con-cepts related to this paper. Sections 3 and 4 propose the Rectified Chi2 algorithm and DSM, respectively. Section 5 evaluates the proposed algorithm by experiments. Fi-nally, Section 6 concludes this paper. Several conceptions about discretization are introduced as follows: terval. Adjacent two intervals have a comm on node. Discretization algorithm of real value attributes actually is in the pro cess of removing node and merging adjacent intervals based on definite rules. puted in this discretization algorithm. where interval;  X  intervals and significance level  X  . In statistics, asymptotical distribution of 2  X  statistic  X  distribution. 2 gven a In formula  X  1  X  X  X  N C j / is the proportion of number of patterns in jth class ac-indicates the equality degree of the jth class distribution of adjacent two intervals. The smaller 2  X  value is, the more similar is class distribution, the more unimportant node is. So, it should be merged. 
For statistic 2  X  , the smaller 2  X  value is, the more similar is class distribution, the more unimportant is node. Determining the importance of a nodes by 2 2  X   X   X   X  = D intend to farthestly enhance statistic of nodes in system under definite significance two intervals, but also relates with the num ber of system classes, namely: the higher the number of system classes are, the more number of classes of adjacent two inter-vals are, the greater degrees of freedom of adjacent two intervals are. Thus, 2  X  value calculated is different. So, significance of nodes should be determined by decreasing prematurely significance level  X  and illuminating dequately rationality of node merging. Based on the study a new modified algorithm called Rectified Chi2 algorithm is proposed. The new algorithm regards a new merging criterion The rectified Chi2 algorithm is described as follows: Step2: Sort data in ascending order for each attribute and calculate 2  X  value of each Step3: Merge. Step4  X if if  X  can not be decreased Step5  X  do until no attribute can be merged The variational range of difference 2 2  X   X   X   X  = D , difference v D 2 / and differ-ence 2 2 ) / ( '  X   X   X  k v D  X  = are influenced by degrees of freedom. In fact, nodes shoule be merged with a measure of probability. Perhaps the node which has the maximal difference value can be merged with a greater probability. So, the original algorithm that only adopted maximal difference as standard of interval merger is unreasonable and unfair. Thus, a difference sequence method (DSM) is proposed to solve the problem that the algorithms of M odified Chi2 algorithm and Extended Chi2 have, i.e., the two algorithms only adopting maximal difference as standard of interval merger. When adjacent two intervals that have the maximal difference can not be merged, we should search in sequece until finding mergable node, resolving the unfair problem and given the chance of merger for all of the nodes. 
As seen from Table 1, a , b , and c are condition attributes, d is decision attrib-nodes and five groups of adjacent intervals in Table 1. Take Extended Chi2 algorithm of references [14] for example  X  merge criterion: v D 2 /  X  : We may get signifi-cance level 9 . 0 =  X  and difference value of adjacent two intervals of attribute a is maximal by calculating  X  degree of freedom 2 1 = v , table look-up: 61 . 4 2 =  X   X   X  , namely: 6383 . 1 1 = D . For adjacent two intervals of attribute b  X  pattern 1 and maximal difference value can not be merged. Though 1 2 D D &lt; , the adjacent inter-vals of attribute b can still be merged, which should be given the chance of merger, achieving better discretization effect. The experiment results show that Modified Chi2 algorithm based on DSM, Extended Chi2 algorithm based on DSM and Rectified Chi2 algorithm based on DSM have better performance. 
In the following, we analyze the improvement of ij E value of 2  X  statistic. In the  X  formula, if either number of classes of adjacent two intervals are less than the number of system classes, some of inner items of 2  X  equal to 0.1. There is unreasonable to do so. For more than the number of classes of the other, and the two groups of adjacent intervals do not equal to the number of system classes. Thus, class distribution of adjacent intervals with the greater number of classes is not always fluctuation, while class distribution of adjacent intervals with the smaller number of classes is not always smoothness. There is unfair to plus 0.1 synchronously for missing class item of two should be decreased properly (decreasing properly 2  X  value). If the number of de-properly. The two groups of adjacent intervals which have different degrees of free-dom can get to balance. So, we have the following two improved method: 1. Unified Standard 2  X  : There is a unified ij E standard to calculate 2  X  . where v is number of degrees of freedom of adjacent two intervals and t s sum of number of patterns of adjacent two intervals. Explanation for the above ij E  X  is 0.4548. So, the upper limit of according to the number of degrees of freedom of adjacent two intervals. Namely, if the number of degrees of freedom of adjacent two intervals is greater, ij E value should be decreased properly. If the number of degrees of freedom of adjacent two v in ering the number of degrees of freedom of adjacent two intervals have connection with the size of adjacent two intervals, we also select member of ij E . 2. Optimized 2  X  : Finding ij E value which can assist to achieve higher predictive accuracy and is usually between 0.0 and 0.4. To contrast experiment  X  we adopt the data sets of UCI machine learning database [8]  X  see Table 2  X  ,The UCI machine learning data sets are common used in data mining experiment. 
Eight data sets were discreted by Rectified Chi2 algorithm which is proposed in this paper (shortened form Rec) and References [15] method (shortened form Mod) and References [14] method (shortened form Ext) and Rectified Chi2 algorithm based DSM (shortened form Rec DSM) and Modi fied Chi2 algorithm based DSM (short-ened form Mod DSM) and Extended Chi2 algorithm based DSM (shortened form Ext DSM). Meanwhile, Eight data sets were discreted by the above six algorithms which adopt Optimized 2  X  and Unified standard 2  X  , denoted by  X  X hortened form  X  2  X   X   X , for example  X  Mod ( 2  X  ), Ext DSM ( 2  X  ) etc. 
We ran C4.5 [13] on the discreted data. Choosing randomly 80 percent of Exam-the average numbers of nodes of decision tree and the average numbers of rules ex-tracted are computed and compar ed by different algorithms  X  see Table 3 to Table 8). Meanwhile, discreted data is classified by multi-class classification method [17] of one to one of SVM. Choosing randomly 80 percent of Examples are training sets, the rest are testiing sets. Model type: C-SVC. Kernel function type: RBF function. Search range of penalty C: [1, 100]. Kernel function parameter (acc) and the number of support vector (svs) are computed and compared for the above twelve algorithms  X  see Table 9 to Table 12  X  . Considering computational complexity of kernel function depends on vector inner product of samples and attrib-normalized: 1 method can be used in training sets and testing sets. As seen from Tables 3 and 4, compared Rectified Chi2 algorithm with Modified Chi2 algorithm and Extended Chi2 algori thm, the average predictive accuracy are accuracy have raised greatly. The average numbers of nodes of decision tree of Rectified Chi2 algorithm and the average numbers of rules extracted of Rectified Chi2 algorithm have decreased apparently except for Pima data set, which the two Chi2algorithm. Compared with the above three algorithms, th e average predictive accuracy of the three algorithms based on DSM have essentially enhancement except for Glass data set and the average numbers of nodes of decision tree and the average numbers of rules extracted have decreased apparently for the majority of data sets, manifesting superiority of the three algorithms based on DSM. Besides, Rectified Chi2 algorithm based on DSM still has better performance. 
Tables 5 and 6 have presented the results of the the above six discretization algo-rithms based on improved 2  X  (Optimized 2  X  ). Compared with unimproved 2  X  , the average predictive accuracy of the six discretization algorithms based on Opti-mized 2  X  have enhanced greatly except the average predictive accuracy of Breast  X  specially characterist of Optimized 2  X  . The average numbers of nodes of decision tree and the average numbers of rules extracted of the six discretization algorithms pre-sented in Tables 3 and 4 based on Optimized 2  X  have decreased apparently for the majority of data sets, manifesting advantage of the six algorithms based on Opti-mized 2  X  . Besides, either Rectified Chi2 algorithm based on Optimized 2  X  or Recti-fied Chi2 algorithm based on DSM and Optimized 2  X  still has better performance. 
Tables 7 and 8 have presented the results of the six discretization algorithms pre-sented in Table 3 and Table 4 based on improved 2  X  (Unified Standard 2  X  ). Com-pared with unimproved 2  X  , the average predictive accuracy of Breast, Bupa and Pima data sets which have only two classes is the same as before. For the rest of data sets, the average predictive accuracy of Modi fied Chi2 algorithms and Extended Chi2 algorithms presented in Tables 3 and 4 based on Unified Standard 2  X  have greatly enhanced, especially Auto and Machine data sets. The average predictive accuracy of Machine data sets of Rectified Chi2 algorithms based on Unified Standard 2  X  have enhanced apparently. Meanwhile, the average numbers of nodes of decision tree and the average numbers of rules extracted of the six discretization algorithms presented Standard 2  X  . Study of discretization algorithm of real va lue attributes operates an important effect for many aspects of computer application. Though many discretization algorithm were proposed, the methods and theory should be developed in practical. Series of algorithms of Chi2 algorithm based on probability statistics theory offer a new way of on application sense of 2  X  statistic of these algorithms, improving Modified Chi2 algorithm and Extended Chi2 algorithm. An d, a new modified algorithm called Recti-fied Chi2 algorithm is proposed. The new algorithm regards a new merging standard as basis of interval merging and proves through theory and experiment that it can discrete the real value attributes exactly and reasonably. Besides, a difference se-quence method (DSM) is proposed to solve the problem that the algorithms of Modi-fied Chi2 algorithm and Extended Chi2 have, i.e., the two algorithms only adopting maximal difference as standard of interval merger, and he advantage performance of DS over that the two algorithms is manifested by experiments. Finally, the imprecise problem of ij E value in the 2  X  statistic are analyzed and two modified schemes are proposed. The experiment results show that algorithms based on DSM can greatly enhance the effect of the three discretization algorithm. Besides, Rectified Chi2 algo-rithm based on DSM still has better performance and discretization algorithms based on improved 2  X  are effective. There is practical function to promote the study of the aspects. Acknowledgments. This work is supported by Science and Technology Plan of Dalian, China (2007A10GX117). In the modern financial engineering discipline, even though there are numerous differ-ent portfolio management theories proposed where their contents and arguments may be criticizing each others, their underlying principles always converge to one single theme: all investment decisions should be made to achieve an optimal tradeoff between risk and return [11]. As a portfolio manager, her main task is therefore to identify an appropriate combination of assets from the ex isting opportunities, su ch that this partic-ular combination of assets would result in t he lowest level of expected risk according to a desired level of return. A combination of assets is known as a portfolio.

While the underlying principle of governing how the portfolios should be constructed is simple and intuitive  X  minimize the expected risk and maximize the expected return, the definition of expected risk and the definition of expected return are usually ambigu-ous. In fact, this is exactly what lead to the m ajor differences among different portfolio management theories. Little consensus exists among different theories regarding to their meanings and their measurements [12,1,9,5,6].

Traditionally, the portfolio X  X  expected return is computed by combining the multipli-cation of the expected return of each asset in t he portfolio and its contribution in there in a linear manner. Under this formulation, the dependence relationship among the assets in the portfolio will be ignored. For instance, given two financial assets, A and B ,where their expected returns are respectively r A = $10 and r B = $10. Furthermore, assume that we will formulate a portfolio, P , by buying equal shares of them (50% each). Consider for an ideal situation that A and B are complementary of each others  X  whenever the price of A rises, the price of B will drop, and vice versa. Obviously, the expected return of P would be $0, i.e. we should not expect to gain any profit by buying A and B simul-taneously with equal shares. Unfortunately, if we compute the expected return of P by using the traditional approach, then it becomes $10  X  0 . 5 + $10  X  0 . 5 = $10, which is nevertheless subject to debate.

In this paper, we proposed a model, called co-movement model, to compute the portfolio X  X  expected return by considering the dependency relationships among assets in the portfolio. If we know the price of two assets will frequently rise together, then a portfolio constructed by including these two assets should be expected to result in a somewhat higher return than simply combining their individual expected return.
Apart from manipulating the portfolio X  X  expected return, our proposed co-movement model also alters the computation of the portfolio X  X  expected risk out of the traditional perspective. Traditionally, the portfolio X  X  expected risk is defined as the variance the portfolio X  X  expected return . 1 While we agreed that there is a strong relationship between the expected risk and the expected return of a portfolio, we claim that expected risk may not necessarily be tied with the variance of the expected return.

In fact, there are ample evidences that most investors characterize risk as failure to meet some target rate of return rather than variance[12]. Instead, the major essence of risk should be the magnitude of loss, the chance of loss and the exposure to loss [9,8]. Accordingly, in this paper, we try to quantif y the expected risk of a portfolio as hazard of loss against a specified desired level of return rather than the distribution of all possible outcomes (i.e. variance) against a specified desired level of return.

Specifically, our co-movement model quantifies the expected risk by mining the co-movement patterns among the time series of the assets in the portfolio. Again, given two financial assets, A and B , where we know that whenever the price of A drops, the price of B will drop, and vice versa. One should agree that it is not that appropriate to construct a portfolio by including both A and B concurrently, as the exposure of loss will be increased (the price of one asset drops, the other must also drop). Yet, such kind of relationship cannot be captured by the co-variance between A and B . Details would be discussed in the later sections.

Up to our knowledge, none of the existing literatures proposed the direction of com-puting the portfolio X  X  expected risk and its expected return by using the formulation discussed in this paper. We are the first mover in this area. In order to evaluate the ap-propriateness and the suitability of the pr oposed co-movement model, we have archived four years X  financial data from the Morgan Stanley Capital International(MSCI) index. It consists of daily equity indices of seven countries. We compared our co-movement model with the standard mean-variance model. The favorable experimental results in-dicated that the co-movement model is highly effective and feasible. A
B
The rest of this paper is organized as follows  X  Section 1 presents the proposed co-movement model in details. Section 2 evaluates the effectiveness of our proposed work and reports our findings. Section 3 addresses the major works that are directly related to the problem in this paper in brief. Section 4 summarizes this paper and discusses its possible extensions. We will present our proposed work in this section. Section 1.1 will define four differ-ent kinds of co-movements between two tim e series, whereas Sec tion 1.2 and Section 1.3 will respectively present how the exp ected return and expected risk are computed based on the idea of co-movements. Finally, Section 1.4 will show how a portfolio will eventually be constructed by applying these novel measures and concepts. 1.1 Four Different Types of Co-movements In order to account for different types of co-movements, let us first refer to figure 1. In Figure 1, it shows two time series: i and j . For time series i , there are totally five segments, whereas for time series j , there are totally three segments. With respect to time series i , four different types of co-movements can be identified: (1) up-up; (2) up-down; (3) down-up; and (4) down-down. Let us explain this in the next paragraph.
In figure 1, the co-movement between i and j forms a total number of seven co-movement patterns (1) Two up-up co-movements, from t 0 to t 1 and t 6 to t 7 ;(2)Two down-up co-movements, from t 1 to t 2 and t 5 to t 6 ; (3) Two down-down co-movements, which has the longest cumulative duration, from t 2 to t 3 and t 4 to t 5 ; and (4) one up-down co-movements, from t 3 to t 4 . 1.2 Co-movement Based Expected Return As discussed previously, the existing literatures compute the portfolio X  X  expected return when they will ignore the dependence relationships among assets in the portfolio. Its computation is based on a linear combination of the multiplication between the expected return of each individual asset a nd its contribution in the portfolio. In this paper, we try to incorporate the dependence relationships among assets by using the concept of co-movement, which has been defined in the previous section. Given two financial assets, i and j , conceptually, our co-movement ba sed expected return is as follows: 1. If whenever the price of i rises(drops), the price of j will rise(drop) (i.e. a lot of up-2. If whenever the price of i rises(drops), the price of j will drop(rise) (i.e. a lot of
As a result, there are four different kinds of returns, which in-turn depends on the four different kinds of co-movements. Intuitively, the overall expected return of a portfolio should be the summation of these four kinds of returns. In the followings, we will provide the mathematical details for this formulation step by step.

In the most simplest case, let us consid er there are only two financial assets, i and j , in the portfolio. The expected return of the portfolio is therefore: where s denotes different types of co-movements (up-up, up-down, down-up and down-down); r s ij denotes the expected return between asset i and asset j when their type of co-movement is s ; P s ij is the probability that the type of co-movement between asset i and asset j is s . Hence, Eq. (1) computes the portf olio X  X  expected return by summing the expected return of each type of co-movement with its probability of occurrence. The probability, P s ij , in Eq.(1) is computed as follows: where T s ij denotes the accumulative length of w hich the type of co-movement between asset i and asset j is s and T is the total length of the time series. Let us illustrate the idea of Eq. (2) with the help of Figure 1. In the figure, there are two periods for the up-up co-movement between asset A and asset B : from t 0 to t 1 and from t 6 to t 7 . Assume further that the differences between t 0 and t 1 is 10, between t 6 and t 7 is 12 and between t
The expected return, r s ij , in Eq.(1) is computed as follows: where w i and w j are respectively the weights of asset i and asset j in the portfolio; N ij denotes the number of segments of type s in the time series; k is a segment be-longs to the co-movement of type s ; r k i and r k j are respectively the expected return of asset i and asset j in segment k . Again, let us use Figure 1 to illustrate the idea of this equation. Consider for the case where s denotes for the up-up co-movement. In this situation, there are two segments, from time t 0 to t 1 and from time t 6 to time t 7 , belong to s . Assume further that the expected return of asset A in the two segments r
AB =( 1 / 2 )(( w A
Now, we extend this simple portfolio with two assets only into a portfolio with n multiple assets. In the multiple assets situation, we can obtain the expected return of the portfolio, r  X  , by a pairwise linear summation of every two assets: where r ij is preciously defined in Eq. (1). In order for efficient computation by using linear programming[15], Eq. (4) can be formulized in this form: where the component 1 / ( n  X  1 ) in Eq. (6) is added for the reason of normalization and r is the return generated from asset i in its co-movement with all the other assets. 1.3 Co-movement Based Expected Risk In this section, we will present how we quantify and derive the expected risk of the portfolio X  X  expected risk is usually defined as the variance of the portfolio X  X  expected return (a statistical perspective), in this pap er, we define the portfolio X  X  expected risk as the chance of exposure to loss , by considering the co-movement among the time series of the assets in the portfolio (a data mining perspective).

In the beginning, let us use Figure 1 again to illustrate our idea. For simplicity, as-sume we only have two choices for formulating the portfolio: either buy asset i solely (i.e. w i = 1and w j = 0) or buy equal shares of asset i and asset j (i.e. w i = 0 . 5and w j = 0 . 5). Let option) and risk associated with buying asset i and asset j simultaneously (the second option), respectively. Suppose we have identified  X  1 . Now, if we switch to the second option (buy asset i and asset j simultaneously), would the chance of exposure to loss be increased or decreased? That is, would  X  2 &gt;  X  1 or  X  2  X   X  1 ?
In Figure 1, from t 2 to t 3 , the type of co-movement between asset i and asset j belongs to down-down. If this type of co-movement occurs frequently in the entire time series, we might have to expect the price drops of asset i would usually accompany with the price drops of asset j .Inotherwords, the chance of exposure to loss is increased .Soif two assets frequently involved in the down-down co-movement, they should generate a higher level of risk with respect to having either of themselves alone. To conclude, if the down-down co-movement frequently appears among the two time-series, then  X  2 &gt;  X  1 .
On the other hand, from t 3 to t 4 in Figure 1, the type of co-movement with respect to asset i is up-down. This means that when the price of asset i goes up, the price of asset j will goes down. From asset i  X  X  stand point of view, the chance of exposure to loss is also increased in this situation. Thus, the more frequently this situation happens, the higher the risk should be expected. Yet, it is worth noting that this type of risk is not symmetric. From asset j  X  X  angle, the chance of exposure to loss is decreased . This is because when the price of asset j decreases, the price of asset i will increase. This compensate effect will surely reduce the loss from asset j  X  X  perspective. As a result, the expected risk of the portfolio should take the considerations of both sides. Therefore, in our co-movement model, the underlying idea is to consider the frequency of this up-down co-movement as well as the relative magnitude of their movements. The mathematical details of how the expected risk should be computed will be discussed in the later paragraphs. Finally, the co-movements with respect to asset i for the remaining time periods in Figure 1 are either up-up co-movement or down-up co-movement. Both of these co-movements imply the chance of exposure to gain increase (a kind of up-side risk). Hence, these two types of co-movements do not need to be included in computing the portfolio X  X  expected risk, as our definition of risk is the chance of exposure to loss (a kind of down-side risk).

To summarize, following these discussions, the expected risk of a portfolio is com-puted by having a pairwise comparison of the assets in the portfolio against the fre-quencies and magnitudes of this down-down co-movement or up-down co-movement. Mathematically, the risk of the portfolio,  X   X  , is computed as follows: where  X  ij is the risk of buying asset j with respect to asset i . Note that  X  ij =  X  ji accord-ing to the previous discussion about the asymmetric relationship of risk. Note that the summation in Eq.(7) includes  X  ii , which can be interpreted as risk of buying the asset i itself.

Now, the only remaining question here is how to quantify  X  ij . Unfortunately, it is not a trivial task. To account for it, let us refer to Figure 2 which shows the down-down and up-down co-movements with different kinds of slopes. In the figure, the first row ( A , B , C and D ) compares four different cases of down-down co-movements, whereas the second row ( E , F , G and H ) compares four different cases of up-down co-movements.
In case A , both time series, S 1and S 2, go down straightly, whereas in case B, only the time series S 2 goes down straightly. Comparing case A and case B , one should agree that the risk (chance of exposure loss) associated with case B is higher than that of case A , because case B outlines a small drop of S 1 will lead to big drop of S 2. Similarly, in case C ,abigdropof S 1 will only lead to a small drop of S 2, whereas in case D , a small drop in S 1 will immediately accompany with the same amount of drop of S 1. Accordingly, the risk (chance of exposure loss) associated with case C should be less because the relative magnitude of dropping in case C is less. Lastly, it is obvious that the risk associated with case D is less than that of case A , meanwhile case C is associated with less risk than case B . To summarize, the impact of  X  ij in down-down correlation for these four cases should be: B &gt; A &gt; D &gt; C .

By using an analysis similar to the above discussion, the impact of  X  ij in the up-down co-movement for case E , case F , case G and case H should be: E &gt; F &gt; G &gt; H .To conclude, in order to quantify the risk, we need to consider the relative magnitude of the slopes in each segment within the en tire time series. Eventually,  X  ij (the risk of buying asset j with respect to asset i ) is formulated as follows: where s belongs to either down-down or up-down co-movements.  X  k i and  X  k j are respec-tively the slope of asset i and asset j in segment k . Hence, the second component  X  k j represents the magnitude of slope of asset j , the third component  X  k j /  X  k i accounts for the relative slope difference between asset i and asset j . c k i and c k j are respectively the coefficient of determination[13] for asset i and asset j in segment k , which denote how confident is the co-movement relationship of that segment for the two time series. P k i is the probability of segment k appears in asset i .  X  i , k is defined as: which identifies the type of co-movements (up-down or down-down) between i and j . 1.4 Co-movement Based Portfolio Construction According to the discussions of the previous sections about how the portfolio X  X  expected return and the portfolio X  X  expected risk ar e derived, we now present how a portfolio could be constructed in our proposed co-movement model in this section.

Underlying for all of the portfolio management theories is that we should choose a portfolio that provides an expected return equals to a desired level of return, R , mean-while it accumulates the lowest level of expected risk. Mathematically, we are trying to solve the following optimization problem by identifying all w i basedonagiven R : where  X  i , w i  X  0 and their summation equals to 1. The r i in Eq. (11) is defined in Eq. (6). This formulation is different from the traditional mean-variance model. In this new model, our target function (Eq. (10)) is a linear function but not a quadratic function. Finally, if short selling is allowed, then constraint w i  X  0 can be omitted. In this section, we evaluate the effectiveness of our proposed co-movement model by using a real life dataset, the Morgan Stan ley Capital International G7 index (MSCI-G7). 2 This dataset, MSCI-G7, consists of equity indices of seven countries (Canada, France, Germany, Italy, Japan, United Kingdom and United States) that are recorded in every working day (5 working days per week). The period that we have archived is from 2003/1/3 to 2007/4/30 (around 4.3 years). Two experiments are conducted to compare and evaluate our proposed co-movement model: 1. Different Portfolio Models Comparison. We compare our proposed co-movement 2. Sensitivity of Segmentation. As discussed in the previous section, since all finan-2.1 Different Portfolio Models Comparison In this section, we compare the performance of different portfolio models with our proposed co-movement model. Here is a list of models that we have implemented: 1. COM: This is the proposed model that we have discussed in Section 1 in this paper. 2. MVM: This is the traditional mean-variance model [11] for portfolio construction. 3. COM-Mean: This model is similar to our proposed co-movement model, except 4. COM-Variance: This model is similar to our proposed co-movement model, ex-where Var ( r ij ) is the variance of the expected return.

In order to evaluate these four alternative allocation strategies in terms of their ex-pected returns and risks, we evaluate their d ifference on realized portfolio performance. We use the first three years X  data (784 days) of MSCI-G7 as training data, so as to com-pute the optimal weights, w i , for each asset, i , in the portfolio. The rest of the data, 15 months, are served as the testing data for evaluation. The portfolio will be re-constructed each month in these 15 months, so as to capture the latest movements of the time series. The oldest month would be discarded whenever for the re-construction, so as to denote for ignoring the outdated information. The average segment number is set to 90 in this experiment. The sensitivity of number of segments will be discussed in the next section. The desired level of return, R , in Eq. (11) is set as the mean return of their corresponding efficient frontiers [14].

Figure 3 shows the returns that we get from each model in each month, as well as the cumulative return for the four approaches over the 15 months in the bottom right corner in the box. Although our proposed co-movement model, COM, is not a sole winner for all of the months, the cumulative return of COM does outperform the other approaches significantly. The highest cumulative return that we get is came from COM, with a profit of 33.88%, which is 11% higher than that of MVM (the traditional Mean-Variance model, the benchmark approach), which obtains only 24.5%.

Furthermore, it is worth noting that all the approaches that are related to co-movements (COM, COM-Mean and COM-Variance) outperform the MVM (the bench-mark approach). It demonstrates clearly that the importance of identifying different types of co-movements is justifiable.

By comparing between COM and COM-Variance, one can justify the possibility of regarding risk as the exposure of loss rather than the traditional point of view  X  vari-ance of return . Although COM-Variance performs better than COM in a few months, it performs inferior than COM most of the time. Similarly, COM-Mean performs in-ferior than COM, which demonstrates the importance of considering the dependency relationship when computing the portfolio X  X  expected return. 2.2 Sensitivity of Segmentation Since our proposed co-movement model requires data pre-processing by using bottom-up segmentation and regression analysis for identifying the trends of the time series, a question commonly asked would be: how sensitivity is the co-movement model with the number of segments? As a result, we try to evaluate how the average number of segments will affect the p erformance of our approach in this experiment.

The settings of this experiment is the same as the previous one (Section 2.1), except that we alter the number segments from 10 to 300, and then we record down the cumu-lative returns. Figure 4 shows the empirical result of this experiment. We address some interesting findings below. 1. The cumulative returns for all kinds of segments are always positive. This is a 2. The overall shape of the graph is concave. This suggests that the cumulative re-3. When the number of segment is too few (less than 50) or too many (more than 300) Most of the modern portfolio theory is motivated by the mean-variance model[12], which is also know as Markowitz model. It uses the minimum variance strategy to select the portfolio which can be summarized as below: where  X  ij is the covariance of the asset i and the asset j , w i is the weight of asset i in this portfolio, R is the desired rate of return, and r i is the expected return of asset i . Hence, if r and  X  ij are identified and R is specified, we can identify the weight, w i , of each asset in this portfolio. Intuitively, given a specified return, a portfolio with the minimum risk can always be formulated. It is obvious that the covariance of two asset i and j is taken as the measure of risk for the allocation of the two assets.

Yet, this mean-variance model perceives the expected risk is a combination of and is equal for both the down-side (the possibility of earning less than the desired level of return) and up-side (the possibility of earning more than the desired level of return). Obviously, this cannot mimic the real-life situations in the financial markets, where the investors always concern with the down-side risk only rather than the up-side risk. Accordingly, many different theories are proposed which are targeting for modeling the expected risk as down-side risk only [5,10,9,8]. Nevertheless, their concepts for modeling the down-side risk are the same  X  the variance of the portfolio X  X  expected return below a specified desired level of return.

In respect to downside risk, even the pioneering work by Markowitz acknowledged the relevance of risk associated with failu re to achieve a target return. Harlow and Rao [5] discussed asset allocation to minimize portfolio downside risk for any given level of expected return. The downside risk approach is more attractive than traditional mean-variance approach because of its consistenc y with the observation that investors are averse to downside results but not upside variability[3]. In this regards, Harvey et.al[2] indicated that the usual measure of correla tion represents average co-movement in both up and down markets. Separate correlation e stimates in different return environments would permit detection of whether correlatio n increases or decreases in down markets. Increased correlation in down markets reduces the benefit of portfolio diversification. However, different from the definition in our model, they define the down(up) market as the time with return below(upon) average return and compute the correlation in the same way as semi-variance measure[10].

In our preprocess for time series, we explore segmentation algorithm to discretize the time series[7,4]. Keogh [7] gives a good survey of segmentation algorithm on time series where the the bottom-up algorithm can achieve best performance on the selected financial dataset. Among the work of discovering correlation in time series, StatStream [16] also monitor thousands of time series data but focus on finding largest correlation of them. In this paper, we describe a co-movement m odel for constructing financial portfolio by analyzing and mining the co-movement patterns among multiple time series. Differ-ent from traditional statistical approach in fin ancial world, which takes the co-variance among the portfolio as risk and the summation of individual expect return as portfo-lio return, our approach models the risk from the co-movement patterns and computes portfolio returns by considering all dependency relationships among assets. As the first step of this area, the promising experiment results on real financial data show the new formulation of our return and risk can bring g reat benefits for effective asset allocation. Acknowledgment. This work was supported by a grant of RGC, Hong Kong SAR, China (No. 418206).

