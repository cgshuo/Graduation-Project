 We present an assessment platform for gathering online rel-evance judgments for mobile push notifications that will be deployed in the newly-created TREC 2016 Real-Time Sum-marization (RTS) track. There is emerging interest in build-ing systems that filter social media streams such as tweets to identify interesting and novel content in real time, putatively for delivery to users X  mobile phones. In our evaluation de-sign, all participants subscribe to the Twitter streaming API to identify relevant tweets with respect to a set of interest profiles. As the systems generate results, they are pushed in real time to our evaluation broker via a REST API. The bro-ker then  X  X outes X  the tweets to assessors who have installed a custom app on their mobile phones. We detail the design of this platform and discuss a number of challenges that need to be tackled in this type of  X  X iving Labs X  setup. It is our goal that such an evaluation design will mitigate any issues that have arisen in traditional batch-style evaluations of this type of task.
There is emerging interest in building push notification systems that filter social media streams, such as Twitter, to deliver relevant content to users X  mobile phones. For exam-ple, the user might be a political news junkie interested in polls for the 2016 U.S. presidential elections and wishes to be notified whenever new results are posted on Twitter. She might also be interested in commentary by political pundits and reactions by the candidates. Such notifications must rel-evant (i.e., on topic), timely (i.e., the user desires poll results as soon as they are available), and novel (i.e., the user does not want tweets from multiple sources that cite the same poll). Techniques to address such information needs are be-coming increasingly important as mobile devices continue to gain prominence for information access.

The TREC Microblog track in 2015 operationalized the push notification task in the so-called  X  X cenario A X  variant of the real-time filtering task [3]. Over the official evaluation Figure 1: A high-level overview of the real-time push notifi-cation assessment platform. relevant for the purposes of this work, which focuses on the assessment platform. The most substantial change from the previous evaluation is that systems now must submit their results in real time. This live submission is accomplished via a REST API and evaluation infrastructure that we have developed. The entire platform, including both server code and the mobile assessment app, is available open source. 1
The high-level architecture of our assessment platform is shown in Figure 1. Our general approach builds on growing interest in so-called Living Labs [6] and related Evaluation-as-a-Service (EaaS) [2] approaches that attempt to better align evaluation methodologies with user task models and real-world constraints to increase the fidelity of research ex-periments. In our architecture, participating systems all subscribe to the Twitter streaming API (a sample stream is freely available to all registered users) to identify rele-vant tweets with respect to interest profiles. Since tweets are being posted in real time, the evaluation organizers do not distribute any data ahead of time X  X hey listen to the stream just like all participants to gather an archival copy of the tweets. A pilot study in 2015 [4] confirmed that multi-ple geographically-distributed listeners to the public Twitter sample stream receive effectively the same tweets (Jaccard overlap of 0.999 across six independent crawls over a three day sample in March 2015). As the participating systems identify relevant tweets, they are pushed in real time to the evaluation broker, which then routes the tweets to assessors who have installed a custom app on their mobile phones. We intend to recruit students to serve as the assessors.
This setup has a number of distinct advantages:  X  Gathering relevance judgments in an online fashion has  X  An online evaluation platform allows for the possibility of  X  An online evaluation platform opens the door to provid- X  Tweet interleaving : It is unlikely that we will be able
The evaluation broker (see Figure 1) serves as an interme-diary between systems participating in the evaluation and the mobile assessors. The main role of the broker is to dis-tribute interest profiles to participating systems, record sys-tem submissions, route submitted results to mobile asses-sors, and record judgments rendered by the assessors. The broker is implemented in Node.js and backed by a MySQL database for persistent storage of result submissions and assessor judgments. Broker functionalities are imple-mented via different REST API endpoints. For example, systems submit a tweet for a particular interest profile using the following call:
POST /tweet/:topid/:tweetid/:clientid where :topid specifies the topic (interest profile) identifier, :tweetid specifies the unique tweet identifier of the post, and :clientid specifies the client X  X  unique identifier. The broker returns a 204 status code on success. It is expected that all participating systems will properly interface with the appropriate API endpoint during the evaluation period.
Currently, we have designed the broker to rate-limit the number of submissions by a system to ten tweets per topic per day, following the TREC 2015 Microblog track protocol. The broker further employs some common sense safeguards, e.g., to not bombard any individual assessor with an undue number of push notifications. The actual routing policy of how tweets are assigned to assessors is still currently under development, although we have already implemented a basic round robin approach. For TREC 2016, we plan to recruit students from the University of Waterloo to serve as mobile assessors in a user study centered around the Real-Time Summarization task. As discussed, assessors will receive tweets as they are iden-tified by participating systems in real time, on their mobile phones as push notifications through our custom app. A screenshot of the current app is shown in Figure 2. We en-vision the experimental study to proceed as follows:  X  Assessors will be given a brief description of the task  X  Assessors will log in to the app to indicate that they  X  Assessors will receive new tweets to judge as they be-the mechanism also allows us to offload tweet rendering (in-cluding complex multimedia content) to the device. Using OEmbed, we can render a tweet natively with minimal ef-fort yet still provide a user experience similar to that of the official Twitter client.
Although the main focus of this demonstration is the mo-bile assessment platform itself, we also present a baseline implementation to facilitate system development and partic-ipation in the track. Our baseline system, called YoGosling, is a modified and extended version of the system that gener-ated the best performing automatic run in TREC 2015 [7]. Following TREC, we performed error analysis and ablation studies to distill the original system down to the components that contributed most to overall effectiveness [8]. This sys-tem is built on the Anserini project, which is the University of Waterloo X  X  Lucene-based search framework. The system is able to incrementally index the Twitter sample stream and provide real-time search capabilities.

YoGosling converts the title field of interest profiles into queries for searching Lucene and applies a simple relevance scoring method to rank tweets. After relevance scoring, du-plicate and near-duplicate tweets are identified by a novelty detection component (based on simple Jaccard similarity), so that users are not given repetitiously annoying informa-tion. One of the most important lessons learned in building YoGosling is the proper setting of score thresholds and ignor-ing tweets that fall below the thresholds. Our simple scoring model is amenable to a global threshold that yields reason-able effectiveness, thus obviating the need for per-topic tun-ing (difficult due to the paucity of training data). Another interesting feature of YoGosling is a simple relevance feed-back mechanism whereby users assess tweets once a day, and these judgments are used to set the score threshold for the next day. We experimentally show that this technique can yield substantial gains in effectiveness [8].
One of the main enhancements we are planning to add is the ability for assessors to supply their own interest profiles directly through an API, so that they can actually receive tweets of personal interest. In TREC 2016, we plan to de-velop interest profiles manually based on assessor input, but the profiles will be vetted by the track organizers before dis-tribution to participating systems. An automatic API for interest profile submission creates several non-trivial prob-lems, including near-duplicate detection, limiting the release of personal information, and topic termination (since some topics may only be important for a limited time). These issues are exacerbated if there is the expectation that be-havioral traces will be released as part of a training set (as is the case with TREC evaluations), and ethics issues similar to the public release of web query logs come into focus.
At present, we have a few ideas of how to assign interest profiles to assessors, but lack concrete empirical evidence as to what strategy will actually  X  X ork X . As discussed, fore-seeable issues include inter-assessor consistency, assessor la-tency (how quickly they respond to push notifications), and assessment volume (how many judgments they ultimately provide). Related, the strategy used for interleaving tweets has the potential to affect the judgments rendered, and in
