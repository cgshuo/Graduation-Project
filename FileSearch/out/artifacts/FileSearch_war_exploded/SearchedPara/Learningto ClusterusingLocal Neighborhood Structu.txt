 Romer Rosales + romer@csail.mit.edu Kannan Achan kannan@psi.tor onto.edu Brendan Frey frey@psi.tor onto.edu A fundamen tal problem in data analysis is that of clustering/classi cation with partly or only unlab eled data. This is kno wn as semi-sup ervised or unsup er-vised classi cation resp ectiv ely. Here, we will refer to clustering or classi c ation as the problem of assigning a class lab el to (unlab eled) points in a dataset. Some well studied type of approac hes for this task consider (sometimes implicitly) the information pro vided by the underlying glob al structure of the dataset.
 In cases where there exists a relev ant distance measure asso ciated to the data coordinate system, pairwise dis-tances could de ne a global function of the data (favor-ing certain prop erties, suc h as high intra-cluster or low inter-cluster similarit y), that could be (appro ximately) optimized over all class assignmen ts. In the case of un-sup ervised classi cation, a well kno wn approac h that uses this idea is spectral clustering ( e.g., (Shi &amp; Malik, 2000; Ng et al., 2002)). Further, if the data is kno wn to lie in a submanifold of the space, the structure of this submanifold migh t be critical for classi cation since it may pro vide a sometimes more meaningful way to compute pairwise similarities for unsup ervised or semi-sup ervised classi cation(Szummer &amp; Jaakk ola, 2002; Belkin &amp; Niy ogi, 2004). These concepts are not lim-ited to clustering and have also been used as the basis for dimensionalit y reduction ((Ro weis &amp; Saul, 2000; Tenen baum et al., 2000)). One way to formalize the role played by the global data distribution was recen tly prop osed in the con text of semi-sup ervised learning by introducing Information Regularization (Cordunean u &amp; Jaakk ola, 2003).
 The above pro vides a rather general viewp oint, but will serv e us to better illustrate the ideas introduced in this pap er. The assumption in the above approac hes was that changes in the class lab els should occur in ar-eas of low data densit y as opp osed to areas of high data densit y, where class lab els should remain the same. In this pap er, we pursue a somewhat di eren t notion. For some problems, the higher-order prop erties of the local data structure, rather than just its high/lo w densit y, could be the basis to correct classi cation. Under this concept, changes in class lab els should be asso ciated to changes in the local structure (also referred as neigh-borho od structur e or local topology ) of the dataset. Global, unsup ervised clustering algorithms usually su er from the problem that a distance measure or class dep enden t pairwise likeliho od is set in an ad-ho c way ( e.g., (Shi &amp; Malik, 2000; Ng et al., 2002; Kannan et al., 2000)). Moreo ver, in man y cases, it is set in-dep enden tly of the preferred clustering prop erties for the problem at hand. Semi-sup ervised classi cation pro vides a way around this problem by incorp orating some lab eled examples. In semi-sup ervised classi ca-tion, the role of the unlab eled examples is generally to unco ver, someho w, the structure of the data ( e.g., a low-dimensional manifold) (Szummer &amp; Jaakk ola, 2002; Belkin &amp; Niy ogi, 2004). In (Meila &amp; Shi, 2001; Szummer &amp; Jaakk ola, 2002) the data points were as-sociated to a Mark ov random walk whose transition matrix was computed locally ; as a consequence it im-plicitly caused the unlab eled data densit y to play a ma jor role in de ning the pairwise similarit y between points. The underlying assumption in these metho ds is that areas of low data densit y (rather than areas of high data densit y) should propitiate changes in clus-ter/class lab el. However, this is just one possible as-sumption on the join t distribution of class lab els and data points.
 In some clustering problems, what determines the point classes migh t be more related to the high-order (local) structure of the data. To illustrate this, let us use the simple example in Fig. 1(a). In general, most metho ds would pro duce a rather objectionable result for this example. In the case of an unsup ervised clustering task, we migh t be motiv ated to cluster the data into a path of high densit y and a cloud formed by sparser noise points (Rosales &amp; Frey, 2003). Of course, this migh t be just one sub jectiv e answ er that man y could agree upon in the absence of more informa-tion. However, it is useful at illustrating two ideas: (1) unsup ervised clustering is in general ill-de ned if we do not restrict the criteria used to characterize a good clustering and (2) the local data structure migh t be more relev ant in order to distinguish clusters in some type of problems; as a consequence, di eren t measures could be asso ciated with eac h class. In this pap er we focus on both of these ideas.
 A reasonable approac h to better de ne what we mean by clustering is to learn to cluster from examples (pre-viously lab eled data), suc h as that sho wn in Fig. 1(b). Di eren t versions of this idea have been tried (Bac h &amp; Jordan, 2004; Xing et al., 2003), and it is in general a dicult problem. Successful clustering paradigms often involve a complicated function of the distance measure and the lab els. Popular clustering approac hes suc h as spectral clustering su er from the fact that the problem of learning the distance measure from data, recen tly prop osed in (Bac h &amp; Jordan, 2004), does not have a straigh tforw ard solution. Thus, appro ximations are needed even for the parameterized measure used in that approac h. By using the point of view of Mark ov random walks, (Meila &amp; Shi, 2001) also pro vided a rather elegan t approac h for learning the measure. In other con texts, suc h as the con vex problem form ulated by (Xing et al., 2003), learning distance measures is a less demanding task computationally . However, it all app ears as if for computational tractabilit y, the form of the measure and the type of objectiv e function are rather limited. Our form ulation pro vides a sim-ple and natural way to learn about clusters from pre-lab eled data and pro duce results suc h as that sho wn in Fig. 1(c). In addition, none of these approac hes at-tempts to learn or use the concept of class dep enden t distance measures introduced here.
 In order to formalize the above and introduce our metho d for learning to cluster by using the local neigh-borho od structure (LC-LNS), rst we de ne a genera-tive mo del of data clustering that uses the local data structure to de ne what a cluster should be. We then explain how we could easily learn from previously la-beled data using this framew ork and how to infer the posterior probabilit y distribution over class lab els for eac h data point. Finally we exp erimen tally evaluate the metho ds and concepts introduced here by means of two di eren t application domains. Let Z = f z 1 ;:::; z N g be a set of observ ed data points whic h we would like to classify or cluster into classes or groups. Eac h point z i is exp ected to belong to one of M classes. We will emplo y the random variable c i tak-ing values in the discrete space C = f 1 ;:::;M g to rep-resen t the class asso ciated to data point z i . Our goal will be to infer the class lab els C = ( c 1 ;:::;c N ) 2 C N of all points in Z . We will consider local neigh bor-hoods (groups of points) indexed by some set G . Eac h neigh borho od 2 G will con tain a subset of points from Z whose indices are given by the set , with j the j -th elemen t of the neigh borho od set. Denote the sup er set of all neigh borho ods = f j 2 G g . Asso ciated to eac h local neigh borho od is a random variable y that will describ e the neigh borho od; we let y = f y j 2 G g . In other words, y = f ( f z i g i 2 ) for some function f . Our approac h mak es use of local neigh borho od descriptions f , whic h in practice could be of div erse nature, and thus the domain of y could vary . For example, y could enco de simple binary re-lationships between data points in the neigh borho od ( i.e., f ( : ) = f f ij g for all pairs ( i;j ) of elemen ts in any given neigh borho od).
 Since we consider the problem of classi cation, a neigh borho od will be assigned the random variable x to describ e its class. A particular asp ect of this approac h is the use of local high order descriptions of the data. In the extreme case, one neigh borho od could be poten tially formed by elemen ts from the M di er-ent classes. Giv en our de nition of C , in this extreme case the domain of x would be the Cartesian pro duct space C K , where K is the neigh borho od size. However, it is sensible to assume that most ( e.g., more than half ) elemen ts in a local neigh borho od belong to a single common class, we will use K in &lt; K to denote this ( in-class ) num ber. This is a reasonable assumption since it is equiv alen t to exp ecting that when a local neigh bor-hood is chosen at random, most of the elemen ts will belong to one particular (but unkno wn) class. Still, even under this represen tation, the num ber of ways to form a neigh borho od by assigning a class to eac h point could be large. In this pap er, we consider a local neigh borho od represen tation where K in points belong to a common class, and the rest ( K out = K K in ) may be out of this common class.
 Let S denote the domain of x . Using the represen-tation above, S will have cardinalit y M K K is, from K elemen ts, choose any K out elemen ts to be considered not part of the common class. We will call this common class, the neigh borho od class, with the caveat that sometimes not all the elemen ts in a neigh-borho od belong to this class. It is natural to de ne x = ( ` ;s ), with ` 2 C represen ting the neigh bor-hood class and s enco ding the choice of in-class and out-of-class elemen ts. We will use s = ( s 1 ;:::;s K ), with s i 2 f 0 ; 1 g denoting if its i th elemen t is in the neigh borho od class (1) or not (0). Finally , denote ( s ) = f j j j = j and s the set of indices of the points z i that are in-class), likewise for 0 . 2.0.1. Example Let K = 5 and K out = 2, then s = (0 ; 1 ; 1 ; 0 ; 1) is a valid state. Moreo ver, it denotes that as far as the ran-dom variable x is concerned, from the ve elemen ts in , the second, third, and fth elemen ts belong to class ` , the rst and fourth ones may not belong to class ` but poten tially to another class (they are wild-c ards ). In Sec. 3.1 it will be clear the role played by K and K out in the trade o between algorithmic complexit y and mo deling power.
 This represen tation allo ws us to de ne the conditional probabilit y of a neigh borho od description given its class in a simple form. In particular, we consider class-conditional mixture distributions of the form: with ! as the index for the mixture comp onen t. Of particular interest will be: with ! 2 f 1 ;:::;T g , T the desired num ber of mix-ture comp onen ts, and y ( s ) = f ( f z i g i 2 1 ( s ) ) ( f ( : ) is computed using only the neigh borho od elemen ts z i for whic h s i = 1). This is a class-conditional Gaus-sian mixture de ned on the range of f ; thus the sub-scripts ( l ;! ) are simply the class and mixture indices for the mean and covariance. So far, we have emphasized the use of attributes of local neigh borho ods to characterize local structure. However, the larger the neigh borho od the more com-plex its structure can poten tially become. At a large enough scale, computational mo deling can become in-feasible. Generally speaking, some datasets may re-quire a very complex mo del; however here we de ne a simple large scale mo del whic h is valid for our cluster-ing approac h and whose complexit y is manageable. We choose to let multiple neigh borho ods share elemen ts; we can thus enforce large scale consistency simply by noticing that the class of a given point should agree among all the neigh borho ods to whic h the point be-longs. This is a fairly general statemen t and simple to specify formally .
 We shall use a neigh borho od-based Mark ov mo del, where two neigh borho ods are probabilistically related if they share at least one elemen t (in this case we say that the neigh borho ods are proximal ). Speci cally , if x and x are two pro ximal neigh borho ods: ( x ; x ) / exp f X where P indexes common elemen t pairs, and ( : ) is the indicator function (see Fig. 2). We let (1 ; 1) = 1 and zero otherwise (as in the logical AND); thus, if two pro ximal neigh borho ods have a di eren t class, their compatibilit y decreases with the num ber of times their common elemen ts disagree (variations on this could also be of interest). Note that using this de nition, same-class neigh borho ods pairs are equally compati-ble, indep enden tly of the num ber of shared elemen ts. 2.1. Learning Neigh borho od Structure One of the adv antages of this setting is that learning to cluster from lab eled data can be easily de ned since we only need to learn class-dep enden t neigh borho od distributions 1 . Let us for the momen t assume that, besides the unlab eled dataset Z , we were given one or more lab eled datasets L i , consisting of data points and their corresp onding lab els. We can form ulate the learning problem as that of estimating class-dep enden t local neigh borho od structure. This is possible for a num ber of distributions p ( y j x ), including distributions of the form in Eq. 1( e.g., using the EM algorithm). Giv en a set of neigh borho ods i = f i 1 ;:::; ( i ) L eac h lab eled dataset i , we t the corresp onding distri-bution using a maxim um likeliho od approac h, assum-ing that neigh borho ods and datasets where dra wn in-dep enden tly at random. Using this criterion, the goal is to maximize: for eac h class c with parameters c . From Eq. 2, c = ( c! ; c! ;p ( ! j c )) for eac h ! . Eq. 4 factorizes across neigh borho ods and classes; thus, learning becomes a simple task. Note that the neigh borho od structure distributions are de ned in a K in dimensional space since ` is given ( i.e., the data is lab eled). We should remark that x de nes the state of the neigh borho od as a whole. So far no explicit con-cept has been asso ciated to the class of individual data points C = ( c 1 ;:::;c N ), since the di eren t neigh bor-hoods that a point may belong to could have di eren t classes. In order to mak e the class concept explicit, we de ne the poten tial as follo ws: this poten tial is incorp orated in the mo del only if data point i is in the neigh borho od . The full join t prob-abilit y over all the states and classes in the neigh bor-hoods is then given by: P ( y ; x ;C ) = A factor graph depicting the relationships between the de ned random variables is sho wn in Fig. 3. A reason for formalizing our mo del using a factor graph is that complexit y analysis and general algorithms for infer-ence can be easily deriv ed from it. 3.1. Complexit y Analysis and Inferring As we have seen above, x is a discrete random vari-able, de ning the state of a neigh borho od . It tak es values in the set S , with jS j = M K K If we do not allo w elemen ts without a class ( i.e., K out = 0), then the represen tation for x reduces to a more common represen tation (we can think of the neigh borho od in a similar way as we would think of a single point). Using the neigh borho od idea, this would mean that all the elemen ts in the neigh borho od are as-sumed to belong to a single class ( i.e., jS j = C ). How-ever, this assumption is too restrictiv e since the nal clustering assignmen t would be constrained to agree with the sub division (neigh borho ods) given by . An ideal approac h would be to let the neigh borho od de nitions unkno wn and try to disco ver them. This is a very complicated com binatorial problem by itself. However, our approac h is designed with this issue in mind, and is thus geared towards solving this particu-lar problem, but with some constrain ts on the neigh-borho ods allo wed.
 Speci cation of the neigh borho od is done (implicitly) using our represen tation. Since the elemen ts i in the neigh borho od for whic h s i = 0 are not mo deled by p ( y j x ), it is as if the neigh borho ods were non-static and speci ed by the s itself; x = ( s ;` ) is indeed a represen tation for neigh borho od elemen t ownership and class lab el. Thus, neigh borho ods of the type rep-resen ted by s can be accoun ted for during inference. 3.2. Algorithms for inference Here our goal is to nd a posterior distribution p ( c i j y ) for eac h point index i . This is a well studied prob-lem once the join t probabilit y distribution has been speci ed. However, for the distribution speci ed in Eq. 6, there is no kno wn exact algorithm for ecien tly computing the desired posteriors. We will resort to the sum-pro duct algorithm (Pearl, 1988; Ksc hisc hang et al., 2001), whic h is not exact in the factor graph asso ciated to Eq. 6 (due to loops introduced by the dep endencies), but has been sho wn to perform sur-prisingly well in suc h graphs ( e.g., (McEliece et al., 1998)). Also, empirically it has been found to perform well in mo dels with a large num ber of hard constrain ts. The sum-pro duct algorithm is a message passing metho d that computes local updates on the marginal posterior probabilities based on local dep endencies be-tween variables. The sum-pro duct update equations are equiv alen t to iterativ ely solving self-consisten t equations resulting from nding the zero-gradien t points of the Bethe appro ximation to the Gibbs free energy asso ciated to the join t probabilit y in Eq. 6 (Yedidia et al., 2000).
 In our particular implemen tation, we randomly choose the neigh borho ods. This initial step de nes the factor graph to be used. We use the de ned class-conditional probabilities to compute messages to eac h variable x . The posterior probabilities for x are initialized ac-cording to this likeliho od, then a parallel version of the sum-pro duct algorithm is used to update the poste-rior distributions in the subgraph asso ciated to the x variables. The marginal posterior distributions for the classes c i are then computed by marginalizing with re-spect to the states and multiplying appropriately (also equiv alen t to sum-pro duct messages). To evaluate the prop osed concepts and algorithms, we emplo yed datasets where local structure is presumably relev ant for classi cation. The application domains are visual/spatial clustering and gene function prediction. 4.1. Learning Spatial Clustering In this set of tests we used syn thetic spatial data in two dimensions. The scaling prop erties of the algo-rithms do not dep end on the dimensionalit y of the el-emen ts in the dataset; we have used two dimensions to ease visualization. We set K = 10, K out = 3, and f : &lt; K ! &lt; D to be the collection of pairwise (Eu-clidean) distances between the elemen ts in the neigh-borho od; thus D = K ( K 1) = 2 ( f pro vides a descrip-tion of the neigh borho od structure as a vector in &lt; D ). We sorted the vector comp onen ts to mak e the repre-sen tation invarian t to ordering and normalized them to achiev e local scale invariance. We chose the neigh-borho ods to be cen tered at random point locations and to be de ned by the K nearest neigh bors. The num-ber of neigh borho ods was set to 60% of the num ber of data points. For learning, the class conditional neigh-borho od distributions were set as Eq. 2 with T = 3. The algorithm does not require any further parameter setting since the rest can be learned from data. A variet y of datasets are sho wn in Figs. 4-5. The re-sults are of good-qualit y even for noticeably complex patterns considering the com binatorial nature of the task, suggesting that our criterion can be relev ant for clustering some interesting datasets. These may illus-trate poten tial use of this metho d in data visualization. 4.2. Functional Gene Classi cation In our second set of exp erimen ts our goal is to correctly classify gene function based on gene expression data. We used mouse gene expression data obtained under a series of di eren t (55) exp erimen tal conditions. The genes in this dataset were determined using GenomeS-can, generating a set of 41K putativ e gene sequences whic h were used to pro duce DNA microarra ys con tain-ing 60-mer oligon ucleotides (see (Zhang et al., 2004)). We used Gene Ontology Biological Pro cess (GO-BP) annotations (Ash burner et al., 2000), whic h de ned a gene classi cation criterion (related to gene function). We set K = 8; K out , f , T , and the num ber of neigh-borho ods were as in the previous exp erimen t. Our reason to believ e that the classi cation concepts here presen ted are at all useful in this scenario is re-lated to the possibilit y that gene function could be pre-dicted by the pattern of gene expression in whic h they are involved and that this pattern migh t be shared by same-function genes. Thus, di eren t classes migh t (presumably) be distinguished by their collectiv e pat-terns of gene expression.
 We considered 99 GO-BP categories, those for whic h the num ber of lab eled genes was at least 80 (for sta-tistical signi cance) and those whic h did not con tain a high num ber of lab eled genes (because those are too broad in function). We randomly partitioned eac h cat-egory into two sets of genes, 80% for training and the rest for testing, and built binary classi ers using the metho d describ ed in this pap er. The error was mea-sured using the mean absolute di erence between the inferred posterior probabilit y for the class lab el found by our metho d and the (1/0) probabilit y deriv ed from ground-truth. This exp erimen t was rep eated a num ber of times (10). Since we were faced with the question regarding when a gene function should be predicte d , we set our algorithms to mak e decisions about the class of at least a prop ortion of the (unlab eled) genes ob-serv ed. This was done by choosing to classify the genes with highest con dence (posterior probabilit y). Figs. 6(a)-(b) sho w classi cation results for our metho d (LC-LNS) and a K-nearest neigh bor classi er (KNN) resp ectiv ely. In addition to LC-LNS being su-perior overall, the metho ds had very distinct perfor-mance prop erties. LC-LNS performed really well in man y classes (classes whose genes could be unco vered by considering their collectiv e expression pattern) and really badly in few classes (presumably those whic h could not), whereas KNN performance was relativ ely even across classes but not excellen t for any of them. Fig. 6(c) summarizes the overall performance across exp erimen ts and values, sho wing a lower standard deviation on the prediction error overall for LC-LNS. These exp erimen ts suggest that the expression pat-tern of groups of genes (rather than only the similarit y in gene expression) is imp ortan t for correct functional gene classi cation for some categories. Further, it is worth pointing out that LC-LNS is more general, since we could poten tially perform gene classi cation across species by learning the function patterns in one species to classify genes, by the same functions, in another. In this pap er, we have introduced a general classi ca-tion concept driv en by the idea that, in some prob-lems, high-order local structure of the data could be relev ant for classi cation. Under this concept, changes in class lab el are asso ciated to changes in local data structure. Man y successful unsup ervised and semi-sup ervised classi cation metho ds are (implicitly or ex-plicitly) built upon the idea that changes in class lab els should occur in areas of low data densit y. From this viewp oint, this work establishes a connection between these two type of approac hes.
 We presen ted a fully probabilistic mo del based on this classi cation idea and deriv ed a simple and natural criterion for learning to cluster, a problem that has been recen tly studied in other mo dels. The prop osed metho d also gave us the freedom to accoun t for class dep enden t cluster prop erties, unlik e previous metho ds whic h have used a global distance measure (not class speci c). Based on results from probabilistic inference, we used the sum-pro duct algorithm for computing the posterior distributions of class lab els.
 There are sev eral extensions and ideas that we believ e are worth pursuing. As an extension, for certain prob-lems it migh t be relev ant to learn the inter-class neigh-borho od structure rather than just the within-class structure. Another, more immediate area to concen-trate on, is the use of alternativ e poten tials . So far these poten tials only encourage same class points to be in neigh boring groups. A more interesting de nition of could be one that allo ws the preserv ation of the neigh borho od structure itself. A common problem in statistical learning is also the selection of mo del dimen-sionalit y ( e.g., K;K out ). Interesting further applica-tions of this metho d include problems related to infer-ring graphs ( e.g., graph denoising and related areas). The concepts presen ted here extrap olate almost natu-rally to graphs, since neigh borho od structure could be seen as graph connectivit y structure.
 We thank Quaid Morris for pro viding valuable remarks regarding our exp erimen ts using GO-BP categories and for making the gene data available to us. We also thank our review ers for their helpful commen ts. Ash burner, M. et al. (2000). Gene ontology: tool for the uni cation of biology . The gene ontology consor-tium. Nat. Genet. , 25 , 25{29.
 Bac h, F. R., &amp; Jordan, M. I. (2004). Learning spectral clustering. Neur al Inf. Processing Systems . Belkin, M., &amp; Niy ogi, P. (2004). Semi-sup ervised learning on Riemannian manifolds. Journal of Ma-chine Learning Research (to app ear) .
 Cordunean u, A., &amp; Jaakk ola, T. (2003). On informa-tion regularization. Unc ert. in Arti cial Intel ligenc e . Kannan, R., Vempala, S., &amp; Vetta, A. (2000). On clus-terings: good, bad and spectral. 41st Foundations of Computer Scienc e (FOCS 00) .
 Ksc hisc hang, F., Frey, B., &amp; Loeliger, H. (2001). Fac-tor graphs and the sum-pro duct algorithm. IEEE Transactions on Information The ory .
 McEliece, R., MacKa y, D., &amp; Cheng, J. (1998). Turb o deco ding as an instance of pearl's belief propagation algorithm. IEEE J. Sel. Areas in Comm. , 16 . Meila, M., &amp; Shi, J. (2001). Learning segmen tation with random walks. Neur al Inf. Processing Systems . Ng, A., Jordan, M., &amp; Weiss, Y. (2002). On spectral clustering: Analysis and an algorithm. Advanc es in Neur al Inf. Processing Systems .
 Pearl, J. (1988). Probabilistic reasoning in intel ligent systems . Morgan-Kaufman.
 Rosales, R., &amp; Frey, B. (2003). Generativ e mo dels of anit y matrices. Unc ert. in Arti cial Intel ligenc e . Roweis, S., &amp; Saul, L. (2000). Nonlinear dimensional-ity reduction by locally linear embedding. Scienc e , 290 , 2323{2326.
 Shi, J., &amp; Malik, J. (2000). Normalized cuts and im-age segmen tation. Pattern Analysis and Machine Intel ligenc e , 22 , 888{905.
 Szummer, M., &amp; Jaakk ola, T. (2002). Partially lab eled classi cation with mark ov random walks. Neur al Inf. Processing Systems .
 Tenen baum, J., Silv a, V. D., &amp; Langford, J. (2000).
A global geometric framew ork for nonlinear dimen-sionalit y reduction. Scienc e , 290 .
 Xing, E., Ng, A., Jordan, M., &amp; Russell, S. (2003).
Distance metric learning, with application to clus-tering with side-information. Neur al Inf. Processing Systems .
 Yedidia, J., Freeman, W., &amp; Weiss, Y. (2000). Gen-eralized belief propagation. Neur al Inf. Processing Systems (pp. 689{695).
 Zhang, W. et al. (2004). The functional landscap e of
