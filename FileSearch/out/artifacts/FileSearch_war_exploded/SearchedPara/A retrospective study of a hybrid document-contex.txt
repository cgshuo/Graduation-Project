 1. Introduction
Various retrieval models have been developed and investigated over the past several decades based on a vari-ety of mathematical frameworks ( Dominich, 2000 ). For example, Salton, Wong, and Yang (1975) and Wong,
Ziarko, and Wong (1985) worked on retrieval models based on vector spaces. The Binary Independence model the probability theory. The fuzzy retrieval model (e.g., Miyamoto, 1990 ) and the extended Boolean model ( Sal-view of how to retrieve documents that are sufficiently relevant that they satisfy a user X  X  information need. On the other hand, an information retrieval system can be thought of as simulating the human user when making of the documents to the user X  X  information need is in terms of preferences ( Yao &amp; Wong, 1991 ).
In this work, we simulate human relevance decision making in the development of a novel retrieval model that explicitly models a human relevance decision at each location in a document. The relevance decision at the specified location in the document is based on the context at that location so that the relevance decision using contexts in documents to explore term co-occurrence relationships for query expansion is not new, to the best of our knowledge, it is new to model the contexts/windows features explicitly in the retrieval model by incorporating the locations of terms inside a document for re-weighting the query terms. By re-weighting the query terms using the contexts of the query terms in documents, the model assigns context dependent term weights which are aggregated together as the final document similarity score.
 A document context is essentially a concordance or a keyword in context (KWIC) ( Kupiec, Pedersen, &amp;
Chen, 1995 ). Fig. 1 shows some example document contexts containing a query term in the title query,  X  X  X ub-ble Telescope Achievements X  X . The contexts were extracted from a raw (un-processed) document. During retrieval, unlike Fig. 1 , all the terms are stemmed and the stop words are removed. From Fig. 1 , it should be noted that even for a relevant document, not all contexts in the document are relevant.

Our novel model uses current successful retrieval models and techniques to estimate the relevance decision preferences (or context scores) of document contexts containing a query term in the center. The relevance deci-sion preferences are defined as the log-odds estimated using smoothing techniques and they are combined using aggregation operators. More specifically, we used the technique of smoothing ( Chen &amp; Goodman, context similar to that of the BIM model ( Robertson &amp; Sparck-Jones, 1976 ). In order to calculate the docu-ment score for ranking, the document-context log-odds are combined using different evidence aggregation operators based on the extended Boolean model ( Salton et al., 1983 ) and some fuzzy (aggregation) operators ( Dombi, 1982; Paice, 1984; Yager, 1988 ). Therefore, our proposed retrieval model is a hybrid of various past successful retrieval models and techniques.

In predictive experiments, a major source of difficulty in developing novel retrieval models is in determining whether the effectiveness performance is limited by the underlying model or by the poor parameter estimation techniques used. Instead of predictive experiments, we propose to evaluate our novel retrieval model based on retrospective experiments that are performed using relevance information (e.g., the TREC relevance judg-ments), similar to the retrospective experiments in Robertson and Sparck-Jones (1976), Sparck-Jones, Walker, and Robertson (2000) and Hiemstra and Robertson (2001) . The purpose of the retrospective experiments is to (a) evaluate the potential of the underlying novel retrieval model by observing the best effectiveness that can (b) reveal the (near) optimal performance of the model and provide a yardstick for future (predictive) exper-(c) focus on gathering crucial factors (e.g., the size of the context) affecting the performance of the model (d) show whether the model obeys the probability ranking principle ( Robertson, 1977 ); and (e) examine the relevance decision principles in Kong, Luk, Lam, Ho, and Chung (2004) and determine
The problem of estimating parameters with limited or no relevance information is left for future work since it is not known whether the proposed model is worth further investigation. When considering the terms in rel-evant documents, we discard those terms with document frequency equals to one. This avoids finding identi-fiers (e.g., document id) that uniquely identify relevant documents, thereby guaranteeing to obtain high in relevant and irrelevant documents for retrieval.

We emphasize that our document-context based model is a descriptive model in this paper even though it could become a normative model. A descriptive model describes how the decision is made while a normative model specifies how the (optimal) decision should be made. Our document-context based model is descriptive in this paper because it does not feedback any effectiveness performance information (e.g., MAP) to the system for performance optimization (e.g., query optimization ( Buckley &amp; Harman, 2003 ) or model parameter optimization). For instance, our retrieval model directly estimates the probabilities without any effectiveness feedback about the estimation being good or not for document ranking. Also, the retrieval process of our model is a one-pass re-ranking process using the proposed ranking formula (discussed in detail in Section 2 ) that describes how the relevance decision is made.

One may argue that if we know the relevance information, then the retrieval effectiveness performance must be good and it is pointless to do the experiments. However, as mentioned above, we are not finding identifiers of relevant documents (terms with document frequency equals to one are ignored). The descriptive model does not optimize the query or the model parameters using effectiveness performance results from previous runs.
Moreover, the retrieval performance is not guaranteed to be good even when we know the relevance informa-uments may also appear in the irrelevant documents. By using the relevance information, we are not mani-pulating or restricting the term distributions/occurrences in the documents but using existing probabilistic methods to estimate the term distributions/occurrences in the documents. Furthermore, we tested our model with different document collections (TREC-2, TREC-6, TREC-7, TREC-2005 and NTCIR-5) to show that the model is reliable. Finally, doing the retrospective experiments also provides us with an important clue about the potential of the retrieval model because an applicable model should perform well in the presence of rele-vance information. The use of relevance information can reveal the (near) optimal performance and the esti-mation of the relevance information is possible using various techniques such as pseudo relevance feedback.
We will not examine the time-efficiency of our retrieval model or retrieval system because: (a) it is already very challenging to design and develop a highly effective retrieval model; (b) once the effective retrieval model is developed, then we have enough information to design and develop (c) the time-efficiency problem may reduce its significance in time as computers are continually becoming We leave how to make our retrieval model more time-efficient to our future investigation.

The rest of the paper is organized as follows. Section 2 presents the details of our document-context based retrieval model. Section 3 shows the results of the model-oriented experiments which test the model extensively using one data collection, TREC-6. Section 4 shows the results of the scope-oriented experiments which test the model across different data collections and with another language. Section 5 discusses related works.
Finally, Section 6 concludes and describes the future work. 2. Document-context based retrieval model
In this section, we introduce our document-context based retrieval model that ranks documents on the basis of the contexts of the query terms in documents (i.e., document contexts). A document context is uniquely identified by the location where the query term occurs in the document. Therefore, assigning different term weights to the same query term in different contexts can be thought of as assigning different term weights to the same query term in different locations in the document. Hence, we can explicitly incorporate the (query) term locations in a document in our retrieval model as reflecting the relevance of the corresponding contexts to the query. We believe that the term distributions of the contexts are similar for query terms having the same meaning, while the term distributions of the contexts are different when the same query term refers to different meanings in different contexts in documents. By incorporating the document-context information for weight-ing the query terms, we are trying to solve the problem of polysemy (i.e., a term with multiple meanings) in natural language because the meaning of terms without contexts can be ambiguous while terms with contexts should have definite meanings.

Given that each context has a score reflecting its relevance to a particular topic, some methods or bases are needed to combine the scores in a principle manner. Kong et al. (2004) showed that different relevance decision principles (namely the Disjunctive Relevance Decision (DRD) principle, the Aggregate Relevance (AR) princi-ple and the Conjunctive Relevance Decision (CRD) principle) can be applied to passage-based retrieval in dif-ferent scenarios when simulating the human user in making relevance decisions. In this paper, we extend their work by applying the relevance decision principles to guide the selection of aggregation operators to combine the context scores of query terms in a document (e.g., Fig. 1 ), instead of passages.

Wu, Luk, Wong, and Kwok (submitted for publication) showed that incorporating term locations in doc-uments in the retrieval model can be compatible with the existing probabilistic retrieval models, thereby high-lighting assumptions made in the derivation. Our proposed document-context based retrieval model is based on the conceptual framework by Wu et al. (submitted for publication) but our model does not assert the  X  X  X oca-tion invariant decision X  X  assumption. Instead, our model allows the probability of making relevance decision at our model relaxing the location invariant decision assumption could achieve high retrieval effectiveness (i.e., about 36% mean average precision in TREC-6). In here, we further improve our model in order to investigate whether our retrieval model can be more effective than before.

The rest of this section is divided into four parts. First, we define the document context. Second, we develop the context score that reflects the relevance preference of the context to a given topic. Third, we discuss dif-ferent techniques to solve the zero probability problems. Finally, we describe various context score combina-tion methods that are consistent with different relevance decision principles ( Kong et al., 2004 ). For convenience, the symbols used in the rest of this paper are shown in Table 1 . 2.1. Context definition defined by the set of terms surrounding and including the term inside the document, i.e.
We are interested in the contexts that with a query term at the center position (i.e., d that have non-query terms at the center are considered irrelevant. This is equivalent to making the following assumption in our model:
Query-Centric Assumption : For a particular query q and a document d tion for q locates only in the contexts c ( d i , k , n ) for k 2 [1, j d locates around query terms).

The query-centric assumption states that if one can find relevant information in a document, then the relevant information must locate around the query terms inside the document. Note that the query-centric assumption does not require all contexts c ( d i , k , n ) for k 2 [1, j d j ] where d relevant information locates in the contexts c ( d i , k , n ) for k 2 [1, j d j ] where d tion may be invalid because some of the relevant documents found in the TREC and NTCIR collections for some queries do not contain any of the query terms. As long as this kind of relevant documents does not form of the time. Table 2 shows that the average proportions of relevant documents without any query terms per topic using title queries across different data collections (including the Chinese collections in NTCIR-5) are less than 13%. It appeared that the query-centric assumption is not entirely unrealistic.
 include irrelevant information or exclude relevant information respectively. This issue is addressed in our model-oriented experiments (see Section 3.1 ). 2.2. Context score of the context. The context c ( d i , k , n ) contains the set of terms { d p th location relative to d i [ k ] in the i th document, and p 2 [ n , n ]. In other words, a context c ( d terms surrounding and including d i [ k ].

In the probabilistic model ( Sparck-Jones et al., 2000 ), the basic question to ask for each document and each query is  X  X  What is the probability that this document is relevant to this query?  X  X 
Although there are actually implicit assumptions behind the above basic question, we try to extend it and use it as the starting point to develop our document-context based model. Upon defining the notion of context, we can now ask another similar question for each context in each document and each query: What is the probability that this context in this document is relevant to this query?
By query, we actually mean the topic or the user information need in the above question. However, since such questions were framed in this way before and these are well entrenched in the literature ( Sparck-Jones et al., 2000 ), we followed the existing formulation. For a particular query q , we define a binary random variable R for the outcome of the relevance decision. R = r means relevant to q and R = r means irrelevant to q . That is, our model is currently designed for binary relevance (although it can be extended to graded relevance later).
For each context, there are two possible outcomes (events): (a) The context is relevant, i.e., R = r . (b) The context is irrelevant, i.e., R = r .

Similar to the BIM ( Robertson &amp; Sparck-Jones, 1976 ), given a particular context c ( d culate its probability of relevance and irrelevance by the log-odds
This log-odds reflects the relevance decision preference of the concerned context. Using Bayse X  rule, we have
The second term of (2) is a constant and will be eliminated by the linear normalization ( Lee, 1997 ) when combining the context scores of a document (see Section 2.4 ), it can be ignored for ranking the contexts as follows: culate the above probabilities by the individual document terms found in the contexts, our model makes the same assumption as proposed in Cooper (1995) :
Linked-Dependence Assumption ( Cooper, 1995 ): The degree of statistical dependence between the terms in the relevant set is associated with their degree of statistical dependence in the irrelevant set.
The linked-dependence assumption (a) simplifies the mathematical calculations and (b) avoids the problem of data inconsistency pointed out by Cooper (1995) when assuming conditional independence of terms in rel-evant set and irrelevant set individually. Using the linked-dependence assumption, we have The question now is to calculate the probabilities and where t 2 c ( d i , k , n ).

In order to calculate (5) and (6) , we have to obtain the term distributions (i.e., the probability of seeing a be the relevance model defining the term distribution in the relevant set, M ( r , q ) be the irrelevance model tribution in the collection. In general, (5) and (6) are equal to the relative frequency estimates of P M ( r , q ) ( t )and P
Next, we use the collection model to substitute the irrelevance model because almost all of the documents are irrelevant for a query in a sufficiently large collection: Collection-Irrelevance Assumption : For a sufficiently large collection and a query q , the irrelevance model M ( r , q ) and the collection model M ( c , q ) are similar to each other.
 Hence, irrelevance assumption will be addressed in the model-oriented experiments (see Section 3.5 ).
The remaining concern goes to calculating (7) . There are various methods to calculate (7) depending on the training method used where training here refers to estimating the probability distribution of the terms in the the probability of seeing a term in the model). In this paper, we explored two training methods (namely doc-ument-training and context-training ) based on different assumptions and depending on what are the terms that should be included in the relevance model M ( r , q ).
In document -training , we use the whole document (i.e., all terms inside the document) for training the model, based on the following assumption.

Document -Training Assumption : For a particular query q , the entire relevant document d evant so that the terms d i [ k ] for k 2 [1, j d i j ] are included in the relevance model M ( r , q ). The document-training assumption contradicts with our query-centric assumption for the relevant documents.
However, the query-centric assumption is used in the ranking process while the document-training assumption is used in this particular training method. We provide this method to show that inconsistent assumptions in training and retrieval using the proposed model may degrade the retrieval effectiveness and the document-training method was used in our previous study ( Wu et al., 2005 ).

Based on the document-training assumption, f ( t , M ( r , q )) is which is the frequency of occurrence of the term x in the relevant documents. Hence, P frequency estimate of the probability of seeing t in the relevance model M ( r , q ).

In context-training , we use the contexts c ( d i , k , n ) inside a document for k 2 [1, j d following assumption:
Context-Training Assumption : For a particular query q , only the contexts in the relevant documents are rel-evant so that for a document d i relevant to q , the terms in the contexts c ( d d i [ k ] 2 q are included in the relevance model M ( r , q ).

The context-training assumption is consistent with the query-centric assumption but these two assumptions are different. The query-centric assumption does not assume that every contexts c ( d d [ k ] 2 q in the relevant document d i to be relevant. By contrast, the context-training assumption assumes that every context c ( d i , k , n ) for k 2 [1, j d i j ] where d
Based on the context-training assumption, f ( t , M ( r , q )) is which is the frequency of occurrence of the term x in the contexts of relevant documents. Hence, P the relative frequency estimate of the probability of seeing t in the relevance model M ( r , q ). We believe that the context-training assumption is more realistic than the document-training assumption because most of the time only a part of the document contains relevant information (e.g., Fig. 1 ), especially for long documents. 2.3. Estimation issue is the problem of zero probability in estimating P M ( r , q ) assigned a zero probability. Note that the problem of zero probability does not occur when estimating P zero probability will set Eq. (4) to zero and it can cause anomalies in ranking.

Smoothing ( Chen &amp; Goodman, 1996; Zhai &amp; Lafferty, 2004 ) of the term distribution is a solution to the zero probability problem. The basic idea of smoothing is to adjust the term distribution so that zero probability will not assign to unseen terms. In this section, we describe different commonly used interpola-tion-based smoothing techniques (see Zhai &amp; Lafferty, 2004 ) (namely additive smoothing, Jelinek X  X ercer smoothing and absolute discounting) which we will apply them for investigating the effect of smoothing to our model.

Additive smoothing ( Jeffreys, 1948; Johnson, 1932; Lidstone, 1920 ) adds a constant d make unseen terms to have uniform, non-zero probabilities where j M ( r , q ) j is the number of unique terms in the relevance model and d special case of additive smoothing (i.e., d a = 1). Additive smoothing is relatively simpler than the other two smoothing techniques because it does not require the information from the collection model. the relevance model M ( r , q ) and the collection mode M ( c , q ) the probability P M ( r , q ) ( t ) will become (1 d jm )  X  P M ( c , q ).
 terms are decreased by a constant where j M ( r , q ) j is the number of unique terms in the relevance model and d 2.4. Combining context scores query term d i [ k ] 2 q (i.e., weight ( d i , k )) at location k in the document
For combining context scores, we need weight ( d i , k ) to be between zero and one. So we normalize weight ( d by the linear normalization ( Lee, 1997 ) across documents where min( weight ) is the minimum context score obtained among all the retrieved documents and max( weight ) is the maximum context score obtained among all the retrieved document. Using the linear normalization, the second term of Eq. (2) can be eliminated.

A document may contain more than one contexts (i.e., when the query terms occur more then once in the document). Hence, we need to aggregate the context scores for obtaining the document score for ranking. This viously, passage scores are combined using arithmetic mean, as well as taking the maximum ( Callan, 1994 ).
Kong et al. (2004) proposed three principles regarding how to make the relevance decision for a document about a particular topic by combining relevance of document parts, as follows: (a) the Disjunctive Relevance Decision (DRD) principle which states that a document is relevant to a topic if (b) the Aggregate Relevance (AR) principle which states that a document is more relevant to a topic if more (c) the Conjunctive Relevance Decision (CRD) principle which states that a document is relevant to a topic if
According to Harman (2004) , the TREC ad hoc evaluation is recall-oriented and if any part of the document is relevant, the TREC evaluator considers that the entire document is relevant for ad hoc retrieval.
Therefore, the DRD principle seems to be consistent with the TREC evaluation policy for ad hoc retrieval. 2.4.1. Extended Boolean Operators
We used the extended Boolean conjunction and disjunction ( Fox, Betrabet, Koushik, &amp; Lee, 1992 ) (i.e., the p Norm) to test different methods (i.e., AND and OR) for combining the context scores in a document d tion or disjunction is the soft/hard decision parameter and p P 1. The parameter m in Table 3 is the total num-ber of occurrences of the query terms in d i (i.e., the number of interested contexts found in d
For the extended Boolean operators, when p = 1, the extended Boolean conjunction and the disjunction are the same which is the arithmetic mean of the context scores in a document. When p = 1 , the extended
Boolean conjunction (AND) returns the minimum context score in the document while the extended Boolean disjunction (OR) returns the maximum context score in the document. Note that the extended Boolean disjunction is the same as the generalized mean function ( Dyckhoff &amp; Pedrycz, 1984 ) which complies with the AR principle ( Kong et al., 2004 ). 2.4.2. Dombi operators
Besides the extended Boolean operators, we also used the fuzzy operators for combining the context scores in a document. In the framework of fuzzy set theory ( Zadeh, 1965 ), the fuzzy conjunction operator complies with the CRD principle while the fuzzy disjunction operator complies with the DRD principle ( Kong et al., 2004 ). We used the Dombi X  X  ( Dombi, 1982 ) fuzzy operators to experiment the two principles ( Table 4 ), where p is again the soft/hard decision parameter and p P 1. For the Dombi X  X  operators, when p = 1 , similar to the extended Boolean operators, the Dombi X  X  conjunction (AND) returns the minimum context score in the doc-ument while the Dombi X  X  disjunction (OR) returns the maximum context score in the document ( Dombi, 1982 ). 2.4.3. Ordered weighted averaging (OWA) operators
Apart from the extended Boolean operators and Dombi X  X  operators, there are also other aggregation oper-ators in multi-criteria decision making such as the ordered weighted averaging (OWA) operators proposed by ( Yager, 1988 ). OWA operators have been used in applications of decision making, expert system, neural net-works, etc. An OWA operator with dimension m is a mapping F : R vector Y =( y 1 , ... , y m ) T having the properties (a) y that where b j is the j th largest element of the collection of the aggregated objects { a
For our proposed model, let ( a 1 , ... , a m ) be the vector of the m context scores of d ordered ( descending ) vector of the m context scores of d is the aggregated score (i.e., document score) of d i for the query q .

An important issue of the theory of OWA operators is to determine the weighting elements y of the weighting vector Y . There are some special cases of defining Y , for examples: (a) Taking the maximum: y 1 =1, y 2 = = y m =0. (b) Taking the minimum: y 1 = = y m 1 =0, y m =1. (c) Taking the arithmetic mean: y 1 = = y m =1/ m .

From the above examples, it should be clear that different ways of defining the weighting vector Y yield different OWA operators. We investigated two previous retrieval models (namely the MMM model ( Waller &amp; Kraft, 1979 ) and the Paice model ( Paice, 1984 )) that can be considered to be using OWA operators.
The MMM model considers only the minimum and maximum context scores in a document using the coef-ficients Cand 2 [0,1] and Cor 2 [0,1] for AND and OR operations respectively:
The MMM model (AND)
The MMM model (OR) When applying the MMM model in our aggregation of context scores, we only need one equation because
Cor =1 Cand . Therefore, we use a single parameter a 2 [0,1] in the weighting vector Y such that y y = = y m 1 =0, y m =1 a and use Eq. (14) for ranking the documents.

The Paice model ( Paice, 1984 ) uses a normalized geometric series with a parameter r 2 [0,1] for weighting the criteria. Assume that we have m criteria for making decisions (e.g. the dimension of the OWA operator is m ) and let S be the geometric sum The weighting vector Y of the Paice model for AND and OR operators are the normalized geometric series in Table 5 . The ranking formula is using Eq. (14) like the MMM model with the corresponding weighting vector Y . For the Paice model, the weighting vectors for AND and OR operators are the reverse of each other.
Both the MMM model and the OR operator of the Paice model comply with the AR principle. In Table 6 , we group the aggregation operators described above using the three principles in Kong et al. (2004) by their sive because any operators which comply with the CRD/DRD principle also comply with the AR principle.
However, the opposite may or may not be true. 3. Model-oriented experiments
In this section, we present the results of the model-oriented experiments which extensively investigate the factors affecting the effectiveness of the model using the TREC-6 ad-hoc collection. This collection contains 556,077 English documents. We use the TREC-6 title (short) queries 301 X 350 in the experiments. Title queries are used because they have few (1 X 4) query terms which are similar to the lengths of web queries. All the terms in the documents and queries are stemmed using the Porter stemming algorithm ( Porter, 1980 ). Stop words are removed in both the documents and queries. Terms with document frequency equals to one are also removed in the documents. This is because we do not want document-identifying terms such as document ids to be included in the training and retrieval process. For statistical inference, we also performed various non-para-metric (Wilcoxon) statistical significance tests. Non-parametric tests are used because we do not know the underlying distributions of the mean average precision (MAP) performances of the retrieval systems. We also report the precision of the top 10 documents (i.e., P@10), the precision of the top 30 documents (i.e., P@30) and the R-precision in the experiments. The top N document precisions are precision-oriented measures which complement the recall-oriented measures like MAP. R-precision is provided for reference. 3.1. Document-training vs. context-training
In this section, we compare the performances of using document-training and context-training with differ-ent context sizes (defined as 2 n + 1). The objectives of the experiments in this section are to determine (a) whether document-training or context-training is better; and (b) the most suitable n empirically (i.e., the context size).

The smoothing technique used in all the experiments in this section is the Laplace smoothing (i.e., setting d = 1 in Eq. (10) ) for simplicity and the document score is the maximum context score found in the document (i.e., setting p = 1 in the extended Boolean OR ( Table 3 )) similar to some passage-based retrieval ( Callan, 1994 ).
 First, we performed a predictive retrieval using the BM25 term weight of the 2-Poisson model ( Robertson &amp;
Walker, 1994 ) using the standard parameter setting ( Walker, Robertson, Boughanem, Jones, &amp; Sparck-Jones, 1997 ) (i.e., k 1 = 1.2 and b = 0.75) with passage-based retrieval and pseudo relevance feedback (PRF).
The result in Table 7 is our baseline performance. The top 3000 retrieved documents using the 2-Poisson model are re-ranked by our model for later evaluations. Next, we experimented with the document-training (Doc-T) and context-training (Con-T) training methods and compare them with different context sizes 2 n +1 ( Table 8 ).

In Table 8 , the differences between the MAP of context-training over document-training are statistically that the context-training assumption is more realistic than the document-training assumption. Thus, context-training has a higher MAP performance. The results also suggest that using contexts is a viable method in information retrieval. It should be noted that the highest MAP of document-training is obtained when n = 35 (i.e., context size of 71) and the highest MAP of context-training is obtained when n = 50 (i.e., context size of 101). This suggests that document-training favours smaller contexts while context-training favours lar-is preferred over document-training and n should be set to 50 (i.e., context size of 101) to obtain a balance of good effectiveness and efficiency, as increasing n also increases the processing time. 3.2. Smoothing
The experiments in this section aim to discover the effects of smoothing to our proposed model. We tested the model with different smoothing techniques (namely additive smoothing (A), Jelinek X  X ercer smoothing (JM) and absolute discounting (D)) in estimating P M ( r , q ) 2.3 ).

From the results of previous experiments (Section 3.1 ), we are using context-training and context size of 101 score found in the document ( Callan, 1994 ). In Table 9 , d can be the parameter d smoothing (Eq. (10) ), Jelinek X  X ercer smoothing (Eq. (11) ) and absolute discounting (Eq. (12) ), respectively, depending on the columns in the table.

For additive smoothing (A), we can see that from Table 9 when the value of d decreases. The best performance is obtained when d a (Eq. (10) ) is equal to 0.1. We believe that the reason is should be).

For Jelinek X  X ercer smoothing (JM) and absolute discounting (D), we can see that from Table 9 the per-formances are quite stable over the ranges of d jm (Eq. (11) ) and d best performances (MAP) for each of the smoothing techniques are highlighted ( d d = 0.1 in additive smoothing (Eq. (10) ), Jelinek X  X ercer smoothing (Eq. (11) ) and absolute discounting (Eq. (12) ) respectively). The best performances for each of the smoothing techniques are similar to each other.
The best MAP obtained among all the runs in this section is 0.7078 which is absolute discounting with d when the observed best values of the parameters d a , d jm absolute discounting with d d = 0.1 is used. 3.3. Context scores aggregation
The experiments in this section try to discover the best aggregation operator discussed in Section 2.4 for combining the context scores in a document, and find out which of the 3 decision principles (i.e., the CRD, the AR and the DRD principles) ( Kong et al., 2004 ) performs better. From the results of the previous exper-counting with d d = 0.1 (Eq. (12) ). First, we test the extended Boolean operators ( Table 3 ) and the Dombi X  X  fuzzy operators ( Table 4 ). The results are shown in Tables 10 and 11 .
 From Table 10 , when p = 1, the results of extended Boolean AND is the same as that of extended Boolean
OR as the formulae for the two operators when p = 1 are the same. As p increases, the performance difference between extended Boolean AND and extended Boolean OR becomes apparent. From p = 5 onwards, the dif-ferences are statistically significant with 99.9% C.I. using the Wilcoxon matched-pairs signed-ranks test. When p = 1 , the extended Boolean AND is the same as using minimum context score as the document score and the extended Boolean OR is the same as using maximum context score as the document score. In general, the results in Table 10 suggest that extended Boolean OR is better than extended Boolean AND when combining the context scores.

From Table 11 , we can see that Dombi X  X  OR operator performs better than Dombi X  X  AND operator in all cases of p when aggregating the context scores. This suggests that Disjunctive Relevance Decision (DRD) principle is preferred over Conjunctive Relevance Decision (CRD) principle. The reason is that in the TREC relevance judgements, if a part of a document is judged relevant, then the whole document is judged relevant.
This in fact favours the DRD principle. The results also confirm with that in ( Kong et al., 2004 ) in which DRD principle is preferred over CRD principle.

Next, we test the Ordered Weighted Averaging (OWA) operators (Eq. (14) ). Table 12 shows the results of using the MMM model (Eq. (15) ) with different values of a . When a increases from 0.1 to 0.9, the MMM model behaves from AND operator to OR operator. We can see that the OR operator is better than the
AND operator using the MMM model as the performance (MAP) increases while the value of a goes from 0.1 to 0.9. We obtain the best MAP with a = 0.7 (highlighted in Table 12 ).
 Table 13 shows the results of using the Paice model operators ( Table 5 ) with different values of r . From
Table 13 , the performance difference in the AND and OR operators of the Paice model is larger for small increases with r from 0.1 to 0.9. The best MAP (highlighted) for both operators are obtained when r = 0.9. From Tables 10 X 13 , the performance of OR operators is better than that of corresponding AND operators.
While OR operators behave like DRD principle and AND operators behave like CRD principle, all the results suggest that DRD principle is preferred because on average the MAP performance is higher. In Table 6 ,we grouped different operators using the relevance decision principles ( Kong et al., 2004 ), and we compare the results between each of the groups. Table 14 shows the best result obtained using the aggregation operators, which grouped the contexts scores according to the relevance decision principles in Table 6 . Using the best result in each of the operators, we pair-wise compare them with statistical tests in Table 15 . performance between the two operators. We can see that the MAP performance of AR principle is statistically that using the operator based on the CRD principle. This suggests that the AR principle is robust. In Table 14 , the MAP performance using the DRD principle is similar to the AR principle. However, in
Table 15 , there is a statistical significant difference between the MAP using operators based on the AR prin-ciple and the MAP using the operators based on the DRD principle. This difference might be due to the dif-ference between the DRD and AR principles where the DRD assert the additional boundary condition. That is, if any single document part is relevant, then the entire document is considered relevant. In practice, this occurs only for the top ranked context affecting only one document because all the other contexts are assigned nificant difference, it seemed that the top ranked context may have a noticeable impact on the effectiveness the top ranked context is assigned a value of one by the linear transformation ( Lee, 1997 ), the document ranking score is one by the boundary condition of the DRD principle, thereby loosing the differentiability of relevance scores in the context of that document. However, for the AP principle, the document that con-tains the top ranked context does not necessarily have the highest document relevance score of one, because the other context scores in the document may affect the final document relevance score. Also, in practice, many relevant documents require multiple contexts to make relevance judgments and the likelihood of making a relevance judgment on the basis of a single context in a relevant document is not high. These mitigating factors suggest why the operators based on the AR principle appeared to be performing slightly better than the operators based on the DRD principle, even though the DRD principle is consistent with the TREC ad hoc evaluation policy. 3.4. Probability ranking principle ranks the documents in the collection in the order of decreasing probability of relevance to q , then the best overall effectiveness of the model will be achieved with respect to the data available to the model. The prob-ability ranking principle assumes that the relevance of a document d principle, with more and more relevance information for q available to the model, we expect that the model will never decrease the performance for q . Because if the model decreases the performance for q when it has more relevance information, this means that the best overall effectiveness is not achieved for having more rel-evance information and this violates the probability ranking principle. In this section, we provide the exper-imental results to show that our proposed model obeys the probability ranking principle for TREC-6 data.
Using context-training with context size of 101 (i.e., n = 50), additive smoothing with d and the extended Boolean OR operator with p =20( Table 3 ) for combining the context scores in a document, we tested the model using different percentage of relevant documents to train the model. More specifically, we randomly sample x % of the relevant documents (where x = 0, 20, 40, 60, 80 and 100) for a query q and use the randomly sampled relevant documents for training the model (i.e., for constructing M ( r , q ) using context-training). Then, we perform a retrieval for q using the trained model. The procedure is repeated five times for every query of TREC-6 (i.e., for every query, we perform five retrievals using five sets of randomly sampled relevant documents for training). When x =0or x = 100, the performances of the five retrievals are the same.
After the experiments, we discovered that for all 50 queries of TREC-6, the performance (MAP) increases monotonically when x goes from 0 to 100. In Fig. 2 , we show the average MAP for all 50 queries in the five retrievals while the maximum and minimum MAP of the five retrievals at each level of x are also shown. The results suggest that our model obeys the probability ranking principle for the TREC-6 data set. The best per-formance is obtained when 100% of the relevant documents are used for training the model. 3.5. Validation of the collection-irrelevance assumption
When calculating the context score, we used the collection model M ( c , q ) to substitute/estimate the irrele-vance model M ( r , q ) by the collection-irrelevance assumption (Eq. (9) in Section 2.2 ). As the assumption validate the collection-irrelevance assumption. When using the irrelevance model, smoothing should be applied to the model similar to that in the relevance model. From the results of smoothing of the relevance model ( Table 9 ), we are using absolute discounting with d difference when using the collection model and the irrelevance model.
From Table 16 , although the MAP performance when using the irrelevance model is statistically signifi-cantly higher than that when using the collection model, the difference is only about 3%. This shows that, in doing retrospective experiments, using the irrelevance model can improve the MAP performance for most
This confirms to our claim that the irrelevance model and the collection model are similar to each other. In order to reveal the optimal results, we are using the irrelevance model instead of the collection model in the subsequent experiments. 4. Scope-oriented experiments
In this last set of experiments, we test the reliability of the proposed model by experimenting it with differ-ent data collections (the ad-hoc retrieval of TREC-2, TREC-6, TREC-7 and the robust-track retrieval of
TREC-2005) and another language (Chinese NTCIR-5). Similar to the experiments in Section 3 , title (short) queries in each of collections are used as they are commonly found in web search. The performance of TREC-6 has been evaluated in the previous section (Section 3 ) and the TREC-7 data collection is a subset of the TREC-6 data collection.
 Table 17 shows some collection statistics of the data collections for the experiments reported in this section. Based on the results of the experiments in Section 3 , we use context-training with context size 2 n + 1 (=101.
I.e., n = 50), absolute discounting with d d = 0.1 (Eq. (12) ) and the extended Boolean OR operator with p =20 ( Table 3 ) for combining the context scores in a document for all the data collections tested in the scope-ori-ented experiments. 4.1. Different English data collections
Table 18 shows the results of the predictive baseline experiments using BM25 term weight of the 2-Poisson model ( Robertson &amp; Walker, 1994 ) with passage-based retrieval and pseudo relevance feedback (PRF) and our retrospective experiments. The purpose of this comparison is to show that we have used state-of the-art retrieval models (based on our implementation) and to show that our novel retrieval model is worth further investigation. For the latter, we provide the results of the statistical tests in Table 18 for completeness. We caution that comparing the results of retrospective experiments dryly with the predictive baseline experiments is unfair, as the former has relevance information while the latter does not.

In Table 18 , the predictive performance of TREC-7 is not as good as the others. This is probably due to many of the relevant documents in TREC-7 do not contain any of the title query terms (i.e., about 12.9% of the relevant documents per TREC-7 topic do not contain any of the query terms in Table 2 ).
In order to test the robustness of our model in different TREC data collections, we test the results of the experiments using the Wilcoxon two sample test (results shown in Table 19 ). Wilcoxon matched-pairs signed-retrieval effectiveness performance on the basis of the same topics. The Wilcoxon two sample test compare the MAP of two sets of topics in two collections and used the pooled variance that is estimated by summing the standard errors of MAPs of each set of topics. The null hypothesis is that the MAPs of two sets of topics in two different collections are the same.

From Table 19 , the smaller the p value, the larger the confidence interval of the MAP performance of the hypothesis is not rejected at 94% C.I.). The results suggest that our model performs similarly over different
TREC English data collections which show that the proposed model is not unreliable. 4.2. Different language
We also test the proposed model using the Chinese collection, NTCIR-5 ( Kishida et al., 2005 ), for showing that the model can operate with more than one language. Table 20 shows the results of the predictive baseline experiments which were obtained using BM25 of the 2-Poisson model based on bigram indexing with pseudo relevance feedback ( Luk &amp; Kwok, 2002 ). As the NTCIR-5 has two sets of relevance judgments, relax and rigid, we have two results for the same run where Relax-E means evaluated using the relax relevance judg-ments and Rigid-E means evaluated using the rigid relevance judgments.

In the retrospective experiments of NTCIR-5 using our model, we used bigram indexing to collect terms in contexts so as to be consistent with our Chinese indexing strategy for the initial retrieval. The NTCIR-5 data set has two sets of relevance judgements, Relax and Rigid, we can train the model using context-training based on one set of relevance judgements and then evaluate the result using the same or another set of relevance judgments. This yields four combinations of training and evaluation results as shown in
Table 21 (Relax-T means training using the relax judgments, Relax-E means evaluation using the relax judg-ments, Rigid-T means training using the rigid judgments and Rigid-E means evaluation using the rigid judgments).

From Table 21 , we see that the performance is higher when using the same relevance judgments for training and evaluation (i.e.,  X  X  X elax-T, Relax-E X  X  and  X  X  X igid-T, Rigid-E X  X ). When using relax judgments for training for the rigid judgments. When using rigid judgments for training and relax judgments for evaluation (i.e.,  X  X  X igid-T, Relax-E X  X ), the relevance model M ( r , q ) may contain insufficient information for the relax judg-ments. Therefore, the results in Table 21 are not unexpected. In Table 22 , we can see that the difference of using the same judgment for training and for evaluation compared with using different judgments for training and for evaluation is statistically significantly different.

In Table 23 , we compare our retrospective results of the English TREC data collections ( Table 18 ) and our retrospective results of the Chinese NTCIR-5 collections ( Table 21 ) using the Wilcoxon two sample test (as the queries used in different collections are different).

In Table 23 , we see that the results in NTCIR-5 using the same judgment for training and evaluation (i.e., tions based on 99.9% C.I. (i.e., p &lt; 0.001). For other combinations, the differences are not significant at 99.9% C.I. The result of TREC-2 is the most similar to that of using rigid judgment for training and relax judgment for evaluation in NTCIR-5 ( p 6 0.9588). 5. Related work
Vechtomova and Robertson (2000) presented a method of combining corpus-derived data on word co-occurrences with the probabilistic model of information retrieval. Significant collocates are selected using a window-based technique around the node terms. Mutual information (MI) scores and Z statistics were used to filter significant associated collocates. The notion of context in our proposed model is very similar to the window-based technique used by Vechtomova and Robertson. However, our model does not extract colloca-tions and it does not use MI and Z statistics. By contrast, we derive our model using the notion of context at the very beginning.
 Other work has also been done on exploiting word co-occurrence statistics using windowing techniques.
For examples, Vechtomova, Karamuftuoglu, and Robertson (2006) empirically investigated whether the degree of lexical cohesion between the contexts of query terms X  occurrences in a document is related to its rel-evance to the query. By contrast, we concentrate on individual contexts of the query terms in a document to test whether a particular context is relevant. Xu and Croft (2000) implicitly assumed that query terms and expansion terms are related within some context windows using the local context analysis (LCA) in the local collection (i.e., the top ranked documents). However, we do not perform query expansion but utilize the term distributions in relevant and irrelevant documents. In Lund and Burgess (1996), Burgess and Lund (1997) and
Burgess et al. (1998) , the researchers developed the Hyperspace Analogue to Language (HAL) model to auto-matically construct the dependencies of a term with other terms using their co-occurrences ( Bruza &amp; Song, 2003 ) inside a context in a sufficiently large corpus. For a given term, a vector is created which elements are the probabilities of the term co-occurring with other terms. Song and Bruza (2003) proposed an informa-tion inference mechanism in information retrieval for making inferences via computations of information flow in a high dimensional context-sensitive vector space constructed using the HAL model. Gao, Nie, Wu, and
Cao (2004) extended the language modeling approach by incorporating dependencies between terms in the model using term co-occurrence statistics which showed that using the co-occurrence information in language modeling benefits the retrieval effectiveness. Bai, Song, Bruza, Nei, and Cao (2005) proposed the context-dependent query expansion technique in language modeling approach using extended inference model with information flow. All the studies provided evidence that exploiting the term co-occurrence information is cru-cial for increasing retrieval effectiveness. In our model, by (a) considering the contexts of the query terms in documents and (b) estimating the relevant model M ( r , q ) for making relevance decision preferences for the contexts, we further extend the usage of term co-occurrence information to match the conceptual meaning of the query terms and document terms.

Metzler and Croft (2005) developed a novel retrieval model based on Markov random field. Their model assigns different weights to different types of query term occurrence patterns in the document contexts. The weights of different types of query term occurrence patterns are combined together as the document score for ranking. Although Metzler and Croft (2005) used document contexts, their model did not use the non-query terms in the contexts for document ranking. Also, their model was not motivated by modeling or simulating relevance decision making. Instead, their model tunes the parameters using the training data to optimize the MAP performance.

Salton, Allan, &amp; Buckley, 1993 ), a document is divided into passages and each of the passages is evaluated individually by the retrieval model. Xi, Xu-Rong, Khoo, and Lim (2001) investigated a window-based passage retrieval technique but they did not require the center term of a window to be a query term. The passage-based retrieval implicitly agrees that the query term is related to other terms in limited distances but not in large distances. Our model shares the same intuition. However, passage-based retrieval divides a document exhaustively without regard to query term locations (e.g., number of terms, passage tags). By contrast, our model divides a document based on the occurrences of query terms in the document (i.e., terms around query term). 6. Summary and future work
In summary, we proposed a novel hybrid document-context retrieval model which uses existing successful techniques to explore the effectiveness of incorporating term locations inside a document into our retrieval model. We used the log-odds as based on by the well known BIM retrieval model ( Robertson &amp; Sparck-Jones, 1976 ) as the starting point for deriving our document-context based model. We extended the existing proba-bilistic model from the document level to the document-context level, in which relevant information are located using contexts in a document. For probability estimation, we use smoothing techniques ( Chen &amp;
Goodman, 1996; Zhai &amp; Lafferty, 2004 ) similar to that of the language modeling approach to information retrieval. When combining the context scores (i.e., combining the evidence of relevant information), we tried different aggregation operators (Section 2.4 ) including the extended Boolean operators, the Dombi X  X  fuzzy operators and the ordered weighted averaging (OWA) operators (the MMM model, the Paice model AND and OR operators) to aggregate the context scores.

We tested the model extensively using the TREC-6 data collection with different context sizes, training methods, smoothing methods and context scores aggregation methods. We found out that context-training is preferred over document-training as the training method ( Table 8 ). The context size 2 n + 1 should be around 101 (i.e., n = 50) for balanced effectiveness and efficiency. We also compared different smoothing tech-smoothing techniques produce similar results when the optimal parameter is determined. From Table 9 ,we used the absolute discounting with d d = 0.1 (Eq. (12) ). After comparing different aggregation methods, the extended Boolean OR operator produced the best result in our model-oriented experiments.

We also tested the model with different data collections. The experiments in this paper showed that the model is effective for the different reference data collections with various sizes and languages (i.e., TREC-2,
TREC-6, TREC-7, TREC-2005 and NTCIR-5). The main remaining problem in the model is to estimate the model with less relevance information (e.g., relevance feedback with limited top N retrieved relevance information) in order to make it operate effectively in predictive experiments.
 Acknowledgements This work is supported by the CERG Project # PolyU 5183/03E. Robert thanks the Center for Intelligent Information Retrieval, University of Massachusetts (UMASS), for facilitating him to develop in part the basic IR system, when he was on leave at UMASS.
 References
