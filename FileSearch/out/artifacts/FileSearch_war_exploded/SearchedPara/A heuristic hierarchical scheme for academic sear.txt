 1. Introduction
Academic research has been revolutionized since the days of standard library-based research, mostly by the advancements oftheinternetandtheworld-wideweb,andbythebreak-throughadvancementsofcompaniessuchasGoogle.Andyet,search-arenotassatisfactoryasthesearchresultsonmoregenericqueriessuchasthosedepictedonYahoo! X  X  X  X  X rendingNow X  X  X ection.  X  the first one hundred results. The same query when submitted on Microsoft X  X  Academic Search ( http://aca-ceived 8000 citations to date!. Notice that the terms  X  X  X eb X  X  and  X  X  X nformation Retrieval X  X  both appear as keywords in the ascalablepeer-to-peerlookupprotocolforinternetapplications X  X ( Stoicaetal.,2003 )thathasreceivedmorethan1900citations web information retrieval in general has advanced to a very mature state, academic search has still some way to go before reaching maturity ( Beel &amp; Gipp, 2009; Walters, 2007 ).

In this paper, we propose a heuristic hierarchical scheme for re-ranking publications returned from standard digital li-braries such ACM Digital Library and evaluate the scheme based on the feedback of users. The scheme relies on a combina-the document and their relative distance in-between sentences, paragraphs and sections, the annual distribution of the pa-each bucket together with the order of the buckets forms a new ordering of the original results to the submitted query. our conclusions and plans for future work. 1.1. Related work
The main graph representation methods in academic search can be distinguished among two approaches based on: (i) link references of scientific papers and (ii) academic collaboration networks. With respect to approaches that are based on the link references of scientific papers, a new multi-faceted dataset named CiteData for evaluating personalized search between papers X  X hose distribution is shown to obey a power law as expected X  X ut even more, the dataset contains per-sonalized queries and relevance feedback scores on the results of those queries obtained through various algorithms. The authors report that personalized search algorithms produce much better results than non-personalized algorithms for infor-mation retrieval in academic paper corpuses.

Graph-theoretic methods based on social or academic collaboration networks have been used in information retrieval and link analysis have been used to develop a graph database querying system for information retrieval in social networks onstrate collaboration patterns among different scientific fields including the number of publications that authors write, their co-author network, as well as the distance between scientists in the network among others ( Newman, 2001 , 2004 ).
Similarly, an analysis of the proximity of the members of social networks (represented as network vertices) can help esti-mate the likelihood of new interactions occurring among network members in the future by examining the network topology formstronggroupsconsistedofnodeswithonlylooserconnectionshasalsobeenexaminedinordertoidentifysuchgroupsand the boundaries that define them, a concept based on the concept of centrality indices ( Girvan &amp; Newman, 2002 ).
A number of journals from the fields of mathematics and neuroscience covering an 8-year period have been examined in in time as well as a model for capturing the network X  X  evolution in time in addition to numerical simulations. The combi-behaviour and topology of the network are concerned.
 in their paper, shows an improvement over both full-text as well as link-based clustering.

Topic modeling integrated into the random walk framework for academic search has been shown to produce promising tionships between documents in the context of their usage by specific users representing the relevance value of the docu-ment in a specific context rather than the document content can be identified by capturing data from user computer interface interactions ( Campbell et al., 2007 ).

Odysci ( http://www.odysci.com ) is another Web portal application that provides among others, search functionality for user query.

Some of the aforementioned approaches use the constructed graphs in order to provide improved querying functionality based on knowledge originating from the graph structure and topology while other methods attempt to identify the pres-ence of clusters in the graphs revealing patterns of collaboration. The application of such methods proves to be powerful techniques including term frequency are necessary but not sufficient technology for academic paper retrieval. Clustering to enhance retrieval accuracy in an academic research search engine. 2. System design and implementation
In our proposed system, PubSearch, we use a hierarchical combination of heuristics in order to be able to come up with an improved ranking scheme. At the top level of the hierarchy, we use a custom implementation of the term frequency algorithm that considers combinations of term occurrences in different parts of the publication in order to determine which publica-tions are the most relevant ones with respect to a specific query (abbreviated as  X  X  X F heuristic X  X ).
At a second level, our scheme introduces a depreciated citation count method ( X  X  X CC heuristic X  X ) that examines the annual citations is likely more important than a paper having received 50 citations that was published 10 years ago. 10,000 papers in ACM Portal (from more than 15,000 authors) and constructed two distinct types of graphs that connect the index terms so as to reveal how closely various connected index terms are; details of these graphs X  construction and their properties are discussed in Section 2.2 . This information is used to provide an even further improvement to our ranking in the next two sub-sections. 2.1. System architecture cesses. Process P1 implements a focused crawler that crawls the ACM Portal for publications in diverse fields of Computer
Science and Engineering, in order to select information about the relationships between authors/co-authors and the topics (identified by the index terms) they work on.

This information is analyzed in process P2 ( X  X  X nalysis of topic associations and connections among authors and co-Card, &amp; Landay, 2005 ).

Processes P4 X  X 6 form the heart of the prototype search engine we have developed, which includes a web-based applica-re-ordered, along with a user feedback form via which the system got relevance judgement scores from the user, as ex-plained in Section 4 . 2.2. Topic similarity graphs and cliques
As mentioned already, by crawling the ACM Portal web site, we collected an initial corpus of nearly 10,000 publications allowed to run the algorithm that constructed the maximal weighted clique graphs. The following process (schematically shown in Fig. 2 ) continues until there are no more authors to process: 1. The application fetches from persistence the next author that has not been yet processed. 3. The application parses all Google Scholar results and selects only those corresponding to ACM Portal pages. 4. For each of the ACM Portal pages selected in the previous step, the application checks whether the page has already 5. The application extracts and stores in persistence the following publication information: a. index terms, b. names of all authors, c. date of publication, d. citation count, 6. The author names extracted in the previous step that have not been previously encountered are now stored in persis-7. The application sets processed flag to true for the current author. 8. GOTO step 1.

We constructed two types of graphs, each of different semantic strength . The first type of graph corresponds to the most direct relationship between index terms, namely that of index terms being present in the same publication. So, in a Type I graph, two index terms t 1 and t 2 are connected by an edge ( t the collection indexed under both index terms t 1 and t 2 but not t 2 and also at least one paper where t 2 appears but not t idea behind such an association type is that there might in fact be a relation between two seemingly unrelated topics when number of authors publish a number of papers both in the fields of cloud computing (or network architectures in general) this association is the one that a Type II graph aims to address.

We mine heavily-connected clusters in these graphs by computing all maximal weighted cliques in these graphs. Com-has only up to 13 node degree. We further reduce the problem complexity by considering edges whose weight exceeds a pivoting ( Bron &amp; Kerbosch, 1973 ) applied to the restricted graph containing only those edges whose weight exceeds w workstation (these graphs can be interactively visualized via a web-based application we have implemented as part of pro-cess P7 in Fig. 1 , by visiting http://hermes.ait.gr/scholarGraph/index ). 2.3. Ranking heuristic hierarchy The heuristic hierarchy we use for re-ranking search results for a given query is schematically shown in Fig. 3 . At the top-level of our hierarchical heuristic algorithm, we use a custom implementation of the term frequency heuristic .
Term frequency (TF) is used as the primary heuristic in our scheme in order to identify the most relevant publications as far as pure content is concerned (for a detailed description of the now standard TF-IDF scheme see for example Manning,
Raghavan, and Schutze (2009) ,or Jackson and Moulinier (2002) ). When designing the term frequency heuristic we have ta-for the relevance of a specific publication with respect to a specific query.

In order to overcome this limitation, our implementation identifies the number of occurrences of all combinations of the mentations of the term frequency heuristic, the experiment results showed that this approach performs significantly better in identifying relevant documents than the classical case of the sum of all individual term frequencies. Our implementation assigns different weights to term occurrences appearing in different sections of the publication. (See described in most textbooks on Information Retrieval.) Term occurrences in the title are more significant than term occur-encountered terms in different segments of the publication. By proximity level we denote the distance among encountered thermore we distinguish the following two types of term occurrence completeness, complete and partial . A complete term specific term occurrence is based on its completeness as well as the proximity level; complete term occurrences are more significant than partial ones and similarly term occurrences at sentence level are more significant than term occurrences at paragraph level.

Frequency X  X  (IDF) part from our scheme. The reason for omitting IDF is that we cannot maintain a full database of academic another engine provides (e.g. ACM Portal) and simply work with those results. It would be expected then that computing the and initial experiments with the TF scheme proved this intuition is correct.

We now return to the formal description of our custom TF scheme. Let Q ={ T implements this formula.
 splitParagraphIntoSentences( Paragraph ) splits the specified paragraph into a set of sentences. The method findAllUnique-
TermOccurInSentence( Sentence , Q ) returns the subset of all terms in Q that appear in the specified Sentence . Similarly query terms in Q .
 whereas pWeight represents the term occurrence weight at paragraph level. The method determineSectionWeight( t ) deter-all, our term-frequency heuristic is implemented as follows:
Algorithm calculateTF(Publication d , Query q ) 1. Let S {}, T {}, P {}, O {}, tf 0 2. Set T splitPublicationIntoSections( d ) 3. foreach section t in T do 4. endfor 5. return tf 6. end
After calculating the total query term frequency for each publication, the algorithm groups all publications with similar term frequency scores into buckets of specified range. This grouping of the publications allows bringing together publica-tions with similar term frequency scores in order to apply further heuristics to determine an improved ranking scheme. Re-sults placed in higher range term frequency buckets are promoted at the expense of publications placed in lower term frequency buckets.

At the second level of our hierarchical ranking scheme, the results within each bucket created in the previous step are nual depreciation scores are calculated then the scores are summed and produce a total depreciation count score for a par-ticular publication obeying the formulae where c p is the total (time-depreciated) citation-based score for paper p , n has received in a particular year j , n is the current year, d publication year of the paper p .
 mote them in the ranking order at the expense of older publications that might have a higher citation count but a consid-lication date. Once publications have been sorted in decreasing order of the criterion c ond-level buckets of like-score publications.

Within each bucket of the second-level heuristic, we further order the results by examining each publication X  X  index terms and calculate their degree of matching with all topical maximal weighted cliques, the off-line computation of which lication X  X  index terms with all maximal weighted cliques. The calculation details are as follows. lications with a big number of index terms against cliques with a small number of index terms we calculate the percentage match m i = c i / d . For all remaining cases (non-perfect match) the percentage matching is calculated as m m system calculates a weight score representing the overall value of the association of p with c w = w i m i es i ac i where w i is the weight score of the examined maximal weighted clique i , and ac to the association type that the current graph that the current clique belongs to represents ( ac ac in order to promote more recent ones. Since each type of graph has a different significance, we consider recent graphs of
The algorithm calculates for each publication a total clique matching score S score of the publication X  X  index terms with all maximal weighted cliques and determines the final ranking of the results accordingly:
The total clique matching score determines the order of the results within the current second level bucket and eventually determines the final ranking of the results. 3. Experiments design As previously mentioned we have developed a meta-search engine application in order to evaluate our ranking algorithm.
Registered users can submit a number of queries via our meta-search engine X  X  user interface. The search interface allows PubSearch and ACM Portal.

For each query in the processing queue, our system queries ACM Portal using the exact query phrase submitted by the user and crawls ACM Portal X  X  result page in order to extract the top 10 search results. The top ten search results as well as the default ranking order provided by ACM Portal are stored. For each of the returned results, our system automatically crawls each publication X  X  summary page in order to extract all required information. Additionally for each of the returned index terms are available from the result summary page).

When all available publication information is gathered, the system executes our own ranking algorithm with the goal of order to provide feedback. The user is presented with the default top ten results produced by ACM Portal in a random order feedback score scheme where 1 corresponds to  X  X  X east relevant X  X  and 5 corresponds  X  X  X ost relevant X  X .
In order to compare an IR system X  X  ranking performance, we use two commonly encountered metrics: (i) Normalized Dis-counted Cumulative Gain (NDCG) and (ii) Expected Reciprocal Rank (ERR). We also introduce a new metric, the lexicographic ordering metric (LEX), that can be considered a more extreme version of the ERR metric.

Normalized Discounted Cumulative Gain ( J X rvelin &amp; Kek X l X inen, 2000 ) is a metric commonly used for evaluating ranking algorithms in cases where graded relevance judgments exist. Discounted Cumulative Gain (DCG) measures the usefulness of a document based on its rank position. DCG is calculated as follows: relevance scores:
The term IDCG p (acronym for  X  X  X deal DCG till position p  X  X ) is the DCG relevance feedback, so that in a perfect ranking algorithm n DCG ument they need, they stop looking further down the list of results. ERR is defined as follows: where f max is the maximum possible user relevance feedback score (in our case, 5).

Besides the common NDCG and ERR metrics, we also calculate a total feedback score LEX( q ) for the (re-) ranked results of any particular query q by following a lexicographic ordering approach to produce a weighted sum of all independent feed-back result scores: where n is the number of results, d f =( f max 1) 1 , a  X  by the user for the publication p i with values in the set {0, d rankings of some results list produced by two different schemes, the scheme that assigns a higher score for the highest ranked publication always receives a better overall score LEX( q ) regardless of how good or bad the publications in lower we must simply show that if two result-lists ( r 1,1 , ... , r scores ( f norm ( r i ,1 ), ... , f norm ( r i , n )), i = 1, 2 and f ference will be at least equal to d f , and at most equal to 1, we need to show that for all possible values of the quantities f norm f norm  X  r 2 ; i  X  f norm  X  r 1 ; i  X  6 1 8 i  X  2 ; ... ; n , if the value a is such so that d hold for all possible values of the quantities f norm ( r d ordering property always holds regardless of the result list size or feedback values. Clearly, it always holds that
LEX( q ) e [0, 1], with the value 1 being assigned to a result list where all papers were assigned the value f ber of search results) containing values randomly selected from the range { f relevance judgment score, in our case 1) is 0.5. More importantly, LEX, being a weighted linear combination of the normal-above the average of the normalized relevance judgment scores f the top of the list, and vice versa, the LEX score will be below the value f results towards the top of the list, thus providing a statistical decision criterion ( E  X  LEX &gt; E  X  f engine produces ordered results lists with the property that the relevance judgment scores X considered as random variables satisfying the stochastic order X that the stronger condition X norm 6 st LEX  X  X 1 ; ... ; X
The LEX scoring scheme can be considered as a more extreme version of the ERR and NDGC metrics and is inspired from quality of the top 2 X 3 results) that are returned from any search engine than on lower ranked results X  X ee also the related work by Breese, Heckerman, and Kadie (1998) on the R-Score metric on ranking measures. This is probably due to the very that (if it exists) apparently does not have solid grounding with regards to academic search engines X  X t least, not yet. 4. Experimental results the parameters tWeight , pWeight , and sWeight for the proposed TF-scheme. The bucket ranges are as follows:
For the TF-heuristic, we always compute exactly 10 buckets by first computing the proposed TF metric for each publica-tion and then we normalize the calculated scores in the range [0,1] in a linear transformation that assigns the score 1 to the publication with the max. calculated TF score, and then we  X  X  X ucketize X  X  the publications in the 10 intervals [0,0.1], (0.1,0.2], ... (0.9,1].
 For the 2nd level-heuristic, the bucket range is set to 5.20.

Values for the other parameters are set as follows: sWeight = 15.25, pWeight = 4.10, and tWeight based on standard meta-heuristics (namely a GA process implemented in the Open-Source popt4jlib library developed by one of the authors, obtainable through http://www.ait.edu.gr/ait_web_site/faculty/ichr/popt4jlib.zip ) with the objective function being the average LEX score of the training-set queries.

Given these parameters, we proceeded into testing the system by processing 58 new queries that were submitted by 15 puter engineering. The users were selected based on their expertise in different areas of computer science and electrical number of queries and provided feedback for all produced query results without knowing which algorithm produced each ranking. We used the three metrics mentioned before (NDCG, ERR, and LEX) to evaluate the quality of our ranking algorithm. 4.1. Comparisons with ACM Portal
Our ranking approach, PubSearch, compares very well with ACM Portal, and in fact outperforms ACM Portal in most query evaluations as the tests reveal using all three metrics. We illustrate the performance of each system in Table 1 . Table 2 shows the average score of each system using the three different metrics. PubSearch performs much better than ACM Portal in most of the 58 queries used to evaluate our system under all metrics.

On average, the percentage gap of performance between PubSearch and ACM Portal in terms of LEX metric is 907.5%(!), in produces a LEX score close to 1, leading to huge percentage deviations for such queries. formance metrics. In Table 10 (Appendix) we present an analytical comparison of the evaluation scores of the two systems using the three metrics.

To highlight the difference of the ranking orders produced by the two systems, consider query#1 ( X  X uery privacy  X  X  X ensor
Search re-orders the ACM Portal results in a sequence that corresponds to the following relevance judgement: 5,4,3,3,1,2,2,1,1,1. PubSearch produces the best possible ordering of the given search results (with the exception of the document in 5th position that should have been placed in 7th position). Similarly, consider query #46 ( X  X esource manage-ment grid computing X ): ACM Portal orders its top ten results in a sequence that received the following scores: 5,4,3,4,4,3,1,1,3,1, which is a much improved ordering than ACM Portal. 4.2. Comparison with other heuristic configurations
We have compared the performance of our hierarchical heuristic scheme using our custom implementation of the TF heu-results in a LEX average score of 0.499, an average NDCG score of 0.93, and an average ERR score of 0.514, resulting in an average LEX percentage gap of 89.9%, an average NDCG percentage gap of 5.42%, and an average ERR percentage gap of traditional TF heuristic, even when embedded within our hierarchical scheme.
 the best possible score for all metrics considered.

In Fig. 4 we show the performance of our proposed heuristic configuration when comparing it with different hierarchies of the following configurations: (1) TF/DCC/MWC (the proposed scheme), (2) TF/DCC, (3) TF, (4) DCC, (5) MWC, and finally, (6) TF/MWC.
 considered.
 The percentage difference between the proposed full PubSearch configuration (TF/DCC/MWC) and applying the proposed are statistically significant for all the metrics considered at the 95% confidence level .

To make the comparison with TF-IDF methods clearer, we also compare PubSearch against the standard Okapi BM25 tioned we do not maintain an academic paper database but instead simply re-rank the results returned by other engines, when computing the BM25 score for a result list, we assume that the entire database consists of the returned results of queries on each metric for each of the two systems.

The average percentage difference between PubSearch and Okapi BM25 in terms of the LEX metric is 1898% (due to BM25 producing a LEX score of less than 0.004 for some queries while for the same query PubSearch producing scores of more than as BM25 is a generic non-binary information retrieval model that has no specific domain knowledge about academic publications. 4.2.1. Comparison with linear combinations of the heuristics We also compare our approach against linearly combining the three heuristics (TF, DCC, and MWC) via standard Linear descending order of the new score. We tested the linear regression method on the same set of 58 queries in our test-set: the results are shown in Table 4 below. PubSearch outperforms the linear combination of the heuristics, and the t -test,
We also compare our proposed approach against a  X  X  X usion X  X  scheme ( Kuncheva, 2004 ) where for each heuristic h (that can possible sequence of a query X  X  search results (measured against the training set query data).

This score A h is computed as follows: assume the search results for a query q ranked in descending order of relevance feedback by the user are as follows: d q ,1 , ... , d q , n heuristic h scores the documents so that they are ranked according to the following order: d quantity g q , i as follows:
Q train is then defined as A h  X 
H ={ TF , DCC , MWC } that works as follows: each heuristic in the set of heuristics produces a re-ranked order of results d ensemble fusion results are comparable with ACM Portal on the NDCG and ERR metrics (0.5% better in terms of NDCG, and 8.1% better in terms of ERR metric); the ensemble fusion also does a much better job than ACM Portal in terms of the LEX metric (442% better). Still, the ensemble fusion results do not compare well with PubSearch as can be seen in Table 5 .

PubSearch is on average more than 472% better than the fusion heuristic described above in terms of LEX metric, more 4.3. Comparison with other academic search engines
We performed a head-to-head comparison between PubSearch and the three state-of-the-art academic search engines: 1. Google Scholar ( http://scholar.google.com ). 2. Microsoft Academic Search ( http://academic.research.microsoft.com ). 3. ArnetMiner ( http://arnetminer.org ).

The comparison was made on a sizeable subset of our original query set of 58 user queries shown in Table 10 , comprising
Q =  X  X age Rank clustering X , Q 60 =  X  X ocial network information retrieval X , Q ing X  respectively. The 20 queries from the original query set were #[34 X 47], #49, and #[54 X 58].
Each query was given to each of the above mentioned search engines, and the top-10 results (from each of the above search engines) were then presented to the users for relevance feedback in random order. The summary of results produced in Tables 6 X 8 respectively.

The average percentage difference between PubSearch and Microsoft Academic Search is 15% for the LEX metric, 2.6% for are shown in Fig. 5 .
 shows that the improvement is statistically significant at the 95% confidence level.

However the same does not apply for the other two metrics, although the t -test shows that the results for the LEX metric are also statistically significant at the 93% confidence level. A visualization of the results is shown in Fig. 6 . 4.1%,andintermsofNDCGmetricis12.6% .Theresultsof all statisticaltestsarestatisticallysignificantfortheLEXandERRmetrics, results. 4.4. Can PubSearch promote good publications  X  X  X uried X  X  in ACM Portal results?
In the introduction, we mentioned that ACM Portal fails to return the  X  X  X hord: a scalable peer-to-peer lookup protocol for internetapplications X  X  X aperinthetop20resultsofthequery X  X  X eer-to-peerprotocol X  X .PubSearchontheotherhand,whengiven the top 50 results of ACM Portal for the same query, re-ranks them and returns the mentioned paper in the top position. retrieval X  X ( Krieger,2011 ). Thepapersappearingabovethe Page-Rankpaperallshare thefollowingcharacteristics: (i)theyhave theTFheuristicpromotestheotherpapershighintheresultlistsothattheGooglepaperendsupinthe2ndTFbucket,andthen, itscitationcountalonecannotpromoteithigherthanthe5thposition.Still,PubSearchmanagestopromotetheGooglepaperin the top 5 results which is much better than the other academic search engines we experimented with. mation need X  X hat happen to appear much lower than the top ten positions in the results list of ACM Portal, we ran another experiment where the users ranked the top 25 results of 20 queries. The system shows again very significant performance improvement against ACM Portal in all metrics considered, and in fact, it significantly improves its performance gap over
ACM Portal in terms of both the NDCG and the ERR metrics, when compared with the previous case of re-ranking only the top-10 results.
 The results are shown in Table 9 . The percentage improvement of PubSearch over ACM Portal on average in terms of the metric introduced in Breese et al. (1998) . 4.5. PubSearch run-time overhead
The run-time overhead our initial prototype requires to perform the re-ranking of the search results given a query and a computation of the TF-score (by far the most compute intensive process in the whole system) for each document is indepen-processing requirements per document by one order of magnitude to make the system commercially feasible. 5. Conclusions and future directions most 12% andinterms of the ERR metric the average improvement is more than 77%. Similarly,comparing PubSearchagainst the standard Okapi BM25 scheme shows that PubSearch offers very significant advantages for ranking academic search results. Even when comparing PubSearch against the current state-of-the-art academic search engines Google Scholar, Microsoft
Academic Search, and ArnetMiner, the comparisons show that PubSearch outperforms these other engines in all metrics con-sidered, and in the vast majority of cases, by statistically significant margins.
 relevance judgments in the ordered list have an expected LEX score of 0.5, and more importantly (iv) a search engine that average normalized relevance judgments score.

WithoutdetailedknowledgeoftherankingsystembehindACMPortalortheotheracademicsearchengineswecomparedour systemwith,wepostulatethatthemainreasonsforthebetterqualityofourrankingschemeisinthe customimplementationofthe frequency score roughly determines whether a publication is relevant to a particular query, the time-depreciated citation formedintheTypeI&amp;IIgraphsthatconnectindextermstogetherinarelativelysmallbuttypicalpublicationbasecrawledforthis purpose.

We have shown through extensive experimentation that the proposed configuration outperforms all the other configu-rations we have experimented with, as well as linear regression or the popular ensemble fusion approach using linear weights (that is present in many if not most modern classifier, clustering, and recommender systems designed today, e.g. Christou, Gkekas, &amp; Kyrikou, 2012 ).

We are already in the process of fine-tuning and optimizing our initial prototype (mostly in terms of response time) to lications according to the ACM classification scheme, which, besides the obvious advantage of letting us use the full Pub-Search system against any publication it clearly would have many other independent uses.
 Appendix A.

In this Appendix we present the table containing extended information about analytical experiment comparison results, and in particular, containing the queries used in our evaluation test-set (see Table 10 ).
 References
