 There are many association mining algorithms dedicated to frequent itemset mining [1, 2, 3, 4, 5]. These algorithms are defined in such a way that they only find rules with high support and high confidence. A much less explored area in association mining is infrequent itemset mining. Recently, Koh and Rountre e [6] proposed the Apriori-Inverse algorithm to mine infrequent itemsets without generating any frequent rules. It captures so-called sporadic rules using maximum support (maxsup) and minimum confidence (minconf) thresholds. The support of items ets forming each rule is below the maxsup threshold but above a user-defined minimum absolute support value. They define the notion of perfectly sporadic rules, where the itemset forming each rule consists only of items that are below the maxsup threshold. In contrast, imperfectly sporadic rules consist of individual items with high support but the support of the intersection of the items is low.

Apriori-Inverse is not able to find impe rfectly sporadic rules because it never con-siders itemsets that have support above maxsup; therefore no subset of any itemset that it generates can have support above maxsup. Apriori will miss these rules, because the support for the itemsets forming the rules is too low. Apriori-Inverse will miss them as well, because the support for the individual items is too high. Therefore, both algo-rithms will miss rules of the form AB  X  C ,where A and B are individually common, but AB is rare and C is rare. This, for example, is the situation where two symptoms X  both of which commonly occur alone X  X ccur together only rarely; but when they do, the combination indicates a rare and serious disease with high confidence. We consider this to be a very interesting type of rule to be able to find.

The aim of our research is to develop a technique to mine imperfectly sporadic rules efficiently. To force any variant of the Apriori algorithm [2] to find imperfectly spo-radic rules, the minimum support threshold must be set very low. This in turn drasti-cally increases the running time of the algorithm, due to a combinatorial explosion in the number of frequent itemsets. Apriori-Inverse suffers the same problem in reverse: maximum support has to be set so high that too many itemsets qualify as sporadic. In this paper, we propose an algorithm called MIISR (Mining Interesting Imperfectly Sporadic Rules) to find imperfectly sporadic rules using item constraints: we capture rules with a single-item consequent below the maxsup threshold. The maxsup threshold is used to identify all items that are considered rare. These items are then considered to be the only possible consequents for all rules that will be generated. Items in the transactions containing the consequent are then detected. The items found are used to form antecedents that have strong a ssociations with the consequents.

Inherently we are looking for rules with low support that could make them indistin-guishable from coincidences (that is, situations where items fall together no more often than would be allowed by chance). Hence, we use coincidence pruning to remove the occurrences of coincidental itemsets. For an itemset to be considered non-coincidental it must have support above a minimum absolute support (minabssup) value which is generated using a variant of Fisher X  X  exact test. The rest of this paper is organised as follows. Definitions pertinent to infrequent itemset mining and a review of related work are given in Section 2. The MIISR algorithm and an explanation of coincidence prun-ing is presented in Section 3. In Section 4, we evaluate MIISR on synthetic and real datasets, and in Section 5 we conclude the paper. The following is a formal statement of association rule mining for transaction databases. Let I = { i 1 ,i 2 ,...,i m } be the universe of items and D be a set of transactions, where each transaction T is a set of items such that T  X  I .Anassociationruleisanimplica-tion of the form X  X  Y ,where X  X  I , Y  X  I ,and X  X  Y =  X  . X is referred to as the antecedent of the rule, and Y as the consequent .Therule X  X  Y holds in the trans-action set D with confidence c % if c % of transactions in D that contain X also contain Y .Therule X  X  Y has support s % in the transaction set D ,if s % of transactions in D contain XY [2]. Throughout this paper we shall use XY to denote an itemset that contains both X and Y .

One way of forcing low-support items to take part in mined rules is by imposing in a rule and then modifying the mining process to take advantage of that informa-tion [7, 8, 9, 12, 11]. One of the restrictions that may be imposed is called consequent constraint-based rule mining . Among these we shall discuss Dense-Miner, EP (Emerg-ing Pattern), and Fixed-Consequent ARM (Association Rule Mining).

Bayardo et al. [9] proposed a consequent constraint-based rule mining approach called Dense-Miner. They require mined rules to have a given consequent C speci-fied by the user. This approach introduces an additional metric called improvement .The improvement of a rule is defined as the minimum difference between its confidence and the confidence of any proper sub-rule with the same consequent. If the imp of a rule is greater than 0, then removing any non-empty combination of items from the antecedent will lower the confidence by at least the improvement.
Emerging pattern (EP) was proposed by Li et al. [12]. Given a known consequent T , they use a dataset partitioning approach to find  X  X op X ,  X  X ero-confidence X , and  X   X  -level confidence X  rules. The dataset D is divided into sub-datasets D 1 and D 2 ;where D 1 consists of the transactions containing T and D 2 consists of transactions which do not contain T . All items in T are then removed from D 1 and D 2 . Using the transformed dataset, EP then finds all itemsets X which occur in D 1 but not in D 2 . For each X , the rule X  X  T is a  X  X op rule X  in D with confidence of 100%. On the other hand, for all itemsets Z that only occur in D 2 , all transactions in D which contain Z must not contain T . Therefore Z  X  T has a negative association and is a  X  X ero-confidence X  rule. For  X   X  -level confidence X  rules Y  X  T the confidences are greater than or equal to 1  X   X  .

Rahal et al. [11] propose a slightly different approach. Fixed-Consequent ARM gen-erates minimal confidence rules using S E trees and P-trees. Given two rules R 1 and R 2 , with confidence values higher tha n the confidence threshold, where R 1 is A  X  C and R 2 is AB  X  C , R 1 is preferred, because the antecedent of R 2 is a superset of the antecedent of R 1 . The support of R 1 is greater than or equal to R 2 . R 1 is considered a minimal rule and R 2 is considered a non-minimal rule. The algorithm was devised to generate the highest support rules that match the user specified minimum confidence threshold without having the user specify any support threshold.

The drawback to all three approaches is that they are only useful when we have prior knowledge that a particular consequent is of interest. For our application, we are interested in searching for imperfectly sporadic rules, without having to wade through a lot of rules that have high support, without generating a large number of trivial rules, and without needing prior knowledge of which consequents ought to be interesting. In the previous section, the techniques discu ssed might generate some imperfectly spo-radic rules, if there is prior knowledge of a rare and interesting consequent, and if min-sup is set low enough. So, rather than address the problem in the context of frequent itemset mining, we suggest explicitly treating it as a problem of infrequent itemset min-ing. Hence we propose the MIISR algorithm to mine interesting imperfectly sporadic rules. This algorithm uses the same definition of maxsup as in Apriori-Inverse [6]. Since itemsets lose support as they grow larger, and our guiding constraint is maxsup rather than minsup, we can no longer rely on a downward-closure principle.

We begin by searching for any individual items below maxsup and using these as candidate consequents. For each candidate c onsequent, we then generate candidate an-tecedents from the items with in the same transactions. Because we are dealing with candidate itemsets with low support, it is possible that we will see items occurring to-gether in transactions about as many times as chance would allow X  X e refer to this situation as a coincidence . Itemsets that occur within the d atabase due to coincidence do not add meaningful information and should be ignored. Hence we identify a minab-ssup value to filter out these itemsets. The imperfectly sporadic rules are then generated in a similar fashion to Apriori. As we are storing the transactional dataset in an inverted index, we note that our method does not require dataset partitioning. 3.1 Imperfectly Sporadic Rules A rule is considered imperfectly sporadic if it meets the requirements of maxsup and minconf but contains any items that have support above maxsup. For instance, suppose we had an itemset AB with support ( A )=12% , support ( B )=10% , and support ( AB )= 10% , with maxsup = 11% and minconf = 75% .Both A  X  B (confidence = 92% ) and B  X  A (confidence = 100% ) are sporadic in that they have low support and high confidence. Imperfectly sporadic rules are defined as in [6]: Definition: Some imperfectly sporadic rules could be completely trivial or uninteresting: for in-stance, when the antecedent is rare but the consequent has support of 100% . We can characterise four different types o f imperfectly sporadic rule: Ty pe 1 rules have both frequent and infrequent itemsets in ant ecedent and consequent. Ty pe 2 rules have only frequent itemsets in both anteced ent and consequent. They too Ty pe 3 rules have consequents that contain only infrequent itemsets; they will only be Ty pe 4 rules have antecedents that contain only infrequent itemsets; they will only be We would prefer a technique that finds imperfectly sporadic rules that are interesting : for instance when the items in the antecedent are above maxsup but the intersection of these items is below maxsup and the consequent has a support below maxsup. Clearly Type 3 rules are interesting under this defin ition, and the rest of this paper describes our attempt to generate them in a reasonably efficient manner. An example of a Type 3ruleis fever, stiff neck, rash  X  meningitis ,where fever , stiff neck ,and rash are com-mon separately, but just occasionally occu r together. When they do, one can diagnose meningitis with some confidence, even though meningitis is quite rare. 3.2 MIISR Overview Broadly speaking, the MIISR algorithm performs the following steps. On the first pass through the database an inverted index is built using items as keys and the transaction IDs as the data. At this point, t he support of each unique item (the 1 -itemsets) in the database is available as the length of each data chain. Items that fall under maxsup are identified and recorded as candidate c onsequents. For each ca ndidate consequent found, we use the items that reside in the same t ransactions as the candidate consequent to extend the ( k  X  1) -itemsets in precisely the same manner as Apriori to generate candidate k-itemsets. These extensions are c onsidered to be candidate antecedents. We then check the candidate ant ecedent itemsets against the inverted index to ensure they meet the minimum absolute support requirement and prune them out if they do not. This candidate generation process is repeated until no further candidate antecedents are produced. 3.3 Minimum Absolute Support Value When searching for rare itemsets, we consid er two circumstances: occurrences of item-sets due to some non-random process that is generating them, or occurrences of itemsets by random collision (coincidence). It is impor tant to distinguish between them, as item-sets that have a low support but high confidence that seem interesting may be occurring due to chance and should be considered  X  X oise X . Clearly it makes sense only to con-sider candidate itemsets that appear together more often than coincidence. We define coincidence in the following way: for N transactions in which the antecedent A occurs in a transactions and consequent B occurs in b transactions, we can calculate the prob-ability that A and B will occur together exactly c times by chance. We refer to this as  X  X robability of chance collision X  [10]. We can calculate this probability using Pcc in (1). The probability that A and B will occur together exactly c times is:
For example, given N = 1000 , A = B = 500 ,and AB = 250 , we are able to determine that the probability that A and B will occur exactly 250 times is 0 . 05 .This equation is the usual calculation for exact probability of a 2  X  2 contingency table [13]. Now, we want the least number of collisions above which Pcc is smaller than some small value p (say, 0 . 0001 ). This is:
This formula amounts to inverting the usual sense of Fisher X  X  exact test [13]. Usu-ally a 2  X  2 contingency table is provided and a p-value calculated. However here we are providing two of the four values and a p-value, and calculating the minimum value to complete the table. By selecting the mi nabssup value for each itemset we are able to prune out associations that appear in th e dataset by chance. We calculate the cu-mulative Pcc of AB together m times (beginning from 0 and incremented by 1). We stop the incrementation when the cumulative value of Pcc  X  1 . 0  X  p and m is set as the minabssup value. For example given that we set N = 1000 , A = B = 500 ,and p =0 . 0001 , minabsup value is 274. Candidate itemsets that appear above the minab-ssup requirement are considered somewhat interesting, and worth retaining to evaluate their confidence. 3.4 Exclusory Constraint Even after pruning out candidate anteceden ts that are indistinguishable from coinci-dence, a considerable number of itemsets can still be produced for a modest dataset. One solution to this problem is to prune out a larger number of itemsets before gen-erating new candidates. Another solution, and the one we adopt, is to prohibit current candidates from being extended if their extensions are not likely to produce interesting rules. Here we introduce an exclusory constraint for this purpose. Given an imperfectly sporadic rule A  X  C which has high confidence, it becomes less meaningful if C  X  A has confidence that is too low. For a candidate antecedent A to be considered worth expanding with respect to consequent C , it must therefore meet this requirement: Once the confidence of C  X  A falls below minconf or 1  X  sup ( C ) , A may still produce an interesting rule with C ,but AZ is unlikely to do so no matter what Z is. Thus, we wish to keep A in the pool of candidate itemsets, but we do not wish to extend it X  X e exclude it from the next round of candidate generation. 3.5 The MIISR Algorithm Having defined how to calculate the minim um absolute support (minabssup) neces-sary to consider a rule to be non-coincidental, and a procedure to prevent candidate antecedents from being extended if they are unlikely to produce interesting rules, we are now able to define an algorithm for Mining Interesting Imperfectly Sporadic Rules: MIISR.

In Section 3.2 of MIISR,  X  X temsets that extend A i,k  X  1  X  refers to the same process that Apriori uses to turn candidate itemsets of size k  X  1 into itemsets of size k .That is, itemsets that share all but their last item are used to form a new itemset with the same prefix, the suffix of the first itemset, and the suffix of the second itemset. The only difference is that candidate antecedents that do not meet the exclusory requirement outlined in Section 3.4 are never consider ed for extension. The  X  X ount X  function, when given one itemset argument in addition to the inverted index, returns the number of transactions in which the itemset occurs. W hen given two itemset arguments, it returns the number of transactions in which both of the itemsets may be found.

The result of MIISR is a data structure indexed by all 1-itemsets that fall under maximum support. We need not return C , the list of candidate consequents, since if A i is non-empty, i  X  C . For each of these items i , A contains a list of antecedents such that A i,j  X  i should be an imperfectly sporadic rule. Since MIISR does not restrict the antecedents to containing only frequent itemsets, some perfectly sporadic rules may also be produced. To assess the performance of MIISR in discovering Type 3 imperfectly sporadic rules, we developed a synthetic data generator which deliberately injects imperfectly sporadic itemsets. We then tested the MIISR algorith m on six different datasets from the UCI Machine Learning Repository [14].

Our synthetic data generator is a modified version of the data generator proposed by Agrawal and Srikant [2]. In real databases there may be both frequent and infre-quent itemsets and rules, but we are only interested in the imperfectly sporadic itemsets. Table 1 summarizes the characteristics of sev eral of the datasets generated during our tests. Since we are deliberately ignoring large itemsets, we left | I | set to 2 for all exper-iments.

To create a dataset D , our synthetic data generation program takes the following parameters: number of transactions | D | , average size of transactions | T | , average size of large itemsets | I | , number of large itemsets | L | , number of imperfectly sporadic itemsets | S | , and number of items N . We first determine the size of the next transaction which is generated using a Poisson distribution with mean as the average size of the transaction. We then fill the transactions with items. Each transaction is assigned a series of potential large itemsets and/or an imperfectly sporadic itemset. Each itemset in T has a weight associated with it, which co rresponds to the probability that this itemset will be picked. 4.1 Results Three different experiments varying either the number of transactions, average size of transactions or number of imperfectly sporadic itemsets injected were conducted to assess the efficiency and scalability of MIISR. The datasets used were generated using the synthetic data generator described in the previous section. Maxsup was set to 0 . 10 and minconf to 0 . 90 .

In the first experiment, we varied the number of transactions from 10 3 to 10 6 over the T10.L30.S10 dataset. In Figure 1 we see that the time taken to process the data seems to increase linearly with the number of transactions. Figure 2 shows the execu-tion time taken to process a dataset while varying the average size of transactions, from 15 to 60 with an increment of 5 , for the L60.S20.D10K dataset. Note that as the aver-age size of transactions increased, the execu tion time increased somewhat worse than linearly. However, the curvature is gentle over a reasonably practical range of values. In the third experiment, we investigate the scal e-up of the number of deliberately injected imperfectly sporadic itemsets which ranged from 2 to 30 with an interval of 2 for the T20.L100.D10K datasets. Figure 3 shows the results of the execution time to process a dataset with a varying number of injected imperfectly sporadic itemsets. Although the number of rules found for each dataset increases as the number of imperfectly sporadic itemsets increases, notice that the fluctuation of the runtime taken is quite a small per-centage of the total runtime. The difference between the maximum and minimum time taken is 38 seconds.

Testing of the MIISR algorithm was also ca rried out using six different datasets from the UCI Machine Learning Repository [14]. Table 2 displays results from using MIISR with and without the exclusory constraint. Each row of the table represents an attempt to find the number of imperfectly sporadic rules (with minconf 0 . 95 , and lift greater than 1 . 0 ) from the database named in the left-most column. In the table, accept represents the number of itemsets below maxsup but above the minabssup value and reject represents the number of itemsets below the minabssup value. The number of accepted and rejected itemsets depends on the the amount of noise within a certain dataset.

Using MIISR, we were able to find imperfectly sporadic rules below maxsup of 0.10 for the datasets Teaching Assistant Eva luation, Bridges, Zoo, Flag, and Soybean-Large within reasonable time (below 200 seconds). However for the Mushroom dataset, maxsup was lowered to 0.005. Due to the nature of the Mushroom dataset, where strong association holds among most of the items, coincidence pruning is not able to prune out many itemsets. Consequently, we end up with a very large amount of imperfectly sporadic rules, some of which might be less interesting than others.

Finally, we note that both the proportion o f itemsets accepted by the minabssup constraint and the number of rules accepte d by the exclusory constraint seem to be entirely dataset-dependent. Very few existing algorithms try to find infrequent itemsets, despite the fact that the most potentially interesting things that happen in a database are likely to happen in-frequently. In this paper, we present a new algorithm called MIISR for discovering imperfectly sporadic rules. We are particularly interested in infrequently occurring as-sociations of frequent itemsets giving ris e to infrequent consequents. The supports of imperfectly sporadic rules are by definition low, and we therefore run the risk of accept-ing as interesting rules things that have only fallen together by chance. For this reason, the minimum absolute support value proposed in the paper plays an important role be-cause it does not allow rules that have only chance association to be generated. We acknowledge that this approach, and that based on the exclusory constraint, are heuris-tic, but we believe that to be unavoidable when generating low support rules. Since the number of low support rules can be very large, but those that are likely to be interesting not as common, exhaustive techniques tend rapidly to fall into pathological cases.
Currently this approach performs fairly efficiently on synthetic datasets and medium sized real datasets. Our future work will deal with examining the problem of transac-tion length, to which MIISR seems most sensitive in the synthetic datasets. More im-portantly, we need to try to characterise the UCI Mushroom dataset in terms of imper-fectly sporadic rules, and determine an a pproach that would narrow down the candidate antecedent itemsets even further . If it turns out that there simply are a lot of high con-fidence, low support rules in that database, then we need to investigate interestingness metrics that could be used to generate a better subset of them. Hence we are interested in determining the actual interestingness of imperfectly sporadic rules in real domains.
