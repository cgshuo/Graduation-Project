 Massive Open Online Courses (MOOCs) have experienced a recent boom in interest. Problems students struggle with in the discussion forums, such as difficultly in finding interesting discussion opportunities or attracting helpers to address posted problems, provide new opportunities for recommender systems. In contrast to traditional product recommendation, question recommendation in discussion forums should simultaneously consider constraints on both students and questions. These considerations include (1) Load Balancing -students should not be over-burdened with too many requests; and (2) Expertise Matching -students should not be requested to address problems they are not capable of addressing. In this work, we formulate a novel constrained question recommendation problem to address the above considerations. We design a context-aware matrix factorization model to predict students X  preferences over questions, then build a max cost flow model to manage the constraints. Experimental results conducted on three MOOC datasets demonstrate that our method significantly outperforms baseline methods in optimizing overall forum welfare, and in predicting which specific questions students might be interested in.
 H.3.3 [ Information Systems ]: Information Search and Retrieval X  Information Filtering Constrained Question Recommendation; Massive Open Online Courses; Load Balance; Expertise Matching
Massive Open Online Courses (MOOCs) have drawn a great deal of attention recently. MOOC platforms, such as Coursera 1 and EdX 2 , have demonstrated extremely high https://www.coursera.org/ https://www.edx.org/ student registration rates, with equally dramatic rates of attrition. MOOC discussion forums, where course instructors and students ask questions, discuss ideas, provide help to or even socialize with other students, face a similar problem, in that the number of questions asked in a forum increases rapidly, while around half of the posted questions are never resolved. Discussion that is productive for learning must be accessible and engaging to the students involved [4]. With thousands of questions for thousands of students to sift through, the challenge is in directing students to accessible, interesting topics, while simultaneously possibly helping unanswered questions get answered. Recommender systems, which have proven to be powerful in addressing similar problems [10], can be utilized to suggest to each student a short list of questions that they might be interested in. Students will be able to find questions that match their interests, and more otherwise unanswered questions will be addressed by students capable of answering them.
Traditional product recommendation [12] focuses on the extent to which a product will be desirable to users. Additionally, there have been some explorations of question recommendation in general discussion forums [22]. However, in contrast to existing work, question recommendation for MOOC discussion forums must address reciprocal constraints representing the needs of both the students who post questions and the needs of the students who might be able to offer some help or support in response.
 In particular, the related constraints include(1) Load Balancing , respecting that students have limited time for participating in the discussion; (2) Expertise Matching ,stu-dents have differing levels of expertise, and some questions will therefore be too difficult, whereas some are so simple that they do not make use of the students X  particular skills and abilities. These facets of question recommendation have not been fully addressed in existing work. Thus we propose this Constrained Question Recommendation (CQR) task to take into account load balancing and expertise matching constraints. The focus of our work is on the solution formalism, which can be tailored to alternative operationalizations of skills, abilities, and difficulty levels. To the best of our knowledge, this is the first work describing a question recommendation problem with constraints. Our main contributions in this work are as follows:
This paper is organized as follows. In Section 2, we present related work. Section 3 focuses on the problem X  X  motivation and definitions. Section 4.1 gives the context-aware relevance prediction model. Section 4.2 discusses constraint satisfaction, where we adopt a max cost flow model to satisfy constraints. Experimental setup and discussion of results are presented in Section 5. We conclude our work and point out the implications in Section 6.
The field of recommendation can be categorized into two kinds of work. The first is recommendation of products [10], a one-way recommendation where the focus is how desirable an item will be to users. This includes recommending books [12], music [23], Tweets [5], and movies. For instance, Korenetal. [9]proposesacombinationoflatentmodeling and similarity based matching between users and movies to suggest movies that fit users X  needs. The second is social recommendation [21, 11] which requires the match should be attractive in both directions, such as friend recommendation in social media like Facebook and online dating services [3]. For example, Pizzato et al. [15] propose a reciprocal recommender system for online dating where profile messages are utilized to learn users X  interests and consider also preferences of the recommendation recipients.
Traditional question recommender systems [16], such as those that recommend questions to potential answerers in Yahoo! Answers, are mostly focused on approximating the affinity between users and questions [22]. Hu et al. [6] introduces a user modeling method that estimates the interests and professional areas of each user in order to generate a suitable user set to answer a given question. Similarly, Kabutoya et al. [8] uses the question and answer histories of each user, combining collaborative filtering schemas and content based filtering schemas to recommend questions. Most existing question recommenders can be categorized into the product recommendation field because they do not address the issue of reciprocity, or the potential for constraints associated with users and questions. Especially for question recommendation in the context of MOOC forums, recommendations should be based not only on what would be attractive to the recipient [19], but also on how their expertise and question-answering capacity can be balanced against the load of available questions [20].
In this work, we seek to develop a recommender algorithm to suggest questions to users, respecting both their level of expertise and capacity for taking on work (e.g., load balancing). There are many effective recommendation techniques, including content based filtering [13] and col-laborative filtering, etc. Collaborative filtering can be categorized as model based [10] or neighborhood based methods [9]. Matrix factorization models [10], are one of the most successful model-based approaches to predict the affinity between users and items. Rendle et al. [17] proposes a context-aware factorization machine. Such matrix factorization models enable us to integrate rich contextual information, such as implicit feedback [9, 23] and user attribute information [1, 5] for predicting matches. However, these approaches do not address our concerns with respect to load balancing.
In this section, we present our motivation for conducting the constrained question recommendation task and define related terms that will be used in later sections.
Engagement in course-related discussions can have a positive influence on learning [4]. In the MOOC context, where the interaction or guidance of instructors is limited and dropout is very high, it is critical to increase the participation and engagement of students in the course [7]. Guiding students to topics that suit their interests among a large set of questions is challenging. This problem is even more pronounced for students who post questions seeking help as well as the would-be helpers who are unable find the questions they X  X e most able to respond to [18]. Suggesting appropriate questions to students is not simply determined by matching their interests, but is also subject to constraints on the students X  time and the questions X  difficulty.
Traditional question recommendation approaches mostly focus on figuring out the best matches of topic-interest be-tween student question pairs, ignoring practical constraints like load balancing and expertise matching. In fact, ignoring constraints is likely to cause a long tail problem [14] -for example, a large number of students may be directed to just a few popular questions, and too many questions may be recommended to a small set of high-expertise students. However, there are limits on how many questions any one student can answer, and such imbalanced recommendations may leave other students idle. Further, popular questions might not need such a large number of students to offer help. These imbalances will not only lead to a waste of valuable human resources, but also leave many un-popular questions deserted, and their askers at a loss for help.

In this work, we focus on how to recommend a list of questions to students on the given set of constraints on both students and questions (here expressed as load balancing and expertise matching). Our constrained question recommen-dation framework guides appropriate students to questions in a discussion forum such that (1) students will benefit from their interaction with questions that are interesting to them, (2) questions are discussed by students with sufficient expertise, and (3) students are not over-burdened with too many requests. By taking such constraints into account, we produce a set of recommendations that is mutually beneficial to both the community of question-askers and potential question-answerers. To the best of our knowledge, this is the first work to address question recommendation with constraints on both students and questions.
Existing recommender systems usually produce  X  X ele-vance X  scores to rank the pool of available questions for a given user, and offer the highest-ranked ones as recommendations. Here, we make a similar choice (although subject to constraints) and denote the  X  X elevance X  score of question q to student u as r u,q  X  [0 , 1]. For a specific student u , the larger r u,q , the more relevant the question is. In what follows, we introduce our Constrained Question Recommendation (CQR) problem. To model load balancing and expertise matching, we first define several values associated with students and questions, including student capacity, student capability, and question hardness.
Definition 1. Capacity of student u isaloadbalance range [ L u ,R u ] , i.e. the number of questions that would be comfortable for a student u should be no less than L u and no more than R u .

Definition 2. Capability B u is the capability/expertise of student u to solve problems.

Definition 3. Hardness H q stands for how difficult the question q is, or how much expertise it requires. The Load Balancing constraint requires that the number of questions recommended to students be in the range of the students X  Capacity, and that the number of students we guide to a given question should not be too few or too many; while Expertise Matching means that among students we guide to a specific question, there should be at least one student whose Capability is equal to or larger than the Hardness of that question. Those constraints enable us to give the formal definition of CQR. CQR is a general question recommendation framework that recommends a list of questions to students, subject to load balancing and expertise matching. It is worth mentioning that existing question recommender systems are a special case of CQR that ignores all constraints.
Figure 1 presents the overview of our constrained question recommendation framework. To address this constrained question recommendation problem, in the first step, we design a context-aware matrix factorization model to predict students X  preferences over questions (relevance prediction); in what follows, we build a max cost flow network using the predicted relevance scores to meet the constraints (constraint filtering) and show how to solve this model to provide the final recommendation list.
In this section, we briefly present our context-aware relevance prediction model, which can be fit into a class of popular matrix factorization models [17]. The predicted relevance scores are the basis of the subsequent constraint filtering phase. The relevance matrix between students and questions is denoted as R with entry r u,q representing the relevance between student u and question q .The relevance score r u,q is 1 if and only if student u has joined the discussion of question q . 0 entries are obtained by negative sampling as described below where we specify our experimental settings. This context-aware relevance prediction model mainly exploits three aspects of contextual information as follows, i.e. Student Features , Question Features , and finally Implicit Feedback .

Student Features include statistical features related to individual students, such as their posting and replying activities in the discussion forum. Here, Question Count , denoted as  X  u , represents how many unique questions the student has participated in prior to the current week and could indicate that student X  X  typical participation level, which could then be used as a long term activity level expectation. Previous Count  X  u , is calculated to indicate how many posts the student has made in the last week, and can be regarded as a short term or local activity level. Cohort  X  , representing the week in which this student registered for the course, could be treated as a proxy for the level of motivation (i.e. early course registration is typically associated with above average commitment to the course) [25]. This can be formally described as: Here,  X  r u,q is the predicted score of u on q ; bias represents various biases related to student and question, and P u and Q q are latent vectors associated with u and q .  X ,  X  are each one-dimensional feature vectors and  X  (  X  ) is a feature vector with dimension of different cohort numbers.

Question Features are those statistical features related to questions [24]. Reply Number  X  q is utilized to evaluate how many replies and comments a question q has garnered until now. Similarly, Question Length l q is designed to estimate how much effort and expertise is required to answer the current question, which is computed as the total number of words appearing in that question, including all its replies and comments. We formulate this as: where,  X  and L are two one-dimensional features associated with Reply Number and Question Length respectively.
Implicit Feedback [10] represents whether students express their implicit preferences over questions through their participation in other discussions(or not). Denote U as the set of students participating in question q and  X  ( v ) the predicted preference v of u ,thenwehave: q q q q [L 1 ,R 1 ], -f 2
Taking those three feature sets into account, we formulate our context-aware relevance prediction model as below: The relevance score should be in the range [0 , 1] and the observed matrix is binary, thus it is common to use a sigmoid function to ensure the final prediction in the range and logistic loss as the loss function: l ( X  To decrease the risk of overfitting, the l 2 -norm is chosen as the regularization term and  X  is the learning rate. The parameters of this model can be learned by the feature based matrix factorization framework [17], which is a class of state-of-the-art methods that can utilize contextual information and implicit feedback [9].

In this section, we firstly formalize the CQR task as an integer linear programming problem, which is equivalent to a max cost flow model and then we describe how we solve this max cost flow model.
Before giving a detailed description of the max cost flow model, we start with an introduction to our notation. Here, we denote f u,q  X  X  0 , 1 } as an indicator of whether the question q is recommended to the student u .Recallthe definitions in Section 3.2, r u,q  X  [0 , 1] is the relevance score between the question q and the student u . We formulate the constraintsbyusing f u,q in Section 3.2, and we obtain the rephrased representation as below: Taking those constraints into consideration, we then can give a mathematical formulation of our CQR problem. Here, U and Q are student and question sets respectively. I (  X  )is an indicator function. r u,q is generated by context-aware matrix factorization model.  X  ,  X  are parameters used to balance the importance of different components. subject to  X  u  X  U, q  X  Q, f u,q  X  X  0 , 1 }
Here, we want to help each question get response/feedback and do not allow some questions to be left with no reply.
To maximize the objective function in Equation 5, we construct a concave cost network illustrated in the graphical representation in Figure 2. Conducting max cost flow on this network gives an optimal solution for the integer linear programming problem in Equation 5.

Theorem 1. The cost network constructed in Figure 2 is equivalent to the problem defined in Equation 5.
Proof. First, the maximum concave cost flow (MCCF) problem can be formulated as the following optimization problem: subject to Here, the Flow a  X  b is the flow from node a to b , Cost a  X  b is the concave cost function on the edge. In our network, there are only (negative) square functions, linear functions, and constant functions. Then, if we set f u,q by the flows in the network as below: Then, the objective in MCCF is equal to Equation 5. After that, we consider the remaining constraints one by one. In summary, all constraints in CQR problem are satisfied in the network flow model and these constraints are the only limits in the network. Thus the MCCF problem defined here is equivalent to the CQR problem. Then we can formally build the network as shown in Figure 2, solve the MCCF problem by computing the maximum cost flow of network [2], and get the final recommendation plan by examining flows between u and the corresponding q  X  or q &lt; .
In this section, we present a set of experiments, and a discussion of their results. We start with an introduction to the three MOOC datasets, followed by experiments in our framework and a comparison of their performance against several baselines. We also explore some practical tradeoffs by investigating how the number of relevant edges used in the optimization of the objective function affects the overall recommendation performance.
 Accountable 1,148 511 582,945
Our experiments are conducted on the discussion forums from three different types of courses from the Coursera MOOC platform 4 , (1)  X  X ccountable Talk: Conversation that works X , hereafter shortened to  X  X ccountable X , is a professional development course for teachers; (2)  X  X antasy and Science Fiction: the human mind, our modern world X , shortened to  X  X antasy X , is a course about literature writing and discussion; (3)  X  X earn to Program: The Fundamentals X , shortened to  X  X ython X , teaches Python programming skills. Summary statistics are presented in Table 1. For our purposes, a question is defined as a thread starter post asking a question or requesting help. In our datasets, most posts (around 82%) are about proposing and resolving problems -since the Coursera discussion forums mainly focus on Lectures, Exercises, Assignments and Exams. Thus, these datasets are quite appropriate for evaluating our CQR problem.
In this part, we firstly explain our experimental settings and evaluation metrics. Then we describe and discuss the comparison performance of our model with baselines.
To evaluate the performance of relevance prediction, we randomly divide the observed records into train and test sets, with 70% and 30% of records respectively. The ground truth only contains student participation history, namely whether the students participated in a question discussion( r u,q = 1), denoted as positive samples. To simulate a realistic scenario, we randomly generate negative examples ( r u,q = 0 entries) for each positive instance by selecting threads the user did not participate in. Specifically, for each r u,q = 1, we sample 5 different questions p such that r u,p is not observed, and set r u,p = 0. In the context of question recommendation, the relevance score is binary and we use Mean Average Precision (MAP) [5] (with 95% confidence interval) as the evaluation metric. MAP@K focuses on the first top K questions in the recommended list and we use MAP@1, MAP@3, and MAP@5 for the comparison.
We proposed two baselines to compare the performance with the context-aware matrix factorization CAMF ,in order to validate that we can provide accurate relevance score for constraint filtering modeling. Baseline Popularity is a simple method in which only the popularity of questions is considered. Baseline BasicMF is the plain matrix factorization model without incorporation of contextual features. The comparison results are summarized in Figure 3 From this figure, we can observe that, 1) BasicMF works
Permission to conduct research on Coursera datasets was provided by Coursera. better than Popularity, because it identifies the latent match between student preferences and question properties; 2) CAMF archives the best performance among three models due to consideration of MOOC specific contextual features; 3) The performance of MAP@1 achieves 0 . 513 on the Accountable Course, 0 . 455 on the Fantasy Course, and 0 . on the Python Course, which guarantees quite accurate input for constraint filtering.
We begin the introduction to this part with metrics and baselines of constraint filtering. In what follows, we present the performances of our CQR framework applied to predicted scores from the earlier relevance prediction part; Further exploration on how filtering relevant edges affects the overall community benefit is also discussed.
There is no widely used standard metric to evaluate our CQR problem. For possible comparison, we design three evaluation metrics. Two types of coverage: Student Coverage (SC) and Question Coverage (QC) i.e. how many questions/students are recommended to a student/question on average, respectively. These are used to reflect the load balance statistics of students and questions.
 Overall Community Benefit (OB) stands for the objective function value defined in Equation 5, representing the sum of relevance scores we achieve excluding the loss caused by violation of recommendation constraints. Higher OB means better performance and is our most important evaluation metric. Nevertheless, it is important to consider the three metrics together in order to evaluate model performance, i.e. the community spends SC effort and generates QC activity on thread in order to achieve OB welfare. The student question pairs we use here are all unobserved pairs with relevance scores predicted by CAMF in the first phase. To compare the effectiveness of our CQR framework (with max concave cost flow method denoted as MCCF ), we propose three baselines as below: For the first two baselines, we ignore the Expertise Matching constraint and the lower bound of Load Balance Table 2: Community Benefit on Accountable Course
Table 3: Community Benefit on Fantasy Course
The approach we propose in this paper does not make specific assumptions about the constraints on specific stu-dents X  capabilities or capacity, or specific question difficulty. Rather, we present a parameterized approach that can be used with any desired method for estimating these specific constraints. The metadata about student and question constraints are not explicitly expressed in the forum data we have access to. For the experiments presented, we approximated the implicit capability and capacity of students and the hardness of questions based on available forum data, just as an example of one way these might be forumulated. In particular, the hardness H q of a question q is defined here as the length of that question in terms of number of words. The underlying assumption is that longer questions are more complex, and thus require higher expertise in order to offer effective help. However, we acknowledge the limitations of such an operationalization and do not make any formal claims about its adequecy. The
Table 4: Community Benefit on Python Course upper bound R u of capacity of u is defined as how many questions this student has participated so far in the forum; we set the lower bound L u as 1, since every student needs at least one question to be actively involved. To capture the capability B u of student u , we formulate it as B u = max { H q | q  X  T ( u ) } , i.e. the expertise B u of student the maximum hardness of all questions he/she participated in at earlier time points. Here, T ( u ) is the question set that student u has participated in. Various approximation strategies can be adopted in our framework to function within realistic scenarios. We leave this operationalization and validation work to subsequent research. Figure 5: Why top  X  20% edges are enough for OB
In this section, we compare our max cost flow model with three baselines. Results on three datasets are listed in the Table 2, Table 4, Table 3. Empirically, we set  X  as 0.005 and  X  as 0.001. From the three tables, we can observe that the Student-Question and Question-Student baselines have consistently lower SC, QC and OB compared to the two others. This is because in the first step in both baselines, traditional top-k selection is applied such that many students are directed to a small set of popular questions and a lot of popular questions attract almost all attention of students. Therefore, the coverage is low, which leads to the low overall benefit. That is why we consider constraints here: traditional top-k question recommendation results in considerable waste of resources in its assignment. MCCF achieves an OB value of 2652 on the Fantasy course, 327 higher than the OB of Greedy; the SC and QC of MCCF are lower than Greedy, which indicates that our MCCF achieves a significantly higher overall benefit to the community with less work on average per participant, and less extraneous activity on threads. This conclusion is consistent across three datasets.
As shown above, the max cost flow model achieves better performance in optimizing OB under the constraints of students and questions. To explore how many edges we should reserve in the constraint filtering phase, we vary the percentage of top relevant edges in the network per each student and measure the OB obtained. That is, we use the top x % most relevant questions for each student to run CQR model. The machine we used is and Intel(R) Core i7-4700MQ with CPU @2.40GHz. Each subfigure in Figure 4 illustrates two curves; the upper curve represents how OB changes as the edge ratio varies. The lower curve shows how the running time of the corresponding model changes as the edge ratio varies. Based on this we conclude that the top 20% of relevant edges are sufficient to gain a relatively satisfying OB. For example in Figure 4(a), the OB is 2897 if we use the top 100% edges; but if we only keep the top 20% edges, we achieve an OB of 2894. The two OB metrics has no big difference but the later model only needs 57 seconds, which is only 5% of original running time. The reason why OB is quite low when we use top 10% edges is because there is no feasible solution to the CQR problem under this number of edges.

We also conduct statistics to analyze why around 20% edges are enough to get a satisfying OB. Figure 5(a) presents minimum relevance scores of different top x , and Figure 5(b) gives the statistics of the percentage of edges falling in different relevance score bins. The minimum relevance scores after top 20% edges are almost zero, and relevance scores in the range [0 , 0 . 005] accounts for around 80% of all records. Edges with lower relevance scores contribute less to OB. Based on Figure 4, we conclude that using around top 20% edges for our model can achieve high OB and need fair time cost. Thus, this top edge ratio and the minimum relevance score exploration can help speed up our model.
In this paper, we have formulated the constrained ques-tion recommendation (CQR) problem that simultaneously considers potential load balancing and expertise matching constraints. Our framework employs a novel combination of (1) a context-aware matrix factorization model to predict students X  preferences over questions; (2) and a max cost flow model to optimize community benefit under multiple constraints. Experimental results show that our method is significantly better than proposed baselines. It is worthwhile to mention that we could adopt any state-of-the-art strategies to predict the relevance scores and conduct constraint approximation using them. However, since metadata about constraints including student capability, student capacity and question difficulty are not explicitly expressed, how to accurately approximate such constraints remains a challenge for future work. In the future, we plan to conduct online deployment studies in MOOC platforms by using our constrained question recommendation framework to test its effect in practice. In addition, we will also consider how to generalize this recomemndation with constraints problem to other contexts and address scalability issues with the constraint satisfaction.
 This research was supported by NSF Grant IIS-1320064. The authors want to thank Tianqi Chen, Weinan Zhang, Jingbo Shang, Miaomiao Wen for their helpful discussions and the anonymous reviewers for their valuable comments.
