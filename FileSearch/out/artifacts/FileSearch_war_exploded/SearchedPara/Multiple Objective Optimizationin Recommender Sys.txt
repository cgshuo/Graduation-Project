 We address the problem of optimizing recommender sys-tems for multiple relevance objectives that are not neces-sarily aligned. Specifically, given a recommender system that optimizes for one aspect of relevance, semantic match-ing (as defined by any notion of similarity between source and target of recommendation; usually trained on CTR), we want to enhance the system with additional relevance sig-nals that will increase the utility of the recommender system, but that may simultaneously sacrifice the quality of the se-mantic match. The issue is that semantic matching is only one relevance aspect of the utility function that drives the recommender system, albeit a significant aspect.

In talent recommendation systems, job posters want can-didates who are a good match to the job posted, but also prefer those candidates to be open to new opportunities. Recommender systems that recommend discussion groups must ensure that the groups are relevant to the users X  in-terests, but also need to favor active groups over inactive ones. We refer to these additional relevance signals (job-seeking intent and group activity) as extraneous features , and they account for aspects of the utility function that are not captured by the semantic match (i.e. post-CTR down-stream utilities that reflect engagement: time spent reading, sharing, commenting, etc). We want to include these extra-neous features into the recommendations, but we want to do so while satisfying the following requirements: 1) we do not want to drastically sacrifice the quality of the semantic match, and 2) we want to quantify exactly how the semantic match would be affected as we control the different aspects of the utility function. In this paper, we present an approach that satisfies these requirements.

We frame our approach as a general constrained optimiza-tion problem and suggest ways in which it can be solved efficiently by drawing from recent research on optimizing non-smooth rank metrics for information retrieval. Our ap-proach features the following characteristics: 1) it is model and feature agnostic, 2) it does not require additional la-beled training data to be collected, and 3) it can be easily incorporated into an existing model as an additional stage in the computation pipeline. We validate our approach in a revenue-generating recommender system that ranks billions of candidate recommendations on a daily basis and show that a significant improvement in the utility of the recom-mender system can be achieved with an acceptable and pre-dictable degradation in the semantic match quality of the recommendations.
 H.3.3 [ Information Search and Retrieval ]: Information Filtering Recommender systems, multiple objective optimization
In designing recommender systems, we often have to bal-ance multiple competing objectives. An example scenario can be drawn from a revenue-generating product at LinkedIn called TalentMatch , in which the recommender system, trig-gered by a job posted on the site, scours the entire member database to find the best candidates for the job. Those receiving the recommendations, job posters, want the can-didates recommended to be a good fit for the job, but also prefer that the candidates be open to pursuing new oppor-tunities. More specifically, a job poster would rather be recommended a candidate who is a great match for the job and also happens to be looking to change jobs, than the best match who happens to not be interested in exploring new opportunities. On the other hand, recommending a candi-date who will certainly take the job if the offer was made, but who is not a good match for the job, will negatively affect the experience of the job poster. Therefore, given a ranking of candidates according to how well they match a given job and a ranking of candidates with regards to their job-seeking intent, the challenge is to combine both rankings into a final ranking that is optimal with regards to a given utility function.

In most recommender systems, there is a utility function to be maximized: relevant engagement. In TalentMatch , relevant engagement is a multi-faceted objective: a) the job poster decides to purchase the set of candidate recommenda-tions based on a snippet of information for each of the can-didates (see Figure 1), b) the job poster decides to initiate communication with each of the recommended candidates in the purchased set, and c) each of the candidates contacted respond in a favorable fashion to the job poster.

In the TalentMatch system, the semantic model computes the probability that the feature vector representing the mem-ber and the feature vector representing the job are a good match. The model does this by computing similarities be-tween subsets of the member X  X  feature vector and semanti-cally related subsets of the job X  X  feature vector. The var-ious similarities in this vector are then weighted by train-ing against a given CTR metric using a supervised learning algorithm. We refer to the features used to generate the similarity vector as semantic features , a concrete example being the job description in the job posting and the job de-scription of the member X  X  current position. In this case, the semantic features being compared are explicit, however, it may also be the case that the semantic features are latent (as in matrix factorization approaches to recommender sys-tems). Extraneous features , on the other hand, exist only in the entity being recommended, not in the entity being rec-ommended to. An example of an extraneous feature would be the job-seeking intent.

The job-seeking intent of each candidate is generated us-ing another model, which for purposes of this paper, can be treated as a black-box that takes as input a candidate member and based on that member X  X  data (e.g. activity on the site and profile information), outputs a job-seeking propensity score and a probabilistic assignment to each of the job-seeking intent categories: active, passive, and non-job-seeker . Many members who do not self-identify as job-seekers on the site actually display job-seeking behavior and characteristics. Therefore, we can estimate job-seeking in-tent for every member of the site. Though only a proxy, job-seeking intent is a very good indicator of the likelihood with which a member contacted regarding a job opportu-nity will respond favorably (which is one of the aspects of the utility function in TalentMatch ). The other two aspects of the utility function (purchase rate of candidate recom-mendations and likelihood that a job poster will communi-cate with the purchased recommendations) are accounted for by the semantic model. Intuitively, increasing the number of individuals with high job-seeking intent (those classified as active or passive ) in the top-K recommendations, with-out drastically sacrificing the semantic match of the recom-mendations, should increase the utility of the TalentMatch recommender system by connecting job posters with candi-dates who will engage with them. Figure 2 gives a high-level overview of the relevant system components.

Compounding the issue of multi-faceted objectives is the fact that in live production systems, models often need to evolve in a progressive fashion: the model may have initially optimized for only one aspect of relevant engagement (e.g. purchase rate of candidate recommendations) and it would Figure 2: TalentMatch ranks all members according to their semantic match to a given job. The seman-tic score is combined with extraneous features, e.g. job-seeking intent, in the Multiple Objective Opti-mization (MOO) component, which outputs the fi-nal recommendation ranking. be preferable to improve the model incrementally (as soon as a new feature like job seeking intent becomes available), rather than waiting for a complete redesign and development lifecycle of a new model. The incrementally improved model then bridges the old and the new models, and allows for additional analysis on the performance of the new feature, which may in turn influence how it is incorporated in the new model.

In this paper we describe a general approach for incor-porating extraneous features into a semantic model, which result in the need to optimize for objectives which are not necessarily aligned. More specifically, given a model which outputs recommendations ranked according to some notion of semantic relevance, we want to add certain features which contribute to the overall utility of the users of the recom-mender system, but that may negatively affect the semantic relevance of the recommendations. This approach is model and feature agnostic, does not require additional labeled training data to be collected, and can be easily incorporated into an existing model as an additional stage in the compu-tation pipeline. We validate our approach by A/B testing it on TalentMatch system, which currently ranks billions of job-member pairs on a daily basis, and show that a signifi-cant improvement in the utility of the recommender system ( 42% increase on email reply rate ) can be achieved with an acceptable and predictable degradation in the original rel-evance of the recommendations.
In this section we discuss a general template for framing the kinds of problems we are targeting in this paper and in Section 4.1 we discuss the instantiation of this general template for the specific TalentMatch scenario.

We start with a model which is optimized for semantic rel-evance. We then want to enhance this model with additional features which will increase the utility of the recommender system, but at a potential loss in the semantic relevance of the recommendations. Adding these additional features to the model will result in an enhanced model with additional objectives to be optimized. These additional objectives will be optimized conditionally on the semantic relevance objec-tive having already been optimized.
In the simplest case, we would have only one feature to add to the semantic model, which equates to one additional objective to be increased (adding more features that map to only one additional objective can be handled similarly). We also want to penalize enhanced models in a manner that is correlated with the distance between the semantic relevance score distribution of the items in the top-K ranking as output by the semantic model, and the semantic relevance score distribution of the items in the top-K ranking as output by the enhanced model (note that the enhanced model outputs a ranking based on the enhanced scores, but we need to map those scores back to their semantic counterparts to compare the two distributions). These requirements are expressed by the following loss function: L ( w ) =  X  g ( f ( Y , X ,w )) +  X   X (  X  ( Y ) , X  ( f ( Y , X ,w ))) (1) Where:
Alternatively, it may be easier to visualize the objective as a constrained optimization problem where we have a limit on how much we are allowed to deviate from the top-K score distribution based on the semantic model: Where:
Given the constrained optimization perspective, in the simple case where l = 1 we could analyze how g trends as a function of various values for c , from which we could extract the Pareto frontier [6] and which we could use to make a data-driven decision on what value of c is appropri-ate. Note that if, in the Pareto frontier, g turns out to be a linear function of c , then the slope of the line would be the value of the  X  parameter in Equation 1.
In the case where the additional features with which we will enhance the semantic model lead to multiple additional objectives, we have the following general version of the prob-lem: Where:
The functions g and  X  described in Section 2 are non-smooth since they depend on a ranking which in turn de-pends on a sort operation. Therefore, traditional optimiza-tion approaches which leverage the gradient of a function are not directly applicable.

In very small parameter spaces (one or two parameters), grid (exhaustive) search is an acceptable and very simple to implement computational strategy. For larger parameter spaces, we can devise smoothed approximations to g and  X  that are amenable to traditional gradient-based methods and therefore able to handle parameter spaces where grid search would be unfeasible. In this Section we discuss us-ing such approximations in our problem formulation and in Section 4.2 we discuss the computational strategy followed in the TalentMatch case study.

Recent research on  X  X earning to rank X  for information re-trieval addresses the need to optimize non-smooth rank-based metrics. There are two approaches that are particu-larly interesting in this direction: SoftRank [9] and SmoothRank [3]. These approaches develop smooth approximations to IR metrics such as the Normalized Discounted Cumulative Gain (NDCG) and the Average Precision (AP). We can formulate our g and  X  functions so that they have a similar form to those IR metrics and then we can employ the techniques described in [9, 3] for optimizing them.

For example, we can consider the original semantic rele-vance score from the TalentMatch model to be the ground-truth measure of relevance of each candidate member given a job. In an IR setting, we would have queries and docu-ments, where documents have a measure of relevance to a particular query. In the TalentMatch model, a job is equiv-alent to a query and a candidate member is equivalent to a document .

One possible instantiation of g and  X  would be as follows: assume we do not wish to distinguish between active and passive candidates; we would then have a binary notion of relevance for a given candidate, { job-seeker = 1, non-job-seeker = 0 } that we want to maximize in the top-K results. This is a good match for the AP measure. We then need a constraint function which penalizes how much deviation there is from the original relevance-based ranking. It turns out that an adapted form of the NDCG measure would be appropriate here.

This leaves us with the following smooth approximation to our objective function, using the approximation in equation 9 from [3]: Where:
And the following smooth approximation to our constraint function, using the approximation from equation 8 in [3]: Where: f , which is the enhanced model that perturbs the semantic match score, originally defined in Equation 1, enters Equa-tion 5 through h ik : exp (  X 
Where d ( k ) is the index of the recommendation which was ranked at position k by f .

There are many other ways to formulate our approach us-ing these smoothed approximations. For example, if instead of the job-seeking categories ( active, passive, and non-job-seeker ) we wished to use the job-seeking intent score, we could formulate g using ( A ) NDCG instead of ( A ) AP . Ad-ditionally, if the functional form of f in equation 1 is such that a parameter vector w of 0 in the enhanced model yields the equivalent of the semantic model, an Euclidean norm constraint on w could be used instead of ( A ) NDCG for the  X  function.
We illustrate our approach with the TalentMatch system, where given a job posted on the site, we generate a ranked list of candidates with regards to how well the candidates match the job. This semantic model outputs the probability that the candidate is a good match to the job. We want to enhance this model with the job-seeking intent of the candidate so that the candidates being recommended are both good matches for the job, as well as open to new job opportunities. Our hypothesis is that this will contribute to increased engagement between the job poster and the recommended candidates.

There are many ways to incorporate the job-seeking intent signal into the TalentMatch model. As discussed in section 1, the job-seeking intent model ouputs, for each member, a job-seeking propensity score and a probabilistic assignment to each of the job-seeking intent categories: active, passive, and non-job-seeker . Our objective is to increase the average number of active and passive candidates in the top-K rec-ommendations. We want to achieve this objective by per-turbing slightly the semantic ranking so that if there is a candidate C x with a semantic score of 0.9 in rank 1 who has a low job-seeking intent (classified as a non-job-seeker ), and another candidate, C y , in rank 2 with a match score of 0.88, but that happens to have a high job-seeking intent (classified as active or passive ), then we would like to bump C y up to rank 1 and bump C x down to rank 2. We do not necessar-ily want to eliminate C x from the final ranking, nor do we want to excessively bump the candidate down the ranking. More importantly, we want a systematic way to perform this re-ranking perturbation.

A simple strategy would be to remove from the ranked list based on semantic matching scores all those recommended candidates with a job-seeking intent score below a certain threshold t , backfilling if needed to make sure we have K rec-ommendations (we discuss below how this specific heuristic is a special case of our suggested approach). This approach still requires us to estimate the threshold t , but more cru-cially, it also incurs the risk of completely eliminating high-quality matches from the final ranking, an outcome we do not want.
In order to come up with a strategy for re-ranking that satisfies our requirements, we frame our problem using the template described in section 2. The average number of active and passive candidates in the top-K recommendations is actually an instance of a familiar metric: mean precision at K , where our binary relevance measure is an indicator function that returns 1 if the member is active or passive and 0 otherwise, l i  X  { 0 , 1 } . For a given job posted to the site, precision at K is:
Where 1 { A } is the indicator function applied to A , and 1 { A } = 1 is A is true and 0 otherwise, r ( i ) is the ranking of the i th candidate, and n is the number of candidates in the result set. Our objective to be maximized, the mean precision at K, which maps to the g function in equations 1 and 2 is:
The functional form of f , the enhanced model in equa-tions 1 and 2, can also be specified in a variety of ways. One possible option is to use a a linear combination of the TalentMatch semantic and job-seeking intent scores. This would not be ideal: we want both, good matches and likely to be job-seeking candidates; therefore, a multiplicative fea-ture interaction is what we seek. We settled on the following formulation:
This is equivalent to applying a small boost to the seman-tic match score ( y ), and allowing for the boost to be different for actives (  X  ) and passive (  X  ) candidates. Solving the op-timization problem defined in equations 1 and 2, with the specific functional forms defined here will yield appropriate values for  X  and  X  .

Given our chosen functional for f , it can be seen that the simple heuristic suggested earlier is actually a special case in our approach, where  X  and/or  X  are set to large enough values so as to effectively rank all members with a job-seeking intent score above the threshold t over those members with a score below t . Section 5 discusses how this strategy is suboptimal (it causes an unacceptable loss in semantic relevance).

Finally, we need to specify how we will measure the devi-ation of the enhanced model distribution from the semantic model distribution, that is, the functional form for  X  in equations 1 and 2. There are various histogram distance functions to choose from [2], examples of which include Eu-clidean distance and Kullback-Leibler divergence. We set-tled on using the Euclidean distance between the two his-tograms, or more specifically, the sum of squared errors of the histogram buckets, each histogram having b buckets:
Where H s is the histogram of semantic match scores of the top-K candidates ranked by the semantic match score and H e is the histogram of semantic match scores of the top-K candidates ranked by the enhanced score.
Since we only have two parameters:  X  and  X  , and given our intuition that the optimal parameters will probably lie in the interval [1 . 0 , 2 . 0], a grid search turns out to be an ac-ceptable computational strategy in this scenario. We break up the grid search into 2 runs: a coarse run (to see what re-gion of the search space we should focus on) and a fine run (to zero in on the desired values). In each run we generate all the plans to be tested (a plan being an assignment of values to  X  and  X  ) and evaluate our g and  X  functions for each plan generated. For estimating the  X  and  X  parameters to be used in Equation 9, we created a sample dataset of jobs recently posted to the site and computed a maximum of 9000 rec-ommendations for those jobs using the TalentMatch model. We filtered all recommendations with a threshold of 0.6 on the TalentMatch semantic score, and then removed all jobs which did not have at least 6 recommendations (we do not show results on the site unless there are at least 6 relevant matches and we include only the top-24 candidates in the recommendation set). This left us with a total of 760 jobs, each with anywhere from 6 recommendations to 9000 rec-ommendations. We then generated the plans as per Section 4.2 and evaluated our g and  X  functions.

Our g function is the mean precision at K , as defined in Equation 8, where K = 12 since that is how many snip-pets of candidate recommendations we show in a single page. Figure 3: Multiple Objective Optimization trade-off between the objective being maximized and the penalty incurred. Up until a histogram divergence of a little over 60, the relationship is strongly linear (R 2 = 0 . 985 ) on the Pareto front, with a slope of 0.076.
 Table 1: Sample plans from figure 3. Plan 1 is the original plan, where the relevance score of active and passive candidates is not boosted.
 Also, for the measure of divergence, our  X  function, we com-pared the distribution of the minimum score of the top-12 ranking, given that we want to ensure relevant recommen-dations in the worst case on the first results page.
Figure 3 shows the result of the fine grid search run, which illustrates the risk-reward trade-off in our experiment: up until a histogram divergence of a little over 60, we pay a penalty that is linear with regards to the increase in the average number of active and passive members in the top-12 result set. Table 1 shows a few of the points used in the plot, including the original plan (equivalent to setting  X  and  X  to 1.0), which also indicates the average number of active and passive candidates in the top-12 result set of the original plan to be nearly 4. Figure 3 tells us that we can double that number if we are willing to pay a penalty of about 64 in the histogram divergence, and also tells us what to set  X  and  X  to: 1.15 (see table 1). Figures 4(a)-4(d) give an idea of how good/bad a histogram divergence of 64 is. For reference, as per table 1, setting  X  to 1.3 and  X  to 1.0 causes an unacceptable loss in relevance (the histogram divergence is too high and the gain in the objective does not justify it).

All of the plans on the Pareto front in figure 3 have similar coefficients for  X  and  X  , which is tied to the fact that the goal we are trying to maximize is the combined number of active/passive candidates in the top-12, and presumably the values for the weights would diverge more had we favored one category or the other. These results point to reason-able strategies that should be evaluated using A/B testing: a plan where  X  =  X  = 1 . 07 and a plan where  X  =  X  = 1 . 15. A/B testing turns out to be a crucial component to the methodology described in this paper. Our approach pro-vides the tools for generating reasonable values for  X  and  X  : no matter what the desired risk-reward trade-off of a specific application, only plans in the Pareto frontier should be cho-sen. However, our choice for what is an acceptable histogram divergence will only be meaningful if once in production, the rate with which job posters purchase candidate set recom-mendations and the rate with which job posters contact the purchased recommended candidates does not decrease sub-stantially.

As mentioned in Section 1, we would like to increase the likelihood of relevant engagement for TalentMatch . If the job-seeking intent is to be of any use to us, we would expect the rate of replies to InMails (LinkedIn e-mails) from job posters about job opportunities to be higher for members with high job-seeking intent. We determined that members classified as having a high job-seeking intent ( actives/passives ) are 16  X  more likely to reply to an InMail regarding a career opportunity, with a 95% confidence interval of 15-17x (in-tervals computed by the method of E. C. Fieller [4]). These numbers are based on InMail activity that took place over a period of 10 days, during which time the number of non-job-seekers contacted was nearly the same as the number of actives/passives contacted.

Assuming that all members in the top-12 ranking are con-tacted about the job opportunity, our results suggest that we can double the desired relevant engagement. Given that the probability of positively replying for non-job-seekers is p ( reply ) = 0 . 028, and the probability of replying for ac-tives/passives is p a/p ( reply ) = 0 . 45 (as computed using the 10-day sample), and given that our analysis shows that we can double the average number of actives/passives in the top-12 from 4 to 8 at an acceptable relevance loss, we ex-pect to double the expected relevant engagement: 0 . 028  X  8 + 0 . 45  X  4  X  2 versus 0 . 028  X  4 + 0 . 45  X  8  X  4.
We deployed the A/B test experiment and let it run for a couple of weeks before we started collecting data for analysis (until the  X  X ovelty effect X  often caused by a new feature be-ing deployed live had subsided). To measure the change in InMail reply rate, we looked at all emails created in a period of three weeks and observed how many were replied to. Ta-ble 2 shows the increase in response rate for each A/B test treatment bucket along with their confidence intervals. The actual increase follows the expected linear relationship as expected from the Pareto frontier, and though there is high variance, the 95% confidence intervals contain the expected values.

We now turn our attention to the effect of the re-ranking perturbation on the booking rate. More specifically, we want to quantify the effect of the histogram divergence. For this, during the same time period, we looked at the booking rate in each of A/B test buckets. Table 3 summarizes our find-ings. It shows a slight degradation in booking rate for the most extreme treatment bucket. However, looking at the Table 2: InMail response rate ratio of treatment group over control group. Data collected over a period of three weeks around May/27/2012-June/17/2012.
 rate witch which job posters email candidates in the pur-chased set ( InMails per booking) shows a different story.
Table 4 shows that on average, job posters choose to email candidates more often in the treatment group than in the control group. This is evidence that job posters do not email all candidates in the purchased set, but rather pick and choose who they will contact. This fact explains why the actual average increase in InMail response rate was not as high as expected: we had assumed that job posters con-tact all of the candidates in the result set, but this turns out not to be the case. The control group shows that job posters only contact an average of approximately 2 members from the purchased candidate result set. More importantly, since job posters do not have access to the job-seeking in-tent of each candidate (nor to most of the information we use to determine job-seeking intent, such as job searches), there must be something else in the candidate X  X  profile which compels job posters to email candidates we X  X e identified as job-seekers more often than those we have not. Perhaps job-seeking candidates have more complete or more curated pro-files. Nevertheless, this means that the snippet we show job posters, based on which they make the decision of whether or not to purchase, is not well representative of the value of the candidate result set. This finding is something that we plan on exploring further, as it suggests that, given the right snippet (one which better conveys the value of the can-didate result set to the job poster), the booking rate for the treatment groups should be higher than for the control group . Table 3: Booking rate ratio of treatment group over control group. Data collected over a period of three weeks around May/27/2012-June/17/2012.
 Table 4: Inmails/booking ratio of treatment group over control group. Data collected over a period of three weeks around May/27/2012-June/17/2012.

An area we would like to expand this work is in incor-porating tighter bounds into the additional objectives. For example, we were able to say that we could double the aver-age number of candidates with high job-seeking intent into the top-12 TalentMatch ranking, but we have not been spe-cific enough regarding the variance of that average, which was evident in our results.
In [1], the authors present an approach to optimize jointly for clicks and post-click downstream utilities (time-spent, revenue, etc). Similar to this paper, they propose a multi-objective programming approach in which multiple objec-tives are modeled in a constrained optimization framework. However, our approach is different: we want to optimize for downstream utilities given that the recommendations are of high-quality, whereas they manage the trade-off by showing a portion of their customers results optimized for relevance, while another portion of their customers are shown results optimized for another downstream utility measure. Their approach would not be suitable for a system like Talent-Match , since that would mean that some job-posters would see a ranking of candidates which is optimized for seman-tic relevance (which would not increase the desired relevant engagement), while other job posters would see a ranking of candidates based on the job-seeking intent score (result-ing in an excessive loss in relevance which in turn would mean a decrease in our desired relevant engagement, since job-posters would most likely not purchase or contact the candidates provided).

Another related approach is presented in [5], where the authors get much closer to our requirement of explicitly con-trolling any potential loss in relevance as a result of optimiz-ing for other utility aspects. We expand on their formulation in that we consider objective and constraint functions which can be nonlinear. So, if the system performance will be measured with metrics such as NDCG@K and Precision@K, then the system can be optimized precisely on those metrics.
Researchers from the field of Information Retrieval have also considered the problem of  X  X earning to Rank X  with mul-tiple objective functions [8, 7]. In [8], the authors show how multiple measures can be combined into what they refer to as a graded measure that can be learned. This notion of a graded measure allows them to formalize some sources of relevance labels as being more important than others, and so once an ordering is imposed, they can optimize for a given metric provided all other more important metrics have al-ready been optimized. The authors experiment with their suggested approach in the field of IR, where the typical no-tions of relevance have a low cardinality, a common grading scheme being: { perfect, excellent, good, fair, bad } . With this coarse notion of relevance, collisions or ties are likely to occur in a given ranking, and so it seems there is more room to optimize on a secondary metric if a primary one has al-ready been optimized. In the TalentMatch system, however, relevance is determined as the probability that a given can-didate is a good match for the specified job (the model does indeed allow for a very fine granularity in the relevance mea-sure), and unless the scores or probabilities can somehow be discretized, collisions or ties are unlikely to occur, leaving little room for optimizing for a secondary label source such as job-seeking intent.

In [7], the authors do not have a predefined graded mea-sure , but instead learn the relative importance of each la-bel source from data, which implies having to collect addi-tional labelled data, which is a potentially costly and time-consuming proposition.
We describe an approach for framing multiple objective optimization problems in recommender systems. The ap-proach allows for fine control over any potential loss in rel-evance as additional aspects of the system X  X  overall utility function are optimized. We illustrate the approach with a detailed analysis of how it was used to improve TalentMatch , a revenue-generating product which ranks billions of recom-mendations on a daily basis at LinkedIn. We show that we increased relevant engagement with an acceptable and predictable degradation in the relevance of the recommen-dations. [1] D. Agarwal, B.-C. Chen, P. Elango, and X. Wang. Click [2] S.-H. Cha. Comprehensive survey on distance/similarity [3] O. Chapelle and M. Wu. Gradient descent optimization [4] E. C. Fieller. Some problems in interval estimation. [5] T. Jambor and J. Wang. Optimizing multiple [6] Y. Jin and B. Sendhoff. Pareto-based multiobjective [7] C. Kang, X. Wang, Y. Chang, and B. Tseng. Learning [8] K. M. Svore, M. N. Volkovs, and C. J. Burges. Learning [9] M. J. Taylor, J. Guiver, S. Robertson, and T. Minka.
