 Yiteng Zhai YZHAI 1@ NTU . EDU . SG Mingkui Tan TANM 0097@ NTU . EDU . SG Ivor W. Tsang I VOR T SANG @ NTU . EDU . SG Many real-world datasets in text and digital media do-mains are typically represented with very high dimensional features, bringing significant challenges in data mining. Learning performance is often degraded with inflating of dimensions, leading to the well-known notion of  X  X urse of dimensionality X . This problem becomes particularly crit-ical when the number of informative features is relatively small, but involved with a vast variety of irrelevant features and redundant features (Yu &amp; Liu, 2004).
 To address this issue, a plethora of feature selection meth-ods have been developed in the recent decades. In general, these methods have been categorized as three core themes (Guyon, 2008): filter methods (Yu &amp; Liu, 2003; Peng et al., 2005), wrapper methods (Guyon &amp; Elisseeff, 2003; Zhu et al., 2007) and embedded methods (Yuan et al., 2011; Tan et al., 2010; Mao &amp; Tsang, 2011). Specifically, fil-ter methods select informative features based on their in-dividual discriminative power or correlation criterion. The benefits of filter methods lie in their low computational re-quirements. The drawback however is that it may not iden-tify the optimal feature subset suitable for the predictive model of interest. On the contrary, since wrapper meth-ods, such as SVM-RFE (Guyon &amp; Elisseeff, 2003), se-lect the discriminative features solely based on the induc-tive learning rule, they typically exhibit higher predictive performance but at the expense of a lower computational efficiency on large scale and very high dimensional prob-lems. Embedded methods refer to approaches that directly optimize some regularized risk function w.r.t. two sets of parameters: parameter of the learning machine, and param-eter to control the feature sparsity (Guyon, 2008). As such they are usually more efficient than wrapper methods. To date, it is worth noting that most methods in all three cat-egories generally assume a good feature subset has strong prediction ability pertaining to the output labels; mean-while the selected features should also maintain low cor-relations. In other words, correlated features are deemed as redundant, and this redundancy should be minimized (Hall, 1999; Guyon, 2008; Zhao et al., 2011; 2012). Though elim-inating redundant features has been widely used in practice and regarded as the guiding principle behind the develop-ment of feature selection methods, it may not always hold since these correlated features can be useful and informa-tive for the tasks on hand. As also discussed in (O X  X ullivan et al., 2000; Caruana &amp; de Sa, 2003; Xu et al., 2012), such feature redundancy has the benefits of bringing about stable generalization performances. Here, we present an illustrat-ing example using Figure 1. Despite a perfect prediction by the classifier on the male and female face images based on the optimal features identified (denoted here as support features ), these uncorrelated features which are depicted by the white pixels in the middle of Figure 1, can be ob-served to be sparsely spread across the entire image, thus giving little cue to assist the human user in the interpreta-tion of the features. The white pixels shown in the right col-umn of Figure 1, denoting the highly correlated features, on the other hand, are much more informative. To be precise, it is easy for the human user to spot the core group features in the regions of the mustache and beard, which can be help-ful to the users in grasping a better understanding on the critical objects in the images so that further analysis can be made. On the contrary, existing feature selection methods usually eliminate these correlated features and treat them as pure redundancies. Further, the act of identifying these correlated features from high dimensions is typically very compute intensive.
 Taking this cue, in this paper, we introduce an efficient fea-ture selection method that can identify the optimal feature subset to the output labels; while minimizing the correla-tion among the selected features. Each selected feature is defined as Support Feature ; while the correlated features associate with this support feature are denoted as Affiliated Features , and are discovered during the feature selection process without any additional cost. The core contributions of this paper are listed as follows: 1. We develop an efficient Correlation Redundancy 2. For each support feature, the respective associated af-3. Empirical studies on both synthetic and real-world ex-In this paper, we denote a data point by x i  X  R n and the dataset by X = [ x 1 ,..., x n ] = [ f 0 1 ,..., f 0 m ] where f j represents a row vector corresponding to the j th feature of the data points in X . Each x i is associated with an output y i  X  X  X  1 } . We also define y as the vector of the label for the data. Symbols 0 and 1 are the column vectors with all zeros and all ones, respectively. For any vector f , we denote the mean and standard deviation of the entries in f as  X  f and  X  f , respectively. Additionally, we denote the element wise product between two matrices A and B by A B . Finally, we denote |S| to be the size of a set S . As previously stated, feature correlation is of particular in-terest in our present research. In the past decades, a large branch of feature selection approaches have targeted on re-ducing the redundancy among the selected features. The notion of feature redundancy is usually measured by means of feature correlation. The major motivation has been to find the optimal or minimized feature subset corresponding to the output labels. Thus, when a feature is selected, other features that are highly related to this feature is typically rejected so as to minimize feature redundancies. 2.1. Feature Correlation Measures To date, various criteria have been introduced for defining the correlation between features. For instance, a widely used correlation criterion is the Pearson X  X  correlation co-efficient (PCC), which measures the linear correlation be-tween variables. In more details, given two feature vectors f and f k , the metric  X  in PCC can be defined as follows, Notice that  X  is a symmetrical measure that ranges in [  X  1 , 1] . If two variables are fully independent,  X  = 0 . On the other hand, when the two variables are completely cor-related to each other, namely, one variable can exactly pre-dict another variable, we have |  X  | = 1 . Other metrics such as information gain IG ( f j | f k ) and symmetrical uncertainty SU ( f j | f k ) have also been discussed in (Yu &amp; Liu, 2003). 2.2. Feature Redundancy Reduction Based on these correlation measures, several methods have attempted to reduce the redundancy among the selected features. For instance, in Fast Correlation Based Filter ( FCBF ) (Yu &amp; Liu, 2003), feature importance and feature correlation are assessed by means of SU measure. FCBF first selects a set of predominant features that are relevant to the output labels. Subsequently, the informative features to labels are kept while the correlated features are removed based on some elegantly designed intuitive rules (Yu &amp; Liu, 2003). Another notable redundancy reduction method is Minimum Redundancy Maximum Relevance ( mRMR ) (Peng et al., 2005), which selects the most correlated fea-tures to the labels such that they are mutually far apart from each other by maximizing the dependency between the joint distribution of the selected features and the output labels.
 Zhou et al. (2010) proposed Redundancy Constrained Feature Selection ( RCFS ), which first performs feature clustering by using some distance measures (e.g., 1  X  |  X  ( f j , f k ) | ). Hence the correlated features may be grouped into several clusters. After that, some features are then identified from each cluster. Next, a feature subset is further identified from the selected features in each clus-ter group using graph based feature selection criteria (Nie et al., 2008) that capture the global and local intrinsic struc-tures of the data. This strategy however is heavily sensitive to the choice of graph Laplacian matrices used. For ex-ample, the Laplacian score is usually constructed using K nearest neighbor ( K NN). In practice, on very high dimen-sional problems, K NNs can be very far away from each other in reality due to the effect of the curse of dimen-sionality. Besides, the high computational cost of feature clustering on high dimensional data and graph based meth-ods (taking O ( n 2 m ) ) make this approach less attractive on large scale data. Recently, Zhao et al. (2012) proposed a framework to unify different criteria for removing fea-ture redundancies. Nevertheless, existing methods have re-mained to focus on reducing these redundancies. More im-portantly, to date the discovery of correlated yet informa-tive features has been relatively unexplored. In this section, we present an efficient automatic feature grouping method, which identifies groups of discrimina-tive yet correlated features. Similar to (Guyon, 2008), a vector  X  = [  X  1 ,..., X  m ] 0  X  { 0 , 1 } m is introduced to indi-cate whether the corresponding feature is selected (  X  j = 1 ) or not (  X  j = 0 ), such that the decision function is defined as: f ( x ) = w 0 ( x  X  ) , where w = [ w 1 ,  X  X  X  ,w m ] 0 weight vector. To limit the number of selected features to be less than B , the ` 0 constraint k  X  k 0  X  B is imposed for the purpose of feature selection (Nie et al., 2008). 3.1. Correlation Constraints To control the correlation among the selected features, we explicitly introduce the following constraint on  X  such that, which states that any selected feature pair should not be correlated so long as their coefficient defined in (1) does not exceeds ( 1  X   X  ) where  X   X  (0 , 1) . We also define  X  = {  X  | P m j =1  X  j  X  B ;  X  j  X  { 0 , 1 } ,  X  j = 1 ,  X  X  X  ,m ;  X  defines O ( m 2 ) quadratic constraints with m integer vari-ables, thus finding the solution  X   X   X  involves combina-torial subset selection, resulting in high computational cost especially when the dimension m is high. 3.2. Proposed Formulation Here, we aim to find a large margin decision function f ( x ) for robust prediction, and seamlessly identify the informa-tive yet uncorrelated feature subset that satisfies the con-straint in (2). For the sake of simplicity, we use the square hinge loss in SVM, and arrive at the following problem: where  X  i  X  0 is the slack variable,  X / k w k denotes the mar-gin and C is a tradeoff parameter to regulate the function complexity k w k 2 2 and the training error (  X  i  X  X ). Note, as dis-cussed in Section 3.1, the optimization problem in (3) with the constraints defined in  X  is very challenging. 3.3. Cutting Plane Algorithm To tackle this, we first transform the inner minimization in (3) w.r.t. w , X , X  i into the dual of SVM, then (3) becomes a minimax saddle-point problem. Inspired by (Tan et al., 2010), by applying the minimax optimization theory, one can obtain a tight convex relaxation to (3), which is in the form of the following Quadratically Constrained Quadratic Programming (QCQP) problem:  X  = [  X  1 ,..., X  n ] 0 is the vector of dual variables, A = {  X  P n i =1  X  i = 1 , X  i  X  0 ,  X  i = 1 ,  X  X  X  ,n } is the do-main of  X  , and  X  is the upper bound of g  X  (  X  ) . Never-theless, since there are as many as ( P B i =0 m i ) quadratic constraints in (4), it remains computationally expensive to solve (4). Rather than solving the original problem with a large collection of constraints, the cutting plane strategy (Mutapcic &amp; Boyd, 2009) can be employed to iteratively generate a set of active constraints and then solve this re-duced optimization problem with the current constraint set. Since max  X   X   X  g  X  (  X  )  X  g  X  t (  X  ) ,  X   X  t  X   X  , with a re-duced active constraint set C  X   X  , the lower bound ap-proximation of (4) can be obtained by max  X   X   X  g  X  (  X  )  X  max t =1 ,...,T g  X  t (  X  ) with T = |C| , where T is the maxi-mum number of constraints that will be added. This leads to solving a reduced problem of (4) as follows, The details to solve (5) are outlined in Algorithm 1, where some notations will be explained later. Specifically, at each Algorithm 1 Group Discovery Machine Algorithm 2 CRM ( X , y ,B, X ,  X  , S , Q ) iteration of Algorithm 1, one needs to solve the worst case analysis (the same as finding the most violated constraint  X  ) of Problem (4), which shall be described in Section 3.4. Subsequently, the obtained  X  t would be appended into the active constraint set C . Finally, the problem w.r.t. a re-duced active constraint set C can be solved by some effi-cient QCQP solvers (Tan et al., 2010).
 To summarize, the cutting plane algorithm generally con-verges to a robust optimal solution within tens of iterations with the exact worst case analysis and shows good perfor-mance in many real applications (Mutapcic &amp; Boyd, 2009). 3.4. Correlation Redundancy Matching In this subsection, we discuss the worst case analysis of problem (4) (i.e., equivalent to finding the most violated constraints), which plays the key role in cutting plane algo-rithms (Mutapcic &amp; Boyd, 2009). In our setting, this trans-late to solving the following integer optimization problem: In general, solving this problem is NP hard. However, since  X   X  = [  X  1 y 1 ,..., X  n y n ] 0 , this indicates that the informative features accord with the features with the highest | c j |  X  X . In addition, based on this observation, the following proposi-tion 1 will further show that for a set of correlated features, if one of them is informative to the output labels, all of them can be deem as informative to the output labels as well. Proposition 1. Given a nonzero column vector  X   X  , and any two feature vectors f 1 and f 2 that  X  f 1 =  X  f 2 = 1 / and  X  f 1 =  X  f 2 = 0 . Suppose |  X  ( f 1 , f 2 ) |  X  1  X   X  , then || f 1  X   X  | X  X  f 2  X   X  || X  Proof. With |  X  ( f 1 , f 2 ) |  X  1  X   X  , using (1), we have |  X  ( f 1 , f 2 ) | = | f 1 f 0 2 |  X  1  X   X  , namely, f 1 (positive correlation) or f 1 f 0 2  X   X   X  1 (negative correla-tion). Suppose f 1 and f 2 are positive correlated, we have k f 1  X  f 2 k 2 = k f 1 k 2 + k f 2 k 2  X  2 f 1 f 0 2 = 2(1  X  f as k f 1 k 2 = k f 2 k 2 = 1 when  X  2 f |k f 2  X   X   X  0 k 2  X  X  f 1  X   X   X  0 k 2 | = | 2( f 1  X  f 2 f k X  2 Hence, we have || f 1  X   X  | X  X  f 2  X   X  ||  X  of negative correlation, we define a positive correlated vec-tor  X  f 2 =  X  f 2 and the proof follows the derivation of the positive correlation case, we complete the proof.
 The above results state that if two feature vectors f 1 and f are highly correlated, their distance (or correlation) to any exemplar vector  X   X  0 will be very similar to one other. Then a natural question arises, considering the correlated features, which feature poses greater importance to the output labels for a given  X   X  ? To address this question, we first offer the definitions of Support Features and Affiliated Features . In particular, support features refer to informative features with relatively low correlations. affiliated features, on the other hand, refer to the correlated features associated with each support feature.
 Definition 1. Support and Affiliated features: Given any exemplar vector  X   X   X  R n and a collection of feature vec-tors { f i } , where f 0 i  X  R n . The support feature is given by max i | f i  X   X  | for the given  X   X  . The remaining correlated features in { f j } w.r.t. f i denote the affiliated features. For the sake of conciseness, we let S be the index set of the support features and introduce a data structure Q = {G i } to represent the hierarchical structure of features, where G denotes the index set of the affiliated features for the i support feature. Notice that the support feature is corre-lated to itself, we have S  X  Q . By taking this scheme, we can keep all the correlated features rather than omitting them. Based on these definitions, once a support feature is identified (i.e., the feature with the largest c j ), all relevant features that correlate with this support feature will form the corresponding affiliated feature group or cluster. Since the proposed method can discover the correlated feature groups, we name it Group Discovery Machine (GDM) . Note that, alternatively, one could use a brute-force ap-proach to search across all features and identify all features that are correlated to the support feature as the affiliated features to achieve the same goal. However, such a strategy can be computationally infeasible. Fortunately, we show in what follows a theorem to illustrate that in practice one can address this problem by scanning only a small subset of the features on very high dimensional problems.
 Theorem 1. Given a nonzero column vector  X   X  and any two feature vectors f j and f k that  X  f j =  X  f k = 0 and  X   X  f will satisfy | c k | = | f k  X   X  | X | c j | X  Proof. From Proposition 1, we know that || f j  X   X  | X  X  f  X   X  so | f j  X   X  | X | f k  X   X  | , we have | f k  X   X  | X | f j completes the proof.
 The above theorem says that if two features are highly cor-related, their scores will be very close. In other words, for a given support feature f j , the feature with a score lower than | c j | X  feature to f j , at the correlation level of ( 1  X   X  ). Based on this, the worst case analysis can be conducted in Algorithm 2, which is termed here as Correlation Redundancy Match-ing (CRM). The basic idea of CRM is that, for every iter-ation, we first find the support features with larger feature scores. Once the support features are identified, the cor-responding affiliated features are then identified from the remaining unselected features. The whole procedure is re-peated until a maximum of B numbers of support features (see (2)) is selected. Thus, the computational cost of this worst case analysis can be substantially reduced, which we will go through details in the upcoming subsection. Proposition 2. With the Correlation Redundancy Match-ing algorithm, Problem (6) can be globally solved. Proof. From Algorithm 2, once a support feature f z is iden-tified, all corresponding correlated features of f z (features with scores ( | c h | &gt; | c z | X  stored in Q . This fact also implies that all the subsequently selected support features will not be correlated to any of the previously selected support feature. Finally, the top scoring features that satisfy the constraint in (2) then form the sup-port features, hence max  X  , f will be maximized. Inductively, it becomes possible to con-clude that the proposed CRM algorithm can solve (6) glob-ally and exactly. This completes the proof.
 The following theorem indicates that the proposed al-gorithm can globally converge and exhibits the non-monotonic property for feature selection.
 Theorem 2. Given that in each iteration of Algorithm 1, the reduced minimax subproblem (5) and the most active constraint selection (6) can be exactly and globally solved, Algorithm 1 stops after a finite number of iterations with a global solution of (4).
 The proof can be adapted from (Tan et al., 2010). 3.5. Complexity Analysis As finding the most violated  X  can be obtained exactly via the Correlation Redundancy Matching algorithm, which firstly sorts the m features, followed by scanning of the B features and then computes the PCC w.r.t. the other fea-tures. Here, sorting takes O ( m log m ) and finding the sup-port and affiliated features consumes O ( Bmn ) . Therefore, with T iterations on hand, the overall time complexity for CRM is O ( T ( m log m + Bmn )) . As there are at most TB selected features, the training time complexity to solve (5) is O ( TBn ) . Note, by taking benefits from the cutting plane strategy, a relative small T is needed for convergence: as it is noted to converge well within 10 iterations in the experi-mental studies. In this section, we conduct experiments to study the feature selection performances of several state-of-the-art meth-ods, including: 1) ReliefF (Robnik-Sikonja &amp; Kononenko, 2003), 2) mRMR 2 (Peng et al., 2005), 3) FCBF 3 (Yu &amp; Liu, 2003), 4) RCFS (Zhou et al., 2010), 5) SVM-RFE (Guyon &amp; Elisseeff, 2003), 6) L1-SVM 4 (Yuan et al., 2011), 7) FGM 5 (Tan et al., 2010), and 8) our proposed GDM using only support features for prediction. The first four algorithms belong to filter methods. SVM-RFE is a wrapper method, while the last three are embedded methods. For fair comparisons, all methods except Reli-efF , which is integrated in MATLAB R2011b, are imple-mented in C++ with MATLAB interface. Moreover, the parameters of these methods are configured as suggested by the respective authors. For ReliefF , we report the best results of with K  X  { 1 , 2 , 3 ,..., 100 } ( K NN classifier). Further, C is configured to 1 for all methods except L1-SVM , where C varies with different number of selected features. In the experimental study, we set our  X  = 0 . 25 and consider 10, 20, . . . , 200 features for each method and report the corresponding resultant training time. Therefore, to facilitate a fair comparison, standard SVM classifier is used to judge the accuracy according to the number of se-lected features and their indexes. Further, all experiments are conducted on a PC with Intel r Core TM i7 Processor and 24.0GB memory under Windows Server r 2008.
 To evaluate the feature selection performances, three cri-teria, namely, 1) Classification Accuracy , 2) Training Time Complexity and 3) Redundancy Rate are consid-ered. Following the consistent definition of (Zhao et al., 2012), and assuming F is the set of selected feature sub-set with size  X  m , the redundancy rate can be measured by assesses the averaged correlation among all selected fea-ture pairs. Obviously, for the same accuracy level, a smaller redundancy rate is deem to be more meaningful. 4.1. Evaluation on Synthetic Data To illustrate the mechanisms of the proposed method, we first conduct a study on a synthetic dataset where the ground truth correlated features are known in advance. The data contains 2,048 observations and 10,000 features, and the predefined predictive features are categorized as 200 feature groups of different sizes, others then serve as noise. Moreover, each of the 30 out of 200 feature groups contains some inner highly correlated features. While in the remain-ing 170 groups, there is only one feature per group. The predictive ability of each group follows a normal distribu-tion N (0 , 1) . Noticed that the way of constructing testing set is consistent with training. Our goal is to assess whether a method can correctly identify the relevant members of the ground truth feature set. Nevertheless, for this synthetic data, when the correlated features are truly redundant in the data, removing it would lead the classifier to achieve high accuracy performances. To this end, we manually remove the redundant features directly from the data, (i.e., based on the ground truth available) to solve (6). So that correlation constraint on  X  ( f i , f j ) has been imposed. Then we name it as  X  X ethodA X .
 The experimental results obtained from the synthetic dataset are presented in Figure 2, wherein dash line rep-resented the  X  X ethodA X . As expected, we observe from Figure 2(a) that the proposed method outperforms oth-ers in most cases; while filter methods such as ReliefF and mRMR achieve the worst prediction performances. Among the filter methods considered, RCFS achieve the highest accuracy since the use of feature clustering helped remove redundancy among features. However, since filter method such as RCFS does not take the classifier into con-sideration in the feature selection process, the classifica-tion accuracy is generally lower than the wrapper and em-bedded counterparts as expected. FGM outperforms L1-SVM but is noted to be competitive to SVM-RFE when small portion of the features are selected. However, due to the non-convexity optimization formulation of SVM-RFE , which suffers from correlated features, it underperforms FGM when large amount features are considered. More-over, as FGM imposes a tight convex approximation in the ` 0 -model (Tan et al., 2010), it can be observed from Figure 2(a) that both FGM and GDM achieve competi-tive accuracy result when the number of selected features approaches the ground truth. Thus, it is possible to con-clude that the potential of the proposed method can cor-rectly identify the ground truth feature groups.
 The training time incurred by all the methods considered is also reported in Figure 2(b). It can be observed that FGM emerges as the fastest among all, while ReliefF in-curs the highest training effort. Due to the high cost of feature clustering involving 10,000 features, RCFS also consumes significant training time. Though the proposed GDM incurs slightly more time than FGM , it is noted to be more efficient than the state-of-the-art L1-SVM . In addi-tion, the results on metric Redundancy Rate are depicted in Figure 2(c). Overall, GDM achieves competitive low re-dundancy rate and superior accuracy performance on the synthetic problem considered. 4.2. Evaluation on Real-world Data To assess the practical performance of all feature selec-tion methods, we include a variety of datasets, in both data scale and dimension, which consist of two digit recogni-tion datasets: mnist 7 , usps 6 and two other very high di-mensional datasets. The first one is the challenge dataset kdd2010 7 used in the educational data mining competi-tion and the second is the spam webpage data webspam 7 . Detailed information about the datasets is listed in Table 1. For kdd2010 , we use 10,000 points as the training set and maintain the original testing set. On the webspam data, we randomly select 80,000 points as the training set, while 70,000 for testing. To provide further evidence on the ro-bustness performance of each method, we set 50% as the minimum for accuracy expectation and 1 hour as the max-imum training time for all the experiments.
 Figure 3 summarizes the accuracy performance attained by the various methods. By appropriately identifying the sup-port feature set, GDM obtains superb accuracy improve-ments on mnist , kdd2010 , and webspam . On the other hand, SVM-RFE performs well only for small number of selected features while deteriorating hereafter as observed in Figure 3(b). When the correlated features are retained to form the affiliated feature groups, the performance of GDM-affiliated is noted to emerge as superior to all the other methods considered. In particular, the results on digit identification in Figure 6 highlights the significance of affiliated feature set, where a comprehensive explanation shall follows later in the discussion and conclusion section. Further, as shown in Figure 5, GDM achieves relatively low redundancy rate in most cases. Although, FCBF and SVM-RFE exhibits lower redundancy rate than GDM on the mnist dataset, the accuracies have been impeded by their high sensitivity to the noise features, as observed from Figure 3(a). Compared to embedded methods, GDM out-performs over FGM and L1-SVM in terms of redundancy reduction, thus exhibiting the improved accuracy. This also implies that GDM can identify a good feature subset (Hall, 1999). Since the kdd2010 data is sparse, Figure 4 shows that L1-SVM takes advantage of the sparseness in the data to achieve the shortest training time observed. However, on the webspam data which has more than 8 million features, GDM is noted to learn faster than L1-SVM .
 Moreover, there exists the situation that other methods fail in handling the data. For example, RCFS is sensitive to the number of points, thus even on the small mnist dataset with only 11,982 points, the method fails to perform well. SVM-RFE could not maintain an average accuracy of 50% on kdd2010 . Both ReliefF and SVM-RFE fail to con-verge on webspam under the 1 hour maximum training time budget. Moreover, since no filter methods can handle very high dimensional data of webspam and kdd2010 , the contests only hold among L1-SVM , FGM and GDM . In this paper, we have presented a comprehensive study on potential correlated features, leading to the concepts of support feature and affiliated feature. While, superior pre-diction performance is attained through support features, maintaining some feature redundancies as affiliated fea-tures, can be useful for enhanced interpretation of the learn-ing tasks while improving prediction robustness. By taking advantage of the cutting plane strategy, the proposed GDM can handle very high dimensional problems in an efficient way. Notably, the affiliated features are constructed in the proposed method without any additional cost, since they are generated along with the support features.
 In what follows, we conclude with further details on the interpretation of the proposed GDM algorithm along with the affiliated features attained in Figure 6. With respect to the digit identification result of usps 8 , the regions high-lighted by the affiliated features (129 features) can be use-ful in assisting the human user in identifying a  X 0 X  or  X 1 X  from the extracted images of the original pictures. This is consistent to the observation discussed earlier in Figure 1, where the feature groups congregate in the regions of the beard, mustache and silhouette of the face to form the af-filiated feature groups. Information with great significance are reserved for further processing. Referring to the af-filiated features in Figure 6, despite the highest accuracy achieved by SVM-RFE (70 features) as shown in Figure 3(b), the pixels selected correspond only to the background of the image rather than the digits, other methods also can-not manifest the clear structure of entire digits well. To summarize, we have introduced the notion and significance of correlated features, namely the affiliated feature group in the present paper, and have showcased how it can ben-efits the task of feature selection in general. We aspire to explore novel constraints that are suitable for the discovery of new structures in high dimensional tasks.
