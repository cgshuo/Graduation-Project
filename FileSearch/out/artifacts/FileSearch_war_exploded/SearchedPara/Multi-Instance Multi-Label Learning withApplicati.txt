 page , sports page , soccer page , etc.
 Y the set of class labels. Then the task is to learn a function f set { ( X x j  X  X  ( j = 1 , 2 ,  X  X  X  , n i ) B problems in the M IML framework can achieve better performance than solving them in existing frameworks such as multi-instance learning and multi-labe l learning. instances is associated with one class label. Formally, the task is to learn a function f instances { x ( i ) scene classification [3, 7].
 associated with a number of class labels. Formally, the task is to learn a function f from a given data set { ( x a set of labels { y ( i ) have also been successfully applied to scene classification [1]. illustrate the differences among these learning framework s in Figure 1. learning or multi-label learning as the bridge. i.e. to learn a function f function f  X  1 otherwise. The proper labels for a new example X  X  can be determined according to Y  X  = a traditional supervised learning task, i.e. to learn a func tion f a constraint specifying how to derive f i.e. to learn a function f f The proper labels for a new example X  X  can be determined according to Y  X  = f learn a function f the first solution described in Section 2, while M IML S VM works along the second solution. presented in Table 1.
 In the first step, each M IML example ( X number of multi-instance bags, i.e. { [( X bag where ( X ( x number of bags, i.e. { [( X number of bags, that is, ( X (1) , y (1) ) denotes ( X Then, from the data set a multi-instance learning function f plish the desired M IML function because f Here we use M I B OOSTING [9] to implement f tion F ( B ) minimizing the bag-level exponential loss E label, f ( B ) = 1 instance-level classifier h (  X  ) for the j th instance in bag B , and n maximizes P optimizing the exponential loss: of
F ( B ) . 3.2 M IML S VM Given ( X and  X  1 otherwise, where  X  is a function  X  : Z X Y X  X  X  1 , +1 } . The M IML S VM algorithm is presented in Table 2.
 In the first step, the X data item in  X  , i.e. X Hausdorff distance [5] to measure the distance. In detail, g iven two bags A = { a and B = { b Euclidean distance here.
 After the clustering process, we divide the data set  X  into k partitions whose medoids are M example X of structure information of the data, that is, the relationshi p between X framework which has captured more original information tha n other learning frameworks. Then, from the data set a multi-label learning function f plish the desired M IML function because f implement f is with the top (least negative) score. 4.1 Comparison with Multi-Label Learning Algorithms B M M
IML B OOST or M IML S VM . Gaussian kernel L IBSVM [2] is used to implement M L S VM , where k Step 3a of M IML B OOST is also a Gaussian kernel L IBSVM (with default parameters). A ten times of that used by M IML B OOST such that the time cost of them are comparable. 4.2 Comparison with Multi-Instance Learning Algorithms D latter has achieved excellent performance on multi-instan ce benchmark tests [10]. D boosting rounds for M IML B OOST while  X  = . 2 for M IML S VM .
 M connections, maybe we can get deeper understanding of ambig uity. Acknowledgments This work was supported by the National Science Foundation o f China (60325207, 60473046).
