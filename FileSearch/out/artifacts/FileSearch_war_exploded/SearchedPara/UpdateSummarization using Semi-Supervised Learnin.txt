 Update summarization aims to generate brief summaries of recent documents to capture new information different from earlier documents. In this paper, we propose a new method to generate the sentence similarity graph using a novel similarity measure based on Helliger distance and ap-ply semi-supervised learning on the sentence graph to select the sentences with maximum consistency and minimum re-dundancy to form the summaries. We use TAC 2011 data to evaluate our proposed method and compare it with existing baselines. The experimental results show the effectiveness of our proposed method.
 H.3.3 [ Information Storage and Retrieval ]: Information Search and Retrieval Update Summarization; Semi-Supervised Learning; Hellinger Distance
Popular online publishers and social media users produce huge amount of text data every day, so it is critical to ex-tract the most important and up-to-date information to help users quickly understand these documents. Thus update document summarization has been receiving more and more attention, which aims to generate a query-relevant summary of multiple articles, under the assumption that the user has already read the earlier articles. Update summarization pro-vides a useful way to make users stay in the know of devel-oping and evolving events. For example, in the event of the spread of the Ebola virus in 2014, the earlier articles re-ported the Ebola outbreak in West Africa, then there was c  X  news about the infected cases found in Europe and the US, and later reports showed that no new cases was diagnosed in the US after December 2014. In this event, since people highly concerned about the development of disease control, a timely updated event summary will help people understand the situation quickly and conveniently.
 The problem of update summarization was introduced in Document Understanding Conference (DUC) by National Institute of Standards and Technology (NIST) in 2007 and was a main task of the summarization track in Text Analy-sis Conference (TAC) through 2008  X  2011. Given a topic q , it is required to summarize a set of document B under the assumption that the reader has already read and sum-marized an earlier set of documents A . Both the summaries of document sets A and B should focus on the given topic and the summary of B must be the least redundant with the summary of A .

The existing research on update summarization mainly focuses on query-relevant sentence ranking, graph optimiza-tion, and model-based analysis using cosine similarity [12]. For example, Boudin et al. [1] used Maximal Marginal Rel-evance (MMR) to rank the sentences and selected the top-ranked sentences to form the summaries. Delort et al. [2] proposed a topic model to identify the novelty in the doc-ument collection. Shen et al. [11] applied a minimum dom-inating set approximation to find the most important sen-tences on the sentence similarity graph. Wan [15] proposed a co-ranking algorithm to solve the problem. Li et al. [6] proposed a complex three-level hierarchical dirichlet pro-cess model to select sentences. Wang et al. [16] proposed an incremental hierarchical clustering based summarization approach to update summaries in real time.

In this paper, we propose a new similarity measure which is based on Hellinger distance to better capture the sen-tence relationships than the Euclidean distance based co-sine similarity. We apply the MMR strategy to generate the summary for the earlier document set, and then propose a label propagation approach using the Green X  X  function to determine the sentence importance in the later document set. The redundancy in the update summary is also elimi-nated in the final selection procedures. In the experiments, we use TAC 2011 dataset to evaluate the proposed method and compare the results with existing update summarization systems.
In IR, each document is represented by a nonnegative vec-of term t in the document and m is the size of the vocab-ulary. Because x is nonnegative, we may review them as probability and normalize each document to
One of the most widely used similarities between two doc-uments x and y is the cosine similarity, which can be directly derived from Euclidean distance as follows. Assuming each document is normalized to 1 in L 2 norm: Euclidean distance between x and y is
The constant 2 here is unimportant. From Eq.(1), the Eu-clidean distance corresponds directly to the cosine similarity in Eq.(2).
Since the word-document associations are nonnegative, it is expected that they are better treated with probabilistic approaches. However Euclidean distance is generally not a good metric for dealing with probabilities. Thus in this paper, we propose a new similarity, the square-root cosine (sqrt-cos) similarity, based on Hellinger distance which is more appropriate for solving IR problems such as measuring query relevance.

The hellinger distance between probabilities x and y is defined as follows.

H ( x; y ) = since
Hellinger distance has two important properties which makes it a better distance measure in IR tasks. (a.) It is a metric because it is symmetric and satisfies triangle in-equality: (b.) Helinger distance relates closely to the widely used KL divergence (also called information gain, or relative entropy) They are special cases of the -distance between two prob-ability distributions [4, 9]
D ( x; y ) = 1 Helinger distance is a special case at = 1 = 2: while the KL divergence is the case at Therefore, Hellinger distance can be viewed as the symmet-ric middle point of KL divergence.

Assuming each document is normalized to 1 in L 1 norm:  X  i =1 x i = 1, the Hellinger distance leads naturally to the SqrtCos similarty in Eq.(7).

In recent research, there is a trend in IR is to use binary weighting instead of the traditional term frequency ( tf idf ). W e point out that Hellinger distance and SqrtCos similarity bridge between these two (extreme) situations. For exam-ple, if the frequency of word A is 4 and the frequency of word B is 1, in tf id f weighting their relative importance is 4:1. In binary weighting, their relative importance is 1:1. In Hellinger distance, their relative importance is Therefore, Hellinger distance can be alternatively viewed as a compromise between tf id f and binary weighting.
In order to utilize the advantages of Hellinger distance as discussed above, in this paper we use SqrtCos to calculate the pairwise sentence similarity to generate summaries for both the earlier document set A and the later coming set B. In order to summarize document set A , we revise Maximal Marginal Relevance (MMR) which has been successfully ap-plied in query-relevant multi-document summarization sys-tems in the following two ways: (1) The proposed SqrtCos similarity based on Hellinger distance will be used as the similarity measure. (2) The average similarity is used to de-termine the redundancy between a candidate sentence and the selected sentences. We use the average similarity instead of the maximum similarity in the original MMR for docu-ment summarization because we prefer to exclude sentences which are similar to more than one sentences in the selected sentence set.

Thus the respective incremental algorithm optimizes the following condition: where S m 1 is current selected sentence set containing m sentences, x i is a sentence in S , and x j is the candidate for the m  X  th sentence to be selected. Once we have the selected sentences from document set A , we can construct a sentence graph based on the pair wise SqrtCos similarities among sentences in A and B as shown in Figure 1. The sentences from A are labeled as 1 to represent sentences in the summary of A and 0 to represent sentences not selected. The question marks represents the sentence labels to be assigned in document set B . Once the sentence graph is constructed, the sentence selection problem can be treated as a label propagation from labeled data (i.e., sen-tences in A ) to unlabeled data (i.e., sentences in B ). In its simplest form, label propagation is like a random walk on Fi gure 1: Label Propagation on Sentence Similarity Graph. a graph [14]. There are different approaches to solve label propagation problems including using the diffusion kernel [5, 13], the harmonic nature of the diffusive function, etc. In this paper, in order to keep the coherency and consistency of the generated summaries, we emphasize the global and coherent nature of label propagation and apply the Green X  X  function of the Laplace operator to solve the problem [3, 10]. Given a graph with edge weights W , the combinatorial Laplacian is defined to be L = D  X  W; where D is the di-agonal matrix consisting of the row sums of W ; i.e., D = diag( W e ), e = (1  X  X  X  1) T . Green X  X  function for a generic graph is defined as the inverse of L = D  X  W . We construct Green X  X  function using eigenvectors of L : where v 1 ; : : : ; v n are the eigenvectors of L , 1 ; : : : ; eigenvalues of L , such that 0 = 1  X  2  X  X  X  X  X  X  n and such that the inner product of v i and v j is 1 if i = j and 0 otherwise. We assume the graph is connected (otherwise we deal with each connected component one at a time). The first eigenvector is a constant vector v 1 = e = sociated eigenvalue is 1. After discarding this zero-mode, Green X  X  function is defined as the positive definite part of L
Note that Green X  X  function can also be defined on the generalized eigenvectors of the Laplacian matrix: where 0 = 1  X  2  X  X  X  X  X  X  n are the eigenvalues and the zero-mode is again the first eigenvector u 1 = e = we have
In our sentence graph, edge weights W represent the pair-wise SqrtCos similarities among topic q , sentences in doc-ument sets A and B . The sentence selection problem is illustrated in Figure 1. Let y 0 represent the partial labels obtained from document set A , we compute the complete labels as the linear influence propagation: where G is the Green X  X  function built from the constructed sentence graph.

Once we obtain the sentence labels, we only keep the sen-tences with label  X 1 X  in document set B which indicates the sentences are relevant to the given topic and contents in the earlier document set. If the total length of the sentences la-beled to be  X 1 X  is longer than the required summary length, we will eliminate sentences which are most similar to the sentences in the summary of A and least similar to the sen-tences in B and the given query.
In the experiments, we use TAC 2011 update summariza-tion dataset for evaluating our method and comparing it with existing methods. In this dataset, there are 44 topics and for each topic there are 10 newswire articles in both document set A and B (representing the earlier collection of documents and the later documents respectively). A list of aspects for each topic is given and the task requires to generate a 100-word summary for both document set A and B . When generating the summary for document set B , the assumption is that the user has already read the earlier arti-cles. Summaries generated by human labelers are provided in this task for evaluation.
In the evaluation, we will compare the results by dif-ferent methods with the human created summaries using Rouge toolkit (version 1.5.5) [7]. it is widely applied by Document Understanding Conference(DUC) and TAC for document summarization performance evaluation. It mea-sures the quality of a summary by counting the unit over-laps between the candidate summary and a set of refer-ence summaries. Several automatic evaluation methods are implemented in ROUGE, such as ROUGE-N, ROUGE-L, ROUGE-W and ROUGE-SU. ROUGE-N is an n-gram re-call ROUGE-L uses the longest common subsequence (LCS) statistics, while ROUGE-W is based on weighted LCS and ROUGE-SU is based on skip-bigram plus unigram. Intu-itively, the higher the ROUGE scores, the more similar the two summaries.
In the experiments, we use the following widely used up-date summarization methods as the baselines.
Table 1 shows the Rouge scores of our method and the baseline methods using TAC 2011 data.
 T able 1: Update summarization performance com-parison on TAC 2011 data using ROUGE evaluation methods.
 From the results, we have the following observations. (1) The original MMR outperforms the baselines with straight-forward strategies such as Lead and Centroid. It is because MMR maximizes the relevance of the selected sentences with the topic and also reduces the redundancy among the sen-tences. (2) More advanced ranking methods like CoRank and graph-based methods like DomSet outperform MMR because they either take into consideration the consistency or utilize the overall relationships among sentences. (3) Our proposed method has the best results and significantly out-performs the original MMR because we use the Hellinger distance based similarity measure which deals with proba-bilities better. We also use label propagation with Green X  X  function into the update summary generation so that the advantages of semi-supervised learning methods can be ap-plied directly.
In this paper, we propose a new update summarization method which first uses a similarity measure based on Hel-liger distance to capture the semantics among documents and then applies a semi-supervised method using label prop-agation with Green X  X  function to generate the update sum-maries. Experiments on TAC 2011 data show the effective-ness of the proposed method. In this paper, we just use the given topic description as a single query to find related contents in the documents, and the learning problem is a binary classification problem. In the future, we can further detect the aspects in the topics and transfer the problem into multi-class semi-supervised learning problems to obtain more accurate results for better coverage and consistency. The work is partially supported by National Science Foun-dation under grant CNS-1126619, IIS-1213026, and CNS-1461926. [1] F. Boudin, M. El-Beze, and J. Torres-Moreno. The lia [2] J.-Y. Delort and E. Alfonseca. Dualsum: A [3] C. Ding, H. D. Simon, R. Jin, and T. Li. A learning [4] A. O. Hero, B. Ma, O. Michel, and J. Gorman. Alpha [5] R. I. Kondor and J. Lafferty. Diffusion kernels on [6] J. Li, S. Li, X. Wang, Y. Tian, and B. Chang. Update [7] C.-Y. Lin and E. Hovy. From single to multi-document [8] D. Radev, H. Jing, M. Stys, and D. Tam.
 [9] A. Renyi. On measures of entropy and information. [10] B. Shao, D. Wang, T. Li, and M. Ogihara. Music [11] C. Shen and T. Li. Multi-document summarization via [12] A. Singhal. Modern information retrieval: A brief [13] A. J. Smola and R. Kondor. Kernels and [14] M. Szummer and T. Jaakkola. Partially labeled [15] X. Wan. Update summarization based on co-ranking [16] D. Wang and T. Li. Document update summarization
