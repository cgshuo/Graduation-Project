
Combining multiple clusterings arises in various impor-tant data mining scenarios. However, finding a consensus clustering from multiple clusterings is a challenging task because there is no explicit correspondence between the classes from different clusterings. We present a new frame-work based on soft correspondence to directly address the correspondence problem in combining multiple clusterings. Under this framework, we propose a novel algorithm that it-eratively computes the consensus clustering and correspon-dence matrices using multiplicative updating rules. This al-gorithm provides a final consensus clustering as well as correspondence matrices that gives intuitive interpretation of the relations between the consensus clustering and each clustering from clustering ensembles. Extensive experimen-tal evaluations also demonstrate the effectiveness and po-tential of this framework as well as the algorithm for dis-covering a consensus clustering from multiple clusterings.
Clustering is a fundamental tool in unsupervised learn-ing that is used to group together similar objects [2], and has practical importance in a wide variety of applications. Recent research on data clustering increasingly focuses on cluster ensembles [15, 16, 17, 6], which seek to combine multiple clusterings of a given data set to generate a final superior clustering. It is well known that different cluster-ing algorithms or the same clustering algorithm with differ-ent parameter settings may generate very different partitions of the same data due to the exploratory nature of the cluster-ing task. Therefore, combining multiple clusterings to ben-efit from the strengths of individual clusterings offers bet-ter solutions in terms of robustness, novelty, and stability [17, 8, 15].

Distributed data mining also demands efficient methods to integrate clusterings from multiple distributed sources of features or data. For example, a cluster ensemble can be em-ployed in privacy-preserving scenarios where it is not possi-ble to centrally collect all the features for clustering analy-sis because different data sources have different sets of fea-tures and cannot share that information with each other.
Clustering ensembles also have great potential in sev-eral recently emerged data mining fields, such as relational data clustering. Relational data typically have multi-type features. For example, Web document has many different types of features including content, anchor text, URL, and hyperlink. It is difficult to cluster relational data using all multi-type features together. Clustering ensembles provide a solution to it.

Combining multiple clusterings is more challenging task than combining multiple supervised classifications since patterns are unlabeled and thus one must solve a correspon-dence problem, which is difficult due to the fact that the number and shape of clusters provided by the individual so-lutions may vary based on the clustering methods as well as on the particular view of the data presented to that method. Most approaches [15, 16, 17, 6] to combine clustering en-sembles do not explicitly solve the correspondence prob-lem. Re-labeling approach [14, 7] is an exception. However, it is not generally applicable since it makes a simplistic as-sumption of one-to-one correspondence.

In this paper, we present a new framework based on soft correspondence to directly address the correspondence problem of clustering ensembles. By the concept of soft cor-respondence, a cluster from one clustering corresponds to each cluster from another clustering with different weight. Under this framework, we define a correspondence matrix as an optimal solution to a given distance function that re-sults in a new consensus function. Based on the consen-sus function, we propose a novel algorithm that iteratively computes the consensus clustering and correspondence ma-trices using multiplicative updating rules. There are three main advantages to our approach: (1) It directly addresses the core problem of combining multiple clusterings, the cor-respondence problem, which has theoretic as well as prac-tical importance; (2) Except for a final consensus cluster-ing, the algorithm also provides correspondence matrices that give intuitive interpretation of the relations between the consensus clustering and each clustering from a clustering ensemble, which may be desirable in many application sce-narios; (3) it is simple for the algorithm to handle clustering ensembles with missing labels.

Some early works on combining multiple clusterings were based on co-association analysis, which measure the similarity between each pair of objects by the frequency they appear in the same cluster from an ensemble. Kellam et al. [13] used the co-association matrix to find a set of so-called robust clusters with the highest value of support based on object co-occurrences. Fred [9] applied a voting-type algorithm to the co-association matrix to find the fi-nal clustering. Further work by Fred and Jain [8] deter-mined the final clustering by using a hierarchical (single-link) clustering algorithm applied to the co-association ma-trix. Strehl and Ghosh proposed Cluster-Based Similarity Partitioning (CSPA) in [15], which induces a graph from a co-association matrix and clusters it using the METIS al-gorithm [11]. The main problem with co-association based methods is its high computational complexity which is quadratic in the number of data items, i.e., O ( N 2 ) .
Re-labeling approaches seek to directly solve the corre-spondence problem, which is exactly what makes combin-ing multiple clusterings difficult. Dudoit [14] applied the Hungarian algorithm to re-labeling each clustering from a given ensemble with respect to a reference clustering. Af-ter overall consistent re-labeling, voting can be applied to determining cluster membership for each data item. Dimi-triadou et al. [5] proposed a voting/merging procedure that combines clusterings pair-wise and iteratively. The corre-spondence problem is solved at each iteration and fuzzy membership decisions are accumulated during the course of merging. The final clustering is obtained by assigning each object to a derived cluster with the highest member-ship value. A re-labeling approach is not generally applica-ble since it assumes that the number of clusters in every given clustering is the same as in the target clustering.
Graph partitioning techniques have been used to solve for the clustering combination problem under different for-mulations. Metal-CLustering algorithm (MCLA) [15] for-mulates each cluster in a given ensemble as a vertex and the similarity between two clusters as an edge weight. The in-duced graph is partitioned to obtain metaclusters and the weights of data items associated with the metaclusters are used to determine the final clustering. [15] also introduced HyperGraph Partitioning algorithm (HGPA), which repre-sents each cluster as a hyperedge in a graph where the ver-tices correspond to data items. Then, a Hypergraph parti-tion algorithm, such as HMETIS [10], is applied to generate the final clustering. Fern et al. [6] proposed the Hybrid Bi-partite Graph Formulation (HBGF) to formulate both data items and clusters of the ensemble as vertices in a bipar-tite graph. A partition of this bi-partite graph partitions the data item vertices and cluster vertices simultaneously and the partition of the data items is given as the final cluster-ing.

Another common method to solve for the clustering combination problem is to transform it into a standard clus-tering task by representing the given ensemble as a new set of features and then using a clustering algorithm to produce the final clustering. Topchy et al. [16] applied the k-means algorithm in the new binary feature space which is specially transformed from cluster labels of a given ensemble. It is also shown that this procedure is equivalent to maximiz-ing the quadratic mutual information between the empirical probability distribution of labels in the consensus cluster-ing and the labels in the ensemble. In [17], a mixture model of multinomial distributions is used to do clustering in the feature space induced by cluster labels of a given ensem-ble. A final clustering is found as a solution to the corre-sponding maximum likelihood problem using the EM algo-rithm.

To summarize, the problem of combining multiple clus-terings has been approached from combinatorial, graph-based or statistical perspectives. However, there is no suf-ficient research on the core problem of combining multiple clusterings, the general correspondence problem. The main trend of the recent research is to reduce the original problem to a new clustering task which can be solved by one exist-ing clustering algorithm, such as the hierarchical clustering, graph partitioning, k-means, and the model-based cluster-ing. However, this procedure brings back the problems re-sulting from the explanatory nature of the clustering task, such as the problem of robustness. Moreover, the heuris-tic nature of this procedure makes it difficult to develop a unified and solid theoretic framework for ensemble cluster-ing [3]. In this paper, from the perspective of matrix com-putation, we aim to solve the problem of combining multi-ple clusterings by directly addressing the general correspon-dence problem.
Given a set of data points X = { x 1 , x 2 ,..., x n } , a clus-tering of these n objects into k clusters can be represented as a membership matrix M  X  R n  X  k , where M ij  X  0 and j M ij =1 , i.e., the sum of the elements in each row of M equals to 1 . M ij denotes the weight of the i th points as-sociated with the j th cluster. For a hard clustering, M is an indicator matrix, i.e., M ij =1 indicates that the i th points belongs to the j th cluster.

The re-labeling approach tries to solve for the corre-spondence problem by assuming the one-to-one correspon-dence between clusters from two clusterings. This assump-tion makes it only applicable in a special situation where the number of clusters in each given clustering is the same as in the target clustering. Even the number of clusters in two clusterings are the same, if their distributions of the clus-ters are very different and unbalanced, the one-to-one cor-respondence is not an efficient representation of the relation between the two clusterings, since it misses too much infor-mation.

We propose the concept of soft correspondence to for-mulate the relation between two clusterings. Soft corre-spondence means that a cluster of a given clustering cor-responds to every clusters in another clustering with dif-ferent weights. Hence, the corresponding relation between two clusterings may be formulated as a matrix. We call it (soft) correspondence matrix, denoted as S . S ij denotes the weight of the i th cluster of the source clustering cor-responding to the j th cluster of the target clustering and
Under the re-labeling framework, after the label corre-spondence is obtained, an  X  X e-label X  operation is applied and then the labels of two clusterings have consistent mean-ings. Similarly, under the soft correspondence framework, we also need an operation, which is based on the corre-spondence matrix, to transform the membership matrix of a source clustering into the space of the membership matrix of the target clustering to make the two membership matri-ces reach a consistent meaning. The intuitive choice of this operation is the linear transformation with the correspon-dence matrix. Let M (0) denote the membership matrix of a source clustering, M denote the membership matrix of a tar-get clustering, and S denote the correspondence matrix of M (0) with respect to M . Multiplied by S , M (0) is linearly transformed into the space of M , i.e., M (0) S is the trans-formed membership matrix that has the consistent meaning with M . Next step, we need an objective function to de-cide which correspondence matrix is optimal. The distance function for matrices is a good choice, since the smaller the distance between the target membership matrix M and the transformed membership matrix M (0) S , the more pre-cisely the correspondence matrix catches the relation be-tween M (0) and M .

We give the formal definition of the correspondence ma-trix as below.
 Definition 3.1. Given a matrix distance function d and two correspondence matrix, S  X  R k 0  X  k ,of M (0) with respect to M is the minimizer of d ( M, M (0) S ) under the constraints S ij  X  0 and j S ij =1 , where 1  X  i  X  k 0 and 1  X  j  X  k . In this paper, we adopt a widely used distance function, Euclidean distance. Therefore, the correspondence matrix of M (0) with respect to M is given as where  X  denotes Frobenius matrix norm.
 Let us illustrate the above formulation with examples. Suppose three hard clusterings for six data points are given as the following label vectors.
Let M , M (1) , and M (2) denote the membership matri-ces of the above three clusterings, respectively. Assume  X  is the target clustering. Let S (1) and S (2) denote the corre-spondence matrices of M (1) and M (2) with respect to M respectively. M , M (1) , and S (1) which is computed based on (1) are given as follows, respectively.
Examination of the label vectors reveals that there is a perfect one-to-one correspondence relationship between  X  and  X  (1) . Therefore, we expect the distance between the target membership matrix and the transformed member-ship matrix equals to 0 . Simple calculation verifies M = M (1) S (1) . From another perspective,  X  (1) is just a permuta-tion of  X  . Hence , in this situation the correspondence ma-trix S (1) is just a permutation matrix.
 Similarly, we solve (1) with M and M (2) to obtain S (2) . The M (2) ,the S (2) and the transformed membership matrix M (2) S (2) are given in the equation below.  X   X   X   X   X   X   X   X 
The correspondence matrix S (2) indicates that the clus-ter 1 in  X  (2) corresponds to the cluster 1 and cluster 2 in  X  with the same weight and the cluster 2 in  X  (2) corresponds to the cluster 3 in  X  . This is exactly the relationship between  X  (2) and  X  . By the information from the transformed mem-bership matrix M (2) S (2) (the righthand side of the above equation), the first fourth data points do not belong to clus-ter 3 and whether they belong to cluster 1 or cluster 2 can-not be determined, and the last two points belong to cluster 3. This is exactly the best information we can have by trans-forming  X  (2) into the space of  X  .
The problem of clustering ensemble can be de-scribed as follows: given a set of clusterings, number k, combine C into a final consensus cluster-ing M  X  R n  X  k using a consensus function.

Soft correspondence based on Euclidean distance pro-vides a new consensus function for clustering ensemble. Hence, we define the problem of clustering ensemble as an optimization problem below.
 Definition 4.1. Given r membership matrices, M (1)  X  tering represented by M  X  R n  X  k and r correspondence minimization of subject to constraints  X  h, i, j : S ( h ) ij  X  0 and j S Although the consensus function in (2) is not convex in M and each S ( h ) simultaneously, it is convex in M and each S ( h ) respectively. Therefore, (2) can be minimized (local minimum) by alternatively optimizing one of them and fixing the others. We derive an EM [1] style algorithm that converges to a local minimum by iteratively updating the correspondence matrices and the consensus membership matrix using a set of multiplicative updating rules [4].
To derive simple multiplicative updating rules that con-verges to a good consensus clustering, we do two modifica-tions for the consensus function (2).

First, the consensus clustering may converge to a clus-tering with unreasonably small number of clusters. Note that although the consensus clustering M  X  R n  X  k ,the number of clusters in it could be less than k .Thispro-vides the flexibility to explore the structure of the cluster-ing by automatically adjusting the number of clusters un-der given k . However, it also provides the possibility that the number of clusters deteriorates to the trivial small num-ber. We propose the column-sparseness constraint on the correspondence matrices to resolve this problem. A corre-spondence matrix of M ( h ) with respect to M is column-sparse implies that only a small number of clusters from M ( h ) significantly correspond to each cluster in M . Hence, the column-sparseness constraint forces the consensus clus-tering M to provide clusters as many as possible under a given k . Since S ( h ) ij  X  0 and j S ( h ) ij =1 ,thesumof the variation of each column of S ( h ) is a measure of the column-sparseness of S ( h ) , i.e., the greater the value of
S is. Therefore, to enforce the column-sparseness constraint, the consensus function (2), where  X   X  0 is a constant and 1
Second, it is difficult to deal with the external con-straint j S ( h ) ij =1 efficiently. Hence, we transform it to a  X  X oft X  constraint, i.e., we implicitly enforce the constraint the consensus function (2), where  X   X  0 is a constant.
Based on the above modifications, we re-define the prob-lem of clustering ensemble as follows.
 Definition 4.2. Given r membership matrices, M (1)  X  tering represented by M  X  R n  X  k and r correspondence ma-minimization of subject to constraints  X  h, i, j : S ( h ) ij  X  0 .
Taking the derivatives of f with respect to M and S ( h ) where 1  X  h  X  r , and after some algebraic manipulations, the gradients about M and S ( h ) are given as follows. Solving  X  X   X  X  =0 , the update rule for M is given as
On the other hand, directly solving  X  X   X  X  ( h ) =0 does not give a feasible update rule for S ( h ) , because the solution in-volves the computation of the inverse matrix that is usually expensive and unstable. Another choice is the gradient de-scent method, which gives the update rule as where denotes the Hadamard product of two matrices.  X  is a matrix of step size parameters. If each element of  X  is carefully chosen to be a small positive number, the update rule (7) will force the objective function (3) to be minimized at each iteration. However the choice of  X  can be very in-convenient for applications involving large data sets. There-fore, we set  X  as follows to derive the multiplicative updat-ing rules, where the division between two matrices is entrywise divi-sion (it is the same in the rest of this paper) and
Substituting (5), (8), and (9) into (7), we obtain the fol-lowing multiplicative updating rule for each S ( h ) .
Based on (6) and (10), the Soft Correspondence Ensem-ble Clustering (called SCEC) algorithm is listed in Algo-rithm 1. In Step 5 of Algorithm 1, D is computed based on (10) and is a very small positive number used to avoid di-viding by 0 .
 Algorithm 1 SCEC( M (1) ,...,M ( k r ) ,k ) 1: Initialize M, S (1) ,...,S ( r ) . 2: while convergence criterion of M is not satisfied do 3: for h =1 to r do 4: while convergence criterion of S ( h ) is not satisfied 6: end while 7: end for 9: end while
SCEC simply works as follows : First M is fixed, and each S ( h ) is updated to reduce the distance be-tween M ( h ) S ( h ) and M until S ( h ) converges; Second update M as the mean clustering of all of M ( h ) S ( h ) peat above steps until M converges.

SCEC outputs a final consensus clustering as well as cor-respondence matrices that give intuitive interpretation of the relations between the consensus clustering and each clus-tering from clustering ensembles which may be desirable in many application scenarios. For example, in most dis-tributed clustering scenarios, users from different sources not only want to get a final clustering solution but also care about the relationship between the clusterings they provide and the final clustering.

It is easy for SCEC to deal with the clustering with miss-ing labels. Suppose that the label of the i th object in the h th clustering M ( h ) is missing. We simply let M ( h ) ij = 1  X  j  X  k h , i.e., the h th clustering does not provide use-ful information to compute the final membership for the i th object, which is interpolated based on the information from other clusterings.

The computational complexity of SCEC can be shown as
O ( tnrk 2 ) , where t is the number of iterations. It is much faster than CSPA ( O ( n 2 rk ) ) [15], since n is large. SCEC has the same complexity as that of two other efficient algo-rithms, QMI based on k-means [16] and the approach based on the mixture model [17]. In general, the computational complexity of k-means is O ( tnmk ) where m is the number of features. In [16], when applying k-means to the feature space induced by a clustering ensemble, the number of fea-tures is r h =1 k h . Since k h = X ( k ) ,wehave m = X ( rk ) .
To prove SCEC is correct, we must prove that the con-sensus function (3) is non-increasing under update rules (6) and (10). It is obviously true for the update rule (6), since it is derived directly from  X  X   X  X  =0 . The multiplicative up-dating rule (10) can be viewed as a special type of gradi-ent descent method. Since  X  in (8) is not small, it might appear that there is no guarantee that the consensus func-tion is non-increasing under (10). We prove that this is not the case in the rest of this section.

Since the updating rules for all S ( h ) are the same, for convenience, we simplify the problem to the case of the en-semble with one clustering.
 Theorem 5.1. Given two non-negative matrices  X  i, j : S ij  X  0 , the objective function F ( S )= M  X  AS 2  X   X  S  X  is non-increasing under the update rule
S where t denotes the discrete time index.

To prove Theorem 5.1, we make use of the concept of the auxiliary function [1, 12]. G ( S, S t ) is an auxiliary func-tion for F ( S ) if G ( S, S t )  X  F ( S ) and G ( S, S )= F ( S ) . The auxiliary function is useful due to the following lemma. Lemma 5.2. If G is an auxiliary function, then F is non-increasing under the updating rule S
The key of the proof is to define an appropriate auxiliary function. We propose an auxiliary function for the objective function (11) in the following lemma.
 Lemma 5.3. Let U = S S S t . Then is an auxiliary function for (11) , where tr denotes the trace of a matrix.
 Proof. The objective function (11) can be rewritten as:
When S = S t ,wehave U = S . Thus G ( S, S )= F ( S ) .To show G ( S, S t )  X  F ( S ) , we compare (13) with (14) to find that it can be done by showing the following conditions. For convenience, let Q = A T A ; hence Q is a non-negative symmetric matrix. We prove (15) as follows.
 where 1  X  a  X  k and 1  X  i, j  X  k 0 . Similarly, we can prove (16), (17), and (18).
 Now we are ready to prove Theorem 5.1.
 Proof. The derivative of G ( S, S t ) with respect to S is Solving  X  X   X  X  =0 , we obtain the updating rule (12). By Lemma 5.2, F ( S ) is non-increasing under (12).
We conduct experiments on three real world data sets to demonstrate the accuracy and robustness of SCEC in com-parison with four other state-of-the-art algorithms for com-bining multiple clusterings.
Three real-world data sets from the UCI machine learn-ing repository are used in our experiments. The charac-teristics of the data sets are summarized in Table 1. IRIS is a classical data set in the pattern recognition literature. PENDIG is for pen-based recognition of handwritten dig-its and there are ten classes of roughly equal size in the data corresponding to the digits 0 to 9 . ISOLET6 is a sub-set of the ISOLET spoken letter recognition training set and it contains the instances of six classes randomly selected out of twenty six classes.

We compare SCEC with four other state-of-the-art rep-resentative algorithms. Two of them are graph partition-ing based algorithms, CSPA and MCLA [15]. The code for them is available at http://www.strehl.com. The third algo-rithm is QMI that is based on k-means [16]. The last one is based on the mixture model [17] and we call it Mixture Model based Ensemble Clustering (MMEC).

The k-means algorithm is used to generate the cluster-ing ensembles in three ways. For each data set, three types of clustering ensembles are generated as follows. The first Dataset No.of No. of No. of No. of Instances features classes clusters PENDIG 3498 16 10 (5,10,15,20) ISOLET6 1440 617 6 (3,6,9,12) is generated with Random Initiation (RI) of k-means and the number of clusters for each clustering in the ensemble is set to be the number of clusters in the consensus (tar-get) clustering. The second is generated such that the num-ber of clusters for each clustering in the ensemble is a Ran-dom Number (RN) between 2 and 2 c , where c is the true number of classes. The third is generated to simulate dis-tributed clustering scenarios such that each clustering of an ensemble is based on a data set in a Random Subspace (RS) of the original full feature space. The dimension of the sub-space for each data set is set to about a half of the dimen-sion of the full feature space, i.e., 2 , 8 , and 308 are for IRIS, PENDIG and ISOLET6, respectively.

For the number of clusters in the consensus (target) clus-tering k , we do not fix it on the true number of the classes. Since in real applications, usually we do not know the true number of classes, it is desirable to test the robustness of an algorithm to different number of clusters. The last col-umn of Table 1 reports the numbers of clusters used for each data set. For the number of combined clusterings r , we adopt r =5 , 20 , 50 for each data set. For the initial-ization of SCEC algorithm, the consensus clustering M is set as a clustering randomly chosen from the ensemble and each correspondence matrix is initialized with a randomly generated correspondence matrix.

For the evaluation criterion, we select to use an informa-tion theoretic criterion  X  the Normalized Mutual Informa-tion (NMI) criterion [15]. Treating cluster labels and class labels as random variables, NMI measures the mutual infor-mation shared by the two random variables and is normal-ized to a [0 , 1] range.
The results for each data set are presented in Table 2-10. The tables report the mean NMI from 20 independent runs of each combination of r and k . Except for the five al-gorithms, the mean NMIs for the Base Learner (BL), the k-means, are also reported in the tables.

Comparing the base learner, none of the five algorithms leads to the performance improvement over the base learner in all cases. SCEC gives performance improvement over the base learner under 77 out of 99 situations. This is the best re-sult among the five algorithms. An interesting observation is that the most situations when the algorithms fail to improve performance are the situations where the number of clusters is set to be less than the true number of the classes. The pos-sible reason is that under this situation the base learner tend to give more data points random assignments, which make the ensemble provide less useful information.

Comparing the five algorithms with each other, none of the algorithms is the absolute winner that has the best mean NMI in every situation. Each algorithm may achieve bet-ter performance under some specific conditions. For exam-ple, MCLA tends to give good performance under the true number of the classes because that provides nearly-balanced clusters. MMEC works better on a large size data set be-cause reliability of model parameter estimation is improved in this situation. SCEC is observed to be the most robust al-gorithm and it outperforms the other algorithms in most sit-uations.

However, to evaluate the overall performance strictly, di-rect observation of the data is not sufficient and we need to do statistical test on the result. We do the paired t-test on the 99 pairs of NMIs from all the tables for each pair of the al-gorithms. The p-value for each test is reported in Table 11. The ( i, j ) entry of Table 11 presents the p-value for the fol-lowing one-sided paired t-test: H 0 : the mean of the mean NMI for algorithm i equals to the mean of the mean NMI for algorithm j vs H :the mean of the mean NMI for algo-rithm i is greater than the mean of the mean NMI for algo-rithm j , i.e., if p-value in ( i, j ) entry is less than 0 . 05 ,we accept H with confidence level 0 . 95 , which means that we can make a conclusion that algorithm i outperforms algo-rithm j significantly.

By Table 11, SCEC performs significantly better than all other algorithms. The performance of CSPA is significantly worse than all others. The possible reason is that CSPA needs a large number of clusterings to provide a reliable es-timate of the co-association values. However ensembles of a very large size are less important in practice. MCLA is sig-nificantly better than MMEC and there is no significant dif-ference between MCLA and QMI. Also there is no signif-icant difference between QMI and MMEC. When compar-ing the base learner, SCEC is the only one that leads to a significant performance improvement over the base learner.
In this paper, we have proposed a new soft correspon-dence framework for combining multiple clusterings. Un-der this framework, we define a correspondence matrix as an optimal solution to a given distance function and it re-sults in a new consensus function. Based on the consen-sus function, we propose a novel algorithm SCEC that it-eratively computes the consensus clustering and the corre-spondence matrices using the multiplicative updating rules. We have shown the correctness of the SCEC algorithm the-oretically. We have also reported extensive empirical evalu-ations to demonstrate the superior effectiveness of SCEC to several well-known algorithms in the literature on combin-ing multiple clusterings. This research is supported in part by a grant from Air Force Research Laboratory (AFRL) through the award number FA8750-04-1-0234. We acknowledge the ad-vice from Dr. John Salerno at AFRL.

