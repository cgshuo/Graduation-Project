 Keywords: Clustering, kernels, learning theory. Spectral clustering methods are common graph-based approa ches to (unsupervised) clustering of data. Given a dataset of n points { x i } n by some local symmetric and non-negative similarity measur e. A common choice is a Gaussian kernel with width  X  , where k X k denotes the standard Euclidean metric in R p suggested in [1] where a ( S ) = P propose to augment this top-down approach by a bottom-up agg regation of the sub-clusters. As shown in [1] minimization of (2) is equivalent to max onal n  X  n matrix with D y
T D 1 = 0 and y graph Laplacian matrix D  X  W, also known as the Fiedler vector [4].
 We construct a simple example with only two clusters, where w e prove that the minimum of this of spectral clustering as a random walk on the graph and on the intimate connection between the bottom-up clustering approaches and may overcome some of th eir limitations. We show how use of this measure correctly clusters the examples of Section 4, w here spectral clustering fails. be rephrased -is local information sufficient for global clu stering ? to the functional (2). Consider data sampled from a mixture o f two densities in two dimensions clustering algorithms give similar results. where p x 2 &lt; 0 } with L = 8 , X  = 0 . 05 L, (  X  Gaussian ball and the rectangular strip  X  .
 Laplacian with weights W than a cut somewhere along the thin strip.
 measure (1) by 1 / 2  X  X  2 , the numerator is given by where  X  location x = x A similar calculation shows that for a horizontal cut at y = 0 , two cuts in eq. (2) have the same order of magnitude. Therefor e, if L  X  and  X  clusters by using the first three eigenvectors of W v =  X D v . Specifically, denote by {  X  D eigenvalue is  X  random walk converges to the unique equilibrium distributi on  X  equal to a so called  X  X iffusion distance X  between points on t he graph, x ,  X  Therefore, the eigenvalues and eigenvectors {  X  relaxation times and of the corresponding eigenvectors.
 limit n  X   X  ,  X   X  0 , the random walk with transition matrix M on the graph of points sampled from this density converges to a stochastic differential eq uation (SDE) converge to the eigenfunctions of the following Fokker-Pla nck operator eigenvalues are  X  are related by  X  Therefore, in this setting spectral clustering with k eigenvectors works very well. drawn from such a density is shown in fig. 2 (top left).
 2 or 3, approximately given by [20] where x min is the bottom of the deepest well, x max is the saddle point of U ( x ) , and U 00 height [20]. The corresponding eigenfunction  X  A second characteristic time is  X  between wells 1 and 2, then  X  parabolic potential of the form U ( x ) = U ( x 1 )+ U 00 In 1-D the eigenvalues and eigenfunctions are given by  X  of degree k  X  1 . The corresponding intra-well relaxation times are given b y  X  R the mean first passage time between the two smaller wells,  X  R of  X  2 =  X  process inside the wide well, see a plot of  X  separates these two clusters.
  X  ( density, as in the examples in their paper, this approach is s uccessful as it decreases  X  R also fail in the case of uniform density clusters defined sole ly by geometry (see fig. 4). with centers x i isotropic standard deviations  X  consider one large cluster with  X   X  2 =  X  3 = 0 . 5 algorithm [6] and the ZP algorithm [10] for two different wei ght vectors. Example I: Weights (  X  the smaller ones. The first few eigenvectors of M with a uniform  X  = 1 are shown in the first two rows of the figure. The second eigenvector  X  easily separates the larger cluster from the smaller ones. H owever,  X  smaller clusters, capturing the relaxation process in the l arger cluster (  X  the y -direction, hence it is not a function of the x -coordinate). In this example, only  X  therefore as expected and shown in the last row of fig. 2, the ZP algorithm clusters correctly. Example II: Weights (  X  the ZP algorithm fails to correctly cluster this data for all values of the parameter k algorithm. Needless to say, the NJW algorithm also fails to c orrectly cluster this example. Example III: Consider data { x i } uniformly sampled from a domain  X   X  R 2 , which consists of The analysis and examples of Sections 3 and 4 may suggest the u se of more than k eigenvectors a weighted graph of points and let V = S  X  ( V \ S ) be a possible partition (computed by some or reject this partition. To this end, let  X  matrix M corresponding to the full graph G . We define  X  relaxation time of this graph. Similarly,  X  we expect  X  by S and V \ S , then  X  ters while  X  then  X  S c with different approaches [21, 22].
 are (  X  the container roughly into two parts with (  X  normalized cut on the two small disks correctly separates th em giving (  X 
