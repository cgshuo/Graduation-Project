 Naive Bayes and logistic regression perform well in different regimes. While the former is a very simple generative model which is effi-cient to train and performs well empirically in many applications, the latter is a discriminative model which often achieves better ac-curacy and can be shown to outperform naive Bayes asymptotically. In this paper, we propose a novel hybrid model, partitioned logis-tic regression , which has several advantages over both naive Bayes and logistic regression. This model separates the original feature space into several disjoint feature groups. Individual models on these groups of features are learned using logistic regression and their predictions are combined using the naive Bayes principle to produce a robust final estimation. We show that our model is better both theoretically and empirically. In addition, when applying it in a practical application, email spam filtering, it improves the nor-malized AUC score at 10% false-positive rate by 28.8% and 23.6% compared to naive Bayes and logistic regression, when using the exact same training examples.
 I.2.6 [ Artificial Intelligence ]: Learning; H.4.3 [ Communications Applications ]: Electronic mail Algorithms,Theory logistic regression, naive Bayes, email spam filtering
Due to having the same linear functional form when used as a classifier, naive Bayes and logistic regression have been called a generative X  X iscriminative pair of classifiers. The former approach learns a model that estimates the joint probability, P ( X Y is the class label and X is the example feature vector. The poste-rior probability of the class, which is estimated by applying Bayes This work was done while the author was an intern at Microsoft Research.
 Copyright 2008 ACM 978-1-60558-193-4/08/08 ... $ 5.00. rule, is used to predict the most likely label. On the other hand, the latter approach tries to learn a model to directly estimate the poste-rior, P ( Y | X ) . In many applications, discriminatively trained mod-els outperform their generatively trained counterparts (e.g., [25, 19]). This empirical result is also supported from the perspective of computational learning theory [ 35]. Nevertheless, a generatively trained classifier, such as naive Bayes, still enjoys several advan-tages in practice. For example, the learning procedure is usually very simple and efficient. Also, several techniques can improve its performance and make it very competitive when compared with lo-gistic regression (e.g., [27, 19]). When the size of the training data is small, naive Bayes can even outperform logistic regression, re-gardless of whether its strong conditional independence assumption holds or not. This is due to the fact that naive Bayes can converge to its asymptotic accuracy (i.e., the model trained on infinitely number of examples) much faster than logistic regression [24].

In this paper, we introduce a novel learning method, partitioned logistic regression (PLR), which is a hybrid model that can be viewed as part generative and part discriminative. Given training examples, the PLR approach first separates the feature space into k disjoint subsets of features, which are assumed conditionally in-dependent given the class label. Correspondingly, k different mod-els are trained discriminatively using logistic regression on these subsets of features only. The final prediction is based on the mul-tiplication of the posterior probabilities estimated by these k indi-vidual models, following the conditional independence assumption assumed by the naive Bayes model.

Because the PLR model only assumes that the features of differ-ent groups are conditionally indepe ndent, but not features within the same group, it outperforms naive Bayes in our experiments on both synthetic and real datasets. Perhaps more surprisingly, we also found that the PLR model outperforms its discriminative counter-part, logistic regression, even when the conditional independence assumption does not hold. We provide theoretical evidence that suggests why this can happen and provide a set of experiments on synthetic data to illustrate the effectiveness of the PLR model. Fur-thermore, we demonstrate that, when applied to a large real dataset from the email spam filtering domain, the PLR model significantly outperforms both logistic regression and naive Bayes.
 Improved prediction accuracy is not the only advantage of the PLR model. The model is especially suitable for real-world appli-cations. The linear functional form of partitioned logistic regres-sion is fast and easy to implement. Many production systems use models of this form, which means that a PLR model could poten-tially be easily adapted for use in these systems. In fact, typically the only change needed is to replace the weights with those learned using partitioned logistic regression. Having the features separated into several groups enables one to easily tune smoothing parame-ters for different groups of features. As we will demonstrate, this can lead to further significant performance enhancements. Another advantage of partitioned logistic regression is its ability to incor-porate multiple information sources together. This property is es-pecially important when we apply the PLR model to the task of spam filtering. Although traditiona lly spam filtering is treated as a special problem of text classification, using only the email content information often provides unsatisfactory results. This is mainly due to the fact that spam is an adversarial problem and spammers can easily manipulate messages by adding  X  X ood words X  to fool the content-based spam filter [22, 21]. In this paper, we also demon-strate how the PLR model can easily incorporate information such as sender reputation and user preference in a highly scalable way to enhance the performance of a spam filter.

The rest of the paper is organized as follows. We first describe our partitioned logistic regression model in Sec. 2. Theoretical analysis and experiments on synthetical data are presented in Sec. 3 and 4 respectively, which explains why the PLR model can perform better than both naive Bayes and logistic regression. We then depict how we use our model in the task of email spam filtering, with the focus on how it combines several sources of important information, and show the experiments in Sec. 5 and 6. In addition, methods of modeling user preference given only partial information are inves-tigated in Sec. 7. Finally, we describe some related work in Sec. 8 and conclude the paper in Sec. 9.
Conceptually, our partitioned logistic regression (PLR) model can be defined as a set of classifiers that are trained by logistic re-gression using the same examples, but on different partitions of the feature space. Given a testing example, posterior probabilities are first estimated by these classifiers and the product of these values are then used to make the final prediction. Although it can be easily extended to multi-class problems, to simplify the presentation, we focus on the binary case. Formally, let Y  X  X  0 , 1 } be the label of an example X  X  X  n that consists of n features. Assume the fea-ture space can be separated into k disjoint sets according to a pre-defined partitioning. Namely, X =( x 1 1 ,  X  X  X  ,x 1 n 1 ,x  X  X  X  ,x k 1 ,  X  X  X  ,x k and n j is the number of features in the j -th group. The total number of features in this example is thus the same as the sum of the num-bers of features in these groups. That is, n = the sub-vector that consists of the j -th group of features. The origi-nal example can be represented as X = X 1 , X 2 ,  X  X  X  , X training, k linear models are learned using logistic regression on the same training examples, but each model learns on its corresponding set of features. We denote the weights and the bias term of the j -th model as W j and b j , and the posterior estimated by this individual model is: Let  X  o j =  X  P ( Y =1 | X j ) /  X  P ( Y =0 | X j ) be the posterior odds esti-matedbythe j -th model,  X  o =  X  P ( Y =1) /  X  P ( Y =0) the estimated prior odds, which can be derived using the empirical class distribu-tion in the training data. Given a testing example X , the estimated posterior odds is defined as: The label of this example is predicted as 1 when the value of Eq. 1 is larger than 1, and 0 otherwise. When this function is used for Figure 1: Features of different groups ( X 1 , X 2 ,  X  X  X  , assumed conditionally independent given the class label Y ranking, the term  X  o (1  X  k ) can be ignored since it is a constant that does not change as the testing example changes.

Although it may not seem obvious, the PLR model can be viewed as a set of individual logistic regression models, combined follow-ing the naive Bayes principle. In other words, the groups of fea-tures are assumed conditionally independent given the class label, and can be described as Eq. 2.

When the estimated odds and posteriors are correct, it can be shown that Eq. 1 is indeed the true posterior odds when this as-sumption holds. Let o be P ( Y =1) P ( Y =0) and o j be P Note that Eq. 2 is different from the conditional independence as-sumption in the naive Bayes classifier. Here only features of differ-ent groups are conditional independe nt, but not the features within the same group. The relationships among these features can be rep-resented by Fig. 1.

Although the PLR model consists of k logistic regression mod-els, in practice, it is very easy to replace a regular logistic regression model trained on all features with the PLR model, due to their iden-tical functional form. This property is especially clear through the log-odds function, which is the weighted sum in logistic regression.  X  lo ( X )=log In other words, the weights learned by k different models can be used directly as the weights of a global logistic function, and the new bias term is (1  X  k )log X  o , plus the sum of all individual bias terms.

The set of partitioned logistic regression models represents a spectrum of models with naive Bayes at one end and logistic regres-sion at the other. For example, when k =1 , the PLR model reduces to the regular logistic regression model. On the other hand, when k = n , the model is equivalent to naive Bayes modulo smooth-ing; each component logistic regression model learns the empirical posterior probability for one feature.

The PLR model enjoys several advantages over both naive Bayes and logistic regression. Although naive Bayes is a very efficient learning algorithm and has shown good performance in many ap-plications, its main weakness is the strong independence assump-tion that seldom holds in reality. In contrast, the PLR model pro-vides a principled way to control the degree of feature dependence. Features within the same group are not assumed conditionally in-dependent given the class label and can be modeled better by lo-gistic regression. More interestingly, the PLR model can also be better than the regular logistic regression model. It has been argued that with enough examples, a discriminative model such as logistic regression can achieve equal or better performance than its gener-ative counterpart (i.e., naive Bayes) asymptotically [24]. However, as we will show in Sec. 3, under some circumstances partitioned logistic regression can in fact learn a better model with fewer train-ing examples. Partitioned logistic regression also has an additional advantage that is crucial in practice  X  allowing tuning the smooth-ing priors more easily. As will be demonstrated in Sec. 6, the PLR model with the same prior for all the weights already outperforms the regular linear regression model, but by allowing different priors for individual logistic regression models, the performance improve-ment can be further increased.
When the conditional independence assumption holds, following the same argument used in [9], one can show that asymptotically (i.e., with infinitely many training examples) the PLR model per-forms no worse than the LR model. However, when this assump-tion does not hold, a discriminative classifier outperforms its gener-ative counterpart asymptotically, as long as its hypothesis space has finite VC dimension [35]. Intuitively, this is because the true dis-criminate learning algorithm has the freedom to search the whole weight space while the generative model limits the possible weights due to some assumption about the d ata. Although pa rtitioned logis-tic regression has a weaker conditional independence assumption as compared to naive Bayes, it does indeed limit its flexibility to learn the model parameters.

Inspired by [24], we argue that even though PLR may learn a worse model asymptotically, it converges faster than logistic regres-sion as the number of training examples increases. We first present a straightforward result of applying Vapnik X  X  uniform convergence bounds [35] to logistic regression.
 Theorem 1 ([24]) Let h Dis be logistic regression learning an n -dimensional wei ght vector. Then w ith high probability where m is the number of training examples,  X  ( h Dis ) is the error of the learned model and  X  ( h Dis,  X  ) is the asymptotic error.
Theorem 1 states that the sample complexity of discriminative learning (the number of examples needed to approach the asymp-totic error) is at most on the order of n , which is the VC dimen-sion of the logistic regression model. It also implies that when the weight space of the logistic regression learner is smaller, it requires fewer examples to approach its asymptotic error. Consider a com-ponent logistic regression learner and a global logistic regression learner. The former sees only the n j features of each example and learns n j +1 weights, while the latter is given the whole n features and learns the n +1 weights. Suppose the number of features used in each component logistic regression model is the same. With k components, n j = n/k , which indicates each local model only needs O ( n/k ) examples to converge, while the regular logistic re-gression needs O ( n ) examples.

In practice, the size of the training data is often insufficient for the learner to achieve the optimal error rate, especially when the feature space is large. Being able to converge faster explains why the PLR model may still outperform regular logistic regression, even when the conditional independence assumption (Eq. 2) does not hold.
In order to demonstrate the effectiveness of partitioned logistic regression, we show empirically that the PLR model can outper-form regular logistic regression (LR), using several artificial datasets. In particular, we verify two claims in this section. First, if the examples are generated from a distribution where the conditional independence assumption (Eq. 2) holds, PLR indeed outperforms LR. Second, even when the conditional independence assumption does not hold, PLR can still be better than LR, especially when the number of training examples is small.
Let Y  X  X  0 , 1 } be the class label of an example X , which con-sists of two groups of d binary features X 1 and X 2 . We generate this labeled example using the following procedure: where N ( u y ,  X  y ) is a multivariate normal distribution for a given Y . Vector  X  X is a real-valued vector which can be decomposed valued vector R =( R 1 ,R 2 ,  X  X  X  ,R d ) to a binary vector. Each ele-ment R j is first transferred to the corresponding base-2 representa-tion. The sign bit and the bits that correspond to 2 2 , 2 form the binary sub-vector for this element. Therefore, the length of the final binary vector is 6 d .

We generate the covariance matrix  X  y , which needs to be sym-metric and positive semi-definite, in the following way. A 2 d Gram matrix is first created using 2 d randomly generated vectors which can be re-written as: where A , B and C are d  X  d matrices. We then introduce a param-eter  X  to create a different matrix M and use it as the covariance matrix  X  y ,where Note that M is also positive semi-definite when 0  X   X   X  1 ,and therefore is a valid covariance matrix.

Notice that our covariance matrix construction is equivalent to setting cov (  X  X 1 ,  X  X 1 | Y ) , cov (  X  X 1 ,  X  X 2 | A ,  X B and C , respectively. Therefore, we can use the parameter  X  to control the  X  X egree X  of independence of X 1 and X 2 given Y .In particular, they are conditionally independent when  X  =0 , but not when  X &gt; 0 .When  X  =1 , they are closely related.
To simplify the notation, we assume the multivariate normal dis-tribution generates row vectors directly. Let e be a vector of d ones. We set u y = e
The Gram matrix is all possible inner products of the given vec-tors, and is therefore symmetric and positive semi-definite. Figure 2: The performance of three methods in accuracy us-ing different numbers of training examples. The two feature groups are conditionally independent in this synthetic data.
By fixing d to 20 (i.e., 240 binary features) and varying the pa-rameter  X  , we generate several synthetic datasets for experiments. Each dataset is further split for training and testing. While the num-ber of training examples may vary, we fix the number of testing ex-amples to 2,500. The averaged accuracy of 100 rounds is reported for each configuration.
We first verify the case when the conditional independence as-sumption holds. By setting  X  to zero, feature groups X 1 and are conditionally independent given Y . However, features in the same group (either X 1 or X 2 ) are not independent since the co-variance matrices that decide X 1 and X 2 are not diagonal matrices (see Eq. 6). Fig. 2 compares the performance in terms of accuracy of three different methods, naive Bayes (NB), logistic regression (LR) and partitioned logistic regression (PLR), when given differ-ent sizes of training data. As shown in the figure, when the number of training examples is small (e.g., 500), NB is better than LR. However, LR quickly outperforms NB when there is more training data and the performance gap increases as the size of training data grows. In contrast, PLR is better than both NB and LR in all these experiments, which is consistent to what we expected. Notice that all the comparisons here are statistically significant  X  .
It is interesting to know whether PLR can still perform well when the groups of features are not conditionally independent. For this set of experiments, we vary the parameter  X  from 0 to 1 and gener-ate two different sets of data. One of them contains 500 training ex-amples and the other has 2,500 training examples. Fig. 3 shows the results of LR and PLR. Although it is expected that PLR is better than LR when  X  =0 (i.e., the conditional independence assump-tion holds), it is somewhat surprising to see that PLR still outper-forms LR in many cases, especially when number of the training examples is small. This implies that LR may need more training examples to achieve its asymptotic performance compared to PLR. Note that all the comparisons are statistically significant except the difference between LR and PLR when  X  =0 . 8 and the number of training examples is 2,500.
We conduct a paired-t test when comparing two learning methods using the same training and testing data. The difference is consid-ered statistically significant when the p-value is less than 0.05. Figure 3: The performance of LR and PLR using different sizes of training data. Whether the groups of features are condition-ally independent is controlled by the parameter  X  .
Although we have shown theoretically and experimentally (on synthetic datasets) that partitioned logistic regression can perform better than both logistic regression and naive Bayes, it is more im-portant to examine how effective this model is for a real-world ap-plication, such as email spam filtering.

Spam filtering is a problem of classifying incoming messages to either spam or good, and has usually been treated as a binary text classification problem. The approach of building a spam filter as a classifier trained by machine learning algorithms (e.g., naive Bayes) using words or tokens in email as features, has been advo-cated for a decade [28]. Although various feature engineering and sophisticated learning algorithms have been proposed (e.g., [36, 19, 14]), the view that a spam filter is a special text classifier has not been changed much, and a content-based filter is still the core com-ponent in many commercial email spam filtering systems.

Unfortunately, due to the adversarial nature of the spam problem, a content-based filter is usually quite vulnerable and needs to be re-trained frequently to defend new spam attacks. By adding words that are often seen in normal messages, spammers can manipulate the content of an spam message and fool a content-based filter eas-ily [22, 21]. Because of this weakness, researchers have started to treat this problem as a more general classification problem and ex-plored other types of information instead of just the content. One recent example is the use of sender information [20]. Intuitively, if an IP address never sent spam messages in the past, the sender behind this IP is unlikely to be a spammer. In contrast, messages sent from IPs of bad reputation can be predicted as spam with high accuracy, even without checking the content of the messages. Com-pared to the content features, the IP address of the email sender is harder to spoof, which makes it a more reliable information source.
In addition to sender reputation, the recipient information can also help spam filtering. It is often assumed (at least implicitly) that there is a clear distinction on whether an email message is spam or good to a human subject. In practice, however, this as-sumption does not always hold. For example, when receiving unso-licited commercial email, two-thirds of users consider it as a good message as long as they have done business with the sender [11]. Messages belonging to this category are called  X  X ray mail X , which could reasonably be considered either spam or good [37]. The label of this type of mail depends on the individual user preference. For a spam filter to classify gray mail accurately, the user preference needs to be considered.
To build a robust spam filter, it is thus preferable to incorporate all the aforementioned information: content , sender reputation and user preference . While the natural choice is to extract features from each of these three information sources and build a classifier that learns on all features, as we will show in Sec. 6, a PLR model that treats these groups of features separately in fact performs better. In practice, a spam filter trained using labeled messages from one group of users often needs to be applied to messages sent to other users as well. With limited feedback from previously unseen users, how to adjust the filter to handle their user preferences is not trivial. In Sec. 7, we will further investigate this issue and demonstrate that the PLR model can easily incorporate partial information of user preference and still improve the filter. We compare the PLR model with logistic regression and naive Bayes experimentally on the application of email spam filtering in this section. We first describe the experimental setting , followed by the result in different configurations and evaluation metrics.
To compare the PLR model with other models, we conduct sev-eral experiments on three email datasets for spam filtering. The first one is a non-public Hotmail dataset that has been used previ-ously [36, 19]. The other two are the 2005 and 2006 TREC Spam Filtering Track datasets [6, 5], which have also been used in previ-ous works (e.g., [19]).

The Hotmail corpus is collected by polling Hotmail volunteers daily. The system randomly picks messages sent to voluntary users and asks them to label the messages as good or spam . Because the labels are given by the recipients, identical messages that are sent to different users may be labeled differently, according to their own preferences. Therefore, knowing the recipient of the email message can potentially increase the performance of the spam filter.
We use the same Hotmail collection as used in [36] for exper-iments. The training set contains 765,000 messages received be-tween July-01-2005 and Nov-30-2005. The remaining messages are split into validation and testing sets. The former consists of 30,000 messages received between Dec-01-2005 and Dec-03-2005; the latter has 120,000 messages received between Dec-04-2005 and Dec-15-2005. The features used in our system are as follows. The content features are composed of the words in the subject and body that have occurred at least three times in the training set  X  whether a word occurs in the message is used as a binary feature. The number of content features is therefore decided by the size of the vocabu-lary. The sender features are determined by the the IP address of the sender, which include the first 16 bits, the first 24 bits and the whole IP address. Finally, the user features are the recipient ids.
Other than the Hotmail data, we also use the TREC corpora, which consists of chronologically ordered email messages. De-spite the fact that the TREC corpora are arguably the largest pub-licly available email datasets for spam filtering, their sizes are still much smaller compared to the Hotmail collection. Following a similar setting in [19], for the TREC-05 corpus, we use the first 30,726 messages for training, the subsequent 10,242 messages for validation, and the remaining 51,213 messages for testing. For the TREC-06 English corpus, the initial portion of 12,606 messages are used for training. The first 4,202 messages in the remaining portion are used for validation and the rest for testing. Although the mes-sages in these corpora are real email, the way they are collected is somewhat artificial. Because private messages cannot be included, the TREC-05 corpus is a combination of Enron corpus and Spa-mAssassin corpus, and the messages in TREC-06 corpus are email crawled from the Web. In both corpora, the message labels are not given by the original mail recipients, but instead given by human annotators and some spam filters. Therefore, the user preference information is not preserved. As a result, for the experiments on the TREC corpora, only content and sender features are used and they are extracted in the same way as in the Hotmail dataset.
We show the results of four different learning methods on these three datasets. They are naive Bayes (NB), regular logistic regres-sion (LR), partitioned logistic regression with the same smoothing prior for all feature groups (PLR) a nd partitioned logistic regres-sion with different smoothing priors for different groups of fea-tures (PLR+)  X  . For naive Bayes, we use the regular multivariate Bernoulli model with Laplace smoothing (add-one smoothing). For the logistic regression components in LR, PLR and PLR+, we used SCGIS [12] as the actual training method. However, because the solution space is convex and has a global optimum, the choice of training algorithm makes relatively little difference. On the other hand, setting the right smoothing parameter (i.e., the variance of the Gaussian prior) is crucial for good empirical performance. There-fore, for each set of the experiments, we tested it with values among { 0 . 001 , 0 . 003 , 0 . 01 , 0 . 03 , 0 . 1 , 0 . 3 , 1 , 3 , 10 , 30 one according to the results on the validation set. The results of these models on the testing set are reported.

Although spam filtering is a binary classification problem, the accuracy of the filter at the 50-50 decision point is usually not a good performance metric  X  . Because the cost of losing good mail (false-positive) is much higher than receiving spam (false-negative), we present the ROC curves of different models in the low false-positive region, which is a typical method for evaluating spam filters. In addition, we also report two measures that probe the quality of the filters in this region. The first is the normal-ized AUC score [19], which is essentially the area under the ROC curve in the low false-positive region. Assume t is the target false-positive rate (FPR). The normalized AUC (denoted as AUC t the area in the section of the ROC curve, { ( FPR,TPR ):0 FPR  X  t, 0  X  TPR  X  1 } ,dividedby t . The second measure is the true-positive rate of the filter at a specific false-positive rate t (denoted as TPR@FPR= t ), which is a point on the ROC curve. When comparing methods on TPR@FPR= t ,weuseMcNemar X  X  test [7] on the classification results of two compared methods at their decision thresholds that correspond to the false-positive rate t . The difference between two methods is considered statistically significant when the p -value of the test is less than 0.05.
We first show the ROC curves of different methods on the Hot-mail dataset in Fig. 4, focusing on the low false-positive region. To make the presentation clear, we only show the results when all the feature groups are used, as well as the result of a logistic re-gression model that is trained using content features only. As we can see in the figure, adding other non-content features does im-prove the filter. When comparing two logistic regression models, LR.c versus LR.all, the latter is consistently better in most of the region, and these two curves cross at around 0.05 false-positive rate. Learning on all the features together in a single logistic re-gression model, however, is not the best way to use non-content
Note that for PLR+, we use the validation set to select the best smoothing parameter for each component logistic regression model independently , instead of trying all the combinations of parameters.
The differences in accuracy between various models on the email datasets are in fact consistent with what we observed in the exper-iments on synthetic datasets (Sec. 4). However, since accuracy is not the right measure for spam filtering, we choose not to report it. 01 TPR@FPR=0.01 Figure 4: The ROC curves for the Hotmail dataset. Symbol c means only content features are used, and all means all three types of features are used. information. When comparing LR.all and PLR.all, the PLR model achieves 0.1 higher true-positive rate than the logistic regression model consistently throughout the region. More encouragingly, by finding the best smoothing parameter for each of the individual lo-gistic regression components independently, the performance gap does increase quite substantially. This result can be clearly seen when we compare the curves of PLR+.all versus LR.all. Finally, naive Bayes seems to perform worse than logistic regression on this dataset. Even when trained on all features, it is still not as good as the logistic regression model trained on content features only.
We further investigated the impact of adding different groups of features in different learning methods. Tab. 1 shows the results in AUC 0 . 1 and TPR@FPR=0.1. The first column lists the feature groups used in the experiments and other columns present the re-sults of different methods. Since the PLR model is equivalent to the regular logistic regression model when only the content features are used, we leave the corresponding results of PLR and PLR+ empty in the table. Generally speaking, adding new groups of features tend to increase the performance for all the methods we tested. However, given the same groups of features, PLR is consistently better than LR. In addition, by allowing different smoothing param-eters for individual component logistic regression models, PLR+ is Figure 5: The ROC curves of four learning methods using both content and sender features on the TREC-06 datasets. again better than PLR in both quality measures. In contrast, naive Bayes is inferior to any other methods, which is consistent to what we have observed in Fig. 4. Notice that because the number of test-ing examples is quite large (i.e., 12,000), the differences between any two methods when using the same sets of features are statis-tically significant for the true positive-rate measure. Similarly, the differences between any two sets of features when used by the same model are also statistically significant.

The experimental results on the TREC datasets also lead to sim-ilar conclusions. For example, Fig. 5 shows the ROC curves of four different methods using both content and sender features on the TREC-06 dataset. Because this dataset is much easier than the Hotmail data as previously discovered, we focus on a lower false-positive region. Although the performance gaps are much smaller here, we can still see that PLR+ is slightly better than PLR, which is again better than the logistic regression model. Naive Bayes is still inferior to other methods on this dataset. Tab. 2 shows the results of using different feature group combinations and different learning methods in AUC 0 . 01 and TPR@FPR=0.01. As indicated by the results, we can see that using both sender and content fea-tures is indeed better than using only the content features in most of the cases. Moreover, PLR is able to use the additional sender fea-tures more effectively and performs better than LR on both TREC datasets. The performance difference between PLR and PLR+ on the TREC datasets is much smaller.

The improvement of choosing different smoothing parameters (i.e., PLR+ vs. PLR) is relatively smaller compared to what we previously observed in the Hotmail dataset. In addition, PLR+ is better than PLR in only TPR@FPR=0.01 on the TREC-05 data, and only in AUC 0 . 01 on the TREC-06 data. Notice that on the TREC-06 data, the differences in TPR@FPR=0.01 between PLR and PLR+, and two LR models using different features are not statistically sig-nificant. All other comparisons in TPR@FPR=0.01 between dif-ferent methods using the same sets of features, or the same method learned on different sets of features, are statistically significant.
Although in Sec. 6, we have observed that including user infor-mation in the model can improve spam filtering (e.g., Tab. 1 and Fig. 4), such information is not always available. As discussed earlier, a spam filter is often trained using data collected from one group of users but may need to be applied to messages sent to oth-ers. It is thus interesting to explore how we can still extend the user model in various realistic settings.

In this section, we first revisit how we incorporate user features in our PLR model and suggest alternative learning methods. We next study experimentally two different scenarios when the user information is limited and demonstrate how a modified user model can still help to improve the PLR model.
Recall that the features extracted from user information in Sec. 6 are merely the ids of the email recipients. When incorporating this information in the PLR model, the corresponding component logis-tic regression model estimates P ( Y | X u ) of each testing message, where X u is the group of user features of that particular example. Because only one of the binary features that represents the message recipient is active, this model basically predicts how likely a mes-sage received by this user is spam, without knowing any content or sender information regarding the message.

Although this user model can be learned using logistic regres-sion as usual, it is not hard to show that without regularization and the bias term, the estimation of P ( Y | X u ) can be derived using frequency counting directly: where #count ( Y =1 ,u ) and #count ( u ) are the numbers of spam messages and all messages user u receives, respectively. Eq. 7 pro-vides us a more straightforward method to build the user model. As we will see later, by augmenting different smoothing techniques, Eq. 7 can easily be revised to handle incomplete information of user preference.
In order to create scenarios where user information is limited, we split the training and testing data of the Hotmail collection in the following way. We first randomly divide all the email recipients into two user groups of roughly the same size. Messages in the original training set sent to these two groups of users are denoted as U 1 and U 2 respectively, where U 1 contains 381,091 messages and U 2 has 383,909 messages. For the experiments in this sec-tion, the training data includes all messages in U 1 and zero or some messages in U 2 . For testing, only the email in the original test set sent to the recipients in user group 2 is retained, which consists of Figure 6: The results of three models by varying the amount of training data from user group 2. 45,092 messages in total. To simplify the tests, we use only content features and user features.

The first scenario we test is when fewer messages in U 2 are avail-able for training. We generate different sets of training data by adding all the U 1 messages, along with randomly selected portions of U 2 messages in various sizes. Besides the LR and PLR+ model as described in the previous section, we also test a new model, PLR+ mixed . This model is similar to PLR+, but its content-based component model is trained using only messages in U 1 , while its user-based component model is trained using all the given train-ing messages. In other words, the PLR+ mixed model can estimate  X  P ( Y =1 | X u ) for group 2 users using the limited training data, but does not have access of the email content. Since PLR+ uses the same learning method but is given less information, its performance should be bounded by what PLR+ can achieve.

Fig. 6 shows the performance in terms of normalized AUC at 0.1 false-positive rate of these three different models in this setting. When there is no training message from U 2 , three models reduce to the same logistic regression model that only uses content features from messages in U 1 . However, when more messages from U are added for training, the regular logistic regression model can only gain limited improvement in AUC 0 . 1 , compared to the PLR+ model. Perhaps more interestingly, the performance gap between PLR+ and PLR+ mixed tends to be small, which suggests that the improvement of the PLR model mainly comes from the additional user information instead of the email content of messages in U
The second scenario assumes that the training data consists of all U 1 and U 2 messages, but all the messages from U 2 are unla-beled . An interesting question here is whether we can apply the  X  X ootstrapping X  technique in semi-supervised learning to  X  X uess X  the labels of these unlabeled messages from U 2 to derive the cor-responding user model. Suppose  X  P c is the content-based model trained using messages from U 1 , m u is the subset of unlabeled messages in U 2 that are sent to a specific user u ,and X sents the content features of the message i . We use the following formula to predict how likely a message sent to user u is spam.  X  P ( Y =1 | X u )= where  X  is the smoothing parameter.

Eq. 8 can be viewed as an enhanced version of Eq. 7. It uses a content-based model to derive the expected number of spam mes-sages received by this user, and estimates the ratio of the number of spam messages to the number of total messages, with Dirichlet smoothing. Recall that when there is no message from U 2 in the training set, the AUC 0 . 1 score is 0.540 as indicated in Fig. 6. How-ever, by including the user model provided by Eq. 8 with  X  =5 . 0 (tuned using the validation set) in the PLR+ model, the AUC score is increased to 0.566. From Fig. 6, we can see that this has the same effect as including nearly 20% of the labeled messages from U 2 for training.

Notice that Eq. 8 is just one baseline method to derive the user model when the labeled data from the specific user is unavailable. In practice, there could be better sources to collect information of user preference. For example, mos t modern email systems provide various mechanisms for users to report junk mail. By using the count of junk mail, we can estimate the number of spam messages received by this user more robustly to further improve the results.
Our PLR models can be viewed as a spectrum of hybrid gen-erative/discriminative models that range between naive Bayes and logistic regression, where every model in this family has the same linear functional form. Ng and Jordan [24] analyzed logistic re-gression and naive Bayes theoretically and concluded that although not performing better than logistic regression with enough train-ing data, naive Bayes needs much fewer examples (i.e., O (log( n )) vs. O ( n ) ,where n is the dimensionality of the weight space) to ap-proach its asymptotic performance. Following the same arguments, the PLR model may also converge to its asymptotic result faster than logistic regression, although the performance could be subop-timal compared to what logistic regression can potentially achieve.
An alternative hybrid model was proposed by Raina et al. [26] who learn component naive Bayes classifiers first and then com-bine them using the weights learned by logistic regression with leave-one-out estimate. They demonstrated that by separating fea-tures that obviously violate the conditional independence assump-tion into different groups, their hybrid model can outperform naive Bayes, but is still inferior to logistic regression on most of the benchmark datasets. Their model can be treated as a way to im-prove naive Bayes by slightly relaxing the conditional indepen-dence assumption, which can be analogous to several previous ef-forts on enhancing the prediction accuracy of naive Bayes (e.g., [2, 27, 19]). In contrast, our PLR model applies the naive Bayes as-sumption to combine multiple logistic regression models. It is not only better than naive Bayes, but also somewhat unexpectedly outperforms logistic regression on a real-world application, email spam filtering, on a very large dataset. The good empirical perfor-mance of the PLR model may also be explained following the same argument suggested by Domingos and Pazzani [9], where they ar-gued that naive Bayes can be optimal under 0-1 loss in several learning problems which do not satisfy the conditional indepen-dence assumption.

The PLR model can also be viewed as a special form of model combination, where m ultiple classifiers are learned through either different learning algorithms or di fferent samples of the training data and the final prediction is made by combining the results. There exists a considerable am ount of literature on this topic and interested readers can find a survey in, e.g., [18, 8, 3]. In the model combination paradigm, partitioned logistic regression can be cate-gorized as a model of logarithmic opinion pools in Hinton X  X  prod-ucts of experts framework [16], where the final prediction is made by averaging the estimated log-odds of individual classifiers (i.e., experts). Following the same naive Bayes assumption, Kahn pro-posed an improved version of the logarithmic opinion pools by cal-ibrating the probability estimation of the experts and also learn-ing the combining weights discriminatively [17]. Compared to the general model combination framework, the main difference in par-titioned logistic regression is that the individual models or experts are learned specifically on disjoint feature spaces. The products of experts framework has also been applied to conditional ran-dom fields (CRFs) for solving natural language problems such as part-of-speech tagging and nam ed entity recognition [32, 33]. Al-ternative methods of combining experts of CRFs have also been proposed [34]. Because CRFs are a generalization of logistic re-gression for structured output, the PLR model can thus be treated as a special case of this approach. Again, one key difference in our work is that the local experts in the PLR model are trained us-ing completely disjoint sets of features, while in their methods, the CRFs experts are learned using overlapping features. Partitioning the original feature space into separate feature sets follows a clear assumption -features belonging to differen t groups are condition-ally independent given the class. As we showed in Sec. 3, with this assumption, the PLR model may converge to its asymptotic perfor-mance faster, which is important for problems with large feature space such spam filtering and many NLP tasks.

As for the application of spam filtering, statistical approaches have been claimed more effective than rule-based systems and have been used extensively [1]. Linear classifiers, such as naive Bayes [28, 23], logistic regression [13, 36] and linear SVMs [10, 30], are espe-cially popular for this task due to their ability to handle large fea-ture spaces efficiently. Various model combination methods have also been explored. For example, Hershkop and Stolfo [15] ex-perimented with different model combination strategies for models trained using different algorithms and on different features. More traditional ensemble-like methods such as a cascade of classifiers [36] and stacking classifiers [29] have also been applied to spam filter-ing. Most work on spam filtering treats it as a regular text classifi-cation problem. The use of non-content information has been less studied. Using the sender information (i.e., the IP address sending the mail) is first studied in [20], which analyzes the whole SMTP path and uses it to enhance the filter.

Personalized email spam filtering has typically been viewed as training a model that fits better individual user X  X  mail distribution, instead of adjusting the filter to learn user preference. For example, Bickel and Scheffer [4] use a Dirichlet process model to re-sample the training data for each user and to make the distribution of this new training dataset close to the messages this user receives. How-ever, this strategy is quite expensive in computation and may not be feasible for a Web mail system that has hundreds of millions user accounts. Instead of creating new training data, Segal [31] pro-posed a method to combine a globally trained model with a model trained using only personal email. While the globally trained spam filter always outperforms the locally trained one, the combined ap-proach still gains some improvement. Notice that in both cases, the class label of an email message is assumed to be independent of the recipient of the mail. In other words, the user preference issue of the gray mail problem described previously is not handled by either of these approaches. In comparison, our personalized spam filtering system using the PLR model deploys a simple light-weight user model, which is highly scalable for practical applications.
In this paper, we present partitioned logistic regression, which is a novel hybrid model of the generative model, naive Bayes, and its discriminative counterpart, logistic regression. By assuming that features can be grouped into disjoint subsets that are conditionally independent given the class label, individual models are learned by logistic regression using only the corresponding subsets of features, and combined following the naive Bayes principle. Our model not only outperforms both naive Bayes and logistic regression in ex-periments on synthetic and real data, but also enjoys several advan-tages that make it a suitable learning method in practical applica-tions. Its identical functional form makes it easy to be used. By grouping potentially dependent features together, it is also easy to incorporate different types of information and learn a good model with fewer training examples  X  both are crucial in practice to im-prove the performance of the final application.

We demonstrate the superiority of our proposed model on the task of email spam filtering. On a large real data set, the method can easily learn a model that combines the content information and sender reputation, as well as the individual user preference. Its performance in various evaluation metrics is better than both naive Bayes and logistic regression when learning on the same set of fea-tures. Moreover, by tuning the smoothing prior of each individual model, the performance gap can even be further increased.
Our work shows a promising direction for creating hybrid gen-erative/discriminative models, and also raises several interesting questions for future research. In particular, we would like to ex-plore methods that can automatically group features, which would reduce the burden of a domain expert to decide the best feature grouping. We would also like to study whether the individual mod-els can be better combined to yield higher overall accuracy.
We thank Paul Bennet and Joshua Goodman for many helpful discussions. [1] I. Androutsopoulos, J. Koutsias, K. V. Chandrinos, and C. D. [2] P. N. Bennett. Using asymmetric distributions to improve [3] P.N.Bennett. Building Reliable Metaclassifiers for Text [4] S. Bickel and T. Scheffer. Dirichlet-enhanced spam filtering [5] G. Cormack. TREC 2006 spam track overview. In [6] G. Cormack and T. Lynam. TREC 2005 spam track [7] T. G. Dietterich. Approximate statistical test for comparing [8] T. G. Dietterich. Ensemble methods in machine learning. [9] P. Domingos and M. Pazzani. On the optimality of the simple [10] H. Drucker, D. Wu, and V. Vapnik. Support vector machines [11] D. Fallows. Spam: How it is hurting email and degrading life [12] J. Goodman. Sequential conditional generalized iterative [13] J. Goodman and W. Yih. Online discriminative spam filter [14] J. He and B. Thiesson. Asymmetric gradient boosting with [15] S. Hershkop and S. J. Stolfo. Combining email models for [16] G. Hinton. Products of experts. In Proc. of the 9th [17] J. M. Kahn. A generative bayesian model for aggregating [18] J. Kittler, M. Hatef, R. P. Duin, and J. Matas. On combining [19] A. Kolcz and W. Yih. Raising the baseline for high-precision [20] B. Leiba, J. Ossher, V. T. Rajan, R. Segal, and M. N. [21] D. Lowd and C. Meek. Adversarial learning. In KDD-2005 , [22] D. Lowd and C. Meek. Good word attacks on statistical spam [23] V. Metsis, V. Androutsopoulos, and G. Paliouras. Spam [24] A. Ng and M. Jordan. On discriminative vs. generative [25] K. Nigam, J. Lafferty, and A. McCallum. Using maximum [26] R. Raina, Y. Shen, A. Ng, and A. McCallum. Classification [27] J. Rennie, L. Shih, J. Teevan, and D. Karger. Tackling the [28] M. Sahami, S. Dumais, D. Heckerman, and E. Horvitz. A [29] G. Sakkis, I. Androutsopoulos, G. Paliouras, V. Karkaletsis, [30] D. Sculley and G. M. Wachman. Relaxed online SVMs for [31] R. Segal. Combining global and personal anti-spam filtering. [32] A. Smith, T. Cohn, and M. Osborne. Logarithmic opinion [33] A. Smith and M. Osborne. Using gazetteers in discriminative [34] C. Sutton, M. Sindelar, and A. McCallum. Reducing weight [35] V. N. Vapnik. Statistical Learning Theory . John Wiley &amp; [36] W. Yih, J. Goodman, and G. Hulten. Learning at low false [37] W. Yih, R. McCann, and A. Kolcz. Improving spam filtering
