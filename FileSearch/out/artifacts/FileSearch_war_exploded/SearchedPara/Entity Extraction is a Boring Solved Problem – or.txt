 Entity extract ion or named entity reco gnition, as it is somet imes cal led, is a known and fami liar pro b-lem. Named entity ( NE ) tagging has been the su b-ject of numero us shared -task evaluations, including the semi nal MUC 6, MUC 7 and MET evaluations, the C o NLL shared task, the SIGHAN bake -offs , and the ACE evaluations. With this track r e cord, and with comme r cial vendors now selling named -entity tagging for a fee, many natural ly consider entity extract ion to be an essentially solved problem. The pres ent paper challenges th is view .
 gers deve l oped for a speci fic corpus tend not to perfo rm well on other data sets. Kosseim and Po ibeau (2001), for one, show that the info r mal language of em ail or speech tran scriptions befu d-dles ta ggers built for journalistic text. Mi nkov et al (2005) further explore the systemat ic differen ces between journalistic and informal texts, training separat e taggers for each text source of interes t. based on su rface feat ures , it isn X  X  surprising to o b-serv e poor tagger tran sfer acro ss texts with signif i-can tly differen t styles or with unrel ated content. In this paper, we rep ort on the mo re surprising r e sult that tran sfer issues ari se even for texts with closely a ligned co n tent or closely aligned styles.
 business -rel ated texts that are, on the face of it, close in style and/or substance to the journalistic st o ries in existing NE data sets, MUC 6 in part icular. We thu s would have expected these texts to su p-port good tran sfer perfo rman ce from taggers co n-figured to the MUC task. Instead , we found the same kinds of pe r forman ce drops as Kosseim and Po ibeau had noted for informal texts. Our aim here is to shed light on th e how and why of this. mu ch to pres ent new tech nical solutions to NE re c-ognition, as to draw attention to those a s pect s of the problem that remai n unsolved. We cover two mai n thrus ts: (i) a black -box evaluation of se v eral NE taggers (co mm erci al and res earch sy s tems ); and (ii) an erro r analysis of system perfo r m ance. 2.1 Ev alua tion da ta Our evaluation data set contains three distinct se c-tions. The larg est comp onent consists of publ i cly -available financial rep orts filed with the Secu rities and Exchange Co mmi ssion ( SEC ), in pa r ticular the 2003 forms 10 -K filed by eight Fo rtune 500 co m-panies. These corporat e annual rep orts share the same subject mat ter as mu ch business news: sales, profits , acq uisitions, business strat egies and the like. They take, however, a mo re tech nical slant and are rich in acco unting jarg on. They are also longer, ranging in our study fro m 22 to 54 pages. tagger showed these SEC filings to be part icularl y hard to tag. Be cau se their sheer length and tech n i-cal emp hasis seem ed imp licat ed in this poor pe r-forman ce, we assemb led a second corpus of forty Web -hosted business stories fro m such news pr o-viders as MS -NBC , CNN Money , and Motley Fool . These stories focus on the sam e eight co m panies as our 10 -K data set, but are shorter and less tech n i-cal , thus allowing us to isolate length and tech n i-cal ity as factors in tagging bus i ness texts.
 new s stories that were select ed to closely mat ch the kind of data used in past MUC evaluations. They were drawn fro m the New York Times ( NYT ) and Wall Street Journal ( WSJ ) on -line editions, and f o-cus on cu r ren t events, thus providing one mo re comp ara ble dimension of evaluation. 1 2.2 Ev alua ted systems Five systems part icipated in our study, rep res en t-ing a ran ge of comme rci al tools and res earch pr o-t o types. Two of these are state -of -the -art hand -built systems based on rule/pattern inte r pret ers . Two are op en -source statistical sy s tems , one based on HMM s, and the other on CRF s; both were trai ned on the MUC 6 data set. The final system is our own le g acy MUC -style tagger, noted as Ariel in Table 1. E x cept as noted below, all the systems were run out of the b ox, with no adaptation to the data.
 identifying all the systems ; instead this paper r e-ports mo st res ults anonymo usly, using the names of Disney hero ines as system pseudonyms . We have, however e x posed the identi ty of our own system out of fai rness, as it benefi ted somewhat fro m ea r lier tuning to SEC forms 10 -K. 2.3 Ev alua tion metho d We attemp ted to rep licat e the proced ure used in the MUC evaluations, extending it only as required by the chara cteri stics of the taggers . The test data were format ted as in MUC 6 , and where SGML mark up ran afo ul of system I / O charact eri stics, we rem apped the data man ually, res olving, e.g. , cro s s-ing tags that may have stray ed into the output. t he MUC evaluations, we creat ed MUC 6 -comp liant answer keys (Su ndheim, 1995), and rem apped sy s-tem output to this standard . We remo ved system res ponses that were considered non -taggable in MUC ( e.g. , URL s) and conflated fine -grai ned di s-tinctions not mad e in MUC ( e.g. , rem apping cou n-try tags to location ). Sco res were assessed with the vene r able MUC scorer, which provides part ial cred it for system r e sponses that mat ch the key in type but not extent, or vice -vers a. The scorer also provides a full error analysi s, separately charact e r-izing each erro r in a system r e sponse. Table 2, overl eaf, pres ents our overall findings, aggreg ated acro ss the three primary entity types: person , organization , and location (the ENAMEX types in the MUC standard ). We genera lly did not meas ure the MUC TIMEX ( dates , times ) and NUMEX types ( mo n eys , percents ) becau se: (i) neither of the statistical systems generat e them; (ii) those sy s tems that do generat e them tend to do well; (iii) they are ove r whelmi ngly mo re frequent in the SEC data than in new s, thus skew ing res ults. Fo r co m pleteness X  sake, however, Table 2 does provide all -entity new s scores in p a ren theses for those systems that happened to generat e the full set of MUC -6 e n tities. m ents, Table 2 does not pres ent an especi ally pret ty pi c ture. Aside fro m two systems  X  runs on the MUC -like curren t events, all the scores are su b-stantially below those obtained by comp etitive MUC sy s tems , which typical ly reach ed F scores in the mi d -90s, wi th a high of F=96 at MUC -6. for SEC filings, as shown in the first block of rows in Table 2. While precision is general ly poor, r e-cal l is even worse. One reas on for this is the very fr e quent rightward s shorteni ngs of comp any names ( e.g. , fro m 3M Co rporation to the Co rp o ration ), in contras t to the left ward s shortening ( e.g., 3M ) f a-vored in new s texts. Ariel had been tuned to tag all these cas es, but the other systems only tagged a scat te r shot fract ion. To isola te the contribution of these cas es to system r e cal l erro r, we rec alculated the scores by mak ing the cas es optional. The scorer remo ves mi ssing o p tional res ponses fro m the recal l denomi nator, and as expected rec all i m-proved; see the seco nd block in Table 2 .
 ance across sy s tems was ach ieved with business new s, with scores ran ging in F=69 -80. This is a huge imp rovem ent over the gaping F= 36 -75 range we saw with SEC filings (F= 43 -75 with o p tional short names ). This confirms that length and fina n-cial jarg on are imp licat ed in the poor perfo rman ce on forms 10 -K. Noneth e less, these imp roved scores are still 15 -20 points lower than the better MUC scores. Is business language just hard to tag? MUC evaluation data yields an equivocal answer. Two systems ( Pocahontas and Ariel ) ach ieved MUC 6 -level scores; it may not be coincidental that both are next -generat ion vers ions of systems that part icipated at MUC . Of the other systems , MUC trai ned Mulan also showed substantial imp rov e-men t going fro m business news to curren t events. that were explicitly trai ned on MUC (man ually or statistical ly) did well on MUC -like data, it is disqu i-eting to see how poorly this trai ning general ized to other news texts. A finer analysis of our three data sets helps tria n-gulate the fact ors lead ing to the systemat ic pe r-forman ce differ ences shown in Table 2.
 cially stands out: as Table 3 shows, organizations are twice as prev alent in the bus i ness sources as in the MUC -like data. As organization scores gene r-ally trai l scores for pers ons and locat ions (T a ble 4), this part ly e x plains why bu siness texts are hard . plain it all. The profiles in Figure 1 show that cu r-ren t events favor government/quasi -govern men t names ( e.g., X  X o ngress, X   X  X amas X  ). They are less linguistical ly productive than the corpo rat e and quasi -corporat e names in business texts, and so are mo re amen able to being explicitly listed in name gazet teers . Florian et al (2003) note the effect iv e-ness of gazet teers for ta g ging the C o NLL corpus. refl ect a growing portion of Web -hosted texts that rel ax the journalistic editorial rules of trad itional new s sources such as the NYT or WSJ . Fo r i n-stance, our data show the same freq uent omi ssion of corporate design a tors ( e.g.  X  X nc. X  ) that Kosseim noted in informal text. Whereas news sources of reco rd will generally men tion a comp any X  X  desi g-nator at leas t once in a story, our business data fr e-quently fai l to do so at all, thus remo ving a key name -tagging cue. By traci ng the Ariel rule base, we found th at the absence of any designator was imp l i cat ed in 81% of the system X  X  recal l erro r for organizat ion nam es.
 kind of mi ssing evidence by seco nd -passing a text, propagating name men tions identified in the first 
Ta ble 3: Rel ative distribution of entity types pass to mat ching but undetect ed mentions (Mi k-heev , 1999). This strat egy runs foul, though, when the first pass produces preci sion e r rors, as these too can get propagated. Documen t length is imp licat ed in this through the great er cumu lative likelihood of mak in g an erro r on the first pass and of finding a me n tion that matches the error on the second pass. that especi ally affl icts the Fo rms 10 -K is the sim i-lari ty of names and non -names . Non -taggable product names (  X  AM D Athlon X  ) often look like l e-gitimat e subsidiari es, while valid operat ing div i-sions (  X  X  ealth Ca re X  ) are often hard to distinguish fro m generi c designations of mark et segments. What surprised us mo st in conducting this st udy was to find so obvious a tran sfer gap amo ng what a p pear to be very simi lar text sources . We were also surprised by the involvemen t in this of rel axed edit o rial standard s aro und seemi ng trivia (like the keyword  X  X nc. X  ) This suggests, for one, that cu r-ren t tec h niques remai n too dependent on skin -deep word co -occurrence features . It also suggests that the editorially pristine new s texts used in so mu ch NE res earch may be atypical ly eas y to tag.
 with editorially i nformal texts, the absence of su r-face contextual cues poses no noticeab le cha l lenge to human read ers . What cues are left, and there are man y, are sem a n tic in nature: pred icat e -arg umen t structure, select ional res trictions, organizat ion of the lexicon, etc. Recen t efforts to creat e commo n propositional banks and lex i cal ontologies may thus have mu ch to offer. Indeed , cu r ren t res earch in these areas is just beginning to trickle down to the nam e -tagging pro b lem (Mo hit &amp; Hwa, 2005). ency at the whole -documen t level. This mi ght help alleviate the kind of erro r propagation with dual -pass strat egies that part icularl y affl icts long doc u-men ts . Re cen t applicat ions of stati s tical co -refer ence mo dels are beginning to show p ro m ise (Fi nkel et al , 2005; Ji &amp; Grishman, 2005). lar cha l lenge case for tran sfer learn ing, and indeed such work as Su tton and McCal lum X  X  (2005) has looked at the nam e -tagging task fro m a tran sfer learn ing st andpoint.
 work in  X  X nsolved X  areas  X  seman tics, refer ence, and lear n ing  X  could come to play a key role in what is somet imes maligned as yesterday X  X  boring solved pro b lem.

Ta ble 4: Type subcores ( S = SEC , B =biz., M = MUC )
