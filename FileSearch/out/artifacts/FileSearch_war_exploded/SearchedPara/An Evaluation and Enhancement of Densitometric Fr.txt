 Content slicing addresses the need of adaptive systems to reuse open corpus material by converting it into re -composable information objects. However this conversion is highly dependent upon the ability to correctly fragment pages into structurally sound atomic pieces. A recently suggested approach to fragmentation, which relies on densitometric page representation, claims to achieve high accuracy and time performance. Although it has been well received within the r esearch community, a full evaluation of this approach and identification of strengths and weaknesses across a range of characteristics hasn't been performed. This paper proposes an independent evaluation of the approach with respect to granularity control, accuracy, time performance, content diversity and linguistic dependency. Moreover, this paper also provides a significant contribution to address important weaknesses discovered during the analysis, in algorithm within the context of content slicing.
 H.3.3 [ Information Systems ]: Information Search and Retrieval; Analysis, Densitometric, Fragmentation, Open -Corpus Adaptive Hypermedia Systems (AHS) have traditionally attempted to deliver dynamically adapted and personalised presentations to users through the sequencing of re -composable pieces of information [7]. However their inheren t reliance upon bespoke, proprietary content represents a major obstacle to their widespread adoption. The adaptivity that an AHS can deliver is restricted due to a lack of sufficient content in terms of volume, granularity, style and meta -dat a. In re sponse to this challenge, open corpus content (multilingual information accessible on the WWW and digital repositories) is increasingly incorporated within AHS [2]. However, web content returned by traditional Information Retrieval (IR) systems is not dire ctly suitable for re -composition. It is only available as  X  X ne size fits all X  content, with limited control over granularity, format or meta -data, which are critical requirements for re -composability [4 ]. As a result, it is extremely difficult for AHS to i ncorporate externally authored content when generating adaptive offerings. A good example of what could be achieved, if open corpus resources were fully available as re -composable content objects, is the Personal Multilingual Customer Care system developed by Steichen et a l [6]. This system leverages both corporate and user generated resources available in the wild, by integrating and re -composing these resources into single coherent presentations, meeting user specific unique information needs (based on ex pertise and/or prior interaction). However, such systems do so by manually converting targeted resources into re -composable objects using manually crafted, content specific rule -based algorithms. These techniques do not scale if the entire open web is to be tar geted as a potential resource [5]. Slicing techniques address these limitations by automatically converting open corpus resources into re -composable information objects called slices, tailored for consumption by individual AHS. However as open corpus content is very heterogeneous and generally contains unnecessary noise such as navigation bars, advertisements etc, the process of slicing (described in section 2) is highly dependent upon the ability to correctly fragment heterogeneous pages into structurally sound atomic pieces. A recently suggested approach to fragmentation, which relies on dens itometric page representation [3 ] claims to achieve high accuracy and time performance. Although it has been well received within the research community, it appea rs, to the author X  X  knowledge, that a full evaluation of this approach and identification of strengths and weaknesses across a range of characteristics critical for content slicing, such as granularity control, time performance, content type or multilingua l dependency, hasn't been performed. Since fragmentation represents a decisive step within the context of a content slicing system, an evaluation of this approach determining its suitability for slicing purposes is required.
 Contribution: This paper hence proposes i) an independent evaluation of the approach with original results in relation to granularity control, content diversity, linguistic dependency, time performance and accuracy. Moreover, ii) a significant contribution to address some weaknesses discovered during the analysis, and increase the suitability and impact of this fragmentation technique within the context of content slicing systems is provided by this paper. In order to convert open -corpus content into slices, a slicer must automatically process a diverse and large range of documents at very high speed. For the purpose of this research, the slicer framework in question [5 ] consists of a pipeline divided in four distinct components: 1) Harvesting : the first module consists in identifying and harvesting open -corpus content from various large repositories usin g focused crawling techniques [4 ]; 2) Fragmentation: Standard sections of harvested content are then fragmented into structurally sound atomic pieces (such as menus, advertisements and main articles). This phase is critical since  X  X here is an inverse relationship between the potential reusability of content and its granularity X  [4]. Maximizing the reuse potential of a news article from a previously published document, a long with original menus and advertisements, is far less adequate than reusing the article alone, de -contextualized from its original setting and at various levels of granularity ; 3) Semantic Annotations: once fra gmented, each resulting fragment is annotated with semantic label s using pre -selected algorithms; 4) Slice Creation: the fourth step finally combines the resulting fragments and annotations into standardized slice units, ready for delivery to third party s lice consumers. Fragmentation Requirements: Hence, within the context of a slicer, a fragmenter must provide the ability to process very large amount of heterogeneous pages (in the region of tens of millions) in i) multiple languages and ii) of various con tent types (forum pages, product pages, news pages...) with iii) a strong control over granularity of fragments produced, at iv) a very high speed, and v) on demand. Since the set of pages prior to slicing is unknown and hence the number of possible DOM lay out patterns is infinite, fragmentation should also occur without the need for interpreting the vi) meaning or vii) structure of tags within a page. As web pages have evolved over time, content fragmentation has become an increa singly difficult task to perform. Besides the wide variety of pages available, each individual page itself now contains an increasing set of heterogeneous elements such as user comments, advertisements, snippets previews etc. Among the wide variety of algo rithms designed for the purpose of content fragmentation, a densitometric approach [3] appeared to be the most promising with respect to slicing requirements. Its ability to fragment documents regardless of the meaning or structure of xml tags used, allows it to process vi rtually any xml-based document (requirements vi and vii ). Moreover, as it considers words as mere tokens, this would theoretically make it language and content type agnostic (requirements i and ii). Furthermore, as its fragmentation process requires no rendering, this would significantly reduce any computational costs, hence increases ti me performance (requirement iv). The aim of this section is to present the fundamental concepts related to densitometric fragm entation required for the purpose of understanding the subsequent analysis. For a more complete description, the reader is r eferred to the original paper [3 ]. Within the context of densitometric fragmentations, DOM tag tree leaf nodes are converted into a 1 density  X  (  X  between the number of tokens and the number of lines within a) A line is defined as a group of successive characte rs, with a total character number equal to an arbitrary word wrapping value Tags containing only one line of text are assigned a text density value equal to the number of tokens it possesses. Although a line might appear as an arbitrary notion, sharp ch anges in text density correspond relatively well to desired fragmentation. Densitometric fragmentation therefore consists in identifying such variations in text density and correlating them with fragment boundaries. All densitometric fragmentation algorit hms proceed in detecting these variations by considering each leaf tag text density value as one atomic block (or fragment). Each page is hence converted into a single block array. Fragmentation algorithms subsequently iterate through this array by selecti ng individual blocks and then fusing them together into larger compounded blocks to form a new block array. These iterations are performed multiple times, by fusing compounded blocks together, until final fragments are created. Densitometric algorithms dif fer upon the criteria selected for fusion as well as how adjacent blocks are selected prior to fusion. Plain fusion for instance, only considers pairs of adjacent blocks difference  X  arbitrary threshold value resulting compounded block produced is compared to the next and so on. Smooth fusion extends the previous algorithm by considering a blocks predecessor and successor blo ck text densities (block window size of 3). If these text densities are identical and higher than the blocks own density, all three blocks are fused.
 Rule -based fusions on the other hand, attempt to augment the previous algorithms by taking into account th e meaning of specific tags (titles, tables etc...) in order to infer structural composition of pages. Whenever such a tag is encountered, a block fusion or block gap is performed regardless of densitometric values. However, as taking into account the meaning of tags violates requirement v), rule based fusions represent the least desirable densitometric variation within the context of a content slicing pipeline. The purpose of this evaluation was to provide a criti cal analysis of densitometric algorithms with respect to characteristics (section 2) currently unavailable within the literature and critical for slicer pipelines. These concerns are encapsulated within the following interrogations. Can densitometric algor ithms provide control over the resulting granularity of fragments produced (section 4.2)? Is the precision of these algorithms independent of content type or language considered for fragmentation (section 4 .3, 4.4)? Finally, can such a fragmentation approach occur with similar time performances for various chosen granularities (section 4.5)? In based and iv) smooth rule -based fusion variations were implemented and tested for the purpose of this analysis. Since smooth fusions achieved very similar results with respect to plain and pure rule -based variations, these algorithms are omit ted in this paper for clarity purposes. All algorithms were evaluated over a parallel multilingual corpus of approximately 20,000 pages acquired from our commercial partner Microsoft. This corpus consisted of MS Office manuals in four different languages (English, French, German and Spanish).
 Adjusted Random Index (ARI): Accuracies were measured using the standard ARI metric [7 ]. This index measures the similarities between two clustering methods by determining the number of agreements within two vectors, u sing values ranging from 0 (no agree ment) to 1 (perfect agreement). Granularity variation of fragments was measured with respect to thr eshold each page fragmented by calculating the difference in blocks between pre and post fusion and normalizing each value according to the total number of blocks present prior to fragmentation. Values close to 100 hence correspond to very large granularities, while those closer to 0 represent small granularities. Results obtained for plain fusion algorithm (executed over the English subset of the corpus consisting of approximately 5000 pages), depict ed a direct linear increase in granularity across based variations provided granularities with a difference of 76% between smallest and largest fragments produced, rule -based variation only provided a diffe rence of 15%. This suggests non -rule -based fusions provide a much higher range of granularity with respect to their rule -based counter part. However, while rule -based fusions provide a stable standard deviation, non -rule -based fusions possess a standard de viation of up to 30% for ranging between 0.4 and 0.7. This suggests a trade off between predictability of this granularity. In other words, a rule -based approach to fragmentation will offer a very small range of granularity however most pages will possess similar granularity, while non -rule based fragmentations provide a wider range of granularity with less predictability for each fragment.
 The following ana lysis investigated any possible content type dependencies of densitometric fragmentations. Within this experiment, content types refer to 4 general page types namely, En cyclopedias, Forum, Product and News pages. Five human annotators manually annotated a sample corpus of 250 pages (randomly selected and available online) from each content type. In order to provide a comparison baseline with other studies, the paper [3 ]. Figure 1 presents the results obtained using plain fusion fragmentation across content types. As can be seen, results depict an intuitive ranking of content type accuracy by degree of editorial control. News and encyclopedia pages achieve accuracy values close to 60%, however product and in particular forum pages depict very poor accuracies in the low 20%. Subsequent examination of forum pages revealed that forum posts contained both the actual content of the post with post menus, each pos sessing respectively very high and low text densities, which densitometric fusion algorithms are designed to separate. This results in the title and menus of each post being separated from the post content which explains the poor performance of forum content over all fusion versions. Product page densitometric values depict the same pattern. Hence, this experiment strongly suggests a densitometric algorithm dependency upon the type of content being segmented, with higher accuracies obtained for pages contai ning fragments with continuous regions of similar high or low den sities rather than fragments with alternating high/low densities . A lot of care must hence be taken prior to envisaged to process.
 As pointed out in section 1, open corpus content is by nature multilingual. The ability therefore to predict the similarity and accuracy of fragments produced from a set of multilingual parallel documents represents an important slicing requirement. Hence, in this experiment the full 20,000 pages of the multilingual (English, French, German, Spanish) parallel corpora were fragmented using the plain fusion approach. Within th is corpus, each English file possesses an xml structurally identical twin. The only difference Hence, any fragmentation variations between equivalent files can only be due to linguistic differences. Figure 2, presents the results obtained by comparing each fragment produced for every parallel file set combination. Language combinations not presented within the graph were omitted for clarity purposes as they achieved very similar results t o the German w.r.t Spanish combination. Results suggest fragment similarities, although very high, will decrease in average by 2% for every 0.1 increase in Past this value, the accuracy increases again very fast, resulting in very high accuracy sensitivity for deviation of only 2% was also measured across all language combinations, which suggests a high predictability in the fragment similarity expected for given parameters. Hence, although a densitometric fusion appro ach to content fragmentat ion is linguistically agnostic, a predictable decrease in similarity across resulting language fragments must be taken into account while fragmenting open corpus content. Finally, results also reveal how the French/German language combination consistently portrayed accuracies 10% lower than others. Further experiments are currently investigating whether this difference is caused by word length distribution dependency between languages. The lower chart in Figure 3 depicts a time performance analysis carried out upon the plain fusion algorithm. As can be noticed, an increase in time necessary to process pages, for l arge granularity requirements ( unfortunately makes the algorithm very unattractive within the context of slicers since a slicer requires the ability to fragment open corpus pages at different levels of granularity and at high speed. A new greedy densitometric algorithm (algorithm 1) was hence designed in order to stabilize time performance across granularities by reducing the number of iterations necessary through the block array. This algorithm replaces a f ixed block window size with a variable one with the aim of making most fusions occur in early iterations and stabilize time performances for high expansion, including additional adjacent blocks based on l ocal densitometric value variations. This algorithm also di ffers from existing approaches by usi ng a variable threshold value a page based on densitometric values of blocks currently selected within the window. The assumption is that, in addition to driving the window expansion, adjusting fusion threshold values within specific regions of pages should increase the fragmentation accuracy. As one can observe in Figure 3, the greedy window -expanding algorithm reduces significantly the average time needed for block fusion per page for high threshold values And although it is non -rule based, this algorithm depicts very similar time performance behaviors as its rule based alter native with a performance increase of 56% in average with respect to plain fusion and up to 89% im provement for threshold values both Encyclopedia and News content types c an be observed (for Main Function: Input : block_array b[b1,..,bn ], default_ Ouput : fragmented page as compound block array b[] begin | fusedBlocks = true | while fusedBlocks | | fusedBlocks = false, index=0 | | while index &lt; b[].size | | | windowSize= Fusable_Window_Size (index,b[]) | | | if windowSize &gt;0 then | | | | fuse (index, windowSize, b[]) | | | | fusedBlocks = true | | | end | | | index++; | | end | end end Function: Fusable_Window_Size Input : index, block_array b[b1,b2,..bn], default_ Output : window size to fuse begin | | windowExpanded =true; blockMerged =0 | greedyIndex = index+blockMerged | while windowExpanded and (greedyIndex+1)&lt;= b[].size | | = avg ( | | windowExpanded = false | | densitoDifference =  X  (b[greedyIndex],b[greedyIndex+1]) | | if densitoDifference &lt; then | | | | | | blockMerged ++ | | | windowExpanded = true | | | greedyIndex = index+blockMerged | | end | end end re turn blockMergeD The analysis performed upon densitometric fragmentation algorithms confirms the suitability o f this approach with respect to its ability to fragment without the need for interpreting the meaning or structure of tags within pages. It additionally provides the ability to frag ment pages across languages with a predictable control over a wide range of granularities and cross -language accuracies. However this approach is highly content type dependent, with higher accuracies achieved for news and encyclopedia content. A significant time performance decrease for high granularity values was also discovered which makes this algorithm unattractive with in the context of a slicing system. Despite this drawback, a new greedy fusion algorithm, provides a significant increase in time performances , which stabilizes time performance across granularities and provides higher accuracies. This research is supported by the SFI (Grant 07/CE/I1142) as part of the Centre for Next Generation Localisation (www.cngl.ie ). [1] Hearst, M.A. TextTiling : Segmenting Text into Multi -[2] He nze, N. and Nejdl, W . Adaptivity in the KBS Hyperbook. [3] Kohlsch X tter, C. and Nejdl, W. A Densitometric Approach to [4] Lawless, S. Leveraging Content from Open Corpus Sources [5] Levacher, K. , Wade, V et.al . Providing Customized Reuse of [6] Steichen, B. and Wade , V. Providing Personalisation across [7] Strehl, A. Cluster Ensembles  X  A Knowledge Reuse 
